arxiv-16800-1 | The Multiscale Laplacian Graph Kernel | http://arxiv.org/abs/1603.06186 | author:Risi Kondor, Horace Pan category:stat.ML published:2016-03-20 summary:Many real world graphs, such as the graphs of molecules, exhibit structure atmultiple different scales, but most existing kernels between graphs are eitherpurely local or purely global in character. In contrast, by building ahierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLGkernels) that we define in this paper can account for structure at a range ofdifferent scales. At the heart of the MLG construction is another new graphkernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which hasthe property that it can lift a base kernel defined on the vertices of twographs to a kernel between the graphs. The MLG kernel applies such FLG kernelsto subgraphs recursively. To make the MLG kernel computationally feasible, wealso introduce a randomized projection procedure, similar to the Nystr\"ommethod, but for RKHS operators.
arxiv-16800-2 | Extracting Predictive Information from Heterogeneous Data Streams using Gaussian Processes | http://arxiv.org/abs/1603.06202 | author:Sid Ghoshal, Stephen Roberts category:q-fin.ST stat.ML published:2016-03-20 summary:Financial markets are notoriously complex environments, presenting vastamounts of noisy, yet potentially informative data. We consider the problem offorecasting financial time series from a wide range of information sourcesusing online Gaussian Processes with Automatic Relevance Determination (ARD)kernels. We measure the performance gain, quantified in terms of NormalisedRoot Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearsoncorrelation, from fusing each of four separate data domains: time seriestechnicals, sentiment analysis, options market data and broker recommendations.We show evidence that ARD kernels produce meaningful feature rankings that helpretain salient inputs and reduce input dimensionality, providing a frameworkfor sifting through financial complexity. We measure the performance gain fromfusing each domain's heterogeneous data streams into a single probabilisticmodel. In particular our findings highlight the critical value of options datain mapping out the curvature of price space and inspire an intuitive, noveldirection for research in financial prediction.
arxiv-16800-3 | RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation | http://arxiv.org/abs/1603.06208 | author:Asako Kanezaki category:cs.CV published:2016-03-20 summary:The recent popularization of depth sensors and the availability oflarge-scale 3D model databases such as ShapeNet have drawn increased attentionto 3D object recognition. Despite the convenience of using 3D models capturedoffline, we are allowed to observe only a single view of an object at once,with the exception of the use of special environments such as multi-camerastudios. This impedes the recognition of diverse objects in a real environment.If a mechanical system (or a robot) has access to multi-view models of objectsand is able to estimate the viewpoint of a currently observed object, it canrotate the object to a better view for classification. In this paper, wepropose a novel method to learn a deep convolutional neural network that bothclassifies an object and estimates the rotation path to its best view under thepredicted object category. We conduct experiments on a 3D model database aswell as a real image dataset to demonstrate that our system can achieve aneffective strategy of object rotation for category classification.
arxiv-16800-4 | Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science | http://arxiv.org/abs/1603.06212 | author:Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore category:cs.NE cs.AI cs.LG published:2016-03-20 summary:As the field of data science continues to grow, there will be anever-increasing demand for tools that make machine learning accessible tonon-experts. In this paper, we introduce the concept of tree-based pipelineoptimization for automating one of the most tedious parts of machinelearning---pipeline design. We implement an open source Tree-based PipelineOptimization Tool (TPOT) in Python and demonstrate its effectiveness on aseries of simulated and real-world benchmark data sets. In particular, we showthat TPOT can design machine learning pipelines that provide a significantimprovement over a basic machine learning analysis while requiring little to noinput nor prior knowledge from the user. We also address the tendency for TPOTto design overly complex pipelines by integrating Pareto optimization, whichproduces compact pipelines without sacrificing classification accuracy. Assuch, this work represents an important step toward fully automating machinelearning pipeline design.
arxiv-16800-5 | Flow of Information in Feed-Forward Deep Neural Networks | http://arxiv.org/abs/1603.06220 | author:Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan category:cs.IT cs.LG math.IT published:2016-03-20 summary:Feed-forward deep neural networks have been used extensively in variousmachine learning applications. Developing a precise understanding of theunderling behavior of neural networks is crucial for their efficientdeployment. In this paper, we use an information theoretic approach to studythe flow of information in a neural network and to determine how entropy ofinformation changes between consecutive layers. Moreover, using the InformationBottleneck principle, we develop a constrained optimization problem that can beused in the training process of a deep neural network. Furthermore, wedetermine a lower bound for the level of data representation that can beachieved in a deep neural network with an acceptable level of distortion.
arxiv-16800-6 | Multi-Task Cross-Lingual Sequence Tagging from Scratch | http://arxiv.org/abs/1603.06270 | author:Zhilin Yang, Ruslan Salakhutdinov, William Cohen category:cs.CL cs.LG published:2016-03-20 summary:We present a deep hierarchical recurrent neural network for sequence tagging.Given a sequence of words, our model employs deep gated recurrent units on bothcharacter and word levels to encode morphology and context information, andapplies a conditional random field layer to predict the tags. Our model is taskindependent, language independent, and feature engineering free. We furtherextend our model to multi-task and cross-lingual joint training by sharing thearchitecture and parameters. Our model achieves state-of-the-art results inmultiple languages on several benchmark tasks including POS tagging, chunking,and NER. We also demonstrate that multi-task and cross-lingual joint trainingcan improve the performance in various cases.
arxiv-16800-7 | Structured VAEs: Composing Probabilistic Graphical Models and Variational Autoencoders | http://arxiv.org/abs/1603.06277 | author:Matthew J. Johnson, David Duvenaud, Alexander B. Wiltschko, Sandeep R. Datta, Ryan P. Adams category:stat.ML published:2016-03-20 summary:We develop a new framework for unsupervised learning that composesprobabilistic graphical models with deep learning methods and combines theirrespective strengths. Our method uses graphical models to express structuredprobability distributions and recent advances from deep learning to learnflexible feature models and bottom-up recognition networks. All components ofthese models are learned simultaneously using a single objective, and wedevelop scalable fitting algorithms that can leverage natural gradientstochastic variational inference, graphical model message passing, andbackpropagation with the reparameterization trick. We illustrate this frameworkwith a new structured time series model and an application to mouse behavioralphenotyping.
arxiv-16800-8 | Multi-fidelity Gaussian Process Bandit Optimisation | http://arxiv.org/abs/1603.06288 | author:Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff Schneider, Barnabas Poczos category:stat.ML cs.AI cs.LG published:2016-03-20 summary:In many scientific and engineering applications, we are tasked with theoptimisation of an expensive to evaluate black box function $f$. Traditionalmethods for this problem assume just the availability of this single function.However, in many cases, cheap approximations to $f$ may be obtainable. Forexample, the expensive real world behaviour of a robot can be approximated by acheap computer simulation. We can use these approximations to eliminate lowfunction value regions and use the expensive evaluations to $f$ in a smallpromising region and speedily identify the optimum. We formalise this task as a\emph{multi-fidelity} bandit problem where the target function and itsapproximations are sampled from a Gaussian process. We develop a method basedon upper confidence bound techniques and prove that it exhibits precisely theabove behaviour, hence achieving better regret than strategies which ignoremulti-fidelity information. Our method outperforms such naive strategies onseveral synthetic and real experiments.
arxiv-16800-9 | Globally Normalized Transition-Based Neural Networks | http://arxiv.org/abs/1603.06042 | author:Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, Michael Collins category:cs.CL cs.LG cs.NE published:2016-03-19 summary:We introduce a globally normalized transition-based neural network model thatachieves state-of-the-art part-of-speech tagging, dependency parsing andsentence compression results. Our model is a simple feed-forward neural networkthat operates on a task-specific transition system, yet achieves comparable orbetter accuracies than recurrent models. The key insight is based on a novelproof illustrating the label bias problem and showing that globally normalizedmodels can be strictly more expressive than locally normalized models.
arxiv-16800-10 | Tensor Methods and Recommender Systems | http://arxiv.org/abs/1603.06038 | author:Evgeny Frolov, Ivan Oseledets category:cs.LG cs.IR stat.ML published:2016-03-19 summary:A substantial progress in development of new and efficient tensorfactorization techniques has led to an extensive research of theirapplicability in recommender systems field. Tensor-based recommender modelspush the boundaries of traditional collaborative filtering techniques by takinginto account a multifaceted nature of real environments, which allows toproduce more accurate, situational (e.g. context-aware, criteria-driven)recommendations. Despite the promising results, tensor-based methods are poorlycovered in existing recommender systems surveys. This survey aims to complementprevious works and provide a comprehensive overview on the subject. To the bestof our knowledge, this is the first attempt to consolidate studies from variousapplication domains in an easily readable, digestible format, which helps toget a notion of the current state of the field. We also provide a high leveldiscussion of the future perspectives and directions for further improvement oftensor-based recommendation systems.
arxiv-16800-11 | A Fractal-based CNN for Detecting Complicated Curves in AFM Images | http://arxiv.org/abs/1603.06036 | author:Hongteng Xu, Junchi Yan, Nils Persson, Hongyuan Zha category:cs.CV published:2016-03-19 summary:Convolutional neural networks (CNNs) have been widely used in computervision, including low-and middle-level vision problems such as contourdetection and image reconstruction. In this paper, we propose a novelfractal-based CNN model for the problem of complicated curve detection,providing a geometric interpretation of CNN-based image model leveraging localfractal analysis. Utilizing the notion of local self-similarity, we develop alocal fractal model for images. A curve detector is designed based on themodel, consisting of an orientation-adaptive filtering process to enhance thelocal response along a certain orientation. This is followed by apost-processing step to preserve local invariance of the fractal dimension ofimage. We show the iterative framework of such an adaptive filtering processcan be re-instantiated approximately via a CNN model, the nonlinear processinglayer of which preserves fractal dimension approximately and the convolutionlayer achieves orientation enhancement. We demonstrate our fractal-based CNNmodel on the challenging task for detecting complicated curves from thetexture-like images depicting microstructures of materials obtained by atomicforce microscopy (AFM).
arxiv-16800-12 | L0-norm Sparse Graph-regularized SVD for Biclustering | http://arxiv.org/abs/1603.06035 | author:Wenwen Min, Juan Liu, Shihua Zhang category:cs.LG stat.ML published:2016-03-19 summary:Learning the "blocking" structure is a central challenge for high dimensionaldata (e.g., gene expression data). Recently, a sparse singular valuedecomposition (SVD) has been used as a biclustering tool to achieve this goal.However, this model ignores the structural information between variables (e.g.,gene interaction graph). Although typical graph-regularized norm canincorporate such prior graph information to get accurate discovery and betterinterpretability, it fails to consider the opposite effect of variables withdifferent signs. Motivated by the development of sparse coding andgraph-regularized norm, we propose a novel sparse graph-regularized SVD as apowerful biclustering tool for analyzing high-dimensional data. The key of thismethod is to impose two penalties including a novel graph-regularized norm($\pmb{u}\pmb{L}\pmb{u}$) and $L_0$-norm ($\\pmb{u}\_0$) on singularvectors to induce structural sparsity and enhance interpretability. We designan efficient Alternating Iterative Sparse Projection (AISP) algorithm to solveit. Finally, we apply our method and related ones to simulated and real data toshow its efficiency in capturing natural blocking structures.
arxiv-16800-13 | A Survey of Stealth Malware: Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions | http://arxiv.org/abs/1603.06028 | author:Ethan Rudd, Andras Rozsa, Manuel Gunther, Terrance Boult category:cs.CR cs.CV published:2016-03-19 summary:Development of generic and autonomous anti-malware solutions is becomingincreasingly vital as the deployment of stealth malware continues to increaseat an alarming rate. In this paper, we survey malicious stealth technologies aswell as existing autonomous countermeasures. Our findings suggest that whilemachine learning offers promising potential for generic and autonomoussolutions, both at the network level and at the host level, several flawedassumptions inherent to most recognition algorithms prevent a direct mappingbetween the stealth malware recognition problem and a machine learningsolution. The most notable of these flawed assumptions is the closed worldassumption: that no sample belonging to a class outside of a static trainingset will appear at query time. We present a formalized adaptive open worldframework for stealth malware recognition, relating it mathematically toresearch from other machine learning domains.
arxiv-16800-14 | Fast DPP Sampling for Nyström with Application to Kernel Methods | http://arxiv.org/abs/1603.06052 | author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:cs.LG published:2016-03-19 summary:The Nystr\"om method has long been popular for scaling up kernel methods.However, successful use of Nystr\"om depends crucially on the selectedlandmarks. We consider landmark selection by using a Determinantal PointProcess (DPP) to tractably select a diverse subset from the columns of an inputkernel matrix. We prove that the landmarks selected using DPP sampling enjoyguaranteed error bounds; subsequently, we illustrate impact of DPP-sampledlandmarks on kernel ridge regression. Moreover, we show how to efficientlysample from a DPP in linear time using a fast mixing (under certainconstraints) Markov chain, which makes the overall procedure practical.Empirical results support our theoretical analysis: DPP-based landmarkselection shows performance superior to existing approaches.
arxiv-16800-15 | DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout | http://arxiv.org/abs/1603.06060 | author:Abhijit Guha Roy, Debdoot Sheet category:cs.CV cs.LG published:2016-03-19 summary:Domain adaptation deals with adapting behaviour of machine learning basedsystems trained using samples in source domain to their deployment in targetdomain where the statistics of samples in both domains are dissimilar. The taskof directly training or adapting a learner in the target domain is challengedby lack of abundant labeled samples. In this paper we propose a technique fordomain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN)performed in two stages: (i) unsupervised weight adaptation using systematicdropouts in mini-batch training, (ii) supervised fine-tuning with limitednumber of labeled samples in target domain. We experimentally evaluateperformance in the problem of retinal vessel segmentation where the SAE-DNN istrained using large number of labeled samples in the source domain (DRIVEdataset) and adapted using less number of labeled samples in target domain(STARE dataset). The performance of SAE-DNN measured using $logloss$ in sourcedomain is $0.19$, without and with adaptation are $0.40$ and $0.18$, and $0.39$when trained exclusively with limited samples in target domain. The area underROC curve is observed respectively as $0.90$, $0.86$, $0.92$ and $0.87$. Thehigh efficiency of vessel segmentation with DASA strongly substantiates ourclaim.
arxiv-16800-16 | Improving Hypernymy Detection with an Integrated Path-based and Distributional Method | http://arxiv.org/abs/1603.06076 | author:Vered Shwartz, Yoav Goldberg, Ido Dagan category:cs.CL published:2016-03-19 summary:Detecting hypernymy relations is a key task in NLP, which is addressed in theliterature using two complementary approaches. Distributional methods, whosesupervised variants are the current best performers, and path-based methods whoreceive less research attention. We suggest an improved path-based algorithm,in which the dependency paths are encoded using a recurrent neural network, andachieve results comparable to distributional methods. We then extend theapproach to integrate both path-based and distributional signals, significantlyimproving the state-of-the-art on this task.
arxiv-16800-17 | Deep Shading: Convolutional Neural Networks for Screen-Space Shading | http://arxiv.org/abs/1603.06078 | author:Oliver Nalbach, Elena Arabadzhiyska, Dushyant Mehta, Hans-Peter Seidel, Tobias Ritschel category:cs.GR cs.LG I.3.7; I.2.6 published:2016-03-19 summary:In computer vision, Convolutional Neural Networks (CNNs) have recentlyachieved new levels of performance for several inverse problems where RGB pixelappearance is mapped to attributes such as positions, normals or reflectance.In computer graphics, screen-space shading has recently increased the visualquality in interactive image synthesis, where per-pixel attributes such aspositions, normals or reflectance of a virtual 3D scene are converted into RGBpixel appearance, enabling effects like ambient occlusion, indirect light,scattering, depth-of-field, motion blur, or anti-aliasing. In this paper weconsider the diagonal problem: synthesizing appearance from given per-pixelattributes using a CNN. The resulting Deep Shading simulates all screen-spaceeffects as well as arbitrary combinations thereof at competitive quality andspeed while not being programmed by human experts but learned from exampleimages.
arxiv-16800-18 | Large scale near-duplicate image retrieval using Triples of Adjacent Ranked Features (TARF) with embedded geometric information | http://arxiv.org/abs/1603.06093 | author:Sergei Fedorov, Olga Kacher category:cs.CV published:2016-03-19 summary:Most approaches to large-scale image retrieval are based on the constructionof the inverted index of local image descriptors or visual words. A search insuch an index usually results in a large number of candidates. This list ofcandidates is then re-ranked with the help of a geometric verification, using aRANSAC algorithm, for example. In this paper we propose a featurerepresentation, which is built as a combination of three local descriptors. Itallows one to significantly decrease the number of false matches and to shortenthe list of candidates after the initial search in the inverted index. Thiscombination of local descriptors is both reproducible and highlydiscriminative, and thus can be efficiently used for large-scale near-duplicateimage retrieval.
arxiv-16800-19 | Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation | http://arxiv.org/abs/1603.06098 | author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV published:2016-03-19 summary:We introduce a new loss function for the weakly-supervised training ofsemantic image segmentation models based on three guiding principles: to seedwith weak location cues, to expand objects based on the information about whichclasses can occur, and to constrain the segmentations to coincide with imageboundaries. We show experimentally that training a deep convolutional neuralnetwork using the proposed loss function leads to substantially bettersegmentations than previous state-of-the-art methods on the challenging PASCALVOC 2012 dataset. We furthermore give insight into the working mechanism of ourmethod by a detailed experimental study that illustrates how the segmentationquality is affected by each term of the proposed loss function as well as theircombinations.
arxiv-16800-20 | How Transferable are Neural Networks in NLP Applications? | http://arxiv.org/abs/1603.06111 | author:Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE published:2016-03-19 summary:Transfer learning is aimed to make use of valuable knowledge in a sourcedomain to help the model performance in a target domain. It is particularlyimportant to neural networks because neural models are very likely to beoverfitting. In some fields like image processing, many studies have shown theeffectiveness of neural network-based transfer learning. For neural NLP,however, existing studies have only casually applied transfer learning, andconclusions are inconsistent. In this paper, we conduct a series of empiricalstudies and provide an illuminating picture on the transferability of neuralnetworks in NLP.
arxiv-16800-21 | Tree-to-Sequence Attentional Neural Machine Translation | http://arxiv.org/abs/1603.06075 | author:Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL published:2016-03-19 summary:Most of the existing neural machine translation (NMT) models focus on theconversion of sequential data and do not directly take syntax intoconsideration. We propose a novel end-to-end syntactic NMT model, extending asequence-to-sequence model with the source-side phrase structure. Our model hasan attention mechanism that enables the decoder to generate a translated wordwhile softly aligning it with phrases as well as words of the source sentence.Experimental results on the WAT'15 English-to-Japanese dataset demonstrate thatour proposed model outperforms sequence-to-sequence attentional NMT models andcompares favorably with the state-of-the-art tree-to-string SMT system.
arxiv-16800-22 | A Fast Unified Model for Parsing and Sentence Understanding | http://arxiv.org/abs/1603.06021 | author:Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, Christopher Potts category:cs.CL published:2016-03-19 summary:Tree-structured neural networks exploit valuable syntactic parse informationas they interpret the meanings of sentences. However, they suffer from two keytechnical problems that make them slow and unwieldy for large-scale NLP tasks:they can only operate on parsed sentences and they do not directly supportbatched computation. We address these issues by introducing the Stack-augmentedParser-Interpreter Neural Network (SPINN), which combines parsing andinterpretation within a single tree-sequence hybrid model by integratingtree-structured sentence interpretation into the linear sequential structure ofa shift-reduce parser. Our model supports batched computation for a speedup ofup to 25x over other tree-structured models, and its integrated parser allowsit to operate on unparsed data with little loss of accuracy. We evaluate it onthe Stanford NLI entailment task and show that it significantly outperformsother sentence-encoding models.
arxiv-16800-23 | Stochastic Variance Reduction for Nonconvex Optimization | http://arxiv.org/abs/1603.06160 | author:Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG cs.NE stat.ML published:2016-03-19 summary:We study nonconvex finite-sum problems and analyze stochastic variancereduced gradient (SVRG) methods for them. SVRG and related methods haverecently surged into prominence for convex optimization given their edge overstochastic gradient descent (SGD); but their theoretical analysis almostexclusively assumes convexity. In contrast, we prove non-asymptotic rates ofconvergence (to stationary points) of SVRG for nonconvex optimization, and showthat it is provably faster than SGD and gradient descent. We also analyze asubclass of nonconvex problems on which SVRG attains linear convergence to theglobal optimum. We extend our analysis to mini-batch variants of SVRG, showing(theoretical) linear speedup due to mini-batching in parallel settings.
arxiv-16800-24 | Sentence Pair Scoring: Towards Unified Framework for Text Comprehension | http://arxiv.org/abs/1603.06127 | author:Petr Baudiš, Jan Pichl, Tomáš Vyskočil, Jan Šedivý category:cs.CL cs.AI cs.LG cs.NE published:2016-03-19 summary:We review the task of Sentence Pair Scoring, popular in the literature invarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. acomponent of Memory Networks. We argue that all such tasks are similar from the model perspective andpropose new baselines by comparing the performance of common IR metrics andpopular convolutional, recurrent and attention-based neural models across manySentence Pair Scoring tasks and datasets. We discuss the problem of evaluatingrandomized models, propose a statistically grounded methodology, and attempt toimprove comparisons by releasing new datasets that are much harder than some ofthe currently used well explored benchmarks. We introduce a unified open sourcesoftware framework with easily pluggable models and tasks, which enables us toexperiment with multi-task reusability of trained sentence model. We set a newstate-of-art in performance on the Ubuntu Dialogue dataset.
arxiv-16800-25 | Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings | http://arxiv.org/abs/1603.06067 | author:Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL published:2016-03-19 summary:We present a novel method for jointly learning compositional andnon-compositional phrase embeddings by adaptively weighting both types ofembeddings using a compositionality scoring function. The scoring function isused to quantify the level of compositionality of each phrase, and theparameters of the function are jointly optimized with the objective forlearning phrase embeddings. In experiments, we apply the adaptive jointlearning method to the task of learning embeddings of transitive verb phrases,and show that the compositionality scores have strong correlation with humanratings for verb-object compositionality, substantially outperforming theprevious state of the art. Moreover, our embeddings improve upon the previousbest model on a transitive verb disambiguation task. We also show that a simpleensemble technique further improves the results for both tasks.
arxiv-16800-26 | Buried object detection using handheld WEMI with task-driven extended functions of multiple instances | http://arxiv.org/abs/1603.06121 | author:Matthew Cook, Alina Zare, Dominic Ho category:cs.CV published:2016-03-19 summary:Many effective supervised discriminative dictionary learning methods havebeen developed in the literature. However, when training these algorithms,precise ground-truth of the training data is required to provide very accuratepoint-wise labels. Yet, in many applications, accurate labels are not alwaysfeasible. This is especially true in the case of buried object detection inwhich the size of the objects are not consistent. In this paper, a new multipleinstance dictionary learning algorithm for detecting buried objects using ahandheld WEMI sensor is detailed. The new algorithm, Task Driven ExtendedFunctions of Multiple Instances, can overcome data that does not have veryprecise point-wise labels and still learn a highly discriminative dictionary.Results are presented and discussed on measured WEMI data.
arxiv-16800-27 | The Computational Power of Dynamic Bayesian Networks | http://arxiv.org/abs/1603.06125 | author:Joshua Brulé category:cs.AI stat.ML published:2016-03-19 summary:This paper considers the computational power of constant size, dynamicBayesian networks. Although discrete dynamic Bayesian networks are no morepowerful than hidden Markov models, dynamic Bayesian networks with continuousrandom variables and discrete children of continuous parents are capable ofperforming Turing-complete computation. With modified versions of existingalgorithms for belief propagation, such a simulation can be carried out in realtime. This result suggests that dynamic Bayesian networks may be more powerfulthan previously considered. Relationships to causal models and recurrent neuralnetworks are also discussed.
arxiv-16800-28 | Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks | http://arxiv.org/abs/1603.06129 | author:Sahil Bhatia, Rishabh Singh category:cs.PL cs.AI cs.LG cs.SE published:2016-03-19 summary:We present a method for automatically generating repair feedback for syntaxerrors for introductory programming problems. Syntax errors constitute one ofthe largest classes of errors (34%) in our dataset of student submissionsobtained from a MOOC course on edX. The previous techniques for generatingautomated feed- back on programming assignments have focused on functionalcorrectness and style considerations of student programs. These techniquesanalyze the program AST of the program and then perform some dynamic andsymbolic analyses to compute repair feedback. Unfortunately, it is not possibleto generate ASTs for student pro- grams with syntax errors and therefore theprevious feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that usesRecurrent neural networks (RNNs) to model syntactically valid token sequences.Our approach is inspired from the recent work on learning language models fromBig Code (large code corpus). For a given programming assignment, we firstlearn an RNN to model all valid token sequences using the set of syntacticallycorrect student submissions. Then, for a student submission with syntax errors,we query the learnt RNN model with the prefix to- ken sequence to predict tokensequences that can fix the error by either replacing or inserting the predictedtoken sequence at the error location. We evaluate our technique on over 14, 000student submissions with syntax errors. Our technique can completely re- pair31.69% (4501/14203) of submissions with syntax errors and in addition partiallycorrect 6.39% (908/14203) of the submissions.
arxiv-16800-29 | Evolving Shepherding Behavior with Genetic Programming Algorithms | http://arxiv.org/abs/1603.06141 | author:Joshua Brulé, Kevin Engel, Nick Fung, Isaac Julien category:cs.AI cs.NE published:2016-03-19 summary:We apply genetic programming techniques to the `shepherding' problem, inwhich a group of one type of animal (sheep dogs) attempts to control themovements of a second group of animals (sheep) obeying flocking behavior. Ourgenetic programming algorithm evolves an expression tree that governs themovements of each dog. The operands of the tree are hand-selected features ofthe simulation environment that may allow the dogs to herd the sheepeffectively. The algorithm uses tournament-style selection, crossoverreproduction, and a point mutation. We find that the evolved solutionsgeneralize well and outperform a (naive) human-designed algorithm.
arxiv-16800-30 | Adaptive coherence estimator (ACE) for explosive hazard detection using wideband electromagnetic induction (WEMI) | http://arxiv.org/abs/1603.06140 | author:Brendan Alvey, Alina Zare, Matthew Cook, Dominic K. Ho category:cs.CV published:2016-03-19 summary:The adaptive coherence estimator (ACE) estimates the squared cosine of theangle between a known target vector and a sample vector in a whitenedcoordinate space. The space is whitened according to an estimation of thebackground statistics, which directly effects the performance of the statisticas a target detector. In this paper, the ACE detection statistic is used todetect buried explosive hazards with data from a Wideband ElectromagneticInduction (WEMI) sensor. Target signatures are based on a dictionary definedusing a Discrete Spectrum of Relaxation Frequencies (DSRF) model. Results aresummarized as a receiver operator curve (ROC) and compared to other leadingmethods.
arxiv-16800-31 | A Persona-Based Neural Conversation Model | http://arxiv.org/abs/1603.06155 | author:Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan category:cs.CL published:2016-03-19 summary:We present persona-based models for handling the issue of speaker consistencyin neural response generation. A speaker model encodes personas in distributedembeddings that capture individual characteristics such as backgroundinformation and speaking style. A dyadic speaker-addressee model capturesproperties of interactions between two interlocutors. Our models yieldqualitative performance improvements in both perplexity and BLEU scores overbaseline sequence-to-sequence models, with similar gain in speaker consistencyas measured by human judges.
arxiv-16800-32 | Generating Natural Questions About an Image | http://arxiv.org/abs/1603.06059 | author:Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Larry Zitnick, Margaret Mitchell, Xiaodong He, Lucy Vanderwende category:cs.CL cs.AI cs.CV published:2016-03-19 summary:There has been an explosion of work in the vision & language community duringthe past few years from image captioning to video transcription, and answeringquestions about images. These tasks focus on literal descriptions of the image.To move beyond the literal, we choose to explore how questions about an imageoften address abstract events that the objects evoke. In this paper, weintroduce the novel task of 'Visual Question Generation (VQG)', where thesystem is tasked with asking a natural and engaging question when shown animage. We provide three datasets which cover a variety of images fromobject-centric to event-centric, providing different and more abstract trainingdata than the state-of-the-art captioning systems have used thus far. We trainand test several generative and retrieval models to tackle the task of VQG.Evaluation results show that while such models ask reasonable questions givenvarious images, there is still a wide gap with human performance. Our proposedtask offers a new challenge to the community which we hope can spur furtherinterest in exploring deeper connections between vision & language.
arxiv-16800-33 | Learning Image Matching by Simply Watching Video | http://arxiv.org/abs/1603.06041 | author:Gucan Long, Laurent Kneip, Jose M. Alvarez, Hongdong Li category:cs.CV published:2016-03-19 summary:This work presents an unsupervised learning based approach to the ubiquitouscomputer vision problem of image matching. We start from the insight that theproblem of frame-interpolation implicitly solves for inter-framecorrespondences. This permits the application of analysis-by-synthesis: wefirstly train and apply a Convolutional Neural Network for frame-interpolation,then obtain correspondences by inverting the learned CNN. The key benefitbehind this strategy is that the CNN for frame-interpolation can be trained inan unsupervised manner by exploiting the temporal coherency that is naturallycontained in real-world video sequences. The present model therefore learnsimage matching by simply watching videos. Besides a promise to be moregenerally applicable, the presented approach achieves surprising performancecomparable to traditional empirically designed methods.
arxiv-16800-34 | A Character-level Decoder without Explicit Segmentation for Neural Machine Translation | http://arxiv.org/abs/1603.06147 | author:Junyoung Chung, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG published:2016-03-19 summary:The existing machine translation systems, whether phrase-based or neural,have relied almost exclusively on word-level modelling with explicitsegmentation. In this paper, we ask a fundamental question: can neural machinetranslation generate a character sequence without any explicit segmentation? Toanswer this question, we evaluate an attention-based encoder-decoder with asubword-level encoder and a character-level decoder on four languagepairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.Our experiments show that the models with a character-level decoder outperformthe ones with a subword-level decoder on all of the four language pairs.Furthermore, the ensembles of neural models with a character-level decoderoutperform the state-of-the-art non-neural machine translation systems onEn-Cs, En-De and En-Fi and perform comparably on En-Ru.
arxiv-16800-35 | Fast Incremental Method for Nonconvex Optimization | http://arxiv.org/abs/1603.06159 | author:Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG stat.ML published:2016-03-19 summary:We analyze a fast incremental aggregated gradient method for optimizingnonconvex problems of the form $\min_x \sum_i f_i(x)$. Specifically, we analyzethe SAGA algorithm within an Incremental First-order Oracle framework, and showthat it converges to a stationary point provably faster than both gradientdescent and stochastic gradient descent. We also discuss a Polyak's specialclass of nonconvex problems for which SAGA converges at a linear rate to theglobal optimum. Finally, we analyze the practically valuable regularized andminibatch variants of SAGA. To our knowledge, this paper presents the firstanalysis of fast convergence for an incremental aggregated gradient method fornonconvex problems.
arxiv-16800-36 | Katyusha: The First Truly Accelerated Stochastic Gradient Method | http://arxiv.org/abs/1603.05953 | author:Zeyuan Allen-Zhu category:math.OC cs.DS stat.ML published:2016-03-18 summary:We introduce $\mathtt{Katyusha}$, the first direct stochastic gradient methodthat has an accelerated convergence rate. Given an objective that is an average of $n$ convex and smooth functions,$\mathtt{Katyusha}$ converges to an $\varepsilon$-approximate minimizer using$O((n + \sqrt{n \kappa})\cdot \log\frac{f(x_0)-f(x^*)}{\varepsilon})$stochastic iterations, where $\kappa$ is the condition number.$\mathtt{Katyusha}$ also resolves the following open questions in optimizationand machine learning $\bullet$ For weakly convex and smooth objectives (e.g., Lasso, LogisticRegression), $\mathtt{Katyusha}$ is the first stochastic method that achievesthe optimal $1/\sqrt{\varepsilon}$ rate. $\bullet$ For strongly-convex but non-smooth ERM objectives (e.g., SVM),$\mathtt{Katyusha}$ gives the first stochastic method that achieves the optimal$1/\sqrt{\varepsilon}$ rate. $\bullet$ For weakly convex and non-smooth ERM objectives (e.g., L1SVM),$\mathtt{Katyusha}$ gives the first stochastic method that achieves the optimal$1/\varepsilon$ rate.
arxiv-16800-37 | A Readability Analysis of Campaign Speeches from the 2016 US Presidential Campaign | http://arxiv.org/abs/1603.05739 | author:Elliot Schumacher, Maxine Eskenazi category:cs.CL published:2016-03-18 summary:Readability is defined as the reading level of the speech from grade 1 tograde 12. It results from the use of the REAP readability analysis (vocabulary- Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), whichuse the lexical contents and grammatical structure of the sentences in adocument to predict the reading level. After analysis, results were groupedinto the average readability of each candidate, the evolution of thecandidate's speeches' readability over time and the standard deviation, or howmuch each candidate varied their speech from one venue to another. Forcomparison, one speech from four past presidents and the Gettysburg Addresswere also analyzed.
arxiv-16800-38 | From line segments to more organized Gestalts | http://arxiv.org/abs/1603.05763 | author:Boshra Rajaei, Rafael Grompone von Gioi, Jean-Michel Morel category:cs.CV published:2016-03-18 summary:In this paper, we reconsider the early computer vision bottom-up program,according to which higher level features (geometric structures) in an imagecould be built up recursively from elementary features by simple groupingprinciples coming from Gestalt theory. Taking advantage of the (recent)advances in reliable line segment detectors, we propose three feature detectorsthat constitute one step up in this bottom up pyramid. For any digital image,our unsupervised algorithm computes three classic Gestalts from the set ofpredetected line segments: good continuations, nonlocal alignments, and bars.The methodology is based on a common stochastic {\it a contrario model}yielding three simple detection formulas, characterized by their number offalse alarms. This detection algorithm is illustrated on several digitalimages.
arxiv-16800-39 | A Probabilistic Machine Learning Approach to Detect Industrial Plant Faults | http://arxiv.org/abs/1603.05770 | author:Wei Xiao category:stat.ML stat.AP published:2016-03-18 summary:Fault detection in industrial plants is a hot research area as more and moresensor data are being collected throughout the industrial process. Automaticdata-driven approaches are widely needed and seen as a promising area ofinvestment. This paper proposes an effective machine learning algorithm topredict industrial plant faults based on classification methods such aspenalized logistic regression, random forest and gradient boosted tree. Afault's start time and end time are predicted sequentially in two steps byformulating the original prediction problems as classification problems. Thealgorithms described in this paper won first place in the Prognostics andHealth Management Society 2015 Data Challenge.
arxiv-16800-40 | Learning to Navigate the Energy Landscape | http://arxiv.org/abs/1603.05772 | author:Julien Valentin, Angela Dai, Matthias Nießner, Pushmeet Kohli, Philip Torr, Shahram Izadi, Cem Keskin category:cs.CV published:2016-03-18 summary:In this paper, we present a novel and efficient architecture for addressingcomputer vision problems that use `Analysis by Synthesis'. Analysis bysynthesis involves the minimization of the reconstruction error which istypically a non-convex function of the latent target variables.State-of-the-art methods adopt a hybrid scheme where discriminatively trainedpredictors like Random Forests or Convolutional Neural Networks are used toinitialize local search algorithms. While these methods have been shown toproduce promising results, they often get stuck in local optima. Our methodgoes beyond the conventional hybrid architecture by not only proposing multipleaccurate initial solutions but by also defining a navigational structure overthe solution space that can be used for extremely efficient gradient-free localsearch. We demonstrate the efficacy of our approach on the challenging problemof RGB Camera Relocalization. To make the RGB camera relocalization problemparticularly challenging, we introduce a new dataset of 3D environments whichare significantly larger than those found in other publicly-available datasets.Our experiments reveal that the proposed method is able to achievestate-of-the-art camera relocalization results. We also demonstrate thegeneralizability of our approach on Hand Pose Estimation and Image Retrievaltasks.
arxiv-16800-41 | Unsupervised Cross-Media Hashing with Structure Preservation | http://arxiv.org/abs/1603.05782 | author:Xiangyu Wang, Alex Yong-Sang Chia category:cs.CV cs.IR H.3.3 published:2016-03-18 summary:Recent years have seen the exponential growth of heterogeneous multimediadata. The need for effective and accurate data retrieval from heterogeneousdata sources has attracted much research interest in cross-media retrieval.Here, given a query of any media type, cross-media retrieval seeks to findrelevant results of different media types from heterogeneous data sources. Tofacilitate large-scale cross-media retrieval, we propose a novel unsupervisedcross-media hashing method. Our method incorporates local affinity and distancerepulsion constraints into a matrix factorization framework. Correspondingly,the proposed method learns hash functions that generates unified hash codesfrom different media types, while ensuring intrinsic geometric structure of thedata distribution is preserved. These hash codes empower the similarity betweendata of different media types to be evaluated directly. Experimental results ontwo large-scale multimedia datasets demonstrate the effectiveness of theproposed method, where we outperform the state-of-the-art methods.
arxiv-16800-42 | A Comparison between Deep Neural Nets and Kernel Acoustic Models for Speech Recognition | http://arxiv.org/abs/1603.05800 | author:Zhiyun Lu, Dong Guo, Alireza Bagheri Garakani, Kuan Liu, Avner May, Aurelien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, Fei Sha category:cs.LG stat.ML published:2016-03-18 summary:We study large-scale kernel methods for acoustic modeling and compare to DNNson performance metrics related to both acoustic modeling and recognition.Measuring perplexity and frame-level classification accuracy, kernel-basedacoustic models are as effective as their DNN counterparts. However, ontoken-error-rates DNN models can be significantly better. We have discoveredthat this might be attributed to DNN's unique strength in reducing both theperplexity and the entropy of the predicted posterior probabilities. Motivatedby our findings, we propose a new technique, entropy regularized perplexity,for model selection. This technique can noticeably improve the recognitionperformance of both types of models, and reduces the gap between them. Whileeffective on Broadcast News, this technique could be also applicable to othertasks.
arxiv-16800-43 | Comparing Time and Frequency Domain for Audio Event Recognition Using Deep Learning | http://arxiv.org/abs/1603.05824 | author:Lars Hertel, Huy Phan, Alfred Mertins category:cs.NE cs.LG cs.SD published:2016-03-18 summary:Recognizing acoustic events is an intricate problem for a machine and anemerging field of research. Deep neural networks achieve convincing results andare currently the state-of-the-art approach for many tasks. One advantage istheir implicit feature learning, opposite to an explicit feature extraction ofthe input signal. In this work, we analyzed whether more discriminativefeatures can be learned from either the time-domain or the frequency-domainrepresentation of the audio signal. For this purpose, we trained multiple deepnetworks with different architectures on the Freiburg-106 and ESC-10 datasets.Our results show that feature learning from the frequency domain is superior tothe time domain. Moreover, additionally using convolution and pooling layers,to explore local structures of the audio signal, significantly improves therecognition performance and achieves state-of-the-art results.
arxiv-16800-44 | A Flexible Primal-Dual Toolbox | http://arxiv.org/abs/1603.05835 | author:Hendrik Dirks category:math.OC cs.CV cs.MS published:2016-03-18 summary:\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convexvariational problems in image processing and beyond. Such problems oftenconsist of non-differentiable parts and involve linear operators. The toolboxuses a primal-dual scheme to avoid (computationally) inefficient operatorinversion and to get reliable error estimates. From the user-side,\textbf{FlexBox} expects the primal formulation of the problem, automaticallydecouples operators and dualizes the problem. For large-scale problems,\textbf{FlexBox} also comes with a \cpp-module, which can be used stand-aloneor together with MATLAB via MEX-interfaces. Besides various pre-implementeddata-fidelities and regularization-terms, \textbf{FlexBox} is able to handlearbitrary operators while being easily extendable, due to its object-orienteddesign. The toolbox is available at\href{http://www.flexbox.im}{http://www.flexbox.im}
arxiv-16800-45 | N-ary Error Correcting Coding Scheme | http://arxiv.org/abs/1603.05850 | author:Joey Tianyi Zhou, Ivor W. Tsang, Shen-Shyang Ho, Klaus-Robert Muller category:cs.LG published:2016-03-18 summary:The coding matrix design plays a fundamental role in the predictionperformance of the error correcting output codes (ECOC)-based multi-class task.{In many-class classification problems, e.g., fine-grained categorization, itis difficult to distinguish subtle between-class differences under existingcoding schemes due to a limited choices of coding values.} In this paper, weinvestigate whether one can relax existing binary and ternary code design to$N$-ary code design to achieve better classification performance. {Inparticular, we present a novel $N$-ary coding scheme that decomposes theoriginal multi-class problem into simpler multi-class subproblems, which issimilar to applying a divide-and-conquer method.} The two main advantages ofsuch a coding scheme are as follows: (i) the ability to construct morediscriminative codes and (ii) the flexibility for the user to select the best$N$ for ECOC-based classification. We show empirically that the optimal $N$(based on classification performance) lies in $[3, 10]$ with some trade-off incomputational cost. Moreover, we provide theoretical insights on the dependencyof the generalization error bound of an $N$-ary ECOC on the average baseclassifier generalization error and the minimum distance between any two codesconstructed. Extensive experimental results on benchmark multi-class datasetsshow that the proposed coding scheme achieves superior prediction performanceover the state-of-the-art coding methods.
arxiv-16800-46 | Approximated Robust Principal Component Analysis for Improved General Scene Background Subtraction | http://arxiv.org/abs/1603.05875 | author:Salehe Erfanian Ebadi, Valia Guerra Ones, Ebroul Izquierdo category:cs.CV stat.AP published:2016-03-18 summary:The research reported in this paper addresses the fundamental task ofseparation of locally moving or deforming image areas from a static or globallymoving background. It builds on the latest developments in the field of robustprincipal component analysis, specifically, the recently reported practicalsolutions for the long-standing problem of recovering the low-rank and sparseparts of a large matrix made up of the sum of these two components. Thisarticle addresses a few critical issues including: embedding global motionparameters in the matrix decomposition model, i.e., estimation of global motionparameters simultaneously with the foreground/background separation task,considering matrix block-sparsity rather than generic matrix sparsity asnatural feature in video processing applications, attenuating backgroundghosting effects when foreground is subtracted, and more critically providingan extremely efficient algorithm to solve the low-rank/sparse matrixdecomposition task. The first aspect is important for background/foregroundseparation in generic video sequences where the background usually obeys globaldisplacements originated by the camera motion in the capturing process. Thesecond aspect exploits the fact that in video processing applications thesparse matrix has a very particular structure, where the non-zero matrixentries are not randomly distributed but they build small blocks within thesparse matrix. The next feature of the proposed approach addresses removal ofghosting effects originated from foreground silhouettes and the lack ofinformation in the occluded background regions of the image. Finally, theproposed model also tackles algorithmic complexity by introducing an extremelyefficient "SVD-free" technique that can be applied in mostbackground/foreground separation tasks for conventional video processing.
arxiv-16800-47 | Generalized support vector regression: duality and tensor-kernel representation | http://arxiv.org/abs/1603.05876 | author:Saverio Salzo, Johan A. K. Suykens category:math.OC math.FA stat.ML published:2016-03-18 summary:In this paper we study the variational problem associated to support vectorregression in Banach function spaces. Using the Fenchel-Rockafellar dualitytheory, we give explicit formulation of the dual problem as well as of therelated optimality conditions. Moreover, we provide a new computationalframework for solving the problem which relies on a tensor-kernelrepresentation. This analysis overcomes the typical difficulties connected tolearning in Banach spaces. We finally present a large class of tensor-kernelsto which our theory fully applies: power series tensor kernels. This type ofkernels describe Banach spaces of analytic functions and includegeneralizations of the exponential and polynomial kernels as well as, in thecomplex case, generalizations of the Szeg\"o and Bergman kernels.
arxiv-16800-48 | Geometric Hypergraph Learning for Visual Tracking | http://arxiv.org/abs/1603.05930 | author:Dawei Du, Honggang Qi, Longyin Wen, Qi Tian, Qingming Huang, Siwei Lyu category:cs.CV published:2016-03-18 summary:Graph based representation is widely used in visual tracking field by findingcorrect correspondences between target parts in consecutive frames. However,most graph based trackers consider pairwise geometric relations between localparts. They do not make full use of the target's intrinsic structure, therebymaking the representation easily disturbed by errors in pairwise affinitieswhen large deformation and occlusion occur. In this paper, we propose ageometric hypergraph learning based tracking method, which fully exploitshigh-order geometric relations among multiple correspondences of parts inconsecutive frames. Then visual tracking is formulated as the mode-seekingproblem on the hypergraph in which vertices represent correspondence hypothesesand hyperedges describe high-order geometric relations. Besides, aconfidence-aware sampling method is developed to select representative verticesand hyperedges to construct the geometric hypergraph for more robustness andscalability. The experiments are carried out on two challenging datasets(VOT2014 and Deform-SOT) to demonstrate that the proposed method performsfavorable against other existing trackers.
arxiv-16800-49 | Distributed Iterative Learning Control for a Team of Quadrotors | http://arxiv.org/abs/1603.05933 | author:Andreas Hock, Angela P. Schoellig category:cs.RO cs.LG cs.MA published:2016-03-18 summary:The goal of this work is to enable a team of quadrotors to learn how toaccurately track a desired trajectory while holding a given formation. We solvethis problem in a distributed manner, where each vehicle has only access to theinformation of its neighbors. The desired trajectory is only available to one(or few) vehicles. We present a distributed iterative learning control (ILC)approach where each vehicle learns from the experience of its own and itsneighbors' previous task repetitions and adapts its feedforward input toimprove performance. Existing algorithms are extended in theory to make themmore applicable for real-world experiments. In particular, we prove stabilityfor any causal learning function with gains chosen according to a simple scalarcondition. Previous proofs were restricted to a specific learning function,which only depends on the tracking error derivative (D-type ILC). Thisextension provides more degrees of freedom in the ILC design and, as a result,better performance can be achieved. We also show that stability is not affectedby a linear dynamic coupling between neighbors. This allows us to use anadditional consensus feedback controller to compensate for non-repetitivedisturbances. Experiments with two quadrotors attest the practicalapplicability of the proposed distributed multi-agent ILC approach. This is thefirst work to show distributed ILC in experiment.
arxiv-16800-50 | Transferring Learned Microcalcification Group Detection from 2D Mammography to 3D Digital Breast Tomosynthesis Using a Hierarchical Model and Scope-based Normalization Features | http://arxiv.org/abs/1603.05955 | author:Yin Yin, Sergei V. Fotin, Hrishikesh Haldankar, Jeffrey W. Hoffmeister, Senthil Periaswamy category:cs.CV published:2016-03-18 summary:A novel hierarchical model is introduced to solve a general problem ofdetecting groups of similar objects. Under this model, detection of groups isperformed in hierarchically organized layers while each layer represents ascope for target objects. The processing of these layers involves sequentialextraction of appearance features for an individual object, consistencymeasurement features for nearby objects, and finally the distribution featuresfor all objects within the group. Using the concept of scope-basednormalization, the extracted features not only enhance local contrast of anindividual object, but also provide consistent characterization for all relatedobjects. As an example, a microcalcification group detection system for 2Dmammography was developed, and then the learned model was transferred to 3Ddigital breast tomosynthesis without any retraining or fine-tuning. Thedetection system demonstrated state-of-the-art performance and detected 96% ofcancerous lesions at the rate of 1.2 false positives per volume as measured onan independent tomosynthesis test set.
arxiv-16800-51 | Document Neural Autoregressive Distribution Estimation | http://arxiv.org/abs/1603.05962 | author:Stanislas Lauly, Yin Zheng, Alexandre Allauzen, Hugo Larochelle category:cs.LG cs.CL published:2016-03-18 summary:We present an approach based on feed-forward neural networks for learning thedistribution of textual documents. This approach is inspired by the NeuralAutoregressive Distribution Estimator(NADE) model, which has been shown to be agood estimator of the distribution of discrete-valued igh-dimensional vectors.In this paper, we present how NADE can successfully be adapted to the case oftextual data, retaining from NADE the property that sampling or computing theprobability of observations can be done exactly and efficiently. The approachcan also be used to learn deep representations of documents that arecompetitive to those learned by the alternative topic modeling approaches.Finally, we describe how the approach can be combined with a regular neuralnetwork N-gram model and substantially improve its performance, by making itslearned representation sensitive to the larger, document-specific context.
arxiv-16800-52 | A Message Passing Algorithm for the Problem of Path Packing in Graphs | http://arxiv.org/abs/1603.06002 | author:Patrick Eschenfeldt, David Gamarnik category:cs.DS stat.ML published:2016-03-18 summary:We consider the problem of packing node-disjoint directed paths in a directedgraph. We consider a variant of this problem where each path starts within afixed subset of root nodes, subject to a given bound on the length of paths.This problem is motivated by the so-called kidney exchange problem, but haspotential other applications and is interesting in its own right. We propose a new algorithm for this problem based on the messagepassing/belief propagation technique. A priori this problem does not have anassociated graphical model, so in order to apply a belief propagation algorithmwe provide a novel representation of the problem as a graphical model. Standardbelief propagation on this model has poor scaling behavior, so we provide anefficient implementation that significantly decreases the complexity. Weprovide numerical results comparing the performance of our algorithm on bothartificially created graphs and real world networks to several alternativealgorithms, including algorithms based on integer programming (IP) techniques.These comparisons show that our algorithm scales better to large instances thanIP-based algorithms and often finds better solutions than a simple algorithmthat greedily selects the longest path from each root node. In some cases italso finds better solutions than the ones found by IP-based algorithms evenwhen the latter are allowed to run significantly longer than our algorithm.
arxiv-16800-53 | Readability-based Sentence Ranking for Evaluating Text Simplification | http://arxiv.org/abs/1603.06009 | author:Sowmya Vajjala, Detmar Meurers category:cs.CL published:2016-03-18 summary:We propose a new method for evaluating the readability of simplifiedsentences through pair-wise ranking. The validity of the method is establishedthrough in-corpus and cross-corpus evaluation experiments. The approachcorrectly identifies the ranking of simplified and unsimplified sentences interms of their reading level with an accuracy of over 80%, significantlyoutperforming previous results. To gain qualitative insights into the nature ofsimplification at the sentence level, we studied the impact of specificlinguistic features. We empirically confirm that both word-level and syntacticfeatures play a role in comparing the degree of simplification of authenticdata. To carry out this research, we created a new sentence-aligned corpus fromprofessionally simplified news articles. The new corpus resource enriches theempirical basis of sentence-level simplification research, which so far reliedon a single resource. Most importantly, it facilitates cross-corpus evaluationfor simplification, a key step towards generalizable results.
arxiv-16800-54 | A Comprehensive Performance Evaluation of Deformable Face Tracking "In-the-Wild" | http://arxiv.org/abs/1603.06015 | author:Grigorios G. Chrysos, Epameinondas Antonakos, Patrick Snape, Akshay Asthana, Stefanos Zafeiriou category:cs.CV cs.AI published:2016-03-18 summary:Recently, technologies such as face detection, facial landmark localisationand face recognition and verification have matured enough to provide effectiveand efficient solutions for imagery captured under arbitrary conditions(referred to as "in-the-wild"). This is partially attributed to the fact thatcomprehensive "in-the-wild" benchmarks have been developed for face detection,landmark localisation and recognition/verification. A very important technologythat has not been thoroughly evaluated yet is deformable face tracking"in-the-wild". Until now, the performance has mainly been assessedqualitatively by visually assessing the result of a deformable face trackingtechnology on short videos. In this paper, we perform the first, to the best ofour knowledge, thorough evaluation of state-of-the-art deformable face trackingpipelines using the recently introduced 300VW benchmark. We evaluate manydifferent architectures focusing mainly on the task of on-line deformable facetracking. In particular, we compare the following general strategies: (a)generic face detection plus generic facial landmark localisation, (b) genericmodel free tracking plus generic facial landmark localisation, as well as (c)hybrid approaches using state-of-the-art face detection, model free trackingand facial landmark localisation technologies. Our evaluation reveals futureavenues for further research on the topic.
arxiv-16800-55 | Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation | http://arxiv.org/abs/1603.05959 | author:Konstantinos Kamnitsas, Christian Ledig, Virginia F. J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben Glocker category:cs.CV cs.AI published:2016-03-18 summary:We propose a dual pathway, 11-layers deep, three-dimensional ConvolutionalNeural Network for the challenging task of brain lesion segmentation. Thedevised architecture is the result of an in-depth analysis of the limitationsof current networks proposed for similar applications. To overcome thecomputational burden of processing 3D medical scans, we have devised anefficient and effective dense training scheme which joins the processing ofadjacent image patches into one pass through the network while automaticallyadapting to the inherent class imbalance present in the data. Further, weanalyze the development of deeper, thus more discriminative 3D CNNs. In orderto incorporate both local and larger contextual information, we employ a dualpathway architecture that processes the input images at multiple scalessimultaneously. For post-processing of the network's soft segmentation, we usea 3D fully connected Conditional Random Field which effectively removes falsepositives. Our pipeline is extensively evaluated on three challenging tasks oflesion segmentation in multi-channel MRI patient data with traumatic braininjuries, brain tumors, and ischemic stroke. We improve on the state-of-the-artfor all three applications, with top ranking performance on the publicbenchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient,which allows its adoption in a variety of research and clinical settings. Thesource code of our implementation is made publicly available.
arxiv-16800-56 | Convergence of Contrastive Divergence Algorithm in Exponential Family | http://arxiv.org/abs/1603.05729 | author:Tung-Yu Wu, Bai Jiang, Yifan Jin, Wing H. Wong category:stat.ML published:2016-03-17 summary:This paper studies the convergence properties of contrastive divergencealgorithm for parameter inference in exponential family, by relating it toMarkov chain theory and stochastic stability literature. We prove that, undermild conditions and given a finite data sample $X_1,\dots,X_n \simp_{\theta^*}$ i.i.d. in an event with probability approaching to 1, thesequence $\{\theta_t\}_{t \ge 0}$ generated by CD algorithm is a positiveHarris recurrent chain, and thus processes an unique invariant distribution$\pi_n$. The invariant distribution concentrates around the Maximum LikelihoodEstimate at a speed arbitrarily slower than $\sqrt{n}$, and the number of stepsin Markov Chain Monte Carlo only affects the coefficient factor of theconcentration rate. Finally we conclude that as $n \to \infty$, $$\limsup_{t\to \infty} \left\Vert \frac{1}{t} \sum_{s=1}^t \theta_s - \theta^*\right\Vert\overset{p}{\to} 0.$$
arxiv-16800-57 | Optimal Black-Box Reductions Between Optimization Objectives | http://arxiv.org/abs/1603.05642 | author:Zeyuan Allen-Zhu, Elad Hazan category:math.OC cs.DS cs.LG stat.ML published:2016-03-17 summary:The diverse world of machine learning applications has given rise to aplethora of algorithms and optimization methods, finely tuned to the specificregression or classification task at hand. We reduce the complexity ofalgorithm design for machine learning by reductions: we develop reductions thattake a method developed for one setting and apply it to the entire spectrum ofsmoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are OPTIMAL and morePRACTICAL. We show how these new reductions give rise to new and faster runningtimes on training linear classifiers for various families of loss functions,and conclude with experiments showing their successes also in practice.
arxiv-16800-58 | Fast moment estimation for generalized latent Dirichlet models | http://arxiv.org/abs/1603.05324 | author:Shiwen Zhao, Barbara E. Engelhardt, Sayan Mukherjee, David B. Dunson category:math.ST cs.LG stat.AP stat.ME stat.TH published:2016-03-17 summary:We develop a generalized method of moments (GMM) approach for fast parameterestimation in a new class of Dirichlet latent variable models with mixed datatypes. Parameter estimation via GMM has been demonstrated to have computationaland statistical advantages over alternative methods, such as expectationmaximization, variational inference, and Markov chain Monte Carlo. The keycomputational advan- tage of our method (MELD) is that parameter estimationdoes not require instantiation of the latent variables. Moreover, arepresentational advantage of the GMM approach is that the behavior of themodel is agnostic to distributional assumptions of the observations. We derivepopulation moment conditions after marginalizing out the sample-specificDirichlet latent variables. The moment conditions only depend on component meanparameters. We illustrate the utility of our approach on simulated data,comparing results from MELD to alternative methods, and we show the promise ofour approach through the application of MELD to several data sets.
arxiv-16800-59 | Discriminative Embeddings of Latent Variable Models for Structured Data | http://arxiv.org/abs/1603.05629 | author:Hanjun Dai, Bo Dai, Le Song category:cs.LG published:2016-03-17 summary:Kernel classifiers and regressors designed for structured data, such assequences, trees and graphs, have significantly advanced in a number ofinterdisciplinary areas such as computational biology and drug design.Typically, kernel functions are designed beforehand for a data type whicheither exploit statistics of the structures or make use of probabilisticgenerative models, and then a discriminative classifier is learned based on thekernels via convex optimization. However, such an elegant two-stage approachalso limited kernel methods from scaling up to millions of data points, andexploiting discriminative information to learn feature representations. We propose an effective and scalable approach for structured datarepresentation which is based on the idea of embedding latent variable modelsinto feature spaces, and learning such feature spaces using discriminativeinformation. Furthermore, our feature learning algorithm runs a sequence offunction mappings in a way similar to graphical model inference procedures,such as mean field and belief propagation. In real world applications involvingsequences and graphs, we showed that the proposed approach is much morescalable than alternatives while at the same time produce comparable results tothe state-of-the-art in terms of classification and regression.
arxiv-16800-60 | Variable-Length Hashing | http://arxiv.org/abs/1603.05414 | author:Honghai Yu, Pierre Moulin, Hong Wei Ng, Xiaoli Li category:cs.CV cs.IR published:2016-03-17 summary:Hashing has emerged as a popular technique for large-scale similarity search.Most learning-based hashing methods generate compact yet correlated hash codes.However, this redundancy is storage-inefficient. Hence we propose a losslessvariable-length hashing (VLH) method that is both storage- andsearch-efficient. Storage efficiency is achieved by converting the fixed-lengthhash code into a variable-length code. Search efficiency is obtained by using amultiple hash table structure. With VLH, we are able to deliberately addredundancy into hash codes to improve retrieval performance with littlesacrifice in storage efficiency or search complexity. In particular, we proposea block K-means hashing (B-KMH) method to obtain significantly improvedretrieval performance with no increase in storage and marginal increase incomputational cost.
arxiv-16800-61 | Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data | http://arxiv.org/abs/1603.05594 | author:Enmei Tu, Nikola Kasabov, Jie Yang category:cs.NE cs.AI stat.ML published:2016-03-17 summary:This paper proposes a new method for an optimized mapping of temporalvariables, describing a temporal stream data, into the recently proposedNeuCube spiking neural network architecture. This optimized mapping extends theuse of the NeuCube, which was initially designed for spatiotemporal brain data,to work on arbitrary stream data and to achieve a better accuracy of temporalpattern recognition, a better and earlier event prediction and a betterunderstanding of complex temporal stream data through visualization of theNeuCube connectivity. The effect of the new mapping is demonstrated on threebench mark problems. The first one is early prediction of patient sleep stageevent from temporal physiological data. The second one is pattern recognitionof dynamic temporal patterns of traffic in the Bay Area of California and thelast one is the Challenge 2012 contest data set. In all cases the use of theproposed mapping leads to an improved accuracy of pattern recognition and eventprediction and a better understanding of the data when compared to traditionalmachine learning techniques or spiking neural network reservoirs with arbitrarymapping of the variables.
arxiv-16800-62 | Modeling self-organization of vocabularies under phonological similarity effects | http://arxiv.org/abs/1603.05354 | author:Javier Vera category:cs.CL physics.soc-ph published:2016-03-17 summary:This work develops a computational model (by Automata Networks) ofphonological similarity effects involved in the formation of word-meaningassociations on artificial populations of speakers. Classical studies show thatin recalling experiments memory performance was impaired for phonologicallysimilar words versus dissimilar ones. Here, the individuals confoundphonologically similar words according to a predefined parameter. The mainhypothesis is that there is a critical range of the parameter, and with this,of working-memory mechanisms, which implies drastic changes in the finalconsensus of the entire population. Theoretical results present proofs ofconvergence for a particular case of the model within a worst-case complexityframework. Computer simulations describe the evolution of an energy functionthat measures the amount of local agreement between individuals. The mainfinding is the appearance of sudden changes in the energy function at criticalparameters.
arxiv-16800-63 | "What happens if..." Learning to Predict the Effect of Forces in Images | http://arxiv.org/abs/1603.05600 | author:Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi category:cs.CV published:2016-03-17 summary:What happens if one pushes a cup sitting on a table toward the edge of thetable? How about pushing a desk against a wall? In this paper, we study theproblem of understanding the movements of objects as a result of applyingexternal forces to them. For a given force vector applied to a specificlocation in an image, our goal is to predict long-term sequential movementscaused by that force. Doing so entails reasoning about scene geometry, objects,their attributes, and the physical rules that govern the movements of objects.We design a deep neural network model that learns long-term sequentialdependencies of object movements while taking into account the geometry andappearance of the scene by combining Convolutional and Recurrent NeuralNetworks. Training our model requires a large-scale dataset of object movementscaused by external forces. To build a dataset of forces in scenes, wereconstructed all images in SUN RGB-D dataset in a physics simulator toestimate the physical movements of objects caused by external forces applied tothem. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which avariety of external forces are applied to different types of objects resultingin more than 65,000 object movements represented in 3D. Our experimentalevaluations show that the challenging task of predicting long-term movements ofobjects as their reaction to external forces is possible from a single image.
arxiv-16800-64 | Streaming Algorithms for News and Scientific Literature Recommendation: Submodular Maximization with a $d$-Knapsack Constraint | http://arxiv.org/abs/1603.05614 | author:Qilian Yu, Easton Li Xu, Shuguang Cui category:cs.LG cs.DS published:2016-03-17 summary:Submodular maximization problems belong to the family of combinatorialoptimization problems and enjoy wide applications. In this paper, we focus onthe problem of maximizing a monotone submodular function subject to a$d$-knapsack constraint, for which we propose a streaming algorithm thatachieves a $\left(\frac{1}{1+d}-\epsilon\right)$-approximation of the optimalvalue, while it only needs one single pass through the dataset without storingall the data in the memory. In our experiments, we extensively evaluate theeffectiveness of our proposed algorithm via two applications: newsrecommendation and scientific literature recommendation. It is observed thatthe proposed streaming algorithm achieves both execution speedup and memorysaving by several orders of magnitude, compared with existing approaches.
arxiv-16800-65 | Generative Image Modeling using Style and Structure Adversarial Networks | http://arxiv.org/abs/1603.05631 | author:Xiaolong Wang, Abhinav Gupta category:cs.CV published:2016-03-17 summary:Current generative frameworks use end-to-end learning and generate images bysampling from uniform noise distribution. However, these approaches ignore themost basic principle of image formation: images are product of: (a) Structure:the underlying 3D model; (b) Style: the texture mapped onto structure. In thispaper, we factorize the image generation process and propose Style andStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has twocomponents: the Structure-GAN generates a surface normal map; the Style-GANtakes the surface normal map as input and generates the 2D image. Apart from areal vs. generated loss function, we use an additional loss with computedsurface normals from generated images. The two GANs are first trainedindependently, and then merged together via joint learning. We show our S^2-GANmodel is interpretable, generates more realistic images and can be used tolearn unsupervised RGBD representations.
arxiv-16800-66 | Variance Reduction for Faster Non-Convex Optimization | http://arxiv.org/abs/1603.05643 | author:Zeyuan Allen-Zhu, Elad Hazan category:math.OC cs.DS cs.LG cs.NE stat.ML published:2016-03-17 summary:We consider the fundamental problem in non-convex optimization of efficientlyreaching a stationary point. In contrast to the convex case, in the longhistory of this basic problem, the only known theoretical results onfirst-order non-convex optimization remain to be full gradient descent thatconverges in $O(1/\varepsilon)$ iterations for smooth objectives, andstochastic gradient descent that converges in $O(1/\varepsilon^2)$ iterationsfor objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result isbased on the variance reduction trick recently introduced to convexoptimization, as well as a brand new analysis of variance reduction that issuitable for non-convex optimization. For objectives that are sum of smoothfunctions, our first-order minibatch stochastic method converges with an$O(1/\varepsilon)$ rate, and is faster than full gradient descent by$\Omega(n^{1/3})$. We demonstrate the effectiveness of our methods on empirical riskminimizations with non-convex loss functions and training neural nets.
arxiv-16800-67 | Bank distress in the news: Describing events through deep learning | http://arxiv.org/abs/1603.05670 | author:Samuel Rönnqvist, Peter Sarlin category:cs.CL cs.AI cs.IR cs.NE q-fin.CP published:2016-03-17 summary:While many models are purposed for detecting the occurrence of events incomplex systems, the task of providing qualitative detail on the developmentsis not usually as well automated. We present a deep learning approach fordetecting relevant discussion in text and extracting natural languagedescriptions of events. Supervised by only a small set of event information,the model is leveraged by unsupervised learning of semantic vectorrepresentations on extensive text data. We demonstrate applicability to thestudy of financial risk based on news (6.6M articles), particularly bankdistress and government interventions (243 events), where indices can signalthe level of bank-stress-related reporting at the entity level, or aggregatedat country or European level, while being coupled with explanations. Thus, weexemplify how text, as timely and widely available data, can serve as a usefulcomplementary source of information for financial risk analytics.
arxiv-16800-68 | Predicting health inspection results from online restaurant reviews | http://arxiv.org/abs/1603.05673 | author:Samantha Wong, Hamidreza Chinaei, Frank Rudzicz category:cs.CL cs.LG published:2016-03-17 summary:Informatics around public health are increasingly shifting from theprofessional to the public spheres. In this work, we apply linguistic analyticsto restaurant reviews, from Yelp, in order to automatically predict officialhealth inspection reports. We consider two types of feature sets, i.e., keyworddetection and topic model features, and use these in several classificationmethods. Our empirical analysis shows that these extracted features can predictpublic health inspection reports with over 90% accuracy using simple supportvector machines.
arxiv-16800-69 | Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? | http://arxiv.org/abs/1603.05691 | author:Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson category:stat.ML cs.LG published:2016-03-17 summary:Yes, apparently they do. Previous research demonstrated that shallow feed-forward nets sometimes canlearn the complex functions previously learned by deep nets while using asimilar number of parameters as the deep models they mimic. In this paper weinvestigate if shallow models can learn to mimic the functions learned by deepconvolutional models. We experiment with shallow models and models with avarying number of convolutional layers, all trained to mimic a state-of-the-artensemble of CIFAR- 10 models. We demonstrate that we are unable to trainshallow models to be of comparable accuracy to deep convolutional models.Although the student models do not have to be as deep as the teacher modelsthey mimic, the student models apparently need multiple convolutional layers tolearn functions of comparable accuracy.
arxiv-16800-70 | Predicate Gradual Logic and Linguistics | http://arxiv.org/abs/1603.05570 | author:Ryuta Arisaka category:cs.CL published:2016-03-17 summary:There are several major proposals for treating donkey anaphora such asdiscourse representation theory and the likes, or E-Type theories and thelikes. Every one of them works well for a set of specific examples that theyuse to demonstrate validity of their approaches. As I show in this paper,however, they are not very generalisable and do not account for essentially thesame problem that they remedy when it manifests in other examples. I proposeanother logical approach. I develoop logic that extends a recent, propositionalgradual logic, and show that it can treat donkey anaphora generally. I alsoidentify and address a problem around the modern convention on existentialimport. Furthermore, I show that Aristotle's syllogisms and conversion arerealisable in this logic.
arxiv-16800-71 | Reliable Prediction Intervals for Local Linear Regression | http://arxiv.org/abs/1603.05587 | author:Mohammad Ghasemi Hamed, Masoud Ebadi Kivaj category:stat.ME cs.LG published:2016-03-17 summary:This paper introduces two methods for estimating reliable predictionintervals for local linear least-squares regressions, named Bounded OscillationPrediction Intervals (BOPI). It also proposes a new measure for comparinginterval prediction models named Equivalent Gaussian Standard Deviation (EGSD).The experimental results compare BOPI to other methods using coverageprobability, Mean Interval Size and the introduced EGSD measure. The resultswere generally in favor of the BOPI on considered benchmark regressiondatasets. It also, reports simulation studies validating the BOPI method'sreliability.
arxiv-16800-72 | Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent | http://arxiv.org/abs/1603.05544 | author:Linnan Wang, Yi Yang, Martin Renqiang Min, Srimat Chakradhar category:cs.LG cs.DC published:2016-03-17 summary:SGD is the widely adopted method to train CNN. Conceptually it approximatesthe population with a randomly sampled batch; then it evenly trains batches byconducting a gradient update on every batch in an epoch. In this paper, wedemonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle PseudoRandom Sampling differentiate batches in training, which then affect learningspeeds on them. Because of this, the unbiased treatment of batches involved inSGD creates improper load balancing. To address this issue, we presentInconsistent Stochastic Gradient Descent (ISGD) to dynamically vary trainingeffort according to learning statuses on batches. Specifically ISGD leveragestechniques in Statistical Process Control to identify a undertrained batch.Once a batch is undertrained, ISGD solves a new subproblem, a chasing logicplus a conservative constraint, to accelerate the training on the batch whileavoid drastic parameter changes. Extensive experiments on a variety of datasetsdemonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is21.05\% faster than SGD to reach 56\% top1 accuracy under the exactly sameexperiment setup. We also extend ISGD to work on multiGPU or heterogeneousdistributed system based on data parallelism, enabling the batch size to be thekey to scalability. Then we present the study of ISGD batch size to thelearning rate, parallelism, synchronization cost, system saturation andscalability. We conclude the optimal ISGD batch size is machine dependent.Various experiments on a multiGPU system validate our claim. In particular,ISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4NVIDIA TITAN X at the batch size of 1536.
arxiv-16800-73 | Tracking multiple moving objects in images using Markov Chain Monte Carlo | http://arxiv.org/abs/1603.05522 | author:Lan Jiang, Sumeetpal S. Singh category:stat.AP cs.CV stat.CO published:2016-03-17 summary:A new Bayesian state and parameter learning algorithm for multiple targettracking (MTT) models with image observations is proposed. Specifically, aMarkov chain Monte Carlo algorithm is designed to sample from the posteriordistribution of the unknown number of targets, their birth and death times,states and model parameters, which constitutes the complete solution to thetracking problem. The conventional approach is to pre-process the images toextract point observations and then perform tracking. We model the imagegeneration process directly to avoid potential loss of information whenextracting point observations. Numerical examples show that our algorithm hasimproved tracking performance over commonly used techniques, for both syntheticexamples and real florescent microscopy data, especially in the case of dimtargets with overlapping illuminated regions.
arxiv-16800-74 | A flexible state space model for learning nonlinear dynamical systems | http://arxiv.org/abs/1603.05486 | author:Andreas Svensson, Thomas B. Schön category:stat.CO cs.SY stat.ML published:2016-03-17 summary:We consider a nonlinear state space model with the state transition andobservation functions expressed as basis function expansions. We learn thecoefficients in the basis function expansions from data, and with a connectionto Gaussian processes we also develop priors on them for tuning the modelflexibility and to prevent overfitting to data, akin to a Gaussian processstate space model. The priors can alternatively be seen as a regularization,and helps the model in generalizing the data without sacrificing the richnessoffered by the basis function expansion. To learn the coefficients and otherunknown parameters efficiently, we tailor an algorithm for this model usingstate-of-the-art sequential Monte Carlo methods, which comes with theoreticalguarantees on the learning. Our approach indicates promising results whenevaluated on a classical benchmark as well as real data.
arxiv-16800-75 | Online semi-parametric learning for inverse dynamics modeling | http://arxiv.org/abs/1603.05412 | author:Diego Romeres, Mattia Zorzi, Alessandro Chiuso category:math.OC cs.LG stat.ML published:2016-03-17 summary:This paper presents a semi-parametric algorithm for online learning of arobot inverse dynamics model. It combines the strength of the parametric andnon-parametric modeling. The former exploits the rigid body dynamics equation,while the latter exploits a suitable kernel function. We provide an extensivecomparison with other methods from the literature using real data from the iCubhumanoid robot. In doing so we also compare two different techniques, namelycross validation and marginal likelihood optimization, for estimating thehyperparameters of the kernel function.
arxiv-16800-76 | Neural Aggregation Network for Video Face Recognition | http://arxiv.org/abs/1603.05474 | author:Jiaolong Yang, Peiran Ren, Dong Chen, Fang Wen, Hongdong Li, Gang Hua category:cs.CV cs.AI published:2016-03-17 summary:In this paper, we present a Neural Aggregation Network (NAN) for video facerecognition. The network takes a face video or face image set of a person withvariable number of face frames as its input, and produces a compact andfixed-dimension visual representation of that person. The whole network iscomposed of two modules. The feature embedding module is a CNN which maps eachface frame into a feature representation. The neural aggregation module iscomposed of two content based attention blocks which is driven by a memorystoring all the features extracted from the face video through the featureembedding module. The output of the first attention block adapts the second,whose output is adopted as the aggregated representation of the video faces.Due to the attention mechanism, this representation is invariant to the orderof the face frames. The experiments show that the proposed NAN consistentlyoutperforms hand-crafted aggregations such as average pooling, and achievesstate-of-the-art accuracy on three video face recognition datasets: the YouTubeFace, IJB-A and Celebrity-1000 datasets.
arxiv-16800-77 | Self-organization of vocabularies under different interaction orders | http://arxiv.org/abs/1603.05350 | author:Javier Vera category:cs.CL physics.soc-ph published:2016-03-17 summary:Traditionally, the formation of vocabularies has been studied by agent-basedmodels (specially, the Naming Game) in which random pairs of agents negotiateword-meaning associations at each discrete time step. This paper proposes afirst approximation to a novel question: To what extent the negotiation ofword-meaning associations is influenced by the order in which the individualsinteract? Automata Networks provide the adequate mathematical framework toexplore this question. Computer simulations suggest that on two-dimensionallattices the typical features of the formation of word-meaning associations arerecovered under random schemes that update small fractions of the population atthe same time.
arxiv-16800-78 | Cascading Bandits for Large-Scale Recommendation Problems | http://arxiv.org/abs/1603.05359 | author:Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, Branislav Kveton category:cs.LG stat.ML published:2016-03-17 summary:Most recommender systems recommend a list of items. The user examines thelist, from the first item to the last, and often chooses the first attractiveitem and does not examine the rest. This type of user behavior can be modeledby the cascade model. In this work, we study cascading bandits, an onlinelearning variant of the cascade model where the goal is to recommend $K$ mostattractive items from a large set of $L$ candidate items. We propose twoalgorithms for solving this problem, which are based on the idea of lineargeneralization. The key idea in our solutions is that we learn a predictor ofthe attraction probabilities of items from their features, as opposing tolearning the attraction probability of each item independently as in theexisting work. This results in practical learning algorithms whose regret doesnot depend on the number of items $L$. We bound the regret of one algorithm andcomprehensively evaluate the other on a range of recommendation problems. Thealgorithm performs well and outperforms all baselines.
arxiv-16800-79 | Saliency Detection with Spaces of Background-based Distribution | http://arxiv.org/abs/1603.05335 | author:Tong Zhao, Lin Li, Xinghao Ding, Yue Huang, Delu Zeng category:cs.CV published:2016-03-17 summary:In this letter, an effective image saliency detection method is proposed byconstructing some novel spaces to model the background and redefine thedistance of the salient patches away from the background. Concretely, given thebackgroundness prior, eigendecomposition is utilized to create four spaces ofbackground-based distribution (SBD) to model the background, in which a moreappropriate metric (Mahalanobis distance) is quoted to delicately measure thesaliency of every image patch away from the background. After that, a coarsesaliency map is obtained by integrating the four adjusted Mahalanobis distancemaps, each of which is formed by the distances between all the patches andbackground in the corresponding SBD. To be more discriminative, the coarsesaliency map is further enhanced into the posterior probability map withinBayesian perspective. Finally, the final saliency map is generated by properlyrefining the posterior probability map with geodesic distance. Experimentalresults on two usual datasets show that the proposed method is effectivecompared with the state-of-the-art algorithms.
arxiv-16800-80 | Image Labeling by Assignment | http://arxiv.org/abs/1603.05285 | author:Freddie Åström, Stefania Petra, Bernhard Schmitzer, Christoph Schnörr category:cs.CV math.OC published:2016-03-16 summary:We introduce a novel geometric approach to the image labeling problem.Abstracting from specific labeling applications, a general objective functionis defined on a manifold of stochastic matrices, whose elements assign priordata that are given in any metric space, to observed image measurements. Thecorresponding Riemannian gradient flow entails a set of replicator equations,one for each data point, that are spatially coupled by geometric averaging onthe manifold. Starting from uniform assignments at the barycenter as naturalinitialization, the flow terminates at some global maximum, each of whichcorresponds to an image labeling that uniquely assigns the prior data. Ourgeometric variational approach constitutes a smooth non-convex innerapproximation of the general image labeling problem, implemented with sparseinterior-point numerics in terms of parallel multiplicative updates thatconverge efficiently.
arxiv-16800-81 | Near-Optimal Stochastic Approximation for Online Principal Component Estimation | http://arxiv.org/abs/1603.05305 | author:Chris J. Li, Mengdi Wang, Han Liu, Tong Zhang category:math.OC stat.ML published:2016-03-16 summary:Principal component analysis (PCA) has been a prominent tool forhigh-dimensional data analysis. Online algorithms that estimate the principalcomponent by processing streaming data are of tremendous practical andtheoretical interests. Despite its rich applications, theoretical convergenceanalysis remains largely open. In this paper, we cast online PCA into astochastic nonconvex optimization problem, and we analyze the online PCAalgorithm as a stochastic approximation iteration. The stochastic approximationiteration processes data points incrementally and maintains a running estimateof the principal component. We prove for the first time a nearly optimalconvergence rate result for the online PCA algorithm. We show that thefinite-sample error closely matches the minimax information lower bound. Inaddition, we characterize the convergence process using ordinary and stochasticdifferential equation approximations.
arxiv-16800-82 | Identity Mappings in Deep Residual Networks | http://arxiv.org/abs/1603.05027 | author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV cs.LG published:2016-03-16 summary:Deep residual networks have emerged as a family of extremely deeparchitectures showing compelling accuracy and nice convergence behaviors. Inthis paper, we analyze the propagation formulations behind the residualbuilding blocks, which suggest that the forward and backward signals can bedirectly propagated from one block to any other block, when using identitymappings as the skip connections and after-addition activation. A series ofablation experiments support the importance of these identity mappings. Thismotivates us to propose a new residual unit, which further makes training easyand improves generalization. We report improved results using a 1001-layerResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet onImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.
arxiv-16800-83 | Clustering of Sparse and Approximately Sparse Graphs by Semidefinite Programming | http://arxiv.org/abs/1603.05296 | author:Aleksis Pirinen, Brendan Ames category:math.OC stat.ML published:2016-03-16 summary:As a model problem for clustering, we consider the densest k-disjoint-cliqueproblem of partitioning a weighted complete graph into k disjoint subgraphssuch that the sum of the densities of these subgraphs is maximized. Weestablish that such subgraphs can be recovered from the solution of aparticular semidefinite relaxation with high probability if the input graph issampled from a distribution of clusterable graphs. Specifically, thesemidefinite relaxation is exact if the graph consists of k large disjointsubgraphs, corresponding to clusters, with weight concentrated within thesesubgraphs, plus a moderate number of outliers. Further, we establish that ifnoise is weakly obscuring these clusters, i.e, the between-cluster edges areassigned very small weights, then we can recover significantly smallerclusters. For example, we show that in approximately sparse graphs, where thebetween-cluster weights tend to zero as the size n of the graph tends toinfinity, we can recover clusters of size polylogarithmic in n. Empiricalevidence from numerical simulations is also provided to support thesetheoretical phase transitions to perfect recovery of the cluster structure.
arxiv-16800-84 | Persistent Homology of Attractors For Action Recognition | http://arxiv.org/abs/1603.05310 | author:Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, Pavan Turaga category:cs.CG cs.CV published:2016-03-16 summary:In this paper, we propose a novel framework for dynamical analysis of humanactions from 3D motion capture data using topological data analysis. We modelhuman actions using the topological features of the attractor of the dynamicalsystem. We reconstruct the phase-space of time series corresponding to actionsusing time-delay embedding, and compute the persistent homology of thephase-space reconstruction. In order to better represent the topologicalproperties of the phase-space, we incorporate the temporal adjacencyinformation when computing the homology groups. The persistence of thesehomology groups encoded using persistence diagrams are used as features for theactions. Our experiments with action recognition using these featuresdemonstrate that the proposed approach outperforms other baseline methods.
arxiv-16800-85 | Descriptor transition tables for object retrieval using unconstrained cluttered video acquired using a consumer level handheld mobile device | http://arxiv.org/abs/1603.05073 | author:Warren Rieutort-Louis, Ognjen Arandjelovic category:cs.CV published:2016-03-16 summary:Visual recognition and vision based retrieval of objects from large databasesare tasks with a wide spectrum of potential applications. In this paper wepropose a novel recognition method from video sequences suitable for retrievalfrom databases acquired in highly unconstrained conditions e.g. using a mobileconsumer-level device such as a phone. On the lowest level, we represent eachsequence as a 3D mesh of densely packed local appearance descriptors. Whileimage plane geometry is captured implicitly by a large overlap of neighbouringregions from which the descriptors are extracted, 3D information is extractedby means of a descriptor transition table, learnt from a single sequence foreach known gallery object. These allow us to connect local descriptors alongthe 3rd dimension (which corresponds to viewpoint changes), thus resulting in aset of variable length Markov chains for each video. The matching of two setsof such chains is formulated as a statistical hypothesis test, whereby a subsetof each is chosen to maximize the likelihood that the corresponding videosequences show the same object. The effectiveness of the proposed algorithm isempirically evaluated on the Amsterdam Library of Object Images and a newhighly challenging video data set acquired using a mobile phone. On both datasets our method is shown to be successful in recognition in the presence ofbackground clutter and large viewpoint changes.
arxiv-16800-86 | XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks | http://arxiv.org/abs/1603.05279 | author:Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi category:cs.CV published:2016-03-16 summary:We propose two efficient approximations to standard convolutional neuralnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,the filters are approximated with binary values resulting in 32x memory saving.In XNOR-Networks, both the filters and the input to convolutional layers arebinary. XNOR-Networks approximate convolutions using primarily binaryoperations. This results in 58x faster convolutional operations and 32x memorysavings. XNOR-Nets offer the possibility of running state-of-the-art networkson CPUs (rather than GPUs) in real-time. Our binary networks are simple,accurate, efficient, and work on challenging visual tasks. We evaluate ourapproach on the ImageNet classification task. The classification accuracy witha Binary-Weight-Network version of AlexNet is only 2.9% less than thefull-precision AlexNet (in top-1 measure). We compare our method with recentnetwork binarization methods, BinaryConnect and BinaryNets, and outperformthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.
arxiv-16800-87 | Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units | http://arxiv.org/abs/1603.05201 | author:Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee category:cs.LG cs.CV published:2016-03-16 summary:Recently, convolutional neural networks (CNNs) have been used as a powerfultool to solve many problems of machine learning and computer vision. In thispaper, we aim to provide insight on the property of convolutional neuralnetworks, as well as a generic method to improve the performance of many CNNarchitectures. Specifically, we first examine existing CNN models and observean intriguing property that the filters in the lower layers form pairs (i.e.,filters with opposite phase). Inspired by our observation, we propose a novel,simple yet effective activation scheme called concatenated ReLU (CRelu) andtheoretically analyze its reconstruction property in CNNs. We integrate CReluinto several state-of-the-art CNN architectures and demonstrate improvement intheir recognition performance on CIFAR-10/100 and ImageNet datasets with fewertrainable parameters. Our results suggest that better understanding of theproperties of CNNs can lead to significant performance improvement with asimple modification.
arxiv-16800-88 | Distributed Inexact Damped Newton Method: Data Partitioning and Load-Balancing | http://arxiv.org/abs/1603.05191 | author:Chenxin Ma, Martin Takáč category:cs.LG math.OC published:2016-03-16 summary:In this paper we study inexact dumped Newton method implemented in adistributed environment. We start with an original DiSCO algorithm[Communication-Efficient Distributed Optimization of Self-Concordant EmpiricalLoss, Yuchen Zhang and Lin Xiao, 2015]. We will show that this algorithm maynot scale well and propose an algorithmic modifications which will lead to lesscommunications, better load-balancing and more efficient computation. Weperform numerical experiments with an regularized empirical loss minimizationinstance described by a 273GB dataset.
arxiv-16800-89 | Applying Artifical Neural Networks To Predict Nominal Vehicle Performance | http://arxiv.org/abs/1603.05189 | author:Adam J. Last category:cs.NE cs.SY published:2016-03-16 summary:This paper investigates the use of artificial neural networks (ANNs) toreplace traditional algorithms and manual review for identifying anomalies invehicle run data. The specific data used for this study is from underseavehicle qualification tests. Such data is highly non-linear, thereforetraditional algorithms are not adequate and manual review is time consuming. Byusing ANNs to predict nominal vehicle performance based solely on informationavailable pre-run, vehicle deviation from expected performance can beautomatically identified in the post-run data. Such capability is only nowbecoming available due to the rapid increase in understanding of ANN frameworkand available computing power in the past decade. The ANN trained for thepurpose of this investigation is relatively simple, to keep the computingrequirements within the parameters of a modern desktop PC. This ANN showedpotential in predicting vehicle performance, particularly during transientevents within the run data. However, there were also several performance cases,such as steady state operation and cases which did not have sufficient trainingdata, where the ANN showed deficiencies. It is expected that as computationalpower becomes more readily available, ANN understanding matures, and moretraining data is acquired from real world tests, the performance predictions ofthe ANN will surpass traditional algorithms and manual human review.
arxiv-16800-90 | 2D Discrete Fourier Transform with Simultaneous Edge Artifact Removal for Real-Time Applications | http://arxiv.org/abs/1603.05154 | author:Faisal Mahmood, Märt Toots, Lars-Göran Öfverstedt, Ulf Skoglund category:cs.CV cs.AR published:2016-03-16 summary:Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic andcomputationally intensive algorithm, with a vast variety of applications. 2Dimages are, in general, non-periodic, but are assumed to be periodic whilecalculating their DFTs. This leads to cross-shaped artifacts in the frequencydomain due to spectral leakage. These artifacts can have critical consequencesif the DFTs are being used for further processing. In this paper we present anovel FPGA-based design to calculate high-throughput 2D DFTs with simultaneousedge artifact removal. Standard approaches for removing these artifacts usingapodization functions or mirroring, either involve removing criticalfrequencies or a surge in computation by increasing image size. We use aperiodic-plus-smooth decomposition based artifact removal algorithm optimizedfor FPGA implementation, while still achieving real-time ($\ge$23 frames persecond) performance for a 512$\times$512 size image stream. Our optimizationapproach leads to a significant decrease in external memory utilization therebyavoiding memory conflicts and simplifies the design. We have tested our designon a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC whichgives us the advantage to further expand the design for industrialapplications.
arxiv-16800-91 | Feature Selection as a Multiagent Coordination Problem | http://arxiv.org/abs/1603.05152 | author:Kleanthis Malialis, Jun Wang, Gary Brooks, George Frangou category:cs.LG stat.ML published:2016-03-16 summary:Datasets with hundreds to tens of thousands features is the new norm. Featureselection constitutes a central problem in machine learning, where the aim isto derive a representative set of features from which to construct aclassification (or prediction) model for a specific task. Our experimentalstudy involves microarray gene expression datasets, these are high-dimensionaland noisy datasets that contain genetic data typically used for distinguishingbetween benign or malicious tissues or classifying different types of cancer.In this paper, we formulate feature selection as a multiagent coordinationproblem and propose a novel feature selection method using multiagentreinforcement learning. The central idea of the proposed approach is to"assign" a reinforcement learning agent to each feature where each agent learnsto control a single feature, we refer to this approach as MARL. Applying thisto microarray datasets creates an enormous multiagent coordination problembetween thousands of learning agents. To address the scalability challenge weapply a form of reward shaping called CLEAN rewards. We compare in total ninefeature selection methods, including state-of-the-art methods, and show thatthe proposed method using CLEAN rewards can significantly scale-up, thusoutperforming the rest of learning-based methods. We further show that a hybridvariant of MARL achieves the best overall performance.
arxiv-16800-92 | Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions | http://arxiv.org/abs/1603.05145 | author:Qiyang Zhao, Lewis D Griffin category:cs.CV cs.AI cs.LG published:2016-03-16 summary:Many deep Convolutional Neural Networks (CNN) make incorrect predictions onadversarial samples obtained by imperceptible perturbations of clean samples.We hypothesize that this is caused by a failure to suppress unusual signalswithin network layers. As remedy we propose the use of Symmetric ActivationFunctions (SAF) in non-linear signal transducer units. These units suppresssignals of exceptional magnitude. We prove that SAF networks can performclassification tasks to arbitrary precision in a simplified situation. Inpractice, rather than use SAFs alone, we add them into CNNs to improve theirrobustness. The modified CNNs can be easily trained using popular strategieswith the moderate training load. Our experiments on MNIST and CIFAR-10 showthat the modified CNNs perform similarly to plain ones on clean samples, andare remarkably more robust against adversarial and nonsense samples.
arxiv-16800-93 | Recurrent Dropout without Memory Loss | http://arxiv.org/abs/1603.05118 | author:Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth category:cs.CL published:2016-03-16 summary:This paper presents a novel approach to recurrent neural network (RNN)regularization. Differently from the widely adopted dropout method, which isapplied to forward connections of feed-forward architectures or RNNs, wepropose to drop neurons directly in recurrent connections in a way that doesnot cause loss of long-term memory. Our approach is as easy to implement andapply as the regular feed-forward dropout and we demonstrate its effectivenessfor the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory(LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLPbenchmarks show consistent improvements even when combined with conventionalfeed-forward dropout.
arxiv-16800-94 | One-Shot Generalization in Deep Generative Models | http://arxiv.org/abs/1603.05106 | author:Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, Daan Wierstra category:stat.ML cs.AI cs.LG published:2016-03-16 summary:Humans have an impressive ability to reason about new concepts andexperiences from just a single example. In particular, humans have an abilityfor one-shot generalization: an ability to encounter a new concept, understandits structure, and then be able to generate compelling alternative variationsof the concept. We develop machine learning systems with this importantcapacity by developing new deep generative models, models that combine therepresentational power of deep learning with the inferential power of Bayesianreasoning. We develop a class of sequential generative models that are built onthe principles of feedback and attention. These two characteristics lead togenerative models that are among the state-of-the art in density estimation andimage generation. We demonstrate the one-shot generalization ability of ourmodels using three tasks: unconditional sampling, generating new exemplars of agiven concept, and generating new exemplars of a family of concepts. In allcases our models are able to generate compelling and diverse samples---havingseen new examples just once---providing an important class of general-purposemodels for one-shot machine learning.
arxiv-16800-95 | Short-term time series prediction using Hilbert space embeddings of autoregressive processes | http://arxiv.org/abs/1603.05060 | author:Edgar A. Valencia, Mauricio A. Álvarez category:stat.ML published:2016-03-16 summary:Linear autoregressive models serve as basic representations of discrete timestochastic processes. Different attempts have been made to provide non-linearversions of the basic autoregressive process, including different versionsbased on kernel methods. Motivated by the powerful framework of Hilbert spaceembeddings of distributions, in this paper we apply this methodology for thekernel embedding of an autoregressive process of order $p$. By doing so, weprovide a non-linear version of an autoregressive process, that shows increasedperformance over the linear model in highly complex time series. We use themethod proposed for one-step ahead forecasting of different time-series, andcompare its performance against other non-linear methods.
arxiv-16800-96 | Non-linear Dimensionality Regularizer for Solving Inverse Problems | http://arxiv.org/abs/1603.05015 | author:Ravi Garg, Anders Eriksson, Ian Reid category:cs.CV published:2016-03-16 summary:Consider an ill-posed inverse problem of estimating causal factors fromobservations, one of which is known to lie near some (un- known)low-dimensional, non-linear manifold expressed by a predefined Mercer-kernel.Solving this problem requires simultaneous estimation of these factors andlearning the low-dimensional representation for them. In this work, weintroduce a novel non-linear dimensionality regulariza- tion technique forsolving such problems without pre-training. We re-formulate Kernel-PCA as anenergy minimization problem in which low dimensionality constraints areintroduced as regularization terms in the energy. To the best of our knowledge,ours is the first at- tempt to create a dimensionality regularizer in the KPCAframework. Our approach relies on robustly penalizing the rank of the recoveredfac- tors directly in the implicit feature space to create theirlow-dimensional approximations in closed form. Our approach performs robustKPCA in the presence of missing data and noise. We demonstrate state-of-the-artresults on predicting missing entries in the standard oil flow dataset.Additionally, we evaluate our method on the challenging problem of Non-RigidStructure from Motion and our approach delivers promising results on CMU mocapdataset despite the presence of significant occlusions and noise.
arxiv-16800-97 | Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue | http://arxiv.org/abs/1603.04992 | author:Ravi Garg, Vijay Kumar BG, Ian Reid category:cs.CV published:2016-03-16 summary:A significant weakness of most current deep Convolutional Neural Networks isthe need to train them using vast amounts of manu- ally labelled data. In thiswork we propose a unsupervised framework to learn a deep convolutional neuralnetwork for single view depth predic- tion, without requiring a pre-trainingstage or annotated ground truth depths. We achieve this by training the networkin a manner analogous to an autoencoder. At training time we consider a pair ofimages, source and target, with small, known camera motion between the two suchas a stereo pair. We train the convolutional encoder for the task of predictingthe depth map for the source image. To do so, we explicitly generate an inversewarp of the target image using the predicted depth and known inter-viewdisplacement, to reconstruct the source image; the photomet- ric error in thereconstruction is the reconstruction loss for the encoder. The acquisition ofthis training data is considerably simpler than for equivalent systems,requiring no manual annotation, nor calibration of depth sensor to camera. Weshow that our network trained on less than half of the KITTI dataset (withoutany further augmentation) gives com- parable performance to that of the stateof art supervised methods for single view depth estimation.
arxiv-16800-98 | Scaled stochastic gradient descent for low-rank matrix completion | http://arxiv.org/abs/1603.04989 | author:Bamdev Mishra, Rodolphe Sepulchre category:cs.LG math.OC published:2016-03-16 summary:The paper looks at a scaled variant of the stochastic gradient descentalgorithm for the matrix completion problem. Specifically, we propose a novelmatrix-scaling of the partial derivatives that acts as an efficientpreconditioning for the standard stochastic gradient descent algorithm. Thisproposed matrix-scaling provides a trade-off between local and global secondorder information. It also resolves the issue of scale invariance that existsin matrix factorization models. The overall computational complexity is linearwith the number of known entries, thereby extending to a large-scale setup.Numerical comparisons show that the proposed algorithm competes favorably withstate-of-the-art algorithms on various different benchmarks.
arxiv-16800-99 | Comparing Convolutional Neural Networks to Traditional Models for Slot Filling | http://arxiv.org/abs/1603.05157 | author:Heike Adel, Benjamin Roth, Hinrich Schütze category:cs.CL published:2016-03-16 summary:We address relation classification in the context of slot filling, the taskof finding and evaluating fillers like "Steve Jobs" for the slot X in "Xfounded Apple". We propose a convolutional neural network which splits theinput sentence into three parts according to the relation arguments and compareit to state-of-the-art and traditional approaches of relation classification.Finally, we combine different methods and show that the combination is betterthan individual approaches. We also analyze the effect of genre differences onperformance.
arxiv-16800-100 | Regret-optimal Strategies for Playing Repeated Games with Discounted Losses | http://arxiv.org/abs/1603.04981 | author:Vijay Kamble, Patrick Loiseau, Jean Walrand category:cs.GT cs.DS cs.LG stat.ML published:2016-03-16 summary:The regret-minimization paradigm has emerged as a powerful technique fordesigning algorithms for online decision-making in adversarial environments.But so far, designing exact minmax-optimal algorithms for minimizing theworst-case regret has proven to be a difficult task in general, with only a fewknown results in specific settings. In this paper, we present a novelset-valued dynamic programming approach for designing such exact regret-optimalpolicies for playing repeated games with discounted losses. Our approach first draws the connection between regret minimization, anddetermining minimal achievable guarantees in repeated games with vector-valuedlosses. We then characterize the set of these minimal guarantees as the fixedpoint of a dynamic programming operator defined on the space of Paretofrontiers of convex and compact sets. This approach simultaneously results inthe characterization of the optimal strategies that achieve these minimalguarantees, and hence of regret-optimal strategies in the original repeatedgame. As an illustration of our approach, we design a simple near-optimalstrategy for prediction using expert advice for the case of 2 experts.
arxiv-16800-101 | Online Optimization in Dynamic Environments: Improved Regret Rates for Strongly Convex Problems | http://arxiv.org/abs/1603.04954 | author:Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, Alejandro Ribeiro category:cs.LG math.OC published:2016-03-16 summary:In this paper, we address tracking of a time-varying parameter with unknowndynamics. We formalize the problem as an instance of online optimization in adynamic setting. Using online gradient descent, we propose a method thatsequentially predicts the value of the parameter and in turn suffers a loss.The objective is to minimize the accumulation of losses over the time horizon,a notion that is termed dynamic regret. While existing methods focus on convexloss functions, we consider strongly convex functions so as to provide betterguarantees of performance. We derive a regret bound that captures thepath-length of the time-varying parameter, defined in terms of the distancebetween its consecutive values. In other words, the bound represents thenatural connection of tracking quality to the rate of change of the parameter.We provide numerical experiments to complement our theoretical findings.
arxiv-16800-102 | On the Complexity of One-class SVM for Multiple Instance Learning | http://arxiv.org/abs/1603.04947 | author:Zhen Hu, Zhuyin Xue category:cs.LG published:2016-03-16 summary:In traditional multiple instance learning (MIL), both positive and negativebags are required to learn a prediction function. However, a high human cost isneeded to know the label of each bag---positive or negative. Only positive bagscontain our focus (positive instances) while negative bags consist of noise orbackground (negative instances). So we do not expect to spend too much to labelthe negative bags. Contrary to our expectation, nearly all existing MIL methodsrequire enough negative bags besides positive ones. In this paper we propose analgorithm called "Positive Multiple Instance" (PMI), which learns a classifiergiven only a set of positive bags. So the annotation of negative bags becomesunnecessary in our method. PMI is constructed based on the assumption that theunknown positive instances in positive bags be similar each other andconstitute one compact cluster in feature space and the negative instanceslocate outside this cluster. The experimental results demonstrate that PMIachieves the performances close to or a little worse than those of thetraditional MIL algorithms on benchmark and real data sets. However, the numberof training bags in PMI is reduced significantly compared with traditional MILalgorithms.
arxiv-16800-103 | DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding | http://arxiv.org/abs/1603.04922 | author:Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, Jianxiong Xiao category:cs.CV published:2016-03-16 summary:While deep neural networks have led to human-level performance on computervision tasks, they have yet to demonstrate similar gains for holistic sceneunderstanding. In particular, 3D context has been shown to be an extremelyimportant cue for scene understanding - yet very little research has been doneon integrating context information with deep models. This paper presents anapproach to embed 3D context into the topology of a neural network trained toperform holistic scene understanding. Given a depth image depicting a 3D scene,our network aligns the observed scene with a predefined 3D scene template, andthen reasons about the existence and location of each object within the scenetemplate. In doing so, our model recognizes multiple objects in a singleforward pass of a 3D convolutional neural network, capturing both global sceneand local object information simultaneously. To create training data for this3D network, we generate partly hallucinated depth images which are rendered byreplacing real objects with a repository of CAD models of the same objectcategory. Extensive experiments demonstrate the effectiveness of our algorithmcompared to the state-of-the-arts. Source code and data will be available.
arxiv-16800-104 | Deep Fully-Connected Networks for Video Compressive Sensing | http://arxiv.org/abs/1603.04930 | author:Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos category:cs.CV cs.LG cs.MM published:2016-03-16 summary:In this work we present a deep learning framework for video compressivesensing. The proposed formulation enables recovery of video frames in a fewseconds at significantly improved reconstruction quality compared to previousapproaches. Our investigation starts by learning a linear mapping between videosequences and corresponding measured frames which turns out to providepromising results. We then extend the linear formulation to deepfully-connected networks and explore the performance gains using deeperarchitectures. Our analysis is always driven by the applicability of theproposed framework on existing compressive video architectures. Extensivesimulations on several video sequences document the superiority of our approachboth quantitatively and qualitatively. Finally, our analysis offers insightsinto understanding how dataset sizes and number of layers affect reconstructionperformance while raising a few points for future investigation.
arxiv-16800-105 | Hierarchical image simplification and segmentation based on Mumford-Shah-salient level line selection | http://arxiv.org/abs/1603.04838 | author:Yongchao Xu, Thierry Géraud, Laurent Najman category:cs.CV published:2016-03-15 summary:Hierarchies, such as the tree of shapes, are popular representations forimage simplification and segmentation thanks to their multiscale structures.Selecting meaningful level lines (boundaries of shapes) yields to simplifyimage while preserving intact salient structures. Many image simplification andsegmentation methods are driven by the optimization of an energy functional,for instance the celebrated Mumford-Shah functional. In this paper, we proposean efficient approach to hierarchical image simplification and segmentationbased on the minimization of the piecewise-constant Mumford-Shah functional.This method conforms to the current trend that consists in producinghierarchical results rather than a unique partition. Contrary to classicalapproaches which compute optimal hierarchical segmentations from an inputhierarchy of segmentations, we rely on the tree of shapes, a unique andwell-defined representation equivalent to the image. Simply put, we compute foreach level line of the image an attribute function that characterizes itspersistence under the energy minimization. Then we stack the level lines frommeaningless ones to salient ones through a saliency map based on extinctionvalues defined on the tree-based shape space. Qualitative illustrations andquantitative evaluation on Weizmann segmentation evaluation databasedemonstrate the state-of-the-art performance of our method.
arxiv-16800-106 | Domain Adaptation via Maximum Independence of Domain Features | http://arxiv.org/abs/1603.04535 | author:Ke Yan, Lu Kou, David Zhang category:cs.CV cs.AI cs.LG published:2016-03-15 summary:When the distributions of the source and the target domains are different,domain adaptation techniques are needed. For example, in the field of sensorsand measurement, discrete and continuous distributional change often exist indata because of instrumental variation and time-varying sensor drift. In thispaper, we propose maximum independence domain adaptation (MIDA) to address thisproblem. Domain features are first defined to describe the backgroundinformation of a sample, such as the device label and acquisition time. Then,MIDA learns features which have maximal independence with the domain features,so as to reduce the inter-domain discrepancy in distributions. A featureaugmentation strategy is designed so that the learned projection isbackground-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploitingthe label information. The proposed methods can handle not only discretedomains in traditional domain adaptation problems but also continuousdistributional change such as the time-varying drift. In addition, they arenaturally applicable in supervised/semi-supervised/unsupervised classificationor regression problems with multiple domains. This flexibility brings potentialfor a wide range of applications. The effectiveness of our approaches isverified by experiments on synthetic datasets and four real-world ones onsensors, measurement, and computer vision.
arxiv-16800-107 | Revealing the Hidden Patterns of News Photos: Analysis of Millions of News Photos Using GDELT and Deep Learning-based Vision APIs | http://arxiv.org/abs/1603.04531 | author:Haewoon Kwak, Jisun An category:cs.CY cs.CV cs.IR published:2016-03-15 summary:In this work, we analyze more than two million news photos published inJanuary 2016. We demonstrate i) which objects appear the most in news photos;ii) what the sentiments of news photos are; iii) whether the sentiment of newsphotos is aligned with the tone of the text; iv) how gender is treated; and v)how differently political candidates are portrayed. To our best knowledge, thisis the first large-scale study of news photo contents using deep learning-basedvision APIs.
arxiv-16800-108 | Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval | http://arxiv.org/abs/1603.04595 | author:Olivier Morère, Jie Lin, Antoine Veillard, Vijay Chandrasekhar, Tomaso Poggio category:cs.CV cs.IR published:2016-03-15 summary:The goal of this work is the computation of very compact binary hashes forimage instance retrieval. Our approach has two novel contributions. The firstone is Nested Invariance Pooling (NIP), a method inspired from i-theory, amathematical theory for computing group invariant transformations withfeed-forward neural networks. NIP is able to produce compact andwell-performing descriptors with visual representations extracted fromconvolutional neural networks. We specifically incorporate scale, translationand rotation invariances but the scheme can be extended to any arbitrary setsof transformations. We also show that using moments of increasing orderthroughout nesting is important. The NIP descriptors are then hashed to thetarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novelbatch-level regularization scheme specifically designed for the purpose ofhashing (RBMH). A thorough empirical evaluation with state-of-the-art showsthat the results obtained both with the NIP descriptors and the NIP+RBMH hashesare consistently outstanding across a wide range of datasets.
arxiv-16800-109 | Know Your Customer: Multi-armed Bandits with Capacity Constraints | http://arxiv.org/abs/1603.04549 | author:Ramesh Johari, Vijay Kamble, Yash Kanoria category:cs.LG cs.DS stat.ME stat.ML published:2016-03-15 summary:A wide range of resource allocation and platform operation settings exhibitthe following two simultaneous challenges: (1) service resources are capacityconstrained; and (2) clients' preferences are not perfectly known. To studythis pair of challenges, we consider a service system with heterogeneousservers and clients. Server types are known and there is fixed capacity ofservers of each type. Clients arrive over time, with types initially unknownand drawn from some distribution. Each client sequentially brings $N$ jobsbefore leaving. The system operator assigns each job to some server type,resulting in a payoff whose distribution depends on the client and servertypes. Our main contribution is a complete characterization of the structure of theoptimal policy for maximization of the rate of payoff accumulation. Such apolicy must balance three goals: (i) earning immediate payoffs; (ii) learningclient types to increase future payoffs; and (iii) satisfying the capacityconstraints. We construct a policy that has provably optimal regret (to leadingorder as $N$ grows large). Our policy has an appealingly simple three-phasestructure: a short type-"guessing" phase, a type-"confirmation" phase thatbalances payoffs with learning, and finally an "exploitation" phase thatfocuses on payoffs. Crucially, our approach employs the shadow prices of thecapacity constraints in the assignment problem with known types as "externalityprices" on the servers' capacity.
arxiv-16800-110 | Effective Computer Model For Recognizing Nationality From Frontal Image | http://arxiv.org/abs/1603.04550 | author:Bat-Erdene Batsukh, Ganbat Tsend category:cs.CV published:2016-03-15 summary:We are introducing new effective computer model for extracting nationalityfrom frontal image candidate using face part color, size and distances based ondeep research. Determining face part size, color, and distances is depending ona variety of factors including image quality, lighting condition, rotationangle, occlusion and facial emotion. Therefore, first we need to detect a faceon the image then convert an image into the real input. After that, we candetermine image candidate gender, face shape, key points and face parts.Finally, we will return the result, based on the comparison of sizes anddistances with the sample measurement table database. While we were measuringsamples, there were big differences between images by their gender and faceshapes. Input images must be the frontal face image that has smooth lightingand does not have any rotation angle. The model can be used in military,police, defense, healthcare, and technology sectors. Finally, Computer candistinguish nationality from the face image.
arxiv-16800-111 | Object Contour Detection with a Fully Convolutional Encoder-Decoder Network | http://arxiv.org/abs/1603.04530 | author:Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang category:cs.CV cs.LG published:2016-03-15 summary:We develop a deep learning algorithm for contour detection with a fullyconvolutional encoder-decoder network. Different from previous low-level edgedetection, our algorithm focuses on detecting higher-level object contours. Ournetwork is trained end-to-end on PASCAL VOC with refined ground truth frominaccurate polygon annotations, yielding much higher precision in objectcontour detection than previous methods. We find that the learned modelgeneralizes well to unseen object classes from the same super-categories on MSCOCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning.By combining with the multiscale combinatorial grouping algorithm, our methodcan generate high-quality segmented object proposals, which significantlyadvance the state-of-the-art on PASCAL VOC (improving average recall from 0.62to 0.67) with a relatively small amount of candidates ($\sim$1660 per image).
arxiv-16800-112 | Pushing the Limits of Deep CNNs for Pedestrian Detection | http://arxiv.org/abs/1603.04525 | author:Qichang Hu, Peng Wang, Chunhua Shen, Anton van den Hengel, Fatih Porikli category:cs.CV published:2016-03-15 summary:Compared to other applications in computer vision, convolutional neuralnetworks have under-performed on pedestrian detection. A breakthrough was madevery recently by using sophisticated deep CNN models, with a number ofhand-crafted features, or explicit occlusion handling mechanism. In this work,we show that by re-using the convolutional feature maps (CFMs) of a deepconvolutional neural network (DCNN) model as image features to train anensemble of boosted decision models, we are able to achieve the best reportedaccuracy without using specially designed learning algorithms. We empiricallyidentify and disclose important implementation details. We also show that pixellabelling may be simply combined with a detector to boost the detectionperformance. By adding complementary hand-crafted features such as opticalflow, the DCNN based detector can be further improved. We set a new record onthe Caltech pedestrian dataset, lowering the log-average miss rate from$11.7\%$ to $8.9\%$, a relative improvement of $24\%$. We also achieve acomparable result to the state-of-the-art approaches on the KITTI dataset.
arxiv-16800-113 | Multichannel Variable-Size Convolution for Sentence Classification | http://arxiv.org/abs/1603.04513 | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2016-03-15 summary:We propose MVCNN, a convolution neural network (CNN) architecture forsentence classification. It (i) combines diverse versions of pretrained wordembeddings and (ii) extracts features of multigranular phrases withvariable-size convolution filters. We also show that pretraining MVCNN iscritical for good performance. MVCNN achieves state-of-the-art performance onfour tasks: on small-scale binary, small-scale multi-class and largescaleTwitter sentiment prediction and on subjectivity classification.
arxiv-16800-114 | Unsupervised Ranking Model for Entity Coreference Resolution | http://arxiv.org/abs/1603.04553 | author:Xuezhe Ma, Zhengzhong Liu, Eduard Hovy category:cs.CL cs.LG published:2016-03-15 summary:Coreference resolution is one of the first stages in deep languageunderstanding and its importance has been well recognized in the naturallanguage processing community. In this paper, we propose a generative,unsupervised ranking model for entity coreference resolution by introducingresolution mode variables. Our unsupervised system achieves 58.44% F1 score ofthe CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhanet al., 2012), outperforming the Stanford deterministic system (Lee et al.,2013) by 3.01%.
arxiv-16800-115 | On the exact recovery of sparse signals via conic relaxations | http://arxiv.org/abs/1603.04572 | author:Hongbo Dong category:stat.ML math.OC published:2016-03-15 summary:In this note we compare two recently proposed semidefinite relaxations forthe sparse linear regression problem by Pilanci, Wainwright and El Ghaoui(Sparse learning via boolean relaxations, 2015) and Dong, Chen and Linderoth(Relaxation vs. Regularization A conic optimization perspective of statisticalvariable selection, 2015). We focus on the cardinality constrained formulation,and prove that the relaxation proposed by Dong, etc. is theoretically no weakerthan the one proposed by Pilanci, etc. Therefore any sufficient condition ofexact recovery derived by Pilanci can be readily applied to the otherrelaxation, including their results on high probability recovery for Gaussianensemble. Finally we provide empirical evidence that the relaxation by Dong,etc. requires much fewer observations to guarantee the recovery of truesupport.
arxiv-16800-116 | Classification with Repulsion Tensors: A Case Study on Face Recognition | http://arxiv.org/abs/1603.04588 | author:Hawren Fang category:cs.CV I.5.2; I.4.10 published:2016-03-15 summary:We consider dimensionality reduction methods for face recognition in asupervised setting, using an image-as-matrix representation. A common procedureis to project image matrices into a smaller space in which the recognition isperformed. These methods are often called "two-dimensional" in the literatureand there exist counterparts that use an image-as-vector representation. Whentwo face images are close to each other in the input space they may remainclose after projection - but this is not desirable in the situation when thesetwo images are from different classes, and this often affects the recognitionperformance. We extend a previously developed `repulsion Laplacean' techniquebased on adding terms to the objective function with the goal or creation arepulsion energy between such images in the projected space. This scheme, whichrelies on a repulsion graph, is generic and can be incorporated into varioustwo-dimensional methods. It can be regarded as a multilinear generalization ofthe repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp.2392--2402]. Experimental results demonstrate that the proposed methodologyoffers significant recognition improvement relative to the underlyingtwo-dimensional methods.
arxiv-16800-117 | Scalable Image Retrieval by Sparse Product Quantization | http://arxiv.org/abs/1603.04614 | author:Qingqun Ning, Jianke Zhu, Zhiyuan Zhong, Steven C. H. Hoi, Chun Chen category:cs.CV published:2016-03-15 summary:Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensionalfeature indexing and retrieval is the crux of large-scale image retrieval. Arecent promising technique is Product Quantization, which attempts to indexhigh-dimensional image features by decomposing the feature space into aCartesian product of low dimensional subspaces and quantizing each of themseparately. Despite the promising results reported, their quantization approachfollows the typical hard assignment of traditional quantization methods, whichmay result in large quantization errors and thus inferior search performance.Unlike the existing approaches, in this paper, we propose a novel approachcalled Sparse Product Quantization (SPQ) to encoding the high-dimensionalfeature vectors into sparse representation. We optimize the sparserepresentations of the feature vectors by minimizing their quantization errors,making the resulting representation is essentially close to the original datain practice. Experiments show that the proposed SPQ technique is not only ableto compress data, but also an effective encoding technique. We obtainstate-of-the-art results for ANN search on four public image datasets and thepromising results of content-based image retrieval further validate theefficacy of our proposed method.
arxiv-16800-118 | Image Co-localization by Mimicking a Good Detector's Confidence Score Distribution | http://arxiv.org/abs/1603.04619 | author:Yao Li, Linqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-03-15 summary:Given a set of images containing objects from the same category, the task ofimage co-localization is to identify and localize each instance. This papershows that this problem can be solved by a simple but intriguing idea, that is,a common object detector can be learnt by making its detection confidencescores distributed like those of a strongly supervised detector. Morespecifically, we observe that given a set of object proposals extracted from animage that contains the object of interest, an accurate strongly supervisedobject detector should give high scores to only a small minority of proposals,and low scores to most of them. Thus, we devise an entropy-based objectivefunction to enforce the above property when learning the common objectdetector. Once the detector is learnt, we resort to a segmentation approach torefine the localization. We show that despite its simplicity, our approachoutperforms state-of-the-art methods.
arxiv-16800-119 | Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly machine learning | http://arxiv.org/abs/1603.04628 | author:David Stephenson, James R Kermode, Duncan A Lockerby category:stat.ML published:2016-03-15 summary:We present a hybrid continuum-atomistic scheme which combines moleculardynamics (MD) simulations with on-the-fly machine learning techniques for theaccurate and efficient prediction of multiscale fluidic systems. By using aGaussian process as a surrogate model for the computationally expensive MDsimulations, we use Bayesian inference to predict the system behaviour at theatomistic scale, purely by consideration of the macroscopic inputs and outputs.Whenever the uncertainty of this prediction is greater than a predeterminedacceptable threshold, a new MD simulation is performed to continually augmentthe database, which is never required to be complete. This provides asubstantial enhancement to the current generation of hybrid methods, whichoften require many similar atomistic simulations to be performed, discardinginformation after it is used once. We apply our hybrid scheme to nano-confined unsteady flow through ahigh-aspect-ratio converging-diverging channel, and make comparisons betweenthe new scheme and full MD simulations for a range of uncertainty thresholdsand initial databases. For low thresholds, our hybrid solution is highlyaccurate\,---\,within the thermal noise of a full MD simulation. As theuncertainty threshold is raised, the accuracy of our scheme decreases and thecomputational speed-up increases (relative to a full MD simulation), enablingthe compromise between precision and efficiency to be tuned. The speed-up ofour hybrid solution ranges from an order of magnitude, with no initialdatabase, to cases where an extensive initial database ensures no new MDsimulations are required.
arxiv-16800-120 | Modeling Time Series Similarity with Siamese Recurrent Networks | http://arxiv.org/abs/1603.04713 | author:Wenjie Pei, David M. J. Tax, Laurens van der Maaten category:cs.CV published:2016-03-15 summary:Traditional techniques for measuring similarities between time series arebased on handcrafted similarity measures, whereas more recent learning-basedapproaches cannot exploit external supervision. We combine ideas fromtime-series modeling and metric learning, and study siamese recurrent networks(SRNs) that minimize a classification loss to learn a good similarity measurebetween time series. Specifically, our approach learns a vectorialrepresentation for each time series in such a way that similar time series aremodeled by similar representations, and dissimilar time series by dissimilarrepresentations. Because it is a similarity prediction models, SRNs areparticularly well-suited to challenging scenarios such as signaturerecognition, in which each person is a separate class and very few examples perclass are available. We demonstrate the potential merits of SRNs inwithin-domain and out-of-domain classification experiments and in one-shotlearning experiments on tasks such as signature, voice, and sign languagerecognition.
arxiv-16800-121 | Topic Modeling Using Distributed Word Embeddings | http://arxiv.org/abs/1603.04747 | author:Ramandeep S Randhawa, Parag Jain, Gagan Madan category:cs.CL published:2016-03-15 summary:We propose a new algorithm for topic modeling, Vec2Topic, that identifies themain topics in a corpus using semantic information captured viahigh-dimensional distributed word embeddings. Our technique is unsupervised andgenerates a list of topics ranked with respect to importance. We find that itworks better than existing topic modeling techniques such as Latent DirichletAllocation for identifying key topics in user-generated content, such asemails, chats, etc., where topics are diffused across the corpus. We also findthat Vec2Topic works equally well for non-user generated content, such aspapers, reports, etc., and for small corpora such as a single-document.
arxiv-16800-122 | Evaluating the word-expert approach for Named-Entity Disambiguation | http://arxiv.org/abs/1603.04767 | author:Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning, Eneko Agirre category:cs.CL published:2016-03-15 summary:Named Entity Disambiguation (NED) is the task of linking a named-entitymention to an instance in a knowledge-base, typically Wikipedia. This task isclosely related to word-sense disambiguation (WSD), where the supervisedword-expert approach has prevailed. In this work we present the results of theword-expert approach to NED, where one classifier is built for each targetentity mention string. The resources necessary to build the system, adictionary and a set of training instances, have been automatically derivedfrom Wikipedia. We provide empirical evidence of the value of this approach, aswell as a study of the differences between WSD and NED, including ambiguity andsynonymy statistics.
arxiv-16800-123 | A Neural Approach to Blind Motion Deblurring | http://arxiv.org/abs/1603.04771 | author:Ayan Chakrabarti category:cs.CV published:2016-03-15 summary:We present a new method for blind motion deblurring that uses a neuralnetwork trained to compute estimates of sharp image patches from observationsthat are blurred by an unknown motion kernel. Instead of regressing directly topatch intensities, this network learns to predict the complex Fouriercoefficients of a deconvolution filter to be applied to the input patch forrestoration. For inference, we apply the network independently to alloverlapping patches in the observed image, and average its outputs to form aninitial estimate of the sharp image. We then explicitly estimate a singleglobal blur kernel by relating this estimate to the observed image, and finallyperform non-blind deconvolution with this kernel. Our method exhibits accuracyand robustness close to state-of-the-art iterative methods, while being muchfaster when parallelized on GPU hardware.
arxiv-16800-124 | Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images | http://arxiv.org/abs/1603.04833 | author:Debapriya Maji, Anirban Santara, Pabitra Mitra, Debdoot Sheet category:cs.LG cs.CV stat.ML published:2016-03-15 summary:Vision impairment due to pathological damage of the retina can largely beprevented through periodic screening using fundus color imaging. However thechallenge with large scale screening is the inability to exhaustively detectfine blood vessels crucial to disease diagnosis. In this work we present acomputational imaging framework using deep and ensemble learning for reliabledetection of blood vessels in fundus color images. An ensemble of deepconvolutional neural networks is trained to segment vessel and non-vessel areasof a color fundus image. During inference, the responses of the individualConvNets of the ensemble are averaged to form the final segmentation. Inexperimental evaluation with the DRIVE database, we achieve the objective ofvessel detection with maximum average accuracy of 94.7\% and area under ROCcurve of 0.9283.
arxiv-16800-125 | Efficient Globally Optimal Point Cloud Alignment using Bayesian Nonparametric Mixtures | http://arxiv.org/abs/1603.04868 | author:Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III category:cs.CV published:2016-03-15 summary:Point cloud alignment is a common problem in computer vision and robotics,with applications ranging from object recognition to reconstruction. We proposea novel approach to the alignment problem that utilizes Bayesian nonparametricsto describe the point cloud and surface normal densities, and the branch andbound (BB) paradigm to recover the optimal relative transformation. BB relieson a novel, refinable, approximately-uniform tessellation of the rotation spaceusing 4D tetrahedra which leads to more efficient BB operation in comparison tothe common axis-angle tessellation. For this novel tessellation, we provideupper and lower objective function bounds, and prove convergence and optimalityof the BB approach under mild assumptions. Finally, we empirically demonstratethe efficiency of the proposed approach as well as its robustness to suboptimalreal-world conditions such as missing data and partial overlap.
arxiv-16800-126 | Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation | http://arxiv.org/abs/1603.04871 | author:Zhicheng Yan, Hao Zhang, Yangqing Jia, Thomas Breuel, Yizhou Yu category:cs.CV published:2016-03-15 summary:State-of-the-art results of semantic segmentation are established by FullyConvolutional neural Networks (FCNs). FCNs rely on cascaded convolutional andpooling layers to gradually enlarge the receptive fields of neurons, resultingin an indirect way of modeling the distant contextual dependence. In this work,we advocate the use of spatially recurrent layers (i.e. ReNet layers) whichdirectly capture global contexts and lead to improved feature representations.We demonstrate the effectiveness of ReNet layers by building a Naive deep ReNet(N-ReNet), which achieves competitive performance on Stanford Backgrounddataset. Furthermore, we integrate ReNet layers with FCNs, and develop a novelHybrid deep ReNet (H-ReNet). It enjoys a few remarkable properties, includingfull-image receptive fields, end-to-end training, and efficient networkexecution. On the PASCAL VOC 2012 benchmark, the H-ReNet improves the resultsof state-of-the-art approaches Piecewise, CRFasRNN and DeepParsing by 3.6%,2.3% and 0.2%, respectively, and achieves the highest IoUs for 13 out of the 20object classes.
arxiv-16800-127 | Bias Correction for Regularized Regression and its Application in Learning with Streaming Data | http://arxiv.org/abs/1603.04882 | author:Qiang Wu category:stat.ML cs.LG published:2016-03-15 summary:We propose an approach to reduce the bias of ridge regression andregularization kernel network. When applied to a single data set the newalgorithms have comparable learning performance with the original ones. Whenapplied to incremental learning with block wise streaming data the newalgorithms are more efficient due to bias reduction. Both theoreticalcharacterizations and simulation studies are used to verify the effectivenessof these new algorithms.
arxiv-16800-128 | Turing learning: a metric-free approach to inferring behavior and its application to swarms | http://arxiv.org/abs/1603.04904 | author:Wei Li, Melvin Gauci, Roderich Gross category:stat.ML cs.LG cs.NE published:2016-03-15 summary:We propose Turing Learning, a novel system identification method forinferring behavior. Turing Learning simultaneously optimizes models andclassifiers. The classifiers are provided with data samples from both an agentand models under observation, and are rewarded for discriminating between them.Conversely, the models are rewarded for 'tricking' the classifiers intocategorizing them as the agent. Unlike other methods for system identification,Turing Learning does not require predefined metrics to quantify the differencebetween the agent and models. We present two case studies with swarms ofsimulated robots that show that Turing Learning outperforms a metric-basedsystem identification method in terms of model accuracy. The classifiersperform well collectively and could be used to detect abnormal behavior in theswarm. Moreover, we show that Turing Learning also successfully infers thebehavior of physical robot swarms. The results show that collective behaviorscan be directly inferred from motion trajectories of a single agent in theswarm, which may have significant implications for the study of animalcollectives.
arxiv-16800-129 | First Person Action-Object Detection with EgoNet | http://arxiv.org/abs/1603.04908 | author:Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi category:cs.CV published:2016-03-15 summary:Objects afford visual sensation and motor actions. A first person camera,placed at the person's head, captures unscripted moments of our visualsensorimotor object interactions. Can a single first person image tell us aboutour momentary visual attention and motor action with objects, without a gazetracking device or tactile sensors? To study the holistic correlation of visualattention with motor action, we introduce the concept ofaction-objects---objects associated with seeing and touching actions, whichexhibit characteristic 3D spatial distance and orientation with respect to theperson. A predictive action-object model is designed to re-organize the spaceof interactions in terms of visual and tactile sensations, which is realized byour proposed EgoNet network. EgoNet is composed of two convolutional neuralnetworks: 1) Semantic Gaze Pathway that learns 2D appearance cues with firstperson coordinate embedding, and 2) 3D Spatial Pathway that focuses on 3D depthand height measurements relative to the person with brightness reflectanceattached. Retaining two distinct pathways enables effective learning from alimited number of examples, diversified prediction from complementary visualsignals, and flexible architecture that is functional with RGB image withoutdepth information. We show that our model correctly predicts action-objects ina first person image where we outperform the existing approaches acrossdifferent datasets.
arxiv-16800-130 | Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors | http://arxiv.org/abs/1603.04733 | author:Christos Louizos, Max Welling category:stat.ML cs.LG published:2016-03-15 summary:We introduce a variational Bayesian neural network where the parameters aregoverned via a probability distribution on random matrices. Specifically, weemploy a matrix variate Gaussian \cite{gupta1999matrix} parameter posteriordistribution where we explicitly model the covariance among the input andoutput dimensions of each layer. Furthermore, with approximate covariancematrices we can achieve a more efficient way to represent those correlationsthat is also cheaper than fully factorized parameter posteriors. We furthershow that with the "local reprarametrization trick"\cite{kingma2015variational} on this posterior distribution we arrive at aGaussian Process \cite{rasmussen2006gaussian} interpretation of the hiddenunits in each layer and we, similarly with \cite{gal2015dropout}, provideconnections with deep Gaussian processes. We continue in taking advantage ofthis duality and incorporate "pseudo-data" \cite{snelson2005sparse} in ourmodel, which in turn allows for more efficient sampling while maintaining theproperties of the original model. The validity of the proposed approach isverified through extensive experiments.
arxiv-16800-131 | Data Clustering and Graph Partitioning via Simulated Mixing | http://arxiv.org/abs/1603.04918 | author:Shahzad Bhatti, Carolyn Beck, Angelia Nedic category:cs.LG stat.ML published:2016-03-15 summary:Spectral clustering approaches have led to well-accepted algorithms forfinding accurate clusters in a given dataset. However, their application tolarge-scale datasets has been hindered by computational complexity ofeigenvalue decompositions. Several algorithms have been proposed in the recentpast to accelerate spectral clustering, however they compromise on the accuracyof the spectral clustering to achieve faster speed. In this paper, we propose anovel spectral clustering algorithm based on a mixing process on a graph.Unlike the existing spectral clustering algorithms, our algorithm does notrequire computing eigenvectors. Specifically, it finds the equivalent of alinear combination of eigenvectors of the normalized similarity matrix weightedwith corresponding eigenvalues. This linear combination is then used topartition the dataset into meaningful clusters. Simulations on real datasetsshow that partitioning datasets based on such linear combinations ofeigenvectors achieves better accuracy than standard spectral clustering methodsas the number of clusters increase. Our algorithm can easily be implemented ina distributed setting.
arxiv-16800-132 | Revisiting Batch Normalization For Practical Domain Adaptation | http://arxiv.org/abs/1603.04779 | author:Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou category:cs.CV cs.LG published:2016-03-15 summary:Deep neural networks (DNN) have shown unprecedented success in variouscomputer vision applications such as image classification and object detection.However, it is still a common (yet inconvenient) practice to prepare at leasttens of thousands of labeled image to fine-tune a network on every task beforethe model is ready to use. Recent study shows that a DNN has strong dependencytowards the training dataset, and the learned features cannot be easilytransferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive BatchNormalization(AdaBN), to increase the generalization ability of a DNN. Ourapproach is based on the well-known Batch Normalization technique which hasbecome a standard component in modern deep learning. In contrary to other deeplearning domain adaptation methods, our method does not require additionalcomponents, and is parameter-free. It archives state-of-the-art performancedespite its surprising simplicity. Furthermore, we demonstrate that our methodis complementary with other existing methods. Combining AdaBN with existingdomain adaptation treatments may further improve model performance.
arxiv-16800-133 | How deep is knowledge tracing? | http://arxiv.org/abs/1604.02416 | author:Mohammad Khajah, Robert V. Lindsey, Michael C. Mozer category:cs.AI cs.NE published:2016-03-14 summary:In theoretical cognitive science, there is a tension between highlystructured models whose parameters have a direct psychological interpretationand highly complex, general-purpose models whose parameters and representationsare difficult to interpret. The former typically provide more insight intocognition but the latter often perform better. This tension has recentlysurfaced in the realm of educational data mining, where a deep learningapproach to predicting students' performance as they work through a series ofexercises---termed deep knowledge tracing or DKT---has demonstrated a stunningperformance advantage over the mainstay of the field, Bayesian knowledgetracing or BKT. In this article, we attempt to understand the basis for DKT'sadvantage by considering the sources of statistical regularity in the data thatDKT can leverage but which BKT cannot. We hypothesize four forms of regularitythat BKT fails to exploit: recency effects, the contextualized trial sequence,inter-skill similarity, and individual variation in ability. We demonstratethat when BKT is extended to allow it more flexibility in modeling statisticalregularities---using extensions previously proposed in the literature---BKTachieves a level of performance indistinguishable from that of DKT. We arguethat while DKT is a powerful, useful, general-purpose framework for modelingstudent learning, its gains do not come from the discovery of novelrepresentations---the fundamental advantage of deep learning. To answer thequestion posed in our title, knowledge tracing may be a domain that does notrequire `depth'; shallow models like BKT can perform just as well and offer usgreater interpretability and explanatory power.
arxiv-16800-134 | RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape Feature | http://arxiv.org/abs/1603.04134 | author:Xiaoyang Li, Kanzhi Wu, Yong Liu, Ravindra Ranasinghe, Gamini Dissanayake, Rong Xiong category:cs.RO cs.CV published:2016-03-14 summary:In this paper, we present a novel RGB-D feature, RISAS, which is robust toRotation, Illumination and Scale variations through fusing Appearance and Shapeinformation. We propose a keypoint detector which is able to extractinformation rich regions in both appearance and shape using a novel 3Dinformation representation method in combination with grayscale information. Weextend our recent work on Local Ordinal Intensity and Normal Descriptor(LOIND),to further significantly improve its illumination, scale and rotationinvariance using 1) a precise neighbourhood region selection method and 2) amore robust dominant orientation estimation. We also present a dataset forevaluation of RGB-D features, together with comprehensive experiments toillustrate the effectiveness of the proposed RGB-D feature when compared toSIFT, C-SHOT and LOIND. We also show the use of RISAS for point cloud alignmentassociated with many robotics applications and demonstrate its effectiveness ina poorly illuminated environment when compared with SIFT and ORB.
arxiv-16800-135 | Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains | http://arxiv.org/abs/1603.04119 | author:David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert E. Schapire category:cs.AI cs.LG stat.ML published:2016-03-14 summary:High-dimensional observations and complex real-world dynamics present majorchallenges in reinforcement learning for both function approximation andexploration. We address both of these challenges with two complementarytechniques: First, we develop a gradient-boosting style, non-parametricfunction approximator for learning on $Q$-function residuals. And second, wepropose an exploration strategy inspired by the principles of state abstractionand information acquisition under uncertainty. We demonstrate the empiricaleffectiveness of these techniques, first, as a preliminary check, on twostandard tasks (Blackjack and $n$-Chain), and then on two much larger and morerealistic tasks with high-dimensional observation spaces. Specifically, weintroduce two benchmarks built within the game Minecraft where the observationsare pixel arrays of the agent's visual field. A combination of our twoalgorithmic techniques performs competitively on the standardreinforcement-learning tasks while consistently and substantially outperformingbaselines on the two tasks with high-dimensional observation spaces. The newfunction approximator, exploration strategy, and evaluation benchmarks are eachof independent interest in the pursuit of reinforcement-learning methods thatscale to real-world domains.
arxiv-16800-136 | Bandit Approaches to Preference Learning Problems with Multiple Populations | http://arxiv.org/abs/1603.04118 | author:Aniruddha Bhargava, Ravi Ganti, Robert Nowak category:stat.ML cs.AI cs.LG published:2016-03-14 summary:In this paper we study an extension of the stochastic multi-armed bandit(MAB) framework, where in each round a player can play multiple actions andreceive a stochastic reward which depends on the actions played. This problemis motivated by applications in recommendation problems where there aremultiple populations of users and hence no single choice might be good for theentire population. We specifically look at bandit problems where we are allowedto make two choices in each round. We provide algorithms for this problem inboth the noiseless and noisy case. Our algorithms are computationally efficientand have provable sample complexity guarantees. In the process of establishingsample complexity guarantees for our algorithms, we establish new resultsregarding the Nystr{\"o}m method which can be of independent interest. Wesupplement our theoretical results with experimental comparisons.
arxiv-16800-137 | Multi-modal Tracking for Object based SLAM | http://arxiv.org/abs/1603.04117 | author:Prateek Singhal, Ruffin White, Henrik Christensen category:cs.CV published:2016-03-14 summary:We present an on-line 3D visual object tracking framework for monocularcameras by incorporating spatial knowledge and uncertainty from semanticmapping along with high frequency measurements from visual odometry. Using acombination of vision and odometry that are tightly integrated we can increasethe overall performance of object based tracking for semantic mapping. Wepresent a framework for integration of the two data-sources into a coherentframework through information based fusion/arbitration. We demonstrate theframework in the context of OmniMapper[1] and present results on 6 challengingsequences over multiple objects compared to data obtained from a motion capturesystems. We are able to achieve a mean error of 0.23m for per frame trackingshowing 9% relative error less than state of the art tracker.
arxiv-16800-138 | SSSC-AM: A Unified Framework for Video Co-Segmentation by Structured Sparse Subspace Clustering with Appearance and Motion Features | http://arxiv.org/abs/1603.04139 | author:Junlin Yao, Frank Nielsen category:cs.CV published:2016-03-14 summary:Video co-segmentation refers to the task of jointly segmenting common objectsappearing in a given group of videos. In practice, high-dimensional data suchas videos can be conceptually thought as being drawn from a union of subspacescorresponding to categories rather than from a smooth manifold. Therefore,segmenting data into respective subspaces --- subspace clustering --- findswidespread applications in computer vision, including co-segmentation.State-of-the-art methods via subspace clustering seek to solve the problem intwo steps: First, an affinity matrix is built from data, with appearance features ormotion patterns. Second, the data are segmented by applying spectral clusteringto the affinity matrix. However, this process is insufficient to obtain anoptimal solution since it does not take into account the {\em interdependence}of the affinity matrix with the segmentation. In this work, we present a novelunified video co-segmentation framework inspired by the recent StructuredSparse Subspace Clustering ($\mathrm{S^{3}C}$) based on the {\emself-expressiveness} model. Our method yields more consistent segmentationresults. In order to improve the detectability of motion features with missingtrajectories due to occlusion or tracked points moving out of frames, we add anextra-dimensional signature to the motion trajectories. Moreover, wereformulate the $\mathrm{S^{3}C}$ algorithm by adding the affine subspaceconstraint in order to make it more suitable to segment rigid motions lying inaffine subspaces of dimension at most $3$. Experiments on MOViCS datasetdemonstrate the effectiveness of our approaches and its robustness to heavynoise.
arxiv-16800-139 | U-CATCH: Using Color ATtribute of image patCHes in binary descriptors | http://arxiv.org/abs/1603.04408 | author:Ozgur Yilmaz, Alisher Abdulkhaev category:cs.CV published:2016-03-14 summary:In this study, we propose a simple yet very effective method for extractingcolor information through binary feature description framework. Our methodexpands the dimension of binary comparisons into RGB and YCbCr spaces, showingmore than 100% matching improve ment compared to non-color binary descriptorsfor a wide range of hard-to-match cases. The proposed method is general and canbe applied to any binary descriptor to make it color sensitive. It is fasterthan classical binary descriptors for RGB sampling due to the abandonment ofgrayscale conversion and has almost identical complexity (insignificantcompared to smoothing operation) for YCbCr sampling.
arxiv-16800-140 | Regression-based Hypergraph Learning for Image Clustering and Classification | http://arxiv.org/abs/1603.04150 | author:Sheng Huang, Dan Yang, Bo Liu, Xiaohong Zhang category:cs.CV published:2016-03-14 summary:Inspired by the recently remarkable successes of Sparse Representation (SR),Collaborative Representation (CR) and sparse graph, we present a novelhypergraph model named Regression-based Hypergraph (RH) which utilizes theregression models to construct the high quality hypergraphs. Moreover, we plugRH into two conventional hypergraph learning frameworks, namely hypergraphspectral clustering and hypergraph transduction, to present Regression-basedHypergraph Spectral Clustering (RHSC) and Regression-based HypergraphTransduction (RHT) models for addressing the image clustering andclassification issues. Sparse Representation and Collaborative Representationare employed to instantiate two RH instances and their RHSC and RHT algorithms.The experimental results on six popular image databases demonstrate that theproposed RH learning algorithms achieve promising image clustering andclassification performances, and also validate that RH can inherit thedesirable properties from both hypergraph models and regression models.
arxiv-16800-141 | Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal | http://arxiv.org/abs/1603.04153 | author:Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh category:cs.LG cs.IT cs.SI math.IT stat.ML published:2016-03-14 summary:We explore the top-$K$ rank aggregation problem. Suppose a collection ofitems is compared in pairs repeatedly, and we aim to recover a consistentordering that focuses on the top-$K$ ranked items based on partially revealedpreference information. We investigate the Bradley-Terry-Luce model in whichone ranks items according to their perceived utilities modeled as noisyobservations of their underlying true utilities. Our main contributions aretwo-fold. First, in a general comparison model where item pairs to compare aregiven a priori, we attain an upper and lower bound on the sample size forreliable recovery of the top-$K$ ranked items. Second, more importantly,extending the result to a random comparison model where item pairs to compareare chosen independently with some probability, we show that in slightlyrestricted regimes, the gap between the derived bounds reduces to a constantfactor, hence reveals that a spectral method can achieve the minimax optimalityon the (order-wise) sample size required for top-$K$ ranking. That is to say,we demonstrate a spectral method alone to be sufficient to achieve theoptimality and advantageous in terms of computational complexity, as it doesnot require an additional stage of maximum likelihood estimation that astate-of-the-art scheme employs to achieve the optimality. We corroborate ourmain results by numerical experiments.
arxiv-16800-142 | Conformal Predictors for Compound Activity Prediction | http://arxiv.org/abs/1603.04506 | author:Paolo Toccacheli, Ilia Nouretdinov, Alexander Gammerman category:cs.LG published:2016-03-14 summary:The paper presents an application of Conformal Predictors to achemoinformatics problem of identifying activities of chemical compounds. Thepaper addresses some specific challenges of this domain: a large number ofcompounds (training examples), high-dimensionality of feature space, sparsenessand a strong class imbalance. A variant of conformal predictors calledInductive Mondrian Conformal Predictor is applied to deal with thesechallenges. Results are presented for several non-conformity measures (NCM)extracted from underlying algorithms and different kernels. A number ofperformance measures are used in order to demonstrate the flexibility ofInductive Mondrian Conformal Predictors in dealing with such a complex set ofdata. Keywords: Conformal Prediction, Confidence Estimation, Chemoinformatics,Non-Conformity Measure.
arxiv-16800-143 | Extended Object Tracking: Introduction, Overview and Applications | http://arxiv.org/abs/1604.00970 | author:Karl Granstrom, Marcus Baum category:cs.CV cs.SY published:2016-03-14 summary:This article provides an elaborate overview of current research in extendedobject tracking. We provide a clear definition of an extended object anddiscuss its delimitation to other object types and sensor models. Next,different shape models and possibilities to model the number of measurementsare extensively discussed. Subsequently, we give a tutorial introduction to twobasic and well used extended object tracking methods -- the random matrixapproach and random hypersurface approach. The next part treats approaches fortracking multiple extended objects and elaborates how the large number offeasible association hypotheses can be tackled using both Random Finite Set(RFS) and Non-RFS multi-object trackers. The article concludes with a summaryof current applications, where three example applications involving Lidar, RGB,and RGB-D sensors are highlighted.
arxiv-16800-144 | Learning Binary Codes and Binary Weights for Efficient Classification | http://arxiv.org/abs/1603.04116 | author:Fumin Shen, Yadong Mu, Wei Liu, Yang Yang, Heng Tao Shen category:cs.CV published:2016-03-14 summary:This paper proposes a generic formulation that significantly expedites thetraining and deployment of image classification models, particularly under thescenarios of many image categories and high feature dimensions. As a definingproperty, our method represents both the images and learned classifiers usingbinary hash codes, which are simultaneously learned from the training data.Classifying an image thereby reduces to computing the Hamming distance betweenthe binary codes of the image and classifiers and selecting the class withminimal Hamming distance. Conventionally, compact hash codes are primarily usedfor accelerating image search. Our work is first of its kind to representclassifiers using binary codes. Specifically, we formulate multi-class imageclassification as an optimization problem over binary variables. Theoptimization alternatively proceeds over the binary classifiers and image hashcodes. Profiting from the special property of binary codes, we show that thesub-problems can be efficiently solved through either a binary quadraticprogram (BQP) or linear program. In particular, for attacking the BQP problem,we propose a novel bit-flipping procedure which enjoys high efficacy and localoptimality guarantee. Our formulation supports a large family of empirical lossfunctions and is here instantiated by exponential / hinge losses. Comprehensiveevaluations are conducted on several representative image benchmarks. Theexperiments consistently observe reduced complexities of model training anddeployment, without sacrifice of accuracies.
arxiv-16800-145 | Modeling and Estimation of Discrete-Time Reciprocal Processes via Probabilistic Graphical Models | http://arxiv.org/abs/1603.04419 | author:Francesca Paola Carli category:stat.ML math.OC published:2016-03-14 summary:Reciprocal processes are acausal generalizations of Markov processesintroduced by Bernstein in 1932. In the literature, a significant amount ofattention has been focused on developing dynamical models for reciprocalprocesses. In this paper, we provide a probabilistic graphical model forreciprocal processes. This leads to a principled solution of the smoothingproblem via message passing algorithms. For the finite state space case,convergence analysis is revisited via the Hilbert metric.
arxiv-16800-146 | Visual Concept Recognition and Localization via Iterative Introspection | http://arxiv.org/abs/1603.04186 | author:Amir Rosenfeld, Shimon Ullman category:cs.CV cs.LG published:2016-03-14 summary:Convolutional neural networks have been shown to develop internalrepresentations, which correspond closely to semantically meaningful objectsand parts, although trained solely on class labels. Class Activation Mapping(CAM) is a recent method that makes it possible to easily highlight the imageregions contributing to a network's classification decision. We build uponthese two developments to enable a network to re-examine informative imageregions, which we term introspection. We propose a weakly-supervised iterativescheme, which shifts its center of attention to increasingly discriminativeregions as it progresses, by alternating stages of classification andintrospection. We evaluate our method and show its effectiveness over a rangeof several datasets, obtaining a top-1 accuracy 84.48% CUB-200-2011, which isthe highest to-date without using external data or stronger supervision. OnStanford-40 Actions, we set a new state-of the art of 87.89%, and onFGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvementsover baselines, some of which include significantly more supervision.
arxiv-16800-147 | Online Isotonic Regression | http://arxiv.org/abs/1603.04190 | author:Wojciech Kotłowski, Wouter M. Koolen, Alan Malek category:cs.LG stat.ML published:2016-03-14 summary:We consider the online version of the isotonic regression problem. Given aset of linearly ordered points (e.g., on the real line), the learner mustpredict labels sequentially at adversarially chosen positions and is evaluatedby her total squared loss compared against the best isotonic (non-decreasing)function in hindsight. We survey several standard online learning algorithmsand show that none of them achieve the optimal regret exponent; in fact, mostof them (including Online Gradient Descent, Follow the Leader and ExponentialWeights) incur linear regret. We then prove that the Exponential Weightsalgorithm played over a covering net of isotonic functions has regret isbounded by $O(T^{1/3} \log^{2/3}(T))$ and present a matching $\Omega(T^{1/3})$lower bound on regret. We also provide a computationally efficient version ofthis algorithm. We also analyze the noise-free case, in which the revealedlabels are isotonic, and show that the bound can be improved to $O(\log T)$ oreven to $O(1)$ (when the labels are revealed in the isotonic order). Finally,we extend the analysis beyond squared loss and give bounds for log-loss andabsolute loss.
arxiv-16800-148 | An optimal algorithm for bandit convex optimization | http://arxiv.org/abs/1603.04350 | author:Elad Hazan, Yuanzhi Li category:cs.LG cs.DS G.1.6 published:2016-03-14 summary:We consider the problem of online convex optimization against an arbitraryadversary with bandit feedback, known as bandit convex optimization. We givethe first $\tilde{O}(\sqrt{T})$-regret algorithm for this setting based on anovel application of the ellipsoid method to online learning. This bound isknown to be tight up to logarithmic factors. Our analysis introduces new toolsin discrete convex geometry.
arxiv-16800-149 | Graph Based Sinogram Denoising for Tomographic Reconstructions | http://arxiv.org/abs/1603.04203 | author:Faisal Mahmood, Nauman Shahid, Pierre Vandergheynst, Ulf Skoglund category:cs.CV published:2016-03-14 summary:Limited data and low dose constraints are common problems in a variety oftomographic reconstruction paradigms which lead to noisy and incomplete data.Over the past few years sinogram denoising has become an essentialpre-processing step for low dose Computed Tomographic (CT) reconstructions. Wepropose a novel sinogram denoising algorithm inspired by the modern field ofsignal processing on graphs. Graph based methods often perform better thanstandard filtering operations since they can exploit the signal structure. Thismakes the sinogram an ideal candidate for graph based denoising since itgenerally has a piecewise smooth structure. We test our method with a varietyof phantoms and different reconstruction methods. Our numerical study showsthat the proposed algorithm improves the performance of analytical filteredback-projection (FBP) and iterative methods ART (Kaczmarz) and SIRT(Cimmino).We observed that graph denoised sinogram always minimizes the errormeasure and improves the accuracy of the solution as compared to regularreconstructions.
arxiv-16800-150 | Interactive Tools and Tasks for the Hebrew Bible | http://arxiv.org/abs/1603.04236 | author:Nicolai Winther-Nielsen category:cs.CL published:2016-03-14 summary:Ancient texts can support intertextuality in different ways through digitaltools for databases and for tasks that scholars and students do, when theyinterac twith the texts in new ways. This contribution explores how the corpusof the Hebrew Bible created and maintained by the Eep Talstra Center for Bibleand Computer has potential for redefining the way we learn from our ancienttexts as modern knowledge workers. It first describes how the corpus was usedfor development of Bible Online Learner as a persuasive technology to enhancelanguage learning with, in and around a database that drives interactive tasksfor learners. The achievements obtained through so far are very promising, andit can help us explore textual criticism as another target for interactivestudy of the Hebrew Bible through corpus-technology. Because textual criticismis an increasingly specialized area of research which depends on digitalresources. The commercial solution from Logos Bible Software offers advancedscholarly resources from the German Bible Society as a model for how affluentWestern scholars can use technology for the Hebrew corpus. The achievements incorpus-driven learning and the potential of commercial resources can help ussuggest new tasks in textual criticism based on online applications which usecorpora for a new kind of textual corpus criticism. Some promising tools fortext categorization, analysis of translation shifts and interpretation arerecommended as potential models for the future. The main goal in the futuremust be more open global access for the new tools.
arxiv-16800-151 | A Variational Perspective on Accelerated Methods in Optimization | http://arxiv.org/abs/1603.04245 | author:Andre Wibisono, Ashia C. Wilson, Michael I. Jordan category:math.OC cs.LG stat.ML published:2016-03-14 summary:Accelerated gradient methods play a central role in optimization, achievingoptimal rates in many settings. While many generalizations and extensions ofNesterov's original acceleration method have been proposed, it is not yet clearwhat is the natural scope of the acceleration concept. In this paper, we studyaccelerated methods from a continuous-time perspective. We show that there is aLagrangian functional that we call the \emph{Bregman Lagrangian} whichgenerates a large class of accelerated methods in continuous time, including(but not limited to) accelerated gradient descent, its non-Euclidean extension,and accelerated higher-order gradient methods. We show that the continuous-timelimit of all of these methods correspond to traveling the same curve inspacetime at different speeds. From this perspective, Nesterov's technique andmany of its generalizations can be viewed as a systematic way to go from thecontinuous-time curves generated by the Bregman Lagrangian to a family ofdiscrete-time accelerated algorithms.
arxiv-16800-152 | TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems | http://arxiv.org/abs/1603.04467 | author:Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng category:cs.DC cs.LG published:2016-03-14 summary:TensorFlow is an interface for expressing machine learning algorithms, and animplementation for executing such algorithms. A computation expressed usingTensorFlow can be executed with little or no change on a wide variety ofheterogeneous systems, ranging from mobile devices such as phones and tabletsup to large-scale distributed systems of hundreds of machines and thousands ofcomputational devices such as GPU cards. The system is flexible and can be usedto express a wide variety of algorithms, including training and inferencealgorithms for deep neural network models, and it has been used for conductingresearch and for deploying machine learning systems into production across morethan a dozen areas of computer science and other fields, including speechrecognition, computer vision, robotics, information retrieval, natural languageprocessing, geographic information extraction, and computational drugdiscovery. This paper describes the TensorFlow interface and an implementationof that interface that we have built at Google. The TensorFlow API and areference implementation were released as an open-source package under theApache 2.0 license in November, 2015 and are available at www.tensorflow.org.
arxiv-16800-153 | Dynamic Scene Deblurring using a Locally Adaptive Linear Blur Model | http://arxiv.org/abs/1603.04265 | author:Tae Hyun Kim, Seungjun Nah, Kyoung Mu Lee category:cs.CV published:2016-03-14 summary:State-of-the-art video deblurring methods cannot handle blurry videosrecorded in dynamic scenes, since they are built under a strong assumption thatthe captured scenes are static. Contrary to the existing methods, we propose avideo deblurring algorithm that can deal with general blurs inherent in dynamicscenes. To handle general and locally varying blurs caused by various sources,such as moving objects, camera shake, depth variation, and defocus, we estimatepixel-wise non-uniform blur kernels. We infer bidirectional optical flows tohandle motion blurs, and also estimate Gaussian blur maps to remove opticalblur from defocus in our new blur model. Therefore, we propose a single energymodel that jointly estimates optical flows, defocus blur maps and latentframes. We also provide a framework and efficient solvers to minimize theproposed energy model. By optimizing the energy model, we achieve significantimprovements in removing general blurs, estimating optical flows, and extendingdepth-of-field in blurry frames. Moreover, in this work, to evaluate theperformance of non-uniform deblurring methods objectively, we have constructeda new realistic dataset with ground truths. In addition, extensive experimentalon publicly available challenging video data demonstrate that the proposedmethod produces qualitatively superior performance than the state-of-the-artmethods which often fail in either deblurring or optical flow estimation.
arxiv-16800-154 | Universal probability-free conformal prediction | http://arxiv.org/abs/1603.04283 | author:Vladimir Vovk, Dusko Pavlovic category:cs.LG 68T05 I.2.6 published:2016-03-14 summary:We construct a universal prediction system in the spirit of Popper'sfalsifiability and Kolmogorov complexity. This prediction system does notdepend on any statistical assumptions, but under the IID assumption itdominates, although in a rather weak sense, conformal prediction.
arxiv-16800-155 | Diversity in Object Proposals | http://arxiv.org/abs/1603.04308 | author:Anton Winschel, Rainer Lienhart, Christian Eggert category:cs.CV published:2016-03-14 summary:Current top performing object recognition systems build on object proposalsas a preprocessing step. Object proposal algorithms are designed to generatecandidate regions for generic objects, yet current approaches are limited incapturing the vast variety of object characteristics. In this paper we analyzethe error modes of the state-of-the-art Selective Search object proposalalgorithm and suggest extensions to broaden its feature diversity in order tomitigate its error modes. We devise an edge grouping algorithm for handlingobjects without clear boundaries. To further enhance diversity, we incorporatethe Edge Boxes proposal algorithm, which is based on fundamentally differentprinciples than Selective Search. The combination of segmentations and edgesprovides rich image information and feature diversity which is essential forobtaining high quality object proposals for generic objects. For a presetamount of object proposals we achieve considerably better results by using ourcombination of different strategies than using any single strategy alone.
arxiv-16800-156 | Learning Network of Multivariate Hawkes Processes: A Time Series Approach | http://arxiv.org/abs/1603.04319 | author:Jalal Etesami, Negar Kiyavash, Kun Zhang, Kushagra Singhal category:cs.LG cs.AI stat.ML published:2016-03-14 summary:Learning the influence structure of multiple time series data is of greatinterest to many disciplines. This paper studies the problem of recovering thecausal structure in network of multivariate linear Hawkes processes. In suchprocesses, the occurrence of an event in one process affects the probability ofoccurrence of new events in some other processes. Thus, a natural notion ofcausality exists between such processes captured by the support of theexcitation matrix. We show that the resulting causal influence network isequivalent to the Directed Information graph (DIG) of the processes, whichencodes the causal factorization of the joint distribution of the processes.Furthermore, we present an algorithm for learning the support of excitationmatrix (or equivalently the DIG). The performance of the algorithm is evaluatedon synthesized multivariate Hawkes networks as well as a stock market andMemeTracker real-world dataset.
arxiv-16800-157 | Automatic Discrimination of Color Retinal Images using the Bag of Words Approach | http://arxiv.org/abs/1603.04327 | author:Ibrahim Sadek category:cs.CV published:2016-03-14 summary:Diabetic retinopathy (DR) and age related macular degeneration (ARMD) areamong the major causes of visual impairment worldwide. DR is mainlycharacterized by red spots, namely microaneurysms and bright lesions,specifically exudates whereas ARMD is mainly identified by tiny yellow or whitedeposits called drusen. Since exudates might be the only manifestation of theearly diabetic retinopathy, there is an increase demand for automaticretinopathy diagnosis. Exudates and drusen may share similar appearances, thusdiscriminating between them is of interest to enhance screening performance. Inthis research, we investigative the role of bag of words approach in theautomatic diagnosis of retinopathy diabetes. We proposed to use a single basedand multiple based methods for the construction of the visual dictionary bycombining the histogram of word occurrences from each dictionary and building asingle histogram. The introduced approach is evaluated for automatic diagnosisof normal and abnormal color fundus images with bright lesions. This approachhas been implemented on 430 fundus images, including six publicly availabledatasets, in addition to one local dataset. The mean accuracies reported are97.2% and 99.77% for single based and multiple based dictionaries respectively.
arxiv-16800-158 | On the Influence of Momentum Acceleration on Online Learning | http://arxiv.org/abs/1603.04136 | author:Kun Yuan, Bicheng Ying, Ali H. Sayed category:math.OC cs.LG stat.ML published:2016-03-14 summary:The article examines in some detail the convergence rate andmean-square-error performance of momentum stochastic gradient methods in theconstant step-size case and slow adaptation regime. The results establish thatmomentum methods are equivalent to the standard stochastic gradient method witha re-scaled (larger) step-size value. The size of the re-scaling is determinedby the value of the momentum parameter. The equivalence result is establishedfor all time instants and not only in steady-state. The analysis is carried outfor general risk functions, and is not limited to quadratic risks. One notableconclusion is that the well-known benefits of momentum constructions fordeterministic optimization problems do not necessarily carry over to thestochastic (online) setting when adaptation becomes necessary and when the truegradient vectors are not known beforehand. The analysis also suggests a methodto retain some of the advantages of the momentum construction by employing adecaying momentum parameter, as opposed to a decaying step-size. In this way,the enhanced convergence rate during the initial stages of adaptation ispreserved without the often-observed degradation in MSD performance.
arxiv-16800-159 | Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations | http://arxiv.org/abs/1603.04351 | author:Eliyahu Kiperwasser, Yoav Goldberg category:cs.CL published:2016-03-14 summary:We present a simple and effective scheme for dependency parsing which isbased on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated witha BiLSTM vector representing the token in its sentential context, and featurevectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM istrained jointly with the parser objective, resulting in very effective featureextractors for parsing. We demonstrate the effectiveness of the approach byapplying it to a greedy transition based parser as well as to a globallyoptimized graph-based parser. The resulting parsers have very simplearchitectures, and match or surpass the state-of-the-art accuracies on Englishand Chinese.
arxiv-16800-160 | Investigation of event-based memory surfaces for high-speed tracking, unsupervised feature extraction and object recognition | http://arxiv.org/abs/1603.04223 | author:Saeed Afshar, Gregory Cohen, Chetan Singh Thakur, Jonathan Tapson, Tara Julia Hamilton, Andre van Schaik category:cs.NE cs.CV published:2016-03-14 summary:In this paper an event-based tracking, feature extraction, and classificationsystem is presented for performing object recognition using an event-basedcamera. The high-speed recognition task involves detecting and classifyingmodel airplanes that are dropped free-hand close to the camera lens so as togenerate a challenging highly varied dataset of spatio-temporal event patterns.We investigate the use of time decaying memory surfaces to capture the temporalaspect of the event-based data. These surfaces are then used to performunsupervised feature extraction, tracking and recognition. Both linear andexponentially decaying surfaces were found to result in equally highrecognition accuracy. Using only twenty five event-based feature extractingneurons in series with a linear classifier, the system achieves 98.61%recognition accuracy within 156 milliseconds of the airplane entering the fieldof view. By comparing the linear classifier results to a high-capacity ELMclassifier, we find that a small number of event-based feature extractors caneffectively project the complex spatio-temporal event patterns of the data-setto a linearly separable representation in the feature space.
arxiv-16800-161 | Item2Vec: Neural Item Embedding for Collaborative Filtering | http://arxiv.org/abs/1603.04259 | author:Oren Barkan, Noam Koenigstein category:cs.LG cs.AI cs.IR published:2016-03-14 summary:Many Collaborative Filtering (CF) algorithms are item-based in the sense thatthey analyze item-item relations in order to produce item similarities.Recently, several works in the field of Natural Language Processing suggestedto learn a latent representation of words using neural embedding algorithms.Among them, the Skip-gram with Negative Sampling (SGNS), also known asWord2Vec, was shown to provide state-of-the-art results on various linguisticstasks. In this paper, we show that item-based CF can be cast in the sameframework of neural word embedding. Inspired by SGNS, we describe a method wename Item2Vec for item-based CF that produces embedding for items in a latentspace. The method is capable of inferring item-to-item relations even when userinformation is not available. We present experimental results on large scaledatasets that demonstrate the effectiveness of the Item2Vec method and show itis competitive with SVD.
arxiv-16800-162 | Saliency Detection for Improving Object Proposals | http://arxiv.org/abs/1603.04146 | author:Shuhan Chen, Jindong Li, Xuelong Hu, Ping Zhou category:cs.CV published:2016-03-14 summary:Object proposals greatly benefit object detection task in recentstate-of-the-art works, such as R-CNN [2]. However, the existing objectproposals usually have low localization accuracy at high intersection overunion threshold. To address it, we apply saliency detection to each boundingbox to improve their quality in this paper. We first present a geodesicsaliency detection method in contour, which is designed to find closedcontours. Then, we apply it to each candidate box with multi-sizes, and refinedboxes can be easily produced in the obtained saliency maps which are furtherused to calculate saliency scores for proposal ranking. Experiments on PASCALVOC 2007 test dataset demonstrate the proposed refinement approach can greatlyimprove existing models.
arxiv-16800-163 | A Ranking Approach to Global Optimization | http://arxiv.org/abs/1603.04381 | author:Cédric Malherbe, Emile Contal, Nicolas Vayatis category:stat.ML published:2016-03-14 summary:In this paper, we consider the problem of maximizing an unknown function fover a compact and convex set using as few observations f(x) as possible. Weobserve that the optimization of the function f essentially relies on learningthe induced bipartite ranking rule of f. Based on this idea, we relate globaloptimization to bipartite ranking which allows to address problems with highdimensional input space, as well as cases of functions with weak regularityproperties. The paper introduces novel meta-algorithms for global optimizationwhich rely on the choice of any bipartite ranking method. Theoreticalproperties are provided as well as convergence guarantees and equivalencesbetween various optimization methods are obtained as a by-product. Eventually,numerical evidence is given to show that the main algorithm of the paper whichadapts empirically to the underlying ranking structure essentially outperformsexisting state-of-the-art global optimization algorithms in typical benchmarks.
arxiv-16800-164 | Criteria of efficiency for conformal prediction | http://arxiv.org/abs/1603.04416 | author:Vladimir Vovk, Valentina Fedorova, Ilia Nouretdinov, Alex Gammerman category:cs.LG 68T05 I.2.6 published:2016-03-14 summary:We study optimal conformity measures for various criteria of efficiency in anidealized setting. This leads to an important class of criteria of efficiencythat we call probabilistic; it turns out that the most standard criteria ofefficiency used in literature on conformal prediction are not probabilistic.
arxiv-16800-165 | Rapid building detection using machine learning | http://arxiv.org/abs/1603.04392 | author:Joseph Paul Cohen, Wei Ding, Caitlin Kuhlman, Aijun Chen, Liping Di category:cs.CV published:2016-03-14 summary:This work describes algorithms for performing discrete object detection,specifically in the case of buildings, where usually only low quality RGB-onlygeospatial reflective imagery is available. We utilize new candidate search andfeature extraction techniques to reduce the problem to a machine learning (ML)classification task. Here we can harness the complex patterns of contrastfeatures contained in training data to establish a model of buildings. We avoidcostly sliding windows to generate candidates; instead we innovatively stitchtogether well known image processing techniques to produce candidates forbuilding detection that cover 80-85% of buildings. Reducing the number ofpossible candidates is important due to the scale of the problem. Eachcandidate is subjected to classification which, although linear, costs time andprohibits large scale evaluation. We propose a candidate alignment algorithm toboost classification performance to 80-90% precision with a linear timealgorithm and show it has negligible cost. Also, we propose a new conceptcalled a Permutable Haar Mesh (PHM) which we use to form and traverse a searchspace to recover candidate buildings which were lost in the initialpreprocessing phase.
arxiv-16800-166 | A Novel Method for Extrinsic Calibration of a 2-D Laser-Rangefinder and a Camera | http://arxiv.org/abs/1603.04132 | author:Wenbo Dong, Volkan Isler category:cs.CV cs.RO published:2016-03-14 summary:We present a novel solution for extrinsically calibrating a camera and aLaser Rangefinder (LRF) by computing the transformation between the cameraframe and the LRF frame. Our method is applicable for LRFs which measure only asingle plane. It does not rely on observing the laser plane in the cameraimage. Instead, we show that point-to-plane constraints from a singleobservation of a V-shaped calibration pattern composed of two non-coplanartriangles suffice to uniquely constrain the transformation. Next, we present amethod to obtain a solution using point-to-plane constraints from single ormultiple observations. Along the way, we also show that previous solutions, incontrast to our method, have inherent ambiguities and therefore must rely on agood initial estimate. Real and synthetic experiments validate our method andshow that it achieves better accuracy than previous methods.
arxiv-16800-167 | Clustering Financial Time Series: How Long is Enough? | http://arxiv.org/abs/1603.04017 | author:Gautier Marti, Sébastien Andler, Frank Nielsen, Philippe Donnat category:stat.ML q-fin.ST published:2016-03-13 summary:Researchers have used from 30 days to several years of daily returns assource data for clustering financial time series based on their correlations.This paper sets up a statistical framework to study the validity of suchpractices. We first show that clustering correlated random variables from theirobserved values is statistically consistent. Then, we also give a firstempirical answer to the much debated question: How long should the time seriesbe? If too short, the clusters found can be spurious; if too long, dynamics canbe smoothed out.
arxiv-16800-168 | A Grothendieck-type inequality for local maxima | http://arxiv.org/abs/1603.04064 | author:Andrea Montanari category:math.OC stat.ML published:2016-03-13 summary:A large number of problems in optimization, machine learning, signalprocessing can be effectively addressed by suitable semidefinite programming(SDP) relaxations. Unfortunately, generic SDP solvers hardly scale beyondinstances with a few hundreds variables (in the underlying combinatorialproblem). On the other hand, it has been observed empirically that an effectivestrategy amounts to introducing a (non-convex) rank constraint, and solving theresulting smooth optimization problem by ascent methods. This non-convexproblem has --generically-- a large number of local maxima, and the reason forthis success is therefore unclear. This paper provides rigorous support for this approach. For the problem ofmaximizing a linear functional over the elliptope, we prove that all localmaxima are within a small gap from the SDP optimum. In several problems ofinterest, arbitrarily small relative error can be achieved by taking the rankconstraint $k$ to be of order one, independently of the problem size.
arxiv-16800-169 | Image and Depth from a Single Defocused Image Using Coded Aperture Photography | http://arxiv.org/abs/1603.04046 | author:Mina Masoudifar, Hamid Reza Pourreza category:cs.CV published:2016-03-13 summary:Depth from defocus and defocus deblurring from a single image are twochallenging problems that are derived from the finite depth of field inconventional cameras. Coded aperture imaging is one of the techniques that isused for improving the results of these two problems. Up to now, differentmethods have been proposed for improving the results of either defocusdeblurring or depth estimation. In this paper, a multi-objective function isproposed for evaluating and designing aperture patterns with the aim ofimproving the results of both depth from defocus and defocus deblurring.Pattern evaluation is performed by considering the scene illumination conditionand camera system specification. Based on the proposed criteria, a singleasymmetric pattern is designed that is used for restoring a sharp image and adepth map from a single input. Since the designed pattern is asymmetric,defocus objects on the two sides of the focal plane can be distinguished. Depthestimation is performed by using a new algorithm, which is based on imagequality assessment criteria and can distinguish between blurred objects lyingin front or behind the focal plane. Extensive simulations as well asexperiments on a variety of real scenes are conducted to compare our aperturewith previously proposed ones.
arxiv-16800-170 | Deep Interactive Object Selection | http://arxiv.org/abs/1603.04042 | author:Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas Huang category:cs.CV published:2016-03-13 summary:Interactive object selection is a very important research problem and hasmany applications. Previous algorithms require substantial user interactions toestimate the foreground and background distributions. In this paper, we presenta novel deep learning based algorithm which has a much better understanding ofobjectness and thus can reduce user interactions to just a few clicks. Ouralgorithm transforms user provided positive and negative clicks into twoEuclidean distance maps which are then concatenated with the RGB channels ofimages to compose (image, user interactions) pairs. We generate many of suchpairs by combining several random sampling strategies to model user clickpatterns and use them to fine tune deep Fully Convolutional Networks (FCNs).Finally the output probability maps of our FCN 8s model is integrated withgraph cut optimization to refine the boundary segments. Our model is trained onthe PASCAL segmentation dataset and evaluated on other datasets with differentobject classes. Experimental results on both seen and unseen objects clearlydemonstrate that our algorithm has a good generalization ability and issuperior to all existing interactive object selection approaches.
arxiv-16800-171 | A Stochastic Approach to STDP | http://arxiv.org/abs/1603.04080 | author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, André van Schaik category:cs.NE published:2016-03-13 summary:We present a digital implementation of the Spike Timing Dependent Plasticity(STDP) learning rule. The proposed digital implementation consists of anexponential decay generator array and a STDP adaptor array. On the arrival of apre- and post-synaptic spike, the STDP adaptor will send a digital spike to thedecay generator. The decay generator will then generate an exponential decay,which will be used by the STDP adaptor to perform the weight adaption. Theexponential decay, which is computational expensive, is efficiently implementedby using a novel stochastic approach, which we analyse and characterise here.We use a time multiplexing approach to achieve 8192 (8k) virtual STDP adaptorsand decay generators with only one physical implementation of each. We havevalidated our stochastic STDP approach with measurement results of a balancedexcitation/inhibition experiment. Our stochastic approach is ideal forimplementing the STDP learning rule in large-scale spiking neural networksrunning in real time.
arxiv-16800-172 | Pose for Action - Action for Pose | http://arxiv.org/abs/1603.04037 | author:Umar Iqbal, Martin Garbade, Juergen Gall category:cs.CV published:2016-03-13 summary:In this work we propose to utilize information about human actions to improvepose estimation in monocular videos. To this end, we present a pictorialstructure model that exploits high-level information about activities toincorporate higher-order part dependencies by modeling action specificappearance models and pose priors. However, instead of using an additionalexpensive action recognition framework, the action priors are efficientlyestimated by our pose estimation framework. This is achieved by starting with auniform action prior and updating the action prior during pose estimation. Wealso show that learning the right amount of appearance sharing among actionclasses improves the pose estimation. Our proposed model achievesstate-of-the-art performance on two challenging datasets for pose estimationand action recognition with over 80,000 test images.
arxiv-16800-173 | Privacy-preserving Analysis of Correlated Data | http://arxiv.org/abs/1603.03977 | author:Yizhen Wang, Shuang Song, Kamalika Chaudhuri category:cs.LG cs.CR stat.ML published:2016-03-13 summary:Many modern machine learning applications involve sensitive correlated data,such private information on users connected together in a social network, andmeasurements of physical activity of a single user across time. However, thecurrent standard of privacy in machine learning, differential privacy, cannotadequately address privacy issues in this kind of data. This work looks at a recent generalization of differential privacy, calledPufferfish, that can be used to address privacy in correlated data. The mainchallenge in applying Pufferfish to correlated data problems is the lack ofsuitable mechanisms. In this paper, we provide a general mechanism, called theWasserstein Mechanism, which applies to any Pufferfish framework. Since theWasserstein Mechanism may be computationally inefficient, we provide anadditional mechanism, called Markov Quilt Mechanism, that applies to somepractical cases such as physical activity measurements across time, and iscomputationally efficient.
arxiv-16800-174 | On Learning High Dimensional Structured Single Index Models | http://arxiv.org/abs/1603.03980 | author:Nikhil Rao, Ravi Ganti, Laura Balzano, Rebecca Willett, Robert Nowak category:stat.ML cs.AI cs.LG published:2016-03-13 summary:Single Index Models (SIMs) are simple yet flexible semi-parametric models forclassification and regression, where response variables are modeled as anonlinear, monotonic function of a linear combination of features. Estimationin this context requires learning both the feature weights and the nonlinearfunction that relates features to observations. While methods have beendescribed to learn SIMs in the low dimensional regime, a method that canefficiently learn SIMs in high dimensions, and under general structuralassumptions, has not been forthcoming. In this paper, we proposecomputationally efficient algorithms for SIM inference in high dimensions usingatomic norm regularization. This general approach to imposing structure inhigh-dimensional modeling specializes to sparsity, group sparsity, and low-rankassumptions among others. We also provide a scalable, stochastic version of themethod. Experiments show that the method we propose enjoys superior predictiveperformance when compared to generalized linear models such as logisticregression, on several real-world datasets.
arxiv-16800-175 | A comprehensive study of sparse codes on abnormality detection | http://arxiv.org/abs/1603.04026 | author:Huamin Ren, Hong Pan, Søren Ingvor Olsen, Thomas B. Moeslund category:cs.CV published:2016-03-13 summary:Sparse representation has been applied successfully in abnormal eventdetection, in which the baseline is to learn a dictionary accompanied by sparsecodes. While much emphasis is put on discriminative dictionary construction,there are no comparative studies of sparse codes regarding abnormalitydetection. We comprehensively study two types of sparse codes solutions -greedy algorithms and convex L1-norm solutions - and their impact onabnormality detection performance. We also propose our framework of combiningsparse codes with different detection methods. Our comparative experiments arecarried out from various angles to better understand the applicability ofsparse codes, including computation time, reconstruction error, sparsity,detection accuracy, and their performance combining various detection methods.Experiments show that combining OMP codes with maximum coordinate detectioncould achieve state-of-the-art performance on the UCSD dataset [14].
arxiv-16800-176 | An efficient Exact-PGA algorithm for constant curvature manifolds | http://arxiv.org/abs/1603.03984 | author:Rudrasis Chakraborty, Dohyung Seo, Baba C. Vemuri category:cs.CV published:2016-03-13 summary:Manifold-valued datasets are widely encountered in many computer visiontasks. A non-linear analog of the PCA, called the Principal Geodesic Analysis(PGA) suited for data lying on Riemannian manifolds was reported in literaturea decade ago. Since the objective function in PGA is highly non-linear and hardto solve efficiently in general, researchers have proposed a linearapproximation. Though this linear approximation is easy to compute, it lacksaccuracy especially when the data exhibits a large variance. Recently, analternative called exact PGA was proposed which tries to solve the optimizationwithout any linearization. For general Riemannian manifolds, though it givesbetter accuracy than the original (linearized) PGA, for data that exhibit largevariance, the optimization is not computationally efficient. In this paper, wepropose an efficient exact PGA for constant curvature Riemannian manifolds(CCM-EPGA). CCM-EPGA differs significantly from existing PGA algorithms in twoaspects, (i) the distance between a given manifold-valued data point and theprincipal submanifold is computed analytically and thus no optimization isrequired as in existing methods. (ii) Unlike the existing PGA algorithms, thedescent into codimension-1 submanifolds does not require any optimization butis accomplished through the use of the Rimeannian inverse Exponential map andthe parallel transport operations. We present theoretical and experimentalresults for constant curvature Riemannian manifolds depicting favorableperformance of CCM-EPGA compared to existing PGA algorithms. We also presentdata reconstruction from principal components and directions which has not beenpresented in literature in this setting.
arxiv-16800-177 | Learning Typographic Style | http://arxiv.org/abs/1603.04000 | author:Shumeet Baluja category:cs.CV cs.LG cs.NE published:2016-03-13 summary:Typography is a ubiquitous art form that affects our understanding,perception, and trust in what we read. Thousands of different font-faces havebeen created with enormous variations in the characters. In this paper, welearn the style of a font by analyzing a small subset of only four letters.From these four letters, we learn two tasks. The first is a discriminationtask: given the four letters and a new candidate letter, does the new letterbelong to the same font? Second, given the four basis letters, can we generateall of the other letters with the same characteristics as those in the basisset? We use deep neural networks to address both tasks, quantitatively andqualitatively measure the results in a variety of novel manners, and present athorough investigation of the weaknesses and strengths of the approach.
arxiv-16800-178 | Learning zeroth class dictionary for human action recognition | http://arxiv.org/abs/1603.04015 | author:Jia-xin Cai, Xin Tang, Lifang Zhang, Guocan Feng category:cs.CV published:2016-03-13 summary:In this paper, a discriminative two-layer dictionary learning framework isproposed for classifying human action by sparse shape representations, in whichthe first-layer dictionary is learned on the selected discriminative frames andthe second-layer dictionary is built for recognition using reconstructionerrors of the first-layer dictionary as input features. We propose a "zerothclass" trick for detecting undiscriminating frames of the test video andeliminating them before voting on the action categories. Experimental resultson benchmarks demonstrate the effectiveness of our method.
arxiv-16800-179 | Fast Learning from Distributed Datasets without Entity Matching | http://arxiv.org/abs/1603.04002 | author:Giorgio Patrini, Richard Nock, Stephen Hardy, Tiberio Caetano category:cs.LG cs.DC I.2.6 published:2016-03-13 summary:Consider the following data fusion scenario: two datasets/peers contain thesame real-world entities described using partially shared features, e.g.banking and insurance company records of the same customer base. Our goal is tolearn a classifier in the cross product space of the two domains, in the hardcase in which no shared ID is available -- e.g. due to anonymization.Traditionally, the problem is approached by first addressing entity matchingand subsequently learning the classifier in a standard manner. We present anend-to-end solution which bypasses matching entities, based on the recentlyintroduced concept of Rademacher observations (rados). Informally, we replacethe minimisation of a loss over examples, which requires to solve entityresolution, by the equivalent minimisation of a (different) loss over rados.Among others, key properties we show are (i) a potentially huge subset of theserados does not require to perform entity matching, and (ii) the algorithm thatprovably minimizes the rado loss over these rados has time and spacecomplexities smaller than the algorithm minimizing the equivalent example loss.Last, we relax a key assumption of the model, that the data is verticallypartitioned among peers --- in this case, we would not even know the existenceof a solution to entity resolution. In this more general setting, experimentsvalidate the possibility of significantly beating even the optimal peer inhindsight.
arxiv-16800-180 | Template Adaptation for Face Verification and Identification | http://arxiv.org/abs/1603.03958 | author:Nate Crosswhite, Jeffrey Byrne, Omkar M. Parkhi, Chris Stauffer, Qiong Cao, Andrew Zisserman category:cs.CV published:2016-03-12 summary:Face recognition performance evaluation has traditionally focused onone-to-one verification, popularized by the Labeled Faces in the Wild datasetfor imagery and the YouTubeFaces dataset for videos. In contrast, the newlyreleased IJB-A face recognition dataset unifies evaluation of one-to-many faceidentification with one-to-one face verification over templates, or sets ofimagery and videos for a subject. In this paper, we study the problem oftemplate adaptation, a form of transfer learning to the set of media in atemplate. Extensive performance evaluations on IJB-A show a surprising result,that perhaps the simplest method of template adaptation, combining deepconvolutional network features with template specific linear SVMs, outperformsthe state-of-the-art by a wide margin. We study the effects of template size,negative set construction and classifier fusion on performance, then comparetemplate adaptation to convolutional networks with metric learning, 2D and 3Dalignment. Our unexpected conclusion is that these other methods, when combinedwith template adaptation, all achieve nearly the same top performance on IJB-Afor template-based face verification and identification.
arxiv-16800-181 | Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks | http://arxiv.org/abs/1603.03827 | author:Ji Young Lee, Franck Dernoncourt category:cs.CL cs.AI cs.LG cs.NE stat.ML published:2016-03-12 summary:Recent approaches based on artificial neural networks (ANNs) have shownpromising results for short-text classification. However, many short textsoccur in sequences (e.g., sentences in a document or utterances in a dialog),and most existing ANN-based systems do not leverage the preceding short textswhen classifying a subsequent one. In this work, we present a model based onrecurrent neural networks and convolutional neural networks that incorporatesthe preceding short texts. Our model achieves state-of-the-art results on threedifferent datasets for dialog act prediction.
arxiv-16800-182 | Variational Neural Discourse Relation Recognizer | http://arxiv.org/abs/1603.03876 | author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL published:2016-03-12 summary:Implicit discourse relation recognition is a crucial component for automaticdiscourse-level analysis and nature language understanding. Previous studiesexploit discriminative models that are built on either powerful manual featuresor deep discourse representations. In this paper, instead, we exploregenerative models and propose a variational neural discourse relationrecognizer. We refer to this model as VIRILE. VIRILE establishes a directedprobabilistic model with a latent continuous variable that generates both adiscourse and the relation between the two arguments of the discourse. In orderto perform efficient inference and learning, we introduce a neural discourserelation model to approximate the posterior of the latent variable, and employthis approximated posterior to optimize a reparameterized variational lowerbound. This allows VIRILE to be trained with standard stochastic gradientmethods. Experiments on the benchmark data set show that VIRILE can achievecompetitive results against state-of-the-art baselines.
arxiv-16800-183 | Image Captioning with Semantic Attention | http://arxiv.org/abs/1603.03925 | author:Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo category:cs.CV published:2016-03-12 summary:Automatically generating a natural language description of an image hasattracted interests recently both because of its importance in practicalapplications and because it connects two major artificial intelligence fields:computer vision and natural language processing. Existing approaches are eithertop-down, which start from a gist of an image and convert it into words, orbottom-up, which come up with words describing various aspects of an image andthen combine them. In this paper, we propose a new algorithm that combines bothapproaches through a model of semantic attention. Our algorithm learns toselectively attend to semantic concept proposals and fuse them into hiddenstates and outputs of recurrent neural networks. The selection and fusion forma feedback connecting the top-down and bottom-up computation. We evaluate ouralgorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimentalresults show that our algorithm significantly outperforms the state-of-the-artapproaches consistently across different evaluation metrics.
arxiv-16800-184 | Robust Scene Text Recognition with Automatic Rectification | http://arxiv.org/abs/1603.03915 | author:Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai category:cs.CV published:2016-03-12 summary:Recognizing text in natural images is a challenging task with many unsolvedproblems. Different from those in documents, words in natural images oftenpossess irregular shapes, which are caused by perspective distortion, curvedcharacter placement, etc. We propose RARE (Robust text recognizer withAutomatic REctification), a recognition model that is robust to irregular text.RARE is a specially-designed deep neural network, which consists of a SpatialTransformer Network (STN) and a Sequence Recognition Network (SRN). In testing,an image is firstly rectified via a predicted Thin-Plate-Spline (TPS)transformation, into a more "readable" image for the following SRN, whichrecognizes text through a sequence recognition approach. We show that the modelis able to recognize several types of irregular text, including perspectivetext and curved text. RARE is end-to-end trainable, requiring only images andassociated text labels, making it convenient to train and deploy the model inpractical systems. State-of-the-art or highly-competitive performance achievedon several benchmarks well demonstrates the effectiveness of the proposedmodel.
arxiv-16800-185 | Optical Flow with Semantic Segmentation and Localized Layers | http://arxiv.org/abs/1603.03911 | author:Laura Sevilla-Lara, Deqing Sun, Varun Jampani, Michael J. Black category:cs.CV published:2016-03-12 summary:Existing optical flow methods make generic, spatially homogeneous,assumptions about the spatial structure of the flow. In reality, optical flowvaries across an image depending on object class. Simply put, different objectsmove differently. Here we exploit recent advances in static semantic scenesegmentation to segment the image into objects of different types. We definedifferent models of image motion in these regions depending on the type ofobject. For example, we model the motion on roads with homographies, vegetationwith spatially smooth flow, and independently moving objects like cars andplanes with affine motion plus deviations. We then pose the flow estimationproblem using a novel formulation of localized layers, which addresseslimitations of traditional layered models for dealing with complex scenemotion. Our semantic flow method achieves the lowest error of any publishedmonocular method in the KITTI-2015 flow benchmark and produces qualitativelybetter flow and segmentation than recent top methods on a wide range of naturalvideos.
arxiv-16800-186 | Neural Discourse Relation Recognition with Semantic Memory | http://arxiv.org/abs/1603.03873 | author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL published:2016-03-12 summary:Humans comprehend the meanings and relations of discourses heavily relying ontheir semantic memory that encodes general knowledge about concepts and facts.Inspired by this, we propose a neural recognizer for implicit discourserelation analysis, which builds upon a semantic memory that stores knowledge ina distributed fashion. We refer to this recognizer as SeMDER. Starting fromword embeddings of discourse arguments, SeMDER employs a shallow encoder togenerate a distributed surface representation for a discourse. A semanticencoder with attention to the semantic memory matrix is further establishedover surface representations. It is able to retrieve a deep semantic meaningrepresentation for the discourse from the memory. Using the surface andsemantic representations as input, SeMDER finally predicts implicit discourserelations via a neural recognizer. Experiments on the benchmark data set showthat SeMDER benefits from the semantic memory and achieves substantialimprovements of 2.56\% on average over current state-of-the-art baselines interms of F1-score.
arxiv-16800-187 | Towards Building an RGBD-M Scanner | http://arxiv.org/abs/1603.03875 | author:Zhe Wu, Sai-Kit Yeung, Ping Tan category:cs.CV published:2016-03-12 summary:We present a portable device to capture both shape and reflectance of anindoor scene. Consisting of a Kinect, an IR camera and several IR LEDs, ourdevice allows the user to acquire data in a similar way as he/she scans with asingle Kinect. Scene geometry is reconstructed by KinectFusion. To estimatereflectance from incomplete and noisy observations, 3D vertices of the samematerial are identified by our material segmentation propagation algorithm.Then BRDF observations at these vertices are merged into a more complete andaccurate BRDF for the material. Effectiveness of our device is demonstrated byquality results on real-world scenes.
arxiv-16800-188 | Real-time 3D scene description using Spheres, Cones and Cylinders | http://arxiv.org/abs/1603.03856 | author:Kristiyan Georgiev, Motaz Al-Hami, Rolf Lakaemper category:cs.CV published:2016-03-12 summary:The paper describes a novel real-time algorithm for finding 3D geometricprimitives (cylinders, cones and spheres) from 3D range data. In its core, itperforms a fast model fitting with a model update in constant time (O(1)) foreach new data point added to the model. We use a three stage approach.The firststep inspects 1.5D sub spaces, to find ellipses. The next stage uses theseellipses as input by examining their neighborhood structure to form sets ofcandidates for the 3D geometric primitives. Finally, candidate ellipses arefitted to the geometric primitives. The complexity for point processing isO(n); additional time of lower order is needed for working on significantlysmaller amount of mid-level objects. This allows the approach to process 30frames per second on Kinect depth data, which suggests this approach as apre-processing step for 3D real-time higher level tasks in robotics, liketracking or feature based mapping.
arxiv-16800-189 | Temporally Robust Global Motion Compensation by Keypoint-based Congealing | http://arxiv.org/abs/1603.03968 | author:S. Morteza Safdarnejad, Yousef Atoum, Xiaoming Liu category:cs.CV published:2016-03-12 summary:Global motion compensation (GMC) removes the impact of camera motion andcreates a video in which the background appears static over the progression oftime. Various vision problems, such as human activity recognition, backgroundreconstruction, and multi-object tracking can benefit from GMC. Existing GMCalgorithms rely on sequentially processing consecutive frames, by estimatingthe transformation mapping the two frames, and obtaining a compositetransformation to a global motion compensated coordinate. Sequential GMCsuffers from temporal drift of frames from the accurate global coordinate, dueto either error accumulation or sporadic failures of motion estimation at a fewframes. We propose a temporally robust global motion compensation (TRGMC)algorithm which performs accurate and stable GMC, despite complicated andlong-term camera motion. TRGMC densely connects pairs of frames, by matchinglocal keypoints of each frame. A joint alignment of these frames is formulatedas a novel keypoint-based congealing problem, where the transformation of eachframe is updated iteratively, such that the spatial coordinates for the startand end points of matched keypoints are identical. Experimental resultsdemonstrate that TRGMC has superior performance in a wide range of scenarios.
arxiv-16800-190 | Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements | http://arxiv.org/abs/1603.03972 | author:Keith Levin, Vince Lyzinski category:stat.ML published:2016-03-12 summary:Manifold learning and dimensionality reduction techniques are ubiquitous inscience and engineering, but can be computationally expensive procedures whenapplied to large data sets or when similarities are expensive to compute. Todate, little work has been done to investigate the tradeoff betweencomputational resources and the quality of learned representations. We presentboth theoretical and experimental explorations of this question. In particular,we consider Laplacian eigenmaps embeddings based on a kernel matrix, andexplore how the embeddings behave when this kernel matrix is corrupted byocclusion and noise. Our main theoretical result shows that under modest noiseand occlusion assumptions, we can (with high probability) recover a goodapproximation to the Laplacian eigenmaps embedding based on the uncorruptedkernel matrix. Our results also show how regularization can aid thisapproximation. Experimentally, we explore the effects of noise and occlusion onLaplacian eigenmaps embeddings of two real-world data sets, one from speechprocessing and one from neuroscience, as well as a synthetic data set.
arxiv-16800-191 | Towards using social media to identify individuals at risk for preventable chronic illness | http://arxiv.org/abs/1603.03784 | author:Dane Bell, Daniel Fried, Luwen Huangfu, Mihai Surdeanu, Stephen Kobourov category:cs.CL cs.CY cs.SI published:2016-03-11 summary:We describe a strategy for the acquisition of training data necessary tobuild a social-media-driven early detection system for individuals at risk for(preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-likequiz with data and questions acquired semi-automatically from Twitter. Thequestions are designed to inspire participant engagement and collect relevantdata to train a public-health model applied to individuals. Prior systemsdesigned to use social media such as Twitter to predict obesity (a risk factorfor T2DM) operate on entire communities such as states, counties, or cities,based on statistics gathered by government agencies. Because there isconsiderable variation among individuals within these groups, training data onthe individual level would be more effective, but this data is difficult toacquire. The approach proposed here aims to address this issue. Our strategyhas two steps. First, we trained a random forest classifier on data gatheredfrom (public) Twitter statuses and state-level statistics with state-of-the-artaccuracy. We then converted this classifier into a 20-questions-style quiz andmade it available online. In doing so, we achieved high engagement withindividuals that took the quiz, while also building a training set ofvoluntarily supplied individual-level data for future classification.
arxiv-16800-192 | A Primer on the Signature Method in Machine Learning | http://arxiv.org/abs/1603.03788 | author:Ilya Chevyrev, Andrey Kormilitzin category:stat.ML cs.LG stat.ME published:2016-03-11 summary:In these notes, we wish to provide an introduction to the signature method,focusing on its basic theoretical properties and recent numerical applications. The notes are split into two parts. The first part focuses on the definitionand fundamental properties of the signature of a path, or the path signature.We have aimed for a minimalistic approach, assuming only familiarity withclassical real analysis and integration theory, and supplementing theory withstraightforward examples. We have chosen to focus in detail on the principleproperties of the signature which we believe are fundamental to understandingits role in applications. We also present an informal discussion on some of itsdeeper properties and briefly mention the role of the signature in rough pathstheory, which we hope could serve as a light introduction to rough paths forthe interested reader. The second part of these notes discusses practical applications of the pathsignature to the area of machine learning. The signature approach represents anon-parametric way for extraction of characteristic features from data. Thedata are converted into a multi-dimensional path by means of various embeddingalgorithms and then processed for computation of individual terms of thesignature which summarise certain information contained in the data. Thesignature thus transforms raw data into a set of features which are used inmachine learning tasks. We will review current progress in applications ofsignatures to machine learning problems.
arxiv-16800-193 | Region Graph Based Method for Multi-Object Detection and Tracking using Depth Cameras | http://arxiv.org/abs/1603.03783 | author:Sachin Mehta, Balakrishnan Prabhakaran category:cs.CV published:2016-03-11 summary:In this paper, we propose a multi-object detection and tracking method usingdepth cameras. Depth maps are very noisy and obscure in object detection. Wefirst propose a region-based method to suppress high magnitude noise whichcannot be filtered using spatial filters. Second, the proposed method detectRegion of Interests by temporal learning which are then tracked using weightedgraph-based approach. We demonstrate the performance of the proposed method onstandard depth camera datasets with and without object occlusions. Experimentalresults show that the proposed method is able to suppress high magnitude noisein depth maps and detect/track the objects (with and without occlusion).
arxiv-16800-194 | An investigation of coreference phenomena in the biomedical domain | http://arxiv.org/abs/1603.03758 | author:Dane Bell, Gus Hahn-Powell, Marco A. Valenzuela-Escárcega, Mihai Surdeanu category:cs.CL published:2016-03-11 summary:We describe challenges and advantages unique to coreference resolution in thebiomedical domain, and a sieve-based architecture that leverages domainknowledge for both entity and event coreference resolution. Domain-generalcoreference resolution algorithms perform poorly on biomedical documents,because the cues they rely on such as gender are largely absent in this domain,and because they do not encode domain-specific knowledge such as the number andtype of participants required in chemical reactions. Moreover, it is difficultto directly encode this knowledge into most coreference resolution algorithmsbecause they are not rule-based. Our rule-based architecture uses sequentiallyapplied hand-designed "sieves", with the output of each sieve informing andconstraining subsequent sieves. This architecture provides a 3.2% increase inthroughput to our Reach event extraction system with precision parallel to thatof the stricter system that relies solely on syntactic patterns for extraction.
arxiv-16800-195 | Training with Exploration Improves a Greedy Stack-LSTM Parser | http://arxiv.org/abs/1603.03793 | author:Miguel Ballesteros, Yoav Goldberg, Chris Dyer, Noah A. Smith category:cs.CL published:2016-03-11 summary:We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) tosupport a training-with-exploration procedure using dynamic oracles(Goldbergand Nivre, 2013) instead of cross-entropy minimization. This form of training,which accounts for model predictions at training time rather than assuming anerror-free action history, improves parsing accuracies for both English andChinese, obtaining very strong results for both languages. We discuss somemodifications needed in order to get training with exploration to work well fora probabilistic neural-network.
arxiv-16800-196 | Demonstrating the Feasibility of Automatic Game Balancing | http://arxiv.org/abs/1603.03795 | author:Vanessa Volz, Günter Rudolph, Boris Naujoks category:cs.HC cs.AI cs.NE published:2016-03-11 summary:Game balancing is an important part of the (computer) game design process, inwhich designers adapt a game prototype so that the resulting gameplay is asentertaining as possible. In industry, the evaluation of a game is often basedon costly playtests with human players. It suggests itself to automate thisprocess using surrogate models for the prediction of gameplay and outcome. Inthis paper, the feasibility of automatic balancing using simulation- anddeck-based objectives is investigated for the card game top trumps.Additionally, the necessity of a multi-objective approach is asserted by acomparison with the only known (single-objective) method. We apply amulti-objective evolutionary algorithm to obtain decks that optimiseobjectives, e.g. win rate and average number of tricks, developed to expressthe fairness and the excitement of a game of top trumps. The results arecompared with decks from published top trumps decks using simulation-basedobjectives. The possibility to generate decks better or at least as good asdecks from published top trumps decks in terms of these objectives isdemonstrated. Our results indicate that automatic balancing with the presentedapproach is feasible even for more complex games such as real-time strategygames.
arxiv-16800-197 | Efficient Clustering of Correlated Variables and Variable Selection in High-Dimensional Linear Models | http://arxiv.org/abs/1603.03724 | author:Niharika Gauraha, Swapan K. Parui category:stat.ML cs.LG published:2016-03-11 summary:In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variableselection in high dimensional sparse regression models with strongly correlatedvariables. To handle correlated variables, the concept of clustering orgrouping variables and then pursuing model fitting is widely accepted. When thedimension is very high, finding an appropriate group structure is as difficultas the original problem. The ACL is a three-stage procedure where, at the firststage, we use the Lasso(or its adaptive or thresholded version) to do initialselection, then we also include those variables which are not selected by theLasso but are strongly correlated with the variables selected by the Lasso. Atthe second stage we cluster the variables based on the reduced set ofpredictors and in the third stage we perform sparse estimation such as Lasso oncluster representatives or the group Lasso based on the structures generated byclustering procedure. We show that our procedure is consistent and efficient infinding true underlying population group structure(under assumption ofirrepresentable and beta-min conditions). We also study the group selectionconsistency of our method and we support the theory using simulated andpseudo-real dataset examples.
arxiv-16800-198 | Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow | http://arxiv.org/abs/1603.03805 | author:Huishuai Zhang, Yuejie Chi, Yingbin Liang category:stat.ML published:2016-03-11 summary:Solving systems of quadratic equations is a central problem in machinelearning and signal processing. One important example is phase retrieval, whichaims to recover a signal from only magnitudes of its linear measurements. Thispaper focuses on the situation when the measurements are corrupted by arbitraryoutliers, for which the recently developed non-convex gradient descentWirtinger flow (WF) and truncated Wirtinger flow (TWF) algorithms likely fail.We develop a novel median-TWF algorithm that exploits robustness of samplemedian to resist arbitrary outliers in the initialization and the gradientupdate in each iteration. We show that such a non-convex algorithm provablyrecovers the signal from a near-optimal number of measurements composed ofi.i.d. Gaussian entries, up to a logarithmic factor, even when a constantportion of the measurements are corrupted by arbitrary outliers. We furthershow that median-TWF is also robust when measurements are corrupted by botharbitrary outliers and bounded noise. Our analysis of performance guarantee isaccomplished by development of non-trivial concentration measures ofmedian-related quantities, which may be of independent interest. We furtherprovide numerical experiments to demonstrate the effectiveness of the approach.
arxiv-16800-199 | Searching for Topological Symmetry in Data Haystack | http://arxiv.org/abs/1603.03703 | author:Kallol Roy, Anh Tong, Jaesik Choi category:cs.LG published:2016-03-11 summary:Finding interesting symmetrical topological structures in high-dimensionalsystems is an important problem in statistical machine learning. Limited amountof available high-dimensional data and its sensitivity to noise posecomputational challenges to find symmetry. Our paper presents a new method tofind local symmetries in a low-dimensional 2-D grid structure which is embeddedin high-dimensional structure. To compute the symmetry in a grid structure, weintroduce three legal grid moves (i) Commutation (ii) Cyclic Permutation (iii)Stabilization on sets of local grid squares, grid blocks. The three grid movesare legal transformations as they preserve the statistical distribution ofhamming distances in each grid block. We propose and coin the term of gridsymmetry of data on the 2-D data grid as the invariance of statisticaldistributions of hamming distance are preserved after a sequence of grid moves.We have computed and analyzed the grid symmetry of data on multivariateGaussian distributions and Gamma distributions with noise.
arxiv-16800-200 | Determination of the edge of criticality in echo state networks through Fisher information maximization | http://arxiv.org/abs/1603.03685 | author:Lorenzo Livi, Filippo Maria Bianchi, Cesare Alippi category:cs.LG cs.NE published:2016-03-11 summary:It is a widely accepted fact that the computational capability of recurrentneural networks is maximized on the so-called "edge of criticality". Once inthis configuration, the network performs efficiently on a specific applicationboth in terms of (i) low prediction error and (ii) high short-term memorycapacity. Since the behavior of recurrent networks is strongly influenced bythe particular input signal driving the dynamics, a universal,application-independent method for determining the edge of criticality is stillmissing. In this paper, we propose a theoretically motivated method based onFisher information for determining the edge of criticality in recurrent neuralnetworks. It is proven that Fisher information is maximized for (finite-size)systems operating in such critical regions. However, Fisher information isnotoriously difficult to compute and either requires the probability densityfunction or the conditional dependence of the system states with respect to themodel parameters. The paper exploits a recently-developed non-parametricestimator of the Fisher information matrix and provides a method to determinethe critical region of echo state networks, a particular class of recurrentnetworks. The considered control parameters, which indirectly affect the echostate network performance, are suitably controlled to identify a collection ofnetwork configurations lying on the edge of criticality and, as such,maximizing Fisher information and computational performance.
arxiv-16800-201 | Distance Metric Tracking | http://arxiv.org/abs/1603.03678 | author:Kristjan Greenewald, Stephen Kelley, Alfred Hero category:stat.ML cs.LG published:2016-03-11 summary:Recent work in distance metric learning focused on learning transformationsof data that best align with provided sets of pairwise similarity anddissimilarity constraints. The learned transformations lead to improvedretrieval, classification, and clustering algorithms due to the more accuratedistance or similarity measures. Here, we introduce the problem of learningthese transformations when the underlying constraint generation process isdynamic. These dynamics can be due to changes in either the ground-truth labelsused to generate constraints or changes to the feature subspaces in which theclass structure is apparent. We propose and evaluate an adaptive, onlinealgorithm for learning and tracking metrics as they change over time. Wedemonstrate the proposed algorithm on both real and synthetic data sets andshow significant performance improvements relative to previously proposed batchand online distance metric learning algorithms.
arxiv-16800-202 | Learning Gaze Transitions from Depth to Improve Video Saliency Estimation | http://arxiv.org/abs/1603.03669 | author:G. Leifman, D. Rudoy, T. Swedish, E. Bayro-Corrochano, R. Raskar category:cs.CV published:2016-03-11 summary:In this paper we introduce a novel Depth-Aware Video Saliency approach topredict human focus of attention when viewing RGBD videos on regular 2Dscreens. We train a generative convolutional neural network which predicts asaliency map for a frame, given the fixation map of the previous frame.Saliency estimation in this scenario is highly important since in the nearfuture 3D video content will be easily acquired and yet hard to display. Thiscan be explained, on the one hand, by the dramatic improvement of 3D-capableacquisition equipment. On the other hand, despite the considerable progress in3D display technologies, most of the 3D displays are still expensive andrequire wearing special glasses. To evaluate the performance of our approach,we present a new comprehensive database of eye-fixation ground-truth for RGBDvideos. Our experiments indicate that integrating depth into video saliencycalculation is beneficial. We demonstrate that our approach outperformsstate-of-the-art methods for video saliency, achieving 15% relativeimprovement.
arxiv-16800-203 | Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting | http://arxiv.org/abs/1603.03657 | author:Koen Groenland, Sander Bohte category:cs.LG cs.CV cs.NE published:2016-03-11 summary:When a Convolutional Neural Network is used for on-the-fly evaluation ofcontinuously updating time-sequences, many redundant convolution operations areperformed. We propose the method of Deep Shifting, which remembers previouslycalculated results of convolution operations in order to minimize the number ofcalculations. The reduction in complexity is at least a constant and in thebest case quadratic. We demonstrate that this method does indeed savesignificant computation time in a practical implementation, especially when thenetworks receives a large number of time-frames.
arxiv-16800-204 | Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive Dependencies | http://arxiv.org/abs/1603.03629 | author:David I. Inouye, Pradeep Ravikumar, Inderjit S. Dhillon category:stat.ML published:2016-03-11 summary:We develop a novel class of parametric graphical models, called Square RootGraphical Models (SQR), that provides multivariate generalizations ofunivariate exponential family distributions---e.g. discrete, Gaussian,exponential and Poisson distributions. Previous multivariate graphical modelsdid not allow positive dependencies for the exponential and Poissongeneralizations. However, in many real-world datasets, variables clearly havepositive dependencies. For example, the airport delay time in NewYork---modeled as an exponential distribution---is positively related to thedelay time in Boston. With this motivation, we give an example of our modelclass derived from the univariate exponential distribution that allows foralmost arbitrary positive and negative dependencies with only a mild conditionon the parameter matrix---a condition akin to the positive definiteness of theGaussian covariance matrix. Our Poisson generalization allows for both positiveand negative dependencies without any constraints on the parameter values. Wealso develop parameter estimation methods using node-wise regressions with$\ell_1$ regularization and likelihood approximation methods using sampling.Finally, we demonstrate our exponential generalization on a dataset of airportdelay times.
arxiv-16800-205 | Learning from Imbalanced Multiclass Sequential Data Streams Using Dynamically Weighted Conditional Random Fields | http://arxiv.org/abs/1603.03627 | author:Roberto L. Shinmoto Torres, Damith C. Ranasinghe, Qinfeng Shi, Anton van den Hengel category:cs.LG published:2016-03-11 summary:The present study introduces a method for improving the classificationperformance of imbalanced multiclass data streams from wireless body wornsensors. Data imbalance is an inherent problem in activity recognition causedby the irregular time distribution of activities, which are sequential anddependent on previous movements. We use conditional random fields (CRF), agraphical model for structured classification, to take advantage ofdependencies between activities in a sequence. However, CRFs do not considerthe negative effects of class imbalance during training. We propose aclass-wise dynamically weighted CRF (dWCRF) where weights are automaticallydetermined during training by maximizing the expected overall F-score. Ourresults based on three case studies from a healthcare application using abatteryless body worn sensor, demonstrate that our method, in general, improvesoverall and minority class F-score when compared to other CRF based classifiersand achieves similar or better overall and class-wise performance when comparedto SVM based classifiers under conditions of limited training data. We alsoconfirm the performance of our approach using an additional battery poweredbody worn sensor dataset, achieving similar results in cases of high classimbalance.
arxiv-16800-206 | A Recursive Born Approach to Nonlinear Inverse Scattering | http://arxiv.org/abs/1603.03768 | author:Ulugbek S. Kamilov, Dehong Liu, Hassan Mansour, Petros T. Boufounos category:cs.LG physics.optics published:2016-03-11 summary:The Iterative Born Approximation (IBA) is a well-known method for describingwaves scattered by semi-transparent objects. In this paper, we present a novelnonlinear inverse scattering method that combines IBA with an edge-preservingtotal variation (TV) regularizer. The proposed method is obtained by relatingiterations of IBA to layers of a feedforward neural network and developing acorresponding error backpropagation algorithm for efficiently estimating thepermittivity of the object. Simulations illustrate that, by accounting formultiple scattering, the method successfully recovers the permittivitydistribution where the traditional linear inverse scattering fails.
arxiv-16800-207 | A short proof that $O_2$ is an MCFL | http://arxiv.org/abs/1603.03610 | author:Mark-Jan Nederhof category:cs.FL cs.CL 68T50 I.2.7; F.4.2 published:2016-03-11 summary:We present a new proof that $O_2$ is a multiple context-free language. Itcontrasts with a recent proof by Salvati (2015) in its avoidance of conceptsthat seem specific to two-dimensional geometry, such as the complex exponentialfunction. Our simple proof creates realistic prospects of widening the resultsto higher dimensions. This finding is of central importance to the relationbetween extreme free word order and classes of grammars used to describe thesyntax of natural language.
arxiv-16800-208 | Fast Optical Flow using Dense Inverse Search | http://arxiv.org/abs/1603.03590 | author:Till Kroeger, Radu Timofte, Dengxin Dai, Luc Van Gool category:cs.CV cs.RO published:2016-03-11 summary:Most recent works in optical flow extraction focus on the accuracy andneglect the time complexity. However, in real-life visual applications, such astracking, activity detection and recognition, the time complexity is critical. We propose a solution with very low time complexity and competitive accuracyfor the computation of dense optical flow. It consists of three parts: 1)inverse search for patch correspondences; 2) dense displacement field creationthrough patch aggregation along multiple scales; 3) variational refinement. Atthe core of our Dense Inverse Search-based method (DIS) is the efficient searchof correspondences inspired by the inverse compositional image alignmentproposed by Baker and Matthews in 2001. DIS is competitive on standard optical flow benchmarks with largedisplacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching thetemporal resolution of human's biological vision system. It is order(s) ofmagnitude faster than state-of-the-art methods in the same range of accuracy,making DIS ideal for visual applications.
arxiv-16800-209 | Watch-n-Patch: Unsupervised Learning of Actions and Relations | http://arxiv.org/abs/1603.03541 | author:Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.LG cs.RO published:2016-03-11 summary:There is a large variation in the activities that humans perform in theireveryday lives. We consider modeling these composite human activities whichcomprises multiple basic level actions in a completely unsupervised setting.Our model learns high-level co-occurrence and temporal relations between theactions. We consider the video as a sequence of short-term action clips, whichcontains human-words and object-words. An activity is about a set ofaction-topics and object-topics indicating which actions are present and whichobjects are interacting with. We then propose a new probabilistic modelrelating the words and the topics. It allows us to model long-range actionrelations that commonly exist in the composite activities, which is challengingin previous works. We apply our model to the unsupervised action segmentationand clustering, and to a novel application that detects forgotten actions,which we call action patching. For evaluation, we contribute a new challengingRGB-D activity video dataset recorded by the new Kinect v2, which containsseveral human daily activities as compositions of multiple actions interactingwith different objects. Moreover, we develop a robotic system that watchespeople and reminds people by applying our action patching algorithm. Ourrobotic setup can be easily deployed on any assistive robot.
arxiv-16800-210 | Dimension Coupling: Optimal Active Learning of Halfspaces via Query Synthesis | http://arxiv.org/abs/1603.03515 | author:Lin Chen, Hamed Hassani, Amin Karbasi category:cs.AI cs.IT cs.LG math.IT published:2016-03-11 summary:In this paper, we consider the problem of actively learning a linearclassifier through query synthesis where the learner can construct artificialqueries in order to estimate the true decision boundaries. This problem hasrecently gained a lot of interest in automated science and adversarial reverseengineering for which only heuristic algorithms are known. In suchapplications, queries can be constructed de novo to elicit information (e.g.,automated science) or to evade detection with minimal cost (e.g., adversarialreverse engineering). We develop a general framework, called dimension coupling (DC), that 1)reduces a d-dimensional learning problem to d-1 low-dimensional sub-problems,2) solves each sub-problem efficiently, and 3) appropriately aggregates theresults and outputs a linear classifier. We consider the three most commonscenarios in the literature: idealized noise-free, independent noiserealizations, and agnostic settings. We show that the DC framework avoids thecurse of dimensionality: its computational complexity in all three cases scaleslinearly with the dimension. Moreover, in the noiseless and noisy cases, weshow that the query complexity of DC is near optimal (within a constant factorof the optimum algorithm). We also develop an agnostic variant of DC for whichwe provide strong theoretical guarantees. To further support our theoreticalanalysis, we compare the performance of DC with the existing work in all threesettings. We observe that DC consistently outperforms the prior arts in termsof query complexity while often running orders of magnitude faster.
arxiv-16800-211 | Cost-sensitive Learning for Bidding in Online Advertising Auctions | http://arxiv.org/abs/1603.03713 | author:Flavian Vasile, Damien Lefortier category:cs.LG published:2016-03-11 summary:One of the most challenging problems in computational advertising is theprediction of ad click and conversion rates for bidding in online advertisingauctions. State-of- the-art prediction methods include using the maximumentropy framework (also known as logistic regression) and log linear models.However, one unaddressed problem in the previous approaches is the existence ofhighly non-uniform misprediction costs. In this paper, we present our approachfor making cost-sensitive predictions for bidding in online advertisingauctions. We show that one can get significant lifts in offline and onlineperformance by using a simple modification of the logistic loss function.
arxiv-16800-212 | Distribution Free Learning with Local Queries | http://arxiv.org/abs/1603.03714 | author:Galit Bary-Weisberg, Amit Daniely, Shai Shalev-Shwartz category:cs.LG published:2016-03-11 summary:The model of learning with \emph{local membership queries} interpolatesbetween the PAC model and the membership queries model by allowing the learnerto query the label of any example that is similar to an example in the trainingset. This model, recently proposed and studied by Awasthi, Feldman and Kanade,aims to facilitate practical use of membership queries. We continue this line of work, proving both positive and negative results inthe {\em distribution free} setting. We restrict to the boolean cube $\{-1,1\}^n$, and say that a query is $q$-local if it is of a hamming distance $\leq$ from some training example. On the positive side, we show that $1$-localqueries already give an additional strength, and allow to learn a certain typeof DNF formulas. On the negative side, we show that even$\left(n^{0.99}\right)$-local queries cannot help to learn various classesincluding Automata, DNFs and more. Likewise, $q$-local queries for any constant$q$ cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more.Moreover, for these classes, an algorithm that uses$\left(\log^{0.99}(n)\right)$-local queries would lead to a breakthrough in thebest known running times.
arxiv-16800-213 | Low-rank passthrough neural networks | http://arxiv.org/abs/1603.03116 | author:Antonio Valerio Miceli Barone category:cs.LG cs.NE published:2016-03-10 summary:Deep learning consists in training neural networks to perform computationsthat sequentially unfold in many steps over a time dimension or an intrinsicdepth dimension. Effective learning in this setting is usually accomplished byspecialized network architectures that are designed to mitigate the vanishinggradient problem of naive deep networks. Many of these architectures, such asLSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a singlestructural principle: the state passthrough. We observe that these architectures, hereby characterized as PassthroughNetworks, in addition to the mitigation of the vanishing gradient problem,enable the decoupling of the network state size from the number of parametersof the network, a possibility that is exploited in some recent works but notthoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plusdiagonal matrix parametrizations for Passthrough Networks which exploit thisdecoupling property, reducing the data complexity and memory requirements ofthe network while preserving its memory capacity. We present competitiveexperimental results on synthetic tasks and a near state of the art result onsequential randomly-permuted MNIST classification, a hard task on natural data.
arxiv-16800-214 | Texture Networks: Feed-forward Synthesis of Textures and Stylized Images | http://arxiv.org/abs/1603.03417 | author:Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky category:cs.CV published:2016-03-10 summary:Gatys et al. recently demonstrated that deep networks can generate beautifultextures and stylized images from a single texture example. However, theirmethods requires a slow and memory-consuming optimization process. We proposehere an alternative approach that moves the computational burden to a learningstage. Given a single example of a texture, our approach trains compactfeed-forward convolutional networks to generate multiple samples of the sametexture of arbitrary size and to transfer artistic style from a given image toany other image. The resulting networks are remarkably light-weight and cangenerate textures of quality comparable to Gatys~et~al., but hundreds of timesfaster. More generally, our approach highlights the power and flexibility ofgenerative feed-forward models trained with complex and expressive lossfunctions.
arxiv-16800-215 | Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre) | http://arxiv.org/abs/1603.03112 | author:Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji category:cs.CL cs.AI published:2016-03-10 summary:Recent research has shown great progress on fine-grained entity typing. Mostexisting methods require pre-defining a set of types and training a multi-classclassifier from a large labeled data set based on multi-level linguisticfeatures. They are thus limited to certain domains, genres and languages. Inthis paper, we propose a novel unsupervised entity typing framework bycombining symbolic and distributional semantics. We start from learning generalembeddings for each entity mention, compose the embeddings of specific contextsusing linguistic structures, link the mention to knowledge bases and learn itsrelated knowledge representations. Then we develop a novel joint hierarchicalclustering and linking algorithm to type all mentions using theserepresentations. This framework doesn't rely on any annotated data, predefinedtyping schema, or hand-crafted features, therefore it can be quickly adapted toa new domain, genre and language. Furthermore, it has great flexibility atincorporating linguistic structures (e.g., Abstract Meaning Representation(AMR), dependency relations) to improve specific context representation.Experiments on genres (news and discussion forum) show comparable performancewith state-of-the-art supervised typing systems trained from a large amount oflabeled data. Results on various languages (English, Chinese, Japanese, Hausa,and Yoruba) and domains (general and biomedical) demonstrate the portability ofour framework.
arxiv-16800-216 | Exploring Context with Deep Structured models for Semantic Segmentation | http://arxiv.org/abs/1603.03183 | author:Guosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid category:cs.CV published:2016-03-10 summary:State-of-the-art semantic image segmentation methods are mostly based ontraining deep convolutional neural networks (CNNs). In this work, we proffer toimprove semantic segmentation with the use of contextual information. Inparticular, we explore `patch-patch' context and `patch-background' context indeep CNNs. We formulate deep structured models by combining CNNs andConditional Random Fields (CRFs) for learning the patch-patch context betweenimage regions. Specifically, we formulate CNN-based pairwise potentialfunctions to capture semantic correlations between neighboring patches.Efficient piecewise training of the proposed deep structured model is thenapplied in order to avoid repeated expensive CRF inference during the course ofback propagation. For capturing the patch-background context, we show that anetwork design with traditional multi-scale image inputs and sliding pyramidpooling is very effective for improving performance. We perform comprehensiveevaluation of the proposed method. We achieve new state-of-the-art performanceon a number of challenging semantic segmentation datasets including $NYUDv2$,$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report anintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.
arxiv-16800-217 | Scalable Linear Causal Inference for Irregularly Sampled Time Series with Long Range Dependencies | http://arxiv.org/abs/1603.03336 | author:Francois W. Belletti, Evan R. Sparks, Michael J. Franklin, Alexandre M. Bayen, Joseph E. Gonzalez category:cs.LG stat.ME published:2016-03-10 summary:Linear causal analysis is central to a wide range of important applicationspanning finance, the physical sciences, and engineering. Much of the existingliterature in linear causal analysis operates in the time domain.Unfortunately, the direct application of time domain linear causal analysis tomany real-world time series presents three critical challenges: irregulartemporal sampling, long range dependencies, and scale. Moreover, real-worlddata is often collected at irregular time intervals across vast arrays ofdecentralized sensors and with long range dependencies which make naive timedomain correlation estimators spurious. In this paper we present a frequencydomain based estimation framework which naturally handles irregularly sampleddata and long range dependencies while enabled memory and communicationefficient distributed processing of time series data. By operating in thefrequency domain we eliminate the need to interpolate and help mitigate theeffects of long range dependencies. We implement and evaluate our new work-flowin the distributed setting using Apache Spark and demonstrate on both MonteCarlo simulations and high-frequency financial trading that we can accuratelyrecover causal structure at scale.
arxiv-16800-218 | Template Matching on the Roto-Translation Group | http://arxiv.org/abs/1603.03304 | author:Erik J. Bekkers, Marco Loog, Bart M. ter Haar Romeny, Remco Duits category:cs.CV math.GR published:2016-03-10 summary:We propose a template matching method for the detection of 2D image objectsthat are characterized by orientation patterns. Our method is based on datarepresentations via orientation scores, which are functions on the space ofpositions and orientations, and which are obtained via a wavelet-typetransform. This new representation allows us to detect orientation patterns inan intuitive and direct way, namely via cross-correlations. Additionally, wepropose a generalized linear regression framework for the construction ofsuitable templates using smoothing splines. Here, it is important to recognizea curved geometry on the position-orientation domain, which we identify withthe Lie group SE(2): the roto-translation group. Templates are then optimizedin a B-spline basis, and smoothness is defined with respect to the curvedgeometry. We achieve state-of-the-art results on two important detectionproblems in retinal imaging: detection of the optic nerve head (99.83\% successrate on 1737 images) and detection of the fovea (99.32\% success rate on 1616images). The high performance is due to inclusion of both intensity andorientation features with effective geometric priors in the template matching.Moreover, our method is fast due to a cross-correlation based matchingapproach.
arxiv-16800-219 | UTSig: A Persian Offline Signature Dataset | http://arxiv.org/abs/1603.03235 | author:Amir Soleimani, Kazim Fouladi, Babak N. Araabi category:cs.CV published:2016-03-10 summary:The crucial role of datasets in signature verification systems has motivatedresearchers to collect signature samples. However, with regard to the distinctcharacteristics of Persian signature, existing offline signature datasetscannot be used in Persian systems. This paper presents a new and public Persianoffline signature dataset, UTSig, which consists of 8280 images from 115classes that each class has 27 genuine, 3 opposite-hand signatures of thegenuine signer, and 42 skilled forgeries made by 6 forgers from 230 people.Compared to the other public datasets, UTSig has larger number of samples,classes, and forgers. Meanwhile its samples were collected by consideringvariables such as signing period, writing instrument, signature box size, andnumber of observable samples for forgers. Reviewing the main characteristics ofoffline signature datasets, we statistically show that Persian signatures hasfewer number of branch points and end points. We propose and test fourdifferent training and testing setups for UTSig. Results of our experimentsshow that training genuine samples along with opposite-hand signed samples andrandom forgeries can improve the performance in terms of equal error rate andminimum cost of log likelihood ratio which is an information theoreticcriterion.
arxiv-16800-220 | Part-of-Speech Tagging for Historical English | http://arxiv.org/abs/1603.03144 | author:Yi Yang, Jacob Eisenstein category:cs.CL cs.DL published:2016-03-10 summary:As more historical texts are digitized, there is interest in applying naturallanguage processing tools to these archives. However, the performance of thesetools is often unsatisfactory, due to language change and genre differences.Spelling normalization heuristics are the dominant solution for dealing withhistorical texts, but this approach fails to account for changes in usage andvocabulary. In this empirical paper, we assess the capability of domainadaptation techniques to cope with historical texts, focusing on the classicbenchmark task of part-of-speech tagging. We evaluate several domain adaptationmethods on the task of tagging Early Modern English and Modern British Englishtexts in the Penn Corpora of Historical English. We demonstrate that theFeature Embedding method for unsupervised domain adaptation outperforms wordembeddings and Brown clusters, showing the importance of embedding the entirefeature space, rather than just individual words. Feature Embeddings also givebetter performance than spelling normalization, but the combination of the twomethods is better still, yielding a 5% raw improvement in tagging accuracy onEarly Modern English texts.
arxiv-16800-221 | Temporally coherent 4D reconstruction of complex dynamic scenes | http://arxiv.org/abs/1603.03381 | author:Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton category:cs.CV published:2016-03-10 summary:This paper presents an approach for reconstruction of 4D temporally coherentmodels of complex dynamic scenes. No prior knowledge is required of scenestructure or camera calibration allowing reconstruction from multiple movingcameras. Sparse-to-dense temporal correspondence is integrated with jointmulti-view segmentation and reconstruction to obtain a complete 4Drepresentation of static and dynamic objects. Temporal coherence is exploitedto overcome visual ambiguities resulting in improved reconstruction of complexscenes. Robust joint segmentation and reconstruction of dynamic objects isachieved by introducing a geodesic star convexity constraint. Comparativeevaluation is performed on a variety of unstructured indoor and outdoor dynamicscenes with hand-held cameras and multiple people. This demonstratesreconstruction of complete temporally coherent 4D scene models with improvednonrigid object segmentation and shape reconstruction.
arxiv-16800-222 | An Innovative Imputation and Classification Approach for Accurate Disease Prediction | http://arxiv.org/abs/1603.03281 | author:Yelipe UshaRani, P. Sammulal category:cs.DB cs.IR cs.LG published:2016-03-10 summary:Imputation of missing attribute values in medical datasets for extractinghidden knowledge from medical datasets is an interesting research topic ofinterest which is very challenging. One cannot eliminate missing values inmedical records. The reason may be because some tests may not been conducted asthey are cost effective, values missed when conducting clinical trials, valuesmay not have been recorded to name some of the reasons. Data mining researchershave been proposing various approaches to find and impute missing values toincrease classification accuracies so that disease may be predicted accurately.In this paper, we propose a novel imputation approach for imputation of missingvalues and performing classification after fixing missing values. The approachis based on clustering concept and aims at dimensionality reduction of therecords. The case study discussed shows that missing values can be fixed andimputed efficiently by achieving dimensionality reduction. The importance ofproposed approach for classification is visible in the case study which assignssingle class label in contrary to multi-label assignment if dimensionalityreduction is not performed.
arxiv-16800-223 | Data fluidity in DARIAH -- pushing the agenda forward | http://arxiv.org/abs/1603.03170 | author:Laurent Romary, Mike Mertens, Anne Baillot category:cs.CY cs.CL cs.DL published:2016-03-10 summary:This paper provides both an update concerning the setting up of the EuropeanDARIAH infrastructure and a series of strong action lines related to thedevelopment of a data centred strategy for the humanities in the coming years.In particular we tackle various aspect of data management: data hosting, thesetting up of a DARIAH seal of approval, the establishment of a charter betweencultural heritage institutions and scholars and finally a specific view oncertification mechanisms for data.
arxiv-16800-224 | Summary Transfer: Exemplar-based Subset Selection for Video Summarization | http://arxiv.org/abs/1603.03369 | author:Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman category:cs.CV published:2016-03-10 summary:Video summarization has unprecedented importance to help us digest, browse,and search today's ever-growing video collections. We propose a novel subsetselection technique that leverages supervision in the form of human-createdsummaries to perform automatic keyframe-based video summarization. The mainidea is to nonparametrically transfer summary structures from annotated videosto unseen test videos. We show how to extend our method to exploit semanticside information about the video's category/genre to guide the transfer processby those training videos semantically consistent with the test input. We alsoshow how to generalize our method to subshot-based summarization, which notonly reduces computational costs but also provides more flexible ways ofdefining visual similarity across subshots spanning several frames. We conductextensive evaluation on several benchmarks and demonstrate promising results,outperforming existing methods in several settings.
arxiv-16800-225 | Instance-Aware Hashing for Multi-Label Image Retrieval | http://arxiv.org/abs/1603.03234 | author:Hanjiang Lai, Pan Yan, Xiangbo Shu, Yunchao Wei, Shuicheng Yan category:cs.CV published:2016-03-10 summary:Similarity-preserving hashing is a commonly used method for nearest neighboursearch in large-scale image retrieval. For image retrieval, deep-networks-basedhashing methods are appealing since they can simultaneously learn effectiveimage representations and compact hash codes. This paper focuses ondeep-networks-based hashing for multi-label images, each of which may containobjects of multiple categories. In most existing hashing methods, each image isrepresented by one piece of hash code, which is referred to as semantichashing. This setting may be suboptimal for multi-label image retrieval. Tosolve this problem, we propose a deep architecture that learns\textbf{instance-aware} image representations for multi-label image data, whichare organized in multiple groups, with each group containing the features forone category. The instance-aware representations not only bring advantages tosemantic hashing, but also can be used in category-aware hashing, in which animage is represented by multiple pieces of hash codes and each piece of codecorresponds to a category. Extensive evaluations conducted on several benchmarkdatasets demonstrate that, for both semantic hashing and category-awarehashing, the proposed method shows substantial improvement over thestate-of-the-art supervised and unsupervised hashing methods.
arxiv-16800-226 | Pymanopt: A Python Toolbox for Manifold Optimization using Automatic Differentiation | http://arxiv.org/abs/1603.03236 | author:James Townsend, Niklas Koep, Sebastian Weichwald category:cs.MS cs.LG math.OC stat.ML published:2016-03-10 summary:Manifold optimization is a method for (non-convex) optimization of anobjective function, subject to constraints which are smooth, in the sense thatthe set of points which satisfy the constraints admits the structure of adifferentiable manifold. While many optimization problems are of the describedform, technicalities of differential geometry and the laborious calculation ofderivatives pose a significant barrier for experimenting with manifoldoptimization techniques. We introduce Pymanopt (available at https://pymanopt.github.io), a manifoldoptimization toolbox implemented in Python that - similarly to the ManoptMatlab toolbox - implements several manifold geometries and optimizationalgorithms. Moreover, we lower the barriers to users further by using automateddifferentiation for calculating derivative information, saving users time andsaving them from potential calculation and implementation errors.
arxiv-16800-227 | Global and Local Uncertainty Principles for Signals on Graphs | http://arxiv.org/abs/1603.03030 | author:Nathanael Perraudin, Benjamin Ricaud, David Shuman, Pierre Vandergheynst category:stat.ML math-ph math.MP published:2016-03-10 summary:Uncertainty principles such as Heisenberg's provide limits on thetime-frequency concentration of a signal, and constitute an importanttheoretical tool for designing and evaluating linear signal transforms.Generalizations of such principles to the graph setting can inform dictionarydesign for graph signals, lead to algorithms for reconstructing missinginformation from graph signals via sparse representations, and yield new graphanalysis tools. While previous work has focused on generalizing notions ofspreads of a graph signal in the vertex and graph spectral domains, ourapproach is to generalize the methods of Lieb in order to develop uncertaintyprinciples that provide limits on the concentration of the analysiscoefficients of any graph signal under a dictionary transform whose atoms arejointly localized in the vertex and graph spectral domains. One challenge wehighlight is that due to the inhomogeneity of the underlying graph data domain,the local structure in a single small region of the graph can drasticallyaffect the uncertainty bounds for signals concentrated in different regions ofthe graph, limiting the information provided by global uncertainty principles.Accordingly, we suggest a new way to incorporate a notion of locality, anddevelop local uncertainty principles that bound the concentration of theanalysis coefficients of each atom of a localized graph spectral filter framein terms of quantities that depend on the local structure of the graph aroundthe center vertex of the given atom. Finally, we demonstrate how our proposedlocal uncertainty measures can improve the random sampling of graph signals.
arxiv-16800-228 | Scenario Submodular Cover | http://arxiv.org/abs/1603.03158 | author:Nathaniel Grammel, Lisa Hellerstein, Devorah Kletenik, Patrick Lin category:cs.DS cs.LG published:2016-03-10 summary:Many problems in Machine Learning can be modeled as submodular optimizationproblems. Recent work has focused on stochastic or adaptive versions of theseproblems. We consider the Scenario Submodular Cover problem, which is acounterpart to the Stochastic Submodular Cover problem studied by Golovin andKrause. In Scenario Submodular Cover, the goal is to produce a cover withminimum expected cost, where the expectation is with respect to an empiricaljoint distribution, given as input by a weighted sample of realizations. Incontrast, in Stochastic Submodular Cover, the variables of the inputdistribution are assumed to be independent, and the distribution of eachvariable is given as input. Building on algorithms developed by Cicalese et al.and Golovin and Krause for related problems, we give two approximationalgorithms for Scenario Submodular Cover over discrete distributions. The firstachieves an approximation factor of O(log Qm), where m is the size of thesample and Q is the goal utility. The second, simpler algorithm achieves anapproximation bound of O(log QW), where Q is the goal utility and W is the sumof the integer weights. (Both bounds assume an integer-valued utilityfunction.) Our results yield approximation bounds for other problems involvingnon-independent distributions that are explicitly specified by their support.
arxiv-16800-229 | Real time error detection in metal arc welding process using Artificial Neural Netwroks | http://arxiv.org/abs/1603.03149 | author:Prashant Sharma, Shaju K. Albert, S. Rajeswari category:cs.NE published:2016-03-10 summary:Quality assurance in production line demands reliable weld joints. Human madeerrors is a major cause of faulty production. Promptly Identifying errors inthe weld while welding is in progress will decrease the post inspection costspent on the welding process. Electrical parameters generated during welding,could able to characterize the process efficiently. Parameter values arecollected using high speed data acquisition system. Time series analysis taskssuch as filtering, pattern recognition etc. are performed over the collecteddata. Filtering removes the unwanted noisy signal components and patternrecognition task segregate error patterns in the time series based uponsimilarity, which is performed by Self Organized mapping clustering algorithm.Welder quality is thus compared by detecting and counting number of errorpatterns appeared in his parametric time series. Moreover, Self Organizedmapping algorithm provides the database in which patterns are segregated intotwo classes either desirable or undesirable. Database thus generated is used totrain the classification algorithms, and thereby automating the real time errordetection task. Multi Layer Perceptron and Radial basis function are the twoclassification algorithms used, and their performance has been compared basedon metrics such as specificity, sensitivity, accuracy and time required intraining.
arxiv-16800-230 | Theoretical Comparisons of Learning from Positive-Negative, Positive-Unlabeled, and Negative-Unlabeled Data | http://arxiv.org/abs/1603.03130 | author:Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Masashi Sugiyama category:cs.LG stat.ML published:2016-03-10 summary:In PU learning, a binary classifier is trained only from positive (P) andunlabeled (U) data without negative (N) data. Although N data is missing, itsometimes outperforms PN learning (i.e., supervised learning) in experiments.In this paper, we theoretically compare PU (and the opposite NU) learningagainst PN learning, and prove that, one of PU and NU learning given infinite Udata will almost always improve on PN learning. Our theoretical finding is alsovalidated experimentally.
arxiv-16800-231 | Zipf's law emerges asymptotically during phase transitions in communicative systems | http://arxiv.org/abs/1603.03153 | author:Bohdan B. Khomtchouk, Claes Wahlestedt category:physics.soc-ph cs.CL published:2016-03-10 summary:Zipf's law predicts a power-law relationship between word rank and frequencyin language communication systems, and is widely reported in texts yet remainsenigmatic as to its origins. Computer simulations have shown that languagecommunication systems emerge at an abrupt phase transition in the fidelity ofmappings between symbols and objects. Since the phase transition approximatesthe Heaviside or step function, we show that Zipfian scaling emergesasymptotically at high rank based on the Laplace transform. We therebydemonstrate that Zipf's law gradually emerges from the moment of phasetransition in communicative systems. We show that this power-law scalingbehavior explains the emergence of natural languages at phase transitions. Wefind that the emergence of Zipf's law during language communication suggeststhat the use of rare words in a lexicon is critical for the construction of aneffective communicative system at the phase transition.
arxiv-16800-232 | Personalized Speech recognition on mobile devices | http://arxiv.org/abs/1603.03185 | author:Ian McGraw, Rohit Prabhavalkar, Raziel Alvarez, Montse Gonzalez Arenas, Kanishka Rao, David Rybach, Ouais Alsharif, Hasim Sak, Alexander Gruenstein, Francoise Beaufays, Carolina Parada category:cs.CL cs.LG cs.SD published:2016-03-10 summary:We describe a large vocabulary speech recognition system that is accurate,has low latency, and yet has a small enough memory and computational footprintto run faster than real-time on a Nexus 5 Android smartphone. We employ aquantized Long Short-Term Memory (LSTM) acoustic model trained withconnectionist temporal classification (CTC) to directly predict phonemetargets, and further reduce its memory footprint using an SVD-based compressionscheme. Additionally, we minimize our memory footprint by using a singlelanguage model for both dictation and voice command domains, constructed usingBayesian interpolation. Finally, in order to properly handle device-specificinformation, such as proper names and other context-dependent information, weinject vocabulary items into the decoder graph and bias the language modelon-the-fly. Our system achieves 13.5% word error rate on an open-endeddictation task, running with a median speed that is seven times faster thanreal-time.
arxiv-16800-233 | megaman: Manifold Learning with Millions of points | http://arxiv.org/abs/1603.02763 | author:James McQueen, Marina Meila, Jacob VanderPlas, Zhongyue Zhang category:cs.LG cs.CG stat.ML published:2016-03-09 summary:Manifold Learning is a class of algorithms seeking a low-dimensionalnon-linear representation of high-dimensional data. Thus manifold learningalgorithms are, at least in theory, most applicable to high-dimensional dataand sample sizes to enable accurate estimation of the manifold. Despite this,most existing manifold learning implementations are not particularly scalable.Here we present a Python package that implements a variety of manifold learningalgorithms in a modular and scalable fashion, using fast approximate neighborssearches and fast sparse eigendecompositions. The package incorporatestheoretical advances in manifold learning, such as the unbiased Laplacianestimator and the estimation of the embedding distortion by the Riemannianmetric method. In benchmarks, even on a single-core desktop computer, our codeembeds millions of data points in minutes, and takes just 200 minutes to embedthe main sample of galaxy spectra from the Sloan Digital Sky Survey ---consisting of 0.6 million samples in 3750-dimensions --- a task which has notpreviously been possible.
arxiv-16800-234 | Implicit Discourse Relation Classification via Multi-Task Neural Networks | http://arxiv.org/abs/1603.02776 | author:Yang Liu, Sujian Li, Xiaodong Zhang, Zhifang Sui category:cs.CL cs.AI cs.NE published:2016-03-09 summary:Without discourse connectives, classifying implicit discourse relations is achallenging task and a bottleneck for building a practical discourse parser.Previous research usually makes use of one kind of discourse framework such asPDTB or RST to improve the classification performance on discourse relations.Actually, under different discourse annotation frameworks, there exist multiplecorpora which have internal connections. To exploit the combination ofdifferent discourse corpora, we design related discourse classification tasksspecific to a corpus, and propose a novel Convolutional Neural Network embeddedmulti-task learning system to synthesize these tasks by learning both uniqueand shared representations for each task. The experimental results on the PDTBimplicit discourse relation classification task demonstrate that our modelachieves significant gains over baseline systems.
arxiv-16800-235 | Lexical bundles in computational linguistics academic literature | http://arxiv.org/abs/1603.02905 | author:Adel Rahimi category:cs.CL published:2016-03-09 summary:In this study we analyzed a corpus of 8 million words academic literaturefrom Computational lingustics' academic literature. the lexical bundles fromthis corpus are categorized based on structures and functions.
arxiv-16800-236 | Best-of-K Bandits | http://arxiv.org/abs/1603.02752 | author:Max Simchowitz, Kevin Jamieson, Benjamin Recht category:cs.LG stat.ML published:2016-03-09 summary:This paper studies the Best-of-K Bandit game: At each time the player choosesa subset S among all N-choose-K possible options and observes reward max(X(i) :i in S) where X is a random vector drawn from a joint distribution. Theobjective is to identify the subset that achieves the highest expected rewardwith high probability using as few queries as possible. We presentdistribution-dependent lower bounds based on a particular construction whichforce a learner to consider all N-choose-K subsets, and match naive extensionsof known upper bounds in the bandit setting obtained by treating each subset asa separate arm. Nevertheless, we present evidence that exhaustive search may beavoided for certain, favorable distributions because the influence ofhigh-order order correlations may be dominated by lower order statistics.Finally, we present an algorithm and analysis for independent arms, whichmitigates the surprising non-trivial information occlusion that occurs due toonly observing the max in the subset. This may inform strategies for moregeneral dependent measures, and we complement these result with independent-armlower bounds.
arxiv-16800-237 | Recursive Recurrent Nets with Attention Modeling for OCR in the Wild | http://arxiv.org/abs/1603.03101 | author:Chen-Yu Lee, Simon Osindero category:cs.CV published:2016-03-09 summary:We present recursive recurrent neural networks with attention modeling(R$^2$AM) for lexicon-free optical character recognition in natural sceneimages. The primary advantages of the proposed method are: (1) use of recursiveconvolutional neural networks (CNNs), which allow for parametrically efficientand effective image feature extraction; (2) an implicitly learnedcharacter-level language model, embodied in a recurrent neural network whichavoids the need to use N-grams; and (3) the use of a soft-attention mechanism,allowing the model to selectively exploit image features in a coordinated way,and allowing for end-to-end training within a standard backpropagationframework. We validate our method with state-of-the-art performance onchallenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k.
arxiv-16800-238 | Blind Source Separation: Fundamentals and Recent Advances (A Tutorial Overview Presented at SBrT-2001) | http://arxiv.org/abs/1603.03089 | author:Eleftherios Kofidis category:stat.ML cs.IT math.IT published:2016-03-09 summary:Blind source separation (BSS), i.e., the decoupling of unknown signals thathave been mixed in an unknown way, has been a topic of great interest in thesignal processing community for the last decade, covering a wide range ofapplications in such diverse fields as digital communications, patternrecognition, biomedical engineering, and financial data analysis, among others.This course aims at an introduction to the BSS problem via an exposition ofwell-known and established as well as some more recent approaches to itssolution. A unified way is followed in presenting the various results so as tomore easily bring out their similarities/differences and emphasize theirrelative advantages/disadvantages. Only a representative sample of the existingknowledge on BSS will be included in this course. The interested readers areencouraged to consult the list of bibliographical references for more detailson this exciting and always active research topic.
arxiv-16800-239 | Bipartite Correlation Clustering -- Maximizing Agreements | http://arxiv.org/abs/1603.02782 | author:Megasthenis Asteris, Anastasios Kyrillidis, Dimitris Papailiopoulos, Alexandros G. Dimakis category:cs.DS stat.ML published:2016-03-09 summary:In Bipartite Correlation Clustering (BCC) we are given a complete bipartitegraph $G$ with `+' and `-' edges, and we seek a vertex clustering thatmaximizes the number of agreements: the number of all `+' edges within clustersplus all `-' edges cut across clusters. BCC is known to be NP-hard. We present a novel approximation algorithm for $k$-BCC, a variant of BCC withan upper bound $k$ on the number of clusters. Our algorithm outputs a$k$-clustering that provably achieves a number of agreements within amultiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy$\delta$. It relies on solving a combinatorially constrained bilinearmaximization on the bi-adjacency matrix of $G$. It runs in time exponential in$k$ and $\delta^{-1}$, but linear in the size of the input. Further, we show that, in the (unconstrained) BCC setting, an${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clustersregardless of the size of the graph. In turn, our $k$-BCC algorithm implies anEfficient PTAS for the BCC objective of maximizing agreements.
arxiv-16800-240 | Unsupervised word segmentation and lexicon discovery using acoustic word embeddings | http://arxiv.org/abs/1603.02845 | author:Herman Kamper, Aren Jansen, Sharon Goldwater category:cs.CL published:2016-03-09 summary:In settings where only unlabelled speech data is available, speech technologyneeds to be developed without transcriptions, pronunciation dictionaries, orlanguage modelling text. A similar problem is faced when modelling infantlanguage acquisition. In these cases, categorical linguistic structure needs tobe discovered directly from speech audio. We present a novel unsupervisedBayesian model that segments unlabelled speech and clusters the segments intohypothesized word groupings. The result is a complete unsupervised tokenizationof the input speech in terms of discovered word types. In our approach, apotential word segment (of arbitrary length) is embedded in a fixed-dimensionalacoustic vector space. The model, implemented as a Gibbs sampler, then builds awhole-word acoustic model in this space while jointly performing segmentation.We report word error rates in a small-vocabulary connected digit recognitiontask by mapping the unsupervised decoded output to ground truth transcriptions.The model achieves around 20% error rate, outperforming a previous HMM-basedsystem by about 10% absolute. Moreover, in contrast to the baseline, our modeldoes not require a pre-specified vocabulary size.
arxiv-16800-241 | Fast Training of Triplet-based Deep Binary Embedding Networks | http://arxiv.org/abs/1603.02844 | author:Bohan Zhuang, Guosheng Lin, Chunhua Shen, Ian Reid category:cs.CV published:2016-03-09 summary:In this paper, we aim to learn a mapping (or embedding) from images to acompact binary space in which Hamming distances correspond to a ranking measurefor the image retrieval task. We make use of a triplet loss because this has been shown to be mosteffective for ranking problems. However, training in previous works can be prohibitively expensive due to thefact that optimization is directly performed on the triplet space, where thenumber of possible triplets for training is cubic in the number of trainingexamples. To address this issue, we propose to formulate high-order binary codeslearning as a multi-label classification problem by explicitly separatinglearning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codesinference algorithm to reduce the high-order objective to a standard binaryquadratic problem such that graph cuts can be used to efficiently infer thebinary code which serve as the label of each training datum. In the second stage we propose to map the original image to compact binarycodes via carefully designed deep convolutional neural networks (CNNs) and thehashing function fitting can be solved by training binary CNN classifiers. An incremental/interleaved optimization strategy is proffered to ensure thatthese two steps are interactive with each other during training for betteraccuracy. We conduct experiments on several benchmark datasets, which demonstrate bothimproved training time (by as much as two orders of magnitude) as well asproducing state-of-the-art hashing for various retrieval tasks.
arxiv-16800-242 | Optimized Kernel Entropy Components | http://arxiv.org/abs/1603.02806 | author:Emma Izquierdo-Verdiguier, Valero Laparra, Robert Jenssen, Luis Gómez-Chova, Gustau Camps-Valls category:stat.ML cs.LG published:2016-03-09 summary:This work addresses two main issues of the standard Kernel Entropy ComponentAnalysis (KECA) algorithm: the optimization of the kernel decomposition and theoptimization of the Gaussian kernel parameter. KECA roughly reduces to asorting of the importance of kernel eigenvectors by entropy instead of byvariance as in Kernel Principal Components Analysis. In this work, we proposean extension of the KECA method, named Optimized KECA (OKECA), that directlyextracts the optimal features retaining most of the data entropy by means ofcompacting the information in very few features (often in just one or two). Theproposed method produces features which have higher expressive power. Inparticular, it is based on the Independent Component Analysis (ICA) framework,and introduces an extra rotation to the eigen-decomposition, which is optimizedvia gradient ascent search. This maximum entropy preservation suggests thatOKECA features are more efficient than KECA features for density estimation. Inaddition, a critical issue in both methods is the selection of the kernelparameter since it critically affects the resulting performance. Here weanalyze the most common kernel length-scale selection criteria. Results of bothmethods are illustrated in different synthetic and real problems. Results showthat 1) OKECA returns projections with more expressive power than KECA, 2) themost successful rule for estimating the kernel parameter is based on maximumlikelihood, and 3) OKECA is more robust to the selection of the length-scaleparameter in kernel density estimation.
arxiv-16800-243 | Starting Small -- Learning with Adaptive Sample Sizes | http://arxiv.org/abs/1603.02839 | author:Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann category:cs.LG published:2016-03-09 summary:For many machine learning problems, data is abundant and it may beprohibitive to make multiple passes through the full training set. In thiscontext, we investigate strategies for dynamically increasing the effectivesample size, when using iterative methods such as stochastic gradient descent.Our interest is motivated by the rise of variance-reduced methods, whichachieve linear convergence rates that scale favorably for smaller sample sizes.Exploiting this feature, we show -- theoretically and empirically -- how toobtain significant speed-ups with a novel algorithm that reaches statisticalaccuracy on an $n$-sample in $2n$, instead of $n \log n$ steps.
arxiv-16800-244 | Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training | http://arxiv.org/abs/1603.02836 | author:Anirban Santara, Debapriya Maji, DP Tejas, Pabitra Mitra, Arobinda Gupta category:cs.LG published:2016-03-09 summary:Deep neural networks are capable of modelling highly non-linear functions bycapturing different levels of abstraction of data hierarchically. Whiletraining deep networks, first the system is initialized near a good optimum bygreedy layer-wise unsupervised pre-training. However, with burgeoning data andincreasing dimensions of the architecture, the time complexity of this approachbecomes enormous. Also, greedy pre-training of the layers often turnsdetrimental by over-training a layer causing it to lose harmony with the restof the network. In this paper a synchronized parallel algorithm forpre-training deep networks on multi-core machines has been proposed. Differentlayers are trained by parallel threads running on different cores with regularsynchronization. Thus the pre-training process becomes faster and chances ofover-training are reduced. This is experimentally validated using a stackedautoencoder for dimensionality reduction of MNIST handwritten digit database.The proposed algorithm achieved 26\% speed-up compared to greedy layer-wisepre-training for achieving the same reconstruction accuracy substantiating itspotential as an alternative.
arxiv-16800-245 | XGBoost: A Scalable Tree Boosting System | http://arxiv.org/abs/1603.02754 | author:Tianqi Chen, Carlos Guestrin category:cs.LG published:2016-03-09 summary:Tree boosting is a highly effective and widely used machine learning method.In this paper, we describe a scalable end-to-end tree boosting system calledXGBoost, which is used widely by data scientists to achieve state-of-the-artresults on many machine learning challenges. We propose a novel sparsity-awarealgorithm for sparse data and weighted quantile sketch for approximate treelearning. More importantly, we provide insights on cache access patterns, datacompression and sharding to build a scalable tree boosting system. By combiningthese insights, XGBoost scales beyond billions of examples using far fewerresources than existing systems.
arxiv-16800-246 | Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge | http://arxiv.org/abs/1603.02814 | author:Qi Wu, Chunhua Shen, Anton van den Hengel, Peng Wang, Anthony Dick category:cs.CV published:2016-03-09 summary:Much recent progress in Vision-to-Language problems has been achieved througha combination of Convolutional Neural Networks (CNNs) and Recurrent NeuralNetworks (RNNs). This approach does not explicitly represent high-levelsemantic concepts, but rather seeks to progress directly from image features totext. In this paper we first propose a method of incorporating high-levelconcepts into the successful CNN-RNN approach, and show that it achieves asignificant improvement on the state-of-the-art in both image captioning andvisual question answering. We further show that the same mechanism can be usedto incorporate external knowledge, which is critically important for answeringhigh level visual questions. Specifically, we design a visual questionanswering model that combines an internal representation of the content of animage with information extracted from a general knowledge base to answer abroad range of image-based questions. It particularly allows questions to beasked about the contents of an image, even when the image itself does notcontain a complete answer. Our final model achieves the best reported resultson both image captioning and visual question answering on several benchmarkdatasets.
arxiv-16800-247 | Computing AIC for black-box models using Generalised Degrees of Freedom: a comparison with cross-validation | http://arxiv.org/abs/1603.02743 | author:Severin Hauenstein, Carsten F. Dormann, Simon N Wood category:stat.ML published:2016-03-09 summary:Generalised Degrees of Freedom (GDF), as defined by Ye (1998 JASA93:120-131), represent the sensitivity of model fits to perturbations of thedata. As such they can be computed for any statistical model, making itpossible, in principle, to derive the number of parameters in machine-learningapproaches. Defined originally for normally distributed data only, we hereinvestigate the potential of this approach for Bernoulli-data. GDF-values formodels of simulated and real data are compared to model complexity-estimatesfrom cross-validation. Similarly, we computed GDF-based AICc for randomForest,neural networks and boosted regression trees and demonstrated its similarity tocross-validation. GDF-estimates for binary data were unstable andinconsistently sensitive to the number of data points perturbed simultaneously,while at the same time being extremely computer-intensive in their calculation.Repeated 10-fold cross-validation was more robust, based on fewer assumptionsand faster to compute. Our findings suggest that the GDF-approach does notreadily transfer to Bernoulli data and a wider range of regression approaches.
arxiv-16800-248 | A non-extensive entropy feature and its application to texture classification | http://arxiv.org/abs/1603.02466 | author:Seba Susan, Madasu Hanmandlu category:cs.CV published:2016-03-08 summary:This paper proposes a new probabilistic non-extensive entropy feature fortexture characterization, based on a Gaussian information measure. Thehighlights of the new entropy are that it is bounded by finite limits and thatit is non additive in nature. The non additive property of the proposed entropymakes it useful for the representation of information content in thenon-extensive systems containing some degree of regularity or correlation. Theeffectiveness of the proposed entropy in representing the correlated randomvariables is demonstrated by applying it for the texture classification problemsince textures found in nature are random and at the same time contain somedegree of correlation or regularity at some scale. The gray level co-occurrenceprobabilities (GLCP) are used for computing the entropy function. Theexperimental results indicate high degree of the classification accuracy. Theperformance of the new entropy function is found superior to other forms ofentropy such as Shannon, Renyi, Tsallis and Pal and Pal entropies oncomparison. Using the feature based polar interaction maps (FBIM) the proposedentropy is shown to be the best measure among the entropies compared forrepresenting the correlated textures.
arxiv-16800-249 | A hybrid approach based segmentation technique for brain tumor in MRI Images | http://arxiv.org/abs/1603.02447 | author:D. Anithadevi, K. Perumal category:cs.CV published:2016-03-08 summary:Automatic image segmentation becomes very crucial for tumor detection inmedical image processing.In general, manual and semi automatic segmentationtechniques require more time and knowledge. However these drawbacks hadovercome by automatic segmentation still there needs to develop moreappropriate techniques for medical image segmentation. Therefore, we proposedhybrid approach based image segmentation using the combined features of regiongrowing and threshold based segmentation techniques. It is followed bypre-processing stage to provide an accurate brain tumor extraction by the helpof Magnetic Resonance Imaging (MRI). If the tumor has holes, the region growingsegmentation algorithm cannot reveal but the proposed hybrid segmentationtechnique can be achieved and the result as well improved. Hence the resultused to made assessment with the various performance measures as DICE, JACCARDsimilarity, accuracy, sensitivity and specificity. These similarity measureshave been extensively used for evaluation with the ground truth of eachprocessed image and its results are compared and analyzed.
arxiv-16800-250 | A Bayesian non-parametric method for clustering high-dimensional binary data | http://arxiv.org/abs/1603.02494 | author:Tapesh Santra category:stat.AP cs.LG stat.ML published:2016-03-08 summary:In many real life problems, objects are described by large number of binaryfeatures. For instance, documents are characterized by presence or absence ofcertain keywords; cancer patients are characterized by presence or absence ofcertain mutations etc. In such cases, grouping together similarobjects/profiles based on such high dimensional binary features is desirable,but challenging. Here, I present a Bayesian non parametric algorithm forclustering high dimensional binary data. It uses a Dirichlet Process (DP)mixture model and simulated annealing to not only cluster binary data, but alsofind optimal number of clusters in the data. The performance of the algorithmwas evaluated and compared with other algorithms using simulated datasets. Itoutperformed all other clustering methods that were tested in the simulationstudies. It was also used to cluster real datasets arising from documentanalysis, handwritten image analysis and cancer research. It successfullydivided a set of documents based on their topics, hand written images based ondifferent styles of writing digits and identified tissue and mutationspecificity of chemotherapy treatments.
arxiv-16800-251 | Mixture Proportion Estimation via Kernel Embedding of Distributions | http://arxiv.org/abs/1603.02501 | author:Harish G. Ramaswamy, Clayton Scott, Ambuj Tewari category:cs.LG stat.ML published:2016-03-08 summary:Mixture proportion estimation (MPE) is the problem of estimating the weightof a component distribution in a mixture, given samples from the mixture andcomponent. This problem constitutes a key part in many "weakly supervisedlearning" problems like learning with positive and unlabelled samples, learningwith label noise, anomaly detection and crowdsourcing. While there have beenseveral methods proposed to solve this problem, to the best of our knowledge noefficient algorithm with a proven convergence rate towards the true proportionexists for this problem. We fill this gap by constructing a provably correctalgorithm for MPE, and derive convergence rates under certain assumptions onthe distribution. Our method is based on embedding distributions onto an RKHS,and implementing it only requires solving a simple convex quadratic programmingproblem a few times. We run our algorithm on several standard classificationdatasets, and demonstrate that it performs comparably to or better than otheralgorithms on most datasets.
arxiv-16800-252 | Effective Mean-Field Inference Method for Nonnegative Boltzmann Machines | http://arxiv.org/abs/1603.02434 | author:Muneki Yasuda category:stat.ML published:2016-03-08 summary:Nonnegative Boltzmann machines (NNBMs) are recurrent probabilistic neuralnetwork models that can describe multi-modal nonnegative data. NNBMs formrectified Gaussian distributions that appear in biological neural networkmodels, positive matrix factorization, nonnegative matrix factorization, and soon. In this paper, an effective inference method for NNBMs is proposed thatuses the mean-field method, referred to as the Thouless--Anderson--Palmerequation, and the diagonal consistency method, which was recently proposed.
arxiv-16800-253 | Semi-supervised Variational Autoencoders for Sequence Classification | http://arxiv.org/abs/1603.02514 | author:Weidi Xu, Haoze Sun category:cs.CL cs.LG published:2016-03-08 summary:Semi-supervised learning becomes one of the most significant problemsnowadays since the size of datasets is increasing tremendously while labeleddata is limited. We propose a new semi-supervised learning method for sequenceclassification tasks. Our work is based on both deep generative model forsemi-supervised learning \cite{kingma2014semi} and variational auto-encoder forsequence modeling \cite{bowman2015generating}. We found the introduction ofSc-LSTM is critical to the success in our method. We have obtained somepreliminary experimental results on IMDB sentiment classification dataset,showing that the proposed model improves the classification accuracy comparingto pure supervised classifier.
arxiv-16800-254 | A New Method to Visualize Deep Neural Networks | http://arxiv.org/abs/1603.02518 | author:Luisa M. Zintgraf, Taco S. Cohen, Max Welling category:cs.CV published:2016-03-08 summary:We present a method for visualising the response of a deep neural network toa specific input. For image data for instance our method will highlight areasthat provide evidence in favor of, and against choosing a certain class. Themethod overcomes several shortcomings of previous methods and provides greatadditional insight into the decision making process of convolutional networks,which is important both to improve models and to accelerate the adoption ofsuch methods in e.g. medicine. In experiments on ImageNet data, we illustratehow the method works and can be applied in different ways to understand deepneural nets.
arxiv-16800-255 | Stochastic dual averaging methods using variance reduction techniques for regularized empirical risk minimization problems | http://arxiv.org/abs/1603.02412 | author:Tomoya Murata, Taiji Suzuki category:math.OC cs.LG stat.ML published:2016-03-08 summary:We consider a composite convex minimization problem associated withregularized empirical risk minimization, which often arises in machinelearning. We propose two new stochastic gradient methods that are based onstochastic dual averaging method with variance reduction. Our methods generatea sparser solution than the existing methods because we do not need to take theaverage of the history of the solutions. This is favorable in terms of bothinterpretability and generalization. Moreover, our methods have theoreticalsupport for both a strongly and a non-strongly convex regularizer and achievethe best known convergence rates among existing nonaccelerated stochasticgradient methods.
arxiv-16800-256 | On the inconsistency of $\ell_1$-penalised sparse precision matrix estimation | http://arxiv.org/abs/1603.02532 | author:Otte Heinävaara, Janne Leppä-aho, Jukka Corander, Antti Honkela category:cs.LG stat.CO stat.ML published:2016-03-08 summary:Various $\ell_1$-penalised estimation methods such as graphical lasso andCLIME are widely used for sparse precision matrix estimation. Many of thesemethods have been shown to be consistent under various quantitative assumptionsabout the underlying true covariance matrix. Intuitively, these conditions arerelated to situations where the penalty term will dominate the optimisation. Inthis paper, we explore the consistency of $\ell_1$-based methods for a class ofsparse latent variable -like models, which are strongly motivated by severaltypes of applications. We show that all $\ell_1$-based methods faildramatically for models with nearly linear dependencies between the variables.We also study the consistency on models derived from real gene expression dataand note that the assumptions needed for consistency never hold even for modestsized gene networks and $\ell_1$-based methods also become unreliable inpractice for larger networks.
arxiv-16800-257 | Batched Lazy Decision Trees | http://arxiv.org/abs/1603.02578 | author:Mathieu Guillame-Bert, Artur Dubrawski category:cs.LG published:2016-03-08 summary:We introduce a batched lazy algorithm for supervised classification usingdecision trees. It avoids unnecessary visits to irrelevant nodes when it isused to make predictions with either eagerly or lazily trained decision trees.A set of experiments demonstrate that the proposed algorithm can outperformboth the conventional and lazy decision tree algorithms in terms of computationtime as well as memory consumption, without compromising accuracy.
arxiv-16800-258 | Prediction of Infinite Words with Automata | http://arxiv.org/abs/1603.02597 | author:Tim Smith category:cs.FL cs.LG published:2016-03-08 summary:In the classic problem of sequence prediction, a predictor receives asequence of values from an emitter and tries to guess the next value before itappears. The predictor masters the emitter if there is a point after which allof the predictor's guesses are correct. In this paper we consider the case inwhich the predictor is an automaton and the emitted values are drawn from afinite set; i.e., the emitted sequence is an infinite word. We examine thepredictive capabilities of finite automata, pushdown automata, stack automata(a generalization of pushdown automata), and multihead finite automata. Werelate our predicting automata to purely periodic words, ultimately periodicwords, and multilinear words, describing novel prediction algorithms formastering these sequences.
arxiv-16800-259 | Observing Trends in Automated Multilingual Media Analysis | http://arxiv.org/abs/1603.02604 | author:Ralf Steinberger, Aldo Podavini, Alexandra Balahur, Guillaume Jacquet, Hristo Tanev, Jens Linge, Martin Atkinson, Michele Chinosi, Vanni Zavarella, Yaniv Steiner, Erik van der Goot category:cs.CL published:2016-03-08 summary:Any large organisation, be it public or private, monitors the media forinformation to keep abreast of developments in their field of interest, andusually also to become aware of positive or negative opinions expressed towardsthem. At least for the written media, computer programs have become veryefficient at helping the human analysts significantly in their monitoring taskby gathering media reports, analysing them, detecting trends and - in somecases - even to issue early warnings or to make predictions of likely futuredevelopments. We present here trend recognition-related functionality of theEurope Media Monitor (EMM) system, which was developed by the EuropeanCommission's Joint Research Centre (JRC) for public administrations in theEuropean Union (EU) and beyond. EMM performs large-scale media analysis in upto seventy languages and recognises various types of trends, some of themcombining information from news articles written in different languages andfrom social media posts. EMM also lets users explore the huge amount ofmultilingual media data through interactive maps and graphs, allowing them toexamine the data from various view points and according to multiple criteria. Alot of EMM's functionality is accessibly freely over the internet or via appsfor hand-held devices.
arxiv-16800-260 | Iterative Hough Forest with Histogram of Control Points for 6 DoF Object Registration from Depth Images | http://arxiv.org/abs/1603.02617 | author:Caner Sahin, Rigas Kouskouridas, Tae-Kyun Kim category:cs.CV cs.RO published:2016-03-08 summary:State-of-the-art techniques for 6D object pose recovery depend onocclusion-free point clouds to accurately register objects in the 3D space. Toreduce this dependency, we introduce a novel architecture called IterativeHough forest with Histogram of Control Points that is capable of estimatingoccluded and cluttered objects' 6D pose given a candidate 2D bounding box. Ouriterative Hough forest is learnt using patches extracted only from the positivesamples. These patches are represented with Histogram of Control Points (HoCP),a scale-variant implicit volumetric description, which we derive from recentlyintroduced Implicit B-Splines (IBS). The rich discriminative informationprovided by this scale-variance is leveraged during inference, where theinitial pose estimation of the object is iteratively refined based on morediscriminative control points by using our iterative Hough forest. We conductexperiments on several test objects of a publicly available dataset to test ourarchitecture and to compare with the state-of-the-art.
arxiv-16800-261 | Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling | http://arxiv.org/abs/1603.02644 | author:Christophe Dupuy, Francis Bach category:cs.LG stat.ML published:2016-03-08 summary:We study parameter inference in large-scale latent variable models. We firstpropose an unified treatment of online inference for latent variable modelsfrom a non-canonical exponential family, and draw explicit links betweenseveral previously proposed frequentist or Bayesian methods. We then propose anovel inference method for the frequentist estimation of parameters, thatadapts MCMC methods to online inference of latent variable models with theproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,weprovide an extensive set of experiments and comparisons with existing work,where our new approach outperforms all previously proposed methods. Inparticular, using Gibbs sampling for latent variable inference is superior tovariational inference in terms of test log-likelihoods. Moreover, Bayesianinference through variational methods perform poorly, sometimes leading toworse fits with latent variables of higher dimensionality.
arxiv-16800-262 | Frequency estimation in three-phase power systems with harmonic contamination: A multistage quaternion Kalman filtering approach | http://arxiv.org/abs/1603.02977 | author:Sayed Pouria Talebi, Danilo P. Mandic category:stat.ML stat.AP published:2016-03-08 summary:Motivated by the need for accurate frequency information, a novel algorithmfor estimating the fundamental frequency and its rate of change in three-phasepower systems is developed. This is achieved through two stages of Kalmanfiltering. In the first stage a quaternion extended Kalman filter, whichprovides a unified framework for joint modeling of voltage measurements fromall the phases, is used to estimate the instantaneous phase increment of thethree-phase voltages. The phase increment estimates are then used asobservations of the extended Kalman filter in the second stage that accountsfor the dynamic behavior of the system frequency and simultaneously estimatesthe fundamental frequency and its rate of change. The framework is thenextended to account for the presence of harmonics. Finally, the concept isvalidated through simulation on both synthetic and real-world data.
arxiv-16800-263 | Note on the equivalence of hierarchical variational models and auxiliary deep generative models | http://arxiv.org/abs/1603.02443 | author:Niko Brümmer category:stat.ML published:2016-03-08 summary:This note compares two recently published machine learning methods forconstructing flexible, but tractable families of variational hidden-variableposteriors. The first method, called "hierarchical variational models" enrichesthe inference model with an extra variable, while the other, called "auxiliarydeep generative models", enriches the generative model instead. We concludethat the two methods are mathematically equivalent.
arxiv-16800-264 | The red one!: On learning to refer to things based on their discriminative properties | http://arxiv.org/abs/1603.02618 | author:Angeliki Lazaridou, Nghia The Pham, Marco Baroni category:cs.CL cs.CV published:2016-03-08 summary:As a first step towards agents learning to communicate about their visualenvironment, we propose a system that, given visual representations of areferent (cat) and a context (sofa), identifies their discriminativeattributes, i.e., properties that distinguish them (has_tail). Moreover,despite the lack of direct supervision at the attribute level, the model learnsto assign plausible attributes to objects (sofa-has_cushion). Finally, wepresent a preliminary experiment confirming the referential success of thepredicted discriminative attributes.
arxiv-16800-265 | Hand Segmentation for Hand-Object Interaction from Depth map | http://arxiv.org/abs/1603.02345 | author:Byeongkeun Kang, Kar-Han Tan, Hung-Shuo Tai, Daniel Tretter, Truong Q. Nguyen category:cs.CV published:2016-03-08 summary:Hand-object interaction is important for many applications such as augmentedreality, medical application, and human-robot interaction. Hand segmentation isa necessary pre-process to estimate hand pose and to recognize hand gesture orobject in interaction. However, current hand segmentation method forhand-object interaction is based on color information which is not robust toobjects with skin color, skin pigment difference, and light conditionvariations. Therefore, we propose the first hand segmentation method forhand-object interaction using depth map. This is challenging because of onlysmall depth difference between hand and object during interaction. The proposedmethod includes two-stage randomized decision forest (RDF) with validationprocess, bilateral filtering, decision adjustment, and post-processing. Wedemonstrate the effectiveness of the proposed method by testing for fiveobjects. The proposed method achieves the average F1 score of 0.8826 usingdifferent model for each object and 0.8645 using a global model for entireobjects. Also, the method takes only about 10ms to process each frame. Webelieve that this is the state-of-the-art hand segmentation algorithm usingdepth map for hand-object interaction.
arxiv-16800-266 | DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data | http://arxiv.org/abs/1603.02636 | author:Lucas Beyer, Alexander Hermans, Bastian Leibe category:cs.RO cs.CV cs.LG cs.NE published:2016-03-08 summary:We introduce the DROW detector, a deep learning based detector for 2D rangedata. Laser scanners are lighting invariant, provide accurate range data, andtypically cover a large field of view, making them interesting sensors forrobotics applications. So far, research on detection in laser range data hasbeen dominated by handcrafted features and boosted classifiers, potentiallylosing performance due to suboptimal design choices. We propose a ConvolutionalNeural Network (CNN) based detector for this task. We show how to effectivelyapply CNNs for detection in 2D range data, and propose a depth preprocessingstep and voting scheme that significantly improve CNN performance. Wedemonstrate our approach on wheelchairs and walkers, obtaining state of the artdetection results. Apart from the training data, none of our design choiceslimits the detector to these two classes, though. We provide a ROS node for ourdetector and release our dataset containing 464k laser scans, out of which 24kwere annotated for training.
arxiv-16800-267 | Small ensembles of kriging models for optimization | http://arxiv.org/abs/1603.02638 | author:Hossein Mohammadi, Rodolphe Le Riche, Eric Touboul category:math.OC cs.LG stat.ML published:2016-03-08 summary:The Efficient Global Optimization (EGO) algorithm uses a conditionalGaus-sian Process (GP) to approximate an objective function known at a finitenumber of observation points and sequentially adds new points which maximizethe Expected Improvement criterion according to the GP. The important factorthat controls the efficiency of EGO is the GP covariance function (or kernel)which should be chosen according to the objective function. Traditionally, apa-rameterized family of covariance functions is considered whose parametersare learned through statistical procedures such as maximum likelihood orcross-validation. However, it may be questioned whether statistical proceduresfor learning covariance functions are the most efficient for optimization asthey target a global agreement between the GP and the observations which is notthe ultimate goal of optimization. Furthermore, statistical learning proceduresare computationally expensive. The main alternative to the statistical learningof the GP is self-adaptation, where the algorithm tunes the kernel parametersbased on their contribution to objective function improvement. Afterquestioning the possibility of self-adaptation for kriging based optimizers,this paper proposes a novel approach for tuning the length-scale of the GP inEGO: At each iteration, a small ensemble of kriging models structured by theirlength-scales is created. All of the models contribute to an iterate in anEGO-like fashion. Then, the set of models is densified around the model whoselength-scale yielded the best iterate and further points are produced.Numerical experiments are provided which motivate the use of manylength-scales. The tested implementation does not perform better than theclassical EGO algorithm in a sequential context but show the potential of theapproach for parallel implementations.
arxiv-16800-268 | A regularization-based approach for unsupervised image segmentation | http://arxiv.org/abs/1603.02649 | author:Aleksandar Dimitriev, Matej Kristan category:cs.CV published:2016-03-08 summary:We propose a novel unsupervised image segmentation algorithm, which aims tosegment an image into several coherent parts. It requires no user input, nosupervised learning phase and assumes an unknown number of segments. Itachieves this by first over-segmenting the image into several hundredsuperpixels. These are iteratively joined on the basis of a discriminativeclassifier trained on color and texture information obtained from eachsuperpixel. The output of the classifier is regularized by a Markov randomfield that lends more influence to neighbouring superpixels that are moresimilar. In each iteration, similar superpixels fall under the same label,until only a few coherent regions remain in the image. The algorithm was testedon a standard evaluation data set, where it performs on par withstate-of-the-art algorithms in term of precision and greatly outperforms thestate of the art by reducing the oversegmentation of the object of interest.
arxiv-16800-269 | Extracting Arabic Relations from the Web | http://arxiv.org/abs/1603.02488 | author:Shimaa M. Abd El-salam, Enas M. F. El Houby, A. K. Al Sammak, T. A. El-Shishtawy category:cs.CL published:2016-03-08 summary:The goal of this research is to extract a large list or table from namedentities and relations in a specific domain. A small set of a handful ofinstance relations is required as input from the user. The system exploitssummaries from Google search engine as a source text. These instances are usedto extract patterns. The output is a set of new entities and their relations.The results from four experiments show that precision and recall variesaccording to relation type. Precision ranges from 0.61 to 0.75 while recallranges from 0.71 to 0.83. The best result is obtained for (player, club)relationship, 0.72 and 0.83 for precision and recall respectively.
arxiv-16800-270 | Pairwise Choice Markov Chains | http://arxiv.org/abs/1603.02740 | author:Stephen Ragain, Johan Ugander category:stat.ML cs.AI published:2016-03-08 summary:As datasets capturing human choices grow in richness and scale, particularlyin online domains, there is an increasing need for choice models that explainand predict complex empirical choices that violate traditional choice-theoreticassumptions such as regularity, stochastic transitivity, or Luce's choiceaxiom. In this work we introduce a Pairwise Choice Markov Chain (PCMC) model ofdiscrete choice that is free of all those assumptions while still satisfyingthe attractive foundational axiom of uniform expansion. Uniform expansion isknown to imply Luce's choice axiom in the context of independent random utilitymodels (RUMs), but the PCMC model is not a RUM (let alone an independent RUM).Inference for the PCMC model is straight-forward, and we thus introduce it asthe first inferentially tractable model of discrete choice known to satisfyuniform expansion without the choice axiom, regularity, or strict stochastictransitivity. It is thus more flexible than even Tversky's Elimination ByAspects model, which assumes regularity and is also known to be inferentiallyintractable. We show that our model learns and predicts syntheticnon-transitive data well. Our analysis also synthesizes several recentobservations connecting the Multinomial Logit (MNL) model and Markov chains;the PCMC model retains the Multinomial Logit model as a special case.
arxiv-16800-271 | Discriminative models for robust image classification | http://arxiv.org/abs/1603.02736 | author:Umamahesh Srinivas category:stat.ML cs.CV published:2016-03-08 summary:A variety of real-world tasks involve the classification of images intopre-determined categories. Designing image classification algorithms thatexhibit robustness to acquisition noise and image distortions, particularlywhen the available training data are insufficient to learn accurate models, isa significant challenge. This dissertation explores the development ofdiscriminative models for robust image classification that exploit underlyingsignal structure, via probabilistic graphical models and sparse signalrepresentations. Probabilistic graphical models are widely used in many applications toapproximate high-dimensional data in a reduced complexity set-up. Learninggraphical structures to approximate probability distributions is an area ofactive research. Recent work has focused on learning graphs in a discriminativemanner with the goal of minimizing classification error. In the first part ofthe dissertation, we develop a discriminative learning framework that exploitsthe complementary yet correlated information offered by multiplerepresentations (or projections) of a given signal/image. Specifically, wepropose a discriminative tree-based scheme for feature fusion by explicitlylearning the conditional correlations among such multiple projections in aniterative manner. Experiments reveal the robustness of the resulting graphicalmodel classifier to training insufficiency.
arxiv-16800-272 | Revisiting Active Perception | http://arxiv.org/abs/1603.02729 | author:Ruzena Bajcsy, Yiannis Aloimonos, John K. Tsotsos category:cs.CV cs.RO published:2016-03-08 summary:Despite the recent successes in robotics, artificial intelligence andcomputer vision, a complete artificial agent necessarily must include activeperception. A multitude of ideas and methods for how to accomplish this havealready appeared in the past, their broader utility perhaps impeded byinsufficient computational power or costly hardware. The history of theseideas, perhaps selective due to our perspectives, is presented with the goal oforganizing the past literature and highlighting the seminal contributions. Weargue that those contributions are as relevant today as they were decades agoand, with the state of modern computational tools, are poised to find new lifein the robotic perception systems of the next decade.
arxiv-16800-273 | A matter of words: NLP for quality evaluation of Wikipedia medical articles | http://arxiv.org/abs/1603.01987 | author:Vittoria Cozza, Marinella Petrocchi, Angelo Spognardi category:cs.IR cs.CL published:2016-03-07 summary:Automatic quality evaluation of Web information is a task with many fields ofapplications and of great relevance, especially in critical domains like themedical one. We move from the intuition that the quality of content of medicalWeb documents is affected by features related with the specific domain. First,the usage of a specific vocabulary (Domain Informativeness); then, the adoptionof specific codes (like those used in the infoboxes of Wikipedia articles) andthe type of document (e.g., historical and technical ones). In this paper, wepropose to leverage specific domain features to improve the results of theevaluation of Wikipedia medical articles. In particular, we evaluate thearticles adopting an "actionable" model, whose features are related to thecontent of the articles, so that the model can also directly suggest strategiesfor improving a given article quality. We rely on Natural Language Processing(NLP) and dictionaries-based techniques in order to extract the bio-medicalconcepts in a text. We prove the effectiveness of our approach by classifyingthe medical articles of the Wikipedia Medicine Portal, which have beenpreviously manually labeled by the Wiki Project team. The results of ourexperiments confirm that, by considering domain-oriented features, it ispossible to obtain sensible improvements with respect to existing solutions,mainly for those articles that other approaches have less correctly classified.Other than being interesting by their own, the results call for furtherresearch in the area of domain specific features suitable for Web data qualityassessment.
arxiv-16800-274 | From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators | http://arxiv.org/abs/1603.02003 | author:Paul Upchurch, Noah Snavely, Kavita Bala category:cs.CV published:2016-03-07 summary:We propose a new neural network architecture for solving single-imageanalogies - the generation of an entire set of stylistically similar imagesfrom just a single input image. Solving this problem requires separating imagestyle from content. Our network is a modified variational autoencoder (VAE)that supports supervised training of single-image analogies and in-networkevaluation of outputs with a structured similarity objective that capturespixel covariances. On the challenging task of generating a 62-letter font froma single example letter we produce images with 22.4% lower dissimilarity to theground truth than state-of-the-art.
arxiv-16800-275 | Position paper: Towards an observer-oriented theory of shape comparison | http://arxiv.org/abs/1603.02008 | author:Patrizio Frosini category:cs.CG cs.CV math.AT I.4.7; I.5.1 published:2016-03-07 summary:In this position paper we suggest a possible metric approach to shapecomparison that is based on a mathematical formalization of the concept ofobserver, seen as a collection of suitable operators acting on a metric spaceof functions. These functions represent the set of data that are accessible tothe observer, while the operators describe the way the observer elaborates thedata and enclose the invariance that he/she associates with them. We exposethis model and illustrate some theoretical reasons that justify its possibleuse for shape comparison.
arxiv-16800-276 | On higher order computations and synaptic meta-plasticity in the human brain: IT point of view (March, 2016) | http://arxiv.org/abs/1603.02238 | author:Stanislaw Ambroszkiewicz category:cs.NE q-bio.NC 92B20 F.1.1 published:2016-03-07 summary:Glia modify neuronal connectivity by creating structural changes in theneuronal connectome. Glia also influence the functional connectome by modifyingthe flow of information through neural networks (Fields et al. 2015). There arestrong experimental evidences that glia are responsible for synapticmeta-plasticity. Synaptic plasticity is the modification of the strength ofconnections between neurons. Meta-plasticity, i.e. plasticity of synapticplasticity, may be viewed as mechanisms for dynamic reconfiguration of neuroncircuits. First order computations in the brain are done by static neuroncircuits, whereas higher order computations are done by dynamicreconfigurations of the links (synapses) between the neuron circuits. Staticneuron circuits correspond to first order computable functions. Synapsecreation correspond to the mathematical notion of function composition.Functionals are higher order functions that take functions as their arguments.The construction of functionals is based on dynamic reconfigurations of thefunction composition. Perhaps the functionals correspond to the meta-plasticityin the human brain.
arxiv-16800-277 | Differentially Private Policy Evaluation | http://arxiv.org/abs/1603.02010 | author:Borja Balle, Maziar Gomrokchi, Doina Precup category:cs.LG stat.ML published:2016-03-07 summary:We present the first differentially private algorithms for reinforcementlearning, which apply to the task of evaluating a fixed policy. We establishtwo approaches for achieving differential privacy, provide a theoreticalanalysis of the privacy and utility of the two algorithms, and show promisingresults on simple empirical examples.
arxiv-16800-278 | Adaptive Visualisation System for Construction Building Information Models Using Saliency | http://arxiv.org/abs/1603.02028 | author:Hugo Martin, Sylvain Chevallier, Eric Monacelli category:cs.CV cs.AI published:2016-03-07 summary:Building Information Modeling (BIM) is a recent construction process based ona 3D model, containing every component related to the building achievement.Architects, structure engineers, method engineers, and others participant tothe building process work on this model through the design-to-constructioncycle. The high complexity and the large amount of information included inthese models raise several issues, delaying its wide adoption in the industrialworld. One of the most important is the visualization: professionals havedifficulties to find out the relevant information for their job. Actualsolutions suffer from two limitations: the BIM models information are processedmanually and insignificant information are simply hidden, leading toinconsistencies in the building model. This paper describes a system relying onan ontological representation of the building information to labelautomatically the building elements. Depending on the user's department, thevisualization is modified according to these labels by automatically adjustingthe colors and image properties based on a saliency model. The proposedsaliency model incorporates several adaptations to fit the specificities ofarchitectural images.
arxiv-16800-279 | Deep Contrast Learning for Salient Object Detection | http://arxiv.org/abs/1603.01976 | author:Guanbin Li, Yizhou Yu category:cs.CV published:2016-03-07 summary:Salient object detection has recently witnessed substantial progress due topowerful features extracted using deep convolutional neural networks (CNNs).However, existing CNN-based methods operate at the patch level instead of thepixel level. Resulting saliency maps are typically blurry, especially near theboundary of salient objects. Furthermore, image patches are treated asindependent samples even when they are overlapping, giving rise to significantredundancy in computation and storage. In this CVPR 2016 paper, we propose anend-to-end deep contrast network to overcome the aforementioned limitations.Our deep network consists of two complementary components, a pixel-level fullyconvolutional stream and a segment-wise spatial pooling stream. The firststream directly produces a saliency map with pixel-level accuracy from an inputimage. The second stream extracts segment-wise features very efficiently, andbetter models saliency discontinuities along object boundaries. Finally, afully connected CRF model can be optionally incorporated to improve spatialcoherence and contour localization in the fused result from these two streams.Experimental results demonstrate that our deep model significantly improves thestate of the art.
arxiv-16800-280 | Unscented Bayesian Optimization for Safe Robot Grasping | http://arxiv.org/abs/1603.02038 | author:José Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino, Lorenzo Jamone category:cs.RO cs.AI cs.LG cs.SY published:2016-03-07 summary:We address the robot grasp optimization problem of unknown objectsconsidering uncertainty in the input space. Grasping unknown objects can beachieved by using a trial and error exploration strategy. Bayesian optimizationis a sample efficient optimization algorithm that is especially suitable forthis setups as it actively reduces the number of trials for learning about thefunction to optimize. In fact, this active object exploration is the samestrategy that infants do to learn optimal grasps. One problem that arises whilelearning grasping policies is that some configurations of grasp parameters maybe very sensitive to error in the relative pose between the object and robotend-effector. We call these configurations unsafe because small errors duringgrasp execution may turn good grasps into bad grasps. Therefore, to reduce therisk of grasp failure, grasps should be planned in safe areas. We propose a newalgorithm, Unscented Bayesian optimization that is able to perform sampleefficient optimization while taking into consideration input noise to find safeoptima. The contribution of Unscented Bayesian optimization is twofold as ifprovides a new decision process that drives exploration to safe regions and anew selection procedure that chooses the optimal in terms of its safety withoutextra analysis or computational cost. Both contributions are rooted on thestrong theory behind the unscented transformation, a popular nonlinearapproximation method. We show its advantages with respect to the classicalBayesian optimization both in synthetic problems and in realistic robot graspsimulations. The results highlights that our method achieves optimal and robustgrasping policies after few trials while the selected grasps remain in saferegions.
arxiv-16800-281 | Learning Shared Representations in Multi-task Reinforcement Learning | http://arxiv.org/abs/1603.02041 | author:Diana Borsa, Thore Graepel, John Shawe-Taylor category:cs.AI cs.LG published:2016-03-07 summary:We investigate a paradigm in multi-task reinforcement learning (MT-RL) inwhich an agent is placed in an environment and needs to learn to perform aseries of tasks, within this space. Since the environment does not change,there is potentially a lot of common ground amongst tasks and learning to solvethem individually seems extremely wasteful. In this paper, we explicitly modeland learn this shared structure as it arises in the state-action value space.We will show how one can jointly learn optimal value-functions by modifying thepopular Value-Iteration and Policy-Iteration procedures to accommodate thisshared representation assumption and leverage the power of multi-tasksupervised learning. Finally, we demonstrate that the proposed model andtraining procedures, are able to infer good value functions, even under lowsamples regimes. In addition to data efficiency, we will show in our analysis,that learning abstractions of the state space jointly across tasks leads tomore robust, transferable representations with the potential for bettergeneralization. this shared representation assumption and leverage the power ofmulti-task supervised learning. Finally, we demonstrate that the proposed modeland training procedures, are able to infer good value functions, even under lowsamples regimes. In addition to data efficiency, we will show in our analysis,that learning abstractions of the state space jointly across tasks leads tomore robust, transferable representations with the potential for bettergeneralization.
arxiv-16800-282 | Optimal dictionary for least squares representation | http://arxiv.org/abs/1603.02074 | author:Mohammed Rayyan Sheriff, Debasish Chatterjee category:cs.LG math.OC stat.ML published:2016-03-07 summary:Dictionary Learning problems are concerned with finding a collection ofvectors usually referred to as the dictionary, such that the representation ofrandom vectors with a given distribution using this dictionary is optimal. Mostof the recent research in dictionary learning is focused on developingdictionaries which offer sparse representation, i.e., optimal representation inthe $\ell_0$ sense. We consider the problem of finding an optimal dictionarywith which representation of samples of a random vector on an average isoptimal. Optimality of representation is defined in the sense of attainingminimal average $\ell_2$-norm of the coefficient vector used to represent therandom vector. With the help of recent results related to rank-onedecompositions of real symmetric positive semi-definite matrices, an explicitsolution for an $\ell_2$-optimal dictionary is obtained.
arxiv-16800-283 | A Learning-based Frame Pooling Model For Event Detection | http://arxiv.org/abs/1603.02078 | author:Jiang Liu, Chenqiang Gao, Lan Wang, Deyu Meng category:cs.CV published:2016-03-07 summary:Detecting complex events in a large video collection crawled from videowebsites is a challenging task. When applying directly good image-based featurerepresentation, e.g., HOG, SIFT, to videos, we have to face the problem of howto pool multiple frame feature representations into one feature representation.In this paper, we propose a novel learning-based frame pooling method. Weformulate the pooling weight learning as an optimization problem and thus ourmethod can automatically learn the best pooling weight configuration for eachspecific event category. Experimental results conducted on TRECVID MED 2011reveal that our method outperforms the commonly used average pooling and maxpooling strategies on both high-level and low-level 2D image features.
arxiv-16800-284 | A Latent Variable Recurrent Neural Network for Discourse Relation Language Models | http://arxiv.org/abs/1603.01913 | author:Yangfeng Ji, Gholamreza Haffari, Jacob Eisenstein category:cs.CL cs.LG cs.NE stat.ML published:2016-03-07 summary:This paper presents a novel latent variable recurrent neural networkarchitecture for jointly modeling sequences of words and (possibly latent)discourse relations between adjacent sentences. A recurrent neural networkgenerates individual words, thus reaping the benefits ofdiscriminatively-trained vector representations. The discourse relations arerepresented with a latent variable, which can be predicted or marginalized,depending on the task. The resulting model can therefore employ a trainingobjective that includes not only discourse relation classification, but alsoword prediction. As a result, it outperforms state-of-the-art alternatives fortwo tasks: implicit discourse relation classification in the Penn DiscourseTreebank, and dialog act classification in the Switchboard corpus. Furthermore,by marginalizing over latent discourse relations at test time, we obtain adiscourse informed language model, which improves over a strong LSTM baseline.
arxiv-16800-285 | Partition Functions from Rao-Blackwellized Tempered Sampling | http://arxiv.org/abs/1603.01912 | author:David Carlson, Patrick Stinson, Ari Pakman, Liam Paninski category:stat.ML published:2016-03-07 summary:Partition functions of probability distributions are important quantities formodel evaluation and comparisons. We present a new method to compute partitionfunctions of complex and multimodal distributions. Such distributions are oftensampled using simulated tempering, which augments the target space with anauxiliary inverse temperature variable. Our method exploits the multinomialprobability law of the inverse temperatures, and provides estimates of thepartition function in terms of a simple quotient of Rao-Blackwellized marginalinverse temperature probability estimates, which are updated while sampling. Weshow that the method has interesting connections with several alternativepopular methods, and offers some significant advantages. In particular, weempirically find that the new method provides more accurate estimates thanAnnealed Importance Sampling when calculating partition functions of largeRestricted Boltzmann Machines (RBM); moreover, the method is sufficientlyaccurate to track training and validation log-likelihoods during learning ofRBMs, at minimal computational cost.
arxiv-16800-286 | Learning a Discriminative Null Space for Person Re-identification | http://arxiv.org/abs/1603.02139 | author:Li Zhang, Tao Xiang, Shaogang Gong category:cs.CV published:2016-03-07 summary:Most existing person re-identification (re-id) methods focus on learning theoptimal distance metrics across camera views. Typically a person's appearanceis represented using features of thousands of dimensions, whilst only hundredsof training samples are available due to the difficulties in collecting matchedtraining images. With the number of training samples much smaller than thefeature dimension, the existing methods thus face the classic small sample size(SSS) problem and have to resort to dimensionality reduction techniques and/ormatrix regularisation, which lead to loss of discriminative power. In thiswork, we propose to overcome the SSS problem in re-id distance metric learningby matching people in a discriminative null space of the training data. In thisnull space, images of the same person are collapsed into a single point thusminimising the within-class scatter to the extreme and maximising the relativebetween-class separation simultaneously. Importantly, it has a fixed dimension,a closed-form solution and is very efficient to compute. Extensive experimentscarried out on five person re-identification benchmarks including VIPeR,PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beatsthe state-of-the-art alternatives, often by a big margin.
arxiv-16800-287 | Bayesian Learning of Kernel Embeddings | http://arxiv.org/abs/1603.02160 | author:Seth Flaxman, Dino Sejdinovic, John P. Cunningham, Sarah Filippi category:stat.ML published:2016-03-07 summary:Kernel methods are one of the mainstays of machine learning, but the problemof kernel learning remains challenging, with only a few heuristics and verylittle theory. This is of particular importance in methods based on estimationof kernel mean embeddings of probability measures. For characteristic kernels,which include most commonly used ones, the kernel mean embedding uniquelydetermines its probability measure, so it can be used to design a powerfulstatistical testing framework, which includes nonparametric two-sample andindependence tests. In practice, however, the performance of these tests can bevery sensitive to the choice of kernel and its lengthscale parameters. Toaddress this central issue, we propose a new probabilistic model for kernelmean embeddings, the Bayesian Kernel Embedding model, combining a Gaussianprocess prior over the Reproducing Kernel Hilbert Space containing the meanembedding with a conjugate likelihood function, thus yielding a closed formposterior over the mean embedding. The posterior mean of our model is closelyrelated to recently proposed shrinkage estimators for kernel mean embeddings,while the posterior uncertainty is a new, interesting feature with variouspossible applications. Critically for the purposes of effective and principledkernel learning, our model gives a simple, closed form marginal likelihood ofthe observed data given the kernel hyperparameters. This marginal likelihoodcan either be optimized to inform the hyperparameter choice or fully Bayesianinference can be used.
arxiv-16800-288 | Distributed Multi-Task Learning with Shared Representation | http://arxiv.org/abs/1603.02185 | author:Jialei Wang, Mladen Kolar, Nathan Srebro category:cs.LG stat.ML published:2016-03-07 summary:We study the problem of distributed multi-task learning with sharedrepresentation, where each machine aims to learn a separate, but related, taskin an unknown shared low-dimensional subspaces, i.e. when the predictor matrixhas low rank. We consider a setting where each task is handled by a differentmachine, with samples for the task available locally on the machine, and studycommunication-efficient methods for exploiting the shared structure.
arxiv-16800-289 | A Two-Stage Shape Retrieval (TSR) Method with Global and Local Features | http://arxiv.org/abs/1603.01942 | author:Xiaqing Pan, Sachin Chachada, C. -C. Jay Kuo category:cs.CV published:2016-03-07 summary:A robust two-stage shape retrieval (TSR) method is proposed to address the 2Dshape retrieval problem. Most state-of-the-art shape retrieval methods arebased on local features matching and ranking. Their retrieval performance isnot robust since they may retrieve globally dissimilar shapes in high ranks. Toovercome this challenge, we decompose the decision process into two stages. Inthe first irrelevant cluster filtering (ICF) stage, we consider both global andlocal features and use them to predict the relevance of gallery shapes withrespect to the query. Irrelevant shapes are removed from the candidate shapeset. After that, a local-features-based matching and ranking (LMR) methodfollows in the second stage. We apply the proposed TSR system to MPEG-7,Kimia99 and Tari1000 three datasets and show that it outperforms all otherexisting methods. The robust retrieval performance of the TSR system isdemonstrated.
arxiv-16800-290 | Gaussian Process Regression for Out-of-Sample Extension | http://arxiv.org/abs/1603.02194 | author:Oren Barkan, Jonathan Weill, Amir Averbuch category:cs.LG cs.CV published:2016-03-07 summary:Manifold learning methods are useful for high dimensional data analysis. Manyof the existing methods produce a low dimensional representation that attemptsto describe the intrinsic geometric structure of the original data. Typically,this process is computationally expensive and the produced embedding is limitedto the training data. In many real life scenarios, the ability to produceembedding of unseen samples is essential. In this paper we propose a Bayesiannon-parametric approach for out-of-sample extension. The method is based onGaussian Process Regression and independent of the manifold learning algorithm.Additionally, the method naturally provides a measure for the degree ofabnormality for a newly arrived data point that did not participate in thetraining process. We derive the mathematical connection between the proposedmethod and the Nystrom extension and show that the latter is a special case ofthe former. We present extensive experimental results that demonstrate theperformance of the proposed method and compare it to other existingout-of-sample extension methods.
arxiv-16800-291 | Elastic Functional Coding of Riemannian Trajectories | http://arxiv.org/abs/1603.02200 | author:Rushil Anirudh, Pavan Turaga, Jingyong Su, Anuj Srivastava category:cs.CV math.DG published:2016-03-07 summary:Visual observations of dynamic phenomena, such as human actions, are oftenrepresented as sequences of smoothly-varying features . In cases where thefeature spaces can be structured as Riemannian manifolds, the correspondingrepresentations become trajectories on manifolds. Analysis of thesetrajectories is challenging due to non-linearity of underlying spaces andhigh-dimensionality of trajectories. In vision problems, given the nature ofphysical systems involved, these phenomena are better characterized on alow-dimensional manifold compared to the space of Riemannian trajectories. Forinstance, if one does not impose physical constraints of the human body, indata involving human action analysis, the resulting representation space willhave highly redundant features. Learning an effective, low-dimensionalembedding for action representations will have a huge impact in the areas ofsearch and retrieval, visualization, learning, and recognition. The difficultylies in inherent non-linearity of the domain and temporal variability ofactions that can distort any traditional metric between trajectories. Toovercome these issues, we use the framework based on transported square-rootvelocity fields (TSRVF); this framework has several desirable properties,including a rate-invariant metric and vector space representations. We proposeto learn an embedding such that each action trajectory is mapped to a singlepoint in a low-dimensional Euclidean space, and the trajectories that differonly in temporal rates map to the same point. We utilize the TSRVFrepresentation, and accompanying statistical summaries of Riemanniantrajectories, to extend existing coding methods such as PCA, KSVD and LabelConsistent KSVD to Riemannian trajectories or more generally to Riemannianfunctions.
arxiv-16800-292 | Authenticating users through their arm movement patterns | http://arxiv.org/abs/1603.02211 | author:Rajesh Kumar, Vir V Phoha, Rahul Raina category:cs.CV cs.CR K.6.5 published:2016-03-07 summary:In this paper, we propose four continuous authentication designs by using thecharacteristics of arm movements while individuals walk. The first design usesacceleration of arms captured by a smartwatch's accelerometer sensor, thesecond design uses the rotation of arms captured by a smartwatch's gyroscopesensor, third uses the fusion of both acceleration and rotation at thefeature-level and fourth uses the fusion at score-level. Each of these designsis implemented by using four classifiers, namely, k nearest neighbors (k-NN)with Euclidean distance, Logistic Regression, Multilayer Perceptrons, andRandom Forest resulting in a total of sixteen authentication mechanisms. Theseauthentication mechanisms are tested under three different environments, namelyan intra-session, inter-session on a dataset of 40 users and an inter-phase ona dataset of 12 users. The sessions of data collection were separated by atleast ten minutes, whereas the phases of data collection were separated by atleast three months. Under the intra-session environment, all of the twelveauthentication mechanisms achieve a mean dynamic false accept rate (DFAR) of 0%and dynamic false reject rate (DFRR) of 0%. For the inter-session environment,feature level fusion-based design with classifier k-NN achieves the best errorrates that are a mean DFAR of 2.2% and DFRR of 4.2%. The DFAR and DFRRincreased from 5.68% and 4.23% to 15.03% and 14.62% respectively when featurelevel fusion-based design with classifier k-NN was tested under the inter-phaseenvironment on a dataset of 12 users.
arxiv-16800-293 | Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection | http://arxiv.org/abs/1603.02199 | author:Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen category:cs.LG cs.AI cs.CV cs.RO published:2016-03-07 summary:We describe a learning-based approach to hand-eye coordination for roboticgrasping from monocular images. To learn hand-eye coordination for grasping, wetrained a large convolutional neural network to predict the probability thattask-space motion of the gripper will result in successful grasps, using onlymonocular camera images and independently of camera calibration or the currentrobot pose. This requires the network to observe the spatial relationshipbetween the gripper and objects in the scene, thus learning hand-eyecoordination. We then use this network to servo the gripper in real time toachieve successful grasps. To train our network, we collected over 800,000grasp attempts over the course of two months, using between 6 and 14 roboticmanipulators at any given time, with differences in camera placement andhardware. Our experimental evaluation demonstrates that our method achieveseffective real-time control, can successfully grasp novel objects, and correctsmistakes by continuous servoing.
arxiv-16800-294 | Online Sparse Linear Regression | http://arxiv.org/abs/1603.02250 | author:Dean Foster, Satyen Kale, Howard Karloff category:cs.LG published:2016-03-07 summary:We consider the online sparse linear regression problem, which is the problemof sequentially making predictions observing only a limited number of featuresin each round, to minimize regret with respect to the best sparse linearregressor, where prediction accuracy is measured by square loss. We give aninefficient algorithm that obtains regret bounded by $\tilde{O}(\sqrt{T})$after $T$ prediction rounds. We complement this result by showing that noalgorithm running in polynomial time per iteration can achieve regret boundedby $O(T^{1-\delta})$ for any constant $\delta > 0$ unless $\text{NP} \subseteq\text{BPP}$. This computational hardness result resolves an open problempresented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013).This hardness result holds even if the algorithm is allowed to access morefeatures than the best sparse linear regressor up to a logarithmic factor inthe dimension.
arxiv-16800-295 | Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences | http://arxiv.org/abs/1603.02252 | author:Wenbin Li, Darren Cosker, Matthew Brown category:cs.CV published:2016-03-07 summary:It is hard to densely track a nonrigid object in long term, which is afundamental research issue in the computer vision community. This task oftenrelies on estimating pairwise correspondences between images over time wherethe error is accumulated and leads to a drift issue. In this paper, weintroduce a novel optimization framework with an Anchor Patch constraint. It issupposed to significantly reduce overall errors given long sequences containingnon-rigidly deformable objects. Our framework can be applied to any densetracking algorithm, e.g. optical flow. We demonstrate the success of ourapproach by showing significant error reduction on 6 popular optical flowalgorithms applied to a range of real-world nonrigid benchmarks. We alsoprovide quantitative analysis of our approach given synthetic occlusions andimage noise.
arxiv-16800-296 | Blur Robust Optical Flow using Motion Channel | http://arxiv.org/abs/1603.02253 | author:Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, Darren Cosker category:cs.CV published:2016-03-07 summary:It is hard to estimate optical flow given a realworld video sequence withcamera shake and other motion blur. In this paper, we first investigate theblur parameterization for video footage using near linear motion elements. wethen combine a commercial 3D pose sensor with an RGB camera, in order to filmvideo footage of interest together with the camera motion. We illustrates thatthis additional camera motion/trajectory channel can be embedded into a hybridframework by interleaving an iterative blind deconvolution and warping basedoptical flow scheme. Our method yields improved accuracy within three otherstate-of-the-art baselines given our proposed ground truth blurry sequences;and several other realworld sequences filmed by our imaging system.
arxiv-16800-297 | Confidence-Constrained Maximum Entropy Framework for Learning from Multi-Instance Data | http://arxiv.org/abs/1603.01901 | author:Behrouz Behmardi, Forrest Briggs, Xiaoli Z. Fern, Raviv Raich category:cs.LG cs.IT math.IT stat.ML published:2016-03-07 summary:Multi-instance data, in which each object (bag) contains a collection ofinstances, are widespread in machine learning, computer vision, bioinformatics,signal processing, and social sciences. We present a maximum entropy (ME)framework for learning from multi-instance data. In this approach each bag isrepresented as a distribution using the principle of ME. We introduce theconcept of confidence-constrained ME (CME) to simultaneously learn thestructure of distribution space and infer each distribution. The sharedstructure underlying each density is used to learn from instances inside eachbag. The proposed CME is free of tuning parameters. We devise a fastoptimization algorithm capable of handling large scale multi-instance data. Inthe experimental section, we evaluate the performance of the proposed approachin terms of exact rank recovery in the space of distributions and compare itwith the regularized ME approach. Moreover, we compare the performance of CMEwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show acomparable performance in terms of accuracy with reduced computationalcomplexity.
arxiv-16800-298 | Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive Disambiguation: the eXtended Revised AraMorph (XRAM) | http://arxiv.org/abs/1603.01833 | author:Giuliano Lancioni, Valeria Pettinari, Laura Garofalo, Marta Campanelli, Ivana Pepe, Simona Olivieri, Ilaria Cicola category:cs.CL cs.IR published:2016-03-06 summary:An extended, revised form of Tim Buckwalter's Arabic lexical andmorphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM),is presented which addresses a number of weaknesses and inconsistencies of theoriginal model by allowing a wider coverage of real-world Classical andcontemporary (both formal and informal) Arabic texts. Building upon previousresearch, XRAM enhancements include (i) flag-selectable usage markers, (ii)probabilistic mildly context-sensitive POS tagging, filtering, disambiguationand ranking of alternative morphological analyses, (iii) semi-automaticincrement of lexical coverage through extraction of lexical and morphologicalinformation from existing lexical resources. Testing of XRAM through afront-end Python module showed a remarkable success level.
arxiv-16800-299 | Fast calculation of correlations in recognition systems | http://arxiv.org/abs/1603.01772 | author:Pavel Dourbal, Mikhail Pekker category:cs.CV published:2016-03-06 summary:Computationally efficient classification system architecture is proposed. Itutilizes fast tensor-vector multiplication algorithm to apply linear operatorsupon input signals . The approach is applicable to wide variety of recognitionsystem architectures ranging from single stage matched filter bank classifiersto complex neural networks with unlimited number of hidden layers.
arxiv-16800-300 | Composing inference algorithms as program transformations | http://arxiv.org/abs/1603.01882 | author:Robert Zinkov, Chung-chieh Shan category:stat.ML cs.AI stat.CO stat.ME published:2016-03-06 summary:Probabilistic inference procedures are usually coded painstakingly fromscratch, for each target model and each inference algorithm. We reduce thiscoding effort by generating inference procedures from models automatically. Wemake this code generation modular by decomposing inference algorithms intoreusable program transformations. These source-to-source transformationsperform exact inference as well as generate probabilistic programs that computeexpectations, densities, and MCMC samples. The resulting inference proceduresrun in time comparable to that of handwritten procedures.
