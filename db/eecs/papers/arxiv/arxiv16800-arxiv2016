arxiv-16800-1 | An Ensemble Method to Produce High-Quality Word Embeddings | http://arxiv.org/pdf/1604.01692v1.pdf | author:Robert Speer, Joshua Chin category:cs.CL I.2.7 published:2016-04-06 summary:A currently successful approach to computational semantics is to representwords as embeddings in a machine-learned vector space. We present an ensemblemethod that combines embeddings produced by GloVe (Pennington et al., 2014) andword2vec (Mikolov et al., 2013) with structured knowledge from the semanticnetworks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al.,2013), merging their information into a common representation with a large,multilingual vocabulary. The embeddings it produces achieve state-of-the-artperformance on many word-similarity evaluations. Its score of $\rho = .596$ onan evaluation of rare words (Luong et al., 2013) is 16% higher than theprevious best known system.
arxiv-16800-2 | Equivalence Among Different Variants of One-Class Nearest Neighbours and Creating Their Accurate Ensembles | http://arxiv.org/pdf/1604.01686v1.pdf | author:Shehroz S. Khan, Amir Ahmad category:cs.LG published:2016-04-06 summary:In one-class classification (OCC) problems, only the data for the targetclass is available, whereas the data for the non-target class may be completelyabsent. In this paper, we study one-class nearest neighbour (OCNN) classifiersand their different variants for the OCC problem. We present a theoreticalanalysis to show the equivalence among different variants of OCNN that may usedifferent neighbours or thresholds to identify unseen examples of thenon-target class. We also present a method based on inter-quartile range foroptimizing parameters used in OCNN in the absence of non-target data duringtraining. Then, we propose to use two ensemble approaches based on randomsub-space and random projection approaches to create accurate ensemble thatsignificantly outperforms the baseline OCNN. We tested the proposed methods onvarious benchmark and real word domain-specific datasets to show their superiorperformance. The results give strong evidence that the random projectionensemble of the proposed OCNN with optimized parameters variants performsignificantly and consistently better than the single OCC on all the testeddatasets.
arxiv-16800-3 | Improving Back-Propagation by Adding an Adversarial Gradient | http://arxiv.org/pdf/1510.04189v2.pdf | author:Arild Nøkland category:stat.ML cs.LG published:2015-10-14 summary:The back-propagation algorithm is widely used for learning in artificialneural networks. A challenge in machine learning is to create models thatgeneralize to new data samples not seen in the training data. Recently, acommon flaw in several machine learning algorithms was discovered: smallperturbations added to the input data lead to consistent misclassification ofdata samples. Samples that easily mislead the model are called adversarialexamples. Training a "maxout" network on adversarial examples has shown todecrease this vulnerability, but also increase classification performance. Thispaper shows that adversarial training has a regularizing effect also innetworks with logistic, hyperbolic tangent and rectified linear units. A simpleextension to the back-propagation method is proposed, that adds an adversarialgradient to the training. The extension requires an additional forward andbackward pass to calculate a modified input sample, or mini batch, used asinput for standard back-propagation learning. The first experimental results onMNIST show that the "adversarial back-propagation" method increases theresistance to adversarial examples and boosts the classification performance.The extension reduces the classification error on the permutation invariantMNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in anetwork with rectified linear units. Results on CIFAR-10 indicate that themethod has a regularizing effect similar to dropout in fully connectednetworks. Based on these promising results, adversarial back-propagation isproposed as a stand-alone regularizing method that should be furtherinvestigated.
arxiv-16800-4 | DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks | http://arxiv.org/pdf/1601.00917v5.pdf | author:Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, Tat-Seng Chua category:cs.LG cs.NE published:2016-01-05 summary:The performance of deep neural networks is well-known to be sensitive to thesetting of their hyperparameters. Recent advances in reverse-mode automaticdifferentiation allow for optimizing hyperparameters with gradients. Thestandard way of computing these gradients involves a forward and backward passof computations. However, the backward pass usually needs to consumeunaffordable memory to store all the intermediate variables to exactly reversethe forward training procedure. In this work we propose a simple but effectivemethod, DrMAD, to distill the knowledge of the forward pass into a shortcutpath, through which we approximately reverse the training trajectory.Experiments on several image benchmark datasets show that DrMAD is at least 45times faster and consumes 100 times less memory compared to state-of-the-artmethods for optimizing hyperparameters with minimal compromise to itseffectiveness. To the best of our knowledge, DrMAD is the first researchattempt to make it practical to automatically tune thousands of hyperparametersof deep neural networks. The code can be downloaded fromhttps://github.com/bigaidream-projects/drmad
arxiv-16800-5 | Computing with hardware neurons: spiking or classical? Perspectives of applied Spiking Neural Networks from the hardware side | http://arxiv.org/pdf/1602.02009v2.pdf | author:Sergei Dytckov, Masoud Daneshtalab category:cs.NE published:2016-02-05 summary:While classical neural networks take a position of a leading method in themachine learning community, spiking neuromorphic systems bring attention andlarge projects in neuroscience. Spiking neural networks were shown to be ableto substitute networks of classical neurons in applied tasks. This workexplores recent hardware designs focusing on perspective applications (likeconvolutional neural networks) for both neuron types from the energy efficiencyside to analyse whether there is a possibility for spiking neuromorphichardware to grow up for a wider use. Our comparison shows that spiking hardwareis at least on the same level of energy efficiency or even higher thannon-spiking on a level of basic operations. However, on a system level, spikingsystems are outmatched and consume much more energy due to inefficient datarepresentation with a long series of spikes. If spike-driven applications,minimizing an amount of spikes, are developed, spiking neural systems may reachthe energy efficiency level of classical neural systems. However, in the nearfuture, both type of neuromorphic systems may benefit from emerging memorytechnologies, minimizing the energy consumption of computation and memory forboth neuron types. That would make infrastructure and data transfer energydominant on the system level. We expect that spiking neurons have somebenefits, which would allow achieving better energy results. Still the problemof an amount of spikes will still be the major bottleneck for spiking hardwaresystems.
arxiv-16800-6 | A genetic algorithm to discover flexible motifs with support | http://arxiv.org/pdf/1511.04986v2.pdf | author:Joan Serrà, Aleksandar Matic, Josep Luis Arcos, Alexandros Karatzoglou category:cs.LG cs.NE published:2015-11-16 summary:Finding repeated patterns or motifs in a time series is an importantunsupervised task that has still a number of open issues, starting by thedefinition of motif. In this paper, we revise the notion of motif support,characterizing it as the number of patterns or repetitions that define a motif.We then propose GENMOTIF, a genetic algorithm to discover motifs with supportwhich, at the same time, is flexible enough to accommodate other motifspecifications and task characteristics. GENMOTIF is an anytime algorithm thateasily adapts to many situations: searching in a range of segment lengths,applying uniform scaling, dealing with multiple dimensions, using differentsimilarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: ithas only two intuitive parameters which, if set within reasonable bounds, donot substantially affect its performance. We demonstrate the value of ourapproach in a number of synthetic and real-world settings, considering trafficvolume measurements, accelerometer signals, and telephone call records.
arxiv-16800-7 | Modeling self-organization of vocabularies under phonological similarity effects | http://arxiv.org/pdf/1603.05354v2.pdf | author:Javier Vera category:cs.CL physics.soc-ph published:2016-03-17 summary:This work develops a computational model (by Automata Networks) ofphonological similarity effects involved in the formation of word-meaningassociations on artificial populations of speakers. Classical studies show thatin recalling experiments memory performance was impaired for phonologicallysimilar words versus dissimilar ones. Here, the individuals confoundphonologically similar words according to a predefined parameter. The mainhypothesis is that there is a critical range of the parameter, and with this,of working-memory mechanisms, which implies drastic changes in the finalconsensus of the entire population. Theoretical results present proofs ofconvergence for a particular case of the model within a worst-case complexityframework. Computer simulations describe the evolution of an energy functionthat measures the amount of local agreement between individuals. The mainfinding is the appearance of sudden changes in the energy function at criticalparameters.
arxiv-16800-8 | A Self-Paced Regularization Framework for Multi-Label Learning | http://arxiv.org/pdf/1603.06708v2.pdf | author:Changsheng Li, Fan Wei, Junchi Yan, Weishan Dong, Qingshan Liu, Xiaoyu Zhang, Hongyuan Zha category:cs.LG published:2016-03-22 summary:In this paper, we propose a novel multi-label learning framework, calledMulti-Label Self-Paced Learning (MLSPL), in an attempt to incorporate theself-paced learning strategy into multi-label learning regime. In light of thebenefits of adopting the easy-to-hard strategy proposed by self-paced learning,the devised MLSPL aims to learn multiple labels jointly by gradually includinglabel learning tasks and instances into model training from the easy to thehard. We first introduce a self-paced function as a regularizer in themulti-label learning formulation, so as to simultaneously rank priorities ofthe label learning tasks and the instances in each learning iteration.Considering that different multi-label learning scenarios often need differentself-paced schemes during optimization, we thus propose a general way to findthe desired self-paced functions. Experimental results on three benchmarkdatasets suggest the state-of-the-art performance of our approach.
arxiv-16800-9 | Information Utilization Ratio in Heuristic Optimization Algorithms | http://arxiv.org/pdf/1604.01643v1.pdf | author:Junzhi Li, Ying Tan category:cs.NE published:2016-04-06 summary:Heuristic algorithms are able to optimize objective functions efficientlybecause they use intelligently the information of the objective functions. Thusinformation utilization is vital to the performance of heuristics. However, theconcept of information utilization has remained vague and abstract becausethere is no reliable metric to reflect the extent to which the information ofthe objective function is utilized in heuristic algorithms. In this paper, themetric of information utilization ratio (IUR) is defined, which is the ratiobetween the utilized information quantity and the acquired information quantityin the searching process. IUR proves to be well-defined. Several examples oftypical heuristic algorithms are given to demonstrate the procedure ofcalculating IUR. The results also reveal that elevating IUR is the potentialcause of many algorithmic improving works. IUR can be an index of how exquisitean algorithm is designed and guide the design of new heuristics and theimprovement of existing ones.
arxiv-16800-10 | Nonparametric Detection of Geometric Structures over Networks | http://arxiv.org/pdf/1604.01351v2.pdf | author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor category:stat.ML published:2016-04-05 summary:Nonparametric detection of existence of an anomalous structure over a networkis investigated. Nodes corresponding to the anomalous structure (if one exists)receive samples generated by a distribution q, which is different from adistribution p generating samples for other nodes. If an anomalous structuredoes not exist, all nodes receive samples generated by p. It is assumed thatthe distributions p and q are arbitrary and unknown. The goal is to designstatistically consistent tests with probability of errors converging to zero asthe network size becomes asymptotically large. Kernel-based tests are proposedbased on maximum mean discrepancy that measures the distance between meanembeddings of distributions into a reproducing kernel Hilbert space. Detectionof an anomalous interval over a line network is first studied. Sufficientconditions on minimum and maximum sizes of candidate anomalous intervals arecharacterized in order to guarantee the proposed test to be consistent. It isalso shown that certain necessary conditions must hold to guarantee any test tobe universally consistent. Comparison of sufficient and necessary conditionsyields that the proposed test is order-level optimal and nearly optimalrespectively in terms of minimum and maximum sizes of candidate anomalousintervals. Generalization of the results to other networks is furtherdeveloped. Numerical results are provided to demonstrate the performance of theproposed tests.
arxiv-16800-11 | ASlib: A Benchmark Library for Algorithm Selection | http://arxiv.org/pdf/1506.02465v3.pdf | author:Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Frechette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown, Kevin Tierney, Joaquin Vanschoren category:cs.AI cs.LG published:2015-06-08 summary:The task of algorithm selection involves choosing an algorithm from a set ofalgorithms on a per-instance basis in order to exploit the varying performanceof algorithms over a set of instances. The algorithm selection problem isattracting increasing attention from researchers and practitioners in AI. Yearsof fruitful applications in a number of domains have resulted in a large amountof data, but the community lacks a standard format or repository for this data.This situation makes it difficult to share and compare different approacheseffectively, as is done in other, more established fields. It alsounnecessarily hinders new researchers who want to work in this area. To addressthis problem, we introduce a standardized format for representing algorithmselection scenarios and a repository that contains a growing number of datasets from the literature. Our format has been designed to be able to express awide variety of different scenarios. Demonstrating the breadth and power of ourplatform, we describe a set of example experiments that build and evaluatealgorithm selection models through a common interface. The results display thepotential of algorithm selection to achieve significant performanceimprovements across a broad range of problems and algorithms.
arxiv-16800-12 | Hankel Matrix Nuclear Norm Regularized Tensor Completion for N-dimensional Exponential Signals | http://arxiv.org/pdf/1604.02100v1.pdf | author:Jiaxi Ying, Hengfa Lu, Qingtao Wei, Jian-Feng Cai, Di Guo, Jihui Wu, Zhong Chen, Xiaobo Qu category:stat.ML cs.NA published:2016-04-06 summary:Signals are usually modeled as a superposition of exponential functions inspectroscopy of chemistry, biology and medical imaging. However, for fast dataacquisition or other inevitable reasons, only a small amount of samples may beacquired. How to recover the full signal is then of great interest. Existingapproaches can not efficiently recover N-dimensional exponential signals withN>=3. This paper studies the problem of recovering N-dimensional (particularly$N\geq 3$) exponential signals from partial observations, and we formulate thisproblem as a low-rank tensor completion problem with exponential factors. Thefull signal is reconstructed by simultaneously exploiting the CANDECOMP/PARAFAC(CP) tensor decomposition and the exponential structure of the associatedfactors, of which the latter is promoted by minimizing an objective functioninvolving the nuclear norm of Hankel matrices. Experimental results onsimulated and real magnetic resonance spectroscopy data show that the proposedapproach can successfully recover full signals from very limited samples and isrobust to the estimated tensor rank.
arxiv-16800-13 | Fast $(1+ε)$-approximation of the Löwner extremal matrices of high-dimensional symmetric matrices | http://arxiv.org/pdf/1604.01592v1.pdf | author:Frank Nielsen, Richard Nock category:cs.CG cs.CV published:2016-04-06 summary:Matrix data sets are common nowadays like in biomedical imaging where theDiffusion Tensor Magnetic Resonance Imaging (DT-MRI) modality produces datasets of 3D symmetric positive definite matrices anchored at voxel positions capturingthe anisotropic diffusion properties of water molecules in biological tissues.The space of symmetric matrices can be partially ordered using the L\"ownerordering, and computing extremal matrices dominating a given set of matrices isa basic primitive used in matrix-valued signal processing. In this letter, wedesign a fast and easy-to-implement iterative algorithm to approximatearbitrarily finely these extremal matrices. Finally, we discuss on extensionsto matrix clustering.
arxiv-16800-14 | Computational Sociolinguistics: A Survey | http://arxiv.org/pdf/1508.07544v2.pdf | author:Dong Nguyen, A. Seza Doğruöz, Carolyn P. Rosé, Franciska de Jong category:cs.CL published:2015-08-30 summary:Language is a social phenomenon and variation is inherent to its socialnature. Recently, there has been a surge of interest within the computationallinguistics (CL) community in the social dimension of language. In this articlewe present a survey of the emerging field of "Computational Sociolinguistics"that reflects this increased interest. We aim to provide a comprehensiveoverview of CL research on sociolinguistic themes, featuring topics such as therelation between language and social identity, language use in socialinteraction and multilingual communication. Moreover, we demonstrate thepotential for synergy between the research communities involved, by showing howthe large-scale data-driven methods that are widely used in CL can complementexisting sociolinguistic studies, and how sociolinguistics can inform andchallenge the methods and assumptions employed in CL studies. We hope to conveythe possible benefits of a closer collaboration between the two communities andconclude with a discussion of open challenges.
arxiv-16800-15 | Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem Solvers | http://arxiv.org/pdf/1511.00540v2.pdf | author:Jonathan Binas, Giacomo Indiveri, Michael Pfeiffer category:cs.NE published:2015-11-02 summary:Solving constraint satisfaction problems (CSPs) is a notoriously expensivecomputational task. Recently, it has been proposed that efficient stochasticsolvers can be obtained through appropriately configured spiking neuralnetworks performing Markov Chain Monte Carlo (MCMC) sampling. The possibilityto run such models on massively parallel, low-power neuromorphic hardware holdsgreat promise; however, previously proposed networks are based onprobabilistically spiking neurons, and thus rely on random number generators orexternal noise sources to achieve the necessary stochasticity, leading tosignificant overhead in the implementation. Here we show how stochasticity canbe achieved by implementing deterministic models of integrate and fire neuronsusing subthreshold analog circuits that are affected by thermal noise. Wepresent an efficient implementation of spike-based CSP solvers using areconfigurable neural network VLSI device, and the device's intrinsic noise asa source of randomness. To illustrate the overall concept, we implement ageneric Sudoku solver based on our approach and demonstrate its operation. Weestablish a link between the neuron parameters and the system dynamics,allowing for a simple temperature control mechanism.
arxiv-16800-16 | Training Constrained Deconvolutional Networks for Road Scene Semantic Segmentation | http://arxiv.org/pdf/1604.01545v1.pdf | author:German Ros, Simon Stent, Pablo F. Alcantarilla, Tomoki Watanabe category:cs.CV published:2016-04-06 summary:In this work we investigate the problem of road scene semantic segmentationusing Deconvolutional Networks (DNs). Several constraints limit the practicalperformance of DNs in this context: firstly, the paucity of existing pixel-wiselabelled training data, and secondly, the memory constraints of embeddedhardware, which rule out the practical use of state-of-the-art DN architecturessuch as fully convolutional networks (FCN). To address the first constraint, weintroduce a Multi-Domain Road Scene Semantic Segmentation (MDRS3) dataset,aggregating data from six existing densely and sparsely labelled datasets fortraining our models, and two existing, separate datasets for testing theirgeneralisation performance. We show that, while MDRS3 offers a greater volumeand variety of data, end-to-end training of a memory efficient DN does notyield satisfactory performance. We propose a new training strategy to overcomethis, based on (i) the creation of a best-possible source network (S-Net) fromthe aggregated data, ignoring time and memory constraints; and (ii) thetransfer of knowledge from S-Net to the memory-efficient target network(T-Net). We evaluate different techniques for S-Net creation and T-Nettransferral, and demonstrate that training a constrained deconvolutionalnetwork in this manner can unlock better performance than existing trainingapproaches. Specifically, we show that a target network can be trained toachieve improved accuracy versus an FCN despite using less than 1\% of thememory. We believe that our approach can be useful beyond automotive scenarioswhere labelled data is similarly scarce or fragmented and where practicalconstraints exist on the desired model size. We make available our networkmodels and aggregated multi-domain dataset for reproducibility.
arxiv-16800-17 | Generating Chinese Classical Poems with RNN Encoder-Decoder | http://arxiv.org/pdf/1604.01537v1.pdf | author:Xiaoyuan Yi, Ruoyu Li, Maosong Sun category:cs.CL cs.NE published:2016-04-06 summary:We take the generation of Chinese classical poem lines as asequence-to-sequence learning problem, and build a novel system based on theRNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with atopic word as input. Our system can jointly learn semantic meaning within asingle line, semantic relevance among lines in a poem, and the use ofstructural, rhythmical and tonal patterns, without utilizing any constrainttemplates. Experimental results show that our system outperforms othercompetitive systems. We also find that the attention mechanism can capture theword associations in Chinese classical poetry and inverting target lines intraining can improve performance.
arxiv-16800-18 | Accurate and scalable social recommendation using mixed-membership stochastic block models | http://arxiv.org/pdf/1604.01170v2.pdf | author:Antonia Godoy-Lorite, Roger Guimera, Cristopher Moore, Marta Sales-Pardo category:cs.SI cs.IR cs.LG physics.soc-ph published:2016-04-05 summary:With ever-increasing amounts of online information available, modeling andpredicting individual preferences-for books or articles, for example-isbecoming more and more important. Good predictions enable us to improve adviceto users, and obtain a better understanding of the socio-psychologicalprocesses that determine those preferences. We have developed a collaborativefiltering model, with an associated scalable algorithm, that makes accuratepredictions of individuals' preferences. Our approach is based on the explicitassumption that there are groups of individuals and of items, and that thepreferences of an individual for an item are determined only by their groupmemberships. Importantly, we allow each individual and each item to belongsimultaneously to mixtures of different groups and, unlike many popularapproaches, such as matrix factorization, we do not assume implicitly orexplicitly that individuals in each group prefer items in a single group ofitems. The resulting overlapping groups and the predicted preferences can beinferred with a expectation-maximization algorithm whose running time scaleslinearly (per iteration). Our approach enables us to predict individualpreferences in large datasets, and is considerably more accurate than thecurrent algorithms for such large datasets.
arxiv-16800-19 | Simple and Efficient Learning using Privileged Information | http://arxiv.org/pdf/1604.01518v1.pdf | author:Xinxing Xu, Joey Tianyi Zhou, IvorW. Tsang, Zheng Qin, Rick Siow Mong Goh, Yong Liu category:cs.LG published:2016-04-06 summary:The Support Vector Machine using Privileged Information (SVM+) has beenproposed to train a classifier to utilize the additional privileged informationthat is only available in the training phase but not available in the testphase. In this work, we propose an efficient solution for SVM+ by simplyutilizing the squared hinge loss instead of the hinge loss as in the existingSVM+ formulation, which interestingly leads to a dual form with less variablesand in the same form with the dual of the standard SVM. The proposed algorithmis utilized to leverage the additional web knowledge that is only availableduring training for the image categorization tasks. The extensive experimentalresults on both Caltech101 andWebQueries datasets show that our proposed methodcan achieve a factor of up to hundred times speedup with the comparableaccuracy when compared with the existing SVM+ method.
arxiv-16800-20 | Comments on: "A Random Forest Guided Tour" by G. Biau and E. Scornet | http://arxiv.org/pdf/1604.01515v1.pdf | author:Sylvain Arlot, Robin Genuer category:math.ST stat.ME stat.ML stat.TH published:2016-04-06 summary:This paper is a comment on the survey paper by Biau and Scornet (2016) aboutrandom forests. We focus on the problem of quantifying the impact of eachingredient of random forests on their performance. We show that such aquantification is possible for a simple pure forest , leading to conclusionsthat could apply more generally. Then, we consider "hold-out" random forests,which are a good middle point between "toy" pure forests and Breiman's originalrandom forests.
arxiv-16800-21 | SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size | http://arxiv.org/pdf/1602.07360v3.pdf | author:Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer category:cs.CV cs.AI published:2016-02-24 summary:Recent research on deep neural networks has focused primarily on improvingaccuracy. For a given accuracy level, it is typically possible to identifymultiple DNN architectures that achieve that accuracy level. With equivalentaccuracy, smaller DNN architectures offer at least three advantages: (1)Smaller DNNs require less communication across servers during distributedtraining. (2) Smaller DNNs require less bandwidth to export a new model fromthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy onFPGAs and other hardware with limited memory. To provide all of theseadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNetachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.Additionally, with model compression techniques we are able to compressSqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here:https://github.com/DeepScale/SqueezeNet
arxiv-16800-22 | LOMo: Latent Ordinal Model for Facial Analysis in Videos | http://arxiv.org/pdf/1604.01500v1.pdf | author:Karan Sikka, Gaurav Sharma, Marian Bartlett category:cs.CV published:2016-04-06 summary:We study the problem of facial analysis in videos. We propose a novel weaklysupervised learning method that models the video event (expression, pain etc.)as a sequence of automatically mined, discriminative sub-events (eg. onset andoffset phase for smile, brow lower and cheek raise for pain). The proposedmodel is inspired by the recent works on Multiple Instance Learning and latentSVM/HCRF- it extends such frameworks to model the ordinal or temporal aspect inthe videos, approximately. We obtain consistent improvements over relevantcompetitive baselines on four challenging and publicly available video basedfacial analysis datasets for prediction of expression, clinical pain and intentin dyadic conversations. In combination with complimentary features, we reportstate-of-the-art results on these datasets.
arxiv-16800-23 | How does the Low-Rank Matrix Completion Help Internal and External Learnings for Super-Resolution | http://arxiv.org/pdf/1604.01497v1.pdf | author:Shuang Wang, Bo Yue, Xuefeng Liang, Peiyuan Ji, Licheng Jiao category:cs.CV published:2016-04-06 summary:A new challenge in Super-resolution (SR) problem is how to utilize the prosof internal and external learnings to further enhance the result. To addressthis issue, we analyze the attributes of these two kinds of methods, and findthey are complementary in the feature space and image plan, meanwhile, havesparse estimation error. This finding inspires us to propose a low-ranksolution which effectively integrates them together. We then tailor an internalprior learning and an external dictionary learning to fit the solution. With atheoretical analysis on the algorithm, we also prove that the low-rank solutiondoes not require massive input to guarantee the performance. This simplifiesthe design of the internal and external learning methods for the solution, andreduces the computation cost. Unlike other methods, the proposed solution is aparameter free integration, and can be generalized with more recent internaland external learning methods. Intensive experiments show the proposed solutionreconstructs image details effectively, also outperforms state-of-the-arts inboth visual and quantitative assessments, especially for the noisy images.
arxiv-16800-24 | Parameterized Analysis of Multi-objective Evolutionary Algorithms and the Weighted Vertex Cover Problem | http://arxiv.org/pdf/1604.01495v1.pdf | author:Mojgan Pourhassan, Feng Shi, Frank Neumann category:cs.NE cs.DS published:2016-04-06 summary:A rigorous runtime analysis of evolutionary multi-objective optimization forthe classical vertex cover problem in the context of parameterized complexityanalysis has been presented by Kratsch and Neumann (2013). In this paper, weextend the analysis to the weighted vertex cover problem and provide a fixedparameter evolutionary algorithm with respect to OPT, the cost of the theoptimal solution for the problem. Moreover, using a diversity mechanisms, wepresent a multi-objective evolutionary algorithm that finds a 2-approximationin expected polynomial time and introduce a population-based evolutionaryalgorithm which finds a $(1+\varepsilon)$-approximation in expected time$O(n\cdot 2^{\min \{n,2(1- \varepsilon)OPT \}} + n^3)$.
arxiv-16800-25 | Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs | http://arxiv.org/pdf/1603.09446v2.pdf | author:Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang, Xiang Bai category:cs.CV published:2016-03-31 summary:Object skeleton is a useful cue for object detection, complementary to theobject contour, as it provides a structural representation to describe therelationship among object parts. While object skeleton extraction in naturalimages is a very challenging problem, as it requires the extractor to be ableto capture both local and global image context to determine the intrinsic scaleof each skeleton pixel. Existing methods rely on per-pixel based multi-scalefeature computation, which results in difficult modeling and high timeconsumption. In this paper, we present a fully convolutional network withmultiple scale-associated side outputs to address this problem. By observingthe relationship between the receptive field sizes of the sequential stages inthe network and the skeleton scales they can capture, we introduce ascale-associated side output to each stage. We impose supervision to differentstages by guiding the scale-associated side outputs toward groundtruthskeletons of different scales. The responses of the multiple scale-associatedside outputs are then fused in a scale-specific way to localize skeleton pixelswith multiple scales effectively. Our method achieves promising results on twoskeleton extraction datasets, and significantly outperforms other competitors.
arxiv-16800-26 | Keyboard Based Control of Four Dimensional Rotations | http://arxiv.org/pdf/1604.02013v1.pdf | author:Akira Kageyama category:cs.GR cs.CV published:2016-04-06 summary:Aiming at applications to the scientific visualization of three dimensionalsimulations with time evolution, a keyboard based control method to specifyrotations in four dimensions is proposed. It is known that four dimensionalrotations are generally so-called double rotations, and a double rotation is acombination of simultaneously applied two simple rotations. The proposed methodcan specify both the simple and double rotations by single key typings of thekeyboard. The method is tested in visualizations of a regular pentachoron infour dimensional space by a hyperplane slicing.
arxiv-16800-27 | A Focused Dynamic Attention Model for Visual Question Answering | http://arxiv.org/pdf/1604.01485v1.pdf | author:Ilija Ilievski, Shuicheng Yan, Jiashi Feng category:cs.CV cs.CL cs.NE published:2016-04-06 summary:Visual Question and Answering (VQA) problems are attracting increasinginterest from multiple research disciplines. Solving VQA problems requirestechniques from both computer vision for understanding the visual contents of apresented image or video, as well as the ones from natural language processingfor understanding semantics of the question and generating the answers.Regarding visual content modeling, most of existing VQA methods adopt thestrategy of extracting global features from the image or video, whichinevitably fails in capturing fine-grained information such as spatialconfiguration of multiple objects. Extracting features from auto-generatedregions -- as some region-based image recognition methods do -- cannotessentially address this problem and may introduce some overwhelming irrelevantfeatures with the question. In this work, we propose a novel Focused DynamicAttention (FDA) model to provide better aligned image content representationwith proposed questions. Being aware of the key words in the question, FDAemploys off-the-shelf object detector to identify important regions and fusethe information from the regions and global features via an LSTM unit. Suchquestion-driven representations are then combined with question representationand fed into a reasoning unit for generating the answers. Extensive evaluationon a large-scale benchmark dataset, VQA, clearly demonstrate the superiorperformance of FDA over well-established baselines.
arxiv-16800-28 | Non-iterative rigid 2D/3D point-set registration using semidefinite programming | http://arxiv.org/pdf/1501.00630v3.pdf | author:Yuehaw Khoo, Ankur Kapoor category:cs.CV math.OC 90C22, 92C55 G.1.6; I.4.9 published:2015-01-04 summary:We describe a convex programming framework for pose estimation in 2D/3Dpoint-set registration with unknown point correspondences. We give twomixed-integer nonlinear program (MINP) formulations of the 2D/3D registrationproblem when there are multiple 2D images, and propose convex relaxations forboth of the MINPs to semidefinite programs (SDP) that can be solved efficientlyby interior point methods. Our approach to the 2D/3D registration problem isnon-iterative in nature as we jointly solve for pose and correspondence.Furthermore, these convex programs can readily incorporate feature descriptorsof points to enhance registration results. We prove that the convex programsexactly recover the solution to the original nonconvex 2D/3D registrationproblem under noiseless condition. We apply these formulations to theregistration of 3D models of coronary vessels to their 2D projections obtainedfrom multiple intra-operative fluoroscopic images. For this application, weexperimentally corroborate the exact recovery property in the absence of noiseand further demonstrate robustness of the convex programs in the presence ofnoise.
arxiv-16800-29 | Learning A Deep $\ell_\infty$ Encoder for Hashing | http://arxiv.org/pdf/1604.01475v1.pdf | author:Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, Thomas S. Huang category:cs.LG cs.CV published:2016-04-06 summary:We investigate the $\ell_\infty$-constrained representation whichdemonstrates robustness to quantization errors, utilizing the tool of deeplearning. Based on the Alternating Direction Method of Multipliers (ADMM), weformulate the original convex minimization problem as a feed-forward neuralnetwork, named \textit{Deep $\ell_\infty$ Encoder}, by introducing the novelBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers asnetwork biases. Such a structural prior acts as an effective networkregularization, and facilitates the model initialization. We then investigatethe effective use of the proposed model in the application of hashing, bycoupling the proposed encoders under a supervised pairwise loss, to develop a\textit{Deep Siamese $\ell_\infty$ Network}, which can be optimized from end toend. Extensive experiments demonstrate the impressive performances of theproposed model. We also provide an in-depth analysis of its behaviors againstthe competitors.
arxiv-16800-30 | Self-Paced Multi-Task Learning | http://arxiv.org/pdf/1604.01474v1.pdf | author:Changsheng Li, Fan Wei, Junchi Yan, Weishan Dong, Qingshan Liu, Hongyuan Zha category:cs.LG published:2016-04-06 summary:In this paper, we propose a novel multi-task learning (MTL) framework, calledSelf-Paced Multi-Task Learning (SPMTL). Different from previous works treatingall tasks and instances equally when training, SPMTL attempts to jointly learnthe tasks by taking into consideration the complexities of both tasks andinstances. This is inspired by the cognitive process of human brain that oftenlearns from the easy to the hard. We construct a compact SPMTL formulation byproposing a new task-oriented regularizer that can jointly prioritize the tasksand the instances. Thus it can be interpreted as a self-paced learner for MTL.A simple yet effective algorithm is designed for optimizing the proposedobjective function. An error bound for a simplified formulation is alsoanalyzed theoretically. Experimental results on toy and real-world datasetsdemonstrate the effectiveness of the proposed approach, compared to thestate-of-the-art methods.
arxiv-16800-31 | Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks | http://arxiv.org/pdf/1510.07712v2.pdf | author:Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu category:cs.CV published:2015-10-26 summary:We present an approach that exploits hierarchical Recurrent Neural Networks(RNNs) to tackle the video captioning problem, i.e., generating one or multiplesentences to describe a realistic video. Our hierarchical framework contains asentence generator and a paragraph generator. The sentence generator producesone simple short sentence that describes a specific short video interval. Itexploits both temporal- and spatial-attention mechanisms to selectively focuson visual elements during generation. The paragraph generator captures theinter-sentence dependency by taking as input the sentential embedding producedby the sentence generator, combining it with the paragraph history, andoutputting the new initial state for the sentence generator. We evaluate ourapproach on two large-scale benchmark datasets: YouTubeClips andTACoS-MultiLevel. The experiments demonstrate that our approach significantlyoutperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and0.305 respectively.
arxiv-16800-32 | Template Adaptation for Face Verification and Identification | http://arxiv.org/pdf/1603.03958v3.pdf | author:Nate Crosswhite, Jeffrey Byrne, Omkar M. Parkhi, Chris Stauffer, Qiong Cao, Andrew Zisserman category:cs.CV published:2016-03-12 summary:Face recognition performance evaluation has traditionally focused onone-to-one verification, popularized by the Labeled Faces in the Wild datasetfor imagery and the YouTubeFaces dataset for videos. In contrast, the newlyreleased IJB-A face recognition dataset unifies evaluation of one-to-many faceidentification with one-to-one face verification over templates, or sets ofimagery and videos for a subject. In this paper, we study the problem oftemplate adaptation, a form of transfer learning to the set of media in atemplate. Extensive performance evaluations on IJB-A show a surprising result,that perhaps the simplest method of template adaptation, combining deepconvolutional network features with template specific linear SVMs, outperformsthe state-of-the-art by a wide margin. We study the effects of template size,negative set construction and classifier fusion on performance, then comparetemplate adaptation to convolutional networks with metric learning, 2D and 3Dalignment. Our unexpected conclusion is that these other methods, when combinedwith template adaptation, all achieve nearly the same top performance on IJB-Afor template-based face verification and identification.
arxiv-16800-33 | Recurrent Instance Segmentation | http://arxiv.org/pdf/1511.08250v2.pdf | author:Bernardino Romera-Paredes, Philip H. S. Torr category:cs.CV cs.AI published:2015-11-25 summary:Instance segmentation is the problem of detecting and delineating eachdistinct object of interest appearing in an image. Current instancesegmentation approaches consist of ensembles of modules that are trainedindependently of each other, thus missing learning opportunities. Here wepropose a new instance segmentation paradigm consisting in an end-to-end methodthat learns how to segment instances sequentially. The model is based on arecurrent neural network that sequentially finds objects and theirsegmentations one at a time. This net is provided with a spatial memory thatkeeps track of what pixels have been explained and allows handling occlusion.In order to train the model we designed a new principled loss function thataccurately represents the properties of the instance segmentation problem. Inthe experiments carried out, we found that our method outperforms recentapproaches on multiple person segmentation, and all state of the art approacheson the Plant Phenotyping dataset for leaf counting.
arxiv-16800-34 | Collaborative Representation Learning | http://arxiv.org/pdf/1604.01433v1.pdf | author:Matías Vera, Leonardo Rey Vega, Pablo Piantanida category:cs.IT math.IT stat.ML published:2016-04-05 summary:This paper investigates an information-theoretic approach to the problem ofcollaborative representation learning: how to extract salient features ofstatistical relationships in order to build cooperatively meaningfulrepresentations of some relevant content. Modeling the structure of data andits hidden representations by independently identically distributed samples,our goal is to study fundamental limits of the so-called Two-way CollaborativeRepresentation Learning (TW-CRL) and the Collaborative DistributedRepresentation Learning (CDRL) problems. The TW-CRL problem consists of twodistant encoders that separately observe marginal (dependent) components $X_1$and $X_2$ and can cooperate through multiple exchanges of limited informationwith the aim of learning hidden representations $(Y_1,Y_2)$, which can bearbitrarily dependent on $(X_1,X_2)$. On the other hand, in CDRL there are twocooperating encoders and the learner of the hidden representation $Y$ is athird node which can listen the exchanges between the two encoders. Therelevance (figure-of-merit) of such learned representations is measured interms of a normalized (per-sample) multi-letter mutual information metric.Inner and outer bounds to the complexity-relevance region of these problems arederived from which optimality is characterized for several cases of interest.Our resulting complexity-relevance regions are finally evaluated for binarysymmetric and Gaussian statistical models showing how to identify comparativelyrandom features that represent complexity-constrained statistics for theinference of the hidden representations.
arxiv-16800-35 | Highly accurate gaze estimation using a consumer RGB-depth sensor | http://arxiv.org/pdf/1604.01420v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic category:cs.CV published:2016-04-05 summary:Determining the direction in which a person is looking is an importantproblem in a wide range of HCI applications. In this paper we describe a highlyaccurate algorithm that performs gaze estimation using an affordable and widelyavailable device such as Kinect. The method we propose starts by performingaccurate head pose estimation achieved by fitting a person specific morphablemodel of the face to depth data. The ordinarily competing requirements of highaccuracy and high speed are met concurrently by formulating the fittingobjective function as a combination of terms which excel either in accurate orfast fitting, and then by adaptively adjusting their relative contributionsthroughout fitting. Following pose estimation, pose normalization is done byre-rendering the fitted model as a frontal face. Finally gaze estimates areobtained through regression from the appearance of the eyes in synthetic,normalized images. Using EYEDIAP, the standard public dataset for theevaluation of gaze estimation algorithms from RGB-D data, we demonstrate thatour method greatly outperforms the state of the art.
arxiv-16800-36 | A Hierarchical Deep Temporal Model for Group Activity Recognition | http://arxiv.org/pdf/1511.06040v2.pdf | author:Moustafa Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori category:cs.CV published:2015-11-19 summary:In group activity recognition, the temporal dynamics of the whole activitycan be inferred based on the dynamics of the individual people representing theactivity. We build a deep model to capture these dynamics based on LSTM(long-short term memory) models. To make use of these ob- servations, wepresent a 2-stage deep temporal model for the group activity recognitionproblem. In our model, a LSTM model is designed to represent action dynamics ofin- dividual people in a sequence and another LSTM model is designed toaggregate human-level information for whole activity understanding. We evaluateour model over two datasets: the collective activity dataset and a new volley-ball dataset. Experimental results demonstrate that our proposed model improvesgroup activity recognition perfor- mance with compared to baseline methods.
arxiv-16800-37 | dMath: A Scalable Linear Algebra and Math Library for Heterogeneous GP-GPU Architectures | http://arxiv.org/pdf/1604.01416v1.pdf | author:Steven Eliuk, Cameron Upright, Anthony Skjellum category:cs.NE cs.DC cs.MS published:2016-04-05 summary:A new scalable parallel math library, dMath, is presented in this paper thatdemonstrates leading scaling when using intranode, or internode,hybrid-parallelism for deep-learning. dMath provides easy-to-use distributedbase primitives and a variety of domain-specific algorithms. These includematrix multiplication, convolutions, and others allowing for rapid developmentof highly scalable applications, including Deep Neural Networks (DNN), whereaspreviously one was restricted to libraries that provided effective primitivesfor only a single GPU, like Nvidia cublas and cudnn or DNN primitives fromNervana neon framework. Development of HPC software is difficult,labor-intensive work, requiring a unique skill set. dMath allows a wide rangeof developers to utilize parallel and distributed hardware easily. Onecontribution of this approach is that data is stored persistently on the GPUhardware, avoiding costly transfers between host and device. Advanced memorymanagement techniques are utilized, including caching of transferred data andmemory reuse through pooling. A key contribution of dMath is that it deliversperformance, portability, and productivity to its specific domain of support.It enables algorithm and application programmers to quickly solve problemswithout managing the significant complexity associated with multi-levelparallelism.
arxiv-16800-38 | The Curious Robot: Learning Visual Representations via Physical Interactions | http://arxiv.org/pdf/1604.01360v1.pdf | author:Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav Gupta category:cs.CV cs.AI cs.RO published:2016-04-05 summary:What is the right supervisory signal to train visual representations? Currentapproaches in computer vision use category labels from datasets such asImageNet to train ConvNets. However, in case of biological agents, visualrepresentation learning does not require semantic labels. In fact, we arguethat biological agents use active exploration and physical interactions withthe world to learn visual representations unlike current vision systems whichjust use passive observations (images and videos downloaded from web). Forexample, babies push objects, poke them, put them in their mouth and throw themto learn representations. Towards this goal, we build one of the first systemson a Baxter platform that pushes, pokes, grasps and actively observes objectsin a tabletop environment. It uses four different types of physicalinteractions to collect more than 130K datapoints, with each datapointproviding backprops to a shared ConvNet architecture allowing us to learnvisual representations. We show the quality of learned representations byobserving neuron activations and performing nearest neighbor retrieval on thislearned representation. Finally, we evaluate our learned ConvNet on differentimage classification tasks and show improvements compared to learning withoutexternal data.
arxiv-16800-39 | Heavy hitters via cluster-preserving clustering | http://arxiv.org/pdf/1604.01357v1.pdf | author:Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen, Mikkel Thorup category:cs.DS cs.LG published:2016-04-05 summary:In turnstile $\ell_p$ $\varepsilon$-heavy hitters, one maintains ahigh-dimensional $x\in\mathbb{R}^n$ subject to $\texttt{update}(i,\Delta)$causing $x_i\leftarrow x_i + \Delta$, where $i\in[n]$, $\Delta\in\mathbb{R}$.Upon receiving a query, the goal is to report a small list $L\subset[n]$, $L= O(1/\varepsilon^p)$, containing every "heavy hitter" $i\in[n]$ with $x_i\ge \varepsilon \x_{\overline{1/\varepsilon^p}}\_p$, where $x_{\overline{k}}$denotes the vector obtained by zeroing out the largest $k$ entries of $x$ inmagnitude. For any $p\in(0,2]$ the CountSketch solves $\ell_p$ heavy hitters using$O(\varepsilon^{-p}\log n)$ words of space with $O(\log n)$ update time,$O(n\log n)$ query time to output $L$, and whose output after any query iscorrect with high probability (whp) $1 - 1/poly(n)$. Unfortunately the querytime is very slow. To remedy this, the work [CM05] proposed for $p=1$ in thestrict turnstile model, a whp correct algorithm achieving suboptimal space$O(\varepsilon^{-1}\log^2 n)$, worse update time $O(\log^2 n)$, but much betterquery time $O(\varepsilon^{-1}poly(\log n))$. We show this tradeoff between space and update time versus query time isunnecessary. We provide a new algorithm, ExpanderSketch, which in the mostgeneral turnstile model achieves optimal $O(\varepsilon^{-p}\log n)$ space,$O(\log n)$ update time, and fast $O(\varepsilon^{-p}poly(\log n))$ query time,and whp correctness. Our main innovation is an efficient reduction from theheavy hitters to a clustering problem in which each heavy hitter is encoded assome form of noisy spectral cluster in a much bigger graph, and the goal is toidentify every cluster. Since every heavy hitter must be found, correctnessrequires that every cluster be found. We then develop a "cluster-preservingclustering" algorithm, partitioning the graph into clusters without destroyingany original cluster.
arxiv-16800-40 | Radiometric Scene Decomposition: Scene Reflectance, Illumination, and Geometry from RGB-D Images | http://arxiv.org/pdf/1604.01354v1.pdf | author:Stephen Lombardi, Ko Nishino category:cs.CV published:2016-04-05 summary:Recovering the radiometric properties of a scene (i.e., the reflectance,illumination, and geometry) is a long-sought ability of computer vision thatcan provide invaluable information for a wide range of applications.Deciphering the radiometric ingredients from the appearance of a real-worldscene, as opposed to a single isolated object, is particularly challenging asit generally consists of various objects with different material compositionsexhibiting complex reflectance and light interactions that are also part of theillumination. We introduce the first method for radiometric scene decompositionthat handles those intricacies. We use RGB-D images to bootstrap geometryrecovery and simultaneously recover the complex reflectance and naturalillumination while refining the noisy initial geometry and segmenting the sceneinto different material regions. Most important, we handle real-world scenesconsisting of multiple objects of unknown materials, which necessitates themodeling of spatially-varying complex reflectance, natural illumination,texture, interreflection and shadows. We systematically evaluate theeffectiveness of our method on synthetic scenes and demonstrate its applicationto real-world scenes. The results show that rich radiometric information can berecovered from RGB-D images and demonstrate a new role RGB-D sensors can playfor general scene understanding tasks.
arxiv-16800-41 | Bounded Optimal Exploration in MDP | http://arxiv.org/pdf/1604.01350v1.pdf | author:Kenji Kawaguchi category:cs.AI cs.LG published:2016-04-05 summary:Within the framework of probably approximately correct Markov decisionprocesses (PAC-MDP), much theoretical work has focused on methods to attainnear optimality after a relatively long period of learning and exploration.However, practical concerns require the attainment of satisfactory behaviorwithin a short period of time. In this paper, we relax the PAC-MDP conditionsto reconcile theoretically driven exploration methods and practical needs. Wepropose simple algorithms for discrete and continuous state spaces, andillustrate the benefits of our proposed relaxation via theoretical analyses andnumerical examples. Our algorithms also maintain anytime error bounds andaverage loss bounds. Our approach accommodates both Bayesian and non-Bayesianmethods.
arxiv-16800-42 | Bayesian Optimization with Exponential Convergence | http://arxiv.org/pdf/1604.01348v1.pdf | author:Kenji Kawaguchi, Leslie Pack Kaelbling, Tomás Lozano-Pérez category:stat.ML cs.LG published:2016-04-05 summary:This paper presents a Bayesian optimization method with exponentialconvergence without the need of auxiliary optimization and without thedelta-cover sampling. Most Bayesian optimization methods require auxiliaryoptimization: an additional non-convex global optimization problem, which canbe time-consuming and hard to implement in practice. Also, the existingBayesian optimization method with exponential convergence requires access tothe delta-cover sampling, which was considered to be impractical. Our approacheliminates both requirements and achieves an exponential convergence rate.
arxiv-16800-43 | Marr Revisited: 2D-3D Alignment via Surface Normal Prediction | http://arxiv.org/pdf/1604.01347v1.pdf | author:Aayush Bansal, Bryan Russell, Abhinav Gupta category:cs.CV published:2016-04-05 summary:We introduce an approach that leverages surface normal predictions, alongwith appearance cues, to retrieve 3D models for objects depicted in 2D stillimages from a large CAD object library. Critical to the success of our approachis the ability to recover accurate surface normals for objects in the depictedscene. We introduce a skip-network model built on the pre-trained Oxford VGGconvolutional neural network (CNN) for surface normal prediction. Our modelachieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surfacenormal prediction, and recovers fine object detail compared to previousmethods. Furthermore, we develop a two-stream network over the input image andpredicted surface normals that jointly learns pose and style for CAD modelretrieval. When using the predicted surface normals, our two-stream networkmatches prior work using surface normals computed from RGB-D images on the taskof pose prediction, and achieves state of the art when using RGB-D input.Finally, our two-stream network allows us to retrieve CAD models that bettermatch the style and pose of a depicted object compared with baselineapproaches.
arxiv-16800-44 | Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs | http://arxiv.org/pdf/1603.08358v2.pdf | author:Siddhartha Chandra, Iasonas Kokkinos category:cs.CV cs.LG published:2016-03-28 summary:In this work we propose a combination of the Gaussian Conditional RandomField (G-CRF) with Deep Learning for the task of structured prediction. Ourmethod inherits several virtues of G-CRF and Deep Learning: (a) the structuredprediction task has a unique global optimum that is obtained exactly from thesolution of a linear system (b) structured prediction can be jointly trained inan end-to-end setting in general architectures and with arbitrary lossfunctions, (c) the pairwise terms do not have to be simple hand-craftedexpressions, as in the line of works building on the DenseCRF, but can ratherbe `discovered' from data through deep architectures - in particular we usefully convolutional networks to obtain the unary and pairwise terms of ourG-CRF. Building on standard tools from numerical analysis we develop veryefficient algorithms for inference and learning. This efficiency allows us toexplore more sophisticated architectures for structured prediction in deeplearning: we introduce multi-resolution architectures to couple informationacross scales in a joint optimization framework, yielding systematicimprovement. We demonstrate the utility of our approach on the challenging VOCPASCAL 2012 image segmentation benchmark, where an extensive ablation studyindicates substantial improvements over strong baselines.
arxiv-16800-45 | Discovering Perceptual Attributes in a Deep Local Material Recognition Network | http://arxiv.org/pdf/1604.01345v1.pdf | author:Gabriel Schwartz, Ko Nishino category:cs.CV published:2016-04-05 summary:Perceptual material attributes, intrinsic visual properties of materials, arebeing studied in parallel in computer vision and human vision. In neuroscience,perceptual attributes are shown to be an integral part of the human neuralresponse during material recognition. In computer vision, however, they aremerely an intermediate representation of materials and not integrated into therecognition process. In this paper, we show that perceptual material attributescan indeed be found inside a framework for local, patch-based,object-independent material recognition. We introduce a new CNN architecture,the material attribute-category CNN (MAC-CNN), that uses deep weak supervisionto simultaneously classify materials and discover per-pixel perceptualattributes. We show that these attributes conform with past semantic materialattributes and enhance recognition of novel materials. We also introduce anextensive new database for local material recognition. Our results show thatthe internal representation of the MAC-CNN generalizes well and agrees withhuman perception, which has potential implications for our understanding ofhuman material perception as well as applications in object recognition.
arxiv-16800-46 | Deep Cross Residual Learning for Multitask Visual Recognition | http://arxiv.org/pdf/1604.01335v1.pdf | author:Brendan Jou, Shih-Fu Chang category:cs.CV cs.AI cs.MM published:2016-04-05 summary:Residual learning has recently surfaced as an effective means of constructingvery deep neural networks for object recognition. However, current incarnationsof residual networks do not allow for the modeling and integration of complexrelations between closely coupled recognition tasks or across domains. Suchproblems are often encountered in multimedia and vision applications involvinglarge-scale content recognition. We propose a novel extension of residuallearning for deep networks that enables intuitive learning across multiplerelated tasks using cross-connections called cross-residuals. Thesecross-residuals connections can be viewed as a form of in-networkregularization and enables greater network generalization. We show howcross-residual learning (CRL) can be integrated in multitask networks tojointly train and detect visual concepts across several tasks. We present asingle multitask cross-residual network with >40% less parameters that is ableto achieve competitive, or even better, detection performance on a visualsentiment concept detection problem normally requiring multiple specializedsingle-task networks. The resulting multitask cross-residual network alsoachieves better detection performance by about 10.4% over a standard multitaskresidual network without cross-residuals with even a small amount of cross-taskweighting.
arxiv-16800-47 | A Latent Variable Recurrent Neural Network for Discourse Relation Language Models | http://arxiv.org/pdf/1603.01913v2.pdf | author:Yangfeng Ji, Gholamreza Haffari, Jacob Eisenstein category:cs.CL cs.LG cs.NE stat.ML published:2016-03-07 summary:This paper presents a novel latent variable recurrent neural networkarchitecture for jointly modeling sequences of words and (possibly latent)discourse relations between adjacent sentences. A recurrent neural networkgenerates individual words, thus reaping the benefits ofdiscriminatively-trained vector representations. The discourse relations arerepresented with a latent variable, which can be predicted or marginalized,depending on the task. The resulting model can therefore employ a trainingobjective that includes not only discourse relation classification, but alsoword prediction. As a result, it outperforms state-of-the-art alternatives fortwo tasks: implicit discourse relation classification in the Penn DiscourseTreebank, and dialog act classification in the Switchboard corpus. Furthermore,by marginalizing over latent discourse relations at test time, we obtain adiscourse informed language model, which improves over a strong LSTM baseline.
arxiv-16800-48 | Deep Image Retrieval: Learning global representations for image search | http://arxiv.org/pdf/1604.01325v1.pdf | author:Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus category:cs.CV published:2016-04-05 summary:We propose a novel approach for instance-level image retrieval. It produces aglobal and compact fixed-length representation for each image by aggregatingmany region-wise descriptors. In contrast to previous works employingpre-trained deep networks as a black box to produce features, our methodleverages a deep architecture trained for the specific task of image retrieval.Our contribution is twofold: (i) we introduce a ranking framework to learnconvolution and projection weights that are used to build the region features;and (ii) we employ a region proposal network to learn which regions should bepooled to form the final global descriptor. We show that using clean trainingdata is key to the success of our approach. To that aim, we leverage a largescale but noisy landmark dataset and develop an automatic cleaning approach.The proposed architecture produces a global image representation in a singleforward pass. Our approach significantly outperforms previous approaches basedon global descriptors on standard datasets. It even surpasses most prior worksbased on costly local descriptor indexing and spatial verification. We intendto release our pre-trained model.
arxiv-16800-49 | Cohomology of Cryo-Electron Microscopy | http://arxiv.org/pdf/1604.01319v1.pdf | author:Ke Ye, Lek-Heng Lim category:cs.CV math.AT published:2016-04-05 summary:The goal of cryo-electron microscopy (EM) is to reconstruct the 3-dimensionalstructure of a molecule from a collection of its 2-dimensional projectedimages. In this article, we show that the basic premise of cryo-EM --- patchingtogether 2-dimensional projections to reconstruct a 3-dimensional object --- isnaturally one of Cech cohomology with SO(2)-coefficients. We deduce that everycryo-EM reconstruction problem corresponds to an oriented circle bundle on asimplicial complex, allowing us to classify cryo-EM problems via principalbundles. In practice, the 2-dimensional images are noisy and a main task incryo-EM is to denoise them. We will see how the aforementioned insights can beused towards this end.
arxiv-16800-50 | Towards Label Imbalance in Multi-label Classification with Many Labels | http://arxiv.org/pdf/1604.01304v1.pdf | author:Li Li, Houfeng Wang category:cs.LG published:2016-04-05 summary:In multi-label classification, an instance may be associated with a set oflabels simultaneously. Recently, the research on multi-label classification haslargely shifted its focus to the other end of the spectrum where the number oflabels is assumed to be extremely large. The existing works focus on how todesign scalable algorithms that offer fast training procedures and have a smallmemory footprint. However they ignore and even compound another challenge - thelabel imbalance problem. To address this drawback, we propose a novelRepresentation-based Multi-label Learning with Sampling (RMLS) approach. To thebest of our knowledge, we are the first to tackle the imbalance problem inmulti-label classification with many labels. Our experimentations withreal-world datasets demonstrate the effectiveness of the proposed approach.
arxiv-16800-51 | The Multivariate Generalised von Mises: Inference and applications | http://arxiv.org/pdf/1602.05003v3.pdf | author:Alexandre K. W. Navarro, Jes Frellsen, Richard E. Turner category:stat.ML published:2016-02-16 summary:Circular variables arise in a multitude of data-modelling contexts rangingfrom robots to the social sciences. To correctly predict and analyse circulardata, the field of circular and directional statistics has developed a range ofMCMC methods for low-dimensional problems and small-to-medium-sized datasets.In this paper, we extend the toolbox of circular statistics to higherdimensions as a first step towards bringing this field and probabilisticmachine learning closer together. To achieve this task, we introduce themultivariate Generalised von Mises (mGvM) distribution, a Gaussian Processanalogue for circular variables, and demonstrate how this model naturallyoccurs as the posterior in regression and latent variable modelling withcircular variables. We also outline how to perform variational inference forthis model and present experimental results where the mGvM out-performsstandard probabilistic machine learning approaches that do not account for thetopological properties of circular variables.
arxiv-16800-52 | RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy | http://arxiv.org/pdf/1604.01278v1.pdf | author:Guntis Barzdins, Didzis Gosko category:cs.CL published:2016-04-05 summary:Two extensions to the AMR smatch scoring script are presented. The firstextension com-bines the smatch scoring script with the C6.0 rule-basedclassifier to produce a human-readable report on the error patterns frequencyobserved in the scored AMR graphs. This first extension results in 4% gain overthe state-of-art CAMR baseline parser by adding to it a manually craftedwrapper fixing the identified CAMR parser errors. The second extension combinesa per-sentence smatch with an en-semble method for selecting the best AMR graphamong the set of AMR graphs for the same sentence. This second modificationau-tomatically yields further 0.4% gain when ap-plied to outputs of twonondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-levelneural translation AMR parser. For AMR parsing task the character-level neuraltranslation attains surprising 7% gain over the carefully optimized word-levelneural translation. Overall, we achieve smatch F1=62% on the SemEval-2016official scor-ing set and F1=67% on the LDC2015E86 test set.
arxiv-16800-53 | Feature extraction using Latent Dirichlet Allocation and Neural Networks: A case study on movie synopses | http://arxiv.org/pdf/1604.01272v1.pdf | author:Despoina Christou category:cs.CL cs.AI cs.IR cs.LG stat.ML published:2016-04-05 summary:Feature extraction has gained increasing attention in the field of machinelearning, as in order to detect patterns, extract information, or predictfuture observations from big data, the urge of informative features is crucial.The process of extracting features is highly linked to dimensionality reductionas it implies the transformation of the data from a sparse high-dimensionalspace, to higher level meaningful abstractions. This dissertation employsNeural Networks for distributed paragraph representations, and Latent DirichletAllocation to capture higher level features of paragraph vectors. AlthoughNeural Networks for distributed paragraph representations are considered thestate of the art for extracting paragraph vectors, we show that a quick topicanalysis model such as Latent Dirichlet Allocation can provide meaningfulfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, acollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,for both approaches, we use K-Nearest Neighbors to discover similar movies, andplot the projected representations using T-Distributed Stochastic NeighborEmbedding to depict the context similarities. These similarities, expressed asmovie distances, can be used for movies recommendation. The recommended moviesof this approach are compared with the recommended movies from IMDB, which usea collaborative filtering recommendation approach, to show that our two modelscould constitute either an alternative or a supplementary recommendationapproach.
arxiv-16800-54 | Learning Multiscale Features Directly From Waveforms | http://arxiv.org/pdf/1603.09509v2.pdf | author:Zhenyao Zhu, Jesse H. Engel, Awni Hannun category:cs.CL cs.LG cs.NE cs.SD published:2016-03-31 summary:Deep learning has dramatically improved the performance of speech recognitionsystems through learning hierarchies of features optimized for the task athand. However, true end-to-end learning, where features are learned directlyfrom waveforms, has only recently reached the performance of hand-tailoredrepresentations based on the Fourier transform. In this paper, we detail anapproach to use convolutional filters to push past the inherent tradeoff oftemporal and frequency resolution that exists for spectral representations. Atincreased computational cost, we show that increasing temporal resolution viareduced stride and increasing frequency resolution via additional filtersdelivers significant performance improvements. Further, we find more efficientrepresentations by simultaneously learning at multiple scales, leading to anoverall decrease in word error rate on a difficult internal speech test set by20.7% relative to networks with the same number of parameters trained onspectrograms.
arxiv-16800-55 | Comparative Deep Learning of Hybrid Representations for Image Recommendations | http://arxiv.org/pdf/1604.01252v1.pdf | author:Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li category:cs.CV published:2016-04-05 summary:In many image-related tasks, learning expressive and discriminativerepresentations of images is essential, and deep learning has been studied forautomating the learning of such representations. Some user-centric tasks, suchas image recommendations, call for effective representations of not only imagesbut also preferences and intents of users over images. Such representations aretermed \emph{hybrid} and addressed via a deep learning approach in this paper.We design a dual-net deep network, in which the two sub-networks map inputimages and preferences of users into a same latent semantic space, and then thedistances between images and users in the latent space are calculated to makedecisions. We further propose a comparative deep learning (CDL) method to trainthe deep network, using a pair of images compared against one user to learn thepattern of their relative distances. The CDL embraces much more training datathan naive deep learning, and thus achieves superior performance than thelatter, with no cost of increasing network complexity. Experimental resultswith real-world data sets for image recommendations have shown the proposeddual-net network and CDL greatly outperform other state-of-the-art imagerecommendation solutions.
arxiv-16800-56 | Mental Lexicon Growth Modelling Reveals the Multiplexity of the English Language | http://arxiv.org/pdf/1604.01243v1.pdf | author:Massimo Stella, Markus Brede category:physics.soc-ph cs.CL cs.SI published:2016-04-05 summary:In this work we extend previous analyses of linguistic networks by adopting amulti-layer network framework for modelling the human mental lexicon, i.e. anabstract mental repository where words and concepts are stored together withtheir linguistic patterns. Across a three-layer linguistic multiplex, we modelEnglish words as nodes and connect them according to (i) phonologicalsimilarities, (ii) synonym relationships and (iii) free word associations. Ourmain aim is to exploit this multi-layered structure to explore the influence ofphonological and semantic relationships on lexicon assembly over time. Wepropose a model of lexicon growth which is driven by the phonological layer:words are suggested according to different orderings of insertion (e.g. shorterword length, highest frequency, semantic multiplex features) and accepted orrejected subject to constraints. We then measure times of network assembly andcompare these to empirical data about the age of acquisition of words. Inagreement with empirical studies in psycholinguistics, our results providequantitative evidence for the hypothesis that word acquisition is driven byfeatures at multiple levels of organisation within language.
arxiv-16800-57 | A new TAG Formalism for Tamil and Parser Analytics | http://arxiv.org/pdf/1604.01235v1.pdf | author:Vijay Krishna Menon, S. Rajendran, M. Anand Kumar, K. P. Soman category:cs.CL published:2016-04-05 summary:Tree adjoining grammar (TAG) is specifically suited for morph rich andagglutinated languages like Tamil due to its psycho linguistic features andparse time dependency and morph resolution. Though TAG and LTAG formalisms havebeen known for about 3 decades, efforts on designing TAG Syntax for Tamil havenot been entirely successful due to the complexity of its specification and therich morphology of Tamil language. In this paper we present a minimalistic TAGfor Tamil without much morphological considerations and also introduce a parserimplementation with some obvious variations from the XTAG system
arxiv-16800-58 | Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project | http://arxiv.org/pdf/1604.01221v1.pdf | author:Guntis Barzdins, Steve Renals, Didzis Gosko category:cs.CL published:2016-04-05 summary:The paper steps outside the comfort-zone of the traditional NLP tasks likeautomatic speech recognition (ASR) and machine translation (MT) to addressestwo novel problems arising in the automated multilingual news monitoring:segmentation of the TV and radio program ASR transcripts into individualstories, and clustering of the individual stories coming from various sourcesand languages into storylines. Storyline clustering of stories covering thesame events is an essential task for inquisitorial media monitoring. We addressthese two problems jointly by engaging the low-dimensional semanticrepresentation capabilities of the sequence to sequence neural translationmodels. To enable joint multi-task learning for multilingual neural translationof morphologically rich languages we replace the attention mechanism with thesliding-window mechanism and operate the sequence to sequence neuraltranslation model on the character-level rather than on the word-level. Thestory segmentation and storyline clustering problem is tackled by examining thelow-dimensional vectors produced as a side-product of the neural translationprocess. The results of this paper describe a novel approach to the automaticstory segmentation and storyline clustering problem.
arxiv-16800-59 | Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training | http://arxiv.org/pdf/1602.06727v3.pdf | author:Zhizheng Wu, Simon King category:cs.SD cs.CL cs.NE published:2016-02-22 summary:We propose two novel techniques --- stacking bottleneck features and minimumgeneration error training criterion --- to improve the performance of deepneural network (DNN)-based speech synthesis. The techniques address the relatedissues of frame-by-frame independence and ignorance of the relationship betweenstatic and dynamic features, within current typical DNN-based synthesisframeworks. Stacking bottleneck features, which are an acoustically--informedlinguistic representation, provides an efficient way to include more detailedlinguistic context at the input. The minimum generation error trainingcriterion minimises overall output trajectory error across an utterance, ratherthan minimising the error per frame independently, and thus takes into accountthe interaction between static and dynamic features. The two techniques can beeasily combined to further improve performance. We present both objective andsubjective results that demonstrate the effectiveness of the proposedtechniques. The subjective results show that combining the two techniques leadsto significantly more natural synthetic speech than from conventional DNN orlong short-term memory (LSTM) recurrent neural network (RNN) systems.
arxiv-16800-60 | Learning to Generate Posters of Scientific Papers | http://arxiv.org/pdf/1604.01219v1.pdf | author:Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal category:cs.AI cs.CL cs.HC cs.MM stat.ML published:2016-04-05 summary:Researchers often summarize their work in the form of posters. Postersprovide a coherent and efficient way to convey core ideas from scientificpapers. Generating a good scientific poster, however, is a complex and timeconsuming cognitive task, since such posters need to be readable, informative,and visually aesthetic. In this paper, for the first time, we study thechallenging problem of learning to generate posters from scientific papers. Tothis end, a data-driven framework, that utilizes graphical models, is proposed.Specifically, given content to display, the key elements of a good poster,including panel layout and attributes of each panel, are learned and inferredfrom data. Then, given inferred layout and attributes, composition of graphicalelements within each panel is synthesized. To learn and validate our model, wecollect and make public a Poster-Paper dataset, which consists of scientificpapers and corresponding posters with exhaustively labelled panels andattributes. Qualitative and quantitative results indicate the effectiveness ofour approach.
arxiv-16800-61 | Compilation as a Typed EDSL-to-EDSL Transformation | http://arxiv.org/pdf/1603.08865v2.pdf | author:Emil Axelsson category:cs.CL published:2016-03-29 summary:This article is about an implementation and compilation technique that isused in RAW-Feldspar which is a complete rewrite of the Feldspar embeddeddomain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-levelfunctional language that generates efficient C code to run on embedded targets.The gist of the technique presented in this post is the following: ratherwriting a back end that converts pure Feldspar expressions directly to C, wetranslate them to a low-level monadic EDSL. From the low-level EDSL, C code isthen generated. This approach has several advantages: 1. The translation is simpler to write than a complete C back end. 2. The translation is between two typed EDSLs, which rules out many potentialerrors. 3. The low-level EDSL is reusable and can be shared between severalhigh-level EDSLs. Although the article contains a lot of code, most of it is in fact reusable.As mentioned in Discussion, we can write the same implementation in less than50 lines of code using generic libraries that we have developed to supportFeldspar.
arxiv-16800-62 | Label Distribution Learning | http://arxiv.org/pdf/1408.6027v2.pdf | author:Xin Geng category:cs.LG published:2014-08-26 summary:Although multi-label learning can deal with many problems with labelambiguity, it does not fit some real applications well where the overalldistribution of the importance of the labels matters. This paper proposes anovel learning paradigm named \emph{label distribution learning} (LDL) for suchkind of applications. The label distribution covers a certain number of labels,representing the degree to which each label describes the instance. LDL is amore general learning framework which includes both single-label andmulti-label learning as its special cases. This paper proposes six working LDLalgorithms in three ways: problem transformation, algorithm adaptation, andspecialized algorithm design. In order to compare the performance of the LDLalgorithms, six representative and diverse evaluation measures are selected viaa clustering analysis, and the first batch of label distribution datasets arecollected and made publicly available. Experimental results on one artificialand fifteen real-world datasets show clear advantages of the specializedalgorithms, which indicates the importance of special design for thecharacteristics of the LDL problem.
arxiv-16800-63 | Transfer Prototype-based Fuzzy Clustering | http://arxiv.org/pdf/1409.5686v2.pdf | author:Zhaohong Deng, Yizhang Jiang, Fu-Lai Chung, Hisao Ishibuchi, Kup-Sze Choi, Shitong Wang category:cs.LG published:2014-09-19 summary:The traditional prototype based clustering methods, such as the well-knownfuzzy c-mean (FCM) algorithm, usually need sufficient data to find a goodclustering partition. If the available data is limited or scarce, most of theexisting prototype based clustering algorithms will no longer be effective.While the data for the current clustering task may be scarce, there is usuallysome useful knowledge available in the related scenes/domains. In this study,the concept of transfer learning is applied to prototype based fuzzy clustering(PFC). Specifically, the idea of leveraging knowledge from the source domain isexploited to develop a set of transfer prototype based fuzzy clustering (TPFC)algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM,fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have beenchosen to incorporate with knowledge leveraging mechanism to develop thecorresponding transfer clustering algorithms. Novel objective functions areproposed to integrate the knowledge of source domain with the data of targetdomain for clustering in the target domain. The proposed algorithms have beenvalidated on different synthetic and real-world datasets and the resultsdemonstrate their effectiveness when compared with both the original prototypebased fuzzy clustering algorithms and the related clustering algorithms likemulti-task clustering and co-clustering.
arxiv-16800-64 | Dueling Network Architectures for Deep Reinforcement Learning | http://arxiv.org/pdf/1511.06581v3.pdf | author:Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas category:cs.LG published:2015-11-20 summary:In recent years there have been many successes of using deep representationsin reinforcement learning. Still, many of these applications use conventionalarchitectures, such as convolutional networks, LSTMs, or auto-encoders. In thispaper, we present a new neural network architecture for model-freereinforcement learning. Our dueling network represents two separate estimators:one for the state value function and one for the state-dependent actionadvantage function. The main benefit of this factoring is to generalizelearning across actions without imposing any change to the underlyingreinforcement learning algorithm. Our results show that this architecture leadsto better policy evaluation in the presence of many similar-valued actions.Moreover, the dueling architecture enables our RL agent to outperform thestate-of-the-art on the Atari 2600 domain.
arxiv-16800-65 | Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks | http://arxiv.org/pdf/1604.01178v1.pdf | author:Aliaksei Severyn, Alessandro Moschitti category:cs.CL published:2016-04-05 summary:In this paper, we propose convolutional neural networks for learning anoptimal representation of question and answer sentences. Their main aspect isthe use of relational information given by the matches between words from thetwo members of the pair. The matches are encoded as embeddings with additionalparameters (dimensions), which are tuned by the network. These allows forbetter capturing interactions between questions and answers, resulting in asignificant boost in accuracy. We test our models on two widely used answersentence selection benchmarks. The results clearly show the effectiveness ofour relational information, which allows our relatively simple network toapproach the state of the art.
arxiv-16800-66 | Learning Local Descriptors by Optimizing the Keypoint-Correspondence Criterion | http://arxiv.org/pdf/1603.09095v2.pdf | author:Nenad Markuš, Igor S. Pandžić, Jörgen Ahlberg category:cs.CV published:2016-03-30 summary:Current best local descriptors are learned on a large dataset of matching andnon-matching keypoint pairs. However, data of this kind is not always availablesince detailed keypoint correspondences can be hard to establish. On the otherhand, we can often obtain labels for pairs of keypoint bags. For example,keypoint bags extracted from two images of the same object under differentviews form a matching pair, and keypoint bags extracted from images ofdifferent objects form a non-matching pair. On average, matching pairs shouldcontain more corresponding keypoints than non-matching pairs. We describe anend-to-end differentiable architecture that enables the learning of localkeypoint descriptors from such weakly-labeled data.
arxiv-16800-67 | Restricted Isometry Constants for Gaussian and Rademacher matrices | http://arxiv.org/pdf/1604.01171v1.pdf | author:Sandrine Dallaporta, Yohann de Castro category:math.ST cs.IT math.IT math.PR stat.ML stat.TH published:2016-04-05 summary:Restricted Isometry Constants (RICs) are a pivotal notion in CompressedSensing (CS) as these constants finely assess how a linear operator isconditioned on the set of sparse vectors and hence how it performs in stableand robust sparse regression (SRSR). While it is an open problem to constructdeterministic matrices with apposite RICs, one can prove that such matricesexist using random matrices models. One of the most popular model may be thesub-Gaussian matrices since it encompasses random matrices with Gaussian orRademacher i.i.d. entries. In this paper, we provide a description of the phasetransition on SRSR for those matrices using state-of-the-art (small) deviationestimates on their extreme eigenvalues. In particular, we show new upper boundson RICs for Gaussian and Rademacher matrices. This allows us to derive a newlower bound on the probability of getting SRSR. One of the benefit of thisnovel approach is to broaden the scope of phase transition on RICs and SRSR tothe quest of universality results in Random Matrix Theory.
arxiv-16800-68 | Fast Metric Learning For Deep Neural Networks | http://arxiv.org/pdf/1511.06442v5.pdf | author:Henry Gouk, Bernhard Pfahringer, Michael Cree category:cs.LG cs.CV stat.ML published:2015-11-19 summary:Similarity metrics are a core component of many information retrieval andmachine learning systems. In this work we propose a method capable of learninga similarity metric from data equipped with a binary relation. By consideringonly the similarity constraints, and initially ignoring the features, we areable to learn target vectors for each instance using one of severalappropriately designed loss functions. A regression model can then beconstructed that maps novel feature vectors to the same target vector space,resulting in a feature extractor that computes vectors for which a predefinedmetric is a meaningful measure of similarity. We present results on bothmulticlass and multi-label classification datasets that demonstrateconsiderably faster convergence, as well as higher accuracy on the majority ofthe intrinsic evaluation tasks and all extrinsic evaluation tasks.
arxiv-16800-69 | Staleness-aware Async-SGD for Distributed Deep Learning | http://arxiv.org/pdf/1511.05950v5.pdf | author:Wei Zhang, Suyog Gupta, Xiangru Lian, Ji Liu category:cs.LG published:2015-11-18 summary:Deep neural networks have been shown to achieve state-of-the-art performancein several machine learning tasks. Stochastic Gradient Descent (SGD) is thepreferred optimization algorithm for training these networks and asynchronousSGD (ASGD) has been widely adopted for accelerating the training of large-scaledeep networks in a distributed computing environment. However, in practice itis quite challenging to tune the training hyperparameters (such as learningrate) when using ASGD so as achieve convergence and linear speedup, since thestability of the optimization algorithm is strongly influenced by theasynchronous nature of parameter updates. In this paper, we propose a variantof the ASGD algorithm in which the learning rate is modulated according to thegradient staleness and provide theoretical guarantees for convergence of thisalgorithm. Experimental verification is performed on commonly-used imageclassification benchmarks: CIFAR10 and Imagenet to demonstrate the superioreffectiveness of the proposed approach, compared to SSGD (Synchronous SGD) andthe conventional ASGD algorithm.
arxiv-16800-70 | Less is more: zero-shot learning from online textual documents with noise suppression | http://arxiv.org/pdf/1604.01146v1.pdf | author:Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-04-05 summary:Classifying a visual concept merely from its associated online textualsource, such as a Wikipedia article, is an attractive research topic inzero-shot learning because it alleviates the burden of manually collectingsemantic attributes. Several recent works have pursued this approach byexploring various ways of connecting the visual and text domains. This paperrevisits this idea by stepping further to consider one important factor: thetextual representation is usually too noisy for the zero-shot learningapplication. This consideration motivates us to design a simple-but-effectivezero-shot learning method capable of suppressing noise in the text. More specifically, we propose an $l_{2,1}$-norm based objective functionwhich can simultaneously suppress the noisy signal in the text and learn afunction to match the text document and visual features. We also develop anoptimization algorithm to efficiently solve the resulting problem. Byconducting experiments on two large datasets, we demonstrate that the proposedmethod significantly outperforms the competing methods which rely on onlineinformation sources but without explicit noise suppression. We further make anin-depth analysis of the proposed method and provide insight as to what kind ofinformation in documents is useful for zero-shot learning.
arxiv-16800-71 | Visualizing Large-scale and High-dimensional Data | http://arxiv.org/pdf/1602.00370v2.pdf | author:Jian Tang, Jingzhou Liu, Ming Zhang, Qiaozhu Mei category:cs.LG cs.HC published:2016-02-01 summary:We study the problem of visualizing large-scale and high-dimensional data ina low-dimensional (typically 2D or 3D) space. Much success has been reportedrecently by techniques that first compute a similarity structure of the datapoints and then project them into a low-dimensional space with the structurepreserved. These two steps suffer from considerable computational costs,preventing the state-of-the-art methods such as the t-SNE from scaling tolarge-scale and high-dimensional data (e.g., millions of data points andhundreds of dimensions). We propose the LargeVis, a technique that firstconstructs an accurately approximated K-nearest neighbor graph from the dataand then layouts the graph in the low-dimensional space. Comparing to t-SNE,LargeVis significantly reduces the computational cost of the graph constructionstep and employs a principled probabilistic model for the visualization step,the objective of which can be effectively optimized through asynchronousstochastic gradient descent with a linear time complexity. The whole procedurethus easily scales to millions of high-dimensional data points. Experimentalresults on real-world data sets demonstrate that the LargeVis outperforms thestate-of-the-art methods in both efficiency and effectiveness. Thehyper-parameters of LargeVis are also much more stable over different datasets.
arxiv-16800-72 | Partial Membership Latent Dirichlet Allocation | http://arxiv.org/pdf/1511.02821v2.pdf | author:Chao Chen, Alina Zare, J. Tory Cobb category:stat.ML cs.CV published:2015-11-09 summary:Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmentingimagery. These models are confined to crisp segmentation. Yet, there are manyimages in which some regions cannot be assigned a crisp label (e.g., transitionregions between a foggy sky and the ground or between sand and water at abeach). In these cases, a visual word is best represented with partialmemberships across multiple topics. To address this, we present a partialmembership latent Dirichlet allocation (PM-LDA) model and associated parameterestimation algorithms. Experimental results on two natural image datasets andone SONAR image dataset show that PM-LDA can produce both crisp and softsemantic image segmentations; a capability existing methods do not have.
arxiv-16800-73 | Viewpoint Invariant 3D Human Pose Estimation with Recurrent Error Feedback | http://arxiv.org/pdf/1603.07076v2.pdf | author:Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, Li Fei-Fei category:cs.CV published:2016-03-23 summary:We propose a viewpoint invariant model for 3D human pose estimation from asingle depth image. To achieve viewpoint invariance, our deep discriminativemodel embeds local regions into a learned viewpoint invariant feature space.Formulated as a multi-task learning problem, our model is able to selectivelypredict partial poses in the presence of noise and occlusion. Our approachleverages a convolutional and recurrent network with a top-down error feedbackmechanism to self-correct previous pose estimates in an end-to-end manner. Weevaluate our model on a previously published depth dataset and a newlycollected human pose dataset containing 100K annotated depth images fromextreme viewpoints. Experiments show that our model achieves competitiveperformance on frontal views while achieving state-of-the-art performance onalternate viewpoints.
arxiv-16800-74 | Counting Grid Aggregation for Event Retrieval and Recognition | http://arxiv.org/pdf/1604.01109v1.pdf | author:Zhanning Gao, Gang Hua, Dongqing Zhang, Jianru Xue, Nanning Zheng category:cs.CV published:2016-04-05 summary:Event retrieval and recognition in a large corpus of videos necessitates aholistic fixed-size visual representation at the video clip level that iscomprehensive, compact, and yet discriminative. It shall comprehensivelyaggregate information across relevant video frames, while suppress redundantinformation, leading to a compact representation that can effectivelydifferentiate among different visual events. In search for such arepresentation, we propose to build a spatially consistent counting grid modelto aggregate together deep features extracted from different video frames. Thespatial consistency of the counting grid model is achieved by introducing aprior model estimated from a large corpus of video data. The counting gridmodel produces an intermediate tensor representation for each video, whichautomatically identifies and removes the feature redundancy across thedifferent frames. The tensor representation is subsequently reduced to afixed-size vector representation by averaging over the counting grid. Whencompared to existing methods on both event retrieval and event classificationbenchmarks, we achieve significantly better accuracy with much more compactrepresentation.
arxiv-16800-75 | Science Question Answering using Instructional Materials | http://arxiv.org/pdf/1602.04375v2.pdf | author:Mrinmaya Sachan, Avinava Dubey, Eric P. Xing category:cs.CL cs.AI cs.IR cs.LG published:2016-02-13 summary:We provide a solution for elementary science test using instructionalmaterials. We posit that there is a hidden structure that explains thecorrectness of an answer given the question and instructional materials andpresent a unified max-margin framework that learns to find these hiddenstructures (given a corpus of question-answer pairs and instructionalmaterials), and uses what it learns to answer novel elementary sciencequestions. Our evaluation shows that our framework outperforms several strongbaselines.
arxiv-16800-76 | BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration | http://arxiv.org/pdf/1604.01093v1.pdf | author:Angela Dai, Matthias Nießner, Michael Zollhöfer, Shahram Izadi, Christian Theobalt category:cs.GR cs.CV published:2016-04-05 summary:Real-time, high-quality, 3D scanning of large-scale scenes is key to mixedreality and robotic applications. However, scalability brings challenges ofdrift in pose estimation, introducing significant errors in the accumulatedmodel. Approaches often require hours of offline processing to globally correctmodel errors. Recent online methods demonstrate compelling results, but sufferfrom: (1) needing minutes to perform online correction preventing truereal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimationresulting in many tracking failures; or (3) supporting only unstructuredpoint-based representations, which limit scan quality and applicability. Wesystematically address these issues with a novel, real-time, end-to-endreconstruction framework. At its core is a robust pose estimation strategy,optimizing per frame for a global set of camera poses by considering thecomplete history of RGB-D input with an efficient hierarchical approach. Weremove the heavy reliance on temporal tracking, and continually localize to theglobally optimized frames instead. We contribute a parallelizable optimizationframework, which employs correspondences based on sparse features and densegeometric and photometric matching. Our approach estimates globally optimized(i.e., bundle adjusted) poses in real-time, supports robust tracking withrecovery from gross tracking failures (i.e., relocalization), and re-estimatesthe 3D model in real-time to ensure global consistency; all within a singleframework. Our approach outperforms state-of-the-art online systems withquality on par to offline methods, but with unprecedented speed and scancompleteness. Our framework leads to a comprehensive online scanning solutionfor large indoor environments, enabling ease of use and high-quality results.
arxiv-16800-77 | Relative Deviation Learning Bounds and Generalization with Unbounded Loss Functions | http://arxiv.org/pdf/1310.5796v4.pdf | author:Corinna Cortes, Spencer Greenberg, Mehryar Mohri category:cs.LG published:2013-10-22 summary:We present an extensive analysis of relative deviation bounds, includingdetailed proofs of two-sided inequalities and their implications. We also givedetailed proofs of two-sided generalization bounds that hold in the generalcase of unbounded loss functions, under the assumption that a moment of theloss is bounded. These bounds are useful in the analysis of importanceweighting and other learning tasks such as unbounded regression.
arxiv-16800-78 | Optimal Parameter Settings for the $(1+(λ, λ))$ Genetic Algorithm | http://arxiv.org/pdf/1604.01088v1.pdf | author:Benjamin Doerr category:cs.NE cs.DS published:2016-04-04 summary:The $(1+(\lambda,\lambda))$ genetic algorithm is one of the few algorithmsfor which a super-constant speed-up through the use of crossover could beproven. So far, this algorithm has been used with parameters based also onintuitive considerations. In this work, we rigorously regard the wholeparameter space and show that the asymptotic time complexity proven by Doerrand Doerr (GECCO 2015) for the intuitive choice is best possible among allsettings for population size, mutation probability, and crossover bias.
arxiv-16800-79 | Stochastic Variance Reduction for Nonconvex Optimization | http://arxiv.org/pdf/1603.06160v2.pdf | author:Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG cs.NE stat.ML published:2016-03-19 summary:We study nonconvex finite-sum problems and analyze stochastic variancereduced gradient (SVRG) methods for them. SVRG and related methods haverecently surged into prominence for convex optimization given their edge overstochastic gradient descent (SGD); but their theoretical analysis almostexclusively assumes convexity. In contrast, we prove non-asymptotic rates ofconvergence (to stationary points) of SVRG for nonconvex optimization, and showthat it is provably faster than SGD and gradient descent. We also analyze asubclass of nonconvex problems on which SVRG attains linear convergence to theglobal optimum. We extend our analysis to mini-batch variants of SVRG, showing(theoretical) linear speedup due to mini-batching in parallel settings.
arxiv-16800-80 | A Dynamic Bayesian Network Model for Inventory Level Estimation in Retail Marketing | http://arxiv.org/pdf/1604.01075v1.pdf | author:Luis I. Reyes-Castro, Andres G. Abad category:stat.ML published:2016-04-04 summary:Many retailers today employ inventory management systems based on Re-OrderPoint Policies, most of which rely on the assumption that all decreases inproduct inventory levels result from product sales. Unfortunately, it usuallyhappens that small but random quantities of the product get lost, stolen orbroken without record as time passes, e.g., as a consequence of shoplifting.This is usual for retailers handling large varieties of inexpensive products,e.g., grocery stores. In turn, over time these discrepancies lead to stockfreezing problems, i.e., situations where the system believes the stock isabove the re-order point but the actual stock is at zero, and so noreplenishments or sales occur. Motivated by these issues, we model theinteraction between sales, losses, replenishments and inventory levels as aDynamic Bayesian Network (DBN), where the inventory levels are unobserved(i.e., hidden) variables we wish to estimate. We present anExpectation-Maximization (EM) algorithm to estimate the parameters of the saleand loss distributions, which relies on solving a one-dimensional dynamicprogram for the E-step and on solving two separate one-dimensional nonlinearprograms for the M-step.
arxiv-16800-81 | Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs | http://arxiv.org/pdf/1602.04912v2.pdf | author:Dionysios S. Kalogerias, Athina P. Petropulu category:math.ST math.OC stat.ML stat.TH published:2016-02-16 summary:In this work, we study stability of distributed filtering of Markov chainswith finite state space, partially observed in conditionally Gaussian noise. Weconsider a nonlinear filtering scheme over a Distributed Network of Agents(DNA), which relies on the distributed evaluation of the likelihood part of thecentralized nonlinear filter and is based on a particular specialization of theAlternating Direction Method of Multipliers (ADMM) for fast average consensus.Assuming the same number of consensus steps between any two consecutive noisymeasurements for each sensor in the network, we fully characterize a minimalnumber of such steps, such that the distributed filter remains uniformly stablewith a prescribed accuracy level, {\varepsilon} \in (0,1], within a finiteoperational horizon, T, and across all sensors. Stability is in the sense ofthe of the \ell_1-norm between the centralized and distributed versions of theposterior at each sensor, and at each time within T. Roughly speaking, our mainresult shows that uniform {\varepsilon}-stability of the distributed filteringprocess depends only loglinearly on T and (roughly) the size of the network,and only logarithmically on 1/{\varepsilon}. If this total loglinear bound isfulfilled, any additional consensus iterations will incur a fully quantifiedfurther exponential decay in the consensus error. Our bounds are universal, inthe sense that they are independent of the particular structure of the GaussianHidden Markov Model (HMM) under consideration.
arxiv-16800-82 | Direct Visual Odometry using Bit-Planes | http://arxiv.org/pdf/1604.00990v1.pdf | author:Hatem Alismail, Brett Browning, Simon Lucey category:cs.RO cs.CV published:2016-04-04 summary:Feature descriptors, such as SIFT and ORB, are well-known for theirrobustness to illumination changes, which has made them popular forfeature-based VSLAM\@. However, in degraded imaging conditions such as lowlight, low texture, blur and specular reflections, feature extraction is oftenunreliable. In contrast, direct VSLAM methods which estimate the camera pose byminimizing the photometric error using raw pixel intensities are often morerobust to low textured environments and blur. Nonetheless, at the core ofdirect VSLAM is the reliance on a consistent photometric appearance acrossimages, otherwise known as the brightness constancy assumption. Unfortunately,brightness constancy seldom holds in real world applications. In this work, we overcome brightness constancy by incorporating featuredescriptors into a direct visual odometry framework. This combination resultsin an efficient algorithm that combines the strength of both feature-basedalgorithms and direct methods. Namely, we achieve robustness to arbitraryphotometric variations while operating in low-textured and poorly litenvironments. Our approach utilizes an efficient binary descriptor, which wecall Bit-Planes, and show how it can be used in the gradient-based optimizationrequired by direct methods. Moreover, we show that the squared Euclideandistance between Bit-Planes is equivalent to the Hamming distance. Hence, thedescriptor may be used in least squares optimization without sacrificing itsphotometric invariance. Finally, we present empirical results that demonstratethe robustness of the approach in poorly lit underground environments.
arxiv-16800-83 | Clustering Millions of Faces by Identity | http://arxiv.org/pdf/1604.00989v1.pdf | author:Charles Otto, Dayong Wang, Anil K. Jain category:cs.CV published:2016-04-04 summary:In this work, we attempt to address the following problem: Given a largenumber of unlabeled face images, cluster them into the individual identitiespresent in this data. We consider this a relevant problem in differentapplication scenarios ranging from social media to law enforcement. Inlarge-scale scenarios the number of faces in the collection can be of the orderof hundreds of million, while the number of clusters can range from a fewthousand to millions--leading to difficulties in terms of both run-timecomplexity and evaluating clustering and per-cluster quality. An efficient andeffective Rank-Order clustering algorithm is developed to achieve the desiredscalability, and better clustering accuracy than other well-known algorithmssuch as k-means and spectral clustering. We cluster up to 123 million faceimages into over 10 million clusters, and analyze the results in terms of bothexternal cluster quality measures (known face labels) and internal clusterquality measures (unknown face labels) and run-time. Our algorithm achieves anF-measure of 0.87 on a benchmark unconstrained face dataset (LFW, consisting of13K faces), and 0.27 on the largest dataset considered (13K images in LFW, plus123M distractor images). Additionally, we present preliminary work on videoframe clustering (achieving 0.71 F-measure when clustering all frames in thebenchmark YouTube Faces dataset). A per-cluster quality measure is developedwhich can be used to rank individual clusters and to automatically identify asubset of good quality clusters for manual exploration.
arxiv-16800-84 | Deep Networks with Stochastic Depth | http://arxiv.org/pdf/1603.09382v2.pdf | author:Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger category:cs.LG cs.CV cs.NE published:2016-03-30 summary:Very deep convolutional networks with hundreds or more layers have lead tosignificant reductions in error on competitive benchmarks like the ImageNet orCOCO tasks. Although the unmatched expressiveness of the many deep layers canbe highly desirable at test time, training very deep networks comes with itsown set of challenges. The gradients can vanish, the forward flow oftendiminishes and the training time can be painfully slow even on moderncomputers. In this paper we propose stochastic depth, a training procedure thatenables the seemingly contradictory setup to train short networks and obtaindeep networks. We start with very deep networks but during training, for eachmini-batch, randomly drop a subset of layers and bypass them with the identityfunction. The resulting networks are short (in expectation) during training anddeep during testing. Training Residual Networks with stochastic depth iscompellingly simple to implement, yet effective. We show that this approachsuccessfully addresses the training difficulties of deep networks andcomplements the recent success of Residual and Highway Networks. It reducestraining time substantially and improves the test errors on almost all datasets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, with stochasticdepth we can increase the depth of residual networks even beyond 1200 layersand still yield meaningful improvements in test error (4.91%) on CIFAR-10.
arxiv-16800-85 | Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.00974v1.pdf | author:Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira category:cs.CV stat.ML published:2016-04-04 summary:Automatic Offline Handwritten Signature Verification has been researched overthe last few decades from several perspectives, using insights from graphology,computer vision, signal processing, among others. In spite of the advancementson the field, building classifiers that can separate between genuine signaturesand skilled forgeries (forgeries made targeting a particular signature) isstill hard. We propose approaching the problem from a feature learningperspective. Our hypothesis is that, in the absence of a good model of the datageneration process, it is better to learn the features from data, instead ofusing hand-crafted features that have no resemblance to the signaturegeneration process. To this end, we use Deep Convolutional Neural Networks tolearn features in a writer-independent format, and use this model to obtain afeature representation on another set of users, where we train writer-dependentclassifiers. We tested our method in two datasets: GPDS-960 and BrazilianPUC-PR. Our experimental results show that the features learned in a subset ofthe users are discriminative for the other users, including across differentdatasets, reaching close to the state-of-the-art in the GPDS dataset, andimproving the state-of-the-art in the Brazilian PUC-PR dataset.
arxiv-16800-86 | Part-of-Speech Tagging for Historical English | http://arxiv.org/pdf/1603.03144v2.pdf | author:Yi Yang, Jacob Eisenstein category:cs.CL cs.DL published:2016-03-10 summary:As more historical texts are digitized, there is interest in applying naturallanguage processing tools to these archives. However, the performance of thesetools is often unsatisfactory, due to language change and genre differences.Spelling normalization heuristics are the dominant solution for dealing withhistorical texts, but this approach fails to account for changes in usage andvocabulary. In this empirical paper, we assess the capability of domainadaptation techniques to cope with historical texts, focusing on the classicbenchmark task of part-of-speech tagging. We evaluate several domain adaptationmethods on the task of tagging Early Modern English and Modern British Englishtexts in the Penn Corpora of Historical English. We demonstrate that theFeature Embedding method for unsupervised domain adaptation outperforms wordembeddings and Brown clusters, showing the importance of embedding the entirefeature space, rather than just individual words. Feature Embeddings also givebetter performance than spelling normalization, but the combination of the twomethods is better still, yielding a 5% raw improvement in tagging accuracy onEarly Modern English texts.
arxiv-16800-87 | Partial Recovery Bounds for the Sparse Stochastic Block Model | http://arxiv.org/pdf/1602.00877v2.pdf | author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.SI math.IT stat.ML published:2016-02-02 summary:In this paper, we study the information-theoretic limits of communitydetection in the symmetric two-community stochastic block model, withintra-community and inter-community edge probabilities $\frac{a}{n}$ and$\frac{b}{n}$ respectively. We consider the sparse setting, in which $a$ and$b$ do not scale with $n$, and provide upper and lower bounds on the proportionof community labels recovered on average. We provide a numerical example forwhich the bounds are near-matching for moderate values of $a - b$, and matchingin the limit as $a-b$ grows large.
arxiv-16800-88 | Multi-Field Structural Decomposition for Question Answering | http://arxiv.org/pdf/1604.00938v1.pdf | author:Tomasz Jurczyk, Jinho D. Choi category:cs.CL published:2016-04-04 summary:This paper presents a precursory yet novel approach to the question answeringtask using structural decomposition. Our system first generates linguisticstructures such as syntactic and semantic trees from text, decomposes them intomultiple fields, then indexes the terms in each field. For each question, itdecomposes the question into multiple fields, measures the relevance score ofeach field to the indexed ones, then ranks all documents by their relevancescores and weights associated with the fields, where the weights are learnedthrough statistical modeling. Our final model gives an absolute improvement ofover 40% to the baseline approach using simple search for detecting documentscontaining answers.
arxiv-16800-89 | Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding | http://arxiv.org/pdf/1604.00933v1.pdf | author:Walid Shalaby, Khalifeh Al Jadda, Mohammed Korayem, Trey Grainger category:cs.CL cs.IR published:2016-04-04 summary:We present an ensemble approach for categorizing search query entities in therecruitment domain. Understanding the types of entities expressed in a searchquery (Company, Skill, Job Title, etc.) enables more intelligent informationretrieval based upon those entities compared to a traditional keyword-basedsearch. Because search queries are typically very short, leveraging atraditional bag-of-words model to identify entity types would be inappropriatedue to the lack of contextual information. Our approach instead combines cluesfrom different sources of varying complexity in order to collect real-worldknowledge about query entities. We employ distributional semanticrepresentations of query entities through two models: 1) contextual vectorsgenerated from encyclopedic corpora like Wikipedia, and 2) high dimensionalword embedding vectors generated from millions of job postings using word2vec.Additionally, our approach utilizes both entity linguistic properties obtainedfrom WordNet and ontological properties extracted from DBpedia. We evaluate ourapproach on a data set created at CareerBuilder; the largest job board in theUS. The data set contains entities extracted from millions of jobseekers/recruiters search queries, job postings, and resume documents. Afterconstructing the distributional vectors of search entities, we use supervisedmachine learning to infer search entity types. Empirical results show that ourapproach outperforms the state-of-the-art word2vec distributional semanticsmodel trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of97% using the proposed distributional representations ensemble.
arxiv-16800-90 | Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning | http://arxiv.org/pdf/1604.00923v1.pdf | author:Philip S. Thomas, Emma Brunskill category:cs.LG cs.AI published:2016-04-04 summary:In this paper we present a new way of predicting the performance of areinforcement learning policy given historical data that may have beengenerated by a different policy. The ability to evaluate a policy fromhistorical data is important for applications where the deployment of a badpolicy can be dangerous or costly. We show empirically that our algorithmproduces estimates that often have orders of magnitude lower mean squared errorthan existing methods---it makes more efficient use of the available data. Ournew estimator is based on two advances: an extension of the doubly robustestimator (Jiang and Li, 2015), and a new way to mix between model basedestimates and importance sampling based estimates.
arxiv-16800-91 | Detecting Engagement in Egocentric Video | http://arxiv.org/pdf/1604.00906v1.pdf | author:Yu-Chuan Su, Kristen Grauman category:cs.CV published:2016-04-04 summary:In a wearable camera video, we see what the camera wearer sees. While thismakes it easy to know roughly what he chose to look at, it does not immediatelyreveal when he was engaged with the environment. Specifically, at what momentsdid his focus linger, as he paused to gather more information about somethinghe saw? Knowing this answer would benefit various applications in videosummarization and augmented reality, yet prior work focuses solely on the"what" question (estimating saliency, gaze) without considering the "when"(engagement). We propose a learning-based approach that uses long-termegomotion cues to detect engagement, specifically in browsing scenarios whereone frequently takes in new visual information (e.g., shopping, touring). Weintroduce a large, richly annotated dataset for ego-engagement that is thefirst of its kind. Our approach outperforms a wide array of existing methods.We show engagement can be detected well independent of both scene appearanceand the camera wearer's identity.
arxiv-16800-92 | HDRFusion: HDR SLAM using a low-cost auto-exposure RGB-D sensor | http://arxiv.org/pdf/1604.00895v1.pdf | author:Shuda Li, Ankur Handa, Yang Zhang, Andrew Calway category:cs.CV published:2016-04-04 summary:We describe a new method for comparing frame appearance in a frame-to-model3-D mapping and tracking system using an low dynamic range (LDR) RGB-D camerawhich is robust to brightness changes caused by auto exposure. It is based on anormalised radiance measure which is invariant to exposure changes and not onlyrobustifies the tracking under changing lighting conditions, but also enablesthe following exposure compensation perform accurately to allow online buildingof high dynamic range (HDR) maps. The latter facilitates the frame-to-modeltracking to minimise drift as well as better capturing light variation withinthe scene. Results from experiments with synthetic and real data demonstratethat the method provides both improved tracking and maps with far greaterdynamic range of luminosity.
arxiv-16800-93 | Comparing Convolutional Neural Networks to Traditional Models for Slot Filling | http://arxiv.org/pdf/1603.05157v2.pdf | author:Heike Adel, Benjamin Roth, Hinrich Schütze category:cs.CL published:2016-03-16 summary:We address relation classification in the context of slot filling, the taskof finding and evaluating fillers like "Steve Jobs" for the slot X in "Xfounded Apple". We propose a convolutional neural network which splits theinput sentence into three parts according to the relation arguments and compareit to state-of-the-art and traditional approaches of relation classification.Finally, we combine different methods and show that the combination is betterthan individual approaches. We also analyze the effect of genre differences onperformance.
arxiv-16800-94 | Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings | http://arxiv.org/pdf/1604.00861v1.pdf | author:Giambattista Parascandolo, Heikki Huttunen, Tuomas Virtanen category:cs.SD cs.LG cs.NE published:2016-04-04 summary:In this paper we present an approach to polyphonic sound event detection inreal life recordings based on bi-directional long short term memory (BLSTM)recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained tomap acoustic features of a mixture signal consisting of sounds from multipleclasses, to binary activity indicators of each event class. Our method istested on a large database of real-life recordings, with 61 classes (e.g.music, car, speech) from 10 different everyday contexts. The proposed methodoutperforms previous approaches by a large margin, and the results are furtherimproved using data augmentation techniques. Overall, our system reports anaverage F1-score of 65.5% on 1 second blocks and 64.7% on single frames, arelative improvement over previous state-of-the-art approach of 6.8% and 15.1%respectively.
arxiv-16800-95 | Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms | http://arxiv.org/pdf/1604.01376v1.pdf | author:Valentina Zantedeschi, Rémi Emonet, Marc Sebban category:cs.NA cs.LG published:2016-04-04 summary:Many theoretical results in the machine learning domain stand only forfunctions that are Lipschitz continuous. Lipschitz continuity is a strong formof continuity that linearly bounds the variations of a function. In this paper,we derive tight Lipschitz constants for two families of metrics: Mahalanobisdistances and bounded-space bilinear forms. To our knowledge, this is the firsttime the Mahalanobis distance is formally proved to be Lipschitz continuous andthat such tight Lipschitz constants are derived.
arxiv-16800-96 | In narrative texts punctuation marks obey the same statistics as words | http://arxiv.org/pdf/1604.00834v1.pdf | author:Andrzej Kulig, Jaroslaw Kwapien, Tomasz Stanisz, Stanislaw Drozdz category:cs.CL published:2016-04-04 summary:From a grammar point of view, the role of punctuation marks in a sentence isformally defined and well understood. In semantic analysis punctuation playsalso a crucial role as a method of avoiding ambiguity of the meaning. Adifferent situation can be observed in the statistical analyses of languagesamples, where the decision on whether the punctuation marks should beconsidered or should be neglected is seen rather as arbitrary and at present itbelongs to a researcher's preference. An objective of this work is to shed somelight onto this problem by providing us with an answer to the question whetherthe punctuation marks may be treated as ordinary words and whether they shouldbe included in any analysis of the word co-occurences. We already know from ourprevious study \cite{drozdz2016} that full stops that determine the length ofsentences are the main carrier of long-range correlations. Now we extend thatstudy and analyze statistical properties of the most common punctuation marksin a few Indo-European languages, investigate their frequencies, and locatethem accordingly in the Zipf rank-frequency plots as well as study their rolein the word-adjacency networks. We show that, from a statistical viewpoint, thepunctuation marks reveal properties that are qualitatively similar to theproperties of the most frequent words like articles, conjunctions, pronouns,and prepositions. This refers to both the Zipfian analysis and the networkanalysis. Our results can be exploited in the computer-based analyses of largetext corpora and be incorporated in the related automated systems. As a sideresult, we propose an efficient method of sampling the language corpora for aword-adjacency network analysis.
arxiv-16800-97 | Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers | http://arxiv.org/pdf/1604.00825v1.pdf | author:Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller, Wojciech Samek category:cs.CV published:2016-04-04 summary:Layer-wise relevance propagation is a framework which allows to decompose theprediction of a deep neural network computed over a sample, e.g. an image, downto relevance scores for the single input dimensions of the sample such assubpixels of an image. While this approach can be applied directly togeneralized linear mappings, product type non-linearities are not covered. Thispaper proposes an approach to extend layer-wise relevance propagation to neuralnetworks with local renormalization layers, which is a very common product-typenon-linearity in convolutional neural networks. We evaluate the proposed methodfor local renormalization layers on the CIFAR-10, Imagenet and MIT Placesdatasets.
arxiv-16800-98 | Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors | http://arxiv.org/pdf/1603.04733v3.pdf | author:Christos Louizos, Max Welling category:stat.ML cs.LG published:2016-03-15 summary:We introduce a variational Bayesian neural network where the parameters aregoverned via a probability distribution on random matrices. Specifically, weemploy a matrix variate Gaussian \cite{gupta1999matrix} parameter posteriordistribution where we explicitly model the covariance among the input andoutput dimensions of each layer. Furthermore, with approximate covariancematrices we can achieve a more efficient way to represent those correlationsthat is also cheaper than fully factorized parameter posteriors. We furthershow that with the "local reprarametrization trick"\cite{kingma2015variational} on this posterior distribution we arrive at aGaussian Process \cite{rasmussen2006gaussian} interpretation of the hiddenunits in each layer and we, similarly with \cite{gal2015dropout}, provideconnections with deep Gaussian processes. We continue in taking advantage ofthis duality and incorporate "pseudo-data" \cite{snelson2005sparse} in ourmodel, which in turn allows for more efficient sampling while maintaining theproperties of the original model. The validity of the proposed approach isverified through extensive experiments.
arxiv-16800-99 | Deep Reinforcement Learning in Large Discrete Action Spaces | http://arxiv.org/pdf/1512.07679v2.pdf | author:Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, Ben Coppin category:cs.AI cs.LG cs.NE stat.ML published:2015-12-24 summary:Being able to reason in an environment with a large number of discreteactions is essential to bringing reinforcement learning to a larger class ofproblems. Recommender systems, industrial plants and language models are onlysome of the many real-world tasks involving large numbers of discrete actionsfor which current methods are difficult or even often impossible to apply. Anability to generalize over the set of actions as well as sub-linear complexityrelative to the size of the set are both necessary to handle such tasks.Current approaches are not able to provide both of these, which motivates thework in this paper. Our proposed approach leverages prior information about theactions to embed them in a continuous space upon which it can generalize.Additionally, approximate nearest-neighbor methods allow for logarithmic-timelookup complexity relative to the number of actions, which is necessary fortime-wise tractable training. This combined approach allows reinforcementlearning methods to be applied to large-scale learning problems previouslyintractable with current methods. We demonstrate our algorithm's abilities on aseries of tasks having up to one million actions.
arxiv-16800-100 | Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models | http://arxiv.org/pdf/1412.7461v2.pdf | author:Aki Vehtari, Tommi Mononen, Ville Tolvanen, Tuomas Sivula, Ole Winther category:stat.CO stat.ML published:2014-12-23 summary:The future predictive performance of a Bayesian model can be estimated usingBayesian cross-validation. In this article, we consider Gaussian latentvariable models where the integration over the latent values is approximatedusing the Laplace method or expectation propagation (EP). We study theproperties of several Bayesian leave-one-out (LOO) cross-validationapproximations that in most cases can be computed with a small additional costafter forming the posterior approximation given the whole data. Our mainobjective is to assess the accuracy of the approximative LOO cross-validationestimators. That is, for each method (Laplace and EP) we compare theapproximate fast computation with the exact brute force LOO computation.Secondarily, we evaluate the accuracy of the Laplace and EP approximationsthemselves against a ground truth established through extensive Markov chainMonte Carlo simulation. Our empirical results show that the approach based upona Gaussian approximation to the LOO marginal distribution (the so-called cavitydistribution) gives the most accurate and reliable results among the fastmethods.
arxiv-16800-101 | Deep Learning in Bioinformatics | http://arxiv.org/pdf/1603.06430v4.pdf | author:Seonwoo Min, Byunghan Lee, Sungroh Yoon category:cs.LG q-bio.GN published:2016-03-21 summary:As we are living in the era of big data, transforming biomedical big datainto valuable knowledge has been one of the most important problems inbioinformatics. At the same time, deep learning has advanced rapidly sinceearly 2000s and is recently showing a state-of-the-art performance in variousfields. So naturally, applying deep learning in bioinformatics to gain insightsfrom data is under the spotlight of both the academia and the industry. Thisarticle reviews some research of deep learning in bioinformatics. To provide abig picture, we categorized the research by both bioinformatics domains (i.e.,omics, biomedical imaging, biomedical signal processing) and deep learningarchitectures (i.e., deep neural network, convolutional neural network,recurrent neural network, modified neural network) as well as present briefdescriptions of each work. Additionally, we introduce a few issues of deeplearning in bioinformatics such as problems of class imbalance data and suggestfuture research directions such as multimodal deep learning. We believe thatthis paper could provide valuable insights and be a starting point forresearchers to apply deep learning in their bioinformatics studies.
arxiv-16800-102 | Image Captioning with Deep Bidirectional LSTMs | http://arxiv.org/pdf/1604.00790v1.pdf | author:Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel category:cs.CV cs.CL cs.MM published:2016-04-04 summary:This work presents an end-to-end trainable deep bidirectional LSTM(Long-Short Term Memory) model for image captioning. Our model builds on a deepconvolutional neural network (CNN) and two separate LSTM networks. It iscapable of learning long term visual-language interactions by making use ofhistory and future context information at high level semantic space. Two noveldeep bidirectional variant models, in which we increase the depth ofnonlinearity transition in different way, are proposed to learn hierarchicalvisual-language embeddings. Data augmentation techniques such as multi-crop,multi-scale and vertical mirror are proposed to prevent overfitting in trainingdeep models. We visualize the evolution of bidirectional LSTM internal statesover time and qualitatively analyze how our models "translate" image tosentence. Our proposed models are evaluated on caption generation andimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM modelsachieve highly competitive performance to the state-of-the-art results oncaption generation even without integrating additional mechanism (e.g. objectdetection, attention model etc.) and significantly outperform recent methods onretrieval task.
arxiv-16800-103 | Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models | http://arxiv.org/pdf/1604.00788v1.pdf | author:Minh-Thang Luong, Christopher D. Manning category:cs.CL cs.LG published:2016-04-04 summary:Nearly all previous work in neural machine translation (NMT) has used quiterestricted vocabularies, perhaps with a subsequent method to patch in unknownwords. This paper presents a novel word-character solution to achieving openvocabulary NMT. We build hybrid systems that translate mostly at the word leveland consult the character components for rare words. Our character-levelrecurrent neural networks compute source word representations and recoverunknown target words when needed. The two-fold advantage of such a hybridapproach is that it is much faster and easier to train than character-basedones; at the same time, it never produces unknown words as in the case ofword-based models. On the WMT'15 English to Czech translation task, this hybridapproach offers a boost of up to +7.9 BLEU points over models that do nothandle unknown words. Our best hybrid system has established a newstate-of-the-art result with 19.9 BLEU score. We demonstrate that our charactermodels can successfully learn to not only generate well-formed words for Czech,a highly-inflected language with a very complex vocabulary, but also buildcorrect representations for English source words.
arxiv-16800-104 | Topic Model Based Multi-Label Classification from the Crowd | http://arxiv.org/pdf/1604.00783v1.pdf | author:Divya Padmanabhan, Satyanath Bhat, Shirish Shevade, Y. Narahari category:cs.LG published:2016-04-04 summary:Multi-label classification is a common supervised machine learning problemwhere each instance is associated with multiple classes. The key challenge inthis problem is learning the correlations between the classes. An additionalchallenge arises when the labels of the training instances are provided bynoisy, heterogeneous crowdworkers with unknown qualities. We first assumelabels from a perfect source and propose a novel topic model where the presentas well as the absent classes generate the latent topics and hence the words.We non-trivially extend our topic model to the scenario where the labels areprovided by noisy crowdworkers. Extensive experimentation on real worlddatasets reveals the superior performance of the proposed model. The proposedmodel learns the qualities of the annotators as well, even with minimaltraining data.
arxiv-16800-105 | Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth | http://arxiv.org/pdf/1603.06463v3.pdf | author:Sebastian Bach, Alexander Binder, Klaus-Robert Müller, Wojciech Samek category:cs.CV published:2016-03-21 summary:We present an application of the Layer-wise Relevance Propagation (LRP)algorithm to state of the art deep convolutional neural networks and FisherVector classifiers to compare the image perception and prediction strategies ofboth classifiers with the use of visualized heatmaps. Layer-wise RelevancePropagation (LRP) is a method to compute scores for individual components of aninput image, denoting their contribution to the prediction of the classifierfor one particular test point. We demonstrate the impact of different choicesof decomposition cut-off points during the LRP-process, controlling theresolution and semantics of the heatmap on test images from the PASCAL VOC 2007test data set.
arxiv-16800-106 | The CMA Evolution Strategy: A Tutorial | http://arxiv.org/pdf/1604.00772v1.pdf | author:Nikolaus Hansen category:cs.LG stat.ML published:2016-04-04 summary:This tutorial introduces the CMA Evolution Strategy (ES), where CMA standsfor Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized,method for real-parameter (continuous domain) optimization of non-linear,non-convex functions. We try to motivate and derive the algorithm fromintuitive concepts and from requirements of non-linear, non-convex search incontinuous domain.
arxiv-16800-107 | Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks | http://arxiv.org/pdf/1604.00734v1.pdf | author:Matthew Francis-Landau, Greg Durrett, Dan Klein category:cs.CL published:2016-04-04 summary:A key challenge in entity linking is making effective use of contextualinformation to disambiguate mentions that might refer to different entities indifferent contexts. We present a model that uses convolutional neural networksto capture semantic correspondence between a mention's context and a proposedtarget entity. These convolutional networks operate at multiple granularitiesto exploit various kinds of topic information, and their rich parameterizationgives them the capacity to learn which n-grams characterize different topics.We combine these networks with a sparse linear model to achievestate-of-the-art performance on multiple entity linking datasets, outperformingthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).
arxiv-16800-108 | Waterdrop Stereo | http://arxiv.org/pdf/1604.00730v1.pdf | author:Shaodi You, Robby T. Tan, Rei Kawakami, Yasuhiro Mukaigawa, Katsushi Ikeuchi category:cs.CV published:2016-04-04 summary:This paper introduces depth estimation from water drops. The key idea is thata single water drop adhered to window glass is totally transparent and convex,and thus optically acts like a fisheye lens. If we have more than one waterdrop in a single image, then through each of them we can see the environmentwith different view points, similar to stereo. To realize this idea, we need torectify every water drop imagery to make radially distorted planar surfaceslook flat. For this rectification, we consider two physical properties of waterdrops: (1) A static water drop has constant volume, and its geometric convexshape is determined by the balance between the tension force and gravity. Thisimplies that the 3D geometric shape can be obtained by minimizing the overallpotential energy, which is the sum of the tension energy and the gravitationalpotential energy. (2) The imagery inside a water-drop is determined by thewater-drop 3D shape and total reflection at the boundary. This total reflectiongenerates a dark band commonly observed in any adherent water drops. Hence,once the 3D shape of water drops are recovered, we can rectify the water dropimages through backward raytracing. Subsequently, we can compute depth usingstereo. In addition to depth estimation, we can also apply image refocusing.Experiments on real images and a quantitative evaluation show the effectivenessof our proposed method. To our best knowledge, never before have adherent waterdrops been used to estimate depth.
arxiv-16800-109 | Text Understanding from Scratch | http://arxiv.org/pdf/1502.01710v5.pdf | author:Xiang Zhang, Yann LeCun category:cs.LG cs.CL published:2015-02-05 summary:This article demontrates that we can apply deep learning to textunderstanding from character-level inputs all the way up to abstract textconcepts, using temporal convolutional networks (ConvNets). We apply ConvNetsto various large-scale datasets, including ontology classification, sentimentanalysis, and text categorization. We show that temporal ConvNets can achieveastonishing performance without the knowledge of words, phrases, sentences andany other syntactic or semantic structures with regards to a human language.Evidence shows that our models can work for both English and Chinese.
arxiv-16800-110 | Character-level Convolutional Networks for Text Classification | http://arxiv.org/pdf/1509.01626v3.pdf | author:Xiang Zhang, Junbo Zhao, Yann LeCun category:cs.LG cs.CL published:2015-09-04 summary:This article offers an empirical exploration on the use of character-levelconvolutional networks (ConvNets) for text classification. We constructedseveral large-scale datasets to show that character-level convolutionalnetworks could achieve state-of-the-art or competitive results. Comparisons areoffered against traditional models such as bag of words, n-grams and theirTFIDF variants, and deep learning models such as word-based ConvNets andrecurrent neural networks.
arxiv-16800-111 | Jet-Images -- Deep Learning Edition | http://arxiv.org/pdf/1511.05190v2.pdf | author:Luke de Oliveira, Michael Kagan, Lester Mackey, Benjamin Nachman, Ariel Schwartzman category:hep-ph stat.ML published:2015-11-16 summary:Building on the notion of a particle physics detector as a camera and thecollimated streams of high energy particles, or jets, it measures as an image,we investigate the potential of machine learning techniques based on deeplearning architectures to identify highly boosted W bosons. Modern deeplearning algorithms trained on jet images can out-perform standardphysically-motivated feature driven approaches to jet tagging. We developtechniques for visualizing how these features are learned by the network andwhat additional information is used to improve performance. This interplaybetween physically-motivated feature driven tools and supervised learningalgorithms is general and can be used to significantly increase the sensitivityto discover new particles and new forces, and gain a deeper understanding ofthe physics within jets.
arxiv-16800-112 | Semi-supervised Question Retrieval with Gated Convolutions | http://arxiv.org/pdf/1512.05726v2.pdf | author:Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi Jaakkola, Katerina Tymoshenko, Alessandro Moschitti, Lluis Marquez category:cs.CL cs.NE published:2015-12-17 summary:Question answering forums are rapidly growing in size with no effectiveautomated ability to refer to and reuse answers already available for previousposted questions. In this paper, we develop a methodology for findingsemantically related questions. The task is difficult since 1) key pieces ofinformation are often buried in extraneous details in the question body and 2)available annotations on similar questions are scarce and fragmented. We designa recurrent and convolutional model (gated convolution) to effectively mapquestions to their semantic representations. The models are pre-trained withinan encoder-decoder framework (from body to title) on the basis of the entireraw corpus, and fine-tuned discriminatively from limited annotations. Ourevaluation demonstrates that our model yields substantial gains over a standardIR baseline and various neural network architectures (including CNNs, LSTMs andGRUs).
arxiv-16800-113 | Top-down Tree Long Short-Term Memory Networks | http://arxiv.org/pdf/1511.00060v3.pdf | author:Xingxing Zhang, Liang Lu, Mirella Lapata category:cs.CL cs.LG published:2015-10-31 summary:Long Short-Term Memory (LSTM) networks, a type of recurrent neural networkwith a more complex computational unit, have been successfully applied to avariety of sequence modeling tasks. In this paper we develop Tree LongShort-Term Memory (TreeLSTM), a neural network model based on LSTM, which isdesigned to predict a tree rather than a linear sequence. TreeLSTM defines theprobability of a sentence by estimating the generation probability of itsdependency tree. At each time step, a node is generated based on therepresentation of the generated sub-tree. We further enhance the modeling powerof TreeLSTM by explicitly representing the correlations between left and rightdependents. Application of our model to the MSR sentence completion challengeachieves results beyond the current state of the art. We also report results ondependency parsing reranking achieving competitive performance.
arxiv-16800-114 | ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering | http://arxiv.org/pdf/1511.05960v2.pdf | author:Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia category:cs.CV published:2015-11-18 summary:We propose a novel attention based deep learning architecture for visualquestion answering task (VQA). Given an image and an image related naturallanguage question, VQA generates the natural language answer for the question.Generating the correct answers requires the model's attention to focus on theregions corresponding to the question, because different questions inquireabout the attributes of different image regions. We introduce an attentionbased configurable convolutional neural network (ABC-CNN) to learn suchquestion-guided attention. ABC-CNN determines an attention map for animage-question pair by convolving the image feature map with configurableconvolutional kernels derived from the question's semantics. We evaluate theABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR,and VQA dataset. ABC-CNN model achieves significant improvements overstate-of-the-art methods on these datasets. The question-guided attentiongenerated by ABC-CNN is also shown to reflect the regions that are highlyrelevant to the questions.
arxiv-16800-115 | A New Learning Method for Inference Accuracy, Core Occupation, and Performance Co-optimization on TrueNorth Chip | http://arxiv.org/pdf/1604.00697v1.pdf | author:Wei Wen, Chunpeng Wu, Yandan Wang, Kent Nixon, Qing Wu, Mark Barnell, Hai Li, Yiran Chen category:cs.NE cs.AI published:2016-04-03 summary:IBM TrueNorth chip uses digital spikes to perform neuromorphic computing andachieves ultrahigh execution parallelism and power efficiency. However, inTrueNorth chip, low quantization resolution of the synaptic weights and spikessignificantly limits the inference (e.g., classification) accuracy of thedeployed neural network model. Existing workaround, i.e., averaging the resultsover multiple copies instantiated in spatial and temporal domains, rapidlyexhausts the hardware resources and slows down the computation. In this work,we propose a novel learning method on TrueNorth platform that constrains therandom variance of each computation copy and reduces the number of neededcopies. Compared to the existing learning method, our method can achieve up to68.8% reduction of the required neuro-synaptic cores or 6.5X speedup, with evenslightly improved inference accuracy.
arxiv-16800-116 | Noisy Activation Functions | http://arxiv.org/pdf/1603.00391v3.pdf | author:Caglar Gulcehre, Marcin Moczulski, Misha Denil, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2016-03-01 summary:Common nonlinear activation functions used in neural networks can causetraining difficulties due to the saturation behavior of the activationfunction, which may hide dependencies that are not visible to vanilla-SGD(using first order gradients only). Gating mechanisms that use softlysaturating activation functions to emulate the discrete switching of digitallogic circuits are good examples of this. We propose to exploit the injectionof appropriate noise so that the gradients may flow easily, even if thenoiseless application of the activation function would yield zero gradient.Large noise will dominate the noise-free gradient and allow stochastic gradientdescent toexplore more. By adding noise only to the problematic parts of theactivation function, we allow the optimization procedure to explore theboundary between the degenerate (saturating) and the well-behaved parts of theactivation function. We also establish connections to simulated annealing, whenthe amount of noise is annealed down, making it easier to optimize hardobjective functions. We find experimentally that replacing such saturatingactivation functions by noisy variants helps training in many contexts,yielding state-of-the-art or competitive results on different datasets andtask, especially when training seems to be the most difficult, e.g., whencurriculum learning is necessary to obtain good results.
arxiv-16800-117 | The Offset Tree for Learning with Partial Labels | http://arxiv.org/pdf/0812.4044v3.pdf | author:Alina Beygelzimer, John Langford category:cs.LG cs.AI published:2008-12-21 summary:We present an algorithm, called the Offset Tree, for learning to makedecisions in situations where the payoff of only one choice is observed, ratherthan all choices. The algorithm reduces this setting to binary classification,allowing one to reuse of any existing, fully supervised binary classificationalgorithm in this partial information setting. We show that the Offset Tree isan optimal reduction to binary classification. In particular, it has regret atmost $(k-1)$ times the regret of the binary classifier it uses (where $k$ isthe number of choices), and no reduction to binary classification can dobetter. This reduction is also computationally optimal, both at training andtest time, requiring just $O(\log_2 k)$ work to train on an example or make aprediction. Experiments with the Offset Tree show that it generally performs better thanseveral alternative approaches.
arxiv-16800-118 | Pointing the Unknown Words | http://arxiv.org/pdf/1603.08148v2.pdf | author:Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio category:cs.CL cs.LG cs.NE published:2016-03-26 summary:The problem of rare and unknown words is an important issue that canpotentially effect the performance of many NLP systems, including bothtraditional count based and deep learning models. We propose a novel way todeal with the rare and unseen words for the neural network models withattention. Our model uses two softmax layers in order to predict the next wordin conditional language models: one of the softmax layers predicts the locationof a word in the source sentence, and the other softmax layer predicts a wordin the shortlist vocabulary. The decision of which softmax layer to use at eachtimestep is adaptively made by an MLP which is conditioned on the context. Wemotivate this work from a psychological evidence that humans naturally have atendency to point towards objects in the context or the environment when thename of an object is not known. Using our proposed model, we observeimprovements in two tasks, neural machine translation on the Europarl Englishto French parallel corpora and text summarization on the Gigaword dataset.
arxiv-16800-119 | Multi-Bias Non-linear Activation in Deep Neural Networks | http://arxiv.org/pdf/1604.00676v1.pdf | author:Hongyang Li, Wanli Ouyang, Xiaogang Wang category:cs.CV published:2016-04-03 summary:As a widely used non-linear activation, Rectified Linear Unit (ReLU)separates noise and signal in a feature map by learning a threshold or bias.However, we argue that the classification of noise and signal not only dependson the magnitude of responses, but also the context of how the featureresponses would be used to detect more abstract patterns in higher layers. Inorder to output multiple response maps with magnitude in different ranges for aparticular visual pattern, existing networks employing ReLU and its variantshave to learn a large number of redundant filters. In this paper, we propose amulti-bias non-linear activation (MBA) layer to explore the information hiddenin the magnitudes of responses. It is placed after the convolution layer todecouple the responses to a convolution kernel into multiple maps bymulti-thresholding magnitudes, thus generating more patterns in the featurespace at a low computational cost. It provides great flexibility of selectingresponses to different visual patterns in different magnitude ranges to formrich representations in higher layers. Such a simple and yet effective schemeachieves the state-of-the-art performance on several benchmarks.
arxiv-16800-120 | Primal-Dual Active-Set Methods for Isotonic Regression and Trend Filtering | http://arxiv.org/pdf/1508.02452v2.pdf | author:Zheng Han, Frank E. Curtis category:math.OC cs.LG published:2015-08-10 summary:Isotonic regression (IR) is a non-parametric calibration method used insupervised learning. For performing large-scale IR, we propose a primal-dualactive-set (PDAS) algorithm which, in contrast to the state-of-the-art PoolAdjacent Violators (PAV) algorithm, can be parallized and is easilywarm-started thus well-suited in the online settings. We prove that, like thePAV algorithm, our PDAS algorithm for IR is convergent and has a workcomplexity of O(n), though our numerical experiments suggest that our PDASalgorithm is often faster than PAV. In addition, we propose PDAS variants (withsafeguarding to ensure convergence) for solving related trend filtering (TF)problems, providing the results of experiments to illustrate theireffectiveness.
arxiv-16800-121 | A Characterization of the Non-Uniqueness of Nonnegative Matrix Factorizations | http://arxiv.org/pdf/1604.00653v1.pdf | author:W. Pan, F. Doshi-Velez category:cs.LG stat.ML published:2016-04-03 summary:Nonnegative matrix factorization (NMF) is a popular dimension reductiontechnique that produces interpretable decomposition of the data into parts.However, this decompostion is not generally identifiable (even up topermutation and scaling). While other studies have provide criteria under whichNMF is identifiable, we present the first (to our knowledge) characterizationof the non-identifiability of NMF. We describe exactly when and hownon-uniqueness can occur, which has important implications for algorithms toefficiently discover alternate solutions, if they exist.
arxiv-16800-122 | Multi-Relational Learning at Scale with ADMM | http://arxiv.org/pdf/1604.00647v1.pdf | author:Lucas Drumond, Ernesto Diaz-Aviles, Lars Schmidt-Thieme category:stat.ML cs.AI cs.LG published:2016-04-03 summary:Learning from multiple-relational data which contains noise, ambiguities, orduplicate entities is essential to a wide range of applications such asstatistical inference based on Web Linked Data, recommender systems,computational biology, and natural language processing. These tasks usuallyrequire working with very large and complex datasets - e.g., the Web graph -however, current approaches to multi-relational learning are not practical forsuch scenarios due to their high computational complexity and poor scalabilityon large data. In this paper, we propose a novel and scalable approach for multi-relationalfactorization based on consensus optimization. Our model, called ConsMRF, isbased on the Alternating Direction Method of Multipliers (ADMM) framework,which enables us to optimize each target relation using a smaller set ofparameters than the state-of-the-art competitors in this task. Due to ADMM's nature, ConsMRF can be easily parallelized which makes itsuitable for large multi-relational data. Experiments on large Web datasets -derived from DBpedia, Wikipedia and YAGO - show the efficiency and performanceimprovement of ConsMRF over strong competitors. In addition, ConsMRFnear-linear scalability indicates great potential to tackle Web-scale problemsizes.
arxiv-16800-123 | Multi-objective design of quantum circuits using genetic programming | http://arxiv.org/pdf/1604.00642v1.pdf | author:Moein Sarvaghad-Moghaddam category:cs.ET cs.NE published:2016-04-03 summary:Quantum computing is a new way of data processing based on the concept ofquantum mechanics. Quantum circuit design is a process of converting a quantumgate to a series of basic gates and is divided into two general categoriesbased on the decomposition and composition. In the second group, usingevolutionary algorithms and especially genetic algorithms, multiplication ofmatrix gates was used to achieve the final characteristic of quantum circuit.Genetic programming is a subfield of evolutionary computing in which computerprograms evolve to solve studied problems. In past research that has been donein the field of quantum circuits design, only one cost metrics (usually quantumcost) has been investigated. In this paper for the first time, amulti-objective approach has been provided to design quantum circuits usinggenetic programming that considers the depth and the cost of nearest neighbormetrics in addition to quantum cost metric. Another innovation of this articleis the use of two-step fitness function and taking into account the equivalenceof global phase in quantum gates. The results show that the proposed method isable to find a good answer in a short time.
arxiv-16800-124 | Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1512.05830v2.pdf | author:Li Shen, Zhouchen Lin, Qingming Huang category:cs.CV cs.LG published:2015-12-18 summary:Learning deeper convolutional neural networks becomes a tendency in recentyears. However, many empirical evidences suggest that performance improvementcannot be gained by simply stacking more layers. In this paper, we consider theissue from an information theoretical perspective, and propose a novel methodRelay Backpropagation, that encourages the propagation of effective informationthrough the network in training stage. By virtue of the method, we achieved thefirst place in ILSVRC 2015 Scene Classification Challenge. Extensiveexperiments on two challenging large scale datasets demonstrate theeffectiveness of our method is not restricted to a specific dataset or networkarchitecture. Our models will be available to the research community later.
arxiv-16800-125 | GAL: A Global-Attributes Assisted Labeling System for Outdoor Scenes | http://arxiv.org/pdf/1604.00606v1.pdf | author:Yuzhuo Ren, Chen Chen, Shangwen Li, C. -C. Jay Kuo category:cs.CV published:2016-04-03 summary:An approach that extracts global attributes from outdoor images to facilitategeometric layout labeling is investigated in this work. The proposedGlobal-attributes Assisted Labeling (GAL) system exploits both local featuresand global attributes. First, by following a classical method, we use localfeatures to provide initial labels for all super-pixels. Then, we develop a setof techniques to extract global attributes from 2D outdoor images. They includesky lines, ground lines, vanishing lines, etc. Finally, we propose the GALsystem that integrates global attributes in the conditional random field (CRF)framework to improve initial labels so as to offer a more robust labelingresult. The performance of the proposed GAL system is demonstrated andbenchmarked with several state-of-the-art algorithms against a popular outdoorscene layout dataset.
arxiv-16800-126 | HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection | http://arxiv.org/pdf/1604.00600v1.pdf | author:Tao Kong, Anbang Yao, Yurong Chen, Fuchun Sun category:cs.CV published:2016-04-03 summary:Almost all of the current top-performing object detection networks employregion proposals to guide the search for object instances. State-of-the-artregion proposal methods usually need several thousand proposals to get highrecall, thus hurting the detection efficiency. Although the latest RegionProposal Network method gets promising detection accuracy with several hundredproposals, it still struggles in small-size object detection and preciselocalization (e.g., large IoU thresholds), mainly due to the coarseness of itsfeature maps. In this paper, we present a deep hierarchical network, namelyHyperNet, for handling region proposal generation and object detection jointly.Our HyperNet is primarily based on an elaborately designed Hyper Feature whichaggregates hierarchical feature maps first and then compresses them into auniform space. The Hyper Features well incorporate deep but highly semantic,intermediate but really complementary, and shallow but naturallyhigh-resolution features of the image, thus enabling us to construct HyperNetby sharing them both in generating proposals and detecting objects via anend-to-end joint training strategy. For the deep VGG16 model, our methodachieves completely leading recall and state-of-the-art object detectionaccuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. Itruns with a speed of 5 fps (including all steps) on a GPU, thus having thepotential for real-time processing.
arxiv-16800-127 | Convex block-sparse linear regression with expanders -- provably | http://arxiv.org/pdf/1603.06313v2.pdf | author:Anastasios Kyrillidis, Bubacarr Bah, Rouzbeh Hasheminezhad, Quoc Tran-Dinh, Luca Baldassarre, Volkan Cevher category:cs.IT math.IT math.OC stat.ML published:2016-03-21 summary:Sparse matrices are favorable objects in machine learning and optimization.When such matrices are used, in place of dense ones, the overall complexityrequirements in optimization can be significantly reduced in practice, both interms of space and run-time. Prompted by this observation, we study a convexoptimization scheme for block-sparse recovery from linear measurements. Toobtain linear sketches, we use expander matrices, i.e., sparse matricescontaining only few non-zeros per column. Hitherto, to the best of ourknowledge, such algorithmic solutions have been only studied from a non-convexperspective. Our aim here is to theoretically characterize the performance ofconvex approaches under such setting. Our key novelty is the expression of the recovery error in terms of themodel-based norm, while assuring that solution lives in the model. To achievethis, we show that sparse model-based matrices satisfy a group version of thenull-space property. Our experimental findings on synthetic and realapplications support our claims for faster recovery in the convex setting -- asopposed to using dense sensing matrices, while showing a competitive recoveryperformance.
arxiv-16800-128 | Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection | http://arxiv.org/pdf/1603.02199v3.pdf | author:Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen category:cs.LG cs.AI cs.CV cs.RO published:2016-03-07 summary:We describe a learning-based approach to hand-eye coordination for roboticgrasping from monocular images. To learn hand-eye coordination for grasping, wetrained a large convolutional neural network to predict the probability thattask-space motion of the gripper will result in successful grasps, using onlymonocular camera images and independently of camera calibration or the currentrobot pose. This requires the network to observe the spatial relationshipbetween the gripper and objects in the scene, thus learning hand-eyecoordination. We then use this network to servo the gripper in real time toachieve successful grasps. To train our network, we collected over 800,000grasp attempts over the course of two months, using between 6 and 14 roboticmanipulators at any given time, with differences in camera placement andhardware. Our experimental evaluation demonstrates that our method achieveseffective real-time control, can successfully grasp novel objects, and correctsmistakes by continuous servoing.
arxiv-16800-129 | Reasoning About Pragmatics with Neural Listeners and Speakers | http://arxiv.org/pdf/1604.00562v1.pdf | author:Jacob Andreas, Dan Klein category:cs.CL cs.NE published:2016-04-02 summary:We present a model for pragmatically describing scenes, in which contrastivebehavior results from a combination of inference-driven pragmatics and learnedsemantics. Like previous learned approaches to language generation, our modeluses a simple feature-driven architecture (here a pair of neural "listener" and"speaker" models) to ground language in the world. Like inference-drivenapproaches to pragmatics, our model actively reasons about listener behaviorwhen selecting utterances. For training, our approach requires only ordinarycaptions, annotated _without_ demonstration of the pragmatic behavior the modelultimately exhibits. In human evaluations on a referring expression game, ourapproach succeeds 81% of the time, compared to a 64% success rate usingexisting techniques.
arxiv-16800-130 | Channel Equalization Using Multilayer Perceptron Networks | http://arxiv.org/pdf/1604.00558v1.pdf | author:Saba Baloch, Javed Ali Baloch, Mukhtiar Ali Unar category:cs.NE published:2016-04-02 summary:In most digital communication systems, bandwidth limited channel along withmultipath propagation causes ISI (Inter Symbol Interference) to occur. Thisphenomenon causes distortion of the given transmitted symbol due to othertransmitted symbols. With the help of equalization ISI can be reduced. Thispaper presents a solution to the ISI problem by performing blind equalizationusing ANN (Artificial Neural Networks). The simulated network is a multilayerfeedforward Perceptron ANN, which has been trained by utilizing the errorback-propagation algorithm. The weights of the network are updated inaccordance with training of the network. This paper presents a very effectivemethod for blind channel equalization, being more efficient than thepre-existing algorithms. The obtained results show a visible reduction in thenoise content.
arxiv-16800-131 | SAM: Support Vector Machine Based Active Queue Management | http://arxiv.org/pdf/1604.00557v1.pdf | author:Muhammad Saleh Shah, Asim Imdad Wagan, Mukhtiar Ali Unar category:cs.NI cs.LG published:2016-04-02 summary:Recent years have seen an increasing interest in the design of AQM (ActiveQueue Management) controllers. The purpose of these controllers is to managethe network congestion under varying loads, link delays and bandwidth. In thispaper, a new AQM controller is proposed which is trained by using the SVM(Support Vector Machine) with the RBF (Radial Basis Function) kernal. Theproposed controller is called the support vector based AQM (SAM) controller.The performance of the proposed controller has been compared with threeconventional AQM controllers, namely the Random Early Detection, Blue andProportional Plus Integral Controller. The preliminary simulation studies showthat the performance of the proposed controller is comparable to theconventional controllers. However, the proposed controller is more efficient incontrolling the queue size than the conventional controllers.
arxiv-16800-132 | pH Prediction by Artificial Neural Networks for the Drinking Water of the Distribution System of Hyderabad City | http://arxiv.org/pdf/1604.00552v1.pdf | author:Niaz Ahmed Memon, Mukhtiar Ali Unar, Abdul Khalique Ansari category:cs.NE published:2016-04-02 summary:In this research, feedforward ANN (Artificial Neural Network) model isdeveloped and validated for predicting the pH at 10 different locations of thedistribution system of drinking water of Hyderabad city. The developed model isMLP (Multilayer Perceptron) with back propagation algorithm.The data for thetraining and testing of the model are collected through an experimentalanalysis on weekly basis in a routine examination for maintaining the qualityof drinking water in the city. 17 parameters are taken into considerationincluding pH. These all parameters are taken as input variables for the modeland then pH is predicted for 03 phases;raw water of river Indus,treated waterin the treatment plants and then treated water in the distribution system ofdrinking water. The training and testing results of this model reveal that MLPneural networks are exceedingly extrapolative for predicting the pH of riverwater, untreated and treated water at all locations of the distribution systemof drinking water of Hyderabad city. The optimum input and output weights aregenerated with minimum MSE (Mean Square Error) < 5%.Experimental, predicted andtested values of pH are plotted and the effectiveness of the model isdetermined by calculating the coefficient of correlation (R2=0.999) of trainedand tested results.
arxiv-16800-133 | Image Quality Assessment for Performance Evaluation of Focus Measure Operators | http://arxiv.org/pdf/1604.00546v1.pdf | author:Farida Memon, Mukhtiar Ali Unar, Sheeraz Memon category:cs.CV published:2016-04-02 summary:This paper presents the performance evaluation of eight focus measureoperators namely Image CURV (Curvature), GRAE (Gradient Energy), HISE(Histogram Entropy), LAPM (Modified Laplacian), LAPV (Variance of Laplacian),LAPD (Diagonal Laplacian), LAP3 (Laplacian in 3D Window) and WAVS (Sum ofWavelet Coefficients). Statistical matrics such as MSE (Mean Squared Error),PNSR (Peak Signal to Noise Ratio), SC (Structural Content), NCC (NormalizedCross Correlation), MD (Maximum Difference) and NAE (Normalized Absolute Error)are used to evaluate stated focus measures in this research. . FR (FullReference) method of the image quality assessment is utilized in this paper.Results indicate that LAPD method is comparatively better than other sevenfocus operators at typical imaging conditions.
arxiv-16800-134 | Voronoi Region-Based Adaptive Unsupervised Color Image Segmentation | http://arxiv.org/pdf/1604.00533v1.pdf | author:R. Hettiarachchi, J. F. Peters category:cs.CV published:2016-04-02 summary:Color image segmentation is a crucial step in many computer vision andpattern recognition applications. This article introduces an adaptive andunsupervised clustering approach based on Voronoi regions, which can be appliedto solve the color image segmentation problem. The proposed method performsregion splitting and merging within Voronoi regions of the DirichletTessellated image (also called a Voronoi diagram) , which improves theefficiency and the accuracy of the number of clusters and cluster centroidsestimation process. Furthermore, the proposed method uses cluster centroidproximity to merge proximal clusters in order to find the final number ofclusters and cluster centroids. In contrast to the existing adaptiveunsupervised cluster-based image segmentation algorithms, the proposed methoduses K-means clustering algorithm in place of the Fuzzy C-means algorithm tofind the final segmented image. The proposed method was evaluated on threedifferent unsupervised image segmentation evaluation benchmarks and its resultswere compared with two other adaptive unsupervised cluster-based imagesegmentation algorithms. The experimental results reported in this articleconfirm that the proposed method outperforms the existing algorithms in termsof the quality of image segmentation results. Also, the proposed method resultsin the lowest average execution time per image compared to the existing methodsreported in this article.
arxiv-16800-135 | Multilingual Language Processing From Bytes | http://arxiv.org/pdf/1512.00103v2.pdf | author:Dan Gillick, Cliff Brunk, Oriol Vinyals, Amarnag Subramanya category:cs.CL published:2015-12-01 summary:We describe an LSTM-based model which we call Byte-to-Span (BTS) that readstext as bytes and outputs span annotations of the form [start, length, label]where start positions, lengths, and labels are separate entries in ourvocabulary. Because we operate directly on unicode bytes rather thanlanguage-specific words or characters, we can analyze text in many languageswith a single model. Due to the small vocabulary size, these multilingualmodels are very compact, but produce results similar to or better than thestate-of- the-art in Part-of-Speech tagging and Named Entity Recognition thatuse only the provided training datasets (no external data sources). Our modelsare learning "from scratch" in that they do not rely on any elements of thestandard pipeline in Natural Language Processing (including tokenization), andthus can run in standalone fashion on raw text.
arxiv-16800-136 | Discriminative Phrase Embedding for Paraphrase Identification | http://arxiv.org/pdf/1604.00503v1.pdf | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2016-04-02 summary:This work, concerning paraphrase identification task, on one hand contributesto expanding deep learning embeddings to include continuous and discontinuouslinguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNNto learn the discriminative weights of words and phrases specific to paraphrasetask, so that a weighted sum of embeddings can represent sentences moreeffectively. Based on these two innovations we get competitive state-of-the-artperformance on paraphrase identification.
arxiv-16800-137 | Online Updating of Word Representations for Part-of-Speech Tagging | http://arxiv.org/pdf/1604.00502v1.pdf | author:Wenpeng Yin, Tobias Schnabel, Hinrich Schütze category:cs.CL published:2016-04-02 summary:We propose online unsupervised domain adaptation (DA), which is performedincrementally as data comes in and is applicable when batch DA is not possible.In a part-of-speech (POS) tagging evaluation, we find that online unsupervisedDA performs as well as batch DA.
arxiv-16800-138 | A Fully Convolutional Neural Network for Cardiac Segmentation in Short-Axis MRI | http://arxiv.org/pdf/1604.00494v1.pdf | author:Phi Vu Tran category:cs.CV published:2016-04-02 summary:Automated cardiac segmentation from magnetic resonance imaging datasets is anessential step in the timely diagnosis and management of cardiac pathologies.We propose to tackle the problem of automated left and right ventriclesegmentation through the application of a deep fully convolutional neuralnetwork architecture. Our model is efficiently trained end-to-end in a singlelearning stage from whole-image inputs and ground truths to make inference atevery pixel. To our knowledge, this is the first application of a fullyconvolutional neural network architecture for pixel-wise labeling in cardiacmagnetic resonance imaging. Numerical experiments demonstrate that our model isrobust to outperform previous fully automated methods across multipleevaluation measures on a range of cardiac datasets. It is equally noteworthythat our model leverages commodity compute resources such as the graphicsprocessing unit to enable fast, state-of-the-art cardiac segmentation atmassive scales. The models and code will be released open-source in the nearfuture.
arxiv-16800-139 | Robust video object tracking via Bayesian model averaging based feature fusion | http://arxiv.org/pdf/1604.00475v1.pdf | author:Yi Dai, Bin Liu category:cs.CV published:2016-04-02 summary:In this article, we are concerned with tracking an object of interest invideo stream. We propose an algorithm that is robust against occlusion, thepresence of confusing colors, abrupt changes in the object feature space andchanges in object size. We develop the algorithm within a Bayesian modelingframework. The state space model is used for capturing the temporal correlationin the sequence of frame images by modeling the underlying dynamics of thetracking system. The Bayesian model averaging (BMA) strategy is proposed forfusing multi-clue information in the observations. Any number of objectfeatures are allowed to be involved in the proposed framework. Every featurerepresents one source of information to be fused and is associated with anobservation model. The state inference is performed by employing the particlefilter methods. In comparison with related approaches, the BMA based tracker isshown to have robustness, expressivity, and comprehensibility.
arxiv-16800-140 | Overlay Text Extraction From TV News Broadcast | http://arxiv.org/pdf/1604.00470v1.pdf | author:Raghvendra Kannao, Prithwijit Guha category:cs.CV published:2016-04-02 summary:The text data present in overlaid bands convey brief descriptions of newsevents in broadcast videos. The process of text extraction becomes challengingas overlay text is presented in widely varying formats and often with animationeffects. We note that existing edge density based methods are well suited forour application on account of their simplicity and speed of operation. However,these methods are sensitive to thresholds and have high false positive rates.In this paper, we present a contrast enhancement based preprocessing stage foroverlay text detection and a parameter free edge density based scheme forefficient text band detection. The second contribution of this paper is a novelapproach for multiple text region tracking with a formal identification of allpossible detection failure cases. The tracking stage enables us to establishthe temporal presence of text bands and their linking over time. The thirdcontribution is the adoption of Tesseract OCR for the specific task of overlaytext recognition using web news articles. The proposed approach is tested andfound superior on news videos acquired from three Indian English televisionnews channels along with benchmark datasets.
arxiv-16800-141 | Sherlock: Scalable Fact Learning in Images | http://arxiv.org/pdf/1511.04891v4.pdf | author:Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal category:cs.CV cs.CL cs.LG published:2015-11-16 summary:We study scalable and uniform understanding of facts in images. Existingvisual recognition systems are typically modeled differently for each fact typesuch as objects, actions, and interactions. We propose a setting where allthese facts can be modeled simultaneously with a capacity to understandunbounded number of facts in a structured way. The training data comes asstructured facts in images, including (1) objects (e.g., $<$boy$>$), (2)attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and(4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semanticlanguage view (e.g., $<$ boy, playing$>$) and a visual view (an image with thisfact). We show that learning visual facts in a structured way enables not onlya uniform but also generalizable visual understanding. We propose andinvestigate recent and strong approaches from the multiview learning literatureand also introduce two learning representation models as potential baselines.We applied the investigated methods on several datasets that we augmented withstructured facts and a large scale dataset of more than 202,000 facts and814,000 images. Our experiments show the advantage of relating facts by thestructure by the proposed models compared to the designed baselines onbidirectional fact retrieval.
arxiv-16800-142 | Centralized and Decentralized Global Outer-synchronization of Asymmetric Recurrent Time-varying Neural Network by Data-sampling | http://arxiv.org/pdf/1604.00462v1.pdf | author:Wenlian Lu, Ren Zheng, Tianping Chen category:cs.NE cs.SY math.CA published:2016-04-02 summary:In this paper, we discuss the outer-synchronization of the asymmetricallyconnected recurrent time-varying neural networks. By both centralized anddecentralized discretization data sampling principles, we derive severalsufficient conditions based on diverse vector norms that guarantee that any twotrajectories from different initial values of the identical neural networksystem converge together. The lower bounds of the common time intervals betweendata samples in centralized and decentralized principles are proved to bepositive, which guarantees exclusion of Zeno behavior. A numerical example isprovided to illustrate the efficiency of the theoretical results.
arxiv-16800-143 | Embedding Lexical Features via Low-Rank Tensors | http://arxiv.org/pdf/1604.00461v1.pdf | author:Mo Yu, Mark Dredze, Raman Arora, Matthew Gormley category:cs.CL cs.AI cs.LG published:2016-04-02 summary:Modern NLP models rely heavily on engineered features, which often combineword and contextual information into complex lexical features. Such combinationresults in large numbers of features, which can lead to over-fitting. Wepresent a new model that represents complex lexical features---comprised ofparts for words, contextual information and labels---in a tensor that capturesconjunction information among these parts. We apply low-rank tensorapproximations to the corresponding parameter tensors to reduce the parameterspace and improve prediction speed. Furthermore, we investigate two methods forhandling features that include $n$-grams of mixed lengths. Our model achievesstate-of-the-art results on tasks in relation extraction, PP-attachment, andpreposition disambiguation.
arxiv-16800-144 | Stability of Analytic Neural Networks with Event-triggered Synaptic Feedbacks | http://arxiv.org/pdf/1604.00457v1.pdf | author:Ren Zheng, Xinlei Yi, Wenlian Lu, Tianping Chen category:cs.NE math.DS nlin.AO published:2016-04-02 summary:In this paper, we investigate stability of a class of analytic neuralnetworks with the synaptic feedback via event-triggered rules. This model isgeneral and include Hopfield neural network as a special case. Theseevent-trigger rules can efficiently reduces loads of computation andinformation transmission at synapses of the neurons. The synaptic feedback ofeach neuron keeps a constant value based on the outputs of the other neurons atits latest triggering time but changes at its next triggering time, which isdetermined by certain criterion. It is proved that every trajectory of theanalytic neural network converges to certain equilibrium under thisevent-triggered rule for all initial values except a set of zero measure. Themain technique of the proof is the Lojasiewicz inequality to prove thefiniteness of trajectory length. The realization of this event-triggered ruleis verified by the exclusion of Zeno behaviors. Numerical examples are providedto illustrate the efficiency of the theoretical results.
arxiv-16800-145 | 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction | http://arxiv.org/pdf/1604.00449v1.pdf | author:Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, Silvio Savarese category:cs.CV cs.AI published:2016-04-02 summary:Inspired by the recent success of methods that employ shape priors to achieverobust 3D reconstructions, we propose a novel recurrent neural networkarchitecture that we call the 3D Recurrent Reconstruction Neural Network(3D-R2N2). The network learns a mapping from images of objects to theirunderlying 3D shapes from a large collection of synthetic data. Our networktakes in one or more images of an object instance from arbitrary viewpoints andoutputs a reconstruction of the object in the form of a 3D occupancy grid.Unlike most of the previous works, our network does not require any imageannotations or object class labels for training or testing. Our extensiveexperimental analysis shows that our reconstruction framework i) outperformsthe state-of-the-art methods for single view reconstruction, and ii) enablesthe 3D reconstruction of objects in situations when traditional SFM/SLAMmethods fail (because of lack of texture and/or wide baseline).
arxiv-16800-146 | Recurrent Neural Network Grammars | http://arxiv.org/pdf/1602.07776v2.pdf | author:Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith category:cs.CL cs.NE published:2016-02-25 summary:We introduce recurrent neural network grammars, probabilistic models ofsentences with explicit phrase structure. We explain efficient inferenceprocedures that allow application to both parsing and language modeling.Experiments show that they provide better parsing in English than any singlepreviously published supervised generative model and better language modelingthan state-of-the-art sequential RNNs in English and Chinese.
arxiv-16800-147 | Cross Quality Distillation | http://arxiv.org/pdf/1604.00433v1.pdf | author:Jong-Chyi Su, Subhransu Maji category:cs.CV published:2016-04-01 summary:We propose a technique for training recognition models when high-quality datais available at training time but not at testing time. Our approach, calledCross Quality Distillation (CQD), first trains a model on the high-quality dataand encourages a second model trained on the low-quality data to generalize inthe same way as the first. The technique is fairly general and only requiresthe ability to generate low-quality data from the high-quality data. We applythis to learn models for recognizing low-resolution images using labeledhigh-resolution images, non-localized objects using labeled localized objects,edge images using labeled color images, etc. Experiments on variousfine-grained recognition datasets demonstrate that the technique leads to largeimprovements in recognition accuracy on the low-quality data. We also establishconnections of CQD to other areas of machine learning such as domainadaptation, model compression, and learning using privileged information, andshow that the technique is general and can be applied to other settings.Finally, we present further insights into why the technique works throughvisualizations and establishing its relationship to curriculum learning.
arxiv-16800-148 | Leaving Some Stones Unturned: Dynamic Feature Prioritization for Activity Detection in Streaming Video | http://arxiv.org/pdf/1604.00427v1.pdf | author:Yu-Chuan Su, Kristen Grauman category:cs.CV published:2016-04-01 summary:Current approaches for activity recognition often ignore constraints oncomputational resources: 1) they rely on extensive feature computation toobtain rich descriptors on all frames, and 2) they assume batch-mode access tothe entire test video at once. We propose a new active approach to activityrecognition that prioritizes "what to compute when" in order to make timelypredictions. The main idea is to learn a policy that dynamically schedules thesequence of features to compute on selected frames of a given test video. Incontrast to traditional static feature selection, our approach continuallyre-prioritizes computation based on the accumulated history of observations andaccounts for the transience of those observations in ongoing video. We developvariants to handle both the batch and streaming settings. On two challengingdatasets, our method provides significantly better accuracy than alternativetechniques for a wide range of computational budgets.
arxiv-16800-149 | Cross-lingual Models of Word Embeddings: An Empirical Comparison | http://arxiv.org/pdf/1604.00425v1.pdf | author:Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth category:cs.CL published:2016-04-01 summary:Despite interest in using cross-lingual knowledge to learn word embeddingsfor various tasks, a systematic comparison of the possible approaches islacking in the literature. We perform an extensive evaluation of four popularapproaches of inducing cross-lingual embeddings, each requiring a differentform of supervision, on four typographically different language pairs. Ourevaluation setup spans four different tasks, including intrinsic evaluation onmono-lingual and cross-lingual similarity, and extrinsic evaluation ondownstream semantic and syntactic applications. We show that models whichrequire expensive cross-lingual knowledge almost always perform better, butcheaply supervised models often prove competitive on certain tasks.
arxiv-16800-150 | Structure from Motion on a Sphere | http://arxiv.org/pdf/1604.00409v1.pdf | author:Jonathan Ventura category:cs.CV published:2016-04-01 summary:We describe a special case of structure from motion where the camera rotateson a sphere. The camera's optical axis lies normal to the sphere's surface. Inthis case, the camera's pose is minimally represented by three rotationparameters. From analysis of the epipolar geometry we derive a novel andefficient solution for the essential matrix relating two images, requiring onlythree point correspondences in the minimal case. We apply this solver in astructure-from-motion pipeline that aggregates pairwise relations by rotationaveraging followed by bundle adjustment with an inverse depth parameterization.Our methods enable scene modeling with an outward-facing camera and objectscanning with an inward-facing camera.
arxiv-16800-151 | Revisiting Summarization Evaluation for Scientific Articles | http://arxiv.org/pdf/1604.00400v1.pdf | author:Arman Cohan, Nazli Goharian category:cs.CL published:2016-04-01 summary:Evaluation of text summarization approaches have been mostly based on metricsthat measure similarities of system generated summaries with a set of humanwritten gold-standard summaries. The most widely used metric in summarizationevaluation has been the ROUGE family. ROUGE solely relies on lexical overlapsbetween the terms and phrases in the sentences; therefore, in cases ofterminology variations and paraphrasing, ROUGE is not as effective. Scientificarticle summarization is one such case that is different from general domainsummarization (e.g. newswire data). We provide an extensive analysis of ROUGE'seffectiveness as an evaluation metric for scientific summarization; we showthat, contrary to the common belief, ROUGE is not much reliable in evaluatingscientific summaries. We furthermore show how different variants of ROUGEresult in very different correlations with the manual Pyramid scores. Finally,we propose an alternative metric for summarization evaluation which is based onthe content relevance between a system generated summary and the correspondinghuman written summaries. We call our metric SERA (Summarization Evaluation byRelevance Analysis). Unlike ROUGE, SERA consistently achieves high correlationswith manual scores which shows its effectiveness in evaluation of scientificarticle summarization.
arxiv-16800-152 | Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation | http://arxiv.org/pdf/1603.05959v2.pdf | author:Konstantinos Kamnitsas, Christian Ledig, Virginia F. J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben Glocker category:cs.CV cs.AI published:2016-03-18 summary:We propose a dual pathway, 11-layers deep, three-dimensional ConvolutionalNeural Network for the challenging task of brain lesion segmentation. Thedevised architecture is the result of an in-depth analysis of the limitationsof current networks proposed for similar applications. To overcome thecomputational burden of processing 3D medical scans, we have devised anefficient and effective dense training scheme which joins the processing ofadjacent image patches into one pass through the network while automaticallyadapting to the inherent class imbalance present in the data. Further, weanalyze the development of deeper, thus more discriminative 3D CNNs. In orderto incorporate both local and larger contextual information, we employ a dualpathway architecture that processes the input images at multiple scalessimultaneously. For post-processing of the network's soft segmentation, we usea 3D fully connected Conditional Random Field which effectively removes falsepositives. Our pipeline is extensively evaluated on three challenging tasks oflesion segmentation in multi-channel MRI patient data with traumatic braininjuries, brain tumors, and ischemic stroke. We improve on the state-of-the-artfor all three applications, with top ranking performance on the publicbenchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient,which allows its adoption in a variety of research and clinical settings. Thesource code of our implementation is made publicly available.
arxiv-16800-153 | Large-Scale Electron Microscopy Image Segmentation in Spark | http://arxiv.org/pdf/1604.00385v1.pdf | author:Stephen M. Plaza, Stuart E. Berg category:q-bio.QM cs.CV published:2016-04-01 summary:The emerging field of connectomics aims to unlock the mysteries of the brainby understanding the connectivity between neurons. To map this connectivity, weacquire thousands of electron microscopy (EM) images with nanometer-scaleresolution. After aligning these images, the resulting dataset has thepotential to reveal the shapes of neurons and the synaptic connections betweenthem. However, imaging the brain of even a tiny organism like the fruit flyyields terabytes of data. It can take years of manual effort to examine suchimage volumes and trace their neuronal connections. One solution is to applyimage segmentation algorithms to help automate the tracing tasks. In thispaper, we propose a novel strategy to apply such segmentation on very largedatasets that exceed the capacity of a single machine. Our solution is robustto potential segmentation errors which could otherwise severely compromise thequality of the overall segmentation, for example those due to poor classifiergeneralizability or anomalies in the image dataset. We implement our algorithmsin a Spark application which minimizes disk I/O, and apply them to a few largeEM datasets, revealing both their effectiveness and scalability. We hope thiswork will encourage external contributions to EM segmentation by providing 1) aflexible plugin architecture that deploys easily on different clusterenvironments and 2) an in-memory representation of segmentation that could beconducive to new advances.
arxiv-16800-154 | Person Re-identification in Appearance Impaired Scenarios | http://arxiv.org/pdf/1604.00367v1.pdf | author:Mengran Gou, Xikang Zhang, Angels Rates-Borras, Sadjad Asghari-Esfeden, Mario Sznaier, Octavia Camps category:cs.CV published:2016-04-01 summary:Person re-identification is critical in surveillance applications. Currentapproaches rely on appearance based features extracted from a single ormultiple shots of the target and candidate matches. These approaches are at adisadvantage when trying to distinguish between candidates dressed in similarcolors or when targets change their clothing. In this paper we propose adynamics-based feature to overcome this limitation. The main idea is to capturesoft biometrics from gait and motion patterns by gathering dense shorttrajectories (tracklets) which are Fisher vector encoded. To illustrate themerits of the proposed features we introduce three new "appearance-impaired"datasets. Our experiments on the original and the appearance impaired datasetsdemonstrate the benefits of incorporating dynamics-based information withappearance-based information to re-identification algorithms.
arxiv-16800-155 | COCO: The Bi-objective Black Box Optimization Benchmarking (bbob-biobj) Test Suite | http://arxiv.org/pdf/1604.00359v1.pdf | author:Tea Tusar, Dimo Brockhoff, Nikolaus Hansen, Anne Auger category:cs.AI cs.NE published:2016-04-01 summary:The bbob-biobj test suite contains 55 bi-objective functions in continuousdomain which are derived from combining functions of the well-knownsingle-objective noiseless bbob test suite. Besides giving the actual functiondefinitions and presenting their (known) properties, this documentation alsoaims at giving the rationale behind our approach in terms of function groups,instances, and potential objective space normalization.
arxiv-16800-156 | Minimax Optimal Variable Clustering in G-models via Cord | http://arxiv.org/pdf/1508.01939v2.pdf | author:Florentina Bunea, Christophe Giraud, Xi Luo category:stat.ME math.ST stat.ML stat.TH published:2015-08-08 summary:The goal of variable clustering is to partition a random vector ${\bf X} \inR^p$ in sub-groups of similar probabilistic behavior. Popular methods such ashierarchical clustering or $K$-means are algorithmic procedures applied toobservations on ${\bf X}$, while no population level target is defined prior toestimation. We take a different view in this paper, where we propose andinvestigate model based variable clustering. We consider three models, ofincreasing level of complexity, termed generically $G$-models, with $G$standing for the partition to be estimated. Motivated by the potential lack ofidentifiability of the $G$-latent models, which are currently used in problemsinvolving variable clustering, we introduce two new classes of models, the$G$-exchangeable and the $G$-block covariance models. We show that both classesare identifiable, for any distribution of ${\bf X}$. Our focus is on clusters that are invariant with respect to unknown monotonetransformations of the data, and that can be estimated in a computationallyfeasible manner. Both desiderata can be met if the clusters correspond toblocks in the copula correlation matrix of ${\bf X}$, assumed to have aGaussian copula distribution. This motivates the introduction of a newsimilarity metric for cluster membership, CORD, and a homonymous method forcluster estimation. Central to our work is the derivation of the minimax valueof the CORD cluster separation for exact partition recovery. We obtained thesurprising result that this value is of order $\sqrt{{\log (p)}/{n}}$,irrespective of the number of clusters, or of the size of the smallest cluster.Our new procedure, CORD, available on CRAN, achieves this bound, is easy toimplement and has computational complexity that is polynomial in $p$.
arxiv-16800-157 | How to Transfer? Zero-Shot Object Recognition via Hierarchical Transfer of Semantic Attributes | http://arxiv.org/pdf/1604.00326v1.pdf | author:Ziad Al-Halah, Rainer Stiefelhagen category:cs.CV published:2016-04-01 summary:Attribute based knowledge transfer has proven very successful in visualobject analysis and learning previously unseen classes. However, the commonapproach learns and transfers attributes without taking into consideration theembedded structure between the categories in the source set. Such informationprovides important cues on the intra-attribute variations. We propose tocapture these variations in a hierarchical model that expands the knowledgesource with additional abstraction levels of attributes. We also provide anovel transfer approach that can choose the appropriate attributes to be sharedwith an unseen class. We evaluate our approach on three public datasets:aPascal, Animals with Attributes and CUB-200-2011 Birds. The experimentsdemonstrate the effectiveness of our model with significant improvement overstate-of-the-art.
arxiv-16800-158 | A Semisupervised Approach for Language Identification based on Ladder Networks | http://arxiv.org/pdf/1604.00317v1.pdf | author:Ehud Ben-Reuven, Jacob Goldberger category:cs.CL cs.LG cs.NE published:2016-04-01 summary:In this study we address the problem of training a neuralnetwork for languageidentification using both labeled and unlabeled speech samples in the form ofi-vectors. We propose a neural network architecture that can also handleout-of-set languages. We utilize a modified version of the recently proposedLadder Network semisupervised training procedure that optimizes thereconstruction costs of a stack of denoising autoencoders. We show that thisapproach can be successfully applied to the case where the training dataset iscomposed of both labeled and unlabeled acoustic data. The results show enhancedlanguage identification on the NIST 2015 language identification dataset.
arxiv-16800-159 | Automated Alertness and Emotion Detection for Empathic Feedback During E-Learning | http://arxiv.org/pdf/1604.00312v1.pdf | author:S L Happy, A. Dasgupta, P. Patnaik, A. Routray category:cs.CV cs.CY cs.HC published:2016-04-01 summary:In the context of education technology, empathic interaction with the userand feedback by the learning system using multiple inputs such as video, voiceand text inputs is an important area of research. In this paper, anonintrusive, standalone model for intelligent assessment of alertness andemotional state as well as generation of appropriate feedback has beenproposed. Using the non-intrusive visual cues, the system classifies emotionand alertness state of the user, and provides appropriate feedback according tothe detected cognitive state using facial expressions, ocular parameters,postures, and gestures. Assessment of alertness level using ocular parameterssuch as PERCLOS and saccadic parameters, emotional state from facial expressionanalysis, and detection of both relevant cognitive and emotional states fromupper body gestures and postures has been proposed. Integration of such asystem in e-learning environment is expected to enhance students performancethrough interaction, feedback, and positive mood induction.
arxiv-16800-160 | Using Recurrent Neural Networks to Optimize Dynamical Decoupling for Quantum Memory | http://arxiv.org/pdf/1604.00279v1.pdf | author:Moritz August, Xiaotong Ni category:quant-ph cs.LG cs.NE published:2016-04-01 summary:We utilize machine learning models which are based on recurrent neuralnetworks to optimize dynamical decoupling (DD) sequences. DD is a relativelysimple technique for suppressing the errors in quantum memory for certain noisemodels. In numerical simulations, we show that with minimum use of priorknowledge and starting from random sequences, the models are able to improveover time and eventually output DD-sequences with performance better than thatof the well known DD-families. Furthermore, our algorithm is easy to implementin experiments to find solutions tailored to the specific hardware, as ittreats the figure of merit as a black box.
arxiv-16800-161 | Network structure, metadata and the prediction of missing nodes | http://arxiv.org/pdf/1604.00255v1.pdf | author:Darko Hric, Tiago P. Peixoto, Santo Fortunato category:physics.soc-ph cs.SI stat.ML published:2016-04-01 summary:The empirical validation of community detection methods is often based onavailable annotations on the nodes that serve as putative indicators of thelarge-scale network structure. Most often, the suitability of the annotationsas topological descriptors itself is not assessed, and without this it is notpossible to ultimately distinguish between actual shortcomings of the communitydetection algorithms on one hand, and the incompleteness, inaccuracy orstructured nature of the data annotations themselves on the other. In this workwe present a principled method to access both aspects simultaneously. Weconstruct a joint generative model for the data and metadata, and anon-parametric Bayesian framework to infer its parameters from annotateddatasets. We assess the quality of the metadata not according to its directalignment with the network communities, but rather in its capacity to predictthe placement of edges in the network. We also show how this feature can beused to predict the connections to missing nodes when only the metadata isavailable. By investigating a wide range of datasets, we show that while thereare seldom exact agreements between metadata tokens and the inferred datagroups, the metadata is often informative of the network structurenevertheless, and can improve the prediction of missing nodes. This shows thatthe method uncovers meaningful patterns in both the data and metadata, withoutrequiring or expecting a perfect agreement between the two.
arxiv-16800-162 | COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution | http://arxiv.org/pdf/1507.02293v2.pdf | author:Mehrdad Farajtabar, Yichen Wang, Manuel Gomez Rodriguez, Shuang Li, Hongyuan Zha, Le Song category:cs.SI cs.LG physics.soc-ph stat.ML published:2015-07-08 summary:Information diffusion in online social networks is affected by the underlyingnetwork topology, but it also has the power to change it. Online users areconstantly creating new links when exposed to new information sources, and inturn these links are alternating the way information spreads. However, thesetwo highly intertwined stochastic processes, information diffusion and networkevolution, have been predominantly studied separately, ignoring theirco-evolutionary dynamics. We propose a temporal point process model, COEVOLVE, for such joint dynamics,allowing the intensity of one process to be modulated by that of the other.This model allows us to efficiently simulate interleaved diffusion and networkevents, and generate traces obeying common diffusion and network patternsobserved in real-world networks. Furthermore, we also develop a convexoptimization framework to learn the parameters of the model from historicaldiffusion and network evolution traces. We experimented with both syntheticdata and data gathered from Twitter, and show that our model provides a goodfit to the data as well as more accurate predictions than alternatives.
arxiv-16800-163 | Tensor Representations via Kernel Linearization for Action Recognition from 3D Skeletons | http://arxiv.org/pdf/1604.00239v1.pdf | author:Piotr Koniusz, Anoop Cherian, Fatih Porikli category:cs.CV published:2016-04-01 summary:In this paper, we explore tensor representations that can compactly capturehigher-order relationships between skeleton joints for 3D action recognition.We first define RBF kernels on 3D joint sequences, which are then linearized toform kernel descriptors. The higher-order outer-products of these kerneldescriptors form our tensor representations. We present two different kernelsfor action recognition, namely (i) a sequence compatibility kernel thatcaptures the spatio-temporal compatibility of joints in one sequence againstthose in the other, and (ii) a dynamics compatibility kernel that explicitlymodels the action dynamics of a sequence. Tensors formed from these kernels arethen used to train an SVM. We present experiments on several benchmark datasetsand demonstrate state of the art results, substantiating the effectiveness ofour representations.
arxiv-16800-164 | Reliable Prediction Intervals for Local Linear Regression | http://arxiv.org/pdf/1603.05587v4.pdf | author:Mohammad Ghasemi Hamed, Masoud Ebadi Kivaj category:stat.ME cs.LG published:2016-03-17 summary:This paper introduces two methods for estimating reliable predictionintervals for local linear least-squares regressions, named Bounded OscillationPrediction Intervals (BOPI). It also proposes a new measure for comparinginterval prediction models named Equivalent Gaussian Standard Deviation (EGSD).The experimental results compare BOPI to other methods using coverageprobability, Mean Interval Size and the introduced EGSD measure. The resultswere generally in favor of the BOPI on considered benchmark regressiondatasets. It also, reports simulation studies validating the BOPI method'sreliability.
arxiv-16800-165 | Gaussian process optimization through sampling from the maximum distribution | http://arxiv.org/pdf/1604.00169v1.pdf | author:Hildo Bijl, Thomas B. Schön, Jan-Willem van Wingerden, Michel Verhaegen category:stat.ML cs.SY published:2016-04-01 summary:This paper first presents a novel algorithm approximating the distribution ofthe maximum (both its position and its value) of a Gaussian process. Thisalgorithm uses particles in a similar way as Sequential Monte Carlo samplers.It is subsequently applied to the problem of Gaussian Process Optimization(GPO). The resulting GPO algorithm does not use an acquisition function, whichmakes it different from other GPO algorithms. Through various example problems,including a wind turbine load mitigation example, we find that the resultingalgorithm on average outperforms existing GPO algorithms. In addition, becauseno acquisition function has to be optimized, the algorithm can easily andefficiently be applied to problems with high-dimensional input spaces.
arxiv-16800-166 | Gradient-based learning algorithms with constant-error estimators: stability and convergence | http://arxiv.org/pdf/1604.00151v1.pdf | author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY stat.ML 93E15, 93E35 published:2016-04-01 summary:Implementations of stochastic gradient search algorithms such as backpropagation typically rely on finite difference ($FD$) approximation methods.These methods are used to approximate the objective function gradient insteepest descent algorithms as well as the gradient and Hessian inverse inNewton based schemes. The convergence analyses of such schemes criticallyrequire that perturbation parameters in the estimators of the gradient/Hessianapproach zero. However, in practice, the perturbation parameter is often heldfixed to a `small' constant resulting in constant-error estimates. We presentin this paper a theoretical framework based on set-valued dynamical systems toanalyze the aforementioned. Easily verifiable conditions are presented forstability and convergence when using such $FD$ estimators for thegradient/Hessian. In addition, our framework dispenses with a criticalrestriction on the step-sizes (learning rate) when using FD estimators.
arxiv-16800-167 | Learning a Pose Lexicon for Semantic Action Recognition | http://arxiv.org/pdf/1604.00147v1.pdf | author:Lijuan Zhou, Wanqing Li, Philip Ogunbona category:cs.CV published:2016-04-01 summary:This paper presents a novel method for learning a pose lexicon comprisingsemantic poses defined by textual instructions and their associated visualposes defined by visual features. The proposed method simultaneously takes twoinput streams, semantic poses and visual pose candidates, and statisticallylearns a mapping between them to construct the lexicon. With the learnedlexicon, action recognition can be cast as the problem of finding the maximumtranslation probability of a sequence of semantic poses given a stream ofvisual pose candidates. Experiments evaluating pre-trained and zero-shot actionrecognition conducted on MSRC-12 gesture and WorkoutSu-10 exercise datasetswere used to verify the efficacy of the proposed method.
arxiv-16800-168 | Multi-task Recurrent Model for Speech and Speaker Recognition | http://arxiv.org/pdf/1603.09643v2.pdf | author:Zhiyuan Tang, Lantian Li, Dong Wang category:cs.CL cs.LG cs.NE stat.ML published:2016-03-31 summary:Although highly correlated, speech and speaker recognition have been regardedas two independent tasks and studied by two communities. This is certainly notthe way that people behave: we decipher both speech content and speaker traitsat the same time. This paper presents a unified model to perform speech andspeaker recognition simultaneously and altogether. The model is based on aunified neural network where the output of one task is fed to the input of theother, leading to a multi-task recurrent network. Experiments show that thejoint model outperforms the task-specific models on both the two tasks.
arxiv-16800-169 | It's Moving! A Probabilistic Model for Causal Motion Segmentation in Moving Camera Videos | http://arxiv.org/pdf/1604.00136v1.pdf | author:Pia Bideau, Erik Learned-Miller category:cs.CV published:2016-04-01 summary:The human ability to detect and segment moving objects works in the presenceof multiple objects, complex background geometry, motion of the observer, andeven camouflage. In addition to all of this, the ability to detect motion isnearly instantaneous. While there has been much recent progress in motionsegmentation, it still appears we are far from human capabilities. In thiswork, we derive from first principles a new likelihood function for assessingthe probability of an optical flow vector given the 3D motion direction of anobject. This likelihood uses a novel combination of the angle and magnitude ofthe optical flow to maximize the information about the true motions of objects.Using this new likelihood and several innovations in initialization, we developa motion segmentation algorithm that beats current state-of-the-art methods bya large margin. We compare to five state-of-the-art methods on two establishedbenchmarks, and a third new data set of camouflaged animals, which we introduceto push motion segmentation to the next level.
arxiv-16800-170 | Good Practice in CNN Feature Transfer | http://arxiv.org/pdf/1604.00133v1.pdf | author:Liang Zheng, Yali Zhao, Shengjin Wang, Jingdong Wang, Qi Tian category:cs.CV published:2016-04-01 summary:The objective of this paper is the effective transfer of the ConvolutionalNeural Network (CNN) feature in image search and classification.Systematically, we study three facts in CNN transfer. 1) We demonstrate theadvantage of using images with a properly large size as input to CNN instead ofthe conventionally resized one. 2) We benchmark the performance of differentCNN layers improved by average/max pooling on the feature maps. Our observationsuggests that the Conv5 feature yields very competitive accuracy under suchpooling step. 3) We find that the simple combination of pooled featuresextracted across various CNN layers is effective in collecting evidences fromboth low and high level descriptors. Following these good practices, we arecapable of improving the state of the art on a number of benchmarks to a largemargin.
arxiv-16800-171 | The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations | http://arxiv.org/pdf/1511.02301v4.pdf | author:Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston category:cs.CL published:2015-11-07 summary:We introduce a new test of how well language models capture meaning inchildren's books. Unlike standard language modelling benchmarks, itdistinguishes the task of predicting syntactic function words from that ofpredicting lower-frequency words, which carry greater semantic content. Wecompare a range of state-of-the-art models, each with a different way ofencoding what has been previously read. We show that models which storeexplicit representations of long-term contexts outperform state-of-the-artneural language models at predicting semantic content words, although thisadvantage is not observed for syntactic function words. Interestingly, we findthat the amount of text encoded in a single memory representation is highlyinfluential to the performance: there is a sweet-spot, not too big and not toosmall, between single words and full sentences that allows the most meaningfulinformation in a text to be effectively retained and recalled. Further, theattention over such window-based memories can be trained effectively throughself-supervision. We then assess the generality of this principle by applyingit to the CNN QA benchmark, which involves identifying named entities inparaphrased summaries of news articles, and achieve state-of-the-artperformance.
arxiv-16800-172 | Nonparametric Spherical Topic Modeling with Word Embeddings | http://arxiv.org/pdf/1604.00126v1.pdf | author:Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, Sam Gershman category:cs.CL cs.IR cs.LG stat.ML published:2016-04-01 summary:Traditional topic models do not account for semantic regularities inlanguage. Recent distributional representations of words exhibit semanticconsistency over directional metrics such as cosine similarity. However,neither categorical nor Gaussian observational distributions used in existingtopic models are appropriate to leverage such correlations. In this paper, wepropose to use the von Mises-Fisher distribution to model the density of wordsover a unit sphere. Such a representation is well-suited for directional data.We use a Hierarchical Dirichlet Process for our base topic model and propose anefficient inference algorithm based on Stochastic Variational Inference. Thismodel enables us to naturally exploit the semantic structures of wordembeddings while flexibly discovering the number of topics. Experimentsdemonstrate that our method outperforms competitive approaches in terms oftopic coherence on two different text corpora while offering efficientinference.
arxiv-16800-173 | AttSum: Joint Learning of Focusing and Summarization with Neural Attention | http://arxiv.org/pdf/1604.00125v1.pdf | author:Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei category:cs.IR cs.CL published:2016-04-01 summary:Query relevance ranking and sentence saliency ranking are the two main tasksin extractive query-focused summarization. Previous supervised summarizationsystems often perform the two tasks in isolation. However, since referencesummaries are the trade-off between relevance and saliency, using them assupervision, neither of the two rankers could be trained well. This paperproposes a novel summarization system called AttSum, which tackles the twotasks jointly. It automatically learns distributed representations forsentences as well as the document cluster. Meanwhile, it applies the attentionmechanism to simulate the attentive reading of human behavior when a query isgiven. Extensive experiments are conducted on DUC query-focused summarizationbenchmark datasets. Without using any hand-crafted features, AttSum achievescompetitive performance. It is also observed that the sentences recognized tofocus on the query indeed meet the query need.
arxiv-16800-174 | Zipf's law emerges asymptotically during phase transitions in communicative systems | http://arxiv.org/pdf/1603.03153v2.pdf | author:Bohdan B. Khomtchouk, Claes Wahlestedt category:physics.soc-ph cs.CL published:2016-03-10 summary:Zipf's law predicts a power-law relationship between word rank and frequencyin language communication systems, and is widely reported in texts yet remainsenigmatic as to its origins. Computer simulations have shown that languagecommunication systems emerge at an abrupt phase transition in the fidelity ofmappings between symbols and objects. Since the phase transition approximatesthe Heaviside or step function, we show that Zipfian scaling emergesasymptotically at high rank based on the Laplace transform. We therebydemonstrate that Zipf's law gradually emerges from the moment of phasetransition in communicative systems. We show that this power-law scalingbehavior explains the emergence of natural languages at phase transitions. Wefind that the emergence of Zipf's law during language communication suggeststhat the use of rare words in a lexicon is critical for the construction of aneffective communicative system at the phase transition.
arxiv-16800-175 | Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding | http://arxiv.org/pdf/1604.00117v1.pdf | author:Aaron Jaech, Larry Heck, Mari Ostendorf category:cs.CL published:2016-04-01 summary:The goal of this paper is to use multi-task learning to efficiently scaleslot filling models for natural language understanding to handle multipletarget tasks or domains. The key to scalability is reducing the amount oftraining data needed to learn a model for a new task. The proposed multi-taskmodel delivers better performance with less data by leveraging patterns that itlearns from the other tasks. The approach supports an open vocabulary, whichallows the models to generalize to unseen words, which is particularlyimportant when very little training data is used. A newly collectedcrowd-sourced data set, covering four different domains, is used to demonstratethe effectiveness of the domain adaptation and open vocabulary techniques.
arxiv-16800-176 | Studying Very Low Resolution Recognition Using Deep Networks | http://arxiv.org/pdf/1601.04153v2.pdf | author:Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, Thomas S. Huang category:cs.CV cs.AI cs.LG published:2016-01-16 summary:Visual recognition research often assumes a sufficient resolution of theregion of interest (ROI). That is usually violated in practice, inspiring us toexplore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROIin a VLRR problem can be smaller than $16 \times 16$ pixels, and is challengingto be recognized even by human experts. We attempt to solve the VLRR problemusing deep learning methods. Taking advantage of techniques primarily in superresolution, domain adaptation and robust regression, we formulate a dedicateddeep learning method and demonstrate how these techniques are incorporated stepby step. Any extra complexity, when introduced, is fully justified by bothanalysis and simulation results. The resulting \textit{Robust Partially CoupledNetworks} achieves feature enhancement and recognition simultaneously. Itallows for both the flexibility to combat the LR-HR domain mismatch, and therobustness to outliers. Finally, the effectiveness of the proposed models isevaluated on three different VLRR tasks, including face identification, digitrecognition and font recognition, all of which obtain very impressiveperformances.
arxiv-16800-177 | The Open World of Micro-Videos | http://arxiv.org/pdf/1603.09439v2.pdf | author:Phuc Xuan Nguyen, Gregory Rogez, Charless Fowlkes, Deva Ramanan category:cs.CV published:2016-03-31 summary:Micro-videos are six-second videos popular on social media networks withseveral unique properties. Firstly, because of the authoring process, theycontain significantly more diversity and narrative structure than existingcollections of video "snippets". Secondly, because they are often captured byhand-held mobile cameras, they contain specialized viewpoints includingthird-person, egocentric, and self-facing views seldom seen in traditionalproduced video. Thirdly, due to to their continuous production and publicationon social networks, aggregate micro-video content contains interestingopen-world dynamics that reflects the temporal evolution of tag topics. Theseaspects make micro-videos an appealing well of visual data for developinglarge-scale models for video understanding. We analyze a novel dataset ofmicro-videos labeled with 58 thousand tags. To analyze this data, we introduceviewpoint-specific and temporally-evolving models for video understanding,defined over state-of-the-art motion and deep visual features. We conclude thatour dataset opens up new research opportunities for large-scale video analysis,novel viewpoints, and open-world dynamics.
arxiv-16800-178 | A Compositional Approach to Language Modeling | http://arxiv.org/pdf/1604.00100v1.pdf | author:Kushal Arora, Anand Rangarajan category:cs.CL published:2016-04-01 summary:Traditional language models treat language as a finite state automaton on aprobability space over words. This is a very strong assumption when modelingsomething inherently complex such as language. In this paper, we challenge thisby showing how the linear chain assumption inherent in previous work can betranslated into a sequential composition tree. We then propose a new model thatmarginalizes over all possible composition trees thereby removing anyunderlying structural assumptions. As the partition function of this new modelis intractable, we use a recently proposed sentence level evaluation metricContrastive Entropy to evaluate our model. Given this new evaluation metric, wereport more than 100% improvement across distortion levels over current stateof the art recurrent neural network based language models.
arxiv-16800-179 | Variational reaction-diffusion systems for semantic segmentation | http://arxiv.org/pdf/1604.00092v1.pdf | author:Paul Vernaza category:cs.CV cs.LG published:2016-04-01 summary:A novel global energy model for multi-class semantic image segmentation isproposed that admits very efficient exact inference and derivative calculationsfor learning. Inference in this model is equivalent to MAP inference in aparticular kind of vector-valued Gaussian Markov random field, and ultimatelyreduces to solving a linear system of linear PDEs known as a reaction-diffusionsystem. Solving this system can be achieved in time scaling near-linearly inthe number of image pixels by reducing it to sequential FFTs, after a linearchange of basis. The efficiency and differentiability of the model make itespecially well-suited for integration with convolutional neural networks, evenallowing it to be used in interior, feature-generating layers and stackedmultiple times. Experimental results are shown demonstrating that the model canbe employed profitably in conjunction with different convolutional netarchitectures, and that doing so compares favorably to joint training of afully-connected CRF with a convolutional net.
arxiv-16800-180 | Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection | http://arxiv.org/pdf/1604.00077v1.pdf | author:Sheng-syun Shen, Hung-yi Lee category:cs.CL published:2016-03-31 summary:Recurrent neural network architectures combining with attention mechanism, orneural attention model, have shown promising performance recently for the tasksincluding speech recognition, image caption generation, visual questionanswering and machine translation. In this paper, neural attention model isapplied on two sequence classification tasks, dialogue act detection and keyterm extraction. In the sequence labeling tasks, the model input is a sequence,and the output is the label of the input sequence. The major difficulty ofsequence labeling is that when the input sequence is long, it can include manynoisy or irrelevant part. If the information in the whole sequence is treatedequally, the noisy or irrelevant part may degrade the classificationperformance. The attention mechanism is helpful for sequence classificationtask because it is capable of highlighting important part among the entiresequence for the classification task. The experimental results show that withthe attention mechanism, discernible improvements were achieved in the sequencelabeling task considered here. The roles of the attention mechanism in thetasks are further analyzed and visualized in this paper.
arxiv-16800-181 | Modeling cumulative biological phenomena with Suppes-Bayes causal networks | http://arxiv.org/pdf/1602.07857v2.pdf | author:Daniele Ramazzotti, Alex Graudenzi, Marco Antoniotti category:cs.AI cs.LG published:2016-02-25 summary:Several statistical techniques have been recently developed for the inferenceof cancer progression models from the increasingly available NGScross-sectional mutational profiles. A particular algorithm, CAPRI, was provento be the most efficient with respect to sample size and level of noise in thedata. The algorithm combines structural constraints based on Suppes' theory ofprobabilistic causation and maximum likelihood fit with regularization, anddefines constrained Bayesian networks, named Suppes-Bayes Causal Networks(SBCNs), which account for the selective advantage relations among genomicevents. In general, SBCNs are effective in modeling any phenomenon driven bycumulative dynamics, as long as the modeled events are persistent. Here wediscuss on the effectiveness of the SBCN theoretical framework and weinvestigate the influence of: (i) the priors based on Suppes' theory and (ii)different maximum likelihood regularization parameters on the inferenceperformance estimated on large synthetically generated datasets.
arxiv-16800-182 | To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction | http://arxiv.org/pdf/1604.00066v1.pdf | author:Wenbin Li, Seyedmajid Azimi, Aleš Leonardis, Mario Fritz category:cs.CV cs.AI cs.RO published:2016-03-31 summary:Understanding physical phenomena is a key competence that enables humans andanimals to act and interact under uncertain perception in previously unseenenvironments containing novel object and their configurations. Developmentalpsychology has shown that such skills are acquired by infants from observationsat a very early stage. In this paper, we contrast a more traditional approach of taking amodel-based route with explicit 3D representations and physical simulation byan end-to-end approach that directly predicts stability and related quantitiesfrom appearance. We ask the question if and to what extent and quality such askill can directly be acquired in a data-driven way bypassing the need for anexplicit simulation. We present a learning-based approach based on simulated data that predictsstability of towers comprised of wooden blocks under different conditions andquantities related to the potential fall of the towers. The evaluation iscarried out on synthetic data and compared to human judgments on the samestimuli.
arxiv-16800-183 | Kernel Methods for the Approximation of Some Key Quantities of Nonlinear Systems | http://arxiv.org/pdf/1204.0563v2.pdf | author:Jake Bouvrie, Boumediene Hamzi category:math.OC math.DS stat.ML published:2012-04-03 summary:We introduce a data-based approach to estimating key quantities which arisein the study of nonlinear control systems and random nonlinear dynamicalsystems. Our approach hinges on the observation that much of the existinglinear theory may be readily extended to nonlinear systems - with a reasonableexpectation of success - once the nonlinear system has been mapped into a highor infinite dimensional feature space. In particular, we develop computable,non-parametric estimators approximating controllability and observabilityenergy functions for nonlinear systems, and study the ellipsoids they induce.In all cases the relevant quantities are estimated from simulated or observeddata. It is then shown that the controllability energy estimator provides a keymeans for approximating the invariant measure of an ergodic, stochasticallyforced nonlinear system.
arxiv-16800-184 | Kernel Methods for the Approximation of Nonlinear Systems | http://arxiv.org/pdf/1108.2903v3.pdf | author:Jake Bouvrie, Boumediene Hamzi category:math.OC cs.SY math.DS stat.ML published:2011-08-14 summary:We introduce a data-driven order reduction method for nonlinear controlsystems, drawing on recent progress in machine learning and statisticaldimensionality reduction. The method rests on the assumption that the nonlinearsystem behaves linearly when lifted into a high (or infinite) dimensionalfeature space where balanced truncation may be carried out implicitly. Thisleads to a nonlinear reduction map which can be combined with a representationof the system belonging to a reproducing kernel Hilbert space to give a closed,reduced order dynamical system which captures the essential input-outputcharacteristics of the original model. Empirical simulations illustrating theapproach are also provided.
arxiv-16800-185 | Undecidability of the Lambek calculus with a relevant modality | http://arxiv.org/pdf/1601.06303v2.pdf | author:Max Kanovich, Stepan Kuznetsov, Andre Scedrov category:math.LO cs.CL 03B47 published:2016-01-23 summary:Morrill and Valentin in the paper "Computational coverage of TLG:Nonlinearity" considered an extension of the Lambek calculus enriched by aso-called "exponential" modality. This modality behaves in the "relevant"style, that is, it allows contraction and permutation, but not weakening.Morrill and Valentin stated an open problem whether this system is decidable.Here we show its undecidability. Our result remains valid if we consider thefragment where all division operations have one direction. We also show thatthe derivability problem in a restricted case, where the modality can beapplied only to variables (primitive types), is decidable and belongs to the NPclass.
arxiv-16800-186 | Modeling Visual Compatibility through Hierarchical Mid-level Elements | http://arxiv.org/pdf/1604.00036v1.pdf | author:Jose Oramas, Tinne Tuytelaars category:cs.CV published:2016-03-31 summary:In this paper we present a hierarchical method to discover mid-level elementswith the objective of modeling visual compatibility between objects. At thebase-level, our method identifies patterns of CNN activations with the aim ofmodeling different variations/styles in which objects of the classes ofinterest may occur. At the top-level, the proposed method discovers patterns ofco-occurring activations of base-level elements that define visualcompatibility between pairs of object classes. Experiments on the massiveAmazon dataset show the strength of our method at describing object classes andthe characteristics that drive the compatibility between them.
arxiv-16800-187 | Improving Neural Machine Translation Models with Monolingual Data | http://arxiv.org/pdf/1511.06709v3.pdf | author:Rico Sennrich, Barry Haddow, Alexandra Birch category:cs.CL published:2015-11-20 summary:Neural Machine Translation (NMT) has obtained state-of-the art performancefor several language pairs, while only using parallel data for training.Target-side monolingual data plays an important role in boosting fluency forphrase-based statistical machine translation, and we investigate the use ofmonolingual data for NMT. In contrast to previous work, which combines NMTmodels with separately trained language models, we note that encoder-decoderNMT architectures already have the capacity to learn the same information as alanguage model, and we explore strategies to train with monolingual datawithout changing the neural network architecture. By pairing monolingualtraining data with an automatic back-translation, we can treat it as additionalparallel training data, and we obtain substantial improvements on the WMT 15task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 taskTurkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. Wealso show that fine-tuning on in-domain monolingual and parallel data givessubstantial improvements for the IWSLT 15 task English->German.
arxiv-16800-188 | Hierarchical Quickest Change Detection via Surrogates | http://arxiv.org/pdf/1603.09739v1.pdf | author:Prithwish Chakraborty, Sathappan Muthiah, Ravi Tandon, Naren Ramakrishnan category:cs.LG cs.IT math.IT stat.ML published:2016-03-31 summary:Change detection (CD) in time series data is a critical problem as it revealchanges in the underlying generative processes driving the time series. Despitehaving received significant attention, one important unexplored aspect is howto efficiently utilize additional correlated information to improve thedetection and the understanding of changepoints. We propose hierarchicalquickest change detection (HQCD), a framework that formalizes the process ofincorporating additional correlated sources for early changepoint detection.The core ideas behind HQCD are rooted in the theory of quickest detection andHQCD can be regarded as its novel generalization to a hierarchical setting. Thesources are classified into targets and surrogates, and HQCD leverages thisstructure to systematically assimilate observed data to update changepointstatistics across layers. The decision on actual changepoints are provided byminimizing the delay while still maintaining reliability bounds. In addition,HQCD also uncovers interesting relations between changes at targets fromchanges across surrogates. We validate HQCD for reliability and performanceagainst several state-of-the-art methods for both synthetic dataset (knownchangepoints) and several real-life examples (unknown changepoints). Ourexperiments indicate that we gain significant robustness without loss ofdetection delay through HQCD. Our real-life experiments also showcase theusefulness of the hierarchical setting by connecting the surrogate sources(such as Twitter chatter) to target sources (such as Employment relatedprotests that ultimately lead to major uprisings).
arxiv-16800-189 | Pessimistic Uplift Modeling | http://arxiv.org/pdf/1603.09738v1.pdf | author:Atef Shaar, Talel Abdessalem, Olivier Segard category:cs.LG published:2016-03-31 summary:Uplift modeling is a machine learning technique that aims to model treatmenteffects heterogeneity. It has been used in business and health sectors topredict the effect of a specific action on a given individual. Despite itsadvantages, uplift models show high sensitivity to noise and disturbance, whichleads to unreliable results. In this paper we show different approaches toaddress the problem of uplift modeling, we demonstrate how disturbance in datacan affect uplift measurement. We propose a new approach, we call itPessimistic Uplift Modeling, that minimizes disturbance effects. We comparedour approach with the existing uplift methods, on simulated and real data-sets.The experiments show that our approach outperforms the existing approaches,especially in the case of high noise data environment.
arxiv-16800-190 | Design of false color palettes for grayscale reproduction | http://arxiv.org/pdf/1602.03206v2.pdf | author:Filip A. Sala category:cs.GR cs.CV published:2016-02-06 summary:Design of false color palette is quite easy but some effort has to be done toachieve good dynamic range, contrast and overall appearance of the palette.Such palettes, for instance, are commonly used in scientific papers forpresenting the data. However, to lower the cost of the paper most scientistsdecide to let the data to be printed in grayscale. The same applies to e-bookreaders based on e-ink where most of them are still grayscale. For majority offalse color palettes reproducing them in grayscale results in ambiguous mappingof the colors and may be misleading for the reader. In this article design offalse color palettes suitable for grayscale reproduction is described. Due tothe monotonic change of luminance of these palettes grayscale representation isvery similar to the data directly presented with a grayscale palette. Somesuggestions and examples how to design such palettes are provided.
arxiv-16800-191 | Neural Language Correction with Character-Based Attention | http://arxiv.org/pdf/1603.09727v1.pdf | author:Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Jurafsky, Andrew Y. Ng category:cs.CL cs.AI published:2016-03-31 summary:Natural language correction has the potential to help language learnersimprove their writing skills. While approaches with separate classifiers fordifferent error types have high precision, they do not flexibly handle errorssuch as redundancy or non-idiomatic phrasing. On the other hand, word andphrase-based machine translation methods are not designed to cope withorthographic errors, and have recently been outpaced by neural models.Motivated by these issues, we present a neural network-based approach tolanguage correction. The core component of our method is an encoder-decoderrecurrent neural network with an attention mechanism. By operating at thecharacter level, the network avoids the problem of out-of-vocabulary words. Weillustrate the flexibility of our approach on dataset of noisy, user-generatedtext collected from an English learner forum. When combined with a languagemodel, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014Shared Task. We further demonstrate that training the network on additionaldata with synthesized errors can improve performance.
arxiv-16800-192 | Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion | http://arxiv.org/pdf/1603.09725v1.pdf | author:Israel D. Gebru, Silèye Ba, Xiaofei Li, Radu Horaud category:cs.CV cs.SD published:2016-03-31 summary:Speaker diarization consists of assigning speech signals to speakers engagedin dialog. An audio-visual spatiotemporal diarization model is proposed. Themodel is well suited for challenging scenarios that consist of severalparticipants engaged in multi-party dialog while they move around and turntheir heads towards the other participants rather than facing the cameras andthe microphones. Multiple-person visual tracking is combined with multiplespeech-source localization in order to tackle the person-to-speech associationproblem. The latter is solved within a novel audio-visual fusion method on thefollowing grounds: binaural spectral features are first extracted from amicrophone pair, then a supervised audio-visual alignment technique maps thesefeatures onto an image, and finally a semi-supervised clustering method assignsbinaural spectral features to visible persons. The main advantage of thismethod over previous work is that it processes in a principled way speechsignals uttered simultaneously by multiple persons. The diarization itself iscast into a latent-variable temporal graphical model that infers speakeridentities and speech turns, based on the output of the audio-visualassociation process available at each time slice, and on the dynamics of thediarization variable itself. The proposed formulation yields an efficient exactinference procedure. A novel dataset, that contains audio-visual training dataas well as a number of scenarios involving several participants engaged informal and informal dialog, is introduced. The proposed method is thoroughlytested and benchmarked with respect to several state-of-the art diarizationalgorithms.
arxiv-16800-193 | An Explicit Rate Bound for the Over-Relaxed ADMM | http://arxiv.org/pdf/1512.02063v2.pdf | author:Guilherme França, José Bento category:stat.ML math.OC published:2015-12-07 summary:The framework of Integral Quadratic Constraints of Lessard et al. (2014)reduces the computation of upper bounds on the convergence rate of severaloptimization algorithms to semi-definite programming (SDP). Followup work byNishihara et al. (2015) applies this technique to the entire family ofover-relaxed Alternating Direction Method of Multipliers (ADMM). Unfortunately,they only provide an explicit error bound for sufficiently large values of someof the parameters of the problem, leaving the computation for the general caseas a numerical optimization problem. In this paper we provide an exactanalytical solution to this SDP and obtain a general and explicit upper boundon the convergence rate of the entire family of over-relaxed ADMM. Furthermore,we demonstrate that it is not possible to extract from this SDP a general boundbetter than ours. We end with a few numerical illustrations of our result and acomparison between the convergence rate we obtain for the ADMM with knownconvergence rates for the Gradient Descent.
arxiv-16800-194 | BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies | http://arxiv.org/pdf/1511.06909v7.pdf | author:Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson, Pradeep Dubey category:cs.LG cs.CL cs.NE stat.ML published:2015-11-21 summary:We propose BlackOut, an approximation algorithm to efficiently train massiverecurrent neural network language models (RNNLMs) with million wordvocabularies. BlackOut is motivated by using a discriminative loss, and wedescribe a new sampling strategy which significantly reduces computation whileimproving stability, sample efficiency, and rate of convergence. One way tounderstand BlackOut is to view it as an extension of the DropOut strategy tothe output layer, wherein we use a discriminative training loss and a weightedsampling scheme. We also establish close connections between BlackOut,importance sampling, and noise contrastive estimation (NCE). Our experiments,on the recently released one billion word language modeling benchmark,demonstrate scalability and accuracy of BlackOut; we outperform thestate-of-the art, and achieve the lowest perplexity scores on this dataset.Moreover, unlike other established methods which typically require GPUs or CPUclusters, we show that a carefully implemented version of BlackOut requiresonly 1-10 days on a single machine to train a RNNLM with a million wordvocabulary and billions of parameters on one billion words. Although wedescribe BlackOut in the context of RNNLM training, it can be used to anynetworks with large softmax output layers.
arxiv-16800-195 | Stochastic subGradient Methods with Linear Convergence for Polyhedral Convex Optimization | http://arxiv.org/pdf/1510.01444v5.pdf | author:Tianbao Yang, Qihang Lin category:cs.LG math.OC published:2015-10-06 summary:In this paper, we show that simple {Stochastic} subGradient Decent methodswith multiple Restarting, named {\bf RSGD}, can achieve a \textit{linearconvergence rate} for a class of non-smooth and non-strongly convexoptimization problems where the epigraph of the objective function is apolyhedron, to which we refer as {\bf polyhedral convex optimization}. Itsapplications in machine learning include $\ell_1$ constrained or regularizedpiecewise linear loss minimization and submodular function minimization. To thebest of our knowledge, this is the first result on the linear convergence rateof stochastic subgradient methods for non-smooth and non-strongly convexoptimization problems.
arxiv-16800-196 | Building Better Detection with Privileged Information | http://arxiv.org/pdf/1603.09638v1.pdf | author:Z. Berkay Celik, Patrick McDaniel, Rauf Izmailov, Nicolas Papernot, Ananthram Swami category:cs.CR cs.LG stat.ML published:2016-03-31 summary:Modern detection systems use sensor outputs available in the deploymentenvironment to probabilistically identify attacks. These systems are trained onpast or synthetic feature vectors to create a model of anomalous or normalbehavior. Thereafter, run-time collected sensor outputs are compared to themodel to identify attacks (or the lack of attack). While this approach todetection has been proven to be effective in many environments, it is limitedto training on only features that can be reliably collected at test-time.Hence, they fail to leverage the often vast amount of ancillary informationavailable from past forensic analysis and post-mortem data. In short, detectionsystems don't train (and thus don't learn from) features that are unavailableor too costly to collect at run-time. In this paper, we leverage recentadvances in machine learning to integrate privileged information --featuresavailable at training time, but not at run-time-- into detection algorithms. Weapply three different approaches to model training with privileged information;knowledge transfer, model influence, and distillation, and empirically validatetheir performance in a range of detection domains. Our evaluation shows thatprivileged information can increase detector precision and recall: we observean average of 4.8% decrease in detection error for malware traffic detectionover a system with no privileged information, 3.53% for fast-flux domain botdetection, 3.33% for malware classification, 11.2% for facial userauthentication. We conclude by exploring the limitations and applications ofdifferent privileged information techniques in detection systems.
arxiv-16800-197 | Differentiable Pooling for Unsupervised Acoustic Model Adaptation | http://arxiv.org/pdf/1603.09630v1.pdf | author:Pawel Swietojanski, Steve Renals category:cs.CL cs.LG published:2016-03-31 summary:We present a deep neural network (DNN) acoustic model including parametrisedand differentiable pooling operators. Unsupervised acoustic model adaptation iscast as the problem of updating the decision boundaries implemented by eachpooling operator. In particular, we experiment with two types of poolingparametrisations: learned $L_p$-norm pooling and weighted Gaussian pooling, inwhich the weights of both operators are treated as speaker-dependent. Weperform investigations using three different large vocabulary speechrecognition corpora: AMI meetings, TED talks and Switchboard conversationaltelephone speech. We demonstrate that differentiable pooling operators providea robust and relatively low-dimensional way to adapt acoustic models, with worderror rates reductions ranging from 5--20\% with respect to unadapted systems,which themselves are better than the baseline fully-connected DNN-basedacoustic models. We also investigate how the proposed techniques work undervarious adaptation conditions including the quality of adaptation data andcomplementarity to other feature- and model-space adaptation methods, as wellas providing an analysis of the characteristics of each of the proposedapproaches.
arxiv-16800-198 | Online Optimization with Costly and Noisy Measurements using Random Fourier Expansions | http://arxiv.org/pdf/1603.09620v1.pdf | author:Laurens Bliek, Hans R. G. W. Verstraete, Michel Verhaegen, Sander Wahls category:cs.LG math.OC stat.ML published:2016-03-31 summary:This paper analyzes DONE, an online optimization algorithm that iterativelyminimizes an unknown function with costly and noisy measurements. The algorithmmaintains a surrogate of the unknown function in the form of a random Fourierexpansion (RFE). The surrogate is updated whenever a new measurement isavailable, and then used to determine the next measurement point. The algorithmis comparable to Bayesian optimization algorithms, but its computationalcomplexity per iteration does not depend on the number of measurements. Wederive several theoretical results that provide insight on how thehyperparameters of the algorithm should be chosen. The algorithm is compared toa Bayesian optimization algorithm for a benchmark problem and two opticsapplications, namely, optical coherence tomography and optical beam-formingnetwork tuning. It is found that the DONE algorithm is significantly fasterthan Bayesian optimization in all three discussed problems, while keeping asimilar or better performance.
arxiv-16800-199 | Total Variation Applications in Computer Vision | http://arxiv.org/pdf/1603.09599v1.pdf | author:Vania V. Estrela, Hermes Aguiar Magalhaes, Osamu Saotome category:cs.CV published:2016-03-31 summary:The objectives of this chapter are: (i) to introduce a concise overview ofregularization; (ii) to define and to explain the role of a particular type ofregularization called total variation norm (TV-norm) in computer vision tasks;(iii) to set up a brief discussion on the mathematical background of TVmethods; and (iv) to establish a relationship between models and a few existingmethods to solve problems cast as TV-norm. For the most part, image-processingalgorithms blur the edges of the estimated images, however TV regularizationpreserves the edges with no prior information on the observed and the originalimages. The regularization scalar parameter {\lambda} controls the amount ofregularization allowed and it is an essential to obtain a high-qualityregularized output. A wide-ranging review of several ways to put into practiceTV regularization as well as its advantages and limitations are discussed.
arxiv-16800-200 | Contrastive Entropy: A new evaluation metric for unnormalized language models | http://arxiv.org/pdf/1601.00248v2.pdf | author:Kushal Arora, Anand Rangarajan category:cs.CL published:2016-01-03 summary:Perplexity (per word) is the most widely used metric for evaluating languagemodels. Despite this, there has been no dearth of criticism for this metric.Most of these criticisms center around lack of correlation with extrinsicmetrics like word error rate (WER), dependence upon shared vocabulary for modelcomparison and unsuitability for unnormalized language model evaluation. Inthis paper, we address the last problem and propose a new discriminativeentropy based intrinsic metric that works for both traditional word levelmodels and unnormalized language models like sentence level models. We alsopropose a discriminatively trained sentence level interpretation of recurrentneural network based language model (RNN) as an example of unnormalizedsentence level model. We demonstrate that for word level models, contrastiveentropy shows a strong correlation with perplexity. We also observe that whentrained at lower distortion levels, sentence level RNN considerably outperformstraditional RNNs on this new metric.
arxiv-16800-201 | Sparse Representation of Multivariate Extremes with Applications to Anomaly Ranking | http://arxiv.org/pdf/1603.09584v1.pdf | author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML published:2016-03-31 summary:Extremes play a special role in Anomaly Detection. Beyond inference andsimulation purposes, probabilistic tools borrowed from Extreme Value Theory(EVT), such as the angular measure, can also be used to design novelstatistical learning methods for Anomaly Detection/ranking. This paper proposesa new algorithm based on multivariate EVT to learn how to rank observations ina high dimensional space with respect to their degree of 'abnormality'. Theprocedure relies on an original dimension-reduction technique in the extremedomain that possibly produces a sparse representation of multivariate extremesand allows to gain insight into the dependence structure thereof, escaping thecurse of dimensionality. The representation output by the unsupervisedmethodology we propose here can be combined with any Anomaly Detectiontechnique tailored to non-extreme data. As it performs linearly with thedimension and almost linearly in the data (in O(dn log n)), it fits to largescale problems. The approach in this paper is novel in that EVT has never beenused in its multivariate version in the field of Anomaly Detection.Illustrative experimental results provide strong empirical evidence of therelevance of our approach.
arxiv-16800-202 | An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes | http://arxiv.org/pdf/1509.01520v2.pdf | author:Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero, Radu Horaud category:cs.CV stat.ML published:2015-09-04 summary:Object tracking is an ubiquitous problem that appears in many applicationssuch as remote sensing, audio processing, computer vision, human-machineinterfaces, human-robot interaction, etc. Although thoroughly investigated incomputer vision, tracking a time-varying number of persons remains achallenging open problem. In this paper, we propose an on-line variationalBayesian model for multi-person tracking from cluttered visual observationsprovided by person detectors. The contributions of this paper are thefollowings. First, we propose a variational Bayesian framework for tracking anunknown and varying number of persons. Second, our model results in avariational expectation-maximization (VEM) algorithm with closed-formexpressions for the posterior distributions of the latent variables and for theestimation of the model parameters. Third, the proposed model exploitsobservations from multiple detectors, and it is therefore multimodal by nature.Finally, we propose to embed both object-birth and object-visibility processesin an effort to robustly handle person appearances and disappearances overtime. Evaluated on classical multiple person tracking datasets, our methodshows competitive results with respect to state-of-the-art multiple-objecttracking models, such as the probability hypothesis density (PHD) filter amongothers.
arxiv-16800-203 | Sub-pixel accuracy edge fitting by means of B-spline | http://arxiv.org/pdf/1603.09558v1.pdf | author:R. L. B. Breder, Vania V. Estrela, J. T. de Assis category:cs.CV published:2016-03-31 summary:Local perturbations around contours strongly disturb the final result ofcomputer vision tasks. It is common to introduce a priori information in theestimation process. Improvement can be achieved via a deformable model such asthe snake model. In recent works, the deformable contour is modeled by means ofB-spline snakes which allows local control, concise representation, and the useof fewer parameters. The estimation of the sub-pixel edges using a globalB-spline model relies on the contour global determination according to amaximum likelihood framework and using the observed data likelihood. Thisprocedure guarantees that the noisiest data will be filtered out. The datalikelihood is computed as a consequence of the observation model which includesboth orientation and position information. Comparative experiments of thisalgorithm and the classical spline interpolation have shown that the proposedalgorithm outperforms the classical approach for Gaussian and Salt & Peppernoise.
arxiv-16800-204 | Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction | http://arxiv.org/pdf/1603.07893v2.pdf | author:Hengjian Jia category:cs.NE cs.LG published:2016-03-25 summary:The effectiveness of long short term memory networks trained bybackpropagation through time for stock price prediction is explored in thispaper. A range of different architecture LSTM networks are constructed trainedand tested.
arxiv-16800-205 | Monomer: Non-Metric Mixtures-of-Embeddings for Learning Visual Compatibility Across Categories | http://arxiv.org/pdf/1603.09473v1.pdf | author:Ruining He, Charles Packer, Julian McAuley category:cs.IR cs.CV cs.LG published:2016-03-31 summary:Identifying relationships between items is a key task of an onlinerecommender system, in order to help users discover items that are functionallycomplementary or visually compatible. In domains like clothing recommendation,this task is particularly challenging since a successful system should becapable of handling a large corpus of items, a huge amount of relationshipsamong them, as well as the high-dimensional and semantically complicatedfeatures involved. Furthermore, the human notion of "compatibility" that weneed to capture goes beyond mere similarity: For two items to becompatible---whether jeans and a t-shirt, or a laptop and a charger---theyshould be similar in some ways, but systematically different in others. In this paper we develop a method, Monomer, to uncover complicated andheterogeneous of relationships between items. Recently, scalable methods havebeen developed that address this task by learning embeddings of the visual andtextual characteristics of the products involved, but which ultimately dependon a nearest-neighbor assumption between the learned embeddings. Here we showthat richer notions of compatibility can be learned, principally by relaxingthe metricity assumption inherent in previous work, so as to uncover ways inwhich related items should be systematically similar, and systematicallydifferent. Quantitatively, we show that our system achieves state-of-the-artperformance on large-scale compatibility prediction tasks, especially in caseswhere there is substantial heterogeneity between related items.
arxiv-16800-206 | Hybrid Ant Colony Optimization in solving Multi-Skill Resource-Constrained Project Scheduling Problem | http://arxiv.org/pdf/1603.08538v2.pdf | author:Paweł B. Myszkowski, Marek E. Skowroński, Łukasz P. Olech, Krzysztof Oślizło category:cs.NE published:2016-03-28 summary:In this paper Hybrid Ant Colony Optimization (HAntCO) approach in solvingMulti--Skill Resource Constrained Project Scheduling Problem (MS--RCPSP) hasbeen presented. We have proposed hybrid approach that links classical heuristicpriority rules for project scheduling with Ant Colony Optimization (ACO).Furthermore, a novel approach for updating pheromone value has been proposed,based on both the best and worst solutions stored by ants. The objective ofthis paper is to research the usability and robustness of ACO and its hybridswith priority rules in solving MS--RCPSP. Experiments have been performed usingartificially created dataset instances, based on real--world ones. We publishedthose instances that can be used as a benchmark. Presented results show thatACO--based hybrid method is an efficient approach. More directed search processby hybrids makes this approach more stable and provides mostly better resultsthan classical ACO.
arxiv-16800-207 | A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System | http://arxiv.org/pdf/1603.09469v1.pdf | author:Hyunsuk Ko, Rui Song, C. -C. Jay Kuo category:cs.CV cs.LG published:2016-03-31 summary:The problem of stereoscopic image quality assessment, which findsapplications in 3D visual content delivery such as 3DTV, is investigated inthis work. Specifically, we propose a new ParaBoost (parallel-boosting)stereoscopic image quality assessment (PBSIQA) system. The system consists oftwo stages. In the first stage, various distortions are classified into a fewtypes, and individual quality scorers targeting at a specific distortion typeare developed. These scorers offer complementary performance in face of adatabase consisting of heterogeneous distortion types. In the second stage,scores from multiple quality scorers are fused to achieve the best overallperformance, where the fuser is designed based on the parallel boosting ideaborrowed from machine learning. Extensive experimental results are conducted tocompare the performance of the proposed PBSIQA system with those of existingstereo image quality assessment (SIQA) metrics. The developed quality metriccan serve as an objective function to optimize the performance of a 3D contentdelivery system.
arxiv-16800-208 | Getting started with particle Metropolis-Hastings for inference in nonlinear dynamical models | http://arxiv.org/pdf/1511.01707v4.pdf | author:Johan Dahlin, Thomas B. Schön category:stat.CO q-fin.ST stat.ML published:2015-11-05 summary:We provide a gentle introduction to the particle Metropolis-Hastings (PMH)algorithm for parameter inference in nonlinear state space models (SSMs)together with a software implementation in the statistical programming languageR. Throughout this tutorial, we develop an implementation of the PMH algorithm(and the integrated particle filter), which is distributed as the packagepmhtutorial available from the CRAN repository. Moreover, we provide the readerwith some intuition for how the algorithm operates and discuss some solutionsto numerical problems that might occur in practice. To illustrate the use ofPMH, we consider parameter inference in a linear Gaussian SSM with syntheticdata and a nonlinear stochastic volatility model with real-world data. Weconclude the tutorial by discussing important possible improvements to thealgorithm and we also list suitable references for further study.
arxiv-16800-209 | Cost-sensitive Label Embedding for Multi-label Classification | http://arxiv.org/pdf/1603.09048v2.pdf | author:Kuan-Hao Huang, Hsuan-Tien Lin category:cs.LG stat.ML published:2016-03-30 summary:Label embedding (LE) is an important family of multi-label classificationalgorithms that digest the label information jointly for better performance.Different real-world applications evaluate performance by different costfunctions of interest. Current LE algorithms often aim to optimize one specificcost function, but they can suffer from bad performance with respect to othercost functions. In this paper, we resolve the performance issue by proposing anovel cost-sensitive LE algorithm that takes the cost function of interest intoaccount. The proposed algorithm is based on using distances of the embeddedvectors to approximate the cost information, and takes the classic manifoldlearning approach to compute the embedded vectors. The algorithm can deal withboth symmetric and asymmetric cost functions, and effectively makescost-sensitive decisions by nearest-neighbor decoding within the embeddedvectors. Extensive experimental results justify that the proposed algorithm issignificantly better than a wide spectrum of existing LE algorithms acrossdifferent cost functions.
arxiv-16800-210 | Robust Uncalibrated Stereo Rectification with Constrained Geometric Distortions (USR-CGD) | http://arxiv.org/pdf/1603.09462v1.pdf | author:Hyunsuk Ko, Han Suk Shim, Ouk Choi, C. -C. Jay Kuo category:cs.CV published:2016-03-31 summary:A novel algorithm for uncalibrated stereo image-pair rectification under theconstraint of geometric distortion, called USR-CGD, is presented in this work.Although it is straightforward to define a rectifying transformation (orhomography) given the epipolar geometry, many existing algorithms have unwantedgeometric distortions as a side effect. To obtain rectified images with reducedgeometric distortions while maintaining a small rectification error, weparameterize the homography by considering the influence of various kinds ofgeometric distortions. Next, we define several geometric measures andincorporate them into a new cost function for parameter optimization. Finally,we propose a constrained adaptive optimization scheme to allow a balancedperformance between the rectification error and the geometric error. Extensiveexperimental results are provided to demonstrate the superb performance of theproposed USR-CGD method, which outperforms existing algorithms by a significantmargin.
arxiv-16800-211 | System Combination for Short Utterance Speaker Recognition | http://arxiv.org/pdf/1603.09460v1.pdf | author:Lantian Li, Dong Wang, Thomas Fang Zheng category:cs.CL cs.NE published:2016-03-31 summary:Noticeable performance degradation is often observed in text-independentspeaker recognition with short test utterances. This paper presents acombination approach to improve short utterance speaker recognition (SUSR),where two phonetic-aware systems are combined together: one is the DNN-basedi-vector system and the other is the subregion-based GMM-UBM system proposed byus recently. The former employs phone posteriors to construct an i-vector modelin which the shared statistics offer stronger robustness against limited testdata. The latter establishes a phone-dependent GMM-UBM system which representsspeaker characteristics with more details. A scorelevel system combinationapproach is proposed to integrate the respective advantages of the two systems.Experimental results confirm that on the text-independent SUSR task, both theDNN-based i-vector system and the subregion-based GMM-UBM system outperformtheir respective baselines, and the score-level system combination deliverssignificant performance improvement.
arxiv-16800-212 | Binary Speaker Embedding | http://arxiv.org/pdf/1510.05937v2.pdf | author:Lantian Li, Dong Wang, Chao Xing, Kaimin Yu, Thomas Fang Zheng category:cs.SD cs.LG published:2015-10-20 summary:The popular i-vector model represents speakers as low-dimensional continuousvectors (i-vectors), and hence it is a way of continuous speaker embedding. Inthis paper, we investigate binary speaker embedding, which transforms i-vectorsto binary vectors (codes) by a hash function. We start from locality sensitivehashing (LSH), a simple binarization approach where binary codes are derivedfrom a set of random hash functions. A potential problem of LSH is that therandomly sampled hash functions might be suboptimal. We therefore propose animproved Hamming distance learning approach, where the hash function is learnedby a variable-sized block training that projects each dimension of the originali-vectors to variable-sized binary codes independently. Our experiments showthat binary speaker embedding can deliver competitive or even better results onboth speaker verification and identification tasks, while the memory usage andthe computation cost are significantly reduced.
arxiv-16800-213 | Max-margin Metric Learning for Speaker Recognition | http://arxiv.org/pdf/1510.05940v2.pdf | author:Lantian Li, Dong Wang, Chao Xing, Thomas Fang Zheng category:cs.SD cs.LG published:2015-10-20 summary:Probabilistic linear discriminant analysis (PLDA) is a popular normalizationapproach for the i-vector model, and has delivered state-of-the-art performancein speaker recognition. A potential problem of the PLDA model, however, is thatit essentially assumes Gaussian distributions over speaker vectors, which isnot always true in practice. Additionally, the objective function is notdirectly related to the goal of the task, e.g., discriminating true speakersand imposters. In this paper, we propose a max-margin metric learning approachto solve the problems. It learns a linear transform with a criterion that themargin between target and imposter trials are maximized. Experiments conductedon the SRE08 core test show that compared to PLDA, the new approach can obtaincomparable or even better performance, though the scoring is simply a cosinecomputation.
arxiv-16800-214 | LSTM based Conversation Models | http://arxiv.org/pdf/1603.09457v1.pdf | author:Yi Luan, Yangfeng Ji, Mari Ostendorf category:cs.CL published:2016-03-31 summary:In this paper, we present a conversational model that incorporates bothcontext and participant role for two-party conversations. Differentarchitectures are explored for integrating participant role and contextinformation into a Long Short-term Memory (LSTM) language model. Theconversational model can function as a language model or a language generationmodel. Experiments on the Ubuntu Dialog Corpus show that our model can capturemultiple turn interaction between participants. The proposed method outperformsa traditional LSTM model as measured by language model perplexity and responseranking. Generated responses show characteristic differences between the twoparticipant roles.
arxiv-16800-215 | Exemplar-AMMs: Recognizing Crowd Movements from Pedestrian Trajectories | http://arxiv.org/pdf/1603.09454v1.pdf | author:Wenxi Liu, Rynson W. H. Lau, Xiaogang Wang, Dinesh Manocha category:cs.CV published:2016-03-31 summary:In this paper, we present a novel method to recognize the types of crowdmovement from crowd trajectories using agent-based motion models (AMMs). Ouridea is to apply a number of AMMs, referred to as exemplar-AMMs, to describethe crowd movement. Specifically, we propose an optimization framework thatfilters out the unknown noise in the crowd trajectories and measures theirsimilarity to the exemplar-AMMs to produce a crowd motion feature. We thenaddress our real-world crowd movement recognition problem as a multi-labelclassification problem. Our experiments show that the proposed featureoutperforms the state-of-the-art methods in recognizing both simulated andreal-world crowd movements from their trajectories. Finally, we have created asynthetic dataset, SynCrowd, which contains 2D crowd trajectories in variousscenarios, generated by various crowd simulators. This dataset can serve as atraining set or benchmark for crowd analysis work.
arxiv-16800-216 | A Mathematical Formalization of Hierarchical Temporal Memory's Spatial Pooler | http://arxiv.org/pdf/1601.06116v2.pdf | author:James Mnatzaganian, Ernest Fokoué, Dhireesha Kudithipudi category:stat.ML cs.LG q-bio.NC published:2016-01-22 summary:Hierarchical temporal memory (HTM) is an emerging machine learning algorithm,with the potential to provide a means to perform predictions on spatiotemporaldata. The algorithm, inspired by the neocortex, currently does not have acomprehensive mathematical framework. This work brings together all aspects ofthe spatial pooler (SP), a critical learning component in HTM, under a singleunifying framework. The primary learning mechanism is explored, where a maximumlikelihood estimator for determining the degree of permanence update isproposed. The boosting mechanisms are studied and found to be only relevantduring the initial few iterations of the network. Observations are maderelating HTM to well-known algorithms such as competitive learning andattribute bagging. Methods are provided for using the SP for classification aswell as dimensionality reduction. Empirical evidence verifies that given theproper parameterizations, the SP may be used for feature learning.
arxiv-16800-217 | A Stratified Analysis of Bayesian Optimization Methods | http://arxiv.org/pdf/1603.09441v1.pdf | author:Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra Johnson, George Ke category:cs.LG stat.ML published:2016-03-31 summary:Empirical analysis serves as an important complement to theoretical analysisfor studying practical Bayesian optimization. Often empirical insights exposestrengths and weaknesses inaccessible to theoretical analysis. We define twometrics for comparing the performance of Bayesian optimization methods andpropose a ranking mechanism for summarizing performance within various genresor strata of test functions. These test functions serve to mimic the complexityof hyperparameter optimization problems, the most prominent application ofBayesian optimization, but with a closed form which allows for rapid evaluationand more predictable behavior. This offers a flexible and efficient way toinvestigate functions with specific properties of interest, such as oscillatorybehavior or an optimum on the domain boundary.
arxiv-16800-218 | Rich Image Captioning in the Wild | http://arxiv.org/pdf/1603.09016v2.pdf | author:Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, Chris Buehler, Chris Sienkiewicz category:cs.CV published:2016-03-30 summary:We present an image caption system that addresses new challenges ofautomatically describing images in the wild. The challenges include highquality caption quality with respect to human judgments, out-of-domain datahandling, and low latency required in many applications. Built on top of astate-of-the-art framework, we developed a deep vision model that detects abroad range of visual concepts, an entity recognition model that identifiescelebrities and landmarks, and a confidence model for the caption output.Experimental results show that our caption engine outperforms previousstate-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO)and out of-domain datasets.
arxiv-16800-219 | Investigating practical linear temporal difference learning | http://arxiv.org/pdf/1602.08771v2.pdf | author:Adam White, Martha White category:cs.LG cs.AI stat.ML published:2016-02-28 summary:Off-policy reinforcement learning has many applications including: learningfrom demonstration, learning multiple goal seeking policies in parallel, andrepresenting predictive knowledge. Recently there has been an proliferation ofnew policy-evaluation algorithms that fill a longstanding algorithmic void inreinforcement learning: combining robustness to off-policy sampling, functionapproximation, linear complexity, and temporal difference (TD) updates. Thispaper contains two main contributions. First, we derive two new hybrid TDpolicy-evaluation algorithms, which fill a gap in this collection ofalgorithms. Second, we perform an empirical comparison to elicit which of thesenew linear TD methods should be preferred in different situations, and makeconcrete suggestions about practical use.
arxiv-16800-220 | Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network | http://arxiv.org/pdf/1603.09423v1.pdf | author:Tong He, Weilin Huang, Yu Qiao, Jian Yao category:cs.CV published:2016-03-31 summary:We introduce a new top-down pipeline for scene text detection. We propose anovel Cascaded Convolutional Text Network (CCTN) that joints two customizedconvolutional networks for coarse-to-fine text localization. The CCTN fastdetects text regions roughly from a low-resolution image, and then accuratelylocalizes text lines from each enlarged region. We cast previous characterbased detection into direct text region estimation, avoiding multiple bottom-up post-processing steps. It exhibits surprising robustness and discriminativepower by considering whole text region as detection object which providesstrong semantic information. We customize convolutional network by develop- ingrectangle convolutions and multiple in-network fusions. This enables it tohandle multi-shape and multi-scale text efficiently. Furthermore, the CCTN iscomputationally efficient by sharing convolutional computations, and high-levelproperty allows it to be invariant to various languages and multipleorientations. It achieves 0.84 and 0.86 F-measures on the ICDAR 2011 and ICDAR2013, delivering substantial improvements over state-of-the-art results [23,1].
arxiv-16800-221 | Minimal Gated Unit for Recurrent Neural Networks | http://arxiv.org/pdf/1603.09420v1.pdf | author:Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, Zhi-Hua Zhou category:cs.NE cs.LG published:2016-03-31 summary:Recently recurrent neural networks (RNN) has been very successful in handlingsequence data. However, understanding RNN and finding the best practices forRNN is a difficult task, partly because there are many competing and complexhidden units (such as LSTM and GRU). We propose a gated unit for RNN, named asMinimal Gated Unit (MGU), since it only contains one gate, which is a minimaldesign among all gated hidden units. The design of MGU benefits from evaluationresults on LSTM and GRU in the literature. Experiments on various sequence datashow that MGU has comparable accuracy with GRU, but has a simpler structure,fewer parameters, and faster training. Hence, MGU is suitable in RNN'sapplications. Its simple architecture also means that it is easier to evaluateand tune, and in principle it is easier to study MGU's properties theoreticallyand empirically.
arxiv-16800-222 | Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding | http://arxiv.org/pdf/1603.09405v1.pdf | author:Peng Li, Heng Huang category:cs.CL cs.AI cs.NE published:2016-03-30 summary:Neural network based approaches for sentence relation modeling automaticallygenerate hidden matching features from raw sentence pairs. However, the qualityof matching feature representation may not be satisfied due to complex semanticrelations such as entailment or contradiction. To address this challenge, wepropose a new deep neural network architecture that jointly leveragepre-trained word embedding and auxiliary character embedding to learn sentencemeanings. The two kinds of word sequence representations as inputs intomulti-layer bidirectional LSTM to learn enhanced sentence representation. Afterthat, we construct matching features followed by another temporal CNN to learnhigh-level hidden matching feature representations. Experimental resultsdemonstrate that our approach consistently outperforms the existing methods onstandard evaluation datasets.
arxiv-16800-223 | Higher Order Conditional Random Fields in Deep Neural Networks | http://arxiv.org/pdf/1511.08119v3.pdf | author:Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, Philip Torr category:cs.CV published:2015-11-25 summary:We address the problem of semantic segmentation using deep learning. Mostsegmentation systems include a Conditional Random Field (CRF) to produce astructured output that is consistent with the image's visual features. Recentdeep learning approaches have incorporated CRFs into Convolutional NeuralNetworks (CNNs), with some even training the CRF end-to-end with the rest ofthe network. However, these approaches have not employed higher orderpotentials, which have previously been shown to significantly improvesegmentation performance. In this paper, we demonstrate that two types ofhigher order potential, based on object detections and superpixels, can beincluded in a CRF embedded within a deep network. We design these higher orderpotentials to allow inference with the differentiable mean field algorithm. Asa result, all the parameters of our richer CRF model can be learned end-to-endwith our pixelwise CNN classifier. We achieve state-of-the-art segmentationperformance on the PASCAL VOC benchmark with these trainable higher orderpotentials.
arxiv-16800-224 | SSD: Single Shot MultiBox Detector | http://arxiv.org/pdf/1512.02325v2.pdf | author:Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg category:cs.CV published:2015-12-08 summary:We present a method for detecting objects in images using a single deepneural network. Our approach, named SSD, discretizes the output space ofbounding boxes into a set of default boxes over different aspect ratios andscales per feature map location. At prediction time, the network generatesscores for the presence of each object category in each default box andproduces adjustments to the box to better match the object shape. Additionally,the network combines predictions from multiple feature maps with differentresolutions to naturally handle objects of various sizes. Our SSD model issimple relative to methods that require object proposals because it completelyeliminates proposal generation and subsequent pixel or feature resampling stageand encapsulates all computation in a single network. This makes SSD easy totrain and straightforward to integrate into systems that require a detectioncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasetsconfirm that SSD has comparable accuracy to methods that utilize an additionalobject proposal step and is much faster, while providing a unified frameworkfor both training and inference. Compared to other single stage methods, SSDhas much better accuracy, even with a smaller input image size. For $300\times300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia TitanX and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming acomparable state of the art Faster R-CNN model. Code is available at\url{https://github.com/weiliu89/caffe/tree/ssd} .
arxiv-16800-225 | Clinical Information Extraction via Convolutional Neural Network | http://arxiv.org/pdf/1603.09381v1.pdf | author:Peng Li, Heng Huang category:cs.LG cs.CL cs.NE published:2016-03-30 summary:We report an implementation of a clinical information extraction tool thatleverages deep neural network to annotate event spans and their attributes fromraw clinical notes and pathology reports. Our approach uses context words andtheir part-of-speech tags and shape information as features. Then we hiretemporal (1D) convolutional neural network to learn hidden featurerepresentations. Finally, we use Multilayer Perceptron (MLP) to predict eventspans. The empirical evaluation demonstrates that our approach significantlyoutperforms baselines.
arxiv-16800-226 | Partial Face Detection for Continuous Authentication | http://arxiv.org/pdf/1603.09364v1.pdf | author:Upal Mahbub, Vishal M. Patel, Deepak Chandra, Brandon Barbello, Rama Chellappa category:cs.CV published:2016-03-30 summary:In this paper, a part-based technique for real time detection of users' faceson mobile devices is proposed. This method is specifically designed fordetecting partially cropped and occluded faces captured using a smartphone'sfront-facing camera for continuous authentication. The key idea is to detectfacial segments in the frame and cluster the results to obtain the region whichis most likely to contain a face. Extensive experimentation on a mobile datasetof 50 users shows that our method performs better than many state-of-the-artface detection methods in terms of accuracy and processing speed.
arxiv-16800-227 | Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index | http://arxiv.org/pdf/1603.09326v1.pdf | author:Susan Athey, Raj Chetty, Guido Imbens, Hyunseung Kang category:stat.ME stat.ML published:2016-03-30 summary:Estimating the long-term effects of treatments is of interest in many fields.A common challenge in estimating such treatment effects is that long-termoutcomes are unobserved in the time frame needed to make policy decisions. Oneapproach to overcome this missing data problem is to analyze treatments effectson an intermediate outcome, often called a statistical surrogate, if itsatisfies the condition that treatment and outcome are independent conditionalon the statistical surrogate. The validity of the surrogacy condition is oftencontroversial. Here we exploit that fact that in modern datasets, researchersoften observe a large number, possibly hundreds or thousands, of intermediateoutcomes, thought to lie on or close to the causal chain between the treatmentand the long-term outcome of interest. Even if none of the individual proxiessatisfies the statistical surrogacy criterion by itself, using multiple proxiescan be useful in causal inference. We focus primarily on a setting with twosamples, an experimental sample containing data about the treatment indicatorand the surrogates and an observational sample containing information about thesurrogates and the primary outcome. We state assumptions under which theaverage treatment effect be identified and estimated with a high-dimensionalvector of proxies that collectively satisfy the surrogacy assumption, andderive the bias from violations of the surrogacy assumption, and show that evenif the primary outcome is also observed in the experimental sample, there isstill information to be gained from using surrogates.
arxiv-16800-228 | Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs | http://arxiv.org/pdf/1603.09320v1.pdf | author:Yu. A. Malkov, D. A. Yashunin category:cs.DS cs.CV cs.IR cs.SI published:2016-03-30 summary:We present a new algorithm for the approximate nearest neighbor search basedon navigable small world graphs with controllable hierarchy (Hierarchical NSW)admitting simple insertion, deletion and K-nearest neighbor queries. TheHierarchical NSW is a fully graph-based approach without a need for additionalsearch structures (such as kd-trees or Cartesian concatenation) typically usedat coarse search stage of the most proximity graph techniques. The algorithmincrementally builds a layered structure consisting from hierarchical set ofproximity graphs (layers) for nested subsets of the stored elements. Themaximum layer in which an element is present is selected randomly withexponentially decaying probability distribution. This allows producing graphssimilar to the previously studied Navigable Small World (NSW) structures whileadditionally having the links separated by their characteristic distancescales. Starting search from the upper layer instead of random seeds togetherwith utilizing the scale separation boosts the performance compared to the NSWand allows a logarithmic complexity scaling. Additional employment of a simpleheuristic for selecting proximity graph neighbors increases performance at highrecall and in case of highly clustered data. Performance evaluation on a largenumber of datasets has demonstrated that the proposed general metric spacemethod is able to strongly outperform many previous state-of-art vector-onlyapproaches such as FLANN, FALCONN and Annoy. Similarity of the algorithm to awell-known 1D skip list structure allows straightforward efficient and balanceddistributed implementation.
arxiv-16800-229 | Degrees of Freedom in Deep Neural Networks | http://arxiv.org/pdf/1603.09260v1.pdf | author:Tianxiang Gao, Vladimir Jojic category:cs.LG stat.ML published:2016-03-30 summary:In this paper, we explore degrees of freedom in deep sigmoidal neuralnetworks. We show that the degrees of freedom in these models is related to theexpected optimism, which is the expected difference between test error andtraining error. We provide an efficient Monte-Carlo method to estimate thedegrees of freedom for multi-class classification methods. We show degrees offreedom are lower than the parameter count in a simple XOR network. We extendthese results to neural nets trained on synthetic and real data, andinvestigate impact of network's architecture and different regularizationchoices. The degrees of freedom in deep networks are dramatically smaller thanthe number of parameters, in some real datasets several orders of magnitude.Further, we observe that for fixed number of parameters, deeper networks haveless degrees of freedom exhibiting a regularization-by-depth.
arxiv-16800-230 | A latent-observed dissimilarity measure | http://arxiv.org/pdf/1603.09254v1.pdf | author:Yasushi Terazono category:stat.ML published:2016-03-30 summary:Quantitatively assessing relationships between latent variables and observedvariables is important for understanding and developing generative models andrepresentation learning. In this paper, we propose latent-observeddissimilarity (LOD) to evaluate the dissimilarity between the probabilisticcharacteristics of latent and observed variables. We also define four essentialtypes of generative models with different independence/conditional independenceconfigurations. Experiments using tractable real-world data show that LOD caneffectively capture the differences between models and reflect the capabilityfor higher layer learning. They also show that the conditional independence oflatent variables given observed variables contributes to improving thetransmission of information and characteristics from lower layers to higherlayers.
arxiv-16800-231 | Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles | http://arxiv.org/pdf/1603.09246v1.pdf | author:Mehdi Noroozi, Paolo Favaro category:cs.CV published:2016-03-30 summary:In this paper we study the problem of image representation learning withouthuman annotation. Following the principles of self-supervision, we build aconvolutional neural network (CNN) that can be trained to solve Jigsaw puzzlesas a pretext task, which requires no manual labeling, and then later repurposedto solve object classification and detection. To maintain the compatibilityacross tasks we introduce the context-free network (CFN), a Siamese-ennead CNN.The CFN takes image tiles as input and explicitly limits the receptive field(or context) of its early processing units to one tile at a time. We show thatthe CFN is a more compact version of AlexNet, but with the same semanticlearning capabilities. By training the CFN to solve Jigsaw puzzles, we learnboth a feature mapping of object parts as well as their cor-rect spatialarrangement. Our experimental evaluations show that the learned featurescapture semantically relevant content. The performance in object detection offeatures extracted from the CFN is the highest (51.8%) among unsupervisedlytrained features, and very close to that of supervisedly trained features(56.5%). In object classification the CFN features achieve also the bestaccuracy (38.1%) among unsupervisedly trained features on the ImageNet 2012dataset.
arxiv-16800-232 | Binary Quadratic Programing for Online Tracking of Hundreds of People in Extremely Crowded Scenes | http://arxiv.org/pdf/1603.09240v1.pdf | author:Afshin Dehghan, Mubarak Shah category:cs.CV cs.RO published:2016-03-30 summary:Multi-object tracking has been studied for decades. However, when it comes totracking pedestrians in extremely crowded scenes, we are limited to only fewworks. This is an important problem which gives rise to several challenges.Pre-trained object detectors fail to localize targets in crowded sequences.This consequently limits the use of data-association based multi-targettracking methods which rely on the outcome of an object detector. Additionally,the small apparent target size makes it challenging to extract features todiscriminate targets from their surroundings. Finally, the large number oftargets greatly increases computational complexity which in turn makes it hardto extend existing multi-target tracking approaches to high-density crowdscenarios. In this paper, we propose a tracker that addresses theaforementioned problems and is capable of tracking hundreds of peopleefficiently. We formulate online crowd tracking as Binary Quadratic Programing.Our formulation employs target's individual information in the form ofappearance and motion as well as contextual cues in the form of neighborhoodmotion, spatial proximity and grouping constraints, and solves detection anddata association simultaneously. In order to solve the proposed quadraticoptimization efficiently, where state-of art commercial quadratic programingsolvers fail to find the answer in a reasonable amount of time, we propose touse the most recent version of the Modified Frank Wolfe algorithm, which takesadvantage of SWAP-steps to speed up the optimization. We show that the proposedformulation can track hundreds of targets efficiently and improves state-of-artresults by significant margins on eleven challenging high density crowdsequences.
arxiv-16800-233 | Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs | http://arxiv.org/pdf/1603.09233v1.pdf | author:Rahul Meshram, Aditya Gopalan, D. Manjunath category:cs.LG published:2016-03-30 summary:We describe and study a model for an Automated Online Recommendation System(AORS) in which a user's preferences can be time-dependent and can also dependon the history of past recommendations and play-outs. The three key features ofthe model that makes it more realistic compared to existing models forrecommendation systems are (1) user preference is inherently latent, (2)current recommendations can affect future preferences, and (3) it allows forthe development of learning algorithms with provable performance guarantees.The problem is cast as an average-cost restless multi-armed bandit for a givenuser, with an independent partially observable Markov decision process (POMDP)for each item of content. We analyze the POMDP for a single arm, describe itsstructural properties, and characterize its optimal policy. We then develop aThompson sampling-based online reinforcement learning algorithm to learn theparameters of the model and optimize utility from the binary responses of theusers to continuous recommendations. We then analyze the performance of thelearning algorithm and characterize the regret. Illustrative numerical resultsand directions for extension to the restless hidden Markov multi-armed banditproblem are also presented.
arxiv-16800-234 | Real-Time Depth Refinement for Specular Objects | http://arxiv.org/pdf/1511.08886v2.pdf | author:Roy Or - El, Rom Hershkovitz, Aaron Wetzler, Guy Rosman, Alfred M. Bruckstein, Ron Kimmel category:cs.CV published:2015-11-28 summary:The introduction of consumer RGB-D scanners set off a major boost in 3Dcomputer vision research. Yet, the precision of existing depth scanners is notaccurate enough to recover fine details of a scanned object. While modernshading based depth refinement methods have been proven to work well withLambertian objects, they break down in the presence of specularities. Wepresent a novel shape from shading framework that addresses this issue andenhances both diffuse and specular objects' depth profiles. We take advantageof the built-in monochromatic IR projector and IR images of the RGB-D scannersand present a lighting model that accounts for the specular regions in theinput image. Using this model, we reconstruct the depth map in real-time. Bothquantitative tests and visual evaluations prove that the proposed methodproduces state of the art depth reconstruction results.
arxiv-16800-235 | Unsupervised Understanding of Location and Illumination Changes in Egocentric Videos | http://arxiv.org/pdf/1603.09200v1.pdf | author:Alejandro Betancourt, Natalia Díaz-Rodríguez, Emilia Barakova, Lucio Marcenaro, Matthias Rauterberg, Carlo Regazzoni category:cs.CV published:2016-03-30 summary:Wearable cameras stand out as one of the most promising devices for thecoming years, and as a consequence, the demand of computer algorithms toautomatically understand these videos has been increasing quickly. An automaticunderstanding of these videos is not an easy task, and its mobile natureimplies important challenges to be faced, such as the changing light conditionsand the unrestricted locations recorded. This paper proposes an unsupervisedstrategy based on global features and manifold learning to endow wearablecameras with contextual information regarding the light conditions and thelocation recorded. Results show that non-linear manifold methods can capturecontextual patterns from global features without compromising largecomputational resources. As an application case, the proposed unsupervisedstrategy is used as a switching mechanism to improve the hand-detection problemin egocentric videos under a multi-model approach.
arxiv-16800-236 | Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings | http://arxiv.org/pdf/1603.09188v1.pdf | author:Spandana Gella, Mirella Lapata, Frank Keller category:cs.CL cs.CV published:2016-03-30 summary:We introduce a new task, visual sense disambiguation for verbs: given animage and a verb, assign the correct sense of the verb, i.e., the one thatdescribes the action depicted in the image. Just as textual word sensedisambiguation is useful for a wide range of NLP tasks, visual sensedisambiguation can be useful for multimodal tasks such as image retrieval,image description, and text illustration. We introduce VerSe, a new datasetthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.We propose an unsupervised algorithm based on Lesk which performs visual sensedisambiguation using textual, visual, or multimodal embeddings. We find thattextual embeddings perform well when gold-standard textual annotations (objectlabels and image descriptions) are available, while multimodal embeddingsperform well on unannotated images. We also verify our findings by using thetextual and multimodal embeddings as features in a supervised setting andanalyse the performance of visual sense disambiguation task. VerSe is madepublicly available and can be downloaded at:https://github.com/spandanagella/verse.
arxiv-16800-237 | On higher order computations and synaptic meta-plasticity in the human brain: IT point of view (March, 2016) | http://arxiv.org/pdf/1603.02238v2.pdf | author:Stanislaw Ambroszkiewicz category:cs.NE q-bio.NC 92B20 F.1.1 published:2016-03-07 summary:Glia modify neuronal connectivity by creating structural changes in theneuronal connectome. Glia also influence the functional connectome by modifyingthe flow of information through neural networks (Fields et al. 2015). There arestrong experimental evidences that glia are responsible for synapticmeta-plasticity. Synaptic plasticity is the modification of the strength ofconnections between neurons. Meta-plasticity, i.e. plasticity of synapticplasticity, may be viewed as mechanisms for dynamic reconfiguration of neuroncircuits. First order computations in the brain are done by static neuroncircuits, whereas higher order computations are done by dynamicreconfigurations of the links (synapses) between the neuron circuits. Staticneuron circuits correspond to first order computable functions. Synapsecreation correspond to the mathematical notion of function composition.Functionals are higher order functions that take functions as their arguments.The construction of functionals is based on dynamic reconfigurations of thefunction composition. Perhaps the functionals correspond to the meta-plasticityin the human brain.
arxiv-16800-238 | Nonparametric modal regression | http://arxiv.org/pdf/1412.1716v3.pdf | author:Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, Larry Wasserman category:stat.ME math.ST stat.ML stat.TH published:2014-12-04 summary:Modal regression estimates the local modes of the distribution of $Y$ given$X=x$, instead of the mean, as in the usual regression sense, and can hencereveal important structure missed by usual regression methods. We study asimple nonparametric method for modal regression, based on a kernel densityestimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptoticerror bounds for this method, and propose techniques for constructingconfidence sets and prediction sets. The latter is used to select the smoothingbandwidth of the underlying KDE. The idea behind modal regression is connectedto many others, such as mixture regression and density ridge estimation, and wediscuss these ties as well.
arxiv-16800-239 | Exploiting Facial Landmarks for Emotion Recognition in the Wild | http://arxiv.org/pdf/1603.09129v1.pdf | author:Matthew Day category:cs.CV I.2.10 published:2016-03-30 summary:In this paper, we describe an entry to the third Emotion Recognition in theWild Challenge, EmotiW2015. We detail the associated experiments and show that,through more accurately locating the facial landmarks, and considering only thedistances between them, we can achieve a surprising level of performance. Theresulting system is not only more accurate than the challenge baseline, butalso much simpler.
arxiv-16800-240 | Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders | http://arxiv.org/pdf/1603.09128v1.pdf | author:Simon Šuster, Ivan Titov, Gertjan van Noord category:cs.CL cs.LG stat.ML published:2016-03-30 summary:We present an approach to learning multi-sense word embeddings relying bothon monolingual and bilingual information. Our model consists of an encoder,which uses monolingual and bilingual context (i.e. a parallel sentence) tochoose a sense for a given word, and a decoder which predicts context wordsbased on the chosen sense. The two components are estimated jointly. We observethat the word representations induced from bilingual data outperform themonolingual counterparts across a range of evaluation tasks, even thoughcrosslingual information is not available at test time.
arxiv-16800-241 | deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks | http://arxiv.org/pdf/1603.09123v1.pdf | author:Byunghan Lee, Junghwan Baek, Seunghyun Park, Sungroh Yoon category:cs.LG q-bio.GN published:2016-03-30 summary:MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control theexpression of target messenger RNAs (mRNAs) by binding them. Robust predictionof miRNA-mRNA pairs is of utmost importance in deciphering gene regulations buthas been challenging because of high false positive rates, despite a deluge ofcomputational tools that normally require laborious manual feature extraction.This paper presents an end-to-end machine learning framework for miRNA targetprediction. Leveraged by deep recurrent neural networks-based auto-encoding andsequence-sequence interaction learning, our approach not only delivers anunprecedented level of accuracy but also eliminates the need for manual featureextraction. The performance gap between the proposed method and existingalternatives is substantial (over 25% increase in F-measure), and deepTargetdelivers a quantum leap in the long-standing challenge of robust miRNA targetprediction.
arxiv-16800-242 | LIFT: Learned Invariant Feature Transform | http://arxiv.org/pdf/1603.09114v1.pdf | author:Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, Pascal Fua category:cs.CV published:2016-03-30 summary:We introduce a novel Deep Network architecture that implements the fullfeature point handling pipeline, that is, detection, orientation estimation,and feature description. While previous works have successfully tackled eachone of these problems individually, we show how to learn to do all three in aunified manner while preserving end-to-end differentiability. We thendemonstrate that our Deep pipeline outperforms state-of-the-art methods on anumber of benchmark datasets, without the need of retraining.
arxiv-16800-243 | Active Task Selection for Multi-Task Learning | http://arxiv.org/pdf/1602.06518v2.pdf | author:Anastasia Pentina, Christoph H. Lampert category:stat.ML cs.LG published:2016-02-21 summary:In this paper we consider the problem of multi-task learning, in which alearner is given a collection of prediction tasks that need to be solved. Incontrast to previous work, we give up on the assumption that labeled trainingdata is available for all tasks. Instead, we propose an active task selectionframework, where based only on the unlabeled data, the learner can choose a,typically small, subset of tasks for which he gets some labeled examples. Forthe remaining tasks, which have no available annotation, solutions are found bytransferring information from the selected tasks. We analyze two transferstrategies and develop generalization bounds for each of them. Based on thistheoretical analysis we propose two algorithms for making the choice of labeledtasks in a principled way and show their effectiveness on synthetic and realdata.
arxiv-16800-244 | Structured Feature Learning for Pose Estimation | http://arxiv.org/pdf/1603.09065v1.pdf | author:Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang category:cs.CV published:2016-03-30 summary:In this paper, we propose a structured feature learning framework to reasonthe correlations among body joints at the feature level in human poseestimation. Different from existing approaches of modelling structures on scoremaps or predicted labels, feature maps preserve substantially richerdescriptions of body joints. The relationships between feature maps of jointsare captured with the introduced geometrical transform kernels, which can beeasily implemented with a convolution layer. Features and their relationshipsare jointly learned in an end-to-end learning system. A bi-directional treestructured model is proposed, so that the feature channels at a body joint canwell receive information from other joints. The proposed framework improvesfeature learning substantially. With very simple post processing, it reachesthe best mean PCP on the LSP and FLIC datasets. Compared with the baseline oflearning features at each joint separately with ConvNet, the mean PCP has beenimproved by 18% on FLIC. The code is released to the public.
arxiv-16800-245 | Semi-Supervised Learning for Asymmetric Graphs through Reach and Distance Diffusion | http://arxiv.org/pdf/1603.09064v1.pdf | author:Edith Cohen category:cs.LG published:2016-03-30 summary:Semi-supervised learning algorithms are an indispensable tool when labeledexamples are scarce and there are many unlabeled examples [Blum and Chawla2001, Zhu et. al. 2003]. With graph-based methods, entities (examples)correspond to nodes in a graph and edges correspond to related entities. Thegraph structure is used to infer implicit pairwise affinity values (kernel)which are used to compute the learned labels. Two powerful techniques to define such a kernel are "symmetric" spectralmethods and Personalized Page Rank (PPR). With spectral methods, labels can bescalably learned using Jacobi iterations, but an inherent limiting issue isthat they are applicable to (undirected) graphs, whereas often, such as withlike, follow, or hyperlinks, relations between entities are inherentlyasymmetric. PPR naturally works with directed graphs but even with state of theart techniques does not scale when we want to learn billions of labels. Aiming at both high scalability and handling of directed relations, wepropose here and kernels. Our design is inspired by models for influencediffusion in social networks, formalized and spawned from the seminal work of[Kempe, Kleinberg, and Tardos 2003]. These models apply with directedinteractions and are naturally suited for asymmetry. We tailor these models todefine a natural asymmetric "kernel" and design highly scalable algorithms forparameter setting and label learning.
arxiv-16800-246 | Image Denoising Using Very Deep Fully Convolutional Encoder-Decoder Networks with Symmetric Skip Connections | http://arxiv.org/pdf/1603.09056v1.pdf | author:Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang category:cs.CV published:2016-03-30 summary:Image denoising is a long-standing problem in computer vision and imageprocessing, as well as a test bed for low-level image modeling algorithms. Inthis paper, we propose a very deep encoding-decoding framework for imagedenoising. Instead of using image priors, the proposed framework learnsend-to-end fully convolutional mappings from noisy images to the clean ones.The network is composed of multiple layers of convolution and de-convolutionoperators. With the observation that deeper networks improve denoisingperformance, we propose to use significantly deeper networks than thoseemployed previously for low-level image processing tasks such as denoising. Wepropose to symmetrically link convolutional and de-convolutional layers withskip-layer connections, with which the training converges much faster andattains a higher-quality local optimum. From the image processing point ofview, those symmetric connections help preserve image details.
arxiv-16800-247 | Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs | http://arxiv.org/pdf/1603.09054v1.pdf | author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL published:2016-03-30 summary:In this paper, we claim that vector cosine, which is generally consideredamong the most efficient unsupervised measures for identifying word similarityin Vector Space Models, can be outperformed by an unsupervised measure thatcalculates the extent of the intersection among the most mutually dependentcontexts of the target words. To prove it, we describe and evaluate APSyn, avariant of the Average Precision that, without any optimization, outperformsthe vector cosine and the co-occurrence on the standard ESL test set, with animprovement ranging between +9.00% and +17.98%, depending on the number ofchosen top contexts.
arxiv-16800-248 | Phoenix: A Self-Optimizing Chess Engine | http://arxiv.org/pdf/1603.09051v1.pdf | author:Rahul A R, G Srinivasaraghavan category:cs.AI cs.NE published:2016-03-30 summary:Since the advent of computers, many tasks which required humans to spend alot of time and energy have been trivialized by the computers' ability toperform repetitive tasks extremely quickly. However there are still many areasin which humans excel in comparison with the machines. One such area is chess.Even with great advances in the speed and computational power of modernmachines, Grandmasters often beat the best chess programs in the world withrelative ease. This may be due to the fact that a game of chess cannot be wonby pure calculation. There is more to the goodness of a chess position thansome numerical value which apparently can be seen only by the human brain. Herean effort has been made to improve current chess engines by letting themselvesevolve over a period of time. Firstly, the problem of learning is reduced intoan optimization problem by defining Position Evaluation in terms of PositionalValue Tables (PVTs). Next, the PVTs are optimized using Multi-Niche Crowdingwhich successfully identifies the optima in a multimodal function, therebyarriving at distinctly different solutions which are close to the globaloptimum.
arxiv-16800-249 | Attend, Infer, Repeat: Fast Scene Understanding with Generative Models | http://arxiv.org/pdf/1603.08575v2.pdf | author:S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, Koray Kavukcuoglu, Geoffrey E. Hinton category:cs.CV cs.LG published:2016-03-28 summary:We present a framework for efficient inference in structured image modelsthat explicitly reason about objects. We achieve this by performingprobabilistic inference using a recurrent neural network that attends to sceneelements and processes them one at a time. Crucially, the model itself learnsto choose the appropriate number of inference steps. We use this scheme tolearn to perform inference in partially specified 2D models (variable-sizedvariational auto-encoders) and fully specified 3D models (probabilisticrenderers). We show that such models learn to identify multiple objects -counting, locating and classifying the elements of a scene - without anysupervision, e.g., decomposing 3D images with various numbers of objects in asingle forward pass of a neural network. We further show that the networksproduce accurate inferences when compared to supervised counterparts, and thattheir structure leads to improved generalization.
arxiv-16800-250 | Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification | http://arxiv.org/pdf/1603.09050v1.pdf | author:Nguyen Viet Cuong, Nan Ye, Wee Sun Lee category:cs.LG stat.ML published:2016-03-30 summary:We study the robustness of active learning (AL) algorithms against priormisspecification: whether an algorithm achieves similar performance using aperturbed prior as compared to using the true prior. In both the average andworst cases of the maximum coverage setting, we prove that all$\alpha$-approximate algorithms are robust (i.e., near $\alpha$-approximate) ifthe utility is Lipschitz continuous in the prior. We further show thatrobustness may not be achieved if the utility is non-Lipschitz. This suggestswe should use a Lipschitz utility for AL if robustness is required. For theminimum cost setting, we can also obtain a robustness result for approximate ALalgorithms. Our results imply that many commonly used AL algorithms are robustagainst perturbed priors. We then propose the use of a mixture prior toalleviate the problem of prior misspecification. We analyze the robustness ofthe uniform mixture prior and show experimentally that it performs reasonablywell in practice.
arxiv-16800-251 | Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for Locally Robust Captioning | http://arxiv.org/pdf/1603.09046v1.pdf | author:Andrew Shin, Masataka Yamaguchi, Katsunori Ohnishi, Tatsuya Harada category:cs.CV published:2016-03-30 summary:The workflow of extracting features from images using convolutional neuralnetworks (CNN) and generating captions with recurrent neural networks (RNN) hasbecome a de-facto standard for image captioning task. However, since CNNfeatures are originally designed for classification task, it is mostlyconcerned with the main conspicuous element of the image, and often fails tocorrectly convey information on local, secondary elements. We propose toincorporate coding with vector of locally aggregated descriptors (VLAD) onspatial pyramid for CNN features of sub-regions in order to generate imagerepresentations that better reflect the local information of the images. Ourresults show that our method of compact VLAD coding can match CNN features withas little as 3% of dimensionality and, when combined with spatial pyramid, itresults in image captions that more accurately take local elements intoaccount.
arxiv-16800-252 | Performance of a community detection algorithm based on semidefinite programming | http://arxiv.org/pdf/1603.09045v1.pdf | author:Adel Javanmard, Andrea Montanari, Federico Ricci-Tersenghi category:stat.ML cs.SI physics.soc-ph published:2016-03-30 summary:The problem of detecting communities in a graph is maybe one the most studiedinference problems, given its simplicity and widespread diffusion among severaldisciplines. A very common benchmark for this problem is the stochastic blockmodel or planted partition problem, where a phase transition takes place in thedetection of the planted partition by changing the signal-to-noise ratio.Optimal algorithms for the detection exist which are based on spectral methods,but we show these are extremely sensible to slight modification in thegenerative model. Recently Javanmard, Montanari and Ricci-Tersenghi(arXiv:1511.08769) have used statistical physics arguments, and numericalsimulations to show that finding communities in the stochastic block model viasemidefinite programming is quasi optimal. Further, the resulting semidefiniterelaxation can be solved efficiently, and is very robust with respect tochanges in the generative model. In this paper we study in detail severalpractical aspects of this new algorithm based on semidefinite programming forthe detection of the planted partition. The algorithm turns out to be veryfast, allowing the solution of problems with $O(10^5)$ variables in few secondon a laptop computer.
arxiv-16800-253 | Möbius invariants of shapes and images | http://arxiv.org/pdf/1603.09335v1.pdf | author:Stephen Marsland, Robert McLachlan category:cs.CV math.MG published:2016-03-30 summary:Identifying when different images are of the same object despite changescaused by imaging technologies, or processes such as growth, has manyapplications in fields such as computer vision and biological image analysis.One approach to this problem is to identify the group of possibletransformations of the object and to find invariants to the action of thatgroup, meaning that the object has the same values of the invariants despitethe action of the group. In this paper we study the invariants of planar shapesand images under the M\"obius group $\mathrm{PSL}(2,\mathbb{C})$, which arisesin the conformal camera model of vision and may also correspond to neurologicalaspects of vision, such as grouping of lines and circles. We survey thecomputational requirements of an invariant, and the known M\"obius invariants,and then develop an algorithm by which shapes can be recognised that isM\"obius- and parametrization-invariant, numerically stable, and robust tonoise. We demonstrate the efficacy of this new invariant approach on sets ofcurves, and then develop a M\"obius-invariant signature of grey-scale images.
arxiv-16800-254 | Vector Quantization for Machine Vision | http://arxiv.org/pdf/1603.09037v1.pdf | author:Vincenzo Liguori category:cs.CV published:2016-03-30 summary:This paper shows how to reduce the computational cost for a variety of commonmachine vision tasks by operating directly in the compressed domain,particularly in the context of hardware acceleration. Pyramid VectorQuantization (PVQ) is the compression technique of choice and its propertiesare exploited to simplify Support Vector Machines (SVM), Convolutional NeuralNetworks(CNNs), Histogram of Oriented Gradients (HOG) features, interest pointsmatching and other algorithms.
arxiv-16800-255 | Towards Geo-Distributed Machine Learning | http://arxiv.org/pdf/1603.09035v1.pdf | author:Ignacio Cano, Markus Weimer, Dhruv Mahajan, Carlo Curino, Giovanni Matteo Fumarola category:cs.LG cs.DC stat.ML published:2016-03-30 summary:Latency to end-users and regulatory requirements push large companies tobuild data centers all around the world. The resulting data is "born"geographically distributed. On the other hand, many machine learningapplications require a global view of such data in order to achieve the bestresults. These types of applications form a new class of learning problems,which we call Geo-Distributed Machine Learning (GDML). Such applications needto cope with: 1) scarce and expensive cross-data center bandwidth, and 2)growing privacy concerns that are pushing for stricter data sovereigntyregulations. Current solutions to learning from geo-distributed data sourcesrevolve around the idea of first centralizing the data in one data center, andthen training locally. As machine learning algorithms arecommunication-intensive, the cost of centralizing the data is thought to beoffset by the lower cost of intra-data center communication during training. Inthis work, we show that the current centralized practice can be far fromoptimal, and propose a system for doing geo-distributed training. Furthermore,we argue that the geo-distributed approach is structurally more amenable todealing with regulatory constraints, as raw data never leaves the source datacenter. Our empirical evaluation on three real datasets confirms the generalvalidity of our approach, and shows that GDML is not only possible but alsoadvisable in many scenarios.
arxiv-16800-256 | Filtrated Algebraic Subspace Clustering | http://arxiv.org/pdf/1506.06289v4.pdf | author:Manolis C. Tsakiris, Rene Vidal category:cs.CV published:2015-06-20 summary:Subspace clustering is the problem of clustering data that lie close to aunion of linear subspaces. In the abstract form of the problem, where no noiseor other corruptions are present, the data are assumed to lie in generalposition inside the algebraic variety of a union of subspaces, and theobjective is to decompose the variety into its constituent subspaces. Prioralgebraic-geometric approaches to this problem require the subspaces to be ofequal dimension, or the number of subspaces to be known. Subspaces of arbitrarydimensions can still be recovered in closed form, in terms of all homogeneouspolynomials of degree $m$ that vanish on their union, when an upper bound m onthe number of the subspaces is given. In this paper, we propose an alternative,provably correct, algorithm for addressing a union of at most $m$arbitrary-dimensional subspaces, based on the idea of descending filtrations ofsubspace arrangements. Our algorithm uses the gradient of a vanishingpolynomial at a point in the variety to find a hyperplane containing thesubspace S passing through that point. By intersecting the variety with thishyperplane, we obtain a subvariety that contains S, and recursively applyingthe procedure until no non-trivial vanishing polynomial exists, our algorithmeventually identifies S. By repeating this procedure for other points, ouralgorithm eventually identifies all the subspaces by returning a basis fortheir orthogonal complement. Finally, we develop a variant of the abstractalgorithm, suitable for computations with noisy data. We show by experiments onsynthetic and real data that the proposed algorithm outperformsstate-of-the-art methods on several occasions, thus demonstrating the merit ofthe idea of filtrations.
arxiv-16800-257 | Maximize Pointwise Cost-sensitively Submodular Functions With Budget Constraint | http://arxiv.org/pdf/1603.09029v1.pdf | author:Nguyen Viet Cuong, Huan Xu category:cs.AI cs.DM math.OC stat.ML published:2016-03-30 summary:We study the worst-case adaptive optimization problem with budget constraint.Unlike previous works, we consider the general setting where the cost is a setfunction on sets of decisions. For this setting, we investigate thenear-optimality of greedy policies when the utility function satisfies a novelproperty called pointwise cost-sensitive submodularity. This property is anextension of cost-sensitive submodularity, which in turn is a generalization ofsubmodularity to general cost functions. We prove that two simple greedypolicies for the problem are not near-optimal but the best between them isnear-optimal. With this result, we propose a combined policy that isnear-optimal with respect to the optimal worst-case policy that uses half ofthe budget. We discuss applications of our theoretical results and also reportexperimental results comparing the greedy policies on the active learningproblem.
arxiv-16800-258 | Palmprint Recognition Using Deep Scattering Convolutional Network | http://arxiv.org/pdf/1603.09027v1.pdf | author:Shervin Minaee, Yao Wang category:cs.CV published:2016-03-30 summary:Palmprint recognition has drawn a lot of attention during the recent years.Many algorithms have been proposed for palmprint recognition in the past,majority of them being based on features extracted from the transform domain.Many of these transform domain features are not translation or rotationinvariant, and therefore a great deal of preprocessing is needed to align theimages. In this paper, a powerful image representation, called scatteringnetwork/transform, is used for palmprint recognition. Scattering network is aconvolutional network where its architecture and filters are predefined wavelettransforms. The first layer of scattering network captures similar features toSIFT descriptors and the higher-layer features capture higher-frequency contentof the signal which are lost in SIFT and other similar descriptors. Afterextraction of the scattering features, their dimensionality is reduced byapplying principal component analysis (PCA) which reduces the computationalcomplexity of the recognition task. Two different classifiers are used forrecognition: multi-class SVM and minimum-distance classifier. The proposedscheme has been tested on a well-known palmprint database and achieved accuracyrate of 99.95% and 100% using minimum distance classifier and SVM respectively.
arxiv-16800-259 | Comparative Study of Deep Learning Software Frameworks | http://arxiv.org/pdf/1511.06435v3.pdf | author:Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, Mohak Shah category:cs.LG published:2015-11-19 summary:Deep learning methods have resulted in significant performance improvementsin several application domains and as such several software frameworks havebeen developed to facilitate their implementation. This paper presents acomparative study of five deep learning frameworks, namely Caffe, Neon,TensorFlow, Theano, and Torch, on three aspects: extensibility, hardwareutilization, and speed. The study is performed on several types of deeplearning architectures and we evaluate the performance of the above frameworkswhen employed on a single machine for both (multi-threaded) CPU and GPU (NvidiaTitan X) settings. The speed performance metrics used here include the gradientcomputation time, which is important during the training phase of deepnetworks, and the forward time, which is important from the deploymentperspective of trained networks. For convolutional networks, we also report howeach of these frameworks support various convolutional algorithms and theircorresponding performance. From our experiments, we observe that Theano andTorch are the most easily extensible frameworks. We observe that Torch is bestsuited for any deep architecture on CPU, followed by Theano. It also achievesthe best performance on the GPU for large convolutional and fully connectednetworks, followed closely by Neon. Theano achieves the best performance on GPUfor training and deployment of LSTM networks. Caffe is the easiest forevaluating the performance of standard deep architectures. Finally, TensorFlowis a very flexible framework, similar to Theano, but its performance iscurrently not competitive compared to the other studied frameworks.
arxiv-16800-260 | Dataflow Matrix Machines as a Generalization of Recurrent Neural Networks | http://arxiv.org/pdf/1603.09002v1.pdf | author:Michael Bukatin, Steve Matthews, Andrey Radul category:cs.NE published:2016-03-29 summary:Dataflow matrix machines are a powerful generalization of recurrent neuralnetworks. They work with multiple types of arbitrary linear streams, multipletypes of powerful neurons, and allow to incorporate higher-order constructions.We expect them to be useful in machine learning and probabilistic programming,and in the synthesis of dynamic systems and of deterministic and probabilisticprograms.
arxiv-16800-261 | Online Rules for Control of False Discovery Rate and False Discovery Exceedance | http://arxiv.org/pdf/1603.09000v1.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST cs.LG stat.AP stat.ME stat.ML stat.TH published:2016-03-29 summary:Multiple hypothesis testing is a core problem in statistical inference andarises in almost every scientific field. Given a set of null hypotheses$\mathcal{H}(n) = (H_1,\dotsc, H_n)$, Benjamini and Hochberg introduced thefalse discovery rate (FDR), which is the expected proportion of false positivesamong rejected null hypotheses, and proposed a testing procedure that controlsFDR below a pre-assigned significance level. Nowadays FDR is the criterion ofchoice for large scale multiple hypothesis testing. In this paper we considerthe problem of controlling FDR in an "online manner". Concretely, we consideran ordered --possibly infinite-- sequence of null hypotheses $\mathcal{H} =(H_1,H_2,H_3,\dots )$ where, at each step $i$, the statistician must decidewhether to reject hypothesis $H_i$ having access only to the previousdecisions. This model was introduced by Foster and Stine. We study a class of"generalized alpha-investing" procedures and prove that any rule in this classcontrols online FDR, provided $p$-values corresponding to true nulls areindependent from the other $p$-values. (Earlier work only established mFDRcontrol.) Next, we obtain conditions under which generalized alpha-investingcontrols FDR in the presence of general $p$-values dependencies. Finally, wedevelop a modified set of procedures that also allow to control the falsediscovery exceedance (the tail of the proportion of false discoveries).Numerical simulations and analytical results indicate that online procedures donot incur a large loss in statistical power with respect to offline approaches,such as Benjamini-Hochberg.
arxiv-16800-262 | On the interplay of network structure and gradient convergence in deep learning | http://arxiv.org/pdf/1511.05297v5.pdf | author:Vamsi K Ithapu, Sathya Ravi, Vikas Singh category:cs.LG stat.ML published:2015-11-17 summary:The regularization and output consistency behavior of dropout and layer-wisepretraining for learning deep networks have been fairly well studied. However,our understanding of how the asymptotic convergence of backpropagation in deeparchitectures is related to the structural properties of the network and otherdesign choices (like denoising and dropout rate) is less clear at this time. Aninteresting question one may ask is whether the network architecture and inputdata statistics may guide the choices of learning parameters and vice versa. Inthis work, we explore the association between such structural, distributionaland learnability aspects vis-\`a-vis their interaction with parameterconvergence rates. We present a framework to address these questions based onthe backpropagation convergence for general nonconvex objectives usingfirst-order information. This analysis suggests an interesting relationshipbetween feature denoising and dropout. Building upon the results, we obtain asetup that provides systematic guidance regarding the choice of learningparameters and network sizes that achieve a certain level of convergence (inthe optimization sense) often mediated by statistical attributes of the inputs.Our results are supported by a set of experiments we conducted as well asindependent empirical observations reported by other groups in recent papers.
arxiv-16800-263 | Towards Practical Bayesian Parameter and State Estimation | http://arxiv.org/pdf/1603.08988v1.pdf | author:Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell category:cs.AI cs.LG stat.ML published:2016-03-29 summary:Joint state and parameter estimation is a core problem for dynamic Bayesiannetworks. Although modern probabilistic inference toolkits make it relativelyeasy to specify large and practically relevant probabilistic models, the silverbullet---an efficient and general online inference algorithm for suchproblems---remains elusive, forcing users to write special-purpose code foreach application. We propose a novel blackbox algorithm -- a hybrid of particlefiltering for state variables and assumed density filtering for parametervariables. It has following advantages: (a) it is efficient due to its onlinenature, and (b) it is applicable to both discrete and continuous parameterspaces . On a variety of toy and real models, our system is able to generatemore accurate results within a fixed computation budget. This preliminaryevidence indicates that the proposed approach is likely to be of practical use.
arxiv-16800-264 | SMASH: Data-driven Reconstruction of Physically Valid Collisions | http://arxiv.org/pdf/1603.08984v1.pdf | author:Aron Monszpart, Nils Thuerey, Niloy J. Mitra category:cs.GR cs.CV published:2016-03-29 summary:Collision sequences are commonly used in games and entertainment to add dramaand excitement. Authoring even two body collisions in real world can bedifficult as one has to get timing and the object trajectories to be correctlysynchronized. After trial-and-error iterations, when objects can actually bemade to collide, then they are difficult to acquire in 3D. In contrast,synthetically generating plausible collisions is difficult as it requiresadjusting different collision parameters (e.g., object mass ratio, coefficientof restitution, etc.) and appropriate initial parameters. We present SMASH todirectly `read off' appropriate collision parameters simply based on inputvideo recordings. Specifically, we describe how to use laws of rigid bodycollision to regularize the problem of lifting 2D annotated poses to 3Dreconstruction of collision sequences. The reconstructed sequences can then bemodified and combined to easily author novel and plausible collision sequences.We demonstrate the system on various complex collision sequences.
arxiv-16800-265 | Detecting weak changes in dynamic events over networks | http://arxiv.org/pdf/1603.08981v1.pdf | author:Shuang Li, Yao Xie, Mehrdad Farajtabar, Le Song category:cs.LG stat.ML published:2016-03-29 summary:Large volume of event data are becoming increasingly available in a widevariety of applications, such as social network analysis, Internet trafficmonitoring and healthcare analytics. Event data are observed irregularly incontinuous time, and the precise time interval between two events carries agreat deal of information about the dynamics of the underlying systems. How todetect changes in these systems as quickly as possible based on such eventdata? In this paper, we present a novel online detection algorithm for highdimensional event data over networks. Our method is based on a likelihood ratiotest for point processes, and achieve weak signal detection by aggregatinglocal statistics over time and networks. We also design an online algorithm forefficiently updating the statistics using an EM-like algorithm, and derivehighly accurate theoretical characterization of the false-alarm-rate. Wedemonstrate the good performance of our algorithm via numerical examples andreal-world twitter and memetracker datasets.
arxiv-16800-266 | Multiple Instance Dictionary Learning using Functions of Multiple Instances | http://arxiv.org/pdf/1511.02825v2.pdf | author:Changzhe Jiao, Alina Zare category:cs.CV cs.LG stat.ML published:2015-11-09 summary:A multiple instance dictionary learning method using functions of multipleinstances (DL-FUMI) is proposed to address target detection and two-classclassification problems with inaccurate training labels. Given inaccuratetraining labels, DL-FUMI learns a set of target dictionary atoms that describethe most distinctive and representative features of the true positive class aswell as a set of nontarget dictionary atoms that account for the sharedinformation found in both the positive and negative instances. Experimentalresults show that the estimated target dictionary atoms found by DL-FUMI aremore representative prototypes and identify better discriminative features ofthe true positive class than existing methods in the literature. DL-FUMI isshown to have significantly better performance on several target detection andclassification problems as compared to other multiple instance learning (MIL)dictionary learning algorithms on a variety of MIL problems.
arxiv-16800-267 | FAST: Free Adaptive Super-Resolution via Transfer for Compressed Videos | http://arxiv.org/pdf/1603.08968v1.pdf | author:Zhengdong Zhang, Vivienne Sze category:cs.CV published:2016-03-29 summary:High resolution displays are increasingly popular, requiring most of theexisting video content to be adapted to higher resolution. State-of-the-artsuper-resolution algorithms mainly address the visual quality of the outputinstead of real-time throughput. This paper introduces FAST, a framework toaccelerate any image based super-resolution algorithm running on compressedvideos. FAST leverages the similarity between adjacent frames in a video. Giventhe output of a super-resolution algorithm on one frame, the techniqueadaptively transfers it to the adjacent frames and skips running thesuper-resolution algorithm. The transferring process has negligible computationcost because the required information, including motion vectors, block size,and prediction residual, are embedded in the compressed video for free. In thiswork, we show that FAST accelerates state-of-the-art super-resolutionalgorithms by up to an order of magnitude with acceptable quality loss of up to0.2 dB. Thus, we believe that the FAST framework is an important step towardsenabling real-time super-resolution algorithms that upsample streamed videosfor large displays.
arxiv-16800-268 | Sweep Distortion Removal from THz Images via Blind Demodulation | http://arxiv.org/pdf/1604.03426v1.pdf | author:Alireza Aghasi, Barmak Heshmat, Albert Redo-Sanchez, Justin Romberg, Ramesh Raskar category:cs.CV physics.optics published:2016-03-29 summary:Heavy sweep distortion induced by alignments and inter-reflections of layersof a sample is a major burden in recovering 2D and 3D information in timeresolved spectral imaging. This problem cannot be addressed by conventionaldenoising and signal processing techniques as it heavily depends on the physicsof the acquisition. Here we propose and implement an algorithmic frameworkbased on low-rank matrix recovery and alternating minimization that exploitsthe forward model for THz acquisition. The method allows recovering theoriginal signal in spite of the presence of temporal-spatial distortions. Weaddress a blind-demodulation problem, where based on several observations ofthe sample texture modulated by an undesired sweep pattern, the two classes ofsignals are separated. The performance of the method is examined in bothsynthetic and experimental data, and the successful reconstructions aredemonstrated. The proposed general scheme can be implemented to advanceinspection and imaging applications in THz and other time-resolved sensingmodalities.
arxiv-16800-269 | Cross-modal Supervision for Learning Active Speaker Detection in Video | http://arxiv.org/pdf/1603.08907v1.pdf | author:Punarjay Chakravarty, Tinne Tuytelaars category:cs.CV published:2016-03-29 summary:In this paper, we show how to use audio to supervise the learning of activespeaker detection in video. Voice Activity Detection (VAD) guides the learningof the vision-based classifier in a weakly supervised manner. The classifieruses spatio-temporal features to encode upper body motion - facial expressionsand gesticulations associated with speaking. We further improve a generic modelfor active speaker detection by learning person specific models. Finally, wedemonstrate the online adaptation of generic models learnt on one dataset, topreviously unseen people in a new dataset, again using audio (VAD) for weaksupervision. The use of temporal continuity overcomes the lack of cleantraining data. We are the first to present an active speaker detection systemthat learns on one audio-visual dataset and automatically adapts to speakers ina new dataset. This work can be seen as an example of how the availability ofmulti-modal data allows us to learn a model without the need for supervision,by transferring knowledge from one modality to another.
arxiv-16800-270 | Zero Shot Recognition with Unreliable Attributes | http://arxiv.org/pdf/1409.4327v2.pdf | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV stat.ML published:2014-09-15 summary:In principle, zero-shot learning makes it possible to train a recognitionmodel simply by specifying the category's attributes. For example, withclassifiers for generic attributes like \emph{striped} and \emph{four-legged},one can construct a classifier for the zebra category by enumerating whichproperties it possesses---even without providing zebra training images. Inpractice, however, the standard zero-shot paradigm suffers because attributepredictions in novel images are hard to get right. We propose a novel randomforest approach to train zero-shot models that explicitly accounts for theunreliability of attribute predictions. By leveraging statistics about eachattribute's error tendencies, our method obtains more robust discriminativemodels for the unseen classes. We further devise extensions to handle thefew-shot scenario and unreliable attribute descriptions. On three datasets, wedemonstrate the benefit for visual category learning with zero or few trainingexamples, a critical domain for rare categories or categories defined on thefly.
arxiv-16800-271 | Learning image representations tied to ego-motion | http://arxiv.org/pdf/1505.02206v2.pdf | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV cs.AI stat.ML published:2015-05-08 summary:Understanding how images of objects and scenes behave in response to specificego-motions is a crucial aspect of proper visual development, yet existingvisual learning methods are conspicuously disconnected from the physical sourceof their images. We propose to exploit proprioceptive motor signals to provideunsupervised regularization in convolutional neural networks to learn visualrepresentations from egocentric video. Specifically, we enforce that ourlearned features exhibit equivariance i.e. they respond predictably totransformations associated with distinct ego-motions. With three datasets, weshow that our unsupervised feature learning approach significantly outperformsprevious approaches on visual recognition and next-best-view prediction tasks.In the most challenging test, we show that features learned from video capturedon an autonomous driving platform improve large-scale scene recognition instatic images from a disjoint domain.
arxiv-16800-272 | Anisotropic Mesh Adaptation for Image Representation | http://arxiv.org/pdf/1402.4893v4.pdf | author:Xianping Li category:cs.CV math.NA I.4.2 published:2014-02-20 summary:Triangular meshes have gained much interest in image representation and havebeen widely used in image processing. This paper introduces a framework ofanisotropic mesh adaptation (AMA) methods to image representation and proposesa GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme.Different than many other methods that triangulate sample points to form themesh, the AMA methods start directly with a triangular mesh and then adapt themesh based on a user-defined metric tensor to represent the image. The AMAmethods have clear mathematical framework and provides flexibility for bothimage representation and image reconstruction. A mesh patching technique isdeveloped for the implementation of the GPRAMA method, which leads to animproved version of the popular GPRFS-ED method. The GPRAMA method can achievebetter quality than the GPRFS-ED method but with lower computational cost.
arxiv-16800-273 | Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints | http://arxiv.org/pdf/1603.08887v1.pdf | author:Greg Durrett, Taylor Berg-Kirkpatrick, Dan Klein category:cs.CL published:2016-03-29 summary:We present a discriminative model for single-document summarization thatintegrally combines compression and anaphoricity constraints. Our model selectstextual units to include in the summary based on a rich set of sparse featureswhose weights are learned on a large corpus. We allow for the deletion ofcontent within a sentence when that deletion is licensed by compression rules;in our framework, these are implemented as dependencies between subsententialunits of text. Anaphoricity constraints then improve cross-sentence coherenceby guaranteeing that, for each pronoun included in the summary, the pronoun'santecedent is included as well or the pronoun is rewritten as a full mention.When trained end-to-end, our final system outperforms prior work on both ROUGEas well as on human judgments of linguistic quality.
arxiv-16800-274 | A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data | http://arxiv.org/pdf/1603.08884v1.pdf | author:Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He, Phillip Bachman, Kaheer Suleman category:cs.CL I.2.7 published:2016-03-29 summary:Understanding unstructured text is a major goal within natural languageprocessing. Comprehension tests pose questions based on short text passages toevaluate such understanding. In this work, we investigate machine comprehensionon the challenging {\it MCTest} benchmark. Partly because of its limited size,prior work on {\it MCTest} has focused mainly on engineering better features.We tackle the dataset with a neural approach, harnessing simple neural networksarranged in a parallel hierarchy. The parallel hierarchy enables our model tocompare the passage, question, and answer from a variety of trainableperspectives, as opposed to using a manually designed, rigid feature set.Perspectives range from the word level to sentence fragments to sequences ofsentences; the networks operate only on word-embedding representations of text.When trained with a methodology designed to help cope with limited trainingdata, our Parallel-Hierarchical model sets a new state of the art for {\itMCTest}, outperforming previous feature-engineered approaches slightly andprevious neural approaches by a significant margin (over 15\% absolute).
arxiv-16800-275 | Fusing Face and Periocular biometrics using Canonical correlation analysis | http://arxiv.org/pdf/1604.01683v1.pdf | author:N. S. Lakshmiprabha category:cs.CV published:2016-03-29 summary:This paper presents a novel face and periocular biometric fusion at featurelevel using canonical correlation analysis. Face recognition itself haslimitations such as illumination, pose, expression, occlusion etc. Also,periocular biometrics has spectacles, head angle, hair and expression as itslimitations. Unimodal biometrics cannot surmount all these limitations. Therecognition accuracy can be increased by fusing dual information (face andperiocular) from a single source (face image) using canonical correlationanalysis (CCA). This work also proposes a new wavelet decomposed local binarypattern (WD-LBP) feature extractor which provides sufficient features forfusion. A detailed analysis on face and periocular biometrics shows that WD-LBPfeatures are more accurate and faster than local binary pattern (LBP) and gaborwavelet. The experimental results using Muct face database reveals that theproposed multimodal biometrics performs better than the unimodal biometrics.
arxiv-16800-276 | A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity | http://arxiv.org/pdf/1603.08868v1.pdf | author:Ildikó Pilán, Sowmya Vajjala, Elena Volodina category:cs.CL published:2016-03-29 summary:Corpora and web texts can become a rich language learning resource if we havea means of assessing whether they are linguistically appropriate for learnersat a given proficiency level. In this paper, we aim at addressing this issue bypresenting the first approach for predicting linguistic complexity for Swedishsecond language learning material on a 5-point scale. After showing that thetraditional Swedish readability measure, L\"asbarhetsindex (LIX), is notsuitable for this task, we propose a supervised machine learning model, basedon a range of linguistic features, that can reliably classify texts accordingto their difficulty level. Our model obtained an accuracy of 81.3% and anF-score of 0.8, which is comparable to the state of the art in English and isconsiderably higher than previously reported results for other languages. Wefurther studied the utility of our features with single sentences instead offull texts since sentences are a common linguistic unit in language learningexercises. We trained a separate model on sentence-level data with fiveclasses, which yielded 63.4% accuracy. Although this is lower than the documentlevel performance, we achieved an adjacent accuracy of 92%. Furthermore, wefound that using a combination of different features, compared to using lexicalfeatures alone, resulted in 7% improvement in classification accuracy at thesentence level, whereas at the document level, lexical features were moredominant. Our models are intended for use in a freely accessible web-basedlanguage learning platform for the automatic generation of exercises.
arxiv-16800-277 | Histopathological Image Classification using Discriminative Feature-oriented Dictionary Learning | http://arxiv.org/pdf/1506.05032v5.pdf | author:Tiep Huu Vu, Hojjat Seyed Mousavi, Vishal Monga, Arvind UK Rao, Ganesh Rao category:cs.CV published:2015-06-16 summary:In histopathological image analysis, feature extraction for classification isa challenging task due to the diversity of histology features suitable for eachproblem as well as presence of rich geometrical structures. In this paper, wepropose an automatic feature discovery framework via learning class-specificdictionaries and present a low-complexity method for classification and diseasegrading in histopathology. Essentially, our Discriminative Feature-orientedDictionary Learning (DFDL) method learns class-specific dictionaries such thatunder a sparsity constraint, the learned dictionaries allow representing a newimage sample parsimoniously via the dictionary corresponding to the classidentity of the sample. At the same time, the dictionary is designed to bepoorly capable of representing samples from other classes. Experiments on threechallenging real-world image databases: 1) histopathological images ofintraductal breast lesions, 2) mammalian kidney, lung and spleen imagesprovided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University,and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, revealthe merits of our proposal over state-of-the-art alternatives. {Moreover, wedemonstrate that DFDL exhibits a more graceful decay in classification accuracyagainst the number of training images which is highly desirable in practicewhere generous training is often not available
arxiv-16800-278 | Face Image Analysis using AAM, Gabor, LBP and WD features for Gender, Age, Expression and Ethnicity Classification | http://arxiv.org/pdf/1604.01684v1.pdf | author:N. S. Lakshmiprabha category:cs.CV published:2016-03-29 summary:The growth in electronic transactions and human machine interactions rely onthe information such as gender, age, expression and ethnicity provided by theface image. In order to obtain these information, feature extraction plays amajor role. In this paper, retrieval of age, gender, expression and raceinformation from an individual face image is analysed using different featureextraction methods. The performance of four major feature extraction methodssuch as Active Appearance Model (AAM), Gabor wavelets, Local Binary Pattern(LBP) and Wavelet Decomposition (WD) are analyzed for gender recognition, ageestimation, expression recognition and racial recognition in terms of accuracy(recognition rate), time for feature extraction, neural training and time totest an image. Each of this recognition system is compared with four featureextractors on same dataset (training and validation set) to get a betterunderstanding in its performance. Experiments carried out on FG-NET,Cohn-Kanade, PAL face database shows that each method has its own merits anddemerits. Hence it is practically impossible to define a method which is bestat all circumstances with less computational complexity. Further, a detailedcomparison of age estimation and age estimation using gender information isprovided along with a solution to overcome aging effect in case of genderrecognition. An attempt has been made in obtaining all (i.e. gender, age range,expression and ethnicity) information from a test image in a single go.
arxiv-16800-279 | Revisiting Semi-Supervised Learning with Graph Embeddings | http://arxiv.org/pdf/1603.08861v1.pdf | author:Zhilin Yang, William Cohen, Ruslan Salakhutdinov category:cs.LG published:2016-03-29 summary:We present a semi-supervised learning framework based on graph embeddings.Given a graph between instances, we train an embedding for each instance tojointly predict the class label and the neighborhood context in the graph. Wedevelop both transductive and inductive variants of our method. In thetransductive variant of our method, the class labels are determined by both thelearned embeddings and input feature vectors, while in the inductive variant,the embeddings are defined as a parametric function of the feature vectors, sopredictions can be made on instances not seen during training. On a large anddiverse set of benchmark tasks, including text classification, distantlysupervised entity extraction, and entity classification, we show improvedperformance over many of the existing models.
arxiv-16800-280 | Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community | http://arxiv.org/pdf/1603.08832v1.pdf | author:Ethan Fast, Tina Vachovsky, Michael S. Bernstein category:cs.CL cs.SI published:2016-03-29 summary:Imagine a princess asleep in a castle, waiting for her prince to slay thedragon and rescue her. Tales like the famous Sleeping Beauty clearly divide upgender roles. But what about more modern stories, borne of a generationincreasingly aware of social constructs like sexism and racism? Do thesestories tend to reinforce gender stereotypes, or counter them? In this paper,we present a technique that combines natural language processing with acrowdsourced lexicon of stereotypes to capture gender biases in fiction. Weapply this technique across 1.8 billion words of fiction from the Wattpadonline writing community, investigating gender representation in stories, howmale and female characters behave and are described, and how authors' use ofgender stereotypes is associated with the community's ratings. We find thatmale over-representation and traditional gender stereotypes (e.g., dominant menand submissive women) are common throughout nearly every genre in our corpus.However, only some of these stereotypes, like sexual or violent men, areassociated with highly rated stories. Finally, despite women often being thetarget of negative stereotypes, female authors are equally likely to write suchstereotypes as men.
arxiv-16800-281 | Towards Understanding Sparse Filtering: A Theoretical Perspective | http://arxiv.org/pdf/1603.08831v1.pdf | author:Fabio Massimo Zennaro, Ke Chen category:cs.LG published:2016-03-29 summary:In this paper we present our study on a recent and effective algorithm forunsupervised learning, that is, sparse filtering. The aim of this research isnot to show whether or how well sparse filtering works, but to understand whyand when sparse filtering does work. We provide a thorough study of thisalgorithm through a conceptual evaluation of feature distribution learning, atheoretical analysis of the properties of sparse filtering, and an experimentalvalidation of our conclusions. We argue that sparse filtering works byexplicitly maximizing the informativeness of the learned representation throughthe maximization of the proxy of sparsity, and by implicitly preservinginformation conveyed by the distribution of the original data through theconstraint of structure preservation. In particular, we prove that sparsefiltering preserves the cosine neighborhoodness of the data. We validate ourstatements on artificial and real data sets by applying our theoreticalunderstanding to the explanation of the success of sparse filtering onreal-world problems. Our work provides a strong theoretical framework forunderstanding sparse filtering, it highlights assumptions and conditions forsuccess behind the algorithm, and it provides a fresh insight into developingnew feature distribution learning algorithms.
arxiv-16800-282 | Spectral M-estimation with Applications to Hidden Markov Models | http://arxiv.org/pdf/1603.08815v1.pdf | author:Dustin Tran, Minjae Kim, Finale Doshi-Velez category:stat.CO cs.LG stat.ME published:2016-03-29 summary:Method of moment estimators exhibit appealing statistical properties, such asasymptotic unbiasedness, for nonconvex problems. However, they typicallyrequire a large number of samples and are extremely sensitive to modelmisspecification. In this paper, we apply the framework of M-estimation todevelop both a generalized method of moments procedure and a principled methodfor regularization. Our proposed M-estimator obtains optimal sample efficiencyrates (in the class of moment-based estimators) and the same well-known rateson prediction accuracy as other spectral estimators. It also makes itstraightforward to incorporate regularization into the sample momentconditions. We demonstrate empirically the gains in sample efficiency from ourapproach on hidden Markov models.
arxiv-16800-283 | Locally Epistatic Models for Genome-wide Prediction and Association by Importance Sampling | http://arxiv.org/pdf/1603.08813v1.pdf | author:Deniz Akdemir, Jean-Luc Jannink category:stat.AP q-bio.QM stat.ML published:2016-03-29 summary:In statistical genetics an important task involves building predictive modelsfor the genotype-phenotype relationships and thus attribute a proportion of thetotal phenotypic variance to the variation in genotypes. Numerous models havebeen proposed to incorporate additive genetic effects into models forprediction or association. However, there is a scarcity of models that canadequately account for gene by gene or other forms of genetical interactions.In addition, there is an increased interest in using marker annotations ingenome-wide prediction and association. In this paper, we discuss an hybridmodeling methodology which combines the parametric mixed modeling approach andthe non-parametric rule ensembles. This approach gives us a flexible class ofmodels that can be used to capture additive, locally epistatic genetic effects,gene x background interactions and allows us to incorporate one or moreannotations into the genomic selection or association models. We use benchmarkdata sets covering a range of organisms and traits in addition to simulateddata sets to illustrate the strengths of this approach. The improvement ofmodel accuracies and association results suggest that a part of the "missingheritability" in complex traits can be captured by modeling local epistasis.
arxiv-16800-284 | Scalable Solution for Approximate Nearest Subspace Search | http://arxiv.org/pdf/1603.08810v1.pdf | author:Masakazu Iwamura, Masataka Konishi, Koichi Kise category:cs.CV published:2016-03-29 summary:Finding the nearest subspace is a fundamental problem and influential to manyapplications. In particular, a scalable solution that is fast and accurate fora large problem has a great impact. The existing methods for the problem are,however, useless in a large-scale problem with a large number of subspaces andhigh dimensionality of the feature space. A cause is that they are designedbased on the traditional idea to represent a subspace by a single point. Inthis paper, we propose a scalable solution for the approximate nearest subspacesearch (ANSS) problem. Intuitively, the proposed method represents a subspaceby multiple points unlike the existing methods. This makes a large-scale ANSSproblem tractable. In the experiment with 3036 subspaces in the1024-dimensional space, we confirmed that the proposed method was 7.3 timesfaster than the previous state-of-the-art without loss of accuracy.
arxiv-16800-285 | Session-based Recommendations with Recurrent Neural Networks | http://arxiv.org/pdf/1511.06939v4.pdf | author:Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk category:cs.LG cs.IR cs.NE published:2015-11-21 summary:We apply recurrent neural networks (RNN) on a new domain, namely recommendersystems. Real-life recommender systems often face the problem of having to baserecommendations only on short session-based data (e.g. a small sportswarewebsite) instead of long user histories (as in the case of Netflix). In thissituation the frequently praised matrix factorization approaches are notaccurate. This problem is usually overcome in practice by resorting toitem-to-item recommendations, i.e. recommending similar items. We argue that bymodeling the whole session, more accurate recommendations can be provided. Wetherefore propose an RNN-based approach for session-based recommendations. Ourapproach also considers practical aspects of the task and introduces severalmodifications to classic RNNs such as a ranking loss function that make it moreviable for this specific problem. Experimental results on two data-sets showmarked improvements over widely used approaches.
arxiv-16800-286 | A Computational Model for Amodal Completion | http://arxiv.org/pdf/1511.08418v2.pdf | author:Maria Oliver, Gloria Haro, Mariella Dimiccoli, Baptiste Mazin, Coloma Ballester category:cs.CV published:2015-11-26 summary:This paper presents a computational model to recover the most likelyinterpretation of the 3D scene structure from a planar image, where someobjects may occlude others. The estimated scene interpretation is obtained byintegrating some global and local cues and provides both the completedisoccluded objects that form the scene and their ordering according to depth.Our method first computes several distal scenes which are compatible with theproximal planar image. To compute these different hypothesized scenes, wepropose a perceptually inspired object disocclusion method, which works byminimizing the Euler's elastica as well as by incorporating the relatability ofpartially occluded contours and the convexity of the disoccluded objects. Then,to estimate the preferred scene we rely on a Bayesian model and defineprobabilities taking into account the global complexity of the objects in thehypothesized scenes as well as the effort of bringing these objects in theirrelative position in the planar image, which is also measured by an Euler'selastica-based quantity. The model is illustrated with numerical experimentson, both, synthetic and real images showing the ability of our model toreconstruct the occluded objects and the preferred perceptual order among them.We also present results on images of the Berkeley dataset with providedfigure-ground ground-truth labeling.
arxiv-16800-287 | A Neural Transfer Function for a Smooth and Differentiable Transition Between Additive and Multiplicative Interactions | http://arxiv.org/pdf/1503.05724v3.pdf | author:Sebastian Urban, Patrick van der Smagt category:stat.ML cs.LG cs.NE published:2015-03-19 summary:Existing approaches to combine both additive and multiplicative neural unitseither use a fixed assignment of operations or require discrete optimization todetermine what function a neuron should perform. This leads either to aninefficient distribution of computational resources or an extensive increase inthe computational complexity of the training procedure. We present a novel, parameterizable transfer function based on themathematical concept of non-integer functional iteration that allows theoperation each neuron performs to be smoothly and, most importantly,differentiablely adjusted between addition and multiplication. This allows thedecision between addition and multiplication to be integrated into the standardbackpropagation training procedure.
arxiv-16800-288 | Deep SimNets | http://arxiv.org/pdf/1506.03059v2.pdf | author:Nadav Cohen, Or Sharir, Amnon Shashua category:cs.NE cs.LG published:2015-06-09 summary:We present a deep layered architecture that generalizes convolutional neuralnetworks (ConvNets). The architecture, called SimNets, is driven by twooperators: (i) a similarity function that generalizes inner-product, and (ii) alog-mean-exp function called MEX that generalizes maximum and average. The twooperators applied in succession give rise to a standard neuron but in "featurespace". The feature spaces realized by SimNets depend on the choice of thesimilarity operator. The simplest setting, which corresponds to a convolution,realizes the feature space of the Exponential kernel, while other settingsrealize feature spaces of more powerful kernels (Generalized Gaussian, whichincludes as special cases RBF and Laplacian), or even dynamically learnedfeature spaces (Generalized Multiple Kernel Learning). As a result, the SimNetcontains a higher abstraction level compared to a traditional ConvNet. We arguethat enhanced expressiveness is important when the networks are small due torun-time constraints (such as those imposed by mobile applications). Empiricalevaluation validates the superior expressiveness of SimNets, showing asignificant gain in accuracy over ConvNets when computational resources atrun-time are limited. We also show that in large-scale settings, wherecomputational complexity is less of a concern, the additional capacity ofSimNets can be controlled with proper regularization, yielding accuraciescomparable to state of the art ConvNets.
arxiv-16800-289 | COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting | http://arxiv.org/pdf/1603.08785v1.pdf | author:Nikolaus Hansen, Anne Auger, Olaf Mersmann, Tea Tusar, Dimo Brockhoff category:cs.AI stat.ML published:2016-03-29 summary:COCO is a platform for Comparing Continuous Optimizers in a black-boxsetting. It aims at automatizing the tedious and repetitive task ofbenchmarking numerical optimization algorithms to the greatest possible extent.We present the rationals behind the development of the platform as a generalproposition for a guideline towards better benchmarking. We detail underlyingfundamental concepts of COCO such as its definition of a problem, the idea ofinstances, the relevance of target values, and runtime as central performancemeasure. Finally, we give a quick overview of the basic code structure and theavailable test suites.
arxiv-16800-290 | Machine Learning and Cloud Computing: Survey of Distributed and SaaS Solutions | http://arxiv.org/pdf/1603.08767v1.pdf | author:Daniel Pop category:cs.DC cs.LG published:2016-03-29 summary:Applying popular machine learning algorithms to large amounts of data raisednew challenges for the ML practitioners. Traditional ML libraries does notsupport well processing of huge datasets, so that new approaches were needed.Parallelization using modern parallel computing frameworks, such as MapReduce,CUDA, or Dryad gained in popularity and acceptance, resulting in new MLlibraries developed on top of these frameworks. We will briefly introduce themost prominent industrial and academic outcomes, such as Apache Mahout,GraphLab or Jubatus. We will investigate how cloud computing paradigm impacted the field of ML.First direction is of popular statistics tools and libraries (R system, Python)deployed in the cloud. A second line of products is augmenting existing toolswith plugins that allow users to create a Hadoop cluster in the cloud and runjobs on it. Next on the list are libraries of distributed implementations forML algorithms, and on-premise deployments of complex systems for data analyticsand data mining. Last approach on the radar of this survey is ML asSoftware-as-a-Service, several BigData start-ups (and large companies as well)already opening their solutions to the market.
arxiv-16800-291 | Multi-Cue Zero-Shot Learning with Strong Supervision | http://arxiv.org/pdf/1603.08754v1.pdf | author:Zeynep Akata, Mateusz Malinowski, Mario Fritz, Bernt Schiele category:cs.CV published:2016-03-29 summary:Scaling up visual category recognition to large numbers of classes remainschallenging. A promising research direction is zero-shot learning, which doesnot require any training data to recognize new classes, but rather relies onsome form of auxiliary information describing the new classes. Ultimately, thismay allow to use textbook knowledge that humans employ to learn about newclasses by transferring knowledge from classes they know well. The mostsuccessful zero-shot learning approaches currently require a particular type ofauxiliary information -- namely attribute annotations performed by humans --that is not readily available for most classes. Our goal is to circumvent thisbottleneck by substituting such annotations by extracting multiple pieces ofinformation from multiple unstructured text sources readily available on theweb. To compensate for the weaker form of auxiliary information, we incorporatestronger supervision in the form of semantic part annotations on the classesfrom which we transfer knowledge. We achieve our goal by a joint embeddingframework that maps multiple text parts as well as multiple semantic parts intoa common space. Our results consistently and significantly improve on thestate-of-the-art in zero-short recognition and retrieval.
arxiv-16800-292 | Negative Learning Rates and P-Learning | http://arxiv.org/pdf/1603.08253v2.pdf | author:Devon Merrill category:cs.AI cs.LG published:2016-03-27 summary:We present a method of training a differentiable function approximator for aregression task using negative examples. We effect this training using negativelearning rates. We also show how this method can be used to perform directpolicy learning in a reinforcement learning setting.
arxiv-16800-293 | Multi-Band Image Fusion Based on Spectral Unmixing | http://arxiv.org/pdf/1603.08720v1.pdf | author:Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret, Marcus Chen, Simon Godsill category:cs.CV published:2016-03-29 summary:This paper presents a multi-band image fusion algorithm based on unsupervisedspectral unmixing for combining a high-spatial low-spectral resolution imageand a low-spatial high-spectral resolution image. The widely used linearobservation model (with additive Gaussian noise) is combined with the linearspectral mixture model to form the likelihoods of the observations. Thenon-negativity and sum-to-one constraints resulting from the intrinsic physicalproperties of the abundances are introduced as prior information to regularizethis ill-posed problem. The joint fusion and unmixing problem is thenformulated as maximizing the joint posterior distribution with respect to theendmember signatures and abundance maps, This optimization problem is attackedwith an alternating optimization strategy. The two resulting sub-problems areconvex and are solved efficiently using the alternating direction method ofmultipliers. Experiments are conducted for both synthetic and semi-real data.Simulation results show that the proposed unmixing based fusion scheme improvesboth the abundance and endmember estimation comparing with the state-of-the-artjoint fusion and unmixing algorithms.
arxiv-16800-294 | Echo State Condition at the Critical Point | http://arxiv.org/pdf/1411.6757v7.pdf | author:Norbert Michael Mayer category:cs.NE published:2014-11-25 summary:Recurrent networks that have transfer functions that fulfill the Lipschitzcontinuity with L=1, may be echo state networks if certain limitations on therecurrent connectivity are applied. Initially it has been shown that it issufficient if the largest singular value of the recurrent connectivity S issmaller than 1. The main achievement of this paper is a proof under whichconditions the network is an echo state network even if S=1. It turns out thatin this critical case the exact shape of the transfer function plays a decisiverole whether or not the network still fulfills the echo state condition. Inaddition, several intuitive examples with one neuron networks are outlined toillustrate effects of critical connectivity.
arxiv-16800-295 | Unified View of Matrix Completion under General Structural Constraints | http://arxiv.org/pdf/1603.08708v1.pdf | author:Suriya Gunasekar, Arindam Banerjee, Joydeep Ghosh category:stat.ML published:2016-03-29 summary:In this paper, we present a unified analysis of matrix completion undergeneral low-dimensional structural constraints induced by {\em any} normregularization. We consider two estimators for the general problem ofstructured matrix completion, and provide unified upper bounds on the samplecomplexity and the estimation error. Our analysis relies on results fromgeneric chaining, and we establish two intermediate results of independentinterest: (a) in characterizing the size or complexity of low dimensionalsubsets in high dimensional ambient space, a certain partial complexity measureencountered in the analysis of matrix completion problems is characterized interms of a well understood complexity measure of Gaussian widths, and (b) it isshown that a form of restricted strong convexity holds for matrix completionproblems under general norm regularization. Further, we provide severalnon-trivial examples of structures included in our framework, notably therecently proposed spectral $k$-support norm.
arxiv-16800-296 | ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms | http://arxiv.org/pdf/1603.08705v1.pdf | author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL published:2016-03-29 summary:In this paper, we describe ROOT13, a supervised system for the classificationof hypernyms, co-hyponyms and random words. The system relies on a RandomForest algorithm and 13 unsupervised corpus-based features. We evaluate it witha 10-fold cross validation on 9,600 pairs, equally distributed among the threeclasses and involving several Parts-Of-Speech (i.e. adjectives, nouns andverbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%,against a baseline of 57.6% (vector cosine). When the classification is binary,ROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%),hypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Ourresults are competitive with stateof-the-art models.
arxiv-16800-297 | Interpretability of Multivariate Brain Maps in Brain Decoding: Definition and Quantification | http://arxiv.org/pdf/1603.08704v1.pdf | author:Seyed Mostafa Kia category:stat.ML cs.LG published:2016-03-29 summary:Brain decoding is a popular multivariate approach for hypothesis testing inneuroimaging. It is well known that the brain maps derived from weights oflinear classifiers are hard to interpret because of high correlations betweenpredictors, low signal to noise ratios, and the high dimensionality ofneuroimaging data. Therefore, improving the interpretability of brain decodingapproaches is of primary interest in many neuroimaging studies. Despiteextensive studies of this type, at present, there is no formal definition forinterpretability of multivariate brain maps. As a consequence, there is noquantitative measure for evaluating the interpretability of different braindecoding methods. In this paper, first, we present a theoretical definition ofinterpretability in brain decoding; we show that the interpretability ofmultivariate brain maps can be decomposed into their reproducibility andrepresentativeness. Second, as an application of the proposed theoreticaldefinition, we formalize a heuristic method for approximating theinterpretability of multivariate brain maps in a binary magnetoencephalography(MEG) decoding scenario. Third, we propose to combine the approximatedinterpretability and the performance of the brain decoding model into a newmulti-objective criterion for model selection. Our results for the MEG datashow that optimizing the hyper-parameters of the regularized linear classifierbased on the proposed criterion results in more informative multivariate brainmaps. More importantly, the presented definition provides the theoreticalbackground for quantitative evaluation of interpretability, and hence,facilitates the development of more effective brain decoding algorithms in thefuture.
arxiv-16800-298 | Nine Features in a Random Forest to Learn Taxonomical Semantic Relations | http://arxiv.org/pdf/1603.08702v1.pdf | author:Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin Lu, Chu-Ren Huang category:cs.CL published:2016-03-29 summary:ROOT9 is a supervised system for the classification of hypernyms, co-hyponymsand random words that is derived from the already introduced ROOT13 (Santus etal., 2016). It relies on a Random Forest algorithm and nine unsupervisedcorpus-based features. We evaluate it with a 10-fold cross validation on 9,600pairs, equally distributed among the three classes and involving severalParts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes arepresent, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2%(vector cosine). When the classification is binary, ROOT9 achieves thefollowing results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%,hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. Inorder to compare the performance with the state-of-the-art, we have alsoevaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that itis in fact competitive. Finally, we investigated whether the system learns thesemantic relation or it simply learns the prototypical hypernyms, as claimed byLevy et al. (2015). The second possibility seems to be the most likely, eventhough ROOT9 can be trained on negative examples (i.e., switched hypernyms) todrastically reduce this bias.
arxiv-16800-299 | What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets | http://arxiv.org/pdf/1603.08701v1.pdf | author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL published:2016-03-29 summary:In this paper, we claim that Vector Cosine, which is generally considered oneof the most efficient unsupervised measures for identifying word similarity inVector Space Models, can be outperformed by a completely unsupervised measurethat evaluates the extent of the intersection among the most associatedcontexts of two target words, weighting such intersection according to the rankof the shared contexts in the dependency ranked lists. This claim comes fromthe hypothesis that similar words do not simply occur in similar contexts, butthey share a larger portion of their most relevant contexts compared to otherrelated words. To prove it, we describe and evaluate APSyn, a variant ofAverage Precision that, independently of the adopted parameters, outperformsthe Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In thebest setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracyin the TOEFL dataset, beating therefore the non-English US college applicants(whose average, as reported in the literature, is 64.50%) and severalstate-of-the-art approaches.
arxiv-16800-300 | Learning to Refine Object Segments | http://arxiv.org/pdf/1603.08695v1.pdf | author:Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollàr category:cs.CV published:2016-03-29 summary:Object segmentation requires both object-level information and low-levelpixel data. This presents a challenge for feedforward networks: lower layers inconvolutional nets capture rich spatial information, while upper layers encodeobject-level knowledge but are invariant to factors such as pose andappearance. In this work we propose to augment feedforward nets for objectsegmentation with a novel top-down refinement approach. The resultingbottom-up/top-down architecture is capable of efficiently generatinghigh-fidelity object masks. Similarly to skip connections, our approachleverages features at all layers of the net. Unlike skip connections, ourapproach does not attempt to output independent predictions at each layer.Instead, we first output a coarse `mask encoding' in a feedforward pass, thenrefine this mask encoding in a top-down pass utilizing features at successivelylower layers. The approach is simple, fast, and effective. Building on therecent DeepMask network for generating object proposals, we show accuracyimprovements of 10-20% in average recall for various setups and for smallobjects we improve recall by nearly 2 times. Additionally, by optimizing theoverall network architecture, our approach, which we call SharpMask, is 50\%faster than the original DeepMask network (under .8s per image).
