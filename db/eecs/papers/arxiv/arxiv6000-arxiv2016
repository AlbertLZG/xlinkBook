arxiv-6000-1 | Structure Tensor Based Image Interpolation Method | http://arxiv.org/abs/1402.5564 | author:Ahmadreza Baghaie, Zeyun Yu category:cs.CV published:2014-02-22 summary:Feature preserving image interpolation is an active area in image processingfield. In this paper a new direct edge directed image super-resolutionalgorithm based on structure tensors is proposed. Using an isotropic Gaussianfilter, the structure tensor at each pixel of the input image is computed andthe pixels are classified to three distinct classes; uniform region, cornersand edges, according to the eigenvalues of the structure tensor. Due toapplication of the isotropic Gaussian filter, the classification is robust tonoise presented in image. Based on the tangent eigenvector of the structuretensor, the edge direction is determined and used for interpolation along theedges. In comparison to some previous edge directed image interpolationmethods, the proposed method achieves higher quality in both subjective andobjective aspects. Also the proposed method outperforms previous methods incase of noisy and JPEG compressed images. Furthermore, without the need foroptimization in the process, the algorithm can achieve higher speed.
arxiv-6000-2 | Information Aggregation in Exponential Family Markets | http://arxiv.org/abs/1402.5458 | author:Jacob Abernethy, Sindhu Kutty, Sébastien Lahaie, Rahul Sami category:cs.AI cs.GT stat.ML published:2014-02-22 summary:We consider the design of prediction market mechanisms known as automatedmarket makers. We show that we can design these mechanisms via the mold of\emph{exponential family distributions}, a popular and well-studied probabilitydistribution template used in statistics. We give a full development of thisrelationship and explore a range of benefits. We draw connections between theinformation aggregation of market prices and the belief aggregation of learningagents that rely on exponential family distributions. We develop a very naturalanalysis of the market behavior as well as the price equilibrium under theassumption that the traders exhibit risk aversion according to exponentialutility. We also consider similar aspects under alternative models, such aswhen traders are budget constrained.
arxiv-6000-3 | Efficient Semidefinite Spectral Clustering via Lagrange Duality | http://arxiv.org/abs/1402.5497 | author:Yan Yan, Chunhua Shen, Hanzi Wang category:cs.LG cs.CV published:2014-02-22 summary:We propose an efficient approach to semidefinite spectral clustering (SSC),which addresses the Frobenius normalization with the positive semidefinite(p.s.d.) constraint for spectral clustering. Compared with the originalFrobenius norm approximation based algorithm, the proposed algorithm can moreaccurately find the closest doubly stochastic approximation to the affinitymatrix by considering the p.s.d. constraint. In this paper, SSC is formulatedas a semidefinite programming (SDP) problem. In order to solve the highcomputational complexity of SDP, we present a dual algorithm based on theLagrange dual formalization. Two versions of the proposed algorithm areproffered: one with less memory usage and the other with faster convergencerate. The proposed algorithm has much lower time complexity than that of thestandard interior-point based SDP solvers. Experimental results on both UCIdata sets and real-world image data sets demonstrate that 1) compared with thestate-of-the-art spectral clustering methods, the proposed algorithm achievesbetter clustering performance; and 2) our algorithm is much more efficient andcan solve larger-scale SSC problems than those standard interior-point SDPsolvers.
arxiv-6000-4 | From Predictive to Prescriptive Analytics | http://arxiv.org/abs/1402.5481 | author:Dimitris Bertsimas, Nathan Kallus category:stat.ML cs.LG math.OC published:2014-02-22 summary:In this paper, we combine ideas from machine learning (ML) and operationsresearch and management science (OR/MS) in developing a framework, along withspecific methods, for using data to prescribe decisions in OR/MS problems. In adeparture from other work on data-driven optimization and reflecting ourpractical experience with the data available in applications of OR/MS, weconsider data consisting, not only of observations of quantities with directeffect on costs/revenues, such as demand or returns, but predominantly ofobservations of associated auxiliary quantities. The main problem of interestis a conditional stochastic optimization problem, given imperfect observations,where the joint probability distributions that specify the problem are unknown.We demonstrate that our proposed solution methods are generally applicable to awide range of decision problems. We prove that they are computationallytractable and asymptotically optimal under mild conditions even when data isnot independent and identically distributed (iid) and even for censoredobservations. As an analogue to the coefficient of determination $R^2$, wedevelop a metric $P$ termed the coefficient of prescriptiveness to measure theprescriptive content of data and the efficacy of a policy from an operationsperspective. To demonstrate the power of our approach in a real-world settingwe study an inventory management problem faced by the distribution arm of aninternational media conglomerate, which ships an average of 1 billion units peryear. We leverage both internal data and public online data harvested fromIMDb, Rotten Tomatoes, and Google to prescribe operational decisions thatoutperform baseline measures. Specifically, the data we collect, leveraged byour methods, accounts for an 88% improvement as measured by our coefficient ofprescriptiveness.
arxiv-6000-5 | Pareto-depth for Multiple-query Image Retrieval | http://arxiv.org/abs/1402.5176 | author:Ko-Jen Hsiao, Jeff Calder, Alfred O. Hero III category:cs.IR cs.LG stat.ML published:2014-02-21 summary:Most content-based image retrieval systems consider either one single query,or multiple queries that include the same object or represent the same semanticinformation. In this paper we consider the content-based image retrievalproblem for multiple query images corresponding to different image semantics.We propose a novel multiple-query information retrieval algorithm that combinesthe Pareto front method (PFM) with efficient manifold ranking (EMR). We showthat our proposed algorithm outperforms state of the art multiple-queryretrieval algorithms on real-world image databases. We attribute thisperformance improvement to concavity properties of the Pareto fronts, and provea theoretical result that characterizes the asymptotic concavity of the fronts.
arxiv-6000-6 | Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1$ Updates | http://arxiv.org/abs/1402.5180 | author:Animashree Anandkumar, Rong Ge, Majid Janzamin category:cs.LG math.NA stat.ML published:2014-02-21 summary:In this paper, we provide local and global convergence guarantees forrecovering CP (Candecomp/Parafac) tensor decomposition. The main step of theproposed algorithm is a simple alternating rank-$1$ update which is thealternating version of the tensor power iteration adapted for asymmetrictensors. Local convergence guarantees are established for third order tensorsof rank $k$ in $d$ dimensions, when $k=o \bigl( d^{1.5} \bigr)$ and the tensorcomponents are incoherent. Thus, we can recover overcomplete tensordecomposition. We also strengthen the results to global convergence guaranteesunder stricter rank condition $k \le \beta d$ (for arbitrary constant $\beta >1$) through a simple initialization procedure where the algorithm isinitialized by top singular vectors of random tensor slices. Furthermore, theapproximate local convergence guarantees for $p$-th order tensors are alsoprovided under rank condition $k=o \bigl( d^{p/2} \bigr)$. The guarantees alsoinclude tight perturbation analysis given noisy tensor.
arxiv-6000-7 | An Evolutionary approach for solving Shrödinger Equation | http://arxiv.org/abs/1402.5428 | author:Khalid jebari, Mohammed Madiafi, Abdelaziz Elmoujahid category:cs.NE published:2014-02-21 summary:The purpose of this paper is to present a method of solving the Shr\"odingerEquation (SE) by Genetic Algorithms and Grammatical Evolution. The method formsgenerations of trial solutions expressed in an analytical form. We illustratethe effectiveness of this method providing, for example, the results of itsapplication to a quantum system minimal energy, and we compare these resultswith those produced by traditional analytical methods
arxiv-6000-8 | Important Molecular Descriptors Selection Using Self Tuned Reweighted Sampling Method for Prediction of Antituberculosis Activity | http://arxiv.org/abs/1402.5360 | author:Doreswamy, Chanabasayya M. Vastrad category:cs.LG stat.AP stat.ML published:2014-02-21 summary:In this paper, a new descriptor selection method for selecting an optimalcombination of important descriptors of sulfonamide derivatives data, namedself tuned reweighted sampling (STRS), is developed. descriptors are defined asthe descriptors with large absolute coefficients in a multivariate linearregression model such as partial least squares(PLS). In this study, theabsolute values of regression coefficients of PLS model are used as an indexfor evaluating the importance of each descriptor Then, based on the importancelevel of each descriptor, STRS sequentially selects N subsets of descriptorsfrom N Monte Carlo (MC) sampling runs in an iterative and competitive manner.In each sampling run, a fixed ratio (e.g. 80%) of samples is first randomlyselected to establish a regresson model. Next, based on the regressioncoefficients, a two-step procedure including rapidly decreasing function (RDF)based enforced descriptor selection and self tuned sampling (STS) basedcompetitive descriptor selection is adopted to select the importantdescriptorss. After running the loops, a number of subsets of descriptors areobtained and root mean squared error of cross validation (RMSECV) of PLS modelsestablished with subsets of descriptors is computed. The subset of descriptorswith the lowest RMSECV is considered as the optimal descriptor subset. Theperformance of the proposed algorithm is evaluated by sulfanomide derivativedataset. The results reveal an good characteristic of STRS that it can usuallylocate an optimal combination of some important descriptors which areinterpretable to the biologically of interest. Additionally, our study showsthat better prediction is obtained by STRS when compared to full descriptor setPLS modeling, Monte Carlo uninformative variable elimination (MC-UVE).
arxiv-6000-9 | Convergence results for projected line-search methods on varieties of low-rank matrices via Łojasiewicz inequality | http://arxiv.org/abs/1402.5284 | author:Reinhold Schneider, André Uschmajew category:math.OC cs.LG math.NA published:2014-02-21 summary:The aim of this paper is to derive convergence results for projectedline-search methods on the real-algebraic variety $\mathcal{M}_{\le k}$ of real$m \times n$ matrices of rank at most $k$. Such methods extend Riemannianoptimization methods, which are successfully used on the smooth manifold$\mathcal{M}_k$ of rank-$k$ matrices, to its closure by taking steps alonggradient-related directions in the tangent cone, and afterwards projecting backto $\mathcal{M}_{\le k}$. Considering such a method circumvents thedifficulties which arise from the nonclosedness and the unbounded curvature of$\mathcal{M}_k$. The pointwise convergence is obtained for real-analyticfunctions on the basis of a \L{}ojasiewicz inequality for the projection of theantigradient to the tangent cone. If the derived limit point lies on the smoothpart of $\mathcal{M}_{\le k}$, i.e. in $\mathcal{M}_k$, this boils down to moreor less known results, but with the benefit that asymptotic convergence rateestimates (for specific step-sizes) can be obtained without an a prioricurvature bound, simply from the fact that the limit lies on a smooth manifold.At the same time, one can give a convincing justification for assuming criticalpoints to lie in $\mathcal{M}_k$: if $X$ is a critical point of $f$ on$\mathcal{M}_{\le k}$, then either $X$ has rank $k$, or $\nabla f(X) = 0$.
arxiv-6000-10 | Detecting Opinions in Tweets | http://arxiv.org/abs/1402.5123 | author:Abdelmalek Amine, Reda Mohamed Hamou, Michel Simonet category:cs.CL cs.SI published:2014-02-20 summary:Given the incessant growth of documents describing the opinions of differentpeople circulating on the web, including Web 2.0 has made it possible to givean opinion on any product in the net. In this paper, we examine the variousopinions expressed in the tweets and classify them positive, negative orneutral by using the emoticons for the Bayesian method and adjectives andadverbs for the Turney's method
arxiv-6000-11 | Conclusions from a NAIVE Bayes Operator Predicting the Medicare 2011 Transaction Data Set | http://arxiv.org/abs/1403.7087 | author:Nick Williams category:cs.LG cs.CY 62, 91 published:2014-02-20 summary:Introduction: The United States Federal Government operates one of the worldslargest medical insurance programs, Medicare, to ensure payment for clinicalservices for the elderly, illegal aliens and those without the ability to payfor their care directly. This paper evaluates the Medicare 2011 TransactionData Set which details the transfer of funds from Medicare to private andpublic clinical care facilities for specific clinical services for theoperational year 2011. Methods: Data mining was conducted to establish therelationships between reported and computed transaction values in the data setto better understand the drivers of Medicare transactions at a programmaticlevel. Results: The models averaged 88 for average model accuracy and 38 foraverage Kappa during training. Some reported classes are highly independentfrom the available data as their predictability remains stable regardless ofredaction of supporting and contradictory evidence. DRG or procedure typeappears to be unpredictable from the available financial transaction values.Conclusions: Overlay hypotheses such as charges being driven by the volumeserved or DRG being related to charges or payments is readily false in thisanalysis despite 28 million Americans being billed through Medicare in 2011 andthe program distributing over 70 billion in this transaction set alone. It maybe impossible to predict the dependencies and data structures the payer of lastresort without data from payers of first and second resort. Political concernsabout Medicare would be better served focusing on these first and second orderpayer systems as what Medicare costs is not dependent on Medicare itself.
arxiv-6000-12 | Distribution-Independent Reliable Learning | http://arxiv.org/abs/1402.5164 | author:Varun Kanade, Justin Thaler category:cs.LG cs.CC cs.DS published:2014-02-20 summary:We study several questions in the reliable agnostic learning framework ofKalai et al. (2009), which captures learning tasks in which one type of erroris costlier than others. A positive reliable classifier is one that makes nofalse positive errors. The goal in the positive reliable agnostic framework isto output a hypothesis with the following properties: (i) its false positiveerror rate is at most $\epsilon$, (ii) its false negative error rate is at most$\epsilon$ more than that of the best positive reliable classifier from theclass. A closely related notion is fully reliable agnostic learning, whichconsiders partial classifiers that are allowed to predict "unknown" on someinputs. The best fully reliable partial classifier is one that makes no errorsand minimizes the probability of predicting "unknown", and the goal in fullyreliable learning is to output a hypothesis that is almost as good as the bestfully reliable partial classifier from a class. For distribution-independent learning, the best known algorithms for PAClearning typically utilize polynomial threshold representations, while thestate of the art agnostic learning algorithms use point-wise polynomialapproximations. We show that one-sided polynomial approximations, anintermediate notion between polynomial threshold representations and point-wisepolynomial approximations, suffice for learning in the reliable agnosticsettings. We then show that majorities can be fully reliably learned anddisjunctions of majorities can be positive reliably learned, throughconstructions of appropriate one-sided polynomial approximations. Our fullyreliable algorithm for majorities provides the first evidence that fullyreliable learning may be strictly easier than agnostic learning. Our algorithmsalso satisfy strong attribute-efficiency properties, and provide smoothtradeoffs between sample complexity and running time.
arxiv-6000-13 | Learning the Parameters of Determinantal Point Process Kernels | http://arxiv.org/abs/1402.4862 | author:Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, Ben Taskar category:stat.ML cs.LG published:2014-02-20 summary:Determinantal point processes (DPPs) are well-suited for modeling repulsionand have proven useful in many applications where diversity is desired. WhileDPPs have many appealing properties, such as efficient sampling, learning theparameters of a DPP is still considered a difficult problem due to thenon-convex nature of the likelihood function. In this paper, we propose usingBayesian methods to learn the DPP kernel parameters. These methods areapplicable in large-scale and continuous DPP settings even when the exact formof the eigendecomposition is unknown. We demonstrate the utility of our DPPlearning methods in studying the progression of diabetic neuropathy based onspatial distribution of nerve fibers, and in studying human perception ofdiversity in images.
arxiv-6000-14 | A Quasi-Newton Method for Large Scale Support Vector Machines | http://arxiv.org/abs/1402.4861 | author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG published:2014-02-20 summary:This paper adapts a recently developed regularized stochastic version of theBroyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for thesolution of support vector machine classification problems. The proposed methodis shown to converge almost surely to the optimal classifier at a rate that islinear in expectation. Numerical results show that the proposed method exhibitsa convergence rate that degrades smoothly with the dimensionality of thefeature vectors.
arxiv-6000-15 | Survey on Sparse Coded Features for Content Based Face Image Retrieval | http://arxiv.org/abs/1402.4888 | author:D. Johnvictor, G. Selvavinayagam category:cs.IR cs.CV cs.LG stat.ML published:2014-02-20 summary:Content based image retrieval, a technique which uses visual contents ofimage to search images from large scale image databases according to users'interests. This paper provides a comprehensive survey on recent technology usedin the area of content based face image retrieval. Nowadays digital devices andphoto sharing sites are getting more popularity, large human face photos areavailable in database. Multiple types of facial features are used to representdiscriminality on large scale human facial image database. Searching and miningof facial images are challenging problems and important research issues. Sparserepresentation on features provides significant improvement in indexing relatedimages to query image.
arxiv-6000-16 | Real-time Automatic Emotion Recognition from Body Gestures | http://arxiv.org/abs/1402.5047 | author:Stefano Piana, Alessandra Staglianò, Francesca Odone, Alessandro Verri, Antonio Camurri category:cs.HC cs.CV published:2014-02-20 summary:Although psychological research indicates that bodily expressions conveyimportant affective information, to date research in emotion recognitionfocused mainly on facial expression or voice analysis. In this paper we proposean approach to realtime automatic emotion recognition from body movements. Aset of postural, kinematic, and geometrical features are extracted fromsequences 3D skeletons and fed to a multi-class SVM classifier. The proposedmethod has been assessed on data acquired through two different systems: aprofessionalgrade optical motion capture system, and Microsoft Kinect. Thesystem has been assessed on a "six emotions" recognition problem, and using aleave-one-subject-out cross validation strategy, reached an overall recognitionrate of 61.3% which is very close to the recognition rate of 61.9% obtained byhuman observers. To provide further testing of the system, two games weredeveloped, where one or two users have to interact to understand and expressemotions with their body.
arxiv-6000-17 | Group-sparse Matrix Recovery | http://arxiv.org/abs/1402.5077 | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.LG cs.CV stat.ML published:2014-02-20 summary:We apply the OSCAR (octagonal selection and clustering algorithms forregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)from compressive measurements. We propose a 2D version of OSCAR (2OSCAR)consisting of the $\ell_1$ norm and the pair-wise $\ell_{\infty}$ norm, whichis convex but non-differentiable. We show that the proximity operator of 2OSCARcan be computed based on that of OSCAR. The 2OSCAR problem can thus beefficiently solved by state-of-the-art proximal splitting algorithms.Experiments on group-sparse 2D array recovery show that 2OSCAR regularizationsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm(with debiasing) yields the most accurate results.
arxiv-6000-18 | Anisotropic Mesh Adaptation for Image Representation | http://arxiv.org/abs/1402.4893 | author:Xianping Li category:cs.CV math.NA I.4.2 published:2014-02-20 summary:Triangular meshes have gained much interest in image representation and havebeen widely used in image processing. This paper introduces a framework ofanisotropic mesh adaptation (AMA) methods to image representation and proposesa GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme.Different than many other methods that triangulate sample points to form themesh, the AMA methods start directly with a triangular mesh and then adapt themesh based on a user-defined metric tensor to represent the image. The AMAmethods have clear mathematical framework and provides flexibility for bothimage representation and image reconstruction. A mesh patching technique isdeveloped for the implementation of the GPRAMA method, which leads to animproved version of the popular GPRFS-ED method. The GPRAMA method can achievebetter quality than the GPRFS-ED method but with lower computational cost.
arxiv-6000-19 | Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit | http://arxiv.org/abs/1402.5076 | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT published:2014-02-20 summary:We propose a new method, {\it robust binary fused compressive sensing}(RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressivemeasurements. The proposed method is a modification of our previous {\it binaryfused compressive sensing} (BFCS) algorithm, which is based on the {\it binaryiterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of theobjective function is a one-sided $\ell_1$ (or $\ell_2$) norm. Experiments showthat the proposed algorithm is able to take advantage of the piece-wisesmoothness of the original signal and detect sign flips and correct them,achieving more accurate recovery than BFCS and BIHT.
arxiv-6000-20 | Exploiting Two-Dimensional Group Sparsity in 1-Bit Compressive Sensing | http://arxiv.org/abs/1402.5073 | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT published:2014-02-20 summary:We propose a new approach, {\it two-dimensional fused binary compressivesensing} (2DFBCS) to recover 2D sparse piece-wise signals from 1-bitmeasurements, exploiting 2D group sparsity for 1-bit compressive sensingrecovery. The proposed method is a modified 2D version of the previous {\itbinary iterative hard thresholding} (2DBIHT) algorithm, where the objectivefunction includes a 2D one-sided $\ell_1$ (or $\ell_2$) penalty functionencouraging agreement with the observed data, an indicator function of$K$-sparsity, and a total variation (TV) or modified TV (MTV) constraint. Thesubgradient of the 2D one-sided $\ell_1$ (or $\ell_2$) penalty and theprojection onto the $K$-sparsity and TV or MTV constraint can be computedefficiently, allowing the appliaction of algorithms of the {\itforward-backward splitting} (a.k.a. {\it iterative shrinkage-thresholding})family. Experiments on the recovery of 2D sparse piece-wise smooth signals showthat the proposed approach is able to take advantage of the piece-wisesmoothness of the original signal, achieving more accurate recovery than2DBIHT. More specifically, 2DFBCS with the MTV and the $\ell_2$ penaltyperforms best amongst the algorithms tested.
arxiv-6000-21 | Le Cam meets LeCun: Deficiency and Generic Feature Learning | http://arxiv.org/abs/1402.4884 | author:Brendan van Rooyen, Robert C. Williamson category:stat.ML published:2014-02-20 summary:"Deep Learning" methods attempt to learn generic features in an unsupervisedfashion from a large unlabelled data set. These generic features should performas well as the best hand crafted features for any learning problem that makesuse of this data. We provide a definition of generic features, characterizewhen it is possible to learn them and provide methods closely related to theautoencoder and deep belief network of deep learning. In order to do so we usethe notion of deficiency and illustrate its value in studying certain generallearning problems.
arxiv-6000-22 | Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group Sparsity | http://arxiv.org/abs/1402.5074 | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT published:2014-02-20 summary:We propose a new method, {\it binary fused compressive sensing} (BFCS), torecover sparse piece-wise smooth signals from 1-bit compressive measurements.The proposed algorithm is a modification of the previous {\it binary iterativehard thresholding} (BIHT) algorithm, where, in addition to the sparsityconstraint, the total-variation of the recovered signal is upper constrained.As in BIHT, the data term of the objective function is an one-sided $\ell_1$(or $\ell_2$) norm. Experiments on the recovery of sparse piece-wise smoothsignals show that the proposed algorithm is able to take advantage of thepiece-wise smoothness of the original signal, achieving more accurate recoverythan BIHT.
arxiv-6000-23 | Enhanced Secure Algorithm for Fingerprint Recognition | http://arxiv.org/abs/1402.4936 | author:Amira Mohammad Abdel-Mawgoud Saleh category:cs.CV published:2014-02-20 summary:Fingerprint recognition requires a minimal effort from the user, does notcapture other information than strictly necessary for the recognition process,and provides relatively good performance. A critical step in fingerprintidentification system is thinning of the input fingerprint image. Theperformance of a minutiae extraction algorithm relies heavily on the quality ofthe thinning algorithm. So, a fast fingerprint thinning algorithm is proposed.The algorithm works directly on the gray-scale image as binarization offingerprint causes many spurious minutiae and also removes many importantfeatures. The performance of the thinning algorithm is evaluated andexperimental results show that the proposed thinning algorithm is both fast andaccurate. A new minutiae-based fingerprint matching technique is proposed. Themain idea is that each fingerprint is represented by a minutiae table of justtwo columns in the database. The number of different minutiae types(terminations and bifurcations) found in each track of a certain width aroundthe core point of the fingerprint is recorded in this table. Each row in thetable represents a certain track, in the first column, the number ofterminations in each track is recorded, in the second column, the number ofbifurcations in each track is recorded. The algorithm is rotation andtranslation invariant, and needs less storage size. Experimental results showthat recognition accuracy is 98%, with Equal Error Rate (EER) of 2%. Finally,the integrity of the data transmission via communication channels must besecure all the way from the scanner to the application. After applying Gaussiannoise addition, and JPEG compression with high and moderate quality factors onthe watermarked fingerprint images, recognition accuracy decreases slightly toreach 96%.
arxiv-6000-24 | Vesselness via Multiple Scale Orientation Scores | http://arxiv.org/abs/1402.4963 | author:Julius Hannink, Remco Duits, Erik Bekkers category:cs.CV published:2014-02-20 summary:The multi-scale Frangi vesselness filter is an established tool in (retinal)vascular imaging. However, it cannot cope with crossings or bifurcations, sinceit only looks for elongated structures. Therefore, we disentangle crossingstructures in the image via (multiple scale) invertible orientation scores. Thedescribed vesselness filter via scale-orientation scores performs considerablybetter at enhancing vessels throughout crossings and bifurcations than theFrangi version. Both methods are evaluated on a public dataset. Performance ismeasured by comparing ground truth data to the segmentation results obtained bybasic thresholding and morphological component analysis of the filtered images.
arxiv-6000-25 | Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Noisy Matrix Decomposition | http://arxiv.org/abs/1402.5131 | author:Hanie Sedghi, Anima Anandkumar, Edmond Jonckheere category:cs.LG math.OC stat.ML published:2014-02-20 summary:We propose an efficient ADMM method with guarantees for high-dimensionalproblems. We provide explicit bounds for the sparse optimization problem andthe noisy matrix decomposition problem. For sparse optimization, we establishthat the modified ADMM method has an optimal convergence rate of$\mathcal{O}(s\log d/T)$, where $s$ is the sparsity level, $d$ is the datadimension and $T$ is the number of steps. This matches with the minimax lowerbounds for sparse estimation. For matrix decomposition into sparse and low rankcomponents, we provide the first guarantees for any online method, and prove aconvergence rate of $\tilde{\mathcal{O}}((s+r)\beta^2(p) /T) +\mathcal{O}(1/p)$ for a $p\times p$ matrix, where $s$ is the sparsity level,$r$ is the rank and $\Theta(\sqrt{p})\leq \beta(p)\leq \Theta(p)$. Ourguarantees match the minimax lower bound with respect to $s,r$ and $T$. Inaddition, we match the minimax lower bound with respect to the matrix dimension$p$, i.e. $\beta(p)=\Theta(\sqrt{p})$, for many important statistical modelsincluding the independent noise model, the linear Bayesian network and thelatent Gaussian graphical model under some conditions. Our ADMM method is basedon epoch-based annealing and consists of inexpensive steps which involveprojections on to simple norm balls. Experiments show that for both sparseoptimization and matrix decomposition problems, our algorithm outperforms thestate-of-the-art methods. In particular, we reach higher accuracy with sametime complexity.
arxiv-6000-26 | Analysis of Multibeam SONAR Data using Dissimilarity Representations | http://arxiv.org/abs/1402.6636 | author:Iain Rice, Roger Benton, Les Hart, David Lowe category:cs.CE stat.ML published:2014-02-19 summary:This paper considers the problem of low-dimensional visualisation of veryhigh dimensional information sources for the purpose of situation awareness inthe maritime environment. In response to the requirement for human decisionsupport aids to reduce information overload (and specifically, data amenable tointer-point relative similarity measures) appropriate to the below-watermaritime domain, we are investigating a preliminary prototype topographicvisualisation model. The focus of the current paper is on the mathematicalproblem of exploiting a relative dissimilarity representation of signals in avisual informatics mapping model, driven by real-world sonar systems. Anindependent source model is used to analyse the sonar beams from which a simpleprobabilistic input model to represent uncertainty is mapped to a latentvisualisation space where data uncertainty can be accommodated. The use ofeuclidean and non-euclidean measures are used and the motivation for future useof non-euclidean measures is made. Concepts are illustrated using a simulated64 beam weak SNR dataset with realistic sonar targets.
arxiv-6000-27 | Unsupervised Ranking of Multi-Attribute Objects Based on Principal Curves | http://arxiv.org/abs/1402.4542 | author:Chun-Guo Li, Xing Mei, Bao-Gang Hu category:cs.LG cs.AI stat.ML published:2014-02-19 summary:Unsupervised ranking faces one critical challenge in evaluation applications,that is, no ground truth is available. When PageRank and its variants show agood solution in related subjects, they are applicable only for ranking fromlink-structure data. In this work, we focus on unsupervised ranking frommulti-attribute data which is also common in evaluation tasks. To overcome thechallenge, we propose five essential meta-rules for the design and assessmentof unsupervised ranking approaches: scale and translation invariance, strictmonotonicity, linear/nonlinear capacities, smoothness, and explicitness ofparameter size. These meta-rules are regarded as high level knowledge forunsupervised ranking tasks. Inspired by the works in [8] and [14], we propose aranking principal curve (RPC) model, which learns a one-dimensional manifoldfunction to perform unsupervised ranking tasks on multi-attribute observations.Furthermore, the RPC is modeled to be a cubic B\'ezier curve with controlpoints restricted in the interior of a hypercube, thereby complying with allthe five meta-rules to infer a reasonable ranking list. With control points asthe model parameters, one is able to understand the learned manifold and tointerpret the ranking list semantically. Numerical experiments of the presentedRPC model are conducted on two open datasets of different ranking applications.In comparison with the state-of-the-art approaches, the new model is able toshow more reasonable ranking lists.
arxiv-6000-28 | A Statistical Approach to Set Classification by Feature Selection with Applications to Classification of Histopathology Images | http://arxiv.org/abs/1402.4539 | author:Sungkyu Jung, Xingye Qiao category:stat.ME stat.ML published:2014-02-19 summary:Set classification problems arise when classification tasks are based on setsof observations as opposed to individual observations. In set classification, aclassification rule is trained with $N$ sets of observations, where each set islabeled with class information, and the prediction of a class label isperformed also with a set of observations. Data sets for set classificationappear, for example, in diagnostics of disease based on multiple cell nucleusimages from a single tissue. Relevant statistical models for set classificationare introduced, which motivate a set classification framework based oncontext-free feature extraction. By understanding a set of observations as anempirical distribution, we employ a data-driven method to choose those featureswhich contain information on location and major variation. In particular, themethod of principal component analysis is used to extract the features of majorvariation. Multidimensional scaling is used to represent features asvector-valued points on which conventional classifiers can be applied. Theproposed set classification approaches achieve better classification resultsthan competing methods in a number of simulated data examples. The benefits ofour method are demonstrated in an analysis of histopathology images of cellnuclei related to liver cancer.
arxiv-6000-29 | A Powerful Genetic Algorithm for Traveling Salesman Problem | http://arxiv.org/abs/1402.4699 | author:Shujia Liu category:cs.NE cs.AI published:2014-02-19 summary:This paper presents a powerful genetic algorithm(GA) to solve the travelingsalesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)with a local search procedure to determine good combinations of building blocksof parent solutions for generating even better offspring solutions.Experimental results on well studied TSP benchmarks demonstrate that theproposed GA is competitive in finding very high quality solutions on instanceswith up to 16,862 cities.
arxiv-6000-30 | Sparse Quantile Huber Regression for Efficient and Robust Estimation | http://arxiv.org/abs/1402.4624 | author:Aleksandr Y. Aravkin, Anju Kambadur, Aurelie C. Lozano, Ronny Luss category:stat.ML cs.DS math.OC stat.ME 62F35, 65K10 published:2014-02-19 summary:We consider new formulations and methods for sparse quantile regression inthe high-dimensional setting. Quantile regression plays an important role inmany applications, including outlier-robust exploratory analysis in geneselection. In addition, the sparsity consideration in quantile regressionenables the exploration of the entire conditional distribution of the responsevariable given the predictors and therefore yields a more comprehensive view ofthe important predictors. We propose a generalized OMP algorithm for variableselection, taking the misfit loss to be either the traditional quantile loss ora smooth version we call quantile Huber, and compare the resulting greedyapproaches with convex sparsity-regularized formulations. We apply a recentlyproposed interior point methodology to efficiently solve all convexformulations as well as convex subproblems in the generalized OMP setting, pro-vide theoretical guarantees of consistent estimation, and demonstrate theperformance of our approach using empirical studies of simulated and genomicdatasets.
arxiv-6000-31 | Efficient Inference of Gaussian Process Modulated Renewal Processes with Application to Medical Event Data | http://arxiv.org/abs/1402.4732 | author:Thomas A. Lasko category:stat.ML cs.LG stat.AP published:2014-02-19 summary:The episodic, irregular and asynchronous nature of medical data render themdifficult substrates for standard machine learning algorithms. We would like toabstract away this difficulty for the class of time-stamped categoricalvariables (or events) by modeling them as a renewal process and inferring aprobability density over continuous, longitudinal, nonparametric intensityfunctions modulating that process. Several methods exist for inferring such adensity over intensity functions, but either their constraints and assumptionsprevent their use with our potentially bursty event streams, or their timecomplexity renders their use intractable on our long-duration observations ofhigh-resolution events, or both. In this paper we present a new and efficientmethod for inferring a distribution over intensity functions that uses directnumeric integration and smooth interpolation over Gaussian processes. Wedemonstrate that our direct method is up to twice as accurate and two orders ofmagnitude more efficient than the best existing method (thinning). Importantly,the direct method can infer intensity functions over the full range of burstyto memoryless to regular events, which thinning and many other methods cannot.Finally, we apply the method to clinical event data and demonstrate theface-validity of the abstraction, which is now amenable to standard learningalgorithms.
arxiv-6000-32 | A Survey on Semi-Supervised Learning Techniques | http://arxiv.org/abs/1402.4645 | author:V. Jothi Prakash, Dr. L. M. Nithya category:cs.LG published:2014-02-19 summary:Semisupervised learning is a learning standard which deals with the study ofhow computers and natural systems such as human beings acquire knowledge in thepresence of both labeled and unlabeled data. Semisupervised learning basedmethods are preferred when compared to the supervised and unsupervised learningbecause of the improved performance shown by the semisupervised approaches inthe presence of large volumes of data. Labels are very hard to attain whileunlabeled data are surplus, therefore semisupervised learning is a nobleindication to shrink human labor and improve accuracy. There has been a largespectrum of ideas on semisupervised learning. In this paper we bring out someof the key approaches for semisupervised learning.
arxiv-6000-33 | Near-optimal-sample estimators for spherical Gaussian mixtures | http://arxiv.org/abs/1402.4746 | author:Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha Suresh category:cs.LG cs.DS cs.IT math.IT stat.ML published:2014-02-19 summary:Statistical and machine-learning algorithms are frequently applied tohigh-dimensional data. In many of these applications data is scarce, and oftenmuch more costly than computation time. We provide the first sample-efficientpolynomial-time estimator for high-dimensional spherical Gaussian mixtures. For mixtures of any $k$ $d$-dimensional spherical Gaussians, we derive anintuitive spectral-estimator that uses$\mathcal{O}_k\bigl(\frac{d\log^2d}{\epsilon^4}\bigr)$ samples and runs in time$\mathcal{O}_{k,\epsilon}(d^3\log^5 d)$, both significantly lower thanpreviously known. The constant factor $\mathcal{O}_k$ is polynomial for samplecomplexity and is exponential for the time complexity, again much smaller thanwhat was previously known. We also show that$\Omega_k\bigl(\frac{d}{\epsilon^2}\bigr)$ samples are needed for anyalgorithm. Hence the sample complexity is near-optimal in the number ofdimensions. We also derive a simple estimator for one-dimensional mixtures that uses$\mathcal{O}\bigl(\frac{k \log \frac{k}{\epsilon} }{\epsilon^2} \bigr)$ samplesand runs in time$\widetilde{\mathcal{O}}\left(\bigl(\frac{k}{\epsilon}\bigr)^{3k+1}\right)$.Our other technical contributions include a faster algorithm for choosing adensity estimate from a set of distributions, that minimizes the $\ell_1$distance to an unknown underlying distribution.
arxiv-6000-34 | The Sample Complexity of Subspace Learning with Partial Information | http://arxiv.org/abs/1402.4844 | author:Alon Gonen, Dan Rosenbaum, Yonina Eldar, Shai Shalev-Shwartz category:cs.LG stat.ML published:2014-02-19 summary:The goal of subspace learning is to find a $k$-dimensional subspace of$\mathbb{R}^d$, such that the expected squared distance between instancevectors and the subspace is as small as possible. In this paper we study thesample complexity of subspace learning in a \emph{partial information} setting,in which the learner can only observe $r \le d$ attributes from each instancevector. We derive upper and lower bounds on the sample complexity in differentscenarios. In particular, our upper bounds involve a generalization of vectorsampling techniques, which are often used in bandit problems, to matrices.
arxiv-6000-35 | Diffusion Least Mean Square: Simulations | http://arxiv.org/abs/1402.4845 | author:Jonathan Gelati, Sithan Kanna category:cs.LG cs.MA published:2014-02-19 summary:In this technical report we analyse the performance of diffusion strategiesapplied to the Least-Mean-Square adaptive filter. We configure a network ofcooperative agents running adaptive filters and discuss their behaviour whencompared with a non-cooperative agent which represents the average of thenetwork. The analysis provides conditions under which diversity in the filterparameters is beneficial in terms of convergence and stability. Simulationsdrive and support the analysis.
arxiv-6000-36 | Transduction on Directed Graphs via Absorbing Random Walks | http://arxiv.org/abs/1402.4566 | author:Jaydeep De, Xiaowei Zhang, Li Cheng category:cs.CV cs.LG stat.ML published:2014-02-19 summary:In this paper we consider the problem of graph-based transductiveclassification, and we are particularly interested in the directed graphscenario which is a natural form for many real world applications. Differentfrom existing research efforts that either only deal with undirected graphs orcircumvent directionality by means of symmetrization, we propose a novel randomwalk approach on directed graphs using absorbing Markov chains, which can beregarded as maximizing the accumulated expected number of visits from theunlabeled transient states. Our algorithm is simple, easy to implement, andworks with large-scale graphs. In particular, it is capable of preserving thegraph structure even when the input graph is sparse and changes over time, aswell as retaining weak signals presented in the directed edges. We present itsintimate connections to a number of existing methods, including graph kernels,graph Laplacian based methods, and interestingly, spanning forest of graphs.Its computational complexity and the generalization error are also studied.Empirically our algorithm is systematically evaluated on a wide range ofapplications, where it has shown to perform competitively comparing to a suiteof state-of-the-art methods.
arxiv-6000-37 | Retrieval of Experiments by Efficient Estimation of Marginal Likelihood | http://arxiv.org/abs/1402.4653 | author:Sohan Seth, John Shawe-Taylor, Samuel Kaski category:stat.ML cs.IR cs.LG published:2014-02-19 summary:We study the task of retrieving relevant experiments given a queryexperiment. By experiment, we mean a collection of measurements from a set of`covariates' and the associated `outcomes'. While similar experiments can beretrieved by comparing available `annotations', this approach ignores thevaluable information available in the measurements themselves. To incorporatethis information in the retrieval task, we suggest employing a retrieval metricthat utilizes probabilistic models learned from the measurements. We argue thatsuch a metric is a sensible measure of similarity between two experiments sinceit permits inclusion of experiment-specific prior knowledge. However, accuratemodels are often not analytical, and one must resort to storing posteriorsamples which demands considerable resources. Therefore, we study strategies toselect informative posterior samples to reduce the computational load whilemaintaining the retrieval performance. We demonstrate the efficacy of ourapproach on simulated data with simple linear regression as the models, andreal world datasets.
arxiv-6000-38 | A Bayesian Model of node interaction in networks | http://arxiv.org/abs/1402.4279 | author:Ingmar Schuster category:cs.LG stat.ME stat.ML published:2014-02-18 summary:We are concerned with modeling the strength of links in networks by takinginto account how often those links are used. Link usage is a strong indicatorof how closely two nodes are related, but existing network models in BayesianStatistics and Machine Learning are able to predict only wether a link existsat all. As priors for latent attributes of network nodes we explore the ChineseRestaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality.The model is applied to a social network dataset and a word coocurrencedataset.
arxiv-6000-39 | Classification with Sparse Overlapping Groups | http://arxiv.org/abs/1402.4512 | author:Nikhil Rao, Robert Nowak, Christopher Cox, Timothy Rogers category:cs.LG stat.ML published:2014-02-18 summary:Classification with a sparsity constraint on the solution plays a centralrole in many high dimensional machine learning applications. In some cases, thefeatures can be grouped together so that entire subsets of features can beselected or not selected. In many applications, however, this can be toorestrictive. In this paper, we are interested in a less restrictive form ofstructured sparse feature selection: we assume that while features can begrouped according to some notion of similarity, not all features in a groupneed be selected for the task at hand. When the groups are comprised ofdisjoint sets of features, this is sometimes referred to as the "sparse group"lasso, and it allows for working with a richer class of models than traditionalgroup lasso methods. Our framework generalizes conventional sparse group lassofurther by allowing for overlapping groups, an additional flexiblity needed inmany applications and one that presents further challenges. The maincontribution of this paper is a new procedure called Sparse Overlapping Group(SOG) lasso, a convex optimization program that automatically selects similarfeatures for classification in high dimensions. We establish model selectionerror bounds for SOGlasso classification problems under a fairly generalsetting. In particular, the error bounds are the first such results forclassification using the sparse group lasso. Furthermore, the general SOGlassobound specializes to results for the lasso and the group lasso, some known andsome new. The SOGlasso is motivated by multi-subject fMRI studies in whichfunctional activity is classified using brain voxels as features, sourcelocalization problems in Magnetoencephalography (MEG), and analyzing geneactivation patterns in microarray data analysis. Experiments with real andsynthetic data demonstrate the advantages of SOGlasso compared to the lasso andgroup lasso.
arxiv-6000-40 | A Kernel Independence Test for Random Processes | http://arxiv.org/abs/1402.4501 | author:Kacper Chwialkowski, Arthur Gretton category:stat.ML 62G10 published:2014-02-18 summary:A new non parametric approach to the problem of testing the independence oftwo random process is developed. The test statistic is the Hilbert SchmidtIndependence Criterion (HSIC), which was used previously in testingindependence for i.i.d pairs of variables. The asymptotic behaviour of HSIC isestablished when computed from samples drawn from random processes. It is shownthat earlier bootstrap procedures which worked in the i.i.d. case will fail forrandom processes, and an alternative consistent estimate of the p-values isproposed. Tests on artificial data and real-world Forex data indicate that thenew test procedure discovers dependence which is missed by linear approaches,while the earlier bootstrap procedure returns an elevated number of falsepositives. The code is available online:https://github.com/kacperChwialkowski/HSIC .
arxiv-6000-41 | Ambiguity in language networks | http://arxiv.org/abs/1402.4802 | author:Ricard V. Solé, Luís F. Seoane category:physics.soc-ph cs.CL q-bio.NC published:2014-02-18 summary:Human language defines the most complex outcomes of evolution. The emergenceof such an elaborated form of communication allowed humans to create extremelystructured societies and manage symbols at different levels including, amongothers, semantics. All linguistic levels have to deal with an astronomiccombinatorial potential that stems from the recursive nature of languages. Thisrecursiveness is indeed a key defining trait. However, not all words areequally combined nor frequent. In breaking the symmetry between less and moreoften used and between less and more meaning-bearing units, universal scalinglaws arise. Such laws, common to all human languages, appear on differentstages from word inventories to networks of interacting words. Among theseseemingly universal traits exhibited by language networks, ambiguity appears tobe a specially relevant component. Ambiguity is avoided in most computationalapproaches to language processing, and yet it seems to be a crucial element oflanguage architecture. Here we review the evidence both from language networkarchitecture and from theoretical reasonings based on a least effort argument.Ambiguity is shown to play an essential role in providing a source of languageefficiency, and is likely to be an inevitable byproduct of network growth.
arxiv-6000-42 | Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning | http://arxiv.org/abs/1402.4419 | author:Julien Mairal category:math.OC cs.LG stat.ML published:2014-02-18 summary:Majorization-minimization algorithms consist of successively minimizing asequence of upper bounds of the objective function. These upper bounds aretight at the current estimate, and each iteration monotonically drives theobjective function downhill. Such a simple principle is widely applicable andhas been very popular in various scientific fields, especially in signalprocessing and statistics. In this paper, we propose an incrementalmajorization-minimization scheme for minimizing a large sum of continuousfunctions, a problem of utmost importance in machine learning. We presentconvergence guarantees for non-convex and convex optimization when the upperbounds approximate the objective up to a smooth error; we call such upperbounds "first-order surrogate functions". More precisely, we study asymptoticstationary point guarantees for non-convex problems, and for convex ones, weprovide convergence rates for the expected objective function value. We applyour scheme to composite optimization and obtain a new incremental proximalgradient algorithm with linear convergence rate for strongly convex functions.In our experiments, we show that our method is competitive with the state ofthe art for solving machine learning problems such as logistic regression whenthe number of training samples is large enough, and we demonstrate itsusefulness for sparse estimation with non-convex penalties.
arxiv-6000-43 | Student-t Processes as Alternatives to Gaussian Processes | http://arxiv.org/abs/1402.4306 | author:Amar Shah, Andrew Gordon Wilson, Zoubin Ghahramani category:stat.ML cs.AI cs.LG stat.ME published:2014-02-18 summary:We investigate the Student-t process as an alternative to the Gaussianprocess as a nonparametric prior over functions. We derive closed formexpressions for the marginal likelihood and predictive distribution of aStudent-t process, by integrating away an inverse Wishart process prior overthe covariance kernel of a Gaussian process model. We show surprisingequivalences between different hierarchical Gaussian process models leading toStudent-t processes, and derive a new sampling scheme for the inverse Wishartprocess, which helps elucidate these equivalences. Overall, we show that aStudent-t process can retain the attractive properties of a Gaussian process --a nonparametric representation, analytic marginal and predictive distributions,and easy model selection through covariance kernels -- but has enhancedflexibility, and predictive covariances that, unlike a Gaussian process,explicitly depend on the values of training observations. We verify empiricallythat a Student-t process is especially useful in situations where there arechanges in covariance structure, or in applications like Bayesian optimization,where accurate predictive covariances are critical for good performance. Theseadvantages come at no additional computational cost over Gaussian processes.
arxiv-6000-44 | High Dimensional Semiparametric Scale-Invariant Principal Component Analysis | http://arxiv.org/abs/1402.4507 | author:Fang Han, Han Liu category:stat.ML published:2014-02-18 summary:We propose a new high dimensional semiparametric principal component analysis(PCA) method, named Copula Component Analysis (COCA). The semiparametric modelassumes that, after unspecified marginally monotone transformations, thedistributions are multivariate Gaussian. COCA improves upon PCA and sparse PCAin three aspects: (i) It is robust to modeling assumptions; (ii) It is robustto outliers and data contamination; (iii) It is scale-invariant and yields moreinterpretable results. We prove that the COCA estimators obtain fast estimationrates and are feature selection consistent when the dimension is nearlyexponentially large relative to the sample size. Careful experiments confirmthat COCA outperforms sparse PCA on both synthetic and real-world datasets.
arxiv-6000-45 | Artificial Mutation inspired Hyper-heuristic for Runtime Usage of Multi-objective Algorithms | http://arxiv.org/abs/1402.4442 | author:Donia El Kateb, François Fouquet, Johann Bourcier, Yves Le Traon category:cs.SE cs.NE published:2014-02-18 summary:In the last years, multi-objective evolutionary algorithms (MOEA) have beenapplied to different software engineering problems where many conflictingobjectives have to be optimized simultaneously. In theory, evolutionaryalgorithms feature a nice property for runtime optimization as they can providea solution in any execution time. In practice, based on a Darwinian inspirednatural selection, these evolutionary algorithms produce many deadbornsolutions whose computation results in a computational resources wastage:natural selection is naturally slow. In this paper, we reconsider this foundinganalogy to accelerate convergence of MOEA, by looking at modern biologystudies: artificial selection has been used to achieve an anticipated specificpurpose instead of only relying on crossover and natural selection (i.e.,Muller et al [18] research on artificial mutation of fruits with X-Ray).Putting aside the analogy with natural selection , the present paper proposesan hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificialselective mutation to improve the convergence speed of MOEA. Sputnik leveragesthe past history of mutation efficiency to select the most relevant mutationsto perform. We evaluate Sputnik on a cloud-reasoning engine, which driveson-demand provisioning while considering conflicting performance and costobjectives. We have conducted experiments to highlight the significantperformance improvement of Sputnik in terms of resolution time.
arxiv-6000-46 | Automatic Detection of Font Size Straight from Run Length Compressed Text Documents | http://arxiv.org/abs/1402.4388 | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-02-18 summary:Automatic detection of font size finds many applications in the area ofintelligent OCRing and document image analysis, which has been traditionallypracticed over uncompressed documents, although in real life the documentsexist in compressed form for efficient storage and transmission. It would benovel and intelligent if the task of font size detection could be carried outdirectly from the compressed data of these documents without decompressing,which would result in saving of considerable amount of processing time andspace. Therefore, in this paper we present a novel idea of learning anddetecting font size directly from run-length compressed text documents at linelevel using simple line height features, which paves the way for intelligentOCRing and document analysis directly from compressed documents. In theproposed model, the given mixed-case text documents of different font size aresegmented into compressed text lines and the features extracted such as lineheight and ascender height are used to capture the pattern of font size in theform of a regression line, using which the automatic detection of font size isdone during the recognition stage. The method is experimented with a dataset of50 compressed documents consisting of 780 text lines of single font size and375 text lines of mixed font size resulting in an overall accuracy of 99.67%.
arxiv-6000-47 | Fast X-ray CT image reconstruction using the linearized augmented Lagrangian method with ordered subsets | http://arxiv.org/abs/1402.4381 | author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML published:2014-02-18 summary:The augmented Lagrangian (AL) method that solves convex optimization problemswith linear constraints has drawn more attention recently in imagingapplications due to its decomposable structure for composite cost functions andempirical fast convergence rate under weak conditions. However, for problemssuch as X-ray computed tomography (CT) image reconstruction and large-scalesparse regression with "big data", where there is no efficient way to solve theinner least-squares problem, the AL method can be slow due to the inevitableiterative inner updates. In this paper, we focus on solving regularized(weighted) least-squares problems using a linearized variant of the AL methodthat replaces the quadratic AL penalty term in the scaled augmented Lagrangianwith its separable quadratic surrogate (SQS) function, thus leading to a muchsimpler ordered-subsets (OS) accelerable splitting-based algorithm, OS-LALM,for X-ray CT image reconstruction. To further accelerate the proposedalgorithm, we use a second-order recursive system analysis to design adeterministic downward continuation approach that avoids tedious parametertuning and provides fast convergence. Experimental results show that theproposed algorithm significantly accelerates the "convergence" of X-ray CTimage reconstruction with negligible overhead and greatly reduces the OSartifacts in the reconstructed image when using many subsets for OSacceleration.
arxiv-6000-48 | A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification | http://arxiv.org/abs/1402.4380 | author:Samuel Danso, Eric Atwell, Owen Johnson category:cs.CL published:2014-02-18 summary:A Verbal Autopsy is the record of an interview about the circumstances of anuncertified death. In developing countries, if a death occurs away from healthfacilities, a field-worker interviews a relative of the deceased about thecircumstances of the death; this Verbal Autopsy can be reviewed off-site. Wereport on a comparative study of the processes involved in Text Classificationapplied to classifying Cause of Death: feature value representation; machinelearning classification algorithms; and feature reduction strategies in orderto identify the suitable approaches applicable to the classification of VerbalAutopsy text. We demonstrate that normalised term frequency and the standardTFiDF achieve comparable performance across a number of classifiers. Theresults also show Support Vector Machine is superior to other classificationalgorithms employed in this research. Finally, we demonstrate the effectivenessof employing a "locally-semi-supervised" feature reduction strategy in order toincrease performance accuracy.
arxiv-6000-49 | Automatic Construction and Natural-Language Description of Nonparametric Regression Models | http://arxiv.org/abs/1402.4304 | author:James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani category:stat.ML cs.LG published:2014-02-18 summary:This paper presents the beginnings of an automatic statistician, focusing onregression problems. Our system explores an open-ended space of statisticalmodels to discover a good explanation of a data set, and then produces adetailed report with figures and natural-language text. Our approach treatsunknown regression functions nonparametrically using Gaussian processes, whichhas two important consequences. First, Gaussian processes can model functionsin terms of high-level properties (e.g. smoothness, trends, periodicity,changepoints). Taken together with the compositional structure of our languageof models this allows us to automatically describe functions in simple terms.Second, the use of flexible nonparametric models and a rich language forcomposing them in an open-ended manner also results in state-of-the-artextrapolation performance evaluated over 13 real time series data sets fromvarious domains.
arxiv-6000-50 | Learning the Irreducible Representations of Commutative Lie Groups | http://arxiv.org/abs/1402.4437 | author:Taco Cohen, Max Welling category:cs.LG published:2014-02-18 summary:We present a new probabilistic model of compact commutative Lie groups thatproduces invariant-equivariant and disentangled representations of data. Todefine the notion of disentangling, we borrow a fundamental principle fromphysics that is used to derive the elementary particles of a system from itssymmetries. Our model employs a newfound Bayesian conjugacy relation thatenables fully tractable probabilistic inference over compact commutative Liegroups -- a class that includes the groups that describe the rotation andcyclic translation of images. We train the model on pairs of transformed imagepatches, and show that the learned invariant representation is highly effectivefor classification.
arxiv-6000-51 | A convergence proof of the split Bregman method for regularized least-squares problems | http://arxiv.org/abs/1402.4371 | author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML published:2014-02-18 summary:The split Bregman (SB) method [T. Goldstein and S. Osher, SIAM J. ImagingSci., 2 (2009), pp. 323-43] is a fast splitting-based algorithm that solvesimage reconstruction problems with general l1, e.g., total-variation (TV) andcompressed sensing (CS), regularizations by introducing a single variable splitto decouple the data-fitting term and the regularization term, yielding simplesubproblems that are separable (or partially separable) and easy to minimize.Several convergence proofs have been proposed, and these proofs either impose a"full column rank" assumption to the split or assume exact updates in allsubproblems. However, these assumptions are impractical in many applicationssuch as the X-ray computed tomography (CT) image reconstructions, where theinner least-squares problem usually cannot be solved efficiently due to thehighly shift-variant Hessian. In this paper, we show that when the data-fittingterm is quadratic, the SB method is a convergent alternating direction methodof multipliers (ADMM), and a straightforward convergence proof with inexactupdates is given using [J. Eckstein and D. P. Bertsekas, MathematicalProgramming, 55 (1992), pp. 293-318, Theorem 8]. Furthermore, since the SBmethod is just a special case of an ADMM algorithm, it seems likely that theADMM algorithm will be faster than the SB method if the augmented Largangian(AL) penalty parameters are selected appropriately. To have a concrete example,we conduct a convergence rate analysis of the ADMM algorithm using two splitsfor image restoration problems with quadratic data-fitting term andregularization term. According to our analysis, we can show that the two-splitADMM algorithm can be faster than the SB method if the AL penalty parameter ofthe SB method is suboptimal. Numerical experiments were conducted to verify ouranalysis.
arxiv-6000-52 | Hybrid SRL with Optimization Modulo Theories | http://arxiv.org/abs/1402.4354 | author:Stefano Teso, Roberto Sebastiani, Andrea Passerini category:cs.LG stat.ML published:2014-02-18 summary:Generally speaking, the goal of constructive learning could be seen as, givenan example set of structured objects, to generate novel objects with similarproperties. From a statistical-relational learning (SRL) viewpoint, the taskcan be interpreted as a constraint satisfaction problem, i.e. the generatedobjects must obey a set of soft constraints, whose weights are estimated fromthe data. Traditional SRL approaches rely on (finite) First-Order Logic (FOL)as a description language, and on MAX-SAT solvers to perform inference. Alas,FOL is unsuited for con- structive problems where the objects contain a mixtureof Boolean and numerical variables. It is in fact difficult to implement, e.g.linear arithmetic constraints within the language of FOL. In this paper wepropose a novel class of hybrid SRL methods that rely on Satisfiability ModuloTheories, an alternative class of for- mal languages that allow to describe,and reason over, mixed Boolean-numerical objects and constraints. The resultingmethods, which we call Learning Mod- ulo Theories, are formulated within thestructured output SVM framework, and employ a weighted SMT solver as anoptimization oracle to perform efficient in- ference and discriminative maxmargin weight learning. We also present a few examples of constructive learningapplications enabled by our method.
arxiv-6000-53 | On the properties of $α$-unchaining single linkage hierarchical clustering | http://arxiv.org/abs/1402.4322 | author:A. Martínez-Pérez category:cs.LG 62H30, 68T10 published:2014-02-18 summary:In the election of a hierarchical clustering method, theoretic properties maygive some insight to determine which method is the most suitable to treat aclustering problem. Herein, we study some basic properties of two hierarchicalclustering methods: $\alpha$-unchaining single linkage or $SL(\alpha)$ and amodified version of this one, $SL^*(\alpha)$. We compare the results with theproperties satisfied by the classical linkage-based hierarchical clusteringmethods.
arxiv-6000-54 | The Random Forest Kernel and other kernels for big data from random partitions | http://arxiv.org/abs/1402.4293 | author:Alex Davies, Zoubin Ghahramani category:stat.ML cs.LG published:2014-02-18 summary:We present Random Partition Kernels, a new class of kernels derived bydemonstrating a natural connection between random partitions of objects andkernels between those objects. We show how the construction can be used tocreate kernels from methods that would not normally be viewed as randompartitions, such as Random Forest. To demonstrate the potential of this method,we propose two new kernels, the Random Forest Kernel and the Fast ClusterKernel, and show that these kernels consistently outperform standard kernels onproblems involving real-world datasets. Finally, we show how the form of thesekernels lend themselves to a natural approximation that is appropriate forcertain big data problems, allowing $O(N)$ inference in methods such asGaussian Processes, Support Vector Machines and Kernel PCA.
arxiv-6000-55 | When Learners Surpass their Sources: Mathematical Modeling of Learning from an Inconsistent Source | http://arxiv.org/abs/1402.4678 | author:Yelena Mandelshtam, Natalia Komarova category:cs.CL published:2014-02-18 summary:We present a new algorithm to model and investigate the learning process of alearner mastering a set of grammatical rules from an inconsistent source. Thecompelling interest of human language acquisition is that the learning succeedsin virtually every case, despite the fact that the input data are formallyinadequate to explain the success of learning. Our model explains how a learnercan successfully learn from or even surpass its imperfect source withoutpossessing any additional biases or constraints about the types of patternsthat exist in the language. We use the data collected by Singleton and Newport(2004) on the performance of a 7-year boy Simon, who mastered the American SignLanguage (ASL) by learning it from his parents, both of whom were imperfectspeakers of ASL. We show that the algorithm possesses a frequency-boostingproperty, whereby the frequency of the most common form of the source isincreased by the learner. We also explain several key features of Simon's ASL.
arxiv-6000-56 | Extracting Networks of Characters and Places from Written Works with CHAPLIN | http://arxiv.org/abs/1402.4259 | author:Roberto Marazzato, Amelia Carolina Sparavigna category:cs.CY cs.CL published:2014-02-18 summary:We are proposing a tool able to gather information on social networks fromnarrative texts. Its name is CHAPLIN, CHAracters and PLaces InteractionNetwork, implemented in VB.NET. Characters and places of the narrative worksare extracted in a list of raw words. Aided by the interface, the user selectsnames out of them. After this choice, the tool allows the user to enter someparameters, and, according to them, creates a network where the nodes are thecharacters and places, and the edges their interactions. Edges are labelled byperformances. The output is a GV file, written in the DOT graph scriptinglanguage, which is rendered by means of the free open source software Graphviz.
arxiv-6000-57 | Discretization of Temporal Data: A Survey | http://arxiv.org/abs/1402.4283 | author:P. Chaudhari, D. P. Rana, R. G. Mehta, N. J. Mistry, M. M. Raghuwanshi category:cs.DB cs.LG published:2014-02-18 summary:In real world, the huge amount of temporal data is to be processed in manyapplication areas such as scientific, financial, network monitoring, sensordata analysis. Data mining techniques are primarily oriented to handle discretefeatures. In the case of temporal data the time plays an important role on thecharacteristics of data. To consider this effect, the data discretizationtechniques have to consider the time while processing to resolve the issue byfinding the intervals of data which are more concise and precise with respectto time. Here, this research is reviewing different data discretizationtechniques used in temporal data applications according to the inclusion orexclusion of: class label, temporal order of the data and handling of streamdata to open the research direction for temporal data discretization to improvethe performance of data mining technique.
arxiv-6000-58 | Performance Limits of Dictionary Learning for Sparse Coding | http://arxiv.org/abs/1402.4078 | author:Alexander Jung, Yonina C. Eldar, Norbert Görtz category:stat.ML published:2014-02-17 summary:We consider the problem of dictionary learning under the assumption that theobserved signals can be represented as sparse linear combinations of thecolumns of a single large dictionary matrix. In particular, we analyze theminimax risk of the dictionary learning problem which governs the mean squarederror (MSE) performance of any learning scheme, regardless of its computationalcomplexity. By following an established information-theoretic method based onFanos inequality, we derive a lower bound on the minimax risk for a givendictionary learning problem. This lower bound yields a characterization of thesample-complexity, i.e., a lower bound on the required number of observationssuch that consistent dictionary learning schemes exist. Our bounds may becompared with the performance of a given learning scheme, allowing tocharacterize how far the method is from optimal performance.
arxiv-6000-59 | Selective Sampling with Drift | http://arxiv.org/abs/1402.4084 | author:Edward Moroshko, Koby Crammer category:cs.LG published:2014-02-17 summary:Recently there has been much work on selective sampling, an online activelearning setting, in which algorithms work in rounds. On each round analgorithm receives an input and makes a prediction. Then, it can decide whetherto query a label, and if so to update its model, otherwise the input isdiscarded. Most of this work is focused on the stationary case, where it isassumed that there is a fixed target model, and the performance of thealgorithm is compared to a fixed model. However, in many real-worldapplications, such as spam prediction, the best target function may drift overtime, or have shifts from time to time. We develop a novel selective samplingalgorithm for the drifting setting, analyze it under no assumptions on themechanism generating the sequence of instances, and derive new mistake boundsthat depend on the amount of drift in the problem. Simulations on synthetic andreal-world datasets demonstrate the superiority of our algorithms as aselective sampling algorithm in the drifting setting.
arxiv-6000-60 | FTVd is beyond Fast Total Variation regularized Deconvolution | http://arxiv.org/abs/1402.3869 | author:Yilun Wang category:cs.CV published:2014-02-17 summary:In this paper, we revisit the "FTVd" algorithm for Fast Total VariationRegularized Deconvolution, which has been widely used in the past few years.Both its original version implemented in the MATLAB software FTVd 3.0 and itsrelated variant implemented in the latter version FTVd 4.0 are considered\cite{Wang08FTVdsoftware}. We propose that the intermediate results during theiterations are the solutions of a series of combined Tikhonov and totalvariation regularized image deconvolution models and therefore some of themoften have even better image quality than the final solution, which iscorresponding to the pure total variation regularized model.
arxiv-6000-61 | Statistical Noise Analysis in SENSE Parallel MRI | http://arxiv.org/abs/1402.4067 | author:Santiago Aja-Fernandez, Gonzalo Vegas-Sanchez-Ferrero, Antonio Trsitan-Vega category:cs.CV published:2014-02-17 summary:A complete first and second order statistical characterization of noise inSENSE reconstructed data is proposed. SENSE acquisitions have usually beenmodeled as Rician distributed, since the data reconstruction takes place intothe spatial domain, where Gaussian noise is assumed. However, this model justholds for the first order statistics and obviates other effects induced bycoils correlations and the reconstruction interpolation. Those effects areproperly taken into account in this study, in order to fully justify a finalSENSE noise model. As a result, some interesting features of the reconstructedimage arise: (1) There is a strong correlation between adjacent lines. (2) Theresulting distribution is non-stationary and therefore the variance of noisewill vary from point to point across the image. Closed equations for thecalculation of the variance of noise and the correlation coefficient betweenlines are proposed. The proposed model is totally compatible with g-factorformulations.
arxiv-6000-62 | The Algebraic Approach to Phase Retrieval and Explicit Inversion at the Identifiability Threshold | http://arxiv.org/abs/1402.4053 | author:Franz J Király, Martin Ehler category:math.FA cs.CV cs.IT math.AG math.IT stat.ML published:2014-02-17 summary:We study phase retrieval from magnitude measurements of an unknown signal asan algebraic estimation problem. Indeed, phase retrieval from rank-one and moregeneral linear measurements can be treated in an algebraic way. It is verifiedthat a certain number of generic rank-one or generic linear measurements aresufficient to enable signal reconstruction for generic signals, and slightlymore generic measurements yield reconstructability for all signals. Our resultssolve a few open problems stated in the recent literature. Furthermore, we showhow the algebraic estimation problem can be solved by a closed-form algebraicestimation technique, termed ideal regression, providing non-asymptotic successguarantees.
arxiv-6000-63 | Is Spiking Logic the Route to Memristor-Based Computers? | http://arxiv.org/abs/1402.4036 | author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.ET cs.AR cs.NE 94C-06 C.1.3; B.3.1 published:2014-02-17 summary:Memristors have been suggested as a novel route to neuromorphic computingbased on the similarity between neurons (synapses and ion pumps) andmemristors. The D.C. action of the memristor is a current spike, which we thinkwill be fruitful for building memristor computers. In this paper, we introduce4 different logical assignations to implement sequential logic in the memristorand introduce the physical rules, summation, `bounce-back', directionality and`diminishing returns', elucidated from our investigations. We then demonstratehow memristor sequential logic works by instantiating a NOT gate, an AND gateand a Full Adder with a single memristor. The Full Adder makes use of thememristor's memory to add three binary values together and outputs the value,the carry digit and even the order they were input in.
arxiv-6000-64 | Connecting Spiking Neurons to a Spiking Memristor Network Changes the Memristor Dynamics | http://arxiv.org/abs/1402.4029 | author:Deborah Gater, Attya Iqbal, Jeffrey Davey, Ella Gale category:cs.ET cs.NE physics.bio-ph 92C-06, 94C-06 published:2014-02-17 summary:Memristors have been suggested as neuromorphic computing elements. Spike-timedependent plasticity and the Hodgkin-Huxley model of the neuron have both beenmodelled effectively by memristor theory. The d.c. response of the memristor isa current spike. Based on these three facts we suggest that memristors arewell-placed to interface directly with neurons. In this paper we show thatconnecting a spiking memristor network to spiking neuronal cells causes achange in the memristor network dynamics by: removing the memristor spikes,which we show is due to the effects of connection to aqueous medium; causing achange in current decay rate consistent with a change in memristor state;presenting more-linear $I-t$ dynamics; and increasing the memristor spikingrate, as a consequence of interaction with the spiking neurons. Thisdemonstrates that neurons are capable of communicating directly withmemristors, without the need for computer translation.
arxiv-6000-65 | Does the D.C. Response of Memristors Allow Robotic Short-Term Memory and a Possible Route to Artificial Time Perception? | http://arxiv.org/abs/1402.4007 | author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.RO cs.ET cs.NE 94Cxx published:2014-02-17 summary:Time perception is essential for task switching, and in the mammalian brainappears alongside other processes. Memristors are electronic components used assynapses and as models for neurons. The d.c. response of memristors can beconsidered as a type of short-term memory. Interactions of the memristor d.c.response within networks of memristors leads to the emergence of oscillatorydynamics and intermittent spike trains, which are similar to neural dynamics.Based on this data, the structure of a memristor network control for a robot asit undergoes task switching is discussed and it is suggested that theseemergent network dynamics could improve the performance of role switching andlearning in an artificial intelligence and perhaps create artificial timeperception.
arxiv-6000-66 | Stochastic Gradient Hamiltonian Monte Carlo | http://arxiv.org/abs/1402.4102 | author:Tianqi Chen, Emily B. Fox, Carlos Guestrin category:stat.ME cs.LG stat.ML published:2014-02-17 summary:Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism fordefining distant proposals with high acceptance probabilities in aMetropolis-Hastings framework, enabling more efficient exploration of the statespace than standard random-walk proposals. The popularity of such methods hasgrown significantly in recent years. However, a limitation of HMC methods isthe required gradient computation for simulation of the Hamiltonian dynamicalsystem-such computation is infeasible in problems involving a large sample sizeor streaming data. Instead, we must rely on a noisy gradient estimate computedfrom a subset of the data. In this paper, we explore the properties of such astochastic gradient HMC approach. Surprisingly, the natural implementation ofthe stochastic approximation can be arbitrarily bad. To address this problem weintroduce a variant that uses second-order Langevin dynamics with a frictionterm that counteracts the effects of the noisy gradient, maintaining thedesired target distribution as the invariant distribution. Results on simulateddata validate our theory. We also provide an application of our methods to aclassification task using neural networks and to online Bayesian matrixfactorization.
arxiv-6000-67 | Dimensionality reduction with subgaussian matrices: a unified theory | http://arxiv.org/abs/1402.3973 | author:Sjoerd Dirksen category:cs.IT cs.DS math.IT stat.ML published:2014-02-17 summary:We present a theory for Euclidean dimensionality reduction with subgaussianmatrices which unifies several restricted isometry property andJohnson-Lindenstrauss type results obtained earlier for specific data sets. Inparticular, we recover and, in several cases, improve results for sets ofsparse and structured sparse vectors, low-rank matrices and tensors, and smoothmanifolds. In addition, we establish a new Johnson-Lindenstrauss embedding fordata sets taking the form of an infinite union of subspaces of a Hilbert space.
arxiv-6000-68 | Application of the Ring Theory in the Segmentation of Digital Images | http://arxiv.org/abs/1402.4069 | author:Yasel Garcés, Esley Torres, Osvaldo Pereira, Roberto Rodríguez category:cs.CV published:2014-02-17 summary:Ring theory is one of the branches of the abstract algebra that has beenbroadly used in images. However, ring theory has not been very related withimage segmentation. In this paper, we propose a new index of similarity amongimages using Zn rings and the entropy function. This new index was applied as anew stopping criterion to the Mean Shift Iterative Algorithm with the goal toreach a better segmentation. An analysis on the performance of the algorithmwith this new stopping criterion is carried out. The obtained results provedthat the new index is a suitable tool to compare images.
arxiv-6000-69 | Weyl group orbit functions in image processing | http://arxiv.org/abs/1404.0566 | author:Goce Chadzitaskos, Lenka Háková, Ondřej Kajínek category:cs.CV published:2014-02-17 summary:We deal with the Fourier-like analysis of functions on discrete grids intwo-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. Forthese cases we present the convolution theorem. We provide an example ofapplication of image processing using the $C-$ functions and the convolutionsfor spatial filtering of the treated image.
arxiv-6000-70 | Sparse Coding Approach for Multi-Frame Image Super Resolution | http://arxiv.org/abs/1402.3926 | author:Toshiyuki Kato, Hideitsu Hino, Noboru Murata category:cs.CV published:2014-02-17 summary:An image super-resolution method from multiple observation of low-resolutionimages is proposed. The method is based on sub-pixel accuracy block matchingfor estimating relative displacements of observed images, and sparse signalrepresentation for estimating the corresponding high-resolution image. Relativedisplacements of small patches of observed low-resolution images are accuratelyestimated by a computationally efficient block matching method. Since theestimated displacements are also regarded as a warping component of imagedegradation process, the matching results are directly utilized to generatelow-resolution dictionary for sparse image representation. The matching scoresof the block matching are used to select a subset of low-resolution patches forreconstructing a high-resolution patch, that is, an adaptive selection ofinformative low-resolution images is realized. When there is only onelow-resolution image, the proposed method works as a single-framesuper-resolution method. The proposed method is shown to perform comparable orsuperior to conventional single- and multi-frame super-resolution methodsthrough experiments using various real-world datasets.
arxiv-6000-71 | Performance Evaluation of Machine Learning Classifiers in Sentiment Mining | http://arxiv.org/abs/1402.3891 | author:Vinodhini G Chandrasekaran RM category:cs.LG cs.CL cs.IR published:2014-02-17 summary:In recent years, the use of machine learning classifiers is of great value insolving a variety of problems in text classification. Sentiment mining is akind of text classification in which, messages are classified according tosentiment orientation such as positive or negative. This paper extends the ideaof evaluating the performance of various classifiers to show theireffectiveness in sentiment mining of online product reviews. The productreviews are collected from Amazon reviews. To evaluate the performance ofclassifiers various evaluation methods like random sampling, linear samplingand bootstrap sampling are used. Our results shows that support vector machinewith bootstrap sampling method outperforms others classifiers and samplingmethods in terms of misclassification rate.
arxiv-6000-72 | Sparse Polynomial Learning and Graph Sketching | http://arxiv.org/abs/1402.3902 | author:Murat Kocaoglu, Karthikeyan Shanmugam, Alexandros G. Dimakis, Adam Klivans category:cs.LG published:2014-02-17 summary:Let $f:\{-1,1\}^n$ be a polynomial with at most $s$ non-zero realcoefficients. We give an algorithm for exactly reconstructing f given randomexamples from the uniform distribution on $\{-1,1\}^n$ that runs in timepolynomial in $n$ and $2s$ and succeeds if the function satisfies the uniquesign property: there is one output value which corresponds to a unique set ofvalues of the participating parities. This sufficient condition is satisfiedwhen every coefficient of f is perturbed by a small random noise, or satisfiedwith high probability when s parity functions are chosen randomly or when allthe coefficients are positive. Learning sparse polynomials over the Booleandomain in time polynomial in $n$ and $2s$ is considered notoriously hard in theworst-case. Our result shows that the problem is tractable for almost allsparse polynomials. Then, we show an application of this result to hypergraphsketching which is the problem of learning a sparse (both in the number ofhyperedges and the size of the hyperedges) hypergraph from uniformly drawnrandom cuts. We also provide experimental results on a real world dataset.
arxiv-6000-73 | Dropout Rademacher Complexity of Deep Neural Networks | http://arxiv.org/abs/1402.3811 | author:Wei Gao, Zhi-Hua Zhou category:cs.NE stat.ML published:2014-02-16 summary:Great successes of deep neural networks have been witnessed in various realapplications. Many algorithmic and implementation techniques have beendeveloped, however, theoretical understanding of many aspects of deep neuralnetworks is far from clear. A particular interesting issue is the usefulness ofdropout, which was motivated from the intuition of preventing complexco-adaptation of feature detectors. In this paper, we study the Rademachercomplexity of different types of dropout, and our theoretical results disclosethat for shallow neural networks (with one or none hidden layer) dropout isable to reduce the Rademacher complexity in polynomial, whereas for deep neuralnetworks it can amazingly lead to an exponential reduction of the Rademachercomplexity.
arxiv-6000-74 | Scalable Kernel Clustering: Approximate Kernel k-means | http://arxiv.org/abs/1402.3849 | author:Radha Chitta, Rong Jin, Timothy C. Havens, Anil K. Jain category:cs.CV cs.DS cs.LG published:2014-02-16 summary:Kernel-based clustering algorithms have the ability to capture the non-linearstructure in real world data. Among various kernel-based clustering algorithms,kernel k-means has gained popularity due to its simple iterative nature andease of implementation. However, its run-time complexity and memory footprintincrease quadratically in terms of the size of the data set, and hence, largedata sets cannot be clustered efficiently. In this paper, we propose anapproximation scheme based on randomization, called the Approximate Kernelk-means. We approximate the cluster centers using the kernel similarity betweena few sampled points and all the points in the data set. We show that theproposed method achieves better clustering performance than the traditional lowrank kernel approximation based clustering schemes. We also demonstrate thatits running time and memory requirements are significantly lower than those ofkernel k-means, with only a small reduction in the clustering quality onseveral public domain large data sets. We then employ ensemble clusteringtechniques to further enhance the performance of our algorithm.
arxiv-6000-75 | Auto Spell Suggestion for High Quality Speech Synthesis in Hindi | http://arxiv.org/abs/1402.3648 | author:Shikha Kabra, Ritika Agarwal category:cs.CL cs.SD published:2014-02-15 summary:The goal of Text-to-Speech (TTS) synthesis in a particular language is toconvert arbitrary input text to intelligible and natural sounding speech.However, for a particular language like Hindi, which is a highly confusinglanguage (due to very close spellings), it is not an easy task to identifyerrors/mistakes in input text and an incorrect text degrade the quality ofoutput speech hence this paper is a contribution to the development of highquality speech synthesis with the involvement of Spellchecker which generatesspell suggestions for misspelled words automatically. Involvement ofspellchecker would increase the efficiency of speech synthesis by providingspell suggestions for incorrect input text. Furthermore, we have provided thecomparative study for evaluating the resultant effect on to phonetic text byadding spellchecker on to input text.
arxiv-6000-76 | Privately Solving Linear Programs | http://arxiv.org/abs/1402.3631 | author:Justin Hsu, Aaron Roth, Tim Roughgarden, Jonathan Ullman category:cs.DS cs.CR cs.LG published:2014-02-15 summary:In this paper, we initiate the systematic study of solving linear programsunder differential privacy. The first step is simply to define the problem: tothis end, we introduce several natural classes of private linear programs thatcapture different ways sensitive data can be incorporated into a linearprogram. For each class of linear programs we give an efficient, differentiallyprivate solver based on the multiplicative weights framework, or we give animpossibility result.
arxiv-6000-77 | A Narrative Vehicle Protection Representation for Vehicle Speed Regulator Under Driver Exhaustion -- A Study | http://arxiv.org/abs/1402.3657 | author:V. Karthikeyan, B. Praveen Kumar, S. Suresh Babu, R. Purusothaman, shijin Thomas category:cs.CV cs.HC published:2014-02-15 summary:Driver fatigue is one of the important factors that cause traffic accidents,and the ever-increasing number due to diminished drivers vigilance level hasbecome a problem of serious concern to society. Drivers with a diminishedvigilance level suffer from a marked decline in their abilities of perception,recognition, and vehicle control, and therefore pose serious danger to theirown life and the lives of other people. Exhaustion resulting from sleepdeprivation or sleep disorders is an important factor in the creasing number ofaccidents. In this projected work, we discuss the various methods of theexisting and the proposed method based on a real time online safety prototypethat controls the vehicle speed under driver fatigue. The purpose of such amodel is to advance a system to detect fatigue symptoms in drivers and controlthe speed of vehicle to avoid accidents. This system was tested adequately withsubjects of different technology of various researchers finally the validity ofthe proposed model for vehicle speed controller based on driver fatiguedetection is shown.
arxiv-6000-78 | word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method | http://arxiv.org/abs/1402.3722 | author:Yoav Goldberg, Omer Levy category:cs.CL cs.LG stat.ML published:2014-02-15 summary:The word2vec software of Tomas Mikolov and colleagues(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, andprovides state-of-the-art word embeddings. The learning models behind thesoftware are described in two research papers. We found the description of themodels in these papers to be somewhat cryptic and hard to follow. While themotivations and presentation may be obvious to the neural-networkslanguage-modeling crowd, we had to struggle quite a bit to figure out therationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in"Distributed Representations of Words and Phrases and their Compositionality"by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.
arxiv-6000-79 | Bayesian Inference for NMR Spectroscopy with Applications to Chemical Quantification | http://arxiv.org/abs/1402.3580 | author:Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin, Mick D. Mantle, Lynn F. Gladden, Andrew Blake category:stat.AP stat.ME stat.ML published:2014-02-14 summary:Nuclear magnetic resonance (NMR) spectroscopy exploits the magneticproperties of atomic nuclei to discover the structure, reaction state andchemical environment of molecules. We propose a probabilistic generative modeland inference procedures for NMR spectroscopy. Specifically, we use a weightedsum of trigonometric functions undergoing exponential decay to model freeinduction decay (FID) signals. We discuss the challenges in estimating thecomponents of this general model -- amplitudes, phase shifts, frequencies,decay rates, and noise variances -- and offer practical solutions. We comparewith conventional Fourier transform spectroscopy for estimating the relativeconcentrations of chemicals in a mixture, using synthetic and experimentallyacquired FID signals. We find the proposed model is particularly robust to lowsignal to noise ratios (SNR), and overlapping peaks in the Fourier transform ofthe FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) whichare not possible with conventional spectroscopy (5% sensitivity).
arxiv-6000-80 | Improving Streaming Video Segmentation with Early and Mid-Level Visual Processing | http://arxiv.org/abs/1402.3557 | author:Subarna Tripathi, Youngbae Hwang, Serge Belongie, Truong Nguyen category:cs.CV published:2014-02-14 summary:Despite recent advances in video segmentation, many opportunities remain toimprove it using a variety of low and mid-level visual cues. We proposeimprovements to the leading streaming graph-based hierarchical videosegmentation (streamGBH) method based on early and mid level visual processing.The extensive experimental analysis of our approach validates the improvementof hierarchical supervoxel representation by incorporating motion and colorwith effective filtering. We also pose and illuminate some open questionstowards intermediate level video analysis as further extension to streamGBH. Weexploit the supervoxels as an initialization towards estimation of dominantaffine motion regions, followed by merging of such motion regions in order tohierarchically segment a video in a novel motion-segmentation framework whichaims at subsequent applications such as foreground recognition.
arxiv-6000-81 | A Clockwork RNN | http://arxiv.org/abs/1402.3511 | author:Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber category:cs.NE cs.LG published:2014-02-14 summary:Sequence prediction and classification are ubiquitous and challengingproblems in machine learning that can require identifying complex dependenciesbetween temporally distant inputs. Recurrent Neural Networks (RNNs) have theability, in theory, to cope with these temporal dependencies by virtue of theshort-term memory implemented by their recurrent (feedback) connections.However, in practice they are difficult to train successfully when thelong-term memory is required. This paper introduces a simple, yet powerfulmodification to the standard RNN architecture, the Clockwork RNN (CW-RNN), inwhich the hidden layer is partitioned into separate modules, each processinginputs at its own temporal granularity, making computations only at itsprescribed clock rate. Rather than making the standard RNN models more complex,CW-RNN reduces the number of RNN parameters, improves the performancesignificantly in the tasks tested, and speeds up the network evaluation. Thenetwork is demonstrated in preliminary experiments involving two tasks: audiosignal generation and TIMIT spoken word classification, where it outperformsboth RNN and LSTM networks.
arxiv-6000-82 | Automated Fabric Defect Inspection: A Survey of Classifiers | http://arxiv.org/abs/1405.6177 | author:Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman, Farruk Ahmed category:cs.CV cs.LG published:2014-02-14 summary:Quality control at each stage of production in textile industry has become akey factor to retaining the existence in the highly competitive global market.Problems of manual fabric defect inspection are lack of accuracy and high timeconsumption, where early and accurate fabric defect detection is a significantphase of quality control. Computer vision based, i.e. automated fabric defectinspection systems are thought by many researchers of different countries to bevery useful to resolve these problems. There are two major challenges to beresolved to attain a successful automated fabric defect inspection system. Theyare defect detection and defect classification. In this work, we discussdifferent techniques used for automated fabric defect classification, then showa survey of classifiers used in automated fabric defect inspection systems, andfinally, compare these classifiers by using performance metrics. This work isexpected to be very useful for the researchers in the area of automated fabricdefect inspection to understand and evaluate the many potential options in thisfield.
arxiv-6000-83 | An evaluative baseline for geo-semantic relatedness and similarity | http://arxiv.org/abs/1402.3371 | author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.CL published:2014-02-14 summary:In geographic information science and semantics, the computation of semanticsimilarity is widely recognised as key to supporting a vast number of tasks ininformation integration and retrieval. By contrast, the role of geo-semanticrelatedness has been largely ignored. In natural language processing, semanticrelatedness is often confused with the more specific semantic similarity. Inthis article, we discuss a notion of geo-semantic relatedness based on Lehrer'ssemantic fields, and we compare it with geo-semantic similarity. We thendescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), anew open dataset designed to evaluate computational measures of geo-semanticrelatedness and similarity. This dataset is larger than existing datasets ofthis kind, and includes 97 geographic terms combined into 50 term pairs ratedby 203 human subjects. GeReSiD is available online and can be used as anevaluation baseline to determine empirically to what degree a givencomputational model approximates geo-semantic relatedness and similarity.
arxiv-6000-84 | Authorship Analysis based on Data Compression | http://arxiv.org/abs/1402.3405 | author:Daniele Cerra, Mihai Datcu, Peter Reinartz category:cs.CL cs.DL cs.IR stat.ML published:2014-02-14 summary:This paper proposes to perform authorship analysis using the Fast CompressionDistance (FCD), a similarity measure based on compression with dictionariesdirectly extracted from the written texts. The FCD computes a similaritybetween two documents through an effective binary search on the intersectionset between the two related dictionaries. In the reported experiments theproposed method is applied to documents which are heterogeneous in style,written in five different languages and coming from different historicalperiods. Results are comparable to the state of the art and outperformtraditional compression-based methods.
arxiv-6000-85 | Intrinsically Motivated Learning of Visual Motion Perception and Smooth Pursuit | http://arxiv.org/abs/1402.3344 | author:Chong Zhang, Yu Zhao, Jochen Triesch, Bertram E. Shi category:cs.CV q-bio.NC published:2014-02-14 summary:We extend the framework of efficient coding, which has been used to model thedevelopment of sensory processing in isolation, to model the development of theperception/action cycle. Our extension combines sparse coding and reinforcementlearning so that sensory processing and behavior co-develop to optimize ashared intrinsic motivational signal: the fidelity of the neural encoding ofthe sensory input under resource constraints. Applying this framework to amodel system consisting of an active eye behaving in a time varyingenvironment, we find that this generic principle leads to the simultaneousdevelopment of both smooth pursuit behavior and model neurons whose propertiesare similar to those of primary visual cortical neurons selective for differentdirections of visual motion. We suggest that this general principle may formthe basis for a unified and integrated explanation of many perception/actionloops.
arxiv-6000-86 | Machine Learning of Phonologically Conditioned Noun Declensions For Tamil Morphological Generators | http://arxiv.org/abs/1402.3382 | author:K. Rajan, Dr. V. Ramalingam, Dr. M. Ganesan category:cs.CL published:2014-02-14 summary:This paper presents machine learning solutions to a practical problem ofNatural Language Generation (NLG), particularly the word formation inagglutinative languages like Tamil, in a supervised manner. The morphologicalgenerator is an important component of Natural Language Processing inArtificial Intelligence. It generates word forms given a root and affixes. Themorphophonemic changes like addition, deletion, alternation etc., occur whentwo or more morphemes or words joined together. The Sandhi rules should beexplicitly specified in the rule based morphological analyzers and generators.In machine learning framework, these rules can be learned automatically by thesystem from the training samples and subsequently be applied for new inputs. Inthis paper we proposed the machine learning models which learn themorphophonemic rules for noun declensions from the given training data. Thesemodels are trained to learn sandhi rules using various learning algorithms andthe performance of those algorithms are presented. From this we conclude thatmachine learning of morphological processing such as word form generation canbe successfully learned in a supervised manner, without explicit description ofrules. The performance of Decision trees and Bayesian machine learningalgorithms on noun declensions are discussed.
arxiv-6000-87 | Maximum Entropy Discrimination Denoising Autoencoders | http://arxiv.org/abs/1402.3427 | author:Sotirios P. Chatzis category:cs.LG published:2014-02-14 summary:Denoising autoencoders (DAs) are typically applied to relatively largedatasets for unsupervised learning of representative data encodings, they relyon the idea of making the learned representations robust to partial corruptionof the input pattern, and perform learning using stochastic gradient descentwith relatively large datasets. In this paper, we present a fully Bayesian DAarchitecture that allows for the application of DAs even when data is scarce.Our novel approach formulates the signal encoding problem under a nonparametricBayesian regard, considering a Gaussian process prior over the latent inputencodings generated given the (corrupt) input observations. Subsequently, thedecoder modules of our model are formulated as large-margin regression models,treated under the Bayesian inference paradigm, by exploiting the maximumentropy discrimination (MED) framework. We exhibit the effectiveness of ourapproach using several datasets, dealing with both classification and transferlearning applications.
arxiv-6000-88 | Geometry and Expressive Power of Conditional Restricted Boltzmann Machines | http://arxiv.org/abs/1402.3346 | author:Guido Montufar, Nihat Ay, Keyan Ghazi-Zahedi category:cs.NE cs.LG stat.ML published:2014-02-14 summary:Conditional restricted Boltzmann machines are undirected stochastic neuralnetworks with a layer of input and output units connected bipartitely to alayer of hidden units. These networks define models of conditional probabilitydistributions on the states of the output units given the states of the inputunits, parametrized by interaction weights and biases. We address therepresentational power of these models, proving results their ability torepresent conditional Markov random fields and conditional distributions withrestricted supports, the minimal size of universal approximators, the maximalmodel approximation errors, and on the dimension of the set of representableconditional distributions. We contribute new tools for investigatingconditional probability models, which allow us to improve the results that canbe derived from existing work on restricted Boltzmann machine probabilitymodels.
arxiv-6000-89 | Event Structure of Transitive Verb: A MARVS perspective | http://arxiv.org/abs/1402.3040 | author:Jia-Fei Hong, Kathleen Ahrens, Chu-Ren Huang category:cs.CL published:2014-02-13 summary:Module-Attribute Representation of Verbal Semantics (MARVS) is a theory ofthe representation of verbal semantics that is based on Mandarin Chinese data(Huang et al. 2000). In the MARVS theory, there are two different types ofmodules: Event Structure Modules and Role Modules. There are also two sets ofattributes: Event-Internal Attributes and Role-Internal Attributes, which arelinked to the Event Structure Module and the Role Module, respectively. In thisstudy, we focus on four transitive verbs as chi1(eat), wan2(play),huan4(change) and shao1(burn) and explore their event structures by the MARVStheory.
arxiv-6000-90 | Regularization for Multiple Kernel Learning via Sum-Product Networks | http://arxiv.org/abs/1402.3032 | author:Ziming Zhang category:stat.ML cs.LG published:2014-02-13 summary:In this paper, we are interested in constructing general graph-basedregularizers for multiple kernel learning (MKL) given a structure which is usedto describe the way of combining basis kernels. Such structures are representedby sum-product networks (SPNs) in our method. Accordingly we propose a newconvex regularization method for MLK based on a path-dependent kernel weightingfunction which encodes the entire SPN structure in our method. Under certainconditions and from the view of probability, this function can be considered tofollow multinomial distributions over the weights associated with product nodesin SPNs. We also analyze the convexity of our regularizer and the complexity ofour induced classifiers, and further propose an efficient wrapper algorithm tooptimize our formulation. In our experiments, we apply our method to ......
arxiv-6000-91 | Squeezing bottlenecks: exploring the limits of autoencoder semantic representation capabilities | http://arxiv.org/abs/1402.3070 | author:Parth Gupta, Rafael E. Banchs, Paolo Rosso category:cs.IR cs.LG stat.ML published:2014-02-13 summary:We present a comprehensive study on the use of autoencoders for modellingtext data, in which (differently from previous studies) we focus our attentionon the following issues: i) we explore the suitability of two different modelsbDA and rsDA for constructing deep autoencoders for text data at the sentencelevel; ii) we propose and evaluate two novel metrics for better assessing thetext-reconstruction capabilities of autoencoders; and iii) we propose anautomatic method to find the critical bottleneck dimensionality for textlanguage representations (below which structural information is lost).
arxiv-6000-92 | Software Requirement Specification Using Reverse Speech Technology | http://arxiv.org/abs/1402.3080 | author:Santhy Viswam, Sajeer Karattil category:cs.CL cs.SD published:2014-02-13 summary:Speech analysis had been taken to a new level with the discovery of ReverseSpeech (RS). RS is the discovery of hidden messages, referred as reversals, innormal speech. Works are in progress for exploiting the relevance of RS indifferent real world applications such as investigation, medical field etc. Inthis paper we represent an innovative method for preparing a reliable SoftwareRequirement Specification (SRS) document with the help of reverse speech. AsSRS act as the backbone for the successful completion of any project, areliable method is needed to overcome the inconsistencies. Using RS such areliable method for SRS documentation was developed.
arxiv-6000-93 | Zero-bias autoencoders and the benefits of co-adapting features | http://arxiv.org/abs/1402.3337 | author:Kishore Konda, Roland Memisevic, David Krueger category:stat.ML cs.CV cs.LG cs.NE published:2014-02-13 summary:Regularized training of an autoencoder typically results in hidden unitbiases that take on large negative values. We show that negative biases are anatural result of using a hidden layer whose responsibility is to bothrepresent the input data and act as a selection mechanism that ensures sparsityof the representation. We then show that negative biases impede the learning ofdata distributions whose intrinsic dimensionality is high. We also propose anew activation function that decouples the two roles of the hidden layer andthat allows us to learn representations on data with very high intrinsicdimensionality, where standard autoencoders typically fail. Since the decoupledactivation function acts like an implicit regularizer, the model can be trainedby minimizing the reconstruction error of training data, without requiring anyadditional regularization.
arxiv-6000-94 | Gaussian Process Volatility Model | http://arxiv.org/abs/1402.3085 | author:Yue Wu, Jose Miguel Hernandez Lobato, Zoubin Ghahramani category:stat.ME stat.ML 62P05 published:2014-02-13 summary:The accurate prediction of time-changing variances is an important task inthe modeling of financial data. Standard econometric models are often limitedas they assume rigid functional relationships for the variances. Moreover,function parameters are usually learned using maximum likelihood, which canlead to overfitting. To address these problems we introduce a novel model fortime-changing variances using Gaussian Processes. A Gaussian Process (GP)defines a distribution over functions, which allows us to capture highlyflexible functional relationships for the variances. In addition, we develop anonline algorithm to perform inference. The algorithm has two main advantages.First, it takes a Bayesian approach, thereby avoiding overfitting. Second, itis much quicker than current offline inference procedures. Finally, our newmodel was evaluated on financial data and showed significant improvement inpredictive performance over current standard models.
arxiv-6000-95 | Hand-Eye and Robot-World Calibration by Global Polynomial Optimization | http://arxiv.org/abs/1402.3261 | author:Jan Heller, Didier Henrion, Tomas Pajdla category:cs.CV math.OC published:2014-02-13 summary:The need to relate measurements made by a camera to a different knowncoordinate system arises in many engineering applications. Historically, itappeared for the first time in the connection with cameras mounted on roboticsystems. This problem is commonly known as hand-eye calibration. In this paper,we present several formulations of hand-eye calibration that lead tomultivariate polynomial optimization problems. We show that the method ofconvex linear matrix inequality (LMI) relaxations can be used to effectivelysolve these problems and to obtain globally optimal solutions. Further, we showthat the same approach can be used for the simultaneous hand-eye androbot-world calibration. Finally, we validate the proposed solutions using bothsynthetic and real datasets.
arxiv-6000-96 | A Robust Ensemble Approach to Learn From Positive and Unlabeled Data Using SVM Base Models | http://arxiv.org/abs/1402.3144 | author:Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor category:stat.ML cs.LG published:2014-02-13 summary:We present a novel approach to learn binary classifiers when only positiveand unlabeled instances are available (PU learning). This problem is routinelycast as a supervised task with label noise in the negative set. We use anensemble of SVM models trained on bootstrap resamples of the training data forincreased robustness against label noise. The approach can be considered in abagging framework which provides an intuitive explanation for its mechanics ina semi-supervised setting. We compared our method to state-of-the-artapproaches in simulations using multiple public benchmark data sets. Theincluded benchmark comprises three settings with increasing label noise: (i)fully supervised, (ii) PU learning and (iii) PU learning with false positives.Our approach shows a marginal improvement over existing methods in the secondsetting and a significant improvement in the third.
arxiv-6000-97 | Local Optima Networks: A New Model of Combinatorial Fitness Landscapes | http://arxiv.org/abs/1402.2959 | author:Gabriela Ochoa, Sébastien Verel, Fabio Daolio, Marco Tomassini category:cs.NE cs.AI published:2014-02-12 summary:This chapter overviews a recently introduced network-based model ofcombinatorial landscapes: Local Optima Networks (LON). The model compresses theinformation given by the whole search space into a smaller mathematical objectthat is a graph having as vertices the local optima and as edges the possibleweighted transitions between them. Two definitions of edges have been proposed:basin-transition and escape-edges, which capture relevant topological featuresof the underlying search spaces. This network model brings a new set of metricsto characterize the structure of combinatorial landscapes, those associatedwith the science of complex networks. These metrics are described, and resultsare presented of local optima network extraction and analysis for two selectedcombinatorial landscapes: NK landscapes and the quadratic assignment problem.Network features are found to correlate with and even predict the performanceof heuristic search algorithms operating on these problems.
arxiv-6000-98 | Nonparametric Estimation of Renyi Divergence and Friends | http://arxiv.org/abs/1402.2966 | author:Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman category:stat.ML math.ST stat.TH published:2014-02-12 summary:We consider nonparametric estimation of $L_2$, Renyi-$\alpha$ andTsallis-$\alpha$ divergences between continuous distributions. Our approach isto construct estimators for particular integral functionals of two densitiesand translate them into divergence estimators. For the integral functionals,our estimators are based on corrections of a preliminary plug-in estimator. Weshow that these estimators achieve the parametric convergence rate of$n^{-1/2}$ when the densities' smoothness, $s$, are both at least $d/4$ where$d$ is the dimension. We also derive minimax lower bounds for this problemwhich confirm that $s > d/4$ is necessary to achieve the $n^{-1/2}$ rate ofconvergence. We validate our theoretical guarantees with a number ofsimulations.
arxiv-6000-99 | PR2: A Language Independent Unsupervised Tool for Personality Recognition from Text | http://arxiv.org/abs/1402.2796 | author:Fabio Celli, Massimo Poesio category:cs.CL published:2014-02-12 summary:We present PR2, a personality recognition system available online, thatperforms instance-based classification of Big5 personality types fromunstructured text, using language-independent features. It has been tested onEnglish and Italian, achieving performances up to f=.68.
arxiv-6000-100 | Sparse Estimation From Noisy Observations of an Overdetermined Linear System | http://arxiv.org/abs/1402.2864 | author:Liang Dai, Kristiaan Pelckmans category:cs.SY stat.ML published:2014-02-12 summary:This note studies a method for the efficient estimation of a finite number ofunknown parameters from linear equations, which are perturbed by Gaussiannoise. In case the unknown parameters have only few nonzero entries, the proposedestimator performs more efficiently than a traditional approach. The method consists of three steps: (1) a classical Least Squares Estimate (LSE), (2) the support is recovered through a Linear Programming (LP) optimizationproblem which can be computed using a soft-thresholding step, (3) a de-biasing step using a LSE on the estimated support set. The main contribution of this note is a formal derivation of an associatedORACLE property of the final estimate. That is, when the number of samples is large enough, the estimate is shown toequal the LSE based on the support of the {\em true} parameters.
arxiv-6000-101 | Noise Analysis for Lensless Compressive Imaging | http://arxiv.org/abs/1402.2720 | author:Hong Jiang, Gang Huang, Paul Wilford category:cs.CV published:2014-02-12 summary:We analyze the signal to noise ratio (SNR) in a recently proposed lenslesscompressive imaging architecture. The architecture consists of a sensor of asingle detector element and an aperture assembly of an array of apertureelements, each of which has a programmable transmittance. This lenslesscompressive imaging architecture can be used in conjunction with compressivesensing to capture images in a compressed form of compressive measurements. Inthis paper, we perform noise analysis of this lensless compressive imagingarchitecture and compare it with pinhole aperture imaging and lens apertureimaging. We will show that the SNR in the lensless compressive imaging isindependent of the image resolution, while that in either pinhole apertureimaging or lens aperture imaging decreases as the image resolution increases.Consequently, the SNR in the lensless compressive imaging can be much higher ifthe image resolution is large enough.
arxiv-6000-102 | Sex as Gibbs Sampling: a probability model of evolution | http://arxiv.org/abs/1402.2704 | author:Chris Watkins, Yvonne Buttkewitz category:q-bio.PE cs.NE published:2014-02-12 summary:We show that evolutionary computation can be implemented as standardMarkov-chain Monte-Carlo (MCMC) sampling. With some care, `genetic algorithms'can be constructed that are reversible Markov chains that satisfy detailedbalance; it follows that the stationary distribution of populations is a Gibbsdistribution in a simple factorised form. For some standard and popularnonparametric probability models, we exhibit Gibbs-sampling procedures that areplausible genetic algorithms. At mutation-selection equilibrium, a populationof genomes is analogous to a sample from a Bayesian posterior, and the genomesare analogous to latent variables. We suggest this is a general, tractable, andinsightful formulation of evolutionary computation in terms of standard machinelearning concepts and techniques. In addition, we show that evolutionary processes in which selection acts bydifferences in fecundity are not reversible, and also that it is not possibleto construct reversible evolutionary models in which each child is produced byonly two parents.
arxiv-6000-103 | Ranking via Robust Binary Classification and Parallel Parameter Estimation in Large-Scale Data | http://arxiv.org/abs/1402.2676 | author:Hyokun Yun, Parameswaran Raman, S. V. N. Vishwanathan category:stat.ML cs.DC cs.LG stat.CO published:2014-02-11 summary:We propose RoBiRank, a ranking algorithm that is motivated by observing aclose connection between evaluation metrics for learning to rank and lossfunctions for robust classification. The algorithm shows a very competitiveperformance on standard benchmark datasets against other representativealgorithms in the literature. On the other hand, in large scale problems whereexplicit feature vectors and scores are not given, our algorithm can beefficiently parallelized across a large number of machines; for a task thatrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,our algorithm finds solutions that are of dramatically higher quality than thatcan be found by a state-of-the-art competitor algorithm, given the same amountof wall-clock time for computation.
arxiv-6000-104 | Learning-assisted Theorem Proving with Millions of Lemmas | http://arxiv.org/abs/1402.3578 | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO published:2014-02-11 summary:Large formal mathematical libraries consist of millions of atomic inferencesteps that give rise to a corresponding number of proved statements (lemmas).Analogously to the informal mathematical practice, only a tiny fraction of suchstatements is named and re-used in later proofs by formal mathematicians. Inthis work, we suggest and implement criteria defining the estimated usefulnessof the HOL Light lemmas for proving further theorems. We use these criteria tomine the large inference graph of the lemmas in the HOL Light and Flyspecklibraries, adding up to millions of the best lemmas to the pool of statementsthat can be re-used in later proofs. We show that in combination withlearning-based relevance filtering, such methods significantly strengthenautomated theorem proving of new conjectures over large formal mathematicallibraries such as Flyspeck.
arxiv-6000-105 | Real-Time Hand Shape Classification | http://arxiv.org/abs/1402.2673 | author:Jakub Nalepa, Michal Kawulok category:cs.CV published:2014-02-11 summary:The problem of hand shape classification is challenging since a hand ischaracterized by a large number of degrees of freedom. Numerous shapedescriptors have been proposed and applied over the years to estimate andclassify hand poses in reasonable time. In this paper we discuss our parallelframework for real-time hand shape classification applicable in real-timeapplications. We show how the number of gallery images influences theclassification accuracy and execution time of the parallel algorithm. Wepresent the speedup and efficiency analyses that prove the efficacy of theparallel implementation. Noteworthy, different methods can be used at each stepof our parallel framework. Here, we combine the shape contexts with theappearance-based techniques to enhance the robustness of the algorithm and toincrease the classification score. An extensive experimental study proves thesuperiority of the proposed approach over existing state-of-the-art methods.
arxiv-6000-106 | An evaluation of keyword extraction from online communication for the characterisation of social relations | http://arxiv.org/abs/1402.2427 | author:Jan Hauffa, Tobias Lichtenberg, Georg Groh category:cs.SI cs.CL cs.IR published:2014-02-11 summary:The set of interpersonal relationships on a social network service or asimilar online community is usually highly heterogenous. The concept of tiestrength captures only one aspect of this heterogeneity. Since the unstructuredtext content of online communication artefacts is a salient source ofinformation about a social relationship, we investigate the utility of keywordsextracted from the message body as a representation of the relationship'scharacteristics as reflected by the conversation topics. Keyword extraction isperformed using standard natural language processing methods. Communicationdata and human assessments of the extracted keywords are obtained from Facebookusers via a custom application. The overall positive quality assessmentprovides evidence that the keywords indeed convey relevant information aboutthe relationship.
arxiv-6000-107 | Animation of 3D Human Model Using Markerless Motion Capture Applied To Sports | http://arxiv.org/abs/1402.2363 | author:Ashish Shingade, Archana Ghotkar category:cs.GR cs.CV published:2014-02-11 summary:Markerless motion capture is an active research in 3D virtualization. Inproposed work we presented a system for markerless motion capture for 3D humancharacter animation, paper presents a survey on motion and skeleton trackingtechniques which are developed or are under development. The paper proposed amethod to transform the motion of a performer to a 3D human character (model),the 3D human character performs similar movements as that of a performer inreal time. In the proposed work, human model data will be captured by Kinectcamera, processed data will be applied on 3D human model for animation. 3Dhuman model is created using open source software (MakeHuman). Anticipateddataset for sport activity is considered as input which can be applied to anyHCI application.
arxiv-6000-108 | Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles | http://arxiv.org/abs/1402.2826 | author:Aniket Bera, Dinesh Manocha category:cs.CV published:2014-02-11 summary:We present a novel, realtime algorithm to compute the trajectory of eachpedestrian in moderately dense crowd scenes. Our formulation is based on anadaptive particle filtering scheme that uses a multi-agent motion model basedon velocity-obstacles, and takes into account local interactions as well asphysical and personal constraints of each pedestrian. Our method dynamicallychanges the number of particles allocated to each pedestrian based on differentconfidence metrics. Additionally, we use a new high-definition crowd videodataset, which is used to evaluate the performance of different pedestriantracking algorithms. This dataset consists of videos of indoor and outdoorscenes, recorded at different locations with 30-80 pedestrians. We highlightthe performance benefits of our algorithm over prior techniques using thisdataset. In practice, our algorithm can compute trajectories of tens ofpedestrians on a multi-core desktop CPU at interactive rates (27-30 frames persecond). To the best of our knowledge, our approach is 4-5 times faster thanprior methods, which provide similar accuracy.
arxiv-6000-109 | A comparison of linear and non-linear calibrations for speaker recognition | http://arxiv.org/abs/1402.2447 | author:Niko Brümmer, Albert Swart, David van Leeuwen category:stat.ML cs.LG published:2014-02-11 summary:In recent work on both generative and discriminative score tolog-likelihood-ratio calibration, it was shown that linear transforms give goodaccuracy only for a limited range of operating points. Moreover, these methodsrequired tailoring of the calibration training objective functions in order totarget the desired region of best accuracy. Here, we generalize the linearrecipes to non-linear ones. We experiment with a non-linear, non-parametric,discriminative PAV solution, as well as parametric, generative,maximum-likelihood solutions that use Gaussian, Student's T andnormal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scoressuggest that the non-linear methods provide wider ranges of optimal accuracyand can be trained without having to resort to objective function tailoring.
arxiv-6000-110 | Sparsity averaging for radio-interferometric imaging | http://arxiv.org/abs/1402.2335 | author:Rafael E. Carrillo, Jason D. McEwen, Yves Wiaux category:astro-ph.IM cs.CV published:2014-02-11 summary:We propose a novel regularization method for compressive imaging in thecontext of the compressed sensing (CS) theory with coherent and redundantdictionaries. Natural images are often complicated and several types ofstructures can be present at once. It is well known that piecewise smoothimages exhibit gradient sparsity, and that images with extended structures arebetter encapsulated in wavelet frames. Therefore, we here conjecture thatpromoting average sparsity or compressibility over multiple frames rather thansingle frames is an extremely powerful regularization prior.
arxiv-6000-111 | On Zeroth-Order Stochastic Convex Optimization via Random Walks | http://arxiv.org/abs/1402.2667 | author:Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin category:cs.LG stat.ML published:2014-02-11 summary:We propose a method for zeroth order stochastic convex optimization thatattains the suboptimality rate of $\tilde{\mathcal{O}}(n^{7}T^{-1/2})$ after$T$ queries for a convex bounded function $f:{\mathbb R}^n\to{\mathbb R}$. Themethod is based on a random walk (the \emph{Ball Walk}) on the epigraph of thefunction. The randomized approach circumvents the problem of gradientestimation, and appears to be less sensitive to noisy function evaluationscompared to noiseless zeroth order methods.
arxiv-6000-112 | Justifying Information-Geometric Causal Inference | http://arxiv.org/abs/1402.2499 | author:Dominik Janzing, Bastian Steudel, Naji Shajarisales, Bernhard Schölkopf category:stat.ML published:2014-02-11 summary:Information Geometric Causal Inference (IGCI) is a new approach todistinguish between cause and effect for two variables. It is based on anindependence assumption between input distribution and causal mechanism thatcan be phrased in terms of orthogonality in information space. We describe twointuitive reinterpretations of this approach that makes IGCI more accessible toa broader audience. Moreover, we show that the described independence is related to thehypothesis that unsupervised learning and semi-supervised learning only worksfor predicting the cause from the effect and not vice versa.
arxiv-6000-113 | A Fast Two Pass Multi-Value Segmentation Algorithm based on Connected Component Analysis | http://arxiv.org/abs/1402.2606 | author:Dibyendu Mukherjee category:cs.CV published:2014-02-11 summary:Connected component analysis (CCA) has been heavily used to label binaryimages and classify segments. However, it has not been well-exploited tosegment multi-valued natural images. This work proposes a novel multi-valuesegmentation algorithm that utilizes CCA to segment color images. A userdefined distance measure is incorporated in the proposed modified CCA toidentify and segment similar image regions. The raw output of the algorithmconsists of distinctly labelled segmented regions. The proposed algorithm has aunique design architecture that provides several benefits: 1) it can be used tosegment any multi-channel multi-valued image; 2) the distancemeasure/segmentation criteria can be application-specific and 3) an absolutelinear-time implementation allows easy extension for real-time videosegmentation. Experimental demonstrations of the aforesaid benefits arepresented along with the comparison results on multiple datasets with currentbenchmark algorithms. A number of possible application areas are alsoidentified and results on real-time video segmentation has been presented toshow the promise of the proposed method.
arxiv-6000-114 | Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision | http://arxiv.org/abs/1402.2426 | author:Keith Dillon, Yeshaiahu Fainman category:cs.CV published:2014-02-11 summary:In this paper we broadly consider techniques which utilize projections onrays for data collection, with particular emphasis on optical techniques. Weformulate a variety of imaging techniques as either special cases or extensionsof tomographic reconstruction. We then consider how the techniques must beextended to describe objects containing occlusion, as with a self-occludingopaque object. We formulate the reconstruction problem as a regularizednonlinear optimization problem to simultaneously solve for object brightnessand attenuation, where the attenuation can become infinite. We demonstratevarious simulated examples for imaging opaque objects, including sparse pointsources, a conventional multiview reconstruction technique, and asuper-resolving technique which exploits occlusion to resolve an image.
arxiv-6000-115 | Online Nonparametric Regression | http://arxiv.org/abs/1402.2594 | author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG math.ST stat.TH published:2014-02-11 summary:We establish optimal rates for online regression for arbitrary classes ofregression functions in terms of the sequential entropy introduced in (Rakhlin,Sridharan, Tewari, 2010). The optimal rates are shown to exhibit a phasetransition analogous to the i.i.d./statistical learning case, studied in(Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situationwhen sequential entropy and i.i.d. empirical entropy match, our results pointto the interesting phenomenon that the rates for statistical learning withsquared loss and online nonparametric regression are the same. In addition to a non-algorithmic study of minimax regret, we exhibit ageneric forecaster that enjoys the established optimal rates. We also provide arecipe for designing online regression algorithms that can be computationallyefficient. We illustrate the techniques by deriving existing and newforecasters for the case of finite experts and for online linear regression.
arxiv-6000-116 | Packing and Padding: Coupled Multi-index for Accurate Image Retrieval | http://arxiv.org/abs/1402.2681 | author:Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian category:cs.CV published:2014-02-11 summary:In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a lowdiscriminative power, so false positive matches occur prevalently. Apart fromthe information loss during quantization, another cause is that the SIFTfeature only describes the local gradient distribution. To address thisproblem, this paper proposes a coupled Multi-Index (c-MI) framework to performfeature fusion at indexing level. Basically, complementary features are coupledinto a multi-dimensional inverted index. Each dimension of c-MI corresponds toone kind of feature, and the retrieval process votes for images similar in bothSIFT and other feature spaces. Specifically, we exploit the fusion of localcolor feature into c-MI. While the precision of visual match is greatlyenhanced, we adopt Multiple Assignment to improve recall. The joint cooperationof SIFT and color features significantly reduces the impact of false positivematches. Extensive experiments on several benchmark datasets demonstrate that c-MIimproves the retrieval accuracy significantly, while consuming only half of thequery time compared to the baseline. Importantly, we show that c-MI is wellcomplementary to many prior techniques. Assembling these methods, we haveobtained an mAP of 85.8% and N-S score of 3.85 on Holidays and Ukbenchdatasets, respectively, which compare favorably with the state-of-the-arts.
arxiv-6000-117 | Machine Learner for Automated Reasoning 0.4 and 0.5 | http://arxiv.org/abs/1402.2359 | author:Cezary Kaliszyk, Josef Urban, Jiří Vyskočil category:cs.LG cs.AI cs.LO published:2014-02-11 summary:Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoningsystem for proving in large formal libraries where thousands of theorems areavailable when attacking a new conjecture, and a large number of relatedproblems and proofs can be used to learn specific theorem-proving knowledge.The last version of the system has by a large margin won the 2013 CASC LTBcompetition. This paper describes the motivation behind the methods used inMaLARea, discusses the general approach and the issues arising in evaluation ofsuch system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.
arxiv-6000-118 | Equivalence of Kernel Machine Regression and Kernel Distance Covariance for Multidimensional Trait Association Studies | http://arxiv.org/abs/1402.2679 | author:Wen-Yu Hua, Debashis Ghosh category:stat.ML stat.ME published:2014-02-11 summary:Associating genetic markers with a multidimensional phenotype is an importantyet challenging problem. In this work, we establish the equivalence between twopopular methods: kernel-machine regression (KMR), and kernel distancecovariance (KDC). KMR is a semiparametric regression frameworks that models thecovariate effects parametrically, while the genetic markers are considerednon-parametrically. KDC represents a class of methods that includes distancecovariance (DC) and Hilbert-Schmidt Independence Criterion (HSIC), which arenonparametric tests of independence. We show the equivalence between the scoretest of KMR and the KDC statistic under certain conditions. This result leadsto a novel generalization of the KDC test that incorporates the covariates. Ourcontributions are three-fold: (1) establishing the equivalence between KMR andKDC; (2) showing that the principles of kernel machine regression can beapplied to the interpretation of KDC; (3) the development of a broader class ofKDC statistics, that the members are the quantities of different kernels. Wedemonstrate the proposals using simulation studies. Data from the Alzheimer'sDisease Neuroimaging Initiative (ADNI) is used to explore the associationbetween the genetic variants on gene \emph{FLJ16124} and phenotypes representedin 3D structural brain MR images adjusting for age and gender. The resultssuggest that SNPs of \emph{FLJ16124} exhibit strong pairwise interactioneffects that are correlated to the changes of brain region volumes.
arxiv-6000-119 | Approachability in unknown games: Online learning meets multi-objective optimization | http://arxiv.org/abs/1402.2043 | author:Shie Mannor, Vianney Perchet, Gilles Stoltz category:stat.ML cs.LG math.ST stat.TH published:2014-02-10 summary:In the standard setting of approachability there are two players and a targetset. The players play a repeated vector-valued game where one of them wants tohave the average vector-valued payoff converge to the target set which theother player tries to exclude. We revisit the classical setting and considerthe setting where the player has a preference relation between target sets: shewishes to approach the smallest ("best") set possible given the observedaverage payoffs in hindsight. Moreover, as opposed to previous works onapproachability, and in the spirit of online learning, we do not assume thatthere is a known game structure with actions for two players. Rather, theplayer receives an arbitrary vector-valued reward vector at every round. Weshow that it is impossible, in general, to approach the best target set inhindsight. We further propose a concrete strategy that approaches a non-trivialrelaxation of the best-in-hindsight given the actual rewards. Our approach doesnot require projection onto a target set and amounts to switching betweenscalar regret minimization algorithms that are performed in episodes.
arxiv-6000-120 | Deeply Coupled Auto-encoder Networks for Cross-view Classification | http://arxiv.org/abs/1402.2031 | author:Wen Wang, Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen category:cs.CV cs.LG cs.NE published:2014-02-10 summary:The comparison of heterogeneous samples extensively exists in manyapplications, especially in the task of image classification. In this paper, wepropose a simple but effective coupled neural network, called Deeply CoupledAutoencoder Networks (DCAN), which seeks to build two deep neural networks,coupled with each other in every corresponding layers. In DCAN, each deepstructure is developed via stacking multiple discriminative coupledauto-encoders, a denoising auto-encoder trained with maximum margin criterionconsisting of intra-class compactness and inter-class penalty. This singlelayer component makes our model simultaneously preserve the local consistencyand enhance its discriminative capability. With increasing number of layers,the coupled networks can gradually narrow the gap between the two views.Extensive experiments on cross-view image classification tasks demonstrate thesuperiority of our method over state-of-the-art methods.
arxiv-6000-121 | A Second-order Bound with Excess Losses | http://arxiv.org/abs/1402.2044 | author:Pierre Gaillard, Gilles Stoltz, Tim Van Erven category:stat.ML cs.LG math.ST stat.TH published:2014-02-10 summary:We study online aggregation of the predictions of experts, and first show newsecond-order regret bounds in the standard setting, which are obtained via aversion of the Prod algorithm (and also a version of the polynomially weightedaverage algorithm) with multiple learning rates. These bounds are in terms ofexcess losses, the differences between the instantaneous losses suffered by thealgorithm and the ones of a given expert. We then demonstrate the interest ofthese bounds in the context of experts that report their confidences as anumber in the interval [0,1] using a generic reduction to the standard setting.We conclude by two other applications in the standard setting, which improvethe known bounds in case of small excess losses and show a bounded regretagainst i.i.d. sequences of losses.
arxiv-6000-122 | Signal Reconstruction Framework Based On Projections Onto Epigraph Set Of A Convex Cost Function (PESC) | http://arxiv.org/abs/1402.2088 | author:Mohammad Tofighi, Kivanc Kose, A. Enis Cetin category:math.OC cs.CV published:2014-02-10 summary:A new signal processing framework based on making orthogonal Projections ontothe Epigraph Set of a Convex cost function (PESC) is developed. In this way itis possible to solve convex optimization problems using the well-knownProjections onto Convex Set (POCS) approach. In this algorithm, the dimensionof the minimization problem is lifted by one and a convex set corresponding tothe epigraph of the cost function is defined. If the cost function is a convexfunction in $R^N$, the corresponding epigraph set is also a convex set inR^{N+1}. The PESC method provides globally optimal solutions fortotal-variation (TV), filtered variation (FV), L_1, L_2, and entropic costfunction based convex optimization problems. In this article, the PESC baseddenoising and compressive sensing algorithms are developed. Simulation examplesare presented.
arxiv-6000-123 | Genomic Prediction of Quantitative Traits using Sparse and Locally Epistatic Models | http://arxiv.org/abs/1402.2026 | author:Deniz Akdemir category:stat.AP stat.ML published:2014-02-10 summary:In plant and animal breeding studies a distinction is made between thegenetic value (additive + epistatic genetic effects) and the breeding value(additive genetic effects) of an individual since it is expected that some ofthe epistatic genetic effects will be lost due to recombination. In this paper,we argue that the breeder can take advantage of some of the epistatic markereffects in regions of low recombination. The models introduced here aim toestimate local epistatic line heritability by using the genetic map informationand combine the local additive and epistatic effects. To this end, we have usedsemi-parametric mixed models with multiple local genomic relationship matriceswith hierarchical designs and lasso post-processing for sparsity in the finalmodel. Our models produce good predictive performance along with goodexplanatory information.
arxiv-6000-124 | Near-Optimally Teaching the Crowd to Classify | http://arxiv.org/abs/1402.2092 | author:Adish Singla, Ilija Bogunovic, Gábor Bartók, Amin Karbasi, Andreas Krause category:cs.LG published:2014-02-10 summary:How should we present training examples to learners to teach themclassification rules? This is a natural problem when training workers forcrowdsourcing labeling tasks, and is also motivated by challenges indata-driven online education. We propose a natural stochastic model of thelearners, modeling them as randomly switching among hypotheses based onobserved feedback. We then develop STRICT, an efficient algorithm for selectingexamples to teach to workers. Our solution greedily maximizes a submodularsurrogate objective function in order to select examples to show to thelearners. We prove that our strategy is competitive with the optimal teachingpolicy. Moreover, for the special case of linear separators, we prove that anexponential reduction in error probability can be achieved. Our experiments onsimulated workers as well as three real image annotation tasks on AmazonMechanical Turk show the effectiveness of our teaching algorithm.
arxiv-6000-125 | An Algorithmic Framework for Computing Validation Performance Bounds by Using Suboptimal Models | http://arxiv.org/abs/1402.2148 | author:Yoshiki Suzuki, Kohei Ogawa, Yuki Shinmura, Ichiro Takeuchi category:stat.ML published:2014-02-10 summary:Practical model building processes are often time-consuming because manydifferent models must be trained and validated. In this paper, we introduce anovel algorithm that can be used for computing the lower and the upper boundsof model validation errors without actually training the model itself. A keyidea behind our algorithm is using a side information available from asuboptimal model. If a reasonably good suboptimal model is available, ouralgorithm can compute lower and upper bounds of many useful quantities formaking inferences on the unknown target model. We demonstrate the advantage ofour algorithm in the context of model selection for regularized learningproblems.
arxiv-6000-126 | Leveraging Long-Term Predictions and Online-Learning in Agent-based Multiple Person Tracking | http://arxiv.org/abs/1402.2016 | author:Wenxi Liu, Antoni B. Chan, Rynson W. H. Lau, Dinesh Manocha category:cs.CV published:2014-02-10 summary:We present a multiple-person tracking algorithm, based on combining particlefilters and RVO, an agent-based crowd model that infers collision-freevelocities so as to predict pedestrian's motion. In addition to position andvelocity, our tracking algorithm can estimate the internal goals (desireddestination or desired velocity) of the tracked pedestrian in an online manner,thus removing the need to specify this information beforehand. Furthermore, weleverage the longer-term predictions of RVO by deriving a higher-order particlefilter, which aggregates multiple predictions from different prior time steps.This yields a tracker that can recover from short-term occlusions and spuriousnoise in the appearance model. Experimental results show that our trackingalgorithm is suitable for predicting pedestrians' behaviors online withoutneeding scene priors or hand-annotated goal information, and improves trackingin real-world crowded scenes under low frame rates.
arxiv-6000-127 | Binary Stereo Matching | http://arxiv.org/abs/1402.2020 | author:Kang Zhang, Jiyang Li, Yijing Li, Weidong Hu, Lifeng Sun, Shiqiang Yang category:cs.CV published:2014-02-10 summary:In this paper, we propose a novel binary-based cost computation andaggregation approach for stereo matching problem. The cost volume isconstructed through bitwise operations on a series of binary strings. Then thisapproach is combined with traditional winner-take-all strategy, resulting in anew local stereo matching algorithm called binary stereo matching (BSM). Sincecore algorithm of BSM is based on binary and integer computations, it has ahigher computational efficiency than previous methods. Experimental results onMiddlebury benchmark show that BSM has comparable performance withstate-of-the-art local stereo methods in terms of both quality and speed.Furthermore, experiments on images with radiometric differences demonstratethat BSM is more robust than previous methods under these changes, which iscommon under real illumination.
arxiv-6000-128 | Handwritten Character Recognition In Malayalam Scripts- A Review | http://arxiv.org/abs/1402.2188 | author:Anitha Mary M. O. Chacko, P. M Dhanya category:cs.CV published:2014-02-10 summary:Handwritten character recognition is one of the most challenging and ongoingareas of research in the field of pattern recognition. HCR research is maturedfor foreign languages like Chinese and Japanese but the problem is much morecomplex for Indian languages. The problem becomes even more complicated forSouth Indian languages due to its large character set and the presence ofvowels modifiers and compound characters. This paper provides an overview ofimportant contributions and advances in offline as well as online handwrittencharacter recognition of Malayalam scripts.
arxiv-6000-129 | Foreground segmentation based on multi-resolution and matting | http://arxiv.org/abs/1402.2013 | author:Xintong Yu, Xiaohan Liu, Yisong Chen category:cs.CV published:2014-02-10 summary:We propose a foreground segmentation algorithm that does foregroundextraction under different scales and refines the result by matting. First, theinput image is filtered and resampled to 5 different resolutions. Then each ofthem is segmented by adaptive figure-ground classification and the bestsegmentation is automatically selected by an evaluation score that maximizesthe difference between foreground and background. This segmentation isupsampled to the original size, and a corresponding trimap is built.Closed-form matting is employed to label the boundary region, and the result isrefined by a final figure-ground classification. Experiments show the successof our method in treating challenging images with cluttered background andadapting to loose initial bounding-box.
arxiv-6000-130 | Characterizing the Sample Complexity of Private Learners | http://arxiv.org/abs/1402.2224 | author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.CR cs.LG published:2014-02-10 summary:In 2008, Kasiviswanathan et al. defined private learning as a combination ofPAC learning and differential privacy. Informally, a private learner is appliedto a collection of labeled individual information and outputs a hypothesiswhile preserving the privacy of each individual. Kasiviswanathan et al. gave ageneric construction of private learners for (finite) concept classes, withsample complexity logarithmic in the size of the concept class. This samplecomplexity is higher than what is needed for non-private learners, henceleaving open the possibility that the sample complexity of private learning maybe sometimes significantly higher than that of non-private learning. We give a combinatorial characterization of the sample size sufficient andnecessary to privately learn a class of concepts. This characterization isanalogous to the well known characterization of the sample complexity ofnon-private learning in terms of the VC dimension of the concept class. Weintroduce the notion of probabilistic representation of a concept class, andour new complexity measure RepDim corresponds to the size of the smallestprobabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with samplecomplexity m implies RepDim(C)=O(m), and that there exists a private learningalgorithm with sample complexity m=O(RepDim(C)). We further demonstrate that asimilar characterization holds for the database size needed for privatelycomputing a large class of optimization problems and also for the well studiedproblem of private data release.
arxiv-6000-131 | Image Search Reranking | http://arxiv.org/abs/1402.2232 | author:V Rajakumar, Vipeen V Bopche category:cs.IR cs.CV published:2014-02-10 summary:The existing methods for image search reranking suffer from theunfaithfulness of the assumptions under which the text-based images searchresult. The resulting images contain more irrelevant images. Hence the reranking concept arises to re rank the retrieved images based on the text aroundthe image and data of data of image and visual feature of image. A number ofmethods are differentiated for this re-ranking. The high ranked images are usedas noisy data and a k means algorithm for classification is learned to rectifythe ranking further. We are study the affect ability of the cross validationmethod to this training data. The pre eminent originality of the overall methodis in collecting text/metadata of image and visual features in order to achievean automatic ranking of the images. Supervision is initiated to learn the modelweights offline, previous to reranking process. While model learning needsmanual labeling of the results for a some limited queries, the resulting modelis query autonomous and therefore applicable to any other query .Examples aregiven for a selection of other classes like vehicles, animals and otherclasses.
arxiv-6000-132 | Étude cognitive des processus de construction d'une requête dans un système de gestion de connaissances médicales | http://arxiv.org/abs/1402.2562 | author:Nathalie Chaignaud, Valérie Delavigne, Maryvonne Holzem, Jean-Philippe Kotowicz, Alain Loisel category:cs.IR cs.CL published:2014-02-10 summary:This article presents the Cogni-CISMeF project, which aims at improvingmedical information search in the CISMeF system (Catalog and Index ofFrench-language health resources) by including a conversational agent tointeract with the user in natural language. To study the cognitive processesinvolved during the information search, a bottom-up methodology was adopted.Experimentation has been set up to obtain human dialogs between a user (playingthe role of patient) dealing with medical information search and a CISMeFexpert refining the request. The analysis of these dialogs underlined the useof discursive evidence: vocabulary, reformulation, implicit or explicitexpression of user intentions, conversational sequences, etc. A model ofartificial agent is proposed. It leads the user in its information search byproposing to him examples, assistance and choices. This model was implementedand integrated in the CISMeF system. ---- Cet article d\'ecrit le projetCogni-CISMeF qui propose un module de dialogue Homme-Machine \`a int\'egrerdans le syst\`eme d'indexation de connaissances m\'edicales CISMeF (Catalogueet Index des Sites M\'edicaux Francophones). Nous avons adopt\'e une d\'emarchede mod\'elisation cognitive en proc\'edant \`a un recueil de corpus dedialogues entre un utilisateur (jouant le r\^ole d'un patient) d\'esirant uneinformation m\'edicale et un expert CISMeF af inant cette demande pourconstruire la requ\^ete. Nous avons analys\'e la structure des dialogues ainsiobtenus et avons \'etudi\'e un certain nombre d'indices discursifs :vocabulaire employ\'e, marques de reformulation, commentaires m\'eta et\'epilinguistiques, expression implicite ou explicite des intentions del'utilisateur, encha\^inement conversationnel, etc. De cette analyse, nousavons construit un mod\`ele d'agent artificiel dot\'e de capacit\'es cognitivescapables d'aider l'utilisateur dans sa t\^ache de recherche d'information. Cemod\`ele a \'et\'e impl\'ement\'e et int\'egr\'e dans le syst\`eme CISMeF.
arxiv-6000-133 | Feature and Variable Selection in Classification | http://arxiv.org/abs/1402.2300 | author:Aaron Karper category:cs.LG cs.AI stat.ML published:2014-02-10 summary:The amount of information in the form of features and variables avail- ableto machine learning algorithms is ever increasing. This can lead to classifiersthat are prone to overfitting in high dimensions, high di- mensional models donot lend themselves to interpretable results, and the CPU and memory resourcesnecessary to run on high-dimensional datasets severly limit the applications ofthe approaches. Variable and feature selection aim to remedy this by finding asubset of features that in some way captures the information provided best. Inthis paper we present the general methodology and highlight some specificapproaches.
arxiv-6000-134 | Modeling sequential data using higher-order relational features and predictive training | http://arxiv.org/abs/1402.2333 | author:Vincent Michalski, Roland Memisevic, Kishore Konda category:cs.LG cs.CV stat.ML published:2014-02-10 summary:Bi-linear feature learning models, like the gated autoencoder, were proposedas a way to model relationships between frames in a video. By minimizingreconstruction error of one frame, given the previous frame, these models learn"mapping units" that encode the transformations inherent in a sequence, andthereby learn to encode motion. In this work we extend bi-linear models byintroducing "higher-order mapping units" that allow us to encodetransformations between frames and transformations between transformations. We show that this makes it possible to encode temporal structure that is morecomplex and longer-range than the structure captured within standard bi-linearmodels. We also show that a natural way to train the model is by replacing thecommonly used reconstruction objective with a prediction objective which forcesthe model to correctly predict the evolution of the input multiple steps intothe future. Learning can be achieved by back-propagating the multi-stepprediction through time. We test the model on various temporal predictiontasks, and show that higher-order mappings and predictive training both yield asignificant improvement over bi-linear models in terms of prediction accuracy.
arxiv-6000-135 | Probabilistic Interpretation of Linear Solvers | http://arxiv.org/abs/1402.2058 | author:Philipp Hennig category:math.OC cs.LG cs.NA math.NA math.PR stat.ML 90C53, 65F10 published:2014-02-10 summary:This manuscript proposes a probabilistic framework for algorithms thatiteratively solve unconstrained linear problems $Bx = b$ with positive definite$B$ for $x$. The goal is to replace the point estimates returned by existingmethods with a Gaussian posterior belief over the elements of the inverse of$B$, which can be used to estimate errors. Recent probabilistic interpretationsof the secant family of quasi-Newton optimization algorithms are extended.Combined with properties of the conjugate gradient algorithm, this leads touncertainty-calibrated methods with very limited cost overhead over conjugategradients, a self-contained novel interpretation of the quasi-Newton andconjugate gradient algorithms, and a foundation for new nonlinear optimizationmethods.
arxiv-6000-136 | Universal Matrix Completion | http://arxiv.org/abs/1402.2324 | author:Srinadh Bhojanapalli, Prateek Jain category:stat.ML cs.IT cs.LG math.IT published:2014-02-10 summary:The problem of low-rank matrix completion has recently generated a lot ofinterest leading to several results that offer exact solutions to the problem.However, in order to do so, these methods make assumptions that can be quiterestrictive in practice. More specifically, the methods assume that: a) theobserved indices are sampled uniformly at random, and b) for every new matrix,the observed indices are sampled afresh. In this work, we address these issuesby providing a universal recovery guarantee for matrix completion that worksfor a variety of sampling schemes. In particular, we show that if the set ofsampled indices come from the edges of a bipartite graph with large spectralgap (i.e. gap between the first and the second singular value), then thenuclear norm minimization based method exactly recovers all low-rank matricesthat satisfy certain incoherence properties. Moreover, we also show that undercertain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entriesare enough to recover any rank-$r$ $n\times n$ matrix, in contrast to the$O(nr\log n)$ sample complexity required by other matrix completion algorithmsas well as existing analyses of the nuclear norm method.
arxiv-6000-137 | Computational Limits for Matrix Completion | http://arxiv.org/abs/1402.2331 | author:Moritz Hardt, Raghu Meka, Prasad Raghavendra, Benjamin Weitz category:cs.CC cs.LG published:2014-02-10 summary:Matrix Completion is the problem of recovering an unknown real-valuedlow-rank matrix from a subsample of its entries. Important recent results showthat the problem can be solved efficiently under the assumption that theunknown matrix is incoherent and the subsample is drawn uniformly at random.Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard.However, little is known if make additional assumptions such as incoherence andpermit the algorithm to output a matrix of slightly higher rank. In this paperwe prove that Matrix Completion remains computationally intractable even if theunknown matrix has rank $4$ but we are allowed to output any constant rankmatrix, and even if additionally we assume that the unknown matrix isincoherent and are shown $90%$ of the entries. This result relies on theconjectured hardness of the $4$-Coloring problem. We also consider the positivesemidefinite Matrix Completion problem. Here we show a similar hardness resultunder the standard assumption that $\mathrm{P}\ne \mathrm{NP}.$ Our results greatly narrow the gap between existing feasibility results andcomputational lower bounds. In particular, we believe that our results give thefirst complexity-theoretic justification for why distributional assumptions areneeded beyond the incoherence assumption in order to obtain positive results.On the technical side, we contribute several new ideas on how to encode hardcombinatorial problems in low-rank optimization problems. We hope that thesetechniques will be helpful in further understanding the computational limits ofMatrix Completion and related problems.
arxiv-6000-138 | Dictionary learning for fast classification based on soft-thresholding | http://arxiv.org/abs/1402.1973 | author:Alhussein Fawzi, Mike Davies, Pascal Frossard category:cs.CV cs.LG stat.ML published:2014-02-09 summary:Classifiers based on sparse representations have recently been shown toprovide excellent results in many visual recognition and classification tasks.However, the high cost of computing sparse representations at test time is amajor obstacle that limits the applicability of these methods in large-scaleproblems, or in scenarios where computational power is restricted. We considerin this paper a simple yet efficient alternative to sparse coding for featureextraction. We study a classification scheme that applies the soft-thresholdingnonlinear mapping in a dictionary, followed by a linear classifier. A novelsupervised dictionary learning algorithm tailored for this low complexityclassification architecture is proposed. The dictionary learning problem, whichjointly learns the dictionary and linear classifier, is cast as a difference ofconvex (DC) program and solved efficiently with an iterative DC solver. Weconduct experiments on several datasets, and show that our learning algorithmthat leverages the structure of the classification problem outperforms genericlearning procedures. Our simple classifier based on soft-thresholding alsocompetes with the recent sparse coding classifiers, when the dictionary islearned appropriately. The adopted classification scheme further requires lesscomputational time at the testing stage, compared to other classifiers. Theproposed scheme shows the potential of the adequately trained soft-thresholdingmapping for classification and paves the way towards the development of veryefficient classification methods for vision problems.
arxiv-6000-139 | MCA Learning Algorithm for Incident Signals Estimation: A Review | http://arxiv.org/abs/1402.1931 | author:Rashid Ahmed, John A. Avaritsiotis category:cs.NE published:2014-02-09 summary:Recently there has been many works on adaptive subspace filtering in thesignal processing literature. Most of them are concerned with tracking thesignal subspace spanned by the eigenvectors corresponding to the eigenvalues ofthe covariance matrix of the signal plus noise data. Minor Component Analysis(MCA) is important tool and has a wide application in telecommunications,antenna array processing, statistical parametric estimation, etc. As animportant feature extraction technique, MCA is a statistical method ofextracting the eigenvector associated with the smallest eigenvalue of thecovariance matrix. In this paper, we will present a MCA learning algorithm toextract minor component from input signals, and the learning rate parameter isalso presented, which ensures fast convergence of the algorithm, because it hasdirect effect on the convergence of the weight vector and the error level isaffected by this value. MCA is performed to determine the estimated DOA.Simulation results will be furnished to illustrate the theoretical resultsachieved.
arxiv-6000-140 | Classification Tree Diagrams in Health Informatics Applications | http://arxiv.org/abs/1402.1947 | author:Farrukh Arslan category:cs.IR cs.CV cs.LG published:2014-02-09 summary:Health informatics deal with the methods used to optimize the acquisition,storage and retrieval of medical data, and classify information in healthcareapplications. Healthcare analysts are particularly interested in variouscomputer informatics areas such as; knowledge representation from data, anomalydetection, outbreak detection methods and syndromic surveillance applications.Although various parametric and non-parametric approaches are being proposed toclassify information from data, classification tree diagrams provide aninteractive visualization to analysts as compared to other methods. In thiswork we discuss application of classification tree diagrams to classifyinformation from medical data in healthcare applications.
arxiv-6000-141 | Direct Processing of Run Length Compressed Document Image for Segmentation and Characterization of a Specified Block | http://arxiv.org/abs/1402.1971 | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-02-09 summary:Extracting a block of interest referred to as segmenting a specified block inan image and studying its characteristics is of general research interest, andcould be a challenging if such a segmentation task has to be carried outdirectly in a compressed image. This is the objective of the present researchwork. The proposal is to evolve a method which would segment and extract aspecified block, and carry out its characterization without decompressing acompressed image, for two major reasons that most of the image archives containimages in compressed format and decompressing an image indents additionalcomputing time and space. Specifically in this research work, the proposal isto work on run-length compressed document images.
arxiv-6000-142 | Maximum Entropy, Word-Frequency, Chinese Characters, and Multiple Meanings | http://arxiv.org/abs/1402.1939 | author:Xiao-Yong Yan, Petter Minnhagen category:physics.soc-ph cs.CL published:2014-02-09 summary:The word-frequency distribution of a text written by an author is wellaccounted for by a maximum entropy distribution, the RGF (random groupformation)-prediction. The RGF-distribution is completely determined by the apriori values of the total number of words in the text (M), the number ofdistinct words (N) and the number of repetitions of the most common word(k_max). It is here shown that this maximum entropy prediction also describes atext written in Chinese characters. In particular it is shown that although thesame Chinese text written in words and Chinese characters have quitedifferently shaped distributions, they are nevertheless both well predicted bytheir respective three a priori characteristic values. It is pointed out thatthis is analogous to the change in the shape of the distribution whentranslating a given text to another language. Another consequence of theRGF-prediction is that taking a part of a long text will change the inputparameters (M, N, k_max) and consequently also the shape of the frequencydistribution. This is explicitly confirmed for texts written in Chinesecharacters. Since the RGF-prediction has no system-specific information beyondthe three a priori values (M, N, k_max), any specific language characteristichas to be sought in systematic deviations from the RGF-prediction and themeasured frequencies. One such systematic deviation is identified and, througha statistical information theoretical argument and an extended RGF-model, it isproposed that this deviation is caused by multiple meanings of Chinesecharacters. The effect is stronger for Chinese characters than for Chinesewords. The relation between Zipf's law, the Simon-model for texts and thepresent results are discussed.
arxiv-6000-143 | Better Optimism By Bayes: Adaptive Planning with Rich Models | http://arxiv.org/abs/1402.1958 | author:Arthur Guez, David Silver, Peter Dayan category:cs.AI cs.LG stat.ML published:2014-02-09 summary:The computational costs of inference and planning have confined Bayesianmodel-based reinforcement learning to one of two dismal fates: powerfulBayes-adaptive planning but only for simplistic models, or powerful, Bayesiannon-parametric models but using simple, myopic planning strategies such asThompson sampling. We ask whether it is feasible and truly beneficial tocombine rich probabilistic models with a closer approximation to fully Bayesianplanning. First, we use a collection of counterexamples to show formal problemswith the over-optimism inherent in Thompson sampling. Then we leveragestate-of-the-art techniques in efficient Bayes-adaptive planning andnon-parametric Bayesian methods to perform qualitatively better than bothexisting conventional algorithms and Thompson sampling on two contextualbandit-like problems.
arxiv-6000-144 | A Hybrid Loss for Multiclass and Structured Prediction | http://arxiv.org/abs/1402.1921 | author:Qinfeng Shi, Mark Reid, Tiberio Caetano, Anton van den Hengel, Zhenhua Wang category:cs.LG cs.AI cs.CV published:2014-02-09 summary:We propose a novel hybrid loss for multiclass and structured predictionproblems that is a convex combination of a log loss for Conditional RandomFields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).We provide a sufficient condition for when the hybrid loss is Fisher consistentfor classification. This condition depends on a measure of dominance betweenlabels--specifically, the gap between the probabilities of the best label andthe second best label. We also prove Fisher consistency is necessary forparametric consistency when learning models such as CRFs. We demonstrateempirically that the hybrid loss typically performs least as well as--and oftenbetter than--both of its constituent losses on a variety of tasks, such ashuman action recognition. In doing so we also provide an empirical comparisonof the efficacy of probabilistic and margin based approaches to multiclass andstructured prediction.
arxiv-6000-145 | Thresholding Classifiers to Maximize F1 Score | http://arxiv.org/abs/1402.1892 | author:Zachary Chase Lipton, Charles Elkan, Balakrishnan Narayanaswamy category:stat.ML cs.IR cs.LG published:2014-02-08 summary:This paper provides new insight into maximizing F1 scores in the context ofbinary classification and also in the context of multilabel classification. Theharmonic mean of precision and recall, F1 score is widely used to measure thesuccess of a binary classifier when one class is rare. Micro average, macroaverage, and per instance average F1 scores are used in multilabelclassification. For any classifier that produces a real-valued output, wederive the relationship between the best achievable F1 score and thedecision-making threshold that achieves this optimum. As a special case, if theclassifier outputs are well-calibrated conditional probabilities, then theoptimal threshold is half the optimal F1 score. As another special case, if theclassifier is completely uninformative, then the optimal behavior is toclassify all examples as positive. Since the actual prevalence of positiveexamples typically is low, this behavior can be considered undesirable. As acase study, we discuss the results, which can be surprising, of applying thisprocedure when predicting 26,853 labels for Medline documents.
arxiv-6000-146 | An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning | http://arxiv.org/abs/1402.1864 | author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:cs.LG stat.ML published:2014-02-08 summary:From concentration inequalities for the suprema of Gaussian or Rademacherprocesses an inequality is derived. It is applied to sharpen existing and toderive novel bounds on the empirical Rademacher complexities of unit balls invarious norms appearing in the context of structured sparsity and multitaskdictionary learning or matrix factorization. A key role is played by thelargest eigenvalue of the data covariance matrix.
arxiv-6000-147 | On the Number of Linear Regions of Deep Neural Networks | http://arxiv.org/abs/1402.1869 | author:Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2014-02-08 summary:We study the complexity of functions computable by deep feedforward neuralnetworks with piecewise linear activations in terms of the symmetries and thenumber of linear regions that they have. Deep networks are able to sequentiallymap portions of each layer's input-space to the same output. In this way, deepmodels compute functions that react equally to complicated patterns ofdifferent inputs. The compositional structure of these functions enables themto re-use pieces of computation exponentially often in terms of the network'sdepth. This paper investigates the complexity of such compositional maps andcontributes new theoretical results regarding the advantage of depth for neuralnetworks with piecewise linear activation functions. In particular, ouranalysis is not specific to a single family of models, and as an example, weemploy it for rectifier and maxout networks. We improve complexity bounds frompre-existing work and investigate the behavior of units in higher layers.
arxiv-6000-148 | Sparse Illumination Learning and Transfer for Single-Sample Face Recognition with Image Corruption and Misalignment | http://arxiv.org/abs/1402.1879 | author:Liansheng Zhuang, Tsung-Han Chan, Allen Y. Yang, S. Shankar Sastry, Yi Ma category:cs.CV published:2014-02-08 summary:Single-sample face recognition is one of the most challenging problems inface recognition. We propose a novel algorithm to address this problem based ona sparse representation based classification (SRC) framework. The new algorithmis robust to image misalignment and pixel corruption, and is able to reducerequired gallery images to one sample per class. To compensate for the missingillumination information traditionally provided by multiple gallery images, asparse illumination learning and transfer (SILT) technique is introduced. Theillumination in SILT is learned by fitting illumination examples of auxiliaryface images from one or more additional subjects with a sparsely-usedillumination dictionary. By enforcing a sparse representation of the queryimage in the illumination dictionary, the SILT can effectively recover andtransfer the illumination and pose information from the alignment stage to therecognition stage. Our extensive experiments have demonstrated that the newalgorithms significantly outperform the state of the art in the single-sampleregime and with less restrictions. In particular, the single-sample facealignment accuracy is comparable to that of the well-known Deformable SRCalgorithm using multiple gallery images per class. Furthermore, the facerecognition accuracy exceeds those of the SRC and Extended SRC algorithms usinghand labeled alignment initialization.
arxiv-6000-149 | Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP Modeling | http://arxiv.org/abs/1402.1801 | author:SayedMasoud Hashemi, Soosan Beheshti, Patrick R. Gill, Narinder S. Paul, Richard S. C. Cobbold category:stat.AP cs.CV published:2014-02-08 summary:Ultra low radiation dose in X-ray Computed Tomography (CT) is an importantclinical objective in order to minimize the risk of carcinogenesis. CompressedSensing (CS) enables significant reductions in radiation dose to be achieved byproducing diagnostic images from a limited number of CT projections. However,the excessive computation time that conventional CS-based CT reconstructiontypically requires has limited clinical implementation. In this paper, we firstdemonstrate that a thorough analysis of CT reconstruction through a Maximum aPosteriori objective function results in a weighted compressive sensingproblem. This analysis enables us to formulate a low dose fan beam and helicalcone beam CT reconstruction. Subsequently, we provide an efficient solution tothe formulated CS problem based on a Fast Composite Splitting Algorithm-LatentExpected Maximization (FCSA-LEM) algorithm. In the proposed method we usepseudo polar Fourier transform as the measurement matrix in order to decreasethe computational complexity; and rebinning of the projections to parallel raysin order to extend its application to fan beam and helical cone beam scans. Theweight involved in the proposed weighted CS model, denoted by Error AdaptationWeight (EAW), is calculated based on the statistical characteristics of CTreconstruction and is a function of Poisson measurement noise and rebinninginterpolation error. Simulation results show that low computational complexityof the proposed method made the fast recovery of the CT images possible andusing EAW reduces the reconstruction error by one order of magnitude. Recoveryof a high quality 512$\times$ 512 image was achieved in less than 20 sec on adesktop computer without numerical optimizations.
arxiv-6000-150 | Performance of Hull-Detection Algorithms For Proton Computed Tomography Reconstruction | http://arxiv.org/abs/1402.1720 | author:Blake Schultze, Micah Witt, Yair Censor, Reinhard Schulte, Keith Evan Schubert category:cs.CV physics.med-ph published:2014-02-07 summary:Proton computed tomography (pCT) is a novel imaging modality developed forpatients receiving proton radiation therapy. The purpose of this work was toinvestigate hull-detection algorithms used for preconditioning of the large andsparse linear system of equations that needs to be solved for pCT imagereconstruction. The hull-detection algorithms investigated here includedsilhouette/space carving (SC), modified silhouette/space carving (MSC), andspace modeling (SM). Each was compared to the cone-beam version of filteredbackprojection (FBP) used for hull-detection. Data for testing these algorithmsincluded simulated data sets of a digital head phantom and an experimental dataset of a pediatric head phantom obtained with a pCT scanner prototype at LomaLinda University Medical Center. SC was the fastest algorithm, exceeding thespeed of FBP by more than 100 times. FBP was most sensitive to the presence ofnoise. Ongoing work will focus on optimizing threshold parameters in order todefine a fast and efficient method for hull-detection in pCT imagereconstruction.
arxiv-6000-151 | Two-stage Sampled Learning Theory on Distributions | http://arxiv.org/abs/1402.1754 | author:Zoltan Szabo, Arthur Gretton, Barnabas Poczos, Bharath Sriperumbudur category:math.ST cs.LG math.FA stat.ML stat.TH G.3; I.2.6 published:2014-02-07 summary:We focus on the distribution regression problem: regressing to a real-valuedresponse from a probability distribution. Although there exist a large numberof similarity measures between distributions, very little is known about theirgeneralization performance in specific learning tasks. Learning problemsformulated on distributions have an inherent two-stage sampled difficulty: inpractice only samples from sampled distributions are observable, and one has tobuild an estimate on similarities computed between sets of points. To the bestof our knowledge, the only existing method with consistency guarantees fordistribution regression requires kernel density estimation as an intermediatestep (which suffers from slow convergence issues in high dimensions), and thedomain of the distributions to be compact Euclidean. In this paper, we providetheoretical guarantees for a remarkably simple algorithmic alternative to solvethe distribution regression problem: embed the distributions to a reproducingkernel Hilbert space, and learn a ridge regressor from the embeddings to theoutputs. Our main contribution is to prove the consistency of this technique inthe two-stage sampled setting under mild conditions (on separable, topologicaldomains endowed with kernels). For a given total number of observations, wederive convergence rates as an explicit function of the problem difficulty. Asa special case, we answer a 15-year-old open question: we establish theconsistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002]in regression, and cover more recent kernels on distributions, including thosedue to [Christmann and Steinwart, 2010].
arxiv-6000-152 | Binary Excess Risk for Smooth Convex Surrogates | http://arxiv.org/abs/1402.1792 | author:Mehrdad Mahdavi, Lijun Zhang, Rong Jin category:cs.LG stat.ML published:2014-02-07 summary:In statistical learning theory, convex surrogates of the 0-1 loss are highlypreferred because of the computational and theoretical virtues that convexitybrings in. This is of more importance if we consider smooth surrogates aswitnessed by the fact that the smoothness is further beneficial bothcomputationally- by attaining an {\it optimal} convergence rate foroptimization, and in a statistical sense- by providing an improved {\itoptimistic} rate for generalization bound. In this paper we investigate thesmoothness property from the viewpoint of statistical consistency and show howit affects the binary excess risk. We show that in contrast to optimization andgeneralization errors that favor the choice of smooth surrogate loss, thesmoothness of loss function may degrade the binary excess risk. Motivated bythis negative result, we provide a unified analysis that integratesoptimization error, generalization bound, and the error in translating convexexcess risk into a binary excess risk when examining the impact of smoothnesson the binary excess risk. We show that under favorable conditions appropriatechoice of smooth convex loss will result in a binary excess risk that is betterthan $O(1/\sqrt{n})$.
arxiv-6000-153 | Active Clustering with Model-Based Uncertainty Reduction | http://arxiv.org/abs/1402.1783 | author:Caiming Xiong, David Johnson, Jason J. Corso category:cs.LG cs.CV stat.ML 62H30 published:2014-02-07 summary:Semi-supervised clustering seeks to augment traditional clustering methods byincorporating side information provided via human expertise in order toincrease the semantic meaningfulness of the resulting clusters. However, mostcurrent methods are \emph{passive} in the sense that the side information isprovided beforehand and selected randomly. This may require a large number ofconstraints, some of which could be redundant, unnecessary, or even detrimentalto the clustering results. Thus in order to scale such semi-supervisedalgorithms to larger problems it is desirable to pursue an \emph{active}clustering method---i.e. an algorithm that maximizes the effectiveness of theavailable human labor by only requesting human input where it will have thegreatest impact. Here, we propose a novel online framework for activesemi-supervised spectral clustering that selects pairwise constraints asclustering proceeds, based on the principle of uncertainty reduction. Using afirst-order Taylor expansion, we decompose the expected uncertainty reductionproblem into a gradient and a step-scale, computed via an application of matrixperturbation theory and cluster-assignment entropy, respectively. The resultingmodel is used to estimate the uncertainty reduction potential of each sample inthe dataset. We then present the human user with pairwise queries with respectto only the best candidate sample. We evaluate our method using three differentimage datasets (faces, leaves and dogs), a set of common UCI machine learningdatasets and a gene dataset. The results validate our decomposition formulationand show that our method is consistently superior to existing state-of-the-arttechniques, as well as being robust to noise and to unknown numbers ofclusters.
arxiv-6000-154 | On the Prediction Performance of the Lasso | http://arxiv.org/abs/1402.1700 | author:Arnak S. Dalalyan, Mohamed Hebiri, Johannes Lederer category:math.ST stat.ML stat.TH published:2014-02-07 summary:Although the Lasso has been extensively studied, the relationship between itsprediction performance and the correlations of the covariates is not fullyunderstood. In this paper, we give new insights into this relationship in thecontext of multiple linear regression. We show, in particular, that theincorporation of a simple correlation measure into the tuning parameter leadsto a nearly optimal prediction performance of the Lasso even for highlycorrelated covariates. However, we also reveal that for moderately correlatedcovariates, the prediction performance of the Lasso can be mediocreirrespective of the choice of the tuning parameter. For the illustration of ourapproach with an important application, we deduce nearly optimal rates for theleast-squares estimator with total variation penalty.
arxiv-6000-155 | Evaluation of YTEX and MetaMap for clinical concept recognition | http://arxiv.org/abs/1402.1668 | author:John David Osborne, Binod Gyawali, Thamar Solorio category:cs.IR cs.CL 68 published:2014-02-07 summary:We used MetaMap and YTEX as a basis for the construc- tion of two separatesystems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], therecognition of clinical concepts. No modifications were directly made to thesesystems, but output concepts were filtered using stop concepts, stop concepttext and UMLS semantic type. Con- cept boundaries were also adjusted using asmall collection of rules to increase precision on the strict task. OverallMetaMap had better per- formance than YTEX on the strict task, primarily due toa 20% perfor- mance improvement in precision. In the relaxed task YTEX hadbetter performance in both precision and recall giving it an overall F-Score4.6% higher than MetaMap on the test data. Our results also indicated a 1.3%higher accuracy for YTEX in UMLS CUI mapping.
arxiv-6000-156 | Dissimilarity-based Ensembles for Multiple Instance Learning | http://arxiv.org/abs/1402.1349 | author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.LG published:2014-02-06 summary:In multiple instance learning, objects are sets (bags) of feature vectors(instances) rather than individual feature vectors. In this paper we addressthe problem of how these bags can best be represented. Two standard approachesare to use (dis)similarities between bags and prototype bags, or between bagsand prototype instances. The first approach results in a relativelylow-dimensional representation determined by the number of training bags, whilethe second approach results in a relatively high-dimensional representation,determined by the total number of instances in the training set. In this papera third, intermediate approach is proposed, which links the two approaches andcombines their strengths. Our classifier is inspired by a random subspaceensemble, and considers subspaces of the dissimilarity space, defined bysubsets of instances, as prototypes. We provide guidelines for using such anensemble, and show state-of-the-art performances on a range of multipleinstance learning problems.
arxiv-6000-157 | Real-time Pedestrian Surveillance with Top View Cumulative Grids | http://arxiv.org/abs/1402.1359 | author:Kai Berger, Jeyarajan Thiyagalingam category:cs.CV published:2014-02-06 summary:This manuscript presents an efficient approach to map pedestrian surveillancefootage to an aerial view for global assessment of features. The analysis ofthe footages relies on low level computer vision and enable real-timesurveillance. While we neglect object tracking, we introduce cumulative gridson top view scene flow visualization to highlight situations of interest in thefootage. Our approach is tested on multiview footage both from RGB cameras and,for the first time in the field, on RGB-D-sensors.
arxiv-6000-158 | Quantile Representation for Indirect Immunofluorescence Image Classification | http://arxiv.org/abs/1402.1371 | author:David M. J. Tax, Veronika Cheplygina, Marco Loog category:cs.CV published:2014-02-06 summary:In the diagnosis of autoimmune diseases, an important task is to classifyimages of slides containing several HEp-2 cells. All cells from one slide sharethe same label, and by classifying cells from one slide independently, someinformation on the global image quality and intensity is lost. Considering onewhole slide as a collection (a bag) of feature vectors, however, poses theproblem of how to handle this bag. A simple, and surprisingly effective,approach is to summarize the bag of feature vectors by a few quantile valuesper feature. This characterizes the full distribution of all instances, therebyassuming that all instances in a bag are informative. This representation isparticularly useful when each bag contains many feature vectors, which is thecase in the classification of the immunofluorescence images. Experiments on theclassification of indirect immunofluorescence images show the usefulness ofthis approach.
arxiv-6000-159 | A Three-Phase Search Approach for the Quadratic Minimum Spanning Tree Problem | http://arxiv.org/abs/1402.1379 | author:Zhang-Hua Fu, Jin-Kao Hao category:cs.DS cs.NE published:2014-02-06 summary:Given an undirected graph with costs associated with each edge as well aseach pair of edges, the quadratic minimum spanning tree problem (QMSTP)consists of determining a spanning tree of minimum total cost. This problem canbe used to model many real-life network design applications, in which bothrouting and interference costs should be considered. For this problem, wepropose a three-phase search approach named TPS, which integrates 1) adescent-based neighborhood search phase using two different move operators toreach a local optimum from a given starting solution, 2) a local optimaexploring phase to discover nearby local optima within a given regional searcharea, and 3) a perturbation-based diversification phase to jump out of thecurrent regional search area. Additionally, we introduce dedicated techniquesto reduce the neighborhood to explore and streamline the neighborhoodevaluations. Computational experiments based on hundreds of representativebenchmarks show that TPS produces highly competitive results with respect tothe best performing approaches in the literature by improving the best knownresults for 31 instances and matching the best known results for the remaininginstances only except two cases. Critical elements of the proposed algorithmsare analyzed.
arxiv-6000-160 | An Autoencoder Approach to Learning Bilingual Word Representations | http://arxiv.org/abs/1402.1454 | author:Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M. Khapra, Balaraman Ravindran, Vikas Raykar, Amrita Saha category:cs.CL cs.LG stat.ML published:2014-02-06 summary:Cross-language learning allows us to use training data from one language tobuild models for a different language. Many approaches to bilingual learningrequire that we have word-level alignment of sentences from parallel corpora.In this work we explore the use of autoencoder-based methods for cross-languagelearning of vectorial word representations that are aligned between twolanguages, while not relying on word-level alignments. We show that by simplylearning to reconstruct the bag-of-words representations of aligned sentences,within and between languages, we can in fact learn high-quality representationsand do without word alignments. Since training autoencoders on wordobservations presents certain computational issues, we propose and comparedifferent variations adapted to this setting. We also propose an explicitcorrelation maximizing regularizer that leads to significant improvement in theperformance. We empirically investigate the success of our approach on theproblem of cross-language test classification, where a classifier trained on agiven language (e.g., English) must learn to generalize to a different language(e.g., German). These experiments demonstrate that our approaches arecompetitive with the state-of-the-art, achieving up to 10-14 percentage pointimprovements over the best reported results on this task.
arxiv-6000-161 | Near-Optimal Joint Object Matching via Convex Relaxation | http://arxiv.org/abs/1402.1473 | author:Yuxin Chen, Leonidas J. Guibas, Qi-Xing Huang category:cs.LG cs.CV cs.IT math.IT math.OC stat.ML published:2014-02-06 summary:Joint matching over a collection of objects aims at aggregating informationfrom a large collection of similar instances (e.g. images, graphs, shapes) toimprove maps between pairs of them. Given multiple matches computed between afew object pairs in isolation, the goal is to recover an entire collection ofmaps that are (1) globally consistent, and (2) close to the provided maps ---and under certain conditions provably the ground-truth maps. Despite recentadvances on this problem, the best-known recovery guarantees are limited to asmall constant barrier --- none of the existing methods find theoreticalsupport when more than $50\%$ of input correspondences are corrupted. Moreover,prior approaches focus mostly on fully similar objects, while it is practicallymore demanding to match instances that are only partially similar to eachother. In this paper, we develop an algorithm to jointly match multiple objects thatexhibit only partial similarities, given a few pairwise matches that aredensely corrupted. Specifically, we propose to recover the ground-truth mapsvia a parameter-free convex program called MatchLift, following a spectralmethod that pre-estimates the total number of distinct elements to be matched.Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.in the asymptotic regime it is guaranteed to work even when a dominant fraction$1-\Theta\left(\frac{\log^{2}n}{\sqrt{n}}\right)$ of the input maps behave likerandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,namely, perfect matching can be achieved as soon as the provided maps form aconnected map graph. We evaluate the proposed algorithm on various benchmarkdata sets including synthetic examples and real-world examples, all of whichconfirm the practical applicability of MatchLift.
arxiv-6000-162 | Tracking via Motion Estimation with Physically Motivated Inter-Region Constraints | http://arxiv.org/abs/1402.1503 | author:Omar Arif, Ganesh Sundaramoorthi, Byung-Woo Hong, Anthony Yezzi category:cs.CV published:2014-02-06 summary:In this paper, we propose a method for tracking structures (e.g., ventriclesand myocardium) in cardiac images (e.g., magnetic resonance) by propagatingforward in time a previous estimate of the structures via a new deformationestimation scheme that is motivated by physical constraints of fluid motion.The method employs within structure motion estimation (so that differingmotions among different structures are not mixed) while simultaneouslysatisfying the physical constraint in fluid motion that at the interfacebetween a fluid and a medium, the normal component of the fluid's motion mustmatch the normal component of the motion of the medium. We show how to estimatethe motion according to the previous considerations in a variational framework,and in particular, show that these conditions lead to PDEs with boundaryconditions at the interface that resemble Robin boundary conditions and inducecoupling between structures. We illustrate the use of this motion estimationscheme in propagating a segmentation across frames and show that it leads tomore accurate segmentation than traditional motion estimation that does notmake use of physical constraints. Further, the method is naturally suited tointeractive segmentation methods, which are prominently used in practice incommercial applications for cardiac analysis, where typically a segmentationfrom the previous frame is used to predict a segmentation in the next frame. Weshow that our propagation scheme reduces the amount of user interaction bypredicting more accurate segmentations than commonly used and recentinteractive commercial techniques.
arxiv-6000-163 | A Cellular Automata based Optimal Edge Detection Technique using Twenty-Five Neighborhood Model | http://arxiv.org/abs/1402.1348 | author:Deepak Ranjan Nayak, Sumit Kumar Sahu, Jahangir Mohammed category:cs.CV published:2014-02-06 summary:Cellular Automata (CA) are common and most simple models of parallelcomputations. Edge detection is one of the crucial task in image processing,especially in processing biological and medical images. CA can be successfullyapplied in image processing. This paper presents a new method for edgedetection of binary images based on two dimensional twenty five neighborhoodcellular automata. The method considers only linear rules of CA for extractionof edges under null boundary condition. The performance of this approach iscompared with some existing edge detection techniques. This comparison showsthat the proposed method to be very promising for edge detection of binaryimages. All the algorithms and results used in this paper are prepared inMATLAB.
arxiv-6000-164 | Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models | http://arxiv.org/abs/1402.1389 | author:Yarin Gal, Mark van der Wilk, Carl E. Rasmussen category:stat.ML cs.LG published:2014-02-06 summary:Gaussian processes (GPs) are a powerful tool for probabilistic inference overfunctions. They have been applied to both regression and non-lineardimensionality reduction, and offer desirable properties such as uncertaintyestimates, robustness to over-fitting, and principled ways for tuninghyper-parameters. However the scalability of these models to big datasetsremains an active topic of research. We introduce a novel re-parametrisation ofvariational inference for sparse GP regression and latent variable models thatallows for an efficient distributed algorithm. This is done by exploiting thedecoupling of the data given the inducing points to re-formulate the evidencelower bound in a Map-Reduce setting. We show that the inference scales wellwith data and computational resources, while preserving a balanced distributionof the load among the nodes. We further demonstrate the utility in scalingGaussian processes to big data. We show that GP performance improves withincreasing amounts of data in regression (on flight data with 2 millionrecords) and latent variable modelling (on MNIST). The results show that GPsperform better than many common models often used for big data.
arxiv-6000-165 | Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial | http://arxiv.org/abs/1402.1412 | author:Yarin Gal, Mark van der Wilk category:stat.ML published:2014-02-06 summary:In this tutorial we explain the inference procedures developed for the sparseGaussian process (GP) regression and Gaussian process latent variable model(GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias &Lawrence (2010) is brief, hence getting a full picture of it requirescollecting results from several different sources and a substantial amount ofalgebra to fill-in the gaps. Our main goal is thus to collect all the resultsand full derivations into one place to help speed up understanding this work.In doing so we present a re-parametrisation of the inference that allows it tobe carried out in parallel. A secondary goal for this document is, therefore,to accompany our paper and open-source implementation of the parallel inferencescheme for the models. We hope that this document will bridge the gap betweenthe equations as implemented in code and those published in the originalpapers, in order to make it easier to extend existing work. We assume priorknowledge of Gaussian processes and variational inference, but we also includereferences for further reading where appropriate.
arxiv-6000-166 | An Estimation Method of Measuring Image Quality for Compressed Images of Human Face | http://arxiv.org/abs/1402.1331 | author:Abhishek Bhattacharya, Tanusree Chatterjee category:cs.CV published:2014-02-06 summary:Nowadays digital image compression and decompression techniques are very muchimportant. So our aim is to calculate the quality of face and other regions ofthe compressed image with respect to the original image. Image segmentation istypically used to locate objects and boundaries (lines, curves etc.)in images.After segmentation the image is changed into something which is more meaningfulto analyze. Using Universal Image Quality Index(Q),Structural SimilarityIndex(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can beshown that face region is less compressed than any other region of the image.
arxiv-6000-167 | Localized epidemic detection in networks with overwhelming noise | http://arxiv.org/abs/1402.1263 | author:Eli A. Meirom, Chris Milling, Constantine Caramanis, Shie Mannor, Ariel Orda, Sanjay Shakkottai category:cs.SI cs.LG published:2014-02-06 summary:We consider the problem of detecting an epidemic in a population whereindividual diagnoses are extremely noisy. The motivation for this problem isthe plethora of examples (influenza strains in humans, or computer viruses insmartphones, etc.) where reliable diagnoses are scarce, but noisy dataplentiful. In flu/phone-viruses, exceedingly few infected people/phones areprofessionally diagnosed (only a small fraction go to a doctor) but lessreliable secondary signatures (e.g., people staying home, orgreater-than-typical upload activity) are more readily available. Thesesecondary data are often plagued by unreliability: many people with the flu donot stay home, and many people that stay home do not have the flu. This paperidentifies the precise regime where knowledge of the contact network enablesfinding the needle in the haystack: we provide a distributed, efficient androbust algorithm that can correctly identify the existence of a spreadingepidemic from highly unreliable local data. Our algorithm requires onlylocal-neighbor knowledge of this graph, and in a broad array of settings thatwe describe, succeeds even when false negatives and false positives make up anoverwhelming fraction of the data available. Our results show it succeeds inthe presence of partial information about the contact network, and also whenthere is not a single "patient zero", but rather many (hundreds, in ourexamples) of initial patient-zeroes, spread across the graph.
arxiv-6000-168 | Multispectral Palmprint Encoding and Recognition | http://arxiv.org/abs/1402.2941 | author:Zohaib Khan, Faisal Shafait, Yiqun Hu, Ajmal Mian category:cs.CV published:2014-02-06 summary:Palmprints are emerging as a new entity in multi-modal biometrics for humanidentification and verification. Multispectral palmprint images captured in thevisible and infrared spectrum not only contain the wrinkles and ridge structureof a palm, but also the underlying pattern of veins; making them a highlydiscriminating biometric identifier. In this paper, we propose a featureencoding scheme for robust and highly accurate representation and matching ofmultispectral palmprints. To facilitate compact storage of the feature, wedesign a binary hash table structure that allows for efficient matching inlarge databases. Comprehensive experiments for both identification andverification scenarios are performed on two public datasets -- one capturedwith a contact-based sensor (PolyU dataset), and the other with a contact-freesensor (CASIA dataset). Recognition results in various experimental setups showthat the proposed method consistently outperforms existing state-of-the-artmethods. Error rates achieved by our method (0.003% on PolyU and 0.2% on CASIA)are the lowest reported in literature on both dataset and clearly indicate theviability of palmprint as a reliable and promising biometric. All source codesare publicly available.
arxiv-6000-169 | Dictionary Learning over Distributed Models | http://arxiv.org/abs/1402.1515 | author:Jianshu Chen, Zaid J. Towfic, Ali H. Sayed category:cs.LG cs.DC published:2014-02-06 summary:In this paper, we consider learning dictionary models over a network ofagents, where each agent is only in charge of a portion of the dictionaryelements. This formulation is relevant in Big Data scenarios where largedictionary models may be spread over different spatial locations and it is notfeasible to aggregate all dictionaries in one location due to communication andprivacy considerations. We first show that the dual function of the inferenceproblem is an aggregation of individual cost functions associated withdifferent agents, which can then be minimized efficiently by means of diffusionstrategies. The collaborative inference step generates dual variables that areused by the agents to update their dictionaries without the need to share thesedictionaries or even the coefficient models for the training data. This is apowerful property that leads to an effective distributed procedure for learningdictionaries over large networks (e.g., hundreds of agents in our experiments).Furthermore, the proposed learning strategy operates in an online manner and isable to respond to streaming data, where each data sample is presented to thenetwork once.
arxiv-6000-170 | Phase transitions and sample complexity in Bayes-optimal matrix factorization | http://arxiv.org/abs/1402.1298 | author:Yoshiyuki Kabashima, Florent Krzakala, Marc Mézard, Ayaka Sakata, Lenka Zdeborová category:cs.NA cs.IT cs.LG math.IT stat.ML published:2014-02-06 summary:We analyse the matrix factorization problem. Given a noisy measurement of aproduct of two matrices, the problem is to estimate back the original matrices.It arises in many applications such as dictionary learning, blind matrixcalibration, sparse principal component analysis, blind source separation, lowrank matrix completion, robust principal component analysis or factor analysis.It is also important in machine learning: unsupervised representation learningcan often be studied through matrix factorization. We use the tools ofstatistical mechanics - the cavity and replica methods - to analyze theachievability and computational tractability of the inference problems in thesetting of Bayes-optimal inference, which amounts to assuming that the twomatrices have random independent elements generated from some knowndistribution, and this information is available to the inference algorithm. Inthis setting, we compute the minimal mean-squared-error achievable in principlein any computational time, and the error that can be achieved by an efficientapproximate message passing algorithm. The computation is based on theasymptotic state-evolution analysis of the algorithm. The performance that ouranalysis predicts, both in terms of the achieved mean-squared-error, and interms of sample complexity, is extremely promising and motivating for a furtherdevelopment of the algorithm.
arxiv-6000-171 | Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices | http://arxiv.org/abs/1402.1267 | author:Yudong Chen, Jiaming Xu category:stat.ML math.ST stat.TH published:2014-02-06 summary:We consider two closely related problems: planted clustering and submatrixlocalization. The planted clustering problem assumes that a random graph isgenerated based on some underlying clusters of the nodes; the task is torecover these clusters given the graph. The submatrix localization problemconcerns locating hidden submatrices with elevated means inside a largereal-valued random matrix. Of particular interest is the setting where thenumber of clusters/submatrices is allowed to grow unbounded with the problemsize. These formulations cover several classical models such as planted clique,planted densest subgraph, planted partition, planted coloring, and stochasticblock model, which are widely used for studying community detection andclustering/bi-clustering. For both problems, we show that the space of the model parameters(cluster/submatrix size, cluster density, and submatrix mean) can bepartitioned into four disjoint regions corresponding to decreasing statisticaland computational complexities: (1) the \emph{impossible} regime, where allalgorithms fail; (2) the \emph{hard} regime, where the computationallyexpensive Maximum Likelihood Estimator (MLE) succeeds; (3) the \emph{easy}regime, where the polynomial-time convexified MLE succeeds; (4) the\emph{simple} regime, where a simple counting/thresholding procedure succeeds.Moreover, we show that each of these algorithms provably fails in the previousharder regimes. Our theorems establish the minimax recovery limit, which are tight up toconstants and hold with a growing number of clusters/submatrices, and provide astronger performance guarantee than previously known for polynomial-timealgorithms. Our study demonstrates the tradeoffs between statistical andcomputational considerations, and suggests that the minimax recovery limit maynot be achievable by polynomial-time algorithms.
arxiv-6000-172 | Dual Query: Practical Private Query Release for High Dimensional Data | http://arxiv.org/abs/1402.1526 | author:Marco Gaboardi, Emilio Jesús Gallego Arias, Justin Hsu, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.CR cs.DB cs.LG published:2014-02-06 summary:We present a practical, differentially private algorithm for answering alarge number of queries on high dimensional datasets. Like all algorithms forthis task, ours necessarily has worst-case complexity exponential in thedimension of the data. However, our algorithm packages the computationally hardstep into a concisely defined integer program, which can be solvednon-privately using standard solvers. We prove accuracy and privacy theoremsfor our algorithm, and then demonstrate experimentally that our algorithmperforms well in practice. For example, our algorithm can efficiently andaccurately answer millions of queries on the Netflix dataset, which has over17,000 attributes; this is an improvement on the state of the art by multipleorders of magnitude.
arxiv-6000-173 | Comparative analysis of common edge detection techniques in context of object extraction | http://arxiv.org/abs/1405.6132 | author:S. K. Katiyar, P. V. Arun category:cs.CV published:2014-02-05 summary:Edges characterize boundaries and are therefore a problem of practicalimportance in remote sensing.In this paper a comparative study of various edgedetection techniques and band wise analysis of these algorithms in the contextof object extraction with regard to remote sensing satellite images from theIndian Remote Sensing Satellite (IRS) sensors LISS 3, LISS 4 and Cartosat1 aswell as Google Earth is presented.
arxiv-6000-174 | Quantum Cybernetics and Complex Quantum Systems Science - A Quantum Connectionist Exploration | http://arxiv.org/abs/1402.1141 | author:Carlos Pedro Gonçalves category:cs.NE quant-ph published:2014-02-05 summary:Quantum cybernetics and its connections to complex quantum systems science isaddressed from the perspective of complex quantum computing systems. In thisway, the notion of an autonomous quantum computing system is introduced inregards to quantum artificial intelligence, and applied to quantum artificialneural networks, considered as autonomous quantum computing systems, whichleads to a quantum connectionist framework within quantum cybernetics forcomplex quantum computing systems. Several examples of quantum feedforwardneural networks are addressed in regards to Boolean functions' computation,multilayer quantum computation dynamics, entanglement and quantumcomplementarity. The examples provide a framework for a reflection on the roleof quantum artificial neural networks as a general framework for addressingcomplex quantum systems that perform network-based quantum computation,possible consequences are drawn regarding quantum technologies, as well asfundamental research in complex quantum systems science and quantum biology.
arxiv-6000-175 | Patchwise Joint Sparse Tracking with Occlusion Detection | http://arxiv.org/abs/1402.0978 | author:Ali Zarezade, Hamid R. Rabiee, Ali Soltani-Farani, Ahmad Khajenezhad category:cs.CV published:2014-02-05 summary:This paper presents a robust tracking approach to handle challenges such asocclusion and appearance change. Here, the target is partitioned into a numberof patches. Then, the appearance of each patch is modeled using a dictionarycomposed of corresponding target patches in previous frames. In each frame, thetarget is found among a set of candidates generated by a particle filter, via alikelihood measure that is shown to be proportional to the sum ofpatch-reconstruction errors of each candidate. Since the target's appearanceoften changes slowly in a video sequence, it is assumed that the target in thecurrent frame and the best candidates of a small number of previous frames,belong to a common subspace. This is imposed using joint sparse representationto enforce the target and previous best candidates to have a common sparsitypattern. Moreover, an occlusion detection scheme is proposed that usespatch-reconstruction errors and a prior probability of occlusion, extractedfrom an adaptive Markov chain, to calculate the probability of occlusion perpatch. In each frame, occluded patches are excluded when updating thedictionary. Extensive experimental results on several challenging sequencesshows that the proposed method outperforms state-of-the-art trackers.
arxiv-6000-176 | An enhanced neural network based approach towards object extraction | http://arxiv.org/abs/1405.6137 | author:S. K. Katiyar, P. V. Arun category:cs.CV cs.LG cs.NE published:2014-02-05 summary:The improvements in spectral and spatial resolution of the satellite imageshave facilitated the automatic extraction and identification of the featuresfrom satellite images and aerial photographs. An automatic object extractionmethod is presented for extracting and identifying the various objects fromsatellite images and the accuracy of the system is verified with regard to IRSsatellite images. The system is based on neural network and simulates theprocess of visual interpretation from remote sensing images and hence increasesthe efficiency of image analysis. This approach obtains the basiccharacteristics of the various features and the performance is enhanced by theautomatic learning approach, intelligent interpretation, and intelligentinterpolation. The major advantage of the method is its simplicity and that thesystem identifies the features not only based on pixel value but also based onthe shape, haralick features etc of the objects. Further the system allowsflexibility for identifying the features within the same category based on sizeand shape. The successful application of the system verified its effectivenessand the accuracy of the system were assessed by ground truth verification.
arxiv-6000-177 | Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition | http://arxiv.org/abs/1402.1128 | author:Haşim Sak, Andrew Senior, Françoise Beaufays category:cs.NE cs.CL cs.LG stat.ML published:2014-02-05 summary:Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)architecture that has been designed to address the vanishing and explodinggradient problems of conventional RNNs. Unlike feedforward neural networks,RNNs have cyclic connections making them powerful for modeling sequences. Theyhave been successfully used for sequence labeling and sequence predictiontasks, such as handwriting recognition, language modeling, phonetic labeling ofacoustic frames. However, in contrast to the deep neural networks, the use ofRNNs in speech recognition has been limited to phone recognition in small scaletasks. In this paper, we present novel LSTM based RNN architectures which makemore effective use of model parameters to train acoustic models for largevocabulary speech recognition. We train and compare LSTM, RNN and DNN models atvarious numbers of parameters and configurations. We show that LSTM modelsconverge quickly and give state of the art speech recognition performance forrelatively small sized models.
arxiv-6000-178 | Learning Ordered Representations with Nested Dropout | http://arxiv.org/abs/1402.0915 | author:Oren Rippel, Michael A. Gelbart, Ryan P. Adams category:stat.ML cs.LG published:2014-02-05 summary:In this paper, we study ordered representations of data in which differentdimensions have different degrees of importance. To learn these representationswe introduce nested dropout, a procedure for stochastically removing coherentnested sets of hidden units in a neural network. We first present a sequence oftheoretical results in the simple case of a semi-linear autoencoder. Werigorously show that the application of nested dropout enforces identifiabilityof the units, which leads to an exact equivalence with PCA. We then extend thealgorithm to deep models and demonstrate the relevance of orderedrepresentations to a number of applications. Specifically, we use the orderedproperty of the learned codes to construct hash-based data structures thatpermit very fast retrieval, achieving retrieval in time logarithmic in thedatabase size and independent of the dimensionality of the representation. Thisallows codes that are hundreds of times longer than currently feasible forretrieval. We therefore avoid the diminished quality associated with shortcodes, while still performing retrieval that is competitive in speed withexisting methods. We also show that ordered representations are a promising wayto learn adaptive compression for efficient online data reconstruction.
arxiv-6000-179 | An evolutionary computational based approach towards automatic image registration | http://arxiv.org/abs/1405.6136 | author:P. V. Arun, S. K. Katiyar category:cs.CV cs.NE published:2014-02-05 summary:Image registration is a key component of various image processing operationswhich involve the analysis of different image data sets. Automatic imageregistration domains have witnessed the application of many intelligentmethodologies over the past decade; however inability to properly model objectshape as well as contextual information had limited the attainable accuracy. Inthis paper, we propose a framework for accurate feature shape modeling andadaptive resampling using advanced techniques such as Vector Machines, CellularNeural Network (CNN), SIFT, coreset, and Cellular Automata. CNN has found to beeffective in improving feature matching as well as resampling stages ofregistration and complexity of the approach has been considerably reduced usingcorset optimization The salient features of this work are cellular neuralnetwork approach based SIFT feature point optimisation, adaptive resampling andintelligent object modelling. Developed methodology has been compared withcontemporary methods using different statistical measures. Investigations overvarious satellite images revealed that considerable success was achieved withthe approach. System has dynamically used spectral and spatial information forrepresenting contextual knowledge using CNN-prolog approach. Methodology alsoillustrated to be effective in providing intelligent interpretation andadaptive resampling.
arxiv-6000-180 | Image Acquisition in an Underwater Vision System with NIR and VIS Illumination | http://arxiv.org/abs/1402.1151 | author:Wojciech Biegański, Andrzej Kasiński category:cs.CV published:2014-02-05 summary:The paper describes the image acquisition system able to capture images intwo separated bands of light, used to underwater autonomous navigation. Thechannels are: the visible light spectrum and near infrared spectrum. Thecharacteristics of natural, underwater environment were also described togetherwith the process of the underwater image creation. The results of an experimentwith comparison of selected images acquired in these channels are discussed.
arxiv-6000-181 | Input Warping for Bayesian Optimization of Non-stationary Functions | http://arxiv.org/abs/1402.0929 | author:Jasper Snoek, Kevin Swersky, Richard S. Zemel, Ryan P. Adams category:stat.ML cs.LG published:2014-02-05 summary:Bayesian optimization has proven to be a highly effective methodology for theglobal optimization of unknown, expensive and multimodal functions. The abilityto accurately model distributions over functions is critical to theeffectiveness of Bayesian optimization. Although Gaussian processes provide aflexible prior over functions which can be queried efficiently, there arevarious classes of functions that remain difficult to model. One of the mostfrequently occurring of these is the class of non-stationary functions. Theoptimization of the hyperparameters of machine learning algorithms is a problemdomain in which parameters are often manually transformed a priori, for exampleby optimizing in "log-space," to mitigate the effects of spatially-varyinglength scale. We develop a methodology for automatically learning a wide familyof bijective transformations or warpings of the input space using the Betacumulative distribution function. We further extend the warping framework tomulti-task Bayesian optimization so that multiple tasks can be warped into ajointly stationary space. On a set of challenging benchmark optimization tasks,we observe that the inclusion of warping greatly improves on thestate-of-the-art, producing better results faster and more reliably.
arxiv-6000-182 | Cellular Automata based adaptive resampling technique for the processing of remotely sensed imagery | http://arxiv.org/abs/1405.6135 | author:S. K. Katiyar, P. V. Arun category:cs.CV published:2014-02-05 summary:Resampling techniques are being widely used at different stages of satelliteimage processing. The existing methodologies cannot perfectly recover featuresfrom a completely under sampled image and hence an intelligent adaptiveresampling methodology is required. We address these issues and adopt an errormetric from the available literature to define interpolation quality. We alsopropose a new resampling scheme that adapts itself with regard to the pixel andtexture variation in the image. The proposed CNN based hybrid method has beenfound to perform better than the existing methods as it adapts itself withreference to the image features.
arxiv-6000-183 | A review over the applicability of image entropy in analyses of remote sensing datasets | http://arxiv.org/abs/1405.6133 | author:S. K. Katiyar, P. V. Arun category:cs.CV published:2014-02-05 summary:Entropy is the measure of uncertainty in any data and is adopted formaximisation of mutual information in many remote sensing operations. Theavailability of wide entropy variations motivated us for an investigation overthe suitability preference of these versions to specific operations.
arxiv-6000-184 | An Optimization Method For Slice Interpolation Of Medical Images | http://arxiv.org/abs/1402.0936 | author:Ahmadreza Baghaie, Zeyun Yu category:cs.CV cs.CE published:2014-02-05 summary:Slice interpolation is a fast growing field in medical image processing.Intensity-based interpolation and object-based interpolation are two majorgroups of methods in the literature. In this paper, we describe anobject-oriented, optimization method based on a modified version ofcurvature-based image registration, in which a displacement field is computedfor the missing slice between two known slices and used to interpolate theintensities of the missing slice. The proposed approach is evaluatedquantitatively by using the Mean Squared Difference (MSD) as a metric. Theproduced results also show visual improvement in preserving sharp edges inimages.
arxiv-6000-185 | Generating Extractive Summaries of Scientific Paradigms | http://arxiv.org/abs/1402.0556 | author:Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, Taesun Moon category:cs.IR cs.CL published:2014-02-04 summary:Researchers and scientists increasingly find themselves in the position ofhaving to quickly understand large amounts of technical material. Our goal isto effectively serve this need by using bibliometric text mining andsummarization techniques to generate summaries of scientific literature. Weshow how we can use citations to produce automatically generated, readilyconsumable, technical extractive summaries. We first propose C-LexRank, a modelfor summarizing single scientific articles based on citations, which employscommunity detection and extracts salient information-rich sentences. Next, wefurther extend our experiments to summarize a set of papers, which cover thesame scientific topic. We generate extractive summaries of a set of QuestionAnswering (QA) and Dependency Parsing (DP) papers, their abstracts, and theircitation sentences and show that citations have unique information amenable tocreating a summary.
arxiv-6000-186 | Safe Exploration of State and Action Spaces in Reinforcement Learning | http://arxiv.org/abs/1402.0560 | author:Javier Garcia, Fernando Fernandez category:cs.LG cs.AI published:2014-02-04 summary:In this paper, we consider the important problem of safe exploration inreinforcement learning. While reinforcement learning is well-suited to domainswith complex transition dynamics and high-dimensional state-action spaces, anadditional challenge is posed by the need for safe and efficient exploration.Traditional exploration techniques are not particularly useful for solvingdangerous tasks, where the trial and error process may lead to the selection ofactions whose execution in some states may result in damage to the learningsystem (or any other system). Consequently, when an agent begins an interactionwith a dangerous and high-dimensional state-action space, an important questionarises; namely, that of how to avoid (or at least minimize) damage caused bythe exploration of the state-action space. We introduce the PI-SRL algorithmwhich safely improves suboptimal albeit robust behaviors for continuous stateand action control tasks and which efficiently learns from the experiencegained from the environment. We evaluate the proposed method in four complextasks: automatic car parking, pole-balancing, helicopter hovering, and businessmanagement.
arxiv-6000-187 | Parameterized Complexity Results for Exact Bayesian Network Structure Learning | http://arxiv.org/abs/1402.0558 | author:Sebastian Ordyniak, Stefan Szeider category:cs.AI cs.LG published:2014-02-04 summary:Bayesian network structure learning is the notoriously difficult problem ofdiscovering a Bayesian network that optimally represents a given set oftraining data. In this paper we study the computational worst-case complexityof exact Bayesian network structure learning under graph theoretic restrictionson the (directed) super-structure. The super-structure is an undirected graphthat contains as subgraphs the skeletons of solution networks. We introduce thedirected super-structure as a natural generalization of its undirectedcounterpart. Our results apply to several variants of score-based Bayesiannetwork structure learning where the score of a network decomposes into localscores of its nodes. Results: We show that exact Bayesian network structurelearning can be carried out in non-uniform polynomial time if thesuper-structure has bounded treewidth, and in linear time if in addition thesuper-structure has bounded maximum degree. Furthermore, we show that if thedirected super-structure is acyclic, then exact Bayesian network structurelearning can be carried out in quadratic time. We complement these positiveresults with a number of hardness results. We show that both restrictions(treewidth and degree) are essential and cannot be dropped without loosinguniform polynomial time tractability (subject to a complexity-theoreticassumption). Similarly, exact Bayesian network structure learning remainsNP-hard for "almost acyclic" directed super-structures. Furthermore, we showthat the restrictions remain essential if we do not search for a globallyoptimal network but aim to improve a given network by means of at most k arcadditions, arc deletions, or arc reversals (k-neighborhood local search).
arxiv-6000-188 | Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine Translation | http://arxiv.org/abs/1402.0563 | author:Marta R. Costa-jussà, Carlos A. Henríquez, Rafael E. Banchs category:cs.CL published:2014-02-04 summary:Although, Chinese and Spanish are two of the most spoken languages in theworld, not much research has been done in machine translation for this languagepair. This paper focuses on investigating the state-of-the-art ofChinese-to-Spanish statistical machine translation (SMT), which nowadays is oneof the most popular approaches to machine translation. For this purpose, wereport details of the available parallel corpus which are Basic TravellerExpressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, weconduct experimental work with the largest of these three corpora to explorealternative SMT strategies by means of using a pivot language. Threealternatives are considered for pivoting: cascading, pseudo-corpus andtriangulation. As pivot language, we use either English, Arabic or French.Results show that, for a phrase-based SMT system, English is the best pivotlanguage between Chinese and Spanish. We propose a system output combinationusing the pivot strategies which is capable of outperforming the directtranslation strategy. The main objective of this work is motivating andinvolving the research community to work in this important pair of languagesgiven their demographic impact.
arxiv-6000-189 | A Feature Subset Selection Algorithm Automatic Recommendation Method | http://arxiv.org/abs/1402.0570 | author:Guangtao Wang, Qinbao Song, Heli Sun, Xueying Zhang, Baowen Xu, Yuming Zhou category:cs.LG published:2014-02-04 summary:Many feature subset selection (FSS) algorithms have been proposed, but notall of them are appropriate for a given feature selection problem. At the sametime, so far there is rarely a good way to choose appropriate FSS algorithmsfor the problem at hand. Thus, FSS algorithm automatic recommendation is veryimportant and practically useful. In this paper, a meta learning based FSSalgorithm automatic recommendation method is presented. The proposed methodfirst identifies the data sets that are most similar to the one at hand by thek-nearest neighbor classification algorithm, and the distances among these datasets are calculated based on the commonly-used data set characteristics. Then,it ranks all the candidate FSS algorithms according to their performance onthese similar data sets, and chooses the algorithms with best performance asthe appropriate ones. The performance of the candidate FSS algorithms isevaluated by a multi-criteria metric that takes into account not only theclassification accuracy over the selected features, but also the runtime offeature selection and the number of selected features. The proposedrecommendation method is extensively tested on 115 real world data sets with 22well-known and frequently-used different FSS algorithms for five representativeclassifiers. The results show the effectiveness of our proposed FSS algorithmrecommendation method.
arxiv-6000-190 | Learning to Predict from Textual Data | http://arxiv.org/abs/1402.0574 | author:Kira Radinsky, Sagie Davidovich, Shaul Markovitch category:cs.CL cs.AI cs.IR published:2014-02-04 summary:Given a current news event, we tackle the problem of generating plausiblepredictions of future events it might cause. We present a new methodology formodeling and predicting such future news events using machine learning and datamining techniques. Our Pundit algorithm generalizes examples of causality pairsto infer a causality predictor. To obtain precisely labeled causality examples,we mine 150 years of news articles and apply semantic natural language modelingtechniques to headlines containing certain predefined causality patterns. Forgeneralization, the model uses a vast number of world knowledge ontologies.Empirical evaluation on real news articles shows that our Pundit algorithmperforms as well as non-expert humans.
arxiv-6000-191 | A Survey on Latent Tree Models and Applications | http://arxiv.org/abs/1402.0577 | author:Raphaël Mourad, Christine Sinoquet, Nevin L. Zhang, Tengfei Liu, Philippe Leray category:cs.LG published:2014-02-04 summary:In data analysis, latent variables play a central role because they helpprovide powerful insights into a wide variety of phenomena, ranging frombiological to human sciences. The latent tree model, a particular type ofprobabilistic graphical models, deserves attention. Its simple structure - atree - allows simple and efficient inference, while its latent variablescapture complex relationships. In the past decade, the latent tree model hasbeen subject to significant theoretical and methodological developments. Inthis review, we propose a comprehensive study of this model. First we summarizekey ideas underlying the model. Second we explain how it can be efficientlylearned from data. Third we illustrate its use within three types ofapplications: latent structure discovery, multidimensional clustering, andprobabilistic inference. Finally, we conclude and give promising directions forfuture researches in this field.
arxiv-6000-192 | Natural Language Inference for Arabic Using Extended Tree Edit Distance with Subtrees | http://arxiv.org/abs/1402.0578 | author:Maytham Alabbas, Allan Ramsay category:cs.CL published:2014-02-04 summary:Many natural language processing (NLP) applications require the computationof similarities between pairs of syntactic or semantic trees. Many researchershave used tree edit distance for this task, but this technique suffers from thedrawback that it deals with single node operations only. We have extended thestandard tree edit distance algorithm to deal with subtree transformationoperations as well as single nodes. The extended algorithm with subtreeoperations, TED+ST, is more effective and flexible than the standard algorithm,especially for applications that pay attention to relations among nodes (e.g.in linguistic trees, deleting a modifier subtree should be cheaper than the sumof deleting its components individually). We describe the use of TED+ST forchecking entailment between two Arabic text snippets. The preliminary resultsof using TED+ST were encouraging when compared with two string-based approachesand with the standard algorithm.
arxiv-6000-193 | Topic Segmentation and Labeling in Asynchronous Conversations | http://arxiv.org/abs/1402.0586 | author:Shafiq Rayhan Joty, Giuseppe Carenini, Raymond T Ng category:cs.CL published:2014-02-04 summary:Topic segmentation and labeling is often considered a prerequisite forhigher-level conversation analysis and has been shown to be useful in manyNatural Language Processing (NLP) applications. We present two new corpora ofemail and blog conversations annotated with topics, and evaluate annotatorreliability for the segmentation and labeling tasks in these asynchronousconversations. We propose a complete computational framework for topicsegmentation and labeling in asynchronous conversations. Our approach extendsstate-of-the-art methods by considering a fine-grained structure of anasynchronous conversation, along with other conversational features by applyingrecent graph-based methods for NLP. For topic segmentation, we propose twonovel unsupervised models that exploit the fine-grained conversationalstructure, and a novel graph-theoretic supervised model that combines lexical,conversational and topic features. For topic labeling, we propose two novel(unsupervised) random walk models that respectively capture conversationspecific clues from two different sources: the leading sentences and thefine-grained conversational structure. Empirical evaluation shows that thesegmentation and the labeling performed by our best models beat thestate-of-the-art, and are highly correlated with human annotations.
arxiv-6000-194 | Discovering Latent Network Structure in Point Process Data | http://arxiv.org/abs/1402.0914 | author:Scott W. Linderman, Ryan P. Adams category:stat.ML cs.LG published:2014-02-04 summary:Networks play a central role in modern data analysis, enabling us to reasonabout systems by studying the relationships between their parts. Most often innetwork analysis, the edges are given. However, in many systems it is difficultor impossible to measure the network directly. Examples of latent networksinclude economic interactions linking financial instruments and patterns ofreciprocity in gang violence. In these cases, we are limited to noisyobservations of events associated with each node. To enable analysis of theseimplicit networks, we develop a probabilistic model that combinesmutually-exciting point processes with random graph models. We show how thePoisson superposition principle enables an elegant auxiliary variableformulation and a fully-Bayesian, parallel inference algorithm. We evaluatethis new model empirically on several datasets.
arxiv-6000-195 | Online Stochastic Optimization under Correlated Bandit Feedback | http://arxiv.org/abs/1402.0562 | author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG cs.SY published:2014-02-04 summary:In this paper we consider the problem of online stochastic optimization of alocally smooth function under bandit feedback. We introduce the high-confidencetree (HCT) algorithm, a novel any-time $\mathcal{X}$-armed bandit algorithm,and derive regret bounds matching the performance of existing state-of-the-artin terms of dependency on number of steps and smoothness factor. The mainadvantage of HCT is that it handles the challenging case of correlated rewards,whereas existing methods require that the reward-generating process of each armis an identically and independent distributed (iid) random process. HCT alsoimproves on the state-of-the-art in terms of its memory requirement as well asrequiring a weaker smoothness assumption on the mean-reward function in compareto the previous anytime algorithms. Finally, we discuss how HCT can be appliedto the problem of policy search in reinforcement learning and we reportpreliminary empirical results.
arxiv-6000-196 | Cognitive Aging as Interplay between Hebbian Learning and Criticality | http://arxiv.org/abs/1402.0836 | author:Sakyasingha Dasgupta category:nlin.AO cs.NE q-bio.NC published:2014-02-04 summary:Cognitive ageing seems to be a story of global degradation. As one ages thereare a number of physical, chemical and biological changes that take place.Therefore it is logical to assume that the brain is no exception to thisphenomenon. The principle purpose of this project is to use models of neuraldynamics and learning based on the underlying principle of self-organisedcriticality, to account for the age related cognitive effects. In this regardlearning in neural networks can serve as a model for the acquisition of skillsand knowledge in early development stages i.e. the ageing process andcriticality in the network serves as the optimum state of cognitive abilities.Possible candidate mechanisms for ageing in a neural network are loss ofconnectivity and neurons, increase in the level of noise, reduction in whitematter or more interestingly longer learning history and the competition amongseveral optimization objectives. In this paper we are primarily interested inthe affect of the longer learning history on memory and thus the optimality inthe brain. Hence it is hypothesized that prolonged learning in the form ofassociative memory patterns can destroy the state of criticality in thenetwork. We base our model on Tsodyks and Markrams [49] model of dynamicsynapses, in the process to explore the effect of combining standard Hebbianlearning with the phenomenon of Self-organised criticality. The project mainlyconsists of evaluations and simulations of networks of integrate andfire-neurons that have been subjected to various combinations of neural-levelageing effects, with the aim of establishing the primary hypothesis andunderstanding the decline of cognitive abilities due to ageing, using one ofits important characteristics, a longer learning history.
arxiv-6000-197 | Sequential Model-Based Ensemble Optimization | http://arxiv.org/abs/1402.0796 | author:Alexandre Lacoste, Hugo Larochelle, François Laviolette, Mario Marchand category:cs.LG stat.ML published:2014-02-04 summary:One of the most tedious tasks in the application of machine learning is modelselection, i.e. hyperparameter selection. Fortunately, recent progress has beenmade in the automation of this process, through the use of sequentialmodel-based optimization (SMBO) methods. This can be used to optimize across-validation performance of a learning algorithm over the value of itshyperparameters. However, it is well known that ensembles of learned modelsalmost consistently outperform a single model, even if properly selected. Inthis paper, we thus propose an extension of SMBO methods that automaticallyconstructs such ensembles. This method builds on a recently proposed ensembleconstruction paradigm known as agnostic Bayesian learning. In experiments on 22regression and 39 classification data sets, we confirm the success of thisproposed approach, which is able to outperform model selection with SMBO.
arxiv-6000-198 | Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits | http://arxiv.org/abs/1402.0555 | author:Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, Robert E. Schapire category:cs.LG stat.ML published:2014-02-04 summary:We present a new algorithm for the contextual bandit learning problem, wherethe learner repeatedly takes one of $K$ actions in response to the observedcontext, and observes the reward only for that chosen action. Our methodassumes access to an oracle for solving fully supervised cost-sensitiveclassification problems and achieves the statistically optimal regret guaranteewith only $\tilde{O}(\sqrt{KT/\log N})$ oracle calls across all $T$ rounds,where $N$ is the number of policies in the policy class we compete against. Bydoing so, we obtain the most practical contextual bandit learning algorithmamongst approaches that work for general policy classes. We further conduct aproof-of-concept experiment which demonstrates the excellent computational andprediction performance of (an online variant of) our algorithm relative toseveral baselines.
arxiv-6000-199 | Particle Metropolis adjusted Langevin algorithms for state space models | http://arxiv.org/abs/1402.0694 | author:Chris Nemeth, Paul Fearnhead category:stat.CO stat.ML published:2014-02-04 summary:Particle MCMC is a class of algorithms that can be used to analysestate-space models. They use MCMC moves to update the parameters of the models,and particle filters to propose values for the path of the state-space model.Currently the default is to use random walk Metropolis to update the parametervalues. We show that it is possible to use information from the output of theparticle filter to obtain better proposal distributions for the parameters. Inparticular it is possible to obtain estimates of the gradient of the logposterior from each run of the particle filter, and use these estimates withina Langevin-type proposal. We propose using the recent computationally efficientapproach of Nemeth et al. (2013) for obtaining such estimates. We showempirically that for a variety of state-space models this proposal is moreefficient than the standard random walk Metropolis proposal in terms of:reducing autocorrelation of the posterior samples, reducing the burn-in time ofthe MCMC sampler and increasing the squared jump distance between posteriorsamples.
arxiv-6000-200 | Signal to Noise Ratio in Lensless Compressive Imaging | http://arxiv.org/abs/1402.0785 | author:Hong Jiang, Gang Huang, Paul Wilford category:cs.CV published:2014-02-04 summary:We analyze the signal to noise ratio (SNR) in a lensless compressive imaging(LCI) architecture. The architecture consists of a sensor of a single detectingelement and an aperture assembly of an array of programmable elements. LCI canbe used in conjunction with compressive sensing to capture images in acompressed form of compressive measurements. In this paper, we perform SNRanalysis of the LCI and compare it with imaging with a pinhole or a lens. Wewill show that the SNR in the LCI is independent of the image resolution, whilethe SNR in either pinhole aperture imaging or lens aperture imaging decreasesas the image resolution increases. Consequently, the SNR in the LCI is muchhigher if the image resolution is large enough.
arxiv-6000-201 | Microstrip Coupler Design Using Bat Algorithm | http://arxiv.org/abs/1402.0708 | author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE published:2014-02-04 summary:Evolutionary and swarm algorithms have found many applications in designproblems since todays computing power enables these algorithms to findsolutions to complicated design problems very fast. Newly proposed hybridalgorithm, bat algorithm, has been applied for the design of microwavemicrostrip couplers for the first time. Simulation results indicate that thebat algorithm is a very fast algorithm and it produces very reliable results.
arxiv-6000-202 | Short-term plasticity as cause-effect hypothesis testing in distal reward learning | http://arxiv.org/abs/1402.0710 | author:Andrea Soltoggio category:cs.NE q-bio.NC published:2014-02-04 summary:Asynchrony, overlaps and delays in sensory-motor signals introduce ambiguityas to which stimuli, actions, and rewards are causally related. Only therepetition of reward episodes helps distinguish true cause-effect relationshipsfrom coincidental occurrences. In the model proposed here, a novel plasticityrule employs short and long-term changes to evaluate hypotheses on cause-effectrelationships. Transient weights represent hypotheses that are consolidated inlong-term memory only when they consistently predict or cause future rewards.The main objective of the model is to preserve existing network topologies whenlearning with ambiguous information flows. Learning is also improved by biasingthe exploration of the stimulus-response space towards actions that in the pastoccurred before rewards. The model indicates under which conditions beliefs canbe consolidated in long-term memory, it suggests a solution to theplasticity-stability dilemma, and proposes an interpretation of the role ofshort-term plasticity.
arxiv-6000-203 | Generalization and Exploration via Randomized Value Functions | http://arxiv.org/abs/1402.0635 | author:Ian Osband, Benjamin Van Roy, Zheng Wen category:stat.ML cs.AI cs.LG cs.SY published:2014-02-04 summary:We propose randomized least-squares value iteration (RLSVI) -- a newreinforcement learning algorithm designed to explore and generalize efficientlyvia linearly parameterized value functions. We explain why versions ofleast-squares value iteration that use Boltzmann or epsilon-greedy explorationcan be highly inefficient, and we present computational results thatdemonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establishan upper bound on the expected regret of RLSVI that demonstratesnear-optimality in a tabula rasa learning context. More broadly, our resultssuggest that randomized value functions offer a promising approach to tacklinga critical challenge in reinforcement learning: synthesizing efficientexploration and effective generalization.
arxiv-6000-204 | The Informed Sampler: A Discriminative Approach to Bayesian Inference in Generative Computer Vision Models | http://arxiv.org/abs/1402.0859 | author:Varun Jampani, Sebastian Nowozin, Matthew Loper, Peter V. Gehler category:cs.CV cs.LG stat.ML published:2014-02-04 summary:Computer vision is hard because of a large variability in lighting, shape,and texture; in addition the image signal is non-additive due to occlusion.Generative models promised to account for this variability by accuratelymodelling the image formation process as a function of latent variables withprior beliefs. Bayesian posterior inference could then, in principle, explainthe observation. While intuitively appealing, generative models for computervision have largely failed to deliver on that promise due to the difficulty ofposterior inference. As a result the community has favoured efficientdiscriminative approaches. We still believe in the usefulness of generativemodels in computer vision, but argue that we need to leverage existingdiscriminative or even heuristic computer vision methods. We implement thisidea in a principled way with an "informed sampler" and in careful experimentsdemonstrate it on challenging generative models which contain renderer programsas their components. We concentrate on the problem of inverting an existinggraphics rendering engine, an approach that can be understood as "InverseGraphics". The informed sampler, using simple discriminative proposals based onexisting computer vision technology, achieves significant improvements ofinference.
arxiv-6000-205 | A Study of Local Binary Pattern Method for Facial Expression Detection | http://arxiv.org/abs/1405.6130 | author:Ms. Drashti H. Bhatt, Mr. Kirit R. Rathod, Mr. Shardul J. Agravat category:cs.CV published:2014-02-04 summary:Face detection is a basic task for expression recognition. The reliability offace detection & face recognition approach has a major role on the performanceand usability of the entire system. There are several ways to undergo facedetection & recognition. We can use Image Processing Operations, variousclassifiers, filters or virtual machines for the former. Various strategies arebeing available for Facial Expression Detection. The field of facial expressiondetection can have various applications along with its importance & can beinteracted between human being & computer. Many few options are available toidentify a face in an image in accurate & efficient manner. Local BinaryPattern (LBP) based texture algorithms have gained popularity in these years.LBP is an effective approach to have facial expression recognition & is afeature-based approach.
arxiv-6000-206 | Local Gaussian Regression | http://arxiv.org/abs/1402.0645 | author:Franziska Meier, Philipp Hennig, Stefan Schaal category:cs.LG cs.RO published:2014-02-04 summary:Locally weighted regression was created as a nonparametric learning methodthat is computationally efficient, can learn from very large amounts of dataand add data incrementally. An interesting feature of locally weightedregression is that it can work with spatially varying length scales, abeneficial property, for instance, in control problems. However, it does notprovide a generative model for function values and requires training and testdata to be generated identically, independently. Gaussian (process) regression,on the other hand, provides a fully generative model without significant formalrequirements on the distribution of training data, but has much highercomputational cost and usually works with one global scale per input dimension.Using a localising function basis and approximate inference techniques, we takeGaussian (process) regression to increasingly localised properties and towardthe same computational complexity class as locally weighted regression.
arxiv-6000-207 | UNLocBoX A matlab convex optimization toolbox using proximal splitting methods | http://arxiv.org/abs/1402.0779 | author:Nathanael Perraudin, David Shuman, Gilles Puy, Pierre Vandergheynst category:cs.LG stat.ML published:2014-02-04 summary:Nowadays the trend to solve optimization problems is to use specificalgorithms rather than very general ones. The UNLocBoX provides a generalframework allowing the user to design his own algorithms. To do so, theframework try to stay as close from the mathematical problem as possible. Moreprecisely, the UNLocBoX is a Matlab toolbox designed to solve convexoptimization problem of the form $$ \min_{x \in \mathcal{C}} \sum_{n=1}^Kf_n(x), $$ using proximal splitting techniques. It is mainly composed ofsolvers, proximal operators and demonstration files allowing the user toquickly implement a problem.
arxiv-6000-208 | Scene Labeling with Contextual Hierarchical Models | http://arxiv.org/abs/1402.0595 | author:Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.CV published:2014-02-04 summary:Scene labeling is the problem of assigning an object label to each pixel. Itunifies the image segmentation and object recognition problems. The importanceof using contextual information in scene labeling frameworks has been widelyrealized in the field. We propose a contextual framework, called contextualhierarchical model (CHM), which learns contextual information in a hierarchicalframework for scene labeling. At each level of the hierarchy, a classifier istrained based on downsampled input images and outputs of previous levels. Ourmodel then incorporates the resulting multi-resolution contextual informationinto a classifier to segment the input image at original resolution. Thistraining strategy allows for optimization of a joint posterior probability atmultiple resolutions through the hierarchy. Contextual hierarchical model ispurely based on the input image patches and does not make use of any fragmentsor shape examples. Hence, it is applicable to a variety of problems such asobject segmentation and edge detection. We demonstrate that CHM outperformsstate-of-the-art on Stanford background and Weizmann horse datasets. It alsooutperforms state-of-the-art edge detection methods on NYU depth dataset andachieves state-of-the-art on Berkeley segmentation dataset (BSDS 500).
arxiv-6000-209 | Principled Graph Matching Algorithms for Integrating Multiple Data Sources | http://arxiv.org/abs/1402.0282 | author:Duo Zhang, Benjamin I. P. Rubinstein, Jim Gemmell category:cs.DB cs.LG stat.ML published:2014-02-03 summary:This paper explores combinatorial optimization for problems of max-weightgraph matching on multi-partite graphs, which arise in integrating multipledata sources. Entity resolution-the data integration problem of performingnoisy joins on structured data-typically proceeds by first hashing each recordinto zero or more blocks, scoring pairs of records that are co-blocked forsimilarity, and then matching pairs of sufficient similarity. In the mostcommon case of matching two sources, it is often desirable for the finalmatching to be one-to-one (a record may be matched with at most one other);members of the database and statistical record linkage communities accomplishsuch matchings in the final stage by weighted bipartite graph matching onsimilarity scores. Such matchings are intuitively appealing: they leverage anatural global property of many real-world entity stores-that of being nearlydeduped-and are known to provide significant improvements to precision andrecall. Unfortunately unlike the bipartite case, exact max-weight matching onmulti-partite graphs is known to be NP-hard. Our two-fold algorithmiccontributions approximate multi-partite max-weight matching: our firstalgorithm borrows optimization techniques common to Bayesian probabilisticinference; our second is a greedy approximation algorithm. In addition to atheoretical guarantee on the latter, we present comparisons on a real-world ERproblem from Bing significantly larger than typically found in the literature,publication data, and on a series of synthetic problems. Our results quantifysignificant improvements due to exploiting multiple sources, which are madepossible by global one-to-one constraints linking otherwise independentmatching sub-problems. We also discover that our algorithms are complementary:one being much more robust under noise, and the other being simple to implementand very fast to run.
arxiv-6000-210 | Transductive Learning with Multi-class Volume Approximation | http://arxiv.org/abs/1402.0288 | author:Gang Niu, Bo Dai, Marthinus Christoffel du Plessis, Masashi Sugiyama category:cs.LG stat.ML published:2014-02-03 summary:Given a hypothesis space, the large volume principle by Vladimir Vapnikprioritizes equivalence classes according to their volume in the hypothesisspace. The volume approximation has hitherto been successfully applied tobinary learning problems. In this paper, we extend it naturally to a moregeneral definition which can be applied to several transductive problemsettings, such as multi-class, multi-label and serendipitous learning. Eventhough the resultant learning method involves a non-convex optimizationproblem, the globally optimal solution is almost surely unique and can beobtained in O(n^3) time. We theoretically provide stability and error analysesfor the proposed method, and then experimentally show that it is promising.
arxiv-6000-211 | A Robust Framework for Moving-Object Detection and Vehicular Traffic Density Estimation | http://arxiv.org/abs/1402.0289 | author:Pranam Janney, Glenn Geers category:cs.CV cs.RO cs.SY published:2014-02-03 summary:Intelligent machines require basic information such as moving-objectdetection from videos in order to deduce higher-level semantic information. Inthis paper, we propose a methodology that uses a texture measure to detectmoving objects in video. The methodology is computationally inexpensive,requires minimal parameter fine-tuning and also is resilient to noise,illumination changes, dynamic background and low frame rate. Experimentalresults show that performance of the proposed approach is higher than those ofstate-of-the-art approaches. We also present a framework for vehicular trafficdensity estimation using the foreground object detection technique and presenta comparison between the foreground object detection-based framework and theclassical density state modelling-based framework for vehicular traffic densityestimation.
arxiv-6000-212 | Associative Memories Based on Multiple-Valued Sparse Clustered Networks | http://arxiv.org/abs/1402.0808 | author:Hooman Jarollahi, Naoya Onizawa, Takahiro Hanyu, Warren J. Gross category:cs.NE published:2014-02-03 summary:Associative memories are structures that store data patterns and retrievethem given partial inputs. Sparse Clustered Networks (SCNs) arerecently-introduced binary-weighted associative memories that significantlyimprove the storage and retrieval capabilities over the prior state-of-the art.However, deleting or updating the data patterns result in a significantincrease in the data retrieval error probability. In this paper, we propose analgorithm to address this problem by incorporating multiple-valued weights forthe interconnections used in the network. The proposed algorithm lowers theerror rate by an order of magnitude for our sample network with 60% deletedcontents. We then investigate the advantages of the proposed algorithm forhardware implementations.
arxiv-6000-213 | Multidiscipinary Optimization For Gas Turbines Design | http://arxiv.org/abs/1402.0420 | author:Francesco Bertini, Lorenzo Dal Mas, Luca Vassio, Enrico Ampellio category:math.OC cs.NE published:2014-02-03 summary:State-of-the-art aeronautic Low Pressure gas Turbines (LPTs) are alreadycharacterized by high quality standards, thus they offer very narrow margins ofimprovement. Typical design process starts with a Concept Design (CD) phase,defined using mean-line 1D and other low-order tools, and evolves through aPreliminary Design (PD) phase, which allows the geometric definition indetails. In this framework, multidisciplinary optimization is the only way toproperly handle the complicated peculiarities of the design. The authorspresent different strategies and algorithms that have been implementedexploiting the PD phase as a real-like design benchmark to illustrate results.The purpose of this work is to describe the optimization techniques, theirsettings and how to implement them effectively in a multidisciplinaryenvironment. Starting from a basic gradient method and a semi-random secondorder method, the authors have introduced an Artificial Bee Colony-likeoptimizer, a multi-objective Genetic Diversity Evolutionary Algorithm [1] and amulti-objective response surface approach based on Artificial Neural Network,parallelizing and customizing them for the gas turbine study. Moreover, speedupand improvement arrangements are embedded in different hybrid strategies withthe aim at finding the best solutions for different kind of problems that arisein this field.
arxiv-6000-214 | A high-reproducibility and high-accuracy method for automated topic classification | http://arxiv.org/abs/1402.0422 | author:Andrea Lancichinetti, M. Irmak Sirer, Jane X. Wang, Daniel Acuna, Konrad Körding, Luís A. Nunes Amaral category:stat.ML cs.IR cs.LG physics.soc-ph published:2014-02-03 summary:Much of human knowledge sits in large databases of unstructured text.Leveraging this knowledge requires algorithms that extract and record metadataon unstructured text documents. Assigning topics to documents will enableintelligent search, statistical characterization, and meaningfulclassification. Latent Dirichlet allocation (LDA) is the state-of-the-art intopic classification. Here, we perform a systematic theoretical and numericalanalysis that demonstrates that current optimization techniques for LDA oftenyield results which are not accurate in inferring the most suitable modelparameters. Adapting approaches for community detection in networks, we proposea new algorithm which displays high-reproducibility and high-accuracy, and alsohas high computational efficiency. We apply it to a large set of documents inthe English Wikipedia and reveal its hierarchical structure. Our algorithmpromises to make "big data" text analysis systems more reliable.
arxiv-6000-215 | A Lower Bound for the Variance of Estimators for Nakagami m Distribution | http://arxiv.org/abs/1402.0452 | author:Rangeet Mitra, Amit Kumar Mishra, Tarun Choubisa category:cs.LG published:2014-02-03 summary:Recently, we have proposed a maximum likelihood iterative algorithm forestimation of the parameters of the Nakagami-m distribution. This techniqueperforms better than state of art estimation techniques for this distribution.This could be of particular use in low data or block based estimation problems.In these scenarios, the estimator should be able to give accurate estimates inthe mean square sense with less amounts of data. Also, the estimates shouldimprove with the increase in number of blocks received. In this paper, we seethrough our simulations, that our proposal is well designed for suchrequirements. Further, it is well known in the literature that an efficientestimator does not exist for Nakagami-m distribution. In this paper, we derivea theoretical expression for the variance of our proposed estimator. We findthat this expression clearly fits the experimental curve for the variance ofthe proposed estimator. This expression is pretty close to the cramer-rao lowerbound(CRLB).
arxiv-6000-216 | Applying Supervised Learning Algorithms and a New Feature Selection Method to Predict Coronary Artery Disease | http://arxiv.org/abs/1402.0459 | author:Hubert Haoyang Duan category:cs.LG stat.ML published:2014-02-03 summary:From a fresh data science perspective, this thesis discusses the predictionof coronary artery disease based on genetic variations at the DNA base pairlevel, called Single-Nucleotide Polymorphisms (SNPs), collected from theOntario Heart Genomics Study (OHGS). First, the thesis explains two commonly used supervised learning algorithms,the k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes acomplete proof that the k-NN classifier is universally consistent in any finitedimensional normed vector space. Second, the thesis introduces twodimensionality reduction steps, Random Projections, a known feature extractiontechnique based on the Johnson-Lindenstrauss lemma, and a new method termedMass Transportation Distance (MTD) Feature Selection for discrete domains.Then, this thesis compares the performance of Random Projections with the k-NNclassifier against MTD Feature Selection and Random Forest, for predictingartery disease based on accuracy, the F-Measure, and area under the ReceiverOperating Characteristic (ROC) curve. The comparative results demonstrate that MTD Feature Selection with RandomForest is vastly superior to Random Projections and k-NN. The Random Forestclassifier is able to obtain an accuracy of 0.6660 and an area under the ROCcurve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTDFeature Selection for classification. This area is considerably better than theprevious high score of 0.608 obtained by Davies et al. in 2010 on the samedataset.
arxiv-6000-217 | Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets | http://arxiv.org/abs/1402.0480 | author:Diederik P. Kingma, Max Welling category:cs.LG stat.ML published:2014-02-03 summary:Hierarchical Bayesian networks and neural networks with stochastic hiddenunits are commonly perceived as two separate types of models. We show thateither of these types of models can often be transformed into an instance ofthe other, by switching between centered and differentiable non-centeredparameterizations of the latent variables. The choice of parameterizationgreatly influences the efficiency of gradient-based posterior inference; weshow that they are often complementary to eachother, we clarify when eachparameterization is preferred and show how inference can be made robust. In thenon-centered form, a simple Monte Carlo estimator of the marginal likelihoodcan be used for learning the parameters. Theoretical results are supported byexperiments.
arxiv-6000-218 | Fine-Grained Visual Categorization via Multi-stage Metric Learning | http://arxiv.org/abs/1402.0453 | author:Qi Qian, Rong Jin, Shenghuo Zhu, Yuanqing Lin category:cs.CV cs.LG stat.ML published:2014-02-03 summary:Fine-grained visual categorization (FGVC) is to categorize objects intosubordinate classes instead of basic classes. One major challenge in FGVC isthe co-occurrence of two issues: 1) many subordinate classes are highlycorrelated and are difficult to distinguish, and 2) there exists the largeintra-class variation (e.g., due to object pose). This paper proposes toexplicitly address the above two issues via distance metric learning (DML). DMLaddresses the first issue by learning an embedding so that data points from thesame class will be pulled together while those from different classes should bepushed apart from each other; and it addresses the second issue by allowing theflexibility that only a portion of the neighbors (not all data points) from thesame class need to be pulled together. However, feature representation of animage is often high dimensional, and DML is known to have difficulty in dealingwith high dimensional feature vectors since it would require $\mathcal{O}(d^2)$for storage and $\mathcal{O}(d^3)$ for optimization. To this end, we proposed amulti-stage metric learning framework that divides the large-scale highdimensional learning problem to a series of simple subproblems, achieving$\mathcal{O}(d)$ computational complexity. The empirical study with FVGCbenchmark datasets verifies that our method is both effective and efficientcompared to the state-of-the-art FGVC approaches.
arxiv-6000-219 | Sequential Monte Carlo for Graphical Models | http://arxiv.org/abs/1402.0330 | author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.ME stat.ML published:2014-02-03 summary:We propose a new framework for how to use sequential Monte Carlo (SMC)algorithms for inference in probabilistic graphical models (PGM). Via asequential decomposition of the PGM we find a sequence of auxiliarydistributions defined on a monotonically increasing sequence of probabilityspaces. By targeting these auxiliary distributions using SMC we are able toapproximate the full joint distribution defined by the PGM. One of the keymerits of the SMC sampler is that it provides an unbiased estimate of thepartition function of the model. We also show how it can be used within aparticle Markov chain Monte Carlo framework in order to constructhigh-dimensional block-sampling algorithms for general PGMs.
arxiv-6000-220 | How Does Latent Semantic Analysis Work? A Visualisation Approach | http://arxiv.org/abs/1402.0543 | author:Jan Koeman, William Rea category:cs.CL cs.IR published:2014-02-03 summary:By using a small example, an analogy to photographic compression, and asimple visualization using heatmaps, we show that latent semantic analysis(LSA) is able to extract what appears to be semantic meaning of words from aset of documents by blurring the distinctions between the words.
arxiv-6000-221 | Graph Cuts with Interacting Edge Costs - Examples, Approximations, and Algorithms | http://arxiv.org/abs/1402.0240 | author:Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.CV cs.DM math.OC published:2014-02-02 summary:We study an extension of the classical graph cut problem, wherein we replacethe modular (sum of edge weights) cost function by a submodular set functiondefined over graph edges. Special cases of this problem have appeared indifferent applications in signal processing, machine learning, and computervision. In this paper, we connect these applications via the genericformulation of "cooperative graph cuts", for which we study complexity,algorithms, and connections to polymatroidal network flows. Finally, we comparethe proposed algorithms empirically.
arxiv-6000-222 | Collaborative Receptive Field Learning | http://arxiv.org/abs/1402.0170 | author:Shu Kong, Zhuolin Jiang, Qiang Yang category:cs.CV cs.LG cs.MM stat.ML published:2014-02-02 summary:The challenge of object categorization in images is largely due to arbitrarytranslations and scales of the foreground objects. To attack this difficulty,we propose a new approach called collaborative receptive field learning toextract specific receptive fields (RF's) or regions from multiple images, andthe selected RF's are supposed to focus on the foreground objects of a commoncategory. To this end, we solve the problem by maximizing a submodular functionover a similarity graph constructed by a pool of RF candidates. However,measuring pairwise distance of RF's for building the similarity graph is anontrivial problem. Hence, we introduce a similarity metric calledpyramid-error distance (PED) to measure their pairwise distances throughsumming up pyramid-like matching errors over a set of low-level features.Besides, in consistent with the proposed PED, we construct a simplenonparametric classifier for classification. Experimental results show that ourmethod effectively discovers the foreground objects in images, and improvesclassification performance.
arxiv-6000-223 | Markov Blanket Ranking using Kernel-based Conditional Dependence Measures | http://arxiv.org/abs/1402.0108 | author:Eric V. Strobl, Shyam Visweswaran category:stat.ML cs.LG published:2014-02-01 summary:Developing feature selection algorithms that move beyond a pure correlationalto a more causal analysis of observational data is an important problem in thesciences. Several algorithms attempt to do so by discovering the Markov blanketof a target, but they all contain a forward selection step which variables mustpass in order to be included in the conditioning set. As a result, thesealgorithms may not consider all possible conditional multivariate combinations.We improve on this limitation by proposing a backward elimination method thatuses a kernel-based conditional dependence measure to identify the Markovblanket in a fully multivariate fashion. The algorithm is easy to implement andcompares favorably to other methods on synthetic and real datasets.
arxiv-6000-224 | Dual-to-kernel learning with ideals | http://arxiv.org/abs/1402.0099 | author:Franz J. Király, Martin Kreuzer, Louis Theran category:stat.ML cs.LG math.AC math.AG math.ST stat.TH published:2014-02-01 summary:In this paper, we propose a theory which unifies kernel learning and symbolicalgebraic methods. We show that both worlds are inherently dual to each other,and we use this duality to combine the structure-awareness of algebraic methodswith the efficiency and generality of kernels. The main idea lies in relatingpolynomial rings to feature space, and ideals to manifolds, then exploitingthis generative-discriminative duality on kernel matrices. We illustrate thisby proposing two algorithms, IPCA and AVICA, for simultaneous manifold andfeature learning, and test their accuracy on synthetic and real world data.
arxiv-6000-225 | Randomized Nonlinear Component Analysis | http://arxiv.org/abs/1402.0119 | author:David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, Bernhard Schölkopf category:stat.ML cs.LG published:2014-02-01 summary:Classical methods such as Principal Component Analysis (PCA) and CanonicalCorrelation Analysis (CCA) are ubiquitous in statistics. However, thesetechniques are only able to reveal linear relationships in data. Althoughnonlinear variants of PCA and CCA have been proposed, these are computationallyprohibitive in the large scale. In a separate strand of recent research, randomized methods have beenproposed to construct features that help reveal nonlinear patterns in data. Forbasic tasks such as regression or classification, random features exhibitlittle or no loss in performance, while achieving drastic savings incomputational requirements. In this paper we leverage randomness to design scalable new variants ofnonlinear PCA and CCA; our ideas extend to key multivariate analysis tools suchas spectral clustering or LDA. We demonstrate our algorithms throughexperiments on real-world data, on which we compare against thestate-of-the-art. A simple R implementation of the presented algorithms isprovided.
arxiv-6000-226 | Experiments with Three Approaches to Recognizing Lexical Entailment | http://arxiv.org/abs/1401.8269 | author:Peter D. Turney, Saif M. Mohammad category:cs.CL cs.AI cs.LG published:2014-01-31 summary:Inference in natural language often involves recognizing lexical entailment(RLE); that is, identifying whether one word entails another. For example,"buy" entails "own". Two general strategies for RLE have been proposed: Onestrategy is to manually construct an asymmetric similarity measure for contextvectors (directional similarity) and another is to treat RLE as a problem oflearning to recognize semantic relations using supervised machine learningtechniques (relation classification). In this paper, we experiment with tworecent state-of-the-art representatives of the two general strategies. Thefirst approach is an asymmetric similarity measure (an instance of thedirectional similarity strategy), designed to capture the degree to which thecontexts of a word, a, form a subset of the contexts of another word, b. Thesecond approach (an instance of the relation classification strategy)represents a word pair, a:b, with a feature vector that is the concatenation ofthe context vectors of a and b, and then applies supervised learning to atraining set of labeled feature vectors. Additionally, we introduce a thirdapproach that is a new instance of the relation classification strategy. Thethird approach represents a word pair, a:b, with a feature vector in which thefeatures are the differences in the similarities of a and b to a set ofreference words. All three approaches use vector space models (VSMs) ofsemantics, based on word-context matrices. We perform an extensive evaluationof the three approaches using three different datasets. The proposed newapproach (similarity differences) performs significantly better than the othertwo approaches on some datasets and there is no dataset for which it issignificantly worse. Our results suggest it is beneficial to make connectionsbetween the research in lexical entailment and the research in semanticrelation classification.
arxiv-6000-227 | A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning | http://arxiv.org/abs/1401.8066 | author:Ha Quang Minh, Loris Bazzani, Vittorio Murino category:stat.ML published:2014-01-31 summary:This paper presents a general vector-valued reproducing kernel Hilbert spaces(RKHS) framework for the problem of learning an unknown functional dependencybetween a structured input space and a structured output space. Our formulationencompasses both Vector-valued Manifold Regularization and Co-regularizedMulti-view Learning, providing in particular a unifying framework linking thesetwo important learning approaches. In the case of the least square lossfunction, we provide a closed form solution, which is obtained by solving asystem of linear equations. In the case of Support Vector Machine (SVM)classification, our formulation generalizes in particular both the binaryLaplacian SVM to the multi-class, multi-view settings and the multi-classSimplex Cone SVM to the semi-supervised, multi-view settings. The solution isobtained by solving a single quadratic optimization problem, as in standardSVM, via the Sequential Minimal Optimization (SMO) approach. Empirical resultsobtained on the task of object recognition, using several challenging datasets,demonstrate the competitiveness of our algorithms compared with otherstate-of-the-art methods.
arxiv-6000-228 | Cross-calibration of Time-of-flight and Colour Cameras | http://arxiv.org/abs/1401.8092 | author:Miles Hansard, Georgios Evangelidis, Quentin Pelorson, Radu Horaud category:cs.CV cs.RO published:2014-01-31 summary:Time-of-flight cameras provide depth information, which is complementary tothe photometric appearance of the scene in ordinary images. It is desirable tomerge the depth and colour information, in order to obtain a coherent scenerepresentation. However, the individual cameras will have different viewpoints,resolutions and fields of view, which means that they must be mutuallycalibrated. This paper presents a geometric framework for this multi-view andmulti-modal calibration problem. It is shown that three-dimensional projectivetransformations can be used to align depth and parallax-based representationsof the scene, with or without Euclidean reconstruction. A new evaluationprocedure is also developed; this allows the reprojection error to bedecomposed into calibration and sensor-dependent components. The completeapproach is demonstrated on a network of three time-of-flight and six colourcameras. The applications of such a system, to a range of automaticscene-interpretation problems, are discussed.
arxiv-6000-229 | Marginal and simultaneous predictive classification using stratified graphical models | http://arxiv.org/abs/1401.8078 | author:Henrik Nyman, Jie Xiong, Johan Pensar, Jukka Corander category:stat.ML published:2014-01-31 summary:An inductive probabilistic classification rule must generally obey theprinciples of Bayesian predictive inference, such that all observed andunobserved stochastic quantities are jointly modeled and the parameteruncertainty is fully acknowledged through the posterior predictivedistribution. Several such rules have been recently considered and theirasymptotic behavior has been characterized under the assumption that theobserved features or variables used for building a classifier are conditionallyindependent given a simultaneous labeling of both the training samples andthose from an unknown origin. Here we extend the theoretical results topredictive classifiers acknowledging feature dependencies either throughgraphical models or sparser alternatives defined as stratified graphicalmodels. We also show through experimentation with both synthetic and real datathat the predictive classifiers based on stratified graphical models haveconsistently best accuracy compared with the predictive classifiers based oneither conditionally independent features or on ordinary graphical models.
arxiv-6000-230 | Empirically Evaluating Multiagent Learning Algorithms | http://arxiv.org/abs/1401.8074 | author:Erik Zawadzki, Asher Lipson, Kevin Leyton-Brown category:cs.GT cs.LG published:2014-01-31 summary:There exist many algorithms for learning how to play repeated bimatrix games.Most of these algorithms are justified in terms of some sort of theoreticalguarantee. On the other hand, little is known about the empirical performanceof these algorithms. Most such claims in the literature are based on smallexperiments, which has hampered understanding as well as the development of newmultiagent learning (MAL) algorithms. We have developed a new suite of toolsfor running multiagent experiments: the MultiAgent Learning Testbed (MALT).These tools are designed to facilitate larger and more comprehensiveexperiments by removing the need to build one-off experimental code. MALT alsoprovides baseline implementations of many MAL algorithms, hopefully eliminatingor reducing differences between algorithm implementations and increasing thereproducibility of results. Using this test suite, we ran an experimentunprecedented in size. We analyzed the results according to a variety ofperformance metrics including reward, maxmin distance, regret, and severalnotions of equilibrium convergence. We confirmed several pieces of conventionalwisdom, but also discovered some surprising results. For example, we found thatsingle-agent $Q$-learning outperformed many more complicated and more modernMAL algorithms.
arxiv-6000-231 | Hallucinating optimal high-dimensional subspaces | http://arxiv.org/abs/1401.8053 | author:Ognjen Arandjelovic category:cs.CV published:2014-01-31 summary:Linear subspace representations of appearance variation are pervasive incomputer vision. This paper addresses the problem of robustly matching suchsubspaces (computing the similarity between them) when they are used todescribe the scope of variations within sets of images of different (possiblygreatly so) scales. A naive solution of projecting the low-scale subspace intothe high-scale image space is described first and subsequently shown to beinadequate, especially at large scale discrepancies. A successful approach isproposed instead. It consists of (i) an interpolated projection of thelow-scale subspace into the high-scale space, which is followed by (ii) arotation of this initial estimate within the bounds of the imposed``downsampling constraint''. The optimal rotation is found in the closed-formwhich best aligns the high-scale reconstruction of the low-scale subspace withthe reference it is compared to. The method is evaluated on the problem ofmatching sets of (i) face appearances under varying illumination and (ii)object appearances under varying viewpoint, using two large data sets. Incomparison to the naive matching, the proposed algorithm is shown to greatlyincrease the separation of between-class and within-class similarities, as wellas produce far more meaningful modes of common appearance on which the matchscore is based.
arxiv-6000-232 | Extrinsic Methods for Coding and Dictionary Learning on Grassmann Manifolds | http://arxiv.org/abs/1401.8126 | author:Mehrtash Harandi, Richard Hartley, Chunhua Shen, Brian Lovell, Conrad Sanderson category:cs.LG cs.CV stat.ML published:2014-01-31 summary:Sparsity-based representations have recently led to notable results invarious visual recognition tasks. In a separate line of research, Riemannianmanifolds have been shown useful for dealing with features and models that donot lie in Euclidean spaces. With the aim of building a bridge between the tworealms, we address the problem of sparse coding and dictionary learning overthe space of linear subspaces, which form Riemannian structures known asGrassmann manifolds. To this end, we propose to embed Grassmann manifolds intothe space of symmetric matrices by an isometric mapping. This in turn enablesus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, wepropose closed-form solutions for learning a Grassmann dictionary, atom byatom. Lastly, to handle non-linearity in data, we extend the proposed Grassmannsparse coding and dictionary learning algorithms through embedding into Hilbertspaces. Experiments on several classification tasks (gender recognition, gestureclassification, scene analysis, face recognition, action recognition anddynamic texture classification) show that the proposed approaches achieveconsiderable improvements in discrimination accuracy, in comparison tostate-of-the-art methods such as kernelized Affine Hull Method andgraph-embedding Grassmann discriminant analysis.
arxiv-6000-233 | Homotopy based algorithms for $\ell_0$-regularized least-squares | http://arxiv.org/abs/1406.4802 | author:Charles Soussen, Jérôme Idier, Junbo Duan, David Brie category:cs.NA cs.LG published:2014-01-31 summary:Sparse signal restoration is usually formulated as the minimization of aquadratic cost function $\y-Ax\_2^2$, where A is a dictionary and x is anunknown sparse vector. It is well-known that imposing an $\ell_0$ constraintleads to an NP-hard minimization problem. The convex relaxation approach hasreceived considerable attention, where the $\ell_0$-norm is replaced by the$\ell_1$-norm. Among the many efficient $\ell_1$ solvers, the homotopyalgorithm minimizes $\y-Ax\_2^2+\lambda\x\_1$ with respect to x for acontinuum of $\lambda$'s. It is inspired by the piecewise regularity of the$\ell_1$-regularization path, also referred to as the homotopy path. In thispaper, we address the minimization problem $\y-Ax\_2^2+\lambda\x\_0$ for acontinuum of $\lambda$'s and propose two heuristic search algorithms for$\ell_0$-homotopy. Continuation Single Best Replacement is a forward-backwardgreedy strategy extending the Single Best Replacement algorithm, previouslyproposed for $\ell_0$-minimization at a given $\lambda$. The adaptive search ofthe $\lambda$-values is inspired by $\ell_1$-homotopy. $\ell_0$ RegularizationPath Descent is a more complex algorithm exploiting the structural propertiesof the $\ell_0$-regularization path, which is piecewise constant with respectto $\lambda$. Both algorithms are empirically evaluated for difficult inverseproblems involving ill-conditioned dictionaries. Finally, we show that they canbe easily coupled with usual methods of model order selection.
arxiv-6000-234 | Neural Variational Inference and Learning in Belief Networks | http://arxiv.org/abs/1402.0030 | author:Andriy Mnih, Karol Gregor category:cs.LG stat.ML published:2014-01-31 summary:Highly expressive directed latent variable models, such as sigmoid beliefnetworks, are difficult to train on large datasets because exact inference inthem is intractable and none of the approximate inference methods that havebeen applied to them scale well. We propose a fast non-iterative approximateinference method that uses a feedforward network to implement efficient exactsampling from the variational posterior. The model and this inference networkare trained jointly by maximizing a variational lower bound on thelog-likelihood. Although the naive estimator of the inference model gradient istoo high-variance to be useful, we make it practical by applying severalstraightforward model-independent variance reduction techniques. Applying ourapproach to training sigmoid belief networks and deep autoregressive networks,we show that it outperforms the wake-sleep algorithm on MNIST and achievesstate-of-the-art results on the Reuters RCV1 document dataset.
arxiv-6000-235 | Online Clustering of Bandits | http://arxiv.org/abs/1401.8257 | author:Claudio Gentile, Shuai Li, Giovanni Zappella category:cs.LG stat.ML published:2014-01-31 summary:We introduce a novel algorithmic approach to content recommendation based onadaptive clustering of exploration-exploitation ("bandit") strategies. Weprovide a sharp regret analysis of this algorithm in a standard stochasticnoise setting, demonstrate its scalability properties, and prove itseffectiveness on a number of artificial and real-world datasets. Ourexperiments show a significant increase in prediction performance overstate-of-the-art methods for bandit problems.
arxiv-6000-236 | Human Activity Recognition using Smartphone | http://arxiv.org/abs/1401.8212 | author:Amin Rasekh, Chien-An Chen, Yan Lu category:cs.CY cs.HC cs.LG published:2014-01-30 summary:Human activity recognition has wide applications in medical research andhuman survey system. In this project, we design a robust activity recognitionsystem based on a smartphone. The system uses a 3-dimentional smartphoneaccelerometer as the only sensor to collect time series signals, from which 31features are generated in both time and frequency domain. Activities areclassified using 4 different passive learning methods, i.e., quadraticclassifier, k-nearest neighbor algorithm, support vector machine, andartificial neural networks. Dimensionality reduction is performed through bothfeature extraction and subset selection. Besides passive learning, we alsoapply active learning algorithms to reduce data labeling expense. Experimentresults show that the classification rate of passive learning reaches 84.4% andit is robust to common positions and poses of cellphone. The results of activelearning on real data demonstrate a reduction of labeling labor to achievecomparable performance with passive learning.
arxiv-6000-237 | Support vector comparison machines | http://arxiv.org/abs/1401.8008 | author:Toby Dylan Hocking, Supaporn Spanurattana, Masashi Sugiyama category:stat.ML cs.LG published:2014-01-30 summary:In ranking problems, the goal is to learn a ranking function from labeledpairs of input points. In this paper, we consider the related comparisonproblem, where the label indicates which element of the pair is better, or ifthere is no significant difference. We cast the learning problem as a marginmaximization, and show that it can be solved by converting it to a standardSVM. We use simulated nonlinear patterns and a real learning to rank sushi dataset to show that our proposed SVMcompare algorithm outperforms SVMrank whenthere are equality pairs.
arxiv-6000-238 | Maximum Margin Multiclass Nearest Neighbors | http://arxiv.org/abs/1401.7898 | author:Aryeh Kontorovich, Roi Weiss category:cs.LG math.ST stat.TH published:2014-01-30 summary:We develop a general framework for margin-based multicategory classificationin metric spaces. The basic work-horse is a margin-regularized version of thenearest-neighbor classifier. We prove generalization bounds that match thestate of the art in sample size $n$ and significantly improve the dependence onthe number of classes $k$. Our point of departure is a nearly Bayes-optimalfinite-sample risk bound independent of $k$. Although $k$-free, this bound isunregularized and non-adaptive, which motivates our main result: Rademacher andscale-sensitive margin bounds with a logarithmic dependence on $k$. As the bestprevious risk estimates in this setting were of order $\sqrt k$, our bound isexponentially sharper. From the algorithmic standpoint, in doubling metricspaces our classifier may be trained on $n$ examples in $O(n^2\log n)$ time andevaluated on new points in $O(\log n)$ time.
arxiv-6000-239 | Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method | http://arxiv.org/abs/1401.7743 | author:T. Balaji, Dr. M. Sumathi category:cs.CV published:2014-01-30 summary:Remote sensing image classification can be performed in many different waysto extract meaningful features. One common approach is to perform edgedetection. A second approach is to try and detect whole shapes, given the factthat these shapes usually tend to have distinctive properties such as objectforeground or background. To get optimal results, these two approaches can becombined. This paper adopts a combinatorial optimization method to adaptivelyselect threshold based features to improve remote sensing image. Featureselection is an important combinatorial optimization problem in the remotesensing image classification. The feature selection method has to achieve threecharacteristics: first the performance issues by facilitating data collectionand reducing storage space and classification time, second to perform semanticsanalysis helping to understand the problem, and third to improve predictionaccuracy by avoiding the curse of dimensionality. The goal of this thresholdingan image is to classify pixels as either dark or light and evaluation ofclassification results. Interactive adaptive thresholding is a form ofthresholding that takes into account spatial variations in illumination ofremote sensing image. We present a technique for remote sensing based adaptivethresholding using the interactive satellite image of the input. However, oursolution is more robust to illumination changes in the remote sensing image.Additionally, our method is simple and easy to implement but it is effectivealgorithm to classify the image pixels. This technique is suitable forpreprocessing the remote sensing image classification, making it a valuabletool for interactive remote based applications such as augmented reality of theclassification procedure.
arxiv-6000-240 | Video Compressive Sensing for Dynamic MRI | http://arxiv.org/abs/1401.7715 | author:Jianing V. Shi, Wotao Yin, Aswin C. Sankaranarayanan, Richard G. Baraniuk category:cs.CV math.OC published:2014-01-30 summary:We present a video compressive sensing framework, termed kt-CSLDS, toaccelerate the image acquisition process of dynamic magnetic resonance imaging(MRI). We are inspired by a state-of-the-art model for video compressivesensing that utilizes a linear dynamical system (LDS) to model the motionmanifold. Given compressive measurements, the state sequence of an LDS can befirst estimated using system identification techniques. We then reconstruct theobservation matrix using a joint structured sparsity assumption. In particular,we minimize an objective function with a mixture of wavelet sparsity and jointsparsity within the observation matrix. We derive an efficient convexoptimization algorithm through alternating direction method of multipliers(ADMM), and provide a theoretical guarantee for global convergence. Wedemonstrate the performance of our approach for video compressive sensing, interms of reconstruction accuracy. We also investigate the impact of varioussampling strategies. We apply this framework to accelerate the acquisitionprocess of dynamic MRI and show it achieves the best reconstruction accuracywith the least computational time compared with existing algorithms in theliterature.
arxiv-6000-241 | Sparse Bayesian Unsupervised Learning | http://arxiv.org/abs/1401.8017 | author:Stephane Gaiffas, Bertrand Michel category:stat.ML 62H30 published:2014-01-30 summary:This paper is about variable selection, clustering and estimation in anunsupervised high-dimensional setting. Our approach is based on fittingconstrained Gaussian mixture models, where we learn the number of clusters $K$and the set of relevant variables $S$ using a generalized Bayesian posteriorwith a sparsity inducing prior. We prove a sparsity oracle inequality whichshows that this procedure selects the optimal parameters $K$ and $S$. Thisprocedure is implemented using a Metropolis-Hastings algorithm, based on aclustering-oriented greedy proposal, which makes the convergence to theposterior very fast.
arxiv-6000-242 | Joint Inference of Multiple Label Types in Large Networks | http://arxiv.org/abs/1401.7709 | author:Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, Sofus A. Macskassy category:cs.LG cs.SI stat.ML published:2014-01-30 summary:We tackle the problem of inferring node labels in a partially labeled graphwhere each node in the graph has multiple label types and each label type has alarge number of possible labels. Our primary example, and the focus of thispaper, is the joint inference of label types such as hometown, current city,and employers, for users connected by a social network. Standard labelpropagation fails to consider the properties of the label types and theinteractions between them. Our proposed method, called EdgeExplain, explicitlymodels these, while still enabling scalable inference under a distributedmessage-passing architecture. On a billion-node subset of the Facebook socialnetwork, EdgeExplain significantly outperforms label propagation for severallabel types, with lifts of up to 120% for recall@1 and 60% for recall@3.
arxiv-6000-243 | Security Evaluation of Support Vector Machines in Adversarial Environments | http://arxiv.org/abs/1401.7727 | author:Battista Biggio, Igino Corona, Blaine Nelson, Benjamin I. P. Rubinstein, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli category:cs.LG cs.CR published:2014-01-30 summary:Support Vector Machines (SVMs) are among the most popular classificationtechniques adopted in security applications like malware detection, intrusiondetection, and spam filtering. However, if SVMs are to be incorporated inreal-world security systems, they must be able to cope with attack patternsthat can either mislead the learning algorithm (poisoning), evade detection(evasion), or gain information about their internal parameters (privacybreaches). The main contributions of this chapter are twofold. First, weintroduce a formal general framework for the empirical evaluation of thesecurity of machine-learning systems. Second, according to our framework, wedemonstrate the feasibility of evasion, poisoning and privacy attacks againstSVMs in real-world security problems. For each attack technique, we evaluateits impact and discuss whether (and how) it can be countered through anadversary-aware design of SVMs. Our experiments are easily reproducible thanksto open-source code that we have made available, together with all the employeddatasets, on a public repository.
arxiv-6000-244 | A Generalized Probabilistic Framework for Compact Codebook Creation | http://arxiv.org/abs/1401.7713 | author:Lingqiao Liu, Lei Wang, Chunhua Shen category:cs.CV published:2014-01-30 summary:Compact and discriminative visual codebooks are preferred in many visualrecognition tasks. In the literature, a number of works have taken the approachof hierarchically merging visual words of an initial large-sized codebook, butimplemented this approach with different merging criteria. In this work, wepropose a single probabilistic framework to unify these merging criteria, byidentifying two key factors: the function used to model class-conditionaldistribution and the method used to estimate the distribution parameters. Moreimportantly, by adopting new distribution functions and/or parameter estimationmethods, our framework can readily produce a spectrum of novel mergingcriteria. Three of them are specifically focused in this work. In the firstcriterion, we adopt the multinomial distribution with Bayesian method; In thesecond criterion, we integrate Gaussian distribution with maximum likelihoodparameter estimation. In the third criterion, which shows the best mergingperformance, we propose a max-margin-based parameter estimation method andapply it with multinomial distribution. Extensive experimental study isconducted to systematically analyse the performance of the above three criteriaand compare them with existing ones. As demonstrated, the best criterionobtained in our framework achieves the overall best merging performance amongthe comparable merging criteria developed in the literature.
arxiv-6000-245 | Infrared face recognition: a comprehensive review of methodologies and databases | http://arxiv.org/abs/1401.8261 | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2014-01-29 summary:Automatic face recognition is an area with immense practical potential whichincludes a wide range of commercial and law enforcement applications. Hence itis unsurprising that it continues to be one of the most active research areasof computer vision. Even after over three decades of intense research, thestate-of-the-art in face recognition continues to improve, benefitting fromadvances in a range of different research fields such as image processing,pattern recognition, computer graphics, and physiology. Systems based onvisible spectrum images, the most researched face recognition modality, havereached a significant level of maturity with some practical success. However,they continue to face challenges in the presence of illumination, pose andexpression changes, as well as facial disguises, all of which can significantlydecrease recognition accuracy. Amongst various approaches which have beenproposed in an attempt to overcome these limitations, the use of infrared (IR)imaging has emerged as a particularly promising research direction. This paperpresents a comprehensive and timely review of the literature on this subject.Our key contributions are: (i) a summary of the inherent properties of infraredimaging which makes this modality promising in the context of face recognition,(ii) a systematic review of the most influential approaches, with a focus onemerging common trends as well as key differences between alternativemethodologies, (iii) a description of the main databases of infrared facialimages available to the researcher, and lastly (iv) a discussion of the mostpromising avenues for future research.
arxiv-6000-246 | Bounding Embeddings of VC Classes into Maximum Classes | http://arxiv.org/abs/1401.7388 | author:J. Hyam Rubinstein, Benjamin I. P. Rubinstein, Peter L. Bartlett category:cs.LG math.CO stat.ML published:2014-01-29 summary:One of the earliest conjectures in computational learning theory-the SampleCompression conjecture-asserts that concept classes (equivalently set systems)admit compression schemes of size linear in their VC dimension. To-date thisstatement is known to be true for maximum classes---those that possess maximumcardinality for their VC dimension. The most promising approach to positivelyresolving the conjecture is by embedding general VC classes into maximumclasses without super-linear increase to their VC dimensions, as suchembeddings would extend the known compression schemes to all VC classes. Weshow that maximum classes can be characterised by a local-connectivity propertyof the graph obtained by viewing the class as a cubical complex. This geometriccharacterisation of maximum VC classes is applied to prove a negative embeddingresult which demonstrates VC-d classes that cannot be embedded in any maximumclass of VC dimension lower than 2d. On the other hand, we show that every VC-dclass C embeds in a VC-(d+D) maximum class where D is the deficiency of C,i.e., the difference between the cardinalities of a maximum VC-d class and ofC. For VC-2 classes in binary n-cubes for 4 <= n <= 6, we give best possibleresults on embedding into maximum classes. For some special classes of Booleanfunctions, relationships with maximum classes are investigated. Finally we givea general recursive procedure for embedding VC-d classes into VC-(d+k) maximumclasses for smallest k.
arxiv-6000-247 | Information quantity in a pixel of digital image | http://arxiv.org/abs/1401.7517 | author:M. Kharinov category:cs.CV cs.IT math.IT published:2014-01-29 summary:The paper is devoted to the problem of integer-valued estimating ofinformation quantity in a pixel of digital image. The definition of an integerestimation of information quantity based on constructing of the certain binaryhierarchy of pixel clusters is proposed. The methods for constructinghierarchies of clusters and generating of hierarchical sequences of imageapproximations that minimally differ from the image by a standard deviation aredeveloped. Experimental results on integer-valued estimation of informationquantity are compared with the results obtained by utilizing of the classicalformulas.
arxiv-6000-248 | Use HMM and KNN for classifying corneal data | http://arxiv.org/abs/1401.7486 | author:Payam Porkar Rezaeiye, mehrnoosh bazrafkan, ali akbar movassagh, Mojtaba Sedigh Fazli, Gholam hossein bazyari category:cs.CV published:2014-01-29 summary:These days to gain classification system with high accuracy that can classifycomplicated pattern are so useful in medicine and industry. In this article aprocess for getting the best classifier for Lasik data is suggested. However atfirst it's been tried to find the best line and curve by this classifier inorder to gain classifier fitting, and in the end by using the Markov method aclassifier for topographies is gained.
arxiv-6000-249 | The parametrized probabilistic finite-state transducer probe game player fingerprint model | http://arxiv.org/abs/1401.7406 | author:Jeffrey Tsang category:cs.GT cs.NE published:2014-01-29 summary:Fingerprinting operators generate functional signatures of game players andare useful for their automated analysis independent of representation orencoding. The theory for a fingerprinting operator which returns thelength-weighted probability of a given move pair occurring from playing theinvestigated agent against a general parametrized probabilistic finite-statetransducer (PFT) is developed, applicable to arbitrary iterated games. Resultsfor the distinguishing power of the 1-state opponent model, uniformapproximability of fingerprints of arbitrary players, analyticity and Lipschitzcontinuity of fingerprints for logically possible players, and equicontinuityof the fingerprints of bounded-state probabilistic transducers are derived.Algorithms for the efficient computation of special instances are given; theshortcomings of a previous model, strictly generalized here from a simpleprojection of the new model, are explained in terms of regularity conditionviolations, and the extra power and functional niceness of the new fingerprintsdemonstrated. The 2-state deterministic finite-state transducers (DFTs) arefingerprinted and pairwise distances computed; using this the structure of DFTsin strategy space is elucidated.
arxiv-6000-250 | Bayesian nonparametric comorbidity analysis of psychiatric disorders | http://arxiv.org/abs/1401.7620 | author:Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fernando Perez-Cruz category:stat.ML cs.LG published:2014-01-29 summary:The analysis of comorbidity is an open and complex research field in thebranch of psychiatry, where clinical experience and several studies suggestthat the relation among the psychiatric disorders may have etiological andtreatment implications. In this paper, we are interested in applying latentfeature modeling to find the latent structure behind the psychiatric disordersthat can help to examine and explain the relationships among them. To this end,we use the large amount of information collected in the National EpidemiologicSurvey on Alcohol and Related Conditions (NESARC) database and propose to modelthese data using a nonparametric latent model based on the Indian BuffetProcess (IBP). Due to the discrete nature of the data, we first need to adaptthe observation model for discrete random variables. We propose a generativemodel in which the observations are drawn from a multinomial-logit distributiongiven the IBP matrix. The implementation of an efficient Gibbs sampler isaccomplished using the Laplace approximation, which allows integrating out theweighting factors of the multinomial-logit likelihood model. We also provide avariational inference algorithm for this model, which provides a complementary(and less expensive in terms of computational complexity) alternative to theGibbs sampler allowing us to deal with a larger number of data. Finally, we usethe model to analyze comorbidity among the psychiatric disorders diagnosed byexperts from the NESARC database.
arxiv-6000-251 | Graph matching: relax or not? | http://arxiv.org/abs/1401.7623 | author:Yonathan Aflalo, Alex Bronstein, Ron Kimmel category:cs.DS cs.CG cs.CV math.OC published:2014-01-29 summary:We consider the problem of exact and inexact matching of weighted undirectedgraphs, in which a bijective correspondence is sought to minimize a quadraticweight disagreement. This computationally challenging problem is often relaxedas a convex quadratic program, in which the space of permutations is replacedby the space of doubly-stochastic matrices. However, the applicability of sucha relaxation is poorly understood. We define a broad class of friendly graphscharacterized by an easily verifiable spectral property. We prove that forfriendly graphs, the convex relaxation is guaranteed to find the exactisomorphism or certify its inexistence. This result is further extended toapproximately isomorphic graphs, for which we develop an explicit bound on theamount of weight disagreement under which the relaxation is guaranteed to findthe globally optimal approximate isomorphism. We also show that in many cases,the graph matching problem can be further harmlessly relaxed to a convexquadratic program with only n separable linear equality constraints, which issubstantially more efficient than the standard relaxation involving 2n equalityand n^2 inequality constraints. Finally, we show that our results are stillvalid for unfriendly graphs if additional information in the form of seeds orattributes is allowed, with the latter satisfying an easy to verify spectralcharacteristic.
arxiv-6000-252 | Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted Least Squares Minimization | http://arxiv.org/abs/1401.7413 | author:Canyi Lu, Zhouchen Lin, Shuicheng Yan category:cs.LG cs.CV stat.ML published:2014-01-29 summary:This work presents a general framework for solving the low rank and/or sparsematrix minimization problems, which may involve multiple non-smooth terms. TheIteratively Reweighted Least Squares (IRLS) method is a fast solver, whichsmooths the objective function and minimizes it by alternately updating thevariables and their weights. However, the traditional IRLS can only solve asparse only or low rank only minimization problem with squared loss or anaffine constraint. This work generalizes IRLS to solve joint/mixed low rank andsparse minimization problems, which are essential formulations for many tasks.As a concrete example, we solve the Schatten-$p$ norm and $\ell_{2,q}$-normregularized Low-Rank Representation (LRR) problem by IRLS, and theoreticallyprove that the derived solution is a stationary point (globally optimal if$p,q\geq1$). Our convergence proof of IRLS is more general than previous onewhich depends on the special properties of the Schatten-$p$ norm and$\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data setsdemonstrate that our IRLS is much more efficient.
arxiv-6000-253 | RES: Regularized Stochastic BFGS Algorithm | http://arxiv.org/abs/1401.7625 | author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG math.OC stat.ML published:2014-01-29 summary:RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno(BFGS) quasi-Newton method is proposed to solve convex optimization problemswith stochastic objectives. The use of stochastic gradient descent algorithmsis widespread, but the number of iterations required to approximate optimalarguments can be prohibitive in high dimensional problems. Application ofsecond order methods, on the other hand, is impracticable because computationof objective function Hessian inverses incurs excessive computational cost.BFGS modifies gradient descent by introducing a Hessian approximation matrixcomputed from finite gradient differences. RES utilizes stochastic gradients inlieu of deterministic gradients for both, the determination of descentdirections and the approximation of the objective function's curvature. Sincestochastic gradients can be computed at manageable computational cost RES isrealizable and retains the convergence rate advantages of its deterministiccounterparts. Convergence results show that lower and upper bounds on theHessian egeinvalues of the sample functions are sufficient to guaranteeconvergence to optimal arguments. Numerical experiments showcase reductions inconvergence time relative to stochastic gradient descent algorithms andnon-regularized stochastic versions of BFGS. An application of RES to theimplementation of support vector machines is developed.
arxiv-6000-254 | A Spectral Framework for Anomalous Subgraph Detection | http://arxiv.org/abs/1401.7702 | author:Benjamin A. Miller, Michelle S. Beard, Patrick J. Wolfe, Nadya T. Bliss category:cs.SI stat.ML published:2014-01-29 summary:A wide variety of application domains are concerned with data consisting ofentities and their relationships or connections, formally represented asgraphs. Within these diverse application areas, a common problem of interest isthe detection of a subset of entities whose connectivity is anomalous withrespect to the rest of the data. While the detection of such anomaloussubgraphs has received a substantial amount of attention, noapplication-agnostic framework exists for analysis of signal detectability ingraph-based data. In this paper, we describe a framework that enables suchanalysis using the principal eigenspace of a graph's residuals matrix, commonlycalled the modularity matrix in community detection. Leveraging this analyticaltool, we show that the framework has a natural power metric in the spectralnorm of the anomalous subgraph's adjacency matrix (signal power) and of thebackground graph's residuals matrix (noise power). We propose severalalgorithms based on spectral properties of the residuals matrix, with morecomputationally expensive techniques providing greater detection power.Detection and identification performance are presented for a number of signaland noise models, including clusters and bipartite foregrounds embedded intosimple random backgrounds as well as graphs with community structure andrealistic degree distributions. The trends observed verify intuition gleanedfrom other signal processing areas, such as greater detection power when thesignal is embedded within a less active portion of the background. Wedemonstrate the utility of the proposed techniques in detecting small, highlyanomalous subgraphs in real graphs derived from Internet traffic and productco-purchases.
arxiv-6000-255 | Quantifying literature quality using complexity criteria | http://arxiv.org/abs/1401.7077 | author:Gerardo Febres, Klaus Jaffe category:cs.CL published:2014-01-28 summary:We measured entropy and symbolic diversity for English and Spanish textsincluding literature Nobel laureates and other famous authors. Entropy, symboldiversity and symbol frequency profiles were compared for these four groups. Wealso built a scale sensitive to the quality of writing and evaluated itsrelationship with the Flesch's readability index for English and theSzigriszt's perspicuity index for Spanish. Results suggest a correlationbetween entropy and word diversity with quality of writing. Text genre alsoinfluences the resulting entropy and diversity of the text. Results suggest theplausibility of automated quality assessment of texts.
arxiv-6000-256 | Tempering by Subsampling | http://arxiv.org/abs/1401.7145 | author:Jan-Willem van de Meent, Brooks Paige, Frank Wood category:stat.ML published:2014-01-28 summary:In this paper we demonstrate that tempering Markov chain Monte Carlo samplersfor Bayesian models by recursively subsampling observations without replacementcan improve the performance of baseline samplers in terms of effective samplesize per computation. We present two tempering by subsampling algorithms,subsampled parallel tempering and subsampled tempered transitions. We providean asymptotic analysis of the computational cost of tempering by subsampling,verify that tempering by subsampling costs less than traditional tempering, anddemonstrate both algorithms on Bayesian approaches to learning the mean of ahigh dimensional multivariate Normal and estimating Gaussian processhyperparameters.
arxiv-6000-257 | Bayesian Properties of Normalized Maximum Likelihood and its Fast Computation | http://arxiv.org/abs/1401.7116 | author:Andrew Barron, Teemu Roos, Kazuho Watanabe category:cs.IT cs.LG math.IT stat.ML published:2014-01-28 summary:The normalized maximized likelihood (NML) provides the minimax regretsolution in universal data compression, gambling, and prediction, and it playsan essential role in the minimum description length (MDL) method of statisticalmodeling and estimation. Here we show that the normalized maximum likelihoodhas a Bayes-like representation as a mixture of the component models, even infinite samples, though the weights of linear combination may be both positiveand negative. This representation addresses in part the relationship betweenMDL and Bayes modeling. This representation has the advantage of speeding thecalculation of marginals and conditionals required for coding and predictionapplications.
arxiv-6000-258 | A Stochastic Quasi-Newton Method for Large-Scale Optimization | http://arxiv.org/abs/1401.7020 | author:R. H. Byrd, S. L. Hansen, J. Nocedal, Y. Singer category:math.OC cs.LG stat.ML published:2014-01-27 summary:The question of how to incorporate curvature information in stochasticapproximation methods is challenging. The direct application of classicalquasi- Newton updating techniques for deterministic optimization leads to noisycurvature estimates that have harmful effects on the robustness of theiteration. In this paper, we propose a stochastic quasi-Newton method that isefficient, robust and scalable. It employs the classical BFGS update formula inits limited memory form, and is based on the observation that it is beneficialto collect curvature information pointwise, and at regular intervals, through(sub-sampled) Hessian-vector products. This technique differs from theclassical approach that would compute differences of gradients, and wherecontrolling the quality of the curvature estimates can be difficult. We presentnumerical results on problems arising in machine learning that suggest that theproposed method shows much promise.
arxiv-6000-259 | Safe Sample Screening for Support Vector Machines | http://arxiv.org/abs/1401.6740 | author:Kohei Ogawa, Yoshiki Suzuki, Shinya Suzumura, Ichiro Takeuchi category:stat.ML published:2014-01-27 summary:Sparse classifiers such as the support vector machines (SVM) are efficient intest-phases because the classifier is characterized only by a subset of thesamples called support vectors (SVs), and the rest of the samples (non SVs)have no influence on the classification result. However, the advantage of thesparsity has not been fully exploited in training phases because it isgenerally difficult to know which sample turns out to be SV beforehand. In thispaper, we introduce a new approach called safe sample screening that enables usto identify a subset of the non-SVs and screen them out prior to the trainingphase. Our approach is different from existing heuristic approaches in thesense that the screened samples are guaranteed to be non-SVs at the optimalsolution. We investigate the advantage of the safe sample screening approachthrough intensive numerical experiments, and demonstrate that it cansubstantially decrease the computational cost of the state-of-the-art SVMsolvers such as LIBSVM. In the current big data era, we believe that safesample screening would be of great practical importance since the data size canbe reduced without sacrificing the optimality of the final solution.
arxiv-6000-260 | Sparsistency and agnostic inference in sparse PCA | http://arxiv.org/abs/1401.6978 | author:Jing Lei, Vincent Q. Vu category:math.ST stat.ML stat.TH published:2014-01-27 summary:The presence of a sparse "truth" has been a constant assumption in thetheoretical analysis of sparse PCA and is often implicit in its methodologicaldevelopment. This naturally raises questions about the properties of sparse PCAmethods and how they depend on the assumption of sparsity. Under whatconditions can the relevant variables be selected consistently if the truth isassumed to be sparse? What can be said about the results of sparse PCA withoutassuming a sparse and unique truth? We answer these questions by investigatingthe properties of the recently proposed Fantope projection and selection (FPS)method in the high-dimensional setting. Our results provide general sufficientconditions for sparsistency of the FPS estimator. These conditions are weak andcan hold in situations where other estimators are known to fail. On the otherhand, without assuming sparsity or identifiability, we show that FPS provides asparse, linear dimension-reducing transformation that is close to the bestpossible in terms of maximizing the predictive covariance.
arxiv-6000-261 | Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN | http://arxiv.org/abs/1401.6984 | author:Yajie Miao category:cs.LG cs.CL published:2014-01-27 summary:The Kaldi toolkit is becoming popular for constructing automated speechrecognition (ASR) systems. Meanwhile, in recent years, deep neural networks(DNNs) have shown state-of-the-art performance on various ASR tasks. Thisdocument describes our open-source recipes to implement fully-fledged DNNacoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learningtoolkit developed under the Theano environment. Using these recipes, we canbuild up multiple systems including DNN hybrid systems, convolutional neuralnetwork (CNN) systems and bottleneck feature systems. These recipes aredirectly based on the Kaldi Switchboard 110-hour setup. However, adapting themto new datasets is easy to achieve.
arxiv-6000-262 | Computing support for advanced medical data analysis and imaging | http://arxiv.org/abs/1401.6929 | author:W. Wiślicki, T. Bednarski, P. Białas, E. Czerwiński, Ł. Kapłon, A. Kochanowski, G. Korcyl, J. Kowal, P. Kowalski, T. Kozik, W. Krzemień, M. Molenda, P. Moskal, S. Niedźwiecki, M. Pałka, M. Pawlik, L. Raczyński, Z. Rudy, P. Salabura, N. G. Sharma, M. Silarski, A. Słomski, J. Smyrski, A. Strzelecki, A. Wieczorek, M. Zieliński, N. Zoń category:cs.CV cs.DC physics.med-ph published:2014-01-27 summary:We discuss computing issues for data analysis and image reconstruction ofPET-TOF medical scanner or other medical scanning devices producing largevolumes of data. Service architecture based on the grid and cloud concepts fordistributed processing is proposed and critically discussed.
arxiv-6000-263 | A continuous-time approach to online optimization | http://arxiv.org/abs/1401.6956 | author:Joon Kwon, Panayotis Mertikopoulos category:math.OC cs.LG stat.ML published:2014-01-27 summary:We consider a family of learning strategies for online optimization problemsthat evolve in continuous time and we show that they lead to no regret. From amore traditional, discrete-time viewpoint, this continuous-time approach allowsus to derive the no-regret properties of a large class of discrete-timealgorithms including as special cases the exponential weight algorithm, onlinemirror descent, smooth fictitious play and vanishingly smooth fictitious play.In so doing, we obtain a unified view of many classical regret bounds, and weshow that they can be decomposed into a term stemming from continuous-timeconsiderations and a term which measures the disparity between discrete andcontinuous time. As a result, we obtain a general class of infinite horizonlearning strategies that guarantee an $\mathcal{O}(n^{-1/2})$ regret boundwithout having to resort to a doubling trick.
arxiv-6000-264 | Continuous Localization and Mapping of a Pan Tilt Zoom Camera for Wide Area Tracking | http://arxiv.org/abs/1401.6606 | author:Giuseppe Lisanti, Iacopo Masi, Federico Pernici, Alberto Del Bimbo category:cs.CV published:2014-01-26 summary:Pan-tilt-zoom (PTZ) cameras are powerful to support object identification andrecognition in far-field scenes. However, the effective use of PTZ cameras inreal contexts is complicated by the fact that a continuous on-line cameracalibration is needed and the absolute pan, tilt and zoom positional valuesprovided by the camera actuators cannot be used because are not synchronizedwith the video stream. So, accurate calibration must be directly extracted fromthe visual content of the frames. Moreover, the large and abrupt scale changes,the scene background changes due to the camera operation and the need of cameramotion compensation make target tracking with these cameras extremelychallenging. In this paper, we present a solution that provides continuouson-line calibration of PTZ cameras which is robust to rapid camera motion,changes of the environment due to illumination or moving objects and scalesbeyond thousands of landmarks. The method directly derives the relationshipbetween the position of a target in the 3D world plane and the correspondingscale and position in the 2D image, and allows real-time tracking of multipletargets with high and stable degree of accuracy even at far distances and anyzooming level.
arxiv-6000-265 | Near-Ideal Behavior of Compressed Sensing Algorithms | http://arxiv.org/abs/1401.6623 | author:Mehmet Eren Ahsen, Mathukumalli Vidyasagar category:stat.ML 62J07 published:2014-01-26 summary:In a recent paper, it is shown that the LASSO algorithm exhibits "near-idealbehavior," in the following sense: Suppose $y = Az + \eta$ where $A$ satisfiesthe restricted isometry property (RIP) with a sufficiently small constant, and$\Vert \eta \Vert_2 \leq \epsilon$. Then minimizing $\Vert z \Vert_1$ subjectto $\Vert y - Az \Vert_2 \leq \epsilon$ leads to an estimate $\hat{x}$ whoseerror $\Vert \hat{x} - x \Vert_2$ is bounded by a universal constant times theerror achieved by an "oracle" that knows the location of the nonzero componentsof $x$. In the world of optimization, the LASSO algorithm has been generalizedin several directions such as the group LASSO, the sparse group LASSO, eitherwithout or with tree-structured overlapping groups, and most recently, thesorted LASSO. In this paper, it is shown that {\it any algorithm\/} exhibitsnear-ideal behavior in the above sense, provided only that (i) the norm used todefine the sparsity index is "decomposable," (ii) the penalty norm that isminimized in an effort to enforce sparsity is "$\gamma$-decomposable," and(iii) a "compressibility condition" in terms of a group restricted isometryproperty is satisfied. Specifically, the group LASSO, and the sparse groupLASSO (with some permissible overlap in the groups), as well as the sorted$\ell_1$-norm minimization all exhibit near-ideal behavior. Explicit bounds onthe residual error are derived that contain previously known results as specialcases.
arxiv-6000-266 | Perturbed Message Passing for Constraint Satisfaction Problems | http://arxiv.org/abs/1401.6686 | author:Siamak Ravanbakhsh, Russell Greiner category:cs.AI cs.CC stat.ML published:2014-01-26 summary:We introduce an efficient message passing scheme for solving ConstraintSatisfaction Problems (CSPs), which uses stochastic perturbation of BeliefPropagation (BP) and Survey Propagation (SP) messages to bypass decimation anddirectly produce a single satisfying assignment. Our first CSP solver, calledPerturbed Blief Propagation, smoothly interpolates two well-known inferenceprocedures; it starts as BP and ends as a Gibbs sampler, which produces asingle sample from the set of solutions. Moreover we apply a similarperturbation scheme to SP to produce another CSP solver, Perturbed SurveyPropagation. Experimental results on random and real-world CSPs show thatPerturbed BP is often more successful and at the same time tens to hundreds oftimes more efficient than standard BP guided decimation. Perturbed BP alsocompares favorably with state-of-the-art SP-guided decimation, which has acomputational complexity that generally scales exponentially worse than ourmethod (wrt the cardinality of variable domains and constraints). Furthermore,our experiments with random satisfiability and coloring problems demonstratethat Perturbed SP can outperform SP-guided decimation, making it the bestincomplete random CSP-solver in difficult regimes.
arxiv-6000-267 | Painting Analysis Using Wavelets and Probabilistic Topic Models | http://arxiv.org/abs/1401.6638 | author:Tong Wu, Gungor Polatkan, David Steel, William Brown, Ingrid Daubechies, Robert Calderbank category:cs.CV cs.LG stat.ML published:2014-01-26 summary:In this paper, computer-based techniques for stylistic analysis of paintingsare applied to the five panels of the 14th century Peruzzi Altarpiece by Giottodi Bondone. Features are extracted by combining a dual-tree complex wavelettransform with a hidden Markov tree (HMT) model. Hierarchical clustering isused to identify stylistic keywords in image patches, and keyword frequenciesare calculated for sub-images that each contains many patches. A generativehierarchical Bayesian model learns stylistic patterns of keywords; thesepatterns are then used to characterize the styles of the sub-images; this inturn, permits to discriminate between paintings. Results suggest that suchunsupervised probabilistic topic models can be useful to distill characteristicelements of style.
arxiv-6000-268 | Ensembled Correlation Between Liver Analysis Outputs | http://arxiv.org/abs/1401.6597 | author:Sadi Evren Seker, Y. Unal, Z. Erdem, H. Erdinc Kocer category:stat.ML cs.CE cs.LG published:2014-01-25 summary:Data mining techniques on the biological analysis are spreading for most ofthe areas including the health care and medical information. We have appliedthe data mining techniques, such as KNN, SVM, MLP or decision trees over aunique dataset, which is collected from 16,380 analysis results for a year.Furthermore we have also used meta-classifiers to question the increasedcorrelation rate between the liver disorder and the liver analysis outputs. Theresults show that there is a correlation among ALT, AST, Billirubin Direct andBillirubin Total down to 15% of error rate. Also the correlation coefficient isup to 94%. This makes possible to predict the analysis results from each otheror disease patterns can be applied over the linear correlation of theparameters.
arxiv-6000-269 | Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination | http://arxiv.org/abs/1401.6497 | author:Qibin Zhao, Liqing Zhang, Andrzej Cichocki category:cs.LG cs.CV stat.ML published:2014-01-25 summary:CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerfultechnique for tensor completion through explicitly capturing the multilinearlatent factors. The existing CP algorithms require the tensor rank to bemanually specified, however, the determination of tensor rank remains achallenging problem especially for CP rank. In addition, existing approaches donot take into account uncertainty information of latent factors, as well asmissing entries. To address these issues, we formulate CP factorization using ahierarchical probabilistic model and employ a fully Bayesian treatment byincorporating a sparsity-inducing prior over multiple latent factors and theappropriate hyperpriors over all hyperparameters, resulting in automatic rankdetermination. To learn the model, we develop an efficient deterministicBayesian inference algorithm, which scales linearly with data size. Our methodis characterized as a tuning parameter-free approach, which can effectivelyinfer underlying multilinear factors with a low-rank constraint, while alsoproviding predictive distributions over missing entries. Extensive simulationson synthetic data illustrate the intrinsic capability of our method to recoverthe ground-truth of CP rank and prevent the overfitting problem, even when alarge amount of entries are missing. Moreover, the results from real-worldapplications, including image inpainting and facial image synthesis,demonstrate that our method outperforms state-of-the-art approaches for bothtensor factorization and tensor completion in terms of predictive performance.
arxiv-6000-270 | Category theory, logic and formal linguistics: some connections, old and new | http://arxiv.org/abs/1401.6574 | author:Jean Gillibert, Christian Retoré category:math.CT cs.CL cs.LO math.LO published:2014-01-25 summary:We seize the opportunity of the publication of selected papers from the\emph{Logic, categories, semantics} workshop in the \emph{Journal of AppliedLogic} to survey some current trends in logic, namely intuitionistic and lineartype theories, that interweave categorical, geometrical and computationalconsiderations. We thereafter present how these rich logical frameworks canmodel the way language conveys meaning.
arxiv-6000-271 | Deverbal semantics and the Montagovian generative lexicon | http://arxiv.org/abs/1401.6573 | author:Livy-Maria Real-Coelho, Christian Retoré category:cs.CL cs.LO published:2014-01-25 summary:We propose a lexical account of action nominals, in particular of deverbalnominalisations, whose meaning is related to the event expressed by their baseverb. The literature about nominalisations often assumes that the semantics ofthe base verb completely defines the structure of action nominals. We arguethat the information in the base verb is not sufficient to completely determinethe semantics of action nominals. We exhibit some data from differentlanguages, especially from Romance language, which show that nominalisationsfocus on some aspects of the verb semantics. The selected aspects, however,seem to be idiosyncratic and do not automatically result from the internalstructure of the verb nor from its interaction with the morphological suffix.We therefore propose a partially lexicalist approach view of deverbal nouns. Itis made precise and computable by using the Montagovian Generative Lexicon, atype theoretical framework introduced by Bassac, Mery and Retor\'e in thisjournal in 2010. This extension of Montague semantics with a richer type systemeasily incorporates lexical phenomena like the semantics of action nominals inparticular deverbals, including their polysemy and (in)felicitouscopredications.
arxiv-6000-272 | Identification of Protein Coding Regions in Genomic DNA Using Unsupervised FMACA Based Pattern Classifier | http://arxiv.org/abs/1401.6484 | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG published:2014-01-25 summary:Genes carry the instructions for making proteins that are found in a cell asa specific sequence of nucleotides that are found in DNA molecules. But, theregions of these genes that code for proteins may occupy only a small region ofthe sequence. Identifying the coding regions play a vital role in understandingthese genes. In this paper we propose a unsupervised Fuzzy Multiple AttractorCellular Automata (FMCA) based pattern classifier to identify the coding regionof a DNA sequence. We propose a distinct K-Means algorithm for designing FMACAclassifier which is simple, efficient and produces more accurate classifierthan that has previously been obtained for a range of different sequencelengths. Experimental results confirm the scalability of the proposedUnsupervised FCA based classifier to handle large volume of datasetsirrespective of the number of classes, tuples and attributes. Goodclassification accuracy has been established.
arxiv-6000-273 | Keyword and Keyphrase Extraction Using Centrality Measures on Collocation Networks | http://arxiv.org/abs/1401.6571 | author:Shibamouli Lahiri, Sagnik Ray Choudhury, Cornelia Caragea category:cs.CL cs.IR published:2014-01-25 summary:Keyword and keyphrase extraction is an important problem in natural languageprocessing, with applications ranging from summarization to semantic search todocument clustering. Graph-based approaches to keyword and keyphrase extractionavoid the problem of acquiring a large in-domain training corpus by applyingvariants of PageRank algorithm on a network of words. Although graph-basedapproaches are knowledge-lean and easily adoptable in online systems, itremains largely open whether they can benefit from centrality measures otherthan PageRank. In this paper, we experiment with an array of centralitymeasures on word and noun phrase collocation networks, and analyze theirperformance on four benchmark datasets. Not only are there centrality measuresthat perform as well as or better than PageRank, but they are much simpler(e.g., degree, strength, and neighborhood size). Furthermore, centrality-basedmethods give results that are competitive with and, in some cases, better thantwo strong unsupervised baselines.
arxiv-6000-274 | A Machine Learning Approach for the Identification of Bengali Noun-Noun Compound Multiword Expressions | http://arxiv.org/abs/1401.6567 | author:Vivekananda Gayen, Kamal Sarkar category:cs.CL cs.LG published:2014-01-25 summary:This paper presents a machine learning approach for identification of Bengalimultiword expressions (MWE) which are bigram nominal compounds. Our proposedapproach has two steps: (1) candidate extraction using chunk information andvarious heuristic rules and (2) training the machine learning algorithm calledRandom Forest to classify the candidates into two groups: bigram nominalcompound MWE or not bigram nominal compound MWE. A variety of associationmeasures, syntactic and linguistic clues and a set of WordNet-based similarityfeatures have been used for our MWE identification task. The approach presentedin this paper can be used to identify bigram nominal compound MWE in Bengalirunning text.
arxiv-6000-275 | Word-length entropies and correlations of natural language written texts | http://arxiv.org/abs/1401.6224 | author:Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Konstantinos Karamanos, Fotis K. Diakonos, Harris Papageorgiou category:cs.CL published:2014-01-24 summary:We study the frequency distributions and correlations of the word lengths often European languages. Our findings indicate that a) the word-lengthdistribution of short words quantified by the mean value and the entropydistinguishes the Uralic (Finnish) corpus from the others, b) the tails at longwords, manifested in the high-order moments of the distributions, differentiatethe Germanic languages (except for English) from the Romanic languages andGreek and c) the correlations between nearby word lengths measured by thecomparison of the real entropies with those of the shuffled texts are found tobe smaller in the case of Germanic and Finnish languages.
arxiv-6000-276 | A Statistical Parsing Framework for Sentiment Classification | http://arxiv.org/abs/1401.6330 | author:Li Dong, Furu Wei, Shujie Liu, Ming Zhou, Ke Xu category:cs.CL published:2014-01-24 summary:We present a statistical parsing framework for sentence-level sentimentclassification in this article. Unlike previous works that employ syntacticparsing results for sentiment analysis, we develop a statistical parser todirectly analyze the sentiment structure of a sentence. We show thatcomplicated phenomena in sentiment analysis (e.g., negation, intensification,and contrast) can be handled the same as simple and straightforward sentimentexpressions in a unified and probabilistic way. We formulate the sentimentgrammar upon Context-Free Grammars (CFGs), and provide a formal description ofthe sentiment parsing framework. We develop the parsing model to obtainpossible sentiment parse trees for a sentence, from which the polarity model isproposed to derive the sentiment strength and polarity, and the ranking modelis dedicated to selecting the best sentiment tree. We train the parser directlyfrom examples of sentences annotated only with sentiment polarity labels butwithout any syntactic annotations or polarity annotations of constituentswithin sentences. Therefore we can obtain training data easily. In particular,we train a sentiment parser, s.parser, from a large amount of review sentenceswith users' ratings as rough sentiment polarity labels. Extensive experimentson existing benchmark datasets show significant improvements over baselinesentiment classification approaches.
arxiv-6000-277 | Automatic Detection of Calibration Grids in Time-of-Flight Images | http://arxiv.org/abs/1401.6393 | author:Miles Hansard, Radu Horaud, Michel Amat, Georgios Evangelidis category:cs.CV published:2014-01-24 summary:It is convenient to calibrate time-of-flight cameras by established methods,using images of a chequerboard pattern. The low resolution of the amplitudeimage, however, makes it difficult to detect the board reliably. Heuristicdetection methods, based on connected image-components, perform very poorly onthis data. An alternative, geometrically-principled method is introduced here,based on the Hough transform. The projection of a chequerboard is representedby two pencils of lines, which are identified as oriented clusters in thegradient-data of the image. A projective Hough transform is applied to each ofthe two clusters, in axis-aligned coordinates. The range of each transform isproperly bounded, because the corresponding gradient vectors are approximatelyparallel. Each of the two transforms contains a series of collinear peaks; onefor every line in the given pencil. This pattern is easily detected, bysweeping a dual line through the transform. The proposed Hough-based method iscompared to the standard OpenCV detection routine, by application to severalhundred time-of-flight images. It is shown that the new method detectssignificantly more calibration boards, over a greater variety of poses, withoutany overall loss of accuracy. This conclusion is based on an analysis of bothgeometric and photometric error.
arxiv-6000-278 | The Sampling-and-Learning Framework: A Statistical View of Evolutionary Algorithms | http://arxiv.org/abs/1401.6333 | author:Yang Yu, Hong Qian category:cs.NE cs.LG published:2014-01-24 summary:Evolutionary algorithms (EAs), a large class of general purpose optimizationalgorithms inspired from the natural phenomena, are widely used in variousindustrial optimizations and often show excellent performance. This paperpresents an attempt towards revealing their general power from a statisticalview of EAs. By summarizing a large range of EAs into the sampling-and-learningframework, we show that the framework directly admits a general analysis on theprobable-absolute-approximate (PAA) query complexity. We particularly focus onthe framework with the learning subroutine being restricted as a binaryclassification, which results in the sampling-and-classification (SAC)algorithms. With the help of the learning theory, we obtain a general upperbound on the PAA query complexity of SAC algorithms. We further compare SACalgorithms with the uniform search in different situations. Under theerror-target independence condition, we show that SAC algorithms can achievepolynomial speedup to the uniform search, but not super-polynomial speedup.Under the one-side-error condition, we show that super-polynomial speedup canbe achieved. This work only touches the surface of the framework. Its powerunder other conditions is still open.
arxiv-6000-279 | Local Identification of Overcomplete Dictionaries | http://arxiv.org/abs/1401.6354 | author:Karin Schnass category:cs.IT math.IT stat.ML published:2014-01-24 summary:This paper presents the first theoretical results showing that stableidentification of overcomplete $\mu$-coherent dictionaries $\Phi \in\mathbb{R}^{d\times K}$ is locally possible from training signals with sparsitylevels $S$ up to the order $O(\mu^{-2})$ and signal to noise ratios up to$O(\sqrt{d})$. In particular the dictionary is recoverable as the local maximumof a new maximisation criterion that generalises the K-means criterion. Forthis maximisation criterion results for asymptotic exact recovery for sparsitylevels up to $O(\mu^{-1})$ and stable recovery for sparsity levels up to$O(\mu^{-2})$ as well as signal to noise ratios up to $O(\sqrt{d})$ areprovided. These asymptotic results translate to finite sample size recoveryresults with high probability as long as the sample size $N$ scales as $O(K^3dS\tilde \varepsilon^{-2})$, where the recovery precision $\tilde \varepsilon$can go down to the asymptotically achievable precision. Further, to actuallyfind the local maxima of the new criterion, a very simple IterativeThresholding and K (signed) Means algorithm (ITKM), which has complexity$O(dKN)$ in each iteration, is presented and its local efficiency isdemonstrated in several experiments.
arxiv-6000-280 | Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II) | http://arxiv.org/abs/1401.6240 | author:Shaobo Lin, Xia Liu, Jian Fang, Zongben Xu category:cs.LG 68T05 F.2.2 published:2014-01-24 summary:An extreme learning machine (ELM) can be regarded as a two stage feed-forwardneural network (FNN) learning system which randomly assigns the connectionswith and within hidden neurons in the first stage and tunes the connectionswith output neurons in the second stage. Therefore, ELM training is essentiallya linear learning problem, which significantly reduces the computationalburden. Numerous applications show that such a computation burden reductiondoes not degrade the generalization capability. It has, however, been open thatwhether this is true in theory. The aim of our work is to study the theoreticalfeasibility of ELM by analyzing the pros and cons of ELM. In the previous parton this topic, we pointed out that via appropriate selection of the activationfunction, ELM does not degrade the generalization capability in the expectationsense. In this paper, we launch the study in a different direction and showthat the randomness of ELM also leads to certain negative consequences. On onehand, we find that the randomness causes an additional uncertainty problem ofELM, both in approximation and learning. On the other hand, we theoreticallyjustify that there also exists an activation function such that thecorresponding ELM degrades the generalization capability. In particular, weprove that the generalization capability of ELM with Gaussian kernel isessentially worse than that of FNN with Gaussian kernel. To facilitate the useof ELM, we also provide a remedy to such a degradation. We find that thewell-developed coefficient regularization technique can essentially improve thegeneralization capability. The obtained results reveal the essentialcharacteristic of ELM and give theoretical guidance concerning how to use ELM.
arxiv-6000-281 | Parallel Genetic Algorithm to Solve Traveling Salesman Problem on MapReduce Framework using Hadoop Cluster | http://arxiv.org/abs/1401.6267 | author:Harun Rasit Er, Nadia Erdogan category:cs.DC cs.NE published:2014-01-24 summary:Traveling Salesman Problem (TSP) is one of the most common studied problemsin combinatorial optimization. Given the list of cities and distances betweenthem, the problem is to find the shortest tour possible which visits all thecities in list exactly once and ends in the city where it starts. Despite theTraveling Salesman Problem is NP-Hard, a lot of methods and solutions areproposed to the problem. One of them is Genetic Algorithm (GA). GA is a simplebut an efficient heuristic method that can be used to solve Traveling SalesmanProblem. In this paper, we will show a parallel genetic algorithmimplementation on MapReduce framework in order to solve Traveling SalesmanProblem. MapReduce is a framework used to support distributed computation onclusters of computers. We used free licensed Hadoop implementation as MapReduceframework.
arxiv-6000-282 | Steady-state performance of non-negative least-mean-square algorithm and its variants | http://arxiv.org/abs/1401.6376 | author:Jie Chen, José Carlos M. Bermudez, Cédric Richard category:cs.LG published:2014-01-24 summary:Non-negative least-mean-square (NNLMS) algorithm and its variants have beenproposed for online estimation under non-negativity constraints. The transientbehavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMSalgorithms have been studied in our previous work. In this technical report, wederive closed-form expressions for the steady-state excess mean-square error(EMSE) for the four algorithms. Simulations results illustrate the accuracy ofthe theoretical results. This is a complementary material to our previous work.
arxiv-6000-283 | The EM algorithm and the Laplace Approximation | http://arxiv.org/abs/1401.6276 | author:Niko Brümmer category:stat.ML published:2014-01-24 summary:The Laplace approximation calls for the computation of second derivatives atthe likelihood maximum. When the maximum is found by the EM-algorithm, there isa convenient way to compute these derivatives. The likelihood gradient can beobtained from the EM-auxiliary, while the Hessian can be obtained from thisgradient with the Pearlmutter trick.
arxiv-6000-284 | Improving Statistical Machine Translation for a Resource-Poor Language Using Related Resource-Rich Languages | http://arxiv.org/abs/1401.6876 | author:Preslav Ivanov Nakov, Hwee Tou Ng category:cs.CL published:2014-01-23 summary:We propose a novel language-independent approach for improving machinetranslation for resource-poor languages by exploiting their similarity toresource-rich ones. More precisely, we improve the translation from aresource-poor source language X_1 into a resource-rich language Y given abi-text containing a limited number of parallel sentences for X_1-Y and alarger bi-text for X_2-Y for some resource-rich language X_2 that is closelyrelated to X_1. This is achieved by taking advantage of the opportunities thatvocabulary overlap and similarities between the languages X_1 and X_2 inspelling, word order, and syntax offer: (1) we improve the word alignments forthe resource-poor language, (2) we further augment it with additionaltranslation options, and (3) we take care of potential spelling differencesthrough appropriate transliteration. The evaluation for Indonesian- >Englishusing Malay and for Spanish -> English using Portuguese and pretending Spanishis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,respectively, which is an improvement over the best rivaling approaches, whileusing much less additional data. Overall, our method cuts the amount ofnecessary "real training data by a factor of 2--5.
arxiv-6000-285 | Reasoning about Meaning in Natural Language with Compact Closed Categories and Frobenius Algebras | http://arxiv.org/abs/1401.5980 | author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Stephen Pulman, Bob Coecke category:cs.CL cs.AI math.CT published:2014-01-23 summary:Compact closed categories have found applications in modeling quantuminformation protocols by Abramsky-Coecke. They also provide semantics forLambek's pregroup algebras, applied to formalizing the grammatical structure ofnatural language, and are implicit in a distributional model of word meaningbased on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadehused the product category of pregroups with vector spaces and provided adistributional model of meaning for sentences. We recast this theory in termsof strongly monoidal functors and advance it via Frobenius algebras over vectorspaces. The former are used to formalize topological quantum field theories byAtiyah and Baez-Dolan, and the latter are used to model classical data inquantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable usto work in a single space in which meanings of words, phrases, and sentences ofany structure live. Hence we can compare meanings of different languageconstructs and enhance the applicability of the theory. We report onexperimental results on a number of language tasks and verify the theoreticalpredictions.
arxiv-6000-286 | Matrix factorization with Binary Components | http://arxiv.org/abs/1401.6024 | author:Martin Slawski, Matthias Hein, Pavlo Lutsik category:stat.ML cs.LG published:2014-01-23 summary:Motivated by an application in computational biology, we consider low-rankmatrix factorization with $\{0,1\}$-constraints on one of the factors andoptionally convex constraints on the second one. In addition to thenon-convexity shared with other matrix factorization schemes, our problem isfurther complicated by a combinatorial constraint set of size $2^{m \cdot r}$,where $m$ is the dimension of the data points and $r$ the rank of thefactorization. Despite apparent intractability, we provide - in the line ofrecent work on non-negative matrix factorization by Arora et al. (2012) - analgorithm that provably recovers the underlying factorization in the exact casewith $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain thisresult, we use theory around the Littlewood-Offord lemma from combinatorics.
arxiv-6000-287 | Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection | http://arxiv.org/abs/1401.6050 | author:Hai Zhao, Xiaotian Zhang, Chunyu Kit category:cs.CL published:2014-01-23 summary:Semantic parsing, i.e., the automatic derivation of meaning representationsuch as an instantiated predicate-argument structure for a sentence, plays acritical role in deep processing of natural language. Unlike all other topsystems of semantic dependency parsing that have to rely on a pipelineframework to chain up a series of submodels each specialized for a specificsubtask, the one presented in this article integrates everything into onemodel, in hopes of achieving desirable integrity and practicality for realapplications while maintaining a competitive performance. This integrativeapproach tackles semantic parsing as a word pair classification problem using amaximum entropy classifier. We leverage adaptive pruning of argument candidatesand large-scale feature selection engineering to allow the largest featurespace ever in use so far in this field, it achieves a state-of-the-artperformance on the evaluation data set for CoNLL-2008 shared task, on top ofall but one top pipeline system, confirming its feasibility and effectiveness.
arxiv-6000-288 | A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing | http://arxiv.org/abs/1405.5208 | author:Alexander M. Rush, Michael Collins category:cs.CL cs.AI published:2014-01-23 summary:Dual decomposition, and more generally Lagrangian relaxation, is a classicalmethod for combinatorial optimization; it has recently been applied to severalinference problems in natural language processing (NLP). This tutorial gives anoverview of the technique. We describe example algorithms, describe formalguarantees for the method, and describe practical issues in implementing thealgorithms. While our examples are predominantly drawn from the NLP literature,the material should be of general relevance to inference problems in machinelearning. A central theme of this tutorial is that Lagrangian relaxation isnaturally applied in conjunction with a broad class of combinatorialalgorithms, allowing inference in models that go significantly beyond previouswork on Lagrangian relaxation for inference in graphical models.
arxiv-6000-289 | Toward Supervised Anomaly Detection | http://arxiv.org/abs/1401.6424 | author:Nico Goernitz, Marius Micha Kloft, Konrad Rieck, Ulf Brefeld category:cs.LG published:2014-01-23 summary:Anomaly detection is being regarded as an unsupervised learning task asanomalies stem from adversarial or unlikely events with unknown distributions.However, the predictive performance of purely unsupervised anomaly detectionoften fails to match the required detection rates in many tasks and thereexists a need for labeled data to guide the model generation. Our firstcontribution shows that classical semi-supervised approaches, originating froma supervised classifier, are inappropriate and hardly detect new and unknownanomalies. We argue that semi-supervised anomaly detection needs to ground onthe unsupervised learning paradigm and devise a novel algorithm that meets thisrequirement. Although being intrinsically non-convex, we further show that theoptimization problem has a convex equivalent under relatively mild assumptions.Additionally, we propose an active learning strategy to automatically filtercandidates for labeling. In an empirical study on network intrusion detectiondata, we observe that the proposed learning methodology requires much lesslabeled data than the state-of-the-art, while achieving higher detectionaccuracies.
arxiv-6000-290 | Automatic Aggregation by Joint Modeling of Aspects and Values | http://arxiv.org/abs/1401.6422 | author:Christina Sauper, Regina Barzilay category:cs.CL published:2014-01-23 summary:We present a model for aggregation of product review snippets by joint aspectidentification and sentiment analysis. Our model simultaneously identifies anunderlying set of ratable aspects presented in the reviews of a product (e.g.,sushi and miso for a Japanese restaurant) and determines the correspondingsentiment of each aspect. This approach directly enables discovery ofhighly-rated or inconsistent aspects of a product. Our generative model admitsan efficient variational mean-field inference algorithm. It is also easilyextensible, and we describe several modifications and their effects on modelstructure and inference. We test our model on two tasks, joint aspectidentification and sentiment analysis on a set of Yelp reviews and aspectidentification alone on a set of medical summaries. We evaluate the performanceof the model on aspect identification, sentiment analysis, and per-wordlabeling accuracy. We demonstrate that our model outperforms applicablebaselines by a considerable margin, yielding up to 32% relative error reductionon aspect identification and up to 20% relative error reduction on sentimentanalysis.
arxiv-6000-291 | Identifying Bengali Multiword Expressions using Semantic Clustering | http://arxiv.org/abs/1401.6122 | author:Tanmoy Chakraborty, Dipankar Das, Sivaji Bandyopadhyay category:cs.CL published:2014-01-23 summary:One of the key issues in both natural language understanding and generationis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a hugeproblem to the precise language processing due to their idiosyncratic natureand diversity in lexical, syntactical and semantic properties. The semantics ofa MWE cannot be expressed after combining the semantics of its constituents.Therefore, the formalism of semantic clustering is often viewed as aninstrument for extracting MWEs especially for resource constraint languageslike Bengali. The present semantic clustering approach contributes to locateclusters of the synonymous noun tokens present in the document. These clustersin turn help measure the similarity between the constituent words of apotentially candidate phrase using a vector space model and judge thesuitability of this phrase to be a MWE. In this experiment, we apply thesemantic clustering approach for noun-noun bigram MWEs, though it can beextended to any types of MWEs. In parallel, the well known statistical models,namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),Significance function are also employed to extract MWEs from the Bengalicorpus. The comparative evaluation shows that the semantic clustering approachoutperforms all other competing statistical models. As a by-product of thisexperiment, we have started developing a standard lexicon in Bengali thatserves as a productive Bengali linguistic thesaurus.
arxiv-6000-292 | On Image Block Loss Restoration Using the Sparsity Pattern as Side Information | http://arxiv.org/abs/1401.5966 | author:Hossein Hosseini, Ali Goli, Neda Barzegar Marvasti, Masoume Azghani, Farokh Marvasti category:cs.MM cs.CV published:2014-01-23 summary:In this paper, we propose an image block loss restoration method based on thenotion of sparse representation. The sparsity pattern is exploited as sideinformation to efficiently restore block losses, by iteratively imposing theconstraints of spatial and transform domains on the corrupted image. Two novelfeatures, including a pre-interpolation and a criterion for stopping theiterations, are added for performance improvement. Besides, a scheme ispresented for no-reference quality estimation of the restored image withrespect to the original and sparse images. To the best of our knowledge, thisis the first attempt to estimate the image quality in the restoration methods.Also, to deal with practical applications, we develop a technique to transmitthe side information along with the image. In this technique, we first compressthe side information and then embed its LDPC coded version in the leastsignificant bits of the image pixels. This technique ensures the error-freetransmission of the side information, while causing only a small perturbationon the transmitted image. Mathematical analysis and extensive simulations areperformed to evaluate the method and investigate the efficiency of the proposedtechniques. The results verify that the suggested method outperforms itscounterparts for image block loss restoration.
arxiv-6000-293 | Towards Unsupervised Learning of Temporal Relations between Events | http://arxiv.org/abs/1401.6427 | author:Seyed Abolghasem Mirroshandel, Gholamreza Ghassem-Sani category:cs.LG cs.CL published:2014-01-23 summary:Automatic extraction of temporal relations between event pairs is animportant task for several natural language processing applications such asQuestion Answering, Information Extraction, and Summarization. Since mostexisting methods are supervised and require large corpora, which for manylanguages do not exist, we have concentrated our efforts to reduce the need forannotated data as much as possible. This paper presents two differentalgorithms towards this goal. The first algorithm is a weakly supervisedmachine learning approach for classification of temporal relations betweenevents. In the first stage, the algorithm learns a general classifier from anannotated corpus. Then, inspired by the hypothesis of "one type of temporalrelation per discourse, it extracts useful information from a cluster oftopically related documents. We show that by combining the global informationof such a cluster with local decisions of a general classifier, a bootstrappingcross-document classifier can be built to extract temporal relations betweenevents. Our experiments show that without any additional annotated data, theaccuracy of the proposed algorithm is higher than that of several previoussuccessful systems. The second proposed method for temporal relation extractionis based on the expectation maximization (EM) algorithm. Within EM, we useddifferent techniques such as a greedy best-first search and integer linearprogramming for temporal inconsistency removal. We think that the experimentalresults of our EM based algorithm, as a first step toward a fully unsupervisedtemporal relation extraction method, is encouraging.
arxiv-6000-294 | Hierarchical pixel clustering for image segmentation | http://arxiv.org/abs/1401.5891 | author:M. Kharinov category:cs.CV published:2014-01-23 summary:In the paper a piecewise constant image approximations of sequential numberof pixel clusters or segments are treated. A majorizing of optimalapproximation sequence by hierarchical sequence of image approximations isstudied. Transition from pixel clustering to image segmentation by reducing ofsegment numbers in clusters is provided. Algorithms are proved by elementaryformulas.
arxiv-6000-295 | Riffled Independence for Efficient Inference with Partial Rankings | http://arxiv.org/abs/1401.6421 | author:Jonathan Huang, Ashish Kapoor, Carlos Guestrin category:cs.LG published:2014-01-23 summary:Distributions over rankings are used to model data in a multitude of realworld settings such as preference analysis and political elections. Modelingsuch distributions presents several computational challenges, however, due tothe factorial size of the set of rankings over an item set. Some of thesechallenges are quite familiar to the artificial intelligence community, such ashow to compactly represent a distribution over a combinatorially large space,and how to efficiently perform probabilistic inference with theserepresentations. With respect to ranking, however, there is the additionalchallenge of what we refer to as human task complexity users are rarely willingto provide a full ranking over a long list of candidates, instead oftenpreferring to provide partial ranking information. Simultaneously addressingall of these challenges i.e., designing a compactly representable model whichis amenable to efficient inference and can be learned using partial rankingdata is a difficult task, but is necessary if we would like to scale toproblems with nontrivial size. In this paper, we show that the recentlyproposed riffled independence assumptions cleanly and efficiently address eachof the above challenges. In particular, we establish a tight mathematicalconnection between the concepts of riffled independence and of partialrankings. This correspondence not only allows us to then develop efficient andexact algorithms for performing inference tasks using riffled independencebased represen- tations with partial rankings, but somewhat surprisingly, alsoshows that efficient inference is not possible for riffle independent models(in a certain sense) with observations which do not take the form of partialrankings. Finally, using our inference algorithm, we introduce the first methodfor learning riffled independence based models from partially ranked data.
arxiv-6000-296 | Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image Statistics | http://arxiv.org/abs/1401.5900 | author:Nan Wang, Jan Melchior, Laurenz Wiskott category:cs.NE cs.LG stat.ML published:2014-01-23 summary:We present a theoretical analysis of Gaussian-binary restricted Boltzmannmachines (GRBMs) from the perspective of density models. The key aspect of thisanalysis is to show that GRBMs can be formulated as a constrained mixture ofGaussians, which gives a much better insight into the model's capabilities andlimitations. We show that GRBMs are capable of learning meaningful featuresboth in a two-dimensional blind source separation task and in modeling naturalimages. Further, we show that reported difficulties in training GRBMs are dueto the failure of the training algorithm rather than the model itself. Based onour analysis we are able to propose several training recipes, which allowedsuccessful and fast training in our experiments. Finally, we discuss therelationship of GRBMs to several modifications that have been proposed toimprove the model.
arxiv-6000-297 | Kernel Least Mean Square with Adaptive Kernel Size | http://arxiv.org/abs/1401.5899 | author:Badong Chen, Junli Liang, Nanning Zheng, Jose C. Principe category:stat.ML cs.LG published:2014-01-23 summary:Kernel adaptive filters (KAF) are a class of powerful nonlinear filtersdeveloped in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel isusually the default kernel in KAF algorithms, but selecting the proper kernelsize (bandwidth) is still an open important issue especially for learning withsmall sample sizes. In previous research, the kernel size was set manually orestimated in advance by Silvermans rule based on the sample distribution. Thisstudy aims to develop an online technique for optimizing the kernel size of thekernel least mean square (KLMS) algorithm. A sequential optimization strategyis proposed, and a new algorithm is developed, in which the filter weights andthe kernel size are both sequentially updated by stochastic gradient algorithmsthat minimize the mean square error (MSE). Theoretical results on convergenceare also presented. The excellent performance of the new algorithm is confirmedby simulations on static function estimation and short term chaotic time seriesprediction.
arxiv-6000-298 | Efficiently Detecting Overlapping Communities through Seeding and Semi-Supervised Learning | http://arxiv.org/abs/1401.5888 | author:Changxing Shang, Shengzhong Feng, Zhongying Zhao, Jianping Fan category:cs.SI cs.LG physics.soc-ph published:2014-01-23 summary:Seeding then expanding is a commonly used scheme to discover overlappingcommunities in a network. Most seeding methods are either too complex to scaleto large networks or too simple to select high-quality seeds, and thenon-principled functions used by most expanding methods lead to poorperformance when applied to diverse networks. This paper proposes a new methodthat transforms a network into a corpus where each edge is treated as adocument, and all nodes of the network are treated as terms of the corpus. Aneffective seeding method is also proposed that selects seeds as a training set,then a principled expanding method based on semi-supervised learning is appliedto classify edges. We compare our new algorithm with four other communitydetection algorithms on a wide range of synthetic and empirical networks.Experimental results show that the new algorithm can significantly improveclustering performance in most cases. Furthermore, the time complexity of thenew algorithm is linear to the number of edges, and this low complexity makesthe new algorithm scalable to large networks.
arxiv-6000-299 | Predicting Nearly As Well As the Optimal Twice Differentiable Regressor | http://arxiv.org/abs/1401.6413 | author:N. Denizcan Vanli, Muhammed O. Sayin, Suleyman S. Kozat category:cs.LG stat.ML published:2014-01-23 summary:We study nonlinear regression of real valued data in an individual sequencemanner, where we provide results that are guaranteed to hold without anystatistical assumptions. We address the convergence and undertraining issues ofconventional nonlinear regression methods and introduce an algorithm thatelegantly mitigates these issues via an incremental hierarchical structure,(i.e., via an incremental decision tree). Particularly, we present a piecewiselinear (or nonlinear) regression algorithm that partitions the regressor spacein a data driven manner and learns a linear model at each region. Unlike theconventional approaches, our algorithm gradually increases the number ofdisjoint partitions on the regressor space in a sequential manner according tothe observed data. Through this data driven approach, our algorithmsequentially and asymptotically achieves the performance of the optimal twicedifferentiable regression function for any data sequence with an unknown andarbitrary length. The computational complexity of the introduced algorithm isonly logarithmic in the data length under certain regularity conditions. Weprovide the explicit description of the algorithm and demonstrate thesignificant gains for the well-known benchmark real data sets and chaoticsignals.
arxiv-6000-300 | Iterative Universal Hash Function Generator for Minhashing | http://arxiv.org/abs/1401.6124 | author:Fabricio Olivetti de Franca category:cs.LG cs.IR published:2014-01-23 summary:Minhashing is a technique used to estimate the Jaccard Index between two setsby exploiting the probability of collision in a random permutation. In order tospeed up the computation, a random permutation can be approximated by using anuniversal hash function such as the $h_{a,b}$ function proposed by Carter andWegman. A better estimate of the Jaccard Index can be achieved by using many ofthese hash functions, created at random. In this paper a new iterativeprocedure to generate a set of $h_{a,b}$ functions is devised that eliminatesthe need for a list of random values and avoid the multiplication operationduring the calculation. The properties of the generated hash functions remainsthat of an universal hash function family. This is possible due to the randomnature of features occurrence on sparse datasets. Results show that theuniformity of hashing the features is maintaned while obtaining a speed up ofup to $1.38$ compared to the traditional approach.
