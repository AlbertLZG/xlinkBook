arxiv-6000-1 | Computing Consensus Curves | http://arxiv.org/pdf/1212.0935v5.pdf | author:Livio De La Cruz, Stephen Kobourov, Sergey Pupyrev, Paul Shen, Sankar Veeramoni category:cs.CG cs.CV cs.GT cs.MA published:2012-12-05 summary:We consider the problem of extracting accurate average ant trajectories frommany (possibly inaccurate) input trajectories contributed by citizenscientists. Although there are many generic software tools for motion trackingand specific ones for insect tracking, even untrained humans are much better atthis task, provided a robust method to computing the average trajectories. Weimplemented and tested several local (one ant at a time) and global (all antstogether) method. Our best performing algorithm uses a novel global method,based on finding edge-disjoint paths in an ant-interaction graph constructedfrom the input trajectories. The underlying optimization problem is a new andinteresting variant of network flow. Even though the problem is NP-hard, weimplemented two heuristics, which work very well in practice, outperforming allother approaches, including the best automated system.
arxiv-6000-2 | Understanding Alternating Minimization for Matrix Completion | http://arxiv.org/pdf/1312.0925v3.pdf | author:Moritz Hardt category:cs.LG cs.DS stat.ML published:2013-12-03 summary:Alternating Minimization is a widely used and empirically successfulheuristic for matrix completion and related low-rank optimization problems.Theoretical guarantees for Alternating Minimization have been hard to come byand are still poorly understood. This is in part because the heuristic isiterative and non-convex in nature. We give a new algorithm based onAlternating Minimization that provably recovers an unknown low-rank matrix froma random subsample of its entries under a standard incoherence assumption. Ourresults reduce the sample size requirements of the Alternating Minimizationapproach by at least a quartic factor in the rank and the condition number ofthe unknown matrix. These improvements apply even if the matrix is only closeto low-rank in the Frobenius norm. Our algorithm runs in nearly linear time inthe dimension of the matrix and, in a broad range of parameters, gives thestrongest sample bounds among all subquadratic time algorithms that we areaware of. Underlying our work is a new robust convergence analysis of the well-knownPower Method for computing the dominant singular vectors of a matrix. Thisviewpoint leads to a conceptually simple understanding of AlternatingMinimization. In addition, we contribute a new technique for controlling thecoherence of intermediate solutions arising in iterative algorithms based on asmoothed analysis of the QR factorization. These techniques may be of interestbeyond their application here.
arxiv-6000-3 | Monte Carlo non local means: Random sampling for large-scale image filtering | http://arxiv.org/pdf/1312.7366v3.pdf | author:Stanley H. Chan, Todd Zickler, Yue M. Lu category:cs.CV stat.CO published:2013-12-27 summary:We propose a randomized version of the non-local means (NLM) algorithm forlarge-scale image filtering. The new algorithm, called Monte Carlo non-localmeans (MCNLM), speeds up the classical NLM by computing a small subset of imagepatch distances, which are randomly selected according to a designed samplingpattern. We make two contributions. First, we analyze the performance of theMCNLM algorithm and show that, for large images or large external imagedatabases, the random outcomes of MCNLM are tightly concentrated around thedeterministic full NLM result. In particular, our error probability bounds showthat, at any given sampling ratio, the probability for MCNLM to have a largedeviation from the original NLM solution decays exponentially as the size ofthe image or database grows. Second, we derive explicit formulas for optimalsampling patterns that minimize the error probability bound by exploitingpartial knowledge of the pairwise similarity weights. Numerical experimentsshow that MCNLM is competitive with other state-of-the-art fast NLM algorithmsfor single-image denoising. When applied to denoising images using an externaldatabase containing ten billion patches, MCNLM returns a randomized solutionthat is within 0.2 dB of the full NLM solution while reducing the runtime bythree orders of magnitude.
arxiv-6000-4 | Credal Model Averaging for classification: representing prior ignorance and expert opinions | http://arxiv.org/pdf/1405.3559v1.pdf | author:Giorgio Corani, Andrea Mignatti category:stat.ME cs.AI q-bio.PE stat.ML published:2014-05-14 summary:Bayesian model averaging (BMA) is the state of the art approach forovercoming model uncertainty. Yet, especially on small data sets, the resultsyielded by BMA might be sensitive to the prior over the models. Credal ModelAveraging (CMA) addresses this problem by substituting the single prior overthe models by a set of priors (credal set). Such approach solves the problem ofhow to choose the prior over the models and automates sensitivity analysis. Wediscuss various CMA algorithms for building an ensemble of logistic regressorscharacterized by different sets of covariates. We show how CMA can beappropriately tuned to the case in which one is prior-ignorant and to the casein which instead domain knowledge is available. CMA detects prior-dependentinstances, namely instances in which a different class is more probabledepending on the prior over the models. On such instances CMA suspends thejudgment, returning multiple classes. We thoroughly compare different BMA andCMA variants on a real case study, predicting presence of Alpine marmot burrowsin an Alpine valley. We find that BMA is almost a random guesser on theinstances recognized as prior-dependent by CMA.
arxiv-6000-5 | Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques | http://arxiv.org/pdf/1405.3536v1.pdf | author:Olivier Nicol, Jérémie Mary, Philippe Preux category:stat.ML cs.LG published:2014-05-14 summary:In many recommendation applications such as news recommendation, the itemsthat can be rec- ommended come and go at a very fast pace. This is a challengefor recommender systems (RS) to face this setting. Online learning algorithmsseem to be the most straight forward solution. The contextual bandit frameworkwas introduced for that very purpose. In general the evaluation of a RS is acritical issue. Live evaluation is of- ten avoided due to the potential loss ofrevenue, hence the need for offline evaluation methods. Two options areavailable. Model based meth- ods are biased by nature and are thus difficult totrust when used alone. Data driven methods are therefore what we consider here.Evaluat- ing online learning algorithms with past data is not simple but somemethods exist in the litera- ture. Nonetheless their accuracy is not satisfac-tory mainly due to their mechanism of data re- jection that only allow theexploitation of a small fraction of the data. We precisely address this issuein this paper. After highlighting the limita- tions of the previous methods, wepresent a new method, based on bootstrapping techniques. This new method comeswith two important improve- ments: it is much more accurate and it provides ameasure of quality of its estimation. The latter is a highly desirable propertyin order to minimize the risks entailed by putting online a RS for the firsttime. We provide both theoretical and ex- perimental proofs of its superioritycompared to state-of-the-art methods, as well as an analysis of the convergenceof the measure of quality.
arxiv-6000-6 | Temporal Analysis of Language through Neural Language Models | http://arxiv.org/pdf/1405.3515v1.pdf | author:Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, Slav Petrov category:cs.CL published:2014-05-14 summary:We provide a method for automatically detecting change in language acrosstime through a chronologically trained neural language model. We train themodel on the Google Books Ngram corpus to obtain word vector representationsspecific to each year, and identify words that have changed significantly from1900 to 2009. The model identifies words such as "cell" and "gay" as havingchanged during that time period. The model simultaneously identifies thespecific years during which such words underwent change.
arxiv-6000-7 | Measuring Global Similarity between Texts | http://arxiv.org/pdf/1403.4024v3.pdf | author:Uli Fahrenberg, Fabrizio Biondi, Kevin Corre, Cyrille Jegourel, Simon Kongshøj, Axel Legay category:cs.CL published:2014-03-17 summary:We propose a new similarity measure between texts which, contrary to thecurrent state-of-the-art approaches, takes a global view of the texts to becompared. We have implemented a tool to compute our textual distance andconducted experiments on several corpuses of texts. The experiments show thatour methods can reliably identify different global types of texts.
arxiv-6000-8 | Efficient classification using parallel and scalable compressed model and Its application on intrusion detection | http://arxiv.org/pdf/1405.3410v1.pdf | author:Tieming Chen, Xu Zhang, Shichao Jin, Okhee Kim category:cs.LG cs.CR published:2014-05-14 summary:In order to achieve high efficiency of classification in intrusion detection,a compressed model is proposed in this paper which combines horizontalcompression with vertical compression. OneR is utilized as horizontalcom-pression for attribute reduction, and affinity propagation is employed asvertical compression to select small representative exemplars from largetraining data. As to be able to computationally compress the larger volume oftraining data with scalability, MapReduce based parallelization approach isthen implemented and evaluated for each step of the model compression processabovementioned, on which common but efficient classification methods can bedirectly used. Experimental application study on two publicly availabledatasets of intrusion detection, KDD99 and CMDC2012, demonstrates that theclassification using the compressed model proposed can effectively speed up thedetection procedure at up to 184 times, most importantly at the cost of aminimal accuracy difference with less than 1% on average.
arxiv-6000-9 | Reducing Dueling Bandits to Cardinal Bandits | http://arxiv.org/pdf/1405.3396v1.pdf | author:Nir Ailon, Thorsten Joachims, Zohar Karnin category:cs.LG published:2014-05-14 summary:We present algorithms for reducing the Dueling Bandits problem to theconventional (stochastic) Multi-Armed Bandits problem. The Dueling Banditsproblem is an online model of learning with ordinal feedback of the form "A ispreferred to B" (as opposed to cardinal feedback like "A has value 2.5"),giving it wide applicability in learning from implicit user feedback andrevealed and stated preferences. In contrast to existing algorithms for theDueling Bandits problem, our reductions -- named $\Doubler$, $\MultiSbm$ and$\DoubleSbm$ -- provide a generic schema for translating the extensive body ofknown results about conventional Multi-Armed Bandit algorithms to the DuelingBandits setting. For $\Doubler$ and $\MultiSbm$ we prove regret upper bounds inboth finite and infinite settings, and conjecture about the performance of$\DoubleSbm$ which empirically outperforms the other two as well as previousalgorithms in our experiments. In addition, we provide the first almost optimalregret bound in terms of second order terms, such as the differences betweenthe values of the arms.
arxiv-6000-10 | Active Mining of Parallel Video Streams | http://arxiv.org/pdf/1405.3382v1.pdf | author:Samaneh Khoshrou, Jaime S. Cardoso, Luis F. Teixeira category:cs.CV cs.LG published:2014-05-14 summary:The practicality of a video surveillance system is adversely limited by theamount of queries that can be placed on human resources and their vigilance inresponse. To transcend this limitation, a major effort under way is to includesoftware that (fully or at least semi) automatically mines video footage,reducing the burden imposed to the system. Herein, we propose a semi-supervisedincremental learning framework for evolving visual streams in order to developa robust and flexible track classification system. Our proposed method learnsfrom consecutive batches by updating an ensemble in each time. It tries tostrike a balance between performance of the system and amount of data whichneeds to be labelled. As no restriction is considered, the system can addressmany practical problems in an evolving multi-camera scenario, such as conceptdrift, class evolution and various length of video streams which have not beenaddressed before. Experiments were performed on synthetic as well as real-worldvisual data in non-stationary environments, showing high accuracy with fairlylittle human collaboration.
arxiv-6000-11 | Learning rates for the risk of kernel based quantile regression estimators in additive models | http://arxiv.org/pdf/1405.3379v1.pdf | author:Andreas Christmann, Ding-Xuan Zhou category:stat.ML published:2014-05-14 summary:Additive models play an important role in semiparametric statistics. Thispaper gives learning rates for regularized kernel based methods for additivemodels. These learning rates compare favourably in particular in highdimensions to recent results on optimal learning rates for purely nonparametricregularized kernel based quantile regression using the Gaussian radial basisfunction kernel, provided the assumption of an additive model is valid.Additionally, a concrete example is presented to show that a Gaussian functiondepending only on one variable lies in a reproducing kernel Hilbert spacegenerated by an additive Gaussian kernel, but does not belong to thereproducing kernel Hilbert space generated by the multivariate Gaussian kernelof the same variance.
arxiv-6000-12 | G-AMA: Sparse Gaussian graphical model estimation via alternating minimization | http://arxiv.org/pdf/1405.3034v2.pdf | author:Onkar Dalal, Bala Rajaratnam category:stat.CO stat.ML published:2014-05-13 summary:Several methods have been recently proposed for estimating sparse Gaussiangraphical models using $\ell_{1}$ regularization on the inverse covariancematrix. Despite recent advances, contemporary applications require methods thatare even faster in order to handle ill-conditioned high dimensional modern daydatasets. In this paper, we propose a new method, G-AMA, to solve the sparseinverse covariance estimation problem using Alternating Minimization Algorithm(AMA), that effectively works as a proximal gradient algorithm on the dualproblem. Our approach has several novel advantages over existing methods.First, we demonstrate that G-AMA is faster than the previous best algorithms bymany orders of magnitude and is thus an ideal approach for modern highthroughput applications. Second, global linear convergence of G-AMA isdemonstrated rigorously, underscoring its good theoretical properties. Third,the dual algorithm operates on the covariance matrix, and thus easilyfacilitates incorporating additional constraints on pairwise/marginalrelationships between feature pairs based on domain specific knowledge. Overand above estimating a sparse inverse covariance matrix, we also illustrate howto (1) incorporate constraints on the (bivariate) correlations and, (2)incorporate equality (equisparsity) or linear constraints between individualinverse covariance elements. Fourth, we also show that G-AMA is better adept athandling extremely ill-conditioned problems, as is often the case with realdata. The methodology is demonstrated on both simulated and real datasets toillustrate its superior performance over recently proposed methods.
arxiv-6000-13 | A General Homogeneous Matrix Formulation to 3D Rotation Geometric Transformations | http://arxiv.org/pdf/1404.6055v2.pdf | author:Feng Lu, Ziqiang Chen category:cs.CV published:2014-04-24 summary:We present algebraic projective geometry definitions of 3D rotations so as tobridge a small gap between the applications and the definitions of 3D rotationsin homogeneous matrix form. A general homogeneous matrix formulation to 3Drotation geometric transformations is proposed which suits for the cases whenthe rotation axis is unnecessarily through the coordinate system origin giventheir rotation axes and rotation angles.
arxiv-6000-14 | Group-based Sparse Representation for Image Restoration | http://arxiv.org/pdf/1405.3351v1.pdf | author:Jian Zhang, Debin Zhao, Wen Gao category:cs.CV published:2014-05-14 summary:Traditional patch-based sparse representation modeling of natural imagesusually suffer from two problems. First, it has to solve a large-scaleoptimization problem with high computational complexity in dictionary learning.Second, each patch is considered independently in dictionary learning andsparse coding, which ignores the relationship among patches, resulting ininaccurate sparse coding coefficients. In this paper, instead of using patch asthe basic unit of sparse representation, we exploit the concept of group as thebasic unit of sparse representation, which is composed of nonlocal patches withsimilar structures, and establish a novel sparse representation modeling ofnatural images, called group-based sparse representation (GSR). The proposedGSR is able to sparsely represent natural images in the domain of group, whichenforces the intrinsic local sparsity and nonlocal self-similarity of imagessimultaneously in a unified framework. Moreover, an effective self-adaptivedictionary learning method for each group with low complexity is designed,rather than dictionary learning from natural images. To make GSR tractable androbust, a split Bregman based technique is developed to solve the proposedGSR-driven minimization problem for image restoration efficiently. Extensiveexperiments on image inpainting, image deblurring and image compressive sensingrecovery manifest that the proposed GSR modeling outperforms many currentstate-of-the-art schemes in both PSNR and visual perception.
arxiv-6000-15 | Thresholding Classifiers to Maximize F1 Score | http://arxiv.org/pdf/1402.1892v2.pdf | author:Zachary Chase Lipton, Charles Elkan, Balakrishnan Narayanaswamy category:stat.ML cs.IR cs.LG published:2014-02-08 summary:This paper provides new insight into maximizing F1 scores in the context ofbinary classification and also in the context of multilabel classification. Theharmonic mean of precision and recall, F1 score is widely used to measure thesuccess of a binary classifier when one class is rare. Micro average, macroaverage, and per instance average F1 scores are used in multilabelclassification. For any classifier that produces a real-valued output, wederive the relationship between the best achievable F1 score and thedecision-making threshold that achieves this optimum. As a special case, if theclassifier outputs are well-calibrated conditional probabilities, then theoptimal threshold is half the optimal F1 score. As another special case, if theclassifier is completely uninformative, then the optimal behavior is toclassify all examples as positive. Since the actual prevalence of positiveexamples typically is low, this behavior can be considered undesirable. As acase study, we discuss the results, which can be surprising, of applying thisprocedure when predicting 26,853 labels for Medline documents.
arxiv-6000-16 | Adaptive Monte Carlo via Bandit Allocation | http://arxiv.org/pdf/1405.3318v1.pdf | author:James Neufeld, András György, Dale Schuurmans, Csaba Szepesvári category:cs.AI cs.LG published:2014-05-13 summary:We consider the problem of sequentially choosing between a set of unbiasedMonte Carlo estimators to minimize the mean-squared-error (MSE) of a finalcombined estimate. By reducing this task to a stochastic multi-armed banditproblem, we show that well developed allocation strategies can be used toachieve an MSE that approaches that of the best estimator chosen in retrospect.We then extend these developments to a scenario where alternative estimatorshave different, possibly stochastic costs. The outcome is a new set of adaptiveMonte Carlo strategies that provide stronger guarantees than previousapproaches while offering practical advantages.
arxiv-6000-17 | Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-stationary Rewards | http://arxiv.org/pdf/1405.3316v1.pdf | author:Omar Besbes, Yonatan Gur, Assaf Zeevi category:cs.LG math.OC stat.ML published:2014-05-13 summary:In a multi-armed bandit (MAB) problem a gambler needs to choose at each roundof play one of K arms, each characterized by an unknown reward distribution.Reward realizations are only observed when an arm is selected, and thegambler's objective is to maximize his cumulative expected earnings over somegiven horizon of play T. To do this, the gambler needs to acquire informationabout arms (exploration) while simultaneously optimizing immediate rewards(exploitation); the price paid due to this trade off is often referred to asthe regret, and the main question is how small can this price be as a functionof the horizon length T. This problem has been studied extensively when thereward distributions do not change over time; an assumption that supports asharp characterization of the regret, yet is often violated in practicalsettings. In this paper, we focus on a MAB formulation which allows for a broadrange of temporal uncertainties in the rewards, while still maintainingmathematical tractability. We fully characterize the (regret) complexity ofthis class of MAB problems by establishing a direct link between the extent ofallowable reward "variation" and the minimal achievable regret. Our analysisdraws some connections between two rather disparate strands of literature: theadversarial and the stochastic MAB frameworks.
arxiv-6000-18 | Communication Efficient Distributed Optimization using an Approximate Newton-type Method | http://arxiv.org/pdf/1312.7853v4.pdf | author:Ohad Shamir, Nathan Srebro, Tong Zhang category:cs.LG math.OC stat.ML published:2013-12-30 summary:We present a novel Newton-type method for distributed optimization, which isparticularly well suited for stochastic optimization and learning problems. Forquadratic objectives, the method enjoys a linear rate of convergence whichprovably \emph{improves} with the data size, requiring an essentially constantnumber of iterations under reasonable assumptions. We provide theoretical andempirical evidence of the advantages of our method compared to otherapproaches, such as one-shot parameter averaging and ADMM.
arxiv-6000-19 | Effects of Sampling Methods on Prediction Quality. The Case of Classifying Land Cover Using Decision Trees | http://arxiv.org/pdf/1405.3295v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:stat.ML cs.LG stat.AP published:2014-05-13 summary:Clever sampling methods can be used to improve the handling of big data andincrease its usefulness. The subject of this study is remote sensing,specifically airborne laser scanning point clouds representing differentclasses of ground cover. The aim is to derive a supervised learning model forthe classification using CARTs. In order to measure the effect of differentsampling methods on the classification accuracy, various experiments withvarying types of sampling methods, sample sizes, and accuracy metrics have beendesigned. Numerical results for a subset of a large surveying project coveringthe lower Rhine area in Germany are shown. General conclusions regardingsampling design are drawn and presented.
arxiv-6000-20 | Learning with many experts: model selection and sparsity | http://arxiv.org/pdf/1405.3292v1.pdf | author:Rafael Izbicki, Rafael Bassi Stern category:stat.ME cs.LG published:2014-05-13 summary:Experts classifying data are often imprecise. Recently, several models havebeen proposed to train classifiers using the noisy labels generated by theseexperts. How to choose between these models? In such situations, the truelabels are unavailable. Thus, one cannot perform model selection using thestandard versions of methods such as empirical risk minimization and crossvalidation. In order to allow model selection, we present a surrogate loss andprovide theoretical guarantees that assure its consistency. Next, we discusshow this loss can be used to tune a penalization which introduces sparsity inthe parameters of a traditional class of models. Sparsity provides moreparsimonious models and can avoid overfitting. Nevertheless, it has seldom beendiscussed in the context of noisy labels due to the difficulty in modelselection and, therefore, in choosing tuning parameters. We apply thesetechniques to several sets of simulated and real data.
arxiv-6000-21 | How to Ask for a Favor: A Case Study on the Success of Altruistic Requests | http://arxiv.org/pdf/1405.3282v1.pdf | author:Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky category:cs.CL cs.SI physics.soc-ph I.2.7; J.4 published:2014-05-13 summary:Requests are at the core of many social media systems such as question &answer sites and online philanthropy communities. While the success of suchrequests is critical to the success of the community, the factors that leadcommunity members to satisfy a request are largely unknown. Success of arequest depends on factors like who is asking, how they are asking, when arethey asking, and most critically what is being requested, ranging from smallfavors to substantial monetary donations. We present a case study of altruisticrequests in an online community where all requests ask for the very samecontribution and do not offer anything tangible in return, allowing us todisentangle what is requested from textual and social factors. Drawing fromsocial psychology literature, we extract high-level social features from textthat operationalize social relations between recipient and donor anddemonstrate that these extracted relations are predictive of success. Morespecifically, we find that clearly communicating need through the narrative isessential and that that linguistic indications of gratitude, evidentiality, andgeneralized reciprocity, as well as high status of the asker further increasethe likelihood of success. Building on this understanding, we develop a modelthat can predict the success of unseen requests, significantly improving overseveral baselines. We link these findings to research in psychology on helpingbehavior, providing a basis for further analysis of success in social mediasystems.
arxiv-6000-22 | Scalable sparse covariance estimation via self-concordance | http://arxiv.org/pdf/1405.3263v1.pdf | author:Anastasios Kyrillidis, Rabeeh Karimi Mahabadi, Quoc Tran-Dinh, Volkan Cevher category:stat.ML cs.IT math.IT math.OC published:2014-05-13 summary:We consider the class of convex minimization problems, composed of aself-concordant function, such as the $\log\det$ metric, a convex data fidelityterm $h(\cdot)$ and, a regularizing -- possibly non-smooth -- function$g(\cdot)$. This type of problems have recently attracted a great deal ofinterest, mainly due to their omnipresence in top-notch applications. Underthis \emph{locally} Lipschitz continuous gradient setting, we analyze theconvergence behavior of proximal Newton schemes with the added twist of aprobable presence of inexact evaluations. We prove attractive convergence rateguarantees and enhance state-of-the-art optimization schemes to accommodatesuch developments. Experimental results on sparse covariance estimation showthe merits of our algorithm, both in terms of recovery efficiency andcomplexity.
arxiv-6000-23 | Rate of Convergence and Error Bounds for LSTD($λ$) | http://arxiv.org/pdf/1405.3229v1.pdf | author:Manel Tagorti, Bruno Scherrer category:cs.LG cs.AI math.OC math.ST stat.TH published:2014-05-13 summary:We consider LSTD($\lambda$), the least-squares temporal-difference algorithmwith eligibility traces algorithm proposed by Boyan (2002). It computes alinear approximation of the value function of a fixed policy in a large MarkovDecision Process. Under a $\beta$-mixing assumption, we derive, for any valueof $\lambda \in (0,1)$, a high-probability estimate of the rate of convergenceof this algorithm to its limit. We deduce a high-probability bound on the errorof this algorithm, that extends (and slightly improves) that derived by Lazaricet al. (2012) in the specific case where $\lambda=0$. In particular, ouranalysis sheds some light on the choice of $\lambda$ with respect to thequality of the chosen linear space and the number of samples, that complieswith simulations.
arxiv-6000-24 | Randomized Nonlinear Component Analysis | http://arxiv.org/pdf/1402.0119v2.pdf | author:David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, Bernhard Schölkopf category:stat.ML cs.LG published:2014-02-01 summary:Classical methods such as Principal Component Analysis (PCA) and CanonicalCorrelation Analysis (CCA) are ubiquitous in statistics. However, thesetechniques are only able to reveal linear relationships in data. Althoughnonlinear variants of PCA and CCA have been proposed, these are computationallyprohibitive in the large scale. In a separate strand of recent research, randomized methods have beenproposed to construct features that help reveal nonlinear patterns in data. Forbasic tasks such as regression or classification, random features exhibitlittle or no loss in performance, while achieving drastic savings incomputational requirements. In this paper we leverage randomness to design scalable new variants ofnonlinear PCA and CCA; our ideas extend to key multivariate analysis tools suchas spectral clustering or LDA. We demonstrate our algorithms throughexperiments on real-world data, on which we compare against thestate-of-the-art. A simple R implementation of the presented algorithms isprovided.
arxiv-6000-25 | Locally Boosted Graph Aggregation for Community Detection | http://arxiv.org/pdf/1405.3210v1.pdf | author:Jeremy Kun, Rajmonda Caceres, Kevin Carter category:cs.LG cs.SI physics.soc-ph published:2014-05-13 summary:Learning the right graph representation from noisy, multi-source data hasgarnered significant interest in recent years. A central tenet of this problemis relational learning. Here the objective is to incorporate the partialinformation each data source gives us in a way that captures the trueunderlying relationships. To address this challenge, we present a general,boosting-inspired framework for combining weak evidence of entity associationsinto a robust similarity metric. Building on previous work, we explore theextent to which different local quality measurements yield graphrepresentations that are suitable for community detection. We present empiricalresults on a variety of datasets demonstrating the utility of this framework,especially with respect to real datasets where noise and scale present seriouschallenges. Finally, we prove a convergence theorem in an ideal setting andoutline future research into other application domains.
arxiv-6000-26 | An Intelligent Pixel Replication Technique by Binary Decomposition for Digital Image Zooming | http://arxiv.org/pdf/1405.3195v1.pdf | author:Kaeser M Sabrin, M Haider Ali category:cs.CV published:2014-05-13 summary:Image zooming is the process of enlarging the spatial resolution of a givendigital image. We present a novel technique that intelligently modifies theclassical pixel replication method for zooming. Our method decomposes a givenimage into layer of binary images, interpolates them by magnifying the binarypatterns preserving their geometric shape and finally aggregates them all toobtain the zoomed image. Although the quality of our zoomed images is muchhigher than that of nearest neighbor and bilinear interpolation and comparablewith bicubic interpolation, the running time of our technique is extremely fastlike nearest neighbor interpolation and much faster than bilinear and bicubicinterpolation.
arxiv-6000-27 | Clustering, Hamming Embedding, Generalized LSH and the Max Norm | http://arxiv.org/pdf/1405.3167v1.pdf | author:Behnam Neyshabur, Yury Makarychev, Nathan Srebro category:cs.LG published:2014-05-13 summary:We study the convex relaxation of clustering and hamming embedding, focusingon the asymmetric case (co-clustering and asymmetric hamming embedding),understanding their relationship to LSH as studied by (Charikar 2002) and tothe max-norm ball, and the differences between their symmetric and asymmetricversions.
arxiv-6000-28 | Sparse Matrix Factorization | http://arxiv.org/pdf/1311.3315v3.pdf | author:Behnam Neyshabur, Rina Panigrahy category:cs.LG stat.ML published:2013-11-13 summary:We investigate the problem of factorizing a matrix into several sparsematrices and propose an algorithm for this under randomness and sparsityassumptions. This problem can be viewed as a simplification of the deeplearning problem where finding a factorization corresponds to finding edges indifferent layers and values of hidden units. We prove that under certainassumptions for a sparse linear deep network with $n$ nodes in each layer, ouralgorithm is able to recover the structure of the network and values of toplayer hidden units for depths up to $\tilde O(n^{1/6})$. We further discuss therelation among sparse matrix factorization, deep learning, sparse recovery anddictionary learning.
arxiv-6000-29 | Circulant Binary Embedding | http://arxiv.org/pdf/1405.3162v1.pdf | author:Felix X. Yu, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang category:stat.ML cs.LG published:2014-05-13 summary:Binary embedding of high-dimensional data requires long codes to preserve thediscriminative power of the input space. Traditional binary coding methodsoften suffer from very high computation and storage costs in such a scenario.To address this problem, we propose Circulant Binary Embedding (CBE) whichgenerates binary codes by projecting the data with a circulant matrix. Thecirculant structure enables the use of Fast Fourier Transformation to speed upthe computation. Compared to methods that use unstructured matrices, theproposed method improves the time complexity from $\mathcal{O}(d^2)$ to$\mathcal{O}(d\log{d})$, and the space complexity from $\mathcal{O}(d^2)$ to$\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a noveltime-frequency alternating optimization to learn data-dependent circulantprojections, which alternatively minimizes the objective in original andFourier domains. We show by extensive experiments that the proposed approachgives much better performance than the state-of-the-art approaches for fixedtime, and provides much faster computation with no performance degradation forfixed number of bits.
arxiv-6000-30 | Accelerating Minibatch Stochastic Gradient Descent using Stratified Sampling | http://arxiv.org/pdf/1405.3080v1.pdf | author:Peilin Zhao, Tong Zhang category:stat.ML cs.LG math.OC published:2014-05-13 summary:Stochastic Gradient Descent (SGD) is a popular optimization method which hasbeen applied to many important machine learning tasks such as Support VectorMachines and Deep Neural Networks. In order to parallelize SGD, minibatchtraining is often employed. The standard approach is to uniformly sample aminibatch at each step, which often leads to high variance. In this paper wepropose a stratified sampling strategy, which divides the whole dataset intoclusters with low within-cluster variance; we then take examples from theseclusters using a stratified sampling technique. It is shown that theconvergence rate can be significantly improved by the algorithm. Encouragingexperimental results confirm the effectiveness of the proposed method.
arxiv-6000-31 | Dimensionality reduction for click-through rate prediction: Dense versus sparse representation | http://arxiv.org/pdf/1311.6976v2.pdf | author:Bjarne Ørum Fruergaard, Toke Jansen Hansen, Lars Kai Hansen category:stat.ML cs.LG stat.AP stat.ME published:2013-11-27 summary:In online advertising, display ads are increasingly being placed based onreal-time auctions where the advertiser who wins gets to serve the ad. This iscalled real-time bidding (RTB). In RTB, auctions have very tight timeconstraints on the order of 100ms. Therefore mechanisms for biddingintelligently such as clickthrough rate prediction need to be sufficientlyfast. In this work, we propose to use dimensionality reduction of theuser-website interaction graph in order to produce simplified features of usersand websites that can be used as predictors of clickthrough rate. Wedemonstrate that the Infinite Relational Model (IRM) as a dimensionalityreduction offers comparable predictive performance to conventionaldimensionality reduction schemes, while achieving the most economical usage offeatures and fastest computations at run-time. For applications such asreal-time bidding, where fast database I/O and few computations are key tosuccess, we thus recommend using IRM based features as predictors to exploitthe recommender effects from bipartite graphs.
arxiv-6000-32 | Phonetic based SoundEx & ShapeEx algorithm for Sindhi Spell Checker System | http://arxiv.org/pdf/1405.3033v1.pdf | author:Zeeshan Bhatti, Ahmad Waqas, Imdad Ali Ismaili, Dil Nawaz Hakro, Waseem Javaid Soomro category:cs.CL published:2014-05-13 summary:This paper presents a novel combinational phonetic algorithm for SindhiLanguage, to be used in developing Sindhi Spell Checker which has yet not beendeveloped prior to this work. The compound textual forms and glyphs of Sindhilanguage presents a substantial challenge for developing Sindhi spell checkersystem and generating similar suggestion list for misspelled words. In order toimplement such a system, phonetic based Sindhi language rules and patterns mustbe considered into account for increasing the accuracy and efficiency. Theproposed system is developed with a blend between Phonetic based SoundExalgorithm and ShapeEx algorithm for pattern or glyph matching, generatingaccurate and efficient suggestion list for incorrect or misspelled Sindhiwords. A table of phonetically similar sounding Sindhi characters for SoundExalgorithm is also generated along with another table containing similar glyphor shape based character groups for ShapeEx algorithm. Both these are firstever attempt of any such type of categorization and representation for SindhiLanguage.
arxiv-6000-33 | A Neuron as a Signal Processing Device | http://arxiv.org/pdf/1405.2951v1.pdf | author:Tao Hu, Zaid J. Towfic, Cengiz Pehlevan, Alex Genkin, Dmitri B. Chklovskii category:q-bio.NC stat.ML published:2014-05-12 summary:A neuron is a basic physiological and computational unit of the brain. Whilemuch is known about the physiological properties of a neuron, its computationalrole is poorly understood. Here we propose to view a neuron as a signalprocessing device that represents the incoming streaming data matrix as asparse vector of synaptic weights scaled by an outgoing sparse activity vector.Formally, a neuron minimizes a cost function comprising a cumulative squaredrepresentation error and regularization terms. We derive an online algorithmthat minimizes such cost function by alternating between the minimization withrespect to activity and with respect to synaptic weights. The steps of thisalgorithm reproduce well-known physiological properties of a neuron, such asweighted summation and leaky integration of synaptic inputs, as well as anOja-like, but parameter-free, synaptic learning rule. Our theoretical frameworkmakes several predictions, some of which can be verified by the existing data,others require further experiments. Such framework should allow modeling thefunction of neuronal circuits without necessarily measuring all the microscopicbiophysical parameters, as well as facilitate the design of neuromorphicelectronics.
arxiv-6000-34 | Cross-view Action Modeling, Learning and Recognition | http://arxiv.org/pdf/1405.2941v1.pdf | author:Jiang wang, Xiaohan Nie, Yin Xia, Ying Wu, Song-Chun Zhu category:cs.CV published:2014-05-12 summary:Existing methods on video-based action recognition are generallyview-dependent, i.e., performing recognition from the same views seen in thetraining data. We present a novel multiview spatio-temporal AND-OR graph(MST-AOG) representation for cross-view action recognition, i.e., therecognition is performed on the video from an unknown and unseen view. As acompositional model, MST-AOG compactly represents the hierarchicalcombinatorial structures of cross-view actions by explicitly modeling thegeometry, appearance and motion variations. This paper proposes effectivemethods to learn the structure and parameters of MST-AOG. The inference basedon MST-AOG enables action recognition from novel views. The training of MST-AOGtakes advantage of the 3D human skeleton data obtained from Kinect cameras toavoid annotating enormous multi-view video frames, which is error-prone andtime-consuming, but the recognition does not need 3D information and is basedon 2D video input. A new Multiview Action3D dataset has been created and willbe released. Extensive experiments have demonstrated that this new actionrepresentation significantly improves the accuracy and robustness forcross-view action recognition on 2D videos.
arxiv-6000-35 | Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm | http://arxiv.org/pdf/1405.2936v1.pdf | author:Hadi Daneshmand, Manuel Gomez-Rodriguez, Le Song, Bernhard Schoelkopf category:cs.SI physics.soc-ph stat.ML published:2014-05-12 summary:Information spreads across social and technological networks, but often thenetwork structures are hidden from us and we only observe the traces left bythe diffusion processes, called cascades. Can we recover the hidden networkstructures from these observed cascades? What kind of cascades and how manycascades do we need? Are there some network structures which are more difficultthan others to recover? Can we design efficient inference algorithms withprovable guarantees? Despite the increasing availability of cascade data and methods for inferringnetworks from these data, a thorough theoretical understanding of the abovequestions remains largely unexplored in the literature. In this paper, weinvestigate the network structure inference problem for a general family ofcontinuous-time diffusion models using an $l_1$-regularized likelihoodmaximization framework. We show that, as long as the cascade sampling processsatisfies a natural incoherence condition, our framework can recover thecorrect network structure with high probability if we observe $O(d^3 \log N)$cascades, where $d$ is the maximum number of parents of a node and $N$ is thetotal number of nodes. Moreover, we develop a simple and efficientsoft-thresholding inference algorithm, which we use to illustrate theconsequences of our theoretical results, and show that our frameworkoutperforms other alternatives in practice.
arxiv-6000-36 | Safe Screening With Variational Inequalities and Its Application to LASSO | http://arxiv.org/pdf/1307.7577v3.pdf | author:Jun Liu, Zheng Zhao, Jie Wang, Jieping Ye category:cs.LG stat.ML published:2013-07-29 summary:Sparse learning techniques have been routinely used for feature selection asthe resulting model usually has a small number of non-zero entries. Safescreening, which eliminates the features that are guaranteed to have zerocoefficients for a certain value of the regularization parameter, is atechnique for improving the computational efficiency. Safe screening is gainingincreasing attention since 1) solving sparse learning formulations usually hasa high computational cost especially when the number of features is large and2) one needs to try several regularization parameters to select a suitablemodel. In this paper, we propose an approach called "Sasvi" (Safe screeningwith variational inequalities). Sasvi makes use of the variational inequalitythat provides the sufficient and necessary optimality condition for the dualproblem. Several existing approaches for Lasso screening can be casted asrelaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safescreening rule. We further study the monotone properties of Sasvi for Lasso,based on which a sure removal regularization parameter can be identified foreach feature. Experimental results on both synthetic and real data sets arereported to demonstrate the effectiveness of the proposed Sasvi for Lassoscreening.
arxiv-6000-37 | A Note on Improved Loss Bounds for Multiple Kernel Learning | http://arxiv.org/pdf/1106.6258v2.pdf | author:Zakria Hussain, John Shawe-Taylor, Mario Marchand category:cs.LG published:2011-06-30 summary:In this paper, we correct an upper bound, presented in~\cite{hs-11}, on thegeneralisation error of classifiers learned through multiple kernel learning.The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive}dependence on the logarithm of the number of kernels and the margin achieved bythe classifier. However, there are some errors in parts of the proof which arecorrected in this paper. Unfortunately, the final result turns out to be a riskbound which has a \emph{multiplicative} dependence on the logarithm of thenumber of kernels and the margin achieved by the classifier.
arxiv-6000-38 | Approximate Policy Iteration Schemes: A Comparison | http://arxiv.org/pdf/1405.2878v1.pdf | author:Bruno Scherrer category:cs.AI cs.LG stat.ML published:2014-05-12 summary:We consider the infinite-horizon discounted optimal control problemformalized by Markov Decision Processes. We focus on several approximatevariations of the Policy Iteration algorithm: Approximate Policy Iteration,Conservative Policy Iteration (CPI), a natural adaptation of the Policy Searchby Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For allalgorithms, we describe performance bounds, and make a comparison by paying aparticular attention to the concentrability constants involved, the number ofiterations and the memory required. Our analysis highlights the followingpoints: 1) The performance guarantee of CPI can be arbitrarily better than thatof API/API($\alpha$), but this comes at the cost of a relative---exponential in$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$enjoys the best of both worlds: its performance guarantee is similar to that ofCPI, but within a number of iterations similar to that of API. 3) Contrary toAPI that requires a constant memory, the memory needed by CPI and PSDP$_\infty$is proportional to their number of iterations, which may be problematic whenthe discount factor $\gamma$ is close to 1 or the approximation error$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to makean overall trade-off between memory and performance. Simulations with theseschemes confirm our analysis.
arxiv-6000-39 | Dynamic Rate and Channel Selection in Cognitive Radio Systems | http://arxiv.org/pdf/1402.5666v2.pdf | author:Richard Combes, Alexandre Proutiere category:cs.IT cs.LG math.IT published:2014-02-23 summary:In this paper, we investigate dynamic channel and rate selection in cognitiveradio systems which exploit a large number of channels free from primary users.In such systems, transmitters may rapidly change the selected (channel, rate)pair to opportunistically learn and track the pair offering the highestthroughput. We formulate the problem of sequential channel and rate selectionas an online optimization problem, and show its equivalence to a {\itstructured} Multi-Armed Bandit problem. The structure stems from inherentproperties of the achieved throughput as a function of the selected channel andrate. We derive fundamental performance limits satisfied by {\it any} channeland rate adaptation algorithm, and propose algorithms that achieve (orapproach) these limits. In turn, the proposed algorithms optimally exploit theinherent structure of the throughput. We illustrate the efficiency of ouralgorithms using both test-bed and simulation experiments, in both stationaryand non-stationary radio environments. In stationary environments, the packetsuccessful transmission probabilities at the various channel and rate pairs donot evolve over time, whereas in non-stationary environments, they may evolve.In practical scenarios, the proposed algorithms are able to track the bestchannel and rate quite accurately without the need of any explicit measurementand feedback of the quality of the various channels.
arxiv-6000-40 | Resource-Aware Programming for Robotic Vision | http://arxiv.org/pdf/1405.2908v1.pdf | author:Johny Paul, Walter Stechele, Manfred Kröhnert, Tamim Asfour category:cs.CV cs.DC cs.RO published:2014-05-12 summary:Humanoid robots are designed to operate in human centered environments. Theyface changing, dynamic environments in which they need to fulfill a multitudeof challenging tasks. Such tasks differ in complexity, resource requirements,and execution time. Latest computer architectures of humanoid robots consist ofseveral industrial PCs containing single- or dual-core processors. According tothe SIA roadmap for semiconductors, many-core chips with hundreds to thousandsof cores are expected to be available in the next decade. Utilizing the fullpower of a chip with huge amounts of resources requires new computing paradigmsand methodologies. In this paper, we analyze a resource-aware computing methodology namedInvasive Computing, to address these challenges. The benefits and limitationsof the new programming model is analyzed using two widely used computer visionalgorithms, the Harris Corner detector and SIFT (Scale Invariant FeatureTransform) feature matching. The result indicate that the new programming modeltogether with the extensions within the application layer, makes them highlyadaptable; leading to better quality in the results obtained.
arxiv-6000-41 | Two-Stage Metric Learning | http://arxiv.org/pdf/1405.2798v1.pdf | author:Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros Kalousis category:cs.LG cs.AI stat.ML published:2014-05-12 summary:In this paper, we present a novel two-stage metric learning algorithm. Wefirst map each learning instance to a probability distribution by computing itssimilarities to a set of fixed anchor points. Then, we define the distance inthe input data space as the Fisher information distance on the associatedstatistical manifold. This induces in the input data space a new family ofdistance metric with unique properties. Unlike kernelized metric learning, wedo not require the similarity measure to be positive semi-definite. Moreover,it can also be interpreted as a local metric learning algorithm with welldefined distance approximation. We evaluate its performance on a number ofdatasets. It outperforms significantly other metric learning methods and SVM.
arxiv-6000-42 | Gender identity and lexical variation in social media | http://arxiv.org/pdf/1210.4567v2.pdf | author:David Bamman, Jacob Eisenstein, Tyler Schnoebelen category:cs.CL published:2012-10-16 summary:We present a study of the relationship between gender, linguistic style, andsocial networks, using a novel corpus of 14,000 Twitter users. Priorquantitative work on gender often treats this social variable as a female/malebinary; we argue for a more nuanced approach. By clustering Twitter users, wefind a natural decomposition of the dataset into various styles and topicalinterests. Many clusters have strong gender orientations, but their use oflinguistic resources sometimes directly conflicts with the population-levellanguage statistics. We view these clusters as a more accurate reflection ofthe multifaceted nature of gendered language styles. Previous corpus-based workhas also had little to say about individuals whose linguistic styles defypopulation-level gender patterns. To identify such individuals, we train astatistical classifier, and measure the classifier confidence for eachindividual in the dataset. Examining individuals whose language does not matchthe classifier's model for their gender, we find that they have social networksthat include significantly fewer same-gender social connections and that, ingeneral, social network homophily is correlated with the use of same-genderlanguage markers. Pairing computational methods and social theory thus offers anew perspective on how gender emerges as individuals position themselvesrelative to audiences, topics, and mainstream gender norms.
arxiv-6000-43 | Nonparametric Estimation of Renyi Divergence and Friends | http://arxiv.org/pdf/1402.2966v2.pdf | author:Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman category:stat.ML math.ST stat.TH published:2014-02-12 summary:We consider nonparametric estimation of $L_2$, Renyi-$\alpha$ andTsallis-$\alpha$ divergences between continuous distributions. Our approach isto construct estimators for particular integral functionals of two densitiesand translate them into divergence estimators. For the integral functionals,our estimators are based on corrections of a preliminary plug-in estimator. Weshow that these estimators achieve the parametric convergence rate of$n^{-1/2}$ when the densities' smoothness, $s$, are both at least $d/4$ where$d$ is the dimension. We also derive minimax lower bounds for this problemwhich confirm that $s > d/4$ is necessary to achieve the $n^{-1/2}$ rate ofconvergence. We validate our theoretical guarantees with a number ofsimulations.
arxiv-6000-44 | Multilinguals and Wikipedia Editing | http://arxiv.org/pdf/1312.0976v2.pdf | author:Scott A. Hale category:cs.CY cs.CL cs.DL cs.SI physics.soc-ph H.5.4; H.5.3 published:2013-12-03 summary:This article analyzes one month of edits to Wikipedia in order to examine therole of users editing multiple language editions (referred to as multilingualusers). Such multilingual users may serve an important function in diffusinginformation across different language editions of the encyclopedia, and priorwork has suggested this could reduce the level of self-focus bias in eachedition. This study finds multilingual users are much more active than theirsingle-edition (monolingual) counterparts. They are found in all languageeditions, but smaller-sized editions with fewer users have a higher percentageof multilingual users than larger-sized editions. About a quarter ofmultilingual users always edit the same articles in multiple languages, whilejust over 40% of multilingual users edit different articles in differentlanguages. When non-English users do edit a second language edition, thatedition is most frequently English. Nonetheless, several regional andlinguistic cross-editing patterns are also present.
arxiv-6000-45 | Policy Gradients for CVaR-Constrained MDPs | http://arxiv.org/pdf/1405.2690v1.pdf | author:Prashanth L. A category:stat.ML cs.LG math.OC published:2014-05-12 summary:We study a risk-constrained version of the stochastic shortest path (SSP)problem, where the risk measure considered is Conditional Value-at-Risk (CVaR).We propose two algorithms that obtain a locally risk-optimal policy byemploying four tools: stochastic approximation, mini batches, policy gradientsand importance sampling. Both the algorithms incorporate a CVaR estimationprocedure, along the lines of Bardou et al. [2009], which in turn is based onRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratioprinciple for estimating the gradient of the sum of one cost function(objective of the SSP) and the gradient of the CVaR of the sum of another costfunction (in the constraint of SSP). The algorithms differ in the manner inwhich they approximate the CVaR estimates/necessary gradients - the firstalgorithm uses stochastic approximation, while the second employ mini-batchesin the spirit of Monte Carlo methods. We establish asymptotic convergence ofboth the algorithms. Further, since estimating CVaR is related to rare-eventsimulation, we incorporate an importance sampling based variance reductionscheme into our proposed algorithms.
arxiv-6000-46 | CNN Features off-the-shelf: an Astounding Baseline for Recognition | http://arxiv.org/pdf/1403.6382v3.pdf | author:Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson category:cs.CV published:2014-03-23 summary:Recent results indicate that the generic descriptors extracted from theconvolutional neural networks are very powerful. This paper adds to themounting evidence that this is indeed the case. We report on a series ofexperiments conducted for different recognition tasks using the publiclyavailable code and model of the \overfeat network which was trained to performobject classification on ILSVRC13. We use features extracted from the \overfeatnetwork as a generic image representation to tackle the diverse range ofrecognition tasks of object image classification, scene recognition, finegrained recognition, attribute detection and image retrieval applied to adiverse set of datasets. We selected these tasks and datasets as they graduallymove further away from the original task and data the \overfeat network wastrained to solve. Astonishingly, we report consistent superior results comparedto the highly tuned state-of-the-art systems in all the visual classificationtasks on various datasets. For instance retrieval it consistently outperformslow memory footprint methods except for sculptures dataset. The results areachieved using a linear SVM classifier (or $L2$ distance in case of retrieval)applied to a feature representation of size 4096 extracted from a layer in thenet. The representations are further modified using simple augmentationtechniques e.g. jittering. The results strongly suggest that features obtainedfrom deep learning with convolutional nets should be the primary candidate inmost visual recognition tasks.
arxiv-6000-47 | Stochastic Gradient Hamiltonian Monte Carlo | http://arxiv.org/pdf/1402.4102v2.pdf | author:Tianqi Chen, Emily B. Fox, Carlos Guestrin category:stat.ME cs.LG stat.ML published:2014-02-17 summary:Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism fordefining distant proposals with high acceptance probabilities in aMetropolis-Hastings framework, enabling more efficient exploration of the statespace than standard random-walk proposals. The popularity of such methods hasgrown significantly in recent years. However, a limitation of HMC methods isthe required gradient computation for simulation of the Hamiltonian dynamicalsystem-such computation is infeasible in problems involving a large sample sizeor streaming data. Instead, we must rely on a noisy gradient estimate computedfrom a subset of the data. In this paper, we explore the properties of such astochastic gradient HMC approach. Surprisingly, the natural implementation ofthe stochastic approximation can be arbitrarily bad. To address this problem weintroduce a variant that uses second-order Langevin dynamics with a frictionterm that counteracts the effects of the noisy gradient, maintaining thedesired target distribution as the invariant distribution. Results on simulateddata validate our theory. We also provide an application of our methods to aclassification task using neural networks and to online Bayesian matrixfactorization.
arxiv-6000-48 | Structural Return Maximization for Reinforcement Learning | http://arxiv.org/pdf/1405.2606v1.pdf | author:Joshua Joseph, Javier Velez, Nicholas Roy category:stat.ML cs.LG published:2014-05-12 summary:Batch Reinforcement Learning (RL) algorithms attempt to choose a policy froma designer-provided class of policies given a fixed set of training data.Choosing the policy which maximizes an estimate of return often leads toover-fitting when only limited data is available, due to the size of the policyclass in relation to the amount of data available. In this work, we focus onlearning policy classes that are appropriately sized to the amount of dataavailable. We accomplish this by using the principle of Structural RiskMinimization, from Statistical Learning Theory, which uses Rademachercomplexity to identify a policy class that maximizes a bound on the return ofthe best policy in the chosen policy class, given the available data. Unlikesimilar batch RL approaches, our bound on return requires only extremely weakassumptions on the true system.
arxiv-6000-49 | Learning from networked examples | http://arxiv.org/pdf/1405.2600v1.pdf | author:Yuyi Wang, Jan Ramon, Zheng-Chu Guo category:cs.AI cs.LG stat.ML published:2014-05-11 summary:Many machine learning algorithms are based on the assumption that trainingexamples are drawn identically and independently. However, this assumption doesnot hold anymore when learning from a networked sample because two or moretraining examples may share some common objects, and hence share the featuresof these shared objects. We first show that the classic approach of ignoringthis problem potentially can have a disastrous effect on the accuracy ofstatistics, and then consider alternatives. One of these is to only useindependent examples, discarding other information. However, this is clearlysuboptimal. We analyze sample error bounds in a networked setting, providingboth improved and new results. Next, we propose an efficient weighting methodwhich achieves a better sample error bound than those of previous methods. Ourapproach is based on novel concentration inequalities for networked variables.
arxiv-6000-50 | Sentiment Analysis: A Survey | http://arxiv.org/pdf/1405.2584v1.pdf | author:Rahul Tejwani category:cs.IR cs.CL published:2014-05-11 summary:Sentiment analysis (also known as opinion mining) refers to the use ofnatural language processing, text analysis and computational linguistics toidentify and extract subjective information in source materials. Miningopinions expressed in the user generated content is a challenging yetpractically very useful problem. This survey would cover various approaches andmethodology used in Sentiment Analysis and Opinion Mining in general. The focuswould be on Internet text like, Product review, tweets and other social media.
arxiv-6000-51 | Learning modular structures from network data and node variables | http://arxiv.org/pdf/1405.2566v1.pdf | author:Elham Azizi, James E. Galagan, Edoardo M. Airoldi category:stat.ML cs.SI physics.soc-ph q-bio.QM stat.AP published:2014-05-11 summary:A standard technique for understanding underlying dependency structures amonga set of variables posits a shared conditional probability distribution for thevariables measured on individuals within a group. This approach is oftenreferred to as module networks, where individuals are represented by nodes in anetwork, groups are termed modules, and the focus is on estimating the networkstructure among modules. However, estimation solely from node-specificvariables can lead to spurious dependencies, and unverifiable structuralassumptions are often used for regularization. Here, we propose an extendedmodel that leverages direct observations about the network in addition tonode-specific variables. By integrating complementary data types, we avoid theneed for structural assumptions. We illustrate theoretical and practicalsignificance of the model and develop a reversible-jump MCMC learning procedurefor learning modules and model parameters. We demonstrate the method accuracyin predicting modular structures from synthetic data and capability to learninfluence structures in twitter data and regulatory modules in theMycobacterium tuberculosis gene regulatory network.
arxiv-6000-52 | A Review of Image Mosaicing Techniques | http://arxiv.org/pdf/1405.2539v1.pdf | author:Dushyant Vaghela, Prof. Kapildev Naina category:cs.CV published:2014-05-11 summary:Image Mosaicing is a method of constructing multiple images of the same sceneinto a larger image. The output of the image mosaic will be the union of twoinput images. Image-mosaicing algorithms are used to get mosaiced image. ImageMosaicing processed is basically divided in to 5 phases. Which includes;Feature point extraction, Image registration, Homography computation, Warpingand Blending if Image. Various corner detection algorithm is being used forFeature extraction. This corner produces an efficient and informative outputmosaiced image. Image mosaicing is widely used in creating 3D images, medicalimaging, computer vision, data from satellites, and military automatic targetrecognition.
arxiv-6000-53 | Fast SVM-based Feature Elimination Utilizing Data Radius, Hard-Margin, Soft-Margin | http://arxiv.org/pdf/1210.4460v4.pdf | author:Yaman Aksu category:stat.ML cs.LG published:2012-10-16 summary:Margin maximization in the hard-margin sense, proposed as feature eliminationcriterion by the MFE-LO method, is combined here with data radius utilizationto further aim to lower generalization error, as several published bounds andbound-related formulations pertaining to lowering misclassification risk (orerror) pertain to radius e.g. product of squared radius and weight vectorsquared norm. Additionally, we propose additional novel feature eliminationcriteria that, while instead being in the soft-margin sense, too can utilizedata radius, utilizing previously published bound-related formulations forapproaching radius for the soft-margin sense, whereby e.g. a focus was on theprinciple stated therein as "finding a bound whose minima are in a region withsmall leave-one-out values may be more important than its tightness". Theseadditional criteria we propose combine radius utilization with a novel andcomputationally low-cost soft-margin light classifier retraining approach wedevise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. Wecorrect an error in the MFE-LO description, find MFE-LO achieves the highestgeneralization accuracy among the previously published margin-based featureelimination (MFE) methods, discuss some limitations of MFE-LO, and find ournovel methods herein outperform MFE-LO, attain lower test set classificationerror rate. On several datasets that each both have a large number of featuresand fall into the `large features few samples' dataset category, and ondatasets with lower (low-to-intermediate) number of features, our novel methodsgive promising results. Especially, among our methods the tunable ones, that donot employ (the non-tunable) LO approach, can be tuned more aggressively in thefuture than herein, to aim to demonstrate for them even higher performance thanherein.
arxiv-6000-54 | Image Restoration Using Joint Statistical Modeling in Space-Transform Domain | http://arxiv.org/pdf/1405.3173v1.pdf | author:Jian Zhang, Debin Zhao, Ruiqin Xiong, Siwei Ma, Wen Gao category:cs.MM cs.CV published:2014-05-11 summary:This paper presents a novel strategy for high-fidelity image restoration bycharacterizing both local smoothness and nonlocal self-similarity of naturalimages in a unified statistical manner. The main contributions are three-folds.First, from the perspective of image statistics, a joint statistical modeling(JSM) in an adaptive hybrid space-transform domain is established, which offersa powerful mechanism of combining local smoothness and nonlocal self-similaritysimultaneously to ensure a more reliable and robust estimation. Second, a newform of minimization functional for solving image inverse problem is formulatedusing JSM under regularization-based framework. Finally, in order to make JSMtractable and robust, a new Split-Bregman based algorithm is developed toefficiently solve the above severely underdetermined inverse problem associatedwith theoretical proof of convergence. Extensive experiments on imageinpainting, image deblurring and mixed Gaussian plus salt-and-pepper noiseremoval applications verify the effectiveness of the proposed algorithm.
arxiv-6000-55 | Off-policy reinforcement learning for $ H_\infty $ control design | http://arxiv.org/pdf/1311.6107v3.pdf | author:Biao Luo, Huai-Ning Wu, Tingwen Huang category:cs.SY cs.LG math.OC stat.ML published:2013-11-24 summary:The $H_\infty$ control design problem is considered for nonlinear systemswith unknown internal system model. It is known that the nonlinear $ H_\infty $control problem can be transformed into solving the so-calledHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partialdifferential equation that is generally impossible to be solved analytically.Even worse, model-based approaches cannot be used for approximately solving HJIequation, when the accurate system model is unavailable or costly to obtain inpractice. To overcome these difficulties, an off-policy reinforcement leaning(RL) method is introduced to learn the solution of HJI equation from realsystem data instead of mathematical system model, and its convergence isproved. In the off-policy RL method, the system data can be generated witharbitrary policies rather than the evaluating policy, which is extremelyimportant and promising for practical systems. For implementation purpose, aneural network (NN) based actor-critic structure is employed and a least-squareNN weight update algorithm is derived based on the method of weightedresiduals. Finally, the developed NN-based off-policy RL method is tested on alinear F16 aircraft plant, and further applied to a rotational/translationalactuator system.
arxiv-6000-56 | Anomaly-Sensitive Dictionary Learning for Unsupervised Diagnostics of Solid Media | http://arxiv.org/pdf/1405.2496v1.pdf | author:Jeffrey M. Druce, Jarvis D. Haupt, Stefano Gonella category:cs.CV published:2014-05-11 summary:This paper proposes a strategy for the detection and triangulation ofstructural anomalies in solid media. The method revolves around theconstruction of sparse representations of the medium's dynamic response,obtained by learning instructive dictionaries which form a suitable basis forthe response data. The resulting sparse coding problem is recast as a modifieddictionary learning task with additional spatial sparsity constraints enforcedon the atoms of the learned dictionaries, which provides them with a prescribedspatial topology that is designed to unveil anomalous regions in the physicaldomain. The proposed methodology is model agnostic, i.e., it forsakes the needfor a physical model and requires virtually no a priori knowledge of thestructure's material properties, as all the inferences are exclusively informedby the data through the layers of information that are available in theintrinsic salient structure of the material's dynamic response. Thischaracteristic makes the approach powerful for anomaly identification insystems with unknown or heterogeneous property distribution, for which a modelis unsuitable or unreliable. The method is validated using both synthetically
arxiv-6000-57 | Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation | http://arxiv.org/pdf/1405.2434v1.pdf | author:Chen Lijiang category:cs.CL published:2014-05-10 summary:Minimum error rate training (MERT) is a widely used training procedure forstatistical machine translation. A general problem of this approach is that thesearch space is easy to converge to a local optimum and the acquired weight setis not in accord with the real distribution of feature functions. This paperintroduces coordinate system selection (RSS) into the search algorithm forMERT. Contrary to previous approaches in which every dimension only correspondsto one independent feature function, we create several coordinate systems bymoving one of the dimensions to a new direction. The basic idea is quite simplebut critical that the training procedure of MERT should be based on acoordinate system formed by search directions but not directly on featurefunctions. Experiments show that by selecting coordinate systems with tuningset results, better results can be obtained without any other languageknowledge.
arxiv-6000-58 | Functional Bandits | http://arxiv.org/pdf/1405.2432v1.pdf | author:Long Tran-Thanh, Jia Yuan Yu category:stat.ML cs.LG published:2014-05-10 summary:We introduce the functional bandit problem, where the objective is to find anarm that optimises a known functional of the unknown arm-reward distributions.These problems arise in many settings such as maximum entropy methods innatural language processing, and risk-averse decision-making, but currentbest-arm identification techniques fail in these domains. We propose a newapproach, that combines functional estimation and arm elimination, to tacklethis problem. This method achieves provably efficient performance guarantees.In addition, we illustrate this method on a number of important functionals inrisk management and information theory, and refine our generic theoreticalresults in those cases.
arxiv-6000-59 | A binary differential evolution algorithm learning from explored solutions | http://arxiv.org/pdf/1401.1124v2.pdf | author:Yu Chen, Weicheng Xie, Xiufen Zou category:cs.NE published:2014-01-06 summary:Although real-coded differential evolution (DE) algorithms can perform wellon continuous optimization problems (CoOPs), it is still a challenging task todesign an efficient binary-coded DE algorithm. Inspired by the learningmechanism of particle swarm optimization (PSO) algorithms, we propose a binarylearning differential evolution (BLDE) algorithm that can efficiently locatethe global optimal solutions by learning from the last population. Then, wetheoretically prove the global convergence of BLDE, and compare it with someexisting binary-coded evolutionary algorithms (EAs) via numerical experiments.Numerical results show that BLDE is competitive to the compared EAs, andmeanwhile, further study is performed via the change curves of a renewal metricand a refinement metric to investigate why BLDE cannot outperform some comparedEAs for several selected benchmark problems. Finally, we employ BLDE solvingthe unit commitment problem (UCP) in power systems to show its applicability inpractical problems.
arxiv-6000-60 | Optimal Learners for Multiclass Problems | http://arxiv.org/pdf/1405.2420v1.pdf | author:Amit Daniely, Shai Shalev-Shwartz category:cs.LG published:2014-05-10 summary:The fundamental theorem of statistical learning states that for binaryclassification problems, any Empirical Risk Minimization (ERM) learning rulehas close to optimal sample complexity. In this paper we seek for a genericoptimal learner for multiclass prediction. We start by proving a surprisingresult: a generic optimal multiclass learner must be improper, namely, it musthave the ability to output hypotheses which do not belong to the hypothesisclass, even though it knows that all the labels are generated by somehypothesis from the class. In particular, no ERM learner is optimal. Thisbrings back the fundmamental question of "how to learn"? We give a completeanswer to this question by giving a new analysis of the one-inclusionmulticlass learner of Rubinstein et al (2006) showing that its samplecomplexity is essentially optimal. Then, we turn to study the popularhypothesis class of generalized linear classifiers. We derive optimal learnersthat, unlike the one-inclusion algorithm, are computationally efficient.Furthermore, we show that the sample complexity of these learners is betterthan the sample complexity of the ERM rule, thus settling in negative an openquestion due to Collins (2005).
arxiv-6000-61 | The complexity of learning halfspaces using generalized linear methods | http://arxiv.org/pdf/1211.0616v4.pdf | author:Amit Daniely, Nati Linial, Shai Shalev-Shwartz category:cs.LG cs.DS published:2012-11-03 summary:Many popular learning algorithms (E.g. Regression, Fourier-Transform basedalgorithms, Kernel SVM and Kernel ridge regression) operate by reducing theproblem to a convex optimization problem over a vector space of functions.These methods offer the currently best approach to several central problemssuch as learning half spaces and learning DNF's. In addition they are widelyused in numerous application domains. Despite their importance, there are stillvery few proof techniques to show limits on the power of these algorithms. We study the performance of this approach in the problem of (agnostically andimproperly) learning halfspaces with margin $\gamma$. Let $\mathcal{D}$ be adistribution over labeled examples. The $\gamma$-margin error of a hyperplane$h$ is the probability of an example to fall on the wrong side of $h$ or at adistance $\le\gamma$ from it. The $\gamma$-margin error of the best $h$ isdenoted $\mathrm{Err}_\gamma(\mathcal{D})$. An $\alpha(\gamma)$-approximationalgorithm receives $\gamma,\epsilon$ as input and, using i.i.d. samples of$\mathcal{D}$, outputs a classifier with error rate $\le\alpha(\gamma)\mathrm{Err}_\gamma(\mathcal{D}) + \epsilon$. Such an algorithmis efficient if it uses $\mathrm{poly}(\frac{1}{\gamma},\frac{1}{\epsilon})$samples and runs in time polynomial in the sample size. The best approximation ratio achievable by an efficient algorithm is$O\left(\frac{1/\gamma}{\sqrt{\log(1/\gamma)}}\right)$ and is achieved using analgorithm from the above class. Our main result shows that the approximationratio of every efficient algorithm from this family must be $\ge\Omega\left(\frac{1/\gamma}{\mathrm{poly}\left(\log\left(1/\gamma\right)\right)}\right)$,essentially matching the best known upper bound.
arxiv-6000-62 | A PAC-Bayesian bound for Lifelong Learning | http://arxiv.org/pdf/1311.2838v2.pdf | author:Anastasia Pentina, Christoph H. Lampert category:stat.ML cs.LG 68T05 published:2013-11-12 summary:Transfer learning has received a lot of attention in the machine learningcommunity over the last years, and several effective algorithms have beendeveloped. However, relatively little is known about their theoreticalproperties, especially in the setting of lifelong learning, where the goal isto transfer information to tasks for which no data have been observed so far.In this work we study lifelong learning from a theoretical perspective. Ourmain result is a PAC-Bayesian generalization bound that offers a unified viewon existing paradigms for transfer learning, such as the transfer of parametersor the transfer of low-dimensional representations. We also use the bound toderive two principled lifelong learning algorithms, and we show that theseyield results comparable with existing methods.
arxiv-6000-63 | Hyperspectral pan-sharpening: a variational convex constrained formulation to impose parallel level lines, solved with ADMM | http://arxiv.org/pdf/1405.2403v1.pdf | author:Alexis Huck, François de Vieilleville, Pierre Weiss, Manuel Grizonnet category:cs.CV published:2014-05-10 summary:In this paper, we address the issue of hyperspectral pan-sharpening, whichconsists in fusing a (low spatial resolution) hyperspectral image HX and a(high spatial resolution) panchromatic image P to obtain a high spatialresolution hyperspectral image. The problem is addressed under a variationalconvex constrained formulation. The objective favors high resolution spectralbands with level lines parallel to those of the panchromatic image. This termis balanced with a total variation term as regularizer. Fit-to-P data andfit-to-HX data constraints are effectively considered as mathematicalconstraints, which depend on the statistics of the data noise measurements. Thedeveloped Alternating Direction Method of Multipliers (ADMM) optimizationscheme enables us to solve this problem efficiently despite the nondifferentiabilities and the huge number of unknowns.
arxiv-6000-64 | Predicting Central Topics in a Blog Corpus from a Networks Perspective | http://arxiv.org/pdf/1405.2386v1.pdf | author:Srayan Datta category:cs.IR cs.CL cs.SI physics.soc-ph published:2014-05-10 summary:In today's content-centric Internet, blogs are becoming increasingly popularand important from a data analysis perspective. According to Wikipedia, therewere over 156 million public blogs on the Internet as of February 2011. Blogsare a reflection of our contemporary society. The contents of different blogposts are important from social, psychological, economical and politicalperspectives. Discovery of important topics in the blogosphere is an area whichstill needs much exploring. We try to come up with a procedure usingprobabilistic topic modeling and network centrality measures which identifiesthe central topics in a blog corpus.
arxiv-6000-65 | A Hybrid Monte Carlo Architecture for Parameter Optimization | http://arxiv.org/pdf/1405.2377v1.pdf | author:James Brofos category:stat.ML cs.LG stat.ME published:2014-05-10 summary:Much recent research has been conducted in the area of Bayesian learning,particularly with regard to the optimization of hyper-parameters via Gaussianprocess regression. The methodologies rely chiefly on the method of maximizingthe expected improvement of a score function with respect to adjustments in thehyper-parameters. In this work, we present a novel algorithm that exploitsnotions of confidence intervals and uncertainties to enable the discovery ofthe best optimal within a targeted region of the parameter space. Wedemonstrate the efficacy of our algorithm with respect to machine learningproblems and show cases where our algorithm is competitive with the method ofmaximizing expected improvement.
arxiv-6000-66 | Bounding the Test Log-Likelihood of Generative Models | http://arxiv.org/pdf/1311.6184v4.pdf | author:Yoshua Bengio, Li Yao, Kyunghyun Cho category:cs.LG published:2013-11-24 summary:Several interesting generative learning algorithms involve a complexprobability distribution over many random variables, involving intractablenormalization constants or latent variable normalization. Some of them may evennot have an analytic expression for the unnormalized probability function andno tractable approximation. This makes it difficult to estimate the quality ofthese models, once they have been trained, or to monitor their quality (e.g.for early stopping) while training. A previously proposed method is based onconstructing a non-parametric density estimator of the model's probabilityfunction from samples generated by the model. We revisit this idea, propose amore efficient estimator, and prove that it provides a lower bound on the truetest log-likelihood, and an unbiased estimator as the number of generatedsamples goes to infinity, although one that incorporates the effect of poormixing. We further propose a biased variant of the estimator that can be usedreliably with a finite number of samples for the purpose of model comparison.
arxiv-6000-67 | Image Segmentation Using Frequency Locking of Coupled Oscillators | http://arxiv.org/pdf/1405.2362v1.pdf | author:Yan Fang, Matthew J. Cotter, Donald M. Chiarulli, Steven P. Levitan category:cs.CV q-bio.NC C.1.3 published:2014-05-09 summary:Synchronization of coupled oscillators is observed at multiple levels ofneural systems, and has been shown to play an important function in visualperception. We propose a computing system based on locally coupled oscillatornetworks for image segmentation. The system can serve as the preprocessingfront-end of an image processing pipeline where the common frequencies ofclusters of oscillators reflect the segmentation results. To demonstrate thefeasibility of our design, the system is simulated and tested on a human faceimage dataset and its performance is compared with traditional intensitythreshold based algorithms. Our system shows both better performance and highernoise tolerance than traditional methods.
arxiv-6000-68 | Better Feature Tracking Through Subspace Constraints | http://arxiv.org/pdf/1405.2316v1.pdf | author:Bryan Poling, Gilad Lerman, Arthur Szlam category:cs.CV published:2014-05-09 summary:Feature tracking in video is a crucial task in computer vision. Usually, thetracking problem is handled one feature at a time, using a single-featuretracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives.While this approach works quite well when dealing with high-quality video and"strong" features, it often falters when faced with dark and noisy videocontaining low-quality features. We present a framework for jointly tracking aset of features, which enables sharing information between the differentfeatures in the scene. We show that our method can be employed to trackfeatures for both rigid and nonrigid motions (possibly of few moving bodies)even when some features are occluded. Furthermore, it can be used tosignificantly improve tracking results in poorly-lit scenes (where there is amix of good and bad features). Our approach does not require direct modeling ofthe structure or the motion of the scene, and runs in real time on a single CPUcore.
arxiv-6000-69 | A bi-level view of inpainting - based image compression | http://arxiv.org/pdf/1401.4112v2.pdf | author:Yunjin Chen, René Ranftl, Thomas Pock category:cs.CV published:2014-01-16 summary:Inpainting based image compression approaches, especially linear andnon-linear diffusion models, are an active research topic for lossy imagecompression. The major challenge in these compression models is to find a smallset of descriptive supporting points, which allow for an accuratereconstruction of the original image. It turns out in practice that this is achallenging problem even for the simplest Laplacian interpolation model. Inthis paper, we revisit the Laplacian interpolation compression model andintroduce two fast algorithms, namely successive preconditioning primal dualalgorithm and the recently proposed iPiano algorithm, to solve this problemefficiently. Furthermore, we extend the Laplacian interpolation basedcompression model to a more general form, which is based on principles frombi-level optimization. We investigate two different variants of the Laplacianmodel, namely biharmonic interpolation and smoothed Total Variationregularization. Our numerical results show that significant improvements can beobtained from the biharmonic interpolation model, and it can recover an imagewith very high quality from only 5% pixels.
arxiv-6000-70 | Hellinger Distance Trees for Imbalanced Streams | http://arxiv.org/pdf/1405.2278v1.pdf | author:R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers category:cs.LG astro-ph.IM stat.ML published:2014-05-09 summary:Classifiers trained on data sets possessing an imbalanced class distributionare known to exhibit poor generalisation performance. This is known as theimbalanced learning problem. The problem becomes particularly acute when weconsider incremental classifiers operating on imbalanced data streams,especially when the learning objective is rare class identification. Asaccuracy may provide a misleading impression of performance on imbalanced data,existing stream classifiers based on accuracy can suffer poor minority classperformance on imbalanced streams, with the result being low minority classrecall rates. In this paper we address this deficiency by proposing the use ofthe Hellinger distance measure, as a very fast decision tree split criterion.We demonstrate that by using Hellinger a statistically significant improvementin recall rates on imbalanced data streams can be achieved, with an acceptableincrease in the false positive rate.
arxiv-6000-71 | Training Deep Fourier Neural Networks To Fit Time-Series Data | http://arxiv.org/pdf/1405.2262v1.pdf | author:Michael S. Gashler, Stephen C. Ashmore category:cs.NE cs.LG published:2014-05-09 summary:We present a method for training a deep neural network containing sinusoidalactivation functions to fit to time-series data. Weights are initialized usinga fast Fourier transform, then trained with regularization to improvegeneralization. A simple dynamic parameter tuning method is employed to adjustboth the learning rate and regularization term, such that stability andefficient training are both achieved. We show how deeper layers can be utilizedto model the observed sequence using a sparser set of sinusoid units, and hownon-uniform regularization can improve generalization by promoting the shiftingof weight toward simpler units. The method is demonstrated with time-seriesproblems to show that it leads to effective extrapolation of nonlinear trends.
arxiv-6000-72 | Graph Regularized Non-negative Matrix Factorization By Maximizing Correntropy | http://arxiv.org/pdf/1405.2246v1.pdf | author:Le Li, Jianjun Yang, Kaili Zhao, Yang Xu, Honggang Zhang, Zhuoyi Fan category:cs.CV published:2014-05-09 summary:Non-negative matrix factorization (NMF) has proved effective in manyclustering and classification tasks. The classic ways to measure the errorsbetween the original and the reconstructed matrix are $l_2$ distance orKullback-Leibler (KL) divergence. However, nonlinear cases are not properlyhandled when we use these error measures. As a consequence, alternativemeasures based on nonlinear kernels, such as correntropy, are proposed.However, the current correntropy-based NMF only targets on the low-levelfeatures without considering the intrinsic geometrical distribution of data. Inthis paper, we propose a new NMF algorithm that preserves local invariance byadding graph regularization into the process of max-correntropy-based matrixfactorization. Meanwhile, each feature can learn corresponding kernel from thedata. The experiment results of Caltech101 and Caltech256 show the benefits ofsuch combination against other NMF algorithms for the unsupervised imageclustering.
arxiv-6000-73 | An Overview of Face Liveness Detection | http://arxiv.org/pdf/1405.2227v1.pdf | author:Saptarshi Chakraborty, Dhrubajyoti Das category:cs.CV published:2014-05-09 summary:Face recognition is a widely used biometric approach. Face recognitiontechnology has developed rapidly in recent years and it is more direct, userfriendly and convenient compared to other methods. But face recognition systemsare vulnerable to spoof attacks made by non-real faces. It is an easy way tospoof face recognition systems by facial pictures such as portrait photographs.A secure system needs Liveness detection in order to guard against suchspoofing. In this work, face liveness detection approaches are categorizedbased on the various types techniques used for liveness detection. Thiscategorization helps understanding different spoof attacks scenarios and theirrelation to the developed solutions. A review of the latest works regardingface liveness detection works is presented. The main aim is to provide a simplepath for the future development of novel and more secured face livenessdetection approach.
arxiv-6000-74 | Gaussian-Chain Filters for Heavy-Tailed Noise with Application to Detecting Big Buyers and Big Sellers in Stock Market | http://arxiv.org/pdf/1405.2220v1.pdf | author:Li-Xin Wang category:q-fin.TR cs.CE cs.CV cs.SY q-fin.ST published:2014-05-09 summary:We propose a new heavy-tailed distribution --- Gaussian-Chain (GC)distribution, which is inspirited by the hierarchical structures prevailing insocial organizations. We determine the mean, variance and kurtosis of theGaussian-Chain distribution to show its heavy-tailed property, and compute thetail distribution table to give specific numbers showing how heavy is theheavy-tails. To filter out the heavy-tailed noise, we construct two filters ---2nd and 3rd-order GC filters --- based on the maximum likelihood principle.Simulation results show that the GC filters perform much better than thebenchmark least-squares algorithm when the noise is heavy-tail distributed.Using the GC filters, we propose a trading strategy, named Ride-the-Mood, tofollow the mood of the market by detecting the actions of the big buyers andthe big sellers in the market based on the noisy, heavy-tailed price data.Application of the Ride-the-Mood strategy to five blue-chip Hong Kong stocksover the recent two-year period from April 2, 2012 to March 31, 2014 shows thattheir returns are higher than the returns of the benchmark Buy-and-Holdstrategy and the Hang Seng Index Fund.
arxiv-6000-75 | Evaluation The Efficiency Of Cuckoo Optimization Algorithm | http://arxiv.org/pdf/1405.2168v1.pdf | author:Elham Shadkam, Mehdi Bijari category:cs.NE cs.NA math.NA published:2014-05-09 summary:In this paper a new evolutionary algorithm, for continuous nonlinearoptimization problems, is surveyed. This method is inspired by the life of abird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated byusing the Rastrigin function. The problem is a non-linear continuous functionwhich is used for evaluating optimization algorithms. The efficiency of the COAhas been studied by obtaining optimal solution of various dimensions Rastriginfunction in this paper. The mentioned function also was solved by FA and ABCalgorithms. Comparing the results shows the COA has better performance thanother algorithms. Application of algorithm to test function has proven itscapability to deal with difficult optimization problems.
arxiv-6000-76 | Cognitive-mapping and contextual pyramid based Digital Elevation Model Registration and its effective storage using fractal based compression | http://arxiv.org/pdf/1405.6662v1.pdf | author:Suma Dawn, Vikas Saxena, Bhudev Sharma category:cs.AI cs.CV published:2014-05-09 summary:Digital Elevation models (DEM) are images having terrain information embeddedinto them. Using cognitive mapping concepts for DEM registration, has evolvedfrom this basic idea of using the mapping between the space to objects anddefining their relationships to form the basic landmarks that need to bemarked, stored and manipulated in and about the environment or other candidateenvironments, namely, in our case, the DEMs. The progressive two-levelencapsulation of methods of geo-spatial cognition includes landmark knowledgeand layout knowledge and can be useful for DEM registration. Space-basedapproach, that emphasizes on explicit extent of the environment underconsideration, and object-based approach, that emphasizes on the relationshipsbetween objects in the local environment being the two paradigms of cognitivemapping can be methodically integrated in this three-architecture for DEMregistration. Initially, P-model based segmentation is performed followed bylandmark formation for contextual mapping that uses contextual pyramidformation. Apart from landmarks being used for registration key-point finding,Euclidean distance based deformation calculation has been used fortransformation and change detection. Landmarks have been categorized to belongto either being flat-plain areas without much variation in the land heights;peaks that can be found when there is gradual increase in height as compared tothe flat areas; valleys, marked with gradual decrease in the height seen inDEM; and finally, ripple areas with very shallow crests and nadirs. Fractalbased compression was used for storage of co-registered DEMs. This method mayfurther be extended for DEM-topographic map and DEM-to-remote sensed imageregistration. Experimental results further cement the fact that DEMregistration may be effectively done using the proposed method.
arxiv-6000-77 | New explicit thresholding/shrinkage formulas for one class of regularization problems with overlapping group sparsity and their applications | http://arxiv.org/pdf/1312.6813v3.pdf | author:Gang Liu, Ting-Zhu Huang, Xiao-Guang Lv, Jun Liu category:math.NA cs.CV published:2013-12-24 summary:The least-square regression problems or inverse problems have been widelystudied in many fields such as compressive sensing, signal processing, andimage processing. To solve this kind of ill-posed problems, a regularizationterm (i.e., regularizer) should be introduced, under the assumption that thesolutions have some specific properties, such as sparsity and group sparsity.Widely used regularizers include the $\ell_1$ norm, total variation (TV)semi-norm, and so on. Recently, a new regularization term with overlapping group sparsity has beenconsidered. Majorization minimization iteration method or variable duplicationmethods are often applied to solve them. However, there have been no directmethods for solve the relevant problems because of the difficulty ofoverlapping. In this paper, we proposed new explicit shrinkage formulas for oneclass of these relevant problems, whose regularization terms have translationinvariant overlapping groups. Moreover, we apply our results in TV deblurringand denoising with overlapping group sparsity. We use alternating directionmethod of multipliers to iterate solve it. Numerical results also verify thevalidity and effectiveness of our new explicit shrinkage formulas.
arxiv-6000-78 | On-line PCA with Optimal Regrets | http://arxiv.org/pdf/1306.3895v2.pdf | author:Jiazhong Nie, Wojciech Kotlowski, Manfred K. Warmuth category:cs.LG published:2013-06-17 summary:We carefully investigate the on-line version of PCA, where in each trial alearning algorithm plays a k-dimensional subspace, and suffers the compressionloss on the next instance when projected into the chosen subspace. In thissetting, we analyze two popular on-line algorithms, Gradient Descent (GD) andExponentiated Gradient (EG). We show that both algorithms are essentiallyoptimal in the worst-case. This comes as a surprise, since EG is known toperform sub-optimally when the instances are sparse. This different behavior ofEG for PCA is mainly related to the non-negativity of the loss in this case,which makes the PCA setting qualitatively different from other settings studiedin the literature. Furthermore, we show that when considering regret bounds asfunction of a loss budget, EG remains optimal and strictly outperforms GD.Next, we study the extension of the PCA setting, in which the Nature is allowedto play with dense instances, which are positive matrices with bounded largesteigenvalue. Again we can show that EG is optimal and strictly better than GD inthis setting.
arxiv-6000-79 | Kaggle LSHTC4 Winning Solution | http://arxiv.org/pdf/1405.0546v2.pdf | author:Antti Puurula, Jesse Read, Albert Bifet category:cs.AI cs.CL cs.IR published:2014-05-03 summary:Our winning submission to the 2014 Kaggle competition for Large ScaleHierarchical Text Classification (LSHTC) consists mostly of an ensemble ofsparse generative models extending Multinomial Naive Bayes. Thebase-classifiers consist of hierarchically smoothed models combining document,label, and hierarchy level Multinomials, with feature pre-processing usingvariants of TF-IDF and BM25. Additional diversification is introduced bydifferent types of folds and random search optimization for different measures.The ensemble algorithm optimizes macroFscore by predicting the documents foreach label, instead of the usual prediction of labels per document. Scores fordocuments are predicted by weighted voting of base-classifier outputs with avariant of Feature-Weighted Linear Stacking. The number of documents per labelis chosen using label priors and thresholding of vote scores. This documentdescribes the models and software used to build our solution. Reproducing theresults for our solution can be done by running the scripts included in theKaggle package. A package omitting precomputed result files is alsodistributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0for Weka and Meka dependencies.
arxiv-6000-80 | Variational Image Segmentation Model Coupled with Image Restoration Achievements | http://arxiv.org/pdf/1405.2128v1.pdf | author:Xiaohao Cai category:cs.CV math.NA 65Kxx, 65Yxx G.1.0; G.1.6 published:2014-05-09 summary:Image segmentation and image restoration are two important topics in imageprocessing with great achievements. In this paper, we propose a new multiphasesegmentation model by combining image restoration and image segmentationmodels. Utilizing image restoration aspects, the proposed segmentation modelcan effectively and robustly tackle high noisy images, blurry images, imageswith missing pixels, and vector-valued images. In particular, one of the mostimportant segmentation models, the piecewise constant Mumford-Shah model, canbe extended easily in this way to segment gray and vector-valued imagescorrupted for example by noise, blur or missing pixels after coupling a newdata fidelity term which comes from image restoration topics. It can be solvedefficiently using the alternating minimization algorithm, and we prove theconvergence of this algorithm with three variables under mild condition.Experiments on many synthetic and real-world images demonstrate that our methodgives better segmentation results in comparison to others state-of-the-artsegmentation models especially for blurry images and images with missing pixelsvalues.
arxiv-6000-81 | Highly comparative feature-based time-series classification | http://arxiv.org/pdf/1401.3531v2.pdf | author:Ben D. Fulcher, Nick S. Jones category:cs.LG cs.AI cs.DB q-bio.QM published:2014-01-15 summary:A highly comparative, feature-based approach to time series classification isintroduced that uses an extensive database of algorithms to extract thousandsof interpretable features from time series. These features are derived fromacross the scientific time-series analysis literature, and include summaries oftime series in terms of their correlation structure, distribution, entropy,stationarity, scaling properties, and fits to a range of time-series models.After computing thousands of features for each time series in a training set,those that are most informative of the class structure are selected usinggreedy forward feature selection with a linear classifier. The resultingfeature-based classifiers automatically learn the differences between classesusing a reduced number of time-series properties, and circumvent the need tocalculate distances between time series. Representing time series in this wayresults in orders of magnitude of dimensionality reduction, allowing the methodto perform well on very large datasets containing long time series or timeseries of different lengths. For many of the datasets studied, classificationperformance exceeded that of conventional instance-based classifiers, includingone nearest neighbor classifiers using Euclidean distances and dynamic timewarping and, most importantly, the features selected provide an understandingof the properties of the dataset, insight that can guide further scientificinvestigation.
arxiv-6000-82 | Improving Image Clustering using Sparse Text and the Wisdom of the Crowds | http://arxiv.org/pdf/1405.2102v1.pdf | author:Anna Ma, Arjuna Flenner, Deanna Needell, Allon G. Percus category:cs.LG cs.CV published:2014-05-08 summary:We propose a method to improve image clustering using sparse text and thewisdom of the crowds. In particular, we present a method to fuse two differentkinds of document features, image and text features, and use a commondictionary or "wisdom of the crowds" as the connection between the twodifferent kinds of documents. With the proposed fusion matrix, we use topicmodeling via non-negative matrix factorization to cluster documents.
arxiv-6000-83 | A consistent deterministic regression tree for non-parametric prediction of time series | http://arxiv.org/pdf/1405.1533v2.pdf | author:Pierre Gaillard, Paul Baudin category:math.ST cs.LG stat.ML stat.TH published:2014-05-07 summary:We study online prediction of bounded stationary ergodic processes. To do so,we consider the setting of prediction of individual sequences and build adeterministic regression tree that performs asymptotically as well as the bestL-Lipschitz constant predictors. Then, we show why the obtained regret boundentails the asymptotical optimality with respect to the class of boundedstationary ergodic processes.
arxiv-6000-84 | Privately Solving Linear Programs | http://arxiv.org/pdf/1402.3631v2.pdf | author:Justin Hsu, Aaron Roth, Tim Roughgarden, Jonathan Ullman category:cs.DS cs.CR cs.LG published:2014-02-15 summary:In this paper, we initiate the systematic study of solving linear programsunder differential privacy. The first step is simply to define the problem: tothis end, we introduce several natural classes of private linear programs thatcapture different ways sensitive data can be incorporated into a linearprogram. For each class of linear programs we give an efficient, differentiallyprivate solver based on the multiplicative weights framework, or we give animpossibility result.
arxiv-6000-85 | Reproducing kernel Hilbert space based estimation of systems of ordinary differential equations | http://arxiv.org/pdf/1311.3576v2.pdf | author:Javier González, Ivan Vujačić, Ernst Wit category:stat.ME stat.ML published:2013-11-14 summary:Non-linear systems of differential equations have attracted the interest infields like system biology, ecology or biochemistry, due to their flexibilityand their ability to describe dynamical systems. Despite the importance of suchmodels in many branches of science they have not been the focus of systematicstatistical analysis until recently. In this work we propose a general approachto estimate the parameters of systems of differential equations measured withnoise. Our methodology is based on the maximization of the penalized likelihoodwhere the system of differential equations is used as a penalty. To do so, weuse a Reproducing Kernel Hilbert Space approach that allows to formulate theestimation problem as an unconstrained numeric maximization problem easy tosolve. The proposed method is tested with synthetically simulated data and itis used to estimate the unobserved transcription factor CdaR in Steptomyescoelicolor using gene expression data of the genes it regulates.
arxiv-6000-86 | Image Resolution and Contrast Enhancement of Satellite Geographical Images with Removal of Noise using Wavelet Transforms | http://arxiv.org/pdf/1405.1967v1.pdf | author:Prajakta P. Khairnar, C. A. Manjare category:cs.CV published:2014-05-08 summary:In this paper the technique for resolution and contrast enhancement ofsatellite geographical images based on discrete wavelet transform (DWT),stationary wavelet transform (SWT) and singular value decomposition (SVD) hasbeen proposed. In this, the noise is added in the input low resolution and lowcontrast image. The median filter is used remove noise from the input image.This low resolution, low contrast image without noise is decomposed into foursub-bands by using DWT and SWT. The resolution enhancement technique is basedon the interpolation of high frequency components obtained by DWT and inputimage. SWT is used to enhance input image. DWT is used to decompose an imageinto four frequency sub bands and these four sub-bands are interpolated usingbicubic interpolation technique. All these sub-bands are reconstructed as highresolution image by using inverse DWT (IDWT). To increase the contrast theproposed technique uses DWT and SVD. GHE is used to equalize an image. Theequalized image is decomposed into four sub-bands using DWT and new LL sub-bandis reconstructed using SVD. All sub-bands are reconstructed using IDWT togenerate high resolution and contrast image over conventional techniques. Theexperimental result shows superiority of the proposed technique overconventional techniques. Key words: Discrete wavelet transform (DWT), General histogram equalization(GHE), Median filter, Singular value decomposition (SVD), Stationary wavelettransform (SWT).
arxiv-6000-87 | An Expert System for Automatic Reading of A Text Written in Standard Arabic | http://arxiv.org/pdf/1405.1924v1.pdf | author:Tebbi Hanane, Azzoune Hamid category:cs.CL published:2014-05-08 summary:In this work we present our expert system of Automatic reading or speechsynthesis based on a text written in Standard Arabic, our work is carried outin two great stages: the creation of the sound data base, and thetransformation of the written text into speech (Text To Speech TTS). Thistransformation is done firstly by a Phonetic Orthographical Transcription (POT)of any written Standard Arabic text with the aim of transforming it into hiscorresponding phonetics sequence, and secondly by the generation of the voicesignal which corresponds to the chain transcribed. We spread out the differentof conception of the system, as well as the results obtained compared to othersworks studied to realize TTS based on Standard Arabic.
arxiv-6000-88 | Implementation And Performance Evaluation Of Background Subtraction Algorithms | http://arxiv.org/pdf/1405.1815v1.pdf | author:Deepjoy Das, Dr. Sarat Saharia category:cs.CV published:2014-05-08 summary:The study evaluates three background subtraction techniques. The techniquesranges from very basic algorithm to state of the art published techniquescategorized based on speed, memory requirements and accuracy. Such a review caneffectively guide the designer to select the most suitable method for a givenapplication in a principled way. The algorithms used in the study ranges fromvarying levels of accuracy and computational complexity. Few of them can alsodeal with real time challenges like rain, snow, hails, swaying branches,objects overlapping, varying light intensity or slow moving objects.
arxiv-6000-89 | Geodesic Distance Function Learning via Heat Flow on Vector Fields | http://arxiv.org/pdf/1405.0133v2.pdf | author:Binbin Lin, Ji Yang, Xiaofei He, Jieping Ye category:cs.LG math.DG stat.ML published:2014-05-01 summary:Learning a distance function or metric on a given data manifold is of greatimportance in machine learning and pattern recognition. Many of the previousworks first embed the manifold to Euclidean space and then learn the distancefunction. However, such a scheme might not faithfully preserve the distancefunction if the original manifold is not Euclidean. Note that the distancefunction on a manifold can always be well-defined. In this paper, we propose tolearn the distance function directly on the manifold without embedding. Wefirst provide a theoretical characterization of the distance function by itsgradient field. Based on our theoretical analysis, we propose to first learnthe gradient field of the distance function and then learn the distancefunction itself. Specifically, we set the gradient field of a local distancefunction as an initial vector field. Then we transport it to the whole manifoldvia heat flow on vector fields. Finally, the geodesic distance function can beobtained by requiring its gradient field to be close to the normalized vectorfield. Experimental results on both synthetic and real data demonstrate theeffectiveness of our proposed algorithm.
arxiv-6000-90 | Sparse PCA through Low-rank Approximations | http://arxiv.org/pdf/1303.0551v2.pdf | author:Dimitris S. Papailiopoulos, Alexandros G. Dimakis, Stavros Korokythakis category:stat.ML cs.IT cs.LG math.IT published:2013-03-03 summary:We introduce a novel algorithm that computes the $k$-sparse principalcomponent of a positive semidefinite matrix $A$. Our algorithm is combinatorialand operates by examining a discrete set of special vectors lying in alow-dimensional eigen-subspace of $A$. We obtain provable approximationguarantees that depend on the spectral decay profile of the matrix: the fasterthe eigenvalue decay, the better the quality of our approximation. For example,if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-timeapproximation algorithm for any desired accuracy. A key algorithmic component of our scheme is a combinatorial featureelimination step that is provably safe and in practice significantly reducesthe running complexity of our algorithm. We implement our algorithm and test iton multiple artificial and real data sets. Due to the feature elimination step,it is possible to perform sparse PCA on data sets consisting of millions ofentries in a few minutes. Our experimental evaluation shows that our scheme isnearly optimal while finding very sparse vectors. We compare to the prior stateof the art and show that our scheme matches or outperforms previous algorithmsin all tested data sets.
arxiv-6000-91 | On Tensor Completion via Nuclear Norm Minimization | http://arxiv.org/pdf/1405.1773v1.pdf | author:Ming Yuan, Cun-Hui Zhang category:stat.ML cs.IT math.IT math.NA math.OC math.PR published:2014-05-07 summary:Many problems can be formulated as recovering a low-rank tensor. Although anincreasingly common task, tensor recovery remains a challenging problem becauseof the delicacy associated with the decomposition of higher order tensors. Toovercome these difficulties, existing approaches often proceed by unfoldingtensors into matrices and then apply techniques for matrix completion. We showhere that such matricization fails to exploit the tensor structure and may leadto suboptimal procedure. More specifically, we investigate a convexoptimization approach to tensor completion by directly minimizing a tensornuclear norm and prove that this leads to an improved sample size requirement.To establish our results, we develop a series of algebraic and probabilistictechniques such as characterization of subdifferetial for tensor nuclear normand concentration inequalities for tensor martingales, which may be ofindependent interests and could be useful in other tensor related problems.
arxiv-6000-92 | Learning Alternative Name Spellings | http://arxiv.org/pdf/1405.2048v1.pdf | author:Jeffrey Sukharev, Leonid Zhukov, Alexandrin Popescul category:cs.IR cs.CL published:2014-05-07 summary:Name matching is a key component of systems for entity resolution or recordlinkage. Alternative spellings of the same names are a com- mon occurrence inmany applications. We use the largest collection of genealogy person records inthe world together with user search query logs to build name matching models.The procedure for building a crowd-sourced training set is outlined togetherwith the presentation of our method. We cast the problem of learningalternative spellings as a machine translation problem at the character level.We use in- formation retrieval evaluation methodology to show that this methodsubstantially outperforms on our data a number of standard well known phoneticand string similarity methods in terms of precision and re- call. Additionally,we rigorously compare the performance of standard methods when compared witheach other. Our result can lead to a significant practical impact in entityresolution applications.
arxiv-6000-93 | Entropy Based Cartoon Texture Separation | http://arxiv.org/pdf/1405.1717v1.pdf | author:Kutlu Emre Yilmaz category:cs.CV published:2014-05-07 summary:Separating an image into cartoon and texture components comes useful in imageprocessing applications, such as image compression, image segmentation, imageinpainting. Yves Meyer's influential cartoon texture decomposition modelinvolves deriving an energy functional by choosing appropriate spaces andfunctionals. Minimizers of the derived energy functional are cartoon andtexture components of an image. In this study, cartoon part of an image isseparated, by reconstructing it from pixels of multi scale Total-Variationfiltered versions of the original image which is sought to be decomposed intocartoon and texture parts. An information theoretic pixel by pixel selectioncriteria is employed to choose the contributing pixels and their scales.
arxiv-6000-94 | DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News | http://arxiv.org/pdf/1405.1605v1.pdf | author:Jacopo Staiano, Marco Guerini category:cs.CL cs.CY published:2014-05-07 summary:While many lexica annotated with words polarity are available for sentimentanalysis, very few tackle the harder task of emotion analysis and are usuallyquite limited in coverage. In this paper, we present a novel approach forextracting - in a totally automated way - a high-coverage and high-precisionlexicon of roughly 37 thousand terms annotated with emotion scores, calledDepecheMood. Our approach exploits in an original way 'crowd-sourced' affectiveannotation implicitly provided by readers of news articles from rappler.com. Byproviding new state-of-the-art performances in unsupervised settings forregression and classification tasks, even using a na\"{\i}ve approach, ourexperiments show the beneficial impact of harvesting social media data foraffective lexicon building.
arxiv-6000-95 | PAC-Bayes Mini-tutorial: A Continuous Union Bound | http://arxiv.org/pdf/1405.1580v1.pdf | author:Tim van Erven category:stat.ML published:2014-05-07 summary:When I first encountered PAC-Bayesian concentration inequalities they seemedto me to be rather disconnected from good old-fashioned results likeHoeffding's and Bernstein's inequalities. But, at least for one flavour of thePAC-Bayesian bounds, there is actually a very close relation, and the maininnovation is a continuous version of the union bound, along with someingenious applications. Here's the gist of what's going on, presented from amachine learning perspective.
arxiv-6000-96 | Learning Boolean Halfspaces with Small Weights from Membership Queries | http://arxiv.org/pdf/1405.1535v1.pdf | author:Hasan Abasi, Ali Z. Abdi, Nader H. Bshouty category:cs.LG published:2014-05-07 summary:We consider the problem of proper learning a Boolean Halfspace with integerweights $\{0,1,\ldots,t\}$ from membership queries only. The best knownalgorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$membership queries where the best lower bound for the number of membershipqueries is $n^t$ [Learning Threshold Functions with Small Weights UsingMembership Queries. COLT 1999] In this paper we close this gap and give an adaptive proper learningalgorithm with two rounds that asks $n^{O(t)}$ membership queries. We also givea non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membershipqueries.
arxiv-6000-97 | A Mathematical Theory of Learning | http://arxiv.org/pdf/1405.1513v1.pdf | author:Ibrahim Alabdulmohsin category:cs.LG cs.AI cs.IT math.IT published:2014-05-07 summary:In this paper, a mathematical theory of learning is proposed that has manyparallels with information theory. We consider Vapnik's General Setting ofLearning in which the learning process is defined to be the act of selecting ahypothesis in response to a given training set. Such hypothesis can, forexample, be a decision boundary in classification, a set of centroids inclustering, or a set of frequent item-sets in association rule mining.Depending on the hypothesis space and how the final hypothesis is selected, weshow that a learning process can be assigned a numeric score, called learningcapacity, which is analogous to Shannon's channel capacity and satisfiessimilar interesting properties as well such as the data-processing inequalityand the information-cannot-hurt inequality. In addition, learning capacityprovides the tightest possible bound on the difference between true risk andempirical risk of the learning process for all loss functions that areparametrized by the chosen hypothesis. It is also shown that the notion oflearning capacity equivalently quantifies how sensitive the choice of the finalhypothesis is to a small perturbation in the training set. Consequently,algorithmic stability is both necessary and sufficient for generalization.While the theory does not rely on concentration inequalities, we finally showthat analogs to classical results in learning theory using the ProbablyApproximately Correct (PAC) model can be immediately deduced using this theory,and conclude with information-theoretic bounds to learning capacity.
arxiv-6000-98 | Pulling back error to the hidden-node parameter technology: Single-hidden-layer feedforward network without output weight | http://arxiv.org/pdf/1405.1445v1.pdf | author:Yimin Yang, Q. M. Jonathan Wu, Guangbin Huang, Yaonan Wang category:cs.NE 68Txx F.1.1 published:2014-05-06 summary:According to conventional neural network theories, the feature ofsingle-hidden-layer feedforward neural networks(SLFNs) resorts to parameters ofthe weighted connections and hidden nodes. SLFNs are universal approximatorswhen at least the parameters of the networks including hidden-node parameterand output weight are exist. Unlike above neural network theories, this paperindicates that in order to let SLFNs work as universal approximators, one maysimply calculate the hidden node parameter only and the output weight is notneeded at all. In other words, this proposed neural network architecture can beconsidered as a standard SLFNs with fixing output weight equal to an unitvector. Further more, this paper presents experiments which show that theproposed learning method tends to extremely reduce network output error to avery small number with only 1 hidden node. Simulation results demonstrate thatthe proposed method can provide several to thousands of times faster than otherlearning algorithm including BP, SVM/SVR and other ELM methods.
arxiv-6000-99 | Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models | http://arxiv.org/pdf/1405.1444v1.pdf | author:Robert T. McGibbon, Bharath Ramsundar, Mohammad M. Sultan, Gert Kiss, Vijay S. Pande category:q-bio.BM stat.AP stat.ML published:2014-05-06 summary:We present a machine learning framework for modeling protein dynamics. Ourapproach uses L1-regularized, reversible hidden Markov models to understandlarge protein datasets generated via molecular dynamics simulations. Our modelis motivated by three design principles: (1) the requirement of massivescalability; (2) the need to adhere to relevant physical law; and (3) thenecessity of providing accessible interpretations, critical for both cellularbiology and rational drug design. We present an EM algorithm for learning andintroduce a model selection criteria based on the physical notion ofconvergence in relaxation timescales. We contrast our model with standardmethods in biophysics and demonstrate improved robustness. We implement ouralgorithm on GPUs and apply the method to two large protein simulation datasetsgenerated respectively on the NCSA Bluewaters supercomputer and theFolding@Home distributed computing network. Our analysis identifies theconformational dynamics of the ubiquitin protein critical to cellularsignaling, and elucidates the stepwise activation mechanism of the c-Src kinaseprotein.
arxiv-6000-100 | The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter | http://arxiv.org/pdf/1405.1438v1.pdf | author:Chenhao Tan, Lillian Lee, Bo Pang category:cs.SI cs.CL physics.soc-ph published:2014-05-06 summary:Consider a person trying to spread an important message on a social network.He/she can spend hours trying to craft the message. Does it actually matter?While there has been extensive prior work looking into predicting popularity ofsocial-media content, the effect of wording per se has rarely been studiedsince it is often confounded with the popularity of the author and the topic.To control for these confounding factors, we take advantage of the surprisingfact that there are many pairs of tweets containing the same url and written bythe same user but employing different wording. Given such pairs, we ask: whichversion attracts more retweets? This turns out to be a more difficult task thanpredicting popular topics. Still, humans can answer this question better thanchance (but far from perfectly), and the computational methods we develop cando better than both an average human and a strong competing method trained onnon-controlled data.
arxiv-6000-101 | Training Restricted Boltzmann Machine by Perturbation | http://arxiv.org/pdf/1405.1436v1.pdf | author:Siamak Ravanbakhsh, Russell Greiner, Brendan Frey category:cs.NE cs.LG stat.ML published:2014-05-06 summary:A new approach to maximum likelihood learning of discrete graphical modelsand RBM in particular is introduced. Our method, Perturb and Descend (PD) isinspired by two ideas (I) perturb and MAP method for sampling (II) learning byContrastive Divergence minimization. In contrast to perturb and MAP, PDleverages training data to learn the models that do not allow efficient MAPestimation. During the learning, to produce a sample from the current model, westart from a training data and descend in the energy landscape of the"perturbed model", for a fixed number of steps, or until a local optima isreached. For RBM, this involves linear calculations and thresholding which canbe very fast. Furthermore we show that the amount of perturbation is closelyrelated to the temperature parameter and it can regularize the model byproducing robust features resulting in sparse hidden layer activation.
arxiv-6000-102 | How Community Feedback Shapes User Behavior | http://arxiv.org/pdf/1405.1429v1.pdf | author:Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2014-05-06 summary:Social media systems rely on user feedback and rating mechanisms forpersonalization, ranking, and content filtering. However, when users evaluatecontent contributed by fellow users (e.g., by liking a post or voting on acomment), these evaluations create complex social feedback effects. This paperinvestigates how ratings on a piece of content affect its author's futurebehavior. By studying four large comment-based news communities, we find thatnegative feedback leads to significant behavioral changes that are detrimentalto the community. Not only do authors of negatively-evaluated contentcontribute more, but also their future posts are of lower quality, and areperceived by the community as such. Moreover, these authors are more likely tosubsequently evaluate their fellow users negatively, percolating these effectsthrough the community. In contrast, positive feedback does not carry similareffects, and neither encourages rewarded authors to write more, nor improvesthe quality of their posts. Interestingly, the authors that receive no feedbackare most likely to leave a community. Furthermore, a structural analysis of thevoter network reveals that evaluations polarize the community the most whenpositive and negative votes are equally split.
arxiv-6000-103 | D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving Word Sense Disambiguation | http://arxiv.org/pdf/1405.1406v1.pdf | author:Sallam Abualhaija, Karl-Heinz Zimmermann category:cs.CL published:2014-05-06 summary:Word sense disambiguation (WSD) is a problem in the field of computationallinguistics given as finding the intended sense of a word (or a set of words)when it is activated within a certain context. WSD was recently addressed as acombinatorial optimization problem in which the goal is to find a sequence ofsenses that maximize the semantic relatedness among the target words. In thisarticle, a novel algorithm for solving the WSD problem called D-Bees isproposed which is inspired by bee colony optimization (BCO)where artificial beeagents collaborate to solve the problem. The D-Bees algorithm is evaluated on astandard dataset (SemEval 2007 coarse-grained English all-words task corpus)andis compared to simulated annealing, genetic algorithms, and two ant colonyoptimization techniques (ACO). It will be observed that the BCO and ACOapproaches are on par.
arxiv-6000-104 | Exemplar Dynamics Models of the Stability of Phonological Categories | http://arxiv.org/pdf/1405.0049v2.pdf | author:P. F. Tupper category:cs.CL cs.SD 91F20 published:2014-04-30 summary:We develop a model for the stability and maintenance of phonologicalcategories. Examples of phonological categories are vowel sounds such as "i"and "e". We model such categories as consisting of collections of labeledexemplars that language users store in their memory. Each exemplar is adetailed memory of an instance of the linguistic entity in question. Startingfrom an exemplar-level model we derive integro-differential equations for thelong-term evolution of the density of exemplars in different portions ofphonetic space. Using these latter equations we investigate under whatconditions two phonological categories merge or not. Our main conclusion isthat for the preservation of distinct phonological categories, it is necessarythat anomalous speech tokens of a given category are discarded, and not merelystored in memory as an exemplar of another category.
arxiv-6000-105 | Automatic Method Of Domain Ontology Construction based on Characteristics of Corpora POS-Analysis | http://arxiv.org/pdf/1405.1346v1.pdf | author:Olena Orobinska category:cs.CL published:2014-05-06 summary:It is now widely recognized that ontologies, are one of the fundamentalcornerstones of knowledge-based systems. What is lacking, however, is acurrently accepted strategy of how to build ontology; what kinds of theresources and techniques are indispensables to optimize the expenses and thetime on the one hand and the amplitude, the completeness, the robustness of enontology on the other hand. The paper offers a semi-automatic ontologyconstruction method from text corpora in the domain of radiological protection.This method is composed from next steps: 1) text annotation with part-of-speechtags; 2) revelation of the significant linguistic structures and forming thetemplates; 3) search of text fragments corresponding to these templates; 4)basic ontology instantiation process
arxiv-6000-106 | Combining Multiple Clusterings via Crowd Agreement Estimation and Multi-Granularity Link Analysis | http://arxiv.org/pdf/1405.1297v1.pdf | author:Dong Huang, Jian-Huang Lai, Chang-Dong Wang category:stat.ML cs.LG published:2014-05-06 summary:The clustering ensemble technique aims to combine multiple clusterings into aprobably better and more robust clustering and has been receiving an increasingattention in recent years. There are mainly two aspects of limitations in theexisting clustering ensemble approaches. Firstly, many approaches lack theability to weight the base clusterings without access to the original data andcan be affected significantly by the low-quality, or even ill clusterings.Secondly, they generally focus on the instance level or cluster level in theensemble system and fail to integrate multi-granularity cues into a unifiedmodel. To address these two limitations, this paper proposes to solve theclustering ensemble problem via crowd agreement estimation andmulti-granularity link analysis. We present the normalized crowd agreementindex (NCAI) to evaluate the quality of base clusterings in an unsupervisedmanner and thus weight the base clusterings in accordance with their clusteringvalidity. To explore the relationship between clusters, the source awareconnected triple (SACT) similarity is introduced with regard to their commonneighbors and the source reliability. Based on NCAI and multi-granularityinformation collected among base clusterings, clusters, and data instances, wefurther propose two novel consensus functions, termed weighted evidenceaccumulation clustering (WEAC) and graph partitioning with multi-granularitylink analysis (GP-MGLA) respectively. The experiments are conducted on eightreal-world datasets. The experimental results demonstrate the effectiveness androbustness of the proposed methods.
arxiv-6000-107 | On Lipschitz Continuity and Smoothness of Loss Functions in Learning to Rank | http://arxiv.org/pdf/1405.0586v2.pdf | author:Ambuj Tewari, Sougata Chaudhuri category:cs.LG stat.ML published:2014-05-03 summary:In binary classification and regression problems, it is well understood thatLipschitz continuity and smoothness of the loss function play key roles ingoverning generalization error bounds for empirical risk minimizationalgorithms. In this paper, we show how these two properties affectgeneralization error bounds in the learning to rank problem. The learning torank problem involves vector valued predictions and therefore the choice of thenorm with respect to which Lipschitz continuity and smoothness are definedbecomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitzcontinuity allows us to improve existing bounds. Furthermore, under smoothnessassumptions, our choice enables us to prove rates that interpolate between$1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popularlearning to rank method, gives state-of-the-art performance guarantees.
arxiv-6000-108 | Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes | http://arxiv.org/pdf/1405.1207v1.pdf | author:Jian Yang, Jianjun Qian, Lei Luo, Fanlong Zhang, Yicheng Gao category:cs.CV published:2014-05-06 summary:Recently regression analysis becomes a popular tool for face recognition. Theexisting regression methods all use the one-dimensional pixel-based errormodel, which characterizes the representation error pixel by pixel individuallyand thus neglects the whole structure of the error image. We observe thatocclusion and illumination changes generally lead to a low-rank error image. Tomake use of this low-rank structural information, this paper presents atwo-dimensional image matrix based error model, i.e. matrix regression, forface representation and classification. Our model uses the minimal nuclear normof representation error image as a criterion, and the alternating directionmethod of multipliers method to calculate the regression coefficients. Comparedwith the current regression methods, the proposed Nuclear Norm based MatrixRegression (NMR) model is more robust for alleviating the effect ofillumination, and more intuitive and powerful for removing the structural noisecaused by occlusion. We experiment using four popular face image databases, theExtended Yale B database, the AR database, the Multi-PIE and the FRGC database.Experimental results demonstrate the performance advantage of NMR over thestate-of-the-art regression based face recognition methods.
arxiv-6000-109 | PANDA: Pose Aligned Networks for Deep Attribute Modeling | http://arxiv.org/pdf/1311.5591v2.pdf | author:Ning Zhang, Manohar Paluri, Marc'Aurelio Ranzato, Trevor Darrell, Lubomir Bourdev category:cs.CV published:2013-11-21 summary:We propose a method for inferring human attributes (such as gender, hairstyle, clothes style, expression, action) from images of people under largevariation of viewpoint, pose, appearance, articulation and occlusion.Convolutional Neural Nets (CNN) have been shown to perform very well on largescale object recognition problems. In the context of attribute classification,however, the signal is often subtle and it may cover only a small part of theimage, while the image is dominated by the effects of pose and viewpoint.Discounting for pose variation would require training on very large labeleddatasets which are not presently available. Part-based models, such as poseletsand DPM have been shown to perform well for this problem but they are limitedby shallow low-level features. We propose a new method which combinespart-based models and deep learning by training pose-normalized CNNs. We showsubstantial improvement vs. state-of-the-art methods on challenging attributeclassification tasks in unconstrained settings. Experiments confirm that ourmethod outperforms both the best part-based methods on this problem andconventional CNNs trained on the full bounding box of the person.
arxiv-6000-110 | Learning Bilingual Word Representations by Marginalizing Alignments | http://arxiv.org/pdf/1405.0947v1.pdf | author:Tomáš Kočiský, Karl Moritz Hermann, Phil Blunsom category:cs.CL published:2014-05-05 summary:We present a probabilistic model that simultaneously learns alignments anddistributed representations for bilingual data. By marginalizing over wordalignments the model captures a larger semantic context than prior work relyingon hard alignments. The advantage of this approach is demonstrated in across-lingual classification task, where we outperform the prior publishedstate of the art.
arxiv-6000-111 | Towards a Benchmark of Natural Language Arguments | http://arxiv.org/pdf/1405.0941v1.pdf | author:Elena Cabrio, Serena Villata category:cs.AI cs.CL published:2014-05-05 summary:The connections among natural language processing and argumentation theoryare becoming stronger in the latest years, with a growing amount of works goingin this direction, in different scenarios and applying heterogeneoustechniques. In this paper, we present two datasets we built to cope with thecombination of the Textual Entailment framework and bipolar abstractargumentation. In our approach, such datasets are used to automaticallyidentify through a Textual Entailment system the relations among the arguments(i.e., attack, support), and then the resulting bipolar argumentation graphsare analyzed to compute the accepted arguments.
arxiv-6000-112 | Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device | http://arxiv.org/pdf/1405.1379v1.pdf | author:Ramin Pichevar, Jason Wung, Daniele Giacobello, Joshua Atkins category:cs.SD cs.CL published:2014-05-05 summary:This paper addresses the challenging scenario for the distant-talking controlof a music playback device, a common portable speaker with four smallloudspeakers in close proximity to one microphone. The user controls the devicethrough voice, where the speech-to-music ratio can be as low as -30 dB duringmusic playback. We propose a speech enhancement front-end that relies on knownrobust methods for echo cancellation, double-talk detection, and noisesuppression, as well as a novel adaptive quasi-binary mask that is well suitedfor speech recognition. The optimization of the system is then formulated as alarge scale nonlinear programming problem where the recognition rate ismaximized and the optimal values for the system parameters are found through agenetic algorithm. We validate our methodology by testing over the TIMITdatabase for different music playback levels and noise types. Finally, we showthat the proposed front-end allows a natural interaction with the device forlimited-vocabulary voice commands.
arxiv-6000-113 | Ridge Fusion in Statistical Learning | http://arxiv.org/pdf/1310.3892v3.pdf | author:Bradley S. Price, Charles J. Geyer, Adam J. Rothman category:stat.ML cs.LG stat.CO published:2013-10-15 summary:We propose a penalized likelihood method to jointly estimate multipleprecision matrices for use in quadratic discriminant analysis and model basedclustering. A ridge penalty and a ridge fusion penalty are used to introduceshrinkage and promote similarity between precision matrix estimates. Block-wisecoordinate descent is used for optimization, and validation likelihood is usedfor tuning parameter selection. Our method is applied in quadratic discriminantanalysis and semi-supervised model based clustering.
arxiv-6000-114 | K-NS: Section-Based Outlier Detection in High Dimensional Space | http://arxiv.org/pdf/1405.1027v1.pdf | author:Zhana Bao category:cs.AI cs.LG stat.ML published:2014-05-05 summary:Finding rare information hidden in a huge amount of data from the Internet isa necessary but complex issue. Many researchers have studied this issue andhave found effective methods to detect anomaly data in low dimensional space.However, as the dimension increases, most of these existing methods performpoorly in detecting outliers because of "high dimensional curse". Even thoughsome approaches aim to solve this problem in high dimensional space, they canonly detect some anomaly data appearing in low dimensional space and cannotdetect all of anomaly data which appear differently in high dimensional space.To cope with this problem, we propose a new k-nearest section-based method(k-NS) in a section-based space. Our proposed approach not only detectsoutliers in low dimensional space with section-density ratio but also detectsoutliers in high dimensional space with the ratio of k-nearest section againstaverage value. After taking a series of experiments with the dimension from 10to 10000, the experiment results show that our proposed method achieves 100%precision and 100% recall result in the case of extremely high dimensionalspace, and better improvement in low dimensional space compared to ourpreviously proposed method.
arxiv-6000-115 | Robust Subspace Outlier Detection in High Dimensional Space | http://arxiv.org/pdf/1405.0869v1.pdf | author:Zhana Bao category:cs.AI cs.LG stat.ML published:2014-05-05 summary:Rare data in a large-scale database are called outliers that revealsignificant information in the real world. The subspace-based outlier detectionis regarded as a feasible approach in very high dimensional space. However, theoutliers found in subspaces are only part of the true outliers in highdimensional space, indeed. The outliers hidden in normal-clustered points aresometimes neglected in the projected dimensional subspace. In this paper, wepropose a robust subspace method for detecting such inner outliers in a givendataset, which uses two dimensional-projections: detecting outliers insubspaces with local density ratio in the first projected dimensions; findingoutliers by comparing neighbor's positions in the second projected dimensions.Each point's weight is calculated by summing up all related values got in thetwo steps projected dimensions, and then the points scoring the largest weightvalues are taken as outliers. By taking a series of experiments with the numberof dimensions from 10 to 10000, the results show that our proposed methodachieves high precision in the case of extremely high dimensional space, andworks well in low dimensional space.
arxiv-6000-116 | Generalized Risk-Aversion in Stochastic Multi-Armed Bandits | http://arxiv.org/pdf/1405.0833v1.pdf | author:Alexander Zimin, Rasmus Ibsen-Jensen, Krishnendu Chatterjee category:cs.LG stat.ML published:2014-05-05 summary:We consider the problem of minimizing the regret in stochastic multi-armedbandit, when the measure of goodness of an arm is not the mean return, but somegeneral function of the mean and the variance.We characterize the conditionsunder which learning is possible and present examples for which no naturalalgorithm can achieve sublinear regret.
arxiv-6000-117 | On Exact Learning Monotone DNF from Membership Queries | http://arxiv.org/pdf/1405.0792v1.pdf | author:Hasan Abasi, Nader H. Bshouty, Hanna Mazzawi category:cs.LG published:2014-05-05 summary:In this paper, we study the problem of learning a monotone DNF with at most$s$ terms of size (number of variables in each term) at most $r$ ($s$ term$r$-MDNF) from membership queries. This problem is equivalent to the problem oflearning a general hypergraph using hyperedge-detecting queries, a problemmotivated by applications arising in chemical reactions and genome sequencing. We first present new lower bounds for this problem and then presentdeterministic and randomized adaptive algorithms with query complexities thatare almost optimal. All the algorithms we present in this paper run in timelinear in the query complexity and the number of variables $n$. In addition,all of the algorithms we present in this paper are asymptotically tight forfixed $r$ and/or $s$.
arxiv-6000-118 | A Structural Approach to Coordinate-Free Statistics | http://arxiv.org/pdf/1405.0110v2.pdf | author:Tom LaGatta, P. Richard Hahn category:math.PR math.FA math.ST stat.ML stat.TH published:2014-05-01 summary:We consider the question of learning in general topological vector spaces. Byexploiting known (or parametrized) covariance structures, our Main Theoremdemonstrates that any continuous linear map corresponds to a certainisomorphism of embedded Hilbert spaces. By inverting this isomorphism andextending continuously, we construct a version of the Ordinary Least Squaresestimator in absolute generality. Our Gauss-Markov theorem demonstrates thatOLS is a "best linear unbiased estimator", extending the classical result. Weconstruct a stochastic version of the OLS estimator, which is a continuousdisintegration exactly for the class of "uncorrelated implies independent"(UII) measures. As a consequence, Gaussian measures always exhibit continuousdisintegrations through continuous linear maps, extending a theorem of thefirst author. Applying this framework to some problems in machine learning, weprove a useful representation theorem for covariance tensors, and show that OLSdefines a good kriging predictor for vector-valued arrays on general indexspaces. We also construct a support-vector machine classifier in this setting.We hope that our article shines light on some deeper connections betweenprobability theory, statistics and machine learning, and may serve as a pointof intersection for these three communities.
arxiv-6000-119 | "Translation can't change a name": Using Multilingual Data for Named Entity Recognition | http://arxiv.org/pdf/1405.0701v1.pdf | author:Manaal Faruqui category:cs.CL published:2014-05-04 summary:Named Entities (NEs) are often written with no orthographic changes acrossdifferent languages that share a common alphabet. We show that this can beleveraged so as to improve named entity recognition (NER) by using unsupervisedword clusters from secondary languages as features in state-of-the-artdiscriminative NER systems. We observe significant increases in performance,finding that person and location identification is particularly improved, andthat phylogenetically close languages provide more valuable features than moredistant languages.
arxiv-6000-120 | Review of Statistical Shape Spaces for 3D Data with Comparative Analysis for Human Faces | http://arxiv.org/pdf/1209.6491v3.pdf | author:Alan Brunton, Augusto Salazar, Timo Bolkart, Stefanie Wuhrer category:cs.CV cs.GR published:2012-09-28 summary:With systems for acquiring 3D surface data being evermore commonplace, it hasbecome important to reliably extract specific shapes from the acquired data. Inthe presence of noise and occlusions, this can be done through the use ofstatistical shape models, which are learned from databases of clean examples ofthe shape in question. In this paper, we review, analyze and compare differentstatistical models: from those that analyze the variation in geometry globallyto those that analyze the variation in geometry locally. We first review howdifferent types of models have been used in the literature, then proceed todefine the models and analyze them theoretically, in terms of both theirstatistical and computational aspects. We then perform extensive experimentalcomparison on the task of model fitting, and give intuition about which type ofmodel is better for a few applications. Due to the wide availability ofdatabases of high-quality data, we use the human face as the specific shape wewish to extract from corrupted data.
arxiv-6000-121 | Rule of Three for Superresolution of Still Images with Applications to Compression and Denoising | http://arxiv.org/pdf/1405.0632v1.pdf | author:Mario Mastriani category:cs.CV published:2014-05-04 summary:We describe a new method for superresolution of still images (in the waveletdomain) based on the reconstruction of missing details subbands pixels at agiven ith level via Rule of Three (Ro3) between pixels of approximation subbandof such level, and pixels of approximation and detail subbands of (i+1)thlevel. The histogramic profiles demonstrate that Ro3 is the appropriatemechanism to recover missing detail subband pixels in these cases. Besides,with the elimination of the details subbands pixels (in an eventual compressionscheme), we obtain a bigger compression rate. Experimental results demonstratethat our approach compares favorably to more typical methods of denoising andcompression in wavelet domain. Our method does not compress, but facilitatesthe action of the real compressor, in our case, Joint Photographic ExpertsGroup (JPEG) and JPEg2000, that is, Ro3 acts as a catalyst compression
arxiv-6000-122 | Automated Attribution and Intertextual Analysis | http://arxiv.org/pdf/1405.0616v1.pdf | author:James Brofos, Ajay Kannan, Rui Shu category:cs.CL cs.DL stat.ML published:2014-05-03 summary:In this work, we employ quantitative methods from the realm of statistics andmachine learning to develop novel methodologies for author attribution andtextual analysis. In particular, we develop techniques and software suitablefor applications to Classical study, and we illustrate the efficacy of ourapproach in several interesting open questions in the field. We apply ournumerical analysis techniques to questions of authorship attribution in thecase of the Greek tragedian Euripides, to instances of intertextuality andinfluence in the poetry of the Roman statesman Seneca the Younger, and to casesof "interpolated" text with respect to the histories of Livy.
arxiv-6000-123 | Extracting Family Relationship Networks from Novels | http://arxiv.org/pdf/1405.0603v1.pdf | author:Aibek Makazhanov, Denilson Barbosa, Grzegorz Kondrak category:cs.CL published:2014-05-03 summary:We present an approach to the extraction of family relations from literarynarrative, which incorporates a technique for utterance attribution proposedrecently by Elson and McKeown (2010). In our work this technique is used incombination with the detection of vocatives - the explicit forms of addressused by the characters in a novel. We take advantage of the fact that certainvocatives indicate family relations between speakers. The extracted relationsare then propagated using a set of rules. We report the results of theapplication of our method to Jane Austen's Pride and Prejudice.
arxiv-6000-124 | Why (and When and How) Contrastive Divergence Works | http://arxiv.org/pdf/1405.0602v1.pdf | author:Ian E Fellows category:stat.ML stat.ME published:2014-05-03 summary:Contrastive divergence (CD) is a promising method of inference in highdimensional distributions with intractable normalizing constants, however, thetheoretical foundations justifying its use are somewhat shaky. This documentproposes a framework for understanding CD inference, how/when it works, andprovides multiple justifications for the CD moment conditions, includingframing them as a variational approximation. Algorithms for performinginference are discussed and are applied to social network data using anexponential-family random graph models (ERGM). The framework also providesguidance about how to construct MCMC kernels providing good CD inference, whichturn out to be quite different from those used typically to provide fast globalmixing.
arxiv-6000-125 | Supervised Descent Method for Solving Nonlinear Least Squares Problems in Computer Vision | http://arxiv.org/pdf/1405.0601v1.pdf | author:Xuehan Xiong, Fernando De la Torre category:cs.CV published:2014-05-03 summary:Many computer vision problems (e.g., camera calibration, image alignment,structure from motion) are solved with nonlinear optimization methods. It isgenerally accepted that second order descent methods are the most robust, fast,and reliable approaches for nonlinear optimization of a general smoothfunction. However, in the context of computer vision, second order descentmethods have two main drawbacks: (1) the function might not be analyticallydifferentiable and numerical approximations are impractical, and (2) theHessian may be large and not positive definite. To address these issues, thispaper proposes generic descent maps, which are average "descent directions" andrescaling factors learned in a supervised fashion. Using generic descent maps,we derive a practical algorithm - Supervised Descent Method (SDM) - forminimizing Nonlinear Least Squares (NLS) problems. During training, SDM learnsa sequence of decent maps that minimize the NLS. In testing, SDM minimizes theNLS objective using the learned descent maps without computing the Jacobian orthe Hessian. We prove the conditions under which the SDM is guaranteed toconverge. We illustrate the effectiveness and accuracy of SDM in three computervision problems: rigid image alignment, non-rigid image alignment, and 3D poseestimation. In particular, we show how SDM achieves state-of-the-artperformance in the problem of facial feature detection. The code has been madeavailable at www.humansensing.cs.cmu.edu/intraface.
arxiv-6000-126 | Perceptron-like Algorithms and Generalization Bounds for Learning to Rank | http://arxiv.org/pdf/1405.0591v1.pdf | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG stat.ML published:2014-05-03 summary:Learning to rank is a supervised learning problem where the output space isthe space of rankings but the supervision space is the space of relevancescores. We make theoretical contributions to the learning to rank problem bothin the online and batch settings. First, we propose a perceptron-like algorithmfor learning a ranking function in an online setting. Our algorithm is anextension of the classic perceptron algorithm for the classification problem.Second, in the setting of batch learning, we introduce a sufficient conditionfor convex ranking surrogates to ensure a generalization bound that isindependent of number of objects per query. Our bound holds when linear rankingfunctions are used: a common practice in many learning to rank algorithms. Enroute to developing the online algorithm and generalization bound, we propose anovel family of listwise large margin ranking surrogates. Our novel surrogatefamily is obtained by modifying a well-known pairwise large margin rankingsurrogate and is distinct from the listwise large margin surrogates developedusing the structured prediction framework. Using the proposed family, weprovide a guaranteed upper bound on the cumulative NDCG (or MAP) induced lossunder the perceptron-like algorithm. We also show that the novel surrogatessatisfy the generalization bound condition.
arxiv-6000-127 | Application of Machine Learning Techniques in Aquaculture | http://arxiv.org/pdf/1405.1304v1.pdf | author:Akhlaqur Rahman, Sumaira Tasnim category:cs.CE cs.LG published:2014-05-03 summary:In this paper we present applications of different machine learningalgorithms in aquaculture. Machine learning algorithms learn models fromhistorical data. In aquaculture historical data are obtained from farmpractices, yields, and environmental data sources. Associations between thesedifferent variables can be obtained by applying machine learning algorithms tohistorical data. In this paper we present applications of different machinelearning algorithms in aquaculture applications.
arxiv-6000-128 | Spatial Neural Networks and their Functional Samples: Similarities and Differences | http://arxiv.org/pdf/1405.0573v1.pdf | author:Lucas Antiqueira, Liang Zhao category:cs.NE q-bio.NC published:2014-05-03 summary:Models of neural networks have proven their utility in the development oflearning algorithms in computer science and in the theoretical study of braindynamics in computational neuroscience. We propose in this paper a spatialneural network model to analyze the important class of functional networks,which are commonly employed in computational studies of clinical brain imagingtime series. We developed a simulation framework inspired by multichannel brainsurface recordings (more specifically, EEG -- electroencephalogram) in order tolink the mesoscopic network dynamics (represented by sampled functionalnetworks) and the microscopic network structure (represented by anintegrate-and-fire neural network located in a 3D space -- hence the termspatial neural network). Functional networks are obtained by computing pairwisecorrelations between time-series of mesoscopic electric potential dynamics,which allows the construction of a graph where each node represents onetime-series. The spatial neural network model is central in this study in thesense that it allowed us to characterize sampled functional networks in termsof what features they are able to reproduce from the underlying spatialnetwork. Our modeling approach shows that, in specific conditions of samplesize and edge density, it is possible to precisely estimate several networkmeasurements of spatial networks by just observing functional samples.
arxiv-6000-129 | Classification of Diabetes Mellitus using Modified Particle Swarm Optimization and Least Squares Support Vector Machine | http://arxiv.org/pdf/1405.0549v1.pdf | author:Omar S. Soliman, Eman AboElhamd category:cs.CE cs.NE published:2014-05-03 summary:Diabetes Mellitus is a major health problem all over the world. Manyclassification algorithms have been applied for its diagnoses and treatment. Inthis paper, a hybrid algorithm of Modified-Particle Swarm Optimization andLeast Squares- Support Vector Machine is proposed for the classification oftype II DM patients. LS-SVM algorithm is used for classification by findingoptimal hyper-plane which separates various classes. Since LS-SVM is sosensitive to the changes of its parameter values, Modified-PSO algorithm isused as an optimization technique for LS-SVM parameters. This will Guaranteethe robustness of the hybrid algorithm by searching for the optimal values forLS-SVM parameters. The pro-posed Algorithm is implemented and evaluated usingPima Indians Diabetes Data set from UCI repository of machine learningdatabases. It is also compared with different classifier algorithms which wereapplied on the same database. The experimental results showed the superiorityof the proposed algorithm which could achieve an average classificationaccuracy of 97.833%.
arxiv-6000-130 | A Network Intrusions Detection System based on a Quantum Bio Inspired Algorithm | http://arxiv.org/pdf/1405.1404v1.pdf | author:Omar S. Soliman, Aliaa Rassem category:cs.NE cs.CR published:2014-05-03 summary:Network intrusion detection systems (NIDSs) have a role of identifyingmalicious activities by monitoring the behavior of networks. Due to thecurrently high volume of networks trafic in addition to the increased number ofattacks and their dynamic properties, NIDSs have the challenge of improvingtheir classification performance. Bio-Inspired Optimization Algorithms (BIOs)are used to automatically extract the the discrimination rules of normal orabnormal behavior to improve the classification accuracy and the detectionability of NIDS. A quantum vaccined immune clonal algorithm with the estimationof distribution algorithm (QVICA-with EDA) is proposed in this paper to build anew NIDS. The proposed algorithm is used as classification algorithm of the newNIDS where it is trained and tested using the KDD data set. Also, the new NIDSis compared with another detection system based on particle swarm optimization(PSO). Results shows the ability of the proposed algorithm of achieving highintrusions classification accuracy where the highest obtained accuracy is 94.8%.
arxiv-6000-131 | Optimal measurement of visual motion across spatial and temporal scales | http://arxiv.org/pdf/1405.0545v1.pdf | author:Sergei Gepshtein, Ivan Tyukin category:cs.CV q-bio.NC published:2014-05-03 summary:Sensory systems use limited resources to mediate the perception of a greatvariety of objects and events. Here a normative framework is presented forexploring how the problem of efficient allocation of resources can be solved invisual perception. Starting with a basic property of every measurement,captured by Gabor's uncertainty relation about the location and frequencycontent of signals, prescriptions are developed for optimal allocation ofsensors for reliable perception of visual motion. This study reveals that alarge-scale characteristic of human vision (the spatiotemporal contrastsensitivity function) is similar to the optimal prescription, and it suggeststhat some previously puzzling phenomena of visual sensitivity, adaptation, andperceptual organization have simple principled explanations.
arxiv-6000-132 | Markov Blanket Ranking using Kernel-based Conditional Dependence Measures | http://arxiv.org/pdf/1402.0108v3.pdf | author:Eric V. Strobl, Shyam Visweswaran category:stat.ML cs.LG published:2014-02-01 summary:Developing feature selection algorithms that move beyond a pure correlationalto a more causal analysis of observational data is an important problem in thesciences. Several algorithms attempt to do so by discovering the Markov blanketof a target, but they all contain a forward selection step which variables mustpass in order to be included in the conditioning set. As a result, thesealgorithms may not consider all possible conditional multivariate combinations.We improve on this limitation by proposing a backward elimination method thatuses a kernel-based conditional dependence measure to identify the Markovblanket in a fully multivariate fashion. The algorithm is easy to implement andcompares favorably to other methods on synthetic and real datasets.
arxiv-6000-133 | Uncertainty of visual measurement and efficient allocation of sensory resources | http://arxiv.org/pdf/1007.0210v2.pdf | author:Sergei Gepshtein, Ivan Tyukin category:q-bio.NC cs.CV cs.IT math.IT published:2010-07-01 summary:We review the reasoning underlying two approaches to combination of sensoryuncertainties. First approach is noncommittal, making no assumptions aboutproperties of uncertainty or parameters of stimulation. Then we explain therelationship between this approach and the one commonly used in modeling"higher level" aspects of sensory systems, such as in visual cue integration,where assumptions are made about properties of stimulation. The two approachesfollow similar logic, except in one case maximal uncertainty is minimized, andin the other minimal certainty is maximized. Then we demonstrate how optimalsolutions are found to the problem of resource allocation under uncertainty.
arxiv-6000-134 | A Rank-SVM Approach to Anomaly Detection | http://arxiv.org/pdf/1405.0530v1.pdf | author:Jing Qian, Jonathan Root, Venkatesh Saligrama, Yuting Chen category:stat.ML published:2014-05-02 summary:We propose a novel non-parametric adaptive anomaly detection algorithm forhigh dimensional data based on rank-SVM. Data points are first ranked based onscores derived from nearest neighbor graphs on n-point nominal data. We thentrain a rank-SVM using this ranked data. A test-point is declared as an anomalyat alpha-false alarm level if the predicted score is in the alpha-percentile.The resulting anomaly detector is shown to be asymptotically optimal andadaptive in that for any false alarm rate alpha, its decision region convergesto the alpha-percentile level set of the unknown underlying density. Inaddition we illustrate through a number of synthetic and real-data experimentsboth the statistical performance and computational efficiency of our anomalydetector.
arxiv-6000-135 | Exchangeable Variable Models | http://arxiv.org/pdf/1405.0501v1.pdf | author:Mathias Niepert, Pedro Domingos category:cs.LG cs.AI published:2014-05-02 summary:A sequence of random variables is exchangeable if its joint distribution isinvariant under variable permutations. We introduce exchangeable variablemodels (EVMs) as a novel class of probabilistic models whose basic buildingblocks are partially exchangeable sequences, a generalization of exchangeablesequences. We prove that a family of tractable EVMs is optimal under zero-oneloss for a large class of functions, including parity and threshold functions,and strictly subsumes existing tractable independence-based model families.Extensive experiments show that EVMs outperform state of the art classifierssuch as SVMs and probabilistic models which are solely based on independenceassumptions.
arxiv-6000-136 | Nested Hierarchical Dirichlet Processes | http://arxiv.org/pdf/1210.6738v4.pdf | author:John Paisley, Chong Wang, David M. Blei, Michael I. Jordan category:stat.ML cs.LG published:2012-10-25 summary:We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We derive a stochasticvariational inference algorithm for the model, in addition to a greedy subtreeselection method for each document, which allows for efficient inference usingmassive collections of text documents. We demonstrate our algorithm on 1.8million documents from The New York Times and 3.3 million documents fromWikipedia.
arxiv-6000-137 | Cover Tree Bayesian Reinforcement Learning | http://arxiv.org/pdf/1305.1809v2.pdf | author:Nikolaos Tziortziotis, Christos Dimitrakakis, Konstantinos Blekas category:stat.ML cs.LG published:2013-05-08 summary:This paper proposes an online tree-based Bayesian approach for reinforcementlearning. For inference, we employ a generalised context tree model. Thisdefines a distribution on multivariate Gaussian piecewise-linear models, whichcan be updated in closed form. The tree structure itself is constructed usingthe cover tree method, which remains efficient in high dimensional spaces. Wecombine the model with Thompson sampling and approximate dynamic programming toobtain effective exploration policies in unknown environments. The flexibilityand computational simplicity of the model render it suitable for manyreinforcement learning problems in continuous state spaces. We demonstrate thisin an experimental comparison with least squares policy iteration.
arxiv-6000-138 | Retrieval in Long Surveillance Videos using User Described Motion and Object Attributes | http://arxiv.org/pdf/1405.0234v1.pdf | author:Greg Castanon, Mohamed Elgharib, Venkatesh Saligrama, Pierre-Marc Jodoin category:cs.CV published:2014-05-01 summary:We present a content-based retrieval method for long surveillance videos bothfor wide-area (Airborne) as well as near-field imagery (CCTV). Our goal is toretrieve video segments, with a focus on detecting objects moving on routes,that match user-defined events of interest. The sheer size and remote locationswhere surveillance videos are acquired, necessitates highly compressedrepresentations that are also meaningful for supporting user-defined queries.To address these challenges we archive long-surveillance video throughlightweight processing based on low-level local spatio-temporal extraction ofmotion and object features. These are then hashed into an inverted index usinglocality-sensitive hashing (LSH). This local approach allows for queryflexibility as well as leads to significant gains in compression. Our secondtask is to extract partial matches to the user-created query and assembles theminto full matches using Dynamic Programming (DP). DP exploits causality toassemble the indexed low level features into a video segment which matches thequery route. We examine CCTV and Airborne footage, whose low contrast makesmotion extraction more difficult. We generate robust motion estimates forAirborne data using a tracklets generation algorithm while we use Horn andSchunck approach to generate motion estimates for CCTV. Our approach handleslong routes, low contrasts and occlusion. We derive bounds on the rate of falsepositives and demonstrate the effectiveness of the approach for counting,motion pattern recognition and abandoned object applications.
arxiv-6000-139 | Forest Sparsity for Multi-channel Compressive Sensing | http://arxiv.org/pdf/1211.4657v2.pdf | author:Chen Chen, Yeqing Li, Junzhou Huang category:cs.LG cs.CV cs.IT math.IT stat.ML published:2012-11-20 summary:In this paper, we investigate a new compressive sensing model formulti-channel sparse data where each channel can be represented as ahierarchical tree and different channels are highly correlated. Therefore, thefull data could follow the forest structure and we call this property as\emph{forest sparsity}. It exploits both intra- and inter- channel correlationsand enriches the family of existing model-based compressive sensing theories.The proposed theory indicates that only $\mathcal{O}(Tk+\log(N/k))$measurements are required for multi-channel data with forest sparsity, where$T$ is the number of channels, $N$ and $k$ are the length and sparsity numberof each channel respectively. This result is much better than$\mathcal{O}(Tk+T\log(N/k))$ of tree sparsity, $\mathcal{O}(Tk+k\log(N/k))$ ofjoint sparsity, and far better than $\mathcal{O}(Tk+Tk\log(N/k))$ of standardsparsity. In addition, we extend the forest sparsity theory to the multiplemeasurement vectors problem, where the measurement matrix is a block-diagonalmatrix. The result shows that the required measurement bound can be the same asthat for dense random measurement matrix, when the data shares equal energy ineach channel. A new algorithm is developed and applied on four exampleapplications to validate the benefit of the proposed model. Extensiveexperiments demonstrate the effectiveness and efficiency of the proposed theoryand algorithm.
arxiv-6000-140 | VSCAN: An Enhanced Video Summarization using Density-based Spatial Clustering | http://arxiv.org/pdf/1405.0174v1.pdf | author:Karim M. Mohamed, Mohamed A. Ismail, Nagia M. Ghanem category:cs.CV cs.MM published:2014-05-01 summary:In this paper, we present VSCAN, a novel approach for generating static videosummaries. This approach is based on a modified DBSCAN clustering algorithm tosummarize the video content utilizing both color and texture features of thevideo frames. The paper also introduces an enhanced evaluation method thatdepends on color and texture features. Video Summaries generated by VSCAN arecompared with summaries generated by other approaches found in the literatureand those created by users. Experimental results indicate that the videosummaries generated by VSCAN have a higher quality than those generated byother approaches.
arxiv-6000-141 | Contextual Semantic Parsing using Crowdsourced Spatial Descriptions | http://arxiv.org/pdf/1405.0145v1.pdf | author:Kais Dukes category:cs.CL published:2014-05-01 summary:We describe a contextual parser for the Robot Commands Treebank, a newcrowdsourced resource. In contrast to previous semantic parsers that select themost-probable parse, we consider the different problem of parsing usingadditional situational context to disambiguate between different readings of asentence. We show that multiple semantic analyses can be searched using dynamicprogramming via interaction with a spatial planner, to guide the parsingprocess. We are able to parse sentences in near linear-time by ruling outanalyses early on that are incompatible with spatial context. We report a 34%upper bound on accuracy, as our planner correctly processes spatial context for3,394 out of 10,000 sentences. However, our parser achieves a 96.53%exact-match score for parsing within the subset of sentences recognized by theplanner, compared to 82.14% for a non-contextual parser.
arxiv-6000-142 | Fast MLE Computation for the Dirichlet Multinomial | http://arxiv.org/pdf/1405.0099v1.pdf | author:Max Sklar category:stat.ML cs.LG published:2014-05-01 summary:Given a collection of categorical data, we want to find the parameters of aDirichlet distribution which maximizes the likelihood of that data. Newton'smethod is typically used for this purpose but current implementations requirereading through the entire dataset on each iteration. In this paper, we proposea modification which requires only a single pass through the dataset andsubstantially decreases running time. Furthermore we analyze both theoreticallyand empirically the performance of the proposed algorithm, and provide an opensource implementation.
arxiv-6000-143 | Sparse Principal Component Analysis via Rotation and Truncation | http://arxiv.org/pdf/1403.1430v2.pdf | author:Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu category:cs.LG cs.CV stat.ML published:2014-03-06 summary:Sparse principal component analysis (sparse PCA) aims at finding a sparsebasis to improve the interpretability over the dense basis of PCA, meanwhilethe sparse basis should cover the data subspace as much as possible. Incontrast to most of existing work which deal with the problem by adding somesparsity penalties on various objectives of PCA, in this paper, we propose anew method SPCArt, whose motivation is to find a rotation matrix and a sparsebasis such that the sparse basis approximates the basis of PCA after therotation. The algorithm of SPCArt consists of three alternating steps: rotatePCA basis, truncate small entries, and update the rotation matrix. Itsperformance bounds are also given. SPCArt is efficient, with each iterationscaling linearly with the data dimension. It is easy to choose parameters inSPCArt, due to its explicit physical explanations. Besides, we give a unifiedview to several existing sparse PCA methods and discuss the connection withSPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCAalgorithm, to overcome its drawback. Experimental results demonstrate thatSPCArt achieves the state-of-the-art performance. It also achieves a goodtradeoff among various criteria, including sparsity, explained variance,orthogonality, balance of sparsity among loadings, and computational speed.
arxiv-6000-144 | Relative Facial Action Unit Detection | http://arxiv.org/pdf/1405.0085v1.pdf | author:Mahmoud Khademi, Louis-Philippe Morency category:cs.CV published:2014-05-01 summary:This paper presents a subject-independent facial action unit (AU) detectionmethod by introducing the concept of relative AU detection, for scenarios wherethe neutral face is not provided. We propose a new classification objectivefunction which analyzes the temporal neighborhood of the current frame todecide if the expression recently increased, decreased or showed no change.This approach is a significant change from the conventional absolute methodwhich decides about AU classification using the current frame, without anexplicit comparison with its neighboring frames. Our proposed method improvesrobustness to individual differences such as face scale and shape, age-relatedwrinkles, and transitions among expressions (e.g., lower intensity ofexpressions). Our experiments on three publicly available datasets (ExtendedCohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvementof our approach over conventional absolute techniques. Keywords: facial actioncoding system (FACS); relative facial action unit detection; temporalinformation;
arxiv-6000-145 | Latent Self-Exciting Point Process Model for Spatial-Temporal Networks | http://arxiv.org/pdf/1302.2671v3.pdf | author:Yoon-Sik Cho, Aram Galstyan, P. Jeffrey Brantingham, George Tita category:cs.SI cs.LG stat.ML published:2013-02-12 summary:We propose a latent self-exciting point process model that describesgeographically distributed interactions between pairs of entities. In contrastto most existing approaches that assume fully observable interactions, here weconsider a scenario where certain interaction events lack information aboutparticipants. Instead, this information needs to be inferred from the availableobservations. We develop an efficient approximate algorithm based onvariational expectation-maximization to infer unknown participants in an eventgiven the location and the time of the event. We validate the model onsynthetic as well as real-world data, and obtain very promising results on theidentity-inference task. We also use our model to predict the timing andparticipants of future events, and demonstrate that it compares favorably withbaseline approaches.
arxiv-6000-146 | Piecewise regression mixture for simultaneous functional data clustering and optimal segmentation | http://arxiv.org/pdf/1312.6974v2.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:This paper introduces a novel mixture model-based approach for simultaneousclustering and optimal segmentation of functional data which are curvespresenting regime changes. The proposed model consists in a finite mixture ofpiecewise polynomial regression models. Each piecewise polynomial regressionmodel is associated with a cluster, and within each cluster, each piecewisepolynomial component is associated with a regime (i.e., a segment). We derivetwo approaches for learning the model parameters. The former is an estimationapproach and consists in maximizing the observed-data likelihood via adedicated expectation-maximization (EM) algorithm. A fuzzy partition of thecurves in K clusters is then obtained at convergence by maximizing theposterior cluster probabilities. The latter however is a classificationapproach and optimizes a specific classification likelihood criterion through adedicated classification expectation-maximization (CEM) algorithm. The optimalcurve segmentation is performed by using dynamic programming. In theclassification approach, both the curve clustering and the optimal segmentationare performed simultaneously as the CEM learning proceeds. We show that theclassification approach is the probabilistic version that generalizes thedeterministic K-means-like algorithm proposed in H\'ebrail et al. (2010). Theproposed approach is evaluated using simulated curves and real-world curves.Comparisons with alternatives including regression mixture models and theK-means like algorithm for piecewise regression demonstrate the effectivenessof the proposed approach.
arxiv-6000-147 | Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints | http://arxiv.org/pdf/1404.7073v2.pdf | author:Jie Fu, Ufuk Topcu category:cs.SY cs.LG cs.LO cs.RO 93E35 I.2.8; G.3 published:2014-04-28 summary:We consider synthesis of control policies that maximize the probability ofsatisfying given temporal logic specifications in unknown, stochasticenvironments. We model the interaction between the system and its environmentas a Markov decision process (MDP) with initially unknown transitionprobabilities. The solution we develop builds on the so-called model-basedprobably approximately correct Markov decision process (PAC-MDP) methodology.The algorithm attains an $\varepsilon$-approximately optimal policy withprobability $1-\delta$ using samples (i.e. observations), time and space thatgrow polynomially with the size of the MDP, the size of the automatonexpressing the temporal logic specification, $\frac{1}{\varepsilon}$,$\frac{1}{\delta}$ and a finite time horizon. In this approach, the systemmaintains a model of the initially unknown MDP, and constructs a product MDPbased on its learned model and the specification automaton that expresses thetemporal logic constraints. During execution, the policy is iteratively updatedusing observation of the transitions taken by the system. The iterationterminates in finitely many steps. With high probability, the resulting policyis such that, for any state, the difference between the probability ofsatisfying the specification under this policy and the optimal one is within apredefined bound.
arxiv-6000-148 | Phase transitions in semisupervised clustering of sparse networks | http://arxiv.org/pdf/1404.7789v1.pdf | author:Pan Zhang, Cristopher Moore, Lenka Zdeborová category:cs.SI physics.soc-ph stat.ML published:2014-04-30 summary:Predicting labels of nodes in a network, such as community memberships ordemographic variables, is an important problem with applications in social andbiological networks. A recently-discovered phase transition puts fundamentallimits on the accuracy of these predictions if we have access only to thenetwork topology. However, if we know the correct labels of some fraction$\alpha$ of the nodes, we can do better. We study the phase diagram of this"semisupervised" learning problem for networks generated by the stochasticblock model. We use the cavity method and the associated belief propagationalgorithm to study what accuracy can be achieved as a function of $\alpha$. For$k = 2$ groups, we find that the detectability transition disappears for any$\alpha > 0$, in agreement with previous work. For larger $k$ where a hard butdetectable regime exists, we find that the easy/hard transition (the point atwhich efficient algorithms can do better than chance) becomes a line oftransitions where the accuracy jumps discontinuously at a critical value of$\alpha$. This line ends in a critical point with a second-order transition,beyond which the accuracy is a continuous function of $\alpha$. We demonstratequalitatively similar transitions in two real-world networks.
arxiv-6000-149 | Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction | http://arxiv.org/pdf/1405.0006v1.pdf | author:Moritz Kassner, William Patera, Andreas Bulling category:cs.CV cs.HC published:2014-04-30 summary:Commercial head-mounted eye trackers provide useful features to customers inindustry and research but are expensive and rely on closed source hardware andsoftware. This limits the application areas and use of mobile eye tracking toexpert users and inhibits user-driven development, customisation, andextension. In this paper we present Pupil -- an accessible, affordable, andextensible open source platform for mobile eye tracking and gaze-basedinteraction. Pupil comprises 1) a light-weight headset with high-resolutioncameras, 2) an open source software framework for mobile eye tracking, as wellas 3) a graphical user interface (GUI) to playback and visualize video and gazedata. Pupil features high-resolution scene and eye cameras for monocular andbinocular gaze estimation. The software and GUI are platform-independent andinclude state-of-the-art algorithms for real-time pupil detection and tracking,calibration, and accurate gaze estimation. Results of a performance evaluationshow that Pupil can provide an average gaze estimation accuracy of 0.6 degreeof visual angle (0.08 degree precision) with a latency of the processingpipeline of only 0.045 seconds.
arxiv-6000-150 | A graph-based mathematical morphology reader | http://arxiv.org/pdf/1404.7748v1.pdf | author:Laurent Najman, Jean Cousty category:cs.CV published:2014-04-30 summary:This survey paper aims at providing a "literary" anthology of mathematicalmorphology on graphs. It describes in the English language many ideas stemmingfrom a large number of different papers, hence providing a unified view of anactive and diverse field of research.
arxiv-6000-151 | Dimension Reduction via Colour Refinement | http://arxiv.org/pdf/1307.5697v2.pdf | author:Martin Grohe, Kristian Kersting, Martin Mladenov, Erkal Selman category:cs.DS cs.DM cs.LG math.OC published:2013-07-22 summary:Colour refinement is a basic algorithmic routine for graph isomorphismtesting, appearing as a subroutine in almost all practical isomorphism solvers.It partitions the vertices of a graph into "colour classes" in such a way thatall vertices in the same colour class have the same number of neighbours inevery colour class. Tinhofer (Disc. App. Math., 1991), Ramana, Scheinerman, andUllman (Disc. Math., 1994) and Godsil (Lin. Alg. and its App., 1997)established a tight correspondence between colour refinement and fractionalisomorphisms of graphs, which are solutions to the LP relaxation of a naturalILP formulation of graph isomorphism. We introduce a version of colour refinement for matrices and extend existingquasilinear algorithms for computing the colour classes. Then we generalise thecorrespondence between colour refinement and fractional automorphisms anddevelop a theory of fractional automorphisms and isomorphisms of matrices. We apply our results to reduce the dimensions of systems of linear equationsand linear programs. Specifically, we show that any given LP L can efficientlybe transformed into a (potentially) smaller LP L' whose number of variables andconstraints is the number of colour classes of the colour refinement algorithm,applied to a matrix associated with the LP. The transformation is such that wecan easily (by a linear mapping) map both feasible and optimal solutions backand forth between the two LPs. We demonstrate empirically that colourrefinement can indeed greatly reduce the cost of solving linear programs.
arxiv-6000-152 | Proceedings of The 38th Annual Workshop of the Austrian Association for Pattern Recognition (ÖAGM), 2014 | http://arxiv.org/pdf/1404.3538v2.pdf | author:Vladimir Kolmogorov, Christoph Lampert, Emilie Morvant, Rustem Takhanov category:cs.CV published:2014-04-14 summary:The 38th Annual Workshop of the Austrian Association for Pattern Recognition(\"OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop providesa platform for researchers and industry to discuss traditional and new areas ofcomputer vision. This year the main topic is: Pattern Recognition:interdisciplinary challenges and opportunities.
arxiv-6000-153 | Gabor Filter and Rough Clustering Based Edge Detection | http://arxiv.org/pdf/1405.0921v1.pdf | author:Chandranath Adak category:cs.CV cs.AI published:2014-04-30 summary:This paper introduces an efficient edge detection method based on Gaborfilter and rough clustering. The input image is smoothed by Gabor function, andthe concept of rough clustering is used to focus on edge detection with softcomputational approach. Hysteresis thresholding is used to get the actualoutput, i.e. edges of the input image. To show the effectiveness, the proposedtechnique is compared with some other edge detection methods.
arxiv-6000-154 | Selecting a Small Set of Optimal Gestures from an Extensive Lexicon | http://arxiv.org/pdf/1404.7594v1.pdf | author:Jacob Grosek, J. Nathan Kutz category:cs.CV published:2014-04-30 summary:Finding the best set of gestures to use for a given computer recognitionproblem is an essential part of optimizing the recognition performance whilebeing mindful to those who may articulate the gestures. An objective function,called the ellipsoidal distance ratio metric (EDRM), for determining the bestgestures from a larger lexicon library is presented, along with a numericalmethod for incorporating subjective preferences. In particular, we demonstratean efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$gestures where typically $n \ll m$ using a weighting of both subjective andobjective measures.
arxiv-6000-155 | Dynamic Mode Decomposition for Real-Time Background/Foreground Separation in Video | http://arxiv.org/pdf/1404.7592v1.pdf | author:Jacob Grosek, J. Nathan Kutz category:cs.CV published:2014-04-30 summary:This paper introduces the method of dynamic mode decomposition (DMD) forrobustly separating video frames into background (low-rank) and foreground(sparse) components in real-time. The method is a novel application of atechnique used for characterizing nonlinear dynamical systems in anequation-free manner by decomposing the state of the system into low-rank termswhose Fourier components in time are known. DMD terms with Fourier frequenciesnear the origin (zero-modes) are interpreted as background (low-rank) portionsof the given video frames, and the terms with Fourier frequencies bounded awayfrom the origin are their sparse counterparts. An approximate low-rank/sparseseparation is achieved at the computational cost of just one singular valuedecomposition and one linear equation solve, thus producing results orders ofmagnitude faster than a leading separation method, namely robust principalcomponent analysis (RPCA). The DMD method that is developed here isdemonstrated to work robustly in real-time with personal laptop-class computingpower and without any parameter tuning, which is a transformative improvementin performance that is ideal for video surveillance and recognitionapplications.
arxiv-6000-156 | Image Compressive Sensing Recovery Using Adaptively Learned Sparsifying Basis via L0 Minimization | http://arxiv.org/pdf/1404.7566v1.pdf | author:Jian Zhang, Chen Zhao, Debin Zhao, Wen Gao category:cs.CV published:2014-04-30 summary:From many fewer acquired measurements than suggested by the Nyquist samplingtheory, compressive sensing (CS) theory demonstrates that, a signal can bereconstructed with high probability when it exhibits sparsity in some domain.Most of the conventional CS recovery approaches, however, exploited a set offixed bases (e.g. DCT, wavelet and gradient domain) for the entirety of asignal, which are irrespective of the non-stationarity of natural signals andcannot achieve high enough degree of sparsity, thus resulting in poor CSrecovery performance. In this paper, we propose a new framework for imagecompressive sensing recovery using adaptively learned sparsifying basis via L0minimization. The intrinsic sparsity of natural images is enforcedsubstantially by sparsely representing overlapped image patches using theadaptively learned sparsifying basis in the form of L0 norm, greatly reducingblocking artifacts and confining the CS solution space. To make our proposedscheme tractable and robust, a split Bregman iteration based technique isdeveloped to solve the non-convex L0 minimization problem efficiently.Experimental results on a wide range of natural images for CS recovery haveshown that our proposed algorithm achieves significant performance improvementsover many current state-of-the-art schemes and exhibits good convergenceproperty.
arxiv-6000-157 | Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates | http://arxiv.org/pdf/1305.5029v2.pdf | author:Yuchen Zhang, John C. Duchi, Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH published:2013-05-22 summary:We establish optimal convergence rates for a decomposition-based scalableapproach to kernel ridge regression. The method is simple to describe: itrandomly partitions a dataset of size N into m subsets of equal size, computesan independent kernel ridge regression estimator for each subset, then averagesthe local solutions into a global predictor. This partitioning leads to asubstantial reduction in computation time versus the standard approach ofperforming kernel ridge regression on all N samples. Our two main theoremsestablish that despite the computational speed-up, statistical optimality isretained: as long as m is not too large, the partition-based estimator achievesthe statistical minimax rate over all estimators using the set of N samples. Asconcrete examples, our theory guarantees that the number of processors m maygrow nearly linearly for finite-rank kernels and Gaussian kernels andpolynomially in N for Sobolev spaces, which in turn allows for substantialreductions in computational cost. We conclude with experiments on bothsimulated data and a music-prediction task that complement our theoreticalresults, exhibiting the computational and statistical benefits of our approach.
arxiv-6000-158 | The Information Geometry of Mirror Descent | http://arxiv.org/pdf/1310.7780v2.pdf | author:Garvesh Raskutti, Sayan Mukherjee category:stat.ML cs.LG published:2013-10-29 summary:Information geometry applies concepts in differential geometry to probabilityand statistics and is especially useful for parameter estimation in exponentialfamilies where parameters are known to lie on a Riemannian manifold.Connections between the geometric properties of the induced manifold andstatistical properties of the estimation problem are well-established. Howeverdeveloping first-order methods that scale to larger problems has been less of afocus in the information geometry community. The best known algorithm thatincorporates manifold structure is the second-order natural gradient descentalgorithm introduced by Amari. On the other hand, stochastic approximationmethods have led to the development of first-order methods for optimizing noisyobjective functions. A recent generalization of the Robbins-Monro algorithmknown as mirror descent, developed by Nemirovski and Yudin is a first ordermethod that induces non-Euclidean geometries. However current analysis ofmirror descent does not precisely characterize the induced non-Euclideangeometry nor does it consider performance in terms of statistical relativeefficiency. In this paper, we prove that mirror descent induced by Bregmandivergences is equivalent to the natural gradient descent algorithm on the dualRiemannian manifold. Using this equivalence, it follows that (1) mirror descentis the steepest descent direction along the Riemannian manifold of theexponential family; (2) mirror descent with log-likelihood loss applied toparameter estimation in exponential families asymptotically achieves theclassical Cram\'er-Rao lower bound and (3) natural gradient descent formanifolds corresponding to exponential families can be implemented as afirst-order method through mirror descent.
arxiv-6000-159 | Implementing spectral methods for hidden Markov models with real-valued emissions | http://arxiv.org/pdf/1404.7472v1.pdf | author:Carl Mattfeld category:cs.LG published:2014-04-29 summary:Hidden Markov models (HMMs) are widely used statistical models for modelingsequential data. The parameter estimation for HMMs from time series data is animportant learning problem. The predominant methods for parameter estimationare based on local search heuristics, most notably the expectation-maximization(EM) algorithm. These methods are prone to local optima and oftentimes sufferfrom high computational and sample complexity. Recent years saw the emergenceof spectral methods for the parameter estimation of HMMs, based on a method ofmoments approach. Two spectral learning algorithms as proposed by Hsu, Kakadeand Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012(arXiv:1203.0683) are assessed in this work. Using experiments with syntheticdata, the algorithms are compared with each other. Furthermore, the spectralmethods are compared to the Baum-Welch algorithm, a well-established methodapplying the EM algorithm to HMMs. The spectral algorithms are found to have amuch more favorable computational and sample complexity. Even though thealgorithms readily handle high dimensional observation spaces, instabilityissues are encountered in this regime. In view of learning from real-worldexperimental data, the representation of real-valued observations for the usein spectral methods is discussed, presenting possible methods to represent datafor the use in the learning algorithms.
arxiv-6000-160 | Information-Theoretic Bounds for Adaptive Sparse Recovery | http://arxiv.org/pdf/1402.5731v2.pdf | author:Cem Aksoylar, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.TH published:2014-02-24 summary:We derive an information-theoretic lower bound for sample complexity insparse recovery problems where inputs can be chosen sequentially andadaptively. This lower bound is in terms of a simple mutual informationexpression and unifies many different linear and nonlinear observation models.Using this formula we derive bounds for adaptive compressive sensing (CS),group testing and 1-bit CS problems. We show that adaptivity cannot decreasesample complexity in group testing, 1-bit CS and CS with linear sparsity. Incontrast, we show there might be mild performance gains for CS in the sublinearregime. Our unified analysis also allows characterization of gains due toadaptivity from a wider perspective on sparse problems.
arxiv-6000-161 | Identification of structural features in chemicals associated with cancer drug response: A systematic data-driven analysis | http://arxiv.org/pdf/1312.7734v2.pdf | author:Suleiman A Khan, Seppo Virtanen, Olli P Kallioniemi, Krister Wennerberg, Antti Poso, Samuel Kaski category:stat.ML q-bio.GN stat.AP published:2013-12-30 summary:Motivation: Analysis of relationships of drug structure to biologicalresponse is key to understanding off-target and unexpected drug effects, andfor developing hypotheses on how to tailor drug thera-pies. New methods arerequired for integrated analyses of a large number of chemical features ofdrugs against the corresponding genome-wide responses of multiple cell models.Results: In this paper, we present the first comprehensive multi-set analysison how the chemical structure of drugs impacts on ge-nome-wide gene expressionacross several cancer cell lines (CMap database). The task is formulated assearching for drug response components across multiple cancers to reveal sharedeffects of drugs and the chemical features that may be responsible. Thecom-ponents can be computed with an extension of a very recent ap-proach calledGroup Factor Analysis (GFA). We identify 11 compo-nents that link thestructural descriptors of drugs with specific gene expression responsesobserved in the three cell lines, and identify structural groups that may beresponsible for the responses. Our method quantitatively outperforms thelimited earlier studies on CMap and identifies both the previously reportedassociations and several interesting novel findings, by taking into accountmultiple cell lines and advanced 3D structural descriptors. The novelobservations include: previously unknown similarities in the effects induced by15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3Ddescriptors of the drugs; and the induction by simvastatin of leukemia-specificanti-inflammatory response, resem-bling the effects of corticosteroids.
arxiv-6000-162 | Near-Optimal Adaptive Compressed Sensing | http://arxiv.org/pdf/1306.6239v2.pdf | author:Matthew L. Malloy, Robert D. Nowak category:cs.IT math.IT stat.ML published:2013-06-26 summary:This paper proposes a simple adaptive sensing and group testing algorithm forsparse signal recovery. The algorithm, termed Compressive Adaptive Sense andSearch (CASS), is shown to be near-optimal in that it succeeds at the lowestpossible signal-to-noise-ratio (SNR) levels, improving on previous work inadaptive compressed sensing. Like traditional compressed sensing based onrandom non-adaptive design matrices, the CASS algorithm requires only k log nmeasurements to recover a k-sparse signal of dimension n. However, CASSsucceeds at SNR levels that are a factor log n less than required by standardcompressed sensing. From the point of view of constructing and implementing thesensing operation as well as computing the reconstruction, the proposedalgorithm is substantially less computationally intensive than standardcompressed sensing. CASS is also demonstrated to perform considerably better inpractice through simulation. To the best of our knowledge, this is the firstdemonstration of an adaptive compressed sensing algorithm with near-optimaltheoretical guarantees and excellent practical performance. This paper alsoshows that methods like compressed sensing, group testing, and pooling have anadvantage beyond simply reducing the number of measurements or tests --adaptive versions of such methods can also improve detection and estimationperformance when compared to non-adaptive direct (uncompressed) sensing.
arxiv-6000-163 | Concise comparative summaries (CCS) of large text corpora with a human experiment | http://arxiv.org/pdf/1404.7362v1.pdf | author:Jinzhu Jia, Luke Miratrix, Bin Yu, Brian Gawalt, Laurent El Ghaoui, Luke Barnesmoore, Sophie Clavier category:cs.CL stat.AP published:2014-04-29 summary:In this paper we propose a general framework for topic-specific summarizationof large text corpora and illustrate how it can be used for the analysis ofnews databases. Our framework, concise comparative summarization (CCS), isbuilt on sparse classification methods. CCS is a lightweight and flexible toolthat offers a compromise between simple word frequency based methods currentlyin wide use and more heavyweight, model-intensive methods such as latentDirichlet allocation (LDA). We argue that sparse methods have much to offer fortext analysis and hope CCS opens the door for a new branch of research in thisimportant field. For a particular topic of interest (e.g., China or energy),CSS automatically labels documents as being either on- or off-topic (usuallyvia keyword search), and then uses sparse classification methods to predictthese labels with the high-dimensional counts of all the other words andphrases in the documents. The resulting small set of phrases found aspredictive are then harvested as the summary. To validate our tool, we, usingnews articles from the New York Times international section, designed andconducted a human survey to compare the different summarizers with humanunderstanding. We demonstrate our approach with two case studies, a mediaanalysis of the framing of "Egypt" in the New York Times throughout the ArabSpring and an informal comparison of the New York Times' and Wall StreetJournal's coverage of "energy." Overall, we find that the Lasso with $L^2$normalization can be effectively and usefully used to summarize large corpora,regardless of document size.
arxiv-6000-164 | Generalized Nonconvex Nonsmooth Low-Rank Minimization | http://arxiv.org/pdf/1404.7306v1.pdf | author:Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin category:cs.CV cs.LG stat.ML published:2014-04-29 summary:As surrogate functions of $L_0$-norm, many nonconvex penalty functions havebeen proposed to enhance the sparse vector recovery. It is easy to extend thesenonconvex penalty functions on singular values of a matrix to enhance low-rankmatrix recovery. However, different from convex optimization, solving thenonconvex low-rank minimization problem is much more challenging than thenonconvex sparse minimization problem. We observe that all the existingnonconvex penalty functions are concave and monotonically increasing on$[0,\infty)$. Thus their gradients are decreasing functions. Based on thisproperty, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm tosolve the nonconvex nonsmooth low-rank minimization problem. IRNN iterativelysolves a Weighted Singular Value Thresholding (WSVT) problem. By setting theweight vector as the gradient of the concave penalty function, the WSVT problemhas a closed form solution. In theory, we prove that IRNN decreases theobjective function value monotonically, and any limit point is a stationarypoint. Extensive experiments on both synthetic data and real images demonstratethat IRNN enhances the low-rank matrix recovery compared with state-of-the-artconvex algorithms.
arxiv-6000-165 | Arguments for Nested Patterns in Neural Ensembles | http://arxiv.org/pdf/1403.6274v2.pdf | author:Kieran Greer category:cs.NE q-bio.NC published:2014-03-25 summary:This paper describes a relatively simple way of allowing a brain model toself-organise its concept patterns through nested structures. Time is a keyelement and a simulator would be able to show how patterns may form and thenfire in sequence, as part of a search or thought process. It uses a very simpleequation to show how the inhibitors in particular, can switch off certainareas, to allow other areas to become the prominent ones and thereby define thecurrent brain state. This allows for a small amount of control over whatappears to be a chaotic structure inside of the brain. It is attractive becauseit is still mostly mechanical and therefore can be added as an automaticprocess, or the modelling of that. The paper also describes how the nestedpattern structure can be used as a basic counting mechanism.
arxiv-6000-166 | Code Minimization for Fringe Projection Based 3D Stereo Sensors by Calibration Improvement | http://arxiv.org/pdf/1404.7298v1.pdf | author:Christian Bräuer-Burchardt, Peter Kühmstedt, Gunther Notni category:math.MG cs.CV published:2014-04-29 summary:Code minimization provides a speed-up of the processing time of fringeprojection based stereo sensors and possibly makes them real-time applicable.This paper reports a methodology which enables such sensors to completely omitGray code or other additional code. Only a sequence of sinusoidal images isnecessary. The code reduction is achieved by involvement of the projection unitinto the measurement, double triangulation, and a precise projector calibrationor significant projector calibration improvement, respectively.
arxiv-6000-167 | A Deep Architecture for Semantic Parsing | http://arxiv.org/pdf/1404.7296v1.pdf | author:Edward Grefenstette, Phil Blunsom, Nando de Freitas, Karl Moritz Hermann category:cs.CL published:2014-04-29 summary:Many successful approaches to semantic parsing build on top of the syntacticanalysis of text, and make use of distributional representations or statisticalmodels to match parses to ontology-specific queries. This paper presents anovel deep learning architecture which provides a semantic parsing systemthrough the union of two neural models of language semantics. It allows for thegeneration of ontology-specific queries from natural language statements andquestions without the need for parsing, which makes it especially suitable togrammatically malformed or syntactically atypical text, such as tweets, as wellas permitting the development of semantic parsers for resource-poor languages.
arxiv-6000-168 | Stereo on a budget | http://arxiv.org/pdf/1404.7059v2.pdf | author:Dana Menaker, Shai Avidan category:cs.CV published:2014-04-28 summary:We propose an algorithm for recovering depth using less than two images.Instead of having both cameras send their entire image to the host computer,the left camera sends its image to the host while the right camera sends only afraction $\epsilon$ of its image. The key aspect is that the cameras send theinformation without communicating at all. Hence, the required communicationbandwidth is significantly reduced. While standard image compression techniques can reduce the communicationbandwidth, this requires additional computational resources on the part of theencoder (camera). We aim at designing a light weight encoder that only touchesa fraction of the pixels. The burden of decoding is placed on the decoder(host). We show that it is enough for the encoder to transmit a sparse set of pixels.Using only $1+\epsilon$ images, with $\epsilon$ as little as 2% of the image,the decoder can compute a depth map. The depth map's accuracy is comparable totraditional stereo matching algorithms that require both images as input. Usingthe depth map and the left image, the right image can be synthesized. Nocomputations are required at the encoder, and the decoder's runtime is linearin the images' size.
arxiv-6000-169 | Robust Logistic Regression using Shift Parameters (Long Version) | http://arxiv.org/pdf/1305.4987v2.pdf | author:Julie Tibshirani, Christopher D. Manning category:cs.AI cs.LG stat.ML published:2013-05-21 summary:Annotation errors can significantly hurt classifier performance, yet datasetsare only growing noisier with the increased use of Amazon Mechanical Turk andtechniques like distant supervision that automatically generate labels. In thispaper, we present a robust extension of logistic regression that incorporatesthe possibility of mislabelling directly into the objective. Our model can betrained through nearly the same means as logistic regression, and retains itsefficiency on high-dimensional datasets. Through named entity recognitionexperiments, we demonstrate that our approach can provide a significantimprovement over the standard model when annotation errors are present.
arxiv-6000-170 | Meteorological time series forecasting based on MLP modelling using heterogeneous transfer functions | http://arxiv.org/pdf/1404.7255v1.pdf | author:Cyril Voyant, Marie Laure Nivet, Christophe Paoli, Marc Muselli, Gilles Notton category:cs.LG published:2014-04-29 summary:In this paper, we propose to study four meteorological and seasonal timeseries coupled with a multi-layer perceptron (MLP) modeling. We chose tocombine two transfer functions for the nodes of the hidden layer, and to use atemporal indicator (time index as input) in order to take into account theseasonal aspect of the studied time series. The results of the predictionconcern two years of measurements and the learning step, eight independentyears. We show that this methodology can improve the accuracy of meteorologicaldata estimation compared to a classical MLP modelling with a homogenoustransfer function.
arxiv-6000-171 | High Dimensional Semiparametric Latent Graphical Model for Mixed Data | http://arxiv.org/pdf/1404.7236v1.pdf | author:Jianqing Fan, Han Liu, Yang Ning, Hui Zou category:stat.ML published:2014-04-29 summary:Graphical models are commonly used tools for modeling multivariate randomvariables. While there exist many convenient multivariate distributions such asGaussian distribution for continuous data, mixed data with the presence ofdiscrete variables or a combination of both continuous and discrete variablesposes new challenges in statistical modeling. In this paper, we propose asemiparametric model named latent Gaussian copula model for binary and mixeddata. The observed binary data are assumed to be obtained by dichotomizing alatent variable satisfying the Gaussian copula distribution or thenonparanormal distribution. The latent Gaussian model with the assumption thatthe latent variables are multivariate Gaussian is a special case of theproposed model. A novel rank-based approach is proposed for both latent graphestimation and latent principal component analysis. Theoretically, the proposedmethods achieve the same rates of convergence for both precision matrixestimation and eigenvector estimation, as if the latent variables wereobserved. Under similar conditions, the consistency of graph structure recoveryand feature selection for leading eigenvectors is established. The performanceof the proposed methods is numerically assessed through simulation studies, andthe usage of our methods is illustrated by a genetic dataset.
arxiv-6000-172 | Structural Group Sparse Representation for Image Compressive Sensing Recovery | http://arxiv.org/pdf/1404.7212v1.pdf | author:Jian Zhang, Debin Zhao, Feng Jiang, Wen Gao category:cs.CV published:2014-04-29 summary:Compressive Sensing (CS) theory shows that a signal can be decoded from manyfewer measurements than suggested by the Nyquist sampling theory, when thesignal is sparse in some domain. Most of conventional CS recovery approaches,however, exploited a set of fixed bases (e.g. DCT, wavelet, contourlet andgradient domain) for the entirety of a signal, which are irrespective of thenonstationarity of natural signals and cannot achieve high enough degree ofsparsity, thus resulting in poor rate-distortion performance. In this paper, wepropose a new framework for image compressive sensing recovery via structuralgroup sparse representation (SGSR) modeling, which enforces image sparsity andself-similarity simultaneously under a unified framework in an adaptive groupdomain, thus greatly confining the CS solution space. In addition, an efficientiterative shrinkage/thresholding algorithm based technique is developed tosolve the above optimization problem. Experimental results demonstrate that thenovel CS recovery strategy achieves significant performance improvements overthe current state-of-the-art schemes and exhibits nice convergence.
arxiv-6000-173 | Spatially Directional Predictive Coding for Block-based Compressive Sensing of Natural Images | http://arxiv.org/pdf/1404.7211v1.pdf | author:Jian Zhang, Debin Zhao, Feng Jiang category:cs.CV published:2014-04-29 summary:A novel coding strategy for block-based compressive sens-ing named spatiallydirectional predictive coding (SDPC) is proposed, which efficiently utilizesthe intrinsic spatial cor-relation of natural images. At the encoder, for eachblock of compressive sensing (CS) measurements, the optimal pre-diction isselected from a set of prediction candidates that are generated by fourdesigned directional predictive modes. Then, the resulting residual isprocessed by scalar quantiza-tion (SQ). At the decoder, the same prediction isadded onto the de-quantized residuals to produce the quantized CS measurements,which is exploited for CS reconstruction. Experimental results substantiatesignificant improvements achieved by SDPC-plus-SQ in rate distortionperformance as compared with SQ alone and DPCM-plus-SQ.
arxiv-6000-174 | Randomized Sketches of Convex Programs with Sharp Guarantees | http://arxiv.org/pdf/1404.7203v1.pdf | author:Mert Pilanci, Martin J. Wainwright category:cs.IT cs.DS math.IT math.OC stat.ML published:2014-04-29 summary:Random projection (RP) is a classical technique for reducing storage andcomputational costs. We analyze RP-based approximations of convex programs, inwhich the original optimization problem is approximated by the solution of alower-dimensional problem. Such dimensionality reduction is essential incomputation-limited settings, since the complexity of general convexprogramming can be quite high (e.g., cubic for quadratic programs, andsubstantially higher for semidefinite programs). In addition to computationalsavings, random projection is also useful for reducing memory usage, and hasuseful properties for privacy-sensitive optimization. We prove that theapproximation ratio of this procedure can be bounded in terms of the geometryof constraint set. For a broad class of random projections, including thosebased on various sub-Gaussian distributions as well as randomized Hadamard andFourier transforms, the data matrix defining the cost function can be projecteddown to the statistical dimension of the tangent cone of the constraints at theoriginal solution, which is often substantially smaller than the originaldimension. We illustrate consequences of our theory for various cases,including unconstrained and $\ell_1$-constrained least squares, support vectormachines, low-rank matrix estimation, and discuss implications onprivacy-sensitive optimization and some connections with de-noising andcompressed sensing.
arxiv-6000-175 | Fast Approximation of Rotations and Hessians matrices | http://arxiv.org/pdf/1404.7195v1.pdf | author:Michael Mathieu, Yann LeCun category:cs.LG published:2014-04-29 summary:A new method to represent and approximate rotation matrices is introduced.The method represents approximations of a rotation matrix $Q$ with linearithmiccomplexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates,arranged in an FFT-like fashion. The approximation is "learned" using gradientdescent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ isa diagonal matrix. It can be used to approximate covariance matrix of Gaussianmodels in order to speed up inference, or to estimate and track the inverseHessian of an objective function by relating changes in parameters to changesin gradient along the trajectory followed by the optimization procedure.Experiments were conducted to approximate synthetic matrices, covariancematrices of real data, and Hessian matrices of objective functions involved inmachine learning problems.
arxiv-6000-176 | Kernel-based Distance Metric Learning in the Output Space | http://arxiv.org/pdf/1312.2578v2.pdf | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2013-12-09 summary:In this paper we present two related, kernel-based Distance Metric Learning(DML) methods. Their respective models non-linearly map data from theiroriginal space to an output space, and subsequent distance measurements areperformed in the output space via a Mahalanobis metric. The dimensionality ofthe output space can be directly controlled to facilitate the learning of alow-rank metric. Both methods allow for simultaneous inference of theassociated metric and the mapping to the output space, which can be used tovisualize the data, when the output space is 2- or 3-dimensional. Experimentalresults for a collection of classification tasks illustrate the advantages ofthe proposed methods over other traditional and kernel-based DML approaches.
arxiv-6000-177 | Automatic Differentiation of Algorithms for Machine Learning | http://arxiv.org/pdf/1404.7456v1.pdf | author:Atilim Gunes Baydin, Barak A. Pearlmutter category:cs.LG cs.SC stat.ML G.1.4; I.2.6 published:2014-04-28 summary:Automatic differentiation---the mechanical transformation of numeric computerprograms to calculate derivatives efficiently and accurately---dates to theorigin of the computer age. Reverse mode automatic differentiation bothantedates and generalizes the method of backwards propagation of errors used inmachine learning. Despite this, practitioners in a variety of fields, includingmachine learning, have been little influenced by automatic differentiation, andmake scant use of available tools. Here we review the technique of automaticdifferentiation, describe its two main modes, and explain how it can benefitmachine learning practitioners. To reach the widest possible audience ourtreatment assumes only elementary differential calculus, and does not assumeany knowledge of linear algebra.
arxiv-6000-178 | Poisson noise reduction with non-local PCA | http://arxiv.org/pdf/1206.0338v4.pdf | author:Joseph Salmon, Zachary Harmany, Charles-Alban Deledalle, Rebecca Willett category:cs.CV cs.LG stat.CO published:2012-06-02 summary:Photon-limited imaging arises when the number of photons collected by asensor array is small relative to the number of detector elements. Photonlimitations are an important concern for many applications such as spectralimaging, night vision, nuclear medicine, and astronomy. Typically a Poissondistribution is used to model these observations, and the inherentheteroscedasticity of the data combined with standard noise removal methodsyields significant artifacts. This paper introduces a novel denoising algorithmfor photon-limited images which combines elements of dictionary learning andsparse patch-based representations of images. The method employs both anadaptation of Principal Component Analysis (PCA) for Poisson noise and recentlydeveloped sparsity-regularized convex optimization algorithms forphoton-limited images. A comprehensive empirical evaluation of the proposedmethod helps characterize the performance of this approach relative to otherstate-of-the-art denoising methods. The results reveal that, despite itsconceptual simplicity, Poisson PCA-based denoising appears to be highlycompetitive in very low light regimes.
arxiv-6000-179 | Conditional Density Estimation with Dimensionality Reduction via Squared-Loss Conditional Entropy Minimization | http://arxiv.org/pdf/1404.6876v1.pdf | author:Voot Tangkaratt, Ning Xie, Masashi Sugiyama category:cs.LG stat.ML published:2014-04-28 summary:Regression aims at estimating the conditional mean of output given input.However, regression is not informative enough if the conditional density ismultimodal, heteroscedastic, and asymmetric. In such a case, estimating theconditional density itself is preferable, but conditional density estimation(CDE) is challenging in high-dimensional space. A naive approach to coping withhigh-dimensionality is to first perform dimensionality reduction (DR) and thenexecute CDE. However, such a two-step process does not perform well in practicebecause the error incurred in the first DR step can be magnified in the secondCDE step. In this paper, we propose a novel single-shot procedure that performsCDE and DR simultaneously in an integrated way. Our key idea is to formulate DRas the problem of minimizing a squared-loss variant of conditional entropy, andthis is solved via CDE. Thus, an additional CDE step is not needed after DR. Wedemonstrate the usefulness of the proposed method through extensive experimentson various datasets including humanoid robot transition and computer art.
arxiv-6000-180 | Proximal Iteratively Reweighted Algorithm with Multiple Splitting for Nonconvex Sparsity Optimization | http://arxiv.org/pdf/1404.6871v1.pdf | author:Canyi Lu, Yunchao Wei, Zhouchen Lin, Shuicheng Yan category:cs.NA cs.CV published:2014-04-28 summary:This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm forsolving a general problem, which involves a large body of nonconvex sparse andstructured sparse related problems. Comparing with previous iterative solversfor nonconvex sparse problem, PIRE is much more general and efficient. Thecomputational cost of PIRE in each iteration is usually as low as thestate-of-the-art convex solvers. We further propose the PIRE algorithm withParallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating(PIRE-AU) to handle the multi-variable problems. In theory, we prove that ourproposed methods converge and any limit solution is a stationary point.Extensive experiments on both synthesis and real data sets demonstrate that ourmethods achieve comparative learning performance, but are much more efficient,by comparing with previous nonconvex solvers.
arxiv-6000-181 | Learning Deep Convolutional Features for MRI Based Alzheimer's Disease Classification | http://arxiv.org/pdf/1404.3366v2.pdf | author:Fayao Liu, Chunhua Shen category:cs.CV published:2014-04-13 summary:Effective and accurate diagnosis of Alzheimer's disease (AD) or mildcognitive impairment (MCI) can be critical for early treatment and thus hasattracted more and more attention nowadays. Since first introduced, machinelearning methods have been gaining increasing popularity for AD relatedresearch. Among the various identified biomarkers, magnetic resonance imaging(MRI) are widely used for the prediction of AD or MCI. However, before amachine learning algorithm can be applied, image features need to be extractedto represent the MRI images. While good representations can be pivotal to theclassification performance, almost all the previous studies typically rely onhuman labelling to find the regions of interest (ROI) which may be correlatedto AD, such as hippocampus, amygdala, precuneus, etc. This procedure requiresdomain knowledge and is costly and tedious. Instead of relying on extraction of ROI features, it is more promising toremove manual ROI labelling from the pipeline and directly work on the raw MRIimages. In other words, we can let the machine learning methods to figure outthese informative and discriminative image structures for AD classification. Inthis work, we propose to learn deep convolutional image features usingunsupervised and supervised learning. Deep learning has emerged as a powerfultool in the machine learning community and has been successfully applied tovarious tasks. We thus propose to exploit deep features of MRI images based ona pre-trained large convolutional neural network (CNN) for AD and MCIclassification, which spares the effort of manual ROI annotation process.
arxiv-6000-182 | Subspace clustering of dimensionality-reduced data | http://arxiv.org/pdf/1404.6818v1.pdf | author:Reinhard Heckel, Michael Tschannen, Helmut Bölcskei category:cs.IT math.IT stat.ML published:2014-04-27 summary:Subspace clustering refers to the problem of clustering unlabeledhigh-dimensional data points into a union of low-dimensional linear subspaces,assumed unknown. In practice one may have access to dimensionality-reducedobservations of the data only, resulting, e.g., from "undersampling" due tocomplexity and speed constraints on the acquisition device. More pertinently,even if one has access to the high-dimensional data set it is often desirableto first project the data points into a lower-dimensional space and to performthe clustering task there; this reduces storage requirements and computationalcost. The purpose of this paper is to quantify the impact ofdimensionality-reduction through random projection on the performance of thesparse subspace clustering (SSC) and the thresholding based subspace clustering(TSC) algorithms. We find that for both algorithms dimensionality reductiondown to the order of the subspace dimensions is possible without incurringsignificant performance degradation. The mathematical engine behind ourtheorems is a result quantifying how the affinities between subspaces changeunder random dimensionality reducing projections.
arxiv-6000-183 | Overlapping Trace Norms in Multi-View Learning | http://arxiv.org/pdf/1404.6163v2.pdf | author:Behrouz Behmardi, Cedric Archambeau, Guillaume Bouchard category:cs.LG published:2014-04-24 summary:Multi-view learning leverages correlations between different sources of datato make predictions in one view based on observations in another view. Apopular approach is to assume that, both, the correlations between the viewsand the view-specific covariances have a low-rank structure, leading tointer-battery factor analysis, a model closely related to canonical correlationanalysis. We propose a convex relaxation of this model using structured normregularization. Further, we extend the convex formulation to a robust versionby adding an l1-penalized matrix to our estimator, similarly to convex robustPCA. We develop and compare scalable algorithms for several convex multi-viewmodels. We show experimentally that the view-specific correlations areimproving data imputation performances, as well as labeling accuracy inreal-world multi-label prediction tasks.
arxiv-6000-184 | Robust and Efficient Subspace Segmentation via Least Squares Regression | http://arxiv.org/pdf/1404.6736v1.pdf | author:Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, Shuicheng Yan category:cs.CV published:2014-04-27 summary:This paper studies the subspace segmentation problem which aims to segmentdata drawn from a union of multiple linear subspaces. Recent works by usingsparse representation, low rank representation and their extensions attractmuch attention. If the subspaces from which the data drawn are independent ororthogonal, they are able to obtain a block diagonal affinity matrix, whichusually leads to a correct segmentation. The main differences among them aretheir objective functions. We theoretically show that if the objective functionsatisfies some conditions, and the data are sufficiently drawn from independentsubspaces, the obtained affinity matrix is always block diagonal. Furthermore,the data sampling can be insufficient if the subspaces are orthogonal. Someexisting methods are all special cases. Then we present the Least SquaresRegression (LSR) method for subspace segmentation. It takes advantage of datacorrelation, which is common in real data. LSR encourages a grouping effectwhich tends to group highly correlated data together. Experimental results onthe Hopkins 155 database and Extended Yale Database B show that our methodsignificantly outperforms state-of-the-art methods. Beyond segmentationaccuracy, all experiments demonstrate that LSR is much more efficient.
arxiv-6000-185 | Model Based Clustering of High-Dimensional Binary Data | http://arxiv.org/pdf/1404.3174v2.pdf | author:Yang Tang, Ryan P. Browne, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2014-04-11 summary:We propose a mixture of latent trait models with common slope parameters(MCLT) for model-based clustering of high-dimensional binary data, a data typefor which few established methods exist. Recent work on clustering of binarydata, based on a $d$-dimensional Gaussian latent variable, is extended byincorporating common factor analyzers. Accordingly, our approach facilitates alow-dimensional visual representation of the clusters. We extend the modelfurther by the incorporation of random block effects. The dependencies in eachblock are taken into account through block-specific parameters that areconsidered to be random variables. A variational approximation to thelikelihood is exploited to derive a fast algorithm for determining the modelparameters. Our approach is demonstrated on real and simulated data.
arxiv-6000-186 | A Constrained Matrix-Variate Gaussian Process for Transposable Data | http://arxiv.org/pdf/1404.6702v1.pdf | author:Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh category:stat.ML cs.LG published:2014-04-27 summary:Transposable data represents interactions among two sets of entities, and aretypically represented as a matrix containing the known interaction values.Additional side information may consist of feature vectors specific to entitiescorresponding to the rows and/or columns of such a matrix. Further informationmay also be available in the form of interactions or hierarchies among entitiesalong the same mode (axis). We propose a novel approach for modelingtransposable data with missing interactions given additional side information.The interactions are modeled as noisy observations from a latent noise freematrix generated from a matrix-variate Gaussian process. The construction ofrow and column covariances using side information provides a flexible mechanismfor specifying a-priori knowledge of the row and column correlations in thedata. Further, the use of such a prior combined with the side informationenables predictions for new rows and columns not observed in the training data.In this work, we combine the matrix-variate Gaussian process model with lowrank constraints. The constrained Gaussian process approach is applied to theprediction of hidden associations between genes and diseases using a small setof observed associations as well as prior covariances induced by gene-geneinteraction networks and disease ontologies. The proposed approach is alsoapplied to recommender systems data which involves predicting the item ratingsof users using known associations as well as prior covariances induced bysocial networks. We present experimental results that highlight the performanceof constrained matrix-variate Gaussian process as compared to state of the artapproaches in each domain.
arxiv-6000-187 | One weird trick for parallelizing convolutional neural networks | http://arxiv.org/pdf/1404.5997v2.pdf | author:Alex Krizhevsky category:cs.NE cs.DC cs.LG published:2014-04-23 summary:I present a new way to parallelize the training of convolutional neuralnetworks across multiple GPUs. The method scales significantly better than allalternatives when applied to modern convolutional neural networks.
arxiv-6000-188 | Sinogram constrained TV-minimization for metal artifact reduction in CT | http://arxiv.org/pdf/1404.6691v1.pdf | author:Clemens Schiffer, Kristian Bredies category:math.NA cs.CV physics.med-ph published:2014-04-26 summary:A new method for reducing metal artifacts in X-ray computed tomography (CT)images is presented. It bases on the solution of a convex optimization problemwith inequality constraints on the sinogram, and total variation regularizationfor the reconstructed image. The Chambolle-Pock algorithm is used tonumerically solve the discretized version of the optimization problem. As proofof concept we present and discuss numerical results for synthetic data.
arxiv-6000-189 | A Comparison of First-order Algorithms for Machine Learning | http://arxiv.org/pdf/1404.6674v1.pdf | author:Yu Wei, Pock Thomas category:cs.LG published:2014-04-26 summary:Using an optimization algorithm to solve a machine learning problem is one ofmainstreams in the field of science. In this work, we demonstrate acomprehensive comparison of some state-of-the-art first-order optimizationalgorithms for convex optimization problems in machine learning. We concentrateon several smooth and non-smooth machine learning problems with a loss functionplus a regularizer. The overall experimental results show the superiority ofprimal-dual algorithms in solving a machine learning problem from theperspectives of the ease to construct, running time and accuracy.
arxiv-6000-190 | Estimation of positive definite M-matrices and structure learning for attractive Gaussian Markov Random fields | http://arxiv.org/pdf/1404.6640v1.pdf | author:Martin Slawski, Matthias Hein category:math.ST stat.ML stat.TH published:2014-04-26 summary:Consider a random vector with finite second moments. If its precision matrixis an M-matrix, then all partial correlations are non-negative. If that randomvector is additionally Gaussian, the corresponding Markov random field (GMRF)is called attractive. We study estimation of M-matrices taking the role ofinverse second moment or precision matrices using sign-constrainedlog-determinant divergence minimization. We also treat the high-dimensionalcase with the number of variables exceeding the sample size. The additionalsign-constraints turn out to greatly simplify the estimation problem: weprovide evidence that explicit regularization is no longer required. To solvethe resulting convex optimization problem, we propose an algorithm based onblock coordinate descent, in which each sub-problem can be recast asnon-negative least squares problem. Illustrations on both simulated and realworld data are provided.
arxiv-6000-191 | On Quadratization of Pseudo-Boolean Functions | http://arxiv.org/pdf/1404.6538v1.pdf | author:Endre Boros, Aritanan Gruber category:math.OC cs.CV math.CO published:2014-04-25 summary:We survey current term-wise techniques for quadratizing high-degreepseudo-Boolean functions and introduce a new one, which allows multiple splitsof terms. We also introduce the first aggregative approach, which splits acollection of terms based on their common parts.
arxiv-6000-192 | Quadratization of Symmetric Pseudo-Boolean Functions | http://arxiv.org/pdf/1404.6535v1.pdf | author:Martin Anthony, Endre Boros, Yves Crama, Aritanan Gruber category:math.OC cs.CC cs.CV math.CO published:2014-04-25 summary:A pseudo-Boolean function is a real-valued function$f(x)=f(x_1,x_2,\ldots,x_n)$ of $n$ binary variables; that is, a mapping from$\{0,1\}^n$ to $\mathbb{R}$. For a pseudo-Boolean function $f(x)$ on$\{0,1\}^n$, we say that $g(x,y)$ is a quadratization of $f$ if $g(x,y)$ is aquadratic polynomial depending on $x$ and on $m$ auxiliary binary variables$y_1,y_2,\ldots,y_m$ such that $f(x)= \min \{g(x,y) : y \in \{0,1\}^m \}$ forall $x \in \{0,1\}^n$. By means of quadratizations, minimization of $f$ isreduced to minimization (over its extended set of variables) of the quadraticfunction $g(x,y)$. This is of some practical interest because minimization ofquadratic functions has been thoroughly studied for the last few decades, andmuch progress has been made in solving such problems exactly or heuristically.A related paper \cite{ABCG} initiated a systematic study of the minimum numberof auxiliary $y$-variables required in a quadratization of an arbitraryfunction $f$ (a natural question, since the complexity of minimizing thequadratic function $g(x,y)$ depends, among other factors, on the number ofbinary variables). In this paper, we determine more precisely the number ofauxiliary variables required by quadratizations of symmetric pseudo-Booleanfunctions $f(x)$, those functions whose value depends only on the Hammingweight of the input $x$ (the number of variables equal to $1$).
arxiv-6000-193 | Nonparametric Detection of Anomalous Data via Kernel Mean Embedding | http://arxiv.org/pdf/1405.2294v1.pdf | author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor, Xinghua Shi category:cs.LG stat.ML published:2014-04-25 summary:An anomaly detection problem is investigated, in which there are totally nsequences with s anomalous sequences to be detected. Each normal sequencecontains m independent and identically distributed (i.i.d.) samples drawn froma distribution p, whereas each anomalous sequence contains m i.i.d. samplesdrawn from a distribution q that is distinct from p. The distributions p and qare assumed to be unknown a priori. Two scenarios, respectively with andwithout a reference sequence generated by p, are studied. Distribution-freetests are constructed using maximum mean discrepancy (MMD) as the metric, whichis based on mean embeddings of distributions into a reproducing kernel Hilbertspace (RKHS). For both scenarios, it is shown that as the number n of sequencesgoes to infinity, if the value of s is known, then the number m of samples ineach sequence should be at the order O(log n) or larger in order for thedeveloped tests to consistently detect s anomalous sequences. If the value of sis unknown, then m should be at the order strictly larger than O(log n).Computational complexity of all developed tests is shown to be polynomial.Numerical results demonstrate that our tests outperform (or perform as well as)the tests based on other competitive traditional statistical approaches andkernel-based approaches under various cases. Consistency of the proposed testis also demonstrated on a real data set.
arxiv-6000-194 | Learning Semantic Script Knowledge with Event Embeddings | http://arxiv.org/pdf/1312.5198v4.pdf | author:Ashutosh Modi, Ivan Titov category:cs.LG cs.CL stat.ML I.2.6; I.2.7 published:2013-12-18 summary:Induction of common sense knowledge about prototypical sequences of eventshas recently received much attention. Instead of inducing this knowledge in theform of graphs, as in much of the previous work, in our method, distributedrepresentations of event realizations are computed based on distributedrepresentations of predicates and their arguments, and then theserepresentations are used to predict prototypical event orderings. Theparameters of the compositional process for computing the event representationsand the ranking component of the model are jointly estimated from texts. Weshow that this approach results in a substantial boost in ordering performancewith respect to previous methods.
arxiv-6000-195 | Indoor Activity Detection and Recognition for Sport Games Analysis | http://arxiv.org/pdf/1404.6413v1.pdf | author:Georg Waltner, Thomas Mauthner, Horst Bischof category:cs.CV published:2014-04-25 summary:Activity recognition in sport is an attractive field for computer visionresearch. Game, player and team analysis are of great interest and researchtopics within this field emerge with the goal of automated analysis. The veryspecific underlying rules of sports can be used as prior knowledge for therecognition task and present a constrained environment for evaluation. Thispaper describes recognition of single player activities in sport with specialemphasis on volleyball. Starting from a per-frame player-centered activityrecognition, we incorporate geometry and contextual information via an activitycontext descriptor that collects information about all player's activities overa certain timespan relative to the investigated player. The benefit of thiscontext information on single player activity recognition is evaluated on ournew real-life dataset presenting a total amount of almost 36k annotated framescontaining 7 activity classes within 6 videos of professional volleyball games.Our incorporation of the contextual information improves the averageplayer-centered classification performance of 77.56% by up to 18.35% onspecific classes, proving that spatio-temporal context is an important clue foractivity recognition.
arxiv-6000-196 | Optimization of OFDM radar waveforms using genetic algorithms | http://arxiv.org/pdf/1405.4894v1.pdf | author:Gabriel Lellouch, Amit Kumar Mishra category:cs.NE published:2014-04-25 summary:In this paper, we present our investigations on the use of single objectiveand multiobjective genetic algorithms based optimisation algorithms to improvethe design of OFDM pulses for radar. We discuss these optimization proceduresin the scope of a waveform design intended for two different radar processingsolutions. Lastly, we show how the encoding solution is suited to permit theoptimizations of waveform for OFDM radar related challenges such as enhanceddetection.
arxiv-6000-197 | An Equivalence between the Lasso and Support Vector Machines | http://arxiv.org/pdf/1303.1152v2.pdf | author:Martin Jaggi category:cs.LG stat.ML F.2.2; I.5.1 published:2013-03-05 summary:We investigate the relation of two fundamental tools in machine learning andsignal processing, that is the support vector machine (SVM) for classification,and the Lasso technique used in regression. We show that the resultingoptimization problems are equivalent, in the following sense. Given anyinstance of an $\ell_2$-loss soft-margin (or hard-margin) SVM, we construct aLasso instance having the same optimal solutions, and vice versa. As a consequence, many existing optimization algorithms for both SVMs andLasso can also be applied to the respective other problem instances. Also, theequivalence allows for many known theoretical insights for SVM and Lasso to betranslated between the two settings. One such implication gives a simplekernelized version of the Lasso, analogous to the kernels used in the SVMsetting. Another consequence is that the sparsity of a Lasso solution is equalto the number of support vectors for the corresponding SVM instance, and thatone can use screening rules to prune the set of support vectors. Furthermore,we can relate sublinear time algorithms for the two problems, and give a newsuch algorithm variant for the Lasso. We also study the regularization pathsfor both methods.
arxiv-6000-198 | Applying machine learning to the problem of choosing a heuristic to select the variable ordering for cylindrical algebraic decomposition | http://arxiv.org/pdf/1404.6369v1.pdf | author:Zongyan Huang, Matthew England, David Wilson, James H. Davenport, Lawrence C. Paulson, James Bridge category:cs.SC cs.LG I.2.6 published:2014-04-25 summary:Cylindrical algebraic decomposition(CAD) is a key tool in computationalalgebraic geometry, particularly for quantifier elimination over real-closedfields. When using CAD, there is often a choice for the ordering placed on thevariables. This can be important, with some problems infeasible with onevariable ordering but easy with another. Machine learning is the process offitting a computer model to a complex function based on properties learned frommeasured data. In this paper we use machine learning (specifically a supportvector machine) to select between heuristics for choosing a variable ordering,outperforming each of the separate heuristics.
arxiv-6000-199 | Improving weather radar by fusion and classification | http://arxiv.org/pdf/1404.6351v1.pdf | author:Harald Ganster, Martina Uray, Sylwia Steginska, Gerardus Croonen, Rudolf Kaltenböck, Karin Hennermann category:cs.CV published:2014-04-25 summary:In air traffic management (ATM) all necessary operations (tactical planing,sector configuration, required staffing, runway configuration, routing ofapproaching aircrafts) rely on accurate measurements and predictions of thecurrent weather situation. An essential basis of information is delivered byweather radar images (WXR), which, unfortunately, exhibit a vast amount ofdisturbances. Thus, the improvement of these datasets is the key factor formore accurate predictions of weather phenomena and weather conditions. Imageprocessing methods based on texture analysis and geometric operators allow toidentify regions including artefacts as well as zones of missing information.Correction of these zones is implemented by exploiting multi-spectral satellitedata (Meteosat Second Generation). Results prove that the proposed system forartefact detection and data correction significantly improves the quality ofWXR data and, thus, enables more reliable weather now- and forecast leading toincreased ATM safety.
arxiv-6000-200 | Fast Exact Search in Hamming Space with Multi-Index Hashing | http://arxiv.org/pdf/1307.2982v3.pdf | author:Mohammad Norouzi, Ali Punjani, David J. Fleet category:cs.CV cs.AI cs.DS cs.IR published:2013-07-11 summary:There is growing interest in representing image data and feature descriptorsusing compact binary codes for fast near neighbor search. Although binary codesare motivated by their use as direct indices (addresses) into a hash table,codes longer than 32 bits are not being used as such, as it was thought to beineffective. We introduce a rigorous way to build multiple hash tables onbinary code substrings that enables exact k-nearest neighbor search in Hammingspace. The approach is storage efficient and straightforward to implement.Theoretical analysis shows that the algorithm exhibits sub-linear run-timebehavior for uniformly distributed codes. Empirical results show dramaticspeedups over a linear scan baseline for datasets of up to one billion codes of64, 128, or 256 bits.
arxiv-6000-201 | Solution Path Clustering with Adaptive Concave Penalty | http://arxiv.org/pdf/1404.6289v1.pdf | author:Yuliya Marchetti, Qing Zhou category:stat.ME stat.ML published:2014-04-24 summary:Fast accumulation of large amounts of complex data has created a need formore sophisticated statistical methodologies to discover interesting patternsand better extract information from these data. The large scale of the dataoften results in challenging high-dimensional estimation problems where only aminority of the data shows specific grouping patterns. To address theseemerging challenges, we develop a new clustering methodology that introducesthe idea of a regularization path into unsupervised learning. A regularizationpath for a clustering problem is created by varying the degree of sparsityconstraint that is imposed on the differences between objects via the minimaxconcave penalty with adaptive tuning parameters. Instead of providing a singlesolution represented by a cluster assignment for each object, the methodproduces a short sequence of solutions that determines not only the clusterassignment but also a corresponding number of clusters for each solution. Theoptimization of the penalized loss function is carried out through an MMalgorithm with block coordinate descent. The advantages of this clusteringalgorithm compared to other existing methods are as follows: it does notrequire the input of the number of clusters; it is capable of simultaneouslyseparating irrelevant or noisy observations that show no grouping pattern,which can greatly improve data interpretation; it is a general methodology thatcan be applied to many clustering problems. We test this method on varioussimulated datasets and on gene expression data, where it shows better orcompetitive performance compared against several clustering methods.
arxiv-6000-202 | Automated adaptive inference of coarse-grained dynamical models in systems biology | http://arxiv.org/pdf/1404.6283v1.pdf | author:Bryan C. Daniels, Ilya Nemenman category:q-bio.QM stat.ML published:2014-04-24 summary:Cellular regulatory dynamics is driven by large and intricate networks ofinteractions at the molecular scale, whose sheer size obfuscates understanding.In light of limited experimental data, many parameters of such dynamics areunknown, and thus models built on the detailed, mechanistic viewpoint overfitand are not predictive. At the other extreme, simple ad hoc models of complexprocesses often miss defining features of the underlying systems. Here wepropose an approach that instead constructs phenomenological, coarse-grainedmodels of network dynamics that automatically adapt their complexity to theamount of available data. Such adaptive models lead to accurate predictionseven when microscopic details of the studied systems are unknown due toinsufficient data. The approach is computationally tractable, even for arelatively large number of dynamical variables, allowing its softwarerealization, named Sir Isaac, to make successful predictions even whenimportant dynamic variables are unobserved. For example, it matches the knownphase space structure for simulated planetary motion data, avoids overfittingin a complex biological signaling system, and produces accurate predictions fora yeast glycolysis model with only tens of data points and over half of theinteracting species unobserved.
arxiv-6000-203 | Scalable Similarity Learning using Large Margin Neighborhood Embedding | http://arxiv.org/pdf/1404.6272v1.pdf | author:Zhaowen Wang, Jianchao Yang, Zhe Lin, Jonathan Brandt, Shiyu Chang, Thomas Huang category:cs.CV cs.LG published:2014-04-24 summary:Classifying large-scale image data into object categories is an importantproblem that has received increasing research attention. Given the huge amountof data, non-parametric approaches such as nearest neighbor classifiers haveshown promising results, especially when they are underpinned by a learneddistance or similarity measurement. Although metric learning has been wellstudied in the past decades, most existing algorithms are impractical to handlelarge-scale data sets. In this paper, we present an image similarity learningmethod that can scale well in both the number of images and the dimensionalityof image descriptors. To this end, similarity comparison is restricted to eachsample's local neighbors and a discriminative similarity measure is inducedfrom large margin neighborhood embedding. We also exploit the ensemble ofprojections so that high-dimensional features can be processed in a set oflower-dimensional subspaces in parallel without much performance compromise.The similarity function is learned online using a stochastic gradient descentalgorithm in which the triplet sampling strategy is customized for quickconvergence of classification performance. The effectiveness of our proposedmodel is validated on several data sets with scales varying from tens ofthousands to one million images. Recognition accuracies competitive with thestate-of-the-art performance are achieved with much higher efficiency andscalability.
arxiv-6000-204 | CoRE Kernels | http://arxiv.org/pdf/1404.6216v1.pdf | author:Ping Li category:stat.ML cs.DS cs.LG stat.ME published:2014-04-24 summary:The term "CoRE kernel" stands for correlation-resemblance kernel. In manyapplications (e.g., vision), the data are often high-dimensional, sparse, andnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binarysparse data and demonstrate the effectiveness of the new kernels through aclassification experiment. CoRE kernels are simple with no tuning parameters.However, training nonlinear kernel SVM can be (very) costly in time and memoryand may not be suitable for truly large-scale industrial applications (e.g.search). In order to make the proposed CoRE kernels more practical, we developbasic probabilistic hashing algorithms which transform nonlinear kernels intolinear kernels.
arxiv-6000-205 | How to Construct Deep Recurrent Neural Networks | http://arxiv.org/pdf/1312.6026v5.pdf | author:Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2013-12-20 summary:In this paper, we explore different ways to extend a recurrent neural network(RNN) to a \textit{deep} RNN. We start by arguing that the concept of depth inan RNN is not as clear as it is in feedforward neural networks. By carefullyanalyzing and understanding the architecture of an RNN, however, we find threepoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)hidden-to-hidden transition and (3) hidden-to-output function. Based on thisobservation, we propose two novel architectures of a deep RNN which areorthogonal to an earlier attempt of stacking multiple recurrent layers to builda deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide analternative interpretation of these deep RNNs using a novel framework based onneural operators. The proposed deep RNNs are empirically evaluated on the tasksof polyphonic music prediction and language modeling. The experimental resultsupports our claim that the proposed deep RNNs benefit from the depth andoutperform the conventional, shallow RNNs.
arxiv-6000-206 | Automatic Construction and Natural-Language Description of Nonparametric Regression Models | http://arxiv.org/pdf/1402.4304v3.pdf | author:James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani category:stat.ML cs.LG published:2014-02-18 summary:This paper presents the beginnings of an automatic statistician, focusing onregression problems. Our system explores an open-ended space of statisticalmodels to discover a good explanation of a data set, and then produces adetailed report with figures and natural-language text. Our approach treatsunknown regression functions nonparametrically using Gaussian processes, whichhas two important consequences. First, Gaussian processes can model functionsin terms of high-level properties (e.g. smoothness, trends, periodicity,changepoints). Taken together with the compositional structure of our languageof models this allows us to automatically describe functions in simple terms.Second, the use of flexible nonparametric models and a rich language forcomposing them in an open-ended manner also results in state-of-the-artextrapolation performance evaluated over 13 real time series data sets fromvarious domains.
arxiv-6000-207 | Unsupervised Text Extraction from G-Maps | http://arxiv.org/pdf/1404.6075v1.pdf | author:Chandranath Adak category:cs.CV cs.AI published:2014-04-24 summary:This paper represents an text extraction method from Google maps, GISmaps/images. Due to an unsupervised approach there is no requirement of anyprior knowledge or training set about the textual and non-textual parts. FuzzyCMeans clustering technique is used for image segmentation and Prewitt methodis used to detect the edges. Connected component analysis and griddingtechnique enhance the correctness of the results. The proposed method reaches98.5% accuracy level on the basis of experimental data sets.
arxiv-6000-208 | Classifying pairs with trees for supervised biological network inference | http://arxiv.org/pdf/1404.6074v1.pdf | author:Marie Schrynemackers, Louis Wehenkel, M. Madan Babu, Pierre Geurts category:cs.LG stat.ML published:2014-04-24 summary:Networks are ubiquitous in biology and computational approaches have beenlargely investigated for their inference. In particular, supervised machinelearning methods can be used to complete a partially known network byintegrating various measurements. Two main supervised frameworks have beenproposed: the local approach, which trains a separate model for each networknode, and the global approach, which trains a single model over pairs of nodes.Here, we systematically investigate, theoretically and empirically, theexploitation of tree-based ensemble methods in the context of these twoapproaches for biological network inference. We first formalize the problem ofnetwork inference as classification of pairs, unifying in the processhomogeneous and bipartite graphs and discussing two main sampling schemes. Wethen present the global and the local approaches, extending the later for theprediction of interactions between two unseen network nodes, and discuss theirspecializations to tree-based ensemble methods, highlighting theirinterpretability and drawing links with clustering techniques. Extensivecomputational experiments are carried out with these methods on variousbiological networks that clearly highlight that these methods are competitivewith existing methods.
arxiv-6000-209 | Rough Clustering Based Unsupervised Image Change Detection | http://arxiv.org/pdf/1404.6071v1.pdf | author:Chandranath Adak category:cs.CV cs.AI published:2014-04-24 summary:This paper introduces an unsupervised technique to detect the changed regionof multitemporal images on a same reference plane with the help of roughclustering. The proposed technique is a soft-computing approach, based on theconcept of rough set with rough clustering and Pawlak's accuracy. It is lessnoisy and avoids pre-deterministic knowledge about the distribution of thechanged and unchanged regions. To show the effectiveness, the proposedtechnique is compared with some other approaches.
arxiv-6000-210 | Rényi Divergence and Kullback-Leibler Divergence | http://arxiv.org/pdf/1206.2459v2.pdf | author:Tim van Erven, Peter Harremoës category:cs.IT math.IT math.ST stat.ML stat.TH published:2012-06-12 summary:R\'enyi divergence is related to R\'enyi entropy much like Kullback-Leiblerdivergence is related to Shannon's entropy, and comes up in many settings. Itwas introduced by R\'enyi as a measure of information that satisfies almost thesame axioms as Kullback-Leibler divergence, and depends on a parameter that iscalled its order. In particular, the R\'enyi divergence of order 1 equals theKullback-Leibler divergence. We review and extend the most important properties of R\'enyi divergence andKullback-Leibler divergence, including convexity, continuity, limits of$\sigma$-algebras and the relation of the special order 0 to the Gaussiandichotomy and contiguity. We also show how to generalize the Pythagoreaninequality to orders different from 1, and we extend the known equivalencebetween channel capacity and minimax redundancy to continuous channel inputs(for all orders) and present several other minimax results.
arxiv-6000-211 | Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling | http://arxiv.org/pdf/1403.3080v2.pdf | author:Xi Chen, Qihang Lin, Dengyong Zhou category:cs.LG math.OC stat.ML published:2014-03-12 summary:In crowd labeling, a large amount of unlabeled data instances are outsourcedto a crowd of workers. Workers will be paid for each label they provide, butthe labeling requester usually has only a limited amount of the budget. Sincedata instances have different levels of labeling difficulty and workers havedifferent reliability, it is desirable to have an optimal policy to allocatethe budget among all instance-worker pairs such that the overall labelingaccuracy is maximized. We consider categorical labeling tasks and formulate thebudget allocation problem as a Bayesian Markov decision process (MDP), whichsimultaneously conducts learning and decision making. Using the dynamicprogramming (DP) recurrence, one can obtain the optimal allocation policy.However, DP quickly becomes computationally intractable when the size of theproblem increases. To solve this challenge, we propose a computationallyefficient approximate policy, called optimistic knowledge gradient policy. OurMDP is a quite general framework, which applies to both pull crowdsourcingmarketplaces with homogeneous workers and push marketplaces with heterogeneousworkers. It can also incorporate the contextual information of instances whenthey are available. The experiments on both simulated and real data show thatthe proposed policy achieves a higher labeling accuracy than other existingpolicies at the same budget level.
arxiv-6000-212 | The fshape framework for the variability analysis of functional shapes | http://arxiv.org/pdf/1404.6039v1.pdf | author:Benjamin Charlier, Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG published:2014-04-24 summary:This article introduces a full mathematical and numerical framework fortreating functional shapes (or fshapes) following the landmarks of shape spacesand shape analysis. Functional shapes can be described as signal functionssupported on varying geometrical supports. Analysing variability of fshapes'ensembles require the modelling and quantification of joint variations ingeometry and signal, which have been treated separately in previous approaches.Instead, building on the ideas of shape spaces for purely geometrical objects,we propose the extended concept of fshape bundles and define Riemannian metricsfor fshape metamorphoses to model geometrico-functional transformations withinthese bundles. We also generalize previous works on data attachment terms basedon the notion of varifolds and demonstrate the utility of these distances.Based on these, we propose variational formulations of the atlas estimationproblem on populations of fshapes and prove existence of solutions for thedifferent models. The second part of the article examines the numericalimplementation of the models by detailing discrete expressions for the metricsand gradients and proposing an optimization scheme for the atlas estimationproblem. We present a few results of the methodology on a synthetic dataset aswell as on a population of retinal membranes with thickness maps.
arxiv-6000-213 | Maximum Margin Vector Correlation Filter | http://arxiv.org/pdf/1404.6031v1.pdf | author:Vishnu Naresh Boddeti, B. V. K. Vijaya Kumar category:cs.CV published:2014-04-24 summary:Correlation Filters (CFs) are a class of classifiers which are designed foraccurate pattern localization. Traditionally CFs have been used with scalarfeatures only, which limits their ability to be used with vector featurerepresentations like Gabor filter banks, SIFT, HOG, etc. In this paper wepresent a new CF named Maximum Margin Vector Correlation Filter (MMVCF) whichextends the traditional CF designs to vector features. MMVCF further combinesthe generalization capability of large margin based classifiers like SupportVector Machines (SVMs) and the localization properties of CFs for betterrobustness to outliers. We demonstrate the efficacy of MMVCF for objectdetection and landmark localization on a variety of databases and demonstratethat MMVCF consistently shows improved pattern localization capability incomparison to SVMs.
arxiv-6000-214 | Study design in causal models | http://arxiv.org/pdf/1211.2958v4.pdf | author:Juha Karvanen category:stat.ME stat.AP stat.ML G.3; G.2.2 published:2012-11-13 summary:The causal assumptions, the study design and the data are the elementsrequired for scientific inference in empirical research. The research isadequately communicated only if all of these elements and their relations aredescribed precisely. Causal models with design describe the study design andthe missing data mechanism together with the causal structure and allow thedirect application of causal calculus in the estimation of the causal effects.The flow of the study is visualized by ordering the nodes of the causal diagramin two dimensions by their causal order and the time of the observation.Conclusions whether a causal or observational relationship can be estimatedfrom the collected incomplete data can be made directly from the graph. Causalmodels with design offer a systematic and unifying view scientific inferenceand increase the clarity and speed of communication. Examples on the causalmodels for a case-control study, a nested case-control study, a clinical trialand a two-stage case-cohort study are presented.
arxiv-6000-215 | Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System | http://arxiv.org/pdf/1405.6164v1.pdf | author:Ion Androutsopoulos, Gerasimos Lampouras, Dimitrios Galanis category:cs.CL cs.AI published:2014-04-24 summary:We present NaturalOWL, a natural language generation system that producestexts describing individuals or classes of OWL ontologies. Unlike simpler OWLverbalizers, which typically express a single axiom at a time in controlled,often not entirely fluent natural language primarily for the benefit of domainexperts, we aim to generate fluent and coherent multi-sentence texts forend-users. With a system like NaturalOWL, one can publish information in OWL onthe Web, along with automatically produced corresponding texts in multiplelanguages, making the information accessible not only to computer programs anddomain experts, but also end-users. We discuss the processing stages ofNaturalOWL, the optional domain-dependent linguistic resources that the systemcan use at each stage, and why they are useful. We also present trials showingthat when the domain-dependent llinguistic resources are available, NaturalOWLproduces significantly better texts compared to a simpler verbalizer, and thatthe resources can be created with relatively light effort.
arxiv-6000-216 | On Learning Where To Look | http://arxiv.org/pdf/1405.5488v1.pdf | author:Marc'Aurelio Ranzato category:cs.CV cs.LG published:2014-04-24 summary:Current automatic vision systems face two major challenges: scalability andextreme variability of appearance. First, the computational time required toprocess an image typically scales linearly with the number of pixels in theimage, therefore limiting the resolution of input images to thumbnail size.Second, variability in appearance and pose of the objects constitute a majorhurdle for robust recognition and detection. In this work, we propose a modelthat makes baby steps towards addressing these challenges. We describe alearning based method that recognizes objects through a series of glimpses.This system performs an amount of computation that scales with the complexityof the input rather than its number of pixels. Moreover, the proposed method ispotentially more robust to changes in appearance since its parameters arelearned in a data driven manner. Preliminary experiments on a handwrittendataset of digits demonstrate the computational advantages of this approach.
arxiv-6000-217 | An Account of Opinion Implicatures | http://arxiv.org/pdf/1404.6491v1.pdf | author:Janyce Wiebe, Lingjia Deng category:cs.CL cs.IR published:2014-04-23 summary:While previous sentiment analysis research has concentrated on theinterpretation of explicitly stated opinions and attitudes, this work initiatesthe computational study of a type of opinion implicature (i.e.,opinion-oriented inference) in text. This paper described a rule-basedframework for representing and analyzing opinion implicatures which we hopewill contribute to deeper automatic interpretation of subjective language. Inthe course of understanding implicatures, the system recognizes implicitsentiments (and beliefs) toward various events and entities in the sentence,often attributed to different sources (holders) and of mixed polarities; thus,it produces a richer interpretation than is typical in opinion analysis.
arxiv-6000-218 | Probabilistic graphs using coupled random variables | http://arxiv.org/pdf/1404.6955v1.pdf | author:Kenric P. Nelson, Madalina Barbu, Brian J. Scannell category:cs.LG cs.IT cs.NE math.IT published:2014-04-23 summary:Neural network design has utilized flexible nonlinear processes which canmimic biological systems, but has suffered from a lack of traceability in theresulting network. Graphical probabilistic models ground network design inprobabilistic reasoning, but the restrictions reduce the expressive capabilityof each node making network designs complex. The ability to model coupledrandom variables using the calculus of nonextensive statistical mechanicsprovides a neural node design incorporating nonlinear coupling between inputstates while maintaining the rigor of probabilistic reasoning. A generalizationof Bayes rule using the coupled product enables a single node to modelcorrelation between hundreds of random variables. A coupled Markov random fieldis designed for the inferencing and classification of UCI's MLR 'MultipleFeatures Data Set' such that thousands of linear correlation parameters can bereplaced with a single coupling parameter with just a (3%, 4%) percentreduction in (classification, inference) performance.
arxiv-6000-219 | Learning Human Pose Estimation Features with Convolutional Networks | http://arxiv.org/pdf/1312.7302v6.pdf | author:Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor, Christoph Bregler category:cs.CV cs.LG cs.NE published:2013-12-27 summary:This paper introduces a new architecture for human pose estimation using amulti- layer convolutional network architecture and a modified learningtechnique that learns low-level features and higher-level weak spatial models.Unconstrained human pose estimation is one of the hardest problems in computervision, and our new architecture and learning schema shows significantimprovement over the current state-of-the-art results. The main contribution ofthis paper is showing, for the first time, that a specific variation of deeplearning is able to outperform all existing traditional architectures on thistask. The paper also discusses several lessons learned while researchingalternatives, most notably, that it is possible to learn strong low-levelfeature detectors on features that might even just cover a few pixels in theimage. Higher-level spatial models improve somewhat the overall result, but toa much lesser extent then expected. Many researchers previously argued that thekinematic structure and top-down information is crucial for this domain, butwith our purely bottom up, and weak spatial model, we could improve other morecomplicated architectures that currently produce the best results. This mirrorswhat many other researchers, like those in the speech recognition, objectrecognition, and other domains have experienced.
arxiv-6000-220 | Most Correlated Arms Identification | http://arxiv.org/pdf/1404.5903v1.pdf | author:Che-Yu Liu, Sébastien Bubeck category:stat.ML cs.LG published:2014-04-23 summary:We study the problem of finding the most mutually correlated arms among manyarms. We show that adaptive arms sampling strategies can have significantadvantages over the non-adaptive uniform sampling strategy. Our proposedalgorithms rely on a novel correlation estimator. The use of this accurateestimator allows us to get improved results for a wide range of probleminstances.
arxiv-6000-221 | Representation Learning: A Review and New Perspectives | http://arxiv.org/pdf/1206.5538v3.pdf | author:Yoshua Bengio, Aaron Courville, Pascal Vincent category:cs.LG published:2012-06-24 summary:The success of machine learning algorithms generally depends on datarepresentation, and we hypothesize that this is because differentrepresentations can entangle and hide more or less the different explanatoryfactors of variation behind the data. Although specific domain knowledge can beused to help design representations, learning with generic priors can also beused, and the quest for AI is motivating the design of more powerfulrepresentation-learning algorithms implementing such priors. This paper reviewsrecent work in the area of unsupervised feature learning and deep learning,covering advances in probabilistic models, auto-encoders, manifold learning,and deep networks. This motivates longer-term unanswered questions about theappropriate objectives for learning good representations, for computingrepresentations (i.e., inference), and the geometrical connections betweenrepresentation learning, density estimation and manifold learning.
arxiv-6000-222 | Contractive De-noising Auto-encoder | http://arxiv.org/pdf/1305.4076v5.pdf | author:Fu-qiang Chen, Yan Wu, Guo-dong Zhao, Jun-ming Zhang, Ming Zhu, Jing Bai category:cs.LG published:2013-05-17 summary:Auto-encoder is a special kind of neural network based on reconstruction.De-noising auto-encoder (DAE) is an improved auto-encoder which is robust tothe input by corrupting the original data first and then reconstructing theoriginal input by minimizing the reconstruction error function. And contractiveauto-encoder (CAE) is another kind of improved auto-encoder to learn robustfeature by introducing the Frobenius norm of the Jacobean matrix of the learnedfeature with respect to the original input. In this paper, we combinede-noising auto-encoder and contractive auto- encoder, and propose anotherimproved auto-encoder, contractive de-noising auto- encoder (CDAE), which isrobust to both the original input and the learned feature. We stack CDAE toextract more abstract features and apply SVM for classification. The experimentresult on benchmark dataset MNIST shows that our proposed CDAE performed betterthan both DAE and CAE, proving the effective of our method.
arxiv-6000-223 | Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates | http://arxiv.org/pdf/1404.5767v1.pdf | author:Hendrik Richter category:cs.NE published:2014-04-23 summary:Coevolutionary minimal substrates are simple and abstract models that allowstudying the relationships and codynamics between objective and subjectivefitness. Using these models an approach is presented for defining and analyzingfitness landscapes of coevolutionary problems. We devise similarity measures ofcodynamic fitness landscapes and experimentally study minimal substrates oftest--based and compositional problems for both cooperative and competitiveinteraction.
arxiv-6000-224 | Find my mug: Efficient object search with a mobile robot using semantic segmentation | http://arxiv.org/pdf/1404.5765v1.pdf | author:Daniel Wolf, Markus Bajones, Johann Prankl, Markus Vincze category:cs.CV cs.RO published:2014-04-23 summary:In this paper, we propose an efficient semantic segmentation framework forindoor scenes, tailored to the application on a mobile robot. Semanticsegmentation can help robots to gain a reasonable understanding of theirenvironment, but to reach this goal, the algorithms not only need to beaccurate, but also fast and robust. Therefore, we developed an optimized 3Dpoint cloud processing framework based on a Randomized Decision Forest,achieving competitive results at sufficiently high frame rates. We evaluate thecapabilities of our method on the popular NYU depth dataset and our own dataand demonstrate its feasibility by deploying it on a mobile service robot, forwhich we could optimize an object search procedure using our results.
arxiv-6000-225 | Nested Graph Words for Object Recognition | http://arxiv.org/pdf/1106.2729v2.pdf | author:Svebor Karaman, Jenny Benois-Pineau, Rémi Mégret category:cs.MM cs.CV published:2011-06-14 summary:In this paper, we propose a new, scalable approach for the task of objectbased image search or object recognition. Despite the very large literatureexisting on the scalability issues in CBIR in the sense of retrievalapproaches, the scalability of media and scalability of features remain anissue. In our work we tackle the problem of scalability and structuralorganization of features. The proposed features are nested local graphs builtupon sets of SURF feature points with Delaunay triangulation. ABag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birthto a Bag-of-Graph-Words representation. The nested nature of the descriptorsconsists in scaling from trivial Delaunay graphs - isolated feature points - byincreasing the number of nodes layer by layer up to graphs with maximal numberof nodes. For each layer of graphs its proper visual dictionary is built. Theexperiments conducted on the SIVAL data set reveal that the graph features atdifferent layers exhibit complementary performances on the same content. Thenested approach, the combination of all existing layers, yields significantimprovement of the object recognition performance compared to single levelapproaches.
arxiv-6000-226 | Density Estimation via Adaptive Partition and Discrepancy Control | http://arxiv.org/pdf/1404.1425v3.pdf | author:Kun Yang, Wing Hung Wong category:stat.ML published:2014-04-05 summary:Given iid samples from some unknown continuous density on hyper-rectangle$[0, 1]^d$, we attempt to learn a piecewise constant function that approximatesthis underlying density nonparametrically. Our density estimate is defined on abinary split of $[0, 1]^d$ and built up sequentially according to discrepancycriteria; the key ingredient is to control the discrepancy adaptively in eachsub-rectangle to achieve overall bound. We prove that the estimate, even thoughsimple as it appears, preserves most of the estimation power. By exploiting itsstructure, it can be directly applied to some important pattern recognitiontasks such as mode seeking and density landscape exploration, we demonstrateits applicability through simulations and examples.
arxiv-6000-227 | Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization and Layered Clustering-Based Approach | http://arxiv.org/pdf/1303.2132v2.pdf | author:Xiao-Lei Zhang category:cs.LG published:2013-03-08 summary:One important classifier ensemble for multiclass classification problems isError-Correcting Output Codes (ECOCs). It bridges multiclass problems andbinary-class classifiers by decomposing multiclass problems to a serialbinary-class problems. In this paper, we present a heuristic ternary code,named Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). Itstarts with an arbitrary valid ECOC and iterates the following two steps untilthe training risk converges. The first step, named Layered Clustering basedECOC (LC-ECOC), constructs multiple strong classifiers on the most confusingbinary-class problem. The second step adds the new classifiers to ECOC by anovel Optimized Weighted (OW) decoding algorithm, where the optimizationproblem of the decoding is solved by the cutting plane algorithm. Technically,LC-ECOC makes the heuristic training process not blocked by some difficultbinary-class problem. OW decoding guarantees the non-increase of the trainingrisk for ensuring a small code length. Results on 14 UCI datasets and a musicgenre classification problem demonstrate the effectiveness of WOLC-ECOC.
arxiv-6000-228 | Large Margin Image Set Representation and Classification | http://arxiv.org/pdf/1404.5588v1.pdf | author:Jim Jing-Yan Wang, Majed Alzahrani, Xin Gao category:cs.CV published:2014-04-22 summary:In this paper, we propose a novel image set representation and classificationmethod by maximizing the margin of image sets. The margin of an image set isdefined as the difference of the distance to its nearest image set fromdifferent classes and the distance to its nearest image set of the same class.By modeling the image sets by using both their image samples and their affinehull models, and maximizing the margins of the images sets, the image setrepresentation parameter learning problem is formulated as an minimizationproblem, which is further optimized by an expectation -maximization (EM)strategy with accelerated proximal gradient (APG) optimization in an iterativealgorithm. To classify a given test image set, we assign it to the class whichcould provide the largest margin. Experiments on two applications ofvideo-sequence-based face recognition demonstrate that the proposed methodsignificantly outperforms state-of-the-art image set classification methods interms of both effectiveness and efficiency.
arxiv-6000-229 | A Structural Query System for Han Characters | http://arxiv.org/pdf/1404.5585v1.pdf | author:Matthew Skala category:cs.CL cs.DB H.3.1 published:2014-04-22 summary:The IDSgrep structural query system for Han character dictionaries ispresented. This system includes a data model and syntax for describing thespatial structure of Han characters using Extended Ideographic DescriptionSequences (EIDSes) based on the Unicode IDS syntax; a language for queryingEIDS databases, designed to suit the needs of font developers and foreignlanguage learners; a bit vector index inspired by Bloom filters for fasterquery operations; a freely available implementation; and format translationfrom popular third-party IDS and XML character databases. Experimental resultsare included, with a comparison to other software used for similarapplications.
arxiv-6000-230 | Discrete Restricted Boltzmann Machines | http://arxiv.org/pdf/1301.3529v4.pdf | author:Guido Montufar, Jason Morton category:stat.ML math.AG math.PR G.3 published:2013-01-15 summary:We describe discrete restricted Boltzmann machines: probabilistic graphicalmodels with bipartite interactions between visible and hidden discretevariables. Examples are binary restricted Boltzmann machines and discrete naiveBayes models. We detail the inference functions and distributed representationsarising in these models in terms of configurations of projected products ofsimplices and normal fans of products of simplices. We bound the number ofhidden variables, depending on the cardinalities of their state spaces, forwhich these models can approximate any probability distribution on theirvisible states to any given accuracy. In addition, we use algebraic methods andcoding theory to compute their dimension.
arxiv-6000-231 | Blind Image Deblurring by Spectral Properties of Convolution Operators | http://arxiv.org/pdf/1209.2082v3.pdf | author:Guangcan Liu, Shiyu Chang, Yi Ma category:cs.CV published:2012-09-10 summary:In this paper, we study the problem of recovering a sharp version of a givenblurry image when the blur kernel is unknown. Previous methods often introducean image-independent regularizer (such as Gaussian or sparse priors) on thedesired blur kernel. We shall show that the blurry image itself encodes richinformation about the blur kernel. Such information can be found throughanalyzing and comparing how the spectrum of an image as a convolution operatorchanges before and after blurring. Our analysis leads to an effective convexregularizer on the blur kernel which depends only on the given blurry image. Weshow that the minimizer of this regularizer guarantees to give goodapproximation to the blur kernel if the original image is sharp enough. Bycombining this powerful regularizer with conventional image deblurringtechniques, we show how we could significantly improve the deblurring resultsthrough simulations and experiments on real images. In addition, our analysisand experiments help explaining a widely accepted doctrine; that is, the edgesare good features for deblurring.
arxiv-6000-232 | Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images | http://arxiv.org/pdf/1307.2965v2.pdf | author:Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer, Shaohua Kevin Zhou category:cs.CV published:2013-07-11 summary:The automatic segmentation of human knee cartilage from 3D MR images is auseful yet challenging task due to the thin sheet structure of the cartilagewith diffuse boundaries and inhomogeneous intensities. In this paper, wepresent an iterative multi-class learning method to segment the femoral, tibialand patellar cartilage simultaneously, which effectively exploits the spatialcontextual constraints between bone and cartilage, and also between differentcartilages. First, based on the fact that the cartilage grows in only certainarea of the corresponding bone surface, we extract the distance features of notonly to the surface of the bone, but more informatively, to the denselyregistered anatomical landmarks on the bone surface. Second, we introduce a setof iterative discriminative classifiers that at each iteration, probabilitycomparison features are constructed from the class confidence maps derived bypreviously learned classifiers. These features automatically embed the semanticcontext information between different cartilages of interest. Validated on atotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, theproposed approach demonstrates high robustness and accuracy of segmentation incomparison with existing state-of-the-art MR cartilage segmentation methods.
arxiv-6000-233 | Together we stand, Together we fall, Together we win: Dynamic Team Formation in Massive Open Online Courses | http://arxiv.org/pdf/1404.5521v1.pdf | author:Tanmay Sinha category:cs.SI cs.CY cs.LG cs.MA published:2014-04-22 summary:Massive Open Online Courses (MOOCs) offer a new scalable paradigm fore-learning by providing students with global exposure and opportunities forconnecting and interacting with millions of people all around the world. Veryoften, students work as teams to effectively accomplish course related tasks.However, due to lack of face to face interaction, it becomes difficult for MOOCstudents to collaborate. Additionally, the instructor also faces challenges inmanually organizing students into teams because students flock to these MOOCsin huge numbers. Thus, the proposed research is aimed at developing a robustmethodology for dynamic team formation in MOOCs, the theoretical framework forwhich is grounded at the confluence of organizational team theory, socialnetwork analysis and machine learning. A prerequisite for such an undertakingis that we understand the fact that, each and every informal tie establishedamong students offers the opportunities to influence and be influenced.Therefore, we aim to extract value from the inherent connectedness of studentsin the MOOC. These connections carry with them radical implications for the waystudents understand each other in the networked learning community. Ourapproach will enable course instructors to automatically group students inteams that have fairly balanced social connections with their peers, welldefined in terms of appropriately selected qualitative and quantitative networkmetrics.
arxiv-6000-234 | Approximate Inference for Nonstationary Heteroscedastic Gaussian process Regression | http://arxiv.org/pdf/1404.5443v1.pdf | author:Ville Tolvanen, Pasi Jylänki, Aki Vehtari category:stat.ML published:2014-04-22 summary:This paper presents a novel approach for approximate integration over theuncertainty of noise and signal variances in Gaussian process (GP) regression.Our efficient and straightforward approach can also be applied to integrationover input dependent noise variance (heteroscedasticity) and input dependentsignal variance (nonstationarity) by setting independent GP priors for thenoise and signal variances. We use expectation propagation (EP) for inferenceand compare results to Markov chain Monte Carlo in two simulated data sets andthree empirical examples. The results show that EP produces comparable resultswith less computational burden.
arxiv-6000-235 | Mixed Strategy May Outperform Pure Strategy: An Initial Study | http://arxiv.org/pdf/1303.3154v3.pdf | author:Jun He, Wei Hou, Hongbin Dong, Feidun He category:cs.NE cs.GT published:2013-03-13 summary:In pure strategy meta-heuristics, only one search strategy is applied for alltime. In mixed strategy meta-heuristics, each time one search strategy ischosen from a strategy pool with a probability and then is applied. An exampleis classical genetic algorithms, where either a mutation or crossover operatoris chosen with a probability each time. The aim of this paper is to compare theperformance between mixed strategy and pure strategy meta-heuristic algorithms.First an experimental study is implemented and results demonstrate that mixedstrategy evolutionary algorithms may outperform pure strategy evolutionaryalgorithms on the 0-1 knapsack problem in up to 77.8% instances. ThenComplementary Strategy Theorem is rigorously proven for applying mixed strategyat the population level. The theorem asserts that given two meta-heuristicalgorithms where one uses pure strategy 1 and another uses pure strategy 2, thecondition of pure strategy 2 being complementary to pure strategy 1 issufficient and necessary if there exists a mixed strategy meta-heuristicsderived from these two pure strategies and its expected number of generationsto find an optimal solution is no more than that of using pure strategy 1 forany initial population, and less than that of using pure strategy 1 for someinitial population.
arxiv-6000-236 | A Polynomial Time Approximation Scheme for a Single Machine Scheduling Problem Using a Hybrid Evolutionary Algorithm | http://arxiv.org/pdf/1202.1708v2.pdf | author:Boris Mitavskiy, Jun He category:cs.NE published:2012-02-08 summary:Nowadays hybrid evolutionary algorithms, i.e, heuristic search algorithmscombining several mutation operators some of which are meant to implementstochastically a well known technique designed for the specific problem inquestion while some others playing the role of random search, have becomerather popular for tackling various NP-hard optimization problems. Whileempirical studies demonstrate that hybrid evolutionary algorithms arefrequently successful at finding solutions having fitness sufficiently close tothe optimal, many fewer articles address the computational complexity in amathematically rigorous fashion. This paper is devoted to a mathematicallymotivated design and analysis of a parameterized family of evolutionaryalgorithms which provides a polynomial time approximation scheme for one of thewell-known NP-hard combinatorial optimization problems, namely the "singlemachine scheduling problem without precedence constraints". The authors hopethat the techniques and ideas developed in this article may be applied in manyother situations.
arxiv-6000-237 | Concurrent bandits and cognitive radio networks | http://arxiv.org/pdf/1404.5421v1.pdf | author:Orly Avner, Shie Mannor category:cs.LG cs.MA published:2014-04-22 summary:We consider the problem of multiple users targeting the arms of a singlemulti-armed stochastic bandit. The motivation for this problem comes fromcognitive radio networks, where selfish users need to coexist without any sidecommunication between them, implicit cooperation or common control. Even thenumber of users may be unknown and can vary as users join or leave the network.We propose an algorithm that combines an $\epsilon$-greedy learning rule with acollision avoidance mechanism. We analyze its regret with respect to thesystem-wide optimum and show that sub-linear regret can be obtained in thissetting. Experiments show dramatic improvement compared to other algorithms forthis setting.
arxiv-6000-238 | Attractor Metadynamics in Adapting Neural Networks | http://arxiv.org/pdf/1404.5417v1.pdf | author:Claudius Gros, Mathias Linkerhand, Valentin Walther category:q-bio.NC cs.NE published:2014-04-22 summary:Slow adaption processes, like synaptic and intrinsic plasticity, abound inthe brain and shape the landscape for the neural dynamics occurring onsubstantially faster timescales. At any given time the network is characterizedby a set of internal parameters, which are adapting continuously, albeitslowly. This set of parameters defines the number and the location of therespective adiabatic attractors. The slow evolution of network parameters henceinduces an evolving attractor landscape, a process which we term attractormetadynamics. We study the nature of the metadynamics of the attractorlandscape for several continuous-time autonomous model networks. We find bothfirst- and second-order changes in the location of adiabatic attractors andargue that the study of the continuously evolving attractor landscapeconstitutes a powerful tool for understanding the overall development of theneural dynamics.
arxiv-6000-239 | GP-Localize: Persistent Mobile Robot Localization using Online Sparse Gaussian Process Observation Model | http://arxiv.org/pdf/1404.5165v2.pdf | author:Nuo Xu, Kian Hsiang Low, Jie Chen, Keng Kiat Lim, Etkin Baris Ozgul category:cs.RO cs.LG stat.ML published:2014-04-21 summary:Central to robot exploration and mapping is the task of persistentlocalization in environmental fields characterized by spatially correlatedmeasurements. This paper presents a Gaussian process localization (GP-Localize)algorithm that, in contrast to existing works, can exploit the spatiallycorrelated field measurements taken during a robot's exploration (instead ofrelying on prior training data) for efficiently and scalably learning the GPobservation model online through our proposed novel online sparse GP. As aresult, GP-Localize is capable of achieving constant time and memory (i.e.,independent of the size of the data) per filtering step, which demonstrates thepractical feasibility of using GPs for persistent robot localization andautonomy. Empirical evaluation via simulated experiments with real-worlddatasets and a real robot experiment shows that GP-Localize outperformsexisting GP localization algorithms.
arxiv-6000-240 | A Comparison of Clustering and Missing Data Methods for Health Sciences | http://arxiv.org/pdf/1404.5899v1.pdf | author:Ran Zhao, Deanna Needell, Christopher Johansen, Jerry L. Grenard category:math.NA cs.LG published:2014-04-22 summary:In this paper, we compare and analyze clustering methods with missing data inhealth behavior research. In particular, we propose and analyze the use ofcompressive sensing's matrix completion along with spectral clustering tocluster health related data. The empirical tests and real data results showthat these methods can outperform standard methods like LPA and FIML, in termsof lower misclassification rates in clustering and better matrix completionperformance in missing data problems. According to our examination, a possibleexplanation of these improvements is that spectral clustering takes advantageof high data dimension and compressive sensing methods utilize thenear-to-low-rank property of health data.
arxiv-6000-241 | Linking Geographic Vocabularies through WordNet | http://arxiv.org/pdf/1404.5372v1.pdf | author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.IR cs.CL published:2014-04-22 summary:The linked open data (LOD) paradigm has emerged as a promising approach tostructuring and sharing geospatial information. One of the major obstacles tothis vision lies in the difficulties found in the automatic integration betweenheterogeneous vocabularies and ontologies that provides the semantic backboneof the growing constellation of open geo-knowledge bases. In this article, weshow how to utilize WordNet as a semantic hub to increase the integration ofLOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mappingtechnique between a given vocabulary and WordNet, combining intensional andextensional aspects of the geographic terms. Voc2WordNet is evaluated against asample of human-generated alignments with the OpenStreetMap (OSM) SemanticNetwork, a crowdsourced geospatial resource, and the GeoNames ontology, thevocabulary of a large digital gazetteer. These empirical results indicate thatthe approach can obtain high precision and recall.
arxiv-6000-242 | Real-time Decolorization using Dominant Colors | http://arxiv.org/pdf/1404.2728v2.pdf | author:Wei Hu, Wei Li, Fan Zhang, Qian Du category:cs.GR cs.CV published:2014-04-10 summary:Decolorization is the process to convert a color image or video to itsgrayscale version, and it has received great attention in recent years. Anideal decolorization algorithm should preserve the original color contrast asmuch as possible. Meanwhile, it should provide the final decolorized result asfast as possible. However, most of the current methods are suffering fromeither unsatisfied color information preservation or high computational cost,limiting their application value. In this paper, a simple but effectivetechnique is proposed for real-time decolorization. Based on the typicalrgb2gray() color conversion model, which produces a grayscale image by linearlycombining R, G, and B channels, we propose a dominant color hypothesis and acorresponding distance measurement metric to evaluate the quality of grayscaleconversion. The local optimum scheme provides several "good" candidates in aconfidence interval, from which the "best" result can be extracted.Experimental results demonstrate that remarkable simplicity of the proposedmethod facilitates the process of high resolution images and videos inreal-time using a common CPU.
arxiv-6000-243 | Lexicon Infused Phrase Embeddings for Named Entity Resolution | http://arxiv.org/pdf/1404.5367v1.pdf | author:Alexandre Passos, Vineet Kumar, Andrew McCallum category:cs.CL published:2014-04-22 summary:Most state-of-the-art approaches for named-entity recognition (NER) use semisupervised information in the form of word clusters and lexicons. Recentlyneural network-based language models have been explored, as they as a byproductgenerate highly informative vector representations for words, known as wordembeddings. In this paper we present two contributions: a new form of learningword embeddings that can leverage information from relevant lexicons to improvethe representations, and the first system to use neural word embeddings toachieve state-of-the-art results on named-entity recognition in both CoNLL andOntonotes NER. Our system achieves an F1 score of 90.90 on the test set forCoNLL 2003---significantly better than any previous system trained on publicdata, and matching a system employing massive private industrial query-logdata.
arxiv-6000-244 | Morphological Analysis of the Bishnupriya Manipuri Language using Finite State Transducers | http://arxiv.org/pdf/1404.5357v1.pdf | author:Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha category:cs.CL published:2014-04-22 summary:In this work we present a morphological analysis of Bishnupriya Manipurilanguage, an Indo-Aryan language spoken in the north eastern India. As of now,there is no computational work available for the language. Finite statemorphology is one of the successful approaches applied in a wide variety oflanguages over the year. Therefore we adapted the finite state approach toanalyse morphology of the Bishnupriya Manipuri language.
arxiv-6000-245 | Fast Approximate Matching of Cell-Phone Videos for Robust Background Subtraction | http://arxiv.org/pdf/1404.5351v1.pdf | author:Raffay Hamid, Atish Das Sarma, Dennis DeCoste, Neel Sundaresan category:cs.CV published:2014-04-22 summary:We identify a novel instance of the background subtraction problem thatfocuses on extracting near-field foreground objects captured using handheldcameras. Given two user-generated videos of a scene, one with and the otherwithout the foreground object(s), our goal is to efficiently generate an outputvideo with only the foreground object(s) present in it. We cast this challengeas a spatio-temporal frame matching problem, and propose an efficient solutionfor it that exploits the temporal smoothness of the video sequences. We presenttheoretical analyses for the error bounds of our approach, and validate ourfindings using a detailed set of simulation experiments. Finally, we presentthe results of our approach tested on multiple real videos captured usinghandheld cameras, and compare them to several alternate foreground extractionapproaches.
arxiv-6000-246 | The Frobenius anatomy of word meanings I: subject and object relative pronouns | http://arxiv.org/pdf/1404.5278v1.pdf | author:Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke category:cs.CL published:2014-04-21 summary:This paper develops a compositional vector-based semantics of subject andobject relative pronouns within a categorical framework. Frobenius algebras areused to formalise the operations required to model the semantics of relativepronouns, including passing information between the relative clause and themodified noun phrase, as well as copying, combining, and discarding parts ofthe relative clause. We develop two instantiations of the abstract semantics,one based on a truth-theoretic approach and one based on corpus statistics.
arxiv-6000-247 | Graph Kernels via Functional Embedding | http://arxiv.org/pdf/1404.5214v1.pdf | author:Anshumali Shrivastava, Ping Li category:cs.LG cs.AI stat.ML published:2014-04-21 summary:We propose a representation of graph as a functional object derived from thepower iteration of the underlying adjacency matrix. The proposed functionalrepresentation is a graph invariant, i.e., the functional remains unchangedunder any reordering of the vertices. This property eliminates the difficultyof handling exponentially many isomorphic forms. Bhattacharyya kernelconstructed between these functionals significantly outperforms thestate-of-the-art graph kernels on 3 out of the 4 standard benchmark graphclassification datasets, demonstrating the superiority of our approach. Theproposed methodology is simple and runs in time linear in the number of edges,which makes our kernel more efficient and scalable compared to many widelyadopted graph kernels with running time cubic in the number of vertices.
arxiv-6000-248 | Time-dependent Hierarchical Dirichlet Model for Timeline Generation | http://arxiv.org/pdf/1312.2244v2.pdf | author:Tao Wang category:cs.CL cs.IR published:2013-12-08 summary:Timeline Generation aims at summarizing news from different epochs andtelling readers how an event evolves. It is a new challenge that combinessalience ranking with novelty detection. For long-term public events, the maintopic usually includes various aspects across different epochs and each aspecthas its own evolving pattern. Existing approaches neglect such hierarchicaltopic structure involved in the news corpus in timeline generation. In thispaper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) fortimeline generation. Our model can aptly detect different levels of topicinformation across corpus and such structure is further used for sentenceselection. Based on the topic mined fro HDM, sentences are selected byconsidering different aspects such as relevance, coherence and coverage. Wedevelop experimental systems to evaluate 8 long-term events that publicconcern. Performance comparison between different systems demonstrates theeffectiveness of our model in terms of ROUGE metrics.
arxiv-6000-249 | Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations | http://arxiv.org/pdf/1312.4637v2.pdf | author:Zhen Zhang, Qinfeng Shi, Yanning Zhang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.AI published:2013-12-17 summary:LP relaxation-based message passing algorithms provide an effective tool forMAP inference over Probabilistic Graphical Models. However, different LPrelaxations often have different objective functions and variables of differingdimensions, which presents a barrier to effective comparison and analysis. Inaddition, the computational complexity of LP relaxation-based methods growsquickly with the number of constraints. Reducing the number of constraintswithout sacrificing the quality of the solutions is thus desirable. We propose a unified formulation under which existing MAP LP relaxations maybe compared and analysed. Furthermore, we propose a new tool called MarginalPolytope Diagrams. Some properties of Marginal Polytope Diagrams are exploitedsuch as node redundancy and edge equivalence. We show that using MarginalPolytope Diagrams allows the number of constraints to be reduced withoutloosening the LP relaxations. Then, using Marginal Polytope Diagrams andconstraint reduction, we develop three novel message passing algorithms, anddemonstrate that two of these show a significant improvement in speed overstate-of-art algorithms while delivering a competitive, and sometimes higher,quality of solution.
arxiv-6000-250 | Influence of the learning method in the performance of feedforward neural networks when the activity of neurons is modified | http://arxiv.org/pdf/1404.5144v1.pdf | author:M. Konomi, G. M. Sacha category:cs.NE published:2014-04-21 summary:A method that allows us to give a different treatment to any neuron insidefeedforward neural networks is presented. The algorithm has been implementedwith two very different learning methods: a standard Back-propagation (BP)procedure and an evolutionary algorithm. First, we have demonstrated that theEA training method converges faster and gives more accurate results than BP.Then we have made a full analysis of the effects of turning off differentcombinations of neurons after the training phase. We demonstrate that EA ismuch more robust than BP for all the cases under study. Even in the case whentwo hidden neurons are lost, EA training is still able to give good averageresults. This difference implies that we must be very careful when pruning orredundancy effects are being studied since the network performance when losingneurons strongly depends on the training method. Moreover, the influence of theindividual inputs will also depend on the training algorithm. Since EA keeps agood classification performance when units are lost, this method could be agood way to simulate biological learning systems since they must be robustagainst deficient neuron performance. Although biological systems are much morecomplex than the simulations shown in this article, we propose that a smarttraining strategy such as the one shown here could be considered as a firstprotection against the losing of a certain number of neurons.
arxiv-6000-251 | A Computationally Efficient Limited Memory CMA-ES for Large Scale Optimization | http://arxiv.org/pdf/1404.5520v1.pdf | author:Ilya Loshchilov category:cs.NE published:2014-04-21 summary:We propose a computationally efficient limited memory Covariance MatrixAdaptation Evolution Strategy for large scale optimization, which we call theLM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm fornumerical optimization of non-linear, non-convex optimization problems incontinuous domain. Inspired by the limited memory BFGS method of Liu andNocedal (1989), the LM-CMA-ES samples candidate solutions according to acovariance matrix reproduced from $m$ direction vectors selected during theoptimization process. The decomposition of the covariance matrix into Choleskyfactors allows to reduce the time and memory complexity of the sampling to$O(mn)$, where $n$ is the number of decision variables. When $n$ is large(e.g., $n$ > 1000), even relatively small values of $m$ (e.g., $m=20,30$) aresufficient to efficiently solve fully non-separable problems and to reduce theoverall run-time.
arxiv-6000-252 | Compressed Sensing for Energy-Efficient Wireless Telemonitoring: Challenges and Opportunities | http://arxiv.org/pdf/1311.3995v2.pdf | author:Zhilin Zhang, Bhaskar D. Rao, Tzyy-Ping Jung category:cs.IT math.IT stat.ML published:2013-11-15 summary:As a lossy compression framework, compressed sensing has drawn much attentionin wireless telemonitoring of biosignals due to its ability to reduce energyconsumption and make possible the design of low-power devices. However, thenon-sparseness of biosignals presents a major challenge to compressed sensing.This study proposes and evaluates a spatio-temporal sparse Bayesian learningalgorithm, which has the desired ability to recover such non-sparse biosignals.It exploits both temporal correlation in each individual biosignal andinter-channel correlation among biosignals from different channels. Theproposed algorithm was used for compressed sensing of multichannelelectroencephalographic (EEG) signals for estimating vehicle drivers'drowsiness. Results showed that the drowsiness estimation was almost unaffectedeven if raw EEG signals (containing various artifacts) were compressed by 90%.
arxiv-6000-253 | Near-Ideal Behavior of Compressed Sensing Algorithms | http://arxiv.org/pdf/1401.6623v3.pdf | author:Mehmet Eren Ahsen, Mathukumalli Vidyasagar category:stat.ML 62J07 published:2014-01-26 summary:In a recent paper, it is shown that the LASSO algorithm exhibits "near-idealbehavior," in the following sense: Suppose $y = Az + \eta$ where $A$ satisfiesthe restricted isometry property (RIP) with a sufficiently small constant, and$\Vert \eta \Vert_2 \leq \epsilon$. Then minimizing $\Vert z \Vert_1$ subjectto $\Vert y - Az \Vert_2 \leq \epsilon$ leads to an estimate $\hat{x}$ whoseerror $\Vert \hat{x} - x \Vert_2$ is bounded by a universal constant times theerror achieved by an "oracle" that knows the location of the nonzero componentsof $x$. In the world of optimization, the LASSO algorithm has been generalizedin several directions such as the group LASSO, the sparse group LASSO, eitherwithout or with tree-structured overlapping groups, and most recently, thesorted LASSO. In this paper, it is shown that {\it any algorithm\/} exhibitsnear-ideal behavior in the above sense, provided only that (i) the norm used todefine the sparsity index is "decomposable," (ii) the penalty norm that isminimized in an effort to enforce sparsity is "$\gamma$-decomposable," and(iii) a "compressibility condition" in terms of a group restricted isometryproperty is satisfied. Specifically, the group LASSO, and the sparse groupLASSO (with some permissible overlap in the groups), as well as the sorted$\ell_1$-norm minimization all exhibit near-ideal behavior. Explicit bounds onthe residual error are derived that contain previously known results as specialcases.
arxiv-6000-254 | Multi-Target Regression via Random Linear Target Combinations | http://arxiv.org/pdf/1404.5065v1.pdf | author:Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Aikaterini Vrekou, Ioannis Vlahavas category:cs.LG published:2014-04-20 summary:Multi-target regression is concerned with the simultaneous prediction ofmultiple continuous target variables based on the same set of input variables.It arises in several interesting industrial and environmental applicationdomains, such as ecological modelling and energy forecasting. This paperpresents an ensemble method for multi-target regression that constructs newtarget variables via random linear combinations of existing targets. We discussthe connection of our approach with multi-label classification algorithms, inparticular RA$k$EL, which originally inspired this work, and a family of recentmulti-label classification algorithms that involve output coding. Experimentalresults on 12 multi-target datasets show that it performs significantly betterthan a strong baseline that learns a single model for each target usinggradient boosting and compares favourably to multi-objective random forestapproach, which is a state-of-the-art approach. The experiments further showthat our approach improves more when stronger unconditional dependencies existamong the targets.
arxiv-6000-255 | Inter-rater Agreement on Sentence Formality | http://arxiv.org/pdf/1109.0069v2.pdf | author:Shibamouli Lahiri, Xiaofei Lu category:cs.CL H.3.1; I.2.7 published:2011-09-01 summary:Formality is one of the most important dimensions of writing style variation.In this study we conducted an inter-rater reliability experiment for assessingsentence formality on a five-point Likert scale, and obtained good agreementresults as well as different rating distributions for different sentencecategories. We also performed a difficulty analysis to identify the bottlenecksof our rating procedure. Our main objective is to design an automatic scoringmechanism for sentence-level formality, and this study is important for thatpurpose.
arxiv-6000-256 | Robust Subspace Recovery via Bi-Sparsity Pursuit | http://arxiv.org/pdf/1403.8067v2.pdf | author:Xiao Bian, Hamid Krim category:cs.CV published:2014-03-31 summary:Successful applications of sparse models in computer vision and machinelearning imply that in many real-world applications, high dimensional data isdistributed in a union of low dimensional subspaces. Nevertheless, theunderlying structure may be affected by sparse errors and/or outliers. In thispaper, we propose a bi-sparse model as a framework to analyze this problem andprovide a novel algorithm to recover the union of subspaces in presence ofsparse corruptions. We further show the effectiveness of our method byexperiments on both synthetic data and real-world vision data.
arxiv-6000-257 | Clustering via Mode Seeking by Direct Estimation of the Gradient of a Log-Density | http://arxiv.org/pdf/1404.5028v1.pdf | author:Hiroaki Sasaki, Aapo Hyvärinen, Masashi Sugiyama category:stat.ML published:2014-04-20 summary:Mean shift clustering finds the modes of the data probability density byidentifying the zero points of the density gradient. Since it does not requireto fix the number of clusters in advance, the mean shift has been a popularclustering algorithm in various application fields. A typical implementation ofthe mean shift is to first estimate the density by kernel density estimationand then compute its gradient. However, since good density estimation does notnecessarily imply accurate estimation of the density gradient, such an indirecttwo-step approach is not reliable. In this paper, we propose a method todirectly estimate the gradient of the log-density without going through densityestimation. The proposed method gives the global solution analytically and thusis computationally efficient. We then develop a mean-shift-like fixed-pointalgorithm to find the modes of the density for clustering. As in the meanshift, one does not need to set the number of clusters in advance. Weempirically show that the proposed clustering method works much better than themean shift especially for high-dimensional data. Experimental results furtherindicate that the proposed method outperforms existing clustering methods.
arxiv-6000-258 | Complete Complementary Results Report of the MARF's NLP Approach to the DEFT 2010 Competition | http://arxiv.org/pdf/1006.3787v7.pdf | author:Serguei A. Mokhov category:cs.CL I.2.7; I.5 published:2010-06-18 summary:This companion paper complements the main DEFT'10 article describing the MARFapproach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described athttp://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimedto present the complete result sets of all the conducted experiments and theirsettings in the resulting tables highlighting the approach and the bestresults, but also showing the worse and the worst and their subsequentanalysis. This particular work focuses on application of the MARF's classicaland NLP pipelines to identification tasks within various francophone corpora toidentify decades when certain articles were published for the first track(Piste 1) and place of origin of a publication (Piste 2), such as the journaland location (France vs. Quebec). This is the sixth iteration of the release ofthe results.
arxiv-6000-259 | Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps | http://arxiv.org/pdf/1312.6034v2.pdf | author:Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2013-12-20 summary:This paper addresses the visualisation of image classification models, learntusing deep Convolutional Networks (ConvNets). We consider two visualisationtechniques, based on computing the gradient of the class score with respect tothe input image. The first one generates an image, which maximises the classscore [Erhan et al., 2009], thus visualising the notion of the class, capturedby a ConvNet. The second technique computes a class saliency map, specific to agiven image and class. We show that such maps can be employed for weaklysupervised object segmentation using classification ConvNets. Finally, weestablish the connection between the gradient-based ConvNet visualisationmethods and deconvolutional networks [Zeiler et al., 2013].
arxiv-6000-260 | Geometric Abstraction from Noisy Image-Based 3D Reconstructions | http://arxiv.org/pdf/1404.4942v1.pdf | author:Thomas Holzmann, Christof Hoppe, Stefan Kluckner, Horst Bischof category:cs.CV published:2014-04-19 summary:Creating geometric abstracted models from image-based scene reconstructionsis difficult due to noise and irregularities in the reconstructed model. Inthis paper, we present a geometric modeling method for noisy reconstructionsdominated by planar horizontal and orthogonal vertical structures. We partitionthe scene into horizontal slices and create an inside/outside labelingrepresented by a floor plan for each slice by solving an energy minimizationproblem. Consecutively, we create an irregular discretization of the volumeaccording to the individual floor plans and again label each cell asinside/outside by minimizing an energy function. By adjusting the smoothnessparameter, we introduce different levels of detail. In our experiments, we showresults with varying regularization levels using synthetically generated andreal-world data.
arxiv-6000-261 | Opinion Mining In Hindi Language: A Survey | http://arxiv.org/pdf/1404.4935v1.pdf | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.IR cs.CL published:2014-04-19 summary:Opinions are very important in the life of human beings. These Opinionshelped the humans to carry out the decisions. As the impact of the Web isincreasing day by day, Web documents can be seen as a new source of opinion forhuman beings. Web contains a huge amount of information generated by the usersthrough blogs, forum entries, and social networking websites and so on Toanalyze this large amount of information it is required to develop a methodthat automatically classifies the information available on the Web. This domainis called Sentiment Analysis and Opinion Mining. Opinion Mining or SentimentAnalysis is a natural language processing task that mine information fromvarious text forms such as reviews, news, and blogs and classify them on thebasis of their polarity as positive, negative or neutral. But, from the lastfew years, enormous increase has been seen in Hindi language on the Web.Research in opinion mining mostly carried out in English language but it isvery important to perform the opinion mining in Hindi language also as largeamount of information in Hindi is also available on the Web. This paper givesan overview of the work that has been done Hindi language.
arxiv-6000-262 | CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit | http://arxiv.org/pdf/1404.4893v1.pdf | author:Daniele Codecasa, Fabio Stella category:cs.AI cs.LG cs.MS published:2014-04-18 summary:Continuous time Bayesian network classifiers are designed for temporalclassification of multivariate streaming data when time duration of eventsmatters and the class does not change over time. This paper introduces theCTBNCToolkit: an open source Java toolkit which provides a stand-aloneapplication for temporal classification and a library for continuous timeBayesian network classifiers. CTBNCToolkit implements the inference algorithm,the parameter learning algorithm, and the structural learning algorithm forcontinuous time Bayesian network classifiers. The structural learning algorithmis based on scoring functions: the marginal log-likelihood score and theconditional log-likelihood score are provided. CTBNCToolkit provides also animplementation of the expectation maximization algorithm for clusteringpurpose. The paper introduces continuous time Bayesian network classifiers. Howto use the CTBNToolkit from the command line is described in a specificsection. Tutorial examples are included to facilitate users to understand howthe toolkit must be used. A section dedicate to the Java library is proposed tohelp further code extensions.
arxiv-6000-263 | Rapid and deterministic estimation of probability densities using scale-free field theories | http://arxiv.org/pdf/1312.6661v3.pdf | author:Justin B. Kinney category:cs.LG math.ST q-bio.QM stat.ML stat.TH published:2013-12-23 summary:The question of how best to estimate a continuous probability density fromfinite data is an intriguing open problem at the interface of statistics andphysics. Previous work has argued that this problem can be addressed in anatural way using methods from statistical field theory. Here I describe newresults that allow this field-theoretic approach to be rapidly anddeterministically computed in low dimensions, making it practical for use inday-to-day data analysis. Importantly, this approach does not impose aprivileged length scale for smoothness of the inferred probability density, butrather learns a natural length scale from the data due to the tradeoff betweengoodness-of-fit and an Occam factor. Open source software implementing thismethod in one and two dimensions is provided.
arxiv-6000-264 | Coactive Learning for Locally Optimal Problem Solving | http://arxiv.org/pdf/1404.5511v1.pdf | author:Robby Goetschalckx, Alan Fern, Prasad Tadepalli category:cs.LG published:2014-04-18 summary:Coactive learning is an online problem solving setting where the solutionsprovided by a solver are interactively improved by a domain expert, which inturn drives learning. In this paper we extend the study of coactive learning toproblems where obtaining a globally optimal or near-optimal solution may beintractable or where an expert can only be expected to make small, localimprovements to a candidate solution. The goal of learning in this new settingis to minimize the cost as measured by the expert effort over time. We firstestablish theoretical bounds on the average cost of the existing coactivePerceptron algorithm. In addition, we consider new online algorithms that usecost-sensitive and Passive-Aggressive (PA) updates, showing similar or improvedtheoretical bounds. We provide an empirical evaluation of the learners invarious domains, which show that the Perceptron based algorithms are quiteeffective and that unlike the case for online classification, the PA algorithmsdo not yield significant performance gains.
arxiv-6000-265 | Bias Correction and Modified Profile Likelihood under the Wishart Complex Distribution | http://arxiv.org/pdf/1404.4880v1.pdf | author:Abraão D. C. Nascimento, Alejandro C. Frery, Renato J. Cintra category:cs.CV stat.ME published:2014-04-18 summary:This paper proposes improved methods for the maximum likelihood (ML)estimation of the equivalent number of looks $L$. This parameter has ameaningful interpretation in the context of polarimetric synthetic apertureradar (PolSAR) images. Due to the presence of coherent illumination in theirprocessing, PolSAR systems generate images which present a granular noisecalled speckle. As a potential solution for reducing such interference, theparameter $L$ controls the signal-noise ratio. Thus, the proposal of efficientestimation methodologies for $L$ has been sought. To that end, we considerfirstly that a PolSAR image is well described by the scaled complex Wishartdistribution. In recent years, Anfinsen et al. derived and analyzed estimationmethods based on the ML and on trace statistical moments for obtaining theparameter $L$ of the unscaled version of such probability law. This papergeneralizes that approach. We present the second-order bias expression proposedby Cox and Snell for the ML estimator of this parameter. Moreover, the formulaof the profile likelihood modified by Barndorff-Nielsen in terms of $L$ isdiscussed. Such derivations yield two new ML estimators for the parameter $L$,which are compared to the estimators proposed by Anfinsen et al. Theperformance of these estimators is assessed by means of Monte Carloexperiments, adopting three statistical measures as comparison criterion: themean square error, the bias, and the coefficient of variation. Equivalently tothe simulation study, an application to actual PolSAR data concludes that theproposed estimators outperform all the others in homogeneous scenarios.
arxiv-6000-266 | iPiano: Inertial Proximal Algorithm for Non-Convex Optimization | http://arxiv.org/pdf/1404.4805v1.pdf | author:Peter Ochs, Yunjin Chen, Thomas Brox, Thomas Pock category:cs.CV math.OC published:2014-04-18 summary:In this paper we study an algorithm for solving a minimization problemcomposed of a differentiable (possibly non-convex) and a convex (possiblynon-differentiable) function. The algorithm iPiano combines forward-backwardsplitting with an inertial force. It can be seen as a non-smooth split versionof the Heavy-ball method from Polyak. A rigorous analysis of the algorithm forthe proposed class of problems yields global convergence of the function valuesand the arguments. This makes the algorithm robust for usage on non-convexproblems. The convergence result is obtained based on the \KL inequality. Thisis a very weak restriction, which was used to prove convergence for severalother gradient methods. First, an abstract convergence theorem for a genericalgorithm is proved, and, then iPiano is shown to satisfy the requirements ofthis theorem. Furthermore, a convergence rate is established for the generalproblem class. We demonstrate iPiano on computer vision problems: imagedenoising with learned priors and diffusion based image compression.
arxiv-6000-267 | OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage | http://arxiv.org/pdf/1306.6042v4.pdf | author:Raj Rao Nadakuditi category:math.ST cs.IT math.IT stat.ML stat.TH published:2013-06-25 summary:The truncated singular value decomposition (SVD) of the measurement matrix isthe optimal solution to the_representation_ problem of how to best approximatea noisy measurement matrix using a low-rank matrix. Here, we consider the(unobservable)_denoising_ problem of how to best approximate a low-rank signalmatrix buried in noise by optimal (re)weighting of the singular vectors of themeasurement matrix. We exploit recent results from random matrix theory toexactly characterize the large matrix limit of the optimal weightingcoefficients and show that they can be computed directly from data for a largeclass of noise models that includes the i.i.d. Gaussian noise case. Our analysis brings into sharp focus the shrinkage-and-thresholding form ofthe optimal weights, the non-convex nature of the associated shrinkage function(on the singular values) and explains why matrix regularization via singularvalue thresholding with convex penalty functions (such as the nuclear norm)will always be suboptimal. We validate our theoretical predictions withnumerical simulations, develop an implementable algorithm (OptShrink) thatrealizes the predicted performance gains and show how our methods can be usedto improve estimation in the setting where the measured matrix has missingentries.
arxiv-6000-268 | Robust Face Recognition via Adaptive Sparse Representation | http://arxiv.org/pdf/1404.4780v1.pdf | author:Jing Wang, Canyi Lu, Meng Wang, Peipei Li, Shuicheng Yan, Xuegang Hu category:cs.CV published:2014-04-18 summary:Sparse Representation (or coding) based Classification (SRC) has gained greatsuccess in face recognition in recent years. However, SRC emphasizes thesparsity too much and overlooks the correlation information which has beendemonstrated to be critical in real-world face recognition problems. Besides,some work considers the correlation but overlooks the discriminative ability ofsparsity. Different from these existing techniques, in this paper, we propose aframework called Adaptive Sparse Representation based Classification (ASRC) inwhich sparsity and correlation are jointly considered. Specifically, when thesamples are of low correlation, ASRC selects the most discriminative samplesfor representation, like SRC; when the training samples are highly correlated,ASRC selects most of the correlated and discriminative samples forrepresentation, rather than choosing some related samples randomly. In general,the representation model is adaptive to the correlation structure, whichbenefits from both $\ell_1$-norm and $\ell_2$-norm. Extensive experiments conducted on publicly available data sets verify theeffectiveness and robustness of the proposed algorithm by comparing it withstate-of-the-art methods.
arxiv-6000-269 | Challenges in Persian Electronic Text Analysis | http://arxiv.org/pdf/1404.4740v1.pdf | author:Behrang QasemiZadeh, Saeed Rahimi, Mehdi Safaee Ghalati category:cs.CL 68T50 I.2.7 published:2014-04-18 summary:Farsi, also known as Persian, is the official language of Iran and Tajikistanand one of the two main languages spoken in Afghanistan. Farsi enjoys a unifiedArabic script as its writing system. In this paper we briefly introduce thewriting standards of Farsi and highlight problems one would face when analyzingFarsi electronic texts, especially during development of Farsi corporaregarding to transcription and encoding of Farsi e-texts. The pointes mentionedmay sounds easy but they are crucial when developing and processing writtencorpora of Farsi.
arxiv-6000-270 | Radical-Enhanced Chinese Character Embedding | http://arxiv.org/pdf/1404.4714v1.pdf | author:Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, Xiaolong Wang category:cs.CL published:2014-04-18 summary:We present a method to leverage radical for learning Chinese characterembedding. Radical is a semantic and phonetic component of Chinese character.It plays an important role as characters with the same radical usually havesimilar semantic meaning and grammatical usage. However, existing Chineseprocessing algorithms typically regard word or character as the basic unit butignore the crucial radical information. In this paper, we fill this gap byleveraging radical for learning continuous representation of Chinese character.We develop a dedicated neural architecture to effectively learn characterembedding and apply it on Chinese character similarity judgement and Chineseword segmentation. Experiment results show that our radical-enhanced methodoutperforms existing embedding learning algorithms on both tasks.
arxiv-6000-271 | Nearly Tight Bounds on $\ell_1$ Approximation of Self-Bounding Functions | http://arxiv.org/pdf/1404.4702v1.pdf | author:Vitaly Feldman, Pravesh Kothari, Jan Vondrák category:cs.LG cs.DS published:2014-04-18 summary:We study the complexity of learning and approximation of self-boundingfunctions over the uniform distribution on the Boolean hypercube ${0,1}^n$.Informally, a function $f:{0,1}^n \rightarrow \mathbb{R}$ is self-bounding iffor every $x \in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginaldecreases in the value of the function at $x$. Self-bounding functions includesuch well-known classes of functions as submodular and fractionally-subadditive(XOS) functions. They were introduced by Boucheron et al in the context ofconcentration of measure inequalities. Our main result is a nearly tight$\ell_1$-approximation of self-bounding functions by low-degree juntas.Specifically, all self-bounding functions can be $\epsilon$-approximated in$\ell_1$ by a polynomial of degree $\tilde{O}(1/\epsilon)$ over$2^{\tilde{O}(1/\epsilon)}$ variables. Both the degree and junta-size areoptimal up to logarithmic terms. Previously, the best known bound was$O(1/\epsilon^{2})$ on the degree and $2^{O(1/\epsilon^2)}$ on the number ofvariables (Feldman and Vondr \'{a}k 2013). These results lead to improved andin several cases almost tight bounds for PAC and agnostic learning ofsubmodular, XOS and self-bounding functions. In particular, assuming hardnessof learning juntas, we show that PAC and agnostic learning of self-boundingfunctions have complexity of $n^{\tilde{\Theta}(1/\epsilon)}$.
arxiv-6000-272 | Two Dimensional Array Imaging with Beam Steered Data | http://arxiv.org/pdf/1310.6719v2.pdf | author:Sujeet Patole, Murat Torlak category:cs.CV cs.IT math.IT stat.AP published:2013-10-24 summary:This paper discusses different approaches used for millimeter wave imaging oftwo-dimensional objects. Imaging of a two dimensional object requires reflectedwave data to be collected across two distinct dimensions. In this paper, wepropose a reconstruction method that uses narrowband waveforms along with twodimensional beam steering. The beam is steered in azimuthal and elevationdirection, which forms the two distinct dimensions required for thereconstruction. The Reconstruction technique uses inverse Fourier transformalong with amplitude and phase correction factors. In addition, thisreconstruction technique does not require interpolation of the data in eitherwavenumber or spatial domain. Use of the two dimensional beam steering offersbetter performance in the presence of noise compared with the existing methods,such as switched array imaging system. Effects of RF impairments such asquantization of the phase of beam steering weights and timing jitter which addto phase noise, are analyzed.
arxiv-6000-273 | Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors | http://arxiv.org/pdf/1404.4667v1.pdf | author:Morteza Mardani, Gonzalo Mateos, Georgios B. Giannakis category:stat.ML cs.IT cs.LG math.IT published:2014-04-17 summary:Extracting latent low-dimensional structure from high-dimensional data is ofparamount importance in timely inference tasks encountered with `Big Data'analytics. However, increasingly noisy, heterogeneous, and incomplete datasetsas well as the need for {\em real-time} processing of streaming data pose majorchallenges to this end. In this context, the present paper permeates benefitsfrom rank minimization to scalable imputation of missing data, via trackinglow-dimensional subspaces and unraveling latent (possibly multi-way) structurefrom \emph{incomplete streaming} data. For low-rank matrix data, a subspaceestimator is proposed based on an exponentially-weighted least-squarescriterion regularized with the nuclear norm. After recasting the non-separablenuclear norm into a form amenable to online optimization, real-time algorithmswith complementary strengths are developed and their convergence is establishedunder simplifying technical assumptions. In a stationary setting, theasymptotic estimates obtained offer the well-documented performance guaranteesof the {\em batch} nuclear-norm regularized estimator. Under the same unifyingframework, a novel online (adaptive) algorithm is developed to obtain multi-waydecompositions of \emph{low-rank tensors} with missing entries, and performimputation as a byproduct. Simulated tests with both synthetic as well as realInternet and cardiac magnetic resonance imagery (MRI) data confirm the efficacyof the proposed algorithms, and their superior performance relative tostate-of-the-art alternatives.
arxiv-6000-274 | Learning Fine-grained Image Similarity with Deep Ranking | http://arxiv.org/pdf/1404.4661v1.pdf | author:Jiang Wang, Yang song, Thomas Leung, Chuck Rosenberg, Jinbin Wang, James Philbin, Bo Chen, Ying Wu category:cs.CV published:2014-04-17 summary:Learning fine-grained image similarity is a challenging task. It needs tocapture between-class and within-class image differences. This paper proposes adeep ranking model that employs deep learning techniques to learn similaritymetric directly from images.It has higher learning capability than models basedon hand-crafted features. A novel multiscale network structure has beendeveloped to describe the images effectively. An efficient triplet samplingalgorithm is proposed to learn the model with distributed asynchronizedstochastic gradient. Extensive experiments show that the proposed algorithmoutperforms models based on hand-crafted visual features and deepclassification models.
arxiv-6000-275 | Hierarchical Quasi-Clustering Methods for Asymmetric Networks | http://arxiv.org/pdf/1404.4655v1.pdf | author:Gunnar Carlsson, Facundo Mémoli, Alejandro Ribeiro, Santiago Segarra category:cs.LG stat.ML published:2014-04-17 summary:This paper introduces hierarchical quasi-clustering methods, a generalizationof hierarchical clustering for asymmetric networks where the output structurepreserves the asymmetry of the input data. We show that this output structureis equivalent to a finite quasi-ultrametric space and study admissibility withrespect to two desirable properties. We prove that a modified version of singlelinkage is the only admissible quasi-clustering method. Moreover, we showstability of the proposed method and we establish invariance propertiesfulfilled by it. Algorithms are further developed and the value ofquasi-clustering analysis is illustrated with a study of internal migrationwithin United States.
arxiv-6000-276 | A New Space for Comparing Graphs | http://arxiv.org/pdf/1404.4644v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.ME cs.IR cs.LG stat.ML published:2014-04-17 summary:Finding a new mathematical representations for graph, which allows directcomparison between different graph structures, is an open-ended researchdirection. Having such a representation is the first prerequisite for a varietyof machine learning algorithms like classification, clustering, etc., overgraph datasets. In this paper, we propose a symmetric positive semidefinitematrix with the $(i,j)$-{th} entry equal to the covariance between normalizedvectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representationfor graph with adjacency matrix $A$. We show that the proposed matrixrepresentation encodes the spectrum of the underlying adjacency matrix and italso contains information about the counts of small sub-structures present inthe graph such as triangles and small paths. In addition, we show that thismatrix is a \emph{"graph invariant"}. All these properties make the proposedmatrix a suitable object for representing graphs. The representation, being a covariance matrix in a fixed dimensional metricspace, gives a mathematical embedding for graphs. This naturally leads to ameasure of similarity on graph objects. We define similarity between two givengraphs as a Bhattacharya similarity measure between their correspondingcovariance matrix representations. As shown in our experimental study on thetask of social network classification, such a similarity measure outperformsother widely used state-of-the-art methodologies. Our proposed method is alsocomputationally efficient. The computation of both the matrix representationand the similarity value can be performed in operations linear in the number ofedges. This makes our method scalable in practice. We believe our theoretical and empirical results provide evidence forstudying truncated power iterations, of the adjacency matrix, to characterizesocial networks.
arxiv-6000-277 | Multilingual Models for Compositional Distributed Semantics | http://arxiv.org/pdf/1404.4641v1.pdf | author:Karl Moritz Hermann, Phil Blunsom category:cs.CL published:2014-04-17 summary:We present a novel technique for learning semantic representations, whichextends the distributional hypothesis to multilingual data and joint-spaceembeddings. Our models leverage parallel data and learn to strongly align theembeddings of semantically equivalent sentences, while maintaining sufficientdistance between those of dissimilar sentences. The models do not rely on wordalignments or any syntactic information and are successfully applied to anumber of diverse languages. We extend our approach to learn semanticrepresentations at the document level, too. We evaluate these models on twocross-lingual document classification tasks, outperforming the prior state ofthe art. Through qualitative analysis and the study of pivoting effects wedemonstrate that our representations are semantically plausible and can capturesemantic relationships across languages without parallel data.
arxiv-6000-278 | The Least Wrong Model Is Not in the Data | http://arxiv.org/pdf/1404.0789v3.pdf | author:Oscar Stiffelman category:cs.LG published:2014-04-03 summary:The true process that generated data cannot be determined when multipleexplanations are possible. Prediction requires a model of the probability thata process, chosen randomly from the set of candidate explanations, generatessome future observation. The best model includes all of the informationcontained in the minimal description of the data that is not contained in thedata. It is closely related to the Halting Problem and is logarithmic in thesize of the data. Prediction is difficult because the ideal model is notcomputable, and the best computable model is not "findable." However, the errorfrom any approximation can be bounded by the size of the description using themodel.
arxiv-6000-279 | Random Projections for Linear Support Vector Machines | http://arxiv.org/pdf/1211.6085v5.pdf | author:Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, Petros Drineas category:cs.LG stat.ML published:2012-11-26 summary:Let X be a data matrix of rank \rho, whose rows represent n points ind-dimensional space. The linear support vector machine constructs a hyperplaneseparator that maximizes the 1-norm soft margin. We develop a new obliviousdimension reduction technique which is precomputed and can be applied to anyinput matrix X. We prove that, with high probability, the margin and minimumenclosing ball in the feature space are preserved to within \epsilon-relativeerror, ensuring comparable generalization as in the original space in the caseof classification. For regression, we show that the margin is preserved to\epsilon-relative error with high probability. We present extensive experimentswith real and synthetic data to support our theory.
arxiv-6000-280 | The First Parallel Multilingual Corpus of Persian: Toward a Persian BLARK | http://arxiv.org/pdf/1404.4572v1.pdf | author:Behrang Qasemizadeh, Saeed Rahimi, Behrooz Mahmoodi Bakhtiari category:cs.CL 68T50 I.2.7 published:2014-04-17 summary:In this article, we have introduced the first parallel corpus of Persian withmore than 10 other European languages. This article describes primary stepstoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,we have proposed morphosyntactic specification of Persian based onEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The articleintroduces Persian Language, with emphasis on its orthography andmorphosyntactic features, then a new Part-of-Speech categorization andorthography for Persian in digital environments is proposed. Finally, thecorpus and related statistic will be analyzed.
arxiv-6000-281 | Discovering and Exploiting Entailment Relationships in Multi-Label Learning | http://arxiv.org/pdf/1404.4038v2.pdf | author:Christina Papagiannopoulou, Grigorios Tsoumakas, Ioannis Tsamardinos category:cs.LG published:2014-04-15 summary:This work presents a sound probabilistic method for enforcing adherence ofthe marginal probabilities of a multi-label model to automatically discovereddeterministic relationships among labels. In particular we focus on discoveringtwo kinds of relationships among the labels. The first one concerns pairwisepositive entailement: pairs of labels, where the presence of one implies thepresence of the other in all instances of a dataset. The second concernsexclusion: sets of labels that do not coexist in the same instances of thedataset. These relationships are represented with a Bayesian network. Marginalprobabilities are entered as soft evidence in the network and adjusted throughprobabilistic inference. Our approach offers robust improvements in meanaverage precision compared to the standard binary relavance approach across all12 datasets involved in our experiments. The discovery process helpsinteresting implicit knowledge to emerge, which could be useful in itself.
arxiv-6000-282 | Rate-Distortion Auto-Encoders | http://arxiv.org/pdf/1312.7381v2.pdf | author:Luis G. Sanchez Giraldo, Jose C. Principe category:cs.LG published:2013-12-28 summary:A rekindled the interest in auto-encoder algorithms has been spurred byrecent work on deep learning. Current efforts have been directed towardseffective training of auto-encoder architectures with a large number of codingunits. Here, we propose a learning algorithm for auto-encoders based on arate-distortion objective that minimizes the mutual information between theinputs and the outputs of the auto-encoder subject to a fidelity constraint.The goal is to learn a representation that is minimally committed to the inputdata, but that is rich enough to reconstruct the inputs up to certain level ofdistortion. Minimizing the mutual information acts as a regularization termwhereas the fidelity constraint can be understood as a risk functional in theconventional statistical learning setting. The proposed algorithm uses arecently introduced measure of entropy based on infinitely divisible matricesthat avoids the plug in estimation of densities. Experiments usingover-complete bases show that the rate-distortion auto-encoders can learn aregularized input-output mapping in an implicit manner.
arxiv-6000-283 | A Survey of Data Mining Techniques for Social Media Analysis | http://arxiv.org/pdf/1312.4617v2.pdf | author:Mariam Adedoyin-Olowe, Mohamed Medhat Gaber, Frederic Stahl category:cs.SI cs.CL published:2013-12-17 summary:Social network has gained remarkable attention in the last decade. Accessingsocial network sites such as Twitter, Facebook LinkedIn and Google+ through theinternet and the web 2.0 technologies has become more affordable. People arebecoming more interested in and relying on social network for information, newsand opinion of other users on diverse subject matters. The heavy reliance onsocial network sites causes them to generate massive data characterised bythree computational issues namely; size, noise and dynamism. These issues oftenmake social network data very complex to analyse manually, resulting in thepertinent use of computational means of analysing them. Data mining provides awide range of techniques for detecting useful knowledge from massive datasetslike trends, patterns and rules [44]. Data mining techniques are used forinformation retrieval, statistical modelling and machine learning. Thesetechniques employ data pre-processing, data analysis, and data interpretationprocesses in the course of data analysis. This survey discusses different datamining techniques used in mining diverse aspects of the social network overdecades going from the historical techniques to the up-to-date models,including our novel technique named TRCM. All the techniques covered in thissurvey are listed in the Table.1 including the tools employed as well as namesof their authors.
arxiv-6000-284 | Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes using High-Resolution Neural EM Data | http://arxiv.org/pdf/1405.1965v1.pdf | author:Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Jeff W. Lichtman, Randal Burns, Michael Kazhdan category:cs.CV published:2014-04-16 summary:Accurately estimating the wiring diagram of a brain, known as a connectome,at an ultrastructure level is an open research problem. Specifically, preciselytracking neural processes is difficult, especially across many image slices.Here, we propose a novel method to automatically identify and annotate smallsubcellular structures present in axons, known as axoplasmic reticula, througha 3D volume of high-resolution neural electron microscopy data. Our methodproduces high precision annotations, which can help improve automaticsegmentation by using our results as seeds for segmentation, and as cues to aidsegment merging.
arxiv-6000-285 | Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes | http://arxiv.org/pdf/1404.4800v1.pdf | author:Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Ming Chuang, Priya Manavalan, Dean M. Kleissas, Joshua T. Vogelstein, R. Jacob Vogelstein, Randal Burns, Jeff W. Lichtman, Michael Kazhdan category:cs.CV published:2014-04-16 summary:In this paper, we present a new pipeline which automatically identifies andannotates axoplasmic reticula, which are small subcellular structures presentonly in axons. We run our algorithm on the Kasthuri11 dataset, which was colorcorrected using gradient-domain techniques to adjust contrast. We use abilateral filter to smooth out the noise in this data while preserving edges,which highlights axoplasmic reticula. These axoplasmic reticula are thenannotated using a morphological region growing algorithm. Additionally, weperform Laplacian sharpening on the bilaterally filtered data to enhance edges,and repeat the morphological region growing algorithm to annotate moreaxoplasmic reticula. We track our annotations through the slices to improveprecision, and to create long objects to aid in segment merging. This methodannotates axoplasmic reticula with high precision. Our algorithm can easily beadapted to annotate axoplasmic reticula in different sets of brain data bychanging a few thresholds. The contribution of this work is the introduction ofa straightforward and robust pipeline which annotates axoplasmic reticula withhigh precision, contributing towards advancements in automatic featureannotations in neural EM data.
arxiv-6000-286 | Stable Graphical Models | http://arxiv.org/pdf/1404.4351v1.pdf | author:Navodit Misra, Ercan E. Kuruoglu category:cs.LG stat.ML published:2014-04-16 summary:Stable random variables are motivated by the central limit theorem fordensities with (potentially) unbounded variance and can be thought of asnatural generalizations of the Gaussian distribution to skewed and heavy-tailedphenomenon. In this paper, we introduce stable graphical (SG) models, a classof multivariate stable densities that can also be represented as Bayesiannetworks whose edges encode linear dependencies between random variables. Onemajor hurdle to the extensive use of stable distributions is the lack of aclosed-form analytical expression for their densities. This makes penalizedmaximum-likelihood based learning computationally demanding. We establishtheoretically that the Bayesian information criterion (BIC) can asymptoticallybe reduced to the computationally more tractable minimum dispersion criterion(MDC) and develop StabLe, a structure learning algorithm based on MDC. We usesimulated datasets for five benchmark network topologies to empiricallydemonstrate how StabLe improves upon ordinary least squares (OLS) regression.We also apply StabLe to microarray gene expression data for lymphoblastoidcells from 727 individuals belonging to eight global population groups. Weestablish that StabLe improves test set performance relative to OLS viaten-fold cross-validation. Finally, we develop SGEX, a method for quantifyingdifferential expression of genes between different population groups.
arxiv-6000-287 | Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion | http://arxiv.org/pdf/1404.1377v2.pdf | author:Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu, Jieping Ye category:cs.LG math.NA stat.ML published:2014-04-04 summary:In this paper, we propose an efficient and scalable low rank matrixcompletion algorithm. The key idea is to extend orthogonal matching pursuitmethod from the vector case to the matrix case. We further propose an economicversion of our algorithm by introducing a novel weight updating rule to reducethe time and storage complexity. Both versions are computationally inexpensivefor each matrix pursuit iteration, and find satisfactory results in a fewiterations. Another advantage of our proposed algorithm is that it has only onetunable parameter, which is the rank. It is easy to understand and to use bythe user. This becomes especially important in large-scale learning problems.In addition, we rigorously show that both versions achieve a linear convergencerate, which is significantly better than the previous known results. We alsoempirically compare the proposed algorithms with several state-of-the-artmatrix completion algorithms on many real-world datasets, including thelarge-scale recommendation dataset Netflix as well as the MovieLens datasets.Numerical results show that our proposed algorithm is more efficient thancompeting algorithms while achieving similar or better prediction performance.
arxiv-6000-288 | Open Question Answering with Weakly Supervised Embedding Models | http://arxiv.org/pdf/1404.4326v1.pdf | author:Antoine Bordes, Jason Weston, Nicolas Usunier category:cs.CL cs.LG published:2014-04-16 summary:Building computers able to answer questions on any subject is a long standinggoal of artificial intelligence. Promising progress has recently been achievedby methods that learn to map questions to logical forms or database queries.Such approaches can be effective but at the cost of either large amounts ofhuman-labeled data or by defining lexicons and grammars tailored bypractitioners. In this paper, we instead take the radical approach of learningto map questions to vectorial feature representations. By mapping answers intothe same space one can query any knowledge base independent of its schema,without requiring any grammar or lexicon. Our method is trained with a newoptimization procedure combining stochastic gradient descent followed by afine-tuning step using the weak supervision provided by blending automaticallyand collaboratively generated resources. We empirically demonstrate that ourmodel can capture meaningful signals from its noisy supervision leading tomajor improvements over paralex, the only existing method able to be trained onsimilar weakly labeled data.
arxiv-6000-289 | Generic Object Detection With Dense Neural Patterns and Regionlets | http://arxiv.org/pdf/1404.4316v1.pdf | author:Will Y. Zou, Xiaoyu Wang, Miao Sun, Yuanqing Lin category:cs.CV published:2014-04-16 summary:This paper addresses the challenge of establishing a bridge between deepconvolutional neural networks and conventional object detection frameworks foraccurate and efficient generic object detection. We introduce Dense NeuralPatterns, short for DNPs, which are dense local features derived fromdiscriminatively trained deep convolutional neural networks. DNPs can be easilyplugged into conventional detection frameworks in the same way as other denselocal features(like HOG or LBP). The effectiveness of the proposed approach isdemonstrated with the Regionlets object detection framework. It achieved 46.1%mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCALVOC 2010 dataset, which dramatically improves the original Regionlets approachwithout DNPs.
arxiv-6000-290 | An Empirical Comparison of Parsing Methods for Stanford Dependencies | http://arxiv.org/pdf/1404.4314v1.pdf | author:Lingpeng Kong, Noah A. Smith category:cs.CL published:2014-04-16 summary:Stanford typed dependencies are a widely desired representation of naturallanguage sentences, but parsing is one of the major computational bottlenecksin text analysis systems. In light of the evolving definition of the Stanforddependencies and developments in statistical dependency parsing algorithms,this paper revisits the question of Cer et al. (2010): what is the tradeoffbetween accuracy and speed in obtaining Stanford dependencies in particular? Wealso explore the effects of input representations on this tradeoff:part-of-speech tags, the novel use of an alternative dependency representationas input, and distributional representaions of words. We find that directdependency parsing is a more viable solution than it was found to be in thepast. An accompanying software release can be found at:http://www.ark.cs.cmu.edu/TBSD
arxiv-6000-291 | Communication Communities in MOOCs | http://arxiv.org/pdf/1403.4640v2.pdf | author:Nabeel Gillani, Rebecca Eynon, Michael Osborne, Isis Hjorth, Stephen Roberts category:cs.CY cs.SI stat.ML published:2014-03-18 summary:Massive Open Online Courses (MOOCs) bring together thousands of people fromdifferent geographies and demographic backgrounds -- but to date, little isknown about how they learn or communicate. We introduce a new content-analysedMOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) toextract communities of learners based on the nature of their online forumposts. We see that BNMF yields a superior probabilistic generative model foronline discussions when compared to other models, and that the communities itlearns are differentiated by their composite students' demographic and courseperformance indicators. These findings suggest that computationally efficientprobabilistic generative modelling of MOOCs can reveal important insights foreducational researchers and practitioners and help to develop more intelligentand responsive online learning environments.
arxiv-6000-292 | MEG Decoding Across Subjects | http://arxiv.org/pdf/1404.4175v1.pdf | author:Emanuele Olivetti, Seyed Mostafa Kia, Paolo Avesani category:stat.ML cs.LG q-bio.NC published:2014-04-16 summary:Brain decoding is a data analysis paradigm for neuroimaging experiments thatis based on predicting the stimulus presented to the subject from theconcurrent brain activity. In order to make inference at the group level, astraightforward but sometimes unsuccessful approach is to train a classifier onthe trials of a group of subjects and then to test it on unseen trials from newsubjects. The extreme difficulty is related to the structural and functionalvariability across the subjects. We call this approach "decoding acrosssubjects". In this work, we address the problem of decoding across subjects formagnetoencephalographic (MEG) experiments and we provide the followingcontributions: first, we formally describe the problem and show that it belongsto a machine learning sub-field called transductive transfer learning (TTL).Second, we propose to use a simple TTL technique that accounts for thedifferences between train data and test data. Third, we propose the use ofensemble learning, and specifically of stacked generalization, to address thevariability across subjects within train data, with the aim of producing morestable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, wecompare the standard approach of not modelling the differences across subjects,to the proposed one of combining TTL and ensemble learning. We show that theproposed approach is consistently more accurate than the standard one.
arxiv-6000-293 | Dropout Training for Support Vector Machines | http://arxiv.org/pdf/1404.4171v1.pdf | author:Ning Chen, Jun Zhu, Jianfei Chen, Bo Zhang category:cs.LG published:2014-04-16 summary:Dropout and other feature noising schemes have shown promising results incontrolling over-fitting by artificially corrupting the training data. Thoughextensive theoretical and empirical studies have been performed for generalizedlinear models, little work has been done for support vector machines (SVMs),one of the most successful approaches for supervised learning. This paperpresents dropout training for linear SVMs. To deal with the intractableexpectation of the non-smooth hinge loss under corrupting distributions, wedevelop an iteratively re-weighted least square (IRLS) algorithm by exploringdata augmentation techniques. Our algorithm iteratively minimizes theexpectation of a re-weighted least square problem, where the re-weights haveclosed-form solutions. The similar ideas are applied to develop a new IRLSalgorithm for the expected logistic loss under corrupting distributions. Ouralgorithms offer insights on the connection and difference between the hingeloss and logistic loss in dropout training. Empirical results on several realdatasets demonstrate the effectiveness of dropout training on significantlyboosting the classification accuracy of linear SVMs.
arxiv-6000-294 | Recover Canonical-View Faces in the Wild with Deep Neural Networks | http://arxiv.org/pdf/1404.3543v2.pdf | author:Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-04-14 summary:Face images in the wild undergo large intra-personal variations, such asposes, illuminations, occlusions, and low resolutions, which cause greatchallenges to face-related applications. This paper addresses this challenge byproposing a new deep learning framework that can recover the canonical view offace images. It dramatically reduces the intra-person variances, whilemaintaining the inter-person discriminativeness. Unlike the existing facereconstruction methods that were either evaluated in controlled 2D environmentor employed 3D information, our approach directly learns the transformationfrom the face images with a complex set of variations to their canonical views.At the training stage, to avoid the costly process of labeling canonical-viewimages from the training set by hand, we have devised a new measurement toautomatically select or synthesize a canonical-view image for each identity. Asan application, this face recovery approach is used for face verification.Facial features are learned from the recovered canonical-view face images byusing a facial component-based convolutional neural network. Our approachachieves the state-of-the-art performance on the LFW dataset.
arxiv-6000-295 | Budgeted Influence Maximization for Multiple Products | http://arxiv.org/pdf/1312.2164v2.pdf | author:Nan Du, Yingyu Liang, Maria Florina Balcan, Le Song category:cs.LG cs.SI stat.ML published:2013-12-08 summary:The typical algorithmic problem in viral marketing aims to identify a set ofinfluential users in a social network, who, when convinced to adopt a product,shall influence other users in the network and trigger a large cascade ofadoptions. However, the host (the owner of an online social platform) oftenfaces more constraints than a single product, endless user attentions,unlimited budget and unbounded time; in reality, multiple products need to beadvertised, each user can tolerate only a small number of recommendations,influencing user has a cost and advertisers have only limited budgets, and theadoptions need to be maximized within a short time window. Given theses myriads of user, monetary, and timing constraints, it isextremely challenging for the host to design principled and efficient viralmarket algorithms with provable guarantees. In this paper, we provide a novelsolution by formulating the problem as a submodular maximization in acontinuous-time diffusion model under an intersection of a matroid and multipleknapsack constraints. We also propose an adaptive threshold greedy algorithmwhich can be faster than the traditional greedy algorithm with lazy evaluation,and scalable to networks with million of nodes. Furthermore, our mathematicalformulation allows us to prove that the algorithm can achieve an approximationfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints areactive, which also improves over previous guarantees from combinatorialoptimization literature. In the case when influencing each user has uniformcost, the approximation becomes even better to a factor of $1/3$. Extensivesynthetic and real world experiments demonstrate that our budgeted influencemaximization algorithm achieves the-state-of-the-art in terms of botheffectiveness and scalability, often beating the next best by significantmargins.
arxiv-6000-296 | Bayesian Neural Networks for Genetic Association Studies of Complex Disease | http://arxiv.org/pdf/1404.3989v2.pdf | author:Andrew L. Beam, Alison Motsinger-Reif, Jon Doyle category:q-bio.GN stat.AP stat.ML published:2014-04-15 summary:Discovering causal genetic variants from large genetic association studiesposes many difficult challenges. Assessing which genetic markers are involvedin determining trait status is a computationally demanding task, especially inthe presence of gene-gene interactions. A non-parametric Bayesian approach inthe form of a Bayesian neural network is proposed for use in analyzing geneticassociation studies. Demonstrations on synthetic and real data reveal they areable to efficiently and accurately determine which variants are involved indetermining case-control status. Using graphics processing units (GPUs) thetime needed to build these models is decreased by several orders of magnitude.In comparison with commonly used approaches for detecting genetic interactions,Bayesian neural networks perform very well across a broad spectrum of possiblegenetic relationships while having the computational efficiency needed tohandle large datasets.
arxiv-6000-297 | Sparse Compositional Metric Learning | http://arxiv.org/pdf/1404.4105v1.pdf | author:Yuan Shi, Aurélien Bellet, Fei Sha category:cs.LG stat.ML published:2014-04-15 summary:We propose a new approach for metric learning by framing it as learning asparse combination of locally discriminative metrics that are inexpensive togenerate from the training data. This flexible framework allows us to naturallyderive formulations for global, multi-task and local metric learning. Theresulting algorithms have several advantages over existing methods in theliterature: a much smaller number of parameters to be estimated and aprincipled way to generalize learned metrics to new testing data points. Toanalyze the approach theoretically, we derive a generalization bound thatjustifies the sparse combination. Empirically, we evaluate our algorithms onseveral datasets against state-of-the-art metric learning methods. The resultsare consistent with our theoretical findings and demonstrate the superiority ofour approach in terms of classification performance and scalability.
arxiv-6000-298 | Sparse Bilinear Logistic Regression | http://arxiv.org/pdf/1404.4104v1.pdf | author:Jianing V. Shi, Yangyang Xu, Richard G. Baraniuk category:math.OC cs.CV cs.LG published:2014-04-15 summary:In this paper, we introduce the concept of sparse bilinear logisticregression for decision problems involving explanatory variables that aretwo-dimensional matrices. Such problems are common in computer vision,brain-computer interfaces, style/content factorization, and parallel factoranalysis. The underlying optimization problem is bi-convex; we study itssolution and develop an efficient algorithm based on block coordinate descent.We provide a theoretical guarantee for global convergence and estimate theasymptotical convergence rate using the Kurdyka-{\L}ojasiewicz inequality. Arange of experiments with simulated and real data demonstrate that sparsebilinear logistic regression outperforms current techniques in severalimportant applications.
arxiv-6000-299 | Ensemble Classifiers and Their Applications: A Review | http://arxiv.org/pdf/1404.4088v1.pdf | author:Akhlaqur Rahman, Sumaira Tasnim category:cs.LG published:2014-04-15 summary:Ensemble classifier refers to a group of individual classifiers that arecooperatively trained on data set in a supervised classification problem. Inthis paper we present a review of commonly used ensemble classifiers in theliterature. Some ensemble classifiers are also developed targeting specificapplications. We also present some application driven ensemble classifiers inthis paper.
arxiv-6000-300 | Efficient Regularization of Squared Curvature | http://arxiv.org/pdf/1311.1838v2.pdf | author:Claudia Nieuwenhuis, Eno Toeppe, Lena Gorelick, Olga Veksler, Yuri Boykov category:cs.CV published:2013-11-07 summary:Curvature has received increased attention as an important alternative tolength based regularization in computer vision. In contrast to length, itpreserves elongated structures and fine details. Existing approaches are eitherinefficient, or have low angular resolution and yield results with strong blockartifacts. We derive a new model for computing squared curvature based onintegral geometry. The model counts responses of straight line triple cliques.The corresponding energy decomposes into submodular and supermodular pairwisepotentials. We show that this energy can be efficiently minimized even for highangular resolutions using the trust region framework. Our results confirm thatwe obtain accurate and visually pleasing solutions without strong artifacts atreasonable run times.
