arxiv-15900-1 | NED: An Inter-Graph Node Metric Based On Edit Distance | http://arxiv.org/pdf/1602.02358v3.pdf | author:Haohan Zhu, Xianrui Meng, George Kollios category:cs.DB cs.LG cs.SI published:2016-02-07 summary:Node similarity is a fundamental problem in graph analytics. However, nodesimilarity between nodes in different graphs (inter-graph nodes) has notreceived a lot of attention yet. The inter-graph node similarity is importantin learning a new graph based on the knowledge of an existing graph (transferlearning on graphs) and has applications in biological, communication, andsocial networks. In this paper, we propose a novel distance function formeasuring inter-graph node similarity with edit distance, called NED. In NED,two nodes are compared according to their local neighborhood structures whichare represented as unordered k-adjacent trees, without relying on labels orother assumptions. Since the computation problem of tree edit distance onunordered trees is NP-Complete, we propose a modified tree edit distance,called TED*, for comparing neighborhood trees. TED* is a metric distance, asthe original tree edit distance, but more importantly, TED* is polynomiallycomputable. As a metric distance, NED admits efficient indexing, providesinterpretable results, and shows to perform better than existing approaches ona number of data analysis tasks, including graph de-anonymization. Finally, theefficiency and effectiveness of NED are empirically demonstrated usingreal-world graphs.
arxiv-15900-2 | ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution | http://arxiv.org/pdf/1602.02334v1.pdf | author:Zeinab Bahmani, Leopoldo Bertossi, Nikolaos Vasiloglou category:cs.DB cs.AI cs.LG published:2016-02-07 summary:Entity resolution (ER), an important and common data cleaning problem, isabout detecting data duplicate representations for the same external entities,and merging them into single representations. Relatively recently, declarativerules called "matching dependencies" (MDs) have been proposed for specifyingsimilarity conditions under which attribute values in database records aremerged. In this work we show the process and the benefits of integrating fourcomponents of ER: (a) Building a classifier for duplicate/non-duplicate recordpairs built using machine learning (ML) techniques; (b) Use of MDs forsupporting the blocking phase of ML; (c) Record merging on the basis of theclassifier results; and (d) The use of the declarative language "LogiQL" -anextended form of Datalog supported by the "LogicBlox" platform- for allactivities related to data processing, and the specification and enforcement ofMDs.
arxiv-15900-3 | Scalable Text Mining with Sparse Generative Models | http://arxiv.org/pdf/1602.02332v1.pdf | author:Antti Puurula category:cs.IR cs.AI cs.CL published:2016-02-07 summary:The information age has brought a deluge of data. Much of this is in textform, insurmountable in scope for humans and incomprehensible in structure forcomputers. Text mining is an expanding field of research that seeks to utilizethe information contained in vast document collections. General data miningmethods based on machine learning face challenges with the scale of text data,posing a need for scalable text mining methods. This thesis proposes a solution to scalable text mining: generative modelscombined with sparse computation. A unifying formalization for generative textmodels is defined, bringing together research traditions that have usedformally equivalent models, but ignored parallel developments. This frameworkallows the use of methods developed in different processing tasks such asretrieval and classification, yielding effective solutions across differenttext mining tasks. Sparse computation using inverted indices is proposed forinference on probabilistic models. This reduces the computational complexity ofthe common text mining operations according to sparsity, yielding probabilisticmodels with the scalability of modern search engines. The proposed combination provides sparse generative models: a solution fortext mining that is general, effective, and scalable. Extensive experimentationon text classification and ranked retrieval datasets are conducted, showingthat the proposed solution matches or outperforms the leading task-specificmethods in effectiveness, with a order of magnitude decrease in classificationtimes for Wikipedia article categorization with a million classes. Thedeveloped methods were further applied in two 2014 Kaggle data mining prizecompetitions with over a hundred competing teams, earning first and secondplaces.
arxiv-15900-4 | Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization | http://arxiv.org/pdf/1602.02434v1.pdf | author:Shervin Minaee, Yao Wang category:cs.CV published:2016-02-07 summary:Sparse decomposition has been widely used for different applications, such assource separation, image classification, image denoising and more. This paperpresents a new algorithm for segmentation of an image into background andforeground text and graphics using sparse decomposition and total variationminimization. The proposed method is designed based on the assumption that thebackground part of the image is smoothly varying and can be represented by alinear combination of a few smoothly varying basis functions, while theforeground text and graphics can be modeled with a sparse component overlaid onthe smooth background. The background and foreground are separated using asparse decomposition framework regularized with a few suitable regularizationterms which promotes the sparsity and connectivity of foreground pixels. Thisalgorithm has been tested on a dataset of images extracted from HEVC standardtest sequences for screen content coding, and is shown to have superiorperformance over some prior methods, including least absolute deviationfitting, k-means clustering based segmentation in DjVu and shape primitiveextraction and coding (SPEC) algorithm.
arxiv-15900-5 | Stratified Bayesian Optimization | http://arxiv.org/pdf/1602.02338v2.pdf | author:Saul Toscano-Palmerin, Peter I. Frazier category:cs.LG math.OC stat.ML published:2016-02-07 summary:We consider derivative-free black-box global optimization of expensive noisyfunctions, when most of the randomness in the objective is produced by a fewinfluential scalar random inputs. We present a new Bayesian global optimizationalgorithm, called Stratified Bayesian Optimization (SBO), which uses thisstrong dependence to improve performance. Our algorithm is similar in spirit tostratification, a technique from simulation, which uses strong dependence on acategorical representation of the random input to reduce variance. Wedemonstrate in numerical experiments that SBO outperforms state-of-the-artBayesian optimization benchmarks that do not leverage this dependence.
arxiv-15900-6 | Exploring the Limits of Language Modeling | http://arxiv.org/pdf/1602.02410v2.pdf | author:Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu category:cs.CL published:2016-02-07 summary:In this work we explore recent advances in Recurrent Neural Networks forlarge scale Language Modeling, a task central to language understanding. Weextend current models to deal with two key challenges present in this task:corpora and vocabulary sizes, and complex, long term structure of language. Weperform an exhaustive study on techniques such as character ConvolutionalNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.Our best single model significantly improves state-of-the-art perplexity from51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),while an ensemble of models sets a new record by improving perplexity from 41.0down to 23.7. We also release these models for the NLP and ML community tostudy and improve upon.
arxiv-15900-7 | Ensemble Robustness of Deep Learning Algorithms | http://arxiv.org/pdf/1602.02389v2.pdf | author:Jiashi Feng, Tom Zahavy, Bingyi Kang, Huan Xu, Shie Mannor category:cs.LG cs.CV stat.ML published:2016-02-07 summary:The question why deep learning algorithms perform so well in practice haspuzzled machine learning theoreticians and practitioners alike. However, mostof well-established approaches, such as hypothesis capacity, robustness orsparseness, have not provided complete explanations, due to the high complexityof the deep learning algorithms and their inherent randomness. In this work, weintroduce a new approach -- ensemble robustness -- towards characterizing thegeneralization performance of generic deep learning algorithms. Ensemblerobustness concerns robustness of the population of the hypotheses that may beoutput by a learning algorithm. Through the lens of ensemble robustness, wereveal that a stochastic learning algorithm can generalize well as long as itssensitiveness to adversarial perturbation is bounded in average, orequivalently, the performance variance of the algorithm is small. Quantifyingthe ensemble robustness of various deep learning algorithms may be difficultanalytically. However, extensive simulations for seven common deep learningalgorithms for different network architectures provide supporting evidence forour claims. In addition, as an example for utilizing ensemble robustness, wepropose a novel semi-supervised learning method that outperforms thestate-of-the-art. Furthermore, our work explains the good performance ofseveral published deep learning algorithms.
arxiv-15900-8 | Eye-CU: Sleep Pose Classification for Healthcare using Multimodal Multiview Data | http://arxiv.org/pdf/1602.02343v2.pdf | author:Carlos Torres, Victor Fragoso, Scott D. Hammond, Jeffrey C. Fried, B. S. Manjunath category:cs.CV published:2016-02-07 summary:Manual analysis of body poses of bed-ridden patients requires staff tocontinuously track and record patient poses. Two limitations in thedissemination of pose-related therapies are scarce human resources andunreliable automated systems. This work addresses these issues by introducing anew method and a new system for robust automated classification of sleep posesin an Intensive Care Unit (ICU) environment. The new method,coupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM)data and finds the set of modality trust values that minimizes the differencebetween expected and estimated labels. The new system, Eye-CU, is an affordablemulti-sensor modular system for unobtrusive data collection and analysis inhealthcare. Experimental results indicate that the performance of cc-LS matchesthe performance of existing methods in ideal scenarios. This method outperformsthe latest techniques in challenging scenarios by 13% for those with poorillumination and by 70% for those with both poor illumination and occlusions.Results also show that a reduced Eye-CU configuration can classify poseswithout pressure information with only a slight drop in its performance.
arxiv-15900-9 | Hyperparameter optimization with approximate gradient | http://arxiv.org/pdf/1602.02355v2.pdf | author:Fabian Pedregosa category:stat.ML cs.LG math.OC published:2016-02-07 summary:Most models in machine learning contain at least one hyperparameter tocontrol for model complexity. Choosing an appropriate set of hyperparameters isboth crucial in terms of model accuracy and computationally challenging. Inthis work we propose an algorithm for the optimization of continuoushyperparameters using inexact gradient information. An advantage of this methodis that hyperparameters can be updated before model parameters have fullyconverged. We also give sufficient conditions for the global convergence ofthis method, based on regularity conditions of the involved functions andsummability of errors. Finally, we validate the empirical performance of thismethod on the estimation of regularization constants of L2-regularized logisticregression and kernel Ridge regression. Empirical benchmarks indicate that ourapproach is highly competitive with respect to state of the art methods.
arxiv-15900-10 | How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks | http://arxiv.org/pdf/1602.02282v1.pdf | author:Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, Ole Winther category:stat.ML cs.LG published:2016-02-06 summary:Variational autoencoders are a powerful framework for unsupervised learning.However, previous work has been restricted to shallow models with one or twolayers of fully factorized stochastic latent variables, limiting theflexibility of the latent representation. We propose three advances in trainingalgorithms of variational autoencoders, for the first time allowing to traindeep models of up to five stochastic layers, (1) using a structure similar tothe Ladder network as the inference model, (2) warm-up period to supportstochastic units staying active in early training, and (3) use of batchnormalization. Using these improvements we show state-of-the-art log-likelihoodresults for generative modeling on several benchmark datasets.
arxiv-15900-11 | DOLPHIn - Dictionary Learning for Phase Retrieval | http://arxiv.org/pdf/1602.02263v1.pdf | author:Andreas M. Tillmann, Yonina C. Eldar, Julien Mairal category:math.OC cs.IT cs.LG math.IT stat.ML published:2016-02-06 summary:We propose a new algorithm to learn a dictionary for reconstructing andsparsely encoding signals from measurements without phase. Specifically, weconsider the task of estimating a two-dimensional image from squared-magnitudemeasurements of a complex-valued linear transformation of the original image.Several recent phase retrieval algorithms exploit underlying sparsity of theunknown signal in order to improve recovery performance. In this work, weconsider such a sparse signal prior in the context of phase retrieval, when thesparsifying dictionary is not known in advance. Our algorithm jointlyreconstructs the unknown signal - possibly corrupted by noise - and learns adictionary such that each patch of the estimated image can be sparselyrepresented. Numerical experiments demonstrate that our approach can obtainsignificantly better reconstructions for phase retrieval problems with noisethan methods that cannot exploit such "hidden" sparsity. Moreover, on thetheoretical side, we provide a convergence result for our method.
arxiv-15900-12 | Recovery guarantee of weighted low-rank approximation via alternating minimization | http://arxiv.org/pdf/1602.02262v1.pdf | author:Yuanzhi Li, Yingyu Liang, Andrej Risteski category:cs.LG cs.DS stat.ML published:2016-02-06 summary:Many applications require recovering a ground truth low-rank matrix fromnoisy observations of the entries. In practice, this is typically formulated asweighted low-rank approximation problem and solved using non-convexoptimization heuristics such as alternating minimization. Such non-convextechniques have little guarantees. Even worse, weighted low-rank approximationis NP-hard for even the most simple case when the ground truth is a rank-1matrix. In this paper, we provide provable recovery guarantee in polynomial time fora natural class of matrices and weights. In particular, we bound the spectralnorm of the difference between the recovered matrix and the ground truth, bythe spectral norm of the weighted noise plus an additive error term thatdecreases exponentially with the number of rounds of alternating minimization.This provides the first theoretical result for weighted low-rank approximationvia alternating minimization with non-binary deterministic weights. It is asignificant generalization of the results for matrix completion, the specialcase with binary weights, since our assumptions are similar or weaker thanthose made in existing works. The key technical challenge is that under non-binary deterministic weights,naive alternating minimization steps will destroy the incoherency and spectralproperties of the intermediate solution, which are needed for making progresstowards the ground truth. One of our key technical contributions is a whiteningstep that maintains these properties of the intermediate solution after eachround, which may be applied to alternating minimization for other problems andthus is of independent interest.
arxiv-15900-13 | A Tractable Fully Bayesian Method for the Stochastic Block Model | http://arxiv.org/pdf/1602.02256v1.pdf | author:Kohei Hayashi, Takuya Konishi, Tatsuro Kawamoto category:cs.LG stat.ML published:2016-02-06 summary:The stochastic block model (SBM) is a generative model revealing macroscopicstructures in graphs. Bayesian methods are used for (i) cluster assignmentinference and (ii) model selection for the number of clusters. In this paper,we study the behavior of Bayesian inference in the SBM in the large samplelimit. Combining variational approximation and Laplace's method, a consistentcriterion of the fully marginalized log-likelihood is established. Based onthat, we derive a tractable algorithm that solves tasks (i) and (ii)concurrently, obviating the need for an outer loop to check all modelcandidates. Our empirical and theoretical results demonstrate that our methodis scalable in computation, accurate in approximation, and concise in modelselection.
arxiv-15900-14 | Importance Sampling for Minibatches | http://arxiv.org/pdf/1602.02283v1.pdf | author:Dominik Csiba, Peter Richtárik category:cs.LG math.OC stat.ML published:2016-02-06 summary:Minibatching is a very well studied and highly popular technique insupervised learning, used by practitioners due to its ability to acceleratetraining through better utilization of parallel processing power and reductionof stochastic variance. Another popular technique is importance sampling -- astrategy for preferential sampling of more important examples also capable ofaccelerating the training process. However, despite considerable effort by thecommunity in these areas, and due to the inherent technical difficulty of theproblem, there is no existing work combining the power of importance samplingwith the strength of minibatching. In this paper we propose the first {\emimportance sampling for minibatches} and give simple and rigorous complexityanalysis of its performance. We illustrate on synthetic problems that fortraining data of certain properties, our sampling can lead to several orders ofmagnitude improvement in training time. We then test the new sampling onseveral popular datasets, and show that the improvement can reach an order ofmagnitude.
arxiv-15900-15 | Reducing training requirements through evolutionary based dimension reduction and subject transfer | http://arxiv.org/pdf/1602.02237v1.pdf | author:Adham Atyabi, Martin Luerssena, Sean P. Fitzgibbon, Trent Lewis, David M. W. Powersa category:cs.NE published:2016-02-06 summary:Training Brain Computer Interface (BCI) systems to understand the intentionof a subject through Electroencephalogram (EEG) data currently requiresmultiple training sessions with a subject in order to develop the necessaryexpertise to distinguish signals for different tasks. Conventionally the taskof training the subject is done by introducing a training and calibration stageduring which some feedback is presented to the subject. This training sessioncan take several hours which is not appropriate for on-line EEG-based BCIsystems. An alternative approach is to use previous recording sessions of thesame person or some other subjects that performed the same tasks (subjecttransfer) for training the classifiers. The main aim of this study is togenerate a methodology that allows the use of data from other subjects whilereducing the dimensions of the data. The study investigates severalpossibilities for reducing the necessary training and calibration period insubjects and the classifiers and addresses the impact of i) evolutionarysubject transfer and ii) adapting previously trained methods (retraining) usingother subjects data. Our results suggest reduction to 40% of target subjectdata is sufficient for training the classifier. Our results also indicate thesuperiority of the approaches that incorporated evolutionary subject transferand highlights the feasibility of adapting a system trained on other subjects.
arxiv-15900-16 | Improved Dropout for Shallow and Deep Learning | http://arxiv.org/pdf/1602.02220v1.pdf | author:Zhe Li, Boqing Gong, Tianbao Yang category:cs.LG stat.ML published:2016-02-06 summary:Dropout has been witnessed with great success in training deep neuralnetworks by independently zeroing out the outputs of neurons at random. It hasalso received a surge of interest for shallow learning, e.g., logisticregression. However, the independent sampling for dropout could be suboptimalfor the sake of convergence. In this paper, we propose to use multinomialsampling for dropout, i.e., sampling features or neurons according to amultinomial distribution with different probabilities for differentfeatures/neurons. To exhibit the optimal dropout probabilities, we analyze theshallow learning with multinomial dropout and establish the risk bound forstochastic optimization. By minimizing a sampling dependent factor in the riskbound, we obtain a distribution-dependent dropout with sampling probabilitiesdependent on the second order statistics of the data distribution. To tacklethe issue of evolving distribution of neurons in deep learning, we propose anefficient adaptive dropout (named \textbf{evolutional dropout}) that computesthe sampling probabilities on-the-fly from a mini-batch of examples. Empiricalstudies on several benchmark datasets demonstrate that the proposed dropoutsachieve not only much faster convergence and but also a smaller testing errorthan the standard dropout. For example, on the CIFAR-100 data, the evolutionaldropout achieves relative improvements over 10\% on the prediction performanceand over 50\% on the convergence speed compared to the standard dropout.
arxiv-15900-17 | Variational Hamiltonian Monte Carlo via Score Matching | http://arxiv.org/pdf/1602.02219v1.pdf | author:Cheng Zhang, Babak Shahbaba, Hongkai Zhao category:stat.CO stat.ML published:2016-02-06 summary:Traditionally, the field of computational Bayesian statistics has beendivided into two main subfields: variational methods and Markov chain MonteCarlo (MCMC). In recent years, however, several methods have been proposedbased on combining variational Bayesian inference and MCMC simulation in orderto improve their overall accuracy and computational efficiency. This marriageof fast evaluation and flexible approximation provides a promising means ofdesigning scalable Bayesian inference methods. In this paper, we explore thepossibility of incorporating variational approximation into a state-of-the-artMCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradientcomputation in the simulation of Hamiltonian flow, which is the bottleneck formany applications of HMC in big data problems. To this end, we use a {\itfree-form} approximation induced by a fast and flexible surrogate functionbased on single-hidden layer feedforward neural networks. The surrogateprovides sufficiently accurate approximation while allowing for fastexploration of parameter space, resulting in an efficient approximate inferencealgorithm. We demonstrate the advantages of our method on both synthetic andreal data problems.
arxiv-15900-18 | Strongly-Typed Recurrent Neural Networks | http://arxiv.org/pdf/1602.02218v1.pdf | author:David Balduzzi, Muhammad Ghifary category:cs.LG cs.NE published:2016-02-06 summary:Recurrent neural networks are increasing popular models for sequentiallearning. Unfortunately, although the most effective RNN architectures areperhaps excessively complicated, extensive searches have not found simpleralternatives. This paper imports ideas from physics and functional programminginto RNN design to provide guiding principles. From physics we introduce typeconstraints, analogous to the constraints that disqualify adding meters toseconds in physics. From functional programming, we require that strongly-typedarchitectures factorize into stateless learnware and state-dependent firmware,thereby ameliorating the impact of side-effects. The features learned bystrongly-typed nets have a simple semantic interpretation via dynamicaverage-pooling on one-dimensional convolutions. We also show thatstrongly-typed gradients are better behaved than in classical architectures,and characterize the representational power of strongly-typed nets. Finally,experiments show that, despite being more constrained, strongly-typedarchitectures achieve lower training error and comparable generalization errorto classical architectures.
arxiv-15900-19 | Swivel: Improving Embeddings by Noticing What's Missing | http://arxiv.org/pdf/1602.02215v1.pdf | author:Noam Shazeer, Ryan Doherty, Colin Evans, Chris Waterson category:cs.CL published:2016-02-06 summary:We present Submatrix-wise Vector Embedding Learner (Swivel), a method forgenerating low-dimensional feature embeddings from a feature co-occurrencematrix. Swivel performs approximate factorization of the point-wise mutualinformation matrix via stochastic gradient descent. It uses a piecewise losswith special handling for unobserved co-occurrences, and thus makes use of allthe information in the matrix. While this requires computation proportional tothe size of the entire matrix, we make use of vectorized multiplication toprocess thousands of rows and columns at once to compute millions of predictedvalues. Furthermore, we partition the matrix into shards in order toparallelize the computation across many nodes. This approach results in moreaccurate embeddings than can be achieved with methods that consider onlyobserved co-occurrences, and can scale to much larger corpora than can behandled with sampling methods.
arxiv-15900-20 | Classification Accuracy as a Proxy for Two Sample Testing | http://arxiv.org/pdf/1602.02210v1.pdf | author:Aaditya Ramdas, Aarti Singh, Larry Wasserman category:cs.LG cs.AI math.ST stat.ML stat.TH published:2016-02-06 summary:When data analysts train a classifier and check if its accuracy issignificantly different from random guessing, they are implicitly andindirectly performing a hypothesis test (two sample testing) and it is ofimportance to ask whether this indirect method for testing is statisticallyoptimal or not. Given that hypothesis tests attempt to maximize statisticalpower subject to a bound on the allowable false positive rate, while predictionattempts to minimize statistical risk on future predictions on unseen data, wewish to study whether a predictive approach for an ultimate aim of testing isprudent. We formalize this problem by considering the two-sample mean-testingsetting where one must determine if the means of two Gaussians (with known andequal covariance) are the same or not, but the analyst indirectly does so bychecking whether the accuracy achieved by Fisher's LDA classifier issignificantly different from chance or not. Unexpectedly, we find that theasymptotic power of LDA's sample-splitting classification accuracy is actuallyminimax rate-optimal in terms of problem-dependent parameters. Since predictionis commonly thought to be harder than testing, it might come as a surprise tosome that solving a harder problem does not create a information-theoreticbottleneck for the easier one. On the flip side, even though the power israte-optimal, our derivation suggests that it may be worse by a small constantfactor; hence practitioners must be wary of using (admittedly flexible)prediction methods on disguised testing problems.
arxiv-15900-21 | A Deep Learning Approach to Unsupervised Ensemble Learning | http://arxiv.org/pdf/1602.02285v1.pdf | author:Uri Shaham, Xiuyuan Cheng, Omer Dror, Ariel Jaffe, Boaz Nadler, Joseph Chang, Yuval Kluger category:stat.ML cs.LG published:2016-02-06 summary:We show how deep learning methods can be applied in the context ofcrowdsourcing and unsupervised ensemble learning. First, we prove that thepopular model of Dawid and Skene, which assumes that all classifiers areconditionally independent, is {\em equivalent} to a Restricted BoltzmannMachine (RBM) with a single hidden node. Hence, under this model, the posteriorprobabilities of the true labels can be instead estimated via a trained RBM.Next, to address the more general case, where classifiers may strongly violatethe conditional independence assumption, we propose to apply RBM-based DeepNeural Net (DNN). Experimental results on various simulated and real-worlddatasets demonstrate that our proposed DNN approach outperforms otherstate-of-the-art methods, in particular when the data violates the conditionalindependence assumption.
arxiv-15900-22 | Efficient Second Order Online Learning via Sketching | http://arxiv.org/pdf/1602.02202v1.pdf | author:Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, John Langford category:cs.LG published:2016-02-06 summary:We propose Sketched Online Newton (SON), an online second order learningalgorithm that enjoys substantially improved regret guarantees forill-conditioned data. SON is an enhanced version of the Online Newton Step,which, via sketching techniques enjoys a linear running time. We furtherimprove the computational complexity to linear in the number of nonzero entriesby creating sparse forms of the sketching methods (such as Oja's rule) for topeigenvector extraction. Together, these algorithms eliminate all computationalobstacles in previous second order online learning approaches.
arxiv-15900-23 | BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits | http://arxiv.org/pdf/1602.02196v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML published:2016-02-06 summary:We present efficient algorithms for the problem of contextual bandits withi.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class ofpolicies. Our algorithm BISTRO requires d calls to the empirical riskminimization (ERM) oracle per round, where d is the number of actions. Themethod uses unlabeled data to make the problem computationally simple. When theERM problem itself is computationally hard, we extend the approach by employingmultiplicative approximation algorithms for the ERM. The integrality gap of therelaxation only enters in the regret bound rather than the benchmark. Finally,we show that the adversarial version of the contextual bandit problem islearnable (and efficient) whenever the full-information supervised onlinelearning problem has a non-trivial regret guarantee (and efficient).
arxiv-15900-24 | Rényi Divergence Variational Inference | http://arxiv.org/pdf/1602.02311v2.pdf | author:Yingzhen Li, Richard E. Turner category:stat.ML cs.LG published:2016-02-06 summary:This paper introduces the variational R\'enyi bound (VR) that extendstraditional variational inference to R\'enyi's alpha-divergences. This newfamily of variational methods unifies a number of existing approaches, andenables a smooth interpolation from the evidence lower-bound to the logmarginal likelihood that is controlled by the value of alpha that parametrisesthe divergence. The reparameterization trick, Monte Carlo approximation andstochastic optimisation methods are deployed to obtain a unified framework foroptimisation. We further consider negative alpha values and propose a novelvariational inference method as a new special case in the proposed framework.Experiments on Bayesian neural networks and variational auto-encodersdemonstrate the wide applicability of the VR bound.
arxiv-15900-25 | Design of false color palettes for grayscale reproduction | http://arxiv.org/pdf/1602.03206v2.pdf | author:Filip A. Sala category:cs.GR cs.CV published:2016-02-06 summary:Design of false color palette is quite easy but some effort has to be done toachieve good dynamic range, contrast and overall appearance of the palette.Such palettes, for instance, are commonly used in scientific papers forpresenting the data. However, to lower the cost of the paper most scientistsdecide to let the data to be printed in grayscale. The same applies to e-bookreaders based on e-ink where most of them are still grayscale. For majority offalse color palettes reproducing them in grayscale results in ambiguous mappingof the colors and may be misleading for the reader. In this article design offalse color palettes suitable for grayscale reproduction is described. Due tothe monotonic change of luminance of these palettes grayscale representation isvery similar to the data directly presented with a grayscale palette. Somesuggestions and examples how to design such palettes are provided.
arxiv-15900-26 | A Note on Alternating Minimization Algorithm for the Matrix Completion Problem | http://arxiv.org/pdf/1602.02164v1.pdf | author:David Gamarnik, Sidhant Misra category:stat.ML cs.LG cs.NA published:2016-02-05 summary:We consider the problem of reconstructing a low rank matrix from a subset ofits entries and analyze two variants of the so-called Alternating Minimizationalgorithm, which has been proposed in the past. We establish that when theunderlying matrix has rank $r=1$, has positive bounded entries, and the graph$\mathcal{G}$ underlying the revealed entries has bounded degree and diameterwhich is at most logarithmic in the size of the matrix, both algorithms succeedin reconstructing the matrix approximately in polynomial time starting from anarbitrary initialization. We further provide simulation results which suggestthat the second algorithm which is based on the message passing type updates,performs significantly better.
arxiv-15900-27 | Daleel: Simplifying Cloud Instance Selection Using Machine Learning | http://arxiv.org/pdf/1602.02159v1.pdf | author:Faiza Samreen, Yehia Elkhatib, Matthew Rowe, Gordon S. Blair category:cs.DC cs.LG cs.PF published:2016-02-05 summary:Decision making in cloud environments is quite challenging due to thediversity in service offerings and pricing models, especially considering thatthe cloud market is an incredibly fast moving one. In addition, there are nohard and fast rules, each customer has a specific set of constraints (e.g.budget) and application requirements (e.g. minimum computational resources).Machine learning can help address some of the complicated decisions by carryingout customer-specific analytics to determine the most suitable instance type(s)and the most opportune time for starting or migrating instances. We employmachine learning techniques to develop an adaptive deployment policy, providingan optimal match between the customer demands and the available cloud serviceofferings. We provide an experimental study based on extensive set of jobexecutions over a major public cloud infrastructure.
arxiv-15900-28 | Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters | http://arxiv.org/pdf/1602.02151v1.pdf | author:Zeyuan Allen-Zhu, Yang Yuan, Karthik Sridharan category:cs.LG stat.ML published:2016-02-05 summary:The amount of data available in the world is growing faster and bigger thanour ability to deal with it. However, if we take advantage of the internalstructure, data may become much smaller for machine learning purposes. In thispaper we focus on one of the most fundamental machine learning tasks, empiricalrisk minimization (ERM), and provide faster algorithms with the help from theclustering structure of the data. We introduce a simple notion of raw clustering that can be efficientlyobtained with just one pass of the data, and propose two algorithms. Ourvariance-reduction based algorithm ClusterSVRG introduces a new gradientestimator using the clustering information, and our accelerated algorithmClusterACDM is built on a novel Haar transformation applied to the dual spaceof each cluster. Our algorithms outperform their classical counterparts both intheory and practice.
arxiv-15900-29 | Reducing Runtime by Recycling Samples | http://arxiv.org/pdf/1602.02136v1.pdf | author:Jialei Wang, Hai Wang, Nathan Srebro category:cs.LG stat.ML published:2016-02-05 summary:Contrary to the situation with stochastic gradient descent, we argue thatwhen using stochastic methods with variance reduction, such as SDCA, SAG orSVRG, as well as their variants, it could be beneficial to reuse previouslyused samples instead of fresh samples, even when fresh samples are available.We demonstrate this empirically for SDCA, SAG and SVRG, studying the optimalsample size one should use, and also uncover be-havior that suggests runningSDCA for an integer number of epochs could be wasteful.
arxiv-15900-30 | Mining Software Quality from Software Reviews: Research Trends and Open Issues | http://arxiv.org/pdf/1602.02133v1.pdf | author:Issa Atoum, Ahmed Otoom category:cs.CL cs.IR published:2016-02-05 summary:Software review text fragments have considerably valuable information aboutusers experience. It includes a huge set of properties including the softwarequality. Opinion mining or sentiment analysis is concerned with analyzingtextual user judgments. The application of sentiment analysis on softwarereviews can find a quantitative value that represents software quality.Although many software quality methods are proposed they are considereddifficult to customize and many of them are limited. This article investigatesthe application of opinion mining as an approach to extract software qualityproperties. We found that the major issues of software reviews mining usingsentiment analysis are due to software lifecycle and the diverse users andteams.
arxiv-15900-31 | Sub-cortical brain structure segmentation using F-CNN's | http://arxiv.org/pdf/1602.02130v1.pdf | author:Mahsa Shakeri, Stavros Tsogkas, Enzo Ferrante, Sarah Lippe, Samuel Kadoury, Nikos Paragios, Iasonas Kokkinos category:cs.CV published:2016-02-05 summary:In this paper we propose a deep learning approach for segmenting sub-corticalstructures of the human brain in Magnetic Resonance (MR) image data. We drawinspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN)architecture for semantic segmentation of objects in natural images, and adaptit to our task. Unlike previous CNN-based methods that operate on imagepatches, our model is applied on a full blown 2D image, without any alignmentor registration steps at testing time. We further improve segmentation resultsby interpreting the CNN output as potentials of a Markov Random Field (MRF),whose topology corresponds to a volumetric grid. Alpha-expansion is used toperform approximate inference imposing spatial volumetric homogeneity to theCNN priors. We compare the performance of the proposed pipeline with a similarsystem using Random Forest-based priors, as well as state-of-art segmentationalgorithms, and show promising results on two different brain MRI datasets.
arxiv-15900-32 | Active Information Acquisition | http://arxiv.org/pdf/1602.02181v1.pdf | author:He He, Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG published:2016-02-05 summary:We propose a general framework for sequential and dynamic acquisition ofuseful information in order to solve a particular task. While our goal could inprinciple be tackled by general reinforcement learning, our particular settingis constrained enough to allow more efficient algorithms. In this paper, wework under the Learning to Search framework and show how to formulate the goalof finding a dynamic information acquisition policy in that framework. We applyour formulation on two tasks, sentiment analysis and image recognition, andshow that the learned policies exhibit good statistical performance. As anemergent byproduct, the learned policies show a tendency to focus on the mostprominent parts of each instance and give harder instances more attentionwithout explicitly being trained to do so.
arxiv-15900-33 | Sequence Classification with Neural Conditional Random Fields | http://arxiv.org/pdf/1602.02123v1.pdf | author:Myriam Abramson category:cs.LG published:2016-02-05 summary:The proliferation of sensor devices monitoring human activity generatesvoluminous amount of temporal sequences needing to be interpreted andcategorized. Moreover, complex behavior detection requires the personalizationof multi-sensor fusion algorithms. Conditional random fields (CRFs) arecommonly used in structured prediction tasks such as part-of-speech tagging innatural language processing. Conditional probabilities guide the choice of eachtag/label in the sequence conflating the structured prediction task with thesequence classification task where different models provide differentcategorization of the same sequence. The claim of this paper is that CRF modelsalso provide discriminative models to distinguish between types of sequenceregardless of the accuracy of the labels obtained if we calibrate the classmembership estimate of the sequence. We introduce and compare different neuralnetwork based linear-chain CRFs and we present experiments on two complexsequence classification and structured prediction tasks to support this claim.
arxiv-15900-34 | Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities | http://arxiv.org/pdf/1602.02114v1.pdf | author:Adrien Todeschini, François Caron category:stat.ME cs.SI physics.soc-ph stat.ML published:2016-02-05 summary:We propose a novel statistical model for sparse networks with overlappingcommunity structure. The model is based on representing the graph as anexchangeable point process, and naturally generalizes existing probabilisticmodels with overlapping block-structure to the sparse regime. Our constructionbuilds on vectors of completely random measures, and has interpretableparameters, each node being assigned a vector representing its level ofaffiliation to some latent communities. We develop methods for simulating thisclass of random graphs, as well as to perform posterior inference. We show thatthe proposed approach can recover interpretable structure from two real-worldnetworks and can handle graphs with thousands of nodes and tens of thousands ofedges.
arxiv-15900-35 | From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification | http://arxiv.org/pdf/1602.02068v2.pdf | author:André F. T. Martins, Ramón Fernandez Astudillo category:cs.CL cs.LG stat.ML published:2016-02-05 summary:We propose sparsemax, a new activation function similar to the traditionalsoftmax, but able to output sparse probabilities. After deriving itsproperties, we show how its Jacobian can be efficiently computed, enabling itsuse in a network trained with backpropagation. Then, we propose a new smoothand convex loss function which is the sparsemax analogue of the logistic loss.We reveal an unexpected connection between this new loss and the Huberclassification loss. We obtain promising empirical results in multi-labelclassification problems and in attention-based neural networks for naturallanguage inference. For the latter, we achieve a similar performance as thetraditional softmax, but with a selective, more compact, attention focus.
arxiv-15900-36 | Variance-Reduced and Projection-Free Stochastic Optimization | http://arxiv.org/pdf/1602.02101v1.pdf | author:Elad Hazan, Haipeng Luo category:cs.LG published:2016-02-05 summary:The Frank-Wolfe optimization algorithm has recently regained popularity formachine learning applications due to its projection-free property and itsability to handle structured constraints. However, in the stochastic learningsetting, it is still relatively understudied compared to the gradient descentcounterpart. In this work, leveraging a recent variance reduction technique, wepropose two stochastic Frank-Wolfe variants which substantially improveprevious results in terms of the number of stochastic gradient evaluationsneeded to achieve $1-\epsilon$ accuracy. For example, we improve from$O(\frac{1}{\epsilon})$ to $O(\ln\frac{1}{\epsilon})$ if the objective functionis smooth and strongly convex, and from $O(\frac{1}{\epsilon^2})$ to$O(\frac{1}{\epsilon^{1.5}})$ if the objective function is smooth andLipschitz. The theoretical improvement is also observed in experiments onreal-world datasets for a multiclass classification application.
arxiv-15900-37 | Harmonic Grammar in a DisCo Model of Meaning | http://arxiv.org/pdf/1602.02089v1.pdf | author:Martha Lewis, Bob Coecke category:cs.AI cs.CL published:2016-02-05 summary:The model of cognition developed in (Smolensky and Legendre, 2006) seeks tounify two levels of description of the cognitive process: the connectionist andthe symbolic. The theory developed brings together these two levels into theIntegrated Connectionist/Symbolic Cognitive architecture (ICS). Clark andPulman (2007) draw a parallel with semantics where meaning may be modelled onboth distributional and symbolic levels, developed by Coecke et al, 2010 intothe Distributional Compositional (DisCo) model of meaning. In the current work,we revisit Smolensky and Legendre (S&L)'s model. We describe the DisCoframework, summarise the key ideas in S&L's architecture, and describe howtheir description of harmony as a graded measure of grammaticality may beapplied in the DisCo model.
arxiv-15900-38 | Utilização de Grafos e Matriz de Similaridade na Sumarização Automática de Documentos Baseada em Extração de Frases | http://arxiv.org/pdf/1602.02047v1.pdf | author:Elvys Linhares Pontes category:cs.CL cs.IR published:2016-02-05 summary:The internet increased the amount of information available. However, thereading and understanding of this information are costly tasks. In thisscenario, the Natural Language Processing (NLP) applications enable veryimportant solutions, highlighting the Automatic Text Summarization (ATS), whichproduce a summary from one or more source texts. Automatically summarizing oneor more texts, however, is a complex task because of the difficulties inherentto the analysis and generation of this summary. This master's thesis describesthe main techniques and methodologies (NLP and heuristics) to generatesummaries. We have also addressed and proposed some heuristics based on graphsand similarity matrix to measure the relevance of judgments and to generatesummaries by extracting sentences. We used the multiple languages (English,French and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA(French) corpus to evaluate the developped systems. The results obtained werequite interesting.
arxiv-15900-39 | On Column Selection in Approximate Kernel Canonical Correlation Analysis | http://arxiv.org/pdf/1602.02172v1.pdf | author:Weiran Wang category:cs.LG stat.ML published:2016-02-05 summary:We study the problem of column selection in large-scale kernel canonicalcorrelation analysis (KCCA) using the Nystr\"om approximation, where oneapproximates two positive semi-definite kernel matrices using "landmark" pointsfrom the training set. When building low-rank kernel approximations in KCCA,previous work mostly samples the landmarks uniformly at random from thetraining set. We propose novel strategies for sampling the landmarksnon-uniformly based on a version of statistical leverage scores recentlydeveloped for kernel ridge regression. We study the approximation accuracy ofthe proposed non-uniform sampling strategy, develop an incremental algorithmthat explores the path of approximation ranks and facilitates efficient modelselection, and derive the kernel stability of out-of-sample mapping for ourmethod. Experimental results on both synthetic and real-world datasetsdemonstrate the promise of our method.
arxiv-15900-40 | Efficient Multi-view Performance Capture of Fine-Scale Surface Detail | http://arxiv.org/pdf/1602.02023v1.pdf | author:Nadia Robertini, Edilson De Aguiar, Thomas Helten, Christian Theobalt category:cs.CV cs.GR published:2016-02-05 summary:We present a new effective way for performance capture of deforming mesheswith fine-scale time-varying surface detail from multi-view video. Our methodbuilds up on coarse 4D surface reconstructions, as obtained with commonly usedtemplate-based methods. As they only capture models of coarse-to-medium scaledetail, fine scale deformation detail is often done in a second pass by usingstereo constraints, features, or shading-based refinement. In this paper, wepropose a new effective and stable solution to this second step. Our frameworkcreates an implicit representation of the deformable mesh using a densecollection of 3D Gaussian functions on the surface, and a set of 2D Gaussiansfor the images. The fine scale deformation of all mesh vertices that maximizesphoto-consistency can be efficiently found by densely optimizing a newmodel-to-image consistency energy on all vertex positions. A principaladvantage is that our problem formulation yields a smooth closed form energywith implicit occlusion handling and analytic derivatives. Error-pronecorrespondence finding, or discrete sampling of surface displacement values arealso not needed. We show several reconstructions of human subjects wearingloose clothing, and we qualitatively and quantitatively show that we robustlycapture more detail than related methods.
arxiv-15900-41 | Preoperative Volume Determination for Pituitary Adenoma | http://arxiv.org/pdf/1602.02022v1.pdf | author:Dzenan Zukic, Jan Egger, Miriam H. A. Bauer, Daniela Kuhnt, Barbara Carl, Bernd Freisleben, Andreas Kolb, Christopher Nimsky category:cs.CV cs.CG cs.GR published:2016-02-05 summary:The most common sellar lesion is the pituitary adenoma, and sellar tumors areapproximately 10-15% of all intracranial neoplasms. Manual slice-by-slicesegmentation takes quite some time that can be reduced by using the appropriatealgorithms. In this contribution, we present a segmentation method forpituitary adenoma. The method is based on an algorithm that we have appliedrecently to segmenting glioblastoma multiforme. A modification of this schemeis used for adenoma segmentation that is much harder to perform, due to lack ofcontrast-enhanced boundaries. In our experimental evaluation, neurosurgeonsperformed manual slice-by-slice segmentation of ten magnetic resonance imaging(MRI) cases. The segmentations were compared to the segmentation results of theproposed method using the Dice Similarity Coefficient (DSC). The average DSCfor all datasets was 75.92% +/- 7.24%. A manual segmentation took about fourminutes and our algorithm required about one second.
arxiv-15900-42 | Compressive Spectral Clustering | http://arxiv.org/pdf/1602.02018v1.pdf | author:Nicolas Tremblay, Gilles Puy, Remi Gribonval, Pierre Vandergheynst category:cs.DS cs.LG stat.ML published:2016-02-05 summary:Spectral clustering has become a popular technique due to its highperformance in many contexts. It comprises three main steps: create asimilarity graph between N objects to cluster, compute the first k eigenvectorsof its Laplacian matrix to define a feature vector for each object, and runk-means on these features to separate objects into k classes. Each of thesethree steps becomes computationally intensive for large N and/or k. We proposeto speed up the last two steps based on recent results in the emerging field ofgraph signal processing: graph filtering of random signals, and random samplingof bandlimited graph signals. We prove that our method, with a gain incomputation time that can reach several orders of magnitude, is in fact anapproximation of spectral clustering, for which we are able to control theerror. We test the performance of our method on artificial and real-worldnetwork data.
arxiv-15900-43 | Convex Relaxation Regression: Black-Box Optimization of Smooth Functions by Learning Their Convex Envelopes | http://arxiv.org/pdf/1602.02191v3.pdf | author:Mohammad Gheshlaghi Azar, Eva Dyer, Konrad Kording category:stat.ML cs.LG published:2016-02-05 summary:Finding efficient and provable methods to solve non-convex optimizationproblems is an outstanding challenge in machine learning and optimizationtheory. A popular approach used to tackle non-convex problems is to use convexrelaxation techniques to find a convex surrogate for the problem.Unfortunately, convex relaxations typically must be found on aproblem-by-problem basis. Thus, providing a general-purpose strategy toestimate a convex relaxation would have a wide reaching impact. Here, weintroduce Convex Relaxation Regression (CoRR), an approach for learning convexrelaxations for a class of smooth functions. The main idea behind our approachis to estimate the convex envelope of a function $f$ by evaluating $f$ at a setof $T$ random points and then fitting a convex function to these functionevaluations. We prove that with probability greater than $1-\delta$, thesolution of our algorithm converges to the global optimizer of $f$ with error$\mathcal{O} \Big( \big(\frac{\log(1/\delta) }{T} \big)^{\alpha} \Big)$ forsome $\alpha> 0$. Our approach enables the use of convex optimization tools tosolve a class of non-convex optimization problems.
arxiv-15900-44 | Compressive PCA for Low-Rank Matrices on Graphs | http://arxiv.org/pdf/1602.02070v3.pdf | author:Nauman Shahid, Nathanael Perraudin, Gilles Puy, Pierre Vandergheynst category:cs.LG published:2016-02-05 summary:We introduce a novel framework for an approximate recovery of data matriceswhich are low-rank on graphs, from sampled measurements. The rows and columnsof such matrices belong to the span of the first few eigenvectors of the graphsconstructed between their rows and columns. We leverage this property torecover the non-linear low-rank structures efficiently from sampled datameasurements, with a low cost (linear in $n$). First, a Resrtricted IsometryProperty (RIP) condition is introduced for efficient uniform sampling of therows and columns of such matrices based on the cumulative coherence of grapheigenvectors. Secondly, a state-of-the-art fast low-rank recovery method issuggested for the sampled data. Finally, several efficient, parallel andparameter-free decoders are presented along with their theoretical analysis fordecoding the low-rank and cluster indicators for the full data matrix. Thus, weovercome the computational limitations of the standard \textit{linear} low-rankrecovery methods for big datasets. Our method can also be seen as a major steptowards efficient recovery of non-linear low-rank structures. On a single coremachine, our method gains a speed up of $p^2/k$ over Robust PCA, where $k \llp$ is the subspace dimension. Numerically, we can recover a low-rank matrix ofsize $10304 \times 1000$ in 15 secs, which is 100 times faster than Robust PCA.
arxiv-15900-45 | Computing with hardware neurons: spiking or classical? Perspectives of applied Spiking Neural Networks from the hardware side | http://arxiv.org/pdf/1602.02009v2.pdf | author:Sergei Dytckov, Masoud Daneshtalab category:cs.NE published:2016-02-05 summary:While classical neural networks take a position of a leading method in themachine learning community, spiking neuromorphic systems bring attention andlarge projects in neuroscience. Spiking neural networks were shown to be ableto substitute networks of classical neurons in applied tasks. This workexplores recent hardware designs focusing on perspective applications (likeconvolutional neural networks) for both neuron types from the energy efficiencyside to analyse whether there is a possibility for spiking neuromorphichardware to grow up for a wider use. Our comparison shows that spiking hardwareis at least on the same level of energy efficiency or even higher thannon-spiking on a level of basic operations. However, on a system level, spikingsystems are outmatched and consume much more energy due to inefficient datarepresentation with a long series of spikes. If spike-driven applications,minimizing an amount of spikes, are developed, spiking neural systems may reachthe energy efficiency level of classical neural systems. However, in the nearfuture, both type of neuromorphic systems may benefit from emerging memorytechnologies, minimizing the energy consumption of computation and memory forboth neuron types. That would make infrastructure and data transfer energydominant on the system level. We expect that spiking neurons have somebenefits, which would allow achieving better energy results. Still the problemof an amount of spikes will still be the major bottleneck for spiking hardwaresystems.
arxiv-15900-46 | Automatic and Quantitative evaluation of attribute discovery methods | http://arxiv.org/pdf/1602.01940v1.pdf | author:Liangchen Liu, Arnold Wiliem, Shaokang Chen, Brian C. Lovell category:cs.CV published:2016-02-05 summary:Many automatic attribute discovery methods have been developed to extract aset of visual attributes from images for various tasks. However, despite goodperformance in some image classification tasks, it is difficult to evaluatewhether these methods discover meaningful attributes and which one is the bestto find the attributes for image descriptions. An intuitive way to evaluatethis is to manually verify whether consistent identifiable visual conceptsexist to distinguish between positive and negative images of an attribute. Thismanual checking is tedious, labor intensive and expensive and it is very hardto get quantitative comparisons between different methods. In this work, wetackle this problem by proposing an attribute meaningfulness metric, that canperform automatic evaluation on the meaningfulness of attribute sets as well asachieving quantitative comparisons. We apply our proposed metric to recentautomatic attribute discovery methods and popular hashing methods on threeattribute datasets. A user study is also conducted to validate theeffectiveness of the metric. In our evaluation, we gleaned some insights thatcould be beneficial in developing automatic attribute discovery methods togenerate meaningful attributes. To the best of our knowledge, this is the firstwork to quantitatively measure the semantic content of automatically discoveredattributes.
arxiv-15900-47 | Fantastic 4 system for NIST 2015 Language Recognition Evaluation | http://arxiv.org/pdf/1602.01929v1.pdf | author:Kong Aik Lee, Ville Hautamäki, Anthony Larcher, Wei Rao, Hanwu Sun, Trung Hieu Nguyen, Guangsen Wang, Aleksandr Sizov, Ivan Kukanov, Amir Poorjam, Trung Ngo Trong, Xiong Xiao, Cheng-Lin Xu, Hai-Hua Xu, Bin Ma, Haizhou Li, Sylvain Meignier category:cs.CL published:2016-02-05 summary:This article describes the systems jointly submitted by Institute forInfocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\'e du Maine(LIUM), Nanyang Technology University (NTU) and the University of EasternFinland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). Thesubmitted system is a fusion of nine sub-systems based on i-vectors extractedfrom different types of features. Given the i-vectors, several classifiers areadopted for the language detection task including support vector machines(SVM), multi-class logistic regression (MCLR), Probabilistic LinearDiscriminant Analysis (PLDA) and Deep Neural Networks (DNN).
arxiv-15900-48 | On Feature based Delaunay Triangulation for Palmprint Recognition | http://arxiv.org/pdf/1602.01927v1.pdf | author:Zanobya N. Khan, Rashid Jalal Qureshi, Jamil Ahmad category:cs.CV published:2016-02-05 summary:Authentication of individuals via palmprint based biometric system isbecoming very popular due to its reliability as it contains unique and stablefeatures. In this paper, we present a novel approach for palmprint recognitionand its representation. To extract the palm lines, local thresholding techniqueNiblack binarization algorithm is adopted. The endpoints of these lines aredetermined and a connection is created among them using the Delaunaytriangulation thereby generating a distinct topological structure of eachpalmprint. Next, we extract different geometric as well as quantitativefeatures from the triangles of the Delaunay triangulation that assist inidentifying different individuals. To ensure that the proposed approach isinvariant to rotation and scaling, features were made relative to topologicaland geometrical structure of the palmprint. The similarity of the twopalmprints is computed using the weighted sum approach and compared with thek-nearest neighbor. The experimental results obtained reflect the effectivenessof the proposed approach to discriminate between different palmprint images andthus achieved a recognition rate of 90% over large databases.
arxiv-15900-49 | Massively Multilingual Word Embeddings | http://arxiv.org/pdf/1602.01925v1.pdf | author:Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, Noah A. Smith category:cs.CL published:2016-02-05 summary:We introduce new methods for estimating and evaluating embeddings of wordsfrom dozens of languages in a single shared embedding space. Our estimationmethods, multiCluster and multiCCA, use dictionaries and monolingual data; theydo not require parallel data. Our new evaluation method, multiQVEC+, is shownto correlate better than previous ones with two downstream tasks (textcategorization and parsing). On this evaluation and others, our estimationmethods outperform existing ones. We also describe a web portal for evaluationthat will facilitate further research in this area, along with open-sourcereleases of all our methods.
arxiv-15900-50 | Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model | http://arxiv.org/pdf/1602.01921v1.pdf | author:Haanvid Lee, Minju Jung, Jun Tani category:cs.CV cs.AI cs.LG published:2016-02-05 summary:The current paper proposes a novel dynamic neural network model, multiplespatio-temporal scales recurrent neural network (MSTRNN) used forcategorization of complex human action pattern in video image. The MSTRNN hasbeen developed by newly introducing recurrent connectivity to a prior-proposedmodel, multiple spatio-temporal scales neural network (MSTNN) [1] such that themodel can learn to extract latent spatio-temporal structures more effectivelyby developing adequate recurrent contextual dynamics. The MSTRNN was evaluatedby conducting a set of simulation experiments on learning to categorize humanaction visual patterns. The first experiment on categorizing a set oflong-concatenated human movement patterns showed that MSTRNN outperforms MSTNNin the capability of learning to extract long-ranged correlation in videoimage. The second experiment on categorizing a set of object-directed actionsshowed that the MSTRNN can learn to extract structural relationship betweenactions and directed-objects. Our analysis on the characteristics ofmiscategorization in both cases of object-directed action and pantomime actionsindicated that the model network developed the categorical memories byorganizing relational structure among them. Development of such relationalstructure is considered to be beneficial for gaining generalization incategorization.
arxiv-15900-51 | Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping Clustering | http://arxiv.org/pdf/1602.01910v1.pdf | author:Yangyang Hou, Joyce Jiyoung Whang, David F. Gleich, Inderjit S. Dhillon category:cs.LG published:2016-02-05 summary:Clustering is one of the most fundamental and important tasks in data mining.Traditional clustering algorithms, such as K-means, assign every data point toexactly one cluster. However, in real-world datasets, the clusters may overlapwith each other. Furthermore, often, there are outliers that should not belongto any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive,Overlapping K-Means) objective as a way to address both issues in an integratedfashion. Optimizing this discrete objective is NP-hard, and even though thereis a convex relaxation of the objective, straightforward convex optimizationapproaches are too expensive for large datasets. A practical alternative is touse a low-rank factorization of the solution matrix in the convex formulation.The resulting optimization problem is non-convex, and we can locally optimizethe objective function using an augmented Lagrangian method. In this paper, weconsider two fast multiplier methods to accelerate the convergence of anaugmented Lagrangian scheme: a proximal method of multipliers and analternating direction method of multipliers (ADMM). For the proximal augmentedLagrangian or proximal method of multipliers, we show a convergence result forthe non-convex case with bound-constrained subproblems. These methods are up to13 times faster---with no change in quality---compared with a standardaugmented Lagrangian method on problems with over 10,000 variables and bringruntimes down from over an hour to around 5 minutes.
arxiv-15900-52 | Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features | http://arxiv.org/pdf/1602.01895v1.pdf | author:Shijian Tang, Song Han category:cs.CV cs.CL cs.LG published:2016-02-05 summary:Generating natural language descriptions for images is a challenging task.The traditional way is to use the convolutional neural network (CNN) to extractimage features, followed by recurrent neural network (RNN) to generatesentences. In this paper, we present a new model that added memory cells togate the feeding of image features to the deep neural network. The intuition isenabling our model to memorize how much information from images should be fedat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showedthat our model outperforms other state-of-the-art models with higher BLEUscores.
arxiv-15900-53 | Search Tracker: Human-derived object tracking in-the-wild through large-scale search and retrieval | http://arxiv.org/pdf/1602.01890v1.pdf | author:Archith J. Bency, S. Karthikeyan, Carter De Leo, Santhoshkumar Sunderrajan, B. S. Manjunath category:cs.CV cs.MM published:2016-02-05 summary:Humans use context and scene knowledge to easily localize moving objects inconditions of complex illumination changes, scene clutter and occlusions. Inthis paper, we present a method to leverage human knowledge in the form ofannotated video libraries in a novel search and retrieval based setting totrack objects in unseen video sequences. For every video sequence, a documentthat represents motion information is generated. Documents of the unseen videoare queried against the library at multiple scales to find videos with similarmotion characteristics. This provides us with coarse localization of objects inthe unseen video. We further adapt these retrieved object locations to the newvideo using an efficient warping scheme. The proposed method is validated onin-the-wild video surveillance datasets where we outperform state-of-the-artappearance-based trackers. We also introduce a new challenging dataset withcomplex object appearance changes.
arxiv-15900-54 | NeRD: a Neural Response Divergence Approach to Visual Salience Detection | http://arxiv.org/pdf/1602.01728v1.pdf | author:M. J. Shafiee, P. Siva, C. Scharfenberger, P. Fieguth, A. Wong category:cs.CV published:2016-02-04 summary:In this paper, a novel approach to visual salience detection via NeuralResponse Divergence (NeRD) is proposed, where synaptic portions of deep neuralnetworks, previously trained for complex object recognition, are leveraged tocompute low level cues that can be used to compute image regiondistinctiveness. Based on this concept , an efficient visual salience detectionframework is proposed using deep convolutional StochasticNets. Experimentalresults using CSSD and MSRA10k natural image datasets show that the proposedNeRD approach can achieve improved performance when compared tostate-of-the-art image saliency approaches, while the attaining lowcomputational complexity necessary for near-real-time computer visionapplications.
arxiv-15900-55 | Correntropy Maximization via ADMM - Application to Robust Hyperspectral Unmixing | http://arxiv.org/pdf/1602.01729v1.pdf | author:Fei Zhu, Abderrahim Halimi, Paul Honeine, Badong Chen, Nanning Zheng category:stat.ML cs.CV cs.NE published:2016-02-04 summary:In hyperspectral images, some spectral bands suffer from low signal-to-noiseratio due to noisy acquisition and atmospheric effects, thus requiring robusttechniques for the unmixing problem. This paper presents a robust supervisedspectral unmixing approach for hyperspectral images. The robustness is achievedby writing the unmixing problem as the maximization of the correntropycriterion subject to the most commonly used constraints. Two unmixing problemsare derived: the first problem considers the fully-constrained unmixing, withboth the non-negativity and sum-to-one constraints, while the second one dealswith the non-negativity and the sparsity-promoting of the abundances. Thecorresponding optimization problems are solved efficiently using an alternatingdirection method of multipliers (ADMM) approach. Experiments on synthetic andreal hyperspectral images validate the performance of the proposed algorithmsfor different scenarios, demonstrating that the correntropy-based unmixing isrobust to outlier bands.
arxiv-15900-56 | Appearance Based Robot and Human Activity Recognition System | http://arxiv.org/pdf/1602.01608v2.pdf | author:Bappaditya Mandal category:cs.RO cs.CV published:2016-02-04 summary:In this work, we present an appearance based human activity recognitionsystem. It uses background modeling to segment the foreground object andextracts useful discriminative features for representing activities performedby humans and robots. Subspace based method like principal component analysisis used to extract low dimensional features from large voluminous activityimages. These low dimensional features are then used to classify an activity.An apparatus is designed using a webcam, which watches a robot replicating ahuman fall under indoor environment. In this apparatus, a robot performsvarious activities (like walking, bending, moving arms) replicating humans,which also includes a sudden fall. Experimental results on robot performingvarious activities and standard human activity recognition databases show theefficacy of our proposed method.
arxiv-15900-57 | Asynchronous Methods for Deep Reinforcement Learning | http://arxiv.org/pdf/1602.01783v1.pdf | author:Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu category:cs.LG published:2016-02-04 summary:We propose a conceptually simple and lightweight framework for deepreinforcement learning that uses asynchronous gradient descent for optimizationof deep neural network controllers. We present asynchronous variants of fourstandard reinforcement learning algorithms and show that parallelactor-learners have a stabilizing effect on training allowing all four methodsto successfully train neural network controllers. The best performing method,an asynchronous variant of actor-critic, surpasses the current state-of-the-arton the Atari domain while training for half the time on a single multi-core CPUinstead of a GPU. Furthermore, we show that asynchronous actor-critic succeedson a wide variety of continuous motor control problems as well as on a new taskinvolving finding rewards in random 3D mazes using a visual input.
arxiv-15900-58 | Random Feature Maps via a Layered Random Projection (LaRP) Framework for Object Classification | http://arxiv.org/pdf/1602.01818v1.pdf | author:A. G. Chung, M. J. Shafiee, A. Wong category:cs.CV cs.LG stat.ML published:2016-02-04 summary:The approximation of nonlinear kernels via linear feature maps has recentlygained interest due to their applications in reducing the training and testingtime of kernel-based learning algorithms. Current random projection methodsavoid the curse of dimensionality by embedding the nonlinear feature space intoa low dimensional Euclidean space to create nonlinear kernels. We introduce aLayered Random Projection (LaRP) framework, where we model the linear kernelsand nonlinearity separately for increased training efficiency. The proposedLaRP framework was assessed using the MNIST hand-written digits database andthe COIL-100 object database, and showed notable improvement in objectclassification performance relative to other state-of-the-art random projectionmethods.
arxiv-15900-59 | Discovering Neuronal Cell Types and Their Gene Expression Profiles Using a Spatial Point Process Mixture Model | http://arxiv.org/pdf/1602.01889v1.pdf | author:Furong Huang, Animashree Anandkumar, Christian Borgs, Jennifer Chayes, Ernest Fraenkel, Michael Hawrylycz, Ed Lein, Alessandro Ingrosso, Srinivas Turaga category:q-bio.NC stat.ML published:2016-02-04 summary:Cataloging the neuronal cell types that comprise circuitry of individualbrain regions is a major goal of modern neuroscience and the BRAIN initiative.Single-cell RNA sequencing can now be used to measure the gene expressionprofiles of individual neurons and to categorize neurons based on their geneexpression profiles. However this modern tool is labor intensive, has a highcost per cell, and most importantly, does not provide information on spatialdistribution of cell types in specific regions of the brain. We propose acomputational method for inferring the cell types and their gene expressionprofiles through computational analysis of brain-wide single-cell resolution insitu hybridization (ISH) imagery contained in the Allen Brain Atlas (ABA). Wemeasure the spatial distribution of neurons labeled in the ISH image for eachgene and model it as a spatial point process mixture, whose mixture weights aregiven by the cell types which express that gene. By fitting a point processmixture model jointly to the ISH images for about two thousand genes, we inferboth the spatial point process distribution for each cell type and their geneexpression profile. We validate our predictions of cell type-specific geneexpression profiles using single cell RNA sequencing data, recently publishedfor the mouse somatosensory cortex.
arxiv-15900-60 | The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version | http://arxiv.org/pdf/1602.01711v1.pdf | author:Anthony Bagnall, Aaron Bostrom, James Large, Jason Lines category:cs.LG published:2016-02-04 summary:In the last five years there have been a large number of new time seriesclassification algorithms proposed in the literature. These algorithms havebeen evaluated on subsets of the 47 data sets in the University of California,Riverside time series classification archive. The archive has recently beenexpanded to 85 data sets, over half of which have been donated by researchersat the University of East Anglia. Aspects of previous evaluations have madecomparisons between algorithms difficult. For example, several differentprogramming languages have been used, experiments involved a single train/testsplit and some used normalised data whilst others did not. The relaunch of thearchive provides a timely opportunity to thoroughly evaluate algorithms on alarger number of datasets. We have implemented 18 recently proposed algorithmsin a common Java framework and compared them against two standard benchmarkclassifiers (and each other) by performing 100 resampling experiments on eachof the 85 datasets. We use these results to test several hypotheses relating towhether the algorithms are significantly more accurate than the benchmarks andeach other. Our results indicate that only 9 of these algorithms aresignificantly more accurate than both benchmarks and that one classifier, theCollective of Transformation Ensembles, is significantly more accurate than allof the others. All of our experiments and results are reproducible: we releaseall of our code, results and experimental details and we hope these experimentsform the basis for more rigorous testing of new algorithms in the future.
arxiv-15900-61 | Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors | http://arxiv.org/pdf/1602.01601v1.pdf | author:Johanna Carvajal, Chris McCool, Brian Lovell, Conrad Sanderson category:cs.CV published:2016-02-04 summary:We propose a hierarchical approach to multi-action recognition that performsjoint classification and segmentation. A given video (containing severalconsecutive actions) is processed via a sequence of overlapping temporalwindows. Each frame in a temporal window is represented through selectivelow-level spatio-temporal features which efficiently capture relevant localdynamics. Features from each window are represented as a Fisher vector, whichcaptures first and second order statistics. Instead of directly classifyingeach Fisher vector, it is converted into a vector of class probabilities. Thefinal classification decision for each frame is then obtained by integratingthe class probabilities at the frame level, which exploits the overlapping ofthe temporal windows. Experiments were performed on two datasets: s-KTH (astitched version of the KTH dataset to simulate multi-actions), and thechallenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves anaccuracy of 85.0%, significantly outperforming two recent approaches based onGMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, theproposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMMapproaches which obtained 33.7% and 38.4%, respectively. Furthermore, theproposed system is on average 40 times faster than the GMM based approach.
arxiv-15900-62 | An ensemble diversity approach to supervised binary hashing | http://arxiv.org/pdf/1602.01557v1.pdf | author:Miguel Á. Carreira-Perpiñán, Ramin Raziperchikolaei category:cs.LG cs.CV math.OC stat.ML published:2016-02-04 summary:Binary hashing is a well-known approach for fast approximate nearest-neighborsearch in information retrieval. Much work has focused on affinity-basedobjective functions involving the hash functions or binary codes. Theseobjective functions encode neighborhood information between data points and areoften inspired by manifold learning algorithms. They ensure that the hashfunctions differ from each other through constraints or penalty terms thatencourage codes to be orthogonal or dissimilar across bits, but this couplesthe binary variables and complicates the already difficult optimization. Wepropose a much simpler approach: we train each hash function (or bit)independently from each other, but introduce diversity among them usingtechniques from classifier ensembles. Surprisingly, we find that not only isthis faster and trivially parallelizable, but it also improves over the morecomplex, coupled objective function, and achieves state-of-the-art precisionand recall in experiments with image retrieval.
arxiv-15900-63 | Complex Networks of Words in Fables | http://arxiv.org/pdf/1602.04853v1.pdf | author:Yurij Holovatch, Vasyl Palchykov category:physics.soc-ph cs.CL published:2016-02-04 summary:In this chapter we give an overview of the application of complex networktheory to quantify some properties of language. Our study is based on twofables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists oftwo parts: the analysis of frequency-rank distributions of words and theapplication of complex-network theory. The first part shows that the text sizesare sufficiently large to observe statistical properties. This supports theirselection for the analysis of typical properties of the language networks inthe second part of the chapter. In describing language as a complex network,while words are usually associated with nodes, there is more variability in thechoice of links and different representations result in different networks.Here, we examine a number of such representations of the language network andperform a comparative analysis of their characteristics. Our results suggestthat, irrespective of link representation, the Ukrainian language network usedin the selected fables is a strongly correlated, scale-free, small world. Wediscuss how such empirical approaches may help form a useful basis for atheoretical description of language evolution and how they may be used inanalyses of other textual narratives.
arxiv-15900-64 | Many Languages, One Parser | http://arxiv.org/pdf/1602.01595v3.pdf | author:Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, Noah A. Smith category:cs.CL published:2016-02-04 summary:We train one multilingual model for dependency parsing and use it to parsesentences in several languages. The parsing model uses (i) multilingual wordclusters and embeddings; (ii) token-level language information; and (iii)language-specific features (fine-grained POS tags). This input representationenables the parser not only to parse effectively in multiple languages, butalso to generalize across languages based on linguistic universals andtypological similarities, making it more effective to learn from limitedannotations. Our parser's performance compares favorably to strong baselines ina range of data scenarios, including when the target language has a largetreebank, a small treebank, or no treebank for training.
arxiv-15900-65 | Towards Better Exploiting Convolutional Neural Networks for Remote Sensing Scene Classification | http://arxiv.org/pdf/1602.01517v1.pdf | author:Keiller Nogueira, Otávio A. B. Penatti, Jefersson A. dos Santos category:cs.CV published:2016-02-04 summary:We present an analysis of three possible strategies for exploiting the powerof existing convolutional neural networks (ConvNets) in different scenariosfrom the ones they were trained: full training, fine tuning, and using ConvNetsas feature extractors. In many applications, especially including remotesensing, it is not feasible to fully design and train a new ConvNet, as thisusually requires a considerable amount of labeled data and demands highcomputational costs. Therefore, it is important to understand how to obtain thebest profit from existing ConvNets. We perform experiments with six popularConvNets using three remote sensing datasets. We also compare ConvNets in eachstrategy with existing descriptors and with state-of-the-art baselines. Resultspoint that fine tuning tends to be the best performing strategy. In fact, usingthe features from the fine-tuned ConvNet with linear SVM obtains the bestresults. We also achieved state-of-the-art results for the three datasets used.
arxiv-15900-66 | Minimizing the Maximal Loss: How and Why? | http://arxiv.org/pdf/1602.01690v1.pdf | author:Shai Shalev-Shwartz, Yonatan Wexler category:cs.LG published:2016-02-04 summary:A commonly used learning rule is to approximately minimize the \emph{average}loss over the training set. Other learning algorithms, such as AdaBoost andhard-SVM, aim at minimizing the \emph{maximal} loss over the training set. Theaverage loss is more popular, particularly in deep learning, due to three mainreasons. First, it can be conveniently minimized using online algorithms, thatprocess few examples at each iteration. Second, it is often argued that thereis no sense to minimize the loss on the training set too much, as it will notbe reflected in the generalization loss. Last, the maximal loss is not robustto outliers. In this paper we describe and analyze an algorithm that canconvert any online algorithm to a minimizer of the maximal loss. We prove thatin some situations better accuracy on the training set is crucial to obtaingood performance on unseen examples. Last, we propose robust versions of theapproach that can handle outliers.
arxiv-15900-67 | EIE: Efficient Inference Engine on Compressed Deep Neural Network | http://arxiv.org/pdf/1602.01528v2.pdf | author:Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, William J. Dally category:cs.CV cs.AR published:2016-02-04 summary:State-of-the-art deep neural networks (DNNs) have hundreds of millions ofconnections and are both computationally and memory intensive, making themdifficult to deploy on embedded systems with limited hardware resources andpower budgets. While custom hardware helps the computation, fetching weightsfrom DRAM is two orders of magnitude more expensive than ALU operations, anddominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved bypruning the redundant connections and having multiple connections share thesame weight. We propose an energy efficient inference engine (EIE) thatperforms inference on this compressed network model and accelerates theresulting sparse matrix-vector multiplication with weight sharing. Going fromDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared toCPU and GPU implementations of the same DNN without compression. EIE has aprocessing power of 102GOPS/s working directly on a compressed network,corresponding to 3TOPS/s on an uncompressed network, and processes FC layers ofAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is24,000x and 3,400x more energy efficient than a CPU and GPU respectively.Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energyefficiency and area efficiency.
arxiv-15900-68 | A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling | http://arxiv.org/pdf/1602.01576v1.pdf | author:Anantharaman Palacod category:cs.CL cs.AI published:2016-02-04 summary:Statistical language models are central to many applications that usesemantics. Recurrent Neural Networks (RNN) are known to produce state of theart results for language modelling, outperforming their traditional n-gramcounterparts in many cases. To generate a probability distribution across avocabulary, these models require a softmax output layer that linearly increasesin size with the size of the vocabulary. Large vocabularies need acommensurately large softmax layer and training them on typical laptops/PCsrequires significant time and machine resources. In this paper we present a newtechnique for implementing RNN based large vocabulary language models thatsubstantially speeds up computation while optimally using the limited memoryresources. Our technique, while building on the notion of factorizing theoutput layer by having multiple output layers, improves on the earlier work bysubstantially optimizing on the individual output layer size and alsoeliminating the need for a multistep prediction process.
arxiv-15900-69 | A semi-automatic computer-aided method for surgical template design | http://arxiv.org/pdf/1602.01644v1.pdf | author:Xiaojun Chen, Lu Xu, Yue Yang, Jan Egger category:cs.GR cs.CG cs.CV published:2016-02-04 summary:This paper presents a generalized integrated framework of semi-automaticsurgical template design. Several algorithms were implemented including themesh segmentation, offset surface generation, collision detection, ruledsurface generation, etc., and a special software named TemDesigner wasdeveloped. With a simple user interface, a customized template can be semi-automatically designed according to the preoperative plan. Firstly, meshsegmentation with signed scalar of vertex is utilized to partition the innersurface from the input surface mesh based on the indicated point loop. Then,the offset surface of the inner surface is obtained through contouring thedistance field of the inner surface, and segmented to generate the outersurface. Ruled surface is employed to connect inner and outer surfaces.Finally, drilling tubes are generated according to the preoperative planthrough collision detection and merging. It has been applied to the templatedesign for various kinds of surgeries, including oral implantology, cervicalpedicle screw insertion, iliosacral screw insertion and osteotomy,demonstrating the efficiency, functionality and generality of our method.
arxiv-15900-70 | Fundamental Limits in Multi-image Alignment | http://arxiv.org/pdf/1602.01541v1.pdf | author:Cecilia Aguerrebere, Mauricio Delbracio, Alberto Bartesaghi, Guillermo Sapiro category:cs.CV published:2016-02-04 summary:The performance of multi-image alignment, bringing different images into onecoordinate system, is critical in many applications with varied signal-to-noiseratio (SNR) conditions. A great amount of effort is being invested intodeveloping methods to solve this problem. Several important questions thusarise, including: Which are the fundamental limits in multi-image alignmentperformance? Does having access to more images improve the alignment?Theoretical bounds provide a fundamental benchmark to compare methods and canhelp establish whether improvements can be made. In this work, we tackle theproblem of finding the performance limits in image registration when multipleshifted and noisy observations are available. We derive and analyze theCram\'er-Rao and Ziv-Zakai lower bounds under different statistical models forthe underlying image. The accuracy of the derived bounds is experimentallyassessed through a comparison to the maximum likelihood estimator. We show theexistence of different behavior zones depending on the difficulty level of theproblem, given by the SNR conditions of the input images. We find thatincreasing the number of images is only useful below a certain SNR threshold,above which the pairwise MLE estimation proves to be optimal. The analysis wepresent here brings further insight into the fundamental limitations of themulti-image alignment problem.
arxiv-15900-71 | A Generalised Quantifier Theory of Natural Language in Categorical Compositional Distributional Semantics with Bialgebras | http://arxiv.org/pdf/1602.01635v1.pdf | author:Jules Hedges, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT I.2.7 published:2016-02-04 summary:Categorical compositional distributional semantics is a model of naturallanguage; it combines the statistical vector space models of words with thecompositional models of grammar. We formalise in this model the generalisedquantifier theory of natural language, due to Barwise and Cooper. Theunderlying setting is a compact closed category with bialgebras. We start froma generative grammar formalisation and develop an abstract categoricalcompositional semantics for it, then instantiate the abstract setting to setsand relations and to finite dimensional vector spaces and linear maps. We provethe equivalence of the relational instantiation to the truth theoreticsemantics of generalized quantifiers. The vector space instantiation formalisesthe statistical usages of words and enables us to, for the first time, reasonabout quantified phrases and sentences compositionally in distributionalsemantics.
arxiv-15900-72 | Comparative Evaluation of Action Recognition Methods via Riemannian Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions | http://arxiv.org/pdf/1602.01599v1.pdf | author:Johanna Carvajal, Arnold Wiliem, Chris McCool, Brian Lovell, Conrad Sanderson category:cs.CV published:2016-02-04 summary:We present a comparative evaluation of various techniques for actionrecognition while keeping as many variables as possible controlled. We employtwo categories of Riemannian manifolds: symmetric positive definite matricesand linear subspaces. For both categories we use their corresponding nearestneighbour classifiers, kernels, and recent kernelised sparse representations.We compare against traditional action recognition techniques based on Gaussianmixture models and Fisher vectors (FVs). We evaluate these action recognitiontechniques under ideal conditions, as well as their sensitivity in morechallenging conditions (variations in scale and translation). Despite recentadvancements for handling manifolds, manifold based techniques obtain thelowest performance and their kernel representations are more unstable in thepresence of challenging conditions. The FV approach obtains the highestaccuracy under ideal conditions. Moreover, FV best deals with moderate scaleand translation changes.
arxiv-15900-73 | Face Attribute Prediction with classification CNN | http://arxiv.org/pdf/1602.01827v2.pdf | author:Yang Zhong, Josephine Sullivan, Haibo Li category:cs.CV published:2016-02-04 summary:Predicting facial attributes from faces in the wild is very challenging dueto pose and lighting variations in the real world. The key to this problem isto build proper feature representations to cope with these unfavorableconditions. Given the success of convolutional neural network (CNN) in imageclassification, the high-level CNN feature as an intuitive and reasonablechoice has been widely utilized for this problem. In this paper, however, weconsider the mid-level CNN features as an alternative to the high-level onesfor attribute prediction. This is based on the observation that face attributesare different: some of them are locally oriented while others are globallydefined. Our investigations reveal that the mid-level deep representationsoutperform the prediction accuracy achieved by the high-level abstractions. Wedemonstrate that the mid-level representations achieve state-of-the-artprediction performance on CelebA and LFWA datasets. Our investigations alsoshow that by utilizing the mid-level representations one can employ a singledeep network to achieve both face recognition and attribute prediction.
arxiv-15900-74 | Long-term Planning by Short-term Prediction | http://arxiv.org/pdf/1602.01580v1.pdf | author:Shai Shalev-Shwartz, Nir Ben-Zrihem, Aviad Cohen, Amnon Shashua category:cs.LG published:2016-02-04 summary:We consider planning problems, that often arise in autonomous drivingapplications, in which an agent should decide on immediate actions so as tooptimize a long term objective. For example, when a car tries to merge in aroundabout it should decide on an immediate acceleration/braking command, whilethe long term effect of the command is the success/failure of the merge. Suchproblems are characterized by continuous state and action spaces, and byinteraction with multiple agents, whose behavior can be adversarial. We arguethat dual versions of the MDP framework (that depend on the value function andthe $Q$ function) are problematic for autonomous driving applications due tothe non Markovian of the natural state space representation, and due to thecontinuous state and action spaces. We propose to tackle the planning task bydecomposing the problem into two phases: First, we apply supervised learningfor predicting the near future based on the present. We require that thepredictor will be differentiable with respect to the representation of thepresent. Second, we model a full trajectory of the agent using a recurrentneural network, where unexplained factors are modeled as (additive) inputnodes. This allows us to solve the long-term planning problem using supervisedlearning techniques and direct optimization over the recurrent neural network.Our approach enables us to learn robust policies by incorporating adversarialelements to the environment.
arxiv-15900-75 | Risk estimation for high-dimensional lasso regression | http://arxiv.org/pdf/1602.01522v1.pdf | author:Darren Homrighausen, Daniel J. McDonald category:stat.ME stat.ML published:2016-02-04 summary:In high-dimensional estimation, analysts are faced with more parameters $p$than available observations $n$, and asymptotic analysis of performance allowsthe ratio $p/n\rightarrow \infty$. This situation makes regularization bothnecessary and desirable in order for estimators to possess theoreticalguarantees. However, the amount of regularization, often determined by one ormore tuning parameters, is integral to achieving good performance. In practice,choosing the tuning parameter is done through resampling methods (e.g.cross-validation), generalized information criteria, or reformulating theoptimization problem (e.g. square-root lasso or scaled sparse regression). Eachof these techniques comes with varying levels of theoretical guarantee for thelow- or high-dimensional regimes. However, there are some notable deficienciesin the literature. The theory, and sometimes practice, of many methods relieson either the knowledge or estimation of the variance parameter, which isdifficult to estimate in high dimensions. In this paper, we provide theoreticalintuition suggesting that some previously proposed approaches based oninformation criteria work poorly in high dimensions. We introduce a suite ofnew risk estimators leveraging the burgeoning literature on high-dimensionalvariance estimation. Finally, we compare our proposal to many existing methodsfor choosing the tuning parameters for lasso regression by providing anextensive simulation to examine their finite sample performance. We find thatour new estimators perform quite well, often better than the existingapproaches across a wide range of simulation conditions and evaluationcriteria.
arxiv-15900-76 | Self-Transfer Learning for Fully Weakly Supervised Object Localization | http://arxiv.org/pdf/1602.01625v1.pdf | author:Sangheum Hwang, Hyo-Eun Kim category:cs.CV published:2016-02-04 summary:Recent advances of deep learning have achieved remarkable performances invarious challenging computer vision tasks. Especially in object localization,deep convolutional neural networks outperform traditional approaches based onextraction of data/task-driven features instead of hand-crafted features.Although location information of region-of-interests (ROIs) gives good priorfor object localization, it requires heavy annotation efforts from humanresources. Thus a weakly supervised framework for object localization isintroduced. The term "weakly" means that this framework only uses image-levellabeled datasets to train a network. With the help of transfer learning whichadopts weight parameters of a pre-trained network, the weakly supervisedlearning framework for object localization performs well because thepre-trained network already has well-trained class-specific features. However,those approaches cannot be used for some applications which do not havepre-trained networks or well-localized large scale images. Medical imageanalysis is a representative among those applications because it is impossibleto obtain such pre-trained networks. In this work, we present a "fully" weaklysupervised framework for object localization ("semi"-weakly is the counterpartwhich uses pre-trained filters for weakly supervised localization) named asself-transfer learning (STL). It jointly optimizes both classification andlocalization networks simultaneously. By controlling a supervision level of thelocalization network, STL helps the localization network focus on correct ROIswithout any types of priors. We evaluate the proposed STL framework using twomedical image datasets, chest X-rays and mammograms, and achieve signiticantlybetter localization performance compared to previous weakly supervisedapproaches.
arxiv-15900-77 | Fpga Based Implementation of Deep Neural Networks Using On-chip Memory Only | http://arxiv.org/pdf/1602.01616v1.pdf | author:Jinhwan Park, Wonyong Sung category:cs.AR cs.NE published:2016-02-04 summary:Deep neural networks (DNNs) demand a very large amount of computation andweight storage, and thus efficient implementation using special purposehardware is highly desired. In this work, we have developed an FPGA basedfixed-point DNN system using only on-chip memory not to access external DRAM.The execution time and energy consumption of the developed system is comparedwith a GPU based implementation. Since the capacity of memory in FPGA islimited, only 3-bit weights are used for this implementation, and trainingbased fixed-point weight optimization is employed. The implementation usingXilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmarkand a phoneme recognition task on TIMIT corpus. The obtained speed is about onequarter of a GPU based implementation and much better than that of a PC basedone. The power consumption is less than 5 Watt at the full speed operationresulting in much higher efficiency compared to GPU based systems.
arxiv-15900-78 | SDCA without Duality, Regularization, and Individual Convexity | http://arxiv.org/pdf/1602.01582v1.pdf | author:Shai Shalev-Shwartz category:cs.LG published:2016-02-04 summary:Stochastic Dual Coordinate Ascent is a popular method for solving regularizedloss minimization for the case of convex losses. We describe variants of SDCAthat do not require explicit regularization and do not rely on duality. Weprove linear convergence rates even if individual loss functions arenon-convex, as long as the expected loss is strongly convex.
arxiv-15900-79 | Visual Tracking via Reliable Memories | http://arxiv.org/pdf/1602.01887v2.pdf | author:Shu Wang, Shaoting Zhang, Wei Liu, Dimitris N. Metaxas category:cs.CV published:2016-02-04 summary:In this paper, we propose a novel visual tracking framework thatintelligently discovers reliable patterns from a wide range of video to resistdrift error for long-term tracking tasks. First, we design a Discrete FourierTransform (DFT) based tracker which is able to exploit a large number oftracked samples while still ensures real-time performance. Second, we propose aclustering method with temporal constraints to explore and memorize consistentpatterns from previous frames, named as reliable memories. By virtue of thismethod, our tracker can utilize uncontaminated information to alleviatedrifting issues. Experimental results show that our tracker performs favorablyagainst other state of-the-art methods on benchmark datasets. Furthermore, itis significantly competent in handling drifts and able to robustly trackchallenging long videos over 4000 frames, while most of others lose track atearly frames.
arxiv-15900-80 | k-variates++: more pluses in the k-means++ | http://arxiv.org/pdf/1602.01198v2.pdf | author:Richard Nock, Raphaël Canyasse, Roksana Boreli, Frank Nielsen category:cs.LG H.3.3; I.5.3 published:2016-02-03 summary:k-means++ seeding has become a de facto standard for hard clusteringalgorithms. In this paper, our first contribution is a two-way generalisationof this seeding, k-variates++, that includes the sampling of general densitiesrather than just a discrete set of Dirac densities anchored at the pointlocations, and a generalisation of the well known Arthur-Vassilvitskii (AV)approximation guarantee, in the form of a bias+variance approximation bound ofthe global optimum. This approximation exhibits a reduced dependency on the"noise" component with respect to the optimal potential --- actuallyapproaching the statistical lower bound. We show that k-variates++ reduces toefficient (biased seeding) clustering algorithms tailored to specificframeworks; these include distributed, streaming and on-line clustering, withdirect approximation results for these algorithms. Finally, we present a novelapplication of k-variates++ to differential privacy. For either the specificframeworks considered here, or for the differential privacy setting, there islittle to no prior results on the direct application of k-means++ and itsapproximation bounds --- state of the art contenders appear to be significantlymore complex and / or display less favorable (approximation) properties. Westress that our algorithms can still be run in cases where there is \textit{no}closed form solution for the population minimizer. We demonstrate theapplicability of our analysis via experimental evaluation on several domainsand settings, displaying competitive performances vs state of the art.
arxiv-15900-81 | Unsupervised Regenerative Learning of Hierarchical Features in Spiking Deep Networks for Object Recognition | http://arxiv.org/pdf/1602.01510v1.pdf | author:Priyadarshini Panda, Kaushik Roy category:cs.NE published:2016-02-03 summary:We present a spike-based unsupervised regenerative learning scheme to trainSpiking Deep Networks (SpikeCNN) for object recognition problems usingbiologically realistic leaky integrate-and-fire neurons. The trainingmethodology is based on the Auto-Encoder learning model wherein thehierarchical network is trained layer wise using the encoder-decoder principle.Regenerative learning uses spike-timing information and inherent latencies toupdate the weights and learn representative levels for each convolutional layerin an unsupervised manner. The features learnt from the final layer in thehierarchy are then fed to an output layer. The output layer is trained withsupervision by showing a fraction of the labeled training dataset and performsthe overall classification of the input. Our proposed methodology yields0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparablewith state-of-the-art results. The proposed methodology also introducessparsity in the hierarchical feature representations on account of event-basedcoding resulting in computationally efficient learning.
arxiv-15900-82 | Using Hadoop for Large Scale Analysis on Twitter: A Technical Report | http://arxiv.org/pdf/1602.01248v1.pdf | author:Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis, Giannis Tzimas category:cs.DB cs.CL cs.IR H.2.4 published:2016-02-03 summary:Sentiment analysis (or opinion mining) on Twitter data has attracted muchattention recently. One of the system's key features, is the immediacy incommunication with other users in an easy, user-friendly and fast way.Consequently, people tend to express their feelings freely, which makes Twitteran ideal source for accumulating a vast amount of opinions towards a widediversity of topics. This amount of information offers huge potential and canbe harnessed to receive the sentiment tendency towards these topics. However,since none can invest an infinite amount of time to read through these tweets,an automated decision making approach is necessary. Nevertheless, most existingsolutions are limited in centralized environments only. Thus, they can onlyprocess at most a few thousand tweets. Such a sample, is not representative todefine the sentiment polarity towards a topic due to the massive number oftweets published daily. In this paper, we go one step further and develop anovel method for sentiment learning in the MapReduce framework. Our algorithmexploits the hashtags and emoticons inside a tweet, as sentiment labels, andproceeds to a classification procedure of diverse sentiment types in a paralleland distributed manner. Moreover, we utilize Bloom filters to compact thestorage size of intermediate data and boost the performance of our algorithm.Through an extensive experimental evaluation, we prove that our solution isefficient, robust and scalable and confirm the quality of our sentimentidentification.
arxiv-15900-83 | "Draw My Topics": Find Desired Topics fast from large scale of Corpus | http://arxiv.org/pdf/1602.01428v1.pdf | author:Jason Dou, Ni Sun, Xiaojun Zou category:cs.CL cs.IR published:2016-02-03 summary:We develop the "Draw My Topics" toolkit, which provides a fast way toincorporate social scientists' interest into standard topic modelling. Insteadof using raw corpus with primitive processing as input, an algorithm based onVector Space Model and Conditional Entropy are used to connect socialscientists' willingness and unsupervised topic models' output. Space for users'adjustment on specific corpus of their interest is also accommodated. Wedemonstrate the toolkit's use on the Diachronic People's Daily Corpus inChinese.
arxiv-15900-84 | Learning Discriminative Features via Label Consistent Neural Network | http://arxiv.org/pdf/1602.01168v1.pdf | author:Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic category:cs.CV cs.LG cs.MM cs.NE stat.ML published:2016-02-03 summary:Deep Convolutional Neural Network (CNN) enforces supervised information onlyat the output layer, and hidden layers are trained by back propagating theprediction error from the output layer without explicit supervision. We proposea supervised feature learning approach, Label Consistency Neural Network, whichenforces direct supervision in late hidden layers. We associate each neuron ina hidden layer with a particular class label and encourage it to be activatedfor input signals from the same class. More specifically, we introduce a labelconsistency regularization called "discriminative representation error" lossfor late hidden layers and combine it with classification error loss to buildour overall objective function. This label consistency constraint not onlyalleviates the common problem of gradient vanishing and tends to fasterconvergence, but also makes the features derived from late hidden layersdiscriminative enough for classification even using a simple classifier such as$k$-NN classifier, since input signals from the same class will have verysimilar representations. Experimental results demonstrate that our approach canachieve state-of-the-art performances on several public benchmarks for actionand object category recognition.
arxiv-15900-85 | A General Framework for Fast Image Deconvolution with Incomplete Observations. Applications to Unknown Boundaries, Inpainting, Superresolution, and Demosaicing | http://arxiv.org/pdf/1602.01410v1.pdf | author:Miguel Simões, Luis B. Almeida, José Bioucas-Dias, Jocelyn Chanussot category:cs.CV stat.ML published:2016-02-03 summary:In image deconvolution problems, the diagonalization of the underlyingoperators by means of the FFT usually yields very large speedups. When thereare incomplete observations (e.g., in the case of unknown boundaries), standarddeconvolution techniques normally involve non-diagonalizableoperators---resulting in rather slow methods---or, otherwise, use inexactconvolution models, resulting in the occurrence of artifacts in the enhancedimages. In this paper, we propose a new deconvolution framework for images withincomplete observations that allows us to work with diagonalized convolutionoperators, and therefore is very fast. We iteratively alternate the estimationof the unknown pixels and of the deconvolved image, using, e.g., a FFT-baseddeconvolution method. In principle, any fast deconvolution method can be used.We give an example in which a published method that assumes periodic boundaryconditions is extended, through the use of this framework, to unknown boundaryconditions. Furthermore, we propose an implementation of this framework, basedon the alternating direction method of multipliers (ADMM). We provide a proofof convergence for the resulting algorithm, which can be seen as a "partial"ADMM, in which not all variables are dualized. We report experimentalcomparisons with other primal-dual methods, in which the proposed one performedat the level of the state of the art. Four different kinds of applications weretested in the experiments: deconvolution, deconvolution with inpainting,superresolution, and demosaicing, all with unknown boundaries.
arxiv-15900-86 | A Kronecker-factored approximate Fisher matrix for convolution layers | http://arxiv.org/pdf/1602.01407v1.pdf | author:Roger Grosse, James Martens category:stat.ML cs.LG published:2016-02-03 summary:Second-order optimization methods such as natural gradient descent have thepotential to speed up training of neural networks by correcting for thecurvature of the loss function. Unfortunately, the exact natural gradient isimpractical to compute for large models, and most approximations either requirean expensive iterative procedure or make crude approximations to the curvature.We present Kronecker Factors for Convolution (KFC), a tractable approximationto the Fisher matrix for convolutional networks based on a structuredprobabilistic model for the distribution over backpropagated derivatives.Similarly to the recently proposed Kronecker-Factored Approximate Curvature(K-FAC), each block of the approximate Fisher matrix decomposes as theKronecker product of small matrices, allowing for efficient inversion. KFCcaptures important curvature information while still yielding comparablyefficient updates to stochastic gradient descent (SGD). We show that theupdates are invariant to commonly used reparameterizations, such as centeringof the activations. In our experiments, approximate natural gradient descentwith KFC was able to train convolutional networks several times faster thancarefully tuned SGD. Furthermore, it was able to train the networks in 10-20times fewer iterations than SGD, suggesting its potential applicability in adistributed setting.
arxiv-15900-87 | A Probabilistic Modeling Approach to Hearing Loss Compensation | http://arxiv.org/pdf/1602.01345v1.pdf | author:Thijs van de Laar, Bert de Vries category:stat.ML published:2016-02-03 summary:Hearing Aid (HA) algorithms need to be tuned ("fitted") to match theimpairment of each specific patient. The lack of a fundamental HA fittingtheory is a strong contributing factor to an unsatisfying sound experience forabout 20% of hearing aid patients (Kochkin, 2014). This paper proposes aprobabilistic modeling approach to the design of HA algorithms. The proposedmethod relies on a generative probabilistic model for the hearing loss problemand provides for automated inference of the corresponding (1) signal processingalgorithm, (2) the fitting solution as well as a principled (3) performanceevaluation metric. All three tasks are realized as message passing algorithmsin a factor graph representation of the generative model, which in principleallows for fast implementation on hearing aid or mobile device hardware. Themethods are theoretically worked out and simulated with a custom-built factorgraph toolbox for a specific hearing loss model (Zurek, 2007).
arxiv-15900-88 | Biclustering Readings and Manuscripts via Non-negative Matrix Factorization, with Application to the Text of Jude | http://arxiv.org/pdf/1602.01323v1.pdf | author:Joey McCollum, Stephen Brown category:cs.LG 6U815 I.2.7 published:2016-02-03 summary:The text-critical practice of grouping witnesses into families or texttypesoften faces two obstacles: Contamination in the manuscript tradition, andco-dependence in identifying characteristic readings and manuscripts. Weintroduce non-negative matrix factorization (NMF) as a simple, unsupervised,and efficient way to cluster large numbers of manuscripts and readingssimultaneously while summarizing contamination using an easy-to-interpretmixture model. We apply this method to an extensive collation of the NewTestament epistle of Jude and show that the resulting clusters correspond tohuman-identified textual families from existing research.
arxiv-15900-89 | A simple method for estimating the fractal dimension from digital images: The compression dimension | http://arxiv.org/pdf/1602.02139v1.pdf | author:P. Chamorro-Posada category:cs.GR cs.CV published:2016-02-03 summary:The fractal structure of real world objects is often analyzed using digitalimages. In this context, the compression fractal dimension is put forward. Itprovides a simple method for the direct estimation of the dimension of fractalsstored as digital image files. The computational scheme can be implementedusing readily available free software. Its simplicity also makes it veryinteresting for introductory elaborations of basic concepts of fractalgeometry, complexity, and information theory. A test of the computationalscheme using limited-quality images of well-defined fractal sets obtained fromthe Internet and free software has been performed.
arxiv-15900-90 | High-Dimensional Regularized Discriminant Analysis | http://arxiv.org/pdf/1602.01182v1.pdf | author:John A. Ramey, Caleb K. Stein, Phil D. Young, Dean M. Young category:stat.ML published:2016-02-03 summary:Friedman proposed the popular regularized discriminant analysis (RDA)classifier that utilizes a biased covariance-matrix estimator that partiallypools the sample covariance matrices from linear and quadratic discriminantanalysis and shrinks the resulting estimator towards a scaled identity matrix.The RDA classifier's two tuning parameters are typically estimated via acomputationally burdensome cross-validation procedure that uses a grid search.We formulate a new RDA-based classifier for the small-sample, high-dimensionalsetting and then show that the classification decision rule is equivalent to aclassifier in a subspace having a much lower dimension. As a result, theutilization of the dimension-reduction step yields a substantial reduction incomputation during model selection. Also, our parameterization offersinterpretability that was previously lacking with the RDA classifier. Wedemonstrate that our proposed classifier is often superior to several recentlyproposed sparse and regularized classifiers in terms of classification accuracywith three artificial and six real high-dimensional data sets. Finally, weprovide an implementation of our proposed classifier in the sparsediscrim Rpackage, which is available on CRAN.
arxiv-15900-91 | Learning scale-variant and scale-invariant features for deep image classification | http://arxiv.org/pdf/1602.01255v2.pdf | author:Nanne van Noord, Eric Postma category:cs.CV published:2016-02-03 summary:Convolutional Neural Networks (CNNs) require large image corpora to betrained on classification tasks. The variation in image resolutions, sizes ofobjects and patterns depicted, and image scales, hampers CNN training andperformance, because the task-relevant information varies over spatial scales.Previous work attempting to deal with such scale variations focused onencouraging scale-invariant CNN representations. However, scale-invariantrepresentations are incomplete representations of images, because imagescontain scale-variant information as well. This paper addresses the combineddevelopment of scale-invariant and scale-variant representations. We propose amulti- scale CNN method to encourage the recognition of both types of featuresand evaluate it on a challenging image classification task involvingtask-relevant characteristics at multiple scales. The results show that ourmulti-scale CNN outperforms single-scale CNN. This leads to the conclusion thatencouraging the combined development of a scale-invariant and scale-variantrepresentation in CNNs is beneficial to image recognition performance.
arxiv-15900-92 | Discriminative Sparse Neighbor Approximation for Imbalanced Learning | http://arxiv.org/pdf/1602.01197v1.pdf | author:Chen Huang, Chen Change Loy, Xiaoou Tang category:cs.CV published:2016-02-03 summary:Data imbalance is common in many vision tasks where one or more classes arerare. Without addressing this issue conventional methods tend to be biasedtoward the majority class with poor predictive accuracy for the minority class.These methods further deteriorate on small, imbalanced data that has a largedegree of class overlap. In this study, we propose a novel discriminativesparse neighbor approximation (DSNA) method to ameliorate the effect ofclass-imbalance during prediction. Specifically, given a test sample, we firsttraverse it through a cost-sensitive decision forest to collect a good subsetof training examples in its local neighborhood. Then we generate from thissubset several class-discriminating but overlapping clusters and model each asan affine subspace. From these subspaces, the proposed DSNA iteratively seeksan optimal approximation of the test sample and outputs an unbiased prediction.We show that our method not only effectively mitigates the imbalance issue, butalso allows the prediction to extrapolate to unseen data. The latter capabilityis crucial for achieving accurate prediction on small dataset with limitedsamples. The proposed imbalanced learning method can be applied to bothclassification and regression tasks at a wide range of imbalance levels. Itsignificantly outperforms the state-of-the-art methods that do not possess animbalance handling mechanism, and is found to perform comparably or even betterthan recent deep learning methods by using hand-crafted features only.
arxiv-15900-93 | Image and Information | http://arxiv.org/pdf/1602.01228v1.pdf | author:Frank Nielsen category:cs.CV published:2016-02-03 summary:A well-known old adage says that {\em "A picture is worth a thousand words!"}(attributed to the Chinese philosopher Confucius ca 500 years BC). But moreprecisely, what do we mean by information in images? And how can it beretrieved effectively by machines? We briefly highlight these puzzlingquestions in this column. But first of all, let us start by defining moreprecisely what is meant by an "Image."
arxiv-15900-94 | A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks | http://arxiv.org/pdf/1602.01321v1.pdf | author:Luke B. Godfrey, Michael S. Gashler category:cs.NE published:2016-02-03 summary:We present the soft exponential activation function for artificial neuralnetworks that continuously interpolates between logarithmic, linear, andexponential functions. This activation function is simple, differentiable, andparameterized so that it can be trained as the rest of the network is trained.We hypothesize that soft exponential has the potential to improve neuralnetwork learning, as it can exactly calculate many natural operations thattypical neural networks can only approximate, including addition,multiplication, inner product, distance, polynomials, and sinusoids.
arxiv-15900-95 | Latent-Class Hough Forests for 6 DoF Object Pose Estimation | http://arxiv.org/pdf/1602.01464v1.pdf | author:Rigas Kouskouridas, Alykhan Tejani, Andreas Doumanoglou, Danhang Tang, Tae-Kyun Kim category:cs.CV published:2016-02-03 summary:In this paper we present Latent-Class Hough Forests, a method for objectdetection and 6 DoF pose estimation in heavily cluttered and occludedscenarios. We adapt a state of the art template matching feature into ascale-invariant patch descriptor and integrate it into a regression forestusing a novel template-based split function. We train with positive samplesonly and we treat class distributions at the leaf nodes as latent variables.During testing we infer by iteratively updating these distributions, providingaccurate estimation of background clutter and foreground occlusions and, thus,better detection rate. Furthermore, as a by-product, our Latent-Class HoughForests can provide accurate occlusion aware segmentation masks, even in themulti-instance scenario. In addition to an existing public dataset, whichcontains only single-instance sequences with large amounts of clutter, we havecollected two, more challenging, datasets for multiple-instance detectioncontaining heavy 2D and 3D clutter as well as foreground occlusions. We provideextensive experiments on the various parameters of the framework such as patchsize, number of trees and number of iterations to infer class distributions attest time. We also evaluate the Latent-Class Hough Forests on all datasetswhere we outperform state of the art methods.
arxiv-15900-96 | Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences | http://arxiv.org/pdf/1602.01208v3.pdf | author:Akira Taniguchi, Tadahiro Taniguchi, Tetsunari Inamura category:cs.AI cs.CL cs.RO published:2016-02-03 summary:In this paper, we propose a novel unsupervised learning method for thelexical acquisition of words related to places visited by robots, from humancontinuous speech signals. We address the problem of learning novel words by arobot that has no prior knowledge of these words except for a primitiveacoustic model. Further, we propose a method that allows a robot to effectivelyuse the learned words and their meanings for self-localization tasks. Theproposed method is nonparametric Bayesian spatial concept acquisition method(SpCoA) that integrates the generative model for self-localization and theunsupervised word segmentation in uttered sentences via latent variablesrelated to the spatial concept. We implemented the proposed method SpCoA onSIGVerse, which is a simulation environment, and TurtleBot2, which is a mobilerobot in a real environment. Further, we conducted experiments for evaluatingthe performance of SpCoA. The experimental results showed that SpCoA enabledthe robot to acquire the names of places from speech sentences. They alsorevealed that the robot could effectively utilize the acquired spatial conceptsand reduce the uncertainty in self-localization.
arxiv-15900-97 | How Far are We from Solving Pedestrian Detection? | http://arxiv.org/pdf/1602.01237v1.pdf | author:Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele category:cs.CV published:2016-02-03 summary:Encouraged by the recent progress in pedestrian detection, we investigate thegap between current state-of-the-art methods and the "perfect single framedetector". We enable our analysis by creating a human baseline for pedestriandetection (over the Caltech dataset), and by manually clustering the recurrenterrors of a top detector. Our results characterize both localization andbackground-versus-foreground errors. To address localization errors we studythe impact of training annotation noise on the detector performance, and showthat we can improve even with a small portion of sanitized training data. Toaddress background/foreground discrimination, we study convnets for pedestriandetection, and discuss which factors affect their performance. Other than ourin-depth analysis, we report top performance on the Caltech dataset, andprovide a new sanitized set of training and test annotations.
arxiv-15900-98 | Single-Solution Hypervolume Maximization and its use for Improving Generalization of Neural Networks | http://arxiv.org/pdf/1602.01164v1.pdf | author:Conrado S. Miranda, Fernando J. Von Zuben category:cs.LG cs.NE stat.ML published:2016-02-03 summary:This paper introduces the hypervolume maximization with a single solution asan alternative to the mean loss minimization. The relationship between the twoproblems is proved through bounds on the cost function when an optimal solutionto one of the problems is evaluated on the other, with a hyperparameter tocontrol the similarity between the two problems. This same hyperparameterallows higher weight to be placed on samples with higher loss when computingthe hypervolume's gradient, whose normalized version can range from the meanloss to the max loss. An experiment on MNIST with a neural network is used tovalidate the theory developed, showing that the hypervolume maximization canbehave similarly to the mean loss minimization and can also provide betterperformance, resulting on a 20% reduction of the classification error on thetest set.
arxiv-15900-99 | Learning a Deep Model for Human Action Recognition from Novel Viewpoints | http://arxiv.org/pdf/1602.00828v1.pdf | author:Hossein Rahmani, Ajmal Mian, Mubarak Shah category:cs.CV published:2016-02-02 summary:Recognizing human actions from unknown and unseen (novel) views is achallenging problem. We propose a Robust Non-Linear Knowledge Transfer Model(R-NKTM) for human action recognition from novel views. The proposed R-NKTM isa deep fully-connected neural network that transfers knowledge of human actionsfrom any unknown view to a shared high-level virtual view by finding anon-linear virtual path that connects the views. The R-NKTM is learned fromdense trajectories of synthetic 3D human models fitted to real motion capturedata and generalizes to real videos of human actions. The strength of ourtechnique is that we learn a single R-NKTM for all actions and all viewpointsfor knowledge transfer of any real human action video without the need forre-training or fine-tuning the model. Thus, R-NKTM can efficiently scale toincorporate new action classes. R-NKTM is learned with dummy labels and doesnot require knowledge of the camera viewpoint at any stage. Experiments onthree benchmark cross-view human action datasets show that our methodoutperforms existing state-of-the-art.
arxiv-15900-100 | The Grail theorem prover: Type theory for syntax and semantics | http://arxiv.org/pdf/1602.00812v1.pdf | author:Richard Moot category:cs.CL published:2016-02-02 summary:As the name suggests, type-logical grammars are a grammar formalism based onlogic and type theory. From the prespective of grammar design, type-logicalgrammars develop the syntactic and semantic aspects of linguistic phenomenahand-in-hand, letting the desired semantics of an expression inform thesyntactic type and vice versa. Prototypical examples of the successfulapplication of type-logical grammars to the syntax-semantics interface includecoordination, quantifier scope and extraction.This chapter describes the Grailtheorem prover, a series of tools for designing and testing grammars in variousmodern type-logical grammars which functions as a tool . All tools described inthis chapter are freely available.
arxiv-15900-101 | On distances, paths and connections for hyperspectral image segmentation | http://arxiv.org/pdf/1603.08497v1.pdf | author:Guillaume Noyel, Jesus Angulo, Dominique Jeulin category:cs.CV math.NA published:2016-02-02 summary:The present paper introduces the $\eta$ and {\eta} connections in order toadd regional information on $\lambda$-flat zones, which only take into accounta local information. A top-down approach is considered. First $\lambda$-flatzones are built in a way leading to a sub-segmentation. Then a finersegmentation is obtained by computing $\eta$-bounded regions and $\mu$-geodesicballs inside the $\lambda$-flat zones. The proposed algorithms for theconstruction of new partitions are based on queues with an ordered selection ofseeds using the cumulative distance. $\eta$-bounded regions offers a control onthe variations of amplitude in the class from a point, called center, and$\mu$-geodesic balls controls the "size" of the class. These results areapplied to hyperspectral images.
arxiv-15900-102 | An analytic comparison of regularization methods for Gaussian Processes | http://arxiv.org/pdf/1602.00853v1.pdf | author:Hossein Mohammadi, Rodolphe Le Riche, Nicolas Durrande, Eric Touboul, Xavier Bay category:math.OC math.ST stat.ML stat.TH published:2016-02-02 summary:Gaussian Processes (GPs) are often used to predict the output of aparameterized deterministic experiment. They have many applications in thefield of Computer Experiments, in particular to perform sensitivity analysis,adaptive design of experiments and global optimization. Nearly all of theapplications of GPs to Computer Experiments require the inversion of acovariance matrix. Because this matrix is often ill-conditioned, regularizationtechniques are required. Today, there is still a need to better regularize GPs.The two most classical regularization methods are i) pseudoinverse (PI) and ii)nugget (or jitter or observation noise). This article provides algebraiccalculations which allow comparing PI and nugget regularizations. It is proventhat pseudoinverse regularization averages the output values and makes thevariance null at redundant points. On the opposite, nugget regularization lacksinterpolation properties but preserves a non-zero variance at every point.However , these two regularization techniques become similar as the nuggetvalue decreases. A distribution-wise GP is introduced which interpolatesGaussian distributions instead of data points and mitigates the drawbacks ofpseudoinverse and nugget regularized GPs. Finally, data-model discrepancy isdiscussed and serves as a guide for choosing a regularization technique.
arxiv-15900-103 | Simple Online and Realtime Tracking | http://arxiv.org/pdf/1602.00763v1.pdf | author:Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft category:cs.CV published:2016-02-02 summary:This paper explores a pragmatic approach to multiple object tracking wherethe main focus is to associate objects efficiently for online and realtimeapplications. To this end, detection quality is identified as a key factorinfluencing tracking performance, where changing the detector can improvetracking by up to 18.9%. Despite only using a rudimentary combination offamiliar techniques such as the Kalman Filter and Hungarian algorithm for thetracking components, this approach achieves an accuracy comparable tostate-of-the-art online trackers. Furthermore, due to the simplicity of ourtracking method, the tracker updates at a rate of 260 Hz which is over 20xfaster than other state-of-the-art trackers.
arxiv-15900-104 | Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects | http://arxiv.org/pdf/1602.00753v1.pdf | author:Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi category:cs.AI cs.CV published:2016-02-02 summary:Human vision greatly benefits from the information about sizes of objects.The role of size in several visual reasoning tasks has been thoroughly exploredin human perception and cognition. However, the impact of the information aboutsizes of objects is yet to be determined in AI. We postulate that this ismainly attributed to the lack of a comprehensive repository of sizeinformation. In this paper, we introduce a method to automatically infer objectsizes, leveraging visual and textual information from web. By maximizing thejoint likelihood of textual and visual observations, our method learns reliablerelative size estimates, with no explicit human supervision. We introduce therelative size dataset and show that our method outperforms competitive textualand visual baselines in reasoning about size comparisons.
arxiv-15900-105 | Partial Recovery Bounds for the Sparse Stochastic Block Model | http://arxiv.org/pdf/1602.00877v2.pdf | author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.SI math.IT stat.ML published:2016-02-02 summary:In this paper, we study the information-theoretic limits of communitydetection in the symmetric two-community stochastic block model, withintra-community and inter-community edge probabilities $\frac{a}{n}$ and$\frac{b}{n}$ respectively. We consider the sparse setting, in which $a$ and$b$ do not scale with $n$, and provide upper and lower bounds on the proportionof community labels recovered on average. We provide a numerical example forwhich the bounds are near-matching for moderate values of $a - b$, and matchingin the limit as $a-b$ grows large.
arxiv-15900-106 | Visual descriptors for content-based retrieval of remote sensing images | http://arxiv.org/pdf/1602.00970v1.pdf | author:Paolo Napoletano category:cs.CV published:2016-02-02 summary:In this paper we present an extensive evaluation of visual descriptors forthe content-based retrieval of remote sensing images. The evaluation includesglobal, local, and Convolutional Neural Network (CNNs) features coupled withthree different Content-Based Image Retrieval schemas. We conducted all theexperiments on two publicly available datasets: the 21-class UC Merced LandUse/Land Cover data set and 19-class High-resolution Satellite Scene dataset.Results demonstrate that features extracted from CNNs are the best performingwhatever is the retrieval schema adopted. Local descriptors perform better thanCNN-based descriptors only when dealing with images that contain fine-grainedtextures or objects.
arxiv-15900-107 | Mental State Recognition via Wearable EEG | http://arxiv.org/pdf/1602.00985v1.pdf | author:Pouya Bashivan, Irina Rish, Steve Heisig category:cs.CV cs.HC published:2016-02-02 summary:The increasing quality and affordability of consumer electroencephalogram(EEG) headsets make them attractive for situations where medical grade devicesare impractical. Predicting and tracking cognitive states is possible for tasksthat were previously not conducive to EEG monitoring. For instance, monitoringoperators for states inappropriate to the task (e.g. drowsy drivers), trackingmental health (e.g. anxiety) and productivity (e.g. tiredness) are amongpossible applications for the technology. Consumer grade EEG headsets areaffordable and relatively easy to use, but they lack the resolution and qualityof signal that can be achieved using medical grade EEG devices. Thus, the keyquestions remain: to what extent are wearable EEG devices capable of mentalstate recognition, and what kind of mental states can be accurately recognizedwith these devices? In this work, we examined responses to two different typesof input: instructional (logical) versus recreational (emotional) videos, usinga range of machine-learning methods. We tried SVMs, sparse logistic regression,and Deep Belief Networks, to discriminate between the states of mind induced bydifferent types of video input, that can be roughly labeled as logical vs.emotional. Our results demonstrate a significant potential of wearable EEGdevices in differentiating cognitive states between situations with largecontextual but subtle apparent differences.
arxiv-15900-108 | Head Pose Estimation of Occluded Faces using Regularized Regression | http://arxiv.org/pdf/1602.00997v1.pdf | author:Amit Kumar, Rishabh Bindal, Soumya Indela, Michael Rotkowitz category:cs.CV published:2016-02-02 summary:This paper presents regression methods for estimation of head pose fromoccluded 2-D face images. The process primarily involves reconstructing a facefrom its occluded image, followed by classification. Typical methods forreconstruction assume that the pixel errors of the occluded regions areindependent. However, such an assumption is not true in the case of occlusion,because of its inherent contiguous nature. Hence, we use nuclear norm as ametric that can describe well the structure of the error. We also use LASSORegression based l1 - regularization to improve reconstruction. Next, weimplement Nuclear Norm Regularized Regression (NR), and also our proposedmethod, for reconstruction and subsequent classification. Finally, we comparethe performance of the methods in terms of accuracy of head pose estimation ofoccluded faces.
arxiv-15900-109 | Unsupervised High-level Feature Learning by Ensemble Projection for Semi-supervised Image Classification and Image Clustering | http://arxiv.org/pdf/1602.00955v2.pdf | author:Dengxin Dai, Luc Van Gool category:cs.CV published:2016-02-02 summary:This paper investigates the problem of image classification with limited orno annotations, but abundant unlabeled data. The setting exists in many taskssuch as semi-supervised image classification, image clustering, and imageretrieval. Unlike previous methods, which develop or learn sophisticatedregularizers for classifiers, our method learns a new image representation byexploiting the distribution patterns of all available data for the task athand. Particularly, a rich set of visual prototypes are sampled from allavailable data, and are taken as surrogate classes to train discriminativeclassifiers; images are projected via the classifiers; the projected values,similarities to the prototypes, are stacked to build the new feature vector.The training set is noisy. Hence, in the spirit of ensemble learning we createa set of such training sets which are all diverse, leading to diverseclassifiers. The method is dubbed Ensemble Projection (EP). EP captures notonly the characteristics of individual images, but also the relationships amongimages. It is conceptually simple and computationally efficient, yet effectiveand flexible. Experiments on eight standard datasets show that: (1) EPoutperforms previous methods for semi-supervised image classification; (2) EPproduces promising results for self-taught image classification, whereunlabeled samples are a random collection of images rather than being from thesame distribution as the labeled ones; and (3) EP improves over the originalfeatures for image clustering. The code of the method is available on theproject page.
arxiv-15900-110 | Comparative evaluation of state-of-the-art algorithms for SSVEP-based BCIs | http://arxiv.org/pdf/1602.00904v2.pdf | author:Vangelis P. Oikonomou, Georgios Liaros, Kostantinos Georgiadis, Elisavet Chatzilari, Katerina Adam, Spiros Nikolopoulos, Ioannis Kompatsiaris category:cs.HC cs.CV stat.ML published:2016-02-02 summary:Brain-computer interfaces (BCIs) have been gaining momentum in makinghuman-computer interaction more natural, especially for people withneuro-muscular disabilities. Among the existing solutions the systems relyingon electroencephalograms (EEG) occupy the most prominent place due to theirnon-invasiveness. However, the process of translating EEG signals into computercommands is far from trivial, since it requires the optimization of manydifferent parameters that need to be tuned jointly. In this report, we focus onthe category of EEG-based BCIs that rely on Steady-State-Visual-EvokedPotentials (SSVEPs) and perform a comparative evaluation of the most promisingalgorithms existing in the literature. More specifically, we define a set ofalgorithms for each of the various different parameters composing a BCI system(i.e. filtering, artifact removal, feature extraction, feature selection andclassification) and study each parameter independently by keeping all otherparameters fixed. The results obtained from this evaluation process areprovided together with a dataset consisting of the 256-channel, EEG signals of11 subjects, as well as a processing toolbox for reproducing the results andsupporting further experimentation. In this way, we manage to make availablefor the community a state-of-the-art baseline for SSVEP-based BCIs that can beused as a basis for introducing novel methods and approaches.
arxiv-15900-111 | Interactive algorithms: From pool to stream | http://arxiv.org/pdf/1602.01132v2.pdf | author:Sivan Sabato, Tom Hess category:stat.ML cs.LG math.ST stat.TH published:2016-02-02 summary:We consider interactive algorithms in the pool-based setting, and in thestream-based setting. Interactive algorithms observe suggested elements(representing actions or queries), and interactively select some of them andreceive responses. Stream-based algorithms are not allowed to select suggestedelements after more elements have been observed, while pool-based algorithmscan select elements at any order. We assume that the available elements aregenerated independently according to some distribution, and design stream-basedalgorithms that emulate black-box pool-based interactive algorithms. We providetwo such emulating algorithms. The first algorithm can emulate any pool-basedalgorithm, but the number of suggested elements that need to be observed mightbe exponential in the number of selected elements. The second algorithm appliesto the class of utility-based interactive algorithms, and the number ofsuggested elements that it observes is linear in the number of selectedelements. For the case of utility-based emulation, we also provide a lowerbound showing that near-linearity is necessary.
arxiv-15900-112 | Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks | http://arxiv.org/pdf/1602.00991v2.pdf | author:Peter Ondruska, Ingmar Posner category:cs.LG cs.AI cs.CV cs.NE cs.RO published:2016-02-02 summary:This paper presents to the best of our knowledge the first end-to-end objecttracking approach which directly maps from raw sensor input to object tracks insensor space without requiring any feature engineering or system identificationin the form of plant or sensor models. Specifically, our system accepts astream of raw sensor data at one end and, in real-time, produces an estimate ofthe entire environment state at the output including even occluded objects. Weachieve this by framing the problem as a deep learning task and exploitsequence models in the form of recurrent neural networks to learn a mappingfrom sensor measurements to object tracks. In particular, we propose a learningmethod based on a form of input dropout which allows learning in anunsupervised manner, only based on raw, occluded sensor data without access toground-truth annotations. We demonstrate our approach using a synthetic datasetdesigned to mimic the task of tracking objects in 2D laser data -- as commonlyencountered in robotics applications -- and show that it learns to track manydynamic objects despite occlusions and the presence of sensor noise.
arxiv-15900-113 | Better safe than sorry: Risky function exploitation through safe optimization | http://arxiv.org/pdf/1602.01052v2.pdf | author:Eric Schulz, Quentin J. M. Huys, Dominik R. Bach, Maarten Speekenbrink, Andreas Krause category:stat.AP cs.LG stat.ML published:2016-02-02 summary:Exploration-exploitation of functions, that is learning and optimizing amapping between inputs and expected outputs, is ubiquitous to many real worldsituations. These situations sometimes require us to avoid certain outcomes atall cost, for example because they are poisonous, harmful, or otherwisedangerous. We test participants' behavior in scenarios in which they have tofind the optimum of a function while at the same time avoid outputs below acertain threshold. In two experiments, we find that Safe-Optimization, aGaussian Process-based exploration-exploitation algorithm, describesparticipants' behavior well and that participants seem to care firstly whethera point is safe and then try to pick the optimal point from all such safepoints. This means that their trade-off between exploration and exploitationcan be seen as an intelligent, approximate, and homeostasis-driven strategy.
arxiv-15900-114 | A-expansion for multiple "hedgehog" shapes | http://arxiv.org/pdf/1602.01006v1.pdf | author:Hossam Isack, Yuri Boykov, Olga Veksler category:cs.CV published:2016-02-02 summary:Overlapping colors and cluttered or weak edges are common segmentationproblems requiring additional regularization. For example, star-convexity ispopular for interactive single object segmentation due to simplicity andamenability to exact graph cut optimization. This paper proposes an approach tomultiobject segmentation where objects could be restricted to separate"hedgehog" shapes. We show that a-expansion moves are submodular for ourmulti-shape constraints. Each "hedgehog" shape has its surface normalsconstrained by some vector field, e.g. gradients of a distance transform foruser scribbles. Tight constraint give an extreme case of a shape priorenforcing skeleton consistency with the scribbles. Wider cones of allowednormals gives more relaxed hedgehog shapes. A single click and +/-90 degreesnormal orientation constraints reduce our hedgehog prior to star-convexity. Ifall hedgehogs come from single clicks then our approach defines multi-starprior. Our general method has significantly more applications than standardone-star segmentation. For example, in medical data we can separate multiplenon-star organs with similar appearances and weak or noisy edges.
arxiv-15900-115 | On Deep Multi-View Representation Learning: Objectives and Optimization | http://arxiv.org/pdf/1602.01024v1.pdf | author:Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes category:cs.LG published:2016-02-02 summary:We consider learning representations (features) in the setting in which wehave access to multiple unlabeled views of the data for learning while only oneview is available for downstream tasks. Previous work on this problem hasproposed several techniques based on deep neural networks, typically involvingeither autoencoder-like networks with a reconstruction objective or pairedfeedforward networks with a batch-style correlation-based objective. We analyzeseveral techniques based on prior work, as well as new variants, and comparethem empirically on image, speech, and text tasks. We find an advantage forcorrelation-based representation learning, while the best results on most tasksare obtained with our new variant, deep canonically correlated autoencoders(DCCAE). We also explore a stochastic optimization procedure for minibatchcorrelation-based objectives and discuss the time/performance trade-offs forkernel-based and neural network-based implementations.
arxiv-15900-116 | Minimum Regret Search for Single- and Multi-Task Optimization | http://arxiv.org/pdf/1602.01064v2.pdf | author:Jan Hendrik Metzen category:stat.ML cs.IT cs.LG cs.RO math.IT published:2016-02-02 summary:We propose minimum regret search (MRS), a novel acquisition function forBayesian optimization. MRS bears similarities with information-theoreticapproaches such as entropy search (ES). However, while ES aims in each query atmaximizing the information gain with respect to the global maximum, MRS aims atminimizing the expected immediate regret of its ultimate recommendation for theoptimum. While empirically ES and MRS perform similar in most of the cases, MRSproduces fewer outliers with high regret than ES. We provide empirical resultsboth for a synthetic single-task optimization problem as well as for asimulated multi-task robotic control problem.
arxiv-15900-117 | Improved Achievability and Converse Bounds for Erdős-Rényi Graph Matching | http://arxiv.org/pdf/1602.01042v1.pdf | author:Daniel Cullina, Negar Kiyavash category:cs.IT cs.LG math.IT published:2016-02-02 summary:We consider the problem of perfectly recovering the vertex correspondencebetween two correlated Erd\H{o}s-R\'enyi (ER) graphs. For a pair of correlatedgraphs on the same vertex set, the correspondence between the vertices can beobscured by randomly permuting the vertex labels of one of the graphs. In somecases, the structural information in the graphs allow this correspondence to berecovered. We investigate the information-theoretic threshold for exactrecovery, i.e. the conditions under which the entire vertex correspondence canbe correctly recovered given unbounded computational resources. Pedarsani and Grossglauser provided an achievability result of this type.Their result establishes the scaling dependence of the threshold on the numberof vertices. We improve on their achievability bound. We also provide aconverse bound, establishing conditions under which exact recovery isimpossible. Together, these establish the scaling dependence of the thresholdon the level of correlation between the two graphs. The converse andachievability bounds differ by a factor of two for sparse, significantlycorrelated graphs.
arxiv-15900-118 | Development of an Ideal Observer that Incorporates Nuisance Parameters and Processes List-Mode Data | http://arxiv.org/pdf/1602.01449v1.pdf | author:Christopher J. MacGahan, Matthew A. Kupinski, Nathan R. Hilton, Erik M. Brubaker, William C. Johnson category:cs.CV published:2016-02-02 summary:Observer models were developed to process data in list-mode format in orderto perform binary discrimination tasks for use in an arms-control-treatycontext. Data used in this study was generated using GEANT4 Monte Carlosimulations for photons using custom models of plutonium inspection objects anda radiation imaging system. Observer model performance was evaluated andpresented using the area under the receiver operating characteristic curve. Theideal observer was studied under both signal-known-exactly conditions and inthe presence of unknowns such as object orientation and absolute count-ratevariability; when these additional sources of randomness were present, theirincorporation into the observer yielded superior performance.
arxiv-15900-119 | Do Cascades Recur? | http://arxiv.org/pdf/1602.01107v1.pdf | author:Justin Cheng, Lada A Adamic, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2016-02-02 summary:Cascades of information-sharing are a primary mechanism by which contentreaches its audience on social media, and an active line of research hasstudied how such cascades, which form as content is reshared from person toperson, develop and subside. In this paper, we perform a large-scale analysisof cascades on Facebook over significantly longer time scales, and find that amore complex picture emerges, in which many large cascades recur, exhibitingmultiple bursts of popularity with periods of quiescence in between. Wecharacterize recurrence by measuring the time elapsed between bursts, theiroverlap and proximity in the social network, and the diversity in thedemographics of individuals participating in each peak. We discover thatcontent virality, as revealed by its initial popularity, is a main driver ofrecurrence, with the availability of multiple copies of that content helping tospark new bursts. Still, beyond a certain popularity of content, the rate ofrecurrence drops as cascades start exhausting the population of interestedindividuals. We reproduce these observed patterns in a simple model of contentrecurrence simulated on a real social network. Using only characteristics of acascade's initial burst, we demonstrate strong performance in predictingwhether it will recur in the future.
arxiv-15900-120 | Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions | http://arxiv.org/pdf/1602.01103v2.pdf | author:Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian Lee category:cs.SI cs.CL physics.soc-ph published:2016-02-02 summary:Changing someone's opinion is arguably one of the most important challengesof social interaction. The underlying process proves difficult to study: it ishard to know how someone's opinions are formed and whether and how someone'sviews shift. Fortunately, ChangeMyView, an active community on Reddit, providesa platform where users present their own opinions and reasoning, invite othersto contest them, and acknowledge when the ensuing discussions change theiroriginal views. In this work, we study these interactions to understand themechanisms behind persuasion. We find that persuasive arguments are characterized by interesting patternsof interaction dynamics, such as participant entry-order and degree ofback-and-forth exchange. Furthermore, by comparing similar counterarguments tothe same opinion, we show that language factors play an essential role. Inparticular, the interplay between the language of the opinion holder and thatof the counterargument provides highly predictive cues of persuasiveness.Finally, since even in this favorable setting people may not be persuaded, weinvestigate the problem of determining whether someone's opinion is susceptibleto being changed at all. For this more difficult task, we show that stylisticchoices in how the opinion is expressed carry predictive power.
arxiv-15900-121 | On the Nyström and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets | http://arxiv.org/pdf/1602.01120v1.pdf | author:Darren Homrighausen, Daniel J. McDonald category:stat.ML stat.CO published:2016-02-02 summary:In this paper we analyze approximate methods for undertaking a principalcomponents analysis (PCA) on large data sets. PCA is a classical dimensionreduction method that involves the projection of the data onto the subspacespanned by the leading eigenvectors of the covariance matrix. This projectioncan be used either for exploratory purposes or as an input for furtheranalysis, e.g. regression. If the data have billions of entries or more, thecomputational and storage requirements for saving and manipulating the designmatrix in fast memory is prohibitive. Recently, the Nystr\"om andcolumn-sampling methods have appeared in the numerical linear algebra communityfor the randomized approximation of the singular value decomposition of largematrices. However, their utility for statistical applications remains unclear.We compare these approximations theoretically by bounding the distance betweenthe induced subspaces and the desired, but computationally infeasible, PCAsubspace. Additionally we show empirically, through simulations and a real dataexample involving a corpus of emails, the trade-off of approximation accuracyand computational complexity.
arxiv-15900-122 | Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and Soft Correspondences | http://arxiv.org/pdf/1602.01125v1.pdf | author:Anil Bas, William A. P. Smith, Timo Bolkart, Stefanie Wuhrer category:cs.CV published:2016-02-02 summary:We propose a fully automatic method for fitting a 3D morphable model tosingle face images in arbitrary pose and lighting. Our approach relies ongeometric features (edges and landmarks) and, inspired by the iterated closestpoint algorithm, is based on computing hard correspondences between modelvertices and edge pixels. We demonstrate that this is superior to previous workthat uses soft correspondences to form an edge-derived cost surface that isminimised by nonlinear optimisation.
arxiv-15900-123 | GraphPrints: Towards a Graph Analytic Method for Network Anomaly Detection | http://arxiv.org/pdf/1602.01130v1.pdf | author:Christopher R. Harshaw, Robert A. Bridges, Michael D. Iannacone, Joel W. Reed, John R. Goodall category:cs.CR stat.ML published:2016-02-02 summary:This paper introduces a novel graph-analytic approach for detecting anomaliesin network flow data called GraphPrints. Building on foundationalnetwork-mining techniques, our method represents time slices of traffic as agraph, then counts graphlets -- small induced subgraphs that describe localtopology. By performing outlier detection on the sequence of graphlet counts,anomalous intervals of traffic are identified, and furthermore, individual IPsexperiencing abnormal behavior are singled-out. Initial testing of GraphPrintsis performed on real network data with an implanted anomaly. Evaluation showsfalse positive rates bounded by 2.84% at the time-interval level, and 0.05% atthe IP-level with 100% true positive rates at both.
arxiv-15900-124 | Active Learning Algorithms for Graphical Model Selection | http://arxiv.org/pdf/1602.00354v2.pdf | author:Gautam Dasarathy, Aarti Singh, Maria-Florina Balcan, Jong Hyuk Park category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2016-02-01 summary:The problem of learning the structure of a high dimensional graphical modelfrom data has received considerable attention in recent years. In manyapplications such as sensor networks and proteomics it is often expensive toobtain samples from all the variables involved simultaneously. For instance,this might involve the synchronization of a large number of sensors or thetagging of a large number of proteins. To address this important issue, weinitiate the study of a novel graphical model selection problem, where the goalis to optimize the total number of scalar samples obtained by allowing thecollection of samples from only subsets of the variables. We propose a generalparadigm for graphical model selection where feedback is used to guide thesampling to high degree vertices, while obtaining only few samples from theones with the low degrees. We instantiate this framework with two specificactive learning algorithms, one of which makes mild assumptions but iscomputationally expensive, while the other is more computationally efficientbut requires stronger (nevertheless standard) assumptions. Whereas the samplecomplexity of passive algorithms is typically a function of the maximum degreeof the graph, we show that the sample complexity of our algorithms is provablesmaller and that it depends on a novel local complexity measure that is akin tothe average degree of the graph. We finally demonstrate the efficacy of ourframework via simulations.
arxiv-15900-125 | A Spectral Series Approach to High-Dimensional Nonparametric Regression | http://arxiv.org/pdf/1602.00355v1.pdf | author:Ann B. Lee, Rafael Izbicki category:stat.ME stat.ML published:2016-02-01 summary:A key question in modern statistics is how to make fast and reliableinferences for complex, high-dimensional data. While there has been muchinterest in sparse techniques, current methods do not generalize well to datawith nonlinear structure. In this work, we present an orthogonal seriesestimator for predictors that are complex aggregate objects, such as naturalimages, galaxy spectra, trajectories, and movies. Our series approach tiestogether ideas from kernel machine learning, and Fourier methods. We expand theunknown regression on the data in terms of the eigenfunctions of a kernel-basedoperator, and we take advantage of orthogonality of the basis with respect tothe underlying data distribution, P, to speed up computations and tuning ofparameters. If the kernel is appropriately chosen, then the eigenfunctionsadapt to the intrinsic geometry and dimension of the data. We providetheoretical guarantees for a radial kernel with varying bandwidth, and werelate smoothness of the regression function with respect to P to sparsity inthe eigenbasis. Finally, using simulated and real-world data, we systematicallycompare the performance of the spectral series approach with classical kernelsmoothing, k-nearest neighbors regression, kernel ridge regression, andstate-of-the-art manifold and local regression methods.
arxiv-15900-126 | Adaptive Subgradient Methods for Online AUC Maximization | http://arxiv.org/pdf/1602.00351v1.pdf | author:Yi Ding, Peilin Zhao, Steven C. H. Hoi, Yew-Soon Ong category:cs.LG published:2016-02-01 summary:Learning for maximizing AUC performance is an important research problem inMachine Learning and Artificial Intelligence. Unlike traditional batch learningmethods for maximizing AUC which often suffer from poor scalability, recentyears have witnessed some emerging studies that attempt to maximize AUC bysingle-pass online learning approaches. Despite their encouraging resultsreported, the existing online AUC maximization algorithms often adopt simpleonline gradient descent approaches that fail to exploit the geometricalknowledge of the data observed during the online learning process, and thuscould suffer from relatively larger regret. To address the above limitation, inthis work, we explore a novel algorithm of Adaptive Online AUC Maximization(AdaOAM) which employs an adaptive gradient method that exploits the knowledgeof historical gradients to perform more informative online learning. The newadaptive updating strategy of the AdaOAM is less sensitive to the parametersettings and maintains the same time complexity as previous non-adaptivecounterparts. Additionally, we extend the algorithm to handle high-dimensionalsparse data (SAdaOAM) and address sparsity in the solution by performing lazygradient updating. We analyze the theoretical bounds and evaluate theirempirical performance on various types of data sets. The encouraging empiricalresults obtained clearly highlighted the effectiveness and efficiency of theproposed algorithms.
arxiv-15900-127 | ConfidentCare: A Clinical Decision Support System for Personalized Breast Cancer Screening | http://arxiv.org/pdf/1602.00374v1.pdf | author:Ahmed M. Alaa, Kyeong H. Moon, William Hsu, Mihaela van der Schaar category:cs.LG published:2016-02-01 summary:Breast cancer screening policies attempt to achieve timely diagnosis by theregular screening of apparently healthy women. Various clinical decisions areneeded to manage the screening process; those include: selecting the screeningtests for a woman to take, interpreting the test outcomes, and deciding whetheror not a woman should be referred to a diagnostic test. Such decisions arecurrently guided by clinical practice guidelines (CPGs), which represent aone-size-fits-all approach that are designed to work well on average for apopulation, without guaranteeing that it will work well uniformly over thatpopulation. Since the risks and benefits of screening are functions of eachpatients features, personalized screening policies that are tailored to thefeatures of individuals are needed in order to ensure that the right tests arerecommended to the right woman. In order to address this issue, we presentConfidentCare: a computer-aided clinical decision support system that learns apersonalized screening policy from the electronic health record (EHR) data.ConfidentCare operates by recognizing clusters of similar patients, andlearning the best screening policy to adopt for each cluster. A cluster ofpatients is a set of patients with similar features (e.g. age, breast density,family history, etc.), and the screening policy is a set of guidelines on whatactions to recommend for a woman given her features and screening test scores.ConfidentCare algorithm ensures that the policy adopted for every cluster ofpatients satisfies a predefined accuracy requirement with a high level ofconfidence. We show that our algorithm outperforms the current CPGs in terms ofcost-efficiency and false positive rates.
arxiv-15900-128 | Scene Invariant Crowd Segmentation and Counting Using Scale-Normalized Histogram of Moving Gradients (HoMG) | http://arxiv.org/pdf/1602.00386v1.pdf | author:Parthipan Siva, Mohammad Javad Shafiee, Mike Jamieson, Alexander Wong category:cs.CV published:2016-02-01 summary:The problem of automated crowd segmentation and counting has garneredsignificant interest in the field of video surveillance. This paper proposes anovel scene invariant crowd segmentation and counting algorithm designed withhigh accuracy yet low computational complexity in mind, which is key forwidespread industrial adoption. A novel low-complexity, scale-normalizedfeature called Histogram of Moving Gradients (HoMG) is introduced for highlyeffective spatiotemporal representation of individuals and crowds within avideo. Real-time crowd segmentation is achieved via boosted cascade of weakclassifiers based on sliding-window HoMG features, while linear SVM regressionof crowd-region HoMG features is employed for real-time crowd counting.Experimental results using multi-camera crowd datasets show that the proposedalgorithm significantly outperform state-of-the-art crowd counting algorithms,as well as achieve very promising crowd segmentation results, thusdemonstrating the efficacy of the proposed method for highly-accurate,real-time video-driven crowd analysis.
arxiv-15900-129 | Combining ConvNets with Hand-Crafted Features for Action Recognition Based on an HMM-SVM Classifier | http://arxiv.org/pdf/1602.00749v1.pdf | author:Pichao Wang, Zhaoyang Li, Yonghong Hou, Wanqing Li category:cs.CV published:2016-02-01 summary:This paper proposes a new framework for RGB-D-based action recognition thattakes advantages of hand-designed features from skeleton data and deeplylearned features from depth maps, and exploits effectively both the local andglobal temporal information. Specifically, depth and skeleton data are firstlyaugmented for deep learning and making the recognition insensitive to viewvariance. Secondly, depth sequences are segmented using the hand-craftedfeatures based on skeleton joints motion histogram to exploit the localtemporal information. All training se gments are clustered using an InfiniteGaussian Mixture Model (IGMM) through Bayesian estimation and labelled fortraining Convolutional Neural Networks (ConvNets) on the depth maps. Thus, adepth sequence can be reliably encoded into a sequence of segment labels.Finally, the sequence of labels is fed into a joint Hidden Markov Model andSupport Vector Machine (HMM-SVM) classifier to explore the global temporalinformation for final recognition.
arxiv-15900-130 | DeepCare: A Deep Dynamic Memory Model for Predictive Medicine | http://arxiv.org/pdf/1602.00357v1.pdf | author:Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG published:2016-02-01 summary:Personalized predictive medicine necessitates the modeling of patient illnessand care processes, which inherently have long-term temporal dependencies.Healthcare observations, recorded in electronic medical records, are episodicand irregular in time. We introduce DeepCare, an end-to-end deep dynamic neuralnetwork that reads medical records, stores previous illness history, inferscurrent illness states and predicts future medical outcomes. At the data level,DeepCare represents care episodes as vectors in space, models patient healthstate trajectories through explicit memory of historical records. Built on LongShort-Term Memory (LSTM), DeepCare introduces time parameterizations to handleirregular timed events by moderating the forgetting and consolidation of memorycells. DeepCare also incorporates medical interventions that change the courseof illness and shape future medical risk. Moving up to the health state level,historical and present health states are then aggregated through multiscaletemporal pooling, before passing through a neural network that estimates futureoutcomes. We demonstrate the efficacy of DeepCare for disease progressionmodeling, intervention recommendation, and future risk prediction. On twoimportant cohorts with heavy social and economic burden -- diabetes and mentalhealth -- the results show improved modeling and risk prediction accuracy.
arxiv-15900-131 | Learning Data Triage: Linear Decoding Works for Compressive MRI | http://arxiv.org/pdf/1602.00734v1.pdf | author:Yen-Huan Li, Volkan Cevher category:cs.IT cs.LG math.IT stat.ML published:2016-02-01 summary:The standard approach to compressive sampling considers recovering an unknowndeterministic signal with certain known structure, and designing thesub-sampling pattern and recovery algorithm based on the known structure. Thisapproach requires looking for a good representation that reveals the signalstructure, and solving a non-smooth convex minimization problem (e.g., basispursuit). In this paper, another approach is considered: We learn a goodsub-sampling pattern based on available training signals, without knowing thesignal structure in advance, and reconstruct an accordingly sub-sampled signalby computationally much cheaper linear reconstruction. We provide a theoreticalguarantee on the recovery error, and show via experiments on real-world MRIdata the effectiveness of the proposed compressive MRI scheme.
arxiv-15900-132 | Algorithm-Induced Prior for Image Restoration | http://arxiv.org/pdf/1602.00715v1.pdf | author:Stanley H. Chan category:cs.CV published:2016-02-01 summary:This paper studies a type of image priors that are constructed implicitlythrough the alternating direction method of multiplier (ADMM) algorithm, calledthe algorithm-induced prior. Different from classical image priors which aredefined before running the reconstruction algorithm, algorithm-induced priorsare defined by the denoising procedure used to replace one of the two modulesin the ADMM algorithm. Since such prior is not explicitly defined, analyzingthe performance has been difficult in the past. Focusing on the class of symmetric smoothing filters, this paper presents anexplicit expression of the prior induced by the ADMM algorithm. The new prioris reminiscent to the conventional graph Laplacian but with strongerreconstruction performance. It can also be shown that the overallreconstruction has an efficient closed-form implementation if the associatedsymmetric smoothing filter is low rank. The results are validated withexperiments on image inpainting.
arxiv-15900-133 | Improving Vertebra Segmentation through Joint Vertebra-Rib Atlases | http://arxiv.org/pdf/1602.00585v1.pdf | author:Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2016-02-01 summary:Accurate spine segmentation allows for improved identification andquantitative characterization of abnormalities of the vertebra, such asvertebral fractures. However, in existing automated vertebra segmentationmethods on computed tomography (CT) images, leakage into nearby bones such asribs occurs due to the close proximity of these visibly intense structures in a3D CT volume. To reduce this error, we propose the use of joint vertebra-ribatlases to improve the segmentation of vertebrae via multi-atlas joint labelfusion. Segmentation was performed and evaluated on CTs containing 106 thoracicand lumbar vertebrae from 10 pathological and traumatic spine patients on anindividual vertebra level basis. Vertebra atlases produced errors where thesegmentation leaked into the ribs. The use of joint vertebra-rib atlasesproduced a statistically significant increase in the Dice coefficient from 92.5$\pm$ 3.1% to 93.8 $\pm$ 2.1% for the left and right transverse processes and adecrease in the mean and max surface distance from 0.75 $\pm$ 0.60mm and 8.63$\pm$ 4.44mm to 0.30 $\pm$ 0.27mm and 3.65 $\pm$ 2.87mm, respectively.
arxiv-15900-134 | A Deep Learning Based Fast Image Saliency Detection Algorithm | http://arxiv.org/pdf/1602.00577v1.pdf | author:Hengyue Pan, Hui Jiang category:cs.CV published:2016-02-01 summary:In this paper, we propose a fast deep learning method for object saliencydetection using convolutional neural networks. In our approach, we use agradient descent method to iteratively modify the input images based on thepixel-wise gradients to reduce a pre-defined cost function, which is defined tomeasure the class-specific objectness and clamp the class-irrelevant outputs tomaintain image background. The pixel-wise gradients can be efficiently computedusing the back-propagation algorithm. We further apply SLIC superpixels and LABcolor based low level saliency features to smooth and refine the gradients. Ourmethods are quite computationally efficient, much faster than other deeplearning based saliency methods. Experimental results on two benchmark tasks,namely Pascal VOC 2012 and MSRA10k, have shown that our proposed methods cangenerate high-quality salience maps, at least comparable with many slow andcomplicated deep learning methods. Comparing with the pure low-level methods,our approach excels in handling many difficult images, which contain complexbackground, highly-variable salient objects, multiple objects, and/or verysmall salient objects.
arxiv-15900-135 | Transfer Learning Based on AdaBoost for Feature Selection from Multiple ConvNet Layer Features | http://arxiv.org/pdf/1602.00417v2.pdf | author:Jumabek Alikhanov, Myeong Hyeon Ga, Seunghyun Ko, Geun-Sik Jo category:cs.CV published:2016-02-01 summary:Convolutional Networks (ConvNets) are powerful models that learn hierarchiesof visual features, which could also be used to obtain image representationsfor transfer learning. The basic pipeline for transfer learning is to firsttrain a ConvNet on a large dataset (source task) and then use feed-forwardunits activation of the trained ConvNet as image representation for smallerdatasets (target task). Our key contribution is to demonstrate superiorperformance of multiple ConvNet layer features over single ConvNet layerfeatures. Combining multiple ConvNet layer features will result in more complexfeature space with some features being repetitive. This requires some form offeature selection. We use AdaBoost with single stumps to implicitly select onlydistinct features that are useful towards classification from concatenatedConvNet features. Experimental results show that using multiple ConvNet layeractivation features instead of single ConvNet layer features consistently willproduce superior performance. Improvements becomes significant as we increasethe distance between source task and the target task.
arxiv-15900-136 | Multi-object Classification via Crowdsourcing with a Reject Option | http://arxiv.org/pdf/1602.00575v1.pdf | author:Qunwei Li, Aditya Vempaty, Lav R. Varshney, Pramod K. Varshney category:cs.LG published:2016-02-01 summary:We explore the design of an effective crowdsourcing system for an $M$-aryclassification task. Crowd workers complete simple binary microtasks whoseresults are aggregated to give the final decision. We consider the novelscenario where the workers have a reject option so that they are allowed toskip microtasks when they are unable to or choose not to respond to binarymicrotasks. For example, in mismatched speech transcription, crowd workers whodo not know the language may not be able to respond to certain microtasksoutside their categorical perception. We present an aggregation approach usinga weighted majority voting rule, where each worker's response is assigned anoptimized weight to maximize crowd's classification performance. We evaluatesystem performance in both exact and asymptotic forms. Further, we consider thecase where there may be a set of greedy workers in the crowd for whom reward isvery important. Greedy workers may respond to a microtask even when they areunable to perform it reliably. We consider an oblivious and an expurgationstrategy for crowdsourcing with greedy workers, and develop an algorithm toadaptively switch between the two, based on the estimated fraction of greedyworkers in the anonymous crowd. Simulation results show performance improvementof the proposed approaches compared with conventional majority voting.
arxiv-15900-137 | Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers | http://arxiv.org/pdf/1602.00367v1.pdf | author:Yijun Xiao, Kyunghyun Cho category:cs.CL published:2016-02-01 summary:Document classification tasks were primarily tackled at word level. Recentresearch that works with character-level inputs shows several benefits overword-level approaches such as natural incorporation of morphemes and betterhandling of rare words. We propose a neural network architecture that utilizesboth convolution and recurrent layers to efficiently encode character inputs.We validate the proposed model on eight large scale document classificationtasks and compare with character-level convolution-only models. It achievescomparable performances with much less parameters.
arxiv-15900-138 | Real Time Video Quality Representation Classification of Encrypted HTTP Adaptive Video Streaming - the Case of Safari | http://arxiv.org/pdf/1602.00489v2.pdf | author:Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar, Itay Richman, Ofir Trabelsi category:cs.MM cs.CR cs.LG cs.NI published:2016-02-01 summary:The increasing popularity of HTTP adaptive video streaming services hasdramatically increased bandwidth requirements on operator networks, whichattempt to shape their traffic through Deep Packet Inspection (DPI). However,Google and certain content providers have started to encrypt their videoservices. As a result, operators often encounter difficulties in shaping theirencrypted video traffic via DPI. This highlights the need for new trafficclassification methods for encrypted HTTP adaptive video streaming to enablesmart traffic shaping. These new methods will have to effectively estimate thequality representation layer and playout buffer. We present a new method andshow for the first time that video quality representation classification for(YouTube) encrypted HTTP adaptive streaming is possible. We analyze theperformance of this classification method with Safari over HTTPS. Based on alarge number of offline and online traffic classification experiments, wedemonstrate that it can independently classify, in real time, every videosegment into one of the quality representation layers with 97.18% averageaccuracy.
arxiv-15900-139 | Graph-based Predictable Feature Analysis | http://arxiv.org/pdf/1602.00554v1.pdf | author:Björn Weghenkel, Asja Fischer, Laurenz Wiskott category:cs.LG published:2016-02-01 summary:We propose a new method for the unsupervised extraction of predictablefeatures from high-dimensional time-series, where high predictability isunderstood very generically as low variance in the distribution of the nextdata point given the current one. We show how this objective can be understoodin terms of graph embedding as well as how it corresponds to theinformation-theoretic measure of excess entropy in special cases.Experimentally, we compare the approach to two other algorithms for theextraction of predictable features, namely ForeCA and PFA, and show how it isable to outperform them in certain settings.
arxiv-15900-140 | Visualizing Large-scale and High-dimensional Data | http://arxiv.org/pdf/1602.00370v2.pdf | author:Jian Tang, Jingzhou Liu, Ming Zhang, Qiaozhu Mei category:cs.LG cs.HC published:2016-02-01 summary:We study the problem of visualizing large-scale and high-dimensional data ina low-dimensional (typically 2D or 3D) space. Much success has been reportedrecently by techniques that first compute a similarity structure of the datapoints and then project them into a low-dimensional space with the structurepreserved. These two steps suffer from considerable computational costs,preventing the state-of-the-art methods such as the t-SNE from scaling tolarge-scale and high-dimensional data (e.g., millions of data points andhundreds of dimensions). We propose the LargeVis, a technique that firstconstructs an accurately approximated K-nearest neighbor graph from the dataand then layouts the graph in the low-dimensional space. Comparing to t-SNE,LargeVis significantly reduces the computational cost of the graph constructionstep and employs a principled probabilistic model for the visualization step,the objective of which can be effectively optimized through asynchronousstochastic gradient descent with a linear time complexity. The whole procedurethus easily scales to millions of high-dimensional data points. Experimentalresults on real-world data sets demonstrate that the LargeVis outperforms thestate-of-the-art methods in both efficiency and effectiveness. Thehyper-parameters of LargeVis are also much more stable over different datasets.
arxiv-15900-141 | Semi-supervised K-means++ | http://arxiv.org/pdf/1602.00360v1.pdf | author:Jordan Yoder, Carey E. Priebe category:stat.ML published:2016-02-01 summary:Traditionally, practitioners initialize the {\tt k-means} algorithm withcenters chosen uniformly at random. Randomized initialization with unevenweights ({\tt k-means++}) has recently been used to improve the performanceover this strategy in cost and run-time. We consider the k-means problem withsemi-supervised information, where some of the data are pre-labeled, and weseek to label the rest according to the minimum cost solution. By extending the{\tt k-means++} algorithm and analysis to account for the labels, we derive animproved theoretical bound on expected cost and observe improved performance insimulated and real data examples. This analysis provides theoreticaljustification for a roughly linear semi-supervised clustering algorithm.
arxiv-15900-142 | PAC-Bayesian Online Clustering | http://arxiv.org/pdf/1602.00522v1.pdf | author:Le Li, Benjamin Guedj, Sébastien Loustau category:stat.ML math.ST stat.TH published:2016-02-01 summary:This paper addresses the online clustering problem. When faced with highfrequency streams of data, clustering raises theoretical and algorithmicpitfalls. Working under a sparsity assumption, a new online clusteringalgorithm is introduced. Our procedure relies on the PAC-Bayesian approach,allowing for a dynamic (i.e., time-dependent) estimation of the number ofclusters. Its theoretical merits are supported by sparsity regret bounds, andan RJMCMC-flavored implementation called PACO is proposed along with numericalexperiments to assess its potential.
arxiv-15900-143 | I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video Streaming Title Classification | http://arxiv.org/pdf/1602.00490v1.pdf | author:Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar category:cs.MM cs.LG cs.NI published:2016-02-01 summary:Previous research has shown that information can be extracted from encryptedmultimedia streams. This includes video titles classification of non HTTPadaptive streams (non-HAS). This paper presents an algorithm for\emph{encrypted HTTP adaptive video streaming title classification}. Weevaluated our algorithm on a new YouTube popular videos dataset that wascollected from the internet under real-world network conditions. We provide thedataset and the crawler for future research. Our algorithm's classificationaccuracy is 98\%.
arxiv-15900-144 | Marvin: Semantic annotation using multiple knowledge sources | http://arxiv.org/pdf/1602.00515v2.pdf | author:Nikola Milosevic category:cs.AI cs.CL published:2016-02-01 summary:People are producing more written material then anytime in the history. Theincrease is so high that professionals from the various fields are no more ableto cope with this amount of publications. Text mining tools can offer tools tohelp them and one of the tools that can aid information retrieval andinformation extraction is semantic text annotation. In this report we presentMarvin, a text annotator written in Java, which can be used as a command linetool and as a Java library. Marvin is able to annotate text using multiplesources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.
arxiv-15900-145 | An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection | http://arxiv.org/pdf/1602.00426v1.pdf | author:Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Chia-Hsiang Liu, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG published:2016-02-01 summary:In this work we aim to discover high quality speech features and linguisticunits directly from unlabeled speech data in a zero resource scenario. Theresults are evaluated using the metrics and corpora proposed in the ZeroResource Speech Challenge organized at Interspeech 2015. A Multi-layeredAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple setsof acoustic tokens from the given corpus. Each acoustic token set is specifiedby a set of hyperparameters that describe the model configuration. These setsof acoustic tokens carry different characteristics fof the given corpus and thelanguage behind, thus can be mutually reinforced. The multiple sets of tokenlabels are then used as the targets of a Multi-target Deep Neural Network(MDNN) trained on low-level acoustic features. Bottleneck features extractedfrom the MDNN are then used as the feedback input to the MAT and the MDNNitself in the next iteration. We call this iterative deep learning frameworkthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), whichgenerates both high quality speech features for the Track 1 of the Challengeand acoustic tokens for the Track 2 of the Challenge. In addition, we performedextra experiments on the same corpora on the application of query-by-examplespoken term detection. The experimental results showed the iterative deeplearning framework of MAT-DNN improved the detection performance due to betterunderlying speech features and acoustic tokens.
arxiv-15900-146 | Cluster-Seeking James-Stein Estimators | http://arxiv.org/pdf/1602.00542v2.pdf | author:K. Pavan Srinath, Ramji Venkataramanan category:cs.IT math.IT math.ST stat.ML stat.TH published:2016-02-01 summary:This paper considers the problem of estimating a high-dimensional vector ofparameters $\boldsymbol{\theta} \in \mathbb{R}^n$ from a noisy observation. Thenoise vector is i.i.d. Gaussian with known variance. For a squared-error lossfunction, the James-Stein (JS) estimator is known to dominate the simplemaximum-likelihood (ML) estimator when the dimension $n$ exceeds two. TheJS-estimator shrinks the observed vector towards the origin, and the riskreduction over the ML-estimator is greatest for $\boldsymbol{\theta}$ that lieclose to the origin. JS-estimators can be generalized to shrink the datatowards any target subspace. Such estimators also dominate the ML-estimator,but the risk reduction is significant only when $\boldsymbol{\theta}$ liesclose to the subspace. This leads to the question: in the absence of priorinformation about $\boldsymbol{\theta}$, how do we design estimators that givesignificant risk reduction over the ML-estimator for a wide range of$\boldsymbol{\theta}$? In this paper, we propose shrinkage estimators that attempt to infer thestructure of $\boldsymbol{\theta}$ from the observed data in order to constructa good attracting subspace. In particular, the components of the observedvector are separated into clusters, and the elements in each cluster shrunktowards a common attractor. The number of clusters and the attractor for eachcluster are determined from the observed vector. We provide concentrationresults for the squared-error loss and convergence results for the risk of theproposed estimators. The results show that the estimators give significant riskreduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$,particularly for large $n$. Simulation results are provided to support thetheoretical claims.
arxiv-15900-147 | Greedy Deep Dictionary Learning | http://arxiv.org/pdf/1602.00203v1.pdf | author:Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa category:cs.LG cs.AI stat.ML published:2016-01-31 summary:In this work we propose a new deep learning tool called deep dictionarylearning. Multi-level dictionaries are learnt in a greedy fashion, one layer ata time. This requires solving a simple (shallow) dictionary learning problem,the solution to this is well known. We apply the proposed technique on somebenchmark deep learning datasets. We compare our results with other deeplearning tools like stacked autoencoder and deep belief network; and state ofthe art supervised dictionary learning tools like discriminative KSVD and labelconsistent KSVD. Our method yields better results than all.
arxiv-15900-148 | Unsupervised Deep Hashing for Large-scale Visual Search | http://arxiv.org/pdf/1602.00206v1.pdf | author:Zhaoqiang Xia, Xiaoyi Feng, Jinye Peng, Abdenour Hadid category:cs.CV cs.LG published:2016-01-31 summary:Learning based hashing plays a pivotal role in large-scale visual search.However, most existing hashing algorithms tend to learn shallow models that donot seek representative binary codes. In this paper, we propose a novel hashingapproach based on unsupervised deep learning to hierarchically transformfeatures into hash codes. Within the heterogeneous deep hashing framework, theautoencoder layers with specific constraints are considered to model thenonlinear mapping between features and binary codes. Then, a RestrictedBoltzmann Machine (RBM) layer with constraints is utilized to reduce thedimension in the hamming space. Extensive experiments on the problem of visualsearch demonstrate the competitiveness of our proposed approach compared tostate-of-the-art.
arxiv-15900-149 | Dimensionality Reduction via Regression in Hyperspectral Imagery | http://arxiv.org/pdf/1602.00214v1.pdf | author:Valero Laparra, Jesus Malo, Gustau Camps-Valls category:stat.ML cs.CV published:2016-01-31 summary:This paper introduces a new unsupervised method for dimensionality reductionvia regression (DRR). The algorithm belongs to the family of invertibletransforms that generalize Principal Component Analysis (PCA) by usingcurvilinear instead of linear features. DRR identifies the nonlinear featuresthrough multivariate regression to ensure the reduction in redundancy betweenhe PCA coefficients, the reduction of the variance of the scores, and thereduction in the reconstruction error. More importantly, unlike other nonlineardimensionality reduction methods, the invertibility, volume-preservation, andstraightforward out-of-sample extension, makes DRR interpretable and easy toapply. The properties of DRR enable learning a more broader class of datamanifolds than the recently proposed Non-linear Principal Components Analysis(NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performanceof the representation in reducing the dimensionality of remote sensing data. Inparticular, we tackle two common problems: processing very high dimensionalspectral information such as in hyperspectral image sounding data, and dealingwith spatial-spectral image patches of multispectral images. Both settings posecollinearity and ill-determination problems. Evaluation of the expressive powerof the features is assessed in terms of truncation error, estimatingatmospheric variables, and surface land cover classification error. Resultsshow that DRR outperforms linear PCA and recently proposed invertibleextensions based on neural networks (NLPCA) and univariate regressions (PPA).
arxiv-15900-150 | Learning a low-rank shared dictionary for object classification | http://arxiv.org/pdf/1602.00310v2.pdf | author:Tiep H. Vu, Vishal Monga category:cs.CV published:2016-01-31 summary:Despite the fact that different objects possess distinct class-specificfeatures, they also usually share common patterns. Inspired by thisobservation, we propose a novel method to explicitly and simultaneously learn aset of common patterns as well as class-specific features for classification.Our dictionary learning framework is hence characterized by both a shareddictionary and particular (class-specific) dictionaries. For the shareddictionary, we enforce a low-rank constraint, i.e. claim that its spanningsubspace should have low dimension and the coefficients corresponding to thisdictionary should be similar. For the particular dictionaries, we impose onthem the well-known constraints stated in the Fisher discrimination dictionarylearning (FDDL). Further, we propose a new fast and accurate algorithm to solvethe sparse coding problems in the learning step, accelerating its convergence.The said algorithm could also be applied to FDDL and its extensions.Experimental results on widely used image databases establish the advantages ofour method over state-of-the-art dictionary learning methods.
arxiv-15900-151 | Image Denoising with Kernels based on Natural Image Relations | http://arxiv.org/pdf/1602.00217v1.pdf | author:Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo category:cs.CV stat.ML published:2016-01-31 summary:A successful class of image denoising methods is based on Bayesian approachesworking in wavelet representations. However, analytical estimates can beobtained only for particular combinations of analytical models of signal andnoise, thus precluding its straightforward extension to deal with otherarbitrary noise sources. In this paper, we propose an alternative non-explicitway to take into account the relations among natural image wavelet coefficientsfor denoising: we use support vector regression (SVR) in the wavelet domain toenforce these relations in the estimated signal. Since relations among thecoefficients are specific to the signal, the regularization property of SVR isexploited to remove the noise, which does not share this feature. The specificsignal relations are encoded in an anisotropic kernel obtained from mutualinformation measures computed on a representative image database. Trainingconsiders minimizing the Kullback-Leibler divergence (KLD) between theestimated and actual probability functions of signal and noise in order toenforce similarity. Due to its non-parametric nature, the method can eventuallycope with different noise sources without the need of an explicitre-formulation, as it is strictly necessary under parametric Bayesianformalisms. Results under several noise levels and noise sources show that: (1)the proposed method outperforms conventional wavelet methods that assumecoefficient independence, (2) it is similar to state-of-the-art methods that doexplicitly include these relations when the noise source is Gaussian, and (3)it gives better numerical and visual performance when more complex, realisticnoise sources are considered. Therefore, the proposed machine learning approachcan be seen as a more flexible (model-free) alternative to the explicitdescription of wavelet coefficient relations for image denoising.
arxiv-15900-152 | Tracing liquid level and material boundaries in transparent vessels using the graph cut computer vision approach | http://arxiv.org/pdf/1602.00177v1.pdf | author:Sagi Eppel category:cs.CV published:2016-01-31 summary:Detection of boundaries of materials stored in transparent vessels isessential for identifying properties such as liquid level and phase boundaries,which are vital for controlling numerous processes in the industry andchemistry laboratory. This work presents a computer vision method foridentifying the boundary of materials in transparent vessels using thegraph-cut algorithm. The method receives an image of a transparent vesselcontaining a material and the contour of the vessel in the image. The boundaryof the material in the vessel is found by the graph cut method. In general themethod uses the vessel region of the image to create a graph, where pixels arevertices, and the cost of an edge between two pixels is inversely correlatedwith their intensity difference. The bottom 10% of the vessel region in theimage is assumed to correspond to the material phase and defined as the graphand source. The top 10% of the pixels in the vessels are assumed to correspondto the air phase and defined as the graph sink. The minimal cut that splits theresulting graph between the source and sink (hence, material and air) is tracedusing the max-flow/min-cut approach. This cut corresponds to the boundary ofthe material in the image. The method gave high accuracy in boundaryrecognition for a wide range of liquid, solid, granular and powder materials invarious glass vessels from everyday life and the chemistry laboratory, such asbottles, jars, Glasses, Chromotography colums and separatory funnels.
arxiv-15900-153 | Additive Approximations in High Dimensional Nonparametric Regression via the SALSA | http://arxiv.org/pdf/1602.00287v2.pdf | author:Kirthevasan Kandasamy, Yaoliang Yu category:stat.ML cs.LG published:2016-01-31 summary:High dimensional nonparametric regression is an inherently difficult problemwith known lower bounds depending exponentially in dimension. A popularstrategy to alleviate this curse of dimensionality has been to use additivemodels of \emph{first order}, which model the regression function as a sum ofindependent functions on each dimension. Though useful in controlling thevariance of the estimate, such models are often too restrictive in practicalsettings. Between non-additive models which often have large variance and firstorder additive models which have large bias, there has been little work toexploit the trade-off in the middle via additive models of intermediate order.In this work, we propose SALSA, which bridges this gap by allowing interactionsbetween variables, but controls model capacity by limiting the order ofinteractions. SALSA minimises the residual sum of squares with squared RKHSnorm penalties. Algorithmically, it can be viewed as Kernel Ridge Regressionwith an additive kernel. When the regression function is additive, the excessrisk is only polynomial in dimension. Using the Girard-Newton formulae, weefficiently sum over a combinatorial number of terms in the additive expansion.Via a comparison on $16$ real datasets, we show that our method is competitiveagainst $21$ other alternatives.
arxiv-15900-154 | Novel Views of Objects from a Single Image | http://arxiv.org/pdf/1602.00328v1.pdf | author:Konstantinos Rematas, Chuong Nguyen, Tobias Ritschel, Mario Fritz, Tinne Tuytelaars category:cs.CV cs.GR published:2016-01-31 summary:Taking an image of an object is at its core a lossy process. The richinformation about the three-dimensional structure of the world is flattened toan image plane and decisions such as viewpoint and camera parameters are finaland not easily revertible. As a consequence, possibilities of changingviewpoint are limited. Given a single image depicting an object, novel-viewsynthesis is the task of generating new images that render the object from adifferent viewpoint than the one given. The main difficulty is to synthesizethe parts that are disoccluded; disocclusion occurs when parts of an object arehidden by the object itself under a specific viewpoint. In this work, we showhow to improve novel-view synthesis by making use of the correlations observedin 3D models and applying them to new image instances. We propose a techniqueto use the structural information extracted from a 3D model that matches theimage object in terms of viewpoint and shape. For the latter part, we proposean efficient 2D-to-3D alignment method that associates precisely the imageappearance with the 3D model geometry with minimal user interaction. Ourtechnique is able to simulate plausible viewpoint changes for a variety ofobject classes within seconds. Additionally, we show that our synthesizedimages can be used as additional training data that improves the performance ofstandard object detectors.
arxiv-15900-155 | Principal Polynomial Analysis | http://arxiv.org/pdf/1602.00221v1.pdf | author:Valero Laparra, Sandra Jiménez, Devis Tuia, Gustau Camps-Valls, Jesús Malo category:stat.ML published:2016-01-31 summary:This paper presents a new framework for manifold learning based on a sequenceof principal polynomials that capture the possibly nonlinear nature of thedata. The proposed Principal Polynomial Analysis (PPA) generalizes PCA bymodeling the directions of maximal variance by means of curves, instead ofstraight lines. Contrarily to previous approaches, PPA reduces to performingsimple univariate regressions, which makes it computationally feasible androbust. Moreover, PPA shows a number of interesting analytical properties.First, PPA is a volume-preserving map, which in turn guarantees the existenceof the inverse. Second, such an inverse can be obtained in closed form.Invertibility is an important advantage over other learning methods, because itpermits to understand the identified features in the input domain where thedata has physical meaning. Moreover, it allows to evaluate the performance ofdimensionality reduction in sensible (input-domain) units. Volume preservationalso allows an easy computation of information theoretic quantities, such asthe reduction in multi-information after the transform. Third, the analyticalnature of PPA leads to a clear geometrical interpretation of the manifold: itallows the computation of Frenet-Serret frames (local features) and ofgeneralized curvatures at any point of the space. And fourth, the analyticalJacobian allows the computation of the metric induced by the data, thusgeneralizing the Mahalanobis distance. These properties are demonstratedtheoretically and illustrated experimentally. The performance of PPA isevaluated in dimensionality and redundancy reduction, in both synthetic andreal datasets from the UCI repository.
arxiv-15900-156 | Variance-Reduced Second-Order Methods | http://arxiv.org/pdf/1602.00223v1.pdf | author:Luo Luo, Zihao Chen, Zhihua Zhang, Wu-Jun Li category:cs.LG stat.ML published:2016-01-31 summary:In this paper, we discuss the problem of minimizing the sum of two convexfunctions: a smooth function plus a non-smooth function. Further, the smoothpart can be expressed by the average of a large number of smooth componentfunctions, and the non-smooth part is equipped with a simple proximal mapping.We propose a proximal stochastic second-order method, which is efficient andscalable. It incorporates the Hessian in the smooth part of the function andexploits multistage scheme to reduce the variance of the stochastic gradient.We prove that our method can achieve linear rate of convergence.
arxiv-15900-157 | Bandits meet Computer Architecture: Designing a Smartly-allocated Cache | http://arxiv.org/pdf/1602.00309v1.pdf | author:Yonatan Glassner, Koby Crammer category:cs.LG published:2016-01-31 summary:In many embedded systems, such as imaging sys- tems, the system has a singledesignated purpose, and same threads are executed repeatedly. Profiling threadbehavior, allows the system to allocate each thread its resources in a way thatimproves overall system performance. We study an online resource al-locationproblem,wherearesourcemanagersimulta- neously allocates resources(exploration), learns the impact on the different consumers (learning) and im-proves allocation towards optimal performance (ex- ploitation). We build on therich framework of multi- armed bandits and present online and offline algo-rithms. Through extensive experiments with both synthetic data and real-worldcache allocation to threads we show the merits and properties of our al-gorithms
arxiv-15900-158 | Order-aware Convolutional Pooling for Video Based Action Recognition | http://arxiv.org/pdf/1602.00224v1.pdf | author:Peng Wang, Lingqiao Liu, Chunhua Shen, Heng Tao Shen category:cs.CV published:2016-01-31 summary:Most video based action recognition approaches create the video-levelrepresentation by temporally pooling the features extracted at each frame. Thepooling methods that they adopt, however, usually completely or partiallyneglect the dynamic information contained in the temporal domain, which mayundermine the discriminative power of the resulting video representation sincethe video sequence order could unveil the evolution of a specific event oraction. To overcome this drawback and explore the importance of incorporatingthe temporal order information, in this paper we propose a novel temporalpooling approach to aggregate the frame-level features. Inspired by thecapacity of Convolutional Neural Networks (CNN) in making use of the internalstructure of images for information abstraction, we propose to apply thetemporal convolution operation to the frame-level representations to extractthe dynamic information. However, directly implementing this idea on theoriginal high-dimensional feature would inevitably result in parameterexplosion. To tackle this problem, we view the temporal evolution of the feature valueat each feature dimension as a 1D signal and learn a unique convolutionalfilter bank for each of these 1D signals. We conduct experiments on twochallenging video-based action recognition datasets, HMDB51 and UCF101; anddemonstrate that the proposed method is superior to the conventional poolingmethods.
arxiv-15900-159 | Feature Selection for Regression Problems Based on the Morisita Estimator of Intrinsic Dimension | http://arxiv.org/pdf/1602.00216v5.pdf | author:Jean Golay, Michael Leuenberger, Mikhail Kanevski category:stat.ML cs.LG published:2016-01-31 summary:Data acquisition, storage and management have been improved, while the keyfactors of many phenomena are not well known. Consequently, irrelevant andredundant features artificially increase the size of datasets, whichcomplicates learning tasks, such as regression. To address this problem,feature selection methods have been proposed. This paper introduces a newsupervised filter based on the Morisita estimator of intrinsic dimension. Itcan identify relevant features and distinguish between redundant and irrelevantinformation. Besides, it offers a clear graphical representation of the resultsand it can be easily implemented in different programming languages.Comprehensive numerical experiments are conducted using simulated datasetscharacterized by different levels of complexity, sample size and noise. Thesuggested algorithm is also successfully tested on a selection of real worldapplications and compared with RReliefF using extreme learning machine. Inaddition, a new measure of feature relevance is presented and discussed.
arxiv-15900-160 | Iterative Gaussianization: from ICA to Random Rotations | http://arxiv.org/pdf/1602.00229v1.pdf | author:Valero Laparra, Gustavo Camps-Valls, Jesús Malo category:stat.ML published:2016-01-31 summary:Most signal processing problems involve the challenging task ofmultidimensional probability density function (PDF) estimation. In this work,we propose a solution to this problem by using a family of Rotation-basedIterative Gaussianization (RBIG) transforms. The general framework consists ofthe sequential application of a univariate marginal Gaussianization transformfollowed by an orthonormal transform. The proposed procedure looks fordifferentiable transforms to a known PDF so that the unknown PDF can beestimated at any point of the original domain. In particular, we aim at a zeromean unit covariance Gaussian for convenience. RBIG is formally similar toclassical iterative Projection Pursuit (PP) algorithms. However, we show that,unlike in PP methods, the particular class of rotations used has no specialqualitative relevance in this context, since looking for interestingness is nota critical issue for PDF estimation. The key difference is that our approachfocuses on the univariate part (marginal Gaussianization) of the problem ratherthan on the multivariate part (rotation). This difference implies that one mayselect the most convenient rotation suited to each practical application. Thedifferentiability, invertibility and convergence of RBIG are theoretically andexperimentally analyzed. Relation to other methods, such as RadialGaussianization (RG), one-class support vector domain description (SVDD), anddeep neural networks (DNN) is also pointed out. The practical performance ofRBIG is successfully illustrated in a number of multidimensional problems suchas image synthesis, classification, denoising, and multi-informationestimation.
arxiv-15900-161 | Bit-Planes: Dense Subpixel Alignment of Binary Descriptors | http://arxiv.org/pdf/1602.00307v1.pdf | author:Hatem Alismail, Brett Browning, Simon Lucey category:cs.CV published:2016-01-31 summary:Binary descriptors have been instrumental in the recent evolution ofcomputationally efficient sparse image alignment algorithms. Increasingly,however, the vision community is interested in dense image alignment methods,which are more suitable for estimating correspondences from high frame ratecameras as they do not rely on exhaustive search. However, classic densealignment approaches are sensitive to illumination change. In this paper, wepropose an easy to implement and low complexity dense binary descriptor, whichwe refer to as bit-planes, that can be seamlessly integrated within amulti-channel Lucas & Kanade framework. This novel approach combines therobustness of binary descriptors with the speed and accuracy of dense alignmentmethods. The approach is demonstrated on a template tracking problem achievingstate-of-the-art robustness and faster than real-time performance on consumerlaptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+fps on an iPad Air 2).
arxiv-15900-162 | Trainlets: Dictionary Learning in High Dimensions | http://arxiv.org/pdf/1602.00212v4.pdf | author:Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, Michael Elad category:cs.CV published:2016-01-31 summary:Sparse representations has shown to be a very powerful model for real worldsignals, and has enabled the development of applications with notableperformance. Combined with the ability to learn a dictionary from signalexamples, sparsity-inspired algorithms are often achieving state-of-the-artresults in a wide variety of tasks. Yet, these methods have traditionally beenrestricted to small dimensions mainly due to the computational constraints thatthe dictionary learning problem entails. In the context of image processing,this implies handling small image patches. In this work we show how toefficiently handle bigger dimensions and go beyond the small patches insparsity-based signal and image processing methods. We build our approach basedon a new cropped wavelet decomposition, which enables a multi-scale analysiswith virtually no border effects. We then employ this as the base dictionarywithin a double sparsity model to enable the training of adaptive dictionaries.To cope with the increase of training data, while at the same time improvingthe training performance, we present an Online Sparse Dictionary Learning(OSDL) algorithm to train this model effectively, enabling it to handlemillions of examples. This work shows that dictionary learning can be up-scaledto tackle a new level of signal dimensions, obtaining large adaptable atomsthat we call trainlets.
arxiv-15900-163 | Nonlinearities and Adaptation of Color Vision from Sequential Principal Curves Analysis | http://arxiv.org/pdf/1602.00236v1.pdf | author:Valero Laparra, Sandra Jiménez, Gustavo Camps-Valls, Jesús Malo category:stat.ML q-bio.NC published:2016-01-31 summary:Mechanisms of human color vision are characterized by two phenomenologicalaspects: the system is nonlinear and adaptive to changing environments.Conventional attempts to derive these features from statistics use separatearguments for each aspect. The few statistical approaches that do consider bothphenomena simultaneously follow parametric formulations based on empiricalmodels. Therefore, it may be argued that the behavior does not come directlyfrom the color statistics but from the convenient functional form adopted. Inaddition, many times the whole statistical analysis is based on simplifieddatabases that disregard relevant physical effects in the input signal, as forinstance by assuming flat Lambertian surfaces. Here we address the simultaneousstatistical explanation of (i) the nonlinear behavior of achromatic andchromatic mechanisms in a fixed adaptation state, and (ii) the change of suchbehavior. Both phenomena emerge directly from the samples through a singledata-driven method: the Sequential Principal Curves Analysis (SPCA) with localmetric. SPCA is a new manifold learning technique to derive a set of sensorsadapted to the manifold using different optimality criteria. A new database ofcolorimetrically calibrated images of natural objects under these illuminantswas collected. The results obtained by applying SPCA show that thepsychophysical behavior on color discrimination thresholds, discount of theilluminant and corresponding pairs in asymmetric color matching, emergedirectly from realistic data regularities assuming no a priori functional form.These results provide stronger evidence for the hypothesis of a statisticallydriven organization of color sensors. Moreover, the obtained results suggestthat color perception at this low abstraction level may be guided by an errorminimization strategy rather than by the information maximization principle.
arxiv-15900-164 | DOLDA - a regularized supervised topic model for high-dimensional multi-class regression | http://arxiv.org/pdf/1602.00260v1.pdf | author:Måns Magnusson, Leif Jonsson, Mattias Villani category:stat.ML published:2016-01-31 summary:We introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), asupervised topic model for multi-class classification that can handle both manyclasses as well as many covariates. To handle many classes we use the recentlyproposed Diagonal Orthant (DO) probit model together with an efficienthorseshoe prior for variable selection/shrinkage. An important advantage ofDOLDA is that learned topics are directly connected to individual classeswithout the need for a reference class. We propose a computationally efficientparallel Gibbs sampler for the new model. We study the model properties on anIMDb dataset with roughly 8000 documents, and document preliminary results in abug prediction context where 118 components are predicted using 100 topics frombug reports.
arxiv-15900-165 | WASSUP? LOL : Characterizing Out-of-Vocabulary Words in Twitter | http://arxiv.org/pdf/1602.00293v1.pdf | author:Suman Kalyan Maity, Chaitanya Sarda, Anshit Chaudhary, Abhijeet Patil, Shraman Kumar, Akash Mondal, Animesh Mukherjee category:cs.CL cs.SI published:2016-01-31 summary:Language in social media is mostly driven by new words and spellings that areconstantly entering the lexicon thereby polluting it and resulting in highdeviation from the formal written version. The primary entities of suchlanguage are the out-of-vocabulary (OOV) words. In this paper, we study varioussociolinguistic properties of the OOV words and propose a classification modelto categorize them into at least six categories. We achieve 81.26% accuracywith high precision and recall. We observe that the content features are themost discriminative ones followed by lexical and context features.
arxiv-15900-166 | SCOPE: Scalable Composite Optimization for Learning on Spark | http://arxiv.org/pdf/1602.00133v2.pdf | author:Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, Wu-jun Li category:stat.ML cs.LG published:2016-01-30 summary:Many machine learning models, such as lo- gistic regression (LR) and supportvector ma- chine (SVM), can be formulated as compos- ite optimization problems.Recently, many dis- tributed stochastic optimization (DSO) methods have beenproposed to solve the large-scale com- posite optimization problems, which haveshown better performance than traditional batch meth- ods. However, most ofthese DSO methods might not be scalable enough. In this paper, we propose anovel DSO method, called scalable composite optimization for learning (SCOPE),and implement it on the fault-tolerant distributed platform Spark. SCOPE isboth computation- efficient and communication-efficient. Theoret- ical analysisshows that SCOPE is convergent with linear convergence rate when the objectivefunction is strongly convex. Furthermore, em- pirical results on real datasetsshow that SCOPE can outperform other state-of-the-art distributed learningmethods on Spark, including both batch learning methods and DSO methods.
arxiv-15900-167 | Convolutional Pose Machines | http://arxiv.org/pdf/1602.00134v4.pdf | author:Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh category:cs.CV published:2016-01-30 summary:Pose Machines provide a sequential prediction framework for learning richimplicit spatial models. In this work we show a systematic design for howconvolutional networks can be incorporated into the pose machine framework forlearning image features and image-dependent spatial models for the task of poseestimation. The contribution of this paper is to implicitly model long-rangedependencies between variables in structured prediction tasks such asarticulated pose estimation. We achieve this by designing a sequentialarchitecture composed of convolutional networks that directly operate on beliefmaps from previous stages, producing increasingly refined estimates for partlocations, without the need for explicit graphical model-style inference. Ourapproach addresses the characteristic difficulty of vanishing gradients duringtraining by providing a natural learning objective function that enforcesintermediate supervision, thereby replenishing back-propagated gradients andconditioning the learning procedure. We demonstrate state-of-the-artperformance and outperform competing methods on standard benchmarks includingthe MPII, LSP, and FLIC datasets.
arxiv-15900-168 | Extracting Keyword for Disambiguating Name Based on the Overlap Principle | http://arxiv.org/pdf/1602.00104v1.pdf | author:Mahyuddin K. M. Nasution category:cs.IR cs.CL F.2.2; I.2.7 published:2016-01-30 summary:Name disambiguation has become one of the main themes in the Semantic Webagenda. The semantic web is an extension of the current Web in whichinformation is not only given well-defined meaning, but also has many purposesthat contain the ambiguous naturally or a lot of thing came with the overlap,mainly deals with the persons name. Therefore, we develop an approach toextract keywords from web snippet with utilizing the overlap principle, aconcept to understand things with ambiguous, whereby features of person aregenerated for dealing with the variety of web, the web is steadily gainingground in the semantic research.
arxiv-15900-169 | Spectrum Estimation from Samples | http://arxiv.org/pdf/1602.00061v1.pdf | author:Weihao Kong, Gregory Valiant category:cs.LG stat.ML published:2016-01-30 summary:We consider the problem of approximating the set of eigenvalues of thecovariance matrix of a multivariate distribution (equivalently, the problem ofapproximating the "population spectrum"), given access to samples drawn fromthe distribution. The eigenvalues of the covariance of a distribution containbasic information about the distribution, including the presence or lack ofstructure in the distribution, the effective dimensionality of thedistribution, and the applicability of higher-level machine learning andmultivariate statistical tools. We consider this fundamental recovery problemin the regime where the number of samples is comparable, or even sublinear inthe dimensionality of the distribution in question. First, we propose atheoretically optimal and computationally efficient algorithm for recoveringthe moments of the eigenvalues---the \emph{Schatten} $p$-norms of thepopulation covariance matrix. We then leverage this accurate moment recovery,via an earthmover argument, to show that the vector of eigenvalues can beaccurately recovered. In addition to our theoretical results, we show that ourapproach performs well in practice for a broad range of distributions andsample sizes.
arxiv-15900-170 | DNA-inspired online behavioral modeling and its application to spambot detection | http://arxiv.org/pdf/1602.00110v1.pdf | author:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi category:cs.SI cs.CR cs.LG H.2.8.d; I.2.4 published:2016-01-30 summary:We propose a strikingly novel, simple, and effective approach to model onlineuser behavior: we extract and analyze digital DNA sequences from user onlineactions and we use Twitter as a benchmark to test our proposal. We obtain anincisive and compact DNA-inspired characterization of user actions. Then, weapply standard DNA analysis techniques to discriminate between genuine andspambot accounts on Twitter. An experimental campaign supports our proposal,showing its effectiveness and viability. To the best of our knowledge, we arethe first ones to identify and adapt DNA-inspired techniques to online userbehavioral modeling. While Twitter spambot detection is a specific use case ona specific social media, our proposed methodology is platform and technologyagnostic, hence paving the way for diverse behavioral characterization tasks.
arxiv-15900-171 | Deep Learning For Smile Recognition | http://arxiv.org/pdf/1602.00172v1.pdf | author:Patrick O. Glauner category:cs.CV cs.LG cs.NE published:2016-01-30 summary:Inspired by recent successes of deep learning in computer vision, we proposea novel application of deep convolutional neural networks to facial expressionrecognition, in particular smile recognition. A smile recognition test accuracyof 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action(DISFA) database, significantly outperforming existing approaches based onhand-crafted features with accuracies ranging from 65.55% to 79.67%. Thenovelty of this approach includes a comprehensive model selection of thearchitecture parameters, allowing to find an appropriate architecture for eachexpression such as smile. This is feasible because all experiments were run ona Tesla K40c GPU, allowing a speedup of factor 10 over traditional computationson a CPU.
arxiv-15900-172 | A multiple instance learning approach for sequence data with across bag dependencies | http://arxiv.org/pdf/1602.00163v1.pdf | author:Manel Zoghlami, Sabeur Aridhi, Haitham Sghaier, Mondher Maddouri, Engelbert Mephu Nguifo category:cs.LG published:2016-01-30 summary:In Multiple Instance Learning (MIL) problem for sequence data, the learningdata consist of a set of bags where each bag contains a set ofinstances/sequences. In many real world applications such as bioinformatics,web mining, and text mining, comparing a random couple of sequences makes nosense. In fact, each instance of each bag may have structural and/or temporalrelation with other instances in other bags. Thus, the classification taskshould take into account the relation between semantically related instancesacross bags. In this paper, we present two novel MIL approaches for sequencedata classification: (1) ABClass and (2) ABSim. In ABClass, each sequence isrepresented by one vector of attributes. For each sequence of the unknown bag,a discriminative classifier is applied in order to compute a partialclassification result. Then, an aggregation method is applied to these partialresults in order to generate the final result. In ABSim, we use a similaritymeasure between each sequence of the unknown bag and the correspondingsequences in the learning bags. An unknown bag is labeled with the bag thatpresents more similar sequences. We applied both approaches to the problem ofbacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated anddiscussed the proposed approaches on well known Ionizing Radiation ResistanceBacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented byprimary structure of basal DNA repair proteins. The experimental results showthat both ABClass and ABSim approaches are efficient.
arxiv-15900-173 | Latent common manifold learning with alternating diffusion: analysis and applications | http://arxiv.org/pdf/1602.00078v1.pdf | author:Ronen Talmon, Hau-tieng Wu category:cs.DS math.NA stat.ML published:2016-01-30 summary:The analysis of data sets arising from multiple sensors has drawn significantresearch attention over the years. Traditional methods, including kernel-basedmethods, are typically incapable of capturing nonlinear geometric structures.We introduce a latent common manifold model underlying multiple sensorobservations for the purpose of multimodal data fusion. A method based onalternating diffusion is presented and analyzed; we provide theoreticalanalysis of the method under the latent common manifold model. To exemplify thepower of the proposed framework, experimental results in several applicationsare reported.
arxiv-15900-174 | Selection models of language production support informed text partitioning: an intuitive and practical, bag-of-phrases framework for text analysis | http://arxiv.org/pdf/1601.07969v1.pdf | author:Jake Ryland Williams, James P. Bagrow, Andrew J. Reagan, Sharon E. Alajajian, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL published:2016-01-29 summary:The task of text segmentation, or 'chunking,' may occur at many levels intext analysis, depending on whether it is most beneficial to break it down byparagraphs of a book, sentences of a paragraph, etc. Here, we focus on afine-grained segmentation task, which we refer to as text partitioning, wherewe apply methodologies to segment sentences or clauses into phrases, or lexicalconstructions of one or more words. In the past, we have explored (uniform)stochastic text partitioning---a process on the gaps between words whereby eachspace assumes one from a binary state of fixed (word binding) or broken (wordseparating) by some probability. In that work, we narrowly explored perhaps themost naive version of this process: random, or, uniform stochasticpartitioning, where all word-word gaps are prescribed a uniformly-set breakageprobability, q. Under this framework, the breakage probability is a tunableparameter, and was set to be pure-uniform: q = 1/2. In this work, we explorephrase frequency distributions under variation of the parameter q, and definenon-uniform, or informed stochastic partitions, where q is a function ofsurrounding information. Using a crude but effective function for q, we go onto apply informed partitions to over 20,000 English texts from the ProjectGutenberg eBooks database. In these analyses, we connect selection models togenerate a notion of fit goodness for the 'bag-of-terms' (words or phrases)representations of texts, and find informed (phrase) partitions to be animprovement over the q = 1 (word) and q = 1/2 (phrase) partitions in mostcases. This, together with the scalability of the methods proposed, suggeststhat the bag-of-phrases model should more often than not be implemented inplace of the bag-of-words model, setting the stage for a paradigm shift infeature selection, which lies at the foundation of text analysis methodology.
arxiv-15900-175 | Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain Adaptation | http://arxiv.org/pdf/1601.07977v1.pdf | author:Guo-Sen Xie, Xu-Yao Zhang, Shuicheng Yan, Cheng-Lin Liu category:cs.CV published:2016-01-29 summary:Convolutional neural network (CNN) has achieved state-of-the-art performancein many different visual tasks. Learned from a large-scale training dataset,CNN features are much more discriminative and accurate than the hand-craftedfeatures. Moreover, CNN features are also transferable among different domains.On the other hand, traditional dictionarybased features (such as BoW and SPM)contain much more local discriminative and structural information, which isimplicitly embedded in the images. To further improve the performance, in thispaper, we propose to combine CNN with dictionarybased models for scenerecognition and visual domain adaptation. Specifically, based on the well-tunedCNN models (e.g., AlexNet and VGG Net), two dictionary-based representationsare further constructed, namely mid-level local representation (MLR) andconvolutional Fisher vector representation (CFV). In MLR, an efficienttwo-stage clustering method, i.e., weighted spatial and feature space spectralclustering on the parts of a single image followed by clustering allrepresentative parts of all images, is used to generate a class-mixture or aclassspecific part dictionary. After that, the part dictionary is used tooperate with the multi-scale image inputs for generating midlevelrepresentation. In CFV, a multi-scale and scale-proportional GMM trainingstrategy is utilized to generate Fisher vectors based on the last convolutionallayer of CNN. By integrating the complementary information of MLR, CFV and theCNN features of the fully connected layer, the state-of-the-art performance canbe achieved on scene recognition and domain adaptation problems. An interestedfinding is that our proposed hybrid representation (from VGG net trained onImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained onPlace205) greatly.
arxiv-15900-176 | Efficient Robust Mean Value Calculation of 1D Features | http://arxiv.org/pdf/1601.08003v1.pdf | author:Erik Jonsson, Michael Felsberg category:cs.CV published:2016-01-29 summary:A robust mean value is often a good alternative to the standard mean valuewhen dealing with data containing many outliers. An efficient method forsamples of one-dimensional features and the truncated quadratic error norm ispresented and compared to the method of channel averaging (soft histograms).
arxiv-15900-177 | On the Geometric Ergodicity of Hamiltonian Monte Carlo | http://arxiv.org/pdf/1601.08057v1.pdf | author:Samuel Livingstone, Michael Betancourt, Simon Byrne, Mark Girolami category:stat.CO stat.ME stat.ML published:2016-01-29 summary:We establish general conditions under under which Markov chains produced bythe Hamiltonian Monte Carlo method will and will not be $\pi$-irreducible andgeometrically ergodic. We consider implementations with both fixed and dynamicintegration times. In the fixed case we find that the conditions for geometricergodicity are essentially a non-vanishing gradient of the log-density whichasymptotically points towards the centre of the space and does not grow fasterthan linearly. In an idealised scenario in which the integration time isallowed to change in different regions of the space, we show that geometricergodicity can be recovered for a much broader class of target distributions,leading to some guidelines for the choice of this free parameter in practice.
arxiv-15900-178 | Online Sparse Gaussian Process Training with Input Noise | http://arxiv.org/pdf/1601.08068v1.pdf | author:Hildo Bijl, Thomas B. Schön, Jan-Willem van Wingerden, Michel Verhaegen category:stat.ML cs.LG cs.SY published:2016-01-29 summary:Gaussian process regression traditionally has three important downsides. (1)It is computationally intensive, (2) it cannot efficiently implement newlyobtained measurements online, and (3) it cannot deal with stochastic (noisy)input points. In this paper we present an algorithm tackling all these threeissues simultaneously. The resulting Sparse Online Noisy Input GP (SONIG)regression algorithm can incorporate new measurements in constant runtime. Acomparison has shown that it is more accurate than similar existing regressionalgorithms. In addition, the algorithm can be applied to non-linear black-boxsystem modeling, where its performance is competitive with non-linear ARXmodels.
arxiv-15900-179 | Quantum perceptron over a field and neural network architecture selection in a quantum computer | http://arxiv.org/pdf/1602.00709v1.pdf | author:Adenilton J. da Silva, Teresa B. Ludermir, Wilson R. de Oliveira category:quant-ph cs.NE published:2016-01-29 summary:In this work, we propose a quantum neural network named quantum perceptronover a field (QPF). Quantum computers are not yet a reality and the models andalgorithms proposed in this work cannot be simulated in actual (or classical)computers. QPF is a direct generalization of a classical perceptron and solvessome drawbacks found in previous models of quantum perceptrons. We also presenta learning algorithm named Superposition based Architecture Learning algorithm(SAL) that optimizes the neural network weights and architectures. SAL searchesfor the best architecture in a finite set of neural network architectures withlinear time over the number of patterns in the training set. SAL is the firstlearning algorithm to determine neural network architectures in polynomialtime. This speedup is obtained by the use of quantum parallelism and anon-linear quantum operator.
arxiv-15900-180 | Mapping Tractography Across Subjects | http://arxiv.org/pdf/1601.08165v1.pdf | author:Thien Bao Nguyen, Emanuele Olivetti, Paolo Avesani category:stat.ML cs.CV q-bio.NC published:2016-01-29 summary:Diffusion magnetic resonance imaging (dMRI) and tractography provide means tostudy the anatomical structures within the white matter of the brain. Whenstudying tractography data across subjects, it is usually necessary to align,i.e. to register, tractographies together. This registration step is most oftenperformed by applying the transformation resulting from the registration ofother volumetric images (T1, FA). In contrast with registration methods that"transform" tractographies, in this work, we try to find which streamline inone tractography correspond to which streamline in the other tractography,without any transformation. In other words, we try to find a "mapping" betweenthe tractographies. We propose a graph-based solution for the tractographymapping problem and we explain similarities and differences with the relatedwell-known graph matching problem. Specifically, we define a loss functionbased on the pairwise streamline distance and reformulate the mapping problemas combinatorial optimization of that loss function. We show preliminarypromising results where we compare the proposed method, implemented withsimulated annealing, against a standard registration techniques in a task ofsegmentation of the corticospinal tract.
arxiv-15900-181 | Kernels for sequentially ordered data | http://arxiv.org/pdf/1601.08169v1.pdf | author:Franz J Király, Harald Oberhauser category:stat.ML cs.DM cs.LG math.ST stat.ME stat.TH published:2016-01-29 summary:We present a novel framework for kernel learning with sequential data of anykind, such as time series, sequences of graphs, or strings. Our approach isbased on signature features which can be seen as an ordered variant of sample(cross-)moments; it allows to obtain a "sequentialized" version of any statickernel. The sequential kernels are efficiently computable for discretesequences and are shown to approximate a continuous moment form in a samplingsense. A number of known kernels for sequences arise as "sequentializations" ofsuitable static kernels: string kernels may be obtained as a special case, andalignment kernels are closely related up to a modification that resolves theiropen non-definiteness issue. Our experiments indicate that our signature-basedsequential kernel framework may be a promising approach to learning withsequential data, such as time series, that allows to avoid extensive manualpre-processing.
arxiv-15900-182 | Joint System and Algorithm Design for Computationally Efficient Fan Beam Coded Aperture X-ray Coherent Scatter Imaging | http://arxiv.org/pdf/1603.06400v1.pdf | author:Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P. MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani Krishnamurthy, Anuj Kapadia, Lawrence Carin, David J. Brady category:cs.CV stat.ME published:2016-01-29 summary:In x-ray coherent scatter tomography, tomographic measurements of the forwardscatter distribution are used to infer scatter densities within a volume. Aradiopaque 2D pattern placed between the object and the detector array enablesthe disambiguation between different scatter events. The use of a fan beamsource illumination to speed up data acquisition relative to a pencil beampresents computational challenges. To facilitate the use of iterativealgorithms based on a penalized Poisson log-likelihood function, efficientcomputational implementation of the forward and backward models are needed. Ourproposed implementation exploits physical symmetries and structural propertiesof the system and suggests a joint system-algorithm design, where the systemdesign choices are influenced by computational considerations, and in turn leadto reduced reconstruction time. Computational-time speedups of approximately146 and 32 are achieved in the computation of the forward and backward models,respectively. Results validating the forward model and reconstruction algorithmare presented on simulated analytic and Monte Carlo data.
arxiv-15900-183 | Lipreading with Long Short-Term Memory | http://arxiv.org/pdf/1601.08188v1.pdf | author:Michael Wand, Jan Koutník, Jürgen Schmidhuber category:cs.CV cs.CL published:2016-01-29 summary:Lipreading, i.e. speech recognition from visual-only recordings of aspeaker's face, can be achieved with a processing pipeline based solely onneural networks, yielding significantly better accuracy than conventionalmethods. Feed-forward and recurrent neural network layers (namely LongShort-Term Memory; LSTM) are stacked to form a single structure which istrained by back-propagating error gradients through all the layers. Theperformance of such a stacked network was experimentally evaluated and comparedto a standard Support Vector Machine classifier using conventional computervision features (Eigenlips and Histograms of Oriented Gradients). Theevaluation was performed on data from 19 speakers of the publicly availableGRID corpus. With 51 different words to classify, we report a best wordaccuracy on held-out evaluation speakers of 79.6% using the end-to-end neuralnetwork-based solution (11.6% improvement over the best feature-based solutionevaluated).
arxiv-15900-184 | What Can I Do Around Here? Deep Functional Scene Understanding for Cognitive Robots | http://arxiv.org/pdf/1602.00032v2.pdf | author:Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos category:cs.RO cs.CV published:2016-01-29 summary:For robots that have the capability to interact with the physical environmentthrough their end effectors, understanding the surrounding scenes is not merelya task of image classification or object recognition. To perform actual tasks,it is critical for the robot to have a functional understanding of the visualscene. Here, we address the problem of localizing and recognition of functionalareas from an arbitrary indoor scene, formulated as a two-stage deep learningbased detection pipeline. A new scene functionality testing-bed, which iscomplied from two publicly available indoor scene datasets, is used forevaluation. Our method is evaluated quantitatively on the new dataset,demonstrating the ability to perform efficient recognition of functional areasfrom arbitrary indoor scenes. We also demonstrate that our detection model canbe generalized onto novel indoor scenes by cross validating it with the imagesfrom two different datasets.
arxiv-15900-185 | Spectrally Grouped Total Variation Reconstruction for Scatter Imaging Using ADMM | http://arxiv.org/pdf/1601.08201v1.pdf | author:Ikenna Odinaka, Yan Kaganovsky, Joel A. Greenberg, Mehadi Hassan, David G. Politte, Joseph A. O'Sullivan, Lawrence Carin, David J. Brady category:math.NA cs.CV published:2016-01-29 summary:We consider X-ray coherent scatter imaging, where the goal is to reconstructmomentum transfer profiles (spectral distributions) at each spatial locationfrom multiplexed measurements of scatter. Each material is characterized by aunique momentum transfer profile (MTP) which can be used to discriminatebetween different materials. We propose an iterative image reconstructionalgorithm based on a Poisson noise model that can account for photon-limitedmeasurements as well as various second order statistics of the data. To improveimage quality, previous approaches use edge-preserving regularizers to promotepiecewise constancy of the image in the spatial domain while treating eachspectral bin separately. Instead, we propose spectrally grouped regularizationthat promotes piecewise constant images along the spatial directions but alsoensures that the MTPs of neighboring spatial bins are similar, if they containthe same material. We demonstrate that this group regularization results inimprovement of both spectral and spatial image quality. We pursue anoptimization transfer approach where convex decompositions are used to lift theproblem such that all hyper-voxels can be updated in parallel and inclosed-form. The group penalty introduces a challenge since it is not directlyamendable to these decompositions. We use the alternating directions method ofmultipliers (ADMM) to replace the original problem with an equivalent sequenceof sub-problems that are amendable to convex decompositions, leading to ahighly parallel algorithm. We demonstrate the performance on real data.
arxiv-15900-186 | Face Alignment by Local Deep Descriptor Regression | http://arxiv.org/pdf/1601.07950v1.pdf | author:Amit Kumar, Rajeev Ranjan, Vishal Patel, Rama Chellappa category:cs.CV published:2016-01-29 summary:We present an algorithm for extracting key-point descriptors using deepconvolutional neural networks (CNN). Unlike many existing deep CNNs, our modelcomputes local features around a given point in an image. We also present aface alignment algorithm based on regression using these local descriptors. Theproposed method called Local Deep Descriptor Regression (LDDR) is able tolocalize face landmarks of varying sizes, poses and occlusions with highaccuracy. Deep Descriptors presented in this paper are able to uniquely andefficiently describe every pixel in the image and therefore can potentiallyreplace traditional descriptors such as SIFT and HOG. Extensive evaluations onfive publicly available unconstrained face alignment datasets show that ourdeep descriptor network is able to capture strong local features around a givenlandmark and performs significantly better than many competitive andstate-of-the-art face alignment algorithms.
arxiv-15900-187 | Deep convolutional networks for automated detection of posterior-element fractures on spine CT | http://arxiv.org/pdf/1602.00020v1.pdf | author:Holger R. Roth, Yinong Wang, Jianhua Yao, Le Lu, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2016-01-29 summary:Injuries of the spine, and its posterior elements in particular, are a commonoccurrence in trauma patients, with potentially devastating consequences.Computer-aided detection (CADe) could assist in the detection andclassification of spine fractures. Furthermore, CAD could help assess thestability and chronicity of fractures, as well as facilitate research intooptimization of treatment paradigms. In this work, we apply deep convolutional networks (ConvNets) for theautomated detection of posterior element fractures of the spine. First, thevertebra bodies of the spine with its posterior elements are segmented in spineCT using multi-atlas label fusion. Then, edge maps of the posterior elementsare computed. These edge maps serve as candidate regions for predicting a setof probabilities for fractures along the image edges using ConvNets in a 2.5Dfashion (three orthogonal patches in axial, coronal and sagittal planes). Weexplore three different methods for training the ConvNet using 2.5D patchesalong the edge maps of 'positive', i.e. fractured posterior-elements and'negative', i.e. non-fractured elements. An experienced radiologist retrospectively marked the location of 55displaced posterior-element fractures in 18 trauma patients. We randomly splitthe data into training and testing cases. In testing, we achieve anarea-under-the-curve of 0.857. This corresponds to 71% or 81% sensitivities at5 or 10 false-positives per patient, respectively. Analysis of our set oftrauma patients demonstrates the feasibility of detecting posterior-elementfractures in spine CT images using computer vision techniques such as deepconvolutional networks.
arxiv-15900-188 | Feature Selection: A Data Perspective | http://arxiv.org/pdf/1601.07996v3.pdf | author:Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P. Trevino, Jiliang Tang, Huan Liu category:cs.LG published:2016-01-29 summary:Feature selection, as a data preprocessing strategy, has been proven to beeffective and efficient in preparing high-dimensional data for data mining andmachine learning problems. The objectives of feature selection include:building simpler and more comprehensible models, improving data miningperformance, and preparing clean, understandable data. The recent proliferationof big data has presented some substantial challenges and opportunities offeature selection algorithms. In this survey, we provide a comprehensive andstructured overview of recent advances in feature selection research. Motivatedby current challenges and opportunities in the big data age, we revisit featureselection research from a data perspective, and review representative featureselection algorithms for generic data, structured data, heterogeneous data andstreaming data. Methodologically, to emphasize the differences and similaritiesof most existing feature selection algorithms for generic data, we generallycategorize them into four groups: similarity based, information theoreticalbased, sparse learning based and statistical based methods. Finally, tofacilitate and promote the research in this community, we also present aopen-source feature selection repository that consists of most of the popularfeature selection algorithms (http://featureselection.asu.edu/). At the end ofthis survey, we also have a discussion about some open problems and challengesthat need to be paid more attention in future research.
arxiv-15900-189 | DehazeNet: An End-to-End System for Single Image Haze Removal | http://arxiv.org/pdf/1601.07661v2.pdf | author:Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, Dacheng Tao category:cs.CV published:2016-01-28 summary:Single image haze removal is a challenging ill-posed problem. Existingmethods use various constraints/priors to get plausible dehazing solutions. Thekey to achieve haze removal is to estimate a medium transmission map for aninput hazy image. In this paper, we propose a trainable end-to-end systemcalled DehazeNet, for medium transmission estimation. DehazeNet takes a hazyimage as input, and outputs its medium transmission map that is subsequentlyused to recover a haze-free image via atmospheric scattering model. DehazeNetadopts Convolutional Neural Networks (CNN) based deep architecture, whoselayers are specially designed to embody the established assumptions/priors inimage dehazing. Specifically, layers of Maxout units are used for featureextraction, which can generate almost all haze-relevant features. We alsopropose a novel nonlinear activation function in DehazeNet, called BilateralRectified Linear Unit (BReLU), which is able to improve the quality ofrecovered haze-free image. We establish connections between components of theproposed DehazeNet and those used in existing methods. Experiments on benchmarkimages show that DehazeNet achieves superior performance over existing methods,yet keeps efficient and easy to use.
arxiv-15900-190 | Log-Normal Matrix Completion for Large Scale Link Prediction | http://arxiv.org/pdf/1601.07714v1.pdf | author:Brian Mohtashemi, Thomas Ketseoglou category:cs.SI cs.LG stat.ML published:2016-01-28 summary:The ubiquitous proliferation of online social networks has led to thewidescale emergence of relational graphs expressing unique patterns in linkformation and descriptive user node features. Matrix Factorization andCompletion have become popular methods for Link Prediction due to the low ranknature of mutual node friendship information, and the availability of parallelcomputer architectures for rapid matrix processing. Current Link Predictionliterature has demonstrated vast performance improvement through theutilization of sparsity in addition to the low rank matrix assumption. However,the majority of research has introduced sparsity through the limited L1 orFrobenius norms, instead of considering the more detailed distributions whichled to the graph formation and relationship evolution. In particular, socialnetworks have been found to express either Pareto, or more recently discovered,Log Normal distributions. Employing the convexity-inducing Lovasz Extension, wedemonstrate how incorporating specific degree distribution information can leadto large scale improvements in Matrix Completion based Link prediction. Weintroduce Log-Normal Matrix Completion (LNMC), and solve the complexoptimization problem by employing Alternating Direction Method of Multipliers.Using data from three popular social networks, our experiments yield up to 5%AUC increase over top-performing non-structured sparsity based methods.
arxiv-15900-191 | Distributed Low Rank Approximation of Implicit Functions of a Matrix | http://arxiv.org/pdf/1601.07721v1.pdf | author:David P. Woodruff, Peilin Zhong category:cs.NA cs.LG published:2016-01-28 summary:We study distributed low rank approximation in which the matrix to beapproximated is only implicitly represented across the different servers. Forexample, each of $s$ servers may have an $n \times d$ matrix $A^t$, and we maybe interested in computing a low rank approximation to $A = f(\sum_{t=1}^sA^t)$, where $f$ is a function which is applied entrywise to the matrix$\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible toefficiently compute a $d \times d$ rank-$k$ projection matrix $P$ for which$\A - AP\_F^2 \leq \A - [A]_k\_F^2 + \varepsilon \A\_F^2$, where $AP$denotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes thebest rank-$k$ approximation to $A$ given by the singular value decomposition.The communication cost of our protocols is $d \cdot (sk/\varepsilon)^{O(1)}$,and they succeed with high probability. Our framework allows us to efficientlycompute a low rank approximation to an entry-wise softmax, to a Gaussian kernelexpansion, and to $M$-Estimators applied entrywise (i.e., forms of robust lowrank approximation). We also show that our additive error approximation is bestpossible, in the sense that any protocol achieving relative error for theseproblems requires significantly more communication. Finally, we experimentallyvalidate our algorithms on real datasets.
arxiv-15900-192 | Automatic Generation of Building Models Using 2D Maps and Street View Images | http://arxiv.org/pdf/1601.07630v1.pdf | author:Jiangye Yuan, Anil M. Cheriyadat category:cs.CV published:2016-01-28 summary:We introduce a new approach for generating simple 3D building models bycombining building footprints from 2D maps with street level images. Theapproach works with crowd sourced maps such as the OpenStreetMap and streetlevel images acquired by a calibrated camera mounted on a moving vehicle.Buildings are modeled as boxes extruded from building footprints with theheight information estimated from images. Building footprints are elevated inworld coordinates and projected onto images. Building heights are estimated byscoring projected footprints based on their alignment with visible buildingfeatures in images. However, it is challenging to achieve accurate projectionsdue to camera pose errors inherited from external sensors resulting inincorrect height estimation. We derive a solution to precisely locate camerason maps using correspondence between image features and building footprints. Wetightly couple the camera localization and height estimation steps to producean effective method for 3D building model generation. Experiments using GoogleStreet View images and publicly available map data show the promise of ourmethod.
arxiv-15900-193 | Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor Compressive Sensing | http://arxiv.org/pdf/1601.07804v1.pdf | author:Xin Ding, Wei Chen, Ian J. Wassell category:cs.LG cs.IT math.IT published:2016-01-28 summary:Tensor Compressive Sensing (TCS) is a multidimensional framework ofCompressive Sensing (CS), and it is advantageous in terms of reducing theamount of storage, easing hardware implementations and preservingmultidimensional structures of signals in comparison to a conventional CSsystem. In a TCS system, instead of using a random sensing matrix and apredefined dictionary, the average-case performance can be further improved byemploying an optimized multidimensional sensing matrix and a learnedmultilinear sparsifying dictionary. In this paper, we propose a jointoptimization approach of the sensing matrix and dictionary for a TCS system.For the sensing matrix design in TCS, an extended separable approach with aclosed form solution and a novel iterative non-separable method are proposedwhen the multilinear dictionary is fixed. In addition, a multidimensionaldictionary learning method that takes advantages of the multidimensionalstructure is derived, and the influence of sensing matrices is taken intoaccount in the learning process. A joint optimization is achieved viaalternately iterating the optimization of the sensing matrix and dictionary.Numerical experiments using both synthetic data and real images demonstrate thesuperiority of the proposed approaches.
arxiv-15900-194 | An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning | http://arxiv.org/pdf/1601.07843v1.pdf | author:Nabin K. Mishra, M. Emre Celebi category:cs.CV stat.ML published:2016-01-28 summary:The incidence of malignant melanoma continues to increase worldwide. Thiscancer can strike at any age; it is one of the leading causes of loss of lifein young persons. Since this cancer is visible on the skin, it is potentiallydetectable at a very early stage when it is curable. New developments haveconverged to make fully automatic early melanoma detection a real possibility.First, the advent of dermoscopy has enabled a dramatic boost in clinicaldiagnostic ability to the point that melanoma can be detected in the clinic atthe very earliest stages. The global adoption of this technology has allowedaccumulation of large collections of dermoscopy images of melanomas and benignlesions validated by histopathology. The development of advanced technologiesin the areas of image processing and machine learning have given us the abilityto allow distinction of malignant melanoma from the many benign mimics thatrequire no biopsy. These new technologies should allow not only earlierdetection of melanoma, but also reduction of the large number of needless andcostly biopsy procedures. Although some of the new systems reported for thesetechnologies have shown promise in preliminary trials, widespreadimplementation must await further technical progress in accuracy andreproducibility. In this paper, we provide an overview of computerizeddetection of melanoma in dermoscopy images. First, we discuss the variousaspects of lesion segmentation. Then, we provide a brief overview of clinicalfeature segmentation. Finally, we discuss the classification stage wheremachine learning algorithms are applied to the attributes generated from thesegmented features to predict the existence of melanoma.
arxiv-15900-195 | A Grassmannian Graph Approach to Affine Invariant Feature Matching | http://arxiv.org/pdf/1601.07648v2.pdf | author:Mark Moyou, John Corring, Adrian Peter, Anand Rangarajan category:cs.CV published:2016-01-28 summary:In this work, we present a novel and practical approach to address one of thelongstanding problems in computer vision: 2D and 3D affine invariant featurematching. Our Grassmannian Graph (GrassGraph) framework employs a two stageprocedure that is capable of robustly recovering correspondences between twounorganized, affinely related feature (point) sets. The first stage maps thefeature sets to an affine invariant Grassmannian representation, where thefeatures are mapped into the same subspace. It turns out that coordinaterepresentations extracted from the Grassmannian differ by an arbitraryorthonormal matrix. In the second stage, by approximating the Laplace-Beltramioperator (LBO) on these coordinates, this extra orthonormal factor isnullified, providing true affine-invariant coordinates which we then utilize torecover correspondences via simple nearest neighbor relations. The resultingGrassGraph algorithm is empirically shown to work well in non-ideal scenarioswith noise, outliers, and occlusions. Our validation benchmarks use anunprecedented 440,000+ experimental trials performed on 2D and 3D datasets,with a variety of parameter settings and competing methods. State-of-the-artperformance in the majority of these extensive evaluations confirm the utilityof our method.
arxiv-15900-196 | Towards the Design of an End-to-End Automated System for Image and Video-based Recognition | http://arxiv.org/pdf/1601.07883v1.pdf | author:Rama Chellappa, Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Vishal M. Patel, Carlos D. Castillo category:cs.CV published:2016-01-28 summary:Over many decades, researchers working in object recognition have longed foran end-to-end automated system that will simply accept 2D or 3D image or videosas inputs and output the labels of objects in the input data. Computer visionmethods that use representations derived based on geometric, radiometric andneural considerations and statistical and structural matchers and artificialneural network-based methods where a multi-layer network learns the mappingfrom inputs to class labels have provided competing approaches for imagerecognition problems. Over the last four years, methods based on DeepConvolutional Neural Networks (DCNNs) have shown impressive performanceimprovements on object detection/recognition challenge problems. This has beenmade possible due to the availability of large annotated data, a betterunderstanding of the non-linear mapping between image and class labels as wellas the affordability of GPUs. In this paper, we present a brief history ofdevelopments in computer vision and artificial neural networks over the lastforty years for the problem of image-based recognition. We then present thedesign details of a deep learning system for end-to-end unconstrained faceverification/recognition. Some open issues regarding DCNNs for objectrecognition problems are then discussed. We caution the readers that the viewsexpressed in this paper are from the authors and authors only!
arxiv-15900-197 | Geo-distinctive Visual Element Matching for Location Estimation of Images | http://arxiv.org/pdf/1601.07884v1.pdf | author:Xinchao Li, Martha A. Larson, Alan Hanjalic category:cs.MM cs.CV published:2016-01-28 summary:We propose an image representation and matching approach that substantiallyimproves visual-based location estimation for images. The main novelty of theapproach, called distinctive visual element matching (DVEM), is its use ofrepresentations that are specific to the query image whose location is beingpredicted. These representations are based on visual element clouds, whichrobustly capture the connection between the query and visual evidence fromcandidate locations. We then maximize the influence of visual elements that aregeo-distinctive because they do not occur in images taken at many otherlocations. We carry out experiments and analysis for both geo-constrained andgeo-unconstrained location estimation cases using two large-scale,publicly-available datasets: the San Francisco Landmark dataset with $1.06$million street-view images and the MediaEval '15 Placing Task dataset with$5.6$ million geo-tagged images from Flickr. We present examples thatillustrate the highly-transparent mechanics of the approach, which are based oncommon sense observations about the visual patterns in image collections. Ourresults show that the proposed method delivers a considerable performanceimprovement compared to the state of the art.
arxiv-15900-198 | Parameterized Machine Learning for High-Energy Physics | http://arxiv.org/pdf/1601.07913v1.pdf | author:Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, Daniel Whiteson category:hep-ex cs.LG hep-ph published:2016-01-28 summary:We investigate a new structure for machine learning classifiers applied toproblems in high-energy physics by expanding the inputs to include not onlymeasured features but also physics parameters. The physics parameters representa smoothly varying learning task, and the resulting parameterized classifiercan smoothly interpolate between them and replace sets of classifiers trainedat individual values. This simplifies the training process and gives improvedperformance at intermediate values, even for complex problems requiring deeplearning. Applications include tools parameterized in terms of theoreticalmodel parameters, such as the mass of a particle, which allow for a singlenetwork to provide improved discrimination across a range of masses. Thisconcept is simple to implement and allows for optimized interpolatable results.
arxiv-15900-199 | Non-Gaussian Component Analysis with Log-Density Gradient Estimation | http://arxiv.org/pdf/1601.07665v1.pdf | author:Hiroaki Sasaki, Gang Niu, Masashi Sugiyama category:stat.ML published:2016-01-28 summary:Non-Gaussian component analysis (NGCA) is aimed at identifying a linearsubspace such that the projected data follows a non-Gaussian distribution. Inthis paper, we propose a novel NGCA algorithm based on log-density gradientestimation. Unlike existing methods, the proposed NGCA algorithm identifies thelinear subspace by using the eigenvalue decomposition without any iterativeprocedures, and thus is computationally reasonable. Furthermore, throughtheoretical analysis, we prove that the identified subspace converges to thetrue subspace at the optimal parametric rate. Finally, the practicalperformance of the proposed algorithm is demonstrated on both artificial andbenchmark datasets.
arxiv-15900-200 | Automating biomedical data science through tree-based pipeline optimization | http://arxiv.org/pdf/1601.07925v1.pdf | author:Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, Jason H. Moore category:cs.LG cs.NE published:2016-01-28 summary:Over the past decade, data science and machine learning has grown from amysterious art form to a staple tool across a variety of fields in academia,business, and government. In this paper, we introduce the concept of tree-basedpipeline optimization for automating one of the most tedious parts of machinelearning---pipeline design. We implement a Tree-based Pipeline OptimizationTool (TPOT) and demonstrate its effectiveness on a series of simulated andreal-world genetic data sets. In particular, we show that TPOT can buildmachine learning pipelines that achieve competitive classification accuracy anddiscover novel pipeline operators---such as synthetic featureconstructors---that significantly improve classification accuracy on these datasets. We also highlight the current challenges to pipeline optimization, suchas the tendency to produce pipelines that overfit the data, and suggest futureresearch paths to overcome these challenges. As such, this work represents anearly step toward fully automating machine learning pipeline design.
arxiv-15900-201 | Information-Theoretic Lower Bounds for Recovery of Diffusion Network Structures | http://arxiv.org/pdf/1601.07932v1.pdf | author:Keehwan Park, Jean Honorio category:cs.LG cs.IT math.IT stat.ML published:2016-01-28 summary:We study the information-theoretic lower bound of the sample complexity ofthe correct recovery of diffusion network structures. We introduce adiscrete-time diffusion model based on the Independent Cascade model for whichwe obtain a lower bound of order $\Omega(k \log p)$, for directed graphs of $p$nodes, and at most $k$ parents per node. Next, we introduce a continuous-timediffusion model, for which a similar lower bound of order $\Omega(k \log p)$ isobtained. Our results show that the algorithm of Pouget-Abadie et. at. isstatistically optimal for the discrete-time regime. Our work also opens thequestion of whether it is possible to devise optimal algorithms for thecontinuous-time regime.
arxiv-15900-202 | Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear Subspace Tracking | http://arxiv.org/pdf/1601.07947v1.pdf | author:Fateme Sheikholeslami, Dimitris Berberidis, Georgios B. Giannakis category:stat.ML cs.LG published:2016-01-28 summary:Kernel-based methods enjoy powerful generalization capabilities in handling avariety of learning tasks. When such methods are provided with sufficienttraining data, broadly-applicable classes of nonlinear functions can beapproximated with desired accuracy. Nevertheless, inherent to the nonparametricnature of kernel-based estimators are computational and memory requirementsthat become prohibitive with large-scale datasets. In response to thisformidable challenge, the present work puts forward a low-rank, kernel-based,feature extraction approach that is particularly tailored for online operation,where data streams need not be stored in memory. A novel generative model isintroduced to approximate high-dimensional (possibly infinite) features via alow-rank nonlinear subspace, the learning of which leads to a direct kernelfunction approximation. Offline and online solvers are developed for thesubspace learning task, along with affordable versions, in which the number ofstored data vectors is confined to a predefined budget. Analytical resultsprovide performance bounds on how well the kernel matrix as well askernel-based classification and regression tasks can be approximated byleveraging budgeted online subspace learning and feature extraction schemes.Tests on synthetic and real datasets demonstrate and benchmark the efficiencyof the proposed method when linear classification and regression is applied tothe extracted features.
arxiv-15900-203 | Discriminative Training of Deep Fully-connected Continuous CRF with Task-specific Loss | http://arxiv.org/pdf/1601.07649v1.pdf | author:Fayao Liu, Guosheng Lin, Chunhua Shen category:cs.CV published:2016-01-28 summary:Recent works on deep conditional random fields (CRF) have set new records onmany vision tasks involving structured predictions. Here we propose afully-connected deep continuous CRF model for both discrete and continuouslabelling problems. We exemplify the usefulness of the proposed model onmulti-class semantic labelling (discrete) and the robust depth estimation(continuous) problems. In our framework, we model both the unary and the pairwise potentialfunctions as deep convolutional neural networks (CNN), which are jointlylearned in an end-to-end fashion. The proposed method possesses the mainadvantage of continuously-valued CRF, which is a closed-form solution for theMaximum a posteriori (MAP) inference. To better adapt to different tasks, instead of using the commonly employedmaximum likelihood CRF parameter learning protocol, we propose task-specificloss functions for learning the CRF parameters. It enables direct optimization of the quality of the MAP estimates during thecourse of learning. Specifically, we optimize the multi-class classification loss for thesemantic labelling task and the Turkey's biweight loss for the robust depthestimation problem. Experimental results on the semantic labelling and robust depth estimationtasks demonstrate that the proposed method compare favorably against bothbaseline and state-of-the-art methods. In particular, we show that although the proposed deep CRF model iscontinuously valued, with the equipment of task-specific loss, it achievesimpressive results even on discrete labelling tasks.
arxiv-15900-204 | Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks | http://arxiv.org/pdf/1601.07621v1.pdf | author:Evan Racah, Seyoon Ko, Peter Sadowski, Wahid Bhimji, Craig Tull, Sang-Yun Oh, Pierre Baldi, Prabhat category:stat.ML cs.LG published:2016-01-28 summary:Experiments in particle physics produce enormous quantities of data that mustbe analyzed and interpreted by teams of physicists. This analysis is oftenexploratory, where scientists are unable to enumerate the possible types ofsignal prior to performing the experiment. Thus, tools for summarizing,clustering, visualizing and classifying high-dimensional data are essential. Inthis work, we show that meaningful physical content can be revealed bytransforming the raw data into a learned high-level representation using deepneural networks, with measurements taken at the Daya Bay Neutrino Experiment asa case study. We further show how convolutional deep neural networks canprovide an effective classification filter with greater than 97% accuracyacross different classes of physics events, significantly better than othermachine learning approaches.
arxiv-15900-205 | Font Identification in Historical Documents Using Active Learning | http://arxiv.org/pdf/1601.07252v1.pdf | author:Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard Furuta, Laura Mandell category:cs.CV cs.AI cs.DL stat.AP stat.ML I.5; I.2 published:2016-01-27 summary:Identifying the type of font (e.g., Roman, Blackletter) used in historicaldocuments can help optical character recognition (OCR) systems produce moreaccurate text transcriptions. Towards this end, we present an active-learningstrategy that can significantly reduce the number of labeled samples needed totrain a font classifier. Our approach extracts image-based features thatexploit geometric differences between fonts at the word level, and combinesthem into a bag-of-word representation for each page in a document. We evaluatesix sampling strategies based on uncertainty, dissimilarity and diversitycriteria, and test them on a database containing over 3,000 historicaldocuments with Blackletter, Roman and Mixed fonts. Our results show that acombination of uncertainty and diversity achieves the highest predictiveaccuracy (89% of test cases correctly classified) while requiring only a smallfraction of the data (17%) to be labeled. We discuss the implications of thisresult for mass digitization projects of historical documents.
arxiv-15900-206 | PersonNet: Person Re-identification with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1601.07255v1.pdf | author:Lin Wu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-01-27 summary:In this paper, we propose a deep end-to-end neu- ral network tosimultaneously learn high-level features and a corresponding similarity metricfor person re-identification. The network takes a pair of raw RGB images asinput, and outputs a similarity value indicating whether the two input imagesdepict the same person. A layer of computing neighborhood range differencesacross two input images is employed to capture local relationship betweenpatches. This operation is to seek a robust feature from input images. Byincreasing the depth to 10 weight layers and using very small (3$\times$3)convolution filters, our architecture achieves a remarkable improvement on theprior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp)gradient decent algorithm is integrated into our architecture, which isbeneficial to deep nets. Our method consistently outperforms state-of-the-arton two large datasets (CUHK03 and Market-1501), and a medium-sized data set(CUHK01).
arxiv-15900-207 | Fast Integral Image Estimation at 1% measurement rate | http://arxiv.org/pdf/1601.07258v1.pdf | author:Kuldeep Kulkarni, Pavan Turaga category:cs.CV math.OC published:2016-01-27 summary:We propose a framework called ReFInE to directly obtain integral imageestimates from a very small number of spatially multiplexed measurements of thescene without iterative reconstruction of any auxiliary image, and demonstratetheir practical utility in visual object tracking. Specifically, we designmeasurement matrices which are tailored to facilitate extremely fast estimationof the integral image, by using a single-shot linear operation on the measuredvector. Leveraging a prior model for the images, we formulate a nuclear normminimization problem with second order conic constraints to jointly obtain themeasurement matrix and the linear operator. Through qualitative andquantitative experiments, we show that high quality integral image estimatescan be obtained using our framework at very low measurement rates. Further, ona standard dataset of 50 videos, we present object tracking results which arecomparable to the state-of-the-art methods, even at an extremely lowmeasurement rate of 1%.
arxiv-15900-208 | Deep Learning Driven Visual Path Prediction from a Single Image | http://arxiv.org/pdf/1601.07265v1.pdf | author:Siyu Huang, Xi Li, Zhongfei Zhang, Zhouzhou He, Fei Wu, Wei Liu, Jinhui Tang, Yueting Zhuang category:cs.CV published:2016-01-27 summary:Capabilities of inference and prediction are significant components of visualsystems. In this paper, we address an important and challenging task of them:visual path prediction. Its goal is to infer the future path for a visualobject in a static scene. This task is complicated as it needs high-levelsemantic understandings of both the scenes and motion patterns underlying videosequences. In practice, cluttered situations have also raised higher demands onthe effectiveness and robustness of the considered models. Motivated by theseobservations, we propose a deep learning framework which simultaneouslyperforms deep feature learning for visual representation in conjunction withspatio-temporal context modeling. After that, we propose a unified pathplanning scheme to make accurate future path prediction based on the analyticresults of the context models. The highly effective visual representation anddeep context models ensure that our framework makes a deep semanticunderstanding of the scene and motion pattern, consequently improving theperformance of the visual path prediction task. In order to comprehensivelyevaluate the model's performance on the visual path prediction task, weconstruct two large benchmark datasets from the adaptation of video trackingdatasets. The qualitative and quantitative experimental results show that ourapproach outperforms the existing approaches and owns a better generalizationcapability.
arxiv-15900-209 | Comprehensive Feature-based Robust Video Fingerprinting Using Tensor Model | http://arxiv.org/pdf/1601.07270v1.pdf | author:Xiushan Nie, Yilong Yin, Jiande Sun category:cs.CV published:2016-01-27 summary:Content-based near-duplicate video detection (NDVD) is essential foreffective search and retrieval, and robust video fingerprinting is a goodsolution for NDVD. Most existing video fingerprinting methods use a singlefeature or concatenating different features to generate video fingerprints, andshow a good performance under single-mode modifications such as noise additionand blurring. However, when they suffer combined modifications, the performanceis degraded to a certain extent because such features cannot characterize thevideo content completely. By contrast, the assistance and consensus amongdifferent features can improve the performance of video fingerprinting.Therefore, in the present study, we mine the assistance and consensus amongdifferent features based on tensor model, and present a new comprehensivefeature to fully use them in the proposed video fingerprinting framework. Wealso analyze what the comprehensive feature really is for representing theoriginal video. In this framework, the video is initially set as a high-ordertensor that consists of different features, and the video tensor is decomposedvia the Tucker model with a solution that determines the number of components.Subsequently, the comprehensive feature is generated by the low-order tensorobtained from tensor decomposition. Finally, the video fingerprint is computedusing this feature. A matching strategy used for narrowing the search is alsoproposed based on the core tensor. The robust video fingerprinting framework isresistant not only to single-mode modifications, but also to the combination ofthem.
arxiv-15900-210 | Neighborhood Preserved Sparse Representation for Robust Classification on Symmetric Positive Definite Matrices | http://arxiv.org/pdf/1601.07336v1.pdf | author:Ming Yin, Shengli Xie, Yi Guo, Junbin Gao, Yun Zhang category:cs.CV published:2016-01-27 summary:Due to its promising classification performance, sparse representation basedclassification(SRC) algorithm has attracted great attention in the past fewyears. However, the existing SRC type methods apply only to vector data inEuclidean space. As such, there is still no satisfactory approach to conductclassification task for symmetric positive definite (SPD) matrices which isvery useful in computer vision. To address this problem, in this paper, aneighborhood preserved kernel SRC method is proposed on SPD manifolds.Specifically, by embedding the SPD matrices into a Reproducing Kernel HilbertSpace (RKHS), the proposed method can perform classification on SPD manifoldsthrough an appropriate Log-Euclidean kernel. Through exploiting the geodesicdistance between SPD matrices, our method can effectively characterize theintrinsic local Riemannian geometry within data so as to well unravel theunderlying sub-manifold structure. Despite its simplicity, experimental resultson several famous database demonstrate that the proposed method achieves betterclassification results than the state-of-the-art approaches.
arxiv-15900-211 | Quantum machine learning with glow for episodic tasks and decision games | http://arxiv.org/pdf/1601.07358v1.pdf | author:Jens Clausen, Hans J. Briegel category:quant-ph cs.AI cs.LG published:2016-01-27 summary:We consider a general class of models, where a reinforcement learning (RL)agent learns from cyclic interactions with an external environment viaclassical signals. Perceptual inputs are encoded as quantum states, which aresubsequently transformed by a quantum channel representing the agent's memory,while the outcomes of measurements performed at the channel's output determinethe agent's actions. The learning takes place via stepwise modifications of thechannel properties. They are described by an update rule that is inspired bythe projective simulation (PS) model and equipped with a glow mechanism thatallows for a backpropagation of policy changes, analogous to the eligibilitytraces in RL and edge glow in PS. In this way, the model combines features ofPS with the ability for generalization, offered by its physical embodiment as aquantum system. We apply the agent to various setups of an invasion game and agrid world, which serve as elementary model tasks allowing a direct comparisonwith a basic classical PS agent.
arxiv-15900-212 | A First Attempt to Cloud-Based User Verification in Distributed System | http://arxiv.org/pdf/1601.07446v1.pdf | author:Marcin Wozniak, Dawid Polap, Grzegorz Borowik, Christian Napoli category:cs.NE cs.AI cs.CR cs.DC published:2016-01-27 summary:In this paper, the idea of client verification in distributed systems ispresented. The proposed solution presents a sample system where clientverification through cloud resources using input signature is discussed. Fordifferent signatures the proposed method has been examined. Research resultsare presented and discussed to show potential advantages.
arxiv-15900-213 | Distributed User Association in Energy Harvesting Small Cell Networks: A Probabilistic Model | http://arxiv.org/pdf/1601.07795v1.pdf | author:Setareh Maghsudi, Ekram Hossain category:cs.IT cs.LG math.IT published:2016-01-27 summary:We consider a distributed downlink user association problem in a small cellnetwork, where small cells obtain the required energy for providing wirelessservices to users through ambient energy harvesting. Since energy harvesting isopportunistic in nature, the amount of harvested energy is a random variable,without any a priori known characteristics. Moreover, since users arrive in thenetwork randomly and require different wireless services, the energyconsumption is a random variable as well. In this paper, we propose aprobabilistic framework to mathematically model and analyze the random behaviorof energy harvesting and energy consumption in dense small cell networks.Furthermore, as acquiring (even statistical) channel and network knowledge isvery costly in a distributed dense network, we develop a bandit-theoreticalformulation for distributed user association when no information is availableat users
arxiv-15900-214 | Information-theoretic lower bounds on learning the structure of Bayesian networks | http://arxiv.org/pdf/1601.07460v1.pdf | author:Asish Ghoshal, Jean Honorio category:cs.LG cs.IT math.IT stat.ML published:2016-01-27 summary:In this paper, we study the information theoretic limits of learning thestructure of Bayesian networks from data. We show that for Bayesian networks oncontinuous as well as discrete random variables, there exists aparameterization of the Bayesian network such that, the minimum number ofsamples required to learn the "true" Bayesian network grows as$\mathcal{O}(m)$, where $m$ is the number of variables in the network. Further,for sparse Bayesian networks, where the number of parents of any variable inthe network is restricted to be at most $l$ for $l \ll m$, the minimum numberof samples required grows as $\mathcal{O}(l\log m)$. We discuss conditionsunder which these limits are achieved. For Bayesian networks over continuousvariables, we obtain results for Gaussian regression and Gumbel Bayesiannetworks. While for the discrete variables, we obtain results for Noisy-OR,Conditional Probability Table (CPT) based Bayesian networks and Logisticregression networks. Finally, as a byproduct, we also obtain lower bounds onthe sample complexity of feature selection in logistic regression and show thatthe bounds are sharp.
arxiv-15900-215 | Shape Distributions of Nonlinear Dynamical Systems for Video-based Inference | http://arxiv.org/pdf/1601.07471v1.pdf | author:Vinay Venkataraman, Pavan Turaga category:cs.CV published:2016-01-27 summary:This paper presents a shape-theoretic framework for dynamical analysis ofnonlinear dynamical systems which appear frequently in several video-basedinference tasks. Traditional approaches to dynamical modeling have includedlinear and nonlinear methods with their respective drawbacks. A novel approachwe propose is the use of descriptors of the shape of the dynamical attractor asa feature representation of nature of dynamics. The proposed framework has twomain advantages over traditional approaches: a) representation of the dynamicalsystem is derived directly from the observational data, without any inherentassumptions, and b) the proposed features show stability under differenttime-series lengths where traditional dynamical invariants fail. We illustrateour idea using nonlinear dynamical models such as Lorenz and Rossler systems,where our feature representations (shape distribution) support our hypothesisthat the local shape of the reconstructed phase space can be used as adiscriminative feature. Our experimental analyses on these models also indicatethat the proposed framework show stability for different time-series lengths,which is useful when the available number of samples are small/variable. Thespecific applications of interest in this paper are: 1) activity recognitionusing motion capture and RGBD sensors, 2) activity quality assessment forapplications in stroke rehabilitation, and 3) dynamical scene classification.We provide experimental validation through action and gesture recognitionexperiments on motion capture and Kinect datasets. In all these scenarios, weshow experimental evidence of the favorable properties of the proposedrepresentation.
arxiv-15900-216 | Unsupervised Learning in Neuromemristive Systems | http://arxiv.org/pdf/1601.07482v1.pdf | author:Cory Merkel, Dhireesha Kudithipudi category:cs.ET cs.LG stat.ML published:2016-01-27 summary:Neuromemristive systems (NMSs) currently represent the most promisingplatform to achieve energy efficient neuro-inspired computation. However, sincethe research field is less than a decade old, there are still countlessalgorithms and design paradigms to be explored within these systems. Oneparticular domain that remains to be fully investigated within NMSs isunsupervised learning. In this work, we explore the design of an NMS forunsupervised clustering, which is a critical element of several machinelearning algorithms. Using a simple memristor crossbar architecture andlearning rule, we are able to achieve performance which is on par with MATLAB'sk-means clustering.
arxiv-15900-217 | On the Sample Complexity of Learning Sparse Graphical Games | http://arxiv.org/pdf/1601.07243v1.pdf | author:Jean Honorio category:cs.GT cs.LG stat.ML published:2016-01-27 summary:We analyze the sample complexity of learning sparse graphical games frompurely behavioral data. That is, we assume that we can only observe theplayers' joint actions and not their payoffs. We analyze the sufficient andnecessary number of samples for the correct recovery of the set ofpure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses onsparse directed graphs with $n$ nodes and at most $k$ parents per node. We showthat if the number of samples is greater than ${O(k n \log^2{n})}$, thenmaximum likelihood estimation correctly recovers the PSNE with highprobability. We also show that if the number of samples is less than ${O(k n\log^2{n})}$, then any conceivable method fails to recover the PSNE witharbitrary probability.
arxiv-15900-218 | Learning to Extract Motion from Videos in Convolutional Neural Networks | http://arxiv.org/pdf/1601.07532v1.pdf | author:Damien Teney, Martial Hebert category:cs.CV published:2016-01-27 summary:This paper shows how to extract dense optical flow from videos with aconvolutional neural network (CNN). The proposed model constitutes a potentialbuilding block for deeper architectures to allow using motion without resortingto an external algorithm, \eg for recognition in videos. We derive our networkarchitecture from signal processing principles to provide desired invariancesto image contrast, phase and texture. We constrain weights within the networkto enforce strict rotation invariance and substantially reduce the number ofparameters to learn. We demonstrate end-to-end training on only 8 sequences ofthe Middlebury dataset, orders of magnitude less than competing CNN-basedmotion estimation methods, and obtain comparable performance to classicalmethods on the Middlebury benchmark. Importantly, our method outputs adistributed representation of motion that allows representing multiple,transparent motions, and dynamic textures. Our contributions on network designand rotation invariance offer insights nonspecific to motion estimation.
arxiv-15900-219 | Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers on Subgraphs of Molecules | http://arxiv.org/pdf/1601.07233v1.pdf | author:Andrew Schaumberg, Angela Yu, Tatsuhiro Koshi, Xiaochan Zong, Santoshkalyan Rayadhurgam category:stat.ML cs.LG I.2.1; J.3 published:2016-01-27 summary:In this study, we intend to solve a mutual information problem in interactingmolecules of any type, such as proteins, nucleic acids, and small molecules.Using machine learning techniques, we accurately predict pairwise interactions,which can be of medical and biological importance. Graphs are are useful inthis problem for their generality to all types of molecules, due to theinherent association of atoms through atomic bonds. Subgraphs can representdifferent molecular domains. These domains can be biologically significant asmost molecules only have portions that are of functional significance and caninteract with other domains. Thus, we use subgraphs as features in differentmachine learning algorithms to predict if two drugs interact and predictpotential single molecule effects.
arxiv-15900-220 | Osteoporotic and Neoplastic Compression Fracture Classification on Longitudinal CT | http://arxiv.org/pdf/1601.07533v1.pdf | author:Yinong Wang, Jianhua Yao, Joseph E. Burns, Ronald M. Summers category:cs.CV q-bio.TO published:2016-01-27 summary:Classification of vertebral compression fractures (VCF) having osteoporoticor neoplastic origin is fundamental to the planning of treatment. We developeda fracture classification system by acquiring quantitative morphologic and bonedensity determinants of fracture progression through the use of automatedmeasurements from longitudinal studies. A total of 250 CT studies were acquiredfor the task, each having previously identified VCFs with osteoporosis orneoplasm. Thirty-six features or each identified VCF were computed andclassified using a committee of support vector machines. Ten-fold crossvalidation on 695 identified fractured vertebrae showed classificationaccuracies of 0.812, 0.665, and 0.820 for the measured, longitudinal, andcombined feature sets respectively.
arxiv-15900-221 | Locally-Supervised Deep Hybrid Model for Scene Recognition | http://arxiv.org/pdf/1601.07576v1.pdf | author:Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV published:2016-01-27 summary:Convolutional neural networks (CNN) have recently achieved remarkablesuccesses in various image classification and understanding tasks. The deepfeatures obtained at the top fully-connected layer of the CNN (FC-features)exhibit rich global semantic information and are extremely effective in imageclassification. On the other hand, the convolutional features in the middlelayers of the CNN also contain meaningful local information, but are not fullyexplored for image representation. In this paper, we propose a novelLocally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances andexplores the convolutional features for scene recognition. Firstly, we noticethat the convolutional features capture local objects and fine structures ofscene images, which yield important cues for discriminating ambiguous scenes,whereas these features are significantly eliminated in the highly-compressed FCrepresentation. Secondly, we propose a new Local Convolutional Supervision(LCS) layer to enhance the local structure of the image by directly propagatingthe label information to the convolutional layers. Thirdly, we propose anefficient Fisher Convolutional Vector (FCV) that successfully rescues theorderless mid-level semantic information (e.g. objects and textures) of sceneimage. The FCV encodes the large-sized convolutional maps into a fixed-lengthmid-level representation, and is demonstrated to be strongly complementary tothe high-level FC-features. Finally, both the FCV and FC-features arecollaboratively employed in the LSDHM representation, which achievesoutstanding performance in our experiments. It obtains 83.75% and 67.56%accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397datasets, advancing the stat-of-the-art substantially.
arxiv-15900-222 | Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization | http://arxiv.org/pdf/1601.07596v1.pdf | author:Francisco Chicano, Darrell Whitley, Renato Tinos category:cs.AI cs.NE I.2.8 published:2016-01-27 summary:Local search algorithms and iterated local search algorithms are a basictechnique. Local search can be a stand along search methods, but it can also behybridized with evolutionary algorithms. Recently, it has been shown that it ispossible to identify improving moves in Hamming neighborhoods for k-boundedpseudo-Boolean optimization problems in constant time. This means that localsearch does not need to enumerate neighborhoods to find improving moves. Italso means that evolutionary algorithms do not need to use random mutation as aoperator, except perhaps as a way to escape local optima. In this paper, weshow how improving moves can be identified in constant time for multiobjectiveproblems that are expressed as k-bounded pseudo-Boolean functions. Inparticular, multiobjective forms of NK Landscapes and Mk Landscapes areconsidered.
arxiv-15900-223 | Evolutionary stability implies asymptotic stability under multiplicative weights | http://arxiv.org/pdf/1601.07267v2.pdf | author:Ioannis Avramopoulos category:cs.GT cs.LG math.OC published:2016-01-27 summary:We show that evolutionarily stable states in general (nonlinear) populationgames (which can be viewed as continuous vector fields constrained on apolytope) are asymptotically stable under a multiplicative weights dynamic(under appropriate choices of a parameter called the learning rate or stepsize, which we demonstrate to be crucial to achieve convergence, as otherwiseeven chaotic behavior is possible to manifest). Our result implies thatevolutionary theories based on multiplicative weights are compatible (inprinciple, more general) with those based on the notion of evolutionarystability. However, our result further establishes multiplicative weights as anonlinear programming primitive (on par with standard nonlinear programmingmethods) since various nonlinear optimization problems, such as findingNash/Wardrop equilibria in nonatomic congestion games, which are well-known tobe equipped with a convex potential function, and finding strict local maximaof quadratic programming problems, are special cases of the problem ofcomputing evolutionarily stable states in nonlinear population games.
arxiv-15900-224 | Co-Occurrence Patterns in the Voynich Manuscript | http://arxiv.org/pdf/1601.07435v2.pdf | author:Torsten Timm category:cs.CL cs.CR published:2016-01-27 summary:The Voynich Manuscript is a medieval book written in an unknown script. Thispaper studies the distribution of similarly spelled words in the VoynichManuscript. It shows that the distribution of words within the manuscript isnot compatible with natural languages.
arxiv-15900-225 | Sentiment Analysis of Twitter Data: A Survey of Techniques | http://arxiv.org/pdf/1601.06971v3.pdf | author:Vishal. A. Kharde, Prof. Sheetal. Sonawane category:cs.CL I.2.7 published:2016-01-26 summary:With the advancement of web technology and its growth, there is a huge volumeof data present in the web for internet users and a lot of data is generatedtoo. Internet has become a platform for online learning, exchanging ideas andsharing opinions. Social networking sites like Twitter, Facebook, Google+ arerapidly gaining popularity as they allow people to share and express theirviews about topics,have discussion with different communities, or post messagesacross the world. There has been lot of work in the field of sentiment analysisof twitter data. This survey focuses mainly on sentiment analysis of twitterdata which is helpful to analyze the information in the tweets where opinionsare highly unstructured, heterogeneous and are either positive or negative, orneutral in some cases. In this paper, we provide a survey and a comparativeanalyses of existing techniques for opinion mining like machine learning andlexicon-based approaches, together with evaluation metrics. Using variousmachine learning algorithms like Naive Bayes, Max Entropy, and Support VectorMachine, we provide a research on twitter data streams.General challenges andapplications of Sentiment Analysis on Twitter are also discussed in this paper.
arxiv-15900-226 | Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization | http://arxiv.org/pdf/1601.07213v2.pdf | author:Alexander G. Ororbia II, C. Lee Giles, Daniel Kifer category:cs.LG cs.NE published:2016-01-26 summary:We present DataGrad, a general back-propagation style training procedure fordeep neural architectures that uses regularization of a deep Jacobian-basedpenalty. It can be viewed as a deep extension of the layerwise contractiveauto-encoder penalty. More importantly, it unifies previous proposals foradversarial training of deep neural nets -- this list includes directlymodifying the gradient, training on a mix of original and adversarial examples,using contractive penalties, and approximately optimizing constrainedadversarial objective functions. In an experiment using a Deep Sparse RectifierNetwork, we find that the deep Jacobian regularization of DataGrad (which alsohas L1 and L2 flavors of regularization) outperforms traditional L1 and L2regularization both on the original dataset as well as on adversarial examples.
arxiv-15900-227 | LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization | http://arxiv.org/pdf/1601.07124v1.pdf | author:Elvys Linhares Pontes, Juan-Manuel Torres-Moreno, Andréa Carneiro Linhares category:cs.CL cs.IR published:2016-01-26 summary:This paper aims to introduces a new algorithm for automatic speech-to-textsummarization based on statistical divergences of probabilities and graphs. Theinput is a text from speech conversations with noise, and the output a compacttext summary. Our results, on the pilot task CCCS Multiling 2015 French corpusare very encouraging
arxiv-15900-228 | ReconNet: Non-Iterative Reconstruction of Images from Compressively Sensed Random Measurements | http://arxiv.org/pdf/1601.06892v2.pdf | author:Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, Amit Ashok category:cs.CV published:2016-01-26 summary:The goal of this paper is to present a non-iterative and more importantly anextremely fast algorithm to reconstruct images from compressively sensed (CS)random measurements. To this end, we propose a novel convolutional neuralnetwork (CNN) architecture which takes in CS measurements of an image as inputand outputs an intermediate reconstruction. We call this network, ReconNet. Theintermediate reconstruction is fed into an off-the-shelf denoiser to obtain thefinal reconstructed image. On a standard dataset of images we show significantimprovements in reconstruction results (both in terms of PSNR and timecomplexity) over state-of-the-art iterative CS reconstruction algorithms atvarious measurement rates. Further, through qualitative experiments on realdata collected using our block single pixel camera (SPC), we show that ournetwork is highly robust to sensor noise and can recover visually betterquality images than competitive algorithms at extremely low sensing rates of0.1 and 0.04. To demonstrate that our algorithm can recover semanticallyinformative images even at a low measurement rate of 0.01, we present a veryrobust proof of concept real-time visual tracking application.
arxiv-15900-229 | Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound | http://arxiv.org/pdf/1601.07014v3.pdf | author:Fausto Milletari, Seyed-Ahmad Ahmadi, Christine Kroll, Annika Plate, Verena Rozanski, Juliana Maiostre, Johannes Levin, Olaf Dietrich, Birgit Ertl-Wagner, Kai Bötzel, Nassir Navab category:cs.CV published:2016-01-26 summary:In this work we propose a novel approach to perform segmentation byleveraging the abstraction capabilities of convolutional neural networks(CNNs). Our method is based on Hough voting, a strategy that allows for fullyautomatic localisation and segmentation of the anatomies of interest. Thisapproach does not only use the CNN classification outcomes, but it alsoimplements voting by exploiting the features produced by the deepest portion ofthe network. We show that this learning-based segmentation method is robust,multi-region, flexible and can be easily adapted to different modalities. Inthe attempt to show the capabilities and the behaviour of CNNs when they areapplied to medical image analysis, we perform a systematic study of theperformances of six different network architectures, conceived according tostate-of-the-art criteria, in various situations. We evaluate the impact ofboth different amount of training data and different data dimensionality (2D,2.5D and 3D) on the final results. We show results on both MRI and transcranialUS volumes depicting respectively 26 regions of the basal ganglia and themidbrain.
arxiv-15900-230 | Polyhedron Volume-Ratio-based Classification for Image Recognition | http://arxiv.org/pdf/1601.07021v1.pdf | author:Qingxiang Feng, Jeng-Shyang Pan, Jar-Ferr Yang, Yang-Ting Chou category:cs.CV published:2016-01-26 summary:In this paper, a novel method, called polyhedron volume ratio classification(PVRC) is proposed for image recognition
arxiv-15900-231 | Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction | http://arxiv.org/pdf/1601.06950v1.pdf | author:Michael Waechter, Mate Beljan, Simon Fuhrmann, Nils Moehrle, Johannes Kopf, Michael Goesele category:cs.CV cs.GR I.3.7 published:2016-01-26 summary:The ultimate goal of many image-based modeling systems is to renderphoto-realistic novel views of a scene without visible artifacts. Existingevaluation metrics and benchmarks focus mainly on the geometric accuracy of thereconstructed model, which is, however, a poor predictor of visual accuracy.Furthermore, using only geometric accuracy by itself does not allow evaluatingsystems that either lack a geometric scene representation or utilize coarseproxy geometry. Examples include light field or image-based rendering systems.We propose a unified evaluation approach based on novel view prediction errorthat is able to analyze the visual quality of any method that can render novelviews from input images. One of the key advantages of this approach is that itdoes not require ground truth geometry. This dramatically simplifies thecreation of test datasets and benchmarks. It also allows us to evaluate thequality of an unknown scene during the acquisition and reconstruction process,which is useful for acquisition planning. We evaluate our approach on a rangeof methods including standard geometry-plus-texture pipelines as well asimage-based rendering techniques, compare it to existing geometry-basedbenchmarks, and demonstrate its utility for a range of use cases.
arxiv-15900-232 | A Novel Memetic Feature Selection Algorithm | http://arxiv.org/pdf/1601.06933v1.pdf | author:Mohadeseh Montazeri, Hamid Reza Naji, Mitra Montazeri, Ahmad Faraahi category:cs.LG published:2016-01-26 summary:Feature selection is a problem of finding efficient features among allfeatures in which the final feature set can improve accuracy and reducecomplexity. In feature selection algorithms search strategies are key aspects.Since feature selection is an NP-Hard problem; therefore heuristic algorithmshave been studied to solve this problem. In this paper, we have proposed amethod based on memetic algorithm to find an efficient feature subset for aclassification problem. It incorporates a filter method in the geneticalgorithm to improve classification performance and accelerates the search inidentifying core feature subsets. Particularly, the method adds or deletes afeature from a candidate feature subset based on the multivariate featureinformation. Empirical study on commonly data sets of the university ofCalifornia, Irvine shows that the proposed method outperforms existing methods.
arxiv-15900-233 | Fisher Motion Descriptor for Multiview Gait Recognition | http://arxiv.org/pdf/1601.06931v1.pdf | author:F. M. Castro, M. J. Marín-Jiménez, N. Guil, R. Muñoz-Salinas category:cs.CV cs.AI published:2016-01-26 summary:The goal of this paper is to identify individuals by analyzing their gait.Instead of using binary silhouettes as input data (as done in many previousworks) we propose and evaluate the use of motion descriptors based on denselysampled short-term trajectories. We take advantage of state-of-the-art peopledetectors to define custom spatial configurations of the descriptors around thetarget person, obtaining a rich representation of the gait motion. The localmotion features (described by the Divergence-Curl-Shear descriptor) extractedon the different spatial areas of the person are combined into a singlehigh-level gait descriptor by using the Fisher Vector encoding. The proposedapproach, coined Pyramidal Fisher Motion, is experimentally validated on`CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and therecent `AVA Multiview Gait' dataset. The results show that this new approachachieves state-of-the-art results in the problem of gait recognition, allowingto recognize walking people from diverse viewpoints on single and multiplecamera setups, wearing different clothes, carrying bags, walking at diversespeeds and not limited to straight walking paths.
arxiv-15900-234 | Classification and Verification of Online Handwritten Signatures with Time Causal Information Theory Quantifiers | http://arxiv.org/pdf/1601.06925v1.pdf | author:Osvaldo A. Rosso, Raydonal Ospina, Alejandro C. Frery category:cs.IT cs.CV math.IT published:2016-01-26 summary:We present a new approach for online handwritten signature classification andverification based on descriptors stemming from Information Theory. Theproposal uses the Shannon Entropy, the Statistical Complexity, and the FisherInformation evaluated over the Bandt and Pompe symbolization of the horizontaland vertical coordinates of signatures. These six features are easy and fast tocompute, and they are the input to an One-Class Support Vector Machineclassifier. The results produced surpass state-of-the-art techniques thatemploy higher-dimensional feature spaces which often require specializedsoftware and hardware. We assess the consistency of our proposal with respectto the size of the training sample, and we also use it to classify thesignatures into meaningful groups.
arxiv-15900-235 | Functional archetype and archetypoid analysis | http://arxiv.org/pdf/1601.06911v1.pdf | author:Irene Epifanio category:stat.ME stat.AP stat.ML published:2016-01-26 summary:Archetype and archetypoid analysis can be extended to functional data. Eachfunction is represented as a mixture of actual observations (functionalarchetypoids) or functional archetypes, which are a mixture of observations inthe data set. Well-known Canadian temperature data are used to illustrate theanalysis developed. Computational methods are proposed for performing these analyses, based onthe coefficients of a basis. They are compared with other alternatives in asimulation study using a well-known curve discrimination problem, achievingbetter or similar performance. Unlike a previous attempt to compute functionalarchetypes, which was only valid for an orthogonal basis, the proposedmethodology can be used for any basis. It is computationally less demandingthan the simple approach of discretizing the functions. Multivariate functionalarchetype and archetypoid analysis are also introduced and applied in aninteresting problem about the study of human development around the world overthe last 50 years. These tools can contribute to the understanding of afunctional data set, as in the multivariate case.
arxiv-15900-236 | COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images | http://arxiv.org/pdf/1601.07140v1.pdf | author:Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, Serge Belongie category:cs.CV published:2016-01-26 summary:This paper describes the COCO-Text dataset. In recent years large-scaledatasets like SUN and Imagenet drove the advancement of scene understanding andobject recognition. The goal of COCO-Text is to advance state-of-the-art intext detection and recognition in natural images. The dataset is based on theMS COCO dataset, which contains images of complex everyday scenes. The imageswere not collected with text in mind and thus contain a broad variety of textinstances. To reflect the diversity of text in natural scenes, we annotate textwith (a) location in terms of a bounding box, (b) fine-grained classificationinto machine printed text and handwritten text, (c) classification into legibleand illegible text, (d) script of the text and (e) transcriptions of legibletext. The dataset contains over 173k text annotations in over 63k images. Weprovide a statistical analysis of the accuracy of our annotations. In addition,we present an analysis of three leading state-of-the-art photo OpticalCharacter Recognition (OCR) approaches on our dataset. While scene textdetection and recognition enjoys strong advances in recent years, we identifysignificant shortcomings motivating future work.
arxiv-15900-237 | Recurrent Neural Network Postfilters for Statistical Parametric Speech Synthesis | http://arxiv.org/pdf/1601.07215v1.pdf | author:Prasanna Kumar Muthukumar, Alan W Black category:cs.CL published:2016-01-26 summary:In the last two years, there have been numerous papers that have looked intousing Deep Neural Networks to replace the acoustic model in traditionalstatistical parametric speech synthesis. However, far less attention has beenpaid to approaches like DNN-based postfiltering where DNNs work in conjunctionwith traditional acoustic models. In this paper, we investigate the use ofRecurrent Neural Networks as a potential postfilter for synthesis. We explorethe possibility of replacing existing postfilters, as well as highlight theease with which arbitrary new features can be added as input to the postfilter.We also tried a novel approach of jointly training the Classification AndRegression Tree and the postfilter, rather than the traditional approach oftraining them independently.
arxiv-15900-238 | A network that learns Strassen multiplication | http://arxiv.org/pdf/1601.07227v1.pdf | author:Veit Elser category:math.NA cs.NE published:2016-01-26 summary:We study neural networks whose only non-linear components are multipliers, totest a new training rule in a context where the precise representation of datais paramount. These networks are challenged to discover the rules of matrixmultiplication, given many examples. By limiting the number of multipliers, thenetwork is forced to discover the Strassen multiplication rules. This is themathematical equivalent of finding low rank decompositions of the $n\times n$matrix multiplication tensor, $M_n$. We train these networks with theconservative learning rule, which makes minimal changes to the weights so as togive the correct output for each input at the time the input-output pair isreceived. Conservative learning needs a few thousand examples to find the rank7 decomposition of $M_2$, and $10^5$ for the rank 23 decomposition of $M_3$(the lowest known). High precision is critical, especially for $M_3$, todiscriminate between true decompositions and "border approximations".
arxiv-15900-239 | Investigating echo state networks dynamics by means of recurrence analysis | http://arxiv.org/pdf/1601.07381v2.pdf | author:Filippo Maria Bianchi, Lorenzo Livi, Cesare Alippi category:cs.LG nlin.CD published:2016-01-26 summary:In this paper, we elaborate over the well-known interpretability issue inecho state networks. The idea is to investigate the dynamics of reservoirneurons with time-series analysis techniques taken from research on complexsystems. Notably, we analyze time-series of neuron activations with RecurrencePlots (RPs) and Recurrence Quantification Analysis (RQA), which permit tovisualize and characterize high-dimensional dynamical systems. We show thatthis approach is useful in a number of ways. First, the two-dimensionalrepresentation offered by RPs provides a way for visualizing thehigh-dimensional dynamics of a reservoir. Our results suggest that, if thenetwork is stable, reservoir and input denote similar line patterns in therespective RPs. Conversely, the more unstable the ESN, the more the RP of thereservoir presents instability patterns. As a second result, we show that the$\mathrm{L_{max}}$ measure is highly correlated with the well-establishedmaximal local Lyapunov exponent. This suggests that complexity measures basedon RP diagonal lines distribution provide a valuable tool to quantify thedegree of network stability. Finally, our analysis shows that all RQA measuresfluctuate on the proximity of the so-called edge of stability, where an ESNtypically achieves maximum computational capability. We verify that thedetermination of the edge of stability provided by such RQA measures is moreaccurate than two well-known criteria based on the Jacobian matrix of thereservoir. Therefore, we claim that RPs and RQA-based analyses can be used asvaluable tools to design an effective network given a specific problem.
arxiv-15900-240 | Testing for Causality in Continuous Time Bayesian Network Models of High-Frequency Data | http://arxiv.org/pdf/1601.06651v1.pdf | author:Jonas Hallgren, Timo Koski category:stat.ML q-fin.TR published:2016-01-25 summary:Continuous time Bayesian networks are investigated with a special focus ontheir ability to express causality. A framework is presented for doinginference in these networks. The central contributions are a representation ofthe intensity matrices for the networks and the introduction of a causalitymeasure. A new model for high-frequency financial data is presented. It iscalibrated to market data and by the new causality measure it performs betterthan older models.
arxiv-15900-241 | Time-Varying Gaussian Process Bandit Optimization | http://arxiv.org/pdf/1601.06650v1.pdf | author:Ilija Bogunovic, Jonathan Scarlett, Volkan Cevher category:stat.ML cs.LG published:2016-01-25 summary:We consider the sequential Bayesian optimization problem with banditfeedback, adopting a formulation that allows for the reward function to varywith time. We model the reward function using a Gaussian process whoseevolution obeys a simple Markov model. We introduce two natural extensions ofthe classical Gaussian process upper confidence bound (GP-UCB) algorithm. Thefirst, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB,instead forgets about old data in a smooth fashion. Our main contributioncomprises of novel regret bounds for these algorithms, providing an explicitcharacterization of the trade-off between the time horizon and the rate atwhich the function varies. We illustrate the performance of the algorithms onboth synthetic and real data, and we find the gradual forgetting of TV-GP-UCBto perform favorably compared to the sharp resetting of R-GP-UCB. Moreover,both algorithms significantly outperform classical GP-UCB, since it treatsstale and fresh data equally.
arxiv-15900-242 | Conditional distribution variability measures for causality detection | http://arxiv.org/pdf/1601.06680v1.pdf | author:José A. R. Fonollosa category:stat.ML cs.LG published:2016-01-25 summary:In this paper we derive variability measures for the conditional probabilitydistributions of a pair of random variables, and we study its application inthe inference of causal-effect relationships. We also study the combination ofthe proposed measures with standard statistical measures in the the frameworkof the ChaLearn cause-effect pair challenge. The developed model obtains an AUCscore of 0.82 on the final test database and ranked second in the challenge.
arxiv-15900-243 | Concept Generation in Language Evolution | http://arxiv.org/pdf/1601.06732v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA published:2016-01-25 summary:This thesis investigates the generation of new concepts from combinations ofexisting concepts as a language evolves. We give a method for combiningconcepts, and will be investigating the utility of composite concepts inlanguage evolution and thence the utility of concept generation.
arxiv-15900-244 | A Label Semantics Approach to Linguistic Hedges | http://arxiv.org/pdf/1601.06738v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL published:2016-01-25 summary:We introduce a model for the linguistic hedges `very' and `quite' within thelabel semantics framework, and combined with the prototype and conceptualspaces theories of concepts. The proposed model emerges naturally from therepresentational framework we use and as such, has a clear semantic grounding.We give generalisations of these hedge models and show that they can becomposed with themselves and with other functions, going on to examine theirbehaviour in the limit of composition.
arxiv-15900-245 | The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels | http://arxiv.org/pdf/1601.06755v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA published:2016-01-25 summary:We investigate the emergence of shared concepts in a community of languageusers using a multi-agent simulation. We extend results showing that negatedassertions are of use in developing shared categories, to include assertionsmodified by linguistic hedges. Results show that using hedged assertionspositively affects the emergence of shared categories in two distinct ways.Firstly, using contraction hedges like `very' gives better convergence overtime. Secondly, using expansion hedges such as `quite' reduces concept overlap.However, both these improvements come at a cost of slower speed of development.
arxiv-15900-246 | Emerging Dimension Weights in a Conceptual Spaces Model of Concept Combination | http://arxiv.org/pdf/1601.06763v1.pdf | author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA published:2016-01-25 summary:We investigate the generation of new concepts from combinations of propertiesas an artificial language develops. To do so, we have developed a new frameworkfor conjunctive concept combination. This framework gives a semantic groundingto the weighted sum approach to concept combination seen in the literature. Weimplement the framework in a multi-agent simulation of language evolution andshow that shared combination weights emerge. The expected value and thevariance of these weights across agents may be predicted from the distributionof elements in the conceptual space, as determined by the underlyingenvironment, together with the rate at which agents adopt others' concepts.When this rate is smaller, the agents are able to converge to weights withlower variance. However, the time taken to converge to a steady statedistribution of weights is longer.
arxiv-15900-247 | Bayesian Estimation of Bipartite Matchings for Record Linkage | http://arxiv.org/pdf/1601.06630v1.pdf | author:Mauricio Sadinle category:stat.ME stat.AP stat.ML published:2016-01-25 summary:The bipartite record linkage task consists of merging two disparate datafilescontaining information on two overlapping sets of entities. This is non-trivialin the absence of unique identifiers and it is important for a wide variety ofapplications given that it needs to be solved whenever we have to combineinformation from different sources. Most statistical techniques currently usedfor record linkage are derived from a seminal paper by Fellegi and Sunter(1969). These techniques usually assume independence in the matching statusesof record pairs to derive estimation procedures and optimal point estimators.We argue that this independence assumption is unreasonable and instead target abipartite matching between the two datafiles as our parameter of interest.Bayesian implementations allow us to quantify uncertainty on the matchingdecisions and derive a variety of point estimators using different lossfunctions. We propose partial Bayes estimates that allow uncertain parts of thebipartite matching to be left unresolved. We evaluate our approach to recordlinkage using a variety of challenging scenarios and show that it outperformsthe traditional methodology. We illustrate the advantages of our methodsmerging two datafiles on casualties from the civil war of El Salvador.
arxiv-15900-248 | Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add | http://arxiv.org/pdf/1601.06815v1.pdf | author:Tyler Highlander, Andres Rodriguez category:cs.NE cs.LG published:2016-01-25 summary:Convolutional neural networks (CNNs) are currently state-of-the-art forvarious classification tasks, but are computationally expensive. Propagatingthrough the convolutional layers is very slow, as each kernel in each layermust sequentially calculate many dot products for a single forward and backwardpropagation which equates to $\mathcal{O}(N^{2}n^{2})$ per kernel per layerwhere the inputs are $N \times N$ arrays and the kernels are $n \times n$arrays. Convolution can be efficiently performed as a Hadamard product in thefrequency domain. The bottleneck is the transformation which has a cost of$\mathcal{O}(N^{2}\log_2 N)$ using the fast Fourier transform (FFT). However,the increase in efficiency is less significant when $N\gg n$ as is the case inCNNs. We mitigate this by using the "overlap-and-add" technique reducing thecomputational complexity to $\mathcal{O}(N^2\log_2 n)$ per kernel. This methodincreases the algorithm's efficiency in both the forward and backwardpropagation, reducing the training and testing time for CNNs. Our empiricalresults show our method reduces computational time by a factor of up to 16.3times the traditional convolution implementation for a 8 $\times$ 8 kernel anda 224 $\times$ 224 image.
arxiv-15900-249 | Survey on the attention based RNN model and its applications in computer vision | http://arxiv.org/pdf/1601.06823v1.pdf | author:Feng Wang, David M. J. Tax category:cs.CV cs.LG published:2016-01-25 summary:The recurrent neural networks (RNN) can be used to solve the sequence tosequence problem, where both the input and the output have sequentialstructures. Usually there are some implicit relations between the structures.However, it is hard for the common RNN model to fully explore the relationsbetween the sequences. In this survey, we introduce some attention based RNNmodels which can focus on different parts of the input for each output item, inorder to explore and take advantage of the implicit relations between the inputand the output items. The different attention mechanisms are described indetail. We then introduce some applications in computer vision which apply theattention based RNN models. The superiority of the attention based RNN model isshown by the experimental results. At last some future research directions aregiven.
arxiv-15900-250 | A Taxonomy of Deep Convolutional Neural Nets for Computer Vision | http://arxiv.org/pdf/1601.06615v1.pdf | author:Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita Prabhu, Srinivas S S Kruthiventi, R. Venkatesh Babu category:cs.CV cs.LG cs.MM published:2016-01-25 summary:Traditional architectures for solving computer vision problems and the degreeof success they enjoyed have been heavily reliant on hand-crafted features.However, of late, deep learning techniques have offered a compellingalternative -- that of automatically learning problem-specific features. Withthis new paradigm, every problem in computer vision is now being re-examinedfrom a deep learning perspective. Therefore, it has become important tounderstand what kind of deep networks are suitable for a given problem.Although general surveys of this fast-moving paradigm (i.e. deep-networks)exist, a survey specific to computer vision is missing. We specificallyconsider one form of deep networks widely used in computer vision -convolutional neural networks (CNNs). We start with "AlexNet" as our base CNNand then examine the broad variations proposed over time to suit differentapplications. We hope that our recipe-style survey will serve as a guide,particularly for novice practitioners intending to use deep-learning techniquesfor computer vision.
arxiv-15900-251 | An Unsupervised Method for Detection and Validation of The Optic Disc and The Fovea | http://arxiv.org/pdf/1601.06608v1.pdf | author:Mrinal Haloi, Samarendra Dandapat, Rohit Sinha category:cs.CV 68T45 published:2016-01-25 summary:In this work, we have presented a novel method for detection of retinal imagefeatures, the optic disc and the fovea, from colour fundus photographs ofdilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map basedmethod was used to detect the optic disc followed by an unsupervisedprobabilistic Latent Semantic Analysis for detection validation. The validationconcept is based on distinct vessels structures in the optic disc. By using theclinical information of standard location of the fovea with respect to theoptic disc, the macula region is estimated. Accuracy of 100\% detection isachieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\%detection accuracy on STARE dataset.
arxiv-15900-252 | Egocentric Activity Recognition with Multimodal Fisher Vector | http://arxiv.org/pdf/1601.06603v1.pdf | author:Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal, Jie Lin category:cs.MM cs.CV published:2016-01-25 summary:With the increasing availability of wearable devices, research on egocentricactivity recognition has received much attention recently. In this paper, webuild a Multimodal Egocentric Activity dataset which includes egocentric videosand sensor data of 20 fine-grained and diverse activity categories. We presenta novel strategy to extract temporal trajectory-like features from sensor data.We propose to apply the Fisher Kernel framework to fuse video and temporalenhanced sensor features. Experiment results show that with careful design offeature extraction and fusion algorithm, sensor data can enhanceinformation-rich video data. We make publicly available the MultimodalEgocentric Activity dataset to facilitate future research.
arxiv-15900-253 | Is swarm intelligence able to create mazes? | http://arxiv.org/pdf/1601.06580v1.pdf | author:Dawid Polap, Marcin Wozniak, Christian Napoli, Emiliano Tramontana category:cs.NE cs.AI published:2016-01-25 summary:In this paper, the idea of applying Computational Intelligence in the processof creation board games, in particular mazes, is presented. For two differentalgorithms the proposed idea has been examined. The results of the experimentsare shown and discussed to present advantages and disadvantages.
arxiv-15900-254 | A Kernel Independence Test for Geographical Language Variation | http://arxiv.org/pdf/1601.06579v1.pdf | author:Dong Nguyen, Jacob Eisenstein category:cs.CL published:2016-01-25 summary:Quantifying the degree of spatial dependence for linguistic variables is akey task for analyzing dialectal variation. However, existing approaches haveimportant drawbacks. First, they make unjustified assumptions about the natureof spatial variation: some assume that the geographical distribution oflinguistic variables is Gaussian, while others assume that linguistic variationis aligned to pre-defined geopolitical units such as states or counties.Second, they are not applicable to all types of linguistic data: someapproaches apply only to frequencies, others to boolean indicators of whether alinguistic variable is present. We present a new method for measuringgeographical language variation, which solves both of these problems. Ourapproach builds on reproducing kernel Hilbert space (RKHS) representations fornonparametric statistics, and takes the form of a test statistic that iscomputed from pairs of individual geotagged observations without aggregationinto predefined geographical bins. We compare this test with prior work usingsynthetic data as well as a diverse set of real datasets: a corpus of Dutchtweets, a Dutch syntactic atlas, and a dataset of letters to the editor inNorth American newspapers. Our proposed test is shown to support robustinferences across a broad range of scenarios and types of data.
arxiv-15900-255 | Robust Influence Maximization | http://arxiv.org/pdf/1601.06551v1.pdf | author:Wei Chen, Tian Lin, Zihan Tan, Mingfei Zhao, Xuren Zhou category:cs.SI cs.LG published:2016-01-25 summary:In this paper, we address the important issue of uncertainty in the edgeinfluence probability estimates for the well studied influence maximizationproblem --- the task of finding $k$ seed nodes in a social network to maximizethe influence spread. We propose the problem of robust influence maximization,which maximizes the worse-case ratio between the influence spread of the chosenseed set and the optimal seed set, given the uncertainty of the parameterinput. We design an algorithm that solves this problem with asolution-dependent bound. We further study uniform sampling and adaptivesampling methods to effectively reduce the uncertainty on parameters andimprove the robustness of the influence maximization task. Our empiricalresults show that parameter uncertainty may greatly affect influencemaximization performance and prior studies that learned influence probabilitiescould lead to poor performance in robust influence maximization due torelatively large uncertainty in parameter estimates, and information cascadebased adaptive sampling method may be an effective way to improve therobustness of influence maximization.
arxiv-15900-256 | Clustering from Sparse Pairwise Measurements | http://arxiv.org/pdf/1601.06683v2.pdf | author:Alaa Saade, Marc Lelarge, Florent Krzakala, Lenka Zdeborová category:cs.SI cs.LG published:2016-01-25 summary:We consider the problem of grouping items into clusters based on few randompairwise comparisons between the items. We introduce three closely relatedalgorithms for this task: a belief propagation algorithm approximating theBayes optimal solution, and two spectral algorithms based on thenon-backtracking and Bethe Hessian operators. For the case of two symmetricclusters, we conjecture that these algorithms are asymptotically optimal inthat they detect the clusters as soon as it is information theoreticallypossible to do so. We substantiate this claim for one of the spectralapproaches we introduce.
arxiv-15900-257 | Relief R-CNN : Utilizing Convolutional Feature Interrelationship for Object Detection | http://arxiv.org/pdf/1601.06719v2.pdf | author:Guiying Li, Junlong Liu, Chunhui Jiang, Liangpeng Zhang, Ke Tang, Yufeng Liu category:cs.CV published:2016-01-25 summary:The state-of-the-art object detection pipeline needs a set of object locationhypotheses followed by a deep CNN classifier. Previous research on thisparadigm, usually extracts features from the same image for the two stepsseparately, which is time consuming and is hard to optimize. This work showsthat the high-level patterns of feature values in deep convolutional featuremap contain plenty of useful spatial information and proposes a new deeplearning approach to object detection, namely Relief R-CNN ($R^2$-CNN). Byextracting positions of objects from these high-level patterns, $R^2$-CNNgenerates region proposals and performs deep classification simultaneouslyusing the same forward CNN features, unifying the formerly separated objectdetection process. In this way, $R^2$-CNN does not involve additionalinformation extraction process for region proposal generation, considerablyreducing the total computation costs. In addition, a recursive fine-tunetechnique is also developed for refining coarse proposals. Empirical resultsshowed that our $R^2$-CNN had a very high speed and a reasonable detection rateeven in the presence of limit on the proposal number, indicating that theregion proposals generated by our $R^2$-CNN are in excellent quality.
arxiv-15900-258 | Long Short-Term Memory-Networks for Machine Reading | http://arxiv.org/pdf/1601.06733v5.pdf | author:Jianpeng Cheng, Li Dong, Mirella Lapata category:cs.CL cs.NE published:2016-01-25 summary:Machine reading, the automatic understanding of text, remains a challengingtask of great value for NLP applications. We propose a machine reader whichprocesses text incrementally from left to right, while linking the current wordto previous words stored in memory and implicitly discovering lexicaldependencies facilitating understanding. The reader is equipped with a LongShort-Term Memory architecture, which differs from previous work in that it hasa memory tape (instead of a memory cell) for adaptively storing pastinformation without severe information compression. We also integrate ourreader with a new attention mechanism in encoder-decoder architecture.Experiments on language modeling, sentiment analysis, and natural languageinference show that our model matches or outperforms the state of the art.
arxiv-15900-259 | A new correlation clustering method for cancer mutation analysis | http://arxiv.org/pdf/1601.06476v1.pdf | author:Jack P. Hou, Amin Emad, Gregory J. Puleo, Jian Ma, Olgica Milenkovic category:cs.LG q-bio.QM published:2016-01-25 summary:Cancer genomes exhibit a large number of different alterations that affectmany genes in a diverse manner. It is widely believed that these alterationsfollow combinatorial patterns that have a strong connection with the underlyingmolecular interaction networks and functional pathways. A better understandingof the generative mechanisms behind the mutation rules and their influence ongene communities is of great importance for the process of driver mutationsdiscovery and for identification of network modules related to cancerdevelopment and progression. We developed a new method for cancer mutationpattern analysis based on a constrained form of correlation clustering.Correlation clustering is an agnostic learning method that can be used forgeneral community detection problems in which the number of communities ortheir structure is not known beforehand. The resulting algorithm, named $C^3$,leverages mutual exclusivity of mutations, patient coverage, and driver networkconcentration principles; it accepts as its input a user determined combinationof heterogeneous patient data, such as that available from TCGA (includingmutation, copy number, and gene expression information), and creates a largenumber of clusters containing mutually exclusive mutated genes in a particulartype of cancer. The cluster sizes may be required to obey some useful soft sizeconstraints, without impacting the computational complexity of the algorithm.To test $C^3$, we performed a detailed analysis on TCGA breast cancer andglioblastoma data and showed that our algorithm outperforms thestate-of-the-art CoMEt method in terms of discovering mutually exclusive genemodules and identifying driver genes. Our $C^3$ method represents a unique toolfor efficient and reliable identification of mutation patterns and driverpathways in large-scale cancer genomics studies.
arxiv-15900-260 | Character-Level Incremental Speech Recognition with Recurrent Neural Networks | http://arxiv.org/pdf/1601.06581v2.pdf | author:Kyuyeon Hwang, Wonyong Sung category:cs.CL cs.LG cs.NE published:2016-01-25 summary:In real-time speech recognition applications, the latency is an importantissue. We have developed a character-level incremental speech recognition (ISR)system that responds quickly even during the speech, where the hypotheses aregradually improved while the speaking proceeds. The algorithm employs aspeech-to-character unidirectional recurrent neural network (RNN), which isend-to-end trained with connectionist temporal classification (CTC), and anRNN-based character-level language model (LM). The output values of theCTC-trained RNN are character-level probabilities, which are processed by beamsearch decoding. The RNN LM augments the decoding by providing long-termdependency information. We propose tree-based online beam search withadditional depth-pruning, which enables the system to process infinitely longinput speech with low latency. This system not only responds quickly on speechbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.The proposed model achieves the word error rate (WER) of 8.90% on the WallStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284training set.
arxiv-15900-261 | Expected Similarity Estimation for Large-Scale Batch and Streaming Anomaly Detection | http://arxiv.org/pdf/1601.06602v2.pdf | author:Markus Schneider, Wolfgang Ertel, Fabio Ramos category:cs.LG cs.AI published:2016-01-25 summary:We present a novel algorithm for anomaly detection on very large datasets anddata streams. The method, named EXPected Similarity Estimation (EXPoSE), iskernel-based and able to efficiently compute the similarity between new datapoints and the distribution of regular data. The estimator is formulated as aninner product with a reproducing kernel Hilbert space embedding and makes noassumption about the type or shape of the underlying data distribution. We showthat offline (batch) learning with EXPoSE can be done in linear time and online(incremental) learning takes constant time per instance and model update.Furthermore, EXPoSE can make predictions in constant time, while it requiresonly constant memory. In addition we propose different methodologies forconcept drift adaptation on evolving data streams. On several real datasets wedemonstrate that our approach can compete with state of the art algorithms foranomaly detection while being significant faster than techniques with the samediscriminant power.
arxiv-15900-262 | A Robust UCB Scheme for Active Learning in Regression from Strategic Crowds | http://arxiv.org/pdf/1601.06750v2.pdf | author:Divya Padmanabhan, Satyanath Bhat, Dinesh Garg, Shirish Shevade, Y. Narahari category:cs.LG stat.ML published:2016-01-25 summary:We study the problem of training an accurate linear regression model byprocuring labels from multiple noisy crowd annotators, under a budgetconstraint. We propose a Bayesian model for linear regression in crowdsourcingand use variational inference for parameter estimation. To minimize the numberof labels crowdsourced from the annotators, we adopt an active learningapproach. In this specific context, we prove the equivalence of well-studiedcriteria of active learning like entropy minimization and expected errorreduction. Interestingly, we observe that we can decouple the problems ofidentifying an optimal unlabeled instance and identifying an annotator to labelit. We observe a useful connection between the multi-armed bandit framework andthe annotator selection in active learning. Due to the nature of thedistribution of the rewards on the arms, we use the Robust Upper ConfidenceBound (UCB) scheme with truncated empirical mean estimator to solve theannotator selection problem. This yields provable guarantees on the regret. Wefurther apply our model to the scenario where annotators are strategic anddesign suitable incentives to induce them to put in their best efforts.
arxiv-15900-263 | Pixel Recurrent Neural Networks | http://arxiv.org/pdf/1601.06759v2.pdf | author:Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu category:cs.CV cs.LG cs.NE published:2016-01-25 summary:Modeling the distribution of natural images is a landmark problem inunsupervised learning. This task requires an image model that is at onceexpressive, tractable and scalable. We present a deep neural network thatsequentially predicts the pixels in an image along the two spatial dimensions.Our method models the discrete probability of the raw pixel values and encodesthe complete set of dependencies in the image. Architectural novelties includefast two-dimensional recurrent layers and an effective use of residualconnections in deep recurrent networks. We achieve log-likelihood scores onnatural images that are considerably better than the previous state of the art.Our main results also provide benchmarks on the diverse ImageNet dataset.Samples generated from the model appear crisp, varied and globally coherent.
arxiv-15900-264 | Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach | http://arxiv.org/pdf/1601.06403v3.pdf | author:Ali Moharrer, Shuangqing Wei, George T. Amariucai, Jing Deng category:cs.IT cs.CV math.IT stat.ML published:2016-01-24 summary:In latent Gaussian trees the pairwise correlation signs between the variablesare intrinsically unrecoverable. Such information is vital since it completelydetermines the direction in which two variables are associated. %suchsingularity has been ignored in many studies. In this work, we resort toinformation theoretical approaches to achieve two fundamental goals: First, wequantify the amount of information loss due to unrecoverable sign information.Second, we show the importance of such information in determining the maximumachievable rate region, in which the observed output vector can be synthesized,given its probability density function. In particular, we model the graphicalmodel as a communication channel and propose a new layered encoding frameworkto synthesize observed data using upper layer Gaussian inputs and independentBernoulli correlation sign inputs from each layer. We find the achievable rateregion for the rate tuples of multi-layer latent Gaussian messages tosynthesize the desired observables.
arxiv-15900-265 | Fast Binary Embedding via Circulant Downsampled Matrix -- A Data-Independent Approach | http://arxiv.org/pdf/1601.06342v1.pdf | author:Sung-Hsien Hsieh, Chun-Shien Lu, Soo-Chang Pei category:cs.IT cs.CV cs.LG math.IT published:2016-01-24 summary:Binary embedding of high-dimensional data aims to produce low-dimensionalbinary codes while preserving discriminative power. State-of-the-art methodsoften suffer from high computation and storage costs. We present a simple andfast embedding scheme by first downsampling N-dimensional data intoM-dimensional data and then multiplying the data with an MxM circulant matrix.Our method requires O(N +M log M) computation and O(N) storage costs. We proveif data have sparsity, our scheme can achieve similarity-preserving well.Experiments further demonstrate that though our method is cost-effective andfast, it still achieves comparable performance in image applications.
arxiv-15900-266 | Solving Dense Image Matching in Real-Time using Discrete-Continuous Optimization | http://arxiv.org/pdf/1601.06274v1.pdf | author:Alexander Shekhovtsov, Christian Reinbacher, Gottfried Graber, Thomas Pock category:cs.CV published:2016-01-23 summary:Dense image matching is a fundamental low-level problem in Computer Vision,which has received tremendous attention from both discrete and continuousoptimization communities. The goal of this paper is to combine the advantagesof discrete and continuous optimization in a coherent framework. We devise amodel based on energy minimization, to be optimized by both discrete andcontinuous algorithms in a consistent way. In the discrete setting, we proposea novel optimization algorithm that can be massively parallelized. In thecontinuous setting we tackle the problem of non-convex regularizers by aformulation based on differences of convex functions. The resulting hybriddiscrete-continuous algorithm can be efficiently accelerated by modern GPUs andwe demonstrate its real-time performance for the applications of dense stereomatching and optical flow.
arxiv-15900-267 | Person Re-Identification by Discriminative Selection in Video Ranking | http://arxiv.org/pdf/1601.06260v1.pdf | author:Taiqing Wang, Shaogang Gong, Xiatian Zhu, Shengjin Wang category:cs.CV published:2016-01-23 summary:Current person re-identification (ReID) methods typically rely onsingle-frame imagery features, whilst ignoring space-time information fromimage sequences often available in the practical surveillance scenarios.Single-frame (single-shot) based visual appearance matching is inherentlylimited for person ReID in public spaces due to the challenging visualambiguity and uncertainty arising from non-overlapping camera views whereviewing condition changes can cause significant people appearance variations.In this work, we present a novel model to automatically select the mostdiscriminative video fragments from noisy/incomplete image sequences of peoplefrom which reliable space-time and appearance features can be computed, whilstsimultaneously learning a video ranking function for person ReID. Using thePRID$2011$, iLIDS-VID, and HDA+ image sequence datasets, we extensivelyconducted comparative evaluations to demonstrate the advantages of the proposedmodel over contemporary gait recognition, holistic image sequence matching andstate-of-the-art single-/multi-shot ReID methods.
arxiv-15900-268 | Using compatible shape descriptor for lexicon reduction of printed Farsi subwords | http://arxiv.org/pdf/1601.06251v1.pdf | author:Homa Davoudi, Ehsanollah Kabir category:cs.CV published:2016-01-23 summary:This Paper presents a method for lexicon reduction of Printed Farsi subwordsbased on their holistic shape features. Because of the large number of Persiansubwords variously shaped from a simple letter to a complex combination ofseveral connected characters, it is not easy to find a fixed shape descriptorsuitable for all subwords. In this paper, we propose to select the descriptoraccording to the input shape characteristics. To do this, a neural network istrained to predict the appropriate descriptor of the input image. This networkis implemented in the proposed lexicon reduction system to decide on thedescriptor used for comparison of the query image with the lexicon entries.Evaluating the proposed method on a dataset of Persian subwords allows one toattest the effectiveness of the proposed idea of dealing differently withvarious query shapes.
arxiv-15900-269 | Minimax Lower Bounds for Linear Independence Testing | http://arxiv.org/pdf/1601.06259v1.pdf | author:Aaditya Ramdas, David Isenberg, Aarti Singh, Larry Wasserman category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2016-01-23 summary:Linear independence testing is a fundamental information-theoretic andstatistical problem that can be posed as follows: given $n$ points$\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distributionwhere $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^TX$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n\to \infty$, $(p+q)/n \leq \kappa < \infty$, without sparsity assumptions). Insummary, our results imply that $n$ must be at least as large as $\sqrt{pq}/\\Sigma_{XY}\_F^2$ for any procedure (test) to have non-trivial power,where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also providesome evidence that the lower bound is tight, by connections to two-sampletesting and regression in specific settings.
arxiv-15900-270 | Super-resolution reconstruction of hyperspectral images via low rank tensor modeling and total variation regularization | http://arxiv.org/pdf/1601.06243v1.pdf | author:Shiying He, Haiwei Zhou, Yao Wang, Wenfei Cao, Zhi Han category:cs.CV published:2016-01-23 summary:In this paper, we propose a novel approach to hyperspectral imagesuper-resolution by modeling the global spatial-and-spectral correlation andlocal smoothness properties over hyperspectral images. Specifically, we utilizethe tensor nuclear norm and tensor folded-concave penalty functions to describethe global spatial-and-spectral correlation hidden in hyperspectral images, and3D total variation (TV) to characterize the local spatial-and-spectralsmoothness across all hyperspectral bands. Then, we develop an efficientalgorithm for solving the resulting optimization problem by combing the locallinear approximation (LLA) strategy and alternative direction method ofmultipliers (ADMM). Experimental results on one hyperspectral image datasetillustrate the merits of the proposed approach.
arxiv-15900-271 | Divide and Conquer Local Average Regression | http://arxiv.org/pdf/1601.06239v2.pdf | author:Xiangyu Chang, Shaobo Lin, Yao Wang category:cs.LG math.ST stat.TH published:2016-01-23 summary:The divide and conquer strategy, which breaks a massive data set into a se-ries of manageable data blocks, and then combines the independent results ofdata blocks to obtain a final decision, has been recognized as astate-of-the-art method to overcome challenges of massive data analysis. Inthis paper, we merge the divide and conquer strategy with local averageregression methods to infer the regressive relationship of input-output pairsfrom a massive data set. After theoretically analyzing the pros and cons, wefind that although the divide and conquer local average regression can reachthe optimal learning rate, the restric- tion to the number of data blocks is abit strong, which makes it only feasible for small number of data blocks. Wethen propose two variants to lessen (or remove) this restriction. Our resultsshow that these variants can achieve the optimal learning rate with much milderrestriction (or without such restriction). Extensive experimental studies arecarried out to verify our theoretical assertions.
arxiv-15900-272 | Undecidability of the Lambek calculus with a relevant modality | http://arxiv.org/pdf/1601.06303v2.pdf | author:Max Kanovich, Stepan Kuznetsov, Andre Scedrov category:math.LO cs.CL 03B47 published:2016-01-23 summary:Morrill and Valentin in the paper "Computational coverage of TLG:Nonlinearity" considered an extension of the Lambek calculus enriched by aso-called "exponential" modality. This modality behaves in the "relevant"style, that is, it allows contraction and permutation, but not weakening.Morrill and Valentin stated an open problem whether this system is decidable.Here we show its undecidability. Our result remains valid if we consider thefragment where all division operations have one direction. We also show thatthe derivability problem in a restricted case, where the modality can beapplied only to variables (primitive types), is decidable and belongs to the NPclass.
arxiv-15900-273 | Automatic recognition of element classes and boundaries in the birdsong with variable sequences | http://arxiv.org/pdf/1601.06248v1.pdf | author:Takuya Koumura, Kazuo Okanoya category:q-bio.NC cs.LG cs.SD published:2016-01-23 summary:Researches on sequential vocalization often require analysis of vocalizationsin long continuous sounds. In such studies as developmental ones or studiesacross generations in which days or months of vocalizations must be analyzed,methods for automatic recognition would be strongly desired. Although methodsfor automatic speech recognition for application purposes have been intensivelystudied, blindly applying them for biological purposes may not be an optimalsolution. This is because, unlike human speech recognition, analysis ofsequential vocalizations often requires accurate extraction of timinginformation. In the present study we propose automated systems suitable forrecognizing birdsong, one of the most intensively investigated sequentialvocalizations, focusing on the three properties of the birdsong. First, a songis a sequence of vocal elements, called notes, which can be grouped intocategories. Second, temporal structure of birdsong is precisely controlled,meaning that temporal information is important in song analysis. Finally, notesare produced according to certain probabilistic rules, which may facilitate theaccurate song recognition. We divided the procedure of song recognition intothree sub-steps: local classification, boundary detection, and globalsequencing, each of which corresponds to each of the three properties ofbirdsong. We compared the performances of several different ways to arrangethese three steps. As results, we demonstrated a hybrid model of a deep neuralnetwork and a hidden Markov model is effective in recognizing birdsong withvariable note sequences. We propose suitable arrangements of methods accordingto whether accurate boundary detection is needed. Also we designed the newmeasure to jointly evaluate the accuracy of note classification and boundarydetection. Our methods should be applicable, with small modification andtuning, to the songs in other species that hold the three properties of thesequential vocalization.
arxiv-15900-274 | Universal Collaboration Strategies for Signal Detection: A Sparse Learning Approach | http://arxiv.org/pdf/1601.06201v1.pdf | author:Prashant Khanduri, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Pramod K. Varshney category:cs.LG stat.ML published:2016-01-22 summary:This paper considers the problem of high dimensional signal detection in alarge distributed network. In contrast to conventional distributed detection,the nodes in the network can update their observations by combiningobservations from other one-hop neighboring nodes (spatial collaboration).Under the assumption that only a small subset of nodes are capable ofcommunicating with the Fusion Center (FC), our goal is to design optimalcollaboration strategies which maximize the detection performance at the FC.Note that, if one optimizes the system for the detection of a single knownsignal then the network cannot generalize well to other detection tasks. Hence,we propose to design optimal collaboration strategies which are universal for aclass of equally probable deterministic signals. By establishing theequivalence between the collaboration strategy design problem and Sparse PCA,we seek the answers to the following questions: 1) How much do we gain fromoptimizing the collaboration strategy? 2) What is the effect of dimensionalityreduction for different sparsity constraints? 3) How much do we lose in termsof detection performance by adopting a universal system (cost of universality)?
arxiv-15900-275 | Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing | http://arxiv.org/pdf/1601.06068v1.pdf | author:Shashi Narayan, Siva Reddy, Shay B. Cohen category:cs.CL published:2016-01-22 summary:One of the limitations of semantic parsing approaches to open-domain questionanswering is the lexicosyntactic gap between natural language questions andknowledge base entries -- there are many ways to ask a question, all with thesame answer. In this paper we propose to bridge this gap by generatingparaphrases of the input question with the goal that at least one of them willbe correctly mapped to a knowledge-base query. We introduce a novel grammarmodel for paraphrase generation that does not require any sentence-alignedparaphrase corpus. Our key idea is to leverage the flexibility and scalabilityof latent-variable probabilistic context-free grammars to sample paraphrases.We do an extrinsic evaluation of our paraphrases by plugging them into asemantic parser for Freebase. Our evaluation experiments on the WebQuestionsbenchmark dataset show that the performance of the semantic parsersignificantly improves over strong baselines.
arxiv-15900-276 | Bitwise Neural Networks | http://arxiv.org/pdf/1601.06071v1.pdf | author:Minje Kim, Paris Smaragdis category:cs.LG cs.AI cs.NE published:2016-01-22 summary:Based on the assumption that there exists a neural network that efficientlyrepresents a set of Boolean functions between all binary inputs and outputs, wepropose a process for developing and deploying neural networks whose weightparameters, bias terms, input, and intermediate hidden layer output signals,are all binary-valued, and require only basic bit logic for the feedforwardpass. The proposed Bitwise Neural Network (BNN) is especially suitable forresource-constrained environments, since it replaces either floating orfixed-point arithmetic with significantly more efficient bitwise operations.Hence, the BNN requires for less spatial complexity, less memory bandwidth, andless power consumption in hardware. In order to design such networks, wepropose to add a few training schemes, such as weight compression and noisybackpropagation, which result in a bitwise network that performs almost as wellas its corresponding real-valued network. We test the proposed network on theMNIST dataset, represented using binary features, and show that BNNs result incompetitive performance while offering dramatic computational savings.
arxiv-15900-277 | 3-D/2-D Registration of Cardiac Structures by 3-D Contrast Agent Distribution Estimation | http://arxiv.org/pdf/1601.06062v1.pdf | author:Matthias Hoffmann, Christopher Kowalewski, Andreas Maier, Klaus Kurzidim, Norbert Strobel, Joachim Hornegger category:cs.CV published:2016-01-22 summary:For augmented fluoroscopy during cardiac catheter ablation procedures, apreoperatively acquired 3-D model of the left atrium of the patient can beregistered to X-ray images. Therefore the 3D-model is matched with the contrastagent based appearance of the left atrium. Commonly, only small amounts ofcontrast agent (CA) are used to locate the left atrium. This is why we focus onrobust registration methods that work also if the structure of interest is onlypartially contrasted. In particular, we propose two similarity measures forCA-based registration: The first similarity measure, explicit apparent edges,focuses on edges of the patient anatomy made visible by contrast agent and canbe computed quickly on the GPU. The second novel similarity measure computes acontrast agent distribution estimate (CADE) inside the 3-D model and rates itsconsistency with the CA seen in biplane fluoroscopic images. As the CADEcomputation involves a reconstruction of CA in 3-D using the CA within thefluoroscopic images, it is slower. Using a combination of both methods, ourevaluation on 11 well-contrasted clinical datasets yielded an error of7.9+/-6.3 mm over all frames. For 10 datasets with little CA, we obtained anerror of 8.8+/-6.7 mm. Our new methods outperform a registration based on theprojected shadow significantly (p<0.05).
arxiv-15900-278 | Topological descriptors for 3D surface analysis | http://arxiv.org/pdf/1601.06057v1.pdf | author:Matthias Zeppelzauer, Bartosz Zieliński, Mateusz Juda, Markus Seidl category:cs.CV published:2016-01-22 summary:We investigate topological descriptors for 3D surface analysis, i.e. theclassification of surfaces according to their geometric fine structure. On adataset of high-resolution 3D surface reconstructions we compute persistencediagrams for a 2D cubical filtration. In the next step we investigate differenttopological descriptors and measure their ability to discriminate structurallydifferent 3D surface patches. We evaluate their sensitivity to differentparameters and compare the performance of the resulting topological descriptorsto alternative (non-topological) descriptors. We present a comprehensiveevaluation that shows that topological descriptors are (i) robust, (ii) yieldstate-of-the-art performance for the task of 3D surface analysis and (iii)improve classification performance when combined with non-topologicaldescriptors.
arxiv-15900-279 | Manifold-Kernels Comparison in MKPLS for Visual Speech Recognition | http://arxiv.org/pdf/1601.05861v1.pdf | author:Amr Bakry, Ahmed Elgammal category:cs.CV published:2016-01-22 summary:Speech recognition is a challenging problem. Due to the acoustic limitations,using visual information is essential for improving the recognition accuracy inreal-life unconstraint situations. One common approach is to model the visualrecognition as nonlinear optimization problem. Measuring the distances betweenvisual units is essential for solving this problem. Embedding the visual unitson a manifold and using manifold kernels is one way to measure these distances.This work is intended to evaluate the performance of several manifold kernelsfor solving the problem of visual speech recognition. We show the theory behindeach kernel. We apply manifold kernel partial least squares framework to OuluVsand AvLetters databases, and show empirical comparison between all kernels.This framework provides convenient way to explore different kernels.
arxiv-15900-280 | A Mathematical Formalization of Hierarchical Temporal Memory's Spatial Pooler | http://arxiv.org/pdf/1601.06116v2.pdf | author:James Mnatzaganian, Ernest Fokoué, Dhireesha Kudithipudi category:stat.ML cs.LG q-bio.NC published:2016-01-22 summary:Hierarchical temporal memory (HTM) is an emerging machine learning algorithm,with the potential to provide a means to perform predictions on spatiotemporaldata. The algorithm, inspired by the neocortex, currently does not have acomprehensive mathematical framework. This work brings together all aspects ofthe spatial pooler (SP), a critical learning component in HTM, under a singleunifying framework. The primary learning mechanism is explored, where a maximumlikelihood estimator for determining the degree of permanence update isproposed. The boosting mechanisms are studied and found to be only relevantduring the initial few iterations of the network. Observations are maderelating HTM to well-known algorithms such as competitive learning andattribute bagging. Methods are provided for using the SP for classification aswell as dimensionality reduction. Empirical evidence verifies that given theproper parameterizations, the SP may be used for feature learning.
arxiv-15900-281 | Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem | http://arxiv.org/pdf/1601.06207v3.pdf | author:Alican Nalci, Igor Fedorov, Bhaskar D. Rao category:cs.LG stat.ML published:2016-01-22 summary:In this paper we introduce a hierarchical Bayesian framework to obtain sparseand non-negative solutions to the sparse non-negative least squares problem(S-NNLS). We introduce a new family of scale mixtures, the Rectified GaussianScale Mixture (R-GSM), to model the sparsity enforcing prior distribution forthe signal of interest. One advantage of the R-GSM prior is that through properchoice of the mixing density it encompasses a wide variety of heavy taileddistributions, such as the rectified Laplacian and rectified Student's tdistributions. Similar to the Gaussian Scale Mixture (GSM) approach, a Type IIExpectation-Maximization framework is developed to estimate thehyper-parameters and obtain a point estimate of the parameter of interest. Inthe proposed method, called rectified Sparse Bayesian Learning (R-SBL), weprovide two ways to perform the Expectation step; Markov-Chain Monte-Carlo(MCMC) simulations and a simple yet effective diagonal approximation approach(DA). Through numerical experiments we show that R-SBL outperforms existingS-NNLS solvers in terms of both signal and support recovery and that theproposed DA approach admits both computational efficiency and numericalaccuracy.
arxiv-15900-282 | Why Do Urban Legends Go Viral? | http://arxiv.org/pdf/1601.06081v1.pdf | author:Marco Guerini, Carlo Strapparava category:cs.CL cs.CY cs.SI published:2016-01-22 summary:Urban legends are a genre of modern folklore, consisting of stories aboutrare and exceptional events, just plausible enough to be believed, which tendto propagate inexorably across communities. In our view, while urban legendsrepresent a form of "sticky" deceptive text, they are marked by a tensionbetween the credible and incredible. They should be credible like a newsarticle and incredible like a fairy tale to go viral. In particular we willfocus on the idea that urban legends should mimic the details of news (who,where, when) to be credible, while they should be emotional and readable like afairy tale to be catchy and memorable. Using NLP tools we will provide aquantitative analysis of these prototypical characteristics. We also lay outsome machine learning experiments showing that it is possible to recognize anurban legend using just these simple features.
arxiv-15900-283 | GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach | http://arxiv.org/pdf/1601.05893v1.pdf | author:Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel category:cs.AI cs.CL cs.DB cs.IR published:2016-01-22 summary:Location tagging, also known as geotagging or geolocation, is the process ofassigning geographical coordinates to input data. In this paper we present analgorithm for location tagging of textual documents. Our approach makes use ofprevious work in natural language processing by using a state-of-the-artpart-of-speech tagger and named entity recognizer to find blocks of text whichmay refer to locations. A knowledge base (OpenStreatMap) is then used to find alist of possible locations for each block. Finally, one location is chosen foreach block by assigning distance-based scores to each location and repeatedlyselecting the location and block with the best score. We tested our geolocationalgorithm with Wikipedia articles about topics with a well-defined geographicallocation that are geotagged by the articles' authors, where classificationapproaches have achieved median errors as low as 11 km, with attainableaccuracy limited by the class size. Our approach achieved a 10th percentileerror of 490 metres and median error of 54 kilometres on the Wikipedia datasetwe used. When considering the five location tags with the greatest scores, 50%of articles were assigned at least one tag within 8.5 kilometres of thearticle's author-assigned true location. We also tested our approach on Twittermessages that are tagged with the location from which the message was sent.Twitter texts are challenging because they are short and unstructured and oftendo not contain words referring to the location they were sent from, but weobtain potentially useful results. We explain how we use the Spark frameworkfor data analytics to collect and process our test data. In general,classification-based approaches for location tagging may be reaching theirupper accuracy limit, but our precision-focused approach has high accuracy forsome texts and shows significant potential for improvement overall.
arxiv-15900-284 | Unsupervised convolutional neural networks for motion estimation | http://arxiv.org/pdf/1601.06087v1.pdf | author:Aria Ahmadi, Ioannis Patras category:cs.CV published:2016-01-22 summary:Traditional methods for motion estimation estimate the motion field F betweena pair of images as the one that minimizes a predesigned cost function. In thispaper, we propose a direct method and train a Convolutional Neural Network(CNN) that when, at test time, is given a pair of images as input it produces adense motion field F at its output layer. In the absence of large datasets withground truth motion that would allow classical supervised training, we proposeto train the network in an unsupervised manner. The proposed cost function thatis optimized during training, is based on the classical optical flowconstraint. The latter is differentiable with respect to the motion field and,therefore, allows backpropagation of the error to previous layers of thenetwork. Our method is tested on both synthetic and real image sequences andperforms similarly to the state-of-the-art methods.
arxiv-15900-285 | Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs | http://arxiv.org/pdf/1601.06105v1.pdf | author:Jonathan Root, Venkatesh Saligrama, Jing Qian category:stat.ML cs.LG published:2016-01-22 summary:We propose a non-parametric anomaly detection algorithm for high dimensionaldata. We first rank scores derived from nearest neighbor graphs on $n$-pointnominal training data. We then train limited complexity models to imitate thesescores based on the max-margin learning-to-rank framework. A test-point isdeclared as an anomaly at $\alpha$-false alarm level if the predicted score isin the $\alpha$-percentile. The resulting anomaly detector is shown to beasymptotically optimal in that for any false alarm rate $\alpha$, its decisionregion converges to the $\alpha$-percentile minimum volume level set of theunknown underlying density. In addition, we test both the statisticalperformance and computational efficiency of our algorithm on a number ofsynthetic and real-data experiments. Our results demonstrate the superiority ofour algorithm over existing $K$-NN based anomaly detection algorithms, withsignificant computational savings.
arxiv-15900-286 | Speech vocoding for laboratory phonology | http://arxiv.org/pdf/1601.05991v2.pdf | author:Milos Cernak, Stefan Benus, Alexandros Lazaridis category:cs.CL cs.SD published:2016-01-22 summary:Using phonological speech vocoding, we propose a platform for exploringrelations between phonology and speech processing, and in broader terms, forexploring relations between the abstract and physical structures of a speechsignal. Our goal is to make a step towards bridging phonology and speechprocessing and to contribute to the program of Laboratory Phonology. We showthree application examples for laboratory phonology: compositional phonologicalspeech modelling, a comparison of phonological systems and an experimentalphonological parametric text-to-speech (TTS) system. The featuralrepresentations of the following three phonological systems are considered inthis work: (i) Government Phonology (GP), (ii) the Sound Pattern of English(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocodedspeech, we conclude that the latter achieves slightly better results than theformer. However, GP - the most compact phonological speech representation -performs comparably to the systems with a higher number of phonologicalfeatures. The parametric TTS based on phonological speech representation, andtrained from an unlabelled audiobook in an unsupervised manner, achievesintelligibility of 85% of the state-of-the-art parametric speech synthesis. Weenvision that the presented approach paves the way for researchers in bothfields to form meaningful hypotheses that are explicitly testable using theconcepts developed and exemplified in this paper. On the one hand, laboratoryphonologists might test the applied concepts of their theoretical models, andon the other hand, the speech processing community may utilize the conceptsdeveloped for the theoretical phonological models for improvements of thecurrent state-of-the-art applications.
arxiv-15900-287 | When is Clustering Perturbation Robust? | http://arxiv.org/pdf/1601.05900v1.pdf | author:Margareta Ackerman, Jarrod Moore category:cs.LG cs.CV published:2016-01-22 summary:Clustering is a fundamental data mining tool that aims to divide data intogroups of similar items. Generally, intuition about clustering reflects theideal case -- exact data sets endowed with flawless dissimilarity betweenindividual instances. In practice however, these cases are in the minority, and clusteringapplications are typically characterized by noisy data sets with approximatepairwise dissimilarities. As such, the efficacy of clustering methods inpractical applications necessitates robustness to perturbations. In this paper, we perform a formal analysis of perturbation robustness,revealing that the extent to which algorithms can exhibit this desirablecharacteristic is inherently limited, and identifying the types of structuresthat allow popular clustering paradigms to discover meaningful clusters inspite of faulty data.
arxiv-15900-288 | Orthogonal Echo State Networks and stochastic evaluations of likelihoods | http://arxiv.org/pdf/1601.05911v2.pdf | author:Norbert Michael Mayer, Ying-Hao Yu category:cs.NE published:2016-01-22 summary:In this paper we report the likelihood estimates that are performed on timeseries using a echo state network with orthogonal recurrent connectivity. Theresults indicate that the optimal performance depends on the way of balancingthe input strength with the recurrent activity, which also has an influence onthe network with regard to the quality of the short term prediction versusprediction that accounts for influences that date back a long time in the inputhistory. Finally, sensitivity of such networks against noise/finite accuracy ofnetwork states in the recurrent layer is investigated. In addition, a measurethat bases on mutual information is introduced in order to best quantify theperformance of the network with the time series.
arxiv-15900-289 | On the Latent Variable Interpretation in Sum-Product Networks | http://arxiv.org/pdf/1601.06180v1.pdf | author:Robert Peharz, Robert Gens, Franz Pernkopf, Pedro Domingos category:cs.AI cs.LG 62 published:2016-01-22 summary:One of the central themes in Sum-Product networks (SPNs) is theinterpretation of sum nodes as marginalized latent variables (LVs). Thisinterpretation yields an increased syntactic or semantic structure, allows theapplication of the EM algorithm and to efficiently perform MPE inference. Inliterature, the LV interpretation was justified by explicitly introducing theindicator variables corresponding to the LVs' states. However, as pointed outin this paper, this approach is in conflict with the completeness condition inSPNs and does not fully specify the probabilistic model. We propose a remedyfor this problem by modifying the original approach for introducing the LVs,which we call SPN augmentation. We discuss conditional independencies inaugmented SPNs, formally establish the probabilistic interpretation of thesum-weights and give an interpretation of augmented SPNs as Bayesian networks.Based on these results, we find a sound derivation of the EM algorithm forSPNs, which was presented mistaken in literature. Furthermore, theViterbi-style algorithm for MPE proposed in literature was never proven to becorrect. We show that this is indeed a correct algorithm, when applied toaugmented SPNs. Our theoretical results are confirmed in experiments onsynthetic data and 103 real-world datasets.
arxiv-15900-290 | Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic Modeling in Speech Recognition | http://arxiv.org/pdf/1601.05936v1.pdf | author:Pranay Dighe, Gil Luyet, Afsaneh Asaei, Herve Bourlard category:cs.CL cs.LG stat.ML published:2016-01-22 summary:We propose to model the acoustic space of deep neural network (DNN)class-conditional posterior probabilities as a union of low-dimensionalsubspaces. To that end, the training posteriors are used for dictionarylearning and sparse coding. Sparse representation of the test posteriors usingthis dictionary enables projection to the space of training data. Relying onthe fact that the intrinsic dimensions of the posterior subspaces are indeedvery small and the matrix of all posteriors belonging to a class has a very lowrank, we demonstrate how low-dimensional structures enable further enhancementof the posteriors and rectify the spurious errors due to mismatch conditions.The enhanced acoustic modeling method leads to improvements in continuousspeech recognition task using hybrid DNN-HMM (hidden Markov model) framework inboth clean and noisy conditions, where upto 15.4% relative reduction in worderror rate (WER) is achieved.
arxiv-15900-291 | Efficient Globally Optimal 2D-to-3D Deformable Shape Matching | http://arxiv.org/pdf/1601.06070v2.pdf | author:Zorah Lähner, Emanuele Rodolà, Frank R. Schmidt, Michael M. Bronstein, Daniel Cremers category:cs.CV published:2016-01-22 summary:We propose the first algorithm for non-rigid 2D-to-3D shape matching, wherethe input is a 2D shape represented as a planar curve and a 3D shaperepresented as a surface; the output is a continuous curve on the surface. Wecast the problem as finding the shortest circular path on the prod- uct3-manifold of the surface and the curve. We prove that the optimal matching canbe computed in polynomial time with a (worst-case) complexity of$O(mn^2\log(n))$, where $m$ and $n$ denote the number of vertices on thetemplate curve and the 3D shape respectively. We also demonstrate that inpractice the runtime is essentially linear in $m\!\cdot\! n$ making it anefficient method for shape analysis and shape retrieval. Quantitativeevaluation confirms that the method provides excellent results for sketch-baseddeformable 3D shape re- trieval.
arxiv-15900-292 | Depth and Reflection Total Variation for Single Image Dehazing | http://arxiv.org/pdf/1601.05994v1.pdf | author:Wei Wang, Chuanjiang He category:cs.CV published:2016-01-22 summary:Haze removal has been a very challenging problem due to its ill-posedness,which is more ill-posed if the input data is only a single hazy image. In thispaper, we present a new approach for removing haze from a single input image.The proposed method combines the model widely used to describe the formation ofa haze image with the assumption in Retinex that an image is the product of theillumination and the reflection. We assume that the depth and reflectionfunctions are spatially piecewise smooth in the model, where the totalvariation is used for the regularization. The proposed model is defined as aconstrained optimization problem, which is solved by an alternatingminimization scheme and the fast gradient projection algorithm. Some theoreticanalyses are given for the proposed model and algorithm. Finally, numericalexamples are presented to demonstrate that our method can restore vivid andcontrastive hazy images effectively.
arxiv-15900-293 | Geometric-Algebra LMS Adaptive Filter and its Application to Rotation Estimation | http://arxiv.org/pdf/1601.06044v1.pdf | author:Wilder B. Lopes, Anas Al-Nuaimi, Cassio G. Lopes category:cs.CV cs.CG published:2016-01-22 summary:This paper exploits Geometric (Clifford) Algebra (GA) theory in order todevise and introduce a new adaptive filtering strategy. From a least-squarescost function, the gradient is calculated following results from GeometricCalculus (GC), the extension of GA to handle differential and integralcalculus. The novel GA least-mean-squares (GA-LMS) adaptive filter, whichinherits properties from standard adaptive filters and from GA, is developed torecursively estimate a rotor (multivector), a hypercomplex quantity able todescribe rotations in any dimension. The adaptive filter (AF) performance isassessed via a 3D point-clouds registration problem, which contains a rotationestimation step. Calculating the AF computational complexity suggests that itcan contribute to reduce the cost of a full-blown 3D registration algorithm,especially when the number of points to be processed grows. Moreover, theemployed GA/GC framework allows for easily applying the resulting filter toestimating rotors in higher dimensions.
arxiv-15900-294 | Online Event Recognition from Moving Vessel Trajectories | http://arxiv.org/pdf/1601.06041v1.pdf | author:Kostas Patroumpas, Elias Alevizos, Alexander Artikis, Marios Vodas, Nikos Pelekis, Yannis Theodoridis category:cs.CV cs.AI published:2016-01-22 summary:We present a system for online monitoring of maritime activity over streamingpositions from numerous vessels sailing at sea. It employs an online trackingmodule for detecting important changes in the evolving trajectory of eachvessel across time, and thus can incrementally retain concise, yet reliablesummaries of its recent movement. In addition, thanks to its complex eventrecognition module, this system can also offer instant notification to marineauthorities regarding emergency situations, such as risk of collisions,suspicious moves in protected zones, or package picking at open sea. Not onlydid our extensive tests validate the performance, efficiency, and robustness ofthe system against scalable volumes of real-world and synthetically enlargeddatasets, but its deployment against online feeds from vessels has alsoconfirmed its capabilities for effective, real-time maritime surveillance.
arxiv-15900-295 | Recommender systems inspired by the structure of quantum theory | http://arxiv.org/pdf/1601.06035v1.pdf | author:Cyril Stark category:cs.LG cs.IT math.IT math.OC quant-ph stat.ML published:2016-01-22 summary:Physicists use quantum models to describe the behavior of physical systems.Quantum models owe their success to their interpretability, to their relationto probabilistic models (quantization of classical models) and to their highpredictive power. Beyond physics, these properties are valuable in general datascience. This motivates the use of quantum models to analyze generalnonphysical datasets. Here we provide both empirical and theoretical insightsinto the application of quantum models in data science. In the theoretical partof this paper, we firstly show that quantum models can be exponentially moreefficient than probabilistic models because there exist datasets that admitlow-dimensional quantum models and only exponentially high-dimensionalprobabilistic models. Secondly, we explain in what sense quantum models realizea useful relaxation of compressed probabilistic models. Thirdly, we show thatsparse datasets admit low-dimensional quantum models and finally, we introducea method to compute hierarchical orderings of properties of users (e.g.,personality traits) and items (e.g., genres of movies). In the empirical partof the paper, we evaluate quantum models in item recommendation and observethat the predictive power of quantum-inspired recommender systems can competewith state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,we make use of the interpretability of quantum models by computing hierarchicalorderings of properties of users and items. This work establishes a connectionbetween data science (item recommendation), information theory (communicationcomplexity), mathematical programming (positive semidefinite factorizations)and physics (quantum models).
arxiv-15900-296 | Learning Support Correlation Filters for Visual Tracking | http://arxiv.org/pdf/1601.06032v1.pdf | author:Wangmeng Zuo, Xiaohe Wu, Liang Lin, Lei Zhang, Ming-Hsuan Yang category:cs.CV published:2016-01-22 summary:Sampling and budgeting training examples are two essential factors intracking algorithms based on support vector machines (SVMs) as a trade-offbetween accuracy and efficiency. Recently, the circulant matrix formed by densesampling of translated image patches has been utilized in correlation filtersfor fast tracking. In this paper, we derive an equivalent formulation of a SVMmodel with circulant matrix expression and present an efficient alternatingoptimization method for visual tracking. We incorporate the discrete Fouriertransform with the proposed alternating optimization process, and pose thetracking problem as an iterative learning of support correlation filters (SCFs)which find the global optimal solution with real-time performance. For a givencirculant data matrix with n^2 samples of size n*n, the computationalcomplexity of the proposed algorithm is O(n^2*logn) whereas that of thestandard SVM-based approaches is at least O(n^4). In addition, we extend theSCF-based tracking algorithm with multi-channel features, kernel functions, andscale-adaptive approaches to further improve the tracking performance.Experimental results on a large benchmark dataset show that the proposedSCF-based algorithms perform favorably against the state-of-the-art trackingmethods in terms of accuracy and speed.
arxiv-15900-297 | A Robust Frame-based Nonlinear Prediction System for Automatic Speech Coding | http://arxiv.org/pdf/1601.06008v1.pdf | author:Mahmood Yousefi-Azar, Farbod Razzazi category:cs.SD cs.NE published:2016-01-22 summary:In this paper, we propose a neural-based coding scheme in which an artificialneural network is exploited to automatically compress and decompress speechsignals by a trainable approach. Having a two-stage training phase, the systemcan be fully specified to each speech frame and have robust performance acrossdifferent speakers and wide range of spoken utterances. Indeed, Frame-basednonlinear predictive coding (FNPC) would code a frame in the procedure oftraining to predict the frame samples. The motivating objective is to analyzethe system behavior in regenerating not only the envelope of spectra, but alsothe spectra phase. This scheme has been evaluated in time and discrete cosinetransform (DCT) domains and the output of predicted phonemes show thepotentiality of the FNPC to reconstruct complicated signals. The experimentswere conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domainsversus the number of neurons in the hidden layer. Experiments approve the FNPCcapability as an automatic coding system by which /b/d/g/ phonemes have beenreproduced with a good accuracy. Evaluations revealed that the performance ofFNPC system, trained to predict DCT coefficients is more desirable,particularly for frames with the wider distribution of energy, compared to timesamples.
arxiv-15900-298 | Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA | http://arxiv.org/pdf/1601.05768v1.pdf | author:Daniel Christen category:cs.CL published:2016-01-21 summary:This paper discusses SYNTAGMA, a rule based NLP system addressing the trickyissues of syntactic ambiguity reduction and word sense disambiguation as wellas providing innovative and original solutions for constituent generation andconstraints management. To provide an insight into how it operates, thesystem's general architecture and components, as well as its lexical, syntacticand semantic resources are described. After that, the paper addresses themechanism that performs selective parsing through an interaction betweensyntactic and semantic information, leading the parser to a coherent andaccurate interpretation of the input text.
arxiv-15900-299 | Data-driven Rank Breaking for Efficient Rank Aggregation | http://arxiv.org/pdf/1601.05495v1.pdf | author:Ashish Khetan, Sewoong Oh category:cs.LG stat.ML published:2016-01-21 summary:Rank aggregation systems collect ordinal preferences from individuals toproduce a global ranking that represents the social preference. Rank-breakingis a common practice to reduce the computational complexity of learning theglobal ranking. The individual preferences are broken into pairwise comparisonsand applied to efficient algorithms tailored for independent pairedcomparisons. However, due to the ignored dependencies in the data, naiverank-breaking approaches can result in inconsistent estimates. The key idea toproduce accurate and unbiased estimates is to treat the pairwise comparisonsunequally, depending on the topology of the collected data. In this paper, weprovide the optimal rank-breaking estimator, which not only achievesconsistency but also achieves the best error bound. This allows us tocharacterize the fundamental tradeoff between accuracy and complexity. Further,the analysis identifies how the accuracy depends on the spectral gap of acorresponding comparison graph.
arxiv-15900-300 | Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning | http://arxiv.org/pdf/1601.05675v1.pdf | author:Daniele Calandriello, Alessandro Lazaric, Michal Valko, Ioannis Koutis category:stat.ML cs.LG published:2016-01-21 summary:While the harmonic function solution performs well in many semi-supervisedlearning (SSL) tasks, it is known to scale poorly with the number of samples.Recent successful and scalable methods, such as the eigenfunction method focuson efficiently approximating the whole spectrum of the graph Laplacianconstructed from the data. This is in contrast to various subsampling andquantization methods proposed in the past, which may fail in preserving thegraph spectra. However, the impact of the approximation of the spectrum on thefinal generalization error is either unknown, or requires strong assumptions onthe data. In this paper, we introduce Sparse-HFS, an efficientedge-sparsification algorithm for SSL. By constructing an edge-sparse andspectrally similar graph, we are able to leverage the approximation guaranteesof spectral sparsification methods to bound the generalization error ofSparse-HFS. As a result, we obtain a theoretically-grounded approximationscheme for graph-based SSL that also empirically matches the performance ofknown large-scale methods.
