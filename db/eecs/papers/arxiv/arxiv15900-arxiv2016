arxiv-15900-1 | Iterative Hough Forest with Histogram of Control Points for 6 DoF Object Registration from Depth Images | http://arxiv.org/pdf/1603.02617v1.pdf | author:Caner Sahin, Rigas Kouskouridas, Tae-Kyun Kim category:cs.CV cs.RO published:2016-03-08 summary:State-of-the-art techniques for 6D object pose recovery depend onocclusion-free point clouds to accurately register objects in the 3D space. Toreduce this dependency, we introduce a novel architecture called IterativeHough forest with Histogram of Control Points that is capable of estimatingoccluded and cluttered objects' 6D pose given a candidate 2D bounding box. Ouriterative Hough forest is learnt using patches extracted only from the positivesamples. These patches are represented with Histogram of Control Points (HoCP),a scale-variant implicit volumetric description, which we derive from recentlyintroduced Implicit B-Splines (IBS). The rich discriminative informationprovided by this scale-variance is leveraged during inference, where theinitial pose estimation of the object is iteratively refined based on morediscriminative control points by using our iterative Hough forest. We conductexperiments on several test objects of a publicly available dataset to test ourarchitecture and to compare with the state-of-the-art.
arxiv-15900-2 | Observing Trends in Automated Multilingual Media Analysis | http://arxiv.org/pdf/1603.02604v1.pdf | author:Ralf Steinberger, Aldo Podavini, Alexandra Balahur, Guillaume Jacquet, Hristo Tanev, Jens Linge, Martin Atkinson, Michele Chinosi, Vanni Zavarella, Yaniv Steiner, Erik van der Goot category:cs.CL published:2016-03-08 summary:Any large organisation, be it public or private, monitors the media forinformation to keep abreast of developments in their field of interest, andusually also to become aware of positive or negative opinions expressed towardsthem. At least for the written media, computer programs have become veryefficient at helping the human analysts significantly in their monitoring taskby gathering media reports, analysing them, detecting trends and - in somecases - even to issue early warnings or to make predictions of likely futuredevelopments. We present here trend recognition-related functionality of theEurope Media Monitor (EMM) system, which was developed by the EuropeanCommission's Joint Research Centre (JRC) for public administrations in theEuropean Union (EU) and beyond. EMM performs large-scale media analysis in upto seventy languages and recognises various types of trends, some of themcombining information from news articles written in different languages andfrom social media posts. EMM also lets users explore the huge amount ofmultilingual media data through interactive maps and graphs, allowing them toexamine the data from various view points and according to multiple criteria. Alot of EMM's functionality is accessibly freely over the internet or via appsfor hand-held devices.
arxiv-15900-3 | Prediction of Infinite Words with Automata | http://arxiv.org/pdf/1603.02597v1.pdf | author:Tim Smith category:cs.FL cs.LG published:2016-03-08 summary:In the classic problem of sequence prediction, a predictor receives asequence of values from an emitter and tries to guess the next value before itappears. The predictor masters the emitter if there is a point after which allof the predictor's guesses are correct. In this paper we consider the case inwhich the predictor is an automaton and the emitted values are drawn from afinite set; i.e., the emitted sequence is an infinite word. We examine thepredictive capabilities of finite automata, pushdown automata, stack automata(a generalization of pushdown automata), and multihead finite automata. Werelate our predicting automata to purely periodic words, ultimately periodicwords, and multilinear words, describing novel prediction algorithms formastering these sequences.
arxiv-15900-4 | Batched Lazy Decision Trees | http://arxiv.org/pdf/1603.02578v1.pdf | author:Mathieu Guillame-Bert, Artur Dubrawski category:cs.LG published:2016-03-08 summary:We introduce a batched lazy algorithm for supervised classification usingdecision trees. It avoids unnecessary visits to irrelevant nodes when it isused to make predictions with either eagerly or lazily trained decision trees.A set of experiments demonstrate that the proposed algorithm can outperformboth the conventional and lazy decision tree algorithms in terms of computationtime as well as memory consumption, without compromising accuracy.
arxiv-15900-5 | Network Morphism | http://arxiv.org/pdf/1603.01670v2.pdf | author:Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen category:cs.LG cs.CV cs.NE published:2016-03-05 summary:We present in this paper a systematic study on how to morph a well-trainedneural network to a new one so that its network function can be completelypreserved. We define this as \emph{network morphism} in this research. Aftermorphing a parent network, the child network is expected to inherit theknowledge from its parent network and also has the potential to continuegrowing into a more powerful one with much shortened training time. The firstrequirement for this network morphism is its ability to handle diverse morphingtypes of networks, including changes of depth, width, kernel size, and evensubnet. To meet this requirement, we first introduce the network morphismequations, and then develop novel morphing algorithms for all these morphingtypes for both classic and convolutional neural networks. The secondrequirement for this network morphism is its ability to deal with non-linearityin a network. We propose a family of parametric-activation functions tofacilitate the morphing of any continuous non-linear activation neurons.Experimental results on benchmark datasets and typical neural networksdemonstrate the effectiveness of the proposed network morphism scheme.
arxiv-15900-6 | On the inconsistency of $\ell_1$-penalised sparse precision matrix estimation | http://arxiv.org/pdf/1603.02532v1.pdf | author:Otte Heinävaara, Janne Leppä-aho, Jukka Corander, Antti Honkela category:cs.LG stat.CO stat.ML published:2016-03-08 summary:Various $\ell_1$-penalised estimation methods such as graphical lasso andCLIME are widely used for sparse precision matrix estimation. Many of thesemethods have been shown to be consistent under various quantitative assumptionsabout the underlying true covariance matrix. Intuitively, these conditions arerelated to situations where the penalty term will dominate the optimisation. Inthis paper, we explore the consistency of $\ell_1$-based methods for a class ofsparse latent variable -like models, which are strongly motivated by severaltypes of applications. We show that all $\ell_1$-based methods faildramatically for models with nearly linear dependencies between the variables.We also study the consistency on models derived from real gene expression dataand note that the assumptions needed for consistency never hold even for modestsized gene networks and $\ell_1$-based methods also become unreliable inpractice for larger networks.
arxiv-15900-7 | A New Method to Visualize Deep Neural Networks | http://arxiv.org/pdf/1603.02518v1.pdf | author:Luisa M. Zintgraf, Taco S. Cohen, Max Welling category:cs.CV published:2016-03-08 summary:We present a method for visualising the response of a deep neural network toa specific input. For image data for instance our method will highlight areasthat provide evidence in favor of, and against choosing a certain class. Themethod overcomes several shortcomings of previous methods and provides greatadditional insight into the decision making process of convolutional networks,which is important both to improve models and to accelerate the adoption ofsuch methods in e.g. medicine. In experiments on ImageNet data, we illustratehow the method works and can be applied in different ways to understand deepneural nets.
arxiv-15900-8 | Semi-supervised Variational Autoencoders for Sequence Classification | http://arxiv.org/pdf/1603.02514v1.pdf | author:Weidi Xu, Haoze Sun category:cs.CL cs.LG published:2016-03-08 summary:Semi-supervised learning becomes one of the most significant problemsnowadays since the size of datasets is increasing tremendously while labeleddata is limited. We propose a new semi-supervised learning method for sequenceclassification tasks. Our work is based on both deep generative model forsemi-supervised learning \cite{kingma2014semi} and variational auto-encoder forsequence modeling \cite{bowman2015generating}. We found the introduction ofSc-LSTM is critical to the success in our method. We have obtained somepreliminary experimental results on IMDB sentiment classification dataset,showing that the proposed model improves the classification accuracy comparingto pure supervised classifier.
arxiv-15900-9 | Mixture Proportion Estimation via Kernel Embedding of Distributions | http://arxiv.org/pdf/1603.02501v1.pdf | author:Harish G. Ramaswamy, Clayton Scott, Ambuj Tewari category:cs.LG stat.ML published:2016-03-08 summary:Mixture proportion estimation (MPE) is the problem of estimating the weightof a component distribution in a mixture, given samples from the mixture andcomponent. This problem constitutes a key part in many "weakly supervisedlearning" problems like learning with positive and unlabelled samples, learningwith label noise, anomaly detection and crowdsourcing. While there have beenseveral methods proposed to solve this problem, to the best of our knowledge noefficient algorithm with a proven convergence rate towards the true proportionexists for this problem. We fill this gap by constructing a provably correctalgorithm for MPE, and derive convergence rates under certain assumptions onthe distribution. Our method is based on embedding distributions onto an RKHS,and implementing it only requires solving a simple convex quadratic programmingproblem a few times. We run our algorithm on several standard classificationdatasets, and demonstrate that it performs comparably to or better than otheralgorithms on most datasets.
arxiv-15900-10 | Generalized optimal sub-pattern assignment metric | http://arxiv.org/pdf/1601.05585v2.pdf | author:Abu Sajana Rahmathullah, Ángel F. García-Fernández, Lennart Svensson category:cs.SY cs.CV published:2016-01-21 summary:In this paper, we present the generalized optimal sub-pattern assignment(GOSPA) metric on the space of sets of targets. This metric is a generalizedversion of the unnormalized optimal sub-pattern assignment (OSPA) metric. Thedifference between unnormalized OSPA and GOSPA is that, in the proposed metric,we can choose a range of values for the cardinality mismatch penalty for agiven cut-off distance c. We argue that in multiple target tracking, we shouldselect the cardinality mismatch of GOSPA in a specific way, which is differentfrom OSPA. In this case, the metric can be viewed as sum of target localizationerror and error due to missed and false targets. We also extend the GOSPAmetric to the space of random finite sets, and show that both mean GOSPA androot mean squared GOSPA are metrics, which are useful for performanceevaluation.
arxiv-15900-11 | A Bayesian non-parametric method for clustering high-dimensional binary data | http://arxiv.org/pdf/1603.02494v1.pdf | author:Tapesh Santra category:stat.AP cs.LG stat.ML published:2016-03-08 summary:In many real life problems, objects are described by large number of binaryfeatures. For instance, documents are characterized by presence or absence ofcertain keywords; cancer patients are characterized by presence or absence ofcertain mutations etc. In such cases, grouping together similarobjects/profiles based on such high dimensional binary features is desirable,but challenging. Here, I present a Bayesian non parametric algorithm forclustering high dimensional binary data. It uses a Dirichlet Process (DP)mixture model and simulated annealing to not only cluster binary data, but alsofind optimal number of clusters in the data. The performance of the algorithmwas evaluated and compared with other algorithms using simulated datasets. Itoutperformed all other clustering methods that were tested in the simulationstudies. It was also used to cluster real datasets arising from documentanalysis, handwritten image analysis and cancer research. It successfullydivided a set of documents based on their topics, hand written images based ondifferent styles of writing digits and identified tissue and mutationspecificity of chemotherapy treatments.
arxiv-15900-12 | Extracting Arabic Relations from the Web | http://arxiv.org/pdf/1603.02488v1.pdf | author:Shimaa M. Abd El-salam, Enas M. F. El Houby, A. K. Al Sammak, T. A. El-Shishtawy category:cs.CL published:2016-03-08 summary:The goal of this research is to extract a large list or table from namedentities and relations in a specific domain. A small set of a handful ofinstance relations is required as input from the user. The system exploitssummaries from Google search engine as a source text. These instances are usedto extract patterns. The output is a set of new entities and their relations.The results from four experiments show that precision and recall variesaccording to relation type. Precision ranges from 0.61 to 0.75 while recallranges from 0.71 to 0.83. The best result is obtained for (player, club)relationship, 0.72 and 0.83 for precision and recall respectively.
arxiv-15900-13 | Encoding Prior Knowledge with Eigenword Embeddings | http://arxiv.org/pdf/1509.01007v2.pdf | author:Dominique Osborne, Shashi Narayan, Shay B. Cohen category:cs.CL published:2015-09-03 summary:Canonical correlation analysis (CCA) is a method for reducing the dimensionof data represented using two views. It has been previously used to derive wordembeddings, where one view indicates a word, and the other view indicates itscontext. We describe a way to incorporate prior knowledge into CCA, give atheoretical justification for it, and test it by deriving word embeddings andevaluating them on a myriad of datasets.
arxiv-15900-14 | A non-extensive entropy feature and its application to texture classification | http://arxiv.org/pdf/1603.02466v1.pdf | author:Seba Susan, Madasu Hanmandlu category:cs.CV published:2016-03-08 summary:This paper proposes a new probabilistic non-extensive entropy feature fortexture characterization, based on a Gaussian information measure. Thehighlights of the new entropy are that it is bounded by finite limits and thatit is non additive in nature. The non additive property of the proposed entropymakes it useful for the representation of information content in thenon-extensive systems containing some degree of regularity or correlation. Theeffectiveness of the proposed entropy in representing the correlated randomvariables is demonstrated by applying it for the texture classification problemsince textures found in nature are random and at the same time contain somedegree of correlation or regularity at some scale. The gray level co-occurrenceprobabilities (GLCP) are used for computing the entropy function. Theexperimental results indicate high degree of the classification accuracy. Theperformance of the new entropy function is found superior to other forms ofentropy such as Shannon, Renyi, Tsallis and Pal and Pal entropies oncomparison. Using the feature based polar interaction maps (FBIM) the proposedentropy is shown to be the best measure among the entropies compared forrepresenting the correlated textures.
arxiv-15900-15 | Doubly Decomposing Nonparametric Tensor Regression | http://arxiv.org/pdf/1506.05967v3.pdf | author:Masaaki Imaizumi, Kohei Hayashi category:stat.ML published:2015-06-19 summary:Nonparametric extension of tensor regression is proposed. Nonlinearity in ahigh-dimensional tensor space is broken into simple local functions byincorporating low-rank tensor decomposition. Compared to naive nonparametricapproaches, our formulation considerably improves the convergence rate ofestimation while maintaining consistency with the same function class underspecific conditions. To estimate local functions, we develop a Bayesianestimator with the Gaussian process prior. Experimental results show itstheoretical properties and high performance in terms of predicting a summarystatistic of a real complex network.
arxiv-15900-16 | A hybrid approach based segmentation technique for brain tumor in MRI Images | http://arxiv.org/pdf/1603.02447v1.pdf | author:D. Anithadevi, K. Perumal category:cs.CV published:2016-03-08 summary:Automatic image segmentation becomes very crucial for tumor detection inmedical image processing.In general, manual and semi automatic segmentationtechniques require more time and knowledge. However these drawbacks hadovercome by automatic segmentation still there needs to develop moreappropriate techniques for medical image segmentation. Therefore, we proposedhybrid approach based image segmentation using the combined features of regiongrowing and threshold based segmentation techniques. It is followed bypre-processing stage to provide an accurate brain tumor extraction by the helpof Magnetic Resonance Imaging (MRI). If the tumor has holes, the region growingsegmentation algorithm cannot reveal but the proposed hybrid segmentationtechnique can be achieved and the result as well improved. Hence the resultused to made assessment with the various performance measures as DICE, JACCARDsimilarity, accuracy, sensitivity and specificity. These similarity measureshave been extensively used for evaluation with the ground truth of eachprocessed image and its results are compared and analyzed.
arxiv-15900-17 | Effective Mean-Field Inference Method for Nonnegative Boltzmann Machines | http://arxiv.org/pdf/1603.02434v1.pdf | author:Muneki Yasuda category:stat.ML published:2016-03-08 summary:Nonnegative Boltzmann machines (NNBMs) are recurrent probabilistic neuralnetwork models that can describe multi-modal nonnegative data. NNBMs formrectified Gaussian distributions that appear in biological neural networkmodels, positive matrix factorization, nonnegative matrix factorization, and soon. In this paper, an effective inference method for NNBMs is proposed thatuses the mean-field method, referred to as the Thouless--Anderson--Palmerequation, and the diagonal consistency method, which was recently proposed.
arxiv-15900-18 | DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility | http://arxiv.org/pdf/1602.08191v2.pdf | author:Hanjoo Kim, Jaehong Park, Jaehee Jang, Sungroh Yoon category:cs.LG published:2016-02-26 summary:The increasing complexity of deep neural networks (DNNs) has made itchallenging to exploit existing large-scale data processing pipelines forhandling massive data and parameters involved in DNN training. Distributedcomputing platforms and GPGPU-based acceleration provide a mainstream solutionto this computational challenge. In this paper, we propose DeepSpark, adistributed and parallel deep learning framework that simultaneously exploitsApache Spark for large-scale distributed data management and Caffe forGPU-based acceleration. DeepSpark directly accepts Caffe input specifications,providing seamless compatibility with existing designs and network structures.To support parallel operations, DeepSpark automatically distributes workloadsand parameters to Caffe-running nodes using Spark and iteratively aggregatestraining results by a novel lock-free asynchronous variant of the popularelastic averaging stochastic gradient descent (SGD) update scheme, effectivelycomplementing the synchronized processing capabilities of Spark. DeepSpark isan on-going project, and the current release is available athttp://deepspark.snu.ac.kr.
arxiv-15900-19 | Stochastic dual averaging methods using variance reduction techniques for regularized empirical risk minimization problems | http://arxiv.org/pdf/1603.02412v1.pdf | author:Tomoya Murata, Taiji Suzuki category:math.OC cs.LG stat.ML published:2016-03-08 summary:We consider a composite convex minimization problem associated withregularized empirical risk minimization, which often arises in machinelearning. We propose two new stochastic gradient methods that are based onstochastic dual averaging method with variance reduction. Our methods generatea sparser solution than the existing methods because we do not need to take theaverage of the history of the solutions. This is favorable in terms of bothinterpretability and generalization. Moreover, our methods have theoreticalsupport for both a strongly and a non-strongly convex regularizer and achievethe best known convergence rates among existing nonaccelerated stochasticgradient methods.
arxiv-15900-20 | Frequency estimation in three-phase power systems with harmonic contamination: A multistage quaternion Kalman filtering approach | http://arxiv.org/pdf/1603.02977v1.pdf | author:Sayed Pouria Talebi, Danilo P. Mandic category:stat.ML stat.AP published:2016-03-08 summary:Motivated by the need for accurate frequency information, a novel algorithmfor estimating the fundamental frequency and its rate of change in three-phasepower systems is developed. This is achieved through two stages of Kalmanfiltering. In the first stage a quaternion extended Kalman filter, whichprovides a unified framework for joint modeling of voltage measurements fromall the phases, is used to estimate the instantaneous phase increment of thethree-phase voltages. The phase increment estimates are then used asobservations of the extended Kalman filter in the second stage that accountsfor the dynamic behavior of the system frequency and simultaneously estimatesthe fundamental frequency and its rate of change. The framework is thenextended to account for the presence of harmonics. Finally, the concept isvalidated through simulation on both synthetic and real-world data.
arxiv-15900-21 | Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games | http://arxiv.org/pdf/1503.03467v4.pdf | author:Houman Owhadi category:math.NA cs.AI math.ST stat.ML stat.TH published:2015-03-11 summary:We introduce a near-linear complexity (geometric and meshless/algebraic)multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficientswith rigorous a-priori accuracy and performance estimates. The method isdiscovered through a decision/game theory formulation of the problems of (1)identifying restriction and interpolation operators (2) recovering a signalfrom incomplete measurements based on norm constraints on its image under alinear operator (3) gambling on the value of the solution of the PDE based on ahierarchy of nested measurements of its solution or source term. The resultingelementary gambles form a hierarchy of (deterministic) basis functions of$H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbandswith respect to the scalar product induced by the energy norm of the PDE (2)enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) inducean orthogonal multiresolution operator decomposition. The operating diagram ofthe multigrid method is that of an inverted pyramid in which gamblets arecomputed locally (by virtue of their exponential decay), hierarchically (fromfine to coarse scales) and the PDE is decomposed into a hierarchy ofindependent linear systems with uniformly bounded condition numbers. Theresulting algorithm is parallelizable both in space (via localization) and inbandwith/subscale (subscales can be computed independently from each other).Although the method is deterministic it has a natural Bayesian interpretationunder the measure of probability emerging (as a mixed strategy) from theinformation game formulation and multiresolution approximations form amartingale with respect to the filtration induced by the hierarchy of nestedmeasurements.
arxiv-15900-22 | Formal Ontology Learning on Factual IS-A Corpus in English using Description Logics | http://arxiv.org/pdf/1312.6947v2.pdf | author:Sourish Dasgupta, Ankur Padia, Kushal Shah, Prasenjit Majumder category:cs.CL cs.AI published:2013-12-25 summary:Ontology Learning (OL) is the computational task of generating a knowledgebase in the form of an ontology given an unstructured corpus whose content isin natural language (NL). Several works can be found in this area most of whichare limited to statistical and lexico-syntactic pattern matching basedtechniques Light-Weight OL. These techniques do not lead to very accuratelearning mostly because of several linguistic nuances in NL. Formal OL is analternative (less explored) methodology were deep linguistics analysis is madeusing theory and tools found in computational linguistics to generate formalaxioms and definitions instead simply inducing a taxonomy. In this paper wepropose "Description Logic (DL)" based formal OL framework for learning factualIS-A type sentences in English. We claim that semantic construction of IS-Asentences is non trivial. Hence, we also claim that such sentences requiresspecial studies in the context of OL before any truly formal OL can beproposed. We introduce a learner tool, called DLOL_IS-A, that generated suchontologies in the owl format. We have adopted "Gold Standard" based OLevaluation on IS-A rich WCL v.1.1 dataset and our own Community representativeIS-A dataset. We observed significant improvement of DLOL_IS-A when compared tothe light-weight OL tool Text2Onto and formal OL tool FRED.
arxiv-15900-23 | A Scalable and Extensible Framework for Superposition-Structured Models | http://arxiv.org/pdf/1509.02314v2.pdf | author:Shenjian Zhao, Cong Xie, Zhihua Zhang category:cs.NA math.OC stat.ML published:2015-09-08 summary:In many learning tasks, structural models usually lead to betterinterpretability and higher generalization performance. In recent years,however, the simple structural models such as lasso are frequently proved to beinsufficient. Accordingly, there has been a lot of work on"superposition-structured" models where multiple structural constraints areimposed. To efficiently solve these "superposition-structured" statisticalmodels, we develop a framework based on a proximal Newton-type method.Employing the smoothed conic dual approach with the LBFGS updating formula, wepropose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.Empirical analysis on various datasets shows that our framework is potentiallypowerful, and achieves super-linear convergence rate for optimizing somepopular "superposition-structured" statistical models such as the fused sparsegroup lasso.
arxiv-15900-24 | Negatively Correlated Search | http://arxiv.org/pdf/1504.04914v2.pdf | author:Ke Tang, Peng Yang, Xin Yao category:cs.NE cs.AI published:2015-04-20 summary:Evolutionary Algorithms (EAs) have been shown to be powerful tools forcomplex optimization problems, which are ubiquitous in both communication andbig data analytics. This paper presents a new EA, namely Negatively CorrelatedSearch (NCS), which maintains multiple individual search processes in paralleland models the search behaviors of individual search processes as probabilitydistributions. NCS explicitly promotes negatively correlated search behaviorsby encouraging differences among the probability distributions (searchbehaviors). By this means, individual search processes share information andcooperate with each other to search diverse regions of a search space, whichmakes NCS a promising method for non-convex optimization. The cooperationscheme of NCS could also be regarded as a novel diversity preservation schemethat, different from other existing schemes, directly promotes diversity at thelevel of search behaviors rather than merely trying to maintain diversity amongcandidate solutions. Empirical studies showed that NCS is competitive towell-established search methods in the sense that NCS achieved the best overallperformance on 20 multimodal (non-convex) continuous optimization problems. Theadvantages of NCS over state-of-the-art approaches are also demonstrated with acase study on the synthesis of unequally spaced linear antenna arrays.
arxiv-15900-25 | Statistical physics of inference: Thresholds and algorithms | http://arxiv.org/pdf/1511.02476v3.pdf | author:Lenka Zdeborová, Florent Krzakala category:cs.DS stat.ML published:2015-11-08 summary:Many questions of fundamental interest in todays science can be formulated asinference problems: Some partial, or noisy, observations are performed over aset of variables and the goal is to recover, or infer, the values of thevariables based on the indirect information contained in the measurements. Forsuch problems, the central scientific questions are: Under what conditions isthe information contained in the measurements sufficient for a satisfactoryinference to be possible? What are the most efficient algorithms for this task?A growing body of work has shown that often we can understand and locate thesefundamental barriers by thinking of them as phase transitions in the sense ofstatistical physics. Moreover, it turned out that we can use the gainedphysical insight to develop new promising algorithms. Connection betweeninference and statistical physics is currently witnessing an impressiverenaissance and we review here the current state-of-the-art, with a pedagogicalfocus on the Ising model which formulated as an inference problem we call theplanted spin glass. In terms of applications we review two classes of problems:(i) inference of clusters on graphs and networks, with community detection as aspecial case and (ii) estimating a signal from its noisy linear measurements,with compressed sensing as a case of sparse estimation. Our goal is to providea pedagogical review for researchers in physics and other fields interested inthis fascinating topic.
arxiv-15900-26 | LSTM Neural Reordering Feature for Statistical Machine Translation | http://arxiv.org/pdf/1512.00177v2.pdf | author:Yiming Cui, Shijin Wang, Jianfeng Li, Yuguang Wang category:cs.CL cs.AI cs.NE published:2015-12-01 summary:Artificial neural networks are powerful models, which have been widelyapplied into many aspects of machine translation, such as language modeling andtranslation modeling. Though notable improvements have been made in theseareas, the reordering problem still remains a challenge in statistical machinetranslations. In this paper, we present a novel neural reordering model thatdirectly models word pairs and alignment. By utilizing LSTM recurrent neuralnetworks, much longer context could be learned for reordering prediction.Experimental results on NIST OpenMT12 Arabic-English and Chinese-English1000-best rescoring task show that our LSTM neural reordering feature is robustand achieves significant improvements over various baseline systems.
arxiv-15900-27 | Hand Segmentation for Hand-Object Interaction from Depth map | http://arxiv.org/pdf/1603.02345v1.pdf | author:Byeongkeun Kang, Kar-Han Tan, Hung-Shuo Tai, Daniel Tretter, Truong Q. Nguyen category:cs.CV published:2016-03-08 summary:Hand-object interaction is important for many applications such as augmentedreality, medical application, and human-robot interaction. Hand segmentation isa necessary pre-process to estimate hand pose and to recognize hand gesture orobject in interaction. However, current hand segmentation method forhand-object interaction is based on color information which is not robust toobjects with skin color, skin pigment difference, and light conditionvariations. Therefore, we propose the first hand segmentation method forhand-object interaction using depth map. This is challenging because of onlysmall depth difference between hand and object during interaction. The proposedmethod includes two-stage randomized decision forest (RDF) with validationprocess, bilateral filtering, decision adjustment, and post-processing. Wedemonstrate the effectiveness of the proposed method by testing for fiveobjects. The proposed method achieves the average F1 score of 0.8826 usingdifferent model for each object and 0.8645 using a global model for entireobjects. Also, the method takes only about 10ms to process each frame. Webelieve that this is the state-of-the-art hand segmentation algorithm usingdepth map for hand-object interaction.
arxiv-15900-28 | ReconNet: Non-Iterative Reconstruction of Images from Compressively Sensed Random Measurements | http://arxiv.org/pdf/1601.06892v2.pdf | author:Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, Amit Ashok category:cs.CV published:2016-01-26 summary:The goal of this paper is to present a non-iterative and more importantly anextremely fast algorithm to reconstruct images from compressively sensed (CS)random measurements. To this end, we propose a novel convolutional neuralnetwork (CNN) architecture which takes in CS measurements of an image as inputand outputs an intermediate reconstruction. We call this network, ReconNet. Theintermediate reconstruction is fed into an off-the-shelf denoiser to obtain thefinal reconstructed image. On a standard dataset of images we show significantimprovements in reconstruction results (both in terms of PSNR and timecomplexity) over state-of-the-art iterative CS reconstruction algorithms atvarious measurement rates. Further, through qualitative experiments on realdata collected using our block single pixel camera (SPC), we show that ournetwork is highly robust to sensor noise and can recover visually betterquality images than competitive algorithms at extremely low sensing rates of0.1 and 0.04. To demonstrate that our algorithm can recover semanticallyinformative images even at a low measurement rate of 0.01, we present a veryrobust proof of concept real-time visual tracking application.
arxiv-15900-29 | Coordinate Friendly Structures, Algorithms and Applications | http://arxiv.org/pdf/1601.00863v2.pdf | author:Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, Wotao Yin category:math.OC cs.CE cs.DC math.NA stat.ML published:2016-01-05 summary:This paper focuses on coordinate update methods, which are useful for solvingproblems involving large or high-dimensional datasets. They decompose a probleminto simple subproblems, where each updates one, or a small block of, variableswhile fixing others. These methods can deal with linear and nonlinear mappings,smooth and nonsmooth functions, as well as convex and nonconvex problems. Inaddition, they are easy to parallelize. The great performance of coordinate update methods depends on solving simplesubproblems. To derive simple subproblems for several new classes ofapplications, this paper systematically studies coordinate friendly operatorsthat perform low-cost coordinate updates. Based on the discovered coordinate friendly operators, as well as operatorsplitting techniques, we obtain new coordinate update algorithms for a varietyof problems in machine learning, image processing, as well as sub-areas ofoptimization. Several problems are treated with coordinate update for the firsttime in history. The obtained algorithms are scalable to large instancesthrough parallel and even asynchronous computing. We present numerical examplesto illustrate how effective these algorithms are.
arxiv-15900-30 | $k$-center Clustering under Perturbation Resilience | http://arxiv.org/pdf/1505.03924v3.pdf | author:Maria-Florina Balcan, Nika Haghtalab, Colin White category:cs.DS cs.LG published:2015-05-14 summary:The $k$-center problem is a canonical and long-studied facility location andclustering problem with many applications in both its symmetric and asymmetricforms. Both versions of the problem have tight approximation factors on worstcase instances: a $2$-approximation for symmetric $k$-center and an$O(\log^*(k))$-approximation for the asymmetric version. In this work, we go beyond the worst case and provide strong positive resultsboth for the asymmetric and symmetric $k$-center problems under a very naturalinput stability (promise) condition called $\alpha$-perturbation resilience(Bilu & Linial 2012) , which states that the optimal solution does not changeunder any $\alpha$-factor perturbation to the input distances. We show that byassuming 2-perturbation resilience, the exact solution for the asymmetric$k$-center problem can be found in polynomial time. To our knowledge, this isthe first problem that is hard to approximate to any constant factor in theworst case, yet can be optimally solved in polynomial time under perturbationresilience for a constant value of $\alpha$. Furthermore, we prove our resultis tight by showing symmetric $k$-center under $(2-\epsilon)$-perturbationresilience is hard unless $NP=RP$. This is the first tight result for anyproblem under perturbation resilience, i.e., this is the first time the exactvalue of $\alpha$ for which the problem switches from being NP-hard toefficiently computable has been found. Our results illustrate a surprising relationship between symmetric andasymmetric $k$-center instances under perturbation resilience. Unlikeapproximation ratio, for which symmetric $k$-center is easily solved to afactor of $2$ but asymmetric $k$-center cannot be approximated to any constantfactor, both symmetric and asymmetric $k$-center can be solved optimally underresilience to 2-perturbations.
arxiv-15900-31 | Blur Robust Optical Flow using Motion Channel | http://arxiv.org/pdf/1603.02253v1.pdf | author:Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, Darren Cosker category:cs.CV published:2016-03-07 summary:It is hard to estimate optical flow given a realworld video sequence withcamera shake and other motion blur. In this paper, we first investigate theblur parameterization for video footage using near linear motion elements. wethen combine a commercial 3D pose sensor with an RGB camera, in order to filmvideo footage of interest together with the camera motion. We illustrates thatthis additional camera motion/trajectory channel can be embedded into a hybridframework by interleaving an iterative blind deconvolution and warping basedoptical flow scheme. Our method yields improved accuracy within three otherstate-of-the-art baselines given our proposed ground truth blurry sequences;and several other realworld sequences filmed by our imaging system.
arxiv-15900-32 | Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences | http://arxiv.org/pdf/1603.02252v1.pdf | author:Wenbin Li, Darren Cosker, Matthew Brown category:cs.CV published:2016-03-07 summary:It is hard to densely track a nonrigid object in long term, which is afundamental research issue in the computer vision community. This task oftenrelies on estimating pairwise correspondences between images over time wherethe error is accumulated and leads to a drift issue. In this paper, weintroduce a novel optimization framework with an Anchor Patch constraint. It issupposed to significantly reduce overall errors given long sequences containingnon-rigidly deformable objects. Our framework can be applied to any densetracking algorithm, e.g. optical flow. We demonstrate the success of ourapproach by showing significant error reduction on 6 popular optical flowalgorithms applied to a range of real-world nonrigid benchmarks. We alsoprovide quantitative analysis of our approach given synthetic occlusions andimage noise.
arxiv-15900-33 | Online Sparse Linear Regression | http://arxiv.org/pdf/1603.02250v1.pdf | author:Dean Foster, Satyen Kale, Howard Karloff category:cs.LG published:2016-03-07 summary:We consider the online sparse linear regression problem, which is the problemof sequentially making predictions observing only a limited number of featuresin each round, to minimize regret with respect to the best sparse linearregressor, where prediction accuracy is measured by square loss. We give aninefficient algorithm that obtains regret bounded by $\tilde{O}(\sqrt{T})$after $T$ prediction rounds. We complement this result by showing that noalgorithm running in polynomial time per iteration can achieve regret boundedby $O(T^{1-\delta})$ for any constant $\delta > 0$ unless $\text{NP} \subseteq\text{BPP}$. This computational hardness result resolves an open problempresented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013).This hardness result holds even if the algorithm is allowed to access morefeatures than the best sparse linear regressor up to a logarithmic factor inthe dimension.
arxiv-15900-34 | Hierarchical Models as Marginals of Hierarchical Models | http://arxiv.org/pdf/1508.03606v2.pdf | author:Guido Montufar, Johannes Rauh category:math.PR cs.LG cs.NE math.ST stat.TH published:2015-08-14 summary:We investigate the representation of hierarchical models in terms ofmarginals of other hierarchical models with smaller interactions. We focus onbinary variables and marginals of pairwise interaction models whose hiddenvariables are conditionally independent given the visible variables. In thiscase the problem is equivalent to the representation of linear subspaces ofpolynomials by feedforward neural networks with soft-plus computational units.We show that every hidden variable can freely model multiple interactions amongthe visible variables, which allows us to generalize and improve previousresults. In particular, we show that a restricted Boltzmann machine with lessthan $[ 2(\log(v)+1) / (v+1) ] 2^v-1$ hidden binary variables can approximateevery distribution of $v$ visible binary variables arbitrarily well, comparedto $2^{v-1}-1$ from the best previously known result.
arxiv-15900-35 | Authenticating users through their arm movement patterns | http://arxiv.org/pdf/1603.02211v1.pdf | author:Rajesh Kumar, Vir V Phoha, Rahul Raina category:cs.CV cs.CR K.6.5 published:2016-03-07 summary:In this paper, we propose four continuous authentication designs by using thecharacteristics of arm movements while individuals walk. The first design usesacceleration of arms captured by a smartwatch's accelerometer sensor, thesecond design uses the rotation of arms captured by a smartwatch's gyroscopesensor, third uses the fusion of both acceleration and rotation at thefeature-level and fourth uses the fusion at score-level. Each of these designsis implemented by using four classifiers, namely, k nearest neighbors (k-NN)with Euclidean distance, Logistic Regression, Multilayer Perceptrons, andRandom Forest resulting in a total of sixteen authentication mechanisms. Theseauthentication mechanisms are tested under three different environments, namelyan intra-session, inter-session on a dataset of 40 users and an inter-phase ona dataset of 12 users. The sessions of data collection were separated by atleast ten minutes, whereas the phases of data collection were separated by atleast three months. Under the intra-session environment, all of the twelveauthentication mechanisms achieve a mean dynamic false accept rate (DFAR) of 0%and dynamic false reject rate (DFRR) of 0%. For the inter-session environment,feature level fusion-based design with classifier k-NN achieves the best errorrates that are a mean DFAR of 2.2% and DFRR of 4.2%. The DFAR and DFRRincreased from 5.68% and 4.23% to 15.03% and 14.62% respectively when featurelevel fusion-based design with classifier k-NN was tested under the inter-phaseenvironment on a dataset of 12 users.
arxiv-15900-36 | Elastic Functional Coding of Riemannian Trajectories | http://arxiv.org/pdf/1603.02200v1.pdf | author:Rushil Anirudh, Pavan Turaga, Jingyong Su, Anuj Srivastava category:cs.CV math.DG published:2016-03-07 summary:Visual observations of dynamic phenomena, such as human actions, are oftenrepresented as sequences of smoothly-varying features . In cases where thefeature spaces can be structured as Riemannian manifolds, the correspondingrepresentations become trajectories on manifolds. Analysis of thesetrajectories is challenging due to non-linearity of underlying spaces andhigh-dimensionality of trajectories. In vision problems, given the nature ofphysical systems involved, these phenomena are better characterized on alow-dimensional manifold compared to the space of Riemannian trajectories. Forinstance, if one does not impose physical constraints of the human body, indata involving human action analysis, the resulting representation space willhave highly redundant features. Learning an effective, low-dimensionalembedding for action representations will have a huge impact in the areas ofsearch and retrieval, visualization, learning, and recognition. The difficultylies in inherent non-linearity of the domain and temporal variability ofactions that can distort any traditional metric between trajectories. Toovercome these issues, we use the framework based on transported square-rootvelocity fields (TSRVF); this framework has several desirable properties,including a rate-invariant metric and vector space representations. We proposeto learn an embedding such that each action trajectory is mapped to a singlepoint in a low-dimensional Euclidean space, and the trajectories that differonly in temporal rates map to the same point. We utilize the TSRVFrepresentation, and accompanying statistical summaries of Riemanniantrajectories, to extend existing coding methods such as PCA, KSVD and LabelConsistent KSVD to Riemannian trajectories or more generally to Riemannianfunctions.
arxiv-15900-37 | Gaussian Process Regression for Out-of-Sample Extension | http://arxiv.org/pdf/1603.02194v1.pdf | author:Oren Barkan, Jonathan Weill, Amir Averbuch category:cs.LG cs.CV published:2016-03-07 summary:Manifold learning methods are useful for high dimensional data analysis. Manyof the existing methods produce a low dimensional representation that attemptsto describe the intrinsic geometric structure of the original data. Typically,this process is computationally expensive and the produced embedding is limitedto the training data. In many real life scenarios, the ability to produceembedding of unseen samples is essential. In this paper we propose a Bayesiannon-parametric approach for out-of-sample extension. The method is based onGaussian Process Regression and independent of the manifold learning algorithm.Additionally, the method naturally provides a measure for the degree ofabnormality for a newly arrived data point that did not participate in thetraining process. We derive the mathematical connection between the proposedmethod and the Nystrom extension and show that the latter is a special case ofthe former. We present extensive experimental results that demonstrate theperformance of the proposed method and compare it to other existingout-of-sample extension methods.
arxiv-15900-38 | A Bottom-up Approach for Pancreas Segmentation using Cascaded Superpixels and (Deep) Image Patch Labeling | http://arxiv.org/pdf/1505.06236v2.pdf | author:Amal Farag, Le Lu, Holger R. Roth, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV published:2015-05-22 summary:Robust automated organ segmentation is a prerequisite for computer-aideddiagnosis (CAD), quantitative imaging analysis and surgical assistance. Forhigh-variability organs such as the pancreas, previous approaches reportundesirably low accuracies. We present a bottom-up approach for pancreassegmentation in abdominal CT scans that is based on a hierarchy of informationpropagation by classifying image patches at different resolutions; andcascading superpixels. There are four stages: 1) decomposing CT slice images asa set of disjoint boundary-preserving superpixels; 2) computing pancreas classprobability maps via dense patch labeling; 3) classifying superpixels bypooling both intensity and probability features to form empirical statistics incascaded random forest frameworks; and 4) simple connectivity basedpost-processing. The dense image patch labeling are conducted by: efficientrandom forest classifier on image histogram, location and texture features; andmore expensive (but with better specificity) deep convolutional neural networkclassification on larger image windows (with more spatial contexts). Evaluationof the approach is performed on a database of 80 manually segmented CT volumesin six-fold cross-validation (CV). Our achieved results are comparable, orbetter than the state-of-the-art methods (evaluated by"leave-one-patient-out"), with Dice 70.7% and Jaccard 57.9%. The computationalefficiency has been drastically improved in the order of 6~8 minutes, comparingwith others of ~10 hours per case. Finally, we implement a multi-atlas labelfusion (MALF) approach for pancreas segmentation using the same datasets. Undersix-fold CV, our bottom-up segmentation method significantly outperforms itsMALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNNpatch labeling confidences offer more numerical stability, reflected by smallerstandard deviations.
arxiv-15900-39 | Distributed Multi-Task Learning with Shared Representation | http://arxiv.org/pdf/1603.02185v1.pdf | author:Jialei Wang, Mladen Kolar, Nathan Srebro category:cs.LG stat.ML published:2016-03-07 summary:We study the problem of distributed multi-task learning with sharedrepresentation, where each machine aims to learn a separate, but related, taskin an unknown shared low-dimensional subspaces, i.e. when the predictor matrixhas low rank. We consider a setting where each task is handled by a differentmachine, with samples for the task available locally on the machine, and studycommunication-efficient methods for exploiting the shared structure.
arxiv-15900-40 | Capturing Hands in Action using Discriminative Salient Points and Physics Simulation | http://arxiv.org/pdf/1506.02178v4.pdf | author:Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, Juergen Gall category:cs.CV published:2015-06-06 summary:Hand motion capture is a popular research field, recently gaining moreattention due to the ubiquity of RGB-D sensors. However, even most recentapproaches focus on the case of a single isolated hand. In this work, we focuson hands that interact with other hands or objects and present a framework thatsuccessfully captures motion in such interaction scenarios for both rigid andarticulated objects. Our framework combines a generative model withdiscriminatively trained salient points to achieve a low tracking error andwith collision detection and physics simulation to achieve physically plausibleestimates even in case of occlusions and missing visual data. Since allcomponents are unified in a single objective function which is almosteverywhere differentiable, it can be optimized with standard optimizationtechniques. Our approach works for monocular RGB-D sequences as well as setupswith multiple synchronized RGB cameras. For a qualitative and quantitativeevaluation, we captured 29 sequences with a large variety of interactions andup to 150 degrees of freedom.
arxiv-15900-41 | Bayesian Learning of Kernel Embeddings | http://arxiv.org/pdf/1603.02160v1.pdf | author:Seth Flaxman, Dino Sejdinovic, John P. Cunningham, Sarah Filippi category:stat.ML published:2016-03-07 summary:Kernel methods are one of the mainstays of machine learning, but the problemof kernel learning remains challenging, with only a few heuristics and verylittle theory. This is of particular importance in methods based on estimationof kernel mean embeddings of probability measures. For characteristic kernels,which include most commonly used ones, the kernel mean embedding uniquelydetermines its probability measure, so it can be used to design a powerfulstatistical testing framework, which includes nonparametric two-sample andindependence tests. In practice, however, the performance of these tests can bevery sensitive to the choice of kernel and its lengthscale parameters. Toaddress this central issue, we propose a new probabilistic model for kernelmean embeddings, the Bayesian Kernel Embedding model, combining a Gaussianprocess prior over the Reproducing Kernel Hilbert Space containing the meanembedding with a conjugate likelihood function, thus yielding a closed formposterior over the mean embedding. The posterior mean of our model is closelyrelated to recently proposed shrinkage estimators for kernel mean embeddings,while the posterior uncertainty is a new, interesting feature with variouspossible applications. Critically for the purposes of effective and principledkernel learning, our model gives a simple, closed form marginal likelihood ofthe observed data given the kernel hyperparameters. This marginal likelihoodcan either be optimized to inform the hyperparameter choice or fully Bayesianinference can be used.
arxiv-15900-42 | Communicating Semantics: Reference by Description | http://arxiv.org/pdf/1511.06341v4.pdf | author:Ramanathan V Guha, Vineet Gupta category:cs.CL published:2015-11-19 summary:Messages often refer to entities such as people, places and events. Correctidentification of the intended reference is an essential part of communication.Lack of shared unique names often complicates entity reference. Sharedknowledge can be used to construct uniquely identifying descriptive referencesfor entities with ambiguous names. We introduce a mathematical model for`Reference by Description', derive results on the conditions under which, withhigh probability, programs can construct unambiguous references to mostentities in the domain of discourse and provide empirical validation of theseresults.
arxiv-15900-43 | Learning a Discriminative Null Space for Person Re-identification | http://arxiv.org/pdf/1603.02139v1.pdf | author:Li Zhang, Tao Xiang, Shaogang Gong category:cs.CV published:2016-03-07 summary:Most existing person re-identification (re-id) methods focus on learning theoptimal distance metrics across camera views. Typically a person's appearanceis represented using features of thousands of dimensions, whilst only hundredsof training samples are available due to the difficulties in collecting matchedtraining images. With the number of training samples much smaller than thefeature dimension, the existing methods thus face the classic small sample size(SSS) problem and have to resort to dimensionality reduction techniques and/ormatrix regularisation, which lead to loss of discriminative power. In thiswork, we propose to overcome the SSS problem in re-id distance metric learningby matching people in a discriminative null space of the training data. In thisnull space, images of the same person are collapsed into a single point thusminimising the within-class scatter to the extreme and maximising the relativebetween-class separation simultaneously. Importantly, it has a fixed dimension,a closed-form solution and is very efficient to compute. Extensive experimentscarried out on five person re-identification benchmarks including VIPeR,PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beatsthe state-of-the-art alternatives, often by a big margin.
arxiv-15900-44 | Graph Regularized Low Rank Representation for Aerosol Optical Depth Retrieval | http://arxiv.org/pdf/1602.06818v2.pdf | author:Yubao Sun, Renlong Hang, Qingshan Liu, Fuping Zhu, Hucheng Pei category:cs.LG published:2016-02-22 summary:In this paper, we propose a novel data-driven regression model for aerosoloptical depth (AOD) retrieval. First, we adopt a low rank representation (LRR)model to learn a powerful representation of the spectral response. Then, graphregularization is incorporated into the LRR model to capture the localstructure information and the nonlinear property of the remote-sensing data.Since it is easy to acquire the rich satellite-retrieval results, we use themas a baseline to construct the graph. Finally, the learned featurerepresentation is feeded into support vector machine (SVM) to retrieve AOD.Experiments are conducted on two widely used data sets acquired by differentsensors, and the experimental results show that the proposed method can achievesuperior performance compared to the physical models and other state-of-the-artempirical models.
arxiv-15900-45 | Freshman or Fresher? Quantifying the Geographic Variation of Internet Language | http://arxiv.org/pdf/1510.06786v2.pdf | author:Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL cs.IR cs.LG published:2015-10-22 summary:We present a new computational technique to detect and analyze statisticallysignificant geographic variation in language. Our meta-analysis approachcaptures statistical properties of word usage across geographical regions anduses statistical methods to identify significant changes specific to regions.While previous approaches have primarily focused on lexical variation betweenregions, our method identifies words that demonstrate semantic and syntacticvariation as well. We extend recently developed techniques for neural language models to learnword representations which capture differing semantics across geographicalregions. In order to quantify this variation and ensure robust detection oftrue regional differences, we formulate a null model to determine whetherobserved changes are statistically significant. Our method is the first suchapproach to explicitly account for random variation due to chance whiledetecting regional variation in word meaning. To validate our model, we study and analyze two different massive online datasets: millions of tweets from Twitter spanning not only four differentcountries but also fifty states, as well as millions of phrases contained inthe Google Book Ngrams. Our analysis reveals interesting facets of languagechange at multiple scales of geographic resolution -- from neighboring statesto distant continents. Finally, using our model, we propose a measure of semantic distance betweenlanguages. Our analysis of British and American English over a period of 100years reveals that semantic variation between these dialects is shrinking.
arxiv-15900-46 | Visual object tracking performance measures revisited | http://arxiv.org/pdf/1502.05803v3.pdf | author:Luka Čehovin, Aleš Leonardis, Matej Kristan category:cs.CV published:2015-02-20 summary:The problem of visual tracking evaluation is sporting a large variety ofperformance measures, and largely suffers from lack of consensus about whichmeasures should be used in experiments. This makes the cross-paper trackercomparison difficult. Furthermore, as some measures may be less effective thanothers, the tracking results may be skewed or biased towards particulartracking aspects. In this paper we revisit the popular performance measures andtracker performance visualizations and analyze them theoretically andexperimentally. We show that several measures are equivalent from the point ofinformation they provide for tracker comparison and, crucially, that some aremore brittle than the others. Based on our analysis we narrow down the set ofpotential measures to only two complementary ones, describing accuracy androbustness, thus pushing towards homogenization of the tracker evaluationmethodology. These two measures can be intuitively interpreted and visualizedand have been employed by the recent Visual Object Tracking (VOT) challenges asthe foundation for the evaluation methodology.
arxiv-15900-47 | A Learning-based Frame Pooling Model For Event Detection | http://arxiv.org/pdf/1603.02078v1.pdf | author:Jiang Liu, Chenqiang Gao, Lan Wang, Deyu Meng category:cs.CV published:2016-03-07 summary:Detecting complex events in a large video collection crawled from videowebsites is a challenging task. When applying directly good image-based featurerepresentation, e.g., HOG, SIFT, to videos, we have to face the problem of howto pool multiple frame feature representations into one feature representation.In this paper, we propose a novel learning-based frame pooling method. Weformulate the pooling weight learning as an optimization problem and thus ourmethod can automatically learn the best pooling weight configuration for eachspecific event category. Experimental results conducted on TRECVID MED 2011reveal that our method outperforms the commonly used average pooling and maxpooling strategies on both high-level and low-level 2D image features.
arxiv-15900-48 | Learning Deep Generative Models with Doubly Stochastic MCMC | http://arxiv.org/pdf/1506.04557v4.pdf | author:Chao Du, Jun Zhu, Bo Zhang category:cs.LG published:2015-06-15 summary:We present doubly stochastic gradient MCMC, a simple and generic method for(approximate) Bayesian inference of deep generative models (DGMs) in acollapsed continuous parameter space. At each MCMC sampling step, the algorithmrandomly draws a mini-batch of data samples to estimate the gradient oflog-posterior and further estimates the intractable expectation over hiddenvariables via a neural adaptive importance sampler, where the proposaldistribution is parameterized by a deep neural network and learnt jointly. Wedemonstrate the effectiveness on learning various DGMs in a wide range oftasks, including density estimation, data generation and missing dataimputation. Our method outperforms many state-of-the-art competitors.
arxiv-15900-49 | Optimal dictionary for least squares representation | http://arxiv.org/pdf/1603.02074v1.pdf | author:Mohammed Rayyan Sheriff, Debasish Chatterjee category:cs.LG math.OC stat.ML published:2016-03-07 summary:Dictionary Learning problems are concerned with finding a collection ofvectors usually referred to as the dictionary, such that the representation ofrandom vectors with a given distribution using this dictionary is optimal. Mostof the recent research in dictionary learning is focused on developingdictionaries which offer sparse representation, i.e., optimal representation inthe $\ell_0$ sense. We consider the problem of finding an optimal dictionarywith which representation of samples of a random vector on an average isoptimal. Optimality of representation is defined in the sense of attainingminimal average $\ell_2$-norm of the coefficient vector used to represent therandom vector. With the help of recent results related to rank-onedecompositions of real symmetric positive semi-definite matrices, an explicitsolution for an $\ell_2$-optimal dictionary is obtained.
arxiv-15900-50 | Learning Shared Representations in Multi-task Reinforcement Learning | http://arxiv.org/pdf/1603.02041v1.pdf | author:Diana Borsa, Thore Graepel, John Shawe-Taylor category:cs.AI cs.LG published:2016-03-07 summary:We investigate a paradigm in multi-task reinforcement learning (MT-RL) inwhich an agent is placed in an environment and needs to learn to perform aseries of tasks, within this space. Since the environment does not change,there is potentially a lot of common ground amongst tasks and learning to solvethem individually seems extremely wasteful. In this paper, we explicitly modeland learn this shared structure as it arises in the state-action value space.We will show how one can jointly learn optimal value-functions by modifying thepopular Value-Iteration and Policy-Iteration procedures to accommodate thisshared representation assumption and leverage the power of multi-tasksupervised learning. Finally, we demonstrate that the proposed model andtraining procedures, are able to infer good value functions, even under lowsamples regimes. In addition to data efficiency, we will show in our analysis,that learning abstractions of the state space jointly across tasks leads tomore robust, transferable representations with the potential for bettergeneralization. this shared representation assumption and leverage the power ofmulti-task supervised learning. Finally, we demonstrate that the proposed modeland training procedures, are able to infer good value functions, even under lowsamples regimes. In addition to data efficiency, we will show in our analysis,that learning abstractions of the state space jointly across tasks leads tomore robust, transferable representations with the potential for bettergeneralization.
arxiv-15900-51 | Unscented Bayesian Optimization for Safe Robot Grasping | http://arxiv.org/pdf/1603.02038v1.pdf | author:José Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino, Lorenzo Jamone category:cs.RO cs.AI cs.LG cs.SY published:2016-03-07 summary:We address the robot grasp optimization problem of unknown objectsconsidering uncertainty in the input space. Grasping unknown objects can beachieved by using a trial and error exploration strategy. Bayesian optimizationis a sample efficient optimization algorithm that is especially suitable forthis setups as it actively reduces the number of trials for learning about thefunction to optimize. In fact, this active object exploration is the samestrategy that infants do to learn optimal grasps. One problem that arises whilelearning grasping policies is that some configurations of grasp parameters maybe very sensitive to error in the relative pose between the object and robotend-effector. We call these configurations unsafe because small errors duringgrasp execution may turn good grasps into bad grasps. Therefore, to reduce therisk of grasp failure, grasps should be planned in safe areas. We propose a newalgorithm, Unscented Bayesian optimization that is able to perform sampleefficient optimization while taking into consideration input noise to find safeoptima. The contribution of Unscented Bayesian optimization is twofold as ifprovides a new decision process that drives exploration to safe regions and anew selection procedure that chooses the optimal in terms of its safety withoutextra analysis or computational cost. Both contributions are rooted on thestrong theory behind the unscented transformation, a popular nonlinearapproximation method. We show its advantages with respect to the classicalBayesian optimization both in synthetic problems and in realistic robot graspsimulations. The results highlights that our method achieves optimal and robustgrasping policies after few trials while the selected grasps remain in saferegions.
arxiv-15900-52 | Adaptive Visualisation System for Construction Building Information Models Using Saliency | http://arxiv.org/pdf/1603.02028v1.pdf | author:Hugo Martin, Sylvain Chevallier, Eric Monacelli category:cs.CV cs.AI published:2016-03-07 summary:Building Information Modeling (BIM) is a recent construction process based ona 3D model, containing every component related to the building achievement.Architects, structure engineers, method engineers, and others participant tothe building process work on this model through the design-to-constructioncycle. The high complexity and the large amount of information included inthese models raise several issues, delaying its wide adoption in the industrialworld. One of the most important is the visualization: professionals havedifficulties to find out the relevant information for their job. Actualsolutions suffer from two limitations: the BIM models information are processedmanually and insignificant information are simply hidden, leading toinconsistencies in the building model. This paper describes a system relying onan ontological representation of the building information to labelautomatically the building elements. Depending on the user's department, thevisualization is modified according to these labels by automatically adjustingthe colors and image properties based on a saliency model. The proposedsaliency model incorporates several adaptations to fit the specificities ofarchitectural images.
arxiv-15900-53 | Differentially Private Policy Evaluation | http://arxiv.org/pdf/1603.02010v1.pdf | author:Borja Balle, Maziar Gomrokchi, Doina Precup category:cs.LG stat.ML published:2016-03-07 summary:We present the first differentially private algorithms for reinforcementlearning, which apply to the task of evaluating a fixed policy. We establishtwo approaches for achieving differential privacy, provide a theoreticalanalysis of the privacy and utility of the two algorithms, and show promisingresults on simple empirical examples.
arxiv-15900-54 | Position paper: Towards an observer-oriented theory of shape comparison | http://arxiv.org/pdf/1603.02008v1.pdf | author:Patrizio Frosini category:cs.CG cs.CV math.AT I.4.7; I.5.1 published:2016-03-07 summary:In this position paper we suggest a possible metric approach to shapecomparison that is based on a mathematical formalization of the concept ofobserver, seen as a collection of suitable operators acting on a metric spaceof functions. These functions represent the set of data that are accessible tothe observer, while the operators describe the way the observer elaborates thedata and enclose the invariance that he/she associates with them. We exposethis model and illustrate some theoretical reasons that justify its possibleuse for shape comparison.
arxiv-15900-55 | From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators | http://arxiv.org/pdf/1603.02003v1.pdf | author:Paul Upchurch, Noah Snavely, Kavita Bala category:cs.CV published:2016-03-07 summary:We propose a new neural network architecture for solving single-imageanalogies - the generation of an entire set of stylistically similar imagesfrom just a single input image. Solving this problem requires separating imagestyle from content. Our network is a modified variational autoencoder (VAE)that supports supervised training of single-image analogies and in-networkevaluation of outputs with a structured similarity objective that capturespixel covariances. On the challenging task of generating a 62-letter font froma single example letter we produce images with 22.4% lower dissimilarity to theground truth than state-of-the-art.
arxiv-15900-56 | A matter of words: NLP for quality evaluation of Wikipedia medical articles | http://arxiv.org/pdf/1603.01987v1.pdf | author:Vittoria Cozza, Marinella Petrocchi, Angelo Spognardi category:cs.IR cs.CL published:2016-03-07 summary:Automatic quality evaluation of Web information is a task with many fields ofapplications and of great relevance, especially in critical domains like themedical one. We move from the intuition that the quality of content of medicalWeb documents is affected by features related with the specific domain. First,the usage of a specific vocabulary (Domain Informativeness); then, the adoptionof specific codes (like those used in the infoboxes of Wikipedia articles) andthe type of document (e.g., historical and technical ones). In this paper, wepropose to leverage specific domain features to improve the results of theevaluation of Wikipedia medical articles. In particular, we evaluate thearticles adopting an "actionable" model, whose features are related to thecontent of the articles, so that the model can also directly suggest strategiesfor improving a given article quality. We rely on Natural Language Processing(NLP) and dictionaries-based techniques in order to extract the bio-medicalconcepts in a text. We prove the effectiveness of our approach by classifyingthe medical articles of the Wikipedia Medicine Portal, which have beenpreviously manually labeled by the Wiki Project team. The results of ourexperiments confirm that, by considering domain-oriented features, it ispossible to obtain sensible improvements with respect to existing solutions,mainly for those articles that other approaches have less correctly classified.Other than being interesting by their own, the results call for furtherresearch in the area of domain specific features suitable for Web data qualityassessment.
arxiv-15900-57 | Deep convolutional neural networks for pedestrian detection | http://arxiv.org/pdf/1510.03608v5.pdf | author:Denis Tomè, Federico Monti, Luca Baroffio, Luca Bondi, Marco Tagliasacchi, Stefano Tubaro category:cs.CV published:2015-10-13 summary:Pedestrian detection is a popular research topic due to its paramountimportance for a number of applications, especially in the fields ofautomotive, surveillance and robotics. Despite the significant improvements,pedestrian detection is still an open challenge that calls for more and moreaccurate algorithms. In the last few years, deep learning and in particularconvolutional neural networks emerged as the state of the art in terms ofaccuracy for a number of computer vision tasks such as image classification,object detection and segmentation, often outperforming the previous goldstandards by a large margin. In this paper, we propose a pedestrian detectionsystem based on deep learning, adapting a general-purpose convolutional networkto the task at hand. By thoroughly analyzing and optimizing each step of thedetection pipeline we propose an architecture that outperforms traditionalmethods, achieving a task accuracy close to that of state-of-the-artapproaches, while requiring a low computational time. Finally, we tested thesystem on an NVIDIA Jetson TK1, a 192-core platform that is envisioned to be aforerunner computational brain of future self-driving cars.
arxiv-15900-58 | Brains and pseudorandom generators | http://arxiv.org/pdf/1311.6531v3.pdf | author:Vašek Chvátal, Mark Goldsmith, Nan Yang category:math.DS cs.CR cs.NE math.NA published:2013-11-26 summary:In a pioneering classic, Warren McCulloch and Walter Pitts proposed a modelof the central nervous system; motivated by EEG recordings of normal brainactivity, Chv\' atal and Goldsmith asked whether or not this model can beengineered to provide pseudorandom number generators. We supply evidencesuggesting that the answer is negative.
arxiv-15900-59 | Deep Contrast Learning for Salient Object Detection | http://arxiv.org/pdf/1603.01976v1.pdf | author:Guanbin Li, Yizhou Yu category:cs.CV published:2016-03-07 summary:Salient object detection has recently witnessed substantial progress due topowerful features extracted using deep convolutional neural networks (CNNs).However, existing CNN-based methods operate at the patch level instead of thepixel level. Resulting saliency maps are typically blurry, especially near theboundary of salient objects. Furthermore, image patches are treated asindependent samples even when they are overlapping, giving rise to significantredundancy in computation and storage. In this CVPR 2016 paper, we propose anend-to-end deep contrast network to overcome the aforementioned limitations.Our deep network consists of two complementary components, a pixel-level fullyconvolutional stream and a segment-wise spatial pooling stream. The firststream directly produces a saliency map with pixel-level accuracy from an inputimage. The second stream extracts segment-wise features very efficiently, andbetter models saliency discontinuities along object boundaries. Finally, afully connected CRF model can be optionally incorporated to improve spatialcoherence and contour localization in the fused result from these two streams.Experimental results demonstrate that our deep model significantly improves thestate of the art.
arxiv-15900-60 | Quantifying the information of the prior and likelihood in parametric Bayesian modeling | http://arxiv.org/pdf/1511.01214v7.pdf | author:Giri Gopalan category:stat.ML cs.IT math.IT stat.AP stat.ME published:2015-11-04 summary:I suggest using a pair of metrics to quantify the information of the priorand likelihood functions within a parametric Bayesian model, one of which isclosely related to the reference priors of Berger and Bernardo (Bernardo 1979,Berger and Bernardo 2009) and information measure introduced by Lindley(Lindley 1956). A Monte Carlo algorithm to estimate these metrics is developedand their properties are explored via a combination of theoretical results,simulations, and applications to public medical data sets. This combination oftheoretical, empirical, and computational support provides evidence that thesemetrics may be useful diagnostic tools when performing a Bayesian analysis.
arxiv-15900-61 | Efficient Multiscale Gaussian Process Regression using Hierarchical Clustering | http://arxiv.org/pdf/1511.02258v2.pdf | author:Z. Zhang, K. Duraisamy, N. A. Gumerov category:cs.LG stat.ML published:2015-11-06 summary:Standard Gaussian Process (GP) regression, a powerful machine learning tool,is computationally expensive when it is applied to large datasets, andpotentially inaccurate when data points are sparsely distributed in ahigh-dimensional feature space. To address these challenges, a new multiscale,sparsified GP algorithm is formulated, with the goal of application to largescientific computing datasets. In this approach, the data is partitioned intoclusters and the cluster centers are used to define a reduced training set,resulting in an improvement over standard GPs in terms of training andevaluation costs. Further, a hierarchical technique is used to adaptively mapthe local covariance representation to the underlying sparsity of the featurespace, leading to improved prediction accuracy when the data distribution ishighly non-uniform. A theoretical investigation of the computational complexityof the algorithm is presented. The efficacy of this method is then demonstratedon smooth and discontinuous analytical functions and on data from a directnumerical simulation of turbulent combustion.
arxiv-15900-62 | Confidence-Constrained Maximum Entropy Framework for Learning from Multi-Instance Data | http://arxiv.org/pdf/1603.01901v1.pdf | author:Behrouz Behmardi, Forrest Briggs, Xiaoli Z. Fern, Raviv Raich category:cs.LG cs.IT math.IT stat.ML published:2016-03-07 summary:Multi-instance data, in which each object (bag) contains a collection ofinstances, are widespread in machine learning, computer vision, bioinformatics,signal processing, and social sciences. We present a maximum entropy (ME)framework for learning from multi-instance data. In this approach each bag isrepresented as a distribution using the principle of ME. We introduce theconcept of confidence-constrained ME (CME) to simultaneously learn thestructure of distribution space and infer each distribution. The sharedstructure underlying each density is used to learn from instances inside eachbag. The proposed CME is free of tuning parameters. We devise a fastoptimization algorithm capable of handling large scale multi-instance data. Inthe experimental section, we evaluate the performance of the proposed approachin terms of exact rank recovery in the space of distributions and compare itwith the regularized ME approach. Moreover, we compare the performance of CMEwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show acomparable performance in terms of accuracy with reduced computationalcomplexity.
arxiv-15900-63 | Manifold Matching using Shortest-Path Distance and Joint Neighborhood Selection | http://arxiv.org/pdf/1412.4098v3.pdf | author:Cencheng Shen, Joshua T. Vogelstein, Carey E. Priebe category:stat.ML published:2014-12-12 summary:We propose a nonlinear manifold matching algorithm to match multiple datasets using shortest-path distance and joint neighborhood selection. Based onthe correspondence information, a neighborhood graph is jointly constructed;then the shortest-path distance within each data set is computed from the jointneighborhood graph, followed by embedding into and matching in a commonlow-dimensional Euclidean space. Our approach exhibits superior and robustperformance for matching data from disparate sources, compared to algorithmsthat do not use shortest-path distance or joint neighborhood selection.
arxiv-15900-64 | Composing inference algorithms as program transformations | http://arxiv.org/pdf/1603.01882v1.pdf | author:Robert Zinkov, Chung-chieh Shan category:stat.ML cs.AI stat.CO stat.ME published:2016-03-06 summary:Probabilistic inference procedures are usually coded painstakingly fromscratch, for each target model and each inference algorithm. We reduce thiscoding effort by generating inference procedures from models automatically. Wemake this code generation modular by decomposing inference algorithms intoreusable program transformations. These source-to-source transformationsperform exact inference as well as generate probabilistic programs that computeexpectations, densities, and MCMC samples. The resulting inference proceduresrun in time comparable to that of handwritten procedures.
arxiv-15900-65 | Online Ranking with Top-1 Feedback | http://arxiv.org/pdf/1410.1103v3.pdf | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG published:2014-10-05 summary:We consider a setting where a system learns to rank a fixed set of $m$ items.The goal is produce good item rankings for users with diverse interests whointeract online with the system for $T$ rounds. We consider a novel top-$1$feedback model: at the end of each round, the relevance score for only the topranked object is revealed. However, the performance of the system is judged onthe entire ranked list. We provide a comprehensive set of results regardinglearnability under this challenging setting. For PairwiseLoss and DCG, twopopular ranking measures, we prove that the minimax regret is$\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using anefficient strategy that only spends $O(m \log m)$ time per round. The sameefficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$.Surprisingly, we show that for normalized versions of these ranking measures,i.e., AUC, NDCG \& MAP, no online ranking algorithm can have sublinear regret.
arxiv-15900-66 | Personalized Advertisement Recommendation: A Ranking Approach to Address the Ubiquitous Click Sparsity Problem | http://arxiv.org/pdf/1603.01870v1.pdf | author:Sougata Chaudhuri, Georgios Theocharous, Mohammad Ghavamzadeh category:cs.LG cs.IR published:2016-03-06 summary:We study the problem of personalized advertisement recommendation (PAR),which consist of a user visiting a system (website) and the system displayingone of $K$ ads to the user. The system uses an internal ad recommendationpolicy to map the user's profile (context) to one of the ads. The user eitherclicks or ignores the ad and correspondingly, the system updates itsrecommendation policy. PAR problem is usually tackled by scalable\emph{contextual bandit} algorithms, where the policies are generally based onclassifiers. A practical problem in PAR is extreme click sparsity, due to veryfew users actually clicking on ads. We systematically study the drawback ofusing contextual bandit algorithms based on classifier-based policies, in faceof extreme click sparsity. We then suggest an alternate policy, based onrankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss,which can significantly alleviate the problem of click sparsity. We conductextensive experiments on public datasets, as well as three industry proprietarydatasets, to illustrate the improvement in click-through-rate (CTR) obtained byusing the ranker-based policy over classifier-based policies.
arxiv-15900-67 | General Participative Media Single Image Restoration | http://arxiv.org/pdf/1603.01864v1.pdf | author:Felipe Codevilla, Joel D. O. Gaya, Amanda C. Duarte, Silvia Botelho category:cs.CV published:2016-03-06 summary:This paper describes a method to restore degraded images captured in generalparticipative media --- fog, turbid water, sand storm, etc. To obtaingenerality, we, first, propose a novel interpretation of the participativemedia image formation by considering the color variation of the media. Second,we introduce that joining different image priors is an effective alternativefor image restoration. The proposed method contains a Composite Prior supportedby statistics collected on both haze-free and degraded participativeenvironment images. The key of the method is joining two complementary measures--- local contrast and color. The results presented for a variety of underwaterand haze images demonstrate the power of the method. Moreover, we showed thepotential of our method using a special dataset for which a reference haze-freeimage is available for comparison.
arxiv-15900-68 | Generalization error bounds for learning to rank: Does the length of document lists matter? | http://arxiv.org/pdf/1603.01860v1.pdf | author:Ambuj Tewari, Sougata Chaudhuri category:cs.LG published:2016-03-06 summary:We consider the generalization ability of algorithms for learning to rank ata query level, a problem also called subset ranking. Existing generalizationerror bounds necessarily degrade as the size of the document list associatedwith a query increases. We show that such a degradation is not intrinsic to theproblem. For several loss functions, including the cross-entropy loss used inthe well known ListNet method, there is \emph{no} degradation in generalizationability as document lists become longer. We also provide novel generalizationerror bounds under $\ell_1$ regularization and faster convergence rates if theloss function is smooth.
arxiv-15900-69 | Online Learning to Rank with Feedback at the Top | http://arxiv.org/pdf/1603.01855v1.pdf | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG published:2016-03-06 summary:We consider an online learning to rank setting in which, at each round, anoblivious adversary generates a list of $m$ documents, pertaining to a query,and the learner produces scores to rank the documents. The adversary thengenerates a relevance vector and the learner updates its ranker according tothe feedback received. We consider the setting where the feedback is restrictedto be the relevance levels of only the top $k$ documents in the ranked list for$k \ll m$. However, the performance of learner is judged based on theunrevealed full relevance vectors, using an appropriate learning to rank lossfunction. We develop efficient algorithms for well known losses in thepointwise, pairwise and listwise families. We also prove that no onlinealgorithm can have sublinear regret, with top-1 feedback, for any loss that iscalibrated with respect to NDCG. We apply our algorithms on benchmark datasetsdemonstrating efficient online learning of a ranking function from highlyrestricted feedback.
arxiv-15900-70 | Proximal groupoid patterns In digital images | http://arxiv.org/pdf/1603.01842v1.pdf | author:Enoch A-iyeh, James F. Peters category:cs.CV 54E40 published:2016-03-06 summary:The focus of this article is on the detection and classification of patternsbased on groupoids. The approach hinges on descriptive proximity of points in aset based on the neighborliness property. This approach lends support to imageanalysis and understanding and in studying nearness of image segments. Apractical application of the approach is in terms of the analysis of naturalimages for pattern identification and classification.
arxiv-15900-71 | Hierarchical Decision Making In Electricity Grid Management | http://arxiv.org/pdf/1603.01840v1.pdf | author:Gal Dalal, Elad Gilboa, Shie Mannor category:cs.AI cs.LG stat.AP published:2016-03-06 summary:The power grid is a complex and vital system that necessitates carefulreliability management. Managing the grid is a difficult problem with multipletime scales of decision making and stochastic behavior due to renewable energygenerations, variable demand and unplanned outages. Solving this problem in theface of uncertainty requires a new methodology with tractable algorithms. Inthis work, we introduce a new model for hierarchical decision making in complexsystems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., alevel of abstraction, for real-time power grid reliability. We devise analgorithm that alternates between slow time-scale policy improvement, and fasttime-scale value function approximation. We compare our results to prevailingheuristics, and show the strength of our method.
arxiv-15900-72 | From Word Embeddings to Item Recommendation | http://arxiv.org/pdf/1601.01356v2.pdf | author:Makbule Gulcin Ozsoy category:cs.LG cs.CL cs.IR cs.SI published:2016-01-07 summary:Social network platforms can archive data produced by their users. Then, thearchived data is used to provide better services to the users. One of theservices that these platforms provide is the recommendation service.Recommendation systems can predict the future preferences of users usingvarious different techniques. One of the most popular technique forrecommendation is matrix-factorization, which uses low-rank approximation ofinput data. Similarly, word embedding methods from natural language processingliterature learn low-dimensional vector space representation of input elements.Noticing the similarities among word embedding and matrix factorizationtechniques and based on the previous works that apply techniques from textprocessing to recommendation, Word2Vec's skip-gram technique is employed tomake recommendations. The aim of this work is to make recommendation on nextcheck-in venues. Unlike previous works that use Word2Vec for recommendation, inthis work non-textual features are used. For the experiments, a Foursquarecheck-in dataset is used. The results show that use of vector spacerepresentations of items modeled by skip-gram technique is promising for makingrecommendations.
arxiv-15900-73 | Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive Disambiguation: the eXtended Revised AraMorph (XRAM) | http://arxiv.org/pdf/1603.01833v1.pdf | author:Giuliano Lancioni, Valeria Pettinari, Laura Garofalo, Marta Campanelli, Ivana Pepe, Simona Olivieri, Ilaria Cicola category:cs.CL cs.IR published:2016-03-06 summary:An extended, revised form of Tim Buckwalter's Arabic lexical andmorphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM),is presented which addresses a number of weaknesses and inconsistencies of theoriginal model by allowing a wider coverage of real-world Classical andcontemporary (both formal and informal) Arabic texts. Building upon previousresearch, XRAM enhancements include (i) flag-selectable usage markers, (ii)probabilistic mildly context-sensitive POS tagging, filtering, disambiguationand ranking of alternative morphological analyses, (iii) semi-automaticincrement of lexical coverage through extraction of lexical and morphologicalinformation from existing lexical resources. Testing of XRAM through afront-end Python module showed a remarkable success level.
arxiv-15900-74 | A Parallel Corpus of Translationese | http://arxiv.org/pdf/1509.03611v2.pdf | author:Ella Rabinovich, Shuly Wintner, Ofek Luis Lewinsohn category:cs.CL published:2015-09-11 summary:We describe a set of bilingual English--French and English--German parallelcorpora in which the direction of translation is accurately and reliablyannotated. The corpora are diverse, consisting of parliamentary proceedings,literary works, transcriptions of TED talks and political commentary. They willbe instrumental for research of translationese and its applications to (humanand machine) translation; specifically, they can be used for the task oftranslationese identification, a research direction that enjoys a growinginterest in recent years. To validate the quality and reliability of thecorpora, we replicated previous results of supervised and unsupervisedidentification of translationese, and further extended the experiments toadditional datasets and languages.
arxiv-15900-75 | Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques | http://arxiv.org/pdf/1602.06516v2.pdf | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:cs.LG stat.ML published:2016-02-21 summary:Graph partitioning plays a central role in machine learning, and thedevelopment of graph partitioning algorithms is still an active area ofresearch. The immense demand for such algorithms arises due to the abundance ofapplications that involve pairwise interactions or similarities among entities.Recent studies in computer vision and databases systems have emphasized on thenecessity of considering multi-way interactions, and has led to the study of amore general problem in the form of hypergraph partitioning. This paper focuses on the problem of partitioning uniform hypergraphs, whicharises in applications such as subspace clustering, motion segmentation etc. Weshow that uniform hypergraph partitioning is equivalent to a tensor tracemaximization problem, and hence, a tensor based method is a natural answer tothis problem. We also propose a tensor spectral method that extends the widelyknown spectral clustering algorithm to the case of uniform hypergraphs. Whilethe theoretical guarantees of spectral clustering have been extensivelystudied, very little is known about the statistical properties of tensor basedmethods. To this end, we prove the consistency of the proposed algorithm undera planted partition model. The computational complexity of tensorial approaches has resulted in the useof various tensor sampling strategies. We present the first theoretical studyon the effect of sampling in tensor based hypergraph partitioning. Our resultjustifies the empirical success of iterative sampling techniques often used inpractice. We also present an iteratively sampled variant of the proposedalgorithm for the purpose of subspace clustering, and demonstrate theperformance of this method on a benchmark problem.
arxiv-15900-76 | Variational methods for Conditional Multimodal Learning: Generating Human Faces from Attributes | http://arxiv.org/pdf/1603.01801v1.pdf | author:Gaurav Pandey, Ambedkar Dukkipati category:cs.CV cs.LG stat.ML published:2016-03-06 summary:Prior to this decade, the field of computer vision was primarily focusedaround hand-crafted feature extraction methods used in conjunction withdiscriminative models for specific tasks such as object recognition,detection/localization, tracking etc. A generative image understanding wasneither within reach nor the prime concern of the period. In this paper, weaddress the following problem: Given a description of a human face, can wegenerate the image corresponding to it? We frame this problem as a conditionalmodality learning problem and use variational methods for maximizing thecorresponding conditional log-likelihood. The resultant deep model, which werefer to as conditional multimodal autoencoder (CMMA), forces the latentrepresentation obtained from the attributes alone to be 'close' to the jointrepresentation obtained from both face and attributes. We show that the facesgenerated from attributes using the proposed model, are qualitatively andquantitatively more representative of the attributes from which they weregenerated, than those obtained by other deep generative models. We also proposea secondary task, whereby the existing faces are modified by modifying thecorresponding attributes. We observe that the modifications in face introducedby the proposed model are representative of the corresponding modifications inattributes. Hence, our proposed method solves the above mentioned problem.
arxiv-15900-77 | Fast calculation of correlations in recognition systems | http://arxiv.org/pdf/1603.01772v1.pdf | author:Pavel Dourbal, Mikhail Pekker category:cs.CV published:2016-03-06 summary:Computationally efficient classification system architecture is proposed. Itutilizes fast tensor-vector multiplication algorithm to apply linear operatorsupon input signals . The approach is applicable to wide variety of recognitionsystem architectures ranging from single stage matched filter bank classifiersto complex neural networks with unlimited number of hidden layers.
arxiv-15900-78 | Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks | http://arxiv.org/pdf/1603.01768v1.pdf | author:Alex J. Champandard category:cs.CV published:2016-03-05 summary:Convolutional neural networks (CNNs) have proven highly effective at imagesynthesis and style transfer. For most users, however, using them as tools canbe a challenging task due to their unpredictable behavior that goes againstcommon intuitions. This paper introduces a novel concept to augment suchgenerative architectures with semantic annotations, either by manuallyauthoring pixel labels or using existing solutions for semantic segmentation.The result is a content-aware generative algorithm that offers meaningfulcontrol over the outcome. Thus, we increase the quality of images generated byavoiding common glitches, make the results look significantly more plausible,and extend the functional range of these algorithms---whether for portraits orlandscapes, etc. Applications include semantic style transfer and turningdoodles with few colors into masterful paintings!
arxiv-15900-79 | Fundamental differences between Dropout and Weight Decay in Deep Networks | http://arxiv.org/pdf/1602.04484v2.pdf | author:David P. Helmbold, Philip M. Long category:cs.LG cs.AI cs.NE math.ST stat.ML stat.TH published:2016-02-14 summary:We study dropout and weight decay applied to deep networks with rectifiedlinear units and the quadratic loss. We show how using dropout in this contextcan be viewed as adding a regularization penalty term that grows exponentiallywith the depth of the network when the more traditional weight decay penaltygrows polynomially. We then show how this difference affects the inductive biasof algorithms using one regularizer or the other: we describe a random sourceof data that dropout is unwilling to fit, but that is compatible with theinductive bias of weight decay. We also describe a source that is compatiblewith the inductive bias of dropout, but not weight decay. We also show that, incontrast with the case of generalized linear models, when used with deepnetworks with rectified linear units and the quadratic loss, the regularizationpenalty of dropout (a) is not just a function of the independent variables, butalso depends on the response variables, and (b) can be negative. Finally, thedropout penalty can drive a learning algorithm to use negative weights evenwhen trained with monotone training data.
arxiv-15900-80 | Ask Me Anything: Dynamic Memory Networks for Natural Language Processing | http://arxiv.org/pdf/1506.07285v5.pdf | author:Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher category:cs.CL cs.LG cs.NE published:2015-06-24 summary:Most tasks in natural language processing can be cast into question answering(QA) problems over language input. We introduce the dynamic memory network(DMN), a neural network architecture which processes input sequences andquestions, forms episodic memories, and generates relevant answers. Questionstrigger an iterative attention process which allows the model to condition itsattention on the inputs and the result of previous iterations. These resultsare then reasoned over in a hierarchical recurrent sequence model to generateanswers. The DMN can be trained end-to-end and obtains state-of-the-art resultson several types of tasks and datasets: question answering (Facebook's bAbIdataset), text classification for sentiment analysis (Stanford SentimentTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). Thetraining for these different tasks relies exclusively on trained word vectorrepresentations and input-question-answer triplets.
arxiv-15900-81 | Feature Selection via Binary Simultaneous Perturbation Stochastic Approximation | http://arxiv.org/pdf/1508.07630v3.pdf | author:Vural Aksakalli, Milad Malekipirbazari category:stat.ML cs.LG published:2015-08-30 summary:Feature selection (FS) has become an indispensable task in dealing withtoday's highly complex pattern recognition problems with massive number offeatures. In this study, we propose a new wrapper approach for FS based onbinary simultaneous perturbation stochastic approximation (BSPSA). Thispseudo-gradient descent stochastic algorithm starts with an initial featurevector and moves toward the optimal feature vector via successive iterations.In each iteration, the current feature vector's individual components areperturbed simultaneously by random offsets from a qualified probabilitydistribution. We present computational experiments on datasets with numbers offeatures ranging from a few dozens to thousands using three widely-usedclassifiers as wrappers: nearest neighbor, decision tree, and linear supportvector machine. We compare our methodology against the full set of features aswell as a binary genetic algorithm and sequential FS methods usingcross-validated classification error rate and AUC as the performance criteria.Our results indicate that features selected by BSPSA compare favorably toalternative methods in general and BSPSA can yield superior feature sets fordatasets with tens of thousands of features by examining an extremely smallfraction of the solution space. We are not aware of any other wrapper FSmethods that are computationally feasible with good convergence properties forsuch large datasets.
arxiv-15900-82 | UTA-poly and UTA-splines: additive value functions with polynomial marginals | http://arxiv.org/pdf/1603.02626v1.pdf | author:Olivier Sobrie, Nicolas Gillis, Vincent Mousseau, Marc Pirlot category:math.OC cs.AI cs.LG published:2016-03-05 summary:Additive utility function models are widely used in multiple criteriadecision analysis. In such models, a numerical value is associated to eachalternative involved in the decision problem. It is computed by aggregating thescores of the alternative on the different criteria of the decision problem.The score of an alternative is determined by a marginal value function thatevolves monotonically as a function of the performance of the alternative onthis criterion. Determining the shape of the marginals is not easy for adecision maker. It is easier for him/her to make statements such as"alternative $a$ is preferred to $b$". In order to help the decision maker, UTAdisaggregation procedures use linear programming to approximate the marginalsby piecewise linear functions based only on such statements. In this paper, wepropose to infer polynomials and splines instead of piecewise linear functionsfor the marginals. In this aim, we use semidefinite programming instead oflinear programming. We illustrate this new elicitation method and present someexperimental results.
arxiv-15900-83 | Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for in Vitro Embryo Culture | http://arxiv.org/pdf/1603.01739v1.pdf | author:Viswanath P Sudarshan, Tobias Weiser, Phalgun Chintala, Subhamoy Mandal, Rahul Dutta category:cs.CV physics.med-ph published:2016-03-05 summary:Visual observation of Cumulus Oocyte Complexes provides only limitedinformation about its functional competence, whereas the molecular evaluationsmethods are cumbersome or costly. Image analysis of mammalian oocytes canprovide attractive alternative to address this challenge. However, it iscomplex, given the huge number of oocytes under inspection and the subjectivenature of the features inspected for identification. Supervised machinelearning methods like random forest with annotations from expert biologists canmake the analysis task standardized and reduces inter-subject variability. Wepresent a semi-automatic framework for predicting the class an oocyte belongsto, based on multi-object parametric segmentation on the acquired microscopicimage followed by a feature based classification using random forests.
arxiv-15900-84 | Classifier ensemble creation via false labelling | http://arxiv.org/pdf/1603.01716v1.pdf | author:Bálint Antal category:cs.LG I.2, I.5.2 published:2016-03-05 summary:In this paper, a novel approach to classifier ensemble creation is presented.While other ensemble creation techniques are based on careful selection ofexisting classifiers or preprocessing of the data, the presented approachautomatically creates an optimal labelling for a number of classifiers, whichare then assigned to the original data instances and fed to classifiers. Theapproach has been evaluated on high-dimensional biomedical datasets. Theresults show that the approach outperformed individual approaches in all cases.
arxiv-15900-85 | High-Dimensional Metrics in R | http://arxiv.org/pdf/1603.01700v1.pdf | author:Victor Chernozhukov, Chris Hansen, Martin Spindler category:stat.ML stat.ME published:2016-03-05 summary:The package High-dimensional Metrics (\Rpackage{hdm}) is an evolvingcollection of statistical methods for estimation and quantification ofuncertainty in high-dimensional approximately sparse models. It focuses onproviding semi-parametrically efficient estimators, confidence intervals, andsignificance testing for low-dimensional subcomponents of the high-dimensionalparameter vector. This vignette offers a brief introduction and a tutorial tothe implemented methods. \R and the package \Rpackage{hdm} are open-sourcesoftware projects and can be freely downloaded from CRAN:\texttt{http://cran.r-project.org}.
arxiv-15900-86 | A Feature Learning and Object Recognition Framework for Underwater Fish Images | http://arxiv.org/pdf/1603.01696v1.pdf | author:Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams category:cs.CV published:2016-03-05 summary:Live fish recognition is one of the most crucial elements of fisheries surveyapplications where vast amount of data are rapidly acquired. Different fromgeneral scenarios, challenges to underwater image recognition are posted bypoor image quality, uncontrolled objects and environment, as well as difficultyin acquiring representative samples. Also, most existing feature extractiontechniques are hindered from automation due to involving human supervision.Toward this end, we propose an underwater fish recognition framework thatconsists of a fully unsupervised feature learning technique and anerror-resilient classifier. Object parts are initialized based on saliency andrelaxation labeling to match object parts correctly. A non-rigid part model isthen learned based on fitness, separation and discrimination criteria. For theclassifier, an unsupervised clustering approach generates a binary classhierarchy, where each node is a classifier. To exploit information fromambiguous images, the notion of partial classification is introduced to assigncoarse labels by optimizing the "benefit" of indecision made by the classifier.Experiments show that the proposed framework achieves high accuracy on bothpublic and self-collected underwater fish images with high uncertainty andclass imbalance.
arxiv-15900-87 | Underwater Fish Tracking for Moving Cameras based on Deformable Multiple Kernels | http://arxiv.org/pdf/1603.01695v1.pdf | author:Meng-Che Chuang, Jenq-Neng Hwang, Jian-Hui Ye, Shih-Chia Huang, Kresimir Williams category:cs.CV published:2016-03-05 summary:Fishery surveys that call for the use of single or multiple underwatercameras have been an emerging technology as a non-extractive mean to estimatethe abundance of fish stocks. Tracking live fish in an open aquatic environmentposts challenges that are different from general pedestrian or vehicle trackingin surveillance applications. In many rough habitats fish are monitored bycameras installed on moving platforms, where tracking is even more challengingdue to inapplicability of background models. In this paper, a novel trackingalgorithm based on the deformable multiple kernels (DMK) is proposed to addressthese challenges. Inspired by the deformable part model (DPM) technique, a setof kernels is defined to represent the holistic object and several parts thatare arranged in a deformable configuration. Color histogram, texture histogramand the histogram of oriented gradients (HOG) are extracted and serve as objectfeatures. Kernel motion is efficiently estimated by the mean-shift algorithm oncolor and texture features to realize tracking. Furthermore, the HOG-featuredeformation costs are adopted as soft constraints on kernel positions tomaintain the part configuration. Experimental results on practical video setfrom underwater moving cameras show the reliable performance of the proposedmethod with much less computational cost comparing with state-of-the-arttechniques.
arxiv-15900-88 | Saliency Detection combining Multi-layer Integration algorithm with background prior and energy function | http://arxiv.org/pdf/1603.01684v1.pdf | author:Hanling Zhang, Chenxing Xia category:cs.CV published:2016-03-05 summary:In this paper, we propose an improved mechanism for saliency detection.Firstly,based on a neoteric background prior selecting four corners of an imageas background,we use color and spatial contrast with each superpixel to obtaina salinecy map(CBP). Inspired by reverse-measurement methods to improve theaccuracy of measurement in Engineering,we employ the Objectness labels asforeground prior based on part of information of CBP to construct amap(OFP).Further,an original energy function is applied to optimize both ofthem respectively and a single-layer saliency map(SLP)is formed by merging theabove twos.Finally,to deal with the scale problem,we obtain our multi-layermap(MLP) by presenting an integration algorithm to take advantage of multiplesaliency maps. Quantitative and qualitative experiments on three datasetsdemonstrate that our method performs favorably against the state-of-the-artalgorithm.
arxiv-15900-89 | A single-phase, proximal path-following framework | http://arxiv.org/pdf/1603.01681v1.pdf | author:Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher category:math.OC cs.IT math.IT stat.ML published:2016-03-05 summary:We propose a new proximal, path-following framework for a class of---possiblynon-smooth---constrained convex problems. We consider settings where thenon-smooth part is endowed with a proximity operator, and the constraint set isequipped with a self-concordant barrier. Our main contribution is a newre-parametrization of the optimality condition of the barrier problem, thatallows us to process the objective function with its proximal operator within anew path following scheme. In particular, our approach relies on the followingtwo main ideas. First, we re-parameterize the optimality condition as anauxiliary problem, such that a "good" initial point is available. Second, wecombine the proximal operator of the objective and path-following ideas todesign a single phase, proximal, path-following algorithm. Our method has several advantages. First, it allows handling non-smoothobjectives via proximal operators, this avoids lifting the problem dimensionvia slack variables and additional constraints. Second, it consists of only a\emph{single phase} as compared to a two-phase algorithm in [43] In this work,we show how to overcome this difficulty in the proximal setting and prove thatour scheme has the same $\mathcal{O}(\sqrt{\nu}\log(1/\varepsilon))$ worst-caseiteration-complexity with standard approaches [30, 33], but our method canhandle nonsmooth objectives, where $\nu$ is the barrier parameter and$\varepsilon$ is a desired accuracy. Finally, our framework allows errors inthe calculation of proximal-Newton search directions, without sacrificing theworst-case iteration complexity. We demonstrate the merits of our algorithm viathree numerical examples, where proximal operators play a key role to improvethe performance over off-the-shelf interior-point solvers.
arxiv-15900-90 | Getting More Out Of Syntax with PropS | http://arxiv.org/pdf/1603.01648v1.pdf | author:Gabriel Stanovsky, Jessica Ficler, Ido Dagan, Yoav Goldberg category:cs.CL published:2016-03-04 summary:Semantic NLP applications often rely on dependency trees to recognize majorelements of the proposition structure of sentences. Yet, while much semanticstructure is indeed expressed by syntax, many phenomena are not easily read outof dependency trees, often leading to further ad-hoc heuristic post-processingor to information loss. To directly address the needs of semantic applications,we present PropS -- an output representation designed to explicitly anduniformly express much of the proposition structure which is implied fromsyntax, and an associated tool for extracting it from dependency trees.
arxiv-15900-91 | Feature Selection: A Data Perspective | http://arxiv.org/pdf/1601.07996v3.pdf | author:Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P. Trevino, Jiliang Tang, Huan Liu category:cs.LG published:2016-01-29 summary:Feature selection, as a data preprocessing strategy, has been proven to beeffective and efficient in preparing high-dimensional data for data mining andmachine learning problems. The objectives of feature selection include:building simpler and more comprehensible models, improving data miningperformance, and preparing clean, understandable data. The recent proliferationof big data has presented some substantial challenges and opportunities offeature selection algorithms. In this survey, we provide a comprehensive andstructured overview of recent advances in feature selection research. Motivatedby current challenges and opportunities in the big data age, we revisit featureselection research from a data perspective, and review representative featureselection algorithms for generic data, structured data, heterogeneous data andstreaming data. Methodologically, to emphasize the differences and similaritiesof most existing feature selection algorithms for generic data, we generallycategorize them into four groups: similarity based, information theoreticalbased, sparse learning based and statistical based methods. Finally, tofacilitate and promote the research in this community, we also present aopen-source feature selection repository that consists of most of the popularfeature selection algorithms (http://featureselection.asu.edu/). At the end ofthis survey, we also have a discussion about some open problems and challengesthat need to be paid more attention in future research.
arxiv-15900-92 | Depth Superresolution using Motion Adaptive Regularization | http://arxiv.org/pdf/1603.01633v1.pdf | author:Ulugbek S. Kamilov, Petros T. Boufounos category:cs.CV published:2016-03-04 summary:Spatial resolution of depth sensors is often significantly lower compared tothat of conventional optical cameras. Recent work has explored the idea ofimproving the resolution of depth using higher resolution intensity as a sideinformation. In this paper, we demonstrate that further incorporating temporalinformation in videos can significantly improve the results. In particular, wepropose a novel approach that improves depth resolution, exploiting thespace-time redundancy in the depth and intensity using motion-adaptive low-rankregularization. Experiments confirm that the proposed approach substantiallyimproves the quality of the estimated high-resolution depth. Our approach canbe a first component in systems using vision techniques that rely on highresolution depth information.
arxiv-15900-93 | Towards Universal Paraphrastic Sentence Embeddings | http://arxiv.org/pdf/1511.08198v3.pdf | author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu category:cs.CL cs.LG published:2015-11-25 summary:We consider the problem of learning general-purpose, paraphrastic sentenceembeddings based on supervision from the Paraphrase Database (Ganitkevitch etal., 2013). We compare six compositional architectures, evaluating them onannotated textual similarity datasets drawn both from the same distribution asthe training data and from a wide range of other domains. We find that the mostcomplex architectures, such as long short-term memory (LSTM) recurrent neuralnetworks, perform best on the in-domain data. However, in out-of-domainscenarios, simple architectures such as word averaging vastly outperform LSTMs.Our simplest averaging model is even competitive with systems tuned for theparticular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conductfurther experiments on three supervised NLP tasks: sentence similarity,entailment, and sentiment classification. We again find that the word averagingmodels perform well for sentence similarity and entailment, outperformingLSTMs. However, on sentiment classification, we find that the LSTM performsvery strongly-even recording new state-of-the-art performance on the StanfordSentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings withthese supervised tasks, using them both as a prior and as a black box featureextractor. This leads to performance rivaling the state of the art on the SICKsimilarity and entailment tasks. We release all of our resources to theresearch community with the hope that they can serve as the new baseline forfurther work on universal sentence embeddings.
arxiv-15900-94 | Variational Inference: A Review for Statisticians | http://arxiv.org/pdf/1601.00670v2.pdf | author:David M. Blei, Alp Kucukelbir, Jon D. McAuliffe category:stat.CO cs.LG stat.ML published:2016-01-04 summary:One of the core problems of modern statistics is to approximatedifficult-to-compute probability distributions. This problem is especiallyimportant in Bayesian statistics, which frames all inference about unknownquantities as a calculation about the posterior. In this paper, we reviewvariational inference (VI), a method from machine learning that approximatesprobability distributions through optimization. VI has been used in myriadapplications and tends to be faster than classical methods, such as Markovchain Monte Carlo sampling. The idea behind VI is to first posit a family ofdistributions and then to find the member of that family which is close to thetarget. Closeness is measured by Kullback-Leibler divergence. We review theideas behind mean-field variational inference, discuss the special case of VIapplied to exponential family models, present a full example with a Bayesianmixture of Gaussians, and derive a variant that uses stochastic optimization toscale up to massive data. We discuss modern research in VI and highlightimportant open problems. VI is powerful, but it is not yet well understood. Ourhope in writing this paper is to catalyze statistical research on thiswidely-used class of algorithms.
arxiv-15900-95 | Adversarial Manipulation of Deep Representations | http://arxiv.org/pdf/1511.05122v9.pdf | author:Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet category:cs.CV cs.LG cs.NE published:2015-11-16 summary:We show that the representation of an image in a deep neural network (DNN)can be manipulated to mimic those of other natural images, with only minor,imperceptible perturbations to the original image. Previous methods forgenerating adversarial images focused on image perturbations designed toproduce erroneous class labels, while we concentrate on the internal layers ofDNN representations. In this way our new class of adversarial images differsqualitatively from others. While the adversary is perceptually similar to oneimage, its internal representation appears remarkably similar to a differentimage, one from a different class, bearing little if any apparent similarity tothe input; they appear generic and consistent with the space of natural images.This phenomenon raises questions about DNN representations, as well as theproperties of natural images themselves.
arxiv-15900-96 | Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning | http://arxiv.org/pdf/1603.01597v1.pdf | author:Mike Kestemont, Jeroen De Gussem category:cs.CL cs.LG stat.ML published:2016-03-04 summary:In this paper we consider two sequence tagging tasks for medieval Latin:part-of-speech tagging and lemmatization. These are both basic, yetfoundational preprocessing steps in applications such as text re-use detection.Nevertheless, they are generally complicated by the considerable orthographicvariation which is typical of medieval Latin. In Digital Classics, these tasksare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.For example, a lexicon is used to generate all the potential lemma-tag pairsfor a token, and next, a context-aware PoS-tagger is used to select the mostappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,error percolation is a major downside of such approaches. In this paper weexplore the possibility to elegantly solve these tasks using a single,integrated approach. For this, we make use of a layered neural networkarchitecture from the field of deep representation learning.
arxiv-15900-97 | Sentiment Analysis in Scholarly Book Reviews | http://arxiv.org/pdf/1603.01595v1.pdf | author:Hussam Hamdan, Patrice Bellot, Frederic Bechet category:cs.CL cs.AI published:2016-03-04 summary:So far different studies have tackled the sentiment analysis in severaldomains such as restaurant and movie reviews. But, this problem has not beenstudied in scholarly book reviews which is different in terms of review styleand size. In this paper, we propose to combine different features in order tobe presented to a supervised classifiers which extract the opinion targetexpressions and detect their polarities in scholarly book reviews. We constructa labeled corpus for training and evaluating our methods in French bookreviews. We also evaluate them on English restaurant reviews in order tomeasure their robustness across the domains and languages. The evaluation showsthat our methods are enough robust for English restaurant reviews and Frenchbook reviews.
arxiv-15900-98 | Normalized Hierarchical SVM | http://arxiv.org/pdf/1508.02479v2.pdf | author:Heejin Choi, Yutaka Sasaki, Nathan Srebro category:cs.LG published:2015-08-11 summary:We present improved methods of using structured SVMs in a large-scalehierarchical classification problem, that is when labels are leaves, or sets ofleaves, in a tree or a DAG. We examine the need to normalize both theregularization and the margin and show how doing so significantly improvesperformance, including allowing achieving state-of-the-art results whereunnormalized structured SVMs do not perform better than flat models. We alsodescribe a further extension of hierarchical SVMs that highlight the connectionbetween hierarchical SVMs and matrix factorization models.
arxiv-15900-99 | X-rank and identifiability for a polynomial decomposition model | http://arxiv.org/pdf/1603.01566v1.pdf | author:Pierre Comon, Yang Qi, Konstantin Usevich category:cs.IT math.IT math.NA stat.ML published:2016-03-04 summary:In this paper, we study a polynomial decomposition model that arises inproblems of system identification, signal processing and machine learning. Weshow that this decomposition is a special case of the X-rank decomposition ---a powerful novel concept in algebraic geometry that generalizes the tensor CPdecomposition. We prove new results on generic/maximal rank and onidentifiability of the polynomial decomposition model. In the paper, we try tomake results and basic tools accessible for a general audience (assuming noknowledge of algebraic geometry or its prerequisites).
arxiv-15900-100 | Recurrent Neural Networks Hardware Implementation on FPGA | http://arxiv.org/pdf/1511.05552v4.pdf | author:Andre Xian Ming Chang, Berin Martini, Eugenio Culurciello category:cs.NE published:2015-11-17 summary:Recurrent Neural Networks (RNNs) have the ability to retain memory and learndata sequences. Due to the recurrent nature of RNNs, it is sometimes hard toparallelize all its computations on conventional hardware. CPUs do notcurrently offer large parallelism, while GPUs offer limited parallelism due tosequential components of RNN models. In this paper we present a hardwareimplementation of Long-Short Term Memory (LSTM) recurrent network on theprogrammable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$layers and $128$ hidden units in hardware and it has been tested using acharacter level language model. The implementation is more than $21\times$faster than the ARM CPU embedded on the Zynq 7020 FPGA. This work canpotentially evolve to a RNN co-processor for future mobile devices.
arxiv-15900-101 | Text Understanding with the Attention Sum Reader Network | http://arxiv.org/pdf/1603.01547v1.pdf | author:Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst category:cs.CL published:2016-03-04 summary:Several large cloze-style context-question-answer datasets have beenintroduced recently: the CNN and Daily Mail news data and the Children's BookTest. Thanks to the size of these datasets, the associated text comprehensiontask is well suited for deep-learning techniques that currently seem tooutperform all alternative approaches. We present a new, simple model that usesattention to directly pick the answer from the context as opposed to computingthe answer using a blended representation of words in the document as is usualin similar models. This makes the model particularly suitable forquestion-answering problems where the answer is a single word from thedocument. Our model outperforms models previously proposed for these tasks by alarge margin.
arxiv-15900-102 | Parallel Texts in the Hebrew Bible, New Methods and Visualizations | http://arxiv.org/pdf/1603.01541v1.pdf | author:Martijn Naaijer, Dirk Roorda category:cs.CL published:2016-03-04 summary:In this article we develop an algorithm to detect parallel texts in theMasoretic Text of the Hebrew Bible. The results are presented online andchapters in the Hebrew Bible containing parallel passages can be inspectedsynoptically. Differences between parallel passages are highlighted. In asimilar way the MT of Isaiah is presented synoptically with 1QIsaa. We alsoinvestigate how one can investigate the degree of similarity between parallelpassages with the help of a case study of 2 Kings 19-25 and its parallels inIsaiah, Jeremiah and 2 Chronicles.
arxiv-15900-103 | Keypoint Encoding for Improved Feature Extraction from Compressed Video at Low Bitrates | http://arxiv.org/pdf/1506.08316v2.pdf | author:Jianshu Chao, Eckehard Steinbach category:cs.MM cs.CV cs.IR published:2015-06-27 summary:In many mobile visual analysis applications, compressed video is transmittedover a communication network and analyzed by a server. Typical processing stepsperformed at the server include keypoint detection, descriptor calculation, andfeature matching. Video compression has been shown to have an adverse effect onfeature-matching performance. The negative impact of compression can be reducedby using the keypoints extracted from the uncompressed video to calculatedescriptors from the compressed video. Based on this observation, we propose toprovide these keypoints to the server as side information and to extract onlythe descriptors from the compressed video. First, we introduce four differentframe types for keypoint encoding to address different types of changes invideo content. These frame types represent a new scene, the same scene, aslowly changing scene, or a rapidly moving scene and are determined bycomparing features between successive video frames. Then, we propose Intra,Skip and Inter modes of encoding the keypoints for different frame types. Forexample, keypoints for new scenes are encoded using the Intra mode, andkeypoints for unchanged scenes are skipped. As a result, the bitrate of theside information related to keypoint encoding is significantly reduced.Finally, we present pairwise matching and image retrieval experiments conductedto evaluate the performance of the proposed approach using the Stanford mobileaugmented reality dataset and 720p format videos. The results show that theproposed approach offers significantly improved feature matching and imageretrieval performance at a given bitrate.
arxiv-15900-104 | Proposal Flow | http://arxiv.org/pdf/1511.05065v2.pdf | author:Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce category:cs.CV published:2015-11-16 summary:Finding image correspondences remains a challenging problem in the presenceof intra-class variations and large changes in scene layout, typical in sceneflow computation. We introduce a novel approach to this problem, dubbedproposal flow, that establishes reliable correspondences using objectproposals. Unlike prevailing scene flow approaches that operate on pixels orregularly sampled local regions, proposal flow benefits from thecharacteristics of modern object proposals, that exhibit high repeatability atmultiple scales, and can take advantage of both local and geometric consistencyconstraints among proposals. We also show that proposal flow can effectively betransformed into a conventional dense flow field. We introduce a new datasetthat can be used to evaluate both general scene flow techniques andregion-based approaches such as proposal flow. We use this benchmark to comparedifferent matching algorithms, object proposals, and region features withinproposal flow with the state of the art in scene flow. This comparison, alongwith experiments on standard datasets, demonstrates that proposal flowsignificantly outperforms existing scene flow methods in various settings.
arxiv-15900-105 | A Bayesian Model of Multilingual Unsupervised Semantic Role Induction | http://arxiv.org/pdf/1603.01514v1.pdf | author:Nikhil Garg, James Henderson category:cs.CL published:2016-03-04 summary:We propose a Bayesian model of unsupervised semantic role induction inmultiple languages, and use it to explore the usefulness of parallel corporafor this task. Our joint Bayesian model consists of individual models for eachlanguage plus additional latent variables that capture alignments between rolesacross languages. Because it is a generative Bayesian model, we can doevaluations in a variety of scenarios just by varying the inference procedure,without changing the model, thereby comparing the scenarios directly. Wecompare using only monolingual data, using a parallel corpus, using a parallelcorpus with annotations in the other language, and using small amounts ofannotation in the target language. We find that the biggest impact of adding aparallel corpus to training is actually the increase in mono-lingual data, withthe alignments to another language resulting in small improvements, even withlabeled data for the other language.
arxiv-15900-106 | Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition | http://arxiv.org/pdf/1508.03929v3.pdf | author:Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, Timothée Masquelier category:cs.CV q-bio.NC published:2015-08-17 summary:Deep convolutional neural networks (DCNNs) have attracted much attentionrecently, and have shown to be able to recognize thousands of object categoriesin natural image databases. Their architecture is somewhat similar to that ofthe human visual system: both use restricted receptive fields, and a hierarchyof layers which progressively extract more and more abstracted features. Yet itis unknown whether DCNNs match human performance at the task of view-invariantobject recognition, whether they make similar errors and use similarrepresentations for this task, and whether the answers depend on the magnitudeof the viewpoint variations. To investigate these issues, we benchmarked eightstate-of-the-art DCNNs, the HMAX model, and a baseline shallow model andcompared their results to those of humans with backward masking. Unlike in allprevious DCNN studies, we carefully controlled the magnitude of the viewpointvariations to demonstrate that shallow nets can outperform deep nets and humanswhen variations are weak. When facing larger variations, however, more layerswere needed to match human performance and error distributions, and to haverepresentations that are consistent with human behavior. A very deep net with18 layers even outperformed humans at the highest variation level, using themost human-like representations.
arxiv-15900-107 | Clustering by Hierarchical Nearest Neighbor Descent (H-NND) | http://arxiv.org/pdf/1509.02805v3.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.ME published:2015-09-09 summary:Previously in 2014, we proposed the Nearest Descent (ND) method, capable ofgenerating an efficient Graph, called the in-tree (IT). Due to some beautifuland effective features, this IT structure proves well suited for dataclustering. Although there exist some redundant edges in IT, they usually havesalient features and thus it is not hard to remove them. Subsequently, in order to prevent the seemingly redundant edges fromoccurring, we proposed the Nearest Neighbor Descent (NND) by adding the"Neighborhood" constraint on ND. Consequently, clusters automatically emerged,without the additional requirement of removing the redundant edges. However,NND proved still not perfect, since it brought in a new yet worse problem, the"over-partitioning" problem. Now, in this paper, we propose a method, called the Hierarchical NearestNeighbor Descent (H-NND), which overcomes the over-partitioning problem of NNDvia using the hierarchical strategy. Specifically, H-NND uses ND to effectivelymerge the over-segmented sub-graphs or clusters that NND produces. Like ND,H-NND also generates the IT structure, in which the redundant edges once againappear. This seemingly comes back to the situation that ND faces. However,compared with ND, the redundant edges in the IT structure generated by H-NNDgenerally become more salient, thus being much easier and more reliable to beidentified even by the simplest edge-removing method which takes the edgelength as the only measure. In other words, the IT structure constructed byH-NND becomes more fitted for data clustering. We prove this on severalclustering datasets of varying shapes, dimensions and attributes. Besides,compared with ND, H-NND generally takes less computation time to construct theIT data structure for the input data.
arxiv-15900-108 | Performance Localisation | http://arxiv.org/pdf/1603.01489v1.pdf | author:Brendan Cody-Kenny, Stephen Barrett category:cs.SE cs.NE cs.PF published:2016-03-04 summary:Profiling is a prominent technique for finding the location of performance"bottlenecks" in code. Profiling can be performed by adding code to a programwhich increments a counter for each line of code each time it is executed. Anylines of code which have a large execution count relative to other lines in theprogram can be considered a bottleneck. Though code profiling can determine thelocation of a performance issue or bottleneck, we posit that the code changerequired to improve performance may not always be found at the same location.Developers must frequently trace back through a program to understand what codeis contributing to a bottleneck. We seek to highlight code which is likelycausing or has the most effect on the overall execution cost of a program. Inthis document we compare different methods for localising potential performanceimprovements.
arxiv-15900-109 | Online Adaptive Hidden Markov Model for Multi-Tracker Fusion | http://arxiv.org/pdf/1504.06103v2.pdf | author:Tomas Vojir, Jiri Matas, Jana Noskova category:cs.CV published:2015-04-23 summary:In this paper, we propose a novel method for visual object tracking calledHMMTxD. The method fuses observations from complementary out-of-the boxtrackers and a detector by utilizing a hidden Markov model whose latent statescorrespond to a binary vector expressing the failure of individual trackers.The Markov model is trained in an unsupervised way, relying on an onlinelearned detector to provide a source of tracker-independent information for amodified Baum- Welch algorithm that updates the model w.r.t. the partiallyannotated data. We show the effectiveness of the proposed method on combination of two andthree tracking algorithms. The performance of HMMTxD is evaluated on twostandard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publiclyavailable sequences. The HMMTxD outperforms the state-of-the-art, oftensignificantly, on all datasets in almost all criteria.
arxiv-15900-110 | Fourier descriptors based on the structure of the human primary visual cortex with applications to object recognition | http://arxiv.org/pdf/1507.06617v2.pdf | author:Amine Bohi, Dario Prandi, Vincente Guis, Frédéric Bouchara, Jean-Paul Gauthier category:cs.CV published:2015-07-23 summary:In this paper we propose a supervised object recognition method using newglobal features and inspired by the model of the human primary visual cortex V1as the semidiscrete roto-translation group $SE(2,N) = \mathbb R^2\times \mathbbZ_N$. The proposed technique is based on generalized Fourier descriptors on thelatter group, which are invariant to natural geometric transformations(rotations, translations). These descriptors are then used to feed an SVMclassifier. We have tested our method against the COIL-100 image database andthe ORL face database, and compared it with other techniques based ontraditional descriptors, global and local. The obtained results have shown thatour approach looks extremely efficient and stable to noise, in presence ofwhich it outperforms the other techniques analyzed in the paper.
arxiv-15900-111 | Blending LSTMs into CNNs | http://arxiv.org/pdf/1511.06433v2.pdf | author:Krzysztof J. Geras, Abdel-rahman Mohamed, Rich Caruana, Gregor Urban, Shengjie Wang, Ozlem Aslan, Matthai Philipose, Matthew Richardson, Charles Sutton category:cs.LG published:2015-11-19 summary:We consider whether deep convolutional networks (CNNs) can represent decisionfunctions with similar accuracy as recurrent networks such as LSTMs. First, weshow that a deep CNN with an architecture inspired by the models recentlyintroduced in image recognition can yield better accuracy than previousconvolutional and LSTM networks on the standard 309h Switchboard automaticspeech recognition task. Then we show that even more accurate CNNs can betrained under the guidance of LSTMs using a variant of model compression, whichwe call model blending because the teacher and student models are similar incomplexity but different in inductive bias. Blending further improves theaccuracy of our CNN, yielding a computationally efficient model of accuracyhigher than any of the other individual models. Examining the effect of "darkknowledge" in this model compression task, we find that less than 1% of thehighest probability labels are needed for accurate model compression.
arxiv-15900-112 | Sequential ranking under random semi-bandit feedback | http://arxiv.org/pdf/1603.01450v1.pdf | author:Hossein Vahabi, Paul Lagrée, Claire Vernade, Olivier Cappé category:cs.DS cs.LG published:2016-03-04 summary:In many web applications, a recommendation is not a single item suggested toa user but a list of possibly interesting contents that may be ranked in somecontexts. The combinatorial bandit problem has been studied quite extensivelythese last two years and many theoretical results now exist : lower bounds onthe regret or asymptotically optimal algorithms. However, because of thevariety of situations that can be considered, results are designed to solve theproblem for a specific reward structure such as the Cascade Model. The presentwork focuses on the problem of ranking items when the user is allowed to clickon several items while scanning the list from top to bottom.
arxiv-15900-113 | Censoring Representations with an Adversary | http://arxiv.org/pdf/1511.05897v3.pdf | author:Harrison Edwards, Amos Storkey category:cs.LG cs.AI stat.ML published:2015-11-18 summary:In practice, there are often explicit constraints on what representations ordecisions are acceptable in an application of machine learning. For example itmay be a legal requirement that a decision must not favour a particular group.Alternatively it can be that that representation of data must not haveidentifying information. We address these two related issues by learningflexible representations that minimize the capability of an adversarial critic.This adversary is trying to predict the relevant sensitive variable from therepresentation, and so minimizing the performance of the adversary ensuresthere is little or no information in the representation about the sensitivevariable. We demonstrate this adversarial approach on two problems: makingdecisions free from discrimination and removing private information fromimages. We formulate the adversarial model as a minimax problem, and optimizethat minimax objective using a stochastic gradient alternate min-max optimizer.We demonstrate the ability to provide discriminant free representations forstandard test problems, and compare with previous state of the art methods forfairness, showing statistically significant improvement across most cases. Theflexibility of this method is shown via a novel problem: removing annotationsfrom images, from unaligned training examples of annotated and unannotatedimages, and with no a priori knowledge of the form of annotation provided tothe model.
arxiv-15900-114 | Implicit LOD for processing, visualisation and classification in Point Cloud Servers | http://arxiv.org/pdf/1602.06920v2.pdf | author:Rémi Cura, Julien Perret, Nicolas Paparoditis category:cs.CG cs.CV cs.SE published:2016-02-22 summary:We propose a new paradigm to effortlessly get a portable geometric Level OfDetails (LOD) for a point cloud inside a Point Cloud Server. The point cloud isdivided into groups of points (patch), then each patch is reordered (MidOcordering) so that reading points following this order provides more and moredetails on the patch. This LOD have then multiple applications: point cloudsize reduction for visualisation (point cloud streaming) or speeding of slowalgorithm, fast density peak detection and correction as well as safeguard formethods that may be sensible to density variations. The LOD method also embedsinformation about the sensed object geometric nature, and thus can be used as acrude multi-scale dimensionality descriptor, enabling fast classification andon-the-fly filtering for basic classes.
arxiv-15900-115 | Right Ideals of a Ring and Sublanguages of Science | http://arxiv.org/pdf/1603.01032v2.pdf | author:Javier Arias Navarro category:cs.CL published:2016-03-03 summary:Among Zellig Harris's numerous contributions to linguistics his theory of thesublanguages of science probably ranks among the most underrated. However, notonly has this theory led to some exhaustive and meaningful applications in thestudy of the grammar of immunology language and its changes over time, but italso illustrates the nature of mathematical relations between chunks or subsetsof a grammar and the language as a whole. This becomes most clear when dealingwith the connection between metalanguage and language, as well as whenreflecting on operators. This paper tries to justify the claim that the sublanguages of science standin a particular algebraic relation to the rest of the language they areembedded in, namely, that of right ideals in a ring.
arxiv-15900-116 | Dynamic Memory Networks for Visual and Textual Question Answering | http://arxiv.org/pdf/1603.01417v1.pdf | author:Caiming Xiong, Stephen Merity, Richard Socher category:cs.NE cs.CL cs.CV published:2016-03-04 summary:Neural network architectures with memory and attention mechanisms exhibitcertain reasoning capabilities required for question answering. One sucharchitecture, the dynamic memory network (DMN), obtained high accuracy on avariety of language tasks. However, it was not shown whether the architectureachieves strong results for question answering when supporting facts are notmarked during training or whether it could be applied to other modalities suchas images. Based on an analysis of the DMN, we propose several improvements toits memory and input modules. Together with these changes we introduce a novelinput module for images in order to be able to answer visual questions. Our newDMN+ model improves the state of the art on both the Visual Question Answeringdataset and the \babi-10k text question-answering dataset without supportingfact supervision.
arxiv-15900-117 | Lasso estimation for GEFCom2014 probabilistic electric load forecasting | http://arxiv.org/pdf/1603.01376v1.pdf | author:Florian Ziel, Bidong Liu category:stat.AP stat.ML G.3; I.5 published:2016-03-04 summary:We present a methodology for probabilistic load forecasting that is based onlasso (least absolute shrinkage and selection operator) estimation. The modelconsidered can be regarded as a bivariate time-varying thresholdautoregressive(AR) process for the hourly electric load and temperature. Thejoint modeling approach incorporates the temperature effects directly, andreflects daily, weekly, and annual seasonal patterns and public holidayeffects. We provide two empirical studies, one based on the probabilistic loadforecasting track of the Global Energy Forecasting Competition 2014(GEFCom2014-L), and the other based on another recent probabilistic loadforecasting competition that follows a setup similar to that of GEFCom2014-L.In both empirical case studies, the proposed methodology outperforms twomultiple linear regression based benchmarks from among the top eight entries toGEFCom2014-L.
arxiv-15900-118 | A Unified View of Localized Kernel Learning | http://arxiv.org/pdf/1603.01374v1.pdf | author:John Moeller, Sarathkrishna Swaminathan, Suresh Venkatasubramanian category:cs.LG stat.ML published:2016-03-04 summary:Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting tolearn not only a classifier/regressor but also the best kernel for the trainingtask, usually from a combination of existing kernel functions. Most MKL methodsseek the combined kernel that performs best over every training example,sacrificing performance in some areas to seek a global optimum. Localizedkernel learning (LKL) overcomes this limitation by allowing the trainingalgorithm to match a component kernel to the examples that can exploit it best.Several approaches to the localized kernel learning problem have been exploredin the last several years. We unify many of these approaches under one simplesystem and design a new algorithm with improved performance. We also developenhanced versions of existing algorithms, with an eye on scalability andperformance.
arxiv-15900-119 | Gradient Descent Converges to Minimizers | http://arxiv.org/pdf/1602.04915v2.pdf | author:Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht category:stat.ML cs.LG math.OC published:2016-02-16 summary:We show that gradient descent converges to a local minimizer, almost surelywith random initialization. This is proved by applying the Stable ManifoldTheorem from dynamical systems theory.
arxiv-15900-120 | Learning deep representation of multityped objects and tasks | http://arxiv.org/pdf/1603.01359v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.CV cs.LG published:2016-03-04 summary:We introduce a deep multitask architecture to integrate multitypedrepresentations of multimodal objects. This multitype exposition is lessabstract than the multimodal characterization, but more machine-friendly, andthus is more precise to model. For example, an image can be described bymultiple visual views, which can be in the forms of bag-of-words (counts) orcolor/texture histograms (real-valued). At the same time, the image may haveseveral social tags, which are best described using a sparse binary vector. Ourdeep model takes as input multiple type-specific features, narrows thecross-modality semantic gaps, learns cross-type correlation, and produces ahigh-level homogeneous representation. At the same time, the model supportsheterogeneously typed tasks. We demonstrate the capacity of the model on twoapplications: social image retrieval and multiple concept prediction. The deeparchitecture produces more compact representation, naturally integratesmultiviews and multimodalities, exploits better side information, and mostimportantly, performs competitively against baselines.
arxiv-15900-121 | Training Input-Output Recurrent Neural Networks through Spectral Methods | http://arxiv.org/pdf/1603.00954v2.pdf | author:Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML published:2016-03-03 summary:We consider the problem of training input-output recurrent neural networks(RNN) for sequence labeling tasks. We propose a novel spectral approach forlearning the network parameters. It is based on decomposition of thecross-moment tensor between the output and a non-linear transformation of theinput, based on score functions. We guarantee consistent learning withpolynomial sample and computational complexity under transparent conditionssuch as non-degeneracy of model parameters, polynomial activations for theneurons, and a Markovian evolution of the input sequence. We also extend ourresults to Bidirectional RNN which uses both previous and future information tooutput the label at each time point, and is employed in many NLP tasks such asPOS tagging.
arxiv-15900-122 | Joint Learning Templates and Slots for Event Schema Induction | http://arxiv.org/pdf/1603.01333v1.pdf | author:Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui category:cs.CL published:2016-03-04 summary:Automatic event schema induction (AESI) means to extract meta-event from rawtext, in other words, to find out what types (templates) of event may exist inthe raw text and what roles (slots) may exist in each event type. In thispaper, we propose a joint entity-driven model to learn templates and slotssimultaneously based on the constraints of templates and slots in the samesentence. In addition, the entities' semantic information is also consideredfor the inner connectivity of the entities. We borrow the normalized cutcriteria in image segmentation to divide the entities into more accuratetemplate clusters and slot clusters. The experiment shows that our model gainsa relatively higher result than previous work.
arxiv-15900-123 | Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines | http://arxiv.org/pdf/1508.00451v4.pdf | author:Rein Houthooft, Filip De Turck category:stat.ML cs.CV cs.LG cs.NE published:2015-08-03 summary:Tackling pattern recognition problems in areas such as computer vision,bioinformatics, speech or text recognition is often done best by taking intoaccount task-specific statistical relations between output variables. Instructured prediction, this internal structure is used to predict multipleoutputs simultaneously, leading to more accurate and coherent predictions.Structural support vector machines (SSVMs) are nonprobabilistic models thatoptimize a joint input-output function through margin-based learning. BecauseSSVMs generally disregard the interplay between unary and interaction factorsduring the training phase, final parameters are suboptimal. Moreover, itsfactors are often restricted to linear combinations of input features, limitingits generalization power. To improve prediction accuracy, this paper proposes:(i) Joint inference and learning by integration of back-propagation andloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVMfactors to neural networks that form highly nonlinear functions of inputfeatures. Image segmentation benchmark results demonstrate improvements overconventional SSVM training methods in terms of accuracy, highlighting thefeasibility of end-to-end SSVM training with neural factors.
arxiv-15900-124 | Decision Forests, Convolutional Networks and the Models in-Between | http://arxiv.org/pdf/1603.01250v1.pdf | author:Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, Antonio Criminisi category:cs.CV cs.AI published:2016-03-03 summary:This paper investigates the connections between two state of the artclassifiers: decision forests (DFs, including decision jungles) andconvolutional neural networks (CNNs). Decision forests are computationallyefficient thanks to their conditional computation property (computation isconfined to only a small region of the tree, the nodes along a single branch).CNNs achieve state of the art accuracy, thanks to their representation learningcapabilities. We present a systematic analysis of how to fuse conditionalcomputation with representation learning and achieve a continuum of hybridmodels with different ratios of accuracy vs. efficiency. We call this newfamily of hybrid models conditional networks. Conditional networks can bethought of as: i) decision trees augmented with data transformation operators,or ii) CNNs, with block-diagonal sparse weight matrices, and explicit datarouting functions. Experimental validation is performed on the common task ofimage classification on both the CIFAR and Imagenet datasets. Compared to stateof the art CNNs, our hybrid models yield the same accuracy with a fraction ofthe compute cost and much smaller number of parameters.
arxiv-15900-125 | Multilingual Relation Extraction using Compositional Universal Schema | http://arxiv.org/pdf/1511.06396v2.pdf | author:Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, Andrew McCallum category:cs.CL cs.LG published:2015-11-19 summary:Universal schema builds a knowledge base (KB) of entities and relations byjointly embedding all relation types from input KBs as well as textual patternsexpressing relations from raw text. In most previous applications of universalschema, each textual pattern is represented as a single embedding, preventinggeneralization to unseen patterns. Recent work employs a neural network tocapture patterns' compositional semantics, providing generalization to allpossible input text. In response, this paper introduces significant furtherimprovements to the coverage and flexibility of universal schema relationextraction: predictions for entities unseen in training and multilingualtransfer learning to domains with no annotation. We evaluate our model throughextensive experiments on the English and Spanish TAC KBP benchmark,outperforming the top system from TAC 2013 slot-filling using no handwrittenpatterns or additional annotation. We also consider a multilingual setting inwhich English training data entities overlap with the seed KB, but Spanish textdoes not. Despite having no annotation for Spanish data, we train an accuratepredictor, with additional improvements obtained by tying word embeddingsacross languages. Furthermore, we find that multilingual training improvesEnglish relation extraction accuracy. Our approach is thus suited tobroad-coverage automated knowledge base construction in a variety of languagesand domains.
arxiv-15900-126 | Convex Relaxation Regression: Black-Box Optimization of Smooth Functions by Learning Their Convex Envelopes | http://arxiv.org/pdf/1602.02191v3.pdf | author:Mohammad Gheshlaghi Azar, Eva Dyer, Konrad Kording category:stat.ML cs.LG published:2016-02-05 summary:Finding efficient and provable methods to solve non-convex optimizationproblems is an outstanding challenge in machine learning and optimizationtheory. A popular approach used to tackle non-convex problems is to use convexrelaxation techniques to find a convex surrogate for the problem.Unfortunately, convex relaxations typically must be found on aproblem-by-problem basis. Thus, providing a general-purpose strategy toestimate a convex relaxation would have a wide reaching impact. Here, weintroduce Convex Relaxation Regression (CoRR), an approach for learning convexrelaxations for a class of smooth functions. The main idea behind our approachis to estimate the convex envelope of a function $f$ by evaluating $f$ at a setof $T$ random points and then fitting a convex function to these functionevaluations. We prove that with probability greater than $1-\delta$, thesolution of our algorithm converges to the global optimizer of $f$ with error$\mathcal{O} \Big( \big(\frac{\log(1/\delta) }{T} \big)^{\alpha} \Big)$ forsome $\alpha> 0$. Our approach enables the use of convex optimization tools tosolve a class of non-convex optimization problems.
arxiv-15900-127 | Multi-domain Neural Network Language Generation for Spoken Dialogue Systems | http://arxiv.org/pdf/1603.01232v1.pdf | author:Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL published:2016-03-03 summary:Moving from limited-domain natural language generation (NLG) to open domainis difficult because the number of semantic input combinations growsexponentially with the number of domains. Therefore, it is important toleverage existing resources and exploit similarities between domains tofacilitate domain adaptation. In this paper, we propose a procedure to trainmulti-domain, Recurrent Neural Network-based (RNN) language generators viamultiple adaptation steps. In this procedure, a model is first trained oncounterfeited data synthesised from an out-of-domain dataset, and then finetuned on a small set of in-domain utterances with a discriminative objectivefunction. Corpus-based evaluation results show that the proposed procedure canachieve competitive performance in terms of BLEU score and slot error ratewhile significantly reducing the data needed to train generators in new, unseendomains. In subjective testing, human judges confirm that the procedure greatlyimproves generator performance when only a small amount of data is available inthe domain.
arxiv-15900-128 | Comparative Design Space Exploration of Dense and Semi-Dense SLAM | http://arxiv.org/pdf/1509.04648v3.pdf | author:M. Zeeshan Zia, Luigi Nardi, Andrew Jack, Emanuele Vespa, Bruno Bodin, Paul H. J. Kelly, Andrew J. Davison category:cs.RO cs.CV published:2015-09-15 summary:SLAM has matured significantly over the past few years, and is beginning toappear in serious commercial products. While new SLAM systems are beingproposed at every conference, evaluation is often restricted to qualitativevisualizations or accuracy estimation against a ground truth. This is due tothe lack of benchmarking methodologies which can holistically andquantitatively evaluate these systems. Further investigation at the level ofindividual kernels and parameter spaces of SLAM pipelines is non-existent,which is absolutely essential for systems research and integration. We extendthe recently introduced SLAMBench framework to allow comparing twostate-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along themetrics of accuracy, energy consumption, and processing frame rate on twodifferent hardware platforms, namely a desktop and an embedded device. We alsoanalyze the pipelines at the level of individual kernels and explore theiralgorithmic and hardware design spaces for the first time, yielding valuableinsights.
arxiv-15900-129 | Rank Aggregation for Course Sequence Discovery | http://arxiv.org/pdf/1603.02695v1.pdf | author:Mihai Cucuringu, Charlie Marshak, Dillon Montag, Puck Rombach category:cs.LG published:2016-03-03 summary:In this work, we adapt the rank aggregation framework for the discovery ofoptimal course sequences at the university level. Each student provides apartial ranking of the courses taken throughout his or her undergraduatecareer. We compute pairwise rank comparisons between courses based on the orderstudents typically take them, aggregate the results over the entire studentpopulation, and then obtain a proxy for the rank offset between pairs ofcourses. We extract a global ranking of the courses via several state-of-theart algorithms for ranking with pairwise noisy information, includingSerialRank, Rank Centrality, and the recent SyncRank based on the groupsynchronization problem. We test this application of rank aggregation on 15years of student data from the Department of Mathematics at the University ofCalifornia, Los Angeles (UCLA). Furthermore, we experiment with the aboveapproach on different subsets of the student population conditioned on finalGPA, and highlight several differences in the obtained rankings that uncoverhidden pre-requisites in the Mathematics curriculum.
arxiv-15900-130 | Overdispersed Black-Box Variational Inference | http://arxiv.org/pdf/1603.01140v1.pdf | author:Francisco J. R. Ruiz, Michalis K. Titsias, David M. Blei category:stat.ML published:2016-03-03 summary:We introduce overdispersed black-box variational inference, a method toreduce the variance of the Monte Carlo estimator of the gradient in black-boxvariational inference. Instead of taking samples from the variationaldistribution, we use importance sampling to take samples from an overdisperseddistribution in the same exponential family as the variational approximation.Our approach is general since it can be readily applied to any exponentialfamily distribution, which is the typical choice for the variationalapproximation. We run experiments on two non-conjugate probabilistic models toshow that our method effectively reduces the variance, and the overheadintroduced by the computation of the proposal parameters and the importanceweights is negligible. We find that our overdispersed importance samplingscheme provides lower variance than black-box variational inference, even whenthe latter uses twice the number of samples. This results in faster convergenceof the black-box inference procedure.
arxiv-15900-131 | Deep Reinforcement Learning from Self-Play in Imperfect-Information Games | http://arxiv.org/pdf/1603.01121v1.pdf | author:Johannes Heinrich, David Silver category:cs.LG cs.AI cs.GT published:2016-03-03 summary:Many real-world applications can be described as large-scale games ofimperfect information. To deal with these challenging domains, prior work hasfocused on computing Nash equilibria in a handcrafted abstraction of thedomain. In this paper we introduce the first scalable end-to-end approach tolearning approximate Nash equilibria without any prior knowledge. Our methodcombines fictitious self-play with deep reinforcement learning. When applied toLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,whereas common reinforcement learning methods diverged. In Limit Texas Holdem,a poker game of real-world scale, NFSP learnt a competitive strategy thatapproached the performance of human experts and state-of-the-art methods.
arxiv-15900-132 | Elastic Net Hypergraph Learning for Image Clustering and Semi-supervised Classification | http://arxiv.org/pdf/1603.01096v1.pdf | author:Qingshan Liu, Yubao Sun, Cantian Wang, Tongliang Liu, Dacheng Tao category:cs.CV published:2016-03-03 summary:Graph model is emerging as a very effective tool for learning the complexstructures and relationships hidden in data. Generally, the critical purpose ofgraph-oriented learning algorithms is to construct an informative graph forimage clustering and classification tasks. In addition to the classical$K$-nearest-neighbor and $r$-neighborhood methods for graph construction,$l_1$-graph and its variants are emerging methods for finding the neighboringsamples of a center datum, where the corresponding ingoing edge weights aresimultaneously derived by the sparse reconstruction coefficients of theremaining samples. However, the pair-wise links of $l_1$-graph are not capableof capturing the high order relationships between the center datum and itsprominent data in sparse reconstruction. Meanwhile, from the perspective ofvariable selection, the $l_1$ norm sparse constraint, regarded as a LASSOmodel, tends to select only one datum from a group of data that are highlycorrelated and ignore the others. To simultaneously cope with these drawbacks,we propose a new elastic net hypergraph learning model, which consists of twosteps. In the first step, the Robust Matrix Elastic Net model is constructed tofind the canonically related samples in a somewhat greedy way, achieving thegrouping effect by adding the $l_2$ penalty to the $l_1$ constraint. In thesecond step, hypergraph is used to represent the high order relationshipsbetween each datum and its prominent samples by regarding them as a hyperedge.Subsequently, hypergraph Laplacian matrix is constructed for further analysis.New hypergraph learning algorithms, including unsupervised clustering andmulti-class semi-supervised classification, are then derived. Extensiveexperiments on face and handwriting databases demonstrate the effectiveness ofthe proposed method.
arxiv-15900-133 | Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition | http://arxiv.org/pdf/1512.04280v2.pdf | author:Liang Lu, Steve Renals category:cs.CL cs.LG cs.NE published:2015-12-14 summary:For speech recognition, deep neural networks (DNNs) have significantlyimproved the recognition accuracy in most of benchmark datasets and applicationdomains. However, compared to the conventional Gaussian mixture models,DNN-based acoustic models usually have much larger number of model parameters,making it challenging for their applications in resource constrained platforms,e.g., mobile devices. In this paper, we study the application of the recentlyproposed highway network to train small-footprint DNNs, which are {\it thinner}and {\it deeper}, and have significantly smaller number of model parameterscompared to conventional DNNs. We investigated this approach on the AMI meetingspeech transcription corpus which has around 70 hours of audio data. Thehighway neural networks constantly outperformed their plain DNN counterparts,and the number of model parameters can be reduced significantly withoutsacrificing the recognition accuracy.
arxiv-15900-134 | Camera identification with deep convolutional networks | http://arxiv.org/pdf/1603.01068v1.pdf | author:Luca Baroffio, Luca Bondi, Paolo Bestagini, Stefano Tubaro category:cs.CV cs.MM published:2016-03-03 summary:The possibility of detecting which camera has been used to shoot a specificpicture is of paramount importance for many forensics tasks. This is extremelyuseful for copyright infringement cases, ownership attribution, as well as fordetecting the authors of distributed illicit material (e.g., pedo-pornographicshots). Due to its importance, the forensics community has developed a seriesof robust detectors that exploit characteristic traces left by each camera onthe acquired images during the acquisition pipeline. These traces arereverse-engineered in order to attribute a picture to a camera. In this paper,we investigate an alternative approach to solve camera identification problem.Indeed, we propose a data-driven algorithm based on convolutional neuralnetworks, which learns features characterizing each camera directly from theacquired pictures. The proposed approach is tested on both instance-attributionand model-attribution, providing an accuracy greater than 94% in discriminating27 camera models.
arxiv-15900-135 | Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain Decoding | http://arxiv.org/pdf/1603.01067v1.pdf | author:Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural category:cs.LG cs.AI cs.CV published:2016-03-03 summary:We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)brain volumes recorded during a cognitive stimulus by a graph which consists ofa set of local meshes. The corresponding cognitive process, encoded in thebrain, is then represented by these meshes each of which is estimated assuminga linear relationship among the voxel time series in a predefined locality.First, we define the concept of locality in two neighborhood systems, namely,the spatial and functional neighborhoods. Then, we construct spatially andfunctionally local meshes around each voxel, called seed voxel, by connectingit either to its spatial or functional p-nearest neighbors. The mesh formedaround a voxel is a directed sub-graph with a star topology, where thedirection of the edges is taken towards the seed voxel at the center of themesh. We represent the time series recorded at each seed voxel in terms oflinear combination of the time series of its p-nearest neighbors in the mesh.The relationships between a seed voxel and its neighbors are represented by theedge weights of each mesh, and are estimated by solving a linear regressionequation. The estimated mesh edge weights lead to a better representation ofinformation in the brain for encoding and decoding of the cognitive tasks. Wetest our model on a visual object recognition and emotional memory retrievalexperiments using Support Vector Machines that are trained using the mesh edgeweights as features. In the experimental analysis, we observe that the edgeweights of the spatial and functional meshes perform better than thestate-of-the-art brain decoding models.
arxiv-15900-136 | Knowledge Base Population using Semantic Label Propagation | http://arxiv.org/pdf/1511.06219v2.pdf | author:Lucas Sterckx, Thomas Demeester, Johannes Deleu, Chris Develder category:cs.CL cs.LG published:2015-11-19 summary:A crucial aspect of a knowledge base population system that extracts newfacts from text corpora, is the generation of training data for its relationextractors. In this paper, we present a method that maximizes the effectivenessof newly trained relation extractors at a minimal annotation cost. Manuallabeling can be significantly reduced by Distant Supervision, which is a methodto construct training data automatically by aligning a large text corpus withan existing knowledge base of known facts. For example, all sentencesmentioning both 'Barack Obama' and 'US' may serve as positive traininginstances for the relation born_in(subject,object). However, distantsupervision typically results in a highly noisy training set: many trainingsentences do not really express the intended relation. We propose to combinedistant supervision with minimal manual supervision in a technique calledfeature labeling, to eliminate noise from the large and noisy initial trainingset, resulting in a significant increase of precision. We further improve onthis approach by introducing the Semantic Label Propagation method, which usesthe similarity between low-dimensional representations of candidate traininginstances, to extend the training set in order to increase recall whilemaintaining high precision. Our proposed strategy for generating training datais studied and evaluated on an established test collection designed forknowledge base population tasks. The experimental results show that theSemantic Label Propagation strategy leads to substantial performance gains whencompared to existing approaches, while requiring an almost negligible manualannotation effort.
arxiv-15900-137 | A novel and automatic pectoral muscle identification algorithm for mediolateral oblique (MLO) view mammograms using ImageJ | http://arxiv.org/pdf/1603.01056v1.pdf | author:Chao Wang category:cs.CV published:2016-03-03 summary:Pectoral muscle identification is often required for breast cancer riskanalysis, such as estimating breast density. Traditional methods areoverwhelmingly based on manual visual assessment or straight line fitting forthe pectoral muscle boundary, which are inefficient and inaccurate sincepectoral muscle in mammograms can have curved boundaries. This paper proposes a novel and automatic pectoral muscle identificationalgorithm for MLO view mammograms. It is suitable for both scanned film andfull field digital mammograms. This algorithm is demonstrated using a publicdomain software ImageJ. A validation of this algorithm has been performed usingreal-world data and it shows promising result.
arxiv-15900-138 | Photographic dataset: random peppercorns | http://arxiv.org/pdf/1603.01046v1.pdf | author:Teemu Helenius, Samuli Siltanen category:cs.CV published:2016-03-03 summary:This is a photographic dataset collected for testing image processingalgorithms. The idea is to have sets of different but statistically similarimages. In this work the images show randomly distributed peppercorns. Thedataset is made available at www.fips.fi/photographic_dataset.php .
arxiv-15900-139 | Whitening-Free Least-Squares Non-Gaussian Component Analysis | http://arxiv.org/pdf/1603.01029v1.pdf | author:Hiroaki Shiino, Hiroaki Sasaki, Gang Niu, Masashi Sugiyama category:stat.ML published:2016-03-03 summary:Non-Gaussian component analysis (NGCA) is an unsupervised linear dimensionreduction method that extracts low-dimensional non-Gaussian "signals" fromhigh-dimensional data contaminated with Gaussian noise. NGCA can be regarded asa generalization of projection pursuit (PP) and independent component analysis(ICA) to multi-dimensional and dependent non-Gaussian components. Indeed,seminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCAapproach called least-squares NGCA (LSNGCA) has been developed, which gives asolution analytically through least-squares estimation of log-density gradientsand eigendecomposition. However, since pre-whitening of data is involved inLSNGCA, it performs unreliably when the data covariance matrix isill-conditioned, which is often the case in high-dimensional data analysis. Inthis paper, we propose a whitening-free LSNGCA method and experimentallydemonstrate its superiority.
arxiv-15900-140 | Automatic learning of gait signatures for people identification | http://arxiv.org/pdf/1603.01006v1.pdf | author:F. M. Castro, M. J. Marin-Jimenez, N. Guil, N. Perez de la Blanca category:cs.CV cs.AI published:2016-03-03 summary:This work targets people identification in video based on the way they walk(i.e. gait). While classical methods typically derive gait signatures fromsequences of binary silhouettes, in this work we explore the use ofconvolutional neural networks (CNN) for learning high-level descriptors fromlow-level motion features (i.e. optical flow components). We carry out athorough experimental evaluation of the proposed CNN architecture on thechallenging TUM-GAID dataset. The experimental results indicate that usingspatio-temporal cuboids of optical flow as input data for CNN allows to obtainstate-of-the-art results on the gait task with an image resolution eight timeslower than the previously reported results (i.e. 80x60 pixels).
arxiv-15900-141 | Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization | http://arxiv.org/pdf/1603.00448v2.pdf | author:Chelsea Finn, Sergey Levine, Pieter Abbeel category:cs.LG cs.AI cs.RO published:2016-03-01 summary:Reinforcement learning can acquire complex behaviors from high-levelspecifications. However, defining a cost function that can be optimizedeffectively and encodes the correct task is challenging in practice. We explorehow inverse optimal control (IOC) can be used to learn behaviors fromdemonstrations, with applications to torque control of high-dimensional roboticsystems. Our method addresses two key challenges in inverse optimal control:first, the need for informative features and effective regularization to imposestructure on the cost, and second, the difficulty of learning the cost functionunder unknown dynamics for high-dimensional continuous systems. To address theformer challenge, we present an algorithm capable of learning arbitrarynonlinear cost functions, such as neural networks, without meticulous featureengineering. To address the latter challenge, we formulate an efficientsample-based approximation for MaxEnt IOC. We evaluate our method on a seriesof simulated tasks and real-world robotic manipulation problems, demonstratingsubstantial improvement over prior methods both in terms of task complexity andsample efficiency.
arxiv-15900-142 | Self-localization from Images with Small Overlap | http://arxiv.org/pdf/1603.00993v1.pdf | author:Tanaka Kanji category:cs.CV published:2016-03-03 summary:With the recent success of visual features from deep convolutional neuralnetworks (DCNN) in visual robot self-localization, it has become important andpractical to address more general self-localization scenarios. In this paper,we address the scenario of self-localization from images with small overlap. Weexplicitly introduce a localization difficulty index as a decreasing functionof view overlap between query and relevant database images and investigateperformance versus difficulty for challenging cross-view self-localizationtasks. We then reformulate the self-localization as a scalablebag-of-visual-features (BoVF) scene retrieval and present an efficient solutioncalled PCA-NBNN, aiming to facilitate fast and yet discriminativecorrespondence between partially overlapping images. The proposed approachadopts recent findings in discriminativity preserving encoding of DCNN featuresusing principal component analysis (PCA) and cross-domain scene matching usingnaive Bayes nearest neighbor distance metric (NBNN). We experimentallydemonstrate that the proposed PCA-NBNN framework frequently achieves comparableresults to previous DCNN features and that the BoVF model is significantly moreefficient. We further address an important alternative scenario of"self-localization from images with NO overlap" and report the result.
arxiv-15900-143 | Covariate-assisted spectral clustering | http://arxiv.org/pdf/1411.2158v4.pdf | author:Norbert Binkiewicz, Joshua T. Vogelstein, Karl Rohe category:stat.ML cs.LG math.ST stat.ME stat.TH published:2014-11-08 summary:Biological and social systems consist of myriad interacting units. Theinteractions can be represented in the form of a graph or network. Measurementsof these graphs can reveal the underlying structure of these interactions,which provides insight into the systems that generated the graphs. Moreover, inapplications such as connectomics, social networks, and genomics, graph dataare accompanied by contextualizing measures on each node. We utilize these nodecovariates to help uncover latent communities in a graph, using a modificationof spectral clustering. Statistical guarantees are provided under a jointmixture model that we call the node-contextualized stochastic blockmodel,including a bound on the mis-clustering rate. For most simulated conditions,covariate-assisted spectral clustering yields results superior to regularizedspectral clustering without node covariates and to an adaptation of canonicalcorrelation analysis. We apply our clustering method to large brain graphsderived from diffusion MRI data, using the node locations or neurologicalregion membership as covariates. In both cases, covariate-assisted spectralclustering yields clusters that are easier to interpret neurologically.
arxiv-15900-144 | Sparse Precision Matrix Selection for Fitting Gaussian Random Field Models to Large Data Sets | http://arxiv.org/pdf/1405.5576v4.pdf | author:Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique Del Castillo category:stat.ML stat.CO published:2014-05-21 summary:Iterative methods for fitting a Gaussian Random Field (GRF) model to spatialdata via maximum likelihood (ML) require $\mathcal{O}(n^3)$ floating pointoperations per iteration, where $n$ denotes the number of data locations. Forlarge data sets, the $\mathcal{O}(n^3)$ complexity per iteration together withthe non-convexity of the ML problem render traditional ML methods inefficientfor GRF fitting. The problem is even more aggravated for anisotropic GRFs wherethe number of covariance function parameters increases with the process domaindimension. In this paper, we propose a new two-step GRF estimation procedurewhen the process is second-order stationary. First, a \emph{convex} likelihoodproblem regularized with a weighted $\ell_1$-norm, utilizing the availabledistance information between observation locations, is solved to fit a sparse\emph{{precision} (inverse covariance) matrix to the observed data using theAlternating Direction Method of Multipliers. Second, the parameters of the GRFspatial covariance function are estimated by solving a least squares problem.Theoretical error bounds for the proposed estimator are provided; moreover,convergence of the estimator is shown as the number of samples per locationincreases. The proposed method is numerically compared with state-of-the-artmethods for big $n$. Data segmentation schemes are implemented to handle largedata sets.
arxiv-15900-145 | Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via User-Defined Templates | http://arxiv.org/pdf/1603.00961v1.pdf | author:Tobias Lüddemann, Jan Egger category:cs.CV cs.GR published:2016-03-03 summary:Among all types of cancer, gynecological malignancies belong to the 4th mostfrequent type of cancer among women. Besides chemotherapy and external beamradiation, brachytherapy is the standard procedure for the treatment of thesemalignancies. In the progress of treatment planning, localization of the tumoras the target volume and adjacent organs of risks by segmentation is crucial toaccomplish an optimal radiation distribution to the tumor while simultaneouslypreserving healthy tissue. Segmentation is performed manually and represents atime-consuming task in clinical daily routine. This study focuses on thesegmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecologicalbrachytherapy. The proposed segmentation method uses an interactive,graph-based segmentation scheme with a user-defined template. The schemecreates a directed two dimensional graph, followed by the minimal cost closedset computation on the graph, resulting in an outlining of the rectum. Thegraphs outline is dynamically adapted to the last calculated cut. Evaluationwas performed by comparing manual segmentations of the rectum/sigmoid colon toresults achieved with the proposed method. The comparison of the algorithmic tomanual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%,in comparison to 83.97+/-8.08% for the comparison of two manual segmentationsof the same physician. Utilizing the proposed methodology resulted in a mediantime of 128 seconds per dataset, compared to 300 seconds needed for pure manualsegmentation.
arxiv-15900-146 | Cellular Automata Segmentation of the Boundary between the Compacta of Vertebral Bodies and Surrounding Structures | http://arxiv.org/pdf/1603.00960v1.pdf | author:Jan Egger, Christopher Nimsky category:cs.CV cs.CG cs.GR published:2016-03-03 summary:Due to the aging population, spinal diseases get more and more commonnowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white womenand 13% for white men in the United States. Thus the numbers of surgical spinalprocedures are also increasing with the aging population and precise diagnosisplays a vital role in reducing complication and recurrence of symptoms. Spinalimaging of vertebral column is a tedious process subjected to interpretationerrors. In this contribution, we aim to reduce time and error for vertebralinterpretation by applying and studying the GrowCut-algorithm for boundarysegmentation between vertebral body compacta and surrounding structures.GrowCut is a competitive region growing algorithm using cellular automata. Forour study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans werefirst manually outlined by neurosurgeons. Then, the vertebral bodies weresegmented in the medical images by a GrowCut-trained physician using thesemi-automated GrowCut-algorithm. Afterwards, results of both segmentationprocesses were compared using the Dice Similarity Coefficient (DSC) and theHausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of18.91+/-7.2 voxel, respectively. In addition, the times have been measuredduring the manual and the GrowCut segmentations, showing that aGrowCut-segmentation - with an average time of less than six minutes(5.77+/-0.73) - is significantly shorter than a pure manual outlining.
arxiv-15900-147 | Enhancing Freebase Question Answering Using Textual Evidence | http://arxiv.org/pdf/1603.00957v1.pdf | author:Kun Xu, Yansong Feng, Siva Reddy, Songfang Huang, Dongyan Zhao category:cs.CL published:2016-03-03 summary:Existing knowledge-based question answering systems often rely on smallannotated training data. While shallow methods like information extractiontechniques are robust to data scarcity, they are less expressive than deepunderstanding methods, thereby failing at answering questions involvingmultiple constraints. Here we alleviate this problem by empowering a relationextraction method with additional evidence from Wikipedia. We first present anovel neural network based relation extractor to retrieve the candidate answersfrom Freebase, and then develop a refinement model to validate answers usingWikipedia. We achieve 53.3 F1 on WebQuestions, a substantial improvement overthe state-of-the-art.
arxiv-15900-148 | Sparse model selection in the highly under-sampled regime | http://arxiv.org/pdf/1603.00952v1.pdf | author:Nicola Bulso, Matteo Marsili, Yasser Roudi category:stat.ML published:2016-03-03 summary:We propose a method for recovering the structure of a sparse undirectedgraphical model when very few samples are available. The method decides aboutthe presence or absence of bonds between pairs of variable by considering onepair at a time and using a closed form formula, analytically derived bycalculating the posterior probability for every possible model explaining a twobody system using Jeffreys prior. The approach does not rely on theoptimisation of any cost functions and consequently is much faster thanexisting algorithms. Despite this time and computational advantage, numericalresults show that for several sparse topologies the algorithm is comparable tothe best existing algorithms, and is more accurate in the presence of hiddenvariables. We apply this approach to the analysis of US stock market data andto neural data, in order to show its efficiency in recovering robuststatistical dependencies in real data with non stationary correlations in timeand space.
arxiv-15900-149 | Estimating the number of unseen species: A bird in the hand is worth $\log n $ in the bush | http://arxiv.org/pdf/1511.07428v3.pdf | author:Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu category:math.ST stat.ML stat.TH published:2015-11-23 summary:Estimating the number of unseen species is an important problem in manyscientific endeavors. Its most popular formulation, introduced by Fisher, uses$n$ samples to predict the number $U$ of hitherto unseen species that would beobserved if $t\cdot n$ new samples were collected. Of considerable interest isthe largest ratio $t$ between the number of new and existing samples for which$U$ can be accurately predicted. In seminal works, Good and Toulmin constructed an intriguing estimator thatpredicts $U$ for all $t\le 1$, thereby showing that the number of species canbe estimated for a population twice as large as that observed. SubsequentlyEfron and Thisted obtained a modified estimator that empirically predicts $U$even for some $t>1$, but without provable guarantees. We derive a class of estimators that $\textit{provably}$ predict $U$ not justfor constant $t>1$, but all the way up to $t$ proportional to $\log n$. Thisshows that the number of species can be estimated for a population $\log n$times larger than that observed, a factor that grows arbitrarily large as $n$increases. We also show that this range is the best possible and that theestimators' mean-square error is optimal up to constants for any $t$. Ourapproach yields the first provable guarantee for the Efron-Thisted estimatorand, in addition, a variant which achieves stronger theoretical andexperimental performance than existing methodologies on a variety of syntheticand real datasets. The estimators we derive are simple linear estimators that are computable intime proportional to $n$. The performance guarantees hold uniformly for alldistributions, and apply to all four standard sampling models commonly usedacross various scientific disciplines: multinomial, Poisson, hypergeometric,and Bernoulli product.
arxiv-15900-150 | RandomOut: Using a convolutional gradient norm to win The Filter Lottery | http://arxiv.org/pdf/1602.05931v2.pdf | author:Joseph Paul Cohen, Henry Z. Lo, Wei Ding category:cs.CV published:2016-02-18 summary:Convolutional neural networks are sensitive to the random initialization offilters. We call this The Filter Lottery (TFL) because the random numbers usedto initialize the network determine if you will "win" and converge to asatisfactory local minimum. This issue forces networks to contain more filters(be wider) to achieve higher accuracy because they have better odds of beingtransformed into highly discriminative features at the risk of introducingredundant features. To deal with this, we propose to evaluate and replacespecific convolutional filters that have little impact on the prediction. Weuse the gradient norm to evaluate the impact of a filter on error, andre-initialize filters when the gradient norm of its weights falls below aspecific threshold. This consistently improves accuracy across two datasets byup to 1.8%. Our scheme RandomOut allows us to increase the number of filtersexplored without increasing the size of the network. This yields more compactnetworks which can train and predict with less computation, thus allowing morepowerful CNNs to run on mobile devices.
arxiv-15900-151 | PCANet: An energy perspective | http://arxiv.org/pdf/1603.00944v1.pdf | author:Jiasong Wu, Shijie Qiu, Youyong Kong, Longyu Jiang, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2016-03-03 summary:The principal component analysis network (PCANet), which is one of therecently proposed deep learning architectures, achieves the state-of-the-artclassification accuracy in various databases. However, the explanation of thePCANet is lacked. In this paper, we try to explain why PCANet works well fromenergy perspective point of view based on a set of experiments. The impact ofvarious parameters on the error rate of PCANet is analyzed in depth. It wasfound that this error rate is correlated with the logarithm of energy of image.The proposed energy explanation approach can be used as a testing method forchecking if every step of the constructed networks is necessary.
arxiv-15900-152 | LiDAR Ground Filtering Algorithm for Urban Areas Using Scan Line Based Segmentation | http://arxiv.org/pdf/1603.00912v1.pdf | author:Lei Wang, Yongun Zhang category:cs.CV published:2016-03-02 summary:This paper addresses the task of separating ground points from airborne LiDARpoint cloud data in urban areas. A novel ground filtering method using scanline segmentation is proposed here, which we call SLSGF. It utilizes the scanline information in LiDAR data to segment the LiDAR data. The similaritymeasurements are designed to make it possible to segment complex roofstructures into a single segment as much as possible so the topologicalrelationships between the roof and the ground are simpler, which will benefitthe labeling process. In the labeling process, the initial ground segments aredetected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011are used to test the accuracy of SLSGF; and our analytical and experimentalresults show that this method is computationally-efficient andnoise-insensitive, thereby making a denoising process unnecessary beforefiltering.
arxiv-15900-153 | Counter-fitting Word Vectors to Linguistic Constraints | http://arxiv.org/pdf/1603.00892v1.pdf | author:Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Lina Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG published:2016-03-02 summary:In this work, we present a novel counter-fitting method which injectsantonymy and synonymy constraints into vector space representations in order toimprove the vectors' capability for judging semantic similarity. Applying thismethod to publicly available pre-trained word vectors leads to a new state ofthe art performance on the SimLex-999 dataset. We also show how the method canbe used to tailor the word vector space for the downstream task of dialoguestate tracking, resulting in robust improvements across different dialoguedomains.
arxiv-15900-154 | Molecular Graph Convolutions: Moving Beyond Fingerprints | http://arxiv.org/pdf/1603.00856v1.pdf | author:Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick Riley category:stat.ML cs.LG published:2016-03-02 summary:Molecular "fingerprints" encoding structural information are the workhorse ofcheminformatics and machine learning in drug discovery applications. However,fingerprint representations necessarily emphasize particular aspects of themolecular structure while ignoring others, rather than allowing the model tomake data-driven decisions. We describe molecular graph convolutions, a novelmachine learning architecture for learning from undirected graphs, specificallysmall molecules. Graph convolutions use a simple encoding of the moleculargraph (atoms, bonds, distances, etc.), allowing the model to take greateradvantage of information in the graph structure.
arxiv-15900-155 | Shallow and Deep Convolutional Networks for Saliency Prediction | http://arxiv.org/pdf/1603.00845v1.pdf | author:Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor, Xavier Giro-i-Nieto category:cs.CV cs.LG published:2016-03-02 summary:The prediction of salient areas in images has been traditionally addressedwith hand-crafted features based on neuroscience principles. This paper,however, addresses the problem with a completely data-driven approach bytraining a convolutional neural network (convnet). The learning process isformulated as a minimization of a loss function that measures the Euclideandistance of the predicted saliency map with the provided ground truth. Therecent publication of large datasets of saliency prediction has provided enoughdata to train end-to-end architectures that are both fast and accurate. Twodesigns are proposed: a shallow convnet trained from scratch, and a anotherdeeper solution whose first three layers are adapted from another networktrained for classification. To the authors knowledge, these are the firstend-to-end CNNs trained and tested for the purpose of saliency prediction.
arxiv-15900-156 | Automatic segmentation of lizard spots using an active contour model | http://arxiv.org/pdf/1603.00841v1.pdf | author:Jhony Giraldo, Augusto Salazar category:cs.CV published:2016-03-02 summary:Animal biometrics is a challenging task. In the literature, many algorithmshave been used, e.g. penguin chest recognition, elephant ears recognition andleopard stripes pattern recognition, but to use technology to a large extent inthis area of research, still a lot of work has to be done. One important targetin animal biometrics is to automate the segmentation process, so in this paperwe propose a segmentation algorithm for extracting the spots of Diploglossusmillepunctatus, an endangered lizard species. The automatic segmentation isachieved with a combination of preprocessing, active contours and morphology.The parameters of each stage of the segmentation algorithm are found using anoptimization procedure, which is guided by the ground truth. The results showthat automatic segmentation of spots is possible. A 78.37 % of correctsegmentation in average is reached.
arxiv-15900-157 | Unconstrained Face Verification using Deep CNN Features | http://arxiv.org/pdf/1508.01722v2.pdf | author:Jun-Cheng Chen, Vishal M. Patel, Rama Chellappa category:cs.CV published:2015-08-07 summary:In this paper, we present an algorithm for unconstrained face verificationbased on deep convolutional features and evaluate it on the newly releasedIARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-worldunconstrained faces from 500 subjects with full pose and illuminationvariations which are much harder than the traditional Labeled Face in the Wild(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on theIJB-A dataset are provided.
arxiv-15900-158 | Multimodal sparse representation learning and applications | http://arxiv.org/pdf/1511.06238v3.pdf | author:Miriam Cha, Youngjune Gwon, H. T. Kung category:cs.LG cs.CV stat.ML published:2015-11-19 summary:Unsupervised methods have proven effective for discriminative tasks in asingle-modality scenario. In this paper, we present a multimodal framework forlearning sparse representations that can capture semantic correlation betweenmodalities. The framework can model relationships at a higher level by forcingthe shared sparse representation. In particular, we propose the use of jointdictionary learning technique for sparse coding and formulate the jointrepresentation for concision, cross-modal representations (in case of a missingmodality), and union of the cross-modal representations. Given the acceleratedgrowth of multimodal data posted on the Web such as YouTube, Wikipedia, andTwitter, learning good multimodal features is becoming increasingly important.We show that the shared representations enabled by our framework substantiallyimprove the classification performance under both unimodal and multimodalsettings. We further show how deep architectures built on the proposedframework are effective for the case of highly nonlinear correlations betweenmodalities. The effectiveness of our approach is demonstrated experimentally inimage denoising, multimedia event detection and retrieval on the TRECVIDdataset (audio-video), category classification on the Wikipedia dataset(image-text), and sentiment classification on PhotoTweet (image-text).
arxiv-15900-159 | Discrete Distribution Estimation under Local Privacy | http://arxiv.org/pdf/1602.07387v2.pdf | author:Peter Kairouz, Keith Bonawitz, Daniel Ramage category:stat.ML cs.LG published:2016-02-24 summary:The collection and analysis of user data drives improvements in the app andweb ecosystems, but comes with risks to privacy. This paper examines discretedistribution estimation under local privacy, a setting wherein serviceproviders can learn the distribution of a categorical statistic of interestwithout collecting the underlying data. We present new mechanisms, includinghashed K-ary Randomized Response (KRR), that empirically meet or exceed theutility of existing mechanisms at all privacy levels. New theoretical resultsdemonstrate the order-optimality of KRR and the existing RAPPOR mechanism atdifferent privacy regimes.
arxiv-15900-160 | A Nonlinear Weighted Total Variation Image Reconstruction Algorithm for Electrical Capacitance Tomography | http://arxiv.org/pdf/1603.00816v1.pdf | author:Kezhi Li, Daniel Holland category:cs.CV published:2016-03-02 summary:Based on the techniques of iterative soft thresholding on total variationpenalty and adaptive reweighted compressive sensing, a new iterativereconstruction algorithm for electrical capacitance tomography (ECT) isproposed. This algorithm encourages sharp changes in the ECT image andovercomes the disadvantage of the $l_1$ minimization by equipping the totalvariation an adaptive weighted depending on the reconstructed image. Moreover,the nonlinear effect is also partially reduced due to the adoption of theupdated accurate sensitivity matrix. Simulation results show that it recoversECT images more precisely and therefore suitable for the imaging of multiphasesystems in industrial or medical applications.
arxiv-15900-161 | Multi-armed Bandit Problem with Known Trend | http://arxiv.org/pdf/1508.07091v3.pdf | author:Djallel Bouneffouf, Raphaël Feraud category:cs.LG I.2 published:2015-08-28 summary:We consider a variant of the multi-armed bandit model, which we callmulti-armed bandit problem with known trend, where the gambler knows the shapeof the reward function of each arm but not its distribution. This new problemis motivated by different online problems like active learning, music andinterface recommendation applications, where when an arm is sampled by themodel the received reward change according to a known trend. By adapting thestandard multi-armed bandit algorithm UCB1 to take advantage of this setting,we propose the new algorithm named A-UCB that assumes a stochastic model. Weprovide upper bounds of the regret which compare favourably with the ones ofUCB1. We also confirm that experimentally with different simulations
arxiv-15900-162 | Flies as Ship Captains? Digital Evolution Unravels Selective Pressures to Avoid Collision in Drosophila | http://arxiv.org/pdf/1603.00802v1.pdf | author:Ali Tehrani-Saleh, Christoph Adami category:q-bio.PE cs.CV nlin.AO q-bio.NC published:2016-03-02 summary:Flies that walk in a covered planar arena on straight paths avoid collidingwith each other, but which of the two flies stops is not random.High-throughput video observations, coupled with dedicated experiments withcontrolled robot flies have revealed that flies utilize the type of optic flowon their retina as a determinant of who should stop, a strategy also used byship captains to determine which of two ships on a collision course shouldthrow engines in reverse. We use digital evolution to test whether thisstrategy evolves when collision avoidance is the sole penalty. We find that thestrategy does indeed evolve in a narrow range of cost/benefit ratios, forexperiments in which the "regressive motion" cue is error free. We speculatethat these stringent conditions may not be sufficient to evolve the strategy inreal flies, pointing perhaps to auxiliary costs and benefits not modeled in ourstudy
arxiv-15900-163 | Automatic Differentiation Variational Inference | http://arxiv.org/pdf/1603.00788v1.pdf | author:Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei category:stat.ML cs.AI cs.LG stat.CO published:2016-03-02 summary:Probabilistic modeling is iterative. A scientist posits a simple model, fitsit to her data, refines it according to her analysis, and repeats. However,fitting complex models to large data is a bottleneck in this process. Derivingalgorithms for new models can be both mathematically and computationallychallenging, which makes it difficult to efficiently cycle through the steps.To this end, we develop automatic differentiation variational inference (ADVI).Using our method, the scientist only provides a probabilistic model and adataset, nothing else. ADVI automatically derives an efficient variationalinference algorithm, freeing the scientist to refine and explore many models.ADVI supports a broad class of models-no conjugacy assumptions are required. Westudy ADVI across ten different models and apply it to a dataset with millionsof observations. ADVI is integrated into Stan, a probabilistic programmingsystem; it is available for immediate use.
arxiv-15900-164 | Learning Word Segmentation Representations to Improve Named Entity Recognition for Chinese Social Media | http://arxiv.org/pdf/1603.00786v1.pdf | author:Nanyun Peng, Mark Dredze category:cs.CL published:2016-03-02 summary:Named entity recognition, and other information extraction tasks, frequentlyuse linguistic features such as part of speech tags or chunkings. For languageswhere word boundaries are not readily identified in text, word segmentation isa key first step to generating features for an NER system. While using wordboundary tags as features are helpful, the signals that aid in identifyingthese boundaries may provide richer information for an NER system. Newstate-of-the-art word segmentation systems use neural models to learnrepresentations for predicting word boundaries. We show that these samerepresentations, jointly trained with an NER system, yield significantimprovements in NER for Chinese social media. In our experiments, jointlytraining NER and word segmentation with an LSTM-CRF model yields nearly 5%absolute improvement over previously published results.
arxiv-15900-165 | Equity forecast: Predicting long term stock price movement using machine learning | http://arxiv.org/pdf/1603.00751v1.pdf | author:Nikola Milosevic category:cs.LG q-fin.GN published:2016-03-02 summary:Long term investment is one of the major investment strategies. However,calculating intrinsic value of some company and evaluating shares for long terminvestment is not easy, since analyst have to care about a large number offinancial indicators and evaluate them in a right manner. So far, little helpin predicting the direction of the company value over the longer period of timehas been provided from the machines. In this paper we present a machinelearning aided approach to evaluate the equity's future price over the longtime. Our method is able to correctly predict whether some company's value willbe 10% higher or not over the period of one year in 76.5% of cases.
arxiv-15900-166 | Why Regularized Auto-Encoders learn Sparse Representation? | http://arxiv.org/pdf/1505.05561v3.pdf | author:Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju category:stat.ML cs.CV cs.LG published:2015-05-21 summary:Sparse Distributed representation is the key to learning useful features indeep learning algorithms not just because it is an efficient mode of datarepresentation, but more importantly, because it captures the generationprocess of most real world data. Although a number of regularized auto-encoders(AE) enforce sparsity explicitly in their learned representation while othersdon't, there has been little formal analysis on what encourages sparsity inthese models in general. Therefore, our objective here is to formally studythis general problem for regularized auto-encoders. We show the properties ofboth regularization and activation function that play an important role inencouraging sparsity. We provide sufficient conditions on both these criteriaand show that multiple popular models-- eg. De-noising and Contractive autoencoders-- and activations-- eg. Rectified Linear and Sigmoid-- satisfy theseconditions; thus explaining sparsity in their learned representation. Ourtheoretical and empirical analysis together, throws light on the properties ofregularization/activation that are conducive to sparsity, but also bringstogether a number of existing auto-encoder models and activation functionsunder a unified analytical framework thereby yielding deeper insights intounsupervised representation learning.
arxiv-15900-167 | Continuous Deep Q-Learning with Model-based Acceleration | http://arxiv.org/pdf/1603.00748v1.pdf | author:Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, Sergey Levine category:cs.LG cs.AI cs.RO cs.SY published:2016-03-02 summary:Model-free reinforcement learning has been successfully applied to a range ofchallenging problems, and has recently been extended to handle large neuralnetwork policies and value functions. However, the sample complexity ofmodel-free algorithms, particularly when using high-dimensional functionapproximators, tends to limit their applicability to physical systems. In thispaper, we explore algorithms and representations to reduce the samplecomplexity of deep reinforcement learning for continuous control tasks. Wepropose two complementary techniques for improving the efficiency of suchalgorithms. First, we derive a continuous variant of the Q-learning algorithm,which we call normalized adantage functions (NAF), as an alternative to themore commonly used policy gradient and actor-critic methods. NAF representationallows us to apply Q-learning with experience replay to continuous tasks, andsubstantially improves performance on a set of simulated robotic control tasks.To further improve the efficiency of our approach, we explore the use oflearned models for accelerating model-free reinforcement learning. We show thatiteratively refitted local linear models are especially effective for this, anddemonstrate substantially faster learning on domains where such models areapplicable.
arxiv-15900-168 | Probabilistic Relational Model Benchmark Generation | http://arxiv.org/pdf/1603.00709v1.pdf | author:Mouna Ben Ishak, Rajani Chulyadyo, Philippe Leray category:cs.LG cs.AI published:2016-03-02 summary:The validation of any database mining methodology goes through an evaluationprocess where benchmarks availability is essential. In this paper, we aim torandomly generate relational database benchmarks that allow to checkprobabilistic dependencies among the attributes. We are particularly interestedin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)to a relational data mining context and enable effective and robust reasoningover relational data. Even though a panoply of works have focused, separately ,on the generation of random Bayesian networks and relational databases, no workhas been identified for PRMs on that track. This paper provides an algorithmicapproach for generating random PRMs from scratch to fill this gap. The proposedmethod allows to generate PRMs as well as synthetic relational data from arandomly generated relational schema and a random set of probabilisticdependencies. This can be of interest not only for machine learning researchersto evaluate their proposals in a common framework, but also for databasesdesigners to evaluate the effectiveness of the components of a databasemanagement system.
arxiv-15900-169 | Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms | http://arxiv.org/pdf/1508.00945v3.pdf | author:Jean Honorio, Tommi Jaakkola category:stat.ML cs.LG published:2015-08-05 summary:Margin-based structured prediction commonly uses a maximum loss over allpossible structured outputs \cite{Altun03,Collins04b,Taskar03}. In naturallanguage processing, recent work \cite{Zhang14,Zhang15} has proposed the use ofthe maximum loss over random structured outputs sampled independently from someproposal distribution. This method is linear-time in the number of randomstructured outputs and trivially parallelizable. We study this family of lossfunctions in the PAC-Bayes framework under Gaussian perturbations\cite{McAllester07}. Under some technical conditions and up to statisticalaccuracy, we show that this family of loss functions produces a tighter upperbound of the Gibbs decoder distortion than commonly used methods. Thus, usingthe maximum loss over random structured outputs is a principled way of learningthe parameter of structured prediction models. Besides explaining theexperimental success of \cite{Zhang14,Zhang15}, our theoretical results showthat more general techniques are possible.
arxiv-15900-170 | Optimal approximate matrix product in terms of stable rank | http://arxiv.org/pdf/1507.02268v3.pdf | author:Michael B. Cohen, Jelani Nelson, David P. Woodruff category:cs.DS cs.LG stat.ML published:2015-07-08 summary:We prove, using the subspace embedding guarantee in a black box way, that onecan achieve the spectral norm guarantee for approximate matrix multiplicationwith a dimensionality-reducing map having $m = O(\tilde{r}/\varepsilon^2)$rows. Here $\tilde{r}$ is the maximum stable rank, i.e. squared ratio ofFrobenius and operator norms, of the two matrices being multiplied. This is aquantitative improvement over previous work of [MZ11, KVZ14], and is alsooptimal for any oblivious dimensionality-reducing map. Furthermore, due to theblack box reliance on the subspace embedding property in our proofs, ourtheorem can be applied to a much more general class of sketching matrices thanwhat was known before, in addition to achieving better bounds. For example, onecan apply our theorem to efficient subspace embeddings such as the SubsampledRandomized Hadamard Transform or sparse subspace embeddings, or even withsubspace embedding constructions that may be developed in the future. Our main theorem, via connections with spectral error matrix multiplicationshown in prior work, implies quantitative improvements for approximate leastsquares regression and low rank approximation. Our main result has also alreadybeen applied to improve dimensionality reduction guarantees for $k$-meansclustering [CEMMP14], and implies new results for nonparametric regression[YPW15]. We also separately point out that the proof of the "BSS" deterministicrow-sampling result of [BSS12] can be modified to show that for any matrices$A, B$ of stable rank at most $\tilde{r}$, one can achieve the spectral normguarantee for approximate matrix multiplication of $A^T B$ by deterministicallysampling $O(\tilde{r}/\varepsilon^2)$ rows that can be found in polynomialtime. The original result of [BSS12] was for rank instead of stable rank. Ourobservation leads to a stronger version of a main theorem of [KMST10].
arxiv-15900-171 | Streaming regularization parameter selection via stochastic gradient descent | http://arxiv.org/pdf/1511.02187v2.pdf | author:Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML published:2015-11-06 summary:We propose a framework to perform streaming covariance selection. Ourapproach employs regularization constraints where a time-varying sparsityparameter is iteratively estimated via stochastic gradient descent. This allowsfor the regularization parameter to be efficiently learnt in an online manner.The proposed framework is developed for linear regression models and extendedto graphical models via neighbourhood selection. Under mild assumptions, we areable to obtain convergence results in a non-stochastic setting. Thecapabilities of such an approach are demonstrated using both synthetic data aswell as neuroimaging data.
arxiv-15900-172 | PLATO: Policy Learning using Adaptive Trajectory Optimization | http://arxiv.org/pdf/1603.00622v1.pdf | author:Gregory Kahn, Tianhao Zhang, Sergey Levine, Pieter Abbeel category:cs.LG published:2016-03-02 summary:Policy search can in principle acquire complex strategies for control ofrobots, self-driving vehicles, and other autonomous systems. When the policy istrained to process raw sensory inputs, such as images and depth maps, it canacquire a strategy that combines perception and control. However, effectivelyprocessing such complex inputs requires an expressive policy class, such as alarge neural network. These high-dimensional policies are difficult to train,especially when training must be done for safety-critical systems. We proposePLATO, an algorithm that trains complex control policies with supervisedlearning, using model-predictive control (MPC) to generate the supervision.PLATO uses an adaptive training method to modify the behavior of MPC togradually match the learned policy, in order to generate training samples atstates that are likely to be visited by the policy while avoiding highlyundesirable on-policy actions. We prove that this type of adaptive MPC expertproduces supervision that leads to good long-horizon performance of theresulting policy, and empirically demonstrate that MPC can still avoiddangerous on-policy actions in unexpected situations during training. Comparedto prior methods, our empirical results demonstrate that PLATO learns fasterand often converges to a better solution on a set of challenging simulatedexperiments involving autonomous aerial vehicles.
arxiv-15900-173 | Auxiliary Image Regularization for Deep CNNs with Noisy Labels | http://arxiv.org/pdf/1511.07069v2.pdf | author:Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell category:cs.CV published:2015-11-22 summary:Precisely-labeled data sets with sufficient amount of samples are veryimportant for training deep convolutional neural networks (CNNs). However, manyof the available real-world data sets contain erroneously labeled samples andthose errors substantially hinder the learning of very accurate CNN models. Inthis work, we consider the problem of training a deep CNN model for imageclassification with mislabeled training samples - an issue that is common inreal image data sets with tags supplied by amateur users. To solve thisproblem, we propose an auxiliary image regularization technique, optimized bythe stochastic Alternating Direction Method of Multipliers (ADMM) algorithm,that automatically exploits the mutual context information among trainingimages and encourages the model to select reliable images to robustify thelearning process. Comprehensive experiments on benchmark data sets clearlydemonstrate our proposed regularized CNN model is resistant to label noise intraining data.
arxiv-15900-174 | Preserving Statistical Validity in Adaptive Data Analysis | http://arxiv.org/pdf/1411.2664v3.pdf | author:Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth category:cs.LG cs.DS published:2014-11-10 summary:A great deal of effort has been devoted to reducing the risk of spuriousscientific discoveries, from the use of sophisticated validation techniques, todeep statistical methods for controlling the false discovery rate in multiplehypothesis testing. However, there is a fundamental disconnect between thetheoretical results and the practice of data analysis: the theory ofstatistical inference assumes a fixed collection of hypotheses to be tested, orlearning algorithms to be applied, selected non-adaptively before the data aregathered, whereas in practice data is shared and reused with hypotheses and newanalyses being generated on the basis of data exploration and the outcomes ofprevious analyses. In this work we initiate a principled study of how to guarantee the validityof statistical inference in adaptive data analysis. As an instance of thisproblem, we propose and investigate the question of estimating the expectationsof $m$ adaptively chosen functions on an unknown distribution given $n$ randomsamples. We show that, surprisingly, there is a way to estimate an exponential in $n$number of expectations accurately even if the functions are chosen adaptively.This gives an exponential improvement over standard empirical estimators thatare limited to a linear number of estimates. Our result follows from a generaltechnique that counter-intuitively involves actively perturbing andcoordinating the estimates, using techniques developed for privacypreservation. We give additional applications of this technique to ourquestion.
arxiv-15900-175 | WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation | http://arxiv.org/pdf/1510.08628v2.pdf | author:Jianfei Chen, Kaiwei Li, Jun Zhu, Wenguang Chen category:stat.ML cs.DC cs.IR cs.LG published:2015-10-29 summary:Developing efficient and scalable algorithms for Latent Dirichlet Allocation(LDA) is of wide interest for many applications. Previous work has developed anO(1) Metropolis-Hastings sampling method for each token. However, theperformance is far from being optimal due to random accesses to the parametermatrices and frequent cache misses. In this paper, we first carefully analyze the memory access efficiency ofexisting algorithms for LDA by the scope of random access, which is the size ofthe memory region in which random accesses fall, within a short period of time.We then develop WarpLDA, an LDA sampler which achieves both the best O(1) timecomplexity per token and the best O(K) scope of random access. Our empiricalresults in a wide range of testing conditions demonstrate that WarpLDA isconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings basedLightLDA, and is comparable or faster than the sparsity aware F+LDA. WithWarpLDA, users can learn up to one million topics from hundreds of millions ofdocuments in a few hours, at an unprecedentedly throughput of 11G tokens persecond.
arxiv-15900-176 | Metric Learning with Adaptive Density Discrimination | http://arxiv.org/pdf/1511.05939v2.pdf | author:Oren Rippel, Manohar Paluri, Piotr Dollar, Lubomir Bourdev category:stat.ML cs.LG published:2015-11-18 summary:Distance metric learning (DML) approaches learn a transformation to arepresentation space where distance is in correspondence with a predefinednotion of similarity. While such models offer a number of compelling benefits,it has been difficult for these to compete with modern classificationalgorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address anumber of subtle yet important issues which have stymied earlier DMLalgorithms. It maintains an explicit model of the distributions of thedifferent classes in representation space. It then employs this knowledge toadaptively assess similarity, and achieve local discrimination by penalizingclass distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approachachieves state-of-the-art classification results on a number of fine-grainedvisual recognition datasets, surpassing the standard softmax classifier andoutperforming triplet loss by a relative margin of 30-40%. In terms ofcomputational performance, it alleviates training inefficiencies in thetraditional triplet loss, reaching the same error in 5-30 times feweriterations. Beyond classification, we further validate the saliency of thelearnt representations via their attribute concentration and hierarchy recoveryproperties, achieving 10-25% relative gains on the softmax classifier and25-50% on triplet loss in these tasks.
arxiv-15900-177 | Distributed Estimation of Dynamic Parameters : Regret Analysis | http://arxiv.org/pdf/1603.00576v1.pdf | author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI published:2016-03-02 summary:This paper addresses the estimation of a time- varying parameter in anetwork. A group of agents sequentially receive noisy signals about theparameter (or moving target), which does not follow any particular dynamics.The parameter is not observable to an individual agent, but it is globallyidentifiable for the whole network. Viewing the problem with an onlineoptimization lens, we aim to provide the finite-time or non-asymptotic analysisof the problem. To this end, we use a notion of dynamic regret which suits theonline, non-stationary nature of the problem. In our setting, dynamic regretcan be recognized as a finite-time counterpart of stability in the mean- squaresense. We develop a distributed, online algorithm for tracking the movingtarget. Defining the path-length as the consecutive differences between targetlocations, we express an upper bound on regret in terms of the path-length ofthe target and network errors. We further show the consistency of the resultwith static setting and noiseless observations.
arxiv-15900-178 | Asymptotic behavior of $\ell_p$-based Laplacian regularization in semi-supervised learning | http://arxiv.org/pdf/1603.00564v1.pdf | author:Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan category:cs.LG stat.ML published:2016-03-02 summary:Given a weighted graph with $N$ vertices, consider a real-valued regressionproblem in a semi-supervised setting, where one observes $n$ labeled vertices,and the task is to label the remaining ones. We present a theoretical study of$\ell_p$-based Laplacian regularization under a $d$-dimensional geometricrandom graph model. We provide a variational characterization of theperformance of this regularized learner as $N$ grows to infinity while $n$stays constant, the associated optimality conditions lead to a partialdifferential equation that must be satisfied by the associated functionestimate $\hat{f}$. From this formulation we derive several predictions on thelimiting behavior the $d$-dimensional function $\hat{f}$, including (a) a phasetransition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoffbetween smoothness and sensitivity to the underlying unlabeled datadistribution $P$. Thus, over the range $p \leq d$, the function estimate$\hat{f}$ is degenerate and "spiky," whereas for $p\geq d+1$, the functionestimate $\hat{f}$ is smooth. We show that the effect of the underlying densityvanishes monotonically with $p$, such that in the limit $p = \infty$,corresponding to the so-called Absolutely Minimal Lipschitz Extension, theestimate $\hat{f}$ is independent of the distribution $P$. Under the assumptionof semi-supervised smoothness, ignoring $P$ can lead to poor statisticalperformance, in particular, we construct a specific example for $d=1$ todemonstrate that $p=2$ has lower risk than $p=\infty$ due to the former penaltyadapting to $P$ and the latter ignoring it. We also provide simulations thatverify the accuracy of our predictions for finite sample sizes. Together, theseproperties show that $p = d+1$ is an optimal choice, yielding a functionestimate $\hat{f}$ that is both smooth and non-degenerate, while remainingmaximally sensitive to $P$.
arxiv-15900-179 | Sequential Nonparametric Testing with the Law of the Iterated Logarithm | http://arxiv.org/pdf/1506.03486v2.pdf | author:Akshay Balsubramani, Aaditya Ramdas category:stat.ML cs.LG math.ST stat.ME stat.TH published:2015-06-10 summary:We propose a new algorithmic framework for sequential hypothesis testing withi.i.d. data, which includes A/B testing, nonparametric two-sample testing, andindependence testing as special cases. It is novel in several ways: (a) ittakes linear time and constant space to compute on the fly, (b) it has the samepower guarantee as a non-sequential version of the test with the samecomputational constraints up to a small factor, and (c) it accesses only asmany samples as are required - its stopping time adapts to the unknowndifficulty of the problem. All our test statistics are constructed to bezero-mean martingales under the null hypothesis, and the rejection threshold isgoverned by a uniform non-asymptotic law of the iterated logarithm (LIL). Forthe case of nonparametric two-sample mean testing, we also provide a finitesample power analysis, and the first non-asymptotic stopping time calculationsfor this class of problems. We verify our predictions for type I and II errorsand stopping times using simulations.
arxiv-15900-180 | US-Cut: Interactive Algorithm for rapid Detection and Segmentation of Liver Tumors in Ultrasound Acquisitions | http://arxiv.org/pdf/1603.00546v1.pdf | author:Jan Egger, Philip Voglreiter, Mark Dokter, Michael Hofmann, Xiaojun Chen, Wolfram G. Zoller, Dieter Schmalstieg, Alexander Hann category:cs.CV cs.CE cs.CG cs.GR published:2016-03-02 summary:Ultrasound (US) is the most commonly used liver imaging modality worldwide.It plays an important role in follow-up of cancer patients with livermetastases. We present an interactive segmentation approach for liver tumors inUS acquisitions. Due to the low image quality and the low contrast between thetumors and the surrounding tissue in US images, the segmentation is verychallenging. Thus, the clinical practice still relies on manual measurement andoutlining of the tumors in the US images. We target this problem by applying aninteractive segmentation algorithm to the US data, allowing the user to getreal-time feedback of the segmentation results. The algorithm has beendeveloped and tested hand-in-hand by physicians and computer scientists to makesure a future practical usage in a clinical setting is feasible. To covertypical acquisitions from the clinical routine, the approach has been evaluatedwith dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic(darker) or isoechoic (similar) in comparison to the surrounding liver tissue.Due to the interactive real-time behavior of the approach, it was possible evenin difficult cases to find satisfying segmentations of the tumors withinseconds and without parameter settings, and the average tumor deviation wasonly 1.4mm compared with manual measurements. However, the long term goal is toease the volumetric acquisition of liver tumors in order to evaluate fortreatment response. Additional aim is the registration of intraoperative USimages via the interactive segmentations to the patient's pre-interventional CTacquisitions.
arxiv-15900-181 | Parallel Bayesian Global Optimization of Expensive Functions | http://arxiv.org/pdf/1602.05149v2.pdf | author:Jialei Wang, Scott C. Clark, Eric Liu, Peter I. Frazier category:stat.ML math.OC published:2016-02-16 summary:We consider parallel global optimization of derivative-freeexpensive-to-evaluate functions, and propose an efficient method based onstochastic approximation for implementing a conceptual Bayesian optimizationalgorithm proposed by Ginsbourger et al. (2007). To accomplish this, we useinfinitessimal perturbation analysis (IPA) to construct a stochastic gradientestimator and show that this estimator is unbiased. We also show that thestochastic gradient ascent algorithm using the constructed gradient estimatorconverges to a stationary point of the q-EI surface, and therefore, as thenumber of multiple starts of the gradient ascent algorithm and the number ofsteps for each start grow large, the one-step Bayes optimal set of points isrecovered. We show in numerical experiments that our method for maximizing theq-EI is faster than methods based on closed-form evaluation usinghigh-dimensional integration, when considering many parallel functionevaluations, and is comparable in speed when considering few. We also show thatthe resulting one-step Bayes optimal algorithm for parallel global optimizationfinds high quality solutions with fewer evaluations that a heuristic based onapproximately maximizing the q-EI. A high quality open source implementation ofthis algorithm is available in the open source Metrics Optimization Engine(MOE).
arxiv-15900-182 | LOFS: Library of Online Streaming Feature Selection | http://arxiv.org/pdf/1603.00531v1.pdf | author:Kui Yu, Wei Ding, Xindong Wu category:cs.LG stat.ML published:2016-03-02 summary:As an emerging research direction, online streaming feature selection dealswith sequentially added dimensions in a feature space while the number of datainstances is fixed. Online streaming feature selection provides a new,complementary algorithmic methodology to enrich online feature selection,especially targets to high dimensionality in big data analytics. This paperintroduces the first comprehensive open-source library for use in MATLAB thatimplements the state-of-the-art algorithms of online streaming featureselection. The library is designed to facilitate the development of newalgorithms in this exciting research direction and make comparisons between thenew methods and existing ones available.
arxiv-15900-183 | Bayesian representation learning with oracle constraints | http://arxiv.org/pdf/1506.05011v4.pdf | author:Theofanis Karaletsos, Serge Belongie, Gunnar Rätsch category:stat.ML cs.CV cs.LG published:2015-06-16 summary:Representation learning systems typically rely on massive amounts of labeleddata in order to be trained to high accuracy. Recently, high-dimensionalparametric models like neural networks have succeeded in building richrepresentations using either compressive, reconstructive or supervisedcriteria. However, the semantic structure inherent in observations isoftentimes lost in the process. Human perception excels at understandingsemantics but cannot always be expressed in terms of labels. Thus,\emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing,are often employed to generate similarity constraints using an implicitsimilarity function encoded in human perception. In this work we propose tocombine \emph{generative unsupervised feature learning} with a\emph{probabilistic treatment of oracle information like triplets} in order totransfer implicit privileged oracle knowledge into explicit nonlinear Bayesianlatent factor models of the observations. We use a fast variational algorithmto learn the joint model and demonstrate applicability to a well-known imagedataset. We show how implicit triplet information can provide rich informationto learn representations that outperform previous metric learning approaches aswell as generative models without this side-information in a variety ofpredictive tasks. In addition, we illustrate that the proposed approachcompartmentalizes the latent spaces semantically which allows interpretation ofthe latent variables.
arxiv-15900-184 | Solving Combinatorial Games using Products, Projections and Lexicographically Optimal Bases | http://arxiv.org/pdf/1603.00522v1.pdf | author:Swati Gupta, Michel Goemans, Patrick Jaillet category:cs.LG published:2016-03-01 summary:In order to find Nash-equilibria for two-player zero-sum games where eachplayer plays combinatorial objects like spanning trees, matchings etc, weconsider two online learning algorithms: the online mirror descent (OMD)algorithm and the multiplicative weights update (MWU) algorithm. The OMDalgorithm requires the computation of a certain Bregman projection, that hasclosed form solutions for simple convex sets like the Euclidean ball or thesimplex. However, for general polyhedra one often needs to exploit the generalmachinery of convex optimization. We give a novel primal-style algorithm forcomputing Bregman projections on the base polytopes of polymatroids. Next, inthe case of the MWU algorithm, although it scales logarithmically in the numberof pure strategies or experts $N$ in terms of regret, the algorithm takes timepolynomial in $N$; this especially becomes a problem when learningcombinatorial objects. We give a general recipe to simulate the multiplicativeweights update algorithm in time polynomial in their natural dimension. This isuseful whenever there exists a polynomial time generalized counting oracle(even if approximate) over these objects. Finally, using the combinatorialstructure of symmetric Nash-equilibria (SNE) when both players play bases ofmatroids, we show that these can be found with a single projection or convexminimization (without using online learning).
arxiv-15900-185 | Segmental Recurrent Neural Networks | http://arxiv.org/pdf/1511.06018v2.pdf | author:Lingpeng Kong, Chris Dyer, Noah A. Smith category:cs.CL cs.LG published:2015-11-18 summary:We introduce segmental recurrent neural networks (SRNNs) which define, givenan input sequence, a joint probability distribution over segmentations of theinput and labelings of the segments. Representations of the input segments(i.e., contiguous subsequences of the input) are computed by encoding theirconstituent tokens using bidirectional recurrent neural nets, and these"segment embeddings" are used to define compatibility scores with outputlabels. These local compatibility scores are integrated using a globalsemi-Markov conditional random field. Both fully supervised training -- inwhich segment boundaries and labels are observed -- as well as partiallysupervised training -- in which segment boundaries are latent -- arestraightforward. Experiments on handwriting recognition and joint Chinese wordsegmentation/POS tagging show that, compared to models that do not explicitlyrepresent segments such as BIO tagging schemes and connectionist temporalclassification (CTC), SRNNs obtain substantially higher accuracies.
arxiv-15900-186 | Variable Rate Image Compression with Recurrent Neural Networks | http://arxiv.org/pdf/1511.06085v5.pdf | author:George Toderici, Sean M. O'Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja, Michele Covell, Rahul Sukthankar category:cs.CV cs.LG cs.NE published:2015-11-19 summary:A large fraction of Internet traffic is now driven by requests from mobiledevices with relatively small screens and often stringent bandwidthrequirements. Due to these factors, it has become the norm for moderngraphics-heavy websites to transmit low-resolution, low-bytecount imagepreviews (thumbnails) as part of the initial page load process to improveapparent page responsiveness. Increasing thumbnail compression beyond thecapabilities of existing codecs is therefore a current research focus, as anybyte savings will significantly enhance the experience of mobile device users.Toward this end, we propose a general framework for variable-rate imagecompression and a novel architecture based on convolutional and deconvolutionalLSTM recurrent networks. Our models address the main issues that have preventedautoencoder neural networks from competing with existing image compressionalgorithms: (1) our networks only need to be trained once (not per-image),regardless of input image dimensions and the desired compression rate; (2) ournetworks are progressive, meaning that the more bits are sent, the moreaccurate the image reconstruction; and (3) the proposed architecture is atleast as efficient as a standard purpose-trained autoencoder for a given numberof bits. On a large-scale benchmark of 32$\times$32 thumbnails, our LSTM-basedapproaches provide better visual quality than (headerless) JPEG, JPEG2000 andWebP, with a storage size that is reduced by 10% or more.
arxiv-15900-187 | Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features | http://arxiv.org/pdf/1603.00502v1.pdf | author:JT Turner, Kalyan Gupta, Brendan Morris, David W. Aha category:cs.CV published:2016-03-01 summary:Although recent advances in regional Convolutional Neural Networks (CNNs)enable them to outperform conventional techniques on standard object detectionand classification tasks, their response time is still slow for real-timeperformance. To address this issue, we propose a method for region proposal asan alternative to selective search, which is used in current state-of-the artobject detection algorithms. We evaluate our Keypoint Density-based RegionProposal (KDRP) approach and show that it speeds up detection andclassification on fine-grained tasks by 100% versus the existing selectivesearch region proposal technique without compromising classification accuracy.KDRP makes the application of CNNs to real-time detection and classificationfeasible.
arxiv-15900-188 | Model-based Dashboards for Customer Analytics | http://arxiv.org/pdf/1511.05614v3.pdf | author:Ryan Dew, Asim Ansari category:stat.AP stat.ML published:2015-11-17 summary:Automating the customer analytics process is crucial for companies thatmanage distinct customer bases. In such data-rich and dynamic environments,visualization plays a key role in understanding events of interest. These ideashave led to the popularity of analytics dashboards, yet academic research haspaid scant attention to these managerial needs. We develop a probabilistic,nonparametric framework for understanding and predicting individual-levelspending using Gaussian process priors over latent functions that describecustomer spending along calendar time, interpurchase time, and customerlifetime dimensions. These curves form a dashboard that provides a visualmodel-based representation of purchasing dynamics that is easilycomprehensible. The model flexibly and automatically captures the form andduration of the impact of events that influence spend propensity, even whensuch events are unknown a-priori. We illustrate the use of our Gaussian ProcessPropensity Model (GPPM) on data from two popular mobile games. We show that theGPPM generalizes hazard and buy-till-you-die models by incorporating calendartime dynamics while simultaneously accounting for recency and lifetime effects.It therefore provides insights about spending propensity beyond those availablefrom these models. Finally, we show that the GPPM outperforms these benchmarksboth in fitting and forecasting real and simulated spend data.
arxiv-15900-189 | Fourier ptychographic reconstruction using Poisson maximum likelihood and truncated Wirtinger gradient | http://arxiv.org/pdf/1603.04746v1.pdf | author:Liheng Bian, Jinli Suo, Jaebum Chung, Xiaoze Ou, Changhuei Yang, Feng Chen, Qionghai Dai category:cs.CV physics.optics published:2016-03-01 summary:Fourier ptychographic microscopy (FPM) is a novel computational coherentimaging technique for high space-bandwidth product imaging. Mathematically,Fourier ptychographic (FP) reconstruction can be implemented as a phaseretrieval optimization process, in which we only obtain low resolutionintensity images corresponding to the sub-bands of the sample's high resolution(HR) spatial spectrum, and aim to retrieve the complex HR spectrum. In realsetups, the measurements always suffer from various degenerations such asGaussian noise, Poisson noise, speckle noise and pupil location error, whichwould largely degrade the reconstruction. To efficiently address thesedegenerations, we propose a novel FP reconstruction method under a gradientdescent optimization framework in this paper. The technique utilizes Poissonmaximum likelihood for better signal modeling, and truncated Wirtinger gradientfor error removal. Results on both simulated data and real data captured usingour laser FPM setup show that the proposed method outperforms otherstate-of-the-art algorithms. Also, we have released our source code fornon-commercial use.
arxiv-15900-190 | Crowdsourcing On-street Parking Space Detection | http://arxiv.org/pdf/1603.00441v1.pdf | author:Ruizhi Liao, Cristian Roman, Peter Ball, Shumao Ou, Liping Chen category:cs.HC cs.LG published:2016-03-01 summary:As the number of vehicles continues to grow, parking spaces are at a premiumin city streets. Additionally, due to the lack of knowledge about streetparking spaces, heuristic circling the blocks not only costs drivers' time andfuel, but also increases city congestion. In the wake of recent trend to buildconvenient, green and energy-efficient smart cities, we rethink commontechniques adopted by high-profile smart parking systems, and present auser-engaged (crowdsourcing) and sonar-based prototype to identify urbanon-street parking spaces. The prototype includes an ultrasonic sensor, a GPSreceiver and associated Arduino micro-controllers. It is mounted on thepassenger side of a car to measure the distance from the vehicle to the nearestroadside obstacle. Multiple road tests are conducted around Wheatley, Oxford togather results and emulate the crowdsourcing approach. By extracting parkedvehicles' features from the collected trace, a supervised learning algorithm isdeveloped to estimate roadside parking occupancy and spot illegal parkingvehicles. A quantity estimation model is derived to calculate the requirednumber of sensing units to cover urban streets. The estimation isquantitatively compared to a fixed sensing solution. The results show that thecrowdsourcing way would need substantially fewer sensors compared to the fixedsensing system.
arxiv-15900-191 | Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach | http://arxiv.org/pdf/1603.00438v1.pdf | author:Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid category:cs.CV published:2016-03-01 summary:Convolutional neural networks (CNNs) have recently received a lot ofattention due to their ability to model local stationary structures in naturalimages in a multi-scale fashion, when learning all model parameters withsupervision. While excellent performance was achieved for image classificationwhen large amounts of labeled visual data are available, their success forun-supervised tasks such as image retrieval has been moderate so far. Our paperfocuses on this latter setting and explores several methods for learning patchdescriptors without supervision with application to matching and instance-levelretrieval. To that effect, we propose a new family of convolutional descriptorsfor patch representation , based on the recently introduced convolutionalkernel networks. We show that our descriptor, named Patch-CKN, performs betterthan SIFT as well as other convolutional networks learned by artificiallyintroducing supervision and is significantly faster to train. To demonstrateits effectiveness, we perform an extensive evaluation on standard benchmarksfor patch and image retrieval where we obtain state-of-the-art results. We alsointroduce a new dataset called RomePatches, which allows to simultaneouslystudy descriptor performance for patch and image retrieval.
arxiv-15900-192 | Technical Report: Band selection for nonlinear unmixing of hyperspectral images as a maximal click problem | http://arxiv.org/pdf/1603.00437v1.pdf | author:Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard category:cs.CV published:2016-03-01 summary:Kernel-based nonlinear mixing models have been applied to unmix spectralinformation of hyperspectral images when the type of mixing occurring in thescene is too complex or unknown. Such methods, however, usually require theinversion of matrices of sizes equal to the number of spectral bands. Reducingthe computational load of these methods remains a challenge in large scaleapplications. This paper proposes a centralized method for band selection (BS)in the reproducing kernel Hilbert space (RKHS). It is based upon the coherencecriterion, which sets the largest value allowed for correlations between thebasis kernel functions characterizing the unmixing model. We show that theproposed BS approach is equivalent to solving a maximum clique problem (MCP),that is, searching for the biggest complete subgraph in a graph. Furthermore,we devise a strategy for selecting the coherence threshold and the Gaussiankernel bandwidth using coherence bounds for linearly independent bases.Simulation results illustrate the efficiency of the proposed method.
arxiv-15900-193 | A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear Functionals | http://arxiv.org/pdf/1603.00427v1.pdf | author:Felipe C. Pinheiro, Cássio G. Lopes category:cs.SY cs.LG published:2016-03-01 summary:Nonlinear adaptive filtering allows for modeling of some additional aspectsof a general system and usually relies on highly complex algorithms, such asthose based on the Volterra series. Through the use of the Kronecker productand some basic facts of tensor algebra, we propose a simple model ofnonlinearity, one that can be interpreted as a product of the outputs of K FIRlinear filters, and compute its cost function together with its gradient, whichallows for some analysis of the optimization problem. We use these results itin a stochastic gradient framework, from which we derive an LMS-like algorithmand investigate the problems of multi-modality in the mean-square error surfaceand the choice of adequate initial conditions. Its computational complexity iscalculated. The new algorithm is tested in a system identification setup and iscompared with other polynomial algorithms from the literature, presentingfavorable convergence and/or computational complexity.
arxiv-15900-194 | Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs | http://arxiv.org/pdf/1603.00423v1.pdf | author:Phong Le, Willem Zuidema category:cs.AI cs.CL cs.NE published:2016-03-01 summary:Recursive neural networks (RNN) and their recently proposed extensionrecursive long short term memory networks (RLSTM) are models that computerepresentations for sentences, by recursively combining word embeddingsaccording to an externally provided parse tree. Both models thus, unlikerecurrent networks, explicitly make use of the hierarchical structure of asentence. In this paper, we demonstrate that RNNs nevertheless suffer from thevanishing gradient and long distance dependency problem, and that RLSTMsgreatly improve over RNN's on these problems. We present an artificial learningtask that allows us to quantify the severity of these problems for both models.We further show that a ratio of gradients (at the root node and a focal leafnode) is highly indicative of the success of backpropagation at optimizing therelevant weights low in the tree. This paper thus provides an explanation forexisting, superior results of RLSTMs on tasks such as sentiment analysis, andsuggests that the benefits of including hierarchical structure and of includingLSTM-style gating are complementary.
arxiv-15900-195 | Delving Deeper into Convolutional Networks for Learning Video Representations | http://arxiv.org/pdf/1511.06432v4.pdf | author:Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville category:cs.CV cs.LG cs.NE published:2015-11-19 summary:We propose an approach to learn spatio-temporal features in videos fromintermediate visual representations we call "percepts" usingGated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on perceptsthat are extracted from all level of a deep convolutional network trained onthe large ImageNet dataset. While high-level percepts contain highlydiscriminative information, they tend to have a low-spatial resolution.Low-level percepts, on the other hand, preserve a higher spatial resolutionfrom which we can model finer motion patterns. Using low-level percepts canleads to high-dimensionality video representations. To mitigate this effect andcontrol the model number of parameters, we introduce a variant of the GRU modelthat leverages the convolution operations to enforce sparse connectivity of themodel units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition andVideo Captioning tasks. In particular, we achieve results equivalent tostate-of-art on the YouTube2Text dataset using a simpler text-decoder model andwithout extra 3D CNN features.
arxiv-15900-196 | Multi-Information Source Optimization with General Model Discrepancies | http://arxiv.org/pdf/1603.00389v1.pdf | author:Matthias Poloczek, Jialei Wang, Peter I. Frazier category:stat.ML published:2016-03-01 summary:In the multi-information source optimization problem our goal is to optimizea complex design. However, we only have indirect access to the objective valueof any design via information sources that are subject to model discrepancy,i.e. whose internal model inherently deviates from reality. We present a novelalgorithm that is based on a rigorous mathematical treatment of theuncertainties arising from the model discrepancies. Its optimization decisionsrely on a stringent value of information analysis that trades off the predictedbenefit and its cost. We conduct an experimental evaluation that demonstratesthat the method consistently outperforms other state-of-the-art techniques: itfinds designs of considerably higher objective value and additionally inflictsless cost in the exploration process.
arxiv-15900-197 | Super-Resolution with Deep Convolutional Sufficient Statistics | http://arxiv.org/pdf/1511.05666v4.pdf | author:Joan Bruna, Pablo Sprechmann, Yann LeCun category:cs.CV published:2015-11-18 summary:Inverse problems in image and audio, and super-resolution in particular, canbe seen as high-dimensional structured prediction problems, where the goal isto characterize the conditional distribution of a high-resolution output givenits low-resolution corrupted observation. When the scaling ratio is small,point estimates achieve impressive performance, but soon they suffer from theregression-to-the-mean problem, result of their inability to capture themulti-modality of this conditional distribution. Modeling high-dimensionalimage and audio distributions is a hard task, requiring both the ability tomodel complex geometrical structures and textured regions. In this paper, wepropose to use as conditional model a Gibbs distribution, where its sufficientstatistics are given by deep convolutional neural networks. The featurescomputed by the network are stable to local deformation, and have reducedvariance when the input is a stationary texture. These properties imply thatthe resulting sufficient statistics minimize the uncertainty of the targetsignals given the degraded observations, while being highly informative. Thefilters of the CNN are initialized by multiscale complex wavelets, and then wepropose an algorithm to fine-tune them by estimating the gradient of theconditional log-likelihood, which bears some similarities with GenerativeAdversarial Networks. We evaluate experimentally the proposed approach in theimage super-resolution task, but the approach is general and could be used inother challenging ill-posed problems such as audio bandwidth extension.
arxiv-15900-198 | Easy-First Dependency Parsing with Hierarchical Tree LSTMs | http://arxiv.org/pdf/1603.00375v1.pdf | author:Eliyahu Kiperwasser, Yoav Goldberg category:cs.CL published:2016-03-01 summary:We suggest a compositional vector representation of parse trees that relieson a recursive combination of recurrent-neural network encoders. To demonstrateits effectiveness, we use the representation as the backbone of a greedy,bottom-up dependency parser, achieving state-of-the-art accuracies for Englishand Chinese, without relying on external word embeddings. The parser'simplementation is available for download at the first author's webpage.
arxiv-15900-199 | Numerical Approaches for Linear Left-invariant Diffusions on SE(2), their Comparison to Exact Solutions, and their Applications in Retinal Imaging | http://arxiv.org/pdf/1403.3320v7.pdf | author:Jiong Zhang, Remco Duits, Gonzalo Sanguinetti, Bart M. ter Haar Romeny category:math.NA cs.CV published:2014-03-13 summary:Left-invariant PDE-evolutions on the roto-translation group $SE(2)$ (andtheir resolvent equations) have been widely studied in the fields of corticalmodeling and image analysis. They include hypo-elliptic diffusion (for contourenhancement) proposed by Citti & Sarti, and Petitot, and they include thedirection process (for contour completion) proposed by Mumford. This paperpresents a thorough study and comparison of the many numerical approaches,which, remarkably, is missing in the literature. Existing numerical approachescan be classified into 3 categories: Finite difference methods, Fourier basedmethods (equivalent to $SE(2)$-Fourier methods), and stochastic methods (MonteCarlo simulations). There are also 3 types of exact solutions to thePDE-evolutions that were derived explicitly (in the spatial Fourier domain) inprevious works by Duits and van Almsick in 2005. Here we provide an overview ofthese 3 types of exact solutions and explain how they relate to each of the 3numerical approaches. We compute relative errors of all numerical approaches tothe exact solutions, and the Fourier based methods show us the best performancewith smallest relative errors. We also provide an improvement of Mathematicaalgorithms for evaluating Mathieu-functions, crucial in implementations of theexact solutions. Furthermore, we include an asymptotical analysis of thesingularities within the kernels and we propose a probabilistic extension ofunderlying stochastic processes that overcomes the singular behavior in theorigin of time-integrated kernels. Finally, we show retinal imagingapplications of combining left-invariant PDE-evolutions with invertibleorientation scores.
arxiv-15900-200 | Deep Spatial Autoencoders for Visuomotor Learning | http://arxiv.org/pdf/1509.06113v3.pdf | author:Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel category:cs.LG cs.CV cs.RO published:2015-09-21 summary:Reinforcement learning provides a powerful and flexible framework forautomated acquisition of robotic motion skills. However, applying reinforcementlearning requires a sufficiently detailed representation of the state,including the configuration of task-relevant objects. We present an approachthat automates state-space construction by learning a state representationdirectly from camera images. Our method uses a deep spatial autoencoder toacquire a set of feature points that describe the environment for the currenttask, such as the positions of objects, and then learns a motion skill withthese feature points using an efficient reinforcement learning method based onlocal linear models. The resulting controller reacts continuously to thelearned feature points, allowing the robot to dynamically manipulate objects inthe world with closed-loop control. We demonstrate our method with a PR2 roboton tasks that include pushing a free-standing toy block, picking up a bag ofrice using a spatula, and hanging a loop of rope on a hook at variouspositions. In each task, our method automatically learns to track task-relevantobjects and manipulate their configuration with the robot's arm.
arxiv-15900-201 | Driver Gaze Region Estimation Without Using Eye Movement | http://arxiv.org/pdf/1507.04760v2.pdf | author:Lex Fridman, Philipp Langhans, Joonbum Lee, Bryan Reimer category:cs.CV published:2015-07-16 summary:Automated estimation of the allocation of a driver's visual attention may bea critical component of future Advanced Driver Assistance Systems. In theory,vision-based tracking of the eye can provide a good estimate of gaze location.In practice, eye tracking from video is challenging because of sunglasses,eyeglass reflections, lighting conditions, occlusions, motion blur, and otherfactors. Estimation of head pose, on the other hand, is robust to many of theseeffects, but cannot provide as fine-grained of a resolution in localizing thegaze. However, for the purpose of keeping the driver safe, it is sufficient topartition gaze into regions. In this effort, we propose a system that extractsfacial features and classifies their spatial configuration into six regions inreal-time. Our proposed method achieves an average accuracy of 91.4% at anaverage decision rate of 11 Hz on a dataset of 50 drivers from an on-roadstudy.
arxiv-15900-202 | Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations | http://arxiv.org/pdf/1602.02722v2.pdf | author:Akshay Krishnamurthy, Alekh Agarwal, John Langford category:cs.LG stat.ML published:2016-02-08 summary:We propose and study a new tractable model for reinforcement learning withhigh-dimensional observation called Contextual-MDPs, generalizing contextualbandits to a sequential decision making setting. These models require an agentto take actions based on high-dimensional observations (features) with the goalof achieving long-term performance competitive with a large set of policies.Since the size of the observation space is a primary obstacle tosample-efficient learning, Contextual-MDPs are assumed to be summarizable by asmall number of hidden states. In this setting, we design a new reinforcementlearning algorithm that engages in global exploration while using a functionclass to approximate future performance. We also establish a sample complexityguarantee for this algorithm, proving that it learns near optimal behaviorafter a number of episodes that is polynomial in all relevant parameters,logarithmic in the number of policies, and independent of the size of theobservation space. This represents an exponential improvement on the samplecomplexity of all existing alternative approaches and provides theoreticaljustification for reinforcement learning with function approximation.
arxiv-15900-203 | Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition | http://arxiv.org/pdf/1603.00284v1.pdf | author:Aleksandr Y. Aravkin, Stephen Becker category:stat.ML cs.CV math.OC published:2016-03-01 summary:We focus on the robust principal component analysis (RPCA) problem, andreview a range of old and new convex formulations for the problem and itsvariants. We then review dual smoothing and level set techniques in convexoptimization, present several novel theoretical results, and apply thetechniques on the RPCA problem. In the final sections, we show a range ofnumerical experiments for simulated and real-world problems.
arxiv-15900-204 | Cluster-Seeking James-Stein Estimators | http://arxiv.org/pdf/1602.00542v2.pdf | author:K. Pavan Srinath, Ramji Venkataramanan category:cs.IT math.IT math.ST stat.ML stat.TH published:2016-02-01 summary:This paper considers the problem of estimating a high-dimensional vector ofparameters $\boldsymbol{\theta} \in \mathbb{R}^n$ from a noisy observation. Thenoise vector is i.i.d. Gaussian with known variance. For a squared-error lossfunction, the James-Stein (JS) estimator is known to dominate the simplemaximum-likelihood (ML) estimator when the dimension $n$ exceeds two. TheJS-estimator shrinks the observed vector towards the origin, and the riskreduction over the ML-estimator is greatest for $\boldsymbol{\theta}$ that lieclose to the origin. JS-estimators can be generalized to shrink the datatowards any target subspace. Such estimators also dominate the ML-estimator,but the risk reduction is significant only when $\boldsymbol{\theta}$ liesclose to the subspace. This leads to the question: in the absence of priorinformation about $\boldsymbol{\theta}$, how do we design estimators that givesignificant risk reduction over the ML-estimator for a wide range of$\boldsymbol{\theta}$? In this paper, we propose shrinkage estimators that attempt to infer thestructure of $\boldsymbol{\theta}$ from the observed data in order to constructa good attracting subspace. In particular, the components of the observedvector are separated into clusters, and the elements in each cluster shrunktowards a common attractor. The number of clusters and the attractor for eachcluster are determined from the observed vector. We provide concentrationresults for the squared-error loss and convergence results for the risk of theproposed estimators. The results show that the estimators give significant riskreduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$,particularly for large $n$. Simulation results are provided to support thetheoretical claims.
arxiv-15900-205 | Learning to retrieve out-of-vocabulary words in speech recognition | http://arxiv.org/pdf/1511.05389v4.pdf | author:Imran Sheikh, Irina Illina, Dominique Fohr, Georges Linarès category:cs.CL published:2015-11-17 summary:Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speechrecognition systems used to process diachronic audio data. To help recovery ofthe PNs missed by the system, relevant OOV PNs can be retrieved out of the manyOOVs by exploiting semantic context of the spoken content. In this paper, wepropose two neural network models targeted to retrieve OOV PNs relevant to anaudio document: (a) Document level Continuous Bag of Words (D-CBOW), (b)Document level Continuous Bag of Weighted Words (D-CBOW2). Both these modelstake document words as input and learn with an objective to maximise theretrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a newapproach in which the input embedding layer is augmented with a context anchorlayer. This layer learns to assign importance to input words and has theability to capture (task specific) key-words in a bag-of-word neural networkmodel. With experiments on French broadcast news videos we show that these twomodels outperform the baseline methods based on raw embeddings from LDA,Skip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models givesfaster convergence during training.
arxiv-15900-206 | Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest | http://arxiv.org/pdf/1603.00275v1.pdf | author:Korsuk Sirinukunwattana, Josien P. W. Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J. Matuszewski, Elia Bruni, Urko Sanchez, Anton Böhm, Olaf Ronneberger, Bassem Ben Cheikh, Daniel Racoceanu, Philipp Kainz, Michael Pfeiffer, Martin Urschler, David R. J. Snead, Nasir M. Rajpoot category:cs.CV published:2016-03-01 summary:Colorectal adenocarcinoma originating in intestinal glandular structures isthe most common form of colon cancer. In clinical practice, the morphology ofintestinal glands, including architectural appearance and glandular formation,is used by pathologists to inform prognosis and plan the treatment ofindividual patients. However, achieving good inter-observer as well asintra-observer reproducibility of cancer grading is still a major challenge inmodern pathology. An automated approach which quantifies the morphology ofglands is a solution to the problem. This paper provides an overview to theGland Segmentation in Colon Histology Images Challenge Contest (GlaS) held atMICCAI'2015. Details of the challenge, including organization, dataset andevaluation criteria, are presented, along with the method descriptions andevaluation results from the top performing methods.
arxiv-15900-207 | Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search and Analytics | http://arxiv.org/pdf/1603.00260v1.pdf | author:Dhruv Gupta category:cs.IR cs.CL published:2016-03-01 summary:In this article, I present the questions that I seek to answer in my PhDresearch. I posit to analyze natural language text with the help of semanticannotations and mine important events for navigating large text corpora.Semantic annotations such as named entities, geographic locations, and temporalexpressions can help us mine events from the given corpora. These events thusprovide us with useful means to discover the locked knowledge in them. I posethree problems that can help unlock this knowledge vault in semanticallyannotated text corpora: i. identifying important events; ii. semantic search;and iii. event analytics.
arxiv-15900-208 | Image denoising based on improved data-driven sparse representation | http://arxiv.org/pdf/1502.03273v2.pdf | author:Dai-Qiang Chen category:cs.CV published:2015-02-11 summary:Sparse representation of images under certain transform domain has beenplaying a fundamental role in image restoration tasks. One such representativemethod is the widely used wavelet tight frame systems. Instead of adoptingfixed filters for constructing a tight frame to sparsely model any input image,a data-driven tight frame was proposed for the sparse representation of images,and shown to be very efficient for image denoising very recently. However, inthis method the number of framelet filters used for constructing a tight frameis the same as the length of filters. In fact, through further investigation itis found that part of these filters are unnecessary and even harmful to therecovery effect due to the influence of noise. Therefore, an improveddata-driven sparse representation systems constructed with much less number offilters are proposed. Numerical results on denoising experiments demonstratethat the proposed algorithm overall outperforms the original data-driven tightframe construction scheme on both the recovery quality and computational time.
arxiv-15900-209 | Multi-task Sequence to Sequence Learning | http://arxiv.org/pdf/1511.06114v4.pdf | author:Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser category:cs.LG cs.CL stat.ML published:2015-11-19 summary:Sequence to sequence learning has recently emerged as a new paradigm insupervised learning. To date, most of its applications focused on only one taskand not much work explored this framework for multiple tasks. This paperexamines three multi-task learning (MTL) settings for sequence to sequencemodels: (a) the oneto-many setting - where the encoder is shared betweenseveral tasks such as machine translation and syntactic parsing, (b) themany-to-one setting - useful when only the decoder can be shared, as in thecase of translation and image caption generation, and (c) the many-to-manysetting - where multiple encoders and decoders are shared, which is the casewith unsupervised objectives and translation. Our results show that training ona small amount of parsing and image caption data can improve the translationquality between English and German by up to 1.5 BLEU points over strongsingle-task baselines on the WMT benchmarks. Furthermore, we have established anew state-of-the-art result in constituent parsing with 93.0 F1. Lastly, wereveal interesting properties of the two unsupervised learning objectives,autoencoder and skip-thought, in the MTL context: autoencoder helps less interms of perplexities but more on BLEU scores compared to skip-thought.
arxiv-15900-210 | Segmental Recurrent Neural Networks for End-to-end Speech Recognition | http://arxiv.org/pdf/1603.00223v1.pdf | author:Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith, Steve Renals category:cs.CL cs.LG cs.NE published:2016-03-01 summary:We study the segmental recurrent neural network for end-to-end acousticmodelling. This model connects the segmental conditional random field (CRF)with a recurrent neural network (RNN) used for feature extraction. Compared tomost previous CRF-based acoustic models, it does not rely on an external systemto provide features or segmentation boundaries. Instead, this modelmarginalises out all the possible segmentations, and features are extractedfrom the RNN trained together with the segmental CRF. In essence, this model isself-contained and can be trained end-to-end. In this paper, we discusspractical training and decoding issues as well as the method to speed up thetraining in the context of speech recognition. We performed experiments on theTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-passdecoding --- the best reported result using CRFs, despite the fact that we onlyused a zeroth-order CRF and without using any language model.
arxiv-15900-211 | Reasoning about Entailment with Neural Attention | http://arxiv.org/pdf/1509.06664v4.pdf | author:Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Phil Blunsom category:cs.CL cs.AI cs.LG cs.NE 68T50 I.2.6; I.2.7 published:2015-09-22 summary:While most approaches to automatically recognizing entailment relations haveused classifiers employing hand engineered features derived from complexnatural language processing pipelines, in practice their performance has beenonly slightly better than bag-of-word pair classifiers using only lexicalsimilarity. The only attempt so far to build an end-to-end differentiableneural network for entailment failed to outperform such a simple similarityclassifier. In this paper, we propose a neural model that reads two sentencesto determine entailment using long short-term memory units. We extend thismodel with a word-by-word neural attention mechanism that encourages reasoningover entailments of pairs of words and phrases. Furthermore, we present aqualitative analysis of attention weights produced by this model, demonstratingsuch reasoning capabilities. On a large entailment dataset this modeloutperforms the previous best neural model and a classifier with engineeredfeatures by a substantial margin. It is the first generic end-to-enddifferentiable system that achieves state-of-the-art accuracy on a textualentailment dataset.
arxiv-15900-212 | Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data | http://arxiv.org/pdf/1602.05875v2.pdf | author:Gil Keren, Björn Schuller category:stat.ML cs.CL published:2016-02-18 summary:Traditional convolutional layers extract features from patches of data byapplying a non-linearity on an affine function of the input. We propose a modelthat enhances this feature extraction process for the case of sequential data,by feeding patches of the data into a recurrent neural network and using theoutputs or hidden states of the recurrent units to compute the extractedfeatures. By doing so, we exploit the fact that a window containing a fewframes of the sequential data is a sequence itself and this additionalstructure might encapsulate valuable information. In addition, we allow formore steps of computation in the feature extraction process, which ispotentially beneficial as an affine function followed by a non-linearity canresult in too simple features. Using our convolutional recurrent layers weobtain an improvement in performance in two audio classification tasks,compared to traditional convolutional layers.
arxiv-15900-213 | Herding as a Learning System with Edge-of-Chaos Dynamics | http://arxiv.org/pdf/1602.03014v2.pdf | author:Yutian Chen, Max Welling category:stat.ML cs.LG published:2016-02-09 summary:Herding defines a deterministic dynamical system at the edge of chaos. Itgenerates a sequence of model states and parameters by alternating parameterperturbations with state maximizations, where the sequence of states can beinterpreted as "samples" from an associated MRF model. Herding differs frommaximum likelihood estimation in that the sequence of parameters does notconverge to a fixed point and differs from an MCMC posterior sampling approachin that the sequence of states is generated deterministically. Herding may beinterpreted as a"perturb and map" method where the parameter perturbations aregenerated using a deterministic nonlinear dynamical system rather than randomlyfrom a Gumbel distribution. This chapter studies the distinct statisticalcharacteristics of the herding algorithm and shows that the fast convergencerate of the controlled moments may be attributed to edge of chaos dynamics. Theherding algorithm can also be generalized to models with latent variables andto a discriminative learning setting. The perceptron cycling theorem ensuresthat the fast moment matching property is preserved in the more generalframework.
arxiv-15900-214 | Towards Meaningful Maps of Polish Case Law | http://arxiv.org/pdf/1510.03421v2.pdf | author:Michal Jungiewicz, Michał Łopuszyński category:cs.CL published:2015-10-12 summary:In this work, we analyze the utility of two dimensional document maps forexploratory analysis of Polish case law. We start by comparing two methods ofgenerating such visualizations. First is based on linear principal componentanalysis (PCA). Second makes use of the modern nonlinear t-DistributedStochastic Neighbor Embedding method (t-SNE). We apply both PCA and t-SNE to acorpus of judgments from different courts in Poland. It emerges that t-SNEprovides better, more interpretable results than PCA. As a next test, we applyt-SNE to randomly selected sample of common court judgments corresponding todifferent keywords. We show that t-SNE, in this case, reveals hidden topicalstructure of the documents related to keyword,,pension". In conclusion, we findthat the t-SNE method could be a promising tool to facilitate the exploitativeanalysis of legal texts, e.g., by complementing search or browse functionalityin legal databases.
arxiv-15900-215 | Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation | http://arxiv.org/pdf/1503.00848v4.pdf | author:Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Ferran Marques, Jitendra Malik category:cs.CV published:2015-03-03 summary:We propose a unified approach for bottom-up hierarchical image segmentationand object proposal generation for recognition, called Multiscale CombinatorialGrouping (MCG). For this purpose, we first develop a fast normalized cutsalgorithm. We then propose a high-performance hierarchical segmenter that makeseffective use of multiscale information. Finally, we propose a groupingstrategy that combines our multiscale regions into highly-accurate objectproposals by exploring efficiently their combinatorial space. We also presentSingle-scale Combinatorial Grouping (SCG), a faster version of MCG thatproduces competitive proposals in under five second per image. We conduct anextensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,and COCO datasets, showing that MCG produces state-of-the-art contours,hierarchical regions, and object proposals.
arxiv-15900-216 | Order-Embeddings of Images and Language | http://arxiv.org/pdf/1511.06361v6.pdf | author:Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun category:cs.LG cs.CL cs.CV published:2015-11-19 summary:Hypernymy, textual entailment, and image captioning can be seen as specialcases of a single visual-semantic hierarchy over words, sentences, and images.In this paper we advocate for explicitly modeling the partial order structureof this hierarchy. Towards this goal, we introduce a general method forlearning ordered representations, and show how it can be applied to a varietyof tasks involving images and language. We show that the resultingrepresentations improve performance over current approaches for hypernymprediction and image-caption retrieval.
arxiv-15900-217 | Pattern recognition on the quantum Bloch sphere | http://arxiv.org/pdf/1603.00173v1.pdf | author:Giuseppe Sergioli, Enrica Santucci, Luca Didaci, Jaroslaw A. Miszczak, Roberto Giuntini category:quant-ph cs.CV published:2016-03-01 summary:We introduce a framework suitable for describing pattern recognition taskusing the mathematical language of density matrices. In particular, we providea one-to-one correspondence between patterns and density operators, representedby mixed states when the uncertainty comes into play. The classificationprocess in the quantum framework is performed by the introduction of anormalized trace distance between density operators in place of the Euclideandistance between patterns. We provide a comparison of the introduced method inthe case of 2D data classification.
arxiv-15900-218 | Neural Programmer: Inducing Latent Programs with Gradient Descent | http://arxiv.org/pdf/1511.04834v2.pdf | author:Arvind Neelakantan, Quoc V. Le, Ilya Sutskever category:cs.LG cs.CL stat.ML published:2015-11-16 summary:Deep neural networks have achieved impressive supervised classificationperformance in many tasks including image recognition, speech recognition, andsequence to sequence learning. However, this success has not been translated toapplications like question answering that may involve complex arithmetic andlogic reasoning. A major limitation of these models is in their inability tolearn even simple arithmetic and logic operations. For example, it has beenshown that neural networks fail to learn to add two binary numbers reliably. Inthis work, we propose Neural Programmer, an end-to-end differentiable neuralnetwork augmented with a small set of basic arithmetic and logic operations.Neural Programmer can call these augmented operations over several steps,thereby inducing compositional programs that are more complex than the built-inoperations. The model learns from a weak supervision signal which is the resultof execution of the correct program, hence it does not require expensiveannotation of the correct program itself. The decisions of what operations tocall, and what data segments to apply to are inferred by Neural Programmer.Such decisions, during training, are done in a differentiable fashion so thatthe entire network can be trained jointly by gradient descent. We find thattraining the model is difficult, but it can be greatly improved by addingrandom noise to the gradient. On a fairly complex synthetic table-comprehensiondataset, traditional recurrent networks and attentional models perform poorlywhile Neural Programmer typically obtains nearly perfect accuracy.
arxiv-15900-219 | Convolutional Rectifier Networks as Generalized Tensor Decompositions | http://arxiv.org/pdf/1603.00162v1.pdf | author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG published:2016-03-01 summary:Convolutional rectifier networks, i.e. convolutional neural networks withrectified linear activation and max or average pooling, are the cornerstone ofmodern deep learning. However, despite their wide use and success, ourtheoretical understanding of the expressive properties that drive thesenetworks is partial at best. On other hand, we have a much firmer grasp ofthese issues in the world of arithmetic circuits. Specifically, it is knownthat convolutional arithmetic circuits posses the property of "complete depthefficiency", meaning that besides a negligible set, all functions that can beimplemented by a deep network of polynomial size, require exponential size inorder to be implemented (or even approximated) by a shallow network. In thispaper we describe a construction based on generalized tensor decompositions,that transforms convolutional arithmetic circuits into convolutional rectifiernetworks. We then use mathematical tools available from the world of arithmeticcircuits to prove new results. First, we show that convolutional rectifiernetworks are universal with max pooling but not with average pooling. Second,and more importantly, we show that depth efficiency is weaker withconvolutional rectifier networks than it is with convolutional arithmeticcircuits. This leads us to believe that developing effective methods fortraining convolutional arithmetic circuits, thereby fulfilling their expressivepotential, may give rise to a deep learning architecture that is provablysuperior to convolutional rectifier networks but has so far been overlooked bypractitioners.
arxiv-15900-220 | Auto-JacoBin: Auto-encoder Jacobian Binary Hashing | http://arxiv.org/pdf/1602.08127v2.pdf | author:Xiping Fu, Brendan McCane, Steven Mills, Michael Albert, Lech Szymanski category:cs.CV cs.LG published:2016-02-25 summary:Binary codes can be used to speed up nearest neighbor search tasks in largescale data sets as they are efficient for both storage and retrieval. In thispaper, we propose a robust auto-encoder model that preserves the geometricrelationships of high-dimensional data sets in Hamming space. This is done byconsidering a noise-removing function in a region surrounding the manifoldwhere the training data points lie. This function is defined with the propertythat it projects the data points near the manifold into the manifold wisely,and we approximate this function by its first order approximation. Experimentalresults show that the proposed method achieves better than state-of-the-artresults on three large scale high dimensional data sets.
arxiv-15900-221 | GOGMA: Globally-Optimal Gaussian Mixture Alignment | http://arxiv.org/pdf/1603.00150v1.pdf | author:Dylan Campbell, Lars Petersson category:cs.CV cs.RO published:2016-03-01 summary:Gaussian mixture alignment is a family of approaches that are frequently usedfor robustly solving the point-set registration problem. However, since theyuse local optimisation, they are susceptible to local minima and can onlyguarantee local optimality. Consequently, their accuracy is strongly dependenton the quality of the initialisation. This paper presents the firstglobally-optimal solution to the 3D rigid Gaussian mixture alignment problemunder the L2 distance between mixtures. The algorithm, named GOGMA, employs abranch-and-bound approach to search the space of 3D rigid motions SE(3),guaranteeing global optimality regardless of the initialisation. The geometryof SE(3) was used to find novel upper and lower bounds for the objectivefunction and local optimisation was integrated into the scheme to accelerateconvergence without voiding the optimality guarantee. The evaluationempirically supported the optimality proof and showed that the method performedmuch more robustly on two challenging datasets than an existingglobally-optimal registration solution.
arxiv-15900-222 | Storm Detection by Visual Learning Using Satellite Images | http://arxiv.org/pdf/1603.00146v1.pdf | author:Yu Zhang, Stephen Wistar, Jia Li, Michael Steinberg, James Z. Wang category:cs.CV published:2016-03-01 summary:Computers are widely utilized in today's weather forecasting as a powerfultool to leverage an enormous amount of data. Yet, despite the availability ofsuch data, current techniques often fall short of producing reliable detailedstorm forecasts. Each year severe thunderstorms cause significant damage andloss of life, some of which could be avoided if better forecasts wereavailable. We propose a computer algorithm that analyzes satellite images fromhistorical archives to locate visual signatures of severe thunderstorms forshort-term predictions. While computers are involved in weather forecasts tosolve numerical models based on sensory data, they are less competent inforecasting based on visual patterns from satellite images. In our system, weextract and summarize important visual storm evidence from satellite imagesequences in the way that meteorologists interpret the images. In particular,the algorithm extracts and fits local cloud motion from image sequences tomodel the storm-related cloud patches. Image data from the year 2008 have beenadopted to train the model, and historical thunderstorm reports in continentalUS from 2000 through 2013 have been used as the ground-truth and priors in themodeling process. Experiments demonstrate the usefulness and potential of thealgorithm for producing more accurate thunderstorm forecasts.
arxiv-15900-223 | On Tie Strength Augmented Social Correlation for Inferring Preference of Mobile Telco Users | http://arxiv.org/pdf/1603.00145v1.pdf | author:Shifeng Liu, Zheng Hu, Sujit Dey, Xin Ke category:cs.SI cs.IR cs.LG published:2016-03-01 summary:For mobile telecom operators, it is critical to build preference profiles oftheir customers and connected users, which can help operators make bettermarketing strategies, and provide more personalized services. With thedeployment of deep packet inspection (DPI) in telecom networks, it is possiblefor the telco operators to obtain user online preference. However, DPI has itslimitations and user preference derived only from DPI faces sparsity and coldstart problems. To better infer the user preference, social correlation intelco users network derived from Call Detailed Records (CDRs) with regard toonline preference is investigated. Though widely verified in several onlinesocial networks, social correlation between online preference of users inmobile telco networks, where the CDRs derived relationship are of less socialproperties and user mobile internet surfing activities are not visible toneighbourhood, has not been explored at a large scale. Based on a real worldtelecom dataset including CDRs and preference of more than $550K$ users forseveral months, we verified that correlation does exist between onlinepreference in such \textit{ambiguous} social network. Furthermore, we foundthat the stronger ties that users build, the more similarity between theirpreference may have. After defining the preference inferring task as a Top-$K$recommendation problem, we incorporated Matrix Factorization CollaborativeFiltering model with social correlation and tie strength based on call patternsto generate Top-$K$ preferred categories for users. The proposed Tie StrengthAugmented Social Recommendation (TSASoRec) model takes data sparsity and coldstart user problems into account, considering both the recorded and missingrecorded category entries. The experiment on real dataset shows the proposedmodel can better infer user preference, especially for cold start users.
arxiv-15900-224 | A Geometric Analysis of Phase Retrieval | http://arxiv.org/pdf/1602.06664v2.pdf | author:Ju Sun, Qing Qu, John Wright category:cs.IT math.IT math.OC stat.ML published:2016-02-22 summary:Can we recover a complex signal from its Fourier magnitudes? More generally,given a set of $m$ measurements, $y_k = \mathbf a_k^* \mathbf x$ for $k = 1,\dots, m$, is it possible to recover $\mathbf x \in \mathbb{C}^n$ (i.e.,length-$n$ complex vector)? This **generalized phase retrieval** (GPR) problemis a fundamental task in various disciplines, and has been the subject of muchrecent investigation. Natural nonconvex heuristics often work remarkably wellfor GPR in practice, but lack clear theoretical explanations. In this paper, wetake a step towards bridging this gap. We prove that when the measurementvectors $\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number ofmeasurements is large enough ($m \ge C n \log^3 n$), with high probability, anatural least-squares formulation for GPR has the following benign geometricstructure: (1) there are no spurious local minimizers, and all globalminimizers are equal to the target signal $\mathbf x$, up to a global phase;and (2) the objective function has a negative curvature around each saddlepoint. This structure allows a number of iterative optimization methods toefficiently find a global minimizer, without special initialization. Tocorroborate the claim, we describe and analyze a second-order trust-regionalgorithm.
arxiv-15900-225 | Image Annotation Incorporating Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors | http://arxiv.org/pdf/1508.07468v2.pdf | author:Yuqing Hou category:cs.CV published:2015-08-29 summary:Tag-based image retrieval (TBIR) has drawn much attention in recent years dueto the explosive amount of digital images and crowdsourcing tags. However, TBIRis still suffering from the incomplete and inaccurate tags provided by users,posing a great challenge for tag-based image management applications. In thiswork, we proposed a novel method for image annotation, incorporating severalpriors: Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors.Highly representative CNN feature vectors are adopt to model the tag-visualcorrelation and narrow the semantic gap. And we extract word vectors for tagsto measure similarity between tags in the semantic level, which is moreaccurate than traditional frequency-based or graph-based methods. We utilizethe accelerated proximal gradient (APG) method to solve our model efficiently.Extensive experiments conducted on multiple benchmark datasets demonstrate theeffectiveness and robustness of the proposed method.
arxiv-15900-226 | Image Annotation combining Subspace Clustering , Matrix Completion and Inhomogeneous Errors | http://arxiv.org/pdf/1601.03055v2.pdf | author:Yuqing Hou category:cs.CV published:2016-01-12 summary:Image annotation methods have greatly facilitated image managementapplications. However, existing methods are still suffering from thedegradation of the missing and noisy tags provided by users. In this study, wepropose an image annotation method which performs tag completion and refinementsequentially. We assume that images are sampled from a union of subspaces.Images sampled from the same subspace, as well as their corresponding tags,should form a compatible image-tag sub-matrix. Thus we segment the subspaces bythe Sparse Subspace Clustering (SSC) method and share tags in each subspace. Anovel matrix completion model is designed for tag refinement, taking visual-tagcorrelation, semantic-tag correlation and the inhomogeneous errors property,which is explored in this field for the first time, into consideration. Weexploit CNN features to improve the model. The proposed algorithm outperformsstate-of-the-art approaches when handling missing and noisy tags on multiplebenchmark datasets.
arxiv-15900-227 | A Universal Update-pacing Framework For Visual Tracking | http://arxiv.org/pdf/1603.00132v1.pdf | author:Zexi Hu, Yuefang Gao, Dong Wang, Xuhong Tian category:cs.CV published:2016-03-01 summary:This paper proposes a novel framework to alleviate the model drift problem invisual tracking, which is based on paced updates and trajectory selection.Given a base tracker, an ensemble of trackers is generated, in which eachtracker's update behavior will be paced and then traces the target objectforward and backward to generate a pair of trajectories in an interval. Then,we implicitly perform self-examination based on trajectory pair of each trackerand select the most robust tracker. The proposed framework can effectivelyleverage temporal context of sequential frames and avoid to learn corruptedinformation. Extensive experiments on the standard benchmark suggest that theproposed framework achieves superior performance against state-of-the-arttrackers.
arxiv-15900-228 | Virtualizing Deep Neural Networks for Memory-Efficient Neural Network Design | http://arxiv.org/pdf/1602.08124v2.pdf | author:Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, Stephen W. Keckler category:cs.DC cs.LG cs.NE published:2016-02-25 summary:The most widely used machine learning frameworks require users to carefullytune their memory usage so that the deep neural network (DNN) fits into theDRAM capacity of a GPU. This restriction hampers a researcher's flexibility tostudy different machine learning algorithms, forcing them to either use a lessdesirable network architecture or parallelize the processing across multipleGPUs. We propose a runtime memory manager that virtualizes the memory usage ofDNNs such that both GPU and CPU memory can simultaneously be utilized fortraining larger DNNs. Our virtualized DNN (vDNN) reduces the average memoryusage of AlexNet by 61% and OverFeat by 83%, a significant reduction in memoryrequirements of DNNs. Similar experiments on VGG-16, one of the deepest andmemory hungry DNNs to date, demonstrate the memory-efficiency of our proposal.vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to betrained on a single NVIDIA K40 GPU card containing 12 GB of memory, with 22%performance loss compared to a hypothetical GPU with enough memory to hold theentire DNN.
arxiv-15900-229 | Cascaded Subpatch Networks for Effective CNNs | http://arxiv.org/pdf/1603.00128v1.pdf | author:Xiaoheng Jiang, Yanwei Pang, Manli Sun, Xuelong Li category:cs.CV published:2016-03-01 summary:Conventional Convolutional Neural Networks (CNNs) use either a linear ornon-linear filter to extract features from an image patch (region) of spatialsize $ H\times W $ (Typically, $ H $ is small and is equal to $ W$, e.g., $ H $is 5 or 7). Generally, the size of the filter is equal to the size $ H\times W$ of the input patch. We argue that the representation ability of equal-sizestrategy is not strong enough. To overcome the drawback, we propose to usesubpatch filter whose spatial size $ h\times w $ is smaller than $ H\times W $.The proposed subpatch filter consists of two subsequent filters. The first oneis a linear filter of spatial size $ h\times w $ and is aimed at extractingfeatures from spatial domain. The second one is of spatial size $ 1\times 1 $and is used for strengthening the connection between different input featurechannels and for reducing the number of parameters. The subpatch filterconvolves with the input patch and the resulting network is called a subpatchnetwork. Taking the output of one subpatch network as input, we further repeatconstructing subpatch networks until the output contains only one neuron inspatial domain. These subpatch networks form a new network called CascadedSubpatch Network (CSNet). The feature layer generated by CSNet is called csconvlayer. For the whole input image, we construct a deep neural network bystacking a sequence of csconv layers. Experimental results on four benchmarkdatasets demonstrate the effectiveness and compactness of the proposed CSNet.For example, our CSNet reaches a test error of $ 5.68\% $ on the CIFAR10dataset without model averaging. To the best of our knowledge, this is the bestresult ever obtained on the CIFAR10 dataset.
arxiv-15900-230 | Learning Multilayer Channel Features for Pedestrian Detection | http://arxiv.org/pdf/1603.00124v1.pdf | author:Jiale Cao, Yanwei Pang, Xuelong Li category:cs.CV published:2016-03-01 summary:Pedestrian detection based on the combination of Convolutional Neural Network(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achievedgreat success. Generally, HOG+LUV are used to generate the candidate proposalsand then CNN classifies these proposals. Despite its success, there is stillroom for improvement. For example, CNN classifies these proposals by thefull-connected layer features while proposal scores and the features in theinner-layers of CNN are ignored. In this paper, we propose a unifying frameworkcalled Multilayer Channel Features (MCF) to overcome the drawback. It firstlyintegrates HOG+LUV with each layer of CNN into a multi-layer image channels.Based on the multi-layer image channels, a multi-stage cascade AdaBoost is thenlearned. The weak classifiers in each stage of the multi-stage cascade islearned from the image channels of corresponding layer. With more abundantfeatures, MCF achieves the state-of-the-art on Caltech pedestrian dataset(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves7.98% miss rate. As many non-pedestrian detection windows can be quicklyrejected by the first few stages, it accelerates detection speed by 1.43 times.By eliminating the highly overlapped detection windows with lower scores afterthe first stage, it's 4.07 times faster with negligible performance loss.
arxiv-15900-231 | Learning to Diagnose with LSTM Recurrent Neural Networks | http://arxiv.org/pdf/1511.03677v6.pdf | author:Zachary C. Lipton, David C. Kale, Charles Elkan, Randall Wetzell category:cs.LG published:2015-11-11 summary:Clinical medical data, especially in the intensive care unit (ICU), consistof multivariate time series of observations. For each patient visit (orepisode), sensor data and lab test results are recorded in the patient'sElectronic Health Record (EHR). While potentially containing a wealth ofinsights, the data is difficult to mine effectively, owing to varying length,irregular sampling and missing data. Recurrent Neural Networks (RNNs),particularly those using Long Short-Term Memory (LSTM) hidden units, arepowerful and increasingly popular models for learning from sequence data. Theyeffectively model varying length sequences and capture long range dependencies.We present the first study to empirically evaluate the ability of LSTMs torecognize patterns in multivariate time series of clinical measurements.Specifically, we consider multilabel classification of diagnoses, training amodel to classify 128 diagnoses given 13 frequently but irregularly sampledclinical measurements. First, we establish the effectiveness of a simple LSTMnetwork for modeling clinical data. Then we demonstrate a straightforward andeffective training strategy in which we replicate targets at each sequencestep. Trained only on raw time series, our models outperform several strongbaselines, including a multilayer perceptron trained on hand-engineeredfeatures.
arxiv-15900-232 | Designing Domain Specific Word Embeddings: Applications to Disease Surveillance | http://arxiv.org/pdf/1603.00106v1.pdf | author:Saurav Ghosh, Prithwish Chakraborty, Emily Cohn, John S. Brownstein, Naren Ramakrishnan category:cs.LG cs.CL stat.ML published:2016-03-01 summary:Traditional disease surveillance can be augmented with a wide variety ofrealtime sources such as news and social media. However, these sources are ingeneral unstructured and construction of surveillance tools such as taxonomicalcorrelations and trace mapping involves considerable human supervision. In thispaper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) whichwe use to model diseases and constituent attributes as word embeddings from theHealthMap news corpus. We use these word embeddings to create diseasetaxonomies and evaluate our model accuracy against human annotated taxonomies.We compare our accuracies against several state-of-the art word2vec methods.Our results demonstrate that Dis2Vec outperforms traditional distributed vectorrepresentations in its ability to faithfully capture disease attributes andaccurately forecast outbreaks.
arxiv-15900-233 | Sparse Multivariate Factor Regression | http://arxiv.org/pdf/1502.07334v5.pdf | author:Milad Kharratzadeh, Mark Coates category:stat.ML published:2015-02-25 summary:We consider the problem of multivariate regression in a setting where therelevant predictors could be shared among different responses. We propose analgorithm which decomposes the coefficient matrix into the product of a longmatrix and a wide matrix, with an elastic net penalty on the former and an$\ell_1$ penalty on the latter. The first matrix linearly transforms thepredictors to a set of latent factors, and the second one regresses theresponses on these factors. Our algorithm simultaneously performs dimensionreduction and coefficient estimation and automatically estimates the number oflatent factors from the data. Our formulation results in a non-convexoptimization problem, which despite its flexibility to impose effectivelow-dimensional structure, is difficult, or even impossible, to solve exactlyin a reasonable time. We specify an optimization algorithm based on alternatingminimization with three different sets of updates to solve this non-convexproblem and provide theoretical results on its convergence and optimality.Finally, we demonstrate the effectiveness of our algorithm via experiments onsimulated and real data.
arxiv-15900-234 | Variational Auto-encoded Deep Gaussian Processes | http://arxiv.org/pdf/1511.06455v2.pdf | author:Zhenwen Dai, Andreas Damianou, Javier González, Neil Lawrence category:cs.LG stat.ML published:2015-11-19 summary:We develop a scalable deep non-parametric generative model by augmenting deepGaussian processes with a recognition model. Inference is performed in a novelscalable variational framework where the variational posterior distributionsare reparametrized through a multilayer perceptron. The key aspect of thisreformulation is that it prevents the proliferation of variational parameterswhich otherwise grow linearly in proportion to the sample size. We derive a newformulation of the variational lower bound that allows us to distribute most ofthe computation in a way that enables to handle datasets of the size ofmainstream deep learning tasks. We show the efficacy of the method on a varietyof challenges including deep unsupervised learning and deep Bayesianoptimization.
arxiv-15900-235 | Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks | http://arxiv.org/pdf/1511.06448v3.pdf | author:Pouya Bashivan, Irina Rish, Mohammed Yeasin, Noel Codella category:cs.LG cs.CV published:2015-11-19 summary:One of the challenges in modeling cognitive events from electroencephalogram(EEG) data is finding representations that are invariant to inter- andintra-subject differences, as well as to inherent noise associated with suchdata. Herein, we propose a novel approach for learning such representationsfrom multi-channel EEG time-series, and demonstrate its advantages in thecontext of mental load classification task. First, we transform EEG activitiesinto a sequence of topology-preserving multi-spectral images, as opposed tostandard EEG analysis techniques that ignore such spatial information. Next, wetrain a deep recurrent-convolutional network inspired by state-of-the-art videoclassification to learn robust representations from the sequence of images. Theproposed approach is designed to preserve the spatial, spectral, and temporalstructure of EEG which leads to finding features that are less sensitive tovariations and distortions within each dimension. Empirical evaluation on thecognitive load classification task demonstrated significant improvements inclassification accuracy over current state-of-the-art approaches in this field.
arxiv-15900-236 | Reducing Overfitting in Deep Networks by Decorrelating Representations | http://arxiv.org/pdf/1511.06068v3.pdf | author:Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, Dhruv Batra category:cs.LG stat.ML published:2015-11-19 summary:One major challenge in training Deep Neural Networks is preventingoverfitting. Many techniques such as data augmentation and novel regularizerssuch as Dropout have been proposed to prevent overfitting without requiring amassive amount of training data. In this work, we propose a new regularizercalled DeCov which leads to significantly reduced overfitting (as indicated bythe difference between train and val performance), and better generalization.Our regularizer encourages diverse or non-redundant representations in DeepNeural Networks by minimizing the cross-covariance of hidden activations. Thissimple intuition has been explored in a number of past works but surprisinglyhas never been applied as a regularizer in supervised learning. Experimentsacross a range of datasets and network architectures show that this loss alwaysreduces overfitting while almost always maintaining or increasinggeneralization performance and often improving performance over Dropout.
arxiv-15900-237 | Density Modeling of Images using a Generalized Normalization Transformation | http://arxiv.org/pdf/1511.06281v4.pdf | author:Johannes Ballé, Valero Laparra, Eero P. Simoncelli category:cs.LG cs.CV published:2015-11-19 summary:We introduce a parametric nonlinear transformation that is well-suited forGaussianizing data from natural images. The data are linearly transformed, andeach component is then normalized by a pooled activity measure, computed byexponentiating a weighted sum of rectified and exponentiated components and aconstant. We optimize the parameters of the full transformation (lineartransform, exponents, weights, constant) over a database of natural images,directly minimizing the negentropy of the responses. The optimizedtransformation substantially Gaussianizes the data, achieving a significantlysmaller mutual information between transformed components than alternativemethods including ICA and radial Gaussianization. The transformation isdifferentiable and can be efficiently inverted, and thus induces a densitymodel on images. We show that samples of this model are visually similar tosamples of natural image patches. We demonstrate the use of the model as aprior probability density that can be used to remove additive noise. Finally,we show that the transformation can be cascaded, with each layer optimizedusing the same Gaussianization objective, thus offering an unsupervised methodof optimizing a deep network architecture.
arxiv-15900-238 | Easy Monotonic Policy Iteration | http://arxiv.org/pdf/1602.09118v1.pdf | author:Joshua Achiam category:cs.LG cs.AI stat.ML published:2016-02-29 summary:A key problem in reinforcement learning for control with general functionapproximators (such as deep neural networks and other nonlinear functions) isthat, for many algorithms employed in practice, updates to the policy or$Q$-function may fail to improve performance---or worse, actually cause thepolicy performance to degrade. Prior work has addressed this for policyiteration by deriving tight policy improvement bounds; by optimizing the lowerbound on policy improvement, a better policy is guaranteed. However, existingapproaches suffer from bounds that are hard to optimize in practice becausethey include sup norm terms which cannot be efficiently estimated ordifferentiated. In this work, we derive a better policy improvement bound wherethe sup norm of the policy divergence has been replaced with an averagedivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, thatgenerates sequences of policies with guaranteed non-decreasing returns and iseasy to implement in a sample-based framework.
arxiv-15900-239 | Continuous control with deep reinforcement learning | http://arxiv.org/pdf/1509.02971v5.pdf | author:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra category:cs.LG stat.ML published:2015-09-09 summary:We adapt the ideas underlying the success of Deep Q-Learning to thecontinuous action domain. We present an actor-critic, model-free algorithmbased on the deterministic policy gradient that can operate over continuousaction spaces. Using the same learning algorithm, network architecture andhyper-parameters, our algorithm robustly solves more than 20 simulated physicstasks, including classic problems such as cartpole swing-up, dexterousmanipulation, legged locomotion and car driving. Our algorithm is able to findpolicies whose performance is competitive with those found by a planningalgorithm with full access to the dynamics of the domain and its derivatives.We further demonstrate that for many of the tasks the algorithm can learnpolicies end-to-end: directly from raw pixel inputs.
arxiv-15900-240 | Visual Representations: Definint Properties and Deep Approximations | http://arxiv.org/pdf/1411.7676v9.pdf | author:Stefano Soatto, Alessandro Chiuso category:cs.CV published:2014-11-27 summary:Visual representations are defined in terms of minimal sufficient statisticsof visual data, for a class of tasks, that are also invariant to nuisancevariability. Minimal sufficiency guarantees that we can store a representationin lieu of raw data with smallest complexity and no performance loss on thetask at hand. Invariance guarantees that the statistic is constant with respectto uninformative transformations of the data. We derive analytical expressionsfor such representations and show they are related to feature descriptorscommonly used in computer vision, as well as to convolutional neural networks.This link highlights the assumptions and approximations tacitly assumed bythese methods and explains empirical practices such as clamping, pooling andjoint normalization.
arxiv-15900-241 | Generating Images from Captions with Attention | http://arxiv.org/pdf/1511.02793v2.pdf | author:Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov category:cs.LG cs.CV published:2015-11-09 summary:Motivated by the recent progress in generative models, we introduce a modelthat generates images from natural language descriptions. The proposed modeliteratively draws patches on a canvas, while attending to the relevant words inthe description. After training on Microsoft COCO, we compare our model withseveral baseline generative models on image generation and retrieval tasks. Wedemonstrate that our model produces higher quality samples than otherapproaches and generates images with novel scene compositions corresponding topreviously unseen captions in the dataset.
arxiv-15900-242 | Architectural Complexity Measures of Recurrent Neural Networks | http://arxiv.org/pdf/1602.08210v2.pdf | author:Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, Yoshua Bengio category:cs.LG cs.NE published:2016-02-26 summary:In this paper, we systematically analyse the connecting architectures ofrecurrent neural networks (RNNs). Our main contribution is twofold: first, wepresent a rigorous graph-theoretic framework describing the connectingarchitectures of RNNs in general. Second, we propose three architecturecomplexity measures of RNNs: (a) the recurrent depth, which captures the RNN'sover-time nonlinear complexity, (b) the feedforward depth, which captures thelocal input-output nonlinearity (similar to the "depth" in feedforward neuralnetworks (FNNs)), and (c) the recurrent skip coefficient which captures howrapidly the information propagates over time. Our experimental results showthat RNNs might benefit from larger recurrent depth and feedforward depth. Wefurther demonstrate that increasing recurrent skip coefficient offersperformance boosts on long term dependency problems, as we improve thestate-of-the-art for sequential MNIST dataset.
arxiv-15900-243 | On Complex Valued Convolutional Neural Networks | http://arxiv.org/pdf/1602.09046v1.pdf | author:Nitzan Guberman category:cs.NE published:2016-02-29 summary:Convolutional neural networks (CNNs) are the cutting edge model forsupervised machine learning in computer vision. In recent years CNNs haveoutperformed traditional approaches in many computer vision tasks such asobject detection, image classification and face recognition. CNNs arevulnerable to overfitting, and a lot of research focuses on findingregularization methods to overcome it. One approach is designing task specificmodels based on prior knowledge. Several works have shown that properties of natural images can be easilycaptured using complex numbers. Motivated by these works, we present avariation of the CNN model with complex valued input and weights. We constructthe complex model as a generalization of the real model. Lack of order over thecomplex field raises several difficulties both in the definition and in thetraining of the network. We address these issues and suggest possiblesolutions. The resulting model is shown to be a restricted form of a real valued CNNwith twice the parameters. It is sensitive to phase structure, and we suggestit serves as a regularized model for problems where such structure isimportant. This suggestion is verified empirically by comparing the performanceof a complex and a real network in the problem of cell detection. The twonetworks achieve comparable results, and although the complex model is hard totrain, it is significantly less vulnerable to overfitting. We also demonstratethat the complex network detects meaningful phase structure in the data.
arxiv-15900-244 | Large-Scale Approximate Kernel Canonical Correlation Analysis | http://arxiv.org/pdf/1511.04773v4.pdf | author:Weiran Wang, Karen Livescu category:cs.LG published:2015-11-15 summary:Kernel canonical correlation analysis (KCCA) is a nonlinear multi-viewrepresentation learning technique with broad applicability in statistics andmachine learning. Although there is a closed-form solution for the KCCAobjective, it involves solving an $N\times N$ eigenvalue system where $N$ isthe training set size, making its computational requirements in both memory andtime prohibitive for large-scale problems. Various approximation techniqueshave been developed for KCCA. A commonly used approach is to first transformthe original inputs to an $M$-dimensional random feature space so that innerproducts in the feature space approximate kernel evaluations, and then applylinear CCA to the transformed inputs. In many applications, however, thedimensionality $M$ of the random feature space may need to be very large inorder to obtain a sufficiently good approximation; it then becomes challengingto perform the linear CCA step on the resulting very high-dimensional datamatrices. We show how to use a stochastic optimization algorithm, recentlyproposed for linear CCA and its neural-network extension, to further alleviatethe computation requirements of approximate KCCA. This approach allows us torun approximate KCCA on a speech dataset with $1.4$ million training samplesand a random feature space of dimensionality $M=100000$ on a typicalworkstation.
arxiv-15900-245 | Better Computer Go Player with Neural Network and Long-term Prediction | http://arxiv.org/pdf/1511.06410v3.pdf | author:Yuandong Tian, Yan Zhu category:cs.LG cs.AI published:2015-11-19 summary:Competing with top human players in the ancient game of Go has been along-term goal of artificial intelligence. Go's high branching factor makestraditional search techniques ineffective, even on leading-edge hardware, andGo's evaluation function could change drastically with one stone change. Recentworks [Maddison et al. (2015); Clark & Storkey (2015)] show that search is notstrictly necessary for machine Go players. A pure pattern-matching approach,based on a Deep Convolutional Neural Network (DCNN) that predicts the nextmove, can perform as well as Monte Carlo Tree Search (MCTS)-based open sourceGo engines such as Pachi [Baudis & Gailly (2012)] if its search budget islimited. We extend this idea in our bot named darkforest, which relies on aDCNN designed for long-term predictions. Darkforest substantially improves thewin rate for pattern-matching approaches against MCTS-based approaches, evenwith looser search budgets. Against human players, the newest versions,darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, asubstantial improvement upon the estimated 4k-5k ranks for DCNN reported inClark & Storkey (2015) based on games against other machine players. AddingMCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rolloutsit achieves a stable 5d level in KGS server, on par with state-of-the-art GoAIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al.(2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament.
arxiv-15900-246 | Beyond CCA: Moment Matching for Multi-View Models | http://arxiv.org/pdf/1602.09013v1.pdf | author:Anastasia Podosinnikova, Francis Bach, Simon Lacoste-Julien category:stat.ML cs.LG published:2016-02-29 summary:We introduce three novel semi-parametric extensions of probabilisticcanonical correlation analysis with identifiability guarantees. We considermoment matching techniques for estimation in these models. For that, by drawingexplicit links between the new models and a discrete version of independentcomponent analysis (DICA), we first extend the DICA cumulant tensors to the newdiscrete version of CCA. By further using a close connection with independentcomponent analysis, we introduce generalized covariance matrices, which canreplace the cumulant tensors in the moment matching framework, and, therefore,improve sample complexity and simplify derivations and algorithmssignificantly. As the tensor power method or orthogonal joint diagonalizationare not applicable in the new setting, we use non-orthogonal jointdiagonalization techniques for matching the cumulants. We demonstrateperformance of the proposed models and estimation techniques on experimentswith both synthetic and real datasets.
arxiv-15900-247 | Distributional Smoothing with Virtual Adversarial Training | http://arxiv.org/pdf/1507.00677v8.pdf | author:Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii category:stat.ML cs.LG published:2015-07-02 summary:We propose local distributional smoothness (LDS), a new notion of smoothnessfor statistical model that can be used as a regularization term to promote thesmoothness of the model distribution. We named the LDS based regularization asvirtual adversarial training (VAT). The LDS of a model at an input datapoint isdefined as the KL-divergence based robustness of the model distribution againstlocal perturbation around the datapoint. VAT resembles adversarial training,but distinguishes itself in that it determines the adversarial direction fromthe model distribution alone without using the label information, making itapplicable to semi-supervised learning. The computational cost for VAT isrelatively low. For neural network, the approximated gradient of the LDS can becomputed with no more than three pairs of forward and back propagations. Whenwe applied our technique to supervised and semi-supervised learning for theMNIST dataset, it outperformed all the training methods other than the currentstate of the art method, which is based on a highly advanced generative model.We also applied our method to SVHN and NORB, and confirmed our method'ssuperior performance over the current state of the art semi-supervised methodapplied to these datasets.
arxiv-15900-248 | Pixel Recurrent Neural Networks | http://arxiv.org/pdf/1601.06759v2.pdf | author:Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu category:cs.CV cs.LG cs.NE published:2016-01-25 summary:Modeling the distribution of natural images is a landmark problem inunsupervised learning. This task requires an image model that is at onceexpressive, tractable and scalable. We present a deep neural network thatsequentially predicts the pixels in an image along the two spatial dimensions.Our method models the discrete probability of the raw pixel values and encodesthe complete set of dependencies in the image. Architectural novelties includefast two-dimensional recurrent layers and an effective use of residualconnections in deep recurrent networks. We achieve log-likelihood scores onnatural images that are considerably better than the previous state of the art.Our main results also provide benchmarks on the diverse ImageNet dataset.Samples generated from the model appear crisp, varied and globally coherent.
arxiv-15900-249 | Even Trolls Are Useful: Efficient Link Classification in Signed Networks | http://arxiv.org/pdf/1602.08986v1.pdf | author:Géraud Le Falher, Fabio Vitale category:cs.LG cs.SI physics.soc-ph published:2016-02-29 summary:We address the problem of classifying the links of signed social networksgiven their full structural topology. Motivated by a binary user behaviourassumption, which is supported by decades of research in psychology, we developan efficient and surprisingly simple approach to solve this classificationproblem. Our methods operate both within the active and batch settings. Wedemonstrate that the algorithms we developed are extremely fast in boththeoretical and practical terms. Within the active setting, we provide a newcomplexity measure and a rigorous analysis of our methods that hold forarbitrary signed networks. We validate our theoretical claims carrying out aset of experiments on three well known real-world datasets, showing that ourmethods outperform the competitors while being much faster.
arxiv-15900-250 | Clustering Based Feature Learning on Variable Stars | http://arxiv.org/pdf/1602.08977v1.pdf | author:Cristóbal Mackenzie, Karim Pichara, Pavlos Protopapas category:astro-ph.SR cs.CV published:2016-02-29 summary:The success of automatic classification of variable stars strongly depends onthe lightcurve representation. Usually, lightcurves are represented as a vectorof many statistical descriptors designed by astronomers called features. Thesedescriptors commonly demand significant computational power to calculate,require substantial research effort to develop and do not guarantee goodperformance on the final classification task. Today, lightcurve representationis not entirely automatic; algorithms that extract lightcurve features aredesigned by humans and must be manually tuned up for every survey. The vastamounts of data that will be generated in future surveys like LSST meanastronomers must develop analysis pipelines that are both scalable andautomated. Recently, substantial efforts have been made in the machine learningcommunity to develop methods that prescind from expert-designed and manuallytuned features for features that are automatically learned from data. In thiswork we present what is, to our knowledge, the first unsupervised featurelearning algorithm designed for variable stars. Our method first extracts alarge number of lightcurve subsequences from a given set of photometric data,which are then clustered to find common local patterns in the time series.Representatives of these patterns, called exemplars, are then used to transformlightcurves of a labeled set into a new representation that can then be used totrain an automatic classifier. The proposed algorithm learns the features fromboth labeled and unlabeled lightcurves, overcoming the bias generated when thelearning process is done only with labeled data. We test our method on MACHOand OGLE datasets; the results show that the classification performance weachieve is as good and in some cases better than the performance achieved usingtraditional features, while the computational cost is significantly lower.
arxiv-15900-251 | FALDOI: Large Displacement Optical Flow by Astute Initialization | http://arxiv.org/pdf/1602.08960v1.pdf | author:Roberto P. Palomares, Gloria Haro, Coloma Ballester, Enric Meinhardt-Llopis category:cs.CV published:2016-02-29 summary:We propose a large displacement optical flow method that introduces a newstrategy to compute a good local minimum of any optical flow energy functional.The method requires a given set of discrete matches, which can be extremelysparse, and an energy functional. The matches are used to guide a structuredcoordinate-descent of the energy functional around these keypoints. It resultsin a two-step minimization method at the finest scale which is very robust tothe inevitable outliers of the sparse matcher, and it is better than the multi-scale methods, especially when there are small objects with very largedisplacements, that the multi-scale methods are incapable to find. Indeed, theproposed method recovers the correct motion field of any object which has atleast one correct match, regardless of the magnitude of the displacement. Wevalidate our proposal using several optical flow variational models. Theresults consistently outperform the coarse-to-fine approaches and achieve goodqualitative and quantitative performance on the standard optical flowbenchmarks.
arxiv-15900-252 | Representation of linguistic form and function in recurrent neural networks | http://arxiv.org/pdf/1602.08952v1.pdf | author:Ákos Kádár, Grzegorz Chrupała, Afra Alishahi category:cs.CL cs.LG published:2016-02-29 summary:We present novel methods for analysing the activation patterns of RNNs andidentifying the types of linguistic structure they learn. As a case study, weuse a multi-task gated recurrent network model consisting of two parallelpathways with shared word embeddings trained on predicting the representationsof the visual scene corresponding to an input sentence, and predicting the nextword in the same sentence. We show that the image prediction pathway issensitive to the information structure of the sentence, and pays selectiveattention to lexical categories and grammatical functions that carry semanticinformation. It also learns to treat the same input token differently dependingon its grammatical functions in the sentence. The language model iscomparatively more sensitive to words with a syntactic function. Our analysisof the function of individual hidden units shows that each pathway containsspecialized units tuned to patterns informative for the task, some of which cancarry activations to later time steps to encode long-term dependencies.
arxiv-15900-253 | Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification | http://arxiv.org/pdf/1602.05659v2.pdf | author:Fuqiang Liu, Fukun Bi, Yiding Yang, Liang Chen category:cs.CV cs.LG published:2016-02-18 summary:This paper proposes a universal method, Boost Picking, to train supervisedclassification models mainly by un-labeled data. Boost Picking only adopts twoweak classifiers to estimate and correct the error. It is theoretically provedthat Boost Picking could train a supervised model mainly by un-labeled data aseffectively as the same model trained by 100% labeled data, only if recalls ofthe two weak classifiers are all greater than zero and the sum of precisions isgreater than one. Based on Boost Picking, we present "Test along with Training(TawT)" to improve the generalization of supervised models. Both Boost Pickingand TawT are successfully tested in varied little data sets.
arxiv-15900-254 | $L_2$Boosting in High-Dimensions: Rate of Convergence | http://arxiv.org/pdf/1602.08927v1.pdf | author:Ye Luo, Martin Spindler category:stat.ML cs.LG math.ST stat.ME stat.TH published:2016-02-29 summary:Boosting is one of the most significant developments in machine learning.This paper studies the rate of convergence of $L_2$Boosting, which is tailoredfor regression, in a high-dimensional setting. Moreover, we introduce so-called\textquotedblleft post-Boosting\textquotedblright. This is a post-selectionestimator which applies ordinary least squares to the variables selected in thefirst stage by $L_2$Boosting. Another variant is orthogonal boosting whereafter each step an orthogonal projection is conducted. We show that bothpost-$L_2$Boosting and the orthogonal boosting achieve the same rate ofconvergence as Lasso in a sparse, high-dimensional setting. The\textquotedblleft classical\textquotedblright $L_2$Boosting achieves a slowerconvergence rate for prediction, but no assumptions on the design matrix areimposed for this result in contrast to rates e.g.~established with LASSO. Wealso introduce rules for early stopping which can easily be implemented andwill be used in applied work. Moreover, our results also allow a directcomparison between LASSO and boosting that has been missing in the literature.Finally, we present simulation studies to illustrate the relevance of ourtheoretical results and to provide insights into the practical aspects ofboosting. In the simulation studies post-$L_2$Boosting clearly outperformsLASSO.
arxiv-15900-255 | Neural Programmer-Interpreters | http://arxiv.org/pdf/1511.06279v4.pdf | author:Scott Reed, Nando de Freitas category:cs.LG cs.NE published:2015-11-19 summary:We propose the neural programmer-interpreter (NPI): a recurrent andcompositional neural network that learns to represent and execute programs. NPIhas three learnable components: a task-agnostic recurrent core, a persistentkey-value program memory, and domain-specific encoders that enable a single NPIto operate in multiple perceptually diverse environments with distinctaffordances. By learning to compose lower-level programs to expresshigher-level programs, NPI reduces sample complexity and increasesgeneralization ability compared to sequence-to-sequence LSTMs. The programmemory allows efficient learning of additional tasks by building on existingprograms. NPI can also harness the environment (e.g. a scratch pad withread-write pointers) to cache intermediate results of computation, lesseningthe long-term memory burden on recurrent hidden units. In this work we trainthe NPI with fully-supervised execution traces; each program has examplesequences of calls to the immediate subprograms conditioned on the input.Rather than training on a huge number of relatively weak labels, NPI learnsfrom a small number of rich examples. We demonstrate the capability of ourmodel to learn several types of compositional programs: addition, sorting, andcanonicalizing 3D models. Furthermore, a single NPI learns to execute theseprograms and all 21 associated subprograms.
arxiv-15900-256 | Convergence radius and sample complexity of ITKM algorithms for dictionary learning | http://arxiv.org/pdf/1503.07027v3.pdf | author:Karin Schnass category:cs.LG cs.IT math.IT published:2015-03-24 summary:In this work we show that iterative thresholding and K-means (ITKM)algorithms can recover a generating dictionary with K atoms from noisy $S$sparse signals up to an error $\tilde \varepsilon$ as long as theinitialisation is within a convergence radius, that is up to a $\log K$ factorinversely proportional to the dynamic range of the signals, and the sample sizeis proportional to $K \log K \tilde \varepsilon^{-2}$. The results are validfor arbitrary target errors if the sparsity level is of the order of the squareof the signal dimension $d$ and for target errors down to $K^{-\ell}$ if $S$scales as $S \leq d/(\ell \log K)$.
arxiv-15900-257 | Stochastic bandits on a social network: Collaborative learning with local information sharing | http://arxiv.org/pdf/1602.08886v1.pdf | author:Ravi Kumar Kolla, Krishna Jagannathan, Aditya Gopalan category:cs.LG stat.ML published:2016-02-29 summary:We consider a collaborative online learning paradigm, wherein a group ofagents connected through a social network are engaged in learning a Multi-ArmedBandit problem. Each time an agent takes an action, the corresponding reward isinstantaneously observed by the agent, as well as its neighbours in the socialnetwork. We perform a regret analysis of various policies in this collaborativelearning setting. A key finding of this paper is that appropriate networkextensions of widely-studied single agent learning policies do not perform wellin terms of regret. In particular, we identify a class of non-altruistic andindividually consistent policies, which could suffer a large regret. We alsoshow that the regret performance can be substantially improved by exploitingthe network structure. Specifically, we consider a star network, which is acommon motif in hierarchical social networks, and show that the hub agent canbe used as an information sink, to aid the learning rates of the entirenetwork. We also present numerical experiments to corroborate our analyticalresults.
arxiv-15900-258 | Pandora: Description of a Painting Database for Art Movement Recognition with Baselines and Perspectives | http://arxiv.org/pdf/1602.08855v1.pdf | author:Corneliu Florea, Razvan Condorovici, Constantin Vertan, Raluca Boia, Laura Florea, Ruxandra Vranceanu category:cs.CV published:2016-02-29 summary:To facilitate computer analysis of visual art, in the form of paintings, weintroduce Pandora (Paintings Dataset for Recognizing the Art movement)database, a collection of digitized paintings labelled with respect to theartistic movement. Noting that the set of databases available as benchmarks forevaluation is highly reduced and most existing ones are limited in variabilityand number of images, we propose a novel large scale dataset of digitalpaintings. The database consists of more than 7700 images from 12 artmovements. Each genre is illustrated by a number of images varying from 250 tonearly 1000. We investigate how local and global features and classificationsystems are able to recognize the art movement. Our experimental resultssuggest that accurate recognition is achievable by a combination of variouscategories.To facilitate computer analysis of visual art, in the form ofpaintings, we introduce Pandora (Paintings Dataset for Recognizing the Artmovement) database, a collection of digitized paintings labelled with respectto the artistic movement. Noting that the set of databases available asbenchmarks for evaluation is highly reduced and most existing ones are limitedin variability and number of images, we propose a novel large scale dataset ofdigital paintings. The database consists of more than 7700 images from 12 artmovements. Each genre is illustrated by a number of images varying from 250 tonearly 1000. We investigate how local and global features and classificationsystems are able to recognize the art movement. Our experimental resultssuggest that accurate recognition is achievable by a combination of variouscategories.
arxiv-15900-259 | Simple Bayesian Algorithms for Best Arm Identification | http://arxiv.org/pdf/1602.08448v2.pdf | author:Daniel Russo category:cs.LG published:2016-02-26 summary:This paper considers the optimal adaptive allocation of measurement effortfor identifying the best among a finite set of options or designs. Anexperimenter sequentially chooses designs to measure and observes noisy signalsof their quality with the goal of confidently identifying the best design aftera small number of measurements. I propose three simple Bayesian algorithms foradaptively allocating measurement effort. One is Top-Two Probability sampling,which computes the two designs with the highest posterior probability of beingoptimal, and then randomizes to select among these two. One is a variant atop-two sampling which considers not only the probability a design is optimal,but the expected amount by which it exceeds other designs. The final algorithmis a modified version of Thompson sampling that is tailored for identifying thebest design. I prove that these simple algorithms satisfy a strong optimalityproperty. In a frequestist setting where the true quality of the designs isfixed, the posterior is said to be consistent if it correctly identifies theoptimal design, in the sense that that the posterior probability assigned tothe event that some other design is optimal converges to zero as measurementsare collected. I show that under the proposed algorithms this convergenceoccurs at an exponential rate, and the corresponding exponent is the bestpossible among all allocation
arxiv-15900-260 | Bioinformatics and Classical Literary Study | http://arxiv.org/pdf/1602.08844v1.pdf | author:Pramit Chaudhuri, Joseph P. Dexter category:cs.CL published:2016-02-29 summary:This paper describes a collaborative project between classicists,quantitative biologists, and computer scientists to apply ideas and methodsdrawn from the sciences to the study of literature. A core goal of the projectis the use of computational biology, natural language processing, and machinelearning techniques to investigate intertextuality, reception, and relatedphenomena of literary significance. As a case study in our approach, here wedescribe the use of sequence alignment, a common technique in genomics, todetect intertextuality in Latin literature. Sequence alignment is distinguishedby its ability to find inexact verbal parallels, which makes it ideal foridentifying phonetic resemblances in large corpora of Latin texts. Althoughespecially suited to Latin, sequence alignment in principle can be extended tomany other languages.
arxiv-15900-261 | Seq-NMS for Video Object Detection | http://arxiv.org/pdf/1602.08465v2.pdf | author:Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, Thomas S. Huang category:cs.CV published:2016-02-26 summary:Video object detection is challenging because objects that are easilydetected in one frame may be difficult to detect in another frame within thesame clip. Recently, there have been major advances for doing object detectionin a single image. These methods typically contain three phases: (i) objectproposal generation (ii) object classification and (iii) post-processing. Wepropose a modification of the post-processing phase that uses high-scoringobject detections from nearby frames to boost scores of weaker detectionswithin the same clip. We show that our method obtains superior results tostate-of-the-art single image object detection techniques. Our method placed3rd in the video object detection (VID) task of the ImageNet Large Scale VisualRecognition Challenge 2015 (ILSVRC2015).
arxiv-15900-262 | Exploring the coevolution of predator and prey morphology and behavior | http://arxiv.org/pdf/1602.08802v1.pdf | author:Randal S. Olson, Arend Hintze, Fred C. Dyer, Jason H. Moore, Christoph Adami category:q-bio.PE cs.NE published:2016-02-29 summary:A common idiom in biology education states, "Eyes in the front, the animalhunts. Eyes on the side, the animal hides." In this paper, we explore onepossible explanation for why predators tend to have forward-facing, high-acuityvisual systems. We do so using an agent-based computational model of evolution,where predators and prey interact and adapt their behavior and morphology toone another over successive generations of evolution. In this model, we observea coevolutionary cycle between prey swarming behavior and the predator's visualsystem, where the predator and prey continually adapt their visual system andbehavior, respectively, over evolutionary time in reaction to one another dueto the well-known "predator confusion effect." Furthermore, we provide evidencethat the predator visual system is what drives this coevolutionary cycle, andsuggest that the cycle could be closed if the predator evolves a hybrid visualsystem capable of narrow, high-acuity vision for tracking prey as well asbroad, coarse vision for prey discovery. Thus, the conflicting demands imposedon a predator's visual system by the predator confusion effect could have ledto the evolution of complex eyes in many predators.
arxiv-15900-263 | Iterative Aggregation Method for Solving Principal Component Analysis Problems | http://arxiv.org/pdf/1602.08800v1.pdf | author:Vitaly Bulgakov category:cs.NA cs.IR cs.LG published:2016-02-29 summary:Motivated by the previously developed multilevel aggregation method forsolving structural analysis problems a novel two-level aggregation approach forefficient iterative solution of Principal Component Analysis (PCA) problems isproposed. The course aggregation model of the original covariance matrix isused in the iterative solution of the eigenvalue problem by a power iterationsmethod. The method is tested on several data sets consisting of large number oftext documents.
arxiv-15900-264 | Monomial Gamma Monte Carlo Sampling | http://arxiv.org/pdf/1602.07800v3.pdf | author:Yizhe Zhang, Xiangyu Wang, Changyou Chen, Kai Fan, Lawrence Carin category:stat.ML stat.ME published:2016-02-25 summary:We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling bydemonstrating their connection under the canonical transformation fromHamiltonian mechanics. This insight enables us to extend HMC and slice samplingto a broader family of samplers, called monomial Gamma samplers (MGS). Weanalyze theoretically the mixing performance of such samplers by proving thatthe MGS draws samples from a target distribution with zero-autocorrelation, inthe limit of a single parameter. This property potentially allows us togenerating decorrelated samples, which is not achievable by existing MCMCalgorithms. We further show that this performance gain is obtained at a cost ofincreasing the complexity of numerical integrators. Our theoretical results arevalidated with synthetic data and real-world applications.
arxiv-15900-265 | A Spectral Algorithm for Inference in Hidden Semi-Markov Models | http://arxiv.org/pdf/1407.3422v3.pdf | author:Igor Melnyk, Arindam Banerjee category:stat.ML cs.LG published:2014-07-12 summary:Hidden semi-Markov models (HSMMs) are latent variable models which allowlatent state persistence and can be viewed as a generalization of the popularhidden Markov models (HMMs). In this paper, we introduce a novel spectralalgorithm to perform inference in HSMMs. Unlike expectation maximization (EM),our approach correctly estimates the probability of given observation sequencebased on a set of training sequences. Our approach is based on estimatingmoments from the sample, whose number of dimensions depends onlylogarithmically on the maximum length of the hidden state persistence.Moreover, the algorithm requires only a few matrix inversions and is thereforecomputationally efficient. Empirical evaluations on synthetic and real datademonstrate the advantage of the algorithm over EM in terms of speed andaccuracy, especially for large datasets.
arxiv-15900-266 | SparkNet: Training Deep Networks in Spark | http://arxiv.org/pdf/1511.06051v4.pdf | author:Philipp Moritz, Robert Nishihara, Ion Stoica, Michael I. Jordan category:stat.ML cs.DC cs.LG cs.NE math.OC published:2015-11-19 summary:Training deep networks is a time-consuming process, with networks for objectrecognition often requiring multiple days to train. For this reason, leveragingthe resources of a cluster to speed up training is an important area of work.However, widely-popular batch-processing computational frameworks likeMapReduce and Spark were not designed to support the asynchronous andcommunication-intensive workloads of existing distributed deep learningsystems. We introduce SparkNet, a framework for training deep networks inSpark. Our implementation includes a convenient interface for reading data fromSpark RDDs, a Scala interface to the Caffe deep learning framework, and alightweight multi-dimensional tensor library. Using a simple parallelizationscheme for stochastic gradient descent, SparkNet scales well with the clustersize and tolerates very high-latency communication. Furthermore, it is easy todeploy and use with no parameter tuning, and it is compatible with existingCaffe models. We quantify the dependence of the speedup obtained by SparkNet onthe number of machines, the communication frequency, and the cluster'scommunication overhead, and we benchmark our system's performance on theImageNet dataset.
arxiv-15900-267 | Semi-Markov Switching Vector Autoregressive Model-based Anomaly Detection in Aviation Systems | http://arxiv.org/pdf/1602.06550v2.pdf | author:Igor Melnyk, Arindam Banerjee, Bryan Matthews, Nikunj Oza category:cs.LG stat.AP stat.ML published:2016-02-21 summary:In this work we consider the problem of anomaly detection in heterogeneous,multivariate, variable-length time series datasets. Our focus is on theaviation safety domain, where data objects are flights and time series aresensor readings and pilot switches. In this context the goal is to detectanomalous flight segments, due to mechanical, environmental, or human factorsin order to identifying operationally significant events and provide insightsinto the flight operations and highlight otherwise unavailable potential safetyrisks and precursors to accidents. For this purpose, we propose a frameworkwhich represents each flight using a semi-Markov switching vectorautoregressive (SMS-VAR) model. Detection of anomalies is then based onmeasuring dissimilarities between the model's prediction and data observation.The framework is scalable, due to the inherent parallel nature of mostcomputations, and can be used to perform online anomaly detection. Extensiveexperimental results on simulated and real datasets illustrate that theframework can detect various types of anomalies along with the key parametersinvolved.
arxiv-15900-268 | Does quantification without adjustments work? | http://arxiv.org/pdf/1602.08780v1.pdf | author:Dirk Tasche category:stat.ML cs.LG math.ST stat.TH 62C10 published:2016-02-28 summary:Classification is the task of predicting the class labels of objects based onthe observation of their features. In contrast, quantification has been definedas the task of determining the prevalence of the positive class labels in atarget dataset. The simplest approach to quantification is Classify & Countwhere a classifier is optimised for classification on a training set andapplied to the target dataset for the prediction of positive class labels. Thenumber of predicted positive labels is then used as an estimate of the positiveclass prevalence in the target dataset. Since the performance of Classify &Count for quantification is known to be inferior its results typically aresubject to adjustments. However, some researchers recently have suggested thatClassify & Count might actually work without adjustments if it is based on aclassifier that was specifically trained for quantification. We discuss thetheoretical foundation for this claim and explore its potential and limitationswith a numerical example based on the binormal model with equal variances.
arxiv-15900-269 | Estimating Structured Vector Autoregressive Model | http://arxiv.org/pdf/1602.06606v2.pdf | author:Igor Melnyk, Arindam Banerjee category:math.ST stat.ML stat.TH published:2016-02-21 summary:While considerable advances have been made in estimating high-dimensionalstructured models from independent data using Lasso-type models, limitedprogress has been made for settings when the samples are dependent. We considerestimating structured VAR (vector auto-regressive models), where the structurecan be captured by any suitable norm, e.g., Lasso, group Lasso, order weightedLasso, sparse group Lasso, etc. In VAR setting with correlated noise, althoughthere is strong dependence over time and covariates, we establish bounds on thenon-asymptotic estimation error of structured VAR parameters. Surprisingly, theestimation error is of the same order as that of the corresponding Lasso-typeestimator with independent samples, and the analysis holds for any norm. Ouranalysis relies on results in generic chaining, sub-exponential martingales,and spectral representation of VAR models. Experimental results on syntheticdata with a variety of structures as well as real aviation data are presented,validating theoretical results.
arxiv-15900-270 | Convergent Learning: Do different neural networks learn the same representations? | http://arxiv.org/pdf/1511.07543v3.pdf | author:Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John Hopcroft category:cs.LG cs.NE published:2015-11-24 summary:Recent success in training deep neural networks have prompted activeinvestigation into the features learned on their intermediate layers. Suchresearch is difficult because it requires making sense of non-linearcomputations performed by millions of parameters, but valuable because itincreases our ability to understand current models and create improved versionsof them. In this paper we investigate the extent to which neural networksexhibit what we call convergent learning, which is when the representationslearned by multiple nets converge to a set of features which are eitherindividually similar between networks or where subsets of features span similarlow-dimensional spaces. We propose a specific method of probingrepresentations: training multiple networks and then comparing and contrastingtheir individual, learned representations at the level of neurons or groups ofneurons. We begin research into this question using three techniques toapproximately align different neural networks on a feature level: a bipartitematching approach that makes one-to-one assignments between neurons, a sparseprediction approach that finds one-to-many mappings, and a spectral clusteringapproach that finds many-to-many mappings. This initial investigation reveals afew previously unknown properties of neural networks, and we argue that futureresearch into the question of convergent learning will yield many more. Theinsights described here include (1) that some features are learned reliably inmultiple networks, yet other features are not consistently learned; (2) thatunits learn to span low-dimensional subspaces and, while these subspaces arecommon to multiple networks, the specific basis vectors learned are not; (3)that the representation codes show evidence of being a mix between a local codeand slightly, but not fully, distributed codes across multiple units.
arxiv-15900-271 | Structured Prediction with Test-time Budget Constraints | http://arxiv.org/pdf/1602.08761v1.pdf | author:Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.CL cs.CV cs.LG published:2016-02-28 summary:We study the problem of structured prediction under test-time budgetconstraints. We propose a novel approach applicable to a wide range ofstructured prediction problems in computer vision and natural languageprocessing. Our approach seeks to adaptively generate computationally costlyfeatures during test-time in order to reduce the computational cost ofprediction while maintaining prediction performance. We show that training theadaptive feature generation system can be reduced to a series of structuredlearning problems, resulting in efficient training using existing structuredlearning algorithms. This framework provides theoretical justification forseveral existing heuristic approaches found in literature. We evaluate ourproposed adaptive system on two real-world structured prediction tasks, opticalcharacter recognition (OCR) and dependency parsing. For OCR our method cuts thefeature acquisition time by half coming within a 1% margin of top accuracy. Fordependency parsing we realize an overall runtime gain of 20% withoutsignificant loss in performance.
arxiv-15900-272 | Optimizing the Learning Order of Chinese Characters Using a Novel Topological Sort Algorithm | http://arxiv.org/pdf/1602.08742v1.pdf | author:James C. Loach, Jinzhao Wang category:cs.CL physics.soc-ph published:2016-02-28 summary:We develop a novel algorithm for optimizing the order in which Chinesecharacters are learned, one that incorporates the benefits of learning them inorder of usage frequency and in order of their hierarchal structuralrelationships. We show that our work outperforms previously published orderingsand algorithms. Our algorithm is applicable to any scheduling task where nodeshave intrinsic differences in importance and must be visited in topologicalorder.
arxiv-15900-273 | Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task? | http://arxiv.org/pdf/1602.08741v1.pdf | author:Nikolay N. Vasiliev category:cs.CL published:2016-02-28 summary:The most studied and most successful language models were developed andevaluated mainly for English and other close European languages, such asFrench, German, etc. It is important to study applicability of these models toother languages. The use of vector space models for Russian was recentlystudied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These modelswere evaluated against word semantic similarity task. For our knowledge Twitterwas not considered as a corpus for this task, with this work we fill the gap.Results for vectors trained on Twitter corpus are comparable in accuracy withother single-corpus trained models, although the best performance is currentlyachieved by combination of multiple corpora.
arxiv-15900-274 | A Structured Variational Auto-encoder for Learning Deep Hierarchies of Sparse Features | http://arxiv.org/pdf/1602.08734v1.pdf | author:Tim Salimans category:stat.ML cs.LG stat.CO published:2016-02-28 summary:In this note we present a generative model of natural images consisting of adeep hierarchy of layers of latent random variables, each of which follows anew type of distribution that we call rectified Gaussian. These rectifiedGaussian units allow spike-and-slab type sparsity, while retaining thedifferentiability necessary for efficient stochastic gradient variationalinference. To learn the parameters of the new model, we approximate theposterior of the latent variables with a variational auto-encoder. Rather thanmaking the usual mean-field assumption however, the encoder parameterizes a newtype of structured variational approximation that retains the priordependencies of the generative model. Using this structured posteriorapproximation, we are able to perform joint training of deep models with manylayers of latent random variables, without having to resort to stacking orother layerwise training procedures.
arxiv-15900-275 | Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus | http://arxiv.org/pdf/1602.08715v1.pdf | author:Avi Shmidman, Moshe Koppel, Ely Porat category:cs.CL published:2016-02-28 summary:We propose a method for efficiently finding all parallel passages in a largecorpus, even if the passages are not quite identical due to rephrasing andorthographic variation. The key ideas are the representation of each word inthe corpus by its two most infrequent letters, finding matched pairs of stringsof four or five words that differ by at most one word and then identifyingclusters of such matched pairs. Using this method, over 4600 parallel pairs ofpassages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus ofover 1.8 million words, in just over 30 seconds. Empirical comparisons onsample data indicate that the coverage obtained by our method is essentiallythe same as that obtained using slow exhaustive methods.
arxiv-15900-276 | An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family | http://arxiv.org/pdf/1511.05042v3.pdf | author:Alexandre de Brébisson, Pascal Vincent category:cs.NE cs.LG stat.ML published:2015-11-16 summary:In a multi-class classification problem, it is standard to model the outputof a neural network as a categorical distribution conditioned on the inputs.The output must therefore be positive and sum to one, which is traditionallyenforced by a softmax. This probabilistic mapping allows to use the maximumlikelihood principle, which leads to the well-known log-softmax loss. Howeverthe choice of the softmax function seems somehow arbitrary as there are manyother possible normalizing functions. It is thus unclear why the log-softmaxloss would perform better than other loss alternatives. In particular Vincentet al. (2015) recently introduced a class of loss functions, called thespherical family, for which there exists an efficient algorithm to compute theupdates of the output weights irrespective of the output size. In this paper,we explore several loss functions from this family as possible alternatives tothe traditional log-softmax. In particular, we focus our investigation onspherical bounds of the log-softmax loss and on two spherical log-likelihoodlosses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) andthe log-Taylor Softmax that we introduce. Although these alternatives do notyield as good results as the log-softmax loss on two language modeling tasks,they surprisingly outperform it in our experiments on MNIST and CIFAR-10,suggesting that they might be relevant in a broad range of applications.
arxiv-15900-277 | On the entropy numbers of the mixed smoothness function classes | http://arxiv.org/pdf/1602.08712v1.pdf | author:V. Temlyakov category:math.NA math.FA stat.ML published:2016-02-28 summary:Behavior of the entropy numbers of classes of multivariate functions withmixed smoothness is studied here. This problem has a long history and somefundamental problems in the area are still open. The main goal of this paper isto develop a new method of proving the upper bounds for the entropy numbers.This method is based on recent developments of nonlinear approximation, inparticular, on greedy approximation. This method consists of the following twosteps strategy. At the first step we obtain bounds of the best m-termapproximations with respect to a dictionary. At the second step we use generalinequalities relating the entropy numbers to the best m-term approximations.For the lower bounds we use the volume estimates method, which is a well knownpowerful method for proving the lower bounds for the entropy numbers. It wasused in a number of previous papers.
arxiv-15900-278 | Bilingual Distributed Word Representations from Document-Aligned Comparable Data | http://arxiv.org/pdf/1509.07308v2.pdf | author:Ivan Vulić, Marie-Francine Moens category:cs.CL published:2015-09-24 summary:We propose a new model for learning bilingual word representations fromnon-parallel document-aligned data. Following the recent advances in wordrepresentation learning, our model learns dense real-valued word vectors, thatis, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs whichheavily relied on parallel sentence-aligned corpora and/or readily availabletranslation resources such as dictionaries, the article reveals that BWEs maybe learned solely on the basis of document-aligned comparable data without anyadditional lexical resources nor syntactic information. We present a comparisonof our approach with previous state-of-the-art models for learning bilingualword representations from comparable data that rely on the framework ofmultilingual probabilistic topic modeling (MuPTM), as well as withdistributional local context-counting models. We demonstrate the utility of theinduced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2)suggesting word translations in context for polysemous words. Our simple yeteffective BWE-based models significantly outperform the MuPTM-based andcontext-counting representation models from comparable data as well as priorBWE-based models, and acquire the best reported results on both tasks for allthree tested language pairs.
arxiv-15900-279 | Lie Access Neural Turing Machine | http://arxiv.org/pdf/1602.08671v1.pdf | author:Greg Yang category:cs.NE cs.AI cs.LG published:2016-02-28 summary:Recently, Neural Turing Machine and Memory Networks have shown that adding anexternal memory can greatly ameliorate a traditional recurrent neural network'stendency to forget after a long period of time. Here we present a new design ofan external memory, wherein memories are stored in an Euclidean key space$\mathbb R^n$. An LSTM controller performs read and write via specializedstructures called read and write heads, following the design of Neural TuringMachine. It can move a head by either providing a new address in the key space(aka random access) or moving from its previous position via a Lie group action(aka Lie access). In this way, the "L" and "R" instructions of a traditionalTuring Machine is generalized to arbitrary elements of a fixed Lie groupaction. For this reason, we name this new model the Lie Access Neural TuringMachine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline inseveral basic experiments. As LANTM is differentiable end-to-end, training wasdone with RMSProp. We found the right configuration of LANTM to be capable oflearning different permutation and arithmetic tasks and extrapolating to atleast twice the input size, all with the number of parameters 2 orders ofmagnitude below that for the LSTM baseline. In particular, we trained LANTM onaddition of $k$-digit numbers for $2 \le k \le 16$, but it was able togeneralize almost perfectly to $17 \le k \le 32$.
arxiv-15900-280 | QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source | http://arxiv.org/pdf/1602.08657v1.pdf | author:Luc Herren category:cs.CL published:2016-02-28 summary:The software programs generally used with the TLG (Thesaurus Linguae Graecae)and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not wellsuited for finding quotations and allusions. QuotationFinder uses moresophisticated criteria as it ranks search results based on how closely theymatch the source text, listing search results with literal quotations first andloose verbal parallels last.
arxiv-15900-281 | Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping | http://arxiv.org/pdf/1510.00098v2.pdf | author:Michael Xie, Neal Jean, Marshall Burke, David Lobell, Stefano Ermon category:cs.CV cs.CY published:2015-10-01 summary:The lack of reliable data in developing countries is a major obstacle tosustainable development, food security, and disaster relief. Poverty data, forexample, is typically scarce, sparse in coverage, and labor-intensive toobtain. Remote sensing data such as high-resolution satellite imagery, on theother hand, is becoming increasingly available and inexpensive. Unfortunately,such data is highly unstructured and currently no techniques exist toautomatically extract useful insights to inform policy decisions and helpdirect humanitarian efforts. We propose a novel machine learning approach toextract large-scale socioeconomic indicators from high-resolution satelliteimagery. The main challenge is that training data is very scarce, making itdifficult to apply modern techniques such as Convolutional Neural Networks(CNN). We therefore propose a transfer learning approach where nighttime lightintensities are used as a data-rich proxy. We train a fully convolutional CNNmodel to predict nighttime lights from daytime imagery, simultaneously learningfeatures that are useful for poverty prediction. The model learns filtersidentifying different terrains and man-made structures, including roads,buildings, and farmlands, without any supervision beyond nighttime lights. Wedemonstrate that these learned features are highly informative for povertymapping, even approaching the predictive performance of survey data collectedin the field.
arxiv-15900-282 | A Bayesian baseline for belief in uncommon events | http://arxiv.org/pdf/1602.07836v2.pdf | author:V. Palonen category:stat.AP stat.ML stat.OT published:2016-02-25 summary:The plausibility of uncommon events and miracles based on testimony of suchan event has been much discussed. When analyzing the probabilities involved, ithas mostly been assumed that the common events can be taken as data in thecalculations. However, we usually have only testimonies for the common events.While this difference does not have a significant effect on the inductive partof the inference, it has a large influence on how one should view thereliability of testimonies. In this work, a full Bayesian solution is given forthe more realistic case, where one has a large number of testimonies for acommon event and one testimony for an uncommon event. It is seen that, in orderfor there to be a large amount of testimonies for a common event, thetestimonies will probably be quite reliable. For this reason, because thetestimonies are quite reliable based on the testimonies for the common events,the probability for the uncommon event, given a testimony for it, is alsohigher. Hence, one should be more open-minded when considering the plausibilityof uncommon events.
arxiv-15900-283 | Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization | http://arxiv.org/pdf/1512.07962v2.pdf | author:Changyou Chen, David Carlson, Zhe Gan, Chunyuan Li, Lawrence Carin category:stat.ML cs.LG published:2015-12-25 summary:Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesiananalogs to popular stochastic optimization methods; however, this connection isnot well studied. We explore this relationship by applying simulated annealingto an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with twokey components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)adaptive element-wise momentum weights. The zero-temperature limit gives anovel stochastic optimization method with adaptive element-wise momentumweights, while conventional optimization methods only have a shared, staticmomentum weight. Under certain assumptions, our theoretical analysis suggeststhe proposed simulated annealing approach converges close to the global optima.Experiments on several deep neural network models show state-of-the-art resultscompared to related stochastic optimization algorithms.
arxiv-15900-284 | A Primal Dual Active Set Algorithm for a Class of Nonconvex Sparsity Optimization | http://arxiv.org/pdf/1310.1147v3.pdf | author:Yuling Jiao, Bangti Jin, Xiliang Lu, Weina Ren category:math.OC stat.ML published:2013-10-04 summary:In this paper, we consider the problem of recovering a sparse vector fromnoisy measurement data. Traditionally, it is formulated as a penalizedleast-squares problem with an $\ell^1$ penalty. Recent studies show thatnonconvex penalties, e.g., $\ell^0$ and bridge, allow more effective sparserecovery. We develop an algorithm of primal dual active set type for a class ofnonconvex sparsity-promoting penalties, which cover $\ell^0$, bridge, smoothlyclipped absolute deviation, capped $\ell^1$ and minimax concavity penalty.First we establish the existence of a global minimizer for the class ofoptimization problems. Then we derive a novel necessary optimality conditionfor the global minimizer using the associated thresholding operator. Thesolutions to the optimality system are coordinate-wise minimizers, and underminor conditions, they are also local minimizers. Upon introducing the dualvariable, the active set can be determined from the primal and dual variables.This relation lends itself to an iterative algorithm of active set type whichat each step involves updating the primal variable only on the active set andthen updating the dual variable explicitly. When combined with a continuationstrategy on the regularization parameter, it has a global convergence propertyunder the restricted isometry property. Extensive numerical experimentsdemonstrate its efficiency and accuracy.
arxiv-15900-285 | A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya Parameters and Topic Models | http://arxiv.org/pdf/1510.06646v2.pdf | author:Osama Khalifa, David Wolfe Corne, Mike Chantler category:cs.LG cs.CL stat.ML published:2015-10-22 summary:Hyper-parameters play a major role in the learning and inference process oflatent Dirichlet allocation (LDA). In order to begin the LDA latent variableslearning process, these hyper-parameters values need to be pre-determined. Wepropose an extension for LDA that we call 'Latent Dirichlet allocation GibbsNewton' (LDA-GN), which places non-informative priors over thesehyper-parameters and uses Gibbs sampling to learn appropriate values for them.At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a newtechnique for learning the parameters of multivariate Polya distributions. Wereport Gibbs-Newton performance results compared with two prominent existingapproaches to the latter task: Minka's fixed-point iteration method and theMoments method. We then evaluate LDA-GN in two ways: (i) by comparing it withstandard LDA in terms of the ability of the resulting topic models togeneralize to unseen documents; (ii) by comparing it with standard LDA in itsperformance on a binary classification task.
arxiv-15900-286 | Content-based Video Indexing and Retrieval Using Corr-LDA | http://arxiv.org/pdf/1602.08581v1.pdf | author:Rahul Radhakrishnan Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush Ramsurat, Bhiksha Raj, Rita Singh category:cs.IR cs.CV published:2016-02-27 summary:Existing video indexing and retrieval methods on popular web-based multimediasharing websites are based on user-provided sparse tagging. This paper proposesa very specific way of searching for video clips, based on the content of thevideo. We present our work on Content-based Video Indexing and Retrieval usingthe Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilisticframework. This is a model that provides for auto-annotation of videos in adatabase with textual descriptors, and brings the added benefit of utilizingthe semantic relations between the content of the video and text. We use theconcept-level matching provided by corr-LDA to build correspondences betweentext and multimedia, with the objective of retrieving content with increasedaccuracy. In our experiments, we employ only the audio components of theindividual recordings and compare our results with an SVM-based approach.
arxiv-15900-287 | Single-Image Superresolution Through Directional Representations | http://arxiv.org/pdf/1602.08575v1.pdf | author:Wojciech Czaja, James M. Murphy, Daniel Weinberg category:cs.CV published:2016-02-27 summary:We develop a mathematically-motivated algorithm for image superresolution,based on the discrete shearlet transform. The shearlet transform is stronglydirectional, and is known to provide near-optimally sparse representations fora broad class of images. This often leads to superior performance in edgedetection and image representation, when compared to other isotropic frames. Wejustify the use of shearlet frames for superresolution mathematically beforepresenting a superresolution algorithm that combines the shearlet transformwith the sparse mixing estimators (SME) approach pioneered by Mallat and Yu.Our algorithm is compared with an isotropic superresolution method, a previousprototype of a shearlet superresolution algorithm, and SME superresolution witha discrete wavelet frame. Our numerical results on a variety of image typesshow strong performance in terms of PSNR.
arxiv-15900-288 | Graph clustering, variational image segmentation methods and Hough transform scale detection for object measurement in images | http://arxiv.org/pdf/1602.08574v1.pdf | author:Luca Calatroni, Yves van Gennip, Carola-Bibiane Schönlieb, Hannah Rowland, Arjuna Flenner category:math.AP cs.CV published:2016-02-27 summary:We consider the problem of scale detection in images where a region ofinterest is present together with a measurement tool (e.g. a ruler). For thesegmentation part, we focus on the graph based method by Flenner and Bertozziwhich reinterprets classical continuous Ginzburg-Landau minimisation models ina totally discrete framework. To overcome the numerical difficulties due to thelarge size of the images considered we use matrix completion and splittingtechniques. The scale on the measurement tool is detected via a Hough transformbased algorithm. The method is then applied to some measurement tasks arisingin real-world applications such as zoology, medicine and archaeology.
arxiv-15900-289 | Towards Neural Knowledge DNA | http://arxiv.org/pdf/1602.08571v1.pdf | author:Haoxi Zhang, Cesar Sanin, Edward Szczerbicki category:cs.AI cs.NE published:2016-02-27 summary:In this paper, we propose the Neural Knowledge DNA, a framework that tailorsthe ideas underlying the success of neural networks to the scope of knowledgerepresentation. Knowledge representation is a fundamental field that dedicateto representing information about the world in a form that computer systems canutilize to solve complex tasks. The proposed Neural Knowledge DNA is designedto support discovering, storing, reusing, improving, and sharing knowledgeamong machines and organisation. It is constructed in a similar fashion of howDNA formed: built up by four essential elements. As the DNA producesphenotypes, the Neural Knowledge DNA carries information and knowledge via itsfour essential elements, namely, Networks, Experiences, States, and Actions.
arxiv-15900-290 | Liberating language research from dogmas of the 20th century | http://arxiv.org/pdf/1509.03295v3.pdf | author:Ramon Ferrer-i-Cancho, Carlos Gómez-Rodríguez category:cs.CL cs.SI physics.soc-ph published:2015-09-09 summary:A commentary on the article "Large-scale evidence of dependency lengthminimization in 37 languages" by Futrell, Mahowald & Gibson (PNAS 2015 112 (33)10336-10341).
arxiv-15900-291 | Multiplier-less Artificial Neurons Exploiting Error Resiliency for Energy-Efficient Neural Computing | http://arxiv.org/pdf/1602.08557v1.pdf | author:Syed Shakib Sarwar, Swagath Venkataramani, Anand Raghunathan, Kaushik Roy category:cs.NE published:2016-02-27 summary:Large-scale artificial neural networks have shown significant promise inaddressing a wide range of classification and recognition applications.However, their large computational requirements stretch the capabilities ofcomputing platforms. The fundamental components of these neural networks arethe neurons and its synapses. The core of a digital hardware neuron consists ofmultiplier, accumulator and activation function. Multipliers consume most ofthe processing energy in the digital neurons, and thereby in the hardwareimplementations of artificial neural networks. We propose an approximatemultiplier that utilizes the notion of computation sharing and exploits errorresilience of neural network applications to achieve improved energyconsumption. We also propose Multiplier-less Artificial Neuron (MAN) for evenlarger improvement in energy consumption and adapt the training process toensure minimal degradation in accuracy. We evaluated the proposed design on 5recognition applications. The results show, 35% and 60% reduction in energyconsumption, for neuron sizes of 8 bits and 12 bits, respectively, with amaximum of ~2.83% loss in network accuracy, compared to a conventional neuronimplementation. We also achieve 37% and 62% reduction in area for a neuron sizeof 8 bits and 12 bits, respectively, under iso-speed conditions.
arxiv-15900-292 | Significance Driven Hybrid 8T-6T SRAM for Energy-Efficient Synaptic Storage in Artificial Neural Networks | http://arxiv.org/pdf/1602.08556v1.pdf | author:Gopalakrishnan Srinivasan, Parami Wijesinghe, Syed Shakib Sarwar, Akhilesh Jaiswal, Kaushik Roy category:cs.NE published:2016-02-27 summary:Multilayered artificial neural networks (ANN) have found widespread utilityin classification and recognition applications. The scale and complexity ofsuch networks together with the inadequacies of general purpose computingplatforms have led to a significant interest in the development of efficienthardware implementations. In this work, we focus on designing energy efficienton-chip storage for the synaptic weights. In order to minimize the powerconsumption of typical digital CMOS implementations of such large-scalenetworks, the digital neurons could be operated reliably at scaled voltages byreducing the clock frequency. On the contrary, the on-chip synaptic storagedesigned using a conventional 6T SRAM is susceptible to bitcell failures atreduced voltages. However, the intrinsic error resiliency of NNs to smallsynaptic weight perturbations enables us to scale the operating voltage of the6TSRAM. Our analysis on a widely used digit recognition dataset indicates thatthe voltage can be scaled by 200mV from the nominal operating voltage (950mV)for practically no loss (less than 0.5%) in accuracy (22nm predictivetechnology). Scaling beyond that causes substantial performance degradationowing to increased probability of failures in the MSBs of the synaptic weights.We, therefore propose a significance driven hybrid 8T-6T SRAM, wherein thesensitive MSBs are stored in 8T bitcells that are robust at scaled voltages dueto decoupled read and write paths. In an effort to further minimize the areapenalty, we present a synaptic-sensitivity driven hybrid memory architectureconsisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulationframework shows that the proposed synaptic-sensitivity driven architectureprovides a 30.91% reduction in the memory access power with a 10.41% areaoverhead, for less than 1% loss in the classification accuracy.
arxiv-15900-293 | Deep multi-scale video prediction beyond mean square error | http://arxiv.org/pdf/1511.05440v6.pdf | author:Michael Mathieu, Camille Couprie, Yann LeCun category:cs.LG cs.CV stat.ML published:2015-11-17 summary:Learning to predict future images from a video sequence involves theconstruction of an internal representation that models the image evolutionaccurately, and therefore, to some degree, its content and dynamics. This iswhy pixel-space video prediction may be viewed as a promising avenue forunsupervised feature learning. In addition, while optical flow has been a verystudied problem in computer vision for a long time, future frame prediction israrely approached. Still, many vision applications could benefit from theknowledge of the next frames of videos, that does not require the complexity oftracking every pixel trajectories. In this work, we train a convolutionalnetwork to generate future frames given an input sequence. To deal with theinherently blurry predictions obtained from the standard Mean Squared Error(MSE) loss function, we propose three different and complementary featurelearning strategies: a multi-scale architecture, an adversarial trainingmethod, and an image gradient difference loss function. We compare ourpredictions to different published results based on recurrent neural networkson the UCF101 dataset
arxiv-15900-294 | Patch-Ordering as a Regularization for Inverse Problems in Image Processing | http://arxiv.org/pdf/1602.08510v1.pdf | author:Gregory Vaksman, Michael Zibulevsky, Michael Elad category:cs.CV published:2016-02-26 summary:Recent work in image processing suggests that operating on (overlapping)patches in an image may lead to state-of-the-art results. This has beendemonstrated for a variety of problems including denoising, inpainting,deblurring, and super-resolution. The work reported in [1,2] takes an extrastep forward by showing that ordering these patches to form an approximateshortest path can be leveraged for better processing. The core idea is to applya simple filter on the resulting 1D smoothed signal obtained after thepatch-permutation. This idea has been also explored in combination with awavelet pyramid, leading eventually to a sophisticated and highly effectiveregularizer for inverse problems in imaging. In this work we further study thepatch-permutation concept, and harness it to propose a new simple yet effectiveregularization for image restoration problems. Our approach builds on theclassic Maximum A'posteriori probability (MAP), with a penalty functionconsisting of a regular log-likelihood term and a novel permutation-basedregularization term. Using a plain 1D Laplacian, the proposed regularizationforces robust smoothness (L1) on the permuted pixels. Since the permutationoriginates from patch-ordering, we propose to accumulate the smoothness termsover all the patches' pixels. Furthermore, we take into account the founddistances between adjacent patches in the ordering, by weighting the Laplacianoutcome. We demonstrate the proposed scheme on a diverse set of problems: (i)severe Poisson image denoising, (ii) Gaussian image denoising, (iii) imagedeblurring, and (iv) single image super-resolution. In all these cases, we userecent methods that handle these problems as initialization to our scheme. Thisis followed by an L-BFGS optimization of the above-described penalty function,leading to state-of-the-art results, and especially so for highly ill-posedcases.
arxiv-15900-295 | Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control | http://arxiv.org/pdf/1506.02632v3.pdf | author:Prashanth L. A., Cheng Jie, Michael Fu, Steve Marcus, Csaba Szepesvári category:cs.LG math.OC published:2015-06-08 summary:Cumulative prospect theory (CPT) is known to model human decisions well, withsubstantial empirical evidence supporting this claim. CPT works by distortingprobabilities and is more general than the classic expected utility andcoherent risk measures. We bring this idea to a risk-sensitive reinforcementlearning (RL) setting and design algorithms for both estimation and control.The RL setting presents two particular challenges when CPT is applied:estimating the CPT objective requires estimations of the entire distribution ofthe value function and finding a randomized optimal policy. The estimationscheme that we propose uses the empirical distribution to estimate theCPT-value of a random variable. We then use this scheme in the inner loop of aCPT-value optimization procedure that is based on the well-known simulationoptimization idea of simultaneous perturbation stochastic approximation (SPSA).We provide theoretical convergence guarantees for all the proposed algorithmsand also illustrate the usefulness of CPT-based criteria in a traffic signalcontrol application.
arxiv-15900-296 | A Roadmap towards Machine Intelligence | http://arxiv.org/pdf/1511.08130v2.pdf | author:Tomas Mikolov, Armand Joulin, Marco Baroni category:cs.AI cs.CL published:2015-11-25 summary:The development of intelligent machines is one of the biggest unsolvedchallenges in computer science. In this paper, we propose some fundamentalproperties these machines should have, focusing in particular on communicationand learning. We discuss a simple environment that could be used toincrementally teach a machine the basics of natural-language-basedcommunication, as a prerequisite to more complex interaction with human users.We also present some conjectures on the sort of algorithms the machine shouldsupport in order to profitably learn from the environment.
arxiv-15900-297 | Variance Reduced Stochastic Gradient Descent with Neighbors | http://arxiv.org/pdf/1506.03662v4.pdf | author:Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, Brian McWilliams category:cs.LG math.OC stat.ML G.1.6; I.2.6 published:2015-06-11 summary:Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet itsslow convergence can be a computational bottleneck. Variance reductiontechniques such as SAG, SVRG and SAGA have been proposed to overcome thisweakness, achieving linear convergence. However, these methods are either basedon computations of full gradients at pivot points, or on keeping per data pointcorrections in memory. Therefore speed-ups relative to SGD may need a minimalnumber of epochs in order to materialize. This paper investigates algorithmsthat can exploit neighborhood structure in the training data to share andre-use information about past stochastic gradients across data points, whichoffers advantages in the transient optimization phase. As a side-product weprovide a unified convergence analysis for a family of variance reductionalgorithms, which we call memorization algorithms. We provide experimentalresults supporting our theory.
arxiv-15900-298 | Reasoning in Vector Space: An Exploratory Study of Question Answering | http://arxiv.org/pdf/1511.06426v4.pdf | author:Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, Li Deng, Paul Smolensky category:cs.CL published:2015-11-19 summary:Question answering tasks have shown remarkable progress with distributedvector representation. In this paper, we investigate the recently proposedFacebook bAbI tasks which consist of twenty different categories of questionsthat require complex reasoning. Because the previous work on bAbI are allend-to-end models, errors could come from either an imperfect understanding ofsemantics or in certain steps of the reasoning. For clearer analysis, wepropose two vector space models inspired by Tensor Product Representation (TPR)to perform knowledge encoding and logical reasoning based on common-senseinference. They together achieve near-perfect accuracy on all categoriesincluding positional reasoning and path finding that have proved difficult formost of the previous approaches. We hypothesize that the difficulties in thesecategories are due to the multi-relations in contrast to uni-relationalcharacteristic of other categories. Our exploration sheds light on designingmore sophisticated dataset and moving one step toward integrating transparentand interpretable formalism of TPR into existing learning paradigms.
arxiv-15900-299 | Shape-aware Surface Reconstruction from Sparse Data | http://arxiv.org/pdf/1602.08425v1.pdf | author:Florian Bernard, Luis Salamanca, Johan Thunberg, Alexander Tack, Dennis Jentsch, Hans Lamecker, Stefan Zachow, Frank Hertel, Jorge Goncalves, Peter Gemmar category:cs.CV stat.ML published:2016-02-26 summary:The reconstruction of an object's shape or surface from a set of 3D points isa common topic in materials and life sciences, computationally handled incomputer graphics. Such points usually stem from optical or tactile 3Dcoordinate measuring equipment. Surface reconstruction also appears in medicalimage analysis, e.g. in anatomy reconstruction from tomographic measurements orthe alignment of intra-operative navigation and preoperative planning data. Incontrast to mere 3D point clouds, medical imaging yields contextual informationon the 3D point data that can be used to adopt prior information on the shapethat is to be reconstructed from the measurements. In this work we propose touse a statistical shape model (SSM) as a prior for surface reconstruction. Theprior knowledge is represented by a point distribution model (PDM) that isassociated with a surface mesh. Using the shape distribution that is modelledby the PDM, we reformulate the problem of surface reconstruction from aprobabilistic perspective based on a Gaussian Mixture Model (GMM). In order todo so, the given measurements are interpreted as samples of the GMM. By usingmixture components with anisotropic covariances that are oriented according tothe surface normals at the PDM points, a surface-based fitting is accomplished.By estimating the parameters of the GMM in a maximum a posteriori manner, thereconstruction of the surface from the given measurements is achieved.Extensive experiments suggest that our proposed approach leads to superiorsurface reconstructions compared to Iterative Closest Point (ICP) methods.
arxiv-15900-300 | Multivariate Hawkes Processes for Large-scale Inference | http://arxiv.org/pdf/1602.08418v1.pdf | author:Rémi Lemonnier, Kevin Scaman, Argyris Kalogeratos category:stat.ML published:2016-02-26 summary:In this paper, we present a framework for fitting multivariate Hawkesprocesses for large-scale problems both in the number of events in the observedhistory $n$ and the number of event types $d$ (i.e. dimensions). The proposedLow-Rank Hawkes Process (LRHP) framework introduces a low-rank approximation ofthe kernel matrix that allows to perform the nonparametric learning of the$d^2$ triggering kernels using at most $O(ndr^2)$ operations, where $r$ is therank of the approximation ($r \ll d,n$). This comes as a major improvement tothe existing state-of-the-art inference algorithms that are in $O(nd^2)$.Furthermore, the low-rank approximation allows LRHP to learn representativepatterns of interaction between event types, which may be valuable for theanalysis of such complex processes in real world datasets. The efficiency andscalability of our approach is illustrated with numerical experiments onsimulated as well as real datasets.
