arxiv-15300-1 | Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC-Guided Policy Search | http://arxiv.org/pdf/1509.06791v2.pdf | author:Tianhao Zhang, Gregory Kahn, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO published:2015-09-22 summary:Model predictive control (MPC) is an effective method for controlling roboticsystems, particularly autonomous aerial vehicles such as quadcopters. However,application of MPC can be computationally demanding, and typically requiresestimating the state of the system, which can be challenging in complex,unstructured environments. Reinforcement learning can in principle forego theneed for explicit state estimation and acquire a policy that directly mapssensor readings to actions, but is difficult to apply to unstable systems thatare liable to fail catastrophically during training before an effective policyhas been found. We propose to combine MPC with reinforcement learning in theframework of guided policy search, where MPC is used to generate data attraining time, under full state observations provided by an instrumentedtraining environment. This data is used to train a deep neural network policy,which is allowed to access only the raw observations from the vehicle's onboardsensors. After training, the neural network policy can successfully control therobot without knowledge of the full state, and at a fraction of thecomputational cost of MPC. We evaluate our method by learning obstacleavoidance policies for a simulated quadrotor, using simulated onboard sensorsand no explicit state estimation at test time.
arxiv-15300-2 | A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes | http://arxiv.org/pdf/1602.04921v1.pdf | author:Weiyao Lin, Yang Mi, Weiyue Wang, Jianxin Wu, Jingdong Wang, Tao Mei category:cs.CV cs.AI cs.MM published:2016-02-16 summary:This paper addresses the problem of detecting coherent motions in crowdscenes and presents its two applications in crowd scene understanding: semanticregion detection and recurrent activity mining. It processes input motionfields (e.g., optical flow fields) and produces a coherent motion filed, namedas thermal energy field. The thermal energy field is able to capture bothmotion correlation among particles and the motion trends of individualparticles which are helpful to discover coherency among them. We furtherintroduce a two-step clustering process to construct stable semantic regionsfrom the extracted time-varying coherent motions. These semantic regions can beused to recognize pre-defined activities in crowd scenes. Finally, we introducea cluster-and-merge process which automatically discovers recurrent activitiesin crowd scenes by clustering and merging the extracted coherent motions.Experiments on various videos demonstrate the effectiveness of our approach.
arxiv-15300-3 | Dimensionality Reduction for Nonlinear Regression with Two Predictor Vectors | http://arxiv.org/pdf/1602.04398v2.pdf | author:Yanjun Li, Yoram Bresler category:stat.ML cs.IT cs.LG math.IT published:2016-02-13 summary:Many variables that we would like to predict depend nonlinearly on two typesof attributes. For example, prices are influenced by supply and demand. Movieratings are determined by demographic attributes and genre attributes. Thispaper addresses the dimensionality reduction problem in such regressionproblems with two predictor vectors. In particular, we assume a discriminativemodel where low-dimensional linear embeddings of the two predictor vectors aresufficient statistics for predicting a dependent variable. We show that asimple algorithm involving singular value decomposition can accurately estimatethe embeddings provided that certain sample complexities are satisfied,surprisingly, without specifying the nonlinear regression model. Theseembeddings improve the efficiency and robustness of subsequent training, andcan serve as a pre-training algorithm for neural networks. The main resultsestablish sample complexities under multiple settings. Sample complexities fordifferent regression models only differ by constant factors.
arxiv-15300-4 | Bayesian generalized fused lasso modeling via NEG distribution | http://arxiv.org/pdf/1602.04910v1.pdf | author:Kaito Shimamura, Masao Ueki, Shuichi Kawano, Sadanori Konishi category:stat.ME stat.ML published:2016-02-16 summary:The fused lasso penalizes a loss function by the $L_1$ norm for both theregression coefficients and their successive differences to encourage sparsityof both. In this paper, we propose a Bayesian generalized fused lasso modelingbased on a normal-exponential-gamma (NEG) prior distribution. The NEG prior isassumed into the difference of successive regression coefficients. The proposedmethod enables us to construct a more versatile sparse model than the ordinaryfused lasso by using a flexible regularization term. We also propose a sparsefused algorithm to produce exact sparse solutions. Simulation studies and realdata analyses show that the proposed method has superior performance to theordinary fused lasso.
arxiv-15300-5 | Segmentation Rectification for Video Cutout via One-Class Structured Learning | http://arxiv.org/pdf/1602.04906v1.pdf | author:Junyan Wang, Sai-kit Yeung, Jue Wang, Kun Zhou category:cs.CV cs.GR cs.LG published:2016-02-16 summary:Recent works on interactive video object cutout mainly focus on designingdynamic foreground-background (FB) classifiers for segmentation propagation.However, the research on optimally removing errors from the FB classificationis sparse, and the errors often accumulate rapidly, causing significant errorsin the propagated frames. In this work, we take the initial steps to addressingthis problem, and we call this new task \emph{segmentation rectification}. Ourkey observation is that the possibly asymmetrically distributed false positiveand false negative errors were handled equally in the conventional methods. We,alternatively, propose to optimally remove these two types of errors. To thiseffect, we propose a novel bilayer Markov Random Field (MRF) model for this newtask. We also adopt the well-established structured learning framework to learnthe optimal model from data. Additionally, we propose a novel one-classstructured SVM (OSSVM) which greatly speeds up the structured learning process.Our method naturally extends to RGB-D videos as well. Comprehensive experimentson both RGB and RGB-D data demonstrate that our simple and effective methodsignificantly outperforms the segmentation propagation methods adopted in thestate-of-the-art video cutout systems, and the results also suggest thepotential usefulness of our method in image cutout system.
arxiv-15300-6 | NED: An Inter-Graph Node Metric Based On Edit Distance | http://arxiv.org/pdf/1602.02358v3.pdf | author:Haohan Zhu, Xianrui Meng, George Kollios category:cs.DB cs.LG cs.SI published:2016-02-07 summary:Node similarity is a fundamental problem in graph analytics. However, nodesimilarity between nodes in different graphs (inter-graph nodes) has notreceived a lot of attention yet. The inter-graph node similarity is importantin learning a new graph based on the knowledge of an existing graph (transferlearning on graphs) and has applications in biological, communication, andsocial networks. In this paper, we propose a novel distance function formeasuring inter-graph node similarity with edit distance, called NED. In NED,two nodes are compared according to their local neighborhood structures whichare represented as unordered k-adjacent trees, without relying on labels orother assumptions. Since the computation problem of tree edit distance onunordered trees is NP-Complete, we propose a modified tree edit distance,called TED*, for comparing neighborhood trees. TED* is a metric distance, asthe original tree edit distance, but more importantly, TED* is polynomiallycomputable. As a metric distance, NED admits efficient indexing, providesinterpretable results, and shows to perform better than existing approaches ona number of data analysis tasks, including graph de-anonymization. Finally, theefficiency and effectiveness of NED are empirically demonstrated usingreal-world graphs.
arxiv-15300-7 | Spatial Semantic Scan: Detecting Subtle, Spatially Localized Events in Text Streams | http://arxiv.org/pdf/1511.00352v2.pdf | author:Abhinav Maurya category:cs.LG cs.CL stat.ML published:2015-11-02 summary:Many methods have been proposed for detecting emerging events in text streamsusing topic modeling. However, these methods have shortcomings that make themunsuitable for rapid detection of locally emerging events on massive textstreams. We describe Spatially Compact Semantic Scan (SCSS) that has beendeveloped specifically to overcome the shortcomings of current methods indetecting new spatially compact events in text streams. SCSS employsalternating optimization between using semantic scan to estimate contrastiveforeground topics in documents, and discovering spatial neighborhoods with highoccurrence of documents containing the foreground topics. We evaluate ourmethod on Emergency Department chief complaints dataset (ED dataset) to verifythe effectiveness of our method in detecting real-world disease outbreaks fromfree-text ED chief complaint data.
arxiv-15300-8 | Anomaly Detection in Unstructured Environments using Bayesian Nonparametric Scene Modeling | http://arxiv.org/pdf/1509.07979v2.pdf | author:Yogesh Girdhar, Walter Cho, Matthew Campbell, Jesus Pineda, Elizabeth Clarke, Hanumant Singh category:cs.CV cs.RO published:2015-09-26 summary:This paper explores the use of a Bayesian non-parametric topic modelingtechnique for the purpose of anomaly detection in video data. We presentresults from two experiments. The first experiment shows that the proposedtechnique is automatically able characterize the underlying terrain, and detectanomalous flora in image data collected by an underwater robot. The secondexperiment shows that the same technique can be used on images from a staticcamera in a dynamic unstructured environment. In the second dataset, consistingof video data from a static seafloor camera capturing images of a busy coralreef, the proposed technique was able to detect all three instances of anunderwater vehicle passing in front of the camera, amongst many otherobservations of fishes, debris, lighting changes due to surface waves, andbenthic flora.
arxiv-15300-9 | Fast, Robust, Continuous Monocular Egomotion Computation | http://arxiv.org/pdf/1602.04886v1.pdf | author:Andrew Jaegle, Stephen Phillips, Kostas Daniilidis category:cs.CV cs.RO published:2016-02-16 summary:We propose robust methods for estimating camera egomotion in noisy,real-world monocular image sequences in the general case of unknown observerrotation and translation with two views and a small baseline. This is adifficult problem because of the nonconvex cost function of the perspectivecamera motion equation and because of non-Gaussian noise arising from noisyoptical flow estimates and scene non-rigidity. To address this problem, weintroduce the expected residual likelihood method (ERL), which estimatesconfidence weights for noisy optical flow data using likelihood distributionsof the residuals of the flow field under a range of counterfactual modelparameters. We show that ERL is effective at identifying outliers andrecovering appropriate confidence weights in many settings. We compare ERL to anovel formulation of the perspective camera motion equation using a liftedkernel, a recently proposed optimization framework for joint parameter andconfidence weight estimation with good empirical properties. We incorporatethese strategies into a motion estimation pipeline that avoids falling intolocal minima. We find that ERL outperforms the lifted kernel method andbaseline monocular egomotion estimation strategies on the challenging KITTIdataset, while adding almost no runtime cost over baseline egomotion methods.
arxiv-15300-10 | Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation | http://arxiv.org/pdf/1602.04874v1.pdf | author:Yushi Yao, Zheng Huang category:cs.LG cs.CL published:2016-02-16 summary:Recurrent neural network(RNN) has been broadly applied to natural languageprocessing(NLP) problems. This kind of neural network is designed for modelingsequential data and has been testified to be quite efficient in sequentialtagging tasks. In this paper, we propose to use bi-directional RNN with longshort-term memory(LSTM) units for Chinese word segmentation, which is a crucialpreprocess task for modeling Chinese sentences and articles. Classical methodsfocus on designing and combining hand-craft features from context, whereasbi-directional LSTM network(BLSTM) does not need any prior knowledge orpre-designing, and it is expert in keeping the contextual information in bothdirections. Experiment result shows that our approach gets state-of-the-artperformance in word segmentation on both traditional Chinese datasets andsimplified Chinese datasets.
arxiv-15300-11 | Deep Feature-based Face Detection on Mobile Devices | http://arxiv.org/pdf/1602.04868v1.pdf | author:Sayantan Sarkar, Vishal M. Patel, Rama Chellappa category:cs.CV published:2016-02-16 summary:We propose a deep feature-based face detector for mobile devices to detectuser's face acquired by the front facing camera. The proposed method is able todetect faces in images containing extreme pose and illumination variations aswell as partial faces. The main challenge in developing deep feature-basedalgorithms for mobile devices is the constrained nature of the mobile platformand the non-availability of CUDA enabled GPUs on such devices. Ourimplementation takes into account the special nature of the images captured bythe front-facing camera of mobile devices and exploits the GPUs present inmobile devices without CUDA-based frameorks, to meet these challenges.
arxiv-15300-12 | Graphlet Decomposition: Framework, Algorithms, and Applications | http://arxiv.org/pdf/1506.04322v2.pdf | author:Nesreen K. Ahmed, Jennifer Neville, Ryan A. Rossi, Nick Duffield, Theodore L. Willke category:cs.SI cs.DC cs.IR stat.ML published:2015-06-13 summary:From social science to biology, numerous applications often rely on graphletsfor intuitive and meaningful characterization of networks at both the globalmacro-level as well as the local micro-level. While graphlets have witnessed atremendous success and impact in a variety of domains, there has yet to be afast and efficient approach for computing the frequencies of these subgraphpatterns. However, existing methods are not scalable to large networks withmillions of nodes and edges, which impedes the application of graphlets to newproblems that require large-scale network analysis. To address these problems,we propose a fast, efficient, and parallel algorithm for counting graphlets ofsize k={3,4}-nodes that take only a fraction of the time to compute whencompared with the current methods used. The proposed graphlet countingalgorithms leverages a number of proven combinatorial arguments for differentgraphlets. For each edge, we count a few graphlets, and with these counts alongwith the combinatorial arguments, we obtain the exact counts of others inconstant time. On a large collection of 300+ networks from a variety ofdomains, our graphlet counting strategies are on average 460x faster thancurrent methods. This brings new opportunities to investigate the use ofgraphlets on much larger networks and newer applications as we show in theexperiments. To the best of our knowledge, this paper provides the largestgraphlet computations to date as well as the largest systematic investigationon over 300+ networks from a variety of domains.
arxiv-15300-13 | The Quadrifocal Variety | http://arxiv.org/pdf/1501.01266v2.pdf | author:Luke Oeding category:math.AG cs.CV published:2015-01-06 summary:Multi-view Geometry is reviewed from an Algebraic Geometry perspective andmulti-focal tensors are constructed as equivariant projections of theGrassmannian. A connection to the principal minor assignment problem is made byconsidering several flatlander cameras. The ideal of the quadrifocal variety iscomputed up to degree 8 (and partially in degree 9) using the representationsof $\operatorname{GL}(3)^{\times 4}$ in the polynomial ring on the space of $3\times 3 \times 3 \times 3$ tensors. Further representation-theoretic analysisgives a lower bound for the number of minimal generators. We conjecture thatthe ideal of the quadrifocal variety is minimally generated in degree at most9.
arxiv-15300-14 | Black-box optimization with a politician | http://arxiv.org/pdf/1602.04847v1.pdf | author:Sébastien Bubeck, Yin-Tat Lee category:math.OC cs.DS cs.LG cs.NA published:2016-02-15 summary:We propose a new framework for black-box convex optimization which iswell-suited for situations where gradient computations are expensive. We derivea new method for this framework which leverages several concepts from convexoptimization, from standard first-order methods (e.g. gradient descent orquasi-Newton methods) to analytical centers (i.e. minimizers of self-concordantbarriers). We demonstrate empirically that our new technique compares favorablywith state of the art algorithms (such as BFGS).
arxiv-15300-15 | DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression | http://arxiv.org/pdf/1602.04805v1.pdf | author:Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh category:stat.ML cs.LG stat.CO stat.ME published:2016-02-15 summary:Performing exact posterior inference in complex generative models is oftendifficult or impossible due to an expensive to evaluate or intractablelikelihood function. Approximate Bayesian computation (ABC) is an inferenceframework that constructs an approximation to the true likelihood based on thesimilarity between the observed and simulated data as measured by a predefinedset of summary statistics. Although the choice of appropriate problem-specificsummary statistics crucially influences the quality of the likelihoodapproximation and hence also the quality of the posterior sample in ABC, thereare only few principled general-purpose approaches to the selection orconstruction of such summary statistics. In this paper, we develop a novelframework for this task using kernel-based distribution regression. We modelthe functional relationship between data distributions and the optimal choice(with respect to a loss function) of summary statistics using kernel-baseddistribution regression. We show that our approach can be implemented in acomputationally and statistically efficient way using the random Fourierfeatures framework for large-scale kernel learning. In addition to that, ourframework shows superior performance when compared to related methods on toyand real-world problems.
arxiv-15300-16 | Quantum Perceptron Models | http://arxiv.org/pdf/1602.04799v1.pdf | author:Nathan Wiebe, Ashish Kapoor, Krysta M Svore category:quant-ph cs.LG stat.ML published:2016-02-15 summary:We demonstrate how quantum computation can provide non-trivial improvementsin the computational and statistical complexity of the perceptron model. Wedevelop two quantum algorithms for perceptron learning. The first algorithmexploits quantum information processing to determine a separating hyperplaneusing a number of steps sublinear in the number of data points $N$, namely$O(\sqrt{N})$. The second algorithm illustrates how the classical mistake boundof $O(\frac{1}{\gamma^2})$ can be further improved to$O(\frac{1}{\sqrt{\gamma}})$ through quantum means, where $\gamma$ denotes themargin. Such improvements are achieved through the application of quantumamplitude amplification to the version space interpretation of the perceptronmodel.
arxiv-15300-17 | Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap | http://arxiv.org/pdf/1512.09080v2.pdf | author:Emmanuel Abbe, Colin Sandon category:math.PR cs.CC cs.IT cs.LG cs.SI math.IT published:2015-12-30 summary:In a paper that initiated the modern study of the stochastic block model,Decelle et al., backed up Mossel et al., made a fascinating conjecture: Denoteby $k$ the number of balanced communities, $a/n$ the probability of connectinginside communities and $b/n$ across, and set$\mathrm{SNR}=a-b/\sqrt{k(a+(k-1)b)}$; for any $k \geq 2$, it is possible todetect communities efficiently whenever $\mathrm{SNR}>1$ (the KS threshold),whereas for $k\geq 5$, it is possible to detect communitiesinformation-theoretically for some $\mathrm{SNR}<1$. Massouli\'e, Mossel etal.\ and Bordenave et al.\ succeeded in proving that the KS threshold isefficiently achievable for $k=2$, while Mossel et al.\ proved that it cannot becrossed information-theoretically for $k=2$. The above conjecture remained openfor $k \geq 3$. This paper proves this conjecture. For the efficient part, an acyclic beliefpropagation (ABP) algorithm is developed and proved to detect communities forany $k$ down the KS threshold in time $O(n \log n)$. Achieving this requiresshowing optimality of BP in the presence of cycles and random initialization, achallenge in the realm of graphical models. The paper also connects ABP to apower iteration method on a $r$-nonbacktracking operator, formalizing themessage passing and spectral interplay. Further, it shows that the model can belearned efficiently down the KS threshold, implying that ABP improves upon thestate-of-the-art both in terms of complexity and universality. For theinformation-theoretic (IT) part, a non-efficient algorithm sampling a typicalclustering is shown to break down the KS threshold at $k=5$. The emerging gapis shown to be large in some cases; if $a=0$, the KS threshold reads $b \gtrsimk^2$ whereas the IT bound reads $b \gtrsim k \ln(k)$. This thus makes the SBM agood study case for information-computation gaps. The results extend to generalSBMs.
arxiv-15300-18 | Community Recovery in Graphs with Locality | http://arxiv.org/pdf/1602.03828v2.pdf | author:Yuxin Chen, Govinda Kamath, Changho Suh, David Tse category:cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH published:2016-02-11 summary:Motivated by applications in domains such as social networks andcomputational biology, we study the problem of community recovery in graphswith locality. In this problem, pairwise noisy measurements of whether twonodes are in the same community or different communities come mainly orexclusively from nearby nodes rather than uniformly sampled between all nodespairs, as in most existing models. We present an algorithm that runs nearlylinearly in the number of measurements and which achieves the informationtheoretic limit for exact recovery.
arxiv-15300-19 | Towards the Evolution of Vertical-Axis Wind Turbines using Supershapes | http://arxiv.org/pdf/1204.4107v4.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.CG published:2012-04-18 summary:We have recently presented an initial study of evolutionary algorithms usedto design vertical-axis wind turbines (VAWTs) wherein candidate prototypes areevaluated under approximated wind tunnel conditions after being physicallyinstantiated by a 3D printer. That is, unlike other approaches such ascomputational fluid dynamics simulations, no mathematical formulations are usedand no model assumptions are made. However, the representation usedsignificantly restricted the range of morphologies explored. In this paper, wepresent initial explorations into the use of a simple generative encoding,known as Gielis superformula, that produces a highly flexible 3D shaperepresentation to design VAWT. First, the target-based evolution of 3Dartefacts is investigated and subsequently initial design experiments areperformed wherein each VAWT candidate is physically instantiated and evaluatedunder approximated wind tunnel conditions. It is shown possible to produce veryclosely matching designs of a number of 3D objects through the evolution ofsupershapes produced by Gielis superformula. Moreover, it is shown possible touse artificial physical evolution to identify novel and increasingly efficientsupershape VAWT designs.
arxiv-15300-20 | Discriminative Regularization for Generative Models | http://arxiv.org/pdf/1602.03220v4.pdf | author:Alex Lamb, Vincent Dumoulin, Aaron Courville category:stat.ML cs.LG published:2016-02-09 summary:We explore the question of whether the representations learned by classifierscan be used to enhance the quality of generative models. Our conjecture is thatlabels correspond to characteristics of natural data which are most salient tohumans: identity in faces, objects in images, and utterances in speech. Wepropose to take advantage of this by using the representations fromdiscriminative classifiers to augment the objective function corresponding to agenerative model. In particular we enhance the objective function of thevariational autoencoder, a popular generative model, with a discriminativeregularization term. We show that enhancing the objective function in this wayleads to samples that are clearer and have higher visual quality than thesamples from the standard variational autoencoders.
arxiv-15300-21 | Training of spiking neural networks based on information theoretic costs | http://arxiv.org/pdf/1602.04742v1.pdf | author:Oleg Y. Sinyavskiy category:cs.NE q-bio.NC published:2016-02-15 summary:Spiking neural network is a type of artificial neural network in whichneurons communicate between each other with spikes. Spikes are identicalBoolean events characterized by the time of their arrival. A spiking neuron hasinternal dynamics and responds to the history of inputs as opposed to thecurrent inputs only. Because of such properties a spiking neural network hasrich intrinsic capabilities to process spatiotemporal data. However, becausethe spikes are discontinuous 'yes or no' events, it is not trivial to applytraditional training procedures such as gradient descend to the spikingneurons. In this thesis we propose to use stochastic spiking neuron models inwhich probability of a spiking output is a continuous function of parameters.We formulate several learning tasks as minimization of certaininformation-theoretic cost functions that use spiking output probabilitydistributions. We develop a generalized description of the stochastic spikingneuron and a new spiking neuron model that allows to flexibly process richspatiotemporal data. We formulate and derive learning rules for the followingtasks: - a supervised learning task of detecting a spatiotemporal pattern as aminimization of the negative log-likelihood (the surprisal) of the neuron'soutput - an unsupervised learning task of increasing the stability of neurons outputas a minimization of the entropy - a reinforcement learning task of controlling an agent as a modulatedoptimization of filtered surprisal of the neuron's output. We test the derived learning rules in several experiments such asspatiotemporal pattern detection, spatiotemporal data storing and recall withautoassociative memory, combination of supervised and unsupervised learning tospeed up the learning process, adaptive control of simple virtual agents inchanging environments.
arxiv-15300-22 | Delay and Cooperation in Nonstochastic Bandits | http://arxiv.org/pdf/1602.04741v1.pdf | author:Nicolo' Cesa-Bianchi, Claudio Gentile, Yishay Mansour, Alberto Minora category:cs.LG published:2016-02-15 summary:We study networks of communicating learning agents that cooperate to solve acommon nonstochastic bandit problem. Agents use an underlying communicationnetwork to get messages about actions selected by other agents, and dropmessages that took more than $d$ hops to arrive, where $d$ is a delayparameter. We introduce \textsc{Exp3-Coop}, a cooperative version of the {\scExp3} algorithm and prove that with $K$ actions and $N$ agents the averageper-agent regret after $T$ rounds is at most of order $\sqrt{\bigl(d+1 +\tfrac{K}{N}\alpha_{\le d}\bigr)(T\ln K)}$, where $\alpha_{\le d}$ is theindependence number of the $d$-th power of the connected communication graph$G$. We then show that for any connected graph, for $d=\sqrt{K}$ the regretbound is $K^{1/4}\sqrt{T}$, strictly better than the minimax regret $\sqrt{KT}$for noncooperating agents. More informed choices of $d$ lead to bounds whichare arbitrarily close to the full information minimax regret $\sqrt{T\ln K}$when $G$ is dense. When $G$ has sparse components, we show that a variant of\textsc{Exp3-Coop}, allowing agents to choose their parameters according totheir centrality in $G$, strictly improves the regret. Finally, as a by-productof our analysis, we provide the first characterization of the minimax regretfor bandit learning with delay.
arxiv-15300-23 | A Model of Selective Advantage for the Efficient Inference of Cancer Clonal Evolution | http://arxiv.org/pdf/1602.07614v1.pdf | author:Daniele Ramazzotti category:cs.LG published:2016-02-15 summary:Recently, there has been a resurgence of interest in rigorous algorithms forthe inference of cancer progression from genomic data. The motivations aremanifold: (i) growing NGS and single cell data from cancer patients, (ii) needfor novel Data Science and Machine Learning algorithms to infer models ofcancer progression, and (iii) a desire to understand the temporal andheterogeneous structure of tumor to tame its progression by efficacioustherapeutic intervention. This thesis presents a multi-disciplinary effort tomodel tumor progression involving successive accumulation of geneticalterations, each resulting populations manifesting themselves in a cancerphenotype. The framework presented in this work along with algorithms derivedfrom it, represents a novel approach for inferring cancer progression, whoseaccuracy and convergence rates surpass the existing techniques. The approachderives its power from several fields including algorithms in machine learning,theory of causality and cancer biology. Furthermore, a modular pipeline toextract ensemble-level progression models from sequenced cancer genomes isproposed. The pipeline combines state-of-the-art techniques for samplestratification, driver selection, identification of fitness-equivalentexclusive alterations and progression model inference. Furthermore, the resultsare validated by synthetic data with realistic generative models, andempirically interpreted in the context of real cancer datasets; in the latercase, biologically significant conclusions are also highlighted. Specifically,it demonstrates the pipeline's ability to reproduce much of the knowledge oncolorectal cancer, as well as to suggest novel hypotheses. Lastly, it alsoproves that the proposed framework can be applied to reconstruct theevolutionary history of cancer clones in single patients, as illustrated by anexample from clear cell renal carcinomas.
arxiv-15300-24 | Efficient Representation of Low-Dimensional Manifolds using Deep Networks | http://arxiv.org/pdf/1602.04723v1.pdf | author:Ronen Basri, David Jacobs category:cs.NE cs.LG stat.ML published:2016-02-15 summary:We consider the ability of deep neural networks to represent data that liesnear a low-dimensional manifold in a high-dimensional space. We show that deepnetworks can efficiently extract the intrinsic, low-dimensional coordinates ofsuch data. We first show that the first two layers of a deep network canexactly embed points lying on a monotonic chain, a special type of piecewiselinear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,the network can do this using an almost optimal number of parameters. We alsoshow that this network projects nearby points onto the manifold and then embedsthem with little error. We then extend these results to more general manifolds.
arxiv-15300-25 | A Test of Relative Similarity For Model Selection in Generative Models | http://arxiv.org/pdf/1511.04581v4.pdf | author:Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, Arthur Gretton category:stat.ML cs.LG published:2015-11-14 summary:Probabilistic generative models provide a powerful framework for representingdata that avoids the expense of manual annotation typically needed bydiscriminative approaches. Model selection in this generative setting can bechallenging, however, particularly when likelihoods are not easily accessible.To address this issue, we introduce a statistical test of relative similarity,which is used to determine which of two models generates samples that aresignificantly closer to a real-world reference dataset of interest. We use asour test statistic the difference in maximum mean discrepancies (MMDs) betweenthe reference dataset and each model dataset, and derive a powerful,low-variance test based on the joint asymptotic distribution of the MMDsbetween each reference-model pair. In experiments on deep generative models,including the variational auto-encoder and generative moment matching network,the tests provide a meaningful ranking of model performance as a function ofparameter and training settings.
arxiv-15300-26 | On Design Mining: Coevolution and Surrogate Models | http://arxiv.org/pdf/1506.08781v4.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE published:2015-06-29 summary:Design mining is the use of computational intelligence techniques toiteratively search and model the attribute space of physical objects evaluateddirectly through rapid prototyping to meet given objectives. It enables theexploitation of novel materials and processes without formal models or complexsimulation. In this paper, we focus upon the coevolutionary nature of thedesign process when it is decomposed into concurrent sub-design threads due tothe overall complexity of the task. Using an abstract, tuneable model ofcoevolution we consider strategies to sample sub-thread designs for wholesystem testing, how best to construct and use surrogate models within thecoevolutionary scenario, and the effects of access to multiple whole system(physical) testing equipment on performance. Drawing on our findings, the paperthen describes the effective design of an array of six heterogeneousvertical-axis wind turbines.
arxiv-15300-27 | Maximin Action Identification: A New Bandit Framework for Games | http://arxiv.org/pdf/1602.04676v1.pdf | author:Aurélien Garivier, Emilie Kaufmann, Wouter Koolen category:math.ST cs.GT stat.ML stat.TH published:2016-02-15 summary:We study an original problem of pure exploration in a strategic bandit modelmotivated by Monte Carlo Tree Search. It consists in identifying the bestaction in a game, when the player may sample random outcomes of sequentiallychosen pairs of actions. We propose two strategies for the fixed-confidencesetting: Maximin-LUCB, based on lower-and upper-confidence bounds; andMaximin-Racing, which operates by successively eliminating the sub-optimalactions. We discuss the sample complexity of both methods and compare theirperformance empirically. We sketch a lower bound analysis, and possibleconnections to an optimal algorithm.
arxiv-15300-28 | Deep Exploration via Bootstrapped DQN | http://arxiv.org/pdf/1602.04621v1.pdf | author:Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy category:cs.LG cs.AI cs.SY stat.ML published:2016-02-15 summary:Efficient exploration in complex environments remains a major challenge forreinforcement learning. We propose bootstrapped DQN, a simple algorithm thatexplores in a computationally and statistically efficient manner through use ofrandomized value functions. Unlike dithering strategies such as epsilon-greedyexploration, bootstrapped DQN carries out temporally-extended (or deep)exploration; this can lead to exponentially faster learning. We demonstratethese benefits in complex stochastic MDPs and in the large-scale ArcadeLearning Environment. Bootstrapped DQN substantially improves learning timesand performance across most Atari games.
arxiv-15300-29 | Generalization and Exploration via Randomized Value Functions | http://arxiv.org/pdf/1402.0635v3.pdf | author:Ian Osband, Benjamin Van Roy, Zheng Wen category:stat.ML cs.AI cs.LG cs.SY published:2014-02-04 summary:We propose randomized least-squares value iteration (RLSVI) -- a newreinforcement learning algorithm designed to explore and generalize efficientlyvia linearly parameterized value functions. We explain why versions ofleast-squares value iteration that use Boltzmann or epsilon-greedy explorationcan be highly inefficient, and we present computational results thatdemonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establishan upper bound on the expected regret of RLSVI that demonstratesnear-optimality in a tabula rasa learning context. More broadly, our resultssuggest that randomized value functions offer a promising approach to tacklinga critical challenge in reinforcement learning: synthesizing efficientexploration and effective generalization.
arxiv-15300-30 | Edge Detection for Pattern Recognition: A Survey | http://arxiv.org/pdf/1602.04593v1.pdf | author:Alex Pappachen James category:cs.CV published:2016-02-15 summary:This review provides an overview of the literature on the edge detectionmethods for pattern recognition that inspire from the understanding of humanvision. We note that edge detection is one of the most fundamental processwithin the low level vision and provides the basis for the higher level visualintelligence in primates. The recognition of the patterns within the imagesrelate closely to the spatiotemporal processes of edge formations, and itsimplementation needs a crossdisciplanry approach in neuroscience, computing andpattern recognition. In this review, the edge detectors are grouped in as edgefeatures, gradients and sketch models, and some example applications areprovided for reference. We note a significant increase in the amount ofpublished research in the last decade that utilizes edge features in a widerange of problems in computer vision and image understanding having a directimplication to pattern recognition with images.
arxiv-15300-31 | Optimal Best Arm Identification with Fixed Confidence | http://arxiv.org/pdf/1602.04589v1.pdf | author:Aurélien Garivier, Emilie Kaufmann category:math.ST cs.LG stat.ML stat.TH published:2016-02-15 summary:We provide a complete characterization of the complexity of best-armidentification in one-parameter bandit problems. We prove a new, tight lowerbound on the sample complexity. We propose the 'Track-and-Stop' strategy, whichis proved to be asymptotically optimal. It consists in a new sampling rule(which tracks the optimal proportions of arm draws highlighted by the lowerbound) and in a stopping rule named after Chernoff, for which we give a newanalysis.
arxiv-15300-32 | An Efficient Algorithm for Video Super-Resolution Based On a Sequential Model | http://arxiv.org/pdf/1506.00473v3.pdf | author:Patrick Héas, Angélique Drémeau, Cédric Herzet category:cs.CV published:2015-06-01 summary:In this work, we propose a novel procedure for video super-resolution, thatis the recovery of a sequence of high-resolution images from its low-resolutioncounterpart. Our approach is based on a "sequential" model (i.e., eachhigh-resolution frame is supposed to be a displaced version of the precedingone) and considers the use of sparsity-enforcing priors. Both the recovery ofthe high-resolution images and the motion fields relating them is tackled. Thisleads to a large-dimensional, non-convex and non-smooth problem. We propose analgorithmic framework to address the latter. Our approach relies on fastgradient evaluation methods and modern optimization techniques fornon-differentiable/non-convex problems. Unlike some other previous works, weshow that there exists a provably-convergent method with a complexity linear inthe problem dimensions. We assess the proposed optimization method on {severalvideo benchmarks and emphasize its good performance with respect to the stateof the art.}
arxiv-15300-33 | Secure Approximation Guarantee for Cryptographically Private Empirical Risk Minimization | http://arxiv.org/pdf/1602.04579v1.pdf | author:Toshiyuki Takada, Hiroyuki Hanada, Yoshiji Yamada, Jun Sakuma, Ichiro Takeuchi category:stat.ML cs.CR cs.LG published:2016-02-15 summary:Privacy concern has been increasingly important in many machine learning (ML)problems. We study empirical risk minimization (ERM) problems under securemulti-party computation (MPC) frameworks. Main technical tools for MPC havebeen developed based on cryptography. One of limitations in currentcryptographically private ML is that it is computationally intractable toevaluate non-linear functions such as logarithmic functions or exponentialfunctions. Therefore, for a class of ERM problems such as logistic regressionin which non-linear function evaluations are required, one can only obtainapproximate solutions. In this paper, we introduce a novel cryptographicallyprivate tool called secure approximation guarantee (SAG) method. The keyproperty of SAG method is that, given an arbitrary approximate solution, it canprovide a non-probabilistic assumption-free bound on the approximation qualityunder cryptographically secure computation framework. We demonstrate thebenefit of the SAG method by applying it to several problems including apractical privacy-preserving data analysis task on genomic and clinicalinformation.
arxiv-15300-34 | Personalized Expertise Search at LinkedIn | http://arxiv.org/pdf/1602.04572v1.pdf | author:Viet Ha-Thuc, Ganesh Venkataraman, Mario Rodriguez, Shakti Sinha, Senthil Sundaram, Lin Guo category:cs.IR cs.LG cs.SI published:2016-02-15 summary:LinkedIn is the largest professional network with more than 350 millionmembers. As the member base increases, searching for experts becomes more andmore challenging. In this paper, we propose an approach to address the problemof personalized expertise search on LinkedIn, particularly for exploratorysearch queries containing {\it skills}. In the offline phase, we introduce acollaborative filtering approach based on matrix factorization. Our approachestimates expertise scores for both the skills that members list on theirprofiles as well as the skills they are likely to have but do not explicitlylist. In the online phase (at query time) we use expertise scores on theseskills as a feature in combination with other features to rank the results. Tolearn the personalized ranking function, we propose a heuristic to extracttraining data from search logs while handling position and sample selectionbiases. We tested our models on two products - LinkedIn homepage and LinkedInrecruiter. A/B tests showed significant improvements in click through rates -31% for CTR@1 for recruiter (18% for homepage) as well as downstream messagessent from search - 37% for recruiter (20% for homepage). As of writing thispaper, these models serve nearly all live traffic for skills search on LinkedInhomepage as well as LinkedIn recruiter.
arxiv-15300-35 | Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding | http://arxiv.org/pdf/1510.00149v5.pdf | author:Song Han, Huizi Mao, William J. Dally category:cs.CV cs.NE published:2015-10-01 summary:Neural networks are both computationally intensive and memory intensive,making them difficult to deploy on embedded systems with limited hardwareresources. To address this limitation, we introduce "deep compression", a threestage pipeline: pruning, trained quantization and Huffman coding, that worktogether to reduce the storage requirement of neural networks by 35x to 49xwithout affecting their accuracy. Our method first prunes the network bylearning only the important connections. Next, we quantize the weights toenforce weight sharing, finally, we apply Huffman coding. After the first twosteps we retrain the network to fine tune the remaining connections and thequantized centroids. Pruning, reduces the number of connections by 9x to 13x;Quantization then reduces the number of bits that represent each connectionfrom 32 to 5. On the ImageNet dataset, our method reduced the storage requiredby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our methodreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss ofaccuracy. This allows fitting the model into on-chip SRAM cache rather thanoff-chip DRAM memory. Our compression method also facilitates the use ofcomplex neural networks in mobile applications where application size anddownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,compressed network has 3x to 4x layerwise speedup and 3x to 7x better energyefficiency.
arxiv-15300-36 | Adaptive estimation of the copula correlation matrix for semiparametric elliptical copulas | http://arxiv.org/pdf/1305.6526v3.pdf | author:Marten Wegkamp, Yue Zhao category:stat.ML published:2013-05-28 summary:We study the adaptive estimation of copula correlation matrix $\Sigma$ forthe semi-parametric elliptical copula model. In this context, the correlationsare connected to Kendall's tau through a sine function transformation. Hence, anatural estimate for $\Sigma$ is the plug-in estimator $\hat{\Sigma}$ withKendall's tau statistic. We first obtain a sharp bound on the operator norm of$\hat{\Sigma}-\Sigma$. Then we study a factor model of $\Sigma$, for which wepropose a refined estimator $\widetilde{\Sigma}$ by fitting a low-rank matrixplus a diagonal matrix to $\hat{\Sigma}$ using least squares with a nuclearnorm penalty on the low-rank matrix. The bound on the operator norm of$\hat{\Sigma}-\Sigma$ serves to scale the penalty term, and we obtain finitesample oracle inequalities for $\widetilde{\Sigma}$. We also consider anelementary factor copula model of $\Sigma$, for which we propose closed-formestimators. All of our estimation procedures are entirely data-driven.
arxiv-15300-37 | Adversarial Top-$K$ Ranking | http://arxiv.org/pdf/1602.04567v1.pdf | author:Changho Suh, Vincent Y. F. Tan, Renbo Zhao category:cs.IR cs.IT cs.LG math.IT stat.ML published:2016-02-15 summary:We study the top-$K$ ranking problem where the goal is to recover the set oftop-$K$ ranked items out of a large collection of items based on partiallyrevealed preferences. We consider an adversarial crowdsourced setting wherethere are two population sets, and pairwise comparison samples drawn from oneof the populations follow the standard Bradley-Terry-Luce model (i.e., thechance of item $i$ beating item $j$ is proportional to the relative score ofitem $i$ to item $j$), while in the other population, the corresponding chanceis inversely proportional to the relative score. When the relative size of thetwo populations is known, we characterize the minimax limit on the sample sizerequired (up to a constant) for reliably identifying the top-$K$ items, anddemonstrate how it scales with the relative size. Moreover, by leveraging atensor decomposition method for disambiguating mixture distributions, we extendour result to the more realistic scenario in which the relative population sizeis unknown, thus establishing an upper bound on the fundamental limit of thesample size for recovering the top-$K$ set.
arxiv-15300-38 | Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs | http://arxiv.org/pdf/1601.03778v2.pdf | author:Baichuan Zhang, Sutanay Choudhury, Mohammad Al Hasan, Xia Ning, Khushbu Agarwal, Sumit Purohit, Paola Pesntez Cabrera category:cs.LG cs.AI cs.IR published:2016-01-14 summary:Link prediction, or predicting the likelihood of a link in a knowledge graphbased on its existing state is a key research task. It differs from atraditional link prediction task in that the links in a knowledge graph arecategorized into different predicates and the link prediction performance ofdifferent predicates in a knowledge graph generally varies widely. In thiswork, we propose a latent feature embedding based link prediction model whichconsiders the prediction task for each predicate disjointly. To learn the modelparameters it utilizes a Bayesian personalized ranking based optimizationtechnique. Experimental results on large-scale knowledge bases such as YAGO2show that our link prediction approach achieves substantially higherperformance than several state-of-art approaches. We also show that for a givenpredicate the topological properties of the knowledge graph induced by thegiven predicate edges are key indicators of the link prediction performance ofthat predicate in the knowledge graph.
arxiv-15300-39 | Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining | http://arxiv.org/pdf/1602.04548v1.pdf | author:Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML published:2016-02-15 summary:In this paper we study predictive pattern mining problems where the goal isto construct a predictive model based on a subset of predictive patterns in thedatabase. Our main contribution is to introduce a novel method called safepattern pruning (SPP) for a class of predictive pattern mining problems. TheSPP method allows us to efficiently find a superset of all the predictivepatterns in the database that are needed for the optimal predictive model. Theadvantage of the SPP method over existing boosting-type method is that theformer can find the superset by a single search over the database, while thelatter requires multiple searches. The SPP method is inspired by recentdevelopment of safe feature screening. In order to extend the idea of safefeature screening into predictive pattern mining, we derive a novel pruningrule called safe pattern pruning (SPP) rule that can be used for searching overthe tree defined among patterns in the database. The SPP rule has a propertythat, if a node corresponding to a pattern in the database is pruned out by theSPP rule, then it is guaranteed that all the patterns corresponding to itsdescendant nodes are never needed for the optimal predictive model. We applythe SPP method to graph mining and item-set mining problems, and demonstrateits computational advantage.
arxiv-15300-40 | ICU Patient Flow Prediction via Discriminative Learning of Mutually-Correcting Processes | http://arxiv.org/pdf/1602.05112v1.pdf | author:Hongteng Xu, Weichang Wu, Shamim Nemati, Hongyuan Zha category:cs.LG published:2016-02-14 summary:Over the past decade the rate of intensive care unit (ICU) use in the UnitedStates has been increasing, with a recent study reporting almost one in threeMedicare beneficiaries experiencing an ICU visit during the last month of theirlives. With an aging population and ever-growing demand for critical care,effective management of patient flow and transition among different carefacilities will prove indispensible for shortening lengths of hospital stays,improving patient outcomes, allocating critical resources, and reducingpreventable re-admissions. In this paper, we focus on a new problem ofpredicting the so-called ICU patient flow from longitudinal electronic healthrecords (EHRs), which is not explored via existing machine learning techniques.By treating a sequence of transition events as a point process, we develop anovel framework for modeling patient flow through various ICU care units andpredict patients' destination ICUs and duration days jointly. Instead oflearning a generative point process model via maximum likelihood estimation, wepropose a novel discriminative learning algorithm aiming at improving theprediction of transition events. By parameterizing the proposed model as amutually-correcting process, we formulate the problem as a generalized linearmodel, i.e., multinomial logistic regression, which yields itself to efficientlearning via alternating direction method of multipliers (ADMM). Furthermore,we achieve simultaneous feature selection and learning by adding a group-lassoregularizer to the ADMM algorithm. Using real-world data of ICU patients, weshow that our method obtains superior performance in terms of accuracy ofpredicting the destination ICU transition and duration of each ICU occupancy.
arxiv-15300-41 | Validity and reliability of free software for bidimensional gait analysis | http://arxiv.org/pdf/1602.04513v1.pdf | author:Ana Paula Quixadá, Andrea Naomi Onodera, Norberto Peña, José Garcia Vivas Miranda, Katia Nunes Sá category:q-bio.QM cs.CV physics.med-ph published:2016-02-14 summary:Despite the evaluation systems of human movement that have been advancing inrecent decades, their use are not feasible for clinical practice because it hasa high cost and scarcity of trained operators to interpret their results. Anideal videogrammetry system should be easy to use, low cost, with minimalequipment, and fast realization. The CvMob is a free tool for dynamicevaluation of human movements that express measurements in figures, tables, andgraphics. This paper aims to determine if CvMob is a reliable tool for theevaluation of two dimensional human gait. This is a validity and reliabilitystudy. The sample was composed of 56 healthy individuals who walked on a9-meterlong walkway and were simultaneously filmed by CvMob and Vicon systemcameras. Linear trajectories and angular measurements were compared to validatethe CvMob system, and inter and intrarater findings of the same measurementswere used to determine reliability. A strong correlation (rs mean = 0.988) ofthe linear trajectories between systems and inter and intrarater analysis werefound. According to the Bland-Altman method, the angles that had good agreementbetween systems were maximum flexion and extension (stance and swing) of theknee and dorsiflexion range of motion and stride length. The CvMob is areliable tool for analysis of linear motion and lengths in two-dimensionalevaluations of human gait. The angular measurements demonstrate high agreementfor the knee joint; however, the hip and ankle measurements were limited bydifferences between systems.
arxiv-15300-42 | Learning Granger Causality for Hawkes Processes | http://arxiv.org/pdf/1602.04511v1.pdf | author:Hongteng Xu, Mehrdad Farajtabar, Hongyuan Zha category:cs.LG stat.ML published:2016-02-14 summary:Learning Granger causality for general point processes is a very challengingtask. In this paper, we propose an effective method, learning Grangercausality, for a special but significant type of point processes --- Hawkesprocess. We reveal the relationship between Hawkes process's impact functionand its Granger causality graph. Specifically, our model represents impactfunctions using a series of basis functions and recovers the Granger causalitygraph via group sparsity of the impact functions' coefficients. We propose aneffective learning algorithm combining a maximum likelihood estimator (MLE)with a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility ofour model allows to incorporate the clustering structure event types intolearning framework. We analyze our learning algorithm and propose an adaptiveprocedure to select basis functions. Experiments on both synthetic andreal-world data show that our method can learn the Granger causality graph andthe triggering patterns of the Hawkes processes simultaneously.
arxiv-15300-43 | Stacked What-Where Auto-encoders | http://arxiv.org/pdf/1506.02351v8.pdf | author:Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun category:stat.ML cs.LG cs.NE published:2015-06-08 summary:We present a novel architecture, the "stacked what-where auto-encoders"(SWWAE), which integrates discriminative and generative pathways and provides aunified approach to supervised, semi-supervised and unsupervised learningwithout relying on sampling during training. An instantiation of SWWAE uses aconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, andemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce thereconstruction. The objective function includes reconstruction terms thatinduce the hidden states in the Deconvnet to be similar to those of theConvnet. Each pooling layer produces two sets of variables: the "what" whichare fed to the next layer, and its complementary variable "where" that are fedto the corresponding layer in the generative decoder.
arxiv-15300-44 | Embracing Error to Enable Rapid Crowdsourcing | http://arxiv.org/pdf/1602.04506v1.pdf | author:Ranjay Krishna, Kenji Hata, Stephanie Chen, Joshua Kravitz, David A. Shamma, Li Fei-Fei, Michael S. Bernstein category:cs.HC cs.CV H.5.m published:2016-02-14 summary:Microtask crowdsourcing has enabled dataset advances in social science andmachine learning, but existing crowdsourcing schemes are too expensive to scaleup with the expanding volume of data. To scale and widen the applicability ofcrowdsourcing, we present a technique that produces extremely rapid judgmentsfor binary and categorical labels. Rather than punishing all errors, whichcauses workers to proceed slowly and deliberately, our technique speeds upworkers' judgments to the point where errors are acceptable and even expected.We demonstrate that it is possible to rectify these errors by randomizing taskorder and modeling response latency. We evaluate our technique on a breadth ofcommon labeling tasks such as image verification, word similarity, sentimentanalysis and topic classification. Where prior work typically achieves a 0.25xto 1x speedup over fixed majority vote, our approach often achieves an order ofmagnitude (10x) speedup.
arxiv-15300-45 | Can we still avoid automatic face detection? | http://arxiv.org/pdf/1602.04504v1.pdf | author:Michael J. Wilber, Vitaly Shmatikov, Serge Belongie category:cs.CV published:2016-02-14 summary:After decades of study, automatic face detection and recognition systems arenow accurate and widespread. Naturally, this means users who wish to avoidautomatic recognition are becoming less able to do so. Where do we stand inthis cat-and-mouse race? We currently live in a society where everyone carriesa camera in their pocket. Many people willfully upload most or all of thepictures they take to social networks which invest heavily in automatic facerecognition systems. In this setting, is it still possible forprivacy-conscientious users to avoid automatic face detection and recognition?If so, how? Must evasion techniques be obvious to be effective, or are therestill simple measures that users can use to protect themselves? In this work, we find ways to evade face detection on Facebook, arepresentative example of a popular social network that uses automatic facedetection to enhance their service. We challenge widely-held beliefs aboutevading face detection: do our old techniques such as blurring the face regionor wearing "privacy glasses" still work? We show that in general,state-of-the-art detectors can often find faces even if the subject wearsoccluding clothing or even if the uploader damages the photo to prevent facesfrom being detected.
arxiv-15300-46 | Do We Need Binary Features for 3D Reconstruction? | http://arxiv.org/pdf/1602.04502v1.pdf | author:Bin Fan, Qingqun Kong, Wei Sui, Zhiheng Wang, Xinchao Wang, Shiming Xiang, Chunhong Pan, Pascal Fua category:cs.CV published:2016-02-14 summary:Binary features have been incrementally popular in the past few years due totheir low memory footprints and the efficient computation of Hamming distancebetween binary descriptors. They have been shown with promising results on somereal time applications, e.g., SLAM, where the matching operations are relativefew. However, in computer vision, there are many applications such as 3Dreconstruction requiring lots of matching operations between local features.Therefore, a natural question is that is the binary feature still a promisingsolution to this kind of applications? To get the answer, this paper conducts acomparative study of binary features and their matching methods on the contextof 3D reconstruction in a recently proposed large scale mutliview stereodataset. Our evaluations reveal that not all binary features are capable ofthis task. Most of them are inferior to the classical SIFT based method interms of reconstruction accuracy and completeness with a not significant bettercomputational performance.
arxiv-15300-47 | Convolutional Tables Ensemble: classification in microseconds | http://arxiv.org/pdf/1602.04489v1.pdf | author:Aharon Bar-Hillel, Eyal Krupka, Noam Bloom category:cs.CV cs.LG 68T45 published:2016-02-14 summary:We study classifiers operating under severe classification time constraints,corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble(CTE), an inherently fast architecture for object category recognition. Thearchitecture is based on convolutionally-applied sparse feature extraction,using trees or ferns, and a linear voting layer. Several structure andoptimization variants are considered, including novel decision functions, treelearning algorithm, and distillation from CNN to CTE architecture. Accuracyimprovements of 24-45% over related art of similar speed are demonstrated onstandard object recognition benchmarks. Using Pareto speed-accuracy curves, weshow that CTE can provide better accuracy than Convolutional Neural Networks(CNN) for a certain range of classification time constraints, or alternativelyprovide similar error rates with 5-200X speedup.
arxiv-15300-48 | Benefits of depth in neural networks | http://arxiv.org/pdf/1602.04485v1.pdf | author:Matus Telgarsky category:cs.LG cs.NE stat.ML published:2016-02-14 summary:For any positive integer $k$, there exist neural networks with $\Theta(k^3)$layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters whichcan not be approximated by networks with $\mathcal{O}(k)$ layers unless theyare exponentially large --- they must possess $\Omega(2^k)$ nodes. This resultis proved here for a class of nodes termed "semi-algebraic gates" whichincludes the common choices of ReLU, maximum, indicator, and piecewisepolynomial functions, therefore establishing benefits of depth against not juststandard networks with ReLU gates, but also convolutional networks with ReLUand maximization gates, and boosted decision trees (in this last case with astronger separation: $\Omega(2^{k^3})$ total tree nodes are required).
arxiv-15300-49 | Learning Representations of Affect from Speech | http://arxiv.org/pdf/1511.04747v6.pdf | author:Sayan Ghosh, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer category:cs.CL cs.LG published:2015-11-15 summary:There has been a lot of prior work on representation learning for speechrecognition applications, but not much emphasis has been given to aninvestigation of effective representations of affect from speech, where theparalinguistic elements of speech are separated out from the verbal content. Inthis paper, we explore denoising autoencoders for learning paralinguisticattributes i.e. categorical and dimensional affective traits from speech. Weshow that the representations learnt by the bottleneck layer of the autoencoderare highly discriminative of activation intensity and at separating outnegative valence (sadness and anger) from positive valence (happiness). Weexperiment with different input speech features (such as FFT and log-melspectrograms with temporal context windows), and different autoencoderarchitectures (such as stacked and deep autoencoders). We also learn utterancespecific representations by a combination of denoising autoencoders and BLSTMbased recurrent autoencoders. Emotion classification is performed with thelearnt temporal/dynamic representations to evaluate the quality of therepresentations. Experiments on a well-established real-life speech dataset(IEMOCAP) show that the learnt representations are comparable to state of theart feature extractors (such as voice quality features and MFCCs) and arecompetitive with state-of-the-art approaches at emotion and dimensional affectrecognition.
arxiv-15300-50 | Action Recognition using Visual Attention | http://arxiv.org/pdf/1511.04119v3.pdf | author:Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov category:cs.LG cs.CV published:2015-11-12 summary:We propose a soft attention based model for the task of action recognition invideos. We use multi-layered Recurrent Neural Networks (RNNs) with LongShort-Term Memory (LSTM) units which are deep both spatially and temporally.Our model learns to focus selectively on parts of the video frames andclassifies videos after taking a few glimpses. The model essentially learnswhich parts in the frames are relevant for the task at hand and attaches higherimportance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51and Hollywood2 datasets and analyze how the model focuses its attentiondepending on the scene and the action being performed.
arxiv-15300-51 | On the Expressive Power of Deep Learning: A Tensor Analysis | http://arxiv.org/pdf/1509.05009v2.pdf | author:Nadav Cohen, Or Sharir, Amnon Shashua category:cs.NE cs.LG cs.NA stat.ML published:2015-09-16 summary:It has long been conjectured that hypotheses spaces suitable for data that iscompositional in nature, such as text or images, may be more efficientlyrepresented with deep hierarchical networks than with shallow ones. Despite thevast empirical evidence supporting this belief, theoretical justifications todate are limited. In particular, they do not account for the locality, sharingand pooling constructs of convolutional networks, the most successful deeplearning architecture to date. In this work we derive a deep networkarchitecture based on arithmetic circuits that inherently employs locality,sharing and pooling. An equivalence between the networks and hierarchicaltensor factorizations is established. We show that a shallow networkcorresponds to CP (rank-1) decomposition, whereas a deep network corresponds toHierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides anegligible set, all functions that can be implemented by a deep network ofpolynomial size, require exponential size in order to be realized (or evenapproximated) by a shallow network. Since log-space computation transforms ournetworks into SimNets, the result applies directly to a deep learningarchitecture demonstrating promising empirical performance. The constructionand theory developed in this paper shed new light on various practices andideas employed by the deep learning community.
arxiv-15300-52 | Generalization Properties of Learning with Random Features | http://arxiv.org/pdf/1602.04474v1.pdf | author:Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco category:stat.ML cs.LG published:2016-02-14 summary:We study the generalization properties of regularized learning with randomfeatures in the statistical learning theory framework. We show that optimallearning errors can be achieved with a number of features smaller than thenumber of examples. As a byproduct, we also show that learning with randomfeatures can be seen as a form of regularization, rather than only a way tospeed up computations.
arxiv-15300-53 | Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics | http://arxiv.org/pdf/1602.04450v1.pdf | author:Felix Berkenkamp, Andreas Krause, Angela P. Schoellig category:cs.RO cs.LG cs.SY published:2016-02-14 summary:Robotics algorithms typically depend on various parameters, the choice ofwhich significantly affects the robot's performance. While an initial guess forthe parameters may be obtained from dynamic models of the robot, parameters areusually tuned manually on the real system to achieve the best performance.Optimization algorithms, such as Bayesian optimization, have been used toautomate this process. However, these methods may evaluate parameters duringthe optimization process that lead to safety-critical system failures.Recently, a safe Bayesian optimization algorithm, called SafeOpt, has beendeveloped and applied in robotics, which guarantees that the performance of thesystem never falls below a critical value; that is, safety is defined based onthe performance function. However, coupling performance and safety is notdesirable in most cases. In this paper, we define separate functions forperformance and safety. We present a generalized SafeOpt algorithm that, givenan initial safe guess for the parameters, maximizes performance but onlyevaluates parameters that satisfy all safety constraints with high probability.It achieves this by modeling the underlying and unknown performance andconstraint functions as Gaussian processes. We provide a theoretical analysisand demonstrate in experiments on a quadrotor vehicle that the proposedalgorithm enables fast, automatic, and safe optimization of tuning parameters.Moreover, we show an extension to context- or environment-dependent, safeoptimization in the experiments.
arxiv-15300-54 | Distributed Training of Structured SVM | http://arxiv.org/pdf/1506.02620v2.pdf | author:Ching-pei Lee, Kai-Wei Chang, Shyam Upadhyay, Dan Roth category:stat.ML cs.DC cs.LG published:2015-06-08 summary:Training structured prediction models is time-consuming. However, mostexisting approaches only use a single machine, thus, the advantage of computingpower and the capacity for larger data sets of multiple machines have not beenexploited. In this work, we propose an efficient algorithm for distributedlytraining structured support vector machines based on a distributedblock-coordinate descent method. Both theoretical and experimental resultsindicate that our method is efficient.
arxiv-15300-55 | Distributed Time-Varying Graph Filtering | http://arxiv.org/pdf/1602.04436v1.pdf | author:Elvin Isufi, Andreas Loukas, Andrea Simonetto, Geert Leus category:cs.LG cs.SY stat.ML published:2016-02-14 summary:One of the cornerstones of the field of signal processing on graphs are graphfilters, direct analogues of classical filters, but intended for signalsdefined on graphs. This work brings forth new insights on the distributed graphfiltering problem. We design a family of autoregressive moving average (ARMA)recursions, which (i) are able to approximate any desired graph frequencyresponse, and (ii) give exact solutions for tasks such as graph signaldenoising and interpolation. The design philosophy, which allows us to design the ARMA coefficientsindependently from the underlying graph, renders the ARMA graph filterssuitable in static and, particularly, time-varying settings. The latter occurwhen the graph signal and/or graph are changing over time. We show that in caseof a time-varying graph signal our approach extends naturally to atwo-dimensional filter, operating concurrently in the graph and regular timedomains. We also derive sufficient conditions for filter stability when thegraph and signal are time-varying. The analytical and numerical resultspresented in this paper illustrate that ARMA graph filters are practicallyappealing for static and time-varying settings, accompanied by strongtheoretical guarantees.
arxiv-15300-56 | Random Forest Based Approach for Concept Drift Handling | http://arxiv.org/pdf/1602.04435v1.pdf | author:A. Zhukov, D. Sidorov, A. Foley category:cs.AI cs.LG math.ST stat.TH published:2016-02-14 summary:Concept drift has potential in smart grid analysis because the socio-economicbehaviour of consumers is not governed by the laws of physics. Likewise thereare also applications in wind power forecasting. In this paper we presentdecision tree ensemble classification method based on the Random Forestalgorithm for concept drift. The weighted majority voting ensemble aggregationrule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.Base learner weight in our case is computed for each sample evaluation usingbase learners accuracy and intrinsic proximity measure of Random Forest. Ouralgorithm exploits both temporal weighting of samples and ensemble pruning as aforgetting strategy. We present results of empirical comparison of our methodwith original random forest with incorporated "replace-the-looser" forgettingandother state-of-the-art concept-drfit classifiers like AWE2.
arxiv-15300-57 | Frequency Analysis of Temporal Graph Signals | http://arxiv.org/pdf/1602.04434v1.pdf | author:Andreas Loukas, Damien Foucard category:cs.LG cs.SY stat.ML published:2016-02-14 summary:This letter extends the concept of graph-frequency to graph signals thatevolve with time. Our goal is to generalize and, in fact, unify the familiarconcepts from time- and graph-frequency analysis. To this end, we study a jointtemporal and graph Fourier transform (JFT) and demonstrate its attractiveproperties. We build on our results to create filters which act on the joint(temporal and graph) frequency domain, and show how these can be used toperform interference cancellation. The proposed algorithms are distributed,have linear complexity, and can approximate any desired joint filteringobjective.
arxiv-15300-58 | Unsupervised Domain Adaptation with Residual Transfer Networks | http://arxiv.org/pdf/1602.04433v1.pdf | author:Mingsheng Long, Jianmin Wang, Michael I. Jordan category:cs.LG published:2016-02-14 summary:The recent success of deep neural networks relies on massive amounts oflabeled data. For a target task where labeled data is unavailable, domainadaptation can transfer a learner from a different source domain. In thispaper, we propose a new approach to domain adaptation in deep networks that cansimultaneously learn adaptive classifiers and transferable features fromlabeled data in the source domain and unlabeled data in the target domain. Werelax a shared-classifier assumption made by previous methods and assume thatthe source classifier and target classifier differ by a residual function. Weenable classifier adaptation by plugging several layers into the deep networkto explicitly learn the residual function with reference to the targetclassifier. We embed features of multiple layers into reproducing kernelHilbert spaces (RKHSs) and match feature distributions for feature adaptation.The adaptation behaviors can be achieved in most feed-forward models byextending them with new residual layers and loss functions, which can betrained efficiently using standard back-propagation. Empirical evidenceexhibits that the approach outperforms state of art methods on standard domainadaptation datasets.
arxiv-15300-59 | Interactive algorithms: From pool to stream | http://arxiv.org/pdf/1602.01132v2.pdf | author:Sivan Sabato, Tom Hess category:stat.ML cs.LG math.ST stat.TH published:2016-02-02 summary:We consider interactive algorithms in the pool-based setting, and in thestream-based setting. Interactive algorithms observe suggested elements(representing actions or queries), and interactively select some of them andreceive responses. Stream-based algorithms are not allowed to select suggestedelements after more elements have been observed, while pool-based algorithmscan select elements at any order. We assume that the available elements aregenerated independently according to some distribution, and design stream-basedalgorithms that emulate black-box pool-based interactive algorithms. We providetwo such emulating algorithms. The first algorithm can emulate any pool-basedalgorithm, but the number of suggested elements that need to be observed mightbe exponential in the number of selected elements. The second algorithm appliesto the class of utility-based interactive algorithms, and the number ofsuggested elements that it observes is linear in the number of selectedelements. For the case of utility-based emulation, we also provide a lowerbound showing that near-linearity is necessary.
arxiv-15300-60 | Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents | http://arxiv.org/pdf/1602.04427v1.pdf | author:Zheng Xu, Douglas Burdick, Louiqa Raschid category:cs.CL published:2016-02-14 summary:There is a wealth of information about financial systems that is embedded indocument collections. In this paper, we focus on a specialized text extractiontask for this domain. The objective is to extract mentions of names offinancial institutions, or FI names, from financial prospectus documents, andto identify the corresponding real world entities, e.g., by matching against acorpus of such entities. The tasks are Named Entity Recognition (NER) andEntity Resolution (ER); both are well studied in the literature. Ourcontribution is to develop a rule-based approach that will exploit lists of FInames for both tasks; our solution is labeled Dict-based NER and Rank-based ER.Since the FI names are typically represented by a root, and a suffix thatmodifies the root, we use these lists of FI names to create specialized rootand suffix dictionaries. To evaluate the effectiveness of our specializedsolution for extracting FI names, we compare Dict-based NER with a generalpurpose rule-based NER solution, ORG NER. Our evaluation highlights thebenefits and limitations of specialized versus general purpose approaches, andpresents additional suggestions for tuning and customization for FI nameextraction. To our knowledge, our proposed solutions, Dict-based NER andRank-based ER, and the root and suffix dictionaries, are the first attempt toexploit specialized knowledge, i.e., lists of FI names, for rule-based NER andER.
arxiv-15300-61 | Hi Detector, What's Wrong with that Object? Identifying Irregular Object From Images by Modelling the Detection Score Distribution | http://arxiv.org/pdf/1602.04422v1.pdf | author:Peng Wang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel, Heng Tao Shen category:cs.CV published:2016-02-14 summary:In this work, we study the challenging problem of identifying the irregularstatus of objects from images in an "open world" setting, that is,distinguishing the irregular status of an object category from its regularstatus as well as objects from other categories in the absence of "irregularobject" training data. To address this problem, we propose a novel approach byinspecting the distribution of the detection scores at multiple image regionsbased on the detector trained from the "regular object" and "other objects".The key observation motivating our approach is that for "regular object" imagesas well as "other objects" images, the region-level scores follow their ownessential patterns in terms of both the score values and the spatialdistributions while the detection scores obtained from an "irregular object"image tend to break these patterns. To model this distribution, we propose touse Gaussian Processes (GP) to construct two separate generative models for thecase of the "regular object" and the "other objects". More specifically, wedesign a new covariance function to simultaneously model the detection score ata single region and the score dependencies at multiple regions. We finallydemonstrate the superior performance of our method on a large dataset newlyproposed in this paper.
arxiv-15300-62 | Identifiability assumptions for directed graphical models with feedback | http://arxiv.org/pdf/1602.04418v1.pdf | author:Gunwoong Park, Garvesh Raskutti category:stat.ML cs.LG published:2016-02-14 summary:Directed graphical models provide a useful framework for modeling causal ordirectional relationships for multivariate data. Prior work has largely focusedon identifiability and search algorithms for directed acyclic graphical (DAG)models. In many applications, feedback naturally arises and directed graphicalmodels that permit cycles arise. However theory and methodology for directedgraphical models with feedback are considerably less developed since graphswith cycles pose a number of additional challenges. In this paper we addressthe issue of identifiability for general directed cyclic graphical (DCG) modelssatisfying only the Markov assumption. In particular, in addition to thefaithfulness assumption which has already been introduced for cyclic models, weintroduce two new identifiability assumptions, one based on selecting the modelwith the fewest edges and the other based on selecting the DCG model thatentails the maximum d-separation rules. We provide theoretical resultscomparing these assumptions which shows that: (1) selecting models with thelargest number of d-separation rules is strictly weaker than the faithfulnessassumption; (2) unlike for DAG models, selecting models with the fewest edgesdo not necessarily result in a milder assumption than the faithfulnessassumption. We also provide connections between our two new principles andminimality assumptions which lead to a ranking of how strong and weak variousidentifiability and minimality assumptions are for both DAG and DCG models. Weuse our identifiability assumptions to develop search algorithms forsmall-scale DCG models. Our simulations results using our search algorithmssupport our theoretical results, showing that our two new principles generallyout-perform the faithfulness assumption in terms of selecting the true skeletonfor DCG models.
arxiv-15300-63 | Convolutional neural networks with low-rank regularization | http://arxiv.org/pdf/1511.06067v3.pdf | author:Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, Weinan E category:cs.LG cs.CV stat.ML published:2015-11-19 summary:Large CNNs have delivered impressive performance in various computer visionapplications. But the storage and computation requirements make it problematicfor deploying these models on mobile devices. Recently, tensor decompositionshave been used for speeding up CNNs. In this paper, we further develop thetensor decomposition technique. We propose a new algorithm for computing thelow-rank tensor decomposition for removing the redundancy in the convolutionkernels. The algorithm finds the exact global optimizer of the decompositionand is more effective than iterative methods. Based on the decomposition, wefurther propose a new method for training low-rank constrained CNNs fromscratch. Interestingly, while achieving a significant speedup, sometimes thelow-rank constrained CNNs delivers significantly better performance than theirnon-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rankNIN model achieves $91.31\%$ accuracy (without data augmentation), which alsoimproves upon state-of-the-art result. We evaluated the proposed method onCIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet,NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 isreduced by half while the performance is still comparable. Empirical successsuggests that low-rank tensor decompositions can be a very useful tool forspeeding up large CNNs.
arxiv-15300-64 | Convex Optimization For Non-Convex Problems via Column Generation | http://arxiv.org/pdf/1602.04409v1.pdf | author:Julian Yarkony, Kamalika Chaudhuri category:cs.LG published:2016-02-14 summary:We apply column generation to approximating complex structured objects via aset of primitive structured objects under either the cross entropy or L2 loss.We use L1 regularization to encourage the use of few structured primitiveobjects. We attack approximation using convex optimization over an infinitenumber of variables each corresponding to a primitive structured object thatare generated on demand by easy inference in the Lagrangian dual. We apply ourapproach to producing low rank approximations to large 3-way tensors.
arxiv-15300-65 | Communication Efficient Distributed Kernel Principal Component Analysis | http://arxiv.org/pdf/1503.06858v4.pdf | author:Maria-Florina Balcan, Yingyu Liang, Le Song, David Woodruff, Bo Xie category:cs.LG published:2015-03-23 summary:Kernel Principal Component Analysis (KPCA) is a key machine learningalgorithm for extracting nonlinear features from data. In the presence of alarge volume of high dimensional data collected in a distributed fashion, itbecomes very costly to communicate all of this data to a single data center andthen perform kernel PCA. Can we perform kernel PCA on the entire dataset in adistributed and communication efficient fashion while maintaining provable andstrong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing acommunication efficient algorithm to perform kernel PCA in the distributedsetting. The algorithm is a clever combination of subspace embedding andadaptive sampling techniques, and we show that the algorithm can take as inputan arbitrary configuration of distributed datasets, and compute a set of globalkernel principal components with relative error guarantees independent of thedimension of the feature space or the total number of data points. Inparticular, computing $k$ principal components with relative error $\epsilon$over $s$ workers has communication cost $\tilde{O}(s \rho k/\epsilon+sk^2/\epsilon^3)$ words, where $\rho$ is the average number of nonzero entriesin each data point. Furthermore, we experimented the algorithm with large-scalereal world datasets and showed that the algorithm produces a high qualitykernel PCA solution while using significantly less communication thanalternative approaches.
arxiv-15300-66 | Approximate Subspace-Sparse Recovery with Corrupted Data via Constrained $\ell_1$-Minimization | http://arxiv.org/pdf/1412.7260v2.pdf | author:Ehsan Elhamifar, Mahdi Soltanolkotabi, Shankar Sastry category:stat.ML published:2014-12-23 summary:High-dimensional data often lie in low-dimensional subspaces corresponding todifferent classes they belong to. Finding sparse representations of data pointsin a dictionary built using the collection of data helps to uncoverlow-dimensional subspaces and address problems such as clustering,classification, subset selection and more. In this paper, we address theproblem of recovering sparse representations for noisy data points in adictionary whose columns correspond to corrupted data lying close to a union ofsubspaces. We consider a constrained $\ell_1$-minimization and study conditionsunder which the solution of the proposed optimization satisfies the approximatesubspace-sparse recovery condition. More specifically, we show that each noisydata point, perturbed from a subspace by a noise of the magnitude of$\varepsilon$, will be reconstructed using data points from the same subspacewith a small error of the order of $O(\varepsilon)$ and that the coefficientscorresponding to data points in other subspaces will be sufficiently small,\ie, of the order of $O(\varepsilon)$. We do not impose any randomnessassumption on the arrangement of subspaces or distribution of data points ineach subspace. Our framework is based on a novel generalization of thenull-space property to the setting where data lie in multiple subspaces, thenumber of data points in each subspace exceeds the dimension of the subspace,and all data points are corrupted by noise. Moreover, assuming a randomdistribution for data points, we further show that coefficients from thedesired support not only reconstruct a given point with high accuracy, but alsohave sufficiently large values, \ie, of the order of $O(1)$.
arxiv-15300-67 | Semantic Scan: Detecting Subtle, Spatially Localized Events in Text Streams | http://arxiv.org/pdf/1602.04393v1.pdf | author:Abhinav Maurya, Kenton Murray, Yandong Liu, Chris Dyer, William W. Cohen, Daniel B. Neill category:cs.IR stat.ML published:2016-02-13 summary:Early detection and precise characterization of emerging topics in textstreams can be highly useful in applications such as timely and targeted publichealth interventions and discovering evolving regional business trends. Manymethods have been proposed for detecting emerging events in text streams usingtopic modeling. However, these methods have numerous shortcomings that makethem unsuitable for rapid detection of locally emerging events on massive textstreams. In this paper, we describe Semantic Scan (SS) that has been developedspecifically to overcome these shortcomings in detecting new spatially compactevents in text streams. Semantic Scan integrates novel contrastive topic modeling with onlinedocument assignment and principled likelihood ratio-based spatial scanning toidentify emerging events with unexpected patterns of keywords hidden in textstreams. This enables more timely and accurate detection and characterizationof anomalous, spatially localized emerging events. Semantic Scan does notrequire manual intervention or labeled training data, and is robust to noise inreal-world text data since it identifies anomalous text patterns that occur ina cluster of new documents rather than an anomaly in a single new document. We compare Semantic Scan to alternative state-of-the-art methods such asTopics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) adisease surveillance task monitoring free-text Emergency Department chiefcomplaints in Allegheny County, and (ii) an emerging business trend detectiontask based on Yelp reviews. On both tasks, we find that Semantic Scan providessignificantly better event detection and characterization accuracy thancompeting approaches, while providing up to an order of magnitude speedup.
arxiv-15300-68 | Constrained Multi-Slot Optimization for Ranking Recommendations | http://arxiv.org/pdf/1602.04391v1.pdf | author:Kinjal Basu, Shaunak Chatterjee, Ankan Saha category:stat.ML math.OC stat.AP 90C20, 11K36 G.1.6 published:2016-02-13 summary:Ranking items to be recommended to users is one of the main problems in largescale social media applications. This problem can be set up as amulti-objective optimization problem to allow for trading off multiple,potentially conflicting objectives (that are driven by those items) againsteach other. Most previous approaches to this problem optimize for a single slotwithout considering the interaction effect of these items on one another. In this paper, we develop a constrained multi-slot optimization formulation,which allows for modeling interactions among the items on the different slots.We characterize the solution in terms of problem parameters and identifyconditions under which an efficient solution is possible. The problemformulation results in a quadratically constrained quadratic program (QCQP). Weprovide an algorithm that gives us an efficient solution by relaxing theconstraints of the QCQP minimally. Through simulated experiments, we show thebenefits of modeling interactions in a multi-slot ranking context, and thespeed and accuracy of our QCQP approximate solver against other state of theart methods.
arxiv-15300-69 | Exploration and Exploitation of Victorian Science in Darwin's Reading Notebooks | http://arxiv.org/pdf/1509.07175v3.pdf | author:Jaimie Murdock, Colin Allen, Simon DeDeo category:cs.CL cs.AI cs.CY cs.DL physics.soc-ph published:2015-09-23 summary:Search in an environment with an uncertain distribution of resources involvesa trade-off between exploitation of past discoveries and further exploration.This extends to information foraging, where a knowledge-seeker shifts betweenreading in depth and studying new domains. We study this process in CharlesDarwin by modeling the full-text of books listed in hischronologically-organized reading journals. We use the information-theoreticKullback-Liebler Divergence, or relative surprise, between books for both hislocal (book-to-book) and global (book-to-past) reading decisions. Rather than apattern of surprise-minimization, corresponding to a pure exploitationstrategy, Darwin's behavior shifts from early exploitation to laterexploration, seeking unusually high levels of cognitive surprise relative toprevious eras. These shifts, detected by an unsupervised Bayesian model,correlate with major intellectual epochs of his career as identified both bytraditional, qualitative scholarship and Darwin's own self-commentary. Inaddition to quantifying Darwin's individual-level foraging, our methods allowus to compare his consumption of texts with their publication order. We findDarwin's consumption more exploratory than the culture's production, suggestingthat underneath gradual societal changes are the explorations of individualsynthesis and discovery.
arxiv-15300-70 | Look, Listen and Learn - A Multimodal LSTM for Speaker Identification | http://arxiv.org/pdf/1602.04364v1.pdf | author:Jimmy Ren, Yongtao Hu, Yu-Wing Tai, Chuan Wang, Li Xu, Wenxiu Sun, Qiong Yan category:cs.LG published:2016-02-13 summary:Speaker identification refers to the task of localizing the face of a personwho has the same identity as the ongoing voice in a video. This task not onlyrequires collective perception over both visual and auditory signals, therobustness to handle severe quality degradations and unconstrained contentvariations are also indispensable. In this paper, we describe a novelmultimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifiesboth visual and auditory modalities from the beginning of each sequence input.The key idea is to extend the conventional LSTM by not only sharing weightsacross time steps, but also sharing weights across modalities. We show thatmodeling the temporal dependency across face and voice can significantlyimprove the robustness to content quality degradations and variations. We alsofound that our multimodal LSTM is robustness to distractors, namely thenon-speaking identities. We applied our multimodal LSTM to The Big Bang Theorydataset and showed that our system outperforms the state-of-the-art systems inspeaker identification with lower false alarm rate and higher recognitionaccuracy.
arxiv-15300-71 | "Owl" and "Lizard": Patterns of Head Pose and Eye Pose in Driver Gaze Classification | http://arxiv.org/pdf/1508.04028v2.pdf | author:Lex Fridman, Joonbum Lee, Bryan Reimer, Trent Victor category:cs.CV cs.HC cs.LG published:2015-08-17 summary:Accurate, robust, inexpensive gaze tracking in the car can help keep a driversafe by facilitating the more effective study of how to improve (1) vehicleinterfaces and (2) the design of future Advanced Driver Assistance Systems. Inthis paper, we estimate head pose and eye pose from monocular video usingmethods developed extensively in prior work and ask two new interestingquestions. First, how much better can we classify driver gaze using head andeye pose versus just using head pose? Second, are there individual-specificgaze strategies that strongly correlate with how much gaze classificationimproves with the addition of eye pose information? We answer these questionsby evaluating data drawn from an on-road study of 40 drivers. The main insightof the paper is conveyed through the analogy of an "owl" and "lizard" whichdescribes the degree to which the eyes and the head move when shifting gaze.When the head moves a lot ("owl"), not much classification improvement isattained by estimating eye pose on top of head pose. On the other hand, whenthe head stays still and only the eyes move ("lizard"), classification accuracyincreases significantly from adding in eye pose. We characterize how thataccuracy varies between people, gaze strategies, and gaze regions.
arxiv-15300-72 | Machine olfaction using time scattering of sensor multiresolution graphs | http://arxiv.org/pdf/1602.04358v1.pdf | author:Leonid Gugel, Yoel Shkolnisky, Shai Dekel category:cs.AI cs.DS stat.ML published:2016-02-13 summary:In this paper we construct a learning architecture for high dimensional timeseries sampled by sensor arrangements. Using a redundant wavelet decompositionon a graph constructed over the sensor locations, our algorithm is able toconstruct discriminative features that exploit the mutual information betweenthe sensors. The algorithm then applies scattering networks to the time seriesgraphs to create the feature space. We demonstrate our method on a machineolfaction problem, where one needs to classify the gas type and the locationwhere it originates from data sampled by an array of sensors. Our experimentalresults clearly demonstrate that our method outperforms classical machinelearning techniques used in previous studies.
arxiv-15300-73 | Character Proposal Network for Robust Text Extraction | http://arxiv.org/pdf/1602.04348v1.pdf | author:Shuye Zhang, Mude Lin, Tianshui Chen, Lianwen Jin, Liang Lin category:cs.CV published:2016-02-13 summary:Maximally stable extremal regions (MSER), which is a popular method togenerate character proposals/candidates, has shown superior performance inscene text detection. However, the pixel-level operation limits its capabilityfor handling some challenging cases (e.g., multiple connected characters,separated parts of one character and non-uniform illumination). To bettertackle these cases, we design a character proposal network (CPN) by takingadvantage of the high capacity and fast computing of fully convolutionalnetwork (FCN). Specifically, the network simultaneously predicts characternessscores and refines the corresponding locations. The characterness scores can beused for proposal ranking to reject non-character proposals and the refiningprocess aims to obtain the more accurate locations. Furthermore, consideringthe situation that different characters have different aspect ratios, wepropose a multi-template strategy, designing a refiner for each aspect ratio.The extensive experiments indicate our method achieves recall rates of 93.88%,93.60% and 96.46% on ICDAR 2013, SVT and Chinese2k datasets respectively usingless than 1000 proposals, demonstrating promising performance of our characterproposal network.
arxiv-15300-74 | Attention-Based Convolutional Neural Network for Machine Comprehension | http://arxiv.org/pdf/1602.04341v1.pdf | author:Wenpeng Yin, Sebastian Ebert, Hinrich Schütze category:cs.CL published:2016-02-13 summary:Understanding open-domain text is one of the primary challenges in naturallanguage processing (NLP). Machine comprehension benchmarks evaluate thesystem's ability to understand text based on the text content only. In thiswork, we investigate machine comprehension on MCTest, a question answering (QA)benchmark. Prior work is mainly based on feature engineering approaches. Wecome up with a neural network framework, named hierarchical attention-basedconvolutional neural network (HABCNN), to address this task without anymanually designed features. Specifically, we explore HABCNN for this task bytwo routes, one is through traditional joint modeling of passage, question andanswer, one is through textual entailment. HABCNN employs an attentionmechanism to detect key phrases, key sentences and key snippets that arerelevant to answering the question. Experiments show that HABCNN outperformsprior deep learning approaches by a big margin.
arxiv-15300-75 | Improving Gibbs Sampling Predictions on Unseen Data for Latent Dirichlet Allocation | http://arxiv.org/pdf/1505.02065v2.pdf | author:Yannis Papanikolaou, Timothy N. Rubin, Grigorios Tsoumakas category:stat.ML published:2015-05-08 summary:Latent Dirichlet Allocation (LDA) is a model for discovering the underlyingstructure of a given data set. LDA and its extensions have been used inunsupervised and supervised learning tasks across a variety of data typesincluding textual, image and biological data. Several methods have beenpresented for approximate inference of LDA parameters, including VariationalBayes (VB), Collapsed Gibbs Sampling (CGS) and Collapsed Variational Bayes(CVB) techniques. This work explores three novel methods for generating LDApredictions on unobserved data, given a model trained by CGS. We presentextensive experiments on real-world data sets for both standard unsupervisedLDA and Prior LDA, one of the supervised variants of LDA for multi-label data.In both supervised and unsupervised settings, we perform extensive empiricalcomparison of our prediction methods with the standard predictions generated byCGS and CVB0 (a variant of CVB). The results show a consistent advantage of oneof our methods over CGS under all experimental conditions, and over CVB0 underthe majority of conditions.
arxiv-15300-76 | Learning Over Long Time Lags | http://arxiv.org/pdf/1602.04335v1.pdf | author:Hojjat Salehinejad category:cs.NE published:2016-02-13 summary:The advantage of recurrent neural networks (RNNs) in learning dependenciesbetween time-series data has distinguished RNNs from other deep learningmodels. Recently, many advances are proposed in this emerging field. However,there is a lack of comprehensive review on memory models in RNNs in theliterature. This paper provides a fundamental review on RNNs and long shortterm memory (LSTM) model. Then, provides a surveys of recent advances indifferent memory enhancements and learning techniques for capturing long termdependencies in RNNs.
arxiv-15300-77 | Using Deep Learning to Predict Demographics from Mobile Phone Metadata | http://arxiv.org/pdf/1511.06660v4.pdf | author:Bjarke Felbo, Pål Sundsøy, Alex 'Sandy' Pentland, Sune Lehmann, Yves-Alexandre de Montjoye category:cs.LG published:2015-11-20 summary:Mobile phone metadata are increasingly used to study human behavior atlarge-scale. There has recently been a growing interest in predictingdemographic information from metadata. Previous approaches relied onhand-engineered features. We here apply, for the first time, deep learningmethods to mobile phone metadata using a convolutional network. Our methodprovides high accuracy on both age and gender prediction. These results showgreat potential for deep learning approaches for prediction tasks usingstandard mobile phone metadata.
arxiv-15300-78 | Combining local regularity estimation and total variation optimization for scale-free texture segmentation | http://arxiv.org/pdf/1504.05776v2.pdf | author:Nelly Pustelnik, Herwig Wendt, Patrice Abry, Nicolas Dobigeon category:cs.CV published:2015-04-22 summary:Texture segmentation constitutes a standard image processing task, crucial tomany applications. The present contribution focuses on the particular subset ofscale-free textures and its originality resides in the combination of three keyingredients: First, texture characterization relies on the concept of localregularity ; Second, estimation of local regularity is based on new multiscalequantities referred to as wavelet leaders ; Third, segmentation from localregularity faces a fundamental bias variance trade-off: In nature, localregularity estimation shows high variability that impairs the detection ofchanges, while a posteriori smoothing of regularity estimates precludes fromlocating correctly changes. Instead, the present contribution proposes severalvariational problem formulations based on total variation and proximalresolutions that effectively circumvent this trade-off. Estimation andsegmentation performance for the proposed procedures are quantified andcompared on synthetic as well as on real-world textures.
arxiv-15300-79 | On the Topology of Projective Shape Spaces | http://arxiv.org/pdf/1602.04330v1.pdf | author:Florian Kelma, John T. Kent, Thomas Hotz category:math.ST cs.CV math.GT stat.TH I.4.1; I.4.7 published:2016-02-13 summary:The projective shape of a configuration consists of the information that isinvariant under projective transformations. It encodes the information about anobject reconstructable from uncalibrated camera views. The space of projectiveshapes of k points in d-dimensional real projective space is by definition thequotient space of k copies of that projective space modulo the action of theprojective linear group. A detailed examination of the topology of projectiveshape space is given, and it is shown how to derive subsets that are maximalHausdorff manifolds. A special case are Tyler regular shapes for which one canconstruct a Riemannian metric.
arxiv-15300-80 | Large scale multi-objective optimization: Theoretical and practical challenges | http://arxiv.org/pdf/1602.03131v2.pdf | author:Kinjal Basu, Ankan Saha, Shaunak Chatterjee category:stat.AP math.OC stat.ML published:2016-02-09 summary:Multi-objective optimization (MOO) is a well-studied problem for severalimportant recommendation problems. While multiple approaches have beenproposed, in this work, we focus on using constrained optimization formulations(e.g., quadratic and linear programs) to formulate and solve MOO problems. Thisapproach can be used to pick desired operating points on the trade-off curvebetween multiple objectives. It also works well for internet applications whichserve large volumes of online traffic, by working with Lagrangian dualityformulation to connect dual solutions (computed offline) with the primalsolutions (computed online). We identify some key limitations of this approach -- namely the inability tohandle user and item level constraints, scalability considerations and varianceof dual estimates introduced by sampling processes. We propose solutions foreach of the problems and demonstrate how through these solutions wesignificantly advance the state-of-the-art in this realm. Our proposed methodscan exactly handle user and item (and other such local) constraints, achieve a$100\times$ scalability boost over existing packages in R and reduce varianceof dual estimates by two orders of magnitude.
arxiv-15300-81 | Parallel and Distributed Block-Coordinate Frank-Wolfe Algorithms | http://arxiv.org/pdf/1409.6086v2.pdf | author:Yu-Xiang Wang, Veeranjaneyulu Sadhanala, Wei Dai, Willie Neiswanger, Suvrit Sra, Eric P. Xing category:stat.ML math.OC published:2014-09-22 summary:We develop parallel and distributed Frank-Wolfe algorithms; the former onshared memory machines with mini-batching, and the latter in a delayed updateframework. Whenever possible, we perform computations asynchronously, whichhelps attain speedups on multicore machines as well as in distributedenvironments. Moreover, instead of worst-case bounded delays, our methods onlydepend (mildly) on \emph{expected} delays, allowing them to be robust tostragglers and faulty worker threads. Our algorithms assume block-separableconstraints, and subsume the recent Block-Coordinate Frank-Wolfe (BCFW)method~\citep{lacoste2013block}. Our analysis reveals problem-dependentquantities that govern the speedups of our methods over BCFW. We presentexperiments on structural SVM and Group Fused Lasso, obtaining significantspeedups over competing state-of-the-art (and synchronous) methods.
arxiv-15300-82 | A Minimax Theory for Adaptive Data Analysis | http://arxiv.org/pdf/1602.04287v1.pdf | author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.LG published:2016-02-13 summary:In adaptive data analysis, the user makes a sequence of queries on the data,where at each step the choice of query may depend on the results in previoussteps. The releases are often randomized in order to reduce overfitting forsuch adaptively chosen queries. In this paper, we propose a minimax frameworkfor adaptive data analysis. Assuming Gaussianity of queries, we establish thefirst sharp minimax lower bound on the squared error in the order of$O(\frac{\sqrt{k}\sigma^2}{n})$, where $k$ is the number of queries asked, and$\sigma^2/n$ is the ordinary signal-to-noise ratio for a single query. Ourlower bound is based on the construction of an approximately least favorableadversary who picks a sequence of queries that are most likely to be affectedby overfitting. This approximately least favorable adversary uses only onelevel of adaptivity, suggesting that the minimax risk for 1-step adaptivitywith k-1 initial releases and that for $k$-step adaptivity are on the sameorder. The key technical component of the lower bound proof is a reduction tofinding the convoluting distribution that optimally obfuscates the sign of aGaussian signal. Our lower bound construction also reveals a transparent andelementary proof of the matching upper bound as an alternative approach toRusso and Zou (2015), who used information-theoretic tools to provide the sameupper bound. We believe that the proposed framework opens up opportunities toobtain theoretical insights for many other settings of adaptive data analysis,which would extend the idea to more practical realms.
arxiv-15300-83 | A Subspace Learning Approach to High-Dimensional Matrix Decomposition with Efficient Information Sampling | http://arxiv.org/pdf/1502.00182v2.pdf | author:Mostafa Rahmani, George Atia category:cs.NA cs.DS cs.LG math.NA stat.ML published:2015-02-01 summary:This paper is concerned with the problem of low-rank plus sparse matrixdecomposition for big data. Conventional algorithms for matrix decompositionuse the entire data to extract the low-rank and sparse components, and arebased on optimization problems that scale with the dimension of the data, whichlimit their scalability. Furthermore, the existing randomized approaches mostlyrely on uniform random sampling, which can be quite inefficient for many realworld data matrices that exhibit additional structures (e.g. clustering). Inthis paper, a scalable subspace-pursuit approach that transforms thedecomposition problem to a subspace learning problem is proposed. Thedecomposition is carried out using a small data sketch formed from sampledcolumns/rows. Even when the data is sampled uniformly at random, it is shownthat the sufficient number of sampled columns/rows is roughly O(r \mu), where\mu is the coherency parameter and r the rank of the low-rank component. Inaddition, efficient sampling algorithms are proposed to address the problem ofcolumn/row sampling from structured data. The proposed sampling algorithms canbe independently used for feature selection from high-dimensional data. Theproposed approach is amenable to online implementation and an online scheme isproposed.
arxiv-15300-84 | Regulating Greed Over Time | http://arxiv.org/pdf/1505.05629v2.pdf | author:Stefano Tracà, Cynthia Rudin category:stat.ML cs.LG published:2015-05-21 summary:In retail, there are predictable yet dramatic time-dependent patterns incustomer behavior, such as periodic changes in the number of visitors, orincreases in visitors just before major holidays (e.g., Christmas). The currentparadigm of multi-armed bandit analysis does not take these known patterns intoaccount, which means that despite the firm theoretical foundation of thesemethods, they are fundamentally flawed when it comes to real applications. Thiswork provides a remedy that takes the time-dependent patterns into account, andwe show how this remedy is implemented in the UCB and {\epsilon}-greedymethods. In the corrected methods, exploitation (greed) is regulated over time,so that more exploitation occurs during higher reward periods, and moreexploration occurs in periods of low reward. In order to understand why regretis reduced with the corrected methods, we present a set of bounds that provideinsight into why we would want to exploit during periods of high reward, anddiscuss the impact on regret. Our proposed methods have excellent performancein experiments, and were inspired by a high-scoring entry in the Explorationand Exploitation 3 contest using data from Yahoo! Front Page. That entryheavily used time-series methods to regulate greed over time, which wassubstantially more effective than other contextual bandit methods.
arxiv-15300-85 | Deep Learning on FPGAs: Past, Present, and Future | http://arxiv.org/pdf/1602.04283v1.pdf | author:Griffin Lacey, Graham W. Taylor, Shawki Areibi category:cs.DC cs.LG stat.ML published:2016-02-13 summary:The rapid growth of data size and accessibility in recent years hasinstigated a shift of philosophy in algorithm design for artificialintelligence. Instead of engineering algorithms by hand, the ability to learncomposable systems automatically from massive amounts of data has led toground-breaking performance in important domains such as computer vision,speech recognition, and natural language processing. The most popular class oftechniques used in these domains is called deep learning, and is seeingsignificant attention from industry. However, these models require incredibleamounts of data and compute power to train, and are limited by the need forbetter hardware acceleration to accommodate scaling beyond current data andmodel sizes. While the current solution has been to use clusters of graphicsprocessing units (GPU) as general purpose processors (GPGPU), the use of fieldprogrammable gate arrays (FPGA) provide an interesting alternative. Currenttrends in design tools for FPGAs have made them more compatible with thehigh-level software practices typically practiced in the deep learningcommunity, making FPGAs more accessible to those who build and deploy models.Since FPGA architectures are flexible, this could also allow researchers theability to explore model-level optimizations beyond what is possible on fixedarchitectures such as GPUs. As well, FPGAs tend to provide high performance perwatt of power consumption, which is of particular importance for applicationscientists interested in large scale server-based deployment orresource-limited embedded applications. This review takes a look at deeplearning and FPGAs from a hardware acceleration perspective, identifying trendsand innovations that make these technologies a natural fit, and motivates adiscussion on how FPGAs may best serve the needs of the deep learning communitymoving forward.
arxiv-15300-86 | Conservative Bandits | http://arxiv.org/pdf/1602.04282v1.pdf | author:Yifan Wu, Roshan Shariff, Tor Lattimore, Csaba Szepesvári category:stat.ML cs.LG published:2016-02-13 summary:We study a novel multi-armed bandit problem that models the challenge facedby a company wishing to explore new strategies to maximize revenue whilstsimultaneously maintaining their revenue above a fixed baseline, uniformly overtime. While previous work addressed the problem under the weaker requirement ofmaintaining the revenue constraint only at a given fixed time in the future,the algorithms previously proposed are unsuitable due to their design under themore stringent constraints. We consider both the stochastic and the adversarialsettings, where we propose, natural, yet novel strategies and analyze the pricefor maintaining the constraints. Amongst other things, we prove both highprobability and expectation bounds on the regret, while we also consider boththe problem of maintaining the constraints with high probability orexpectation. For the adversarial setting the price of maintaining theconstraint appears to be higher, at least for the algorithm considered. A lowerbound is given showing that the algorithm for the stochastic setting is almostoptimal. Empirical results obtained in synthetic environments complement ourtheoretical findings.
arxiv-15300-87 | Signer-independent Fingerspelling Recognition with Deep Neural Network Adaptation | http://arxiv.org/pdf/1602.04278v1.pdf | author:Taehwan Kim, Weiran Wang, Hao Tang, Karen Livescu category:cs.CL cs.CV cs.NE published:2016-02-13 summary:We study the problem of recognition of fingerspelled letter sequences inAmerican Sign Language in a signer-independent setting. Fingerspelled sequencesare both challenging and important to recognize, as they are used for manycontent words such as proper nouns and technical terms. Previous work has shownthat it is possible to achieve almost 90% accuracies on fingerspellingrecognition in a signer-dependent setting. However, the more realisticsigner-independent setting presents challenges due to significant variationsamong signers, coupled with the dearth of available training data. Weinvestigate this problem with approaches inspired by automatic speechrecognition. We start with the best-performing approaches from prior work,based on tandem models and segmental conditional random fields (SCRFs), withfeatures based on deep neural network (DNN) classifiers of letters andphonological features. Using DNN adaptation, we find that it is possible tobridge a large part of the gap between signer-dependent and signer-independentperformance. Using only about 115 transcribed words for adaptation from thetarget signer, we obtain letter accuracies of up to 82.7% with frame-leveladaptation labels and 69.7% with only word labels.
arxiv-15300-88 | Cascaded High Dimensional Histograms: A Generative Approach to Density Estimation | http://arxiv.org/pdf/1510.06779v3.pdf | author:Siong Thye Goh, Cynthia Rudin category:stat.ML 62 published:2015-10-22 summary:We present tree- and list- structured density estimation methods for highdimensional binary/categorical data. Our density estimation models are highdimensional analogies to variable bin width histograms. In each leaf of thetree (or list), the density is constant, similar to the flat density within thebin of a histogram. Histograms, however, cannot easily be visualized in higherdimensions, whereas our models can. The accuracy of histograms fades asdimensions increase, whereas our models have priors that help withgeneralization. Our models are sparse, unlike high-dimensional histograms. Wepresent three generative models, where the first one allows the user to specifythe number of desired leaves in the tree within a Bayesian prior. The secondmodel allows the user to specify the desired number of branches within theprior. The third model returns lists (rather than trees) and allows the user tospecify the desired number of rules and the length of rules within the prior.Our results indicate that the new approaches yield a better balance betweensparsity and accuracy of density estimates than other methods for this task.
arxiv-15300-89 | Evaluation of Protein Structural Models Using Random Forests | http://arxiv.org/pdf/1602.04277v1.pdf | author:Renzhi Cao, Taeho Jo, Jianlin Cheng category:cs.LG q-bio.BM q-bio.QM stat.ML published:2016-02-13 summary:Protein structure prediction has been a grand challenge problem in thestructure biology over the last few decades. Protein quality assessment plays avery important role in protein structure prediction. In the paper, we propose anew protein quality assessment method which can predict both local and globalquality of the protein 3D structural models. Our method uses both multi andsingle model quality assessment method for global quality assessment, and useschemical, physical, geo-metrical features, and global quality score for localquality assessment. CASP9 targets are used to generate the features for localquality assessment. We evaluate the performance of our local quality assessmentmethod on CASP10, which is comparable with two stage-of-art QA methods based onthe average absolute distance between the real and predicted distance. Inaddition, we blindly tested our method on CASP11, and the good performanceshows that combining single and multiple model quality assessment method couldbe a good way to improve the accuracy of model quality assessment, and therandom forest technique could be used to train a good local quality assessmentmodel.
arxiv-15300-90 | k-variates++: more pluses in the k-means++ | http://arxiv.org/pdf/1602.01198v2.pdf | author:Richard Nock, Raphaël Canyasse, Roksana Boreli, Frank Nielsen category:cs.LG H.3.3; I.5.3 published:2016-02-03 summary:k-means++ seeding has become a de facto standard for hard clusteringalgorithms. In this paper, our first contribution is a two-way generalisationof this seeding, k-variates++, that includes the sampling of general densitiesrather than just a discrete set of Dirac densities anchored at the pointlocations, and a generalisation of the well known Arthur-Vassilvitskii (AV)approximation guarantee, in the form of a bias+variance approximation bound ofthe global optimum. This approximation exhibits a reduced dependency on the"noise" component with respect to the optimal potential --- actuallyapproaching the statistical lower bound. We show that k-variates++ reduces toefficient (biased seeding) clustering algorithms tailored to specificframeworks; these include distributed, streaming and on-line clustering, withdirect approximation results for these algorithms. Finally, we present a novelapplication of k-variates++ to differential privacy. For either the specificframeworks considered here, or for the differential privacy setting, there islittle to no prior results on the direct application of k-means++ and itsapproximation bounds --- state of the art contenders appear to be significantlymore complex and / or display less favorable (approximation) properties. Westress that our algorithms can still be run in cases where there is \textit{no}closed form solution for the population minimizer. We demonstrate theapplicability of our analysis via experimental evaluation on several domainsand settings, displaying competitive performances vs state of the art.
arxiv-15300-91 | Learning Games and Rademacher Observations Losses | http://arxiv.org/pdf/1512.05244v2.pdf | author:Richard Nock category:cs.LG I.2.6 published:2015-12-16 summary:It has recently been shown that supervised learning with the popular logisticloss is equivalent to optimizing the exponential loss over sufficientstatistics about the class: Rademacher observations (rados). We first show thatthis unexpected equivalence can actually be generalized to other example / radolosses, with necessary and sufficient conditions for the equivalence,exemplified on four losses that bear popular names in various fields:exponential (boosting), mean-variance (finance), Linear Hinge (on-linelearning), ReLU (deep learning), and unhinged (statistics). Second, we showthat the generalization unveils a surprising new connection to regularizedlearning, and in particular a sufficient condition under which regularizing theloss over examples is equivalent to regularizing the rados (with Minkowskisums) in the equivalent rado loss. This brings simple and powerful rado-basedlearning algorithms for sparsity-controlling regularization, that we exemplifyon a boosting algorithm for the regularized exponential rado-loss, whichformally boosts over four types of regularization, including the popular ridgeand lasso, and the recently coined slope --- we obtain the first provenboosting algorithm for this last regularization. Through our first contributionon the equivalence of rado and example-based losses, Omega-R.AdaBoost~appearsto be an efficient proxy to boost the regularized logistic loss over examplesusing whichever of the four regularizers. Experiments display thatregularization consistently improves performances of rado-based learning, andmay challenge or beat the state of the art of example-based learning even whenlearning over small sets of rados. Finally, we connect regularization todifferential privacy, and display how tiny budgets can be afforded on bigdomains while beating (protected) example-based learning.
arxiv-15300-92 | Regularized Estimation in High Dimensional Time Series under Mixing Conditions | http://arxiv.org/pdf/1602.04265v1.pdf | author:Kam Chung Wong, Ambuj Tewari, Zifan Li category:stat.ML cs.LG published:2016-02-12 summary:The Lasso is one of the most popular methods in high dimensional statisticallearning. Most existing theoretical results for the Lasso, however, require thesamples to be iid. Recent work has provided guarantees for the Lasso assumingthat the time series is generated by a sparse Vector Auto-Regressive (VAR)model with Gaussian innovations. Proofs of these results rely critically on thefact that the true data generating mechanism (DGM) is a finite-order GaussianVAR. This assumption is quite brittle: linear transformations, includingselecting a subset of variables, can lead to the violation of this assumption.In order to break free from such assumptions, we derive nonasymptoticinequalities for estimation error and prediction error of the Lasso estimate ofthe best linear predictor without assuming any special parametric form of theDGM. Instead, we rely only on (strict) stationarity and mixing conditions toestablish consistency of the Lasso in the following two scenarios: (a)alpha-mixing Gaussian processes, and (b) beta-mixing sub-Gaussian randomvectors. Our work provides an alternative proof of the consistency of the Lassofor sparse Gaussian VAR models. But the applicability of our results extends tonon-Gaussian and non-linear times series models as the examples we providedemonstrate. In order to prove our results, we derive a novel Hanson-Wrighttype concentration inequality for beta-mixing sub-Gaussian random vectors thatmay be of independent interest.
arxiv-15300-93 | Motion Deblurring for Plenoptic Images | http://arxiv.org/pdf/1408.3686v2.pdf | author:Paramanand Chandramouli, Paolo Favaro, Daniele Perrone category:cs.CV published:2014-08-16 summary:We address for the first time the issue of motion blur in light field imagescaptured from plenoptic cameras. We propose a solution to the estimation of asharp high resolution scene radiance given a blurry light field image, when themotion blur point spread function is unknown, i.e., the so-called blinddeconvolution problem. In a plenoptic camera, the spatial sampling in each viewis not only decimated but also defocused. Consequently, current blinddeconvolution approaches for traditional cameras are not applicable. Due to thecomplexity of the imaging model, we investigate first the case of uniform(shift-invariant) blur of Lambertian objects, i.e., when objects aresufficiently far away from the camera to be approximately invariant to depthchanges and their reflectance does not vary with the viewing direction. Weintroduce a highly parallelizable model for light field motion blur that iscomputationally and memory efficient. We then adapt a regularized blinddeconvolution approach to our model and demonstrate its performance on bothsynthetic and real light field data. Our method handles practical issues inreal cameras such as radial distortion correction and alignment within anenergy minimization framework.
arxiv-15300-94 | Incremental Method for Spectral Clustering of Increasing Orders | http://arxiv.org/pdf/1512.07349v2.pdf | author:Pin-Yu Chen, Baichuan Zhang, Mohammad Al Hasan, Alfred O. Hero category:cs.SI cs.NA stat.ML published:2015-12-23 summary:The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)of a graph Laplacian matrix have been widely used for spectral clustering andcommunity detection. However, in real-life applications the number of clustersor communities (say, K) is generally unknown a-priori. Consequently, themajority of the existing methods either choose K heuristically or they repeatthe clustering method with different choices of K and accept the bestclustering result. The first option, more often, yield suboptimal result, whilethe second option is computationally expensive. In this work, we propose anincremental method for constructing the eigenspectrum of the graph Laplacianmatrix. This method leverages the eigenstructure of graph Laplacian matrix toobtain the K-th eigenpairs of the Laplacian matrix given a collection of allthe K-1 smallest eigenpairs. Our proposed method adapts the Laplacian matrixsuch that the batch eigenvalue decomposition problem transforms into anefficient sequential leading eigenpair computation problem. As a practicalapplication, we consider user-guided spectral clustering. Specifically, wedemonstrate that users can utilize the proposed incremental method foreffective eigenpair computation and determining the desired number of clustersbased on multiple clustering metrics.
arxiv-15300-95 | Scale-free network optimization: foundations and algorithms | http://arxiv.org/pdf/1602.04227v1.pdf | author:Patrick Rebeschini, Sekhar Tatikonda category:stat.ML math.OC published:2016-02-12 summary:We investigate the fundamental principles that drive the development ofscalable algorithms for network optimization. Despite the significant amount ofwork on parallel and decentralized algorithms in the optimization community,the methods that have been proposed typically rely on strict separabilityassumptions for objective function and constraints. Beside sparsity, thesemethods typically do not exploit the strength of the interaction betweenvariables in the system. We propose a notion of correlation in constrainedoptimization that is based on the sensitivity of the optimal solution uponperturbations of the constraints. We develop a general theory of sensitivity ofoptimizers the extends beyond the infinitesimal setting. We present instancesin network optimization where the correlation decays exponentially fast withrespect to the natural distance in the network, and we design algorithms thatcan exploit this decay to yield dimension-free optimization. Our results arethe first of their kind, and open new possibilities in the theory of localalgorithms.
arxiv-15300-96 | A Multiphase Image Segmentation Based on Fuzzy Membership Functions and L1-norm Fidelity | http://arxiv.org/pdf/1504.02206v2.pdf | author:Fang Li, Stanley Osher, Jing Qin, Ming Yan category:math.OC cs.CV published:2015-04-09 summary:In this paper, we propose a variational multiphase image segmentation modelbased on fuzzy membership functions and L1-norm fidelity. Then we apply thealternating direction method of multipliers to solve an equivalent problem. Allthe subproblems can be solved efficiently. Specifically, we propose a fastmethod to calculate the fuzzy median. Experimental results and comparisons showthat the L1-norm based method is more robust to outliers such as impulse noiseand keeps better contrast than its L2-norm counterpart. Theoretically, we provethe existence of the minimizer and analyze the convergence of the algorithm.
arxiv-15300-97 | Pursuits in Structured Non-Convex Matrix Factorizations | http://arxiv.org/pdf/1602.04208v1.pdf | author:Rajiv Khanna, Michael Tschannen, Martin Jaggi category:cs.LG stat.ML published:2016-02-12 summary:Efficiently representing real world data in a succinct and parsimoniousmanner is of central importance in many fields. We present a generalized greedypursuit framework, allowing us to efficiently solve structured matrixfactorization problems, where the factors are allowed to be from arbitrary setsof structured vectors. Such structure may include sparsity, non-negativeness,order, or a combination thereof. The algorithm approximates a given matrix by alinear combination of few rank-1 matrices, each factorized into an outerproduct of two vector atoms of the desired structure. For the non-convexsubproblems of obtaining good rank-1 structured matrix atoms, we employ andanalyze a general atomic power method. In addition to the above applications,we prove linear convergence for generalized pursuit variants in Hilbert spaces- for the task of approximation over the linear span of arbitrary dictionaries- which generalizes OMP and is useful beyond matrix problems. Our experimentson real datasets confirm both the efficiency and also the broad applicabilityof our framework in practice.
arxiv-15300-98 | Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies | http://arxiv.org/pdf/1602.04709v1.pdf | author:Giancarlo Crocetti, Amir A. Delay, Fatemeh Seyedmendhi category:cs.IR cs.AI cs.CL H.3.1; H.3.3 published:2016-02-12 summary:The exploration of social conversations for addressing patient's needs is animportant analytical task in which many scholarly publications are contributingto fill the knowledge gap in this area. The main difficulty remains theinability to turn such contributions into pragmatic processes thepharmaceutical industry can leverage in order to generate insight from socialmedia data, which can be considered as one of the most challenging source ofinformation available today due to its sheer volume and noise. This study isbased on the work by Scott Spangler and Jeffrey Kreulen and applies it toidentify structure in social media through the extraction of a topical taxonomyable to capture the latent knowledge in social conversations in health-relatedsites. The mechanism for automatically identifying and generating a taxonomyfrom social conversations is developed and pressured tested using public datafrom media sites focused on the needs of cancer patients and their families.Moreover, a novel method for generating the category's label and thedetermination of an optimal number of categories is presented which extendsScott and Jeffrey's research in a meaningful way. We assume the reader isfamiliar with taxonomies, what they are and how they are used.
arxiv-15300-99 | An Evolutionary Strategy based on Partial Imitation for Solving Optimization Problems | http://arxiv.org/pdf/1602.04186v1.pdf | author:Marco Alberto Javarone category:cs.NE math.OC published:2016-02-12 summary:In this work we introduce an evolutionary strategy to solve optimizationtasks. In particular, we focus on the Travel Salesman Problem (TSP), i.e., aNP-hard problem with a discrete search space. The solutions of the TSP can becodified by arrays of cities, and can be evaluated by a fitness computedaccording to a cost function (e.g., the length of a path). Our method is basedon the evolution of an agent population by means of a `partial imitation'mechanism. In particular, agents receive a random solution and then,interacting among themselves, imitate the solutions of agents with a higherfitness. Moreover, as stated above, the imitation is only partial, i.e., agentscopy only one, randomly chosen, entry of better (array) solutions. In doing so,the population converges towards a shared solution, behaving like a spin systemundergoing a cooling process, i.e., driven towards an ordered phase. Wehighlight that the adopted `partial imitation' mechanism allows the populationto generate new solutions over time, before reaching the final equilibrium.Remarkably, results of numerical simulations show that our method is able tofind the optimal solution in all considered search spaces.
arxiv-15300-100 | Causal Strength via Shannon Capacity: Axioms, Estimators and Applications | http://arxiv.org/pdf/1602.03476v2.pdf | author:Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath category:cs.IT cs.LG math.IT stat.ML published:2016-02-10 summary:We conduct an axiomatic study of the problem of estimating the strength of aknown causal relationship between a pair of variables. We propose that anestimate of causal strength should be based on the conditional distribution ofthe effect given the cause (and not on the driving distribution of the cause),and study dependence measures on conditional distributions. Shannon capacity,appropriately regularized, emerges as a natural measure under these axioms. Weexamine the problem of calculating Shannon capacity from the observed samplesand propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate itsconsistency. Finally, we demonstrate an application to single-cellflow-cytometry, where the proposed estimators significantly reduce samplecomplexity.
arxiv-15300-101 | Generalized Conjugate Gradient Methods for $\ell_1$ Regularized Convex Quadratic Programming with Finite Convergence | http://arxiv.org/pdf/1511.07837v3.pdf | author:Zhaosong Lu, Xiaojun Chen category:math.OC cs.LG math.NA stat.CO stat.ML published:2015-11-24 summary:The conjugate gradient (CG) method is an efficient iterative method forsolving large-scale strongly convex quadratic programming (QP). In this paperwe propose some generalized CG (GCG) methods for solving the$\ell_1$-regularized (possibly not strongly) convex QP that terminate at anoptimal solution in a finite number of iterations. At each iteration, ourmethods first identify a face of an orthant and then either perform an exactline search along the direction of the negative projected minimum-normsubgradient of the objective function or execute a CG subroutine that conductsa sequence of CG iterations until a CG iterate crosses the boundary of thisface or an approximate minimizer of over this face or a subface is found. Wedetermine which type of step should be taken by comparing the magnitude of somecomponents of the minimum-norm subgradient of the objective function to that ofits rest components. Our analysis on finite convergence of these methods makesuse of an error bound result and some key properties of the aforementionedexact line search and the CG subroutine. We also show that the proposed methodsare capable of finding an approximate solution of the problem by allowing someinexactness on the execution of the CG subroutine. The overall arithmeticoperation cost of our GCG methods for finding an $\epsilon$-optimal solutiondepends on $\epsilon$ in $O(\log(1/\epsilon))$, which is superior to theaccelerated proximal gradient method [2,23] that depends on $\epsilon$ in$O(1/\sqrt{\epsilon})$. In addition, our GCG methods can be extendedstraightforwardly to solve box-constrained convex QP with finite convergence.Numerical results demonstrate that our methods are very favorable for solvingill-conditioned problems.
arxiv-15300-102 | Recovering metric from full ordinal information | http://arxiv.org/pdf/1506.03762v3.pdf | author:Thibaut Le Gouic category:stat.ML math.ST stat.TH published:2015-06-11 summary:Given a geodesic space (E, d), we show that full ordinal knowledge on themetric d-i.e. knowledge of the function D d : (w, x, y, z) $\rightarrow$ 1d(w,x)$\le$d(y,z) , determines uniquely-up to a constant factor-the metric d.For a subspace En of n points of E, converging in Hausdorff distance to E, weconstruct a metric dn on En, based only on the knowledge of D d on En andestablish a sharp upper bound of the Gromov-Hausdorff distance between (En, dn)and (E, d).
arxiv-15300-103 | Learning Communities in the Presence of Errors | http://arxiv.org/pdf/1511.03229v2.pdf | author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG math.ST stat.TH published:2015-11-10 summary:We study the problem of learning communities in the presence of modelingerrors and give robust recovery algorithms for the Stochastic Block Model(SBM). This model, which is also known as the Planted Partition Model, iswidely used for community detection and graph partitioning in various fields,including machine learning, statistics, and social sciences. Many algorithmsexist for learning communities in the Stochastic Block Model, but they do notwork well in the presence of errors. In this paper, we initiate the study of robust algorithms for partialrecovery in SBM with modeling errors or noise. We consider graphs generatedaccording to the Stochastic Block Model and then modified by an adversary. Weallow two types of adversarial errors, Feige---Kilian or monotone errors, andedge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open questionabout whether an almost exact recovery is possible when the adversary isallowed to add $o(n)$ edges. Our work answers this question affirmatively evenin the case of $k>2$ communities. We then show that our algorithms work not only when the instances come fromSBM, but also work when the instances come from any distribution of graphs thatis $\epsilon m$ close to SBM in the Kullback---Leibler divergence. This resultalso works in the presence of adversarial errors. Finally, we present almosttight lower bounds for two communities.
arxiv-15300-104 | Deep Gaussian Processes for Regression using Approximate Expectation Propagation | http://arxiv.org/pdf/1602.04133v1.pdf | author:Thang D. Bui, Daniel Hernández-Lobato, Yingzhen Li, José Miguel Hernández-Lobato, Richard E. Turner category:stat.ML cs.LG published:2016-02-12 summary:Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisationsof Gaussian processes (GPs) and are formally equivalent to neural networks withmultiple, infinitely wide hidden layers. DGPs are nonparametric probabilisticmodels and as such are arguably more flexible, have a greater capacity togeneralise, and provide better calibrated uncertainty estimates thanalternative deep models. This paper develops a new approximate Bayesianlearning scheme that enables DGPs to be applied to a range of medium to largescale regression problems for the first time. The new method uses anapproximate Expectation Propagation procedure and a novel and efficientextension of the probabilistic backpropagation algorithm for learning. Weevaluate the new method for non-linear regression on eleven real-worlddatasets, showing that it always outperforms GP regression and is almost alwaysbetter than state-of-the-art deterministic and sampling-based approximateinference methods for Bayesian neural networks. As a by-product, this workprovides a comprehensive analysis of six approximate Bayesian methods fortraining neural networks.
arxiv-15300-105 | Learning may need only a few bits of synaptic precision | http://arxiv.org/pdf/1602.04129v1.pdf | author:Carlo Baldassi, Federica Gerace, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:q-bio.NC stat.ML published:2016-02-12 summary:Learning in neural networks poses peculiar challenges when using discretizedrather then continuous synaptic states. The choice of discrete synapses ismotivated by biological reasoning and experiments, and possibly by hardwareimplementation considerations as well. In this paper we extend a previous largedeviations analysis which unveiled the existence of peculiar dense regions inthe space of synaptic states which accounts for the possibility of learningefficiently in networks with binary synapses. We extend the analysis tosynapses with multiple states and generally more plausible biological features.The results clearly indicate that the overall qualitative picture is unchangedwith respect to the binary case, and very robust to variation of the details ofthe model. We also provide quantitative results which suggest that using fewsynaptic states is convenient for practical applications, consistently withrecent biological results.
arxiv-15300-106 | From Coin Betting to Parameter-Free Online Learning | http://arxiv.org/pdf/1602.04128v1.pdf | author:Francesco Orabona, Dávid Pál category:cs.LG published:2016-02-12 summary:In the recent years a number of parameter-free algorithms for online linearoptimization over Hilbert spaces and for learning with expert advice have beendeveloped. While these two families of algorithms might seem different to adistract eye, the proof methods are indeed very similar, making the readerwonder if such a connection is only accidental. In this paper, we unify these two families, showing that both can beinstantiated from online coin betting algorithms. We present two new reductionsfrom online coin betting to online linear optimization over Hilbert spaces andto learning with expert advice. We instantiate our framework using a bettingalgorithm based on the Krichevsky-Trofimov estimator. We obtain a simplealgorithm for online linear optimization over any Hilbert space with$O(\norm{u}\sqrt{T \log(1+T \norm{u}}))$ regret with respect to any competitor$u$. For learning with expert advice we obtain an algorithm that has $O(\sqrt{T(1 + \KL{u}{\pi})})$ regret against any competitor $u$ and where $\KL{u}{\pi}$is the Kullback-Leibler divergence between algorithm's prior distribution $\pi$and the competitor. In both cases, no parameters need to be tuned.
arxiv-15300-107 | Fast and Robust Hand Tracking Using Detection-Guided Optimization | http://arxiv.org/pdf/1602.04124v1.pdf | author:Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, Christian Theobalt category:cs.CV published:2016-02-12 summary:Markerless tracking of hands and fingers is a promising enabler forhuman-computer interaction. However, adoption has been limited because oftracking inaccuracies, incomplete coverage of motions, low framerate, complexcamera setups, and high computational requirements. In this paper, we present afast method for accurately tracking rapid and complex articulations of the handusing a single depth camera. Our algorithm uses a novel detection-guidedoptimization strategy that increases the robustness and speed of poseestimation. In the detection step, a randomized decision forest classifiespixels into parts of the hand. In the optimization step, a novel objectivefunction combines the detected part labels and a Gaussian mixturerepresentation of the depth to estimate a pose that best fits the depth. Ourapproach needs comparably less computational resources which makes it extremelyfast (50 fps without GPU support). The approach also supports varying static,or moving, camera-to-scene arrangements. We show the benefits of our method byevaluating on public datasets and comparing against previous work.
arxiv-15300-108 | An Empirical Study on Academic Commentary and Its Implications on Reading and Writing | http://arxiv.org/pdf/1602.04101v1.pdf | author:Tai Wang, Xiangen Hu, Keith Shubeck, Zhiqiang Cai, Jie Tang category:cs.CY cs.CL published:2016-02-12 summary:The relationship between reading and writing (RRW) is one of the major themesin learning science. One of its obstacles is that it is difficult to define ormeasure the latent background knowledge of the individual. However, in anacademic research setting, scholars are required to explicitly list theirbackground knowledge in the citation sections of their manuscripts. This uniqueopportunity was taken advantage of to observe RRW, especially in the publishedacademic commentary scenario. RRW was visualized under a proposed topic processmodel by using a state of the art version of latent Dirichlet allocation (LDA).The empirical study showed that the academic commentary is modulated both byits target paper and the author's background knowledge. Although thisconclusion was obtained in a unique environment, we suggest its implicationscan also shed light on other similar interesting areas, such as dialog andconversation, group discussion, and social media.
arxiv-15300-109 | Using Deep Q-Learning to Control Optimization Hyperparameters | http://arxiv.org/pdf/1602.04062v1.pdf | author:Samantha Hansen category:math.OC cs.LG published:2016-02-12 summary:We present a novel definition of the reinforcement learning state, actionsand reward function that allows a deep Q-network (DQN) to learn to control anoptimization hyperparameter. Using Q-learning with experience replay, we traintwo DQNs to accept a state representation of an objective function as input andoutput the expected discounted return of rewards, or q-values, connected to theactions of either adjusting the learning rate or leaving it unchanged. The twoDQNs learn a policy similar to a line search, but differ in the number ofallowed actions. The trained DQNs in combination with a gradient-based updateroutine form the basis of the Q-gradient descent algorithms. To demonstrate theviability of this framework, we show that the DQN's q-values associated withoptimal action converge and that the Q-gradient descent algorithms outperformgradient descent with an Armijo or nonomonotone line search. Unlike traditionaloptimization methods, Q-gradient descent can incorporate any objectivestatistic and by varying the actions we gain insight into the type of learningrate adjustment strategies that are successful for neural network optimization.
arxiv-15300-110 | Image Restoration and Reconstruction using Variable Splitting and Class-adapted Image Priors | http://arxiv.org/pdf/1602.04052v1.pdf | author:Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo category:cs.CV I.4.5; I.4.4 published:2016-02-12 summary:This paper proposes using a Gaussian mixture model as a prior, for solvingtwo image inverse problems, namely image deblurring and compressive imaging. Wecapitalize on the fact that variable splitting algorithms, like ADMM, are ableto decouple the handling of the observation operator from that of theregularizer, and plug a state-of-the-art algorithm into the pure denoisingstep. Furthermore, we show that, when applied to a specific type of image, aGaussian mixture model trained from an database of images of the same type isable to outperform current state-of-the-art methods.
arxiv-15300-111 | DECOrrelated feature space partitioning for distributed sparse regression | http://arxiv.org/pdf/1602.02575v2.pdf | author:Xiangyu Wang, David Dunson, Chenlei Leng category:stat.ME cs.DC stat.CO stat.ML published:2016-02-08 summary:Fitting statistical models is computationally challenging when the samplesize or the dimension of the dataset is huge. An attractive approach fordown-scaling the problem size is to first partition the dataset into subsetsand then fit using distributed algorithms. The dataset can be partitionedeither horizontally (in the sample space) or vertically (in the feature space).While the majority of the literature focuses on sample space partitioning,feature space partitioning is more effective when $p\gg n$. Existing methodsfor partitioning features, however, are either vulnerable to high correlationsor inefficient in reducing the model dimension. In this paper, we solve theseproblems through a new embarrassingly parallel framework named DECO fordistributed variable selection and parameter estimation. In DECO, variables arefirst partitioned and allocated to $m$ distributed workers. The decorrelatedsubset data within each worker are then fitted via any algorithm designed forhigh-dimensional problems. We show that by incorporating the decorrelationstep, DECO can achieve consistent variable selection and parameter estimationon each subset with (almost) no assumptions. In addition, the convergence rateis nearly minimax optimal for both sparse and weakly sparse models and does NOTdepend on the partition number $m$. Extensive numerical experiments areprovided to illustrate the performance of the new framework.
arxiv-15300-112 | Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View | http://arxiv.org/pdf/1602.03670v2.pdf | author:Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos E. Themelis, Konstantinos D. Koutroumbas category:stat.ML published:2016-02-11 summary:Extracting the underlying low-dimensional space where high-dimensionalsignals often reside has long been at the center of numerous algorithms in thesignal processing and machine learning literature during the past few decades.At the same time, working with incomplete (partly observed) large scaledatasets has recently been commonplace for diverse reasons. This so called {\itbig data era} we are currently living calls for devising online subspacelearning algorithms that can suitably handle incomplete data. Their envisagedobjective is to {\it recursively} estimate the unknown subspace by processingstreaming data sequentially, thus reducing computational complexity, whileobviating the need for storing the whole dataset in memory. In this paper, anonline variational Bayes subspace learning algorithm from partial observationsis presented. To account for the unawareness of the true rank of the subspace,commonly met in practice, low-rankness is explicitly imposed on the soughtsubspace data matrix by exploiting sparse Bayesian learning principles.Moreover, sparsity, {\it simultaneously} to low-rankness, is favored on thesubspace matrix by the sophisticated hierarchical Bayesian scheme that isadopted. In doing so, the proposed algorithm becomes adept in dealing withapplications whereby the underlying subspace may be also sparse, as, e.g., insparse dictionary learning problems. As shown, the new subspace tracking schemeoutperforms its state-of-the-art counterparts in terms of estimation accuracy,in a variety of experiments conducted on simulated and real data.
arxiv-15300-113 | An automatic method for segmentation of fission tracks in epidote crystal photomicrographs | http://arxiv.org/pdf/1602.03995v1.pdf | author:Alexandre Fioravante de Siqueira, Wagner Massayuki Nakasuga, Aylton Pagamisse, Carlos Alberto Tello Saenz, Aldo Eloizo Job category:cs.CV 65T60 published:2016-02-12 summary:Manual identification of fission tracks has practical problems, such asvariation due to observer-observation efficiency. An automatic processingmethod that could identify fission tracks in a photomicrograph could solve thisproblem and improve the speed of track counting. However, separation ofnon-trivial images is one of the most difficult tasks in image processing.Several commercial and free softwares are available, but these softwares aremeant to be used in specific images. In this paper, an automatic method basedon starlet wavelets is presented in order to separate fission tracks in mineralphotomicrographs. Automatization is obtained by Matthews correlationcoefficient, and results are evaluated by precision, recall and accuracy. Thistechnique is an improvement of a method aimed at segmentation of scanningelectron microscopy images. This method is applied in photomicrographs ofepidote phenocrystals, in which accuracy higher than 89% was obtained infission track segmentation, even for difficult images. Algorithms correspondingto the proposed method are available for download. Using the method presentedhere, an user could easily determine fission tracks in photomicrographs ofmineral samples.
arxiv-15300-114 | Stereo Matching by Joint Energy Minimization | http://arxiv.org/pdf/1601.03890v3.pdf | author:Hongyang Xue, Deng Cai category:cs.CV published:2016-01-15 summary:In [18], Mozerov et al. propose to perform stereo matching as a two-stepenergy minimization problem. For the first step they solve a fully connectedMRF model. And in the next step the marginal output is employed as the unarycost for a locally connected MRF model. In this paper we intend to combine the two steps of energy minimization inorder to improve stereo matching results. We observe that the fully connectedMRF leads to smoother disparity maps, while the locally connected MRF achievessuperior results in fine-structured regions. Thus we propose to jointly solvethe fully connected and locally connected models, taking both their advantagesinto account. The joint model is solved by mean field approximations. Whileremaining efficient, our joint model outperforms the two-step energyminimization approach in both time and estimation error on the Middleburystereo benchmark v3.
arxiv-15300-115 | Orthogonal Sparse PCA and Covariance Estimation via Procrustes Reformulation | http://arxiv.org/pdf/1602.03992v1.pdf | author:Konstantinos Benidis, Ying Sun, Prabhu Babu, Daniel P. Palomar category:stat.ML cs.LG math.OC stat.AP published:2016-02-12 summary:The problem of estimating sparse eigenvectors of a symmetric matrix attractsa lot of attention in many applications, especially those with high dimensionaldata set. While classical eigenvectors can be obtained as the solution of amaximization problem, existing approaches formulate this problem by adding apenalty term into the objective function that encourages a sparse solution.However, the resulting methods achieve sparsity at the expense of sacrificingthe orthogonality property. In this paper, we develop a new method to estimatedominant sparse eigenvectors without trading off their orthogonality. Theproblem is highly non-convex and hard to handle. We apply the MM frameworkwhere we iteratively maximize a tight lower bound (surrogate function) of theobjective function over the Stiefel manifold. The inner maximization problemturns out to be a rectangular Procrustes problem, which has a closed formsolution. In addition, we propose a method to improve the covariance estimationproblem when its underlying eigenvectors are known to be sparse. We use theeigenvalue decomposition of the covariance matrix to formulate an optimizationproblem where we impose sparsity on the corresponding eigenvectors. Numericalexperiments show that the proposed eigenvector extraction algorithm matches oroutperforms existing algorithms in terms of support recovery and explainedvariance, while the covariance estimation algorithms improve significantly thesample covariance estimator.
arxiv-15300-116 | Multidimensional Scaling in the Poincare Disk | http://arxiv.org/pdf/1105.5332v3.pdf | author:Andrej Cvetkovski, Mark Crovella category:stat.ML cs.SI published:2011-05-26 summary:Multidimensional scaling (MDS) is a class of projective algorithmstraditionally used in Euclidean space to produce two- or three-dimensionalvisualizations of datasets of multidimensional points or point distances. Morerecently however, several authors have pointed out that for certain datasets,hyperbolic target space may provide a better fit than Euclidean space. In this paper we develop PD-MDS, a metric MDS algorithm designed specificallyfor the Poincare disk (PD) model of the hyperbolic plane. Emphasizing theimportance of proceeding from first principles in spite of the availability ofvarious black box optimizers, our construction is based on an elementaryhyperbolic line search and reveals numerous particulars that need to becarefully addressed when implementing this as well as more sophisticatediterative optimization methods in a hyperbolic space model.
arxiv-15300-117 | TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions | http://arxiv.org/pdf/1602.03960v1.pdf | author:Sujay Kumar Jauhar, Peter Turney, Eduard Hovy category:cs.CL published:2016-02-12 summary:We describe two new related resources that facilitate modelling of generalknowledge reasoning in 4th grade science exams. The first is a collection ofcurated facts in the form of tables, and the second is a large set ofcrowd-sourced multiple-choice questions covering the facts in the tables.Through the setup of the crowd-sourced annotation task we obtain implicitalignment information between questions and tables. We envisage that theresources will be useful not only to researchers working on question answering,but also to people investigating a diverse range of other applications such asinformation extraction, question parsing, answer type identification, andlexical semantic modelling.
arxiv-15300-118 | The Automatic Statistician: A Relational Perspective | http://arxiv.org/pdf/1511.08343v2.pdf | author:Yunseong Hwang, Anh Tong, Jaesik Choi category:cs.LG stat.ML published:2015-11-26 summary:Gaussian Processes (GPs) provide a general and analytically tractable way ofmodeling complex time-varying, nonparametric functions. The Automatic BayesianCovariance Discovery (ABCD) system constructs natural-language description oftime-series data by treating unknown time-series data nonparametrically usingGP with a composite covariance kernel function. Unfortunately, learning acomposite covariance kernel with a single time-series data set often results inless informative kernel that may not give qualitative, distinctive descriptionsof data. We address this challenge by proposing two relational kernel learningmethods which can model multiple time-series data sets by finding common,shared causes of changes. We show that the relational kernel learning methodsfind more accurate models for regression problems on several real-world datasets; US stock data, US house price index data and currency exchange rate data.
arxiv-15300-119 | General Vector Machine | http://arxiv.org/pdf/1602.03950v1.pdf | author:Hong Zhao category:stat.ML cs.LG published:2016-02-12 summary:The support vector machine (SVM) is an important class of learning machinesfor function approach, pattern recognition, and time-serious prediction, etc.It maps samples into the feature space by so-called support vectors of selectedsamples, and then feature vectors are separated by maximum margin hyperplane.The present paper presents the general vector machine (GVM) to replace the SVM.The support vectors are replaced by general project vectors selected from theusual vector space, and a Monte Carlo (MC) algorithm is developed to find thegeneral vectors. The general project vectors improves the feature-extractionability, and the MC algorithm can control the width of the separation margin ofthe hyperplane. By controlling the separation margin, we show that the maximummargin hyperplane can usually induce the overlearning, and the best learningmachine is achieved with a proper separation margin. Applications in functionapproach, pattern recognition, and classification indicate that the developedmethod is very successful, particularly for small-set training problems.Additionally, our algorithm may induce some particular applications, such asfor the transductive inference.
arxiv-15300-120 | Face Attribute Prediction Using Off-The-Shelf Deep Learning Networks | http://arxiv.org/pdf/1602.03935v1.pdf | author:Yang Zhong, Josephine Sullivan, Haibo Li category:cs.CV published:2016-02-12 summary:Attribute prediction from face images in the wild is a challenging problem.To automatically describe face attributes from face containing images,traditionally one needs to cascade three technical blocks --- facelocalization, facial feature extraction, and classification --- in a pipeline.As a typical classification problem, face attribute prediction has beenaddressed by using deep learning networks. Current state-of-the-art performancewas achieved by using two cascaded CNNs, which were specifically trained tolearn face localization and facial attribute prediction. In this paper weexperiment in an alternative way of exploring the power of deep representationsfrom the networks: we employ off-the-shelf CNNs trained for face recognitiontasks to do facial feature extraction, combined with conventional facelocalization techniques. Recognizing that the describable face attributes arediverse, we select representations from different levels of the CNNs andinvestigate their utilities for attribute classification. Experiments on twolarge datasets, LFWA and CeleA, show that the performance is totally comparableto state-of-the-art approach. Our findings suggest two potentially importantquestions in using CNNs for face attribute prediction: 1) How to maximallyleverage the power of CNN representations. 2) How to best combine traditionalcomputer vision techniques with deep learning networks.
arxiv-15300-121 | Global Deconvolutional Networks for Semantic Segmentation | http://arxiv.org/pdf/1602.03930v1.pdf | author:Vladimir Nekrasov, Janghoon Ju, Jaesik Choi category:cs.CV published:2016-02-12 summary:Semantic image segmentation is an important low-level computer vision problemaimed to correctly classify each individual pixel of the image. Recentempirical improvements achieved in this area have primarily been motivated bysuccessful exploitation of Convolutional Neural Networks (CNNs) pre-trained forimage classification and object recognition tasks. However, the pixel-wiselabeling with CNNs has its own unique challenges: (1) an accuratedeconvolution, or upsampling, of low-resolution output into a higher-resolutionsegmentation mask and (2) an inclusion of global information, or context,within locally extracted features. To address these issues, we propose a novelarchitecture to conduct the deconvolution operation and acquire densepredictions, and an additional refinement, which allows to incorporate globalinformation into the network. We demonstrate that these alterations lead toimproved performance of state-of-the-art semantic segmentation models on thePASCAL VOC 2012 benchmark.
arxiv-15300-122 | Exploring the Limits of Language Modeling | http://arxiv.org/pdf/1602.02410v2.pdf | author:Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu category:cs.CL published:2016-02-07 summary:In this work we explore recent advances in Recurrent Neural Networks forlarge scale Language Modeling, a task central to language understanding. Weextend current models to deal with two key challenges present in this task:corpora and vocabulary sizes, and complex, long term structure of language. Weperform an exhaustive study on techniques such as character ConvolutionalNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.Our best single model significantly improves state-of-the-art perplexity from51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),while an ensemble of models sets a new record by improving perplexity from 41.0down to 23.7. We also release these models for the NLP and ML community tostudy and improve upon.
arxiv-15300-123 | Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model | http://arxiv.org/pdf/1602.03860v1.pdf | author:Srinath Sridhar, Helge Rhodin, Hans-Peter Seidel, Antti Oulasvirta, Christian Theobalt category:cs.CV published:2016-02-11 summary:Real-time marker-less hand tracking is of increasing importance inhuman-computer interaction. Robust and accurate tracking of arbitrary handmotion is a challenging problem due to the many degrees of freedom, frequentself-occlusions, fast motions, and uniform skin color. In this paper, wepropose a new approach that tracks the full skeleton motion of the hand frommultiple RGB cameras in real-time. The main contributions include a newgenerative tracking method which employs an implicit hand shape representationbased on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that issmooth and analytically differentiable making fast gradient based poseoptimization possible. This shape representation, together with a fullperspective projection model, enables more accurate hand modeling than arelated baseline method from literature. Our method achieves better accuracythan previous methods and runs at 25 fps. We show these improvements bothqualitatively and quantitatively on publicly available datasets.
arxiv-15300-124 | Validity of time reversal for testing Granger causality | http://arxiv.org/pdf/1509.07636v2.pdf | author:Irene Winkler, Danny Panknin, Daniel Bartz, Klaus-Robert Müller, Stefan Haufe category:math.ST stat.ML stat.TH published:2015-09-25 summary:Inferring causal interactions from observed data is a challenging problem,especially in the presence of measurement noise. To alleviate the problem ofspurious causality, Haufe et al. (2013) proposed to contrast measures ofinformation flow obtained on the original data against the same measuresobtained on time-reversed data. They show that this procedure, time-reversedGranger causality (TRGC), robustly rejects causal interpretations on mixturesof independent signals. While promising results have been achieved insimulations, it was so far unknown whether time reversal leads to validmeasures of information flow in the presence of true interaction. Here we provethat, for linear finite-order autoregressive processes with unidirectionalinformation flow, the application of time reversal for testing Grangercausality indeed leads to correct estimates of information flow and itsdirectionality. Using simulations, we further show that TRGC is able to infercorrect directionality with similar statistical power as the net Grangercausality between two variables, while being much more robust to the presenceof measurement noise.
arxiv-15300-125 | A Theoretically Grounded Application of Dropout in Recurrent Neural Networks | http://arxiv.org/pdf/1512.05287v2.pdf | author:Yarin Gal category:stat.ML published:2015-12-16 summary:Recurrent neural networks (RNNs) stand at the forefront of many recentdevelopments in deep learning. Yet a major difficulty with these models istheir tendency to overfit. Dropout is a widely used tool for regularisation indeep models, but a long strand of empirical research has claimed that it cannotbe applied between the recurrent connections of an RNN. The argument is thatnoise hinders the network's ability to model sequences and therefore dropoutshould be applied to the RNN's inputs and outputs alone. But withoutregularisation in recurrent layers, existing techniques overfit quickly. Inthis paper we make use of a recently developed theoretical framework castingdropout as approximate variational inference. Based on the framework we derivemathematically grounded tools to apply dropout within the recurrent layers ofRNNs, eliminating model overfitting. We apply our new variational inferencebased dropout technique in LSTM and GRU networks, evaluating the techniqueempirically. We show that the new approach outperforms existing techniques onsentiment analysis and language modelling tasks, extending our arsenal ofvariational tools in deep learning.
arxiv-15300-126 | Semi-supervised Learning with Explicit Relationship Regularization | http://arxiv.org/pdf/1602.03808v1.pdf | author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV cs.LG published:2016-02-11 summary:In many learning tasks, the structure of the target space of a function holdsrich information about the relationships between evaluations of functions ondifferent data points. Existing approaches attempt to exploit this relationshipinformation implicitly by enforcing smoothness on function evaluations only.However, what happens if we explicitly regularize the relationships betweenfunction evaluations? Inspired by homophily, we regularize based on a smoothrelationship function, either defined from the data or with labels. Inexperiments, we demonstrate that this significantly improves the performance ofstate-of-the-art algorithms in semi-supervised classification and in spectraldata embedding for constrained clustering and dimensionality reduction.
arxiv-15300-127 | Bayesian Sparsity for Intractable Distributions | http://arxiv.org/pdf/1602.03807v1.pdf | author:John B. Ingraham, Debora S. Marks category:stat.ML q-bio.QM published:2016-02-11 summary:Bayesian approaches for single-variable and group-structured sparsityoutperform L1 regularization, but are challenging to apply to large,potentially intractable models. Here we show how noncentered parameterizations,a common trick for improving the efficiency of exact inference in hierarchicalmodels, can similarly improve the accuracy of variational approximations. Wedevelop this with two contributions: First, we introduce Fadeout, an approachfor variational inference that uses noncentered parameterizations to capture aposteriori correlations between parameters and hyperparameters. Second, weextend stochastic variational inference to undirected models, enablingefficient hierarchical Bayes without approximations of intractable normalizingconstants. We find that this framework substantially improves inferences ofundirected graphical models under both sparse and group-sparse priors.
arxiv-15300-128 | Local High-order Regularization on Data Manifolds | http://arxiv.org/pdf/1602.03805v1.pdf | author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV published:2016-02-11 summary:The common graph Laplacian regularizer is well-established in semi-supervisedlearning and spectral dimensionality reduction. However, as a first-orderregularizer, it can lead to degenerate functions in high-dimensional manifolds.The iterated graph Laplacian enables high-order regularization, but it has ahigh computational complexity and so cannot be applied to large problems. Weintroduce a new regularizer which is globally high order and so does not sufferfrom the degeneracy of the graph Laplacian regularizer, but is also sparse forefficient computation in semi-supervised learning applications. We reducecomputational complexity by building a local first-order approximation of themanifold as a surrogate geometry, and construct our high-order regularizerbased on local derivative evaluations therein. Experiments on human body shapeand pose analysis demonstrate the effectiveness and efficiency of our method.
arxiv-15300-129 | HMM and DTW for evaluation of therapeutical gestures using kinect | http://arxiv.org/pdf/1602.03742v1.pdf | author:Carlos Palma, Augusto Salazar, Francisco Vargas category:cs.HC cs.CV published:2016-02-11 summary:Automatic recognition of the quality of movement in human beings is achallenging task, given the difficulty both in defining the constraints thatmake a movement correct, and the difficulty in using noisy data to determine ifthese constraints were satisfied. This paper presents a method for thedetection of deviations from the correct form in movements from physicaltherapy routines based on Hidden Markov Models, which is compared to DynamicTime Warping. The activities studied include upper an lower limbs movements,the data used comes from a Kinect sensor. Correct repetitions of the activitiesof interest were recorded, as well as deviations from these correct forms. Theability of the proposed approach to detect these deviations was studied.Results show that a system based on HMM is much more likely to determine if acertain movement has deviated from the specification.
arxiv-15300-130 | A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation | http://arxiv.org/pdf/1602.03725v1.pdf | author:Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, Christian Theobalt category:cs.CV published:2016-02-11 summary:Generative reconstruction methods compute the 3D configuration (such as poseand/or geometry) of a shape by optimizing the overlap of the projected 3D shapemodel with images. Proper handling of occlusions is a big challenge, since thevisibility function that indicates if a surface point is seen from a camera canoften not be formulated in closed form, and is in general discrete andnon-differentiable at occlusion boundaries. We present a new scenerepresentation that enables an analytically differentiable closed-formformulation of surface visibility. In contrast to previous methods, this yieldssmooth, analytically differentiable, and efficient to optimize pose similarityenergies with rigorous occlusion handling, fewer local minima, andexperimentally verified improved convergence of numerical optimization. Theunderlying idea is a new image formation model that represents opaque objectsby a translucent medium with a smooth Gaussian density distribution which turnsvisibility into a smooth phenomenon. We demonstrate the advantages of ourversatile scene model in several generative pose estimation problems, namelymarker-less multi-object pose estimation, marker-less human motion capture withfew cameras, and image-based 3D geometry estimation.
arxiv-15300-131 | An End-to-End Neural Network for Polyphonic Piano Music Transcription | http://arxiv.org/pdf/1508.01774v2.pdf | author:Siddharth Sigtia, Emmanouil Benetos, Simon Dixon category:stat.ML cs.LG cs.SD published:2015-08-07 summary:We present a supervised neural network model for polyphonic piano musictranscription. The architecture of the proposed model is analogous to speechrecognition systems and comprises an acoustic model and a music language model.The acoustic model is a neural network used for estimating the probabilities ofpitches in a frame of audio. The language model is a recurrent neural networkthat models the correlations between pitch combinations over time. The proposedmodel is general and can be used to transcribe polyphonic music withoutimposing any constraints on the polyphony. The acoustic and language modelpredictions are combined using a probabilistic graphical model. Inference overthe output variables is performed using the beam search algorithm. We performtwo sets of experiments. We investigate various neural network architecturesfor the acoustic models and also investigate the effect of combining acousticand music language model predictions using the proposed architecture. Wecompare performance of the neural network based acoustic models with twopopular unsupervised acoustic models. Results show that convolutional neuralnetwork acoustic models yields the best performance across all evaluationmetrics. We also observe improved performance with the application of the musiclanguage models. Finally, we present an efficient variant of beam search thatimproves performance and reduces run-times by an order of magnitude, making themodel suitable for real-time applications.
arxiv-15300-132 | Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction | http://arxiv.org/pdf/1602.03686v1.pdf | author:Edward Choi, Andy Schuetz, Walter F. Stewart, Jimeng Sun category:cs.LG cs.NE published:2016-02-11 summary:Objective: To transform heterogeneous clinical data from electronic healthrecords into clinically meaningful constructed features using data drivenmethod that rely, in part, on temporal relations among data. Materials andMethods: The clinically meaningful representations of medical concepts andpatients are the key for health analytic applications. Most of existingapproaches directly construct features mapped to raw data (e.g., ICD or CPTcodes), or utilize some ontology mapping such as SNOMED codes. However, none ofthe existing approaches leverage EHR data directly for learning such conceptrepresentation. We propose a new way to represent heterogeneous medicalconcepts (e.g., diagnoses, medications and procedures) based on co-occurrencepatterns in longitudinal electronic health records. The intuition behind themethod is to map medical concepts that are co-occuring closely in time tosimilar concept vectors so that their distance will be small. We also derive asimple method to construct patient vectors from the related medical conceptvectors. Results: We evaluate similar medical concepts across diagnosis,medication and procedure. The results show xx% relevancy between similar pairsof medical concepts. Our proposed representation significantly improves thepredictive modeling performance for onset of heart failure (HF), whereclassification methods (e.g. logistic regression, neural network, supportvector machine and K-nearest neighbors) achieve up to 23% improvement in areaunder the ROC curve (AUC) using this proposed representation. Conclusion: Weproposed an effective method for patient and medical concept representationlearning. The resulting representation can map relevant concepts together andalso improves predictive modeling performance.
arxiv-15300-133 | Structure Learning of Partitioned Markov Networks | http://arxiv.org/pdf/1504.00624v4.pdf | author:Song Liu, Taiji Suzuki, Masashi Sugiyama, Kenji Fukumizu category:stat.ML published:2015-04-02 summary:We learn the structure of a Markov Network between two groups of randomvariables from joint observations. Since modelling and learning the full MNstructure may be hard, learning the links between two groups directly may be apreferable option. We introduce a novel concept called the \emph{partitionedratio} whose factorization directly associates with the Markovian properties ofrandom variables across two groups. A simple one-shot convex optimizationprocedure is proposed for learning the \emph{sparse} factorizations of thepartitioned ratio and it is theoretically guaranteed to recover the correctinter-group structure under mild conditions. The performance of the proposedmethod is experimentally compared with the state of the art MN structurelearning methods using ROC curves. Real applications on analyzingbipartisanship in US congress and pairwise DNA/time-series alignments are alsoreported.
arxiv-15300-134 | A Universal Approximation Theorem for Mixture of Experts Models | http://arxiv.org/pdf/1602.03683v1.pdf | author:Hien D Nguyen, Luke R Lloyd-Jones, Geoffrey J McLachlan category:stat.ML published:2016-02-11 summary:The mixture of experts (MoE) model is a popular neural network architecturefor nonlinear regression and classification. The class of MoE mean functions isknown to be uniformly convergent to any unknown target function, assuming thatthe target function is from Sobolev space that is sufficiently differentiableand that the domain of estimation is a compact unit hypercube. We provide analternative result, which shows that the class of MoE mean functions is densein the class of all continuous functions over arbitrary compact domains ofestimation. Our result can be viewed as a universal approximation theorem forMoE models.
arxiv-15300-135 | Package equivalence in complex software network | http://arxiv.org/pdf/1602.03681v1.pdf | author:Tomislav Slijepčević category:cs.SI stat.ML published:2016-02-11 summary:The public package registry npm is one of the biggest software registry. Withits 216 911 software packages, it forms a big network of software dependencies.In this paper we evaluate various methods for finding similar packages in thenpm network, using only the structure of the graph. Namely, we want to find away of categorizing similar packages, which would be useful for recommendationsystems. This size enables us to compute meaningful results, as it softened theparticularities of the graph. Npm is also quite famous as it is the defaultpackage repository of Node.js. We believe that it will make our resultsinteresting for more people than a less used package repository. This makes ita good subject of analysis of software networks.
arxiv-15300-136 | On the emergence of syntactic structures: quantifying and modelling duality of patterning | http://arxiv.org/pdf/1602.03661v1.pdf | author:Vittorio Loreto, Pietro Gravino, Vito D. P. Servedio, Francesca Tria category:physics.soc-ph cs.CL published:2016-02-11 summary:The complex organization of syntax in hierarchical structures is one of thecore design features of human language. Duality of patterning refers forinstance to the organization of the meaningful elements in a language at twodistinct levels: a combinatorial level where meaningless forms are combinedinto meaningful forms and a compositional level where meaningful forms arecomposed into larger lexical units. The question remains wide open regardinghow such a structure could have emerged. Furthermore a clear mathematicalframework to quantify this phenomenon is still lacking. The aim of this paperis that of addressing these two aspects in a self-consistent way. First, weintroduce suitable measures to quantify the level of combinatoriality andcompositionality in a language, and present a framework to estimate theseobservables in human natural languages. Second, we show that the theoreticalpredictions of a multi-agents modeling scheme, namely the Blending Game, are insurprisingly good agreement with empirical data. In the Blending Game apopulation of individuals plays language games aiming at success incommunication. It is remarkable that the two sides of duality of patterningemerge simultaneously as a consequence of a pure cultural dynamics in asimulated environment that contains meaningful relations, provided a simpleconstraint on message transmission fidelity is also considered.
arxiv-15300-137 | On the Difficulty of Selecting Ising Models with Approximate Recovery | http://arxiv.org/pdf/1602.03647v1.pdf | author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.LG cs.SI math.IT stat.ML published:2016-02-11 summary:In this paper, we consider the problem of estimating the underlying graphicalmodel of an Ising distribution given a number of independent and identicallydistributed samples. We adopt an \emph{approximate recovery} criterion thatallows for a number of missed edges or incorrectly-included edges, thusdeparting from the extensive literature considering the exact recovery problem.Our main results provide information-theoretic lower bounds on the requirednumber of samples (i.e., the sample complexity) for graph classes imposingconstraints on the number of edges, maximal degree, and sparse separationproperties. We identify a broad range of scenarios where, either up to constantfactors or logarithmic factors, our lower bounds match the best known lowerbounds for the exact recovery criterion, several of which are known to be tightor near-tight. Hence, in these cases, we prove that the approximate recoveryproblem is not much easier than the exact recovery problem. Our bounds are obtained via a modification of Fano's inequality for handlingthe approximate recovery criterion, along with suitably-designed ensembles ofgraphs that can broadly be classed into two categories: (i) Those containinggraphs that contain several isolated edges or cliques and are thus difficult todistinguish from the empty graph; (ii) Those containing graphs for whichcertain groups of nodes are highly correlated, thus making it difficult todetermine precisely which edges connect them. We support our theoreticalresults on these ensembles with numerical experiments.
arxiv-15300-138 | A New Approach to Building the Input--Output Table | http://arxiv.org/pdf/1504.01362v6.pdf | author:Ryohei Hisano category:stat.ML published:2015-04-06 summary:I present a new approach to estimating the interdependence of industries inan economy by applying data science solutions. By exploiting interfirmbuyer--seller network data, I show that the problem of estimating theinterdependence of industries is similar to the problem of uncovering thelatent block structure in network science literature. To estimate theunderlying structure with greater accuracy, I propose an extension of thesparse block model that incorporates node textual information and an unboundednumber of industries and interactions among them. The latter task isaccomplished by extending the well-known Chinese restaurant process to twodimensions. Inference is based on collapsed Gibbs sampling, and the model isevaluated on both synthetic and real-world datasets. I show that the proposedmodel improves in predictive accuracy and successfully provides a satisfactorysolution to the motivated problem. I also discuss issues that affect the futureperformance of this approach.
arxiv-15300-139 | Sparsity in Dynamics of Spontaneous Subtle Emotions: Analysis \& Application | http://arxiv.org/pdf/1601.04805v2.pdf | author:Anh Cat Le Ngo, John See, Raphael Chung-Wei Phan category:cs.CV published:2016-01-19 summary:Spontaneous subtle emotions are expressed through micro-expressions, whichare tiny, sudden and short-lived dynamics of facial muscles; thus poses a greatchallenge for visual recognition. The abrupt but significant dynamics for therecognition task are temporally sparse while the rest, irrelevant dynamics, aretemporally redundant. In this work, we analyze and enforce sparsity constrainsto learn significant temporal and spectral structures while eliminateirrelevant facial dynamics of micro-expressions, which would ease the challengein the visual recognition of spontaneous subtle emotions. The hypothesis isconfirmed through experimental results of automatic spontaneous subtle emotionrecognition with several sparsity levels on CASME II and SMIC, the only twopublicly available spontaneous subtle emotion databases. The overallperformances of the automatic subtle emotion recognition are boosted when onlysignificant dynamics are preserved from the original sequences.
arxiv-15300-140 | Class Probability Estimation via Differential Geometric Regularization | http://arxiv.org/pdf/1503.01436v7.pdf | author:Qinxun Bai, Steven Rosenberg, Zheng Wu, Stan Sclaroff category:cs.LG cs.CG stat.ML published:2015-03-04 summary:We study the problem of supervised learning for both binary and multiclassclassification from a unified geometric perspective. In particular, we proposea geometric regularization technique to find the submanifold corresponding to arobust estimator of the class probability $P(y\pmb{x})$. The regularizationterm measures the volume of this submanifold, based on the intuition thatoverfitting produces rapid local oscillations and hence large volume of theestimator. This technique can be applied to regularize any classificationfunction that satisfies two requirements: firstly, an estimator of the classprobability can be obtained; secondly, first and second derivatives of theclass probability estimator can be calculated. In experiments, we apply ourregularization technique to standard loss functions for classification, ourRBF-based implementation compares favorably to widely used regularizationmethods for both binary and multiclass classification.
arxiv-15300-141 | Simple Search Algorithms on Semantic Networks Learned from Language Use | http://arxiv.org/pdf/1602.03265v2.pdf | author:Aida Nematzadeh, Filip Miscevic, Suzanne Stevenson category:cs.CL published:2016-02-10 summary:Recent empirical and modeling research has focused on the semantic fluencytask because it is informative about semantic memory. An interesting interplayarises between the richness of representations in semantic memory and thecomplexity of algorithms required to process it. It has remained an openquestion whether representations of words and their relations learned fromlanguage use can enable a simple search algorithm to mimic the observedbehavior in the fluency task. Here we show that it is plausible to learn richrepresentations from naturalistic data for which a very simple search algorithm(a random walk) can replicate the human patterns. We suggest that explicitlystructuring knowledge about words into a semantic network plays a crucial rolein modeling human behavior in memory search and retrieval; moreover, this isthe case across a range of semantic information sources.
arxiv-15300-142 | Attentive Pooling Networks | http://arxiv.org/pdf/1602.03609v1.pdf | author:Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou category:cs.CL cs.LG published:2016-02-11 summary:In this work, we propose Attentive Pooling (AP), a two-way attentionmechanism for discriminative model training. In the context of pair-wiseranking or classification with neural networks, AP enables the pooling layer tobe aware of the current input pair, in a way that information from the twoinput items can directly influence the computation of each other'srepresentations. Along with such representations of the paired inputs, APjointly learns a similarity measure over projected segments (e.g. trigrams) ofthe pair, and subsequently, derives the corresponding attention vector for eachinput to guide the pooling. Our two-way attention mechanism is a generalframework independent of the underlying representation learning, and it hasbeen applied to both convolutional neural networks (CNNs) and recurrent neuralnetworks (RNNs) in our studies. The empirical results, from three verydifferent benchmark tasks of question answering/answer selection, demonstratethat our proposed models outperform a variety of strong baselines and achievestate-of-the-art performance in all the benchmarks.
arxiv-15300-143 | Variations of the Similarity Function of TextRank for Automated Summarization | http://arxiv.org/pdf/1602.03606v1.pdf | author:Federico Barrios, Federico López, Luis Argerich, Rosa Wachenchauzer category:cs.CL cs.IR I.2.7 published:2016-02-11 summary:This article presents new alternatives to the similarity function for theTextRank algorithm for automatic summarization of texts. We describe thegeneralities of the algorithm and the different functions we propose. Some ofthese variants achieve a significative improvement using the same metrics anddataset as the original publication.
arxiv-15300-144 | Data-Driven Online Decision Making with Costly Information Acquisition | http://arxiv.org/pdf/1602.03600v1.pdf | author:Onur Atan, William Whoiles, Mihaela van der Schaar category:stat.ML cs.LG published:2016-02-11 summary:Existing work on online learning for decision making takes the informationavailable as a given and focuses solely on choosing best actions given thisinformation. Instead, in this paper, the decision maker needs to simultaneouslylearn both what decisions to make and what source(s) of contextual informationto gather data from in order to inform its decisions such that its reward ismaximized. We propose algorithms that obtain costly source(s) of contextualinformation over time, while simultaneously learning what actions to take basedon the contextual information revealed by the selected source(s). We prove thatour algorithms achieve regret that is logarithmic in time. We demonstrate theperformance of our algorithms using a medical dataset. The proposed algorithmcan be applied in many applications including clinical decision assist systemsfor medical diagnosis, recommender systems, actionable intelligence etc., whereobserving the complete information in every instance or consulting all theavailable sources to gather intelligence before making decisions is costly.
arxiv-15300-145 | Generating Discriminative Object Proposals via Submodular Ranking | http://arxiv.org/pdf/1602.03585v1.pdf | author:Yangmuzi Zhang, Zhuolin Jiang, Xi Chen, Larry S. Davis category:cs.CV published:2016-02-11 summary:A multi-scale greedy-based object proposal generation approach is presented.Based on the multi-scale nature of objects in images, our approach is built ontop of a hierarchical segmentation. We first identify the representative anddiverse exemplar clusters within each scale by using a diversity rankingalgorithm. Object proposals are obtained by selecting a subset from themulti-scale segment pool via maximizing a submodular objective function, whichconsists of a weighted coverage term, a single-scale diversity term and amulti-scale reward term. The weighted coverage term forces the selected set ofobject proposals to be representative and compact; the single-scale diversityterm encourages choosing segments from different exemplar clusters so that theywill cover as many object patterns as possible; the multi-scale reward termencourages the selected proposals to be discriminative and selected frommultiple layers generated by the hierarchical image segmentation. Theexperimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012segmentation dataset demonstrate the accuracy and efficiency of our objectproposal model. Additionally, we validate our object proposals in simultaneoussegmentation and detection and outperform the state-of-art performance.
arxiv-15300-146 | Disaggregation of SMAP L3 Brightness Temperatures to 9km using Kernel Machines | http://arxiv.org/pdf/1601.05350v2.pdf | author:Subit Chakrabarti, Tara Bongiovanni, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV published:2016-01-20 summary:In this study, a machine learning algorithm is used for disaggregation ofSMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. It uses imagesegmentation to cluster the study region based on meteorological and land coversimilarity, followed by a support vector machine based regression that computesthe value of the disaggregated T$_{\textrm{B}}$ at all pixels. High resolutionremote sensing products such as land surface temperature, normalized differencevegetation index, enhanced vegetation index, precipitation, soil texture, andland-cover were used for disaggregation. The algorithm was implemented in Iowa,United States, from April to July 2015, and compared with the SMAP L3_SM_APT$_{\textrm{B}}$ product at 9km. It was found that the disaggregatedT$_{\textrm{B}}$ were very similar to the SMAP-T$_{\textrm{B}}$ product, evenfor vegetated areas with a mean difference $\leq$ 5K. However, the standarddeviation of the disaggregation was lower by 7K than that of the AP product.The probability density functions of the disaggregated T$_{\textrm{B}}$ weresimilar to the SMAP-T$_{\textrm{B}}$. The results indicate that this algorithmmay be used for disaggregating T$_{\textrm{B}}$ using complex non-linearcorrelations on a grid.
arxiv-15300-147 | Acceleration of the PDHGM on strongly convex subspaces | http://arxiv.org/pdf/1511.06566v2.pdf | author:Tuomo Valkonen, Thomas Pock category:math.OC cs.CV published:2015-11-20 summary:We propose several variants of the primal-dual method due to Chambolle andPock. Without requiring full strong convexity of the objective functions, ourmethods are accelerated on subspaces with strong convexity. This yields mixedrates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect tothe dual sequence, and the residual part of the primal sequence. We demonstratethe efficacy of the proposed methods on image processing problems lackingstrong convexity, such as total generalised variation denoising and totalvariation deblurring.
arxiv-15300-148 | High Dimensional Inference with Random Maximum A-Posteriori Perturbations | http://arxiv.org/pdf/1602.03571v1.pdf | author:Tamir Hazan, Francesco Orabona, Anand D. Sarwate, Subhransu Maji, Tommi Jaakkola category:cs.LG cs.IT math.IT stat.ML published:2016-02-10 summary:In this work we present a new approach for high-dimensional statisticalinference that is based on optimization and random perturbations. Thisframework injects randomness to maximum a-posteriori (MAP) predictors byrandomly perturbing its potential function. When the perturbations are of lowdimension, sampling the perturb-max prediction is as efficient as MAPoptimization. A classic result from extreme value statistics asserts thatperturb-max operations generate unbiased samples from the Gibbs distributionusing high-dimensional perturbations. Unfortunately, the computational cost ofgenerating so many high-dimensional random variables can be prohibitive. Inthis work we show that the expected value of perturb-max inference with lowdimensional perturbations can be used sequentially to generate unbiased samplesfrom the Gibbs distribution. We also show that the expected value of themaximal perturbations is a natural bound on the entropy of such perturb-maxmodels. Finally we describe the measure concentration properties of perturb-maxvalues while showing that the deviation of their sampled average from itsexpectation decays exponentially in the number of samples.
arxiv-15300-149 | A Neural Network Anomaly Detector Using the Random Cluster Model | http://arxiv.org/pdf/1501.07227v5.pdf | author:Robert A. Murphy category:cs.LG cs.NE stat.ML 60D05 published:2015-01-28 summary:The random cluster model is used to define an upper bound on a distancemeasure as a function of the number of data points to be classified and theexpected value of the number of classes to form in a hybrid K-means andregression classification methodology, with the intent of detecting anomalies.Conditions are given for the identification of classes which contain anomaliesand individual anomalies within identified classes. A neural network modeldescribes the decision region-separating surface for offline storage and recallin any new anomaly detection.
arxiv-15300-150 | Estimating the Mean Number of K-Means Clusters to Form | http://arxiv.org/pdf/1503.03488v2.pdf | author:Robert A. Murphy category:cs.LG 60D05 published:2015-03-07 summary:Utilizing the sample size of a dataset, the random cluster model is employedin order to derive an estimate of the mean number of K-Means clusters to formduring classification of a dataset.
arxiv-15300-151 | Learning Privately from Multiparty Data | http://arxiv.org/pdf/1602.03552v1.pdf | author:Jihun Hamm, Paul Cao, Mikhail Belkin category:cs.LG cs.CR published:2016-02-10 summary:Learning a classifier from private data collected by multiple parties is animportant problem that has many potential applications. How can we build anaccurate and differentially private global classifier by combininglocally-trained classifiers from different parties, without access to anyparty's private data? We propose to transfer the `knowledge' of the localclassifier ensemble by first creating labeled data from auxiliary unlabeleddata, and then train a global $\epsilon$-differentially private classifier. Weshow that majority voting is too sensitive and therefore propose a new riskweighted by class probabilities estimated from the ensemble. Relative to anon-private solution, our private solution has a generalization error boundedby $O(\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allowsstrong privacy without performance loss when $M$ is large, such as incrowdsensing applications. We demonstrate the performance of our method withrealistic tasks of activity recognition, network intrusion detection, andmalicious URL detection.
arxiv-15300-152 | Knowledge Transfer with Medical Language Embeddings | http://arxiv.org/pdf/1602.03551v1.pdf | author:Stephanie L. Hyland, Theofanis Karaletsos, Gunnar Rätsch category:cs.CL stat.AP published:2016-02-10 summary:Identifying relationships between concepts is a key aspect of scientificknowledge synthesis. Finding these links often requires a researcher tolaboriously search through scien- tific papers and databases, as the size ofthese resources grows ever larger. In this paper we describe how distributionalsemantics can be used to unify structured knowledge graphs with unstructuredtext to predict new relationships between medical concepts, using aprobabilistic generative model. Our approach is also designed to amelioratedata sparsity and scarcity issues in the medical domain, which make languagemodelling more challenging. Specifically, we integrate the medical relationaldatabase (SemMedDB) with text from electronic health records (EHRs) to performknowledge graph completion. We further demonstrate the ability of our model topredict relationships between tokens not appearing in the relational database.
arxiv-15300-153 | Autoencoding beyond pixels using a learned similarity metric | http://arxiv.org/pdf/1512.09300v2.pdf | author:Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther category:cs.LG cs.CV stat.ML published:2015-12-31 summary:We present an autoencoder that leverages learned representations to bettermeasure similarities in data space. By combining a variational autoencoder witha generative adversarial network we can use learned feature representations inthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,we replace element-wise errors with feature-wise errors to better capture thedata distribution while offering invariance towards e.g. translation. We applyour method to images of faces and show that it outperforms VAEs withelement-wise similarity measures in terms of visual fidelity. Moreover, we showthat the method learns an embedding in which high-level abstract visualfeatures (e.g. wearing glasses) can be modified using simple arithmetic.
arxiv-15300-154 | Research Priorities for Robust and Beneficial Artificial Intelligence | http://arxiv.org/pdf/1602.03506v1.pdf | author:Stuart Russell, Daniel Dewey, Max Tegmark category:cs.AI stat.ML published:2016-02-10 summary:Success in the quest for artificial intelligence has the potential to bringunprecedented benefits to humanity, and it is therefore worthwhile toinvestigate how to maximize these benefits while avoiding potential pitfalls.This article gives numerous examples (which should by no means be construed asan exhaustive list) of such worthwhile research aimed at ensuring that AIremains robust and beneficial.
arxiv-15300-155 | Adaptive Image Denoising by Mixture Adaptation | http://arxiv.org/pdf/1601.04770v2.pdf | author:Enming Luo, Stanley H. Chan, Truong Q. Nguyen category:cs.CV stat.ME published:2016-01-19 summary:We propose an adaptive learning procedure to learn effective image priors.The new algorithm, called the Expectation-Maximization (EM) adaptation, takes ageneric prior learned from a generic external database and adapts it to theimage of interest to generate a specific prior. Different from existing methodswhich combine internal and external statistics in an ad-hoc way, the proposedalgorithm learns a single unified prior through an adaptive process. There aretwo major contributions in this paper. First, we rigorously derive the EMadaptation algorithm from the Bayesian hyper-prior perspective and show that itcan be further simplified to improve the computational complexity. Second, inthe absence of the latent clean image, we show how EM adaptation can bemodified and applied on pre-filtered images. We discuss how to estimateinternal parameters and demonstrate how to improve the denoising performance byrunning EM adaptation iteratively. Experimental results show that the adaptedprior is consistently better than the originally un-adapted prior, and issuperior than some state-of-the-art algorithms.
arxiv-15300-156 | Learning Distributed Representations of Sentences from Unlabelled Data | http://arxiv.org/pdf/1602.03483v1.pdf | author:Felix Hill, Kyunghyun Cho, Anna Korhonen category:cs.CL cs.LG published:2016-02-10 summary:Unsupervised methods for learning distributed representations of words areubiquitous in today's NLP research, but far less is known about the best waysto learn distributed phrase or sentence representations from unlabelled data.This paper is a systematic comparison of models that learn suchrepresentations. We find that the optimal approach depends critically on theintended application. Deeper, more complex models are preferable forrepresentations to be used in supervised systems, but shallow log-linear modelswork best for building representation spaces that can be decoded with simplespatial distance metrics. We also propose two new unsupervisedrepresentation-learning objectives designed to optimise the trade-off betweentraining time, domain portability and performance.
arxiv-15300-157 | Reliable Crowdsourcing under the Generalized Dawid-Skene Model | http://arxiv.org/pdf/1602.03481v1.pdf | author:Ashish Khetan, Sewoong Oh category:cs.LG cs.HC cs.SI stat.ML published:2016-02-10 summary:Crowdsourcing systems provide scalable and cost-effective human-poweredsolutions at marginal cost, for classification tasks where humans aresignificantly better than the machines. Although traditional approaches inaggregating crowdsourced labels have relied on the Dawid-Skene model, thisfails to capture how some tasks are inherently more difficult than the others.Several generalizations have been proposed, but inference becomes intractableand typical solutions resort to heuristics. To bridge this gap, we study arecently proposed generalize Dawid-Skene model, and propose a linear-timealgorithm based on spectral methods. We show near-optimality of the proposedapproach, by providing an upper bound on the error and comparing it to afundamental limit. We provide numerical experiments on synthetic data matchingour analyses, and also on real datasets demonstrating that the spectral methodsignificantly improves over simple majority voting and is comparable to othermethods.
arxiv-15300-158 | Online Active Linear Regression via Thresholding | http://arxiv.org/pdf/1602.02845v2.pdf | author:Carlos Riquelme, Ramesh Johari, Baosen Zhang category:stat.ML cs.LG published:2016-02-09 summary:We consider the problem of online active learning to collect data forregression modeling. Specifically, we consider a decision maker that faces alimited experimentation budget but must efficiently learn an underlying linearpopulation model. Our goal is to develop algorithms that provide substantialgains over passive random sampling of observations. To that end, our maincontribution is a novel threshold-based algorithm for selection ofobservations; we characterize its performance and related lower bounds. We alsoapply our approach successfully to regularized regression. Simulations suggestthe algorithm is remarkably robust: it provides significant benefits overpassive random sampling even in several real-world datasets that exhibit highnonlinearity and high dimensionality --- significantly reducing the mean andvariance of the squared error.
arxiv-15300-159 | Super-Resolved Retinal Image Mosaicing | http://arxiv.org/pdf/1602.03458v1.pdf | author:Thomas Köhler, Axel Heinrich, Andreas Maier, Joachim Hornegger, Ralf P. Tornow category:cs.CV published:2016-02-10 summary:The acquisition of high-resolution retinal fundus images with a large fieldof view (FOV) is challenging due to technological, physiological and economicreasons. This paper proposes a fully automatic framework to reconstruct retinalimages of high spatial resolution and increased FOV from multiplelow-resolution images captured with non-mydriatic, mobile and video-capable butlow-cost cameras. Within the scope of one examination, we scan differentregions on the retina by exploiting eye motion conducted by a patient guidance.Appropriate views for our mosaicing method are selected based on optic disktracking to trace eye movements. For each view, one super-resolved image isreconstructed by fusion of multiple video frames. Finally, all super-resolvedviews are registered to a common reference using a novel polynomialregistration scheme and combined by means of image mosaicing. We evaluated ourframework for a mobile and low-cost video fundus camera. In our experiments, wereconstructed retinal images of up to 30{\deg} FOV from 10 complementary viewsof 15{\deg} FOV. An evaluation of the mosaics by human experts as well as aquantitative comparison to conventional color fundus images encourage theclinical usability of our framework.
arxiv-15300-160 | Stochastic Quasi-Newton Langevin Monte Carlo | http://arxiv.org/pdf/1602.03442v1.pdf | author:Umut Şimşekli, Roland Badeau, A. Taylan Cemgil, Gaël Richard category:stat.ML published:2016-02-10 summary:Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods havebeen proposed for scaling up Monte Carlo computations to large data problems.Whilst these approaches have proven useful in many applications, vanillaSG-MCMC might suffer from poor mixing rates when random variables exhibitstrong couplings under the target densities or big scale differences. In thisstudy, we propose a novel SG-MCMC method that takes the local geometry intoaccount by using ideas from Quasi-Newton optimization methods. These secondorder methods directly approximate the inverse Hessian by using a limitedhistory of samples and their gradients. Our method uses dense approximations ofthe inverse Hessian while keeping the time and memory complexities linear withthe dimension of the problem. We provide a formal theoretical analysis where weshow that the proposed method is asymptotically unbiased and consistent withthe posterior expectations. We illustrate the effectiveness of the approach onboth synthetic and real datasets. Our experiments on two challengingapplications show that our method achieves fast convergence rates similar toRiemannian approaches while at the same time having low computationalrequirements similar to diagonal preconditioning approaches.
arxiv-15300-161 | Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video | http://arxiv.org/pdf/1506.01911v3.pdf | author:Lionel Pigou, Aäron van den Oord, Sander Dieleman, Mieke Van Herreweghe, Joni Dambre category:cs.CV cs.AI cs.LG cs.NE stat.ML published:2015-06-05 summary:Recent studies have demonstrated the power of recurrent neural networks formachine translation, image captioning and speech recognition. For the task ofcapturing temporal structure in video, however, there still remain numerousopen research questions. Current research suggests using a simple temporalfeature pooling strategy to take into account the temporal aspect of video. Wedemonstrate that this method is not sufficient for gesture recognition, wheretemporal information is more discriminative compared to general videoclassification tasks. We explore deep architectures for gesture recognition invideo and propose a new end-to-end trainable neural network architectureincorporating temporal convolutions and bidirectional recurrence. Our maincontributions are twofold; first, we show that recurrence is crucial for thistask; second, we show that adding temporal convolutions leads to significantimprovements. We evaluate the different approaches on the Montalbano gesturerecognition dataset, where we achieve state-of-the-art results.
arxiv-15300-162 | Automatic Sarcasm Detection: A Survey | http://arxiv.org/pdf/1602.03426v1.pdf | author:Aditya Joshi, Pushpak Bhattacharyya, Mark James Carman category:cs.CL published:2016-02-10 summary:Automatic detection of sarcasm has witnessed interest from the sentimentanalysis research community. With diverse approaches, datasets and analysesthat have been reported, there is an essential need to have a collectiveunderstanding of the research in this area. In this survey of automatic sarcasmdetection, we describe datasets, approaches (both supervised and rule-based),and trends in sarcasm detection research. We also present a research matrixthat summarizes past work, and list pointers to future work.
arxiv-15300-163 | Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning | http://arxiv.org/pdf/1602.03409v1.pdf | author:Hoo-Chang Shin, Holger R. Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura, Ronald M. Summers category:cs.CV published:2016-02-10 summary:Remarkable progress has been made in image recognition, primarily due to theavailability of large-scale annotated datasets and the revival of deep CNN.CNNs enable learning data-driven, highly representative, layered hierarchicalimage features from sufficient training data. However, obtaining datasets ascomprehensively annotated as ImageNet in the medical imaging domain remains achallenge. There are currently three major techniques that successfully employCNNs to medical image classification: training the CNN from scratch, usingoff-the-shelf pre-trained CNN features, and conducting unsupervised CNNpre-training with supervised fine-tuning. Another effective method is transferlearning, i.e., fine-tuning CNN models pre-trained from natural image datasetto medical image tasks. In this paper, we exploit three important, butpreviously understudied factors of employing deep convolutional neural networksto computer-aided detection problems. We first explore and evaluate differentCNN architectures. The studied models contain 5 thousand to 160 millionparameters, and vary in numbers of layers. We then evaluate the influence ofdataset scale and spatial image context on performance. Finally, we examinewhen and why transfer learning from pre-trained ImageNet (via fine-tuning) canbe useful. We study two specific computer-aided detection (CADe) problems,namely thoraco-abdominal lymph node (LN) detection and interstitial lungdisease (ILD) classification. We achieve the state-of-the-art performance onthe mediastinal LN detection, with 85% sensitivity at 3 false positive perpatient, and report the first five-fold cross-validation classification resultson predicting axial CT slices with ILD categories. Our extensive empiricalevaluation, CNN model analysis and valuable insights can be extended to thedesign of high performance CAD systems for other medical imaging tasks.
arxiv-15300-164 | Peer Grading in a Course on Algorithms and Data Structures: Machine Learning Algorithms do not Improve over Simple Baselines | http://arxiv.org/pdf/1506.00852v2.pdf | author:Mehdi S. M. Sajjadi, Morteza Alamgir, Ulrike von Luxburg category:cs.LG stat.ML published:2015-06-02 summary:Peer grading is the process of students reviewing each others' work, such ashomework submissions, and has lately become a popular mechanism used in massiveopen online courses (MOOCs). Intrigued by this idea, we used it in a course onalgorithms and data structures at the University of Hamburg. Throughout thewhole semester, students repeatedly handed in submissions to exercises, whichwere then evaluated both by teaching assistants and by a peer gradingmechanism, yielding a large dataset of teacher and peer grades. We applieddifferent statistical and machine learning methods to aggregate the peer gradesin order to come up with accurate final grades for the submissions (supervisedand unsupervised, methods based on numeric scores and ordinal rankings).Surprisingly, none of them improves over the baseline of using the mean peergrade as the final grade. We discuss a number of possible explanations forthese results and present a thorough analysis of the generated dataset.
arxiv-15300-165 | Comparison of feature extraction and dimensionality reduction methods for single channel extracellular spike sorting | http://arxiv.org/pdf/1602.03379v1.pdf | author:Anupam Mitra, Anagh Pathak, Kaushik Majumdar category:q-bio.QM cs.CV q-bio.NC published:2016-02-10 summary:Spikes in the membrane electrical potentials of neurons play a major role inthe functioning of nervous systems of animals. Obtaining the spikes fromdifferent neurons has been a challenging problem for decades. Several schemeshave been proposed for spike sorting to isolate the spikes of individualneurons from electrical recordings in extracellular media. However, there ismuch scope for improvement in the accuracies obtained using the prevailingmethods of spike sorting. To determine more effective spike sorting strategiesusing well known methods, we compared different types of signal features andtechniques for dimensionality reduction in feature space. We tried to determinean optimum or near optimum feature extraction and dimensionality reductionmethods and an optimum or near optimum number of features for spike sorting. Weassessed relative performance of well known methods on simulated recordingsspecially designed for development and benchmarking of spike sorting schemes,with varying number of spike classes and the well established method of$k$-means clustering of selected features. We found that almost all well knownmethods performed quite well. Nevertheless, from spike waveforms of 64 samples,sampled at 24 kHz, using principal component analysis (PCA) to select around 46to 55 features led to the better spike sorting performance than most othermethods (Wilcoxon signed rank sum test, $p < 0.001$).
arxiv-15300-166 | Fast model selection by limiting SVM training times | http://arxiv.org/pdf/1602.03368v1.pdf | author:Aydin Demircioglu, Daniel Horn, Tobias Glasmachers, Bernd Bischl, Claus Weihs category:stat.ML cs.LG published:2016-02-10 summary:Kernelized Support Vector Machines (SVMs) are among the best performingsupervised learning methods. But for optimal predictive performance,time-consuming parameter tuning is crucial, which impedes application. Totackle this problem, the classic model selection procedure based on grid-searchand cross-validation was refined, e.g. by data subsampling and direct searchheuristics. Here we focus on a different aspect, the stopping criterion for SVMtraining. We show that by limiting the training time given to the SVM solverduring parameter tuning we can reduce model selection times by an order ofmagnitude.
arxiv-15300-167 | Adaptive Skills, Adaptive Partitions (ASAP) | http://arxiv.org/pdf/1602.03351v1.pdf | author:Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor category:cs.LG cs.AI stat.ML published:2016-02-10 summary:We introduce the Adaptive Skills, Adaptive Partitions (ASAP) algorithm that(1) learns skills (i.e., temporally extended actions or options) as well as (2)where to apply them to solve a Markov decision process. ASAP is initiallyprovided with a misspecified hierarchical model and is able to correct thismodel and learn a near-optimal set of skills to solve a given task. We believethat (1) and (2) are the core components necessary for a truly general skilllearning framework, which is a key building block needed to scale up tolifelong learning agents. ASAP is also able to solve related new tasks simplyby adapting where it applies its existing learned skills. We prove that ASAPconverges to a local optimum under natural conditions. Finally, our extensiveexperimental results, which include a RoboCup domain, demonstrate the abilityof ASAP to learn where to reuse skills as well as solve multiple tasks withconsiderably less experience than solving each task from scratch.
arxiv-15300-168 | Iterative Hierarchical Optimization for Misspecified Problems (IHOMP) | http://arxiv.org/pdf/1602.03348v1.pdf | author:Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor category:cs.LG cs.AI published:2016-02-10 summary:Reinforcement Learning (RL) aims to learn an optimal policy for a MarkovDecision Process (MDP). For complex, high-dimensional MDPs, it may only befeasible to represent the policy with function approximation. If the policyrepresentation used cannot represent good policies, the problem is misspecifiedand the learned policy may be far from optimal. We introduce IHOMP as anapproach for solving misspecified problems. IHOMP iteratively refines a set ofspecialized policies based on a limited representation. We refer to thesepolicies as policy threads. At the same time, IHOMP stitches these policythreads together in a hierarchical fashion to solve a problem that wasotherwise misspecified. We prove that IHOMP enjoys theoretical convergenceguarantees and extend IHOMP to exploit Option Interruption (OI) enabling it tolearn where policy threads can be reused. Our experiments demonstrate thatIHOMP can find near-optimal solutions to otherwise misspecified problems andthat OI can further improve the solutions.
arxiv-15300-169 | DAP3D-Net: Where, What and How Actions Occur in Videos? | http://arxiv.org/pdf/1602.03346v1.pdf | author:Li Liu, Yi Zhou, Ling Shao category:cs.CV published:2016-02-10 summary:Action parsing in videos with complex scenes is an interesting butchallenging task in computer vision. In this paper, we propose a generic 3Dconvolutional neural network in a multi-task learning manner for effective DeepAction Parsing (DAP3D-Net) in videos. Particularly, in the training phase,action localization, classification and attributes learning can be jointlyoptimized on our appearancemotion data via DAP3D-Net. For an upcoming testvideo, we can describe each individual action in the video simultaneously as:Where the action occurs, What the action is and How the action is performed. Towell demonstrate the effectiveness of the proposed DAP3D-Net, we alsocontribute a new Numerous-category Aligned Synthetic Action dataset, i.e.,NASA, which consists of 200; 000 action clips of more than 300 categories andwith 33 pre-defined action attributes in two hierarchical levels (i.e.,low-level attributes of basic body part movements and high-level attributesrelated to action motion). We learn DAP3D-Net using the NASA dataset and thenevaluate it on our collected Human Action Understanding (HAU) dataset.Experimental results show that our approach can accurately localize, categorizeand describe multiple actions in realistic videos.
arxiv-15300-170 | Patterns for Learning with Side Information | http://arxiv.org/pdf/1511.06429v5.pdf | author:Rico Jonschkowski, Sebastian Höfer, Oliver Brock category:cs.LG stat.ML published:2015-11-19 summary:Supervised, semi-supervised, and unsupervised learning estimate a functiongiven input/output samples. Generalization of the learned function to unseendata can be improved by incorporating side information into learning. Sideinformation are data that are neither from the input space nor from the outputspace of the function, but include useful information for learning it. In thispaper we show that learning with side information subsumes a variety of relatedapproaches, e.g. multi-task learning, multi-view learning and learning usingprivileged information. Our main contributions are (i) a new perspective thatconnects these previously isolated approaches, (ii) insights about how thesemethods incorporate different types of prior knowledge, and hence implementdifferent patterns, (iii) facilitating the application of these methods innovel tasks, as well as (iv) a systematic experimental evaluation of thesepatterns in two supervised learning tasks.
arxiv-15300-171 | Word learning under infinite uncertainty | http://arxiv.org/pdf/1412.2487v2.pdf | author:Richard A. Blythe, Andrew D. M. Smith, Kenny Smith category:physics.soc-ph cs.CL published:2014-12-08 summary:Language learners must learn the meanings of many thousands of words, despitethose words occurring in complex environments in which infinitely many meaningsmight be inferred by the learner as a word's true meaning. This problem ofinfinite referential uncertainty is often attributed to Willard Van OrmanQuine. We provide a mathematical formalisation of an ideal cross-situationallearner attempting to learn under infinite referential uncertainty, andidentify conditions under which word learning is possible. As Quine'sintuitions suggest, learning under infinite uncertainty is in fact possible,provided that learners have some means of ranking candidate word meanings interms of their plausibility; furthermore, our analysis shows that this rankingcould in fact be exceedingly weak, implying that constraints which allowlearners to infer the plausibility of candidate word meanings could themselvesbe weak. This approach lifts the burden of explanation from `smart' wordlearning constraints in learners, and suggests a programme of research intoweak, unreliable, probabilistic constraints on the inference of word meaning inreal word learners.
arxiv-15300-172 | Gabor Wavelets in Image Processing | http://arxiv.org/pdf/1602.03308v1.pdf | author:David Barina category:cs.CV cs.GR cs.MM published:2016-02-10 summary:This work shows the use of a two-dimensional Gabor wavelets in imageprocessing. Convolution with such a two-dimensional wavelet can be separatedinto two series of one-dimensional ones. The key idea of this work is toutilize a Gabor wavelet as a multiscale partial differential operator of agiven order. Gabor wavelets are used here to detect edges, corners and blobs. Aperformance of such an interest point detector is compared to detectorsutilizing a Haar wavelet and a derivative of a Gaussian function. The proposedapproach may be useful when a fast implementation of the Gabor transform isavailable or when the transform is already precomputed.
arxiv-15300-173 | A Theory of Generative ConvNet | http://arxiv.org/pdf/1602.03264v1.pdf | author:Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.LG published:2016-02-10 summary:The convolutional neural network (ConvNet or CNN) is a powerfuldiscriminative learning machine. In this paper, we show that a generativerandom field model that we call generative ConvNet can be derived from thediscriminative ConvNet. The probability distribution of the generative ConvNetmodel is in the form of exponential tilting of a reference distribution.Assuming re-lu non-linearity and Gaussian white noise reference distribution,we show that the generative ConvNet model contains a representational structurewith multiple layers of binary activation variables. The model is non-Gaussian,or more precisely, piecewise Gaussian, where each piece is determined by aninstantiation of the binary activation variables that reconstruct the mean ofthe Gaussian piece. The Langevin dynamics for synthesis is driven by thereconstruction error, and the corresponding gradient descent dynamics convergesto a local energy minimum that is auto-encoding. As for learning, we show thatthe contrastive divergence learning tends to reconstruct the observed images.Finally, we show that the maximum likelihood learning algorithm can generaterealistic natural images.
arxiv-15300-174 | A statistical perspective of sampling scores for linear regression | http://arxiv.org/pdf/1507.05870v2.pdf | author:Siheng Chen, Rohan Varma, Aarti Singh, Jelena Kovačević category:stat.ML published:2015-07-21 summary:In this paper, we consider a statistical problem of learning a linear modelfrom noisy samples. Existing work has focused on approximating the leastsquares solution by using leverage-based scores as an importance samplingdistribution. However, no finite sample statistical guarantees and nocomputationally efficient optimal sampling strategies have been proposed. Toevaluate the statistical properties of different sampling strategies, wepropose a simple yet effective estimator, which is easy for theoreticalanalysis and is useful in multitask linear regression. We derive the exact meansquare error of the proposed estimator for any given sampling scores. Based onminimizing the mean square error, we propose the optimal sampling scores forboth estimator and predictor, and show that they are influenced by thenoise-to-signal ratio. Numerical simulations match the theoretical analysiswell.
arxiv-15300-175 | Improved Eigenfeature Regularization for Face Identification | http://arxiv.org/pdf/1602.03256v1.pdf | author:Bappaditya Mandal category:cs.CV published:2016-02-10 summary:In this work, we propose to divide each class (a person) into subclassesusing spatial partition trees which helps in better capturing theintra-personal variances arising from the appearances of the same individual.We perform a comprehensive analysis on within-class and within-subclasseigenspectrums of face images and propose a novel method of eigenspectrummodeling which extracts discriminative features of faces from bothwithin-subclass and total or between-subclass scatter matrices. Effectivelow-dimensional face discriminative features are extracted for face recognition(FR) after performing discriminant evaluation in the entire eigenspace.Experimental results on popular face databases (AR, FERET) and the challengingunconstrained YouTube Face database show the superiority of our proposedapproach on all three databases.
arxiv-15300-176 | A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation | http://arxiv.org/pdf/1602.03253v1.pdf | author:Qiang Liu, Jason D. Lee, Michael I. Jordan category:stat.ML published:2016-02-10 summary:We derive a new discrepancy statistic for measuring differences between twoprobability distributions based on a novel combination of Stein's method andthe reproducing kernel Hilbert space theory. We apply our result to test howwell a probabilistic model fits a set of observations, and derive a new classof powerful goodness-of-fit tests that are widely applicable for complex andhigh dimensional distributions, even for those with computationally intractablenormalization constants. Both theoretical and empirical properties of ourmethods are studied thoroughly.
arxiv-15300-177 | Appearance Based Robot and Human Activity Recognition System | http://arxiv.org/pdf/1602.01608v2.pdf | author:Bappaditya Mandal category:cs.RO cs.CV published:2016-02-04 summary:In this work, we present an appearance based human activity recognitionsystem. It uses background modeling to segment the foreground object andextracts useful discriminative features for representing activities performedby humans and robots. Subspace based method like principal component analysisis used to extract low dimensional features from large voluminous activityimages. These low dimensional features are then used to classify an activity.An apparatus is designed using a webcam, which watches a robot replicating ahuman fall under indoor environment. In this apparatus, a robot performsvarious activities (like walking, bending, moving arms) replicating humans,which also includes a sudden fall. Experimental results on robot performingvarious activities and standard human activity recognition databases show theefficacy of our proposed method.
arxiv-15300-178 | Ensemble Robustness of Deep Learning Algorithms | http://arxiv.org/pdf/1602.02389v2.pdf | author:Jiashi Feng, Tom Zahavy, Bingyi Kang, Huan Xu, Shie Mannor category:cs.LG cs.CV stat.ML published:2016-02-07 summary:The question why deep learning algorithms perform so well in practice haspuzzled machine learning theoreticians and practitioners alike. However, mostof well-established approaches, such as hypothesis capacity, robustness orsparseness, have not provided complete explanations, due to the high complexityof the deep learning algorithms and their inherent randomness. In this work, weintroduce a new approach -- ensemble robustness -- towards characterizing thegeneralization performance of generic deep learning algorithms. Ensemblerobustness concerns robustness of the population of the hypotheses that may beoutput by a learning algorithm. Through the lens of ensemble robustness, wereveal that a stochastic learning algorithm can generalize well as long as itssensitiveness to adversarial perturbation is bounded in average, orequivalently, the performance variance of the algorithm is small. Quantifyingthe ensemble robustness of various deep learning algorithms may be difficultanalytically. However, extensive simulations for seven common deep learningalgorithms for different network architectures provide supporting evidence forour claims. In addition, as an example for utilizing ensemble robustness, wepropose a novel semi-supervised learning method that outperforms thestate-of-the-art. Furthermore, our work explains the good performance ofseveral published deep learning algorithms.
arxiv-15300-179 | Loss factorization, weakly supervised learning and label noise robustness | http://arxiv.org/pdf/1602.02450v2.pdf | author:Giorgio Patrini, Frank Nielsen, Richard Nock, Marcello Carioni category:cs.LG stat.ML published:2016-02-08 summary:We prove that the empirical risk of most well-known loss functions factorsinto a linear term aggregating all labels with a term that is label free, andcan further be expressed by sums of the loss. This holds true even fornon-smooth, non-convex losses and in any RKHS. The first term is a (kernel)mean operator --the focal quantity of this work-- which we characterize as thesufficient statistic for the labels. The result tightens known generalizationbounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. Inparticular, we demonstrate that algorithms like SGD and proximal methods can beadapted with minimal effort to handle weak supervision, once the mean operatorhas been estimated. We apply this idea to learning with asymmetric noisylabels, connecting and extending prior work. Furthermore, we show that mostlosses enjoy a data-dependent (by the mean operator) form of noise robustness,in contrast with known negative results.
arxiv-15300-180 | Image encryption with dynamic chaotic Look-Up Table | http://arxiv.org/pdf/1602.03205v1.pdf | author:Med Karim Abdmouleh, Ali Khalfallah, Med Salim Bouhlel category:cs.CR cs.CV published:2016-02-09 summary:In this paper we propose a novel image encryption scheme. The proposed methodis based on the chaos theory. Our cryptosystem uses the chaos theory to definea dynamic chaotic Look-Up Table (LUT) to compute the new value of the currentpixel to cipher. Applying this process on each pixel of the plain image, wegenerate the encrypted image. The results of different experimental tests, suchas Key space analysis, Information Entropy and Histogram analysis, show thatthe proposed encryption image scheme seems to be protected against variousattacks. A comparison between the plain and encrypted image, in terms ofcorrelation coefficient, proves that the plain image is very different from theencrypted one.
arxiv-15300-181 | Neural Random-Access Machines | http://arxiv.org/pdf/1511.06392v3.pdf | author:Karol Kurach, Marcin Andrychowicz, Ilya Sutskever category:cs.LG cs.NE published:2015-11-19 summary:In this paper, we propose and investigate a new neural network architecturecalled Neural Random Access Machine. It can manipulate and dereference pointersto an external variable-size random-access memory. The model is trained frompure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whosesolutions require pointer manipulation and dereferencing. Our results show thatthe proposed model can learn to solve algorithmic tasks of such type and iscapable of operating on simple data structures like linked-lists and binarytrees. For easier tasks, the learned solutions generalize to sequences ofarbitrary length. Moreover, memory access during inference can be done in aconstant time under some assumptions.
arxiv-15300-182 | A Controller-Recognizer Framework: How necessary is recognition for control? | http://arxiv.org/pdf/1511.06428v4.pdf | author:Marcin Moczulski, Kelvin Xu, Aaron Courville, Kyunghyun Cho category:cs.LG cs.CV published:2015-11-19 summary:Recently there has been growing interest in building active visual objectrecognizers, as opposed to the usual passive recognizers which classifies agiven static image into a predefined set of object categories. In this paper wepropose to generalize these recently proposed end-to-end active visualrecognizers into a controller-recognizer framework. A model in thecontroller-recognizer framework consists of a controller, which interfaces withan external manipulator, and a recognizer which classifies the visual inputadjusted by the manipulator. We describe two most recently proposedcontroller-recognizer models: recurrent attention model and spatial transformernetwork as representative examples of controller-recognizer models. Based onthis description we observe that most existing end-to-endcontroller-recognizers tightly, or completely, couple a controller andrecognizer. We ask a question whether this tight coupling is necessary, and tryto answer this empirically by building a controller-recognizer model with adecoupled controller and recognizer. Our experiments revealed that it is notalways necessary to tightly couple them and that by decoupling a controller andrecognizer, there is a possibility of building a generic controller that ispretrained and works together with any subsequent recognizer.
arxiv-15300-183 | Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization | http://arxiv.org/pdf/1601.07213v2.pdf | author:Alexander G. Ororbia II, C. Lee Giles, Daniel Kifer category:cs.LG cs.NE published:2016-01-26 summary:We present DataGrad, a general back-propagation style training procedure fordeep neural architectures that uses regularization of a deep Jacobian-basedpenalty. It can be viewed as a deep extension of the layerwise contractiveauto-encoder penalty. More importantly, it unifies previous proposals foradversarial training of deep neural nets -- this list includes directlymodifying the gradient, training on a mix of original and adversarial examples,using contractive penalties, and approximately optimizing constrainedadversarial objective functions. In an experiment using a Deep Sparse RectifierNetwork, we find that the deep Jacobian regularization of DataGrad (which alsohas L1 and L2 flavors of regularization) outperforms traditional L1 and L2regularization both on the original dataset as well as on adversarial examples.
arxiv-15300-184 | DCM Bandits: Learning to Rank with Multiple Clicks | http://arxiv.org/pdf/1602.03146v1.pdf | author:Sumeet Katariya, Branislav Kveton, Csaba Szepesvári, Zheng Wen category:cs.LG stat.ML published:2016-02-09 summary:Search engines recommend a list of web pages. The user examines this list,from the first page to the last, and may click on multiple attractive pages.This type of user behavior can be modeled by the \emph{dependent click model(DCM)}. In this work, we propose \emph{DCM bandits}, an online learning variantof the DCM model where the objective is to maximize the probability ofrecommending a satisfactory item. The main challenge of our problem is that thelearning agent does not observe the reward. It only observes the clicks. Thisimbalance between the feedback and rewards makes our setting challenging. Wepropose a computationally-efficient learning algorithm for our problem, whichwe call dcmKL-UCB; derive gap-dependent upper bounds on its regret underreasonable assumptions; and prove a matching lower bound up to logarithmicfactors. We experiment with dcmKL-UCB on both synthetic and real-worldproblems. Our algorithm outperforms a range of baselines and performs well evenwhen our modeling assumptions are violated. To the best of our knowledge, thisis the first regret-optimal online learning algorithm for learning to rank withmultiple clicks in a cascade-like model.
arxiv-15300-185 | A New Spatio-Spectral Morphological Segmentation For Multi-Spectral Remote-Sensing Images | http://arxiv.org/pdf/1602.03145v1.pdf | author:Guillaume Noyel, Jesus Angulo, Dominique Jeulin category:cs.CV published:2016-02-09 summary:A general framework of spatio-spectral segmentation for multi-spectral imagesis introduced in this paper. The method is based on classification-drivenstochastic watershed (WS) by Monte Carlo simulations, and it gives more regularand reliable contours than standard WS. The present approach is decomposed intoseveral sequential steps. First, a dimensionality-reduction stage is performedusing the factor-correspondence analysis method. In this context, a new way toselect the factor axes (eigenvectors) according to their spatial information isintroduced. Then, a spectral classification produces a spectralpre-segmentation of the image. Subsequently, a probability density function(pdf) of contours containing spatial and spectral information is estimated bysimulation using a stochastic WS approach driven by the spectralclassification. The pdf of the contours is finally segmented by a WS controlledby markers from a regularization of the initial classification.
arxiv-15300-186 | Minimum Regret Search for Single- and Multi-Task Optimization | http://arxiv.org/pdf/1602.01064v2.pdf | author:Jan Hendrik Metzen category:stat.ML cs.IT cs.LG cs.RO math.IT published:2016-02-02 summary:We propose minimum regret search (MRS), a novel acquisition function forBayesian optimization. MRS bears similarities with information-theoreticapproaches such as entropy search (ES). However, while ES aims in each query atmaximizing the information gain with respect to the global maximum, MRS aims atminimizing the expected immediate regret of its ultimate recommendation for theoptimum. While empirically ES and MRS perform similar in most of the cases, MRSproduces fewer outliers with high regret than ES. We provide empirical resultsboth for a synthetic single-task optimization problem as well as for asimulated multi-task robotic control problem.
arxiv-15300-187 | Graphical Model Sketch | http://arxiv.org/pdf/1602.03105v1.pdf | author:Branislav Kveton, Hung Bui, Mohammad Ghavamzadeh, Georgios Theocharous, S. Muthukrishnan, Siqi Sun category:cs.DS cs.LG stat.ML published:2016-02-09 summary:Structured high-cardinality data arises in many domains and poses a majorchallenge for both modeling and inference, which is beyond current graphicalmodel frameworks. We view these data as a stream $(x^{(t)})_{t = 1}^n$ of $n$observations from an unknown distribution $P$, where $x^{(t)} \in [M]^K$ is a$K$-dimensional vector and $M$ is the cardinality of its entries, which is verylarge. Suppose that the graphical model $\mathcal{G}$ of $P$ is known, and let$\bar{P}$ be the maximum-likelihood estimate (MLE) of $P$ from $(x^{(t)})_{t =1}^n$ conditioned on $\mathcal{G}$. In this work, we design and analyzealgorithms that approximate $\bar{P}$ with $\hat{P}$, such that $\hat{P}(x)\approx \bar{P}(x)$ for any $x \in [M]^K$ with a high probability, andcrucially in the space independent of $M$. The key idea of our approximationsis to use the structure of $\mathcal{G}$ and approximately estimate its factorsby "sketches". The sketches hash high-cardinality variables using randomprojections. Our approximations are computationally and space efficient, beingindependent of $M$. Our error bounds are multiplicative and provably improveupon those of the count-min (CM) sketch, a state-of-the-art approach toestimating the frequency of values in a stream, in a class of naive Bayesmodels. We evaluate our algorithms on synthetic and real-world problems, andreport an order of magnitude improvements over the CM sketch.
arxiv-15300-188 | Coupled Depth Learning | http://arxiv.org/pdf/1501.04537v6.pdf | author:Mohammad Haris Baig, Lorenzo Torresani category:cs.CV published:2015-01-19 summary:In this paper we propose a method for estimating depth from a single imageusing a coarse to fine approach. We argue that modeling the fine depth detailsis easier after a coarse depth map has been computed. We express a global(coarse) depth map of an image as a linear combination of a depth basis learnedfrom training examples. The depth basis captures spatial and statisticalregularities and reduces the problem of global depth estimation to the task ofpredicting the input-specific coefficients in the linear combination. This isformulated as a regression problem from a holistic representation of the image.Crucially, the depth basis and the regression function are {\bf coupled} andjointly optimized by our learning scheme. We demonstrate that this results in asignificant improvement in accuracy compared to direct regression of depthpixel values or approaches learning the depth basis disjointly from theregression function. The global depth estimate is then used as a guidance by alocal refinement method that introduces depth details that were not captured atthe global level. Experiments on the NYUv2 and KITTI datasets show that ourmethod outperforms the existing state-of-the-art at a considerably lowercomputational cost for both training and testing.
arxiv-15300-189 | On the Quality of the Initial Basin in Overspecified Neural Networks | http://arxiv.org/pdf/1511.04210v2.pdf | author:Itay Safran, Ohad Shamir category:cs.LG stat.ML published:2015-11-13 summary:Deep learning, in the form of artificial neural networks, has achievedremarkable practical success in recent years, for a variety of difficultmachine learning applications. However, a theoretical explanation for thisremains a major open problem, since training neural networks involvesoptimizing a highly non-convex objective function, and is known to becomputationally hard in the worst case. In this work, we study the\emph{geometric} structure of the associated non-convex objective function, inthe context of ReLU networks and starting from a random initialization of thenetwork parameters. We identify some conditions under which it becomes morefavorable to optimization, in the sense of (i) High probability of initializingat a point from which there is a monotonically decreasing path to a globalminimum; and (ii) High probability of initializing at a basin (suitablydefined) with a small minimal objective value. A common theme in our results isthat such properties are more likely to hold for larger ("overspecified")networks, which accords with some recent empirical and theoreticalobservations.
arxiv-15300-190 | Bayesian nonparametric image segmentation using a generalized Swendsen-Wang algorithm | http://arxiv.org/pdf/1602.03048v1.pdf | author:Richard Yi Da Xu, Francois Caron, Arnaud Doucet category:stat.ML published:2016-02-09 summary:Unsupervised image segmentation aims at clustering the set of pixels of animage into spatially homogeneous regions. We introduce here a class of Bayesiannonparametric models to address this problem. These models are based on acombination of a Potts-like spatial smoothness component and a prior onpartitions which is used to control both the number and size of clusters. Thisclass of models is flexible enough to include the standard Potts model and themore recent Potts-Dirichlet Process model \cite{Orbanz2008}. More importantly,any prior on partitions can be introduced to control the global clusteringstructure so that it is possible to penalize small or large clusters ifnecessary. Bayesian computation is carried out using an original generalizedSwendsen-Wang algorithm. Experiments demonstrate that our method is competitivein terms of RAND\ index compared to popular image segmentation methods, such asmean-shift, and recent alternative Bayesian nonparametric models.
arxiv-15300-191 | Asymmetrically Weighted CCA And Hierarchical Kernel Sentence Embedding For Multimodal Retrieval | http://arxiv.org/pdf/1511.06267v4.pdf | author:Youssef Mroueh, Etienne Marcheret, Vaibhava Goel category:cs.LG published:2015-11-19 summary:Joint modeling of language and vision has been drawing increasing interest. Amultimodal data representation allowing for bidirectional retrieval of imagesby sentences and vice versa is a key aspect. In this paper we present threecontributions in canonical correlation analysis (CCA) based multimodalretrieval. Firstly, we show that an asymmetric weighting of the canonicalweights, while achieving a cross-view mapping from the search to the queryspace, it improves the retrieval performance. Secondly, we devise acomputationally efficient model selection - crucial to generalization andstability - in the framework of the Bjork Golub algorithm for regularized CCAvia spectral filtering. Finally, we introduce a Hierarchical Kernel SentenceEmbedding (HKSE) that approximates Kernel CCA for a special similarity kernelbetween words distributions. State of the art results are obtained on MSCOCOand Flickr benchmarks when these three techniques are used in conjunction.
arxiv-15300-192 | The Structured Weighted Violations Perceptron Algorithm | http://arxiv.org/pdf/1602.03040v1.pdf | author:Rotem Dror, Roi Reichart category:cs.LG published:2016-02-09 summary:We present the Structured Weighted Violations Perceptron (SWVP) algorithm, anew perceptron algorithm for structured prediction, that generalizes theCollins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the updaterule of SWVP explicitly exploits the internal structure of the predictedlabels. We prove that for linearly separable training sets, SWVP converges to aweight vector that separates the data, under certain conditions on theparameters of the algorithm. We further prove bounds for SWVP on: (a) thenumber of updates in the separable case; (b) mistakes in the non-separablecase; and (c) the probability to misclassify an unseen example(generalization), and show that for most SWVP variants these bounds are tighterthan those of the CSP special case. In synthetic data experiments where data isdrawn from a generative hidden variable model, SWVP provides substantialimprovements over CSP.
arxiv-15300-193 | Minimax Lower Bounds for Realizable Transductive Classification | http://arxiv.org/pdf/1602.03027v1.pdf | author:Ilya Tolstikhin, David Lopez-Paz category:stat.ML cs.LG published:2016-02-09 summary:Transductive learning considers a training set of $m$ labeled samples and atest set of $u$ unlabeled samples, with the goal of best labeling thatparticular test set. Conversely, inductive learning considers a training set of$m$ labeled samples drawn iid from $P(X,Y)$, with the goal of best labeling anyfuture samples drawn iid from $P(X)$. This comparison suggests thattransduction is a much easier type of inference than induction, but is thisreally the case? This paper provides a negative answer to this question, byproving the first known minimax lower bounds for transductive, realizable,binary classification. Our lower bounds show that $m$ should be at least$\Omega(d/\epsilon + \log(1/\delta)/\epsilon)$ when $\epsilon$-learning aconcept class $\mathcal{H}$ of finite VC-dimension $d<\infty$ with confidence$1-\delta$, for all $m \leq u$. This result draws three important conclusions.First, general transduction is as hard as general induction, since bothproblems have $\Omega(d/m)$ minimax values. Second, the use of unlabeled datadoes not help general transduction, since supervised learning algorithms suchas ERM and (Hanneke, 2015) match our transductive lower bounds while ignoringthe unlabeled test set. Third, our transductive lower bounds imply lower boundsfor semi-supervised learning, which add to the important discussion about therole of unlabeled data in machine learning.
arxiv-15300-194 | EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos | http://arxiv.org/pdf/1602.03012v1.pdf | author:Andru P. Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy category:cs.CV published:2016-02-09 summary:Surgical workflow recognition has numerous potential medical applications,such as the automatic indexing of surgical video databases and the optimizationof real-time OR scheduling, among others. As a result, phase recognition hasbeen studied in the context of several kinds of surgeries, such as cataract,neurological, and laparoscopic surgeries. In the literature, two types offeatures are typically used to perform this task: visual features and toolusage signals. However, the visual features used are mostly handcrafted.Furthermore, since additional equipment is needed to obtain the tool usagesignals automatically, they are usually collected via a tedious manualannotation process. In this paper, we propose a novel method for phaserecognition that uses a convolutional neural network (CNN) to automaticallylearn features from cholecystectomy videos and that relies uniquely on visualinformation. In previous studies, it has been shown that the tool signals canprovide valuable information in performing the phase recognition task. Thus, wepresent a novel CNN architecture, called EndoNet, that is designed to carry outthe phase recognition and tool presence detection tasks in a multi-task manner.To the best of our knowledge, this is the first work proposing to use a CNN formultiple recognition tasks on laparoscopic videos. Extensive experimentalcomparisons to other methods show that EndoNet yields state-of-the-art resultsfor both tasks.
arxiv-15300-195 | A Convolutional Attention Network for Extreme Summarization of Source Code | http://arxiv.org/pdf/1602.03001v1.pdf | author:Miltiadis Allamanis, Hao Peng, Charles Sutton category:cs.LG cs.CL cs.SE published:2016-02-09 summary:Attention mechanisms in neural networks have proved useful for problems inwhich the input and output do not have fixed dimension. Often there existfeatures that are locally translation invariant and would be valuable fordirecting the model's attention, but previous attentional architectures are notconstructed to learn such features specifically. We introduce an attentionalneural network that employs convolution on the input tokens to detect localtime-invariant and long-range topical attention features in a context-dependentway. We apply this architecture to the problem of extreme summarization ofsource code snippets into short, descriptive function name-like summaries.Using those features, the model sequentially generates a summary bymarginalizing over two attention mechanisms: one that predicts the next summarytoken based on the attention weights of the input tokens and another that isable to copy a code token as-is directly into the summary. We demonstrate ourconvolutional attention neural network's performance on 10 popular Javaprojects showing that it achieves better performance compared to previousattentional mechanisms.
arxiv-15300-196 | Face Recognition: Perspectives from the Real-World | http://arxiv.org/pdf/1602.02999v1.pdf | author:Bappaditya Mandal category:cs.CV published:2016-02-09 summary:In this paper, we analyze some of our real-world deployment of facerecognition (FR) systems for various applications and discuss the gaps betweenexpectations of the user and what the system can deliver. We evaluate some ofour proposed algorithms with ad-hoc modifications for applications such as FRon wearable devices (like Google Glass), monitoring of elderly people in seniorcitizens centers, FR of children in child care centers and face matchingbetween a scanned IC/passport face image and a few live webcam images forautomatic hotel/resort checkouts. We describe each of these applications, thechallenges involved and proposed solutions. Since FR is intuitive in nature andwe human beings use it for interactions with the outside world, people havehigh expectations of its performance in real-world scenarios. However, weanalyze and discuss here that it is not the case, machine recognition of facesfor each of these applications poses unique challenges and demands specificresearch components so as to adapt in the actual sites.
arxiv-15300-197 | The world as its own best controller: a case study with anthropomimetic robots | http://arxiv.org/pdf/1602.02990v1.pdf | author:Ralf Der, Georg Martius category:cs.RO cs.LG cs.SY I.2.9; I.2.6 published:2016-02-09 summary:With the accelerated development of robot technologies, optimal controlbecomes one of the central themes of research. In traditional approaches, thecontroller, by its internal functionality, finds appropriate actions on thebasis of the history of sensor values, guided by the goals, intentions,objectives, learning schemes, and so on planted into it. The idea is that thecontroller controls the world---the body plus its environment---as reliably aspossible. This paper advocates for a new paradigm of control, obtained bymaking the world control its controller in the first place. The paper presentsa solution with a controller that is devoid of any functionalities of its own,given by a fixed, explicit and context-free function of the recent history ofthe sensor values. When applying this controller to a muscle-tendon drivenarm-shoulder system from the Myorobotics toolkit, we observe a vast variety ofself-organized behavior patterns: when left alone, the arm realizespseudo-random sequences of different poses but one can also manipulate thesystem into definite motion patterns. But most interestingly, after attachingan object, the controller gets in a functional resonance with the object'sinternal dynamics: when given a half-filled bottle, the system spontaneouslystarts shaking the bottle so that maximum response from the dynamics of thewater is being generated. After attaching a pendulum to the arm, the controllerdrives the pendulum into a circular mode. In this way, the robot discoversaffordances of objects its body is interacting with. We also discussperspectives for using this controller paradigm for intention driven behaviorgeneration.
arxiv-15300-198 | Porting HTM Models to the Heidelberg Neuromorphic Computing Platform | http://arxiv.org/pdf/1505.02142v2.pdf | author:Sebastian Billaudelle, Subutai Ahmad category:q-bio.NC cs.NE published:2015-05-08 summary:Hierarchical Temporal Memory (HTM) is a computational theory of machineintelligence based on a detailed study of the neocortex. The HeidelbergNeuromorphic Computing Platform, developed as part of the Human Brain Project(HBP), is a mixed-signal (analog and digital) large-scale platform for modelingnetworks of spiking neurons. In this paper we present the first effort inporting HTM networks to this platform. We describe a framework for simulatingkey HTM operations using spiking network models. We then describe specificspatial pooling and temporal memory implementations, as well as simulationsdemonstrating that the fundamental properties are maintained. We discuss issuesin implementing the full set of plasticity rules using Spike-Timing DependentPlasticity (STDP), and rough place and route calculations. Although furtherwork is required, our initial studies indicate that it should be possible torun large-scale HTM networks (including plasticity rules) efficiently on theHeidelberg platform. More generally the exercise of porting high level HTMalgorithms to biophysical neuron models promises to be a fruitful area ofinvestigation for future studies.
arxiv-15300-199 | Spoofing detection under noisy conditions: a preliminary investigation and an initial database | http://arxiv.org/pdf/1602.02950v1.pdf | author:Xiaohai Tian, Zhizheng Wu, Xiong Xiao, Eng Siong Chng, Haizhou Li category:cs.LG cs.SD published:2016-02-09 summary:Spoofing detection for automatic speaker verification (ASV), which is todiscriminate between live speech and attacks, has received increasingattentions recently. However, all the previous studies have been done on theclean data without significant additive noise. To simulate the real-lifescenarios, we perform a preliminary investigation of spoofing detection underadditive noisy conditions, and also describe an initial database for this task.The noisy database is based on the ASVspoof challenge 2015 database andgenerated by artificially adding background noises at different signal-to-noiseratios (SNRs). Five different additive noises are included. Our preliminaryresults show that using the model trained from clean data, the systemperformance degrades significantly in noisy conditions. Phase-based feature ismore noise robust than magnitude-based features. And the systems performsignificantly differ under different noise scenarios.
arxiv-15300-200 | Causal Transfer in Machine Learning | http://arxiv.org/pdf/1507.05333v2.pdf | author:Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, Jonas Peters category:stat.ML published:2015-07-19 summary:Methods of domain adaptation try to combine knowledge from several relateddomains (or tasks) to improve performance on a test domain. Inspired by causalmethodology, we assume that the covariate shift assumption holds true for asubset of predictor variables: the conditional of the target variable giventhis subset of predictors is invariant over all tasks. We prove that in anadversarial setting using this subset for prediction is optimal if no examplesfrom the test task are observed. For a specific scenario, in which tasks aredrawn from a meta distribution, further optimality results are available. Weintroduce a practical method which allows for automatic inference of the abovesubset and provide corresponding code. We present results on synthetic datasets and a gene deletion data set.
arxiv-15300-201 | Hyperparameter optimization with approximate gradient | http://arxiv.org/pdf/1602.02355v2.pdf | author:Fabian Pedregosa category:stat.ML cs.LG math.OC published:2016-02-07 summary:Most models in machine learning contain at least one hyperparameter tocontrol for model complexity. Choosing an appropriate set of hyperparameters isboth crucial in terms of model accuracy and computationally challenging. Inthis work we propose an algorithm for the optimization of continuoushyperparameters using inexact gradient information. An advantage of this methodis that hyperparameters can be updated before model parameters have fullyconverged. We also give sufficient conditions for the global convergence ofthis method, based on regularity conditions of the involved functions andsummability of errors. Finally, we validate the empirical performance of thismethod on the estimation of regularization constants of L2-regularized logisticregression and kernel Ridge regression. Empirical benchmarks indicate that ourapproach is highly competitive with respect to state of the art methods.
arxiv-15300-202 | Challenges of Integrating A Priori Information Efficiently in the Discovery of Spatio-Temporal Objects in Large Databases | http://arxiv.org/pdf/1602.02938v1.pdf | author:Benjamin Schott, Johannes Stegmaier, Masanari Takamiya, Ralf Mikut category:cs.CV published:2016-02-09 summary:Using the knowledge discovery framework, it is possible to explore objectdatabases and extract groups of objects with highly heterogeneous movementbehavior by efficiently integrating a priori knowledge through interacting withthe framework. The whole process is modular expandable and is thereforeadaptive to any problem formulation. Further, the flexible use of differentinformation allocation processes reveal a great potential to efficientlyincorporate the a priori knowledge of different users in different ways.Therefore, the stepwise knowledge discovery process embedded in the knowledgediscovery framework is described in detail to point out the flexibility of sucha system incorporating object databases from different applications. Thedescribed framework can be used to gain knowledge out of object databases inmany different fields. This knowledge can be used to gain further insights andimprove the understanding of underlying phenomena. The functionality of theproposed framework is exemplarily demonstrated using a benchmark database basedon real biological object data.
arxiv-15300-203 | Turbocharging Mini-Batch K-Means | http://arxiv.org/pdf/1602.02934v1.pdf | author:James Newling, François Fleuret category:stat.ML cs.LG published:2016-02-09 summary:We propose an accelerated Mini-Batch k-means algorithm which combines threekey improvements. The first is a modified center update which results inconvergence to a local minimum in fewer iterations. The second is an adaptiveincrease of batchsize to meet an increasing requirement for centroid accuracy.The third is the inclusion of distance bounds based on the triangle inequality,which are used to eliminate distance calculations along the same lines asElkan's algorithm. The combination of the two latter constitutes a verypowerful scheme to reuse computation already done over samples untilstatistical accuracy requires the use of additional data points.
arxiv-15300-204 | Empirical Bayes Estimation for the Stochastic Blockmodel | http://arxiv.org/pdf/1405.6070v3.pdf | author:Shakira Suwan, Dominic S. Lee, Runze Tang, Daniel L. Sussman, Minh Tang, Carey E. Priebe category:stat.ME stat.ML published:2014-05-23 summary:Inference for the stochastic blockmodel is currently of burgeoning interestin the statistical community, as well as in various application domains asdiverse as social networks, citation networks, brain connectivity networks(connectomics), etc. Recent theoretical developments have shown that spectralembedding of graphs yields tractable distributional results; in particular, arandom dot product latent position graph formulation of the stochasticblockmodel informs a mixture of normal distributions for the adjacency spectralembedding. We employ this new theory to provide an empirical Bayes methodologyfor estimation of block memberships of vertices in a random graph drawn fromthe stochastic blockmodel, and demonstrate its practical utility. The posteriorinference is conducted using a Metropolis-within-Gibbs algorithm. The theoryand methods are illustrated through Monte Carlo simulation studies, both withinthe stochastic blockmodel and beyond, and experimental results on a Wikipediadata set are presented.
arxiv-15300-205 | Calculus of the exponent of Kurdyka-Łojasiewicz inequality and its applications to linear convergence of first-order methods | http://arxiv.org/pdf/1602.02915v1.pdf | author:Guoyin Li, Ting Kei Pong category:math.OC stat.ML published:2016-02-09 summary:In this paper, we study the Kurdyka-{\L}ojasiewicz (KL) exponent, animportant quantity for analyzing the convergence rate of first-order methods.Specifically, we develop various calculus rules to deduce the KL exponent ofnew (possibly nonconvex and nonsmooth) functions formed from functions withknown KL exponents. In addition, we show that the well-studied Luo-Tseng errorbound together with a mild assumption on the separation of stationary valuesimplies that the KL exponent is $\frac{1}{2}$. The Luo-Tseng error bound isknown to hold for a large class of concrete structured optimization problems,and thus we deduce the KL exponent of a large class of functions whoseexponents were previously unknown. Building upon this and the calculus rules,we are then able to show that for many convex or nonconvex optimization modelsfor applications, such as sparse recovery, their objective function's KLexponent is $\frac{1}{2}$. This includes the least squares problem withsmoothly clipped absolute deviation (SCAD) regularization or minimax concavepenalty (MCP) regularization and the logistic regression problem with $\ell_1$regularization. Since many existing local convergence rate analysis forfirst-order methods in the nonconvex scenario relies on the KL exponent, ourresults enable us to obtain explicit convergence rate for various first-ordermethods when they are applied to a large variety of practical optimizationmodels. Finally, we further illustrate how our results can be applied toanalyzing the local linear convergence rate of the proximal gradient algorithmand the inertial proximal algorithm for some specific models that arise insparse recovery.
arxiv-15300-206 | Generating Images with Perceptual Similarity Metrics based on Deep Networks | http://arxiv.org/pdf/1602.02644v2.pdf | author:Alexey Dosovitskiy, Thomas Brox category:cs.LG cs.CV cs.NE published:2016-02-08 summary:Image-generating machine learning models are typically trained with lossfunctions based on distance in the image space. This often leads toover-smoothed results. We propose a class of loss functions, which we call deepperceptual similarity metrics (DeePSiM), that mitigate this problem. Instead ofcomputing distances in the image space, we compute distances between imagefeatures extracted by deep neural networks. This metric better reflectsperceptually similarity of images and thus leads to better results. We showthree applications: autoencoder training, a modification of a variationalautoencoder, and inversion of deep convolutional networks. In all cases, thegenerated images look sharp and resemble natural images.
arxiv-15300-207 | Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data | http://arxiv.org/pdf/1602.02899v1.pdf | author:Ferhat Özgür Çatak category:cs.CR cs.LG published:2016-02-09 summary:Especially in the Big Data era, the usage of different classification methodsis increasing day by day. The success of these classification methods dependson the effectiveness of learning methods. Extreme learning machine (ELM)classification algorithm is a relatively new learning method built onfeed-forward neural-network. ELM classification algorithm is a simple and fastmethod that can create a model from high-dimensional data sets. Traditional ELMlearning algorithm implicitly assumes complete access to whole data set. Thisis a major privacy concern in most of cases. Sharing of private data (i.e.medical records) is prevented because of security concerns. In this research,we propose an efficient and secure privacy-preserving learning algorithm forELM classification over data that is vertically partitioned among severalparties. The new learning method preserves the privacy on numerical attributes,builds a classification model without sharing private data without disclosingthe data of each party to others.
arxiv-15300-208 | Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM | http://arxiv.org/pdf/1602.02888v1.pdf | author:Ferhat Özgür Çatak category:cs.LG published:2016-02-09 summary:In machine learning area, as the number of labeled input samples becomes verylarge, it is very difficult to build a classification model because of inputdata set is not fit in a memory in training phase of the algorithm, therefore,it is necessary to utilize data partitioning to handle overall data set.Bagging and boosting based data partitioning methods have been broadly used indata mining and pattern recognition area. Both of these methods have shown agreat possibility for improving classification model performance. This study isconcerned with the analysis of data set partitioning with noise removal and itsimpact on the performance of multiple classifier models. In this study, wepropose noise filtering preprocessing at each data set partition to incrementclassifier model performance. We applied Gini impurity approach to find thebest split percentage of noise filter ratio. The filtered sub data set is thenused to train individual ensemble models.
arxiv-15300-209 | Classification with Boosting of Extreme Learning Machine Over Arbitrarily Partitioned Data | http://arxiv.org/pdf/1602.02887v1.pdf | author:Ferhat Özgür Çatak category:cs.LG published:2016-02-09 summary:Machine learning based computational intelligence methods are widely used toanalyze large scale data sets in this age of big data. Extracting usefulpredictive modeling from these types of data sets is a challenging problem dueto their high complexity. Analyzing large amount of streaming data that can beleveraged to derive business value is another complex problem to solve. Withhigh levels of data availability (\textit{i.e. Big Data}) automaticclassification of them has become an important and complex task. Hence, weexplore the power of applying MapReduce based Distributed AdaBoosting ofExtreme Learning Machine (ELM) to build a predictive bag of classificationmodels. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm isused to build weak learners (classifier functions); and (iii) builds a stronglearner from a set of weak learners. We applied this training model to thebenchmark knowledge discovery and data mining data sets.
arxiv-15300-210 | Joint Defogging and Demosaicking | http://arxiv.org/pdf/1602.02885v1.pdf | author:Y. J. Lee, K. Hirakawa, T. Q. Nguyen category:cs.CV published:2016-02-09 summary:Image defogging is a technique used extensively for enhancing visual qualityof images in bad weather condition. Even though defogging algorithms have beenwell studied, defogging performance is degraded by demosaicking artifacts andsensor noise amplification in distant scenes. In order to improve visualquality of restored images, we propose a novel approach to perform defoggingand demosaicking simultaneously. We conclude that better defogging performancewith fewer artifacts can be achieved when a defogging algorithm is combinedwith a demosaicking algorithm simultaneously. We also demonstrate that theproposed joint algorithm has the benefit of suppressing noise amplification indistant scene. In addition, we validate our theoretical analysis andobservations for both synthesized datasets with ground truth fog-free imagesand natural scene datasets captured in a raw format.
arxiv-15300-211 | Detection and Visualization of Endoleaks in CT Data for Monitoring of Thoracic and Abdominal Aortic Aneurysm Stents | http://arxiv.org/pdf/1602.02881v1.pdf | author:Jing Lu, Jan Egger, Andreas Wimmer, Stefan Großkopf, Bernd Freisleben category:cs.CV cs.CG cs.GR published:2016-02-09 summary:In this paper we present an efficient algorithm for the segmentation of theinner and outer boundary of thoratic and abdominal aortic aneurysms (TAA & AAA)in computed tomography angiography (CTA) acquisitions. The aneurysmsegmentation includes two steps: first, the inner boundary is segmented basedon a grey level model with two thresholds; then, an adapted active contourmodel approach is applied to the more complicated outer boundary segmentation,with its initialization based on the available inner boundary segmentation. Anopacity image, which aims at enhancing important features while reducingspurious structures, is calculated from the CTA images and employed to guidethe deformation of the model. In addition, the active contour model is extendedby a constraint force that prevents intersections of the inner and outerboundary and keeps the outer boundary at a distance, given by the thrombusthickness, to the inner boundary. Based upon the segmentation results, we canmeasure the aneurysm size at each centerline point on the centerline orthogonalmultiplanar reformatting (MPR) plane. Furthermore, a 3D TAA or AAA model isreconstructed from the set of segmented contours, and the presence of endoleaksis detected and highlighted. The implemented method has been evaluated on nineclinical CTA data sets with variations in anatomy and location of the pathologyand has shown promising results.
arxiv-15300-212 | Value Iteration Networks | http://arxiv.org/pdf/1602.02867v1.pdf | author:Aviv Tamar, Sergey Levine, Pieter Abbeel category:cs.AI cs.LG cs.NE stat.ML published:2016-02-09 summary:We introduce the value iteration network: a fully differentiable neuralnetwork with a `planning module' embedded within. Value iteration networks aresuitable for making predictions about outcomes that involve planning-basedreasoning, such as predicting a desired trajectory from an observation of amap. Key to our approach is a novel differentiable approximation of thevalue-iteration algorithm, which can be represented as a convolutional neuralnetwork, and trained end-to-end using standard backpropagation. We evaluate ourvalue iteration networks on the task of predicting optimal obstacle-avoidingtrajectories from an image of a landscape, both on synthetic data, and onchallenging raw images of the Mars terrain.
arxiv-15300-213 | The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks | http://arxiv.org/pdf/1602.02865v1.pdf | author:Babak Saleh, Ahmed Elgammal, Jacob Feldman category:cs.CV cs.LG cs.NE published:2016-02-09 summary:Deep artificial neural networks have made remarkable progress in differenttasks in the field of computer vision. However, the empirical analysis of thesemodels and investigation of their failure cases has received attentionrecently. In this work, we show that deep learning models cannot generalize toatypical images that are substantially different from training images. This isin contrast to the superior generalization ability of the visual system in thehuman brain. We focus on Convolutional Neural Networks (CNN) as thestate-of-the-art models in object recognition and classification; investigatethis problem in more detail, and hypothesize that training CNN models sufferfrom unstructured loss minimization. We propose computational models to improvethe generalization capacity of CNNs by considering how typical a training imagelooks like. By conducting an extensive set of experiments we show thatinvolving a typicality measure can improve the classification results on a newset of images by a large margin. More importantly, this significant improvementis achieved without fine-tuning the CNN model on the target image set.
arxiv-15300-214 | A Feature-Based Prediction Model of Algorithm Selection for Constrained Continuous Optimisation | http://arxiv.org/pdf/1602.02862v1.pdf | author:Shayan Poursoltan, Frank Neumann category:cs.NE published:2016-02-09 summary:With this paper, we contribute to the growing research area of feature-basedanalysis of bio-inspired computing. In this research area, problem instancesare classified according to different features of the underlying problem interms of their difficulty of being solved by a particular algorithm. Weinvestigate the impact of different sets of evolved instances for buildingprediction models in the area of algorithm selection. Building on the work ofPoursoltan and Neumann [11,10], we consider how evolved instances can be usedto predict the best performing algorithm for constrained continuousoptimisation from a set of bio-inspired computing methods, namely highperforming variants of differential evolution, particle swarm optimization, andevolution strategies. Our experimental results show that instances evolved witha multi-objective approach in combination with random instances of theunderlying problem allow to build a model that accurately predicts the bestperforming algorithm for a wide range of problem instances.
arxiv-15300-215 | Compliance-Aware Bandits | http://arxiv.org/pdf/1602.02852v1.pdf | author:Nicolás Della Penna, Mark D. Reid, David Balduzzi category:stat.ML cs.LG published:2016-02-09 summary:Motivated by clinical trials, we study bandits with observablenon-compliance. At each step, the learner chooses an arm, after, instead ofobserving only the reward, it also observes the action that took place. We showthat such noncompliance can be helpful or hurtful to the learner in general.Unfortunately, naively incorporating compliance information into banditalgorithms loses guarantees on sublinear regret. We present hybrid algorithmsthat maintain regret bounds up to a multiplicative factor and can incorporatecompliance information. Simulations based on real data from the InternationalStoke Trial show the practical potential of these algorithms.
arxiv-15300-216 | Toward Optimal Feature Selection in Naive Bayes for Text Categorization | http://arxiv.org/pdf/1602.02850v1.pdf | author:Bo Tang, Steven Kay, Haibo He category:stat.ML cs.CL cs.IR cs.LG published:2016-02-09 summary:Automated feature selection is important for text categorization to reducethe feature size and to speed up the learning process of classifiers. In thispaper, we present a novel and efficient feature selection framework based onthe Information Theory, which aims to rank the features with theirdiscriminative capacity for classification. We first revisit two informationmeasures: Kullback-Leibler divergence and Jeffreys divergence for binaryhypothesis testing, and analyze their asymptotic properties relating to type Iand type II errors of a Bayesian classifier. We then introduce a new divergencemeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measuremulti-distribution divergence for multi-class classification. Based on theJMH-divergence, we develop two efficient feature selection methods, termedmaximum discrimination ($MD$) and $MD-\chi^2$ methods, for text categorization.The promising results of extensive experiments demonstrate the effectiveness ofthe proposed approaches.
arxiv-15300-217 | Collaborative filtering via sparse Markov random fields | http://arxiv.org/pdf/1602.02842v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG published:2016-02-09 summary:Recommender systems play a central role in providing individualized access toinformation and services. This paper focuses on collaborative filtering, anapproach that exploits the shared structure among mind-liked users and similaritems. In particular, we focus on a formal probabilistic framework known asMarkov random fields (MRF). We address the open problem of structure learningand introduce a sparsity-inducing algorithm to automatically estimate theinteraction structures between users and between items. Item-item and user-usercorrelation networks are obtained as a by-product. Large-scale experiments onmovie recommendation and date matching datasets demonstrate the power of theproposed method.
arxiv-15300-218 | Machine Learning Model of the Swift/BAT Trigger Algorithm for Long GRB Population Studies | http://arxiv.org/pdf/1509.01228v2.pdf | author:Philip B Graff, Amy Y Lien, John G Baker, Takanori Sakamoto category:astro-ph.HE stat.ML published:2015-09-03 summary:To draw inferences about gamma-ray burst (GRB) source populations based onSwift observations, it is essential to understand the detection efficiency ofthe Swift burst alert telescope (BAT). This study considers the problem ofmodeling the Swift/BAT triggering algorithm for long GRBs, a computationallyexpensive procedure, and models it using machine learning algorithms. A largesample of simulated GRBs from Lien 2014 is used to train various models: randomforests, boosted decision trees (with AdaBoost), support vector machines, andartificial neural networks. The best models have accuracies of $\gtrsim97\%$($\lesssim 3\%$ error), which is a significant improvement on a cut in GRB fluxwhich has an accuracy of $89.6\%$ ($10.4\%$ error). These models are then usedto measure the detection efficiency of Swift as a function of redshift $z$,which is used to perform Bayesian parameter estimation on the GRB ratedistribution. We find a local GRB rate density of $n_0 \sim0.48^{+0.41}_{-0.23} \ {\rm Gpc}^{-3} {\rm yr}^{-1}$ with power-law indices of$n_1 \sim 1.7^{+0.6}_{-0.5}$ and $n_2 \sim -5.9^{+5.7}_{-0.1}$ for GRBs aboveand below a break point of $z_1 \sim 6.8^{+2.8}_{-3.2}$. This methodology isable to improve upon earlier studies by more accurately modeling Swiftdetection and using this for fully Bayesian model fitting. The code used inthis is analysis is publicly available online(https://github.com/PBGraff/SwiftGRB_PEanalysis).
arxiv-15300-219 | Poor starting points in machine learning | http://arxiv.org/pdf/1602.02823v1.pdf | author:Mark Tygert category:cs.LG cs.NE math.OC stat.ML published:2016-02-09 summary:Poor (even random) starting points for learning/training/optimization arecommon in machine learning. In many settings, the method of Robbins and Monro(online stochastic gradient descent) is known to be optimal for good startingpoints, but may not be optimal for poor starting points -- indeed, for poorstarting points Nesterov acceleration can help during the initial iterations,even though Nesterov methods not designed for stochastic approximation couldhurt during later iterations. The common practice of training with nontrivialminibatches enhances the advantage of Nesterov acceleration.
arxiv-15300-220 | Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes On Second Order Statistics | http://arxiv.org/pdf/1602.02822v1.pdf | author:Xiyang Dai, Sameh Khamis, Yangmuzi Zhang, Larry S. Davis category:cs.CV published:2016-02-09 summary:Sparse representations have been successfully applied to signal processing,computer vision and machine learning. Currently there is a trend to learnsparse models directly on structure data, such as region covariance. However,such methods when combined with region covariance often require complexcomputation. We present an approach to transform a structured sparse modellearning problem to a traditional vectorized sparse modeling problem byconstructing a Euclidean space representation for region covariance matrices.Our new representation has multiple advantages. Experiments on several visiontasks demonstrate competitive performance with the state-of-the-art methods.
arxiv-15300-221 | Learning in games via reinforcement and regularization | http://arxiv.org/pdf/1407.6267v2.pdf | author:Panayotis Mertikopoulos, William H. Sandholm category:math.OC cs.GT cs.LG published:2014-07-23 summary:We investigate a class of reinforcement learning dynamics where playersadjust their strategies based on their actions' cumulative payoffs over time -specifically, by playing mixed strategies that maximize their expectedcumulative payoff minus a regularization term. A widely studied example isexponential reinforcement learning, a process induced by an entropicregularization term which leads mixed strategies to evolve according to thereplicator dynamics. However, in contrast to the class of regularizationfunctions used to define smooth best responses in models of stochasticfictitious play, the functions used in this paper need not be infinitely steepat the boundary of the simplex; in fact, dropping this requirement gives riseto an important dichotomy between steep and nonsteep cases. In this generalframework, we extend several properties of exponential learning, including theelimination of dominated strategies, the asymptotic stability of strict Nashequilibria, and the convergence of time-averaged trajectories in zero-sum gameswith an interior Nash equilibrium.
arxiv-15300-222 | Local and Global Convergence of a General Inertial Proximal Splitting Scheme | http://arxiv.org/pdf/1602.02726v1.pdf | author:Patrick R. Johnstone, Pierre Moulin category:math.OC cs.LG math.NA published:2016-02-08 summary:This paper is concerned with convex composite minimization problems in aHilbert space. In these problems, the objective is the sum of two closed,proper, and convex functions where one is smooth and the other admits acomputationally inexpensive proximal operator. We analyze a general family ofinertial proximal splitting algorithms (GIPSA) for solving such problems. Weestablish finiteness of the sum of squared increments of the iterates andoptimality of the accumulation points. Weak convergence of the entire sequencethen follows if the minimum is attained. Our analysis unifies and extendsseveral previous results. We then focus on $\ell_1$-regularized optimization, which is the ubiquitousspecial case where the nonsmooth term is the $\ell_1$-norm. For certainparameter choices, GIPSA is amenable to a local analysis for this problem. Forthese choices we show that GIPSA achieves finite "active manifoldidentification", i.e. convergence in a finite number of iterations to theoptimal support and sign, after which GIPSA reduces to minimizing a localsmooth function. Local linear convergence then holds under certain conditions.We determine the rate in terms of the inertia, stepsize, and local curvature.Our local analysis is applicable to certain recent variants of the FastIterative Shrinkage-Thresholding Algorithm (FISTA), for which we establishactive manifold identification and local linear convergence. Our analysismotivates the use of a momentum restart scheme in these FISTA variants toobtain the optimal local linear convergence rate.
arxiv-15300-223 | Multimodal Remote Sensing Image Registration with Accuracy Estimation at Local and Global Scales | http://arxiv.org/pdf/1602.02720v1.pdf | author:M. L. Uss, B. Vozel, V. V. Lukin, K. Chehdi category:cs.CV published:2016-02-08 summary:This paper investigates and takes advantage of estimation of registrationaccuracy for mono- and multi-modal pairs of remote sensing images, following anintegrated framework from local to global scales. At the local scale, theCramer-Rao lower bound on parameter estimation error is estimated forcharacterizing registration accuracy of local fragment correspondence between acoarsely registered pair of images. Each local correspondence is assigned itsestimated registration accuracy dependent on local image texture and noiseproperties. Opposite to the standard approach, where registration accuracy isfound a posteriori at the output of the registration process, such valuableinformation is used by us as additional a priori information in theregistration process at global scale. It greatly helps detecting and discardingoutliers and refining the estimation of geometrical transformation modelparameters. Based on these ideas, a new area-based registration method calledRAE (Registration with Accuracy Estimation) is proposed. The RAE method is ableto provide registration accuracy at the global scale as error estimationcovariance matrix of geometrical transformation model parameters that can beused to estimate point-wise registration Standard Deviation (SD). This accuracydoes not rely on any ground truth and characterizes each pair of registeredimages individually. The RAE method is proved successful with reaching subpixelaccuracy while registering the three complex multimodal and multitemporal imagepairs: optical to radar, optical to Digital Elevation Model (DEM) images andDEM to radar images. Other methods employed in comparisons fail to provideaccurate results on the same test cases.
arxiv-15300-224 | Indistinguishable Bandits Dueling with Decoys on a Poset | http://arxiv.org/pdf/1602.02706v1.pdf | author:Julien Audiffren, Ralaivola Liva category:cs.LG cs.AI published:2016-02-08 summary:We adress the problem of dueling bandits defined on partially ordered sets,or posets. In this setting, arms may not be comparable, and there may beseveral (incomparable) optimal arms. We propose an algorithm, UnchainedBandits,that efficiently finds the set of optimal arms of any poset even when pairs ofcomparable arms cannot be distinguished from pairs of incomparable arms, with aset of minimal assumptions. This algorithm relies on the concept of decoys,which stems from social psychology. For the easier case where theincomparability information may be accessible, we propose a second algorithm,SlicingBandits, which takes advantage of this information and achieves a verysignificant gain of performance compared to UnchainedBandits. We providetheoretical guarantees and experimental evaluation for both algorithms.
arxiv-15300-225 | Compressed Online Dictionary Learning for Fast fMRI Decomposition | http://arxiv.org/pdf/1602.02701v1.pdf | author:Arthur Mensch, Gaël Varoquaux, Bertrand Thirion category:stat.ML cs.LG published:2016-02-08 summary:We present a method for fast resting-state fMRI spatial decomposi-tions ofvery large datasets, based on the reduction of the temporal dimension beforeapplying dictionary learning on concatenated individual records from groups ofsubjects. Introducing a measure of correspondence between spatialdecompositions of rest fMRI, we demonstrates that time-reduced dictionarylearning produces result as reliable as non-reduced decompositions. We alsoshow that this reduction significantly improves computational scalability.
arxiv-15300-226 | Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks | http://arxiv.org/pdf/1602.02685v1.pdf | author:Cristóbal Esteban, Oliver Staeck, Yinchong Yang, Volker Tresp category:cs.LG cs.AI cs.NE published:2016-02-08 summary:In clinical data sets we often find static information (e.g. gender of thepatients, blood type, etc.) combined with sequences of data that are recordedduring multiple hospital visits (e.g. medications prescribed, tests performed,etc.). Recurrent Neural Networks (RNNs) have proven to be very successful formodelling sequences of data in many areas of Machine Learning. In this work wepresent an approach based on RNNs that is specifically designed for theclinical domain and that combines static and dynamic information in order topredict future events. We work with a database collected in the Charit\'{e}Hospital in Berlin that contains all the information concerning patients thatunderwent a kidney transplantation. After the transplantation three mainendpoints can occur: rejection of the kidney, loss of the kidney and death ofthe patient. Our goal is to predict, given the Electronic Health Record of eachpatient, whether any of those endpoints will occur within the next six ortwelve months after each visit to the clinic. We compared different types ofRNNs that we developed for this work, a model based on a Feedforward NeuralNetwork and a Logistic Regression model. We found that the RNN that wedeveloped based on Gated Recurrent Units provides the best performance for thistask. We also performed an additional experiment using these models to predictnext actions and found that for such use case the model based on a FeedforwardNeural Network outperformed the other models. Our hypothesis is that long-termdependencies are not as relevant in this task.
arxiv-15300-227 | Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks | http://arxiv.org/pdf/1602.02672v1.pdf | author:Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson category:cs.AI cs.LG published:2016-02-08 summary:We propose deep distributed recurrent Q-networks (DDRQN), which enable teamsof agents to learn to solve communication-based coordination tasks. In thesetasks, the agents are not given any pre-designed communication protocol.Therefore, in order to successfully communicate, they must first automaticallydevelop and agree upon their own communication protocol. We present empiricalresults on two multi-agent learning problems based on well-known riddles,demonstrating that DDRQN can successfully solve such tasks and discover elegantcommunication protocols to do so. To our knowledge, this is the first time deepreinforcement learning has succeeded in learning communication protocols. Inaddition, we present ablation experiments that confirm that each of the maincomponents of the DDRQN architecture are critical to its success.
arxiv-15300-228 | The happiness paradox: your friends are happier than you | http://arxiv.org/pdf/1602.02665v1.pdf | author:Johan Bollen, Bruno Gonçalves, Ingrid van de Leemput, Guangchen Ruan category:cs.SI cs.CL cs.HC physics.soc-ph published:2016-02-08 summary:Most individuals in social networks experience a so-called FriendshipParadox: they are less popular than their friends on average. This effect mayexplain recent findings that widespread social network media use leads toreduced happiness. However the relation between popularity and happiness ispoorly understood. A Friendship paradox does not necessarily imply a Happinessparadox where most individuals are less happy than their friends. Here wereport the first direct observation of a significant Happiness Paradox in alarge-scale online social network of $39,110$ Twitter users. Our results revealthat popular individuals are indeed happier and that a majority of individualsexperience a significant Happiness paradox. The magnitude of the latter effectis shaped by complex interactions between individual popularity, happiness, andthe fact that users cluster assortatively by level of happiness. Our resultsindicate that the topology of online social networks and the distribution ofhappiness in some populations can cause widespread psycho-social effects thataffect the well-being of billions of individuals.
arxiv-15300-229 | A Variational Analysis of Stochastic Gradient Algorithms | http://arxiv.org/pdf/1602.02666v1.pdf | author:Stephan Mandt, Matthew D. Hoffman, David M. Blei category:stat.ML cs.LG published:2016-02-08 summary:Stochastic Gradient Descent (SGD) is an important algorithm in machinelearning. With constant learning rates, it is a stochastic process that, afteran initial phase of convergence, generates samples from a stationarydistribution. We show that SGD with constant rates can be effectively used asan approximate posterior inference algorithm for probabilistic modeling.Specifically, we show how to adjust the tuning parameters of SGD such as tomatch the resulting stationary distribution to the posterior. This analysisrests on interpreting SGD as a continuous-time stochastic process and thenminimizing the Kullback-Leibler divergence between its stationary distributionand the target posterior. (This is in the spirit of variational inference.) Inmore detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and thenuse properties of this process to derive the optimal parameters. Thistheoretical framework also connects SGD to modern scalable inferencealgorithms; we analyze the recently proposed stochastic gradient Fisher scoringunder this perspective. We demonstrate that SGD with properly chosen constantrates gives a new way to optimize hyperparameters in probabilistic models.
arxiv-15300-230 | Exploiting Cyclic Symmetry in Convolutional Neural Networks | http://arxiv.org/pdf/1602.02660v1.pdf | author:Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu category:cs.LG cs.CV cs.NE published:2016-02-08 summary:Many classes of images exhibit rotational symmetry. Convolutional neuralnetworks are sometimes trained using data augmentation to exploit this, butthey are still required to learn the rotation equivariance properties from thedata. Encoding these properties into the network architecture, as we arealready used to doing for translation equivariance by using convolutionallayers, could result in a more efficient use of the parameter budget byrelieving the model from learning them. We introduce four operations which canbe inserted into neural network models as layers, and which can be combined tomake these models partially equivariant to rotations. They also enableparameter sharing across different orientations. We evaluate the effect ofthese architectural modifications on three datasets which exhibit rotationalsymmetry and demonstrate improved performance with smaller models.
arxiv-15300-231 | LSTM Deep Neural Networks Postfiltering for Improving the Quality of Synthetic Voices | http://arxiv.org/pdf/1602.02656v1.pdf | author:Marvin Coto-Jiménez, John Goddard-Close category:cs.SD cs.NE published:2016-02-08 summary:Recent developments in speech synthesis have produced systems capable ofoutcome intelligible speech, but now researchers strive to create models thatmore accurately mimic human voices. One such development is the incorporationof multiple linguistic styles in various languages and accents. HMM-based Speech Synthesis is of great interest to many researchers, due toits ability to produce sophisticated features with small footprint. Despitesuch progress, its quality has not yet reached the level of the predominantunit-selection approaches that choose and concatenate recordings of realspeech. Recent efforts have been made in the direction of improving thesesystems. In this paper we present the application of Long-Short Term Memory DeepNeural Networks as a Postfiltering step of HMM-based speech synthesis, in orderto obtain closer spectral characteristics to those of natural speech. Theresults show how HMM-voices could be improved using this approach.
arxiv-15300-232 | Automatic Face Reenactment | http://arxiv.org/pdf/1602.02651v1.pdf | author:Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen, Patrick Perez, Christian Theobalt category:cs.CV cs.GR published:2016-02-08 summary:We propose an image-based, facial reenactment system that replaces the faceof an actor in an existing target video with the face of a user from a sourcevideo, while preserving the original target performance. Our system is fullyautomatic and does not require a database of source expressions. Instead, it isable to produce convincing reenactment results from a short source videocaptured with an off-the-shelf camera, such as a webcam, where the userperforms arbitrary facial gestures. Our reenactment pipeline is conceived aspart image retrieval and part face transfer: The image retrieval is based ontemporal clustering of target frames and a novel image matching metric thatcombines appearance and motion to select candidate frames from the sourcevideo, while the face transfer uses a 2D warping strategy that preserves theuser's identity. Our system excels in simplicity as it does not rely on a 3Dface model, it is robust under head motion and does not require the source andtarget performance to be similar. We show convincing reenactment results forvideos that we recorded ourselves and for low-quality footage taken from theInternet.
arxiv-15300-233 | Guarantees in Wasserstein Distance for the Langevin Monte Carlo Algorithm | http://arxiv.org/pdf/1602.02616v1.pdf | author:Thomas Bonis category:stat.CO math.ST stat.ML stat.TH published:2016-02-08 summary:We study the problem of sampling from a distribution $\target$ using theLangevin Monte Carlo algorithm and provide rate of convergences for thisalgorithm in terms of Wasserstein distance of order $2$. Our result holds aslong as the continuous diffusion process associated with the algorithmconverges exponentially fast to the target distribution along with sometechnical assumptions. While such an exponential convergence holds for examplein the log-concave measure case, it also holds for the more general case ofasymptoticaly log-concave measures. Our results thus extends the known rates ofconvergence in total variation and Wasserstein distances which have only beenobtained in the log-concave case. Moreover, using a sharper approximation boundof the continuous process, we obtain better asymptotic rates than traditionalresults. We also look into variations of the Langevin Monte Carlo algorithmusing other discretization schemes. In a first time, we look into the use ofthe Ozaki's discretization but are unable to obtain any significativeimprovement in terms of convergence rates compared to the Euler's scheme. Wethen provide a (sub-optimal) way to study more general schemes, however ourapproach only holds for the log-concave case.
arxiv-15300-234 | God(s) Know(s): Developmental and Cross-Cultural Patterns in Children Drawings | http://arxiv.org/pdf/1511.03466v2.pdf | author:Ksenia Konyushkova, Nikolaos Arvanitopoulos, Zhargalma Dandarova Robert, Pierre-Yves Brandt, Sabine Süsstrunk category:cs.CV published:2015-11-11 summary:This paper introduces a novel approach to data analysis designed for theneeds of specialists in psychology of religion. We detect developmental andcross-cultural patterns in children's drawings of God(s) and other supernaturalagents. We develop methods to objectively evaluate our empirical observationsof the drawings with respect to: (1) the gravity center, (2) the averageintensities of the colors \emph{green} and \emph{yellow}, (3) the use ofdifferent colors (palette) and (4) the visual complexity of the drawings. Wefind statistically significant differences across ages and countries in thegravity centers and in the average intensities of colors. These findingssupport the hypotheses of the experts and raise new questions for furtherinvestigation.
arxiv-15300-235 | Tumour ROI Estimation in Ultrasound Images via Radon Barcodes in Patients with Locally Advanced Breast Cancer | http://arxiv.org/pdf/1602.02586v1.pdf | author:Hamid R. Tizhoosh, Mehrdad J. Gangeh, Hadi Tadayyon, Gregory J. Czarnota category:cs.CV published:2016-02-08 summary:Quantitative ultrasound (QUS) methods provide a promising framework that cannon-invasively and inexpensively be used to predict or assess the tumourresponse to cancer treatment. The first step in using the QUS methods is toselect a region of interest (ROI) inside the tumour in ultrasound images.Manual segmentation, however, is very time consuming and tedious. In thispaper, a semi-automated approach will be proposed to roughly localize an ROIfor a tumour in ultrasound images of patients with locally advanced breastcancer (LABC). Content-based barcodes, a recently introduced binary descriptorbased on Radon transform, were used in order to find similar cases and estimatea bounding box surrounding the tumour. Experiments with 33 B-scan imagesresulted in promising results with an accuracy of $81\%$.
arxiv-15300-236 | Homogeneity of Cluster Ensembles | http://arxiv.org/pdf/1602.02543v1.pdf | author:Brijnesh J. Jain category:cs.LG cs.CV published:2016-02-08 summary:The expectation and the mean of partitions generated by a cluster ensembleare not unique in general. This issue poses challenges in statistical inferenceand cluster stability. In this contribution, we state sufficient conditions foruniqueness of expectation and mean. The proposed conditions show that a uniquemean is neither exceptional nor generic. To cope with this issue, we introducehomogeneity as a measure of how likely is a unique mean for a sample ofpartitions. We show that homogeneity is related to cluster stability. Thisresult points to a possible conflict between cluster stability and diversity inconsensus clustering. To assess homogeneity in a practical setting, we proposean efficient way to compute a lower bound of homogeneity. Empirical resultsusing the k-means algorithm suggest that uniqueness of the mean partition isnot exceptional for real-world data. Moreover, for samples of high homogeneity,uniqueness can be enforced by increasing the number of data points or byremoving outlier partitions. In a broader context, this contribution can beplaced as a further step towards a statistical theory of partitions.
arxiv-15300-237 | Using Shortlists to Support Decision Making and Improve Recommender System Performance | http://arxiv.org/pdf/1510.07545v2.pdf | author:Tobias Schnabel, Paul N. Bennett, Susan T. Dumais, Thorsten Joachims category:cs.HC cs.IR cs.LG published:2015-10-26 summary:In this paper, we study shortlists as an interface component for recommendersystems with the dual goal of supporting the user's decision process, as wellas improving implicit feedback elicitation for increased recommendationquality. A shortlist is a temporary list of candidates that the user iscurrently considering, e.g., a list of a few movies the user is currentlyconsidering for viewing. From a cognitive perspective, shortlists serve asdigital short-term memory where users can off-load the items underconsideration -- thereby decreasing their cognitive load. From a machinelearning perspective, adding items to the shortlist generates a new implicitfeedback signal as a by-product of exploration and decision making which canimprove recommendation quality. Shortlisting therefore provides additional datafor training recommendation systems without the increases in cognitive loadthat requesting explicit feedback would incur. We perform an user study with a movie recommendation setup to compareinterfaces that offer shortlist support with those that do not. From the userstudies we conclude: (i) users make better decisions with a shortlist; (ii)users prefer an interface with shortlist support; and (iii) the additionalimplicit feedback from sessions with a shortlist improves the quality ofrecommendations by nearly a factor of two.
arxiv-15300-238 | Data-Efficient Reinforcement Learning in Continuous-State POMDPs | http://arxiv.org/pdf/1602.02523v1.pdf | author:Rowan McAllister, Carl Edward Rasmussen category:stat.ML cs.LG cs.SY published:2016-02-08 summary:We present a data-efficient reinforcement learning algorithm resistant toobservation noise. Our method extends the highly data-efficient PILCO algorithm(Deisenroth & Rasmussen, 2011) into partially observed Markov decisionprocesses (POMDPs) by considering the filtering process during policyevaluation. PILCO conducts policy search, evaluating each policy by firstpredicting an analytic distribution of possible system trajectories. Weadditionally predict trajectories w.r.t. a filtering process, achievingsignificantly higher performance than combining a filter with a policyoptimised by the original (unfiltered) framework. Our test setup is thecartpole swing-up task with sensor noise, which involves nonlinear dynamics andrequires nonlinear control.
arxiv-15300-239 | A Semi-Automated Method for Object Segmentation in Infant's Egocentric Videos to Study Object Perception | http://arxiv.org/pdf/1602.02522v1.pdf | author:Qazaleh Mirsharif, Sidharth Sadani, Shishir Shah, Hanako Yoshida, Joseph Burling category:cs.CV published:2016-02-08 summary:Object segmentation in infant's egocentric videos is a fundamental step instudying how children perceive objects in early stages of development. From thecomputer vision perspective, object segmentation in such videos pose quite afew challenges because the child's view is unfocused, often with large headmovements, effecting in sudden changes in the child's point of view which leadsto frequent change in object properties such as size, shape and illumination.In this paper, we develop a semi-automated, domain specific, method to addressthese concerns and facilitate the object annotation process for cognitivescientists allowing them to select and monitor the object under segmentation.The method starts with an annotation from the user of the desired object andemploys graph cut segmentation and optical flow computation to predict theobject mask for subsequent video frames automatically. To maintain accuracy, weuse domain specific heuristic rules to re-initialize the program with new userinput whenever object properties change dramatically. The evaluationsdemonstrate the high speed and accuracy of the presented method for objectsegmentation in voluminous egocentric videos. We apply the proposed method toinvestigate potential patterns in object distribution in child's view atprogressive ages.
arxiv-15300-240 | Multi-view Kernel Completion | http://arxiv.org/pdf/1602.02518v1.pdf | author:Sahely Bhadra, Samuel Kaski, Juho Rousu category:cs.LG stat.ML published:2016-02-08 summary:In this paper, we introduce the first method that (1) can complete kernelmatrices with completely missing rows and columns as opposed to individualmissing kernel values, (2) does not require any of the kernels to be complete apriori, and (3) can tackle non-linear kernels. These aspects are necessary inpractical applications such as integrating legacy data sets, learning undersensor failures and learning when measurements are costly for some of theviews. The proposed approach predicts missing rows by modelling bothwithin-view and between-view relationships among kernel values. We show, bothon simulated data and real world data, that the proposed method outperformsexisting techniques in the restricted settings where they are available, andextends applicability to new settings.
arxiv-15300-241 | From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification | http://arxiv.org/pdf/1602.02068v2.pdf | author:André F. T. Martins, Ramón Fernandez Astudillo category:cs.CL cs.LG stat.ML published:2016-02-05 summary:We propose sparsemax, a new activation function similar to the traditionalsoftmax, but able to output sparse probabilities. After deriving itsproperties, we show how its Jacobian can be efficiently computed, enabling itsuse in a network trained with backpropagation. Then, we propose a new smoothand convex loss function which is the sparsemax analogue of the logistic loss.We reveal an unexpected connection between this new loss and the Huberclassification loss. We obtain promising empirical results in multi-labelclassification problems and in attention-based neural networks for naturallanguage inference. For the latter, we achieve a similar performance as thetraditional softmax, but with a selective, more compact, attention focus.
arxiv-15300-242 | Co-Occurrence Patterns in the Voynich Manuscript | http://arxiv.org/pdf/1601.07435v2.pdf | author:Torsten Timm category:cs.CL cs.CR published:2016-01-27 summary:The Voynich Manuscript is a medieval book written in an unknown script. Thispaper studies the distribution of similarly spelled words in the VoynichManuscript. It shows that the distribution of words within the manuscript isnot compatible with natural languages.
arxiv-15300-243 | Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling | http://arxiv.org/pdf/1602.02485v1.pdf | author:Atsushi Shibagaki, Masayuki Karasuyama, Kohei Hatano, Ichiro Takeuchi category:stat.ML published:2016-02-08 summary:The problem of learning a sparse model is conceptually interpreted as theprocess of identifying active features/samples and then optimizing the modelover them. Recently introduced safe screening allows us to identify a part ofnon-active features/samples. So far, safe screening has been individuallystudied either for feature screening or for sample screening. In this paper, weintroduce a new approach for safely screening features and samplessimultaneously by alternatively iterating feature and sample screening steps. Asignificant advantage of considering them simultaneously rather thanindividually is that they have a synergy effect in the sense that the resultsof the previous safe feature screening can be exploited for improving the nextsafe sample screening performances, and vice-versa. We first theoreticallyinvestigate the synergy effect, and then illustrate the practical advantagethrough intensive numerical experiments for problems with large numbers offeatures and samples.
arxiv-15300-244 | Autonomous Perceptron Neural Network Inspired from Quantum computing | http://arxiv.org/pdf/1510.00556v2.pdf | author:M. Zidan, A. Sagheer, N. Metwally category:quant-ph cs.NE published:2015-10-02 summary:This abstract will be modified after correcting the minor error in Eq.(2)
arxiv-15300-245 | Efficient Algorithms for Adversarial Contextual Learning | http://arxiv.org/pdf/1602.02454v1.pdf | author:Vasilis Syrgkanis, Akshay Krishnamurthy, Robert E. Schapire category:cs.LG published:2016-02-08 summary:We provide the first oracle efficient sublinear regret algorithms foradversarial versions of the contextual bandit problem. In this problem, thelearner repeatedly makes an action on the basis of a context and receivesreward for the chosen action, with the goal of achieving reward competitivewith a large class of policies. We analyze two settings: i) in the transductivesetting the learner knows the set of contexts a priori, ii) in the smallseparator setting, there exists a small set of contexts such that any twopolicies behave differently in one of the contexts in the set. Our algorithmsfall into the follow the perturbed leader family \cite{Kalai2005} and achieveregret $O(T^{3/4}\sqrt{K\log(N)})$ in the transductive setting and $O(T^{2/3}d^{3/4} K\sqrt{\log(N)})$ in the separator setting, where $K$ is the number ofactions, $N$ is the number of baseline policies, and $d$ is the size of theseparator. We actually solve the more general adversarial contextualsemi-bandit linear optimization problem, whilst in the full information settingwe address the even more general contextual combinatorial optimization. Weprovide several extensions and implications of our algorithms, such asswitching regret and efficient learning with predictable sequences.
arxiv-15300-246 | The Effect of Gradient Noise on the Energy Landscape of Deep Networks | http://arxiv.org/pdf/1511.06485v4.pdf | author:Pratik Chaudhari, Stefano Soatto category:cs.LG published:2015-11-20 summary:We analyze the regularization properties of additive gradient noise in thetraining of deep networks by posing it as finding the ground state of theHamiltonian of a spherical spin glass in an external magnetic field. We showthat depending upon the magnitude of the magnetic field, the Hamiltonianchanges dramatically from a highly non-convex energy landscape withexponentially many critical points to a regime with polynomially many criticalpoints and finally, "trivializes"' to exactly one minimum. This phenomenon,known as topology trivialization in the physics literature, can be leveraged todevise annealing schemes for additive noise such that the training starts inthe polynomial regime but gradually morphs the energy landscape into theoriginal one as training progresses. We demonstrate through experiments onfully-connected and convolutional neural networks that annealing schemes basedon trivialization lead to accelerated training and also improve generalizationerror.
arxiv-15300-247 | A Simple Practical Accelerated Method for Finite Sums | http://arxiv.org/pdf/1602.02442v1.pdf | author:Aaron Defazio category:stat.ML cs.LG published:2016-02-08 summary:We describe a novel optimization method for finite sums (such as empiricalrisk minimization problems) building on the recently introduced SAGA method.Our method achieves an accelerated convergence rate on strongly convex smoothproblems, matching the conjectured optimal rate. Our method has only oneparameter (a step size), and is radically simpler than other acceleratedmethods for finite sums.
arxiv-15300-248 | Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization | http://arxiv.org/pdf/1602.02434v1.pdf | author:Shervin Minaee, Yao Wang category:cs.CV published:2016-02-07 summary:Sparse decomposition has been widely used for different applications, such assource separation, image classification, image denoising and more. This paperpresents a new algorithm for segmentation of an image into background andforeground text and graphics using sparse decomposition and total variationminimization. The proposed method is designed based on the assumption that thebackground part of the image is smoothly varying and can be represented by alinear combination of a few smoothly varying basis functions, while theforeground text and graphics can be modeled with a sparse component overlaid onthe smooth background. The background and foreground are separated using asparse decomposition framework regularized with a few suitable regularizationterms which promotes the sparsity and connectivity of foreground pixels. Thisalgorithm has been tested on a dataset of images extracted from HEVC standardtest sequences for screen content coding, and is shown to have superiorperformance over some prior methods, including least absolute deviationfitting, k-means clustering based segmentation in DjVu and shape primitiveextraction and coding (SPEC) algorithm.
arxiv-15300-249 | Feature Representation for ICU Mortality | http://arxiv.org/pdf/1512.05294v2.pdf | author:Harini Suresh category:cs.AI cs.LG stat.ML published:2015-12-16 summary:Good predictors of ICU Mortality have the potential to identify high-riskpatients earlier, improve ICU resource allocation, or create more accuratepopulation-level risk models. Machine learning practitioners typically makechoices about how to represent features in a particular model, but thesechoices are seldom evaluated quantitatively. This study compares theperformance of different representations of clinical event data from MIMIC IIin a logistic regression model to predict 36-hour ICU mortality. The mostcommon representations are linear (normalized counts) and binary (yes/no).These, along with a new representation termed "hill", are compared using bothL1 and L2 regularization. Results indicate that the introduced "hill"representation outperforms both the binary and linear representations, the hillrepresentation thus has the potential to improve existing models of ICUmortality.
arxiv-15300-250 | Training CNNs with Low-Rank Filters for Efficient Image Classification | http://arxiv.org/pdf/1511.06744v3.pdf | author:Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, Antonio Criminisi category:cs.CV cs.LG cs.NE published:2015-11-20 summary:We propose a new method for creating computationally efficient convolutionalneural networks (CNNs) by using low-rank representations of convolutionalfilters. Rather than approximating filters in previously-trained networks withmore efficient versions, we learn a set of small basis filters from scratch;during training, the network learns to combine these basis filters into morecomplex filters that are discriminative for image classification. To train suchnetworks, a novel weight initialization scheme is used. This allows effectiveinitialization of connection weights in convolutional layers composed of groupsof differently-shaped filters. We validate our approach by applying it toseveral existing CNN architectures and training these networks from scratchusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar orhigher accuracy than conventional CNNs with much less compute. Applying ourmethod to an improved version of VGG-11 network using global max-pooling, weachieve comparable validation accuracy using 41% less compute and only 24% ofthe original VGG-11 model parameters; another variant of our method gives a 1percentage point increase in accuracy over our improved VGG-11 model, giving atop-5 center-crop validation accuracy of 89.7% while reducing computation by16% relative to the original VGG-11 model. Applying our method to the GoogLeNetarchitecture for ILSVRC, we achieved comparable accuracy with 26% less computeand 41% fewer model parameters. Applying our method to a near state-of-the-artnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and55% fewer parameters.
arxiv-15300-251 | Early Inference in Energy-Based Models Approximates Back-Propagation | http://arxiv.org/pdf/1510.02777v2.pdf | author:Yoshua Bengio, Asja Fischer category:cs.LG published:2015-10-09 summary:We show that Langevin MCMC inference in an energy-based model with latentvariables has the property that the early steps of inference, starting from astationary point, correspond to propagating error gradients into internallayers, similarly to back-propagation. The error that is back-propagated iswith respect to visible units that have received an outside driving forcepushing them away from the stationary point. Back-propagated error gradientscorrespond to temporal derivatives of the activation of hidden units. Thisobservation could be an element of a theory for explaining how brains performcredit assignment in deep hierarchies as efficiently as back-propagation does.In this theory, the continuous-valued latent variables correspond to averagedvoltage potential (across time, spikes, and possibly neurons in the sameminicolumn), and neural computation corresponds to approximate inference anderror back-propagation at the same time.
arxiv-15300-252 | Interpretable Selection and Visualization of Features and Interactions Using Bayesian Forests | http://arxiv.org/pdf/1506.02371v4.pdf | author:Viktoriya Krakovna, Jiong Du, Jun S. Liu category:stat.ML published:2015-06-08 summary:It is becoming increasingly important for machine learning methods to makepredictions that are interpretable as well as accurate. In many practicalapplications, it is of interest which features and feature interactions arerelevant to the prediction task. We present a novel method, Selective BayesianForest Classifier, that strikes a balance between predictive power andinterpretability by simultaneously performing classification, featureselection, feature interaction detection and visualization. It buildsparsimonious yet flexible models using tree-structured Bayesian networks, andsamples an ensemble of such models using Markov chain Monte Carlo. We build infeature selection by dividing the trees into two groups according to theirrelevance to the outcome of interest. Our method performs competitively onclassification and feature selection benchmarks in low and high dimensions, andincludes a visualization tool that provides insight into relevant features andinteractions.
arxiv-15300-253 | Train faster, generalize better: Stability of stochastic gradient descent | http://arxiv.org/pdf/1509.01240v2.pdf | author:Moritz Hardt, Benjamin Recht, Yoram Singer category:cs.LG math.OC stat.ML published:2015-09-03 summary:We show that parametric models trained by a stochastic gradient method (SGM)with few iterations have vanishing generalization error. We prove our resultsby arguing that SGM is algorithmically stable in the sense of Bousquet andElisseeff. Our analysis only employs elementary tools from convex andcontinuous optimization. We derive stability bounds for both convex andnon-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for whymultiple epochs of stochastic gradient methods generalize well in practice. Inthe non-convex case, we give a new interpretation of common practices in neuralnetworks, and formally show that popular techniques for training large deepmodels are indeed stability-promoting. Our findings conceptually underscore theimportance of reducing training time beyond its obvious benefit.
arxiv-15300-254 | Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams | http://arxiv.org/pdf/1403.6348v5.pdf | author:Blaz Sovdat category:cs.AI cs.LG I.2.6 published:2014-03-25 summary:Despite growing interest in data stream mining the most successfulincremental learners, such as VFDT, still use periodic recomputation to updateattribute information gains and Gini indices. This note provides simpleincremental formulas and algorithms for computing entropy and Gini index fromtime-changing data streams.
arxiv-15300-255 | The Optimal Sample Complexity of PAC Learning | http://arxiv.org/pdf/1507.00473v4.pdf | author:Steve Hanneke category:cs.LG stat.ML published:2015-07-02 summary:This work establishes a new upper bound on the number of samples sufficientfor PAC learning in the realizable case. The bound matches known lower boundsup to numerical constant factors. This solves a long-standing open problem onthe sample complexity of PAC learning. The technique and analysis build on arecent breakthrough by Hans Simon.
arxiv-15300-256 | Nonparametric Canonical Correlation Analysis | http://arxiv.org/pdf/1511.04839v4.pdf | author:Tomer Michaeli, Weiran Wang, Karen Livescu category:cs.LG stat.ML published:2015-11-16 summary:Canonical correlation analysis (CCA) is a classical representation learningtechnique for finding correlated variables in multi-view data. Severalnonlinear extensions of the original linear CCA have been proposed, includingkernel and deep neural network methods. These approaches seek maximallycorrelated projections among families of functions, which the user specifies(by choosing a kernel or neural network structure), and are computationallydemanding. Interestingly, the theory of nonlinear CCA, without functionalrestrictions, had been studied in the population setting by Lancaster alreadyin the 1950s, but these results have not inspired practical algorithms. Werevisit Lancaster's theory to devise a practical algorithm for nonparametricCCA (NCCA). Specifically, we show that the solution can be expressed in termsof the singular value decomposition of a certain operator associated with thejoint density of the views. Thus, by estimating the population density fromdata, NCCA reduces to solving an eigenvalue system, superficially like kernelCCA but, importantly, without requiring the inversion of any kernel matrix. Wealso derive a partially linear CCA (PLCCA) variant in which one of the viewsundergoes a linear projection while the other is nonparametric. Using a kerneldensity estimate based on a small number of nearest neighbors, our NCCA andPLCCA algorithms are memory-efficient, often run much faster, and performbetter than kernel CCA and comparable to deep CCA.
arxiv-15300-257 | Network Inference by Learned Node-Specific Degree Prior | http://arxiv.org/pdf/1602.02386v1.pdf | author:Qingming Tang, Lifu Tu, Weiran Wang, Jinbo Xu category:stat.ML cs.LG published:2016-02-07 summary:We propose a novel method for network inference from partially observed edgesusing a node-specific degree prior. The degree prior is derived from observededges in the network to be inferred, and its hyper-parameters are determined bycross validation. Then we formulate network inference as a matrix completionproblem regularized by our degree prior. Our theoretical analysis indicatesthat this prior favors a network following the learned degree distribution, andmay lead to improved network recovery error bound than previous work.Experimental results on both simulated and real biological networks demonstratethe superior performance of our method in various settings.
arxiv-15300-258 | Disentangled Representations in Neural Models | http://arxiv.org/pdf/1602.02383v1.pdf | author:William Whitney category:cs.LG cs.NE published:2016-02-07 summary:Representation learning is the foundation for the recent success of neuralnetwork models. However, the distributed representations generated by neuralnetworks are far from ideal. Due to their highly entangled nature, they are dicult to reuse and interpret, and they do a poor job of capturing the sparsitywhich is present in real- world transformations. In this paper, I describemethods for learning disentangled representations in the two domains ofgraphics and computation. These methods allow neural methods to learnrepresentations which are easy to interpret and reuse, yet they incur little orno penalty to performance. In the Graphics section, I demonstrate the abilityof these methods to infer the generating parameters of images and rerenderthose images under novel conditions. In the Computation section, I describe amodel which is able to factorize a multitask learning problem into subtasks andwhich experiences no catastrophic forgetting. Together these techniques providethe tools to design a wide range of models that learn disentangledrepresentations and better model the factors of variation in the real world.
arxiv-15300-259 | Supervised and Semi-Supervised Text Categorization using One-Hot LSTM for Region Embeddings | http://arxiv.org/pdf/1602.02373v1.pdf | author:Rie Johnson, Tong Zhang category:stat.ML cs.CL cs.LG published:2016-02-07 summary:One-hot CNN (convolutional neural network) has been shown to be effective fortext categorization in our previous work. We view it as a special case of ageneral framework which jointly trains a linear model with a non-linear featuregenerator consisting of `text region embedding + pooling'. Under thisframework, we explore a more sophisticated region embedding method using LongShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possiblylarge) sizes, whereas the region size needs to be fixed in a CNN. We seek thebest use of LSTM for the purpose in the supervised and semi-supervisedsettings, starting with the idea of one-hot LSTM, which eliminates thecustomarily used word embedding layer. Our results indicate that on this task,embeddings of text regions, which can convey higher concepts than single wordsin isolation, are more useful than word embeddings. We report performancesexceeding the previous best results on four benchmark datasets.
arxiv-15300-260 | Learning Contextual Dependencies with Convolutional Hierarchical Recurrent Neural Networks | http://arxiv.org/pdf/1509.03877v2.pdf | author:Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingxing Wang, Bing Wang category:cs.CV published:2015-09-13 summary:Existing deep convolutional neural networks (CNNs) have shown their greatsuccess on image classification. CNNs mainly consist of convolutional andpooling layers, both of which are performed on local image areas withoutconsidering the dependencies among different image regions. However, suchdependencies are very important for generating explicit image representation.In contrast, recurrent neural networks (RNNs) are well known for their abilityof encoding contextual information among sequential data, and they only requirea limited number of network parameters. General RNNs can hardly be directlyapplied on non-sequential data. Thus, we proposed the hierarchical RNNs(HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependenciesamong image regions from the same scale but different locations. While thecross RNN scale connections target on modeling scale dependencies among regionsfrom the same location but different scales. Specifically, we propose tworecurrent neural network models: 1) hierarchical simple recurrent network(HSRN), which is fast and has low computational cost; and 2) hierarchicallong-short term memory recurrent network (HLSTM), which performs better thanHSRN with the price of more computational cost. In this manuscript, we integrate CNNs with HRNNs, and develop end-to-endconvolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs notonly make use of the representation power of CNNs, but also efficiently encodesspatial and scale dependencies among different image regions. On four of themost challenging object/scene image classification benchmarks, our C-HRNNsachieve state-of-the-art results on Places 205, SUN 397, MIT indoor, andcompetitive results on ILSVRC 2012.
arxiv-15300-261 | Solving Ridge Regression using Sketched Preconditioned SVRG | http://arxiv.org/pdf/1602.02350v1.pdf | author:Alon Gonen, Francesco Orabona, Shai Shalev-Shwartz category:cs.LG published:2016-02-07 summary:We develop a novel preconditioning method for ridge regression, based onrecent linear sketching methods. By equipping Stochastic Variance ReducedGradient (SVRG) with this preconditioning process, we obtain a significantspeed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.
arxiv-15300-262 | SCOPE: Scalable Composite Optimization for Learning on Spark | http://arxiv.org/pdf/1602.00133v2.pdf | author:Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, Wu-jun Li category:stat.ML cs.LG published:2016-01-30 summary:Many machine learning models, such as lo- gistic regression (LR) and supportvector ma- chine (SVM), can be formulated as compos- ite optimization problems.Recently, many dis- tributed stochastic optimization (DSO) methods have beenproposed to solve the large-scale com- posite optimization problems, which haveshown better performance than traditional batch meth- ods. However, most ofthese DSO methods might not be scalable enough. In this paper, we propose anovel DSO method, called scalable composite optimization for learning (SCOPE),and implement it on the fault-tolerant distributed platform Spark. SCOPE isboth computation- efficient and communication-efficient. Theoret- ical analysisshows that SCOPE is convergent with linear convergence rate when the objectivefunction is strongly convex. Furthermore, em- pirical results on real datasetsshow that SCOPE can outperform other state-of-the-art distributed learningmethods on Spark, including both batch learning methods and DSO methods.
arxiv-15300-263 | ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution | http://arxiv.org/pdf/1602.02334v1.pdf | author:Zeinab Bahmani, Leopoldo Bertossi, Nikolaos Vasiloglou category:cs.DB cs.AI cs.LG published:2016-02-07 summary:Entity resolution (ER), an important and common data cleaning problem, isabout detecting data duplicate representations for the same external entities,and merging them into single representations. Relatively recently, declarativerules called "matching dependencies" (MDs) have been proposed for specifyingsimilarity conditions under which attribute values in database records aremerged. In this work we show the process and the benefits of integrating fourcomponents of ER: (a) Building a classifier for duplicate/non-duplicate recordpairs built using machine learning (ML) techniques; (b) Use of MDs forsupporting the blocking phase of ML; (c) Record merging on the basis of theclassifier results; and (d) The use of the declarative language "LogiQL" -anextended form of Datalog supported by the "LogicBlox" platform- for allactivities related to data processing, and the specification and enforcement ofMDs.
arxiv-15300-264 | Scalable Text Mining with Sparse Generative Models | http://arxiv.org/pdf/1602.02332v1.pdf | author:Antti Puurula category:cs.IR cs.AI cs.CL published:2016-02-07 summary:The information age has brought a deluge of data. Much of this is in textform, insurmountable in scope for humans and incomprehensible in structure forcomputers. Text mining is an expanding field of research that seeks to utilizethe information contained in vast document collections. General data miningmethods based on machine learning face challenges with the scale of text data,posing a need for scalable text mining methods. This thesis proposes a solution to scalable text mining: generative modelscombined with sparse computation. A unifying formalization for generative textmodels is defined, bringing together research traditions that have usedformally equivalent models, but ignored parallel developments. This frameworkallows the use of methods developed in different processing tasks such asretrieval and classification, yielding effective solutions across differenttext mining tasks. Sparse computation using inverted indices is proposed forinference on probabilistic models. This reduces the computational complexity ofthe common text mining operations according to sparsity, yielding probabilisticmodels with the scalability of modern search engines. The proposed combination provides sparse generative models: a solution fortext mining that is general, effective, and scalable. Extensive experimentationon text classification and ranked retrieval datasets are conducted, showingthat the proposed solution matches or outperforms the leading task-specificmethods in effectiveness, with a order of magnitude decrease in classificationtimes for Wikipedia article categorization with a million classes. Thedeveloped methods were further applied in two 2014 Kaggle data mining prizecompetitions with over a hundred competing teams, earning first and secondplaces.
arxiv-15300-265 | Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions | http://arxiv.org/pdf/1602.01103v2.pdf | author:Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian Lee category:cs.SI cs.CL physics.soc-ph published:2016-02-02 summary:Changing someone's opinion is arguably one of the most important challengesof social interaction. The underlying process proves difficult to study: it ishard to know how someone's opinions are formed and whether and how someone'sviews shift. Fortunately, ChangeMyView, an active community on Reddit, providesa platform where users present their own opinions and reasoning, invite othersto contest them, and acknowledge when the ensuing discussions change theiroriginal views. In this work, we study these interactions to understand themechanisms behind persuasion. We find that persuasive arguments are characterized by interesting patternsof interaction dynamics, such as participant entry-order and degree ofback-and-forth exchange. Furthermore, by comparing similar counterarguments tothe same opinion, we show that language factors play an essential role. Inparticular, the interplay between the language of the opinion holder and thatof the counterargument provides highly predictive cues of persuasiveness.Finally, since even in this favorable setting people may not be persuaded, weinvestigate the problem of determining whether someone's opinion is susceptibleto being changed at all. For this more difficult task, we show that stylisticchoices in how the opinion is expressed carry predictive power.
arxiv-15300-266 | MPBART - Multinomial Probit Bayesian Additive Regression Trees | http://arxiv.org/pdf/1309.7821v2.pdf | author:Bereket P. Kindo, Hao Wang, Edsel A. Peña category:stat.ML published:2013-09-30 summary:This article proposes Multinomial Probit Bayesian Additive Regression Trees(MPBART) as a multinomial probit extension of BART - Bayesian AdditiveRegression Trees (Chipman et al (2010)). MPBART is flexible to allow inclusionof predictors that describe the observed units as well as the available choicealternatives. Through two simulation studies and four real data examples, weshow that MPBART exhibits very good predictive performance in comparison toother discrete choice and multiclass classification methods. To implementMPBART, we have developed an R package mpbart available freely from CRANrepositories.
arxiv-15300-267 | A Deep Learning Approach to Unsupervised Ensemble Learning | http://arxiv.org/pdf/1602.02285v1.pdf | author:Uri Shaham, Xiuyuan Cheng, Omer Dror, Ariel Jaffe, Boaz Nadler, Joseph Chang, Yuval Kluger category:stat.ML cs.LG published:2016-02-06 summary:We show how deep learning methods can be applied in the context ofcrowdsourcing and unsupervised ensemble learning. First, we prove that thepopular model of Dawid and Skene, which assumes that all classifiers areconditionally independent, is {\em equivalent} to a Restricted BoltzmannMachine (RBM) with a single hidden node. Hence, under this model, the posteriorprobabilities of the true labels can be instead estimated via a trained RBM.Next, to address the more general case, where classifiers may strongly violatethe conditional independence assumption, we propose to apply RBM-based DeepNeural Net (DNN). Experimental results on various simulated and real-worlddatasets demonstrate that our proposed DNN approach outperforms otherstate-of-the-art methods, in particular when the data violates the conditionalindependence assumption.
arxiv-15300-268 | Importance Sampling for Minibatches | http://arxiv.org/pdf/1602.02283v1.pdf | author:Dominik Csiba, Peter Richtárik category:cs.LG math.OC stat.ML published:2016-02-06 summary:Minibatching is a very well studied and highly popular technique insupervised learning, used by practitioners due to its ability to acceleratetraining through better utilization of parallel processing power and reductionof stochastic variance. Another popular technique is importance sampling -- astrategy for preferential sampling of more important examples also capable ofaccelerating the training process. However, despite considerable effort by thecommunity in these areas, and due to the inherent technical difficulty of theproblem, there is no existing work combining the power of importance samplingwith the strength of minibatching. In this paper we propose the first {\emimportance sampling for minibatches} and give simple and rigorous complexityanalysis of its performance. We illustrate on synthetic problems that fortraining data of certain properties, our sampling can lead to several orders ofmagnitude improvement in training time. We then test the new sampling onseveral popular datasets, and show that the improvement can reach an order ofmagnitude.
arxiv-15300-269 | How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks | http://arxiv.org/pdf/1602.02282v1.pdf | author:Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, Ole Winther category:stat.ML cs.LG published:2016-02-06 summary:Variational autoencoders are a powerful framework for unsupervised learning.However, previous work has been restricted to shallow models with one or twolayers of fully factorized stochastic latent variables, limiting theflexibility of the latent representation. We propose three advances in trainingalgorithms of variational autoencoders, for the first time allowing to traindeep models of up to five stochastic layers, (1) using a structure similar tothe Ladder network as the inference model, (2) warm-up period to supportstochastic units staying active in early training, and (3) use of batchnormalization. Using these improvements we show state-of-the-art log-likelihoodresults for generative modeling on several benchmark datasets.
arxiv-15300-270 | DOLPHIn - Dictionary Learning for Phase Retrieval | http://arxiv.org/pdf/1602.02263v1.pdf | author:Andreas M. Tillmann, Yonina C. Eldar, Julien Mairal category:math.OC cs.IT cs.LG math.IT stat.ML published:2016-02-06 summary:We propose a new algorithm to learn a dictionary for reconstructing andsparsely encoding signals from measurements without phase. Specifically, weconsider the task of estimating a two-dimensional image from squared-magnitudemeasurements of a complex-valued linear transformation of the original image.Several recent phase retrieval algorithms exploit underlying sparsity of theunknown signal in order to improve recovery performance. In this work, weconsider such a sparse signal prior in the context of phase retrieval, when thesparsifying dictionary is not known in advance. Our algorithm jointlyreconstructs the unknown signal - possibly corrupted by noise - and learns adictionary such that each patch of the estimated image can be sparselyrepresented. Numerical experiments demonstrate that our approach can obtainsignificantly better reconstructions for phase retrieval problems with noisethan methods that cannot exploit such "hidden" sparsity. Moreover, on thetheoretical side, we provide a convergence result for our method.
arxiv-15300-271 | Recovery guarantee of weighted low-rank approximation via alternating minimization | http://arxiv.org/pdf/1602.02262v1.pdf | author:Yuanzhi Li, Yingyu Liang, Andrej Risteski category:cs.LG cs.DS stat.ML published:2016-02-06 summary:Many applications require recovering a ground truth low-rank matrix fromnoisy observations of the entries. In practice, this is typically formulated asweighted low-rank approximation problem and solved using non-convexoptimization heuristics such as alternating minimization. Such non-convextechniques have little guarantees. Even worse, weighted low-rank approximationis NP-hard for even the most simple case when the ground truth is a rank-1matrix. In this paper, we provide provable recovery guarantee in polynomial time fora natural class of matrices and weights. In particular, we bound the spectralnorm of the difference between the recovered matrix and the ground truth, bythe spectral norm of the weighted noise plus an additive error term thatdecreases exponentially with the number of rounds of alternating minimization.This provides the first theoretical result for weighted low-rank approximationvia alternating minimization with non-binary deterministic weights. It is asignificant generalization of the results for matrix completion, the specialcase with binary weights, since our assumptions are similar or weaker thanthose made in existing works. The key technical challenge is that under non-binary deterministic weights,naive alternating minimization steps will destroy the incoherency and spectralproperties of the intermediate solution, which are needed for making progresstowards the ground truth. One of our key technical contributions is a whiteningstep that maintains these properties of the intermediate solution after eachround, which may be applied to alternating minimization for other problems andthus is of independent interest.
arxiv-15300-272 | A Tractable Fully Bayesian Method for the Stochastic Block Model | http://arxiv.org/pdf/1602.02256v1.pdf | author:Kohei Hayashi, Takuya Konishi, Tatsuro Kawamoto category:cs.LG stat.ML published:2016-02-06 summary:The stochastic block model (SBM) is a generative model revealing macroscopicstructures in graphs. Bayesian methods are used for (i) cluster assignmentinference and (ii) model selection for the number of clusters. In this paper,we study the behavior of Bayesian inference in the SBM in the large samplelimit. Combining variational approximation and Laplace's method, a consistentcriterion of the fully marginalized log-likelihood is established. Based onthat, we derive a tractable algorithm that solves tasks (i) and (ii)concurrently, obviating the need for an outer loop to check all modelcandidates. Our empirical and theoretical results demonstrate that our methodis scalable in computation, accurate in approximation, and concise in modelselection.
arxiv-15300-273 | Reducing training requirements through evolutionary based dimension reduction and subject transfer | http://arxiv.org/pdf/1602.02237v1.pdf | author:Adham Atyabi, Martin Luerssena, Sean P. Fitzgibbon, Trent Lewis, David M. W. Powersa category:cs.NE published:2016-02-06 summary:Training Brain Computer Interface (BCI) systems to understand the intentionof a subject through Electroencephalogram (EEG) data currently requiresmultiple training sessions with a subject in order to develop the necessaryexpertise to distinguish signals for different tasks. Conventionally the taskof training the subject is done by introducing a training and calibration stageduring which some feedback is presented to the subject. This training sessioncan take several hours which is not appropriate for on-line EEG-based BCIsystems. An alternative approach is to use previous recording sessions of thesame person or some other subjects that performed the same tasks (subjecttransfer) for training the classifiers. The main aim of this study is togenerate a methodology that allows the use of data from other subjects whilereducing the dimensions of the data. The study investigates severalpossibilities for reducing the necessary training and calibration period insubjects and the classifiers and addresses the impact of i) evolutionarysubject transfer and ii) adapting previously trained methods (retraining) usingother subjects data. Our results suggest reduction to 40% of target subjectdata is sufficient for training the classifier. Our results also indicate thesuperiority of the approaches that incorporated evolutionary subject transferand highlights the feasibility of adapting a system trained on other subjects.
arxiv-15300-274 | Scalable Recommendation from Web Usage Mining using Method of Moments | http://arxiv.org/pdf/1511.00792v8.pdf | author:Sayantan Dasgupta category:cs.LG published:2015-11-03 summary:With the advent of mass-available Internet, twenty-first century observed asteady growth in web based commercial services and technology companies. Mostof them are based on web applications that receive huge amount of usertraffics, and generate massive amount of web usage data containing user-iteminteractions. We attempt to build a recommendation algorithm based on such webusage data. It is essential that recommendation algorithms for suchapplications are highly scalable in nature. Existing algorithms such as matrixfactorization run several iterations through the dataset, and therefore may notbe suitable for large web-scale datasets. Here we propose a highly scalablerecommendation algorithm based on recently proposed Method of Moments (alsoknown as Spectral Method). Our method takes only two to three passes throughthe entire dataset to extract the model parameters during the training phase.We demonstrate the competitive performance of our algorithm in comparison withthe existing algorithms on various publicly available datasets through severalempirical measures.
arxiv-15300-275 | Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning | http://arxiv.org/pdf/1510.03519v2.pdf | author:Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran category:cs.CL published:2015-10-13 summary:Recently there has been a lot of interest in learning common representationsfor multiple views of data. These views could belong to different modalities orlanguages. Typically, such common representations are learned using a parallelcorpus between the two views (say, 1M images and their English captions). Inthis work, we address a real-world scenario where no direct parallel data isavailable between two views of interest (say, V1 and V2) but parallel data isavailable between each of these views and a pivot view (V3). We propose a modelfor learning a common representation for V1, V2 and V3 using only the paralleldata available between V1V3 and V2V3. The proposed model is generic and evenworks when there are n views of interest and only one pivot view which acts asa bridge between them. There are two specific downstream applications that wefocus on (i) Transfer learning between languages L1,L2,...,Ln using a pivotlanguage L and (ii) cross modal access between images and a language L1 using apivot language L2. We evaluate our model using two datasets : (i) publiclyavailable multilingual TED corpus and (ii) a new multilingual multimodaldataset created and released as a part of this work. On both these datasets,our model outperforms state of the art approaches.
arxiv-15300-276 | Improved Dropout for Shallow and Deep Learning | http://arxiv.org/pdf/1602.02220v1.pdf | author:Zhe Li, Boqing Gong, Tianbao Yang category:cs.LG stat.ML published:2016-02-06 summary:Dropout has been witnessed with great success in training deep neuralnetworks by independently zeroing out the outputs of neurons at random. It hasalso received a surge of interest for shallow learning, e.g., logisticregression. However, the independent sampling for dropout could be suboptimalfor the sake of convergence. In this paper, we propose to use multinomialsampling for dropout, i.e., sampling features or neurons according to amultinomial distribution with different probabilities for differentfeatures/neurons. To exhibit the optimal dropout probabilities, we analyze theshallow learning with multinomial dropout and establish the risk bound forstochastic optimization. By minimizing a sampling dependent factor in the riskbound, we obtain a distribution-dependent dropout with sampling probabilitiesdependent on the second order statistics of the data distribution. To tacklethe issue of evolving distribution of neurons in deep learning, we propose anefficient adaptive dropout (named \textbf{evolutional dropout}) that computesthe sampling probabilities on-the-fly from a mini-batch of examples. Empiricalstudies on several benchmark datasets demonstrate that the proposed dropoutsachieve not only much faster convergence and but also a smaller testing errorthan the standard dropout. For example, on the CIFAR-100 data, the evolutionaldropout achieves relative improvements over 10\% on the prediction performanceand over 50\% on the convergence speed compared to the standard dropout.
arxiv-15300-277 | Variational Hamiltonian Monte Carlo via Score Matching | http://arxiv.org/pdf/1602.02219v1.pdf | author:Cheng Zhang, Babak Shahbaba, Hongkai Zhao category:stat.CO stat.ML published:2016-02-06 summary:Traditionally, the field of computational Bayesian statistics has beendivided into two main subfields: variational methods and Markov chain MonteCarlo (MCMC). In recent years, however, several methods have been proposedbased on combining variational Bayesian inference and MCMC simulation in orderto improve their overall accuracy and computational efficiency. This marriageof fast evaluation and flexible approximation provides a promising means ofdesigning scalable Bayesian inference methods. In this paper, we explore thepossibility of incorporating variational approximation into a state-of-the-artMCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradientcomputation in the simulation of Hamiltonian flow, which is the bottleneck formany applications of HMC in big data problems. To this end, we use a {\itfree-form} approximation induced by a fast and flexible surrogate functionbased on single-hidden layer feedforward neural networks. The surrogateprovides sufficiently accurate approximation while allowing for fastexploration of parameter space, resulting in an efficient approximate inferencealgorithm. We demonstrate the advantages of our method on both synthetic andreal data problems.
arxiv-15300-278 | Strongly-Typed Recurrent Neural Networks | http://arxiv.org/pdf/1602.02218v1.pdf | author:David Balduzzi, Muhammad Ghifary category:cs.LG cs.NE published:2016-02-06 summary:Recurrent neural networks are increasing popular models for sequentiallearning. Unfortunately, although the most effective RNN architectures areperhaps excessively complicated, extensive searches have not found simpleralternatives. This paper imports ideas from physics and functional programminginto RNN design to provide guiding principles. From physics we introduce typeconstraints, analogous to the constraints that disqualify adding meters toseconds in physics. From functional programming, we require that strongly-typedarchitectures factorize into stateless learnware and state-dependent firmware,thereby ameliorating the impact of side-effects. The features learned bystrongly-typed nets have a simple semantic interpretation via dynamicaverage-pooling on one-dimensional convolutions. We also show thatstrongly-typed gradients are better behaved than in classical architectures,and characterize the representational power of strongly-typed nets. Finally,experiments show that, despite being more constrained, strongly-typedarchitectures achieve lower training error and comparable generalization errorto classical architectures.
arxiv-15300-279 | Swivel: Improving Embeddings by Noticing What's Missing | http://arxiv.org/pdf/1602.02215v1.pdf | author:Noam Shazeer, Ryan Doherty, Colin Evans, Chris Waterson category:cs.CL published:2016-02-06 summary:We present Submatrix-wise Vector Embedding Learner (Swivel), a method forgenerating low-dimensional feature embeddings from a feature co-occurrencematrix. Swivel performs approximate factorization of the point-wise mutualinformation matrix via stochastic gradient descent. It uses a piecewise losswith special handling for unobserved co-occurrences, and thus makes use of allthe information in the matrix. While this requires computation proportional tothe size of the entire matrix, we make use of vectorized multiplication toprocess thousands of rows and columns at once to compute millions of predictedvalues. Furthermore, we partition the matrix into shards in order toparallelize the computation across many nodes. This approach results in moreaccurate embeddings than can be achieved with methods that consider onlyobserved co-occurrences, and can scale to much larger corpora than can behandled with sampling methods.
arxiv-15300-280 | Classification Accuracy as a Proxy for Two Sample Testing | http://arxiv.org/pdf/1602.02210v1.pdf | author:Aaditya Ramdas, Aarti Singh, Larry Wasserman category:cs.LG cs.AI math.ST stat.ML stat.TH published:2016-02-06 summary:When data analysts train a classifier and check if its accuracy issignificantly different from random guessing, they are implicitly andindirectly performing a hypothesis test (two sample testing) and it is ofimportance to ask whether this indirect method for testing is statisticallyoptimal or not. Given that hypothesis tests attempt to maximize statisticalpower subject to a bound on the allowable false positive rate, while predictionattempts to minimize statistical risk on future predictions on unseen data, wewish to study whether a predictive approach for an ultimate aim of testing isprudent. We formalize this problem by considering the two-sample mean-testingsetting where one must determine if the means of two Gaussians (with known andequal covariance) are the same or not, but the analyst indirectly does so bychecking whether the accuracy achieved by Fisher's LDA classifier issignificantly different from chance or not. Unexpectedly, we find that theasymptotic power of LDA's sample-splitting classification accuracy is actuallyminimax rate-optimal in terms of problem-dependent parameters. Since predictionis commonly thought to be harder than testing, it might come as a surprise tosome that solving a harder problem does not create a information-theoreticbottleneck for the easier one. On the flip side, even though the power israte-optimal, our derivation suggests that it may be worse by a small constantfactor; hence practitioners must be wary of using (admittedly flexible)prediction methods on disguised testing problems.
arxiv-15300-281 | Efficient Second Order Online Learning via Sketching | http://arxiv.org/pdf/1602.02202v1.pdf | author:Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, John Langford category:cs.LG published:2016-02-06 summary:We propose Sketched Online Newton (SON), an online second order learningalgorithm that enjoys substantially improved regret guarantees forill-conditioned data. SON is an enhanced version of the Online Newton Step,which, via sketching techniques enjoys a linear running time. We furtherimprove the computational complexity to linear in the number of nonzero entriesby creating sparse forms of the sketching methods (such as Oja's rule) for topeigenvector extraction. Together, these algorithms eliminate all computationalobstacles in previous second order online learning approaches.
arxiv-15300-282 | BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits | http://arxiv.org/pdf/1602.02196v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML published:2016-02-06 summary:We present efficient algorithms for the problem of contextual bandits withi.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class ofpolicies. Our algorithm BISTRO requires d calls to the empirical riskminimization (ERM) oracle per round, where d is the number of actions. Themethod uses unlabeled data to make the problem computationally simple. When theERM problem itself is computationally hard, we extend the approach by employingmultiplicative approximation algorithms for the ERM. The integrality gap of therelaxation only enters in the regret bound rather than the benchmark. Finally,we show that the adversarial version of the contextual bandit problem islearnable (and efficient) whenever the full-information supervised onlinelearning problem has a non-trivial regret guarantee (and efficient).
arxiv-15300-283 | Active Information Acquisition | http://arxiv.org/pdf/1602.02181v1.pdf | author:He He, Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG published:2016-02-05 summary:We propose a general framework for sequential and dynamic acquisition ofuseful information in order to solve a particular task. While our goal could inprinciple be tackled by general reinforcement learning, our particular settingis constrained enough to allow more efficient algorithms. In this paper, wework under the Learning to Search framework and show how to formulate the goalof finding a dynamic information acquisition policy in that framework. We applyour formulation on two tasks, sentiment analysis and image recognition, andshow that the learned policies exhibit good statistical performance. As anemergent byproduct, the learned policies show a tendency to focus on the mostprominent parts of each instance and give harder instances more attentionwithout explicitly being trained to do so.
arxiv-15300-284 | On Column Selection in Approximate Kernel Canonical Correlation Analysis | http://arxiv.org/pdf/1602.02172v1.pdf | author:Weiran Wang category:cs.LG stat.ML published:2016-02-05 summary:We study the problem of column selection in large-scale kernel canonicalcorrelation analysis (KCCA) using the Nystr\"om approximation, where oneapproximates two positive semi-definite kernel matrices using "landmark" pointsfrom the training set. When building low-rank kernel approximations in KCCA,previous work mostly samples the landmarks uniformly at random from thetraining set. We propose novel strategies for sampling the landmarksnon-uniformly based on a version of statistical leverage scores recentlydeveloped for kernel ridge regression. We study the approximation accuracy ofthe proposed non-uniform sampling strategy, develop an incremental algorithmthat explores the path of approximation ranks and facilitates efficient modelselection, and derive the kernel stability of out-of-sample mapping for ourmethod. Experimental results on both synthetic and real-world datasetsdemonstrate the promise of our method.
arxiv-15300-285 | A Note on Alternating Minimization Algorithm for the Matrix Completion Problem | http://arxiv.org/pdf/1602.02164v1.pdf | author:David Gamarnik, Sidhant Misra category:stat.ML cs.LG cs.NA published:2016-02-05 summary:We consider the problem of reconstructing a low rank matrix from a subset ofits entries and analyze two variants of the so-called Alternating Minimizationalgorithm, which has been proposed in the past. We establish that when theunderlying matrix has rank $r=1$, has positive bounded entries, and the graph$\mathcal{G}$ underlying the revealed entries has bounded degree and diameterwhich is at most logarithmic in the size of the matrix, both algorithms succeedin reconstructing the matrix approximately in polynomial time starting from anarbitrary initialization. We further provide simulation results which suggestthat the second algorithm which is based on the message passing type updates,performs significantly better.
arxiv-15300-286 | Daleel: Simplifying Cloud Instance Selection Using Machine Learning | http://arxiv.org/pdf/1602.02159v1.pdf | author:Faiza Samreen, Yehia Elkhatib, Matthew Rowe, Gordon S. Blair category:cs.DC cs.LG cs.PF published:2016-02-05 summary:Decision making in cloud environments is quite challenging due to thediversity in service offerings and pricing models, especially considering thatthe cloud market is an incredibly fast moving one. In addition, there are nohard and fast rules, each customer has a specific set of constraints (e.g.budget) and application requirements (e.g. minimum computational resources).Machine learning can help address some of the complicated decisions by carryingout customer-specific analytics to determine the most suitable instance type(s)and the most opportune time for starting or migrating instances. We employmachine learning techniques to develop an adaptive deployment policy, providingan optimal match between the customer demands and the available cloud serviceofferings. We provide an experimental study based on extensive set of jobexecutions over a major public cloud infrastructure.
arxiv-15300-287 | Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters | http://arxiv.org/pdf/1602.02151v1.pdf | author:Zeyuan Allen-Zhu, Yang Yuan, Karthik Sridharan category:cs.LG stat.ML published:2016-02-05 summary:The amount of data available in the world is growing faster and bigger thanour ability to deal with it. However, if we take advantage of the internalstructure, data may become much smaller for machine learning purposes. In thispaper we focus on one of the most fundamental machine learning tasks, empiricalrisk minimization (ERM), and provide faster algorithms with the help from theclustering structure of the data. We introduce a simple notion of raw clustering that can be efficientlyobtained with just one pass of the data, and propose two algorithms. Ourvariance-reduction based algorithm ClusterSVRG introduces a new gradientestimator using the clustering information, and our accelerated algorithmClusterACDM is built on a novel Haar transformation applied to the dual spaceof each cluster. Our algorithms outperform their classical counterparts both intheory and practice.
arxiv-15300-288 | Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives | http://arxiv.org/pdf/1506.01972v2.pdf | author:Zeyuan Allen-Zhu, Yang Yuan category:cs.LG cs.DS math.OC stat.ML published:2015-06-05 summary:Many classical algorithms are found until several years later to outlive theconfines in which they were conceived, and continue to be relevant inunforeseen settings. In this paper, we show that SVRG is one such method:originally designed for strongly convex objectives, is also very robust undernon-strongly convex or sum-of-non-convex settings. If $f(x)$ is a sum of smooth, convex functions but $f$ is not strongly convex(such as Lasso or logistic regression), we propose a variant SVRG++ that makesa novel choice of growing epoch length on top of SVRG. SVRG++ is a direct,faster variant of SVRG in this setting. If $f(x)$ is a sum of non-convex functions but $f$ is strongly convex, weshow that the convergence of SVRG linearly depends on the non-convexityparameter of the summands. This improves the best known result in this setting,and gives better running time for stochastic PCA.
arxiv-15300-289 | Reducing Runtime by Recycling Samples | http://arxiv.org/pdf/1602.02136v1.pdf | author:Jialei Wang, Hai Wang, Nathan Srebro category:cs.LG stat.ML published:2016-02-05 summary:Contrary to the situation with stochastic gradient descent, we argue thatwhen using stochastic methods with variance reduction, such as SDCA, SAG orSVRG, as well as their variants, it could be beneficial to reuse previouslyused samples instead of fresh samples, even when fresh samples are available.We demonstrate this empirically for SDCA, SAG and SVRG, studying the optimalsample size one should use, and also uncover be-havior that suggests runningSDCA for an integer number of epochs could be wasteful.
arxiv-15300-290 | Mining Software Quality from Software Reviews: Research Trends and Open Issues | http://arxiv.org/pdf/1602.02133v1.pdf | author:Issa Atoum, Ahmed Otoom category:cs.CL cs.IR published:2016-02-05 summary:Software review text fragments have considerably valuable information aboutusers experience. It includes a huge set of properties including the softwarequality. Opinion mining or sentiment analysis is concerned with analyzingtextual user judgments. The application of sentiment analysis on softwarereviews can find a quantitative value that represents software quality.Although many software quality methods are proposed they are considereddifficult to customize and many of them are limited. This article investigatesthe application of opinion mining as an approach to extract software qualityproperties. We found that the major issues of software reviews mining usingsentiment analysis are due to software lifecycle and the diverse users andteams.
arxiv-15300-291 | Sub-cortical brain structure segmentation using F-CNN's | http://arxiv.org/pdf/1602.02130v1.pdf | author:Mahsa Shakeri, Stavros Tsogkas, Enzo Ferrante, Sarah Lippe, Samuel Kadoury, Nikos Paragios, Iasonas Kokkinos category:cs.CV published:2016-02-05 summary:In this paper we propose a deep learning approach for segmenting sub-corticalstructures of the human brain in Magnetic Resonance (MR) image data. We drawinspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN)architecture for semantic segmentation of objects in natural images, and adaptit to our task. Unlike previous CNN-based methods that operate on imagepatches, our model is applied on a full blown 2D image, without any alignmentor registration steps at testing time. We further improve segmentation resultsby interpreting the CNN output as potentials of a Markov Random Field (MRF),whose topology corresponds to a volumetric grid. Alpha-expansion is used toperform approximate inference imposing spatial volumetric homogeneity to theCNN priors. We compare the performance of the proposed pipeline with a similarsystem using Random Forest-based priors, as well as state-of-art segmentationalgorithms, and show promising results on two different brain MRI datasets.
arxiv-15300-292 | Sequence Classification with Neural Conditional Random Fields | http://arxiv.org/pdf/1602.02123v1.pdf | author:Myriam Abramson category:cs.LG published:2016-02-05 summary:The proliferation of sensor devices monitoring human activity generatesvoluminous amount of temporal sequences needing to be interpreted andcategorized. Moreover, complex behavior detection requires the personalizationof multi-sensor fusion algorithms. Conditional random fields (CRFs) arecommonly used in structured prediction tasks such as part-of-speech tagging innatural language processing. Conditional probabilities guide the choice of eachtag/label in the sequence conflating the structured prediction task with thesequence classification task where different models provide differentcategorization of the same sequence. The claim of this paper is that CRF modelsalso provide discriminative models to distinguish between types of sequenceregardless of the accuracy of the labels obtained if we calibrate the classmembership estimate of the sequence. We introduce and compare different neuralnetwork based linear-chain CRFs and we present experiments on two complexsequence classification and structured prediction tasks to support this claim.
arxiv-15300-293 | Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities | http://arxiv.org/pdf/1602.02114v1.pdf | author:Adrien Todeschini, François Caron category:stat.ME cs.SI physics.soc-ph stat.ML published:2016-02-05 summary:We propose a novel statistical model for sparse networks with overlappingcommunity structure. The model is based on representing the graph as anexchangeable point process, and naturally generalizes existing probabilisticmodels with overlapping block-structure to the sparse regime. Our constructionbuilds on vectors of completely random measures, and has interpretableparameters, each node being assigned a vector representing its level ofaffiliation to some latent communities. We develop methods for simulating thisclass of random graphs, as well as to perform posterior inference. We show thatthe proposed approach can recover interpretable structure from two real-worldnetworks and can handle graphs with thousands of nodes and tens of thousands ofedges.
arxiv-15300-294 | Low-cost and Faster Tracking Systems Using Core-sets for Pose-Estimation | http://arxiv.org/pdf/1511.09120v2.pdf | author:Soliman Nasser, Ibrahim Jubran, Dan Feldman category:cs.RO cs.CV published:2015-11-30 summary:How can a \$20 toy quadcopter hover using a weak "Internet of Things"mini-board and a web-cam? In the pose-estimation problem we need to rotate aset of $n$ marker (points) and choose one of their n! permutations, so that thesum of squared corresponding distances to another ordered set of $n$ markers isminimized. A popular heuristic for this problem is ICP. We prove that \emph{every} set has a weighted subset (core-set) of constantsize (independent of $n$), such that computing the optimal orientation of thesmall core-set would yield \emph{exactly} the same result as using the full setof $n$ markers. A deterministic algorithm for computing this core-set in $O(n)$time is provided, using the Caratheodory Theorem from computational geometry. We developed a system that enables low-cost and real-time tracking bycomputing this core-set on the cloud in one thread, and uses the last computedcore-set locally in a parallel thread. Our experimental results show how thesecore-sets can boost the tracking time and quality for large and even small setsof both IR and RGB markers on a toy quadcopter. Open source code for the systemand algorithm is provided
arxiv-15300-295 | Variance-Reduced and Projection-Free Stochastic Optimization | http://arxiv.org/pdf/1602.02101v1.pdf | author:Elad Hazan, Haipeng Luo category:cs.LG published:2016-02-05 summary:The Frank-Wolfe optimization algorithm has recently regained popularity formachine learning applications due to its projection-free property and itsability to handle structured constraints. However, in the stochastic learningsetting, it is still relatively understudied compared to the gradient descentcounterpart. In this work, leveraging a recent variance reduction technique, wepropose two stochastic Frank-Wolfe variants which substantially improveprevious results in terms of the number of stochastic gradient evaluationsneeded to achieve $1-\epsilon$ accuracy. For example, we improve from$O(\frac{1}{\epsilon})$ to $O(\ln\frac{1}{\epsilon})$ if the objective functionis smooth and strongly convex, and from $O(\frac{1}{\epsilon^2})$ to$O(\frac{1}{\epsilon^{1.5}})$ if the objective function is smooth andLipschitz. The theoretical improvement is also observed in experiments onreal-world datasets for a multiclass classification application.
arxiv-15300-296 | Harmonic Grammar in a DisCo Model of Meaning | http://arxiv.org/pdf/1602.02089v1.pdf | author:Martha Lewis, Bob Coecke category:cs.AI cs.CL published:2016-02-05 summary:The model of cognition developed in (Smolensky and Legendre, 2006) seeks tounify two levels of description of the cognitive process: the connectionist andthe symbolic. The theory developed brings together these two levels into theIntegrated Connectionist/Symbolic Cognitive architecture (ICS). Clark andPulman (2007) draw a parallel with semantics where meaning may be modelled onboth distributional and symbolic levels, developed by Coecke et al, 2010 intothe Distributional Compositional (DisCo) model of meaning. In the current work,we revisit Smolensky and Legendre (S&L)'s model. We describe the DisCoframework, summarise the key ideas in S&L's architecture, and describe howtheir description of harmony as a graded measure of grammaticality may beapplied in the DisCo model.
arxiv-15300-297 | Utilização de Grafos e Matriz de Similaridade na Sumarização Automática de Documentos Baseada em Extração de Frases | http://arxiv.org/pdf/1602.02047v1.pdf | author:Elvys Linhares Pontes category:cs.CL cs.IR published:2016-02-05 summary:The internet increased the amount of information available. However, thereading and understanding of this information are costly tasks. In thisscenario, the Natural Language Processing (NLP) applications enable veryimportant solutions, highlighting the Automatic Text Summarization (ATS), whichproduce a summary from one or more source texts. Automatically summarizing oneor more texts, however, is a complex task because of the difficulties inherentto the analysis and generation of this summary. This master's thesis describesthe main techniques and methodologies (NLP and heuristics) to generatesummaries. We have also addressed and proposed some heuristics based on graphsand similarity matrix to measure the relevance of judgments and to generatesummaries by extracting sentences. We used the multiple languages (English,French and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA(French) corpus to evaluate the developped systems. The results obtained werequite interesting.
arxiv-15300-298 | Efficient Multi-view Performance Capture of Fine-Scale Surface Detail | http://arxiv.org/pdf/1602.02023v1.pdf | author:Nadia Robertini, Edilson De Aguiar, Thomas Helten, Christian Theobalt category:cs.CV cs.GR published:2016-02-05 summary:We present a new effective way for performance capture of deforming mesheswith fine-scale time-varying surface detail from multi-view video. Our methodbuilds up on coarse 4D surface reconstructions, as obtained with commonly usedtemplate-based methods. As they only capture models of coarse-to-medium scaledetail, fine scale deformation detail is often done in a second pass by usingstereo constraints, features, or shading-based refinement. In this paper, wepropose a new effective and stable solution to this second step. Our frameworkcreates an implicit representation of the deformable mesh using a densecollection of 3D Gaussian functions on the surface, and a set of 2D Gaussiansfor the images. The fine scale deformation of all mesh vertices that maximizesphoto-consistency can be efficiently found by densely optimizing a newmodel-to-image consistency energy on all vertex positions. A principaladvantage is that our problem formulation yields a smooth closed form energywith implicit occlusion handling and analytic derivatives. Error-pronecorrespondence finding, or discrete sampling of surface displacement values arealso not needed. We show several reconstructions of human subjects wearingloose clothing, and we qualitatively and quantitatively show that we robustlycapture more detail than related methods.
arxiv-15300-299 | Preoperative Volume Determination for Pituitary Adenoma | http://arxiv.org/pdf/1602.02022v1.pdf | author:Dzenan Zukic, Jan Egger, Miriam H. A. Bauer, Daniela Kuhnt, Barbara Carl, Bernd Freisleben, Andreas Kolb, Christopher Nimsky category:cs.CV cs.CG cs.GR published:2016-02-05 summary:The most common sellar lesion is the pituitary adenoma, and sellar tumors areapproximately 10-15% of all intracranial neoplasms. Manual slice-by-slicesegmentation takes quite some time that can be reduced by using the appropriatealgorithms. In this contribution, we present a segmentation method forpituitary adenoma. The method is based on an algorithm that we have appliedrecently to segmenting glioblastoma multiforme. A modification of this schemeis used for adenoma segmentation that is much harder to perform, due to lack ofcontrast-enhanced boundaries. In our experimental evaluation, neurosurgeonsperformed manual slice-by-slice segmentation of ten magnetic resonance imaging(MRI) cases. The segmentations were compared to the segmentation results of theproposed method using the Dice Similarity Coefficient (DSC). The average DSCfor all datasets was 75.92% +/- 7.24%. A manual segmentation took about fourminutes and our algorithm required about one second.
arxiv-15300-300 | Compressive Spectral Clustering | http://arxiv.org/pdf/1602.02018v1.pdf | author:Nicolas Tremblay, Gilles Puy, Remi Gribonval, Pierre Vandergheynst category:cs.DS cs.LG stat.ML published:2016-02-05 summary:Spectral clustering has become a popular technique due to its highperformance in many contexts. It comprises three main steps: create asimilarity graph between N objects to cluster, compute the first k eigenvectorsof its Laplacian matrix to define a feature vector for each object, and runk-means on these features to separate objects into k classes. Each of thesethree steps becomes computationally intensive for large N and/or k. We proposeto speed up the last two steps based on recent results in the emerging field ofgraph signal processing: graph filtering of random signals, and random samplingof bandlimited graph signals. We prove that our method, with a gain incomputation time that can reach several orders of magnitude, is in fact anapproximation of spectral clustering, for which we are able to control theerror. We test the performance of our method on artificial and real-worldnetwork data.
