arxiv-1409-5185 | Deeply-Supervised Nets |  http://arxiv.org/abs/1409.5185  | author:Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu category:stat.ML cs.LG cs.NE published:2014-09-18 summary:Our proposed deeply-supervised nets (DSN) method simultaneously minimizesclassification error while making the learning process of hidden layers directand transparent. We make an attempt to boost the classification performance bystudying a new formulation in deep networks. Three aspects in convolutionalneural networks (CNN) style architectures are being looked at: (1) transparencyof the intermediate layers to the overall classification; (2)discriminativeness and robustness of learned features, especially in the earlylayers; (3) effectiveness in training due to the presence of the exploding andvanishing gradients. We introduce "companion objective" to the individualhidden layers, in addition to the overall objective at the output layer (adifferent strategy to layer-wise pre-training). We extend techniques fromstochastic gradient methods to analyze our algorithm. The advantage of ourmethod is evident and our experimental result on benchmark datasets showssignificant performance gain over existing methods (e.g. all state-of-the-artresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).
arxiv-1409-5209 | Pedestrian Detection with Spatially Pooled Features and Structured Ensemble Learning |  http://arxiv.org/abs/1409.5209  | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG published:2014-09-18 summary:Many typical applications of object detection operate within a prescribedfalse-positive range. In this situation the performance of a detector should beassessed on the basis of the area under the ROC curve over that range, ratherthan over the full curve, as the performance outside the range is irrelevant.This measure is labelled as the partial area under the ROC curve (pAUC). Wepropose a novel ensemble learning method which achieves a maximal detectionrate at a user-defined range of false positive rates by directly optimizing thepartial AUC using structured learning. In order to achieve a high object detection performance, we propose a newapproach to extract low-level visual features based on spatial pooling.Incorporating spatial pooling improves the translational invariance and thusthe robustness of the detection process. Experimental results on both syntheticand real-world data sets demonstrate the effectiveness of our approach, and weshow that it is possible to train state-of-the-art pedestrian detectors usingthe proposed structured ensemble learning method with spatially pooledfeatures. The result is the current best reported performance on theCaltech-USA pedestrian detection dataset.
arxiv-1409-5402 | SAME but Different: Fast and High-Quality Gibbs Parameter Estimation |  http://arxiv.org/abs/1409.5402  | author:Huasha Zhao, Biye Jiang, John Canny category:cs.LG stat.ML K.3.2; D.1.3 published:2014-09-18 summary:Gibbs sampling is a workhorse for Bayesian inference but has severallimitations when used for parameter estimation, and is often much slower thannon-sampling inference methods. SAME (State Augmentation for MarginalEstimation) \cite{Doucet99,Doucet02} is an approach to MAP parameter estimationwhich gives improved parameter estimates over direct Gibbs sampling. SAME canbe viewed as cooling the posterior parameter distribution and allows annealedsearch for the MAP parameters, often yielding very high quality (lower loss)estimates. But it does so at the expense of additional samples per iterationand generally slower performance. On the other hand, SAME dramaticallyincreases the parallelism in the sampling schedule, and is an excellent matchfor modern (SIMD) hardware. In this paper we explore the application of SAME tographical model inference on modern hardware. We show that combining SAME withfactored sample representation (or approximation) gives throughput competitivewith the fastest symbolic methods, but with potentially better quality. Wedescribe experiments on Latent Dirichlet Allocation, achieving speeds similarto the fastest reported methods (online Variational Bayes) and lowercross-validated loss than other LDA implementations. The method is simple toimplement and should be applicable to many other models.
arxiv-1409-5400 | Visual Landmark Recognition from Internet Photo Collections: A Large-Scale Evaluation |  http://arxiv.org/abs/1409.5400  | author:Tobias Weyand, Bastian Leibe category:cs.CV published:2014-09-18 summary:The task of a visual landmark recognition system is to identify photographedbuildings or objects in query photos and to provide the user with relevantinformation on them. With their increasing coverage of the world's landmarkbuildings and objects, Internet photo collections are now being used as asource for building such systems in a fully automatic fashion. This processtypically consists of three steps: clustering large amounts of images by theobjects they depict; determining object names from user-provided tags; andbuilding a robust, compact, and efficient recognition index. To this date,however, there is little empirical information on how well current approachesfor those steps perform in a large-scale open-set mining and recognition task.Furthermore, there is little empirical information on how recognitionperformance varies for different types of landmark objects and where there isstill potential for improvement. With this paper, we intend to fill these gaps.Using a dataset of 500k images from Paris, we analyze each component of thelandmark recognition pipeline in order to answer the following questions: Howmany and what kinds of objects can be discovered automatically? How can we bestuse the resulting image clusters to recognize the object in a query? How canthe object be efficiently represented in memory for recognition? How reliablycan semantic information be extracted? And finally: What are the limitingfactors in the resulting pipeline from query to semantics? We evaluate howdifferent choices of methods and parameters for the individual pipeline stepsaffect overall system performance and examine their effects for different querycategories such as buildings, paintings or sculptures.
arxiv-1409-5391 | Fused Lasso Additive Model |  http://arxiv.org/abs/1409.5391  | author:Ashley Petersen, Daniela Witten, Noah Simon category:stat.ME stat.ML published:2014-09-18 summary:We consider the problem of predicting an outcome variable using $p$covariates that are measured on $n$ independent observations, in the setting inwhich flexible and interpretable fits are desirable. We propose the fused lassoadditive model (FLAM), in which each additive function is estimated to bepiecewise constant with a small number of adaptively-chosen knots. FLAM is thesolution to a convex optimization problem, for which a simple algorithm withguaranteed convergence to the global optimum is provided. FLAM is shown to beconsistent in high dimensions, and an unbiased estimator of its degrees offreedom is proposed. We evaluate the performance of FLAM in a simulation studyand on two data sets.
arxiv-1409-5330 | Learning and approximation capability of orthogonal super greedy algorithm |  http://arxiv.org/abs/1409.5330  | author:Jian Fang, Shaobo Lin, Zongben Xu category:cs.LG F.2.2 published:2014-09-18 summary:We consider the approximation capability of orthogonal super greedyalgorithms (OSGA) and its applications in supervised learning. OSGA isconcerned with selecting more than one atoms in each iteration step, which, ofcourse, greatly reduces the computational burden when compared with theconventional orthogonal greedy algorithm (OGA). We prove that even for functionclasses that are not the convex hull of the dictionary, OSGA does not degradethe approximation capability of OGA provided the dictionary is incoherent.Based on this, we deduce a tight generalization error bound for OSGA learning.Our results show that in the realm of supervised learning, OSGA provides apossibility to further reduce the computational burden of OGA in the premise ofmaintaining its prominent generalization capability.
arxiv-1409-5188 | Fingerprint Classification Based on Depth Neural Network |  http://arxiv.org/abs/1409.5188  | author:Ruxin Wang, Congying Han, Yanping Wu, Tiande Guo category:cs.CV published:2014-09-18 summary:Fingerprint classification is an effective technique for reducing thecandidate numbers of fingerprints in the stage of matching in automaticfingerprint identification system (AFIS). In recent years, deep learning is anemerging technology which has achieved great success in many fields, such asimage processing, natural language processing and so on. In this paper, we onlychoose the orientation field as the input feature and adopt a new method(stacked sparse autoencoders) based on depth neural network for fingerprintclassification. For the four-class problem, we achieve a classification of 93.1percent using the depth network structure which has three hidden layers (with1.8% rejection) in the NIST-DB4 database. And then we propose a novel methodusing two classification probabilities for fuzzy classification which caneffectively enhance the accuracy of classification. By only adjusting theprobability threshold, we get the accuracy of classification is 96.1% (settingthreshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (settingthreshold is 0.95). Using the fuzzy method, we obtain higher accuracy thanother methods.
arxiv-1409-5230 | Deep Regression for Face Alignment |  http://arxiv.org/abs/1409.5230  | author:Baoguang Shi, Xiang Bai, Wenyu Liu, Jingdong Wang category:cs.CV published:2014-09-18 summary:In this paper, we present a deep regression approach for face alignment. Thedeep architecture consists of a global layer and multi-stage local layers. Weapply the back-propagation algorithm with the dropout strategy to jointlyoptimize the regression parameters. We show that the resulting deep regressorgradually and evenly approaches the true facial landmarks stage by stage,avoiding the tendency to yield over-strong early stage regressors whileover-weak later stage regressors. Experimental results show that our approachachieves the state-of-the-art
arxiv-1409-5326 | Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX): Comparing multi-electrode recordings from simulated and biological mammalian cortical tissue |  http://arxiv.org/abs/1409.5326  | author:Richard J. Tomsett, Matt Ainsworth, Alexander Thiele, Mehdi Sanayei, Xing Chen, Alwin Gieselmann, Miles A. Whittington, Mark O. Cunningham, Marcus Kaiser category:q-bio.NC cs.AI cs.NE published:2014-09-18 summary:Local field potentials (LFPs) sampled with extracellular electrodes arefrequently used as a measure of population neuronal activity. However, relatingsuch measurements to underlying neuronal behaviour and connectivity isnon-trivial. To help study this link, we developed the Virtual ElectrodeRecording Tool for EXtracellular potentials (VERTEX). We first identified areduced neuron model that retained the spatial and frequency filteringcharacteristics of extracellular potentials from neocortical neurons. We thendeveloped VERTEX as an easy-to-use Matlab tool for simulating LFPs from largepopulations (>100 000 neurons). A VERTEX-based simulation successfullyreproduced features of the LFPs from an in vitro multi-electrode arrayrecording of macaque neocortical tissue. Our model, with virtual electrodesplaced anywhere in 3D, allows direct comparisons with the in vitro recordingsetup. We envisage that VERTEX will stimulate experimentalists, clinicians, andcomputational neuroscientists to use models to understand the mechanismsunderlying measured brain dynamics in health and disease.
arxiv-1409-5114 | A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-resolution |  http://arxiv.org/abs/1409.5114  | author:Shuxin Ouyang, Timothy Hospedales, Yi-Zhe Song, Xueming Li category:cs.CV published:2014-09-17 summary:Heterogeneous face recognition (HFR) refers to matching face imagery acrossdifferent domains. It has received much interest from the research community asa result of its profound implications in law enforcement. A wide variety of newinvariant features, cross-modality matching models and heterogeneous datasetsbeing established in recent years. This survey provides a comprehensive reviewof established techniques and recent developments in HFR. Moreover, we offer adetailed account of datasets and benchmarks commonly used for evaluation. Wefinish by assessing the state of the field and discussing promising directionsfor future research.
arxiv-1409-5165 | A Method for Stopping Active Learning Based on Stabilizing Predictions and the Need for User-Adjustable Stopping |  http://arxiv.org/abs/1409.5165  | author:Michael Bloodgood, K. Vijay-Shanker category:cs.LG cs.CL stat.ML published:2014-09-17 summary:A survey of existing methods for stopping active learning (AL) reveals theneeds for methods that are: more widely applicable; more aggressive in savingannotations; and more stable across changing datasets. A new method forstopping AL based on stabilizing predictions is presented that addresses theseneeds. Furthermore, stopping methods are required to handle a broad range ofdifferent annotation/performance tradeoff valuations. Despite this, theexisting body of work is dominated by conservative methods with little (if any)attention paid to providing users with control over the behavior of stoppingmethods. The proposed method is shown to fill a gap in the level ofaggressiveness available for stopping AL and supports providing users withcontrol over stopping behavior.
arxiv-1409-6689 | Visual Words for Automatic Lip-Reading |  http://arxiv.org/abs/1409.6689  | author:Ahmad Basheer Hassanat category:cs.CV published:2014-09-17 summary:Lip reading is used to understand or interpret speech without hearing it, atechnique especially mastered by people with hearing difficulties. The abilityto lip read enables a person with a hearing impairment to communicate withothers and to engage in social activities, which otherwise would be difficult.Recent advances in the fields of computer vision, pattern recognition, andsignal processing has led to a growing interest in automating this challengingtask of lip reading. Indeed, automating the human ability to lip read, aprocess referred to as visual speech recognition, could open the door for othernovel applications. This thesis investigates various issues faced by anautomated lip-reading system and proposes a novel "visual words" based approachto automatic lip reading. The proposed approach includes a novel automatic facelocalisation scheme and a lip localisation method.
arxiv-1409-4835 | Taking into Account the Differences between Actively and Passively Acquired Data: The Case of Active Learning with Support Vector Machines for Imbalanced Datasets |  http://arxiv.org/abs/1409.4835  | author:Michael Bloodgood, K. Vijay-Shanker category:cs.LG cs.CL stat.ML published:2014-09-17 summary:Actively sampled data can have very different characteristics than passivelysampled data. Therefore, it's promising to investigate using differentinference procedures during AL than are used during passive learning (PL). Thisgeneral idea is explored in detail for the focused case of AL withcost-weighted SVMs for imbalanced data, a situation that arises for many HLTtasks. The key idea behind the proposed InitPA method for addressing imbalanceis to base cost models during AL on an estimate of overall corpus imbalancecomputed via a small unbiased sample rather than the imbalance in the labeledtraining data, which is the leading method used during PL.
arxiv-1409-5009 | Distance Shrinkage and Euclidean Embedding via Regularized Kernel Estimation |  http://arxiv.org/abs/1409.5009  | author:Luwan Zhang, Grace Wahba, Ming Yuan category:stat.ML math.ST stat.ME stat.TH published:2014-09-17 summary:Although recovering an Euclidean distance matrix from noisy observations is acommon problem in practice, how well this could be done remains largelyunknown. To fill in this void, we study a simple distance matrix estimate basedupon the so-called regularized kernel estimate. We show that such an estimatecan be characterized as simply applying a constant amount of shrinkage to allobserved pairwise distances. This fact allows us to establish risk bounds forthe estimate implying that the true distances can be estimated consistently inan average sense as the number of objects increases. In addition, such acharacterization suggests an efficient algorithm to compute the distance matrixestimator, as an alternative to the usual second order cone programming knownnot to scale well for large problems. Numerical experiments and an applicationin visualizing the diversity of Vpu protein sequences from a recent HIV-1 studyfurther demonstrate the practical merits of the proposed method.
arxiv-1409-4988 | An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery |  http://arxiv.org/abs/1409.4988  | author:Filippo Maria Bianchi, Enrico Maiorino, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.LG cs.DC cs.MA published:2014-09-17 summary:We propose a multi-agent algorithm able to automatically discover relevantregularities in a given dataset, determining at the same time the set ofconfigurations of the adopted parametric dissimilarity measure yielding compactand separated clusters. Each agent operates independently by performing aMarkovian random walk on a suitable weighted graph representation of the inputdataset. Such a weighted graph representation is induced by the specificparameter configuration of the dissimilarity measure adopted by the agent,which searches and takes decisions autonomously for one cluster at a time.Results show that the algorithm is able to discover parameter configurationsthat yield a consistent and interpretable collection of clusters. Moreover, wedemonstrate that our algorithm shows comparable performances with other similarstate-of-the-art algorithms when facing specific clustering problems.
arxiv-1409-4958 | Tensity Research Based on the Information of Eye Movement |  http://arxiv.org/abs/1409.4958  | author:Yi Wang category:cs.RO cs.CV published:2014-09-17 summary:User's mental state is concerned gradually, during the interaction course ofhuman robot. As the measurement and identification method of psychologicalstate, tension, has certain practical significance role. At presents there isno suitable method of measuring the tension. Firstly, sum up some availabilityof eye movement index. And then parameters extraction on eye movementcharacteristics of normal illumination is studied, including the location ofthe face, eyes location, access to the pupil diameter, the eye pupil centercharacteristic parameters. And with the judgment of the tension in eye images,extract exact information of gaze direction. Finally, through the experiment toprove the proposed method is effective.
arxiv-1409-4936 | Ensembles of Random Sphere Cover Classifiers |  http://arxiv.org/abs/1409.4936  | author:Anthony Bagnall, Reda Younsi category:cs.LG cs.AI stat.ML published:2014-09-17 summary:We propose and evaluate alternative ensemble schemes for a new instance basedlearning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fusesinstances into spheres, then bases classification on distance to spheres ratherthan distance to instances. The randomised nature of RSC makes it ideal for usein ensembles. We propose two ensemble methods tailored to the RSC classifier;$\alpha \beta$RSE, an ensemble based on instance resampling and $\alpha$RSSE, asubspace ensemble. We compare $\alpha \beta$RSE and $\alpha$RSSE to tree basedensembles on a set of UCI datasets and demonstrates that RSC ensembles performsignificantly better than some of these ensembles, and not significantly worsethan the others. We demonstrate via a case study on six gene expression datasets that $\alpha$RSSE can outperform other subspace ensemble methods on highdimensional data when used in conjunction with an attribute filter. Finally, weperform a set of Bias/Variance decomposition experiments to analyse the sourceof improvement in comparison to a base classifier.
arxiv-1409-4928 | Statistical inference with probabilistic graphical models |  http://arxiv.org/abs/1409.4928  | author:Angélique Drémeau, Christophe Schülke, Yingying Xu, Devavrat Shah category:cs.LG stat.ML published:2014-09-17 summary:These are notes from the lecture of Devavrat Shah given at the autumn school"Statistical Physics, Optimization, Inference, and Message-Passing Algorithms",that took place in Les Houches, France from Monday September 30th, 2013, tillFriday October 11th, 2013. The school was organized by Florent Krzakala fromUPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, LenkaZdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from PolitecnicoTorino. This lecture of Devavrat Shah (MIT) covers the basics of inference andlearning. It explains how inference problems are represented within structuresknown as graphical models. The theoretical basis of the belief propagationalgorithm is then explained and derived. This lecture sets the stage forgeneralizations and applications of message passing algorithms.
arxiv-1409-4842 | Going Deeper with Convolutions |  http://arxiv.org/abs/1409.4842  | author:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich category:cs.CV published:2014-09-17 summary:We propose a deep convolutional neural network architecture codenamed"Inception", which was responsible for setting the new state of the art forclassification and detection in the ImageNet Large-Scale Visual RecognitionChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is theimproved utilization of the computing resources inside the network. This wasachieved by a carefully crafted design that allows for increasing the depth andwidth of the network while keeping the computational budget constant. Tooptimize quality, the architectural decisions were based on the Hebbianprinciple and the intuition of multi-scale processing. One particularincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22layers deep network, the quality of which is assessed in the context ofclassification and detection.
arxiv-1409-4995 | Adaptive Tag Selection for Image Annotation |  http://arxiv.org/abs/1409.4995  | author:Xixi He, Xirong Li, Gang Yang, Jieping Xu, Qin Jin category:cs.CV published:2014-09-17 summary:Not all tags are relevant to an image, and the number of relevant tags isimage-dependent. Although many methods have been proposed for imageauto-annotation, the question of how to determine the number of tags to beselected per image remains open. The main challenge is that for a large tagvocabulary, there is often a lack of ground truth data for acquiring optimalcutoff thresholds per tag. In contrast to previous works that pre-specify thenumber of tags to be selected, we propose in this paper adaptive tag selection.The key insight is to divide the vocabulary into two disjoint subsets, namely aseen set consisting of tags having ground truth available for optimizing theirthresholds and a novel set consisting of tags without any ground truth. Such adivision allows us to estimate how many tags shall be selected from the novelset according to the tags that have been selected from the seen set. Theeffectiveness of the proposed method is justified by our participation in theImageCLEF 2014 image annotation task. On a set of 2,065 test images with groundtruth available for 207 tags, the benchmark evaluation shows that compared tothe popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tagselection achieves a higher F-score of 0.223. Moreover, by treating theunderlying image annotation system as a black box, the new method can be usedas an easy plug-in to boost the performance of existing systems.
arxiv-1409-5103 | Optimality of Poisson processes intensity learning with Gaussian processes |  http://arxiv.org/abs/1409.5103  | author:Alisa Kirichenko, Harry van Zanten category:math.ST stat.ML stat.TH published:2014-09-17 summary:In this paper we provide theoretical support for the so-called "SigmoidalGaussian Cox Process" approach to learning the intensity of an inhomogeneousPoisson process on a $d$-dimensional domain. This method was proposed by Adams,Murray and MacKay (ICML, 2009), who developed a tractable computationalapproach and showed in simulation and real data experiments that it can workquite satisfactorily. The results presented in the present paper providetheoretical underpinning of the method. In particular, we show how to tune thepriors on the hyper parameters of the model in order for the procedure toautomatically adapt to the degree of smoothness of the unknown intensity and toachieve optimal convergence rates.
arxiv-1409-4698 | A Mixtures-of-Experts Framework for Multi-Label Classification |  http://arxiv.org/abs/1409.4698  | author:Charmgil Hong, Iyad Batal, Milos Hauskrecht category:cs.LG I.2.6 published:2014-09-16 summary:We develop a novel probabilistic approach for multi-label classification thatis based on the mixtures-of-experts architecture combined with recentlyintroduced conditional tree-structured Bayesian networks. Our approach capturesdifferent input-output relations from multi-label data using the efficienttree-structured classifiers, while the mixtures-of-experts architecture aims tocompensate for the tree-structured restrictions and build a more accuratemodel. We develop and present algorithms for learning the model from data andfor performing multi-label predictions on future data instances. Experiments onmultiple benchmark datasets demonstrate that our approach achieves highlycompetitive results and outperforms the existing state-of-the-art multi-labelclassification methods.
arxiv-1409-4565 | Improving files availability for BitTorrent using a diffusion model |  http://arxiv.org/abs/1409.4565  | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NI cs.NE published:2014-09-16 summary:The BitTorrent mechanism effectively spreads file fragments by copying therarest fragments first. We propose to apply a mathematical model for thediffusion of fragments on a P2P in order to take into account both the effectsof peer distances and the changing availability of peers while time goes on.Moreover, we manage to provide a forecast on the availability of a torrentthanks to a neural network that models the behaviour of peers on the P2Psystem. The combination of the mathematical model and the neural networkprovides a solution for choosing file fragments that need to be copied first,in order to ensure their continuous availability, counteracting possibledisconnections by some peers.
arxiv-1409-4689 | Compute Less to Get More: Using ORC to Improve Sparse Filtering |  http://arxiv.org/abs/1409.4689  | author:Johannes Lederer, Sergio Guadarrama category:cs.CV cs.LG published:2014-09-16 summary:Sparse Filtering is a popular feature learning algorithm for imageclassification pipelines. In this paper, we connect the performance of SparseFiltering with spectral properties of the corresponding feature matrices. Thisconnection provides new insights into Sparse Filtering; in particular, itsuggests early stopping of Sparse Filtering. We therefore introduce the OptimalRoundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. Weshow that this stopping criterion is related with pre-processing proceduressuch as Statistical Whitening and demonstrate that it can make imageclassification with Sparse Filtering considerably faster and more accurate.
arxiv-1409-5079 | Predictive Capacity of Meteorological Data - Will it rain tomorrow |  http://arxiv.org/abs/1409.5079  | author:Bilal Ahmed category:cs.LG published:2014-09-16 summary:With the availability of high precision digital sensors and cheap storagemedium, it is not uncommon to find large amounts of data collected on almostall measurable attributes, both in nature and man-made habitats. Weather inparticular has been an area of keen interest for researchers to develop moreaccurate and reliable prediction models. This paper presents a set ofexperiments which involve the use of prevalent machine learning techniques tobuild models to predict the day of the week given the weather data for thatparticular day i.e. temperature, wind, rain etc., and test their reliabilityacross four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. Theresults provide a comparison of accuracy of these machine learning techniquesand their reliability to predict the day of the week by analysing the weatherdata. We then apply the models to predict weather conditions based on theavailable data.
arxiv-1409-4714 | Modeling the average shortest path length in growth of word-adjacency networks |  http://arxiv.org/abs/1409.4714  | author:Andrzej Kulig, Stanislaw Drozdz, Jaroslaw Kwapien, Pawel Oswiecimka category:cs.CL physics.soc-ph published:2014-09-16 summary:We investigate properties of evolving linguistic networks defined by theword-adjacency relation. Such networks belong to the category of networks withaccelerated growth but their shortest path length appears to reveal the networksize dependence of different functional form than the ones known so far. Wethus compare the networks created from literary texts with their artificialsubstitutes based on different variants of the Dorogovtsev-Mendes model andobserve that none of them is able to properly simulate the novel asymptotics ofthe shortest path length. Then, we identify the local chain-like linear growthinduced by grammar and style as a missing element in this model and extend itby incorporating such effects. It is in this way that a satisfactory agreementwith the empirical result is obtained.
arxiv-1409-4617 | The Role of Emotions in Propagating Brands in Social Networks |  http://arxiv.org/abs/1409.4617  | author:Ronald Hochreiter, Christoph Waldhauser category:cs.SI cs.CL stat.ML published:2014-09-16 summary:A key aspect of word of mouth marketing are emotions. Emotions in texts helppropagating messages in conventional advertising. In word of mouth scenarios,emotions help to engage consumers and incite to propagate the message further.While the function of emotions in offline marketing in general and word ofmouth marketing in particular is rather well understood, online marketing canonly offer a limited view on the function of emotions. In this contribution weseek to close this gap. We therefore investigate how emotions function insocial media. To do so, we collected more than 30,000 brand marketing messagesfrom the Google+ social networking site. Using state of the art computationallinguistics classifiers, we compute the sentiment of these messages. Startingout with Poisson regression-based baseline models, we seek to replicate earlierfindings using this large data set. We extend upon earlier research bycomputing multi-level mixed effects models that compare the function ofemotions across different industries. We find that while the well known notionof activating emotions propagating messages holds in general for our data aswell. But there are significant differences between the observed industries.
arxiv-1409-4481 | Real-time Crowd Tracking using Parameter Optimized Mixture of Motion Models |  http://arxiv.org/abs/1409.4481  | author:Aniket Bera, David Wolinski, Julien Pettré, Dinesh Manocha category:cs.CV published:2014-09-16 summary:We present a novel, real-time algorithm to track the trajectory of eachpedestrian in moderately dense crowded scenes. Our formulation is based on anadaptive particle-filtering scheme that uses a combination of variousmulti-agent heterogeneous pedestrian simulation models. We automaticallycompute the optimal parameters for each of these different models based onprior tracked data and use the best model as motion prior for ourparticle-filter based tracking algorithm. We also use our "mixture of motionmodels" for adaptive particle selection and accelerate the performance of theonline tracking algorithm. The motion model parameter estimation is formulatedas an optimization problem, and we use an approach that solves thiscombinatorial optimization problem in a model independent manner and hencescalable to any multi-agent pedestrian motion model. We evaluate theperformance of our approach on different crowd video datasets and highlight theimprovement in accuracy over homogeneous motion models and a baselinemean-shift based tracker. In practice, our formulation can compute trajectoriesof tens of pedestrians on a multi-core desktop CPU in in real time and offerhigher accuracy as compared to prior real time pedestrian tracking algorithms.
arxiv-1409-4573 | Non-linear Causal Inference using Gaussianity Measures |  http://arxiv.org/abs/1409.4573  | author:Daniel Hernández-Lobato, Pablo Morales-Mombiela, David Lopez-Paz, Alberto Suárez category:stat.ML published:2014-09-16 summary:We provide theoretical and empirical evidence for a type of asymmetry betweencauses and effects that is present when these are related via linear modelscontaminated with additive non-Gaussian noise. Assuming that the causes and theeffects have the same distribution, we show that the distribution of theresiduals of a linear fit in the anti-causal direction is closer to a Gaussianthan the distribution of the residuals in the causal direction. ThisGaussianization effect is characterized by reduction of the magnitude of thehigh-order cumulants and by an increment of the differential entropy of theresiduals. The problem of non-linear causal inference is addressed byperforming an embedding in an expanded feature space, in which the relationbetween causes and effects can be assumed to be linear. The effectiveness of amethod to discriminate between causes and effects based on this type ofasymmetry is illustrated in a variety of experiments using different measuresof Gaussianity. The proposed method is shown to be competitive withstate-of-the-art techniques for causal inference.
arxiv-1409-4504 | Voting for Deceptive Opinion Spam Detection |  http://arxiv.org/abs/1409.4504  | author:Tao Wang, Hua Zhu category:cs.CL cs.SI published:2014-09-16 summary:Consumers' purchase decisions are increasingly influenced by user-generatedonline reviews. Accordingly, there has been growing concern about the potentialfor posting deceptive opinion spam fictitious reviews that have beendeliberately written to sound authentic, to deceive the readers. Existingapproaches mainly focus on developing automatic supervised learning basedmethods to help users identify deceptive opinion spams. This work, we used the LSI and Sprinkled LSI technique to reduce thedimension for deception detection. We make our contribution to demonstrate whatLSI is capturing in latent semantic space and reveal how deceptive opinions canbe recognized automatically from truthful opinions. Finally, we proposed avoting scheme which integrates different approaches to further improve theclassification performance.
arxiv-1409-4627 | DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF Features |  http://arxiv.org/abs/1409.4627  | author:Petra Budikova, Jan Botorek, Michal Batko, Pavel Zezula category:cs.IR cs.CV published:2014-09-16 summary:This paper constitutes an extension to the report on DISA-MU teamparticipation in the ImageCLEF 2014 Scalable Concept Image Annotation Task aspublished in [3]. Specifically, we introduce a new similarity search componentthat was implemented into the system, report on the results achieved byutilizing this component, and analyze the influence of different similaritysearch parameters on the annotation quality.
arxiv-1409-4757 | Collapsed Variational Bayes Inference of Infinite Relational Model |  http://arxiv.org/abs/1409.4757  | author:Katsuhiko Ishiguro, Issei Sato, Naonori Ueda category:cs.LG stat.ML published:2014-09-16 summary:The Infinite Relational Model (IRM) is a probabilistic model for relationaldata clustering that partitions objects into clusters based on observedrelationships. This paper presents Averaged CVB (ACVB) solutions for IRM,convergence-guaranteed and practically useful fast Collapsed Variational Bayes(CVB) inferences. We first derive ordinary CVB and CVB0 for IRM based on thelower bound maximization. CVB solutions yield deterministic iterativeprocedures for inferring IRM given the truncated number of clusters. Ourproposal includes CVB0 updates of hyperparameters including the concentrationparameter of the Dirichlet Process, which has not been studied in theliterature. To make the CVB more practically useful, we further study the CVBinference in two aspects. First, we study the convergence issues and develop aconvergence-guaranteed algorithm for any CVB-based inferences called ACVB,which enables automatic convergence detection and frees non-expertpractitioners from difficult and costly manual monitoring of inferenceprocesses. Second, we present a few techniques for speeding up IRM inferences.In particular, we describe the linear time inference of CVB0, allowing the IRMfor larger relational data uses. The ACVB solutions of IRM showed comparable orbetter performance compared to existing inference methods in experiments, andprovide deterministic, faster, and easier convergence detection.
arxiv-1409-4559 | A Combined Method Of Fractal And GLCM Features For MRI And CT Scan Images Classification |  http://arxiv.org/abs/1409.4559  | author:Redouan Korchiyne, Sidi Mohamed Farssi, Abderrahmane Sbihi, Rajaa Touahni, Mustapha Tahiri Alaoui category:cs.CV published:2014-09-16 summary:Fractal analysis has been shown to be useful in image processing forcharacterizing shape and gray-scale complexity. The fractal feature is acompact descriptor used to give a numerical measure of the degree ofirregularity of the medical images. This descriptor property does not giveownership of the local image structure. In this paper, we present a combinationof this parameter based on Box Counting with GLCM Features. This powerfulcombination has proved good results especially in classification of medicaltexture from MRI and CT Scan images of trabecular bone. This method has thepotential to improve clinical diagnostics tests for osteoporosis pathologies.
arxiv-1409-4566 | Multivariate Comparison of Classification Algorithms |  http://arxiv.org/abs/1409.4566  | author:Olcay Taner Yildiz, Ethem Alpaydin category:stat.ML cs.LG published:2014-09-16 summary:Statistical tests that compare classification algorithms are univariate anduse a single performance measure, e.g., misclassification error, $F$ measure,AUC, and so on. In multivariate tests, comparison is done using multiplemeasures simultaneously. For example, error is the sum of false positives andfalse negatives and a univariate test on error cannot make a distinctionbetween these two sources, but a 2-variate test can. Similarly, instead ofcombining precision and recall in $F$ measure, we can have a 2-variate test on(precision, recall). We use Hotelling's multivariate $T^2$ test for comparingtwo algorithms, and when we have three or more algorithms we use themultivariate analysis of variance (MANOVA) followed by pairwise post hoc tests.In our experiments, we see that multivariate tests have higher power thanunivariate tests, that is, they can detect differences that univariate testscannot. We also discuss how multivariate analysis allows us to automaticallyextract performance measures that best distinguish the behavior of multiplealgorithms.
arxiv-1409-4747 | Anomaly Detection Based on Indicators Aggregation |  http://arxiv.org/abs/1409.4747  | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2014-09-16 summary:Automatic anomaly detection is a major issue in various areas. Beyond meredetection, the identification of the source of the problem that produced theanomaly is also essential. This is particularly the case in aircraft enginehealth monitoring where detecting early signs of failure (anomalies) andhelping the engine owner to implement efficiently the adapted maintenanceoperations (fixing the source of the anomaly) are of crucial importance toreduce the costs attached to unscheduled maintenance. This paper introduces ageneral methodology that aims at classifying monitoring signals into normalones and several classes of abnormal ones. The main idea is to leverage expertknowledge by generating a very large number of binary indicators. Eachindicator corresponds to a fully parametrized anomaly detector built fromparametric anomaly scores designed by experts. A feature selection method isused to keep only the most discriminant indicators which are used at inputs ofa Naive Bayes classifier. This give an interpretable classifier based oninterpretable anomaly detectors whose parameters have been optimized indirectlyby the selection process. The proposed methodology is evaluated on simulateddata designed to reproduce some of the anomaly types observed in real worldengines.
arxiv-1409-4614 | Lexical Normalisation of Twitter Data |  http://arxiv.org/abs/1409.4614  | author:Bilal Ahmed category:cs.CL published:2014-09-16 summary:Twitter with over 500 million users globally, generates over 100,000 tweetsper minute . The 140 character limit per tweet, perhaps unintentionally,encourages users to use shorthand notations and to strip spellings to theirbare minimum "syllables" or elisions e.g. "srsly". The analysis of twittermessages which typically contain misspellings, elisions, and grammaticalerrors, poses a challenge to established Natural Language Processing (NLP)tools which are generally designed with the assumption that the data conformsto the basic grammatical structure commonly used in English language. In orderto make sense of Twitter messages it is necessary to first transform them intoa canonical form, consistent with the dictionary or grammar. This process,performed at the level of individual tokens ("words"), is called lexicalnormalisation. This paper investigates various techniques for lexicalnormalisation of Twitter data and presents the findings as the techniques areapplied to process raw data from Twitter.
arxiv-1409-4271 | The Ordered Weighted $\ell_1$ Norm: Atomic Formulation, Projections, and Algorithms |  http://arxiv.org/abs/1409.4271  | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.DS cs.CV cs.IT cs.LG math.IT published:2014-09-15 summary:The ordered weighted $\ell_1$ norm (OWL) was recently proposed, with twodifferent motivations: its good statistical properties as a sparsity promotingregularizer; the fact that it generalizes the so-called {\it octagonalshrinkage and clustering algorithm for regression} (OSCAR), which has theability to cluster/group regression variables that are highly correlated. Thispaper contains several contributions to the study and application of OWLregularization: the derivation of the atomic formulation of the OWL norm; thederivation of the dual of the OWL norm, based on its atomic formulation; a newand simpler derivation of the proximity operator of the OWL norm; an efficientscheme to compute the Euclidean projection onto an OWL ball; the instantiationof the conditional gradient (CG, also known as Frank-Wolfe) algorithm forlinear regression problems under OWL regularization; the instantiation ofaccelerated projected gradient algorithms for the same class of problems.Finally, a set of experiments give evidence that accelerated projected gradientalgorithms are considerably faster than CG, for the class of problemsconsidered.
arxiv-1409-4366 | The Randomized Causation Coefficient |  http://arxiv.org/abs/1409.4366  | author:David Lopez-Paz, Krikamol Muandet, Benjamin Recht category:stat.ML published:2014-09-15 summary:We are interested in learning causal relationships between pairs of randomvariables, purely from observational data. To effectively address this task,the state-of-the-art relies on strong assumptions regarding the mechanismsmapping causes to effects, such as invertibility or the existence of additivenoise, which only hold in limited situations. On the contrary, this short paperproposes to learn how to perform causal inference directly from data, andwithout the need of feature engineering. In particular, we pose causality as akernel mean embedding classification problem, where inputs are samples fromarbitrary probability distributions on pairs of random variables, and labelsare types of causal relationships. We validate the performance of our method onsynthetic and real-world data against the state-of-the-art. Moreover, wesubmitted our algorithm to the ChaLearn's "Fast Causation CoefficientChallenge" competition, with which we won the fastest code prize and rankedthird in the overall leaderboard.
arxiv-1409-4139 | A feasible roadmap for developing volumetric probability atlas of localized prostate cancer |  http://arxiv.org/abs/1409.4139  | author:Liang Zhao, Jianhua Xuan, Yue Wang category:q-bio.QM cs.CV published:2014-09-15 summary:A statistical volumetric model, showing the probability map of localizedprostate cancer within the host anatomical structure, has been developed from90 optically-imaged surgical specimens. This master model permits an accuratecharacterization of prostate cancer distribution patterns and an atlas-informedbiopsy sampling strategy. The model is constructed by mapping individualprostate models onto a site model, together with localized tumors. An accuratemulti-object non-rigid warping scheme is developed based on a mixture ofprincipal-axis registrations. We report our evaluation and pilot studies on theeffectiveness of the method and its application to optimizing needle biopsystrategies.
arxiv-1409-4141 | Probabilistic Network Metrics: Variational Bayesian Network Centrality |  http://arxiv.org/abs/1409.4141  | author:Harold Soh category:stat.ML published:2014-09-15 summary:Network metrics form a fundamental part of the network analysis toolbox. Usedto quantitatively measure different aspects of the network, these metrics cangive insights into the underlying network structure and function. In this work,we connect network metrics to modern probabilistic machine learning. We focuson the centrality metric, which is used a wide variety of applications from websearch to gene-analysis. First, we formulate an eigenvector-based Bayesiancentrality model for determining node importance. Compared to existing methods,our probabilistic model allows for the assimilation of multiple edge weightobservations, the inclusion of priors and the extraction of uncertainties. Toenable tractable inference, we develop a variational lower bound (VBC) that isdemonstrated to be effective on a variety of networks (two synthetic and fivereal-world graphs). We then bridge this model to sparse Gaussian processes. Thesparse variational Bayesian centrality Gaussian process (VBC-GP) learns amapping between node attributes to latent centrality and hence, is capable ofpredicting centralities from node features and can potentially represent alarge number of nodes using only a limited number of inducing inputs.Experiments show that the VBC-GP learns high-quality mappings and comparesfavorably to a two-step baseline, i.e., a full GP trained on the nodeattributes and pre-computed centralities. Finally, we present two case-studiesusing the VBC-GP: first, to ascertain relevant features in a taxi transportnetwork and second, to distribute a limited number of vaccines to mitigate theseverity of a viral outbreak.
arxiv-1409-4327 | Zero Shot Recognition with Unreliable Attributes |  http://arxiv.org/abs/1409.4327  | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV stat.ML published:2014-09-15 summary:In principle, zero-shot learning makes it possible to train a recognitionmodel simply by specifying the category's attributes. For example, withclassifiers for generic attributes like \emph{striped} and \emph{four-legged},one can construct a classifier for the zebra category by enumerating whichproperties it possesses---even without providing zebra training images. Inpractice, however, the standard zero-shot paradigm suffers because attributepredictions in novel images are hard to get right. We propose a novel randomforest approach to train zero-shot models that explicitly accounts for theunreliability of attribute predictions. By leveraging statistics about eachattribute's error tendencies, our method obtains more robust discriminativemodels for the unseen classes. We further devise extensions to handle thefew-shot scenario and unreliable attribute descriptions. On three datasets, wedemonstrate the benefit for visual category learning with zero or few trainingexamples, a critical domain for rare categories or categories defined on thefly.
arxiv-1409-4169 | An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech Synthesis of Sanskrit Verse |  http://arxiv.org/abs/1409.4169  | author:Rama N., Meenakshi Lakshmanan category:cs.CL published:2014-09-15 summary:The rendering of Sanskrit poetry from text to speech is a problem that hasnot been solved before. One reason may be the complications in the languageitself. We present unique algorithms based on extensive empirical analysis, tosynthesize speech from a given text input of Sanskrit verses. Using apre-recorded audio units database which is itself tremendously reduced in sizecompared to the colossal size that would otherwise be required, the algorithmswork on producing the best possible, tunefully rendered chanting of the givenverse. His would enable the visually impaired and those with readingdisabilities to easily access the contents of Sanskrit verses otherwiseavailable only in writing.
arxiv-1409-4364 | Computational Algorithms Based on the Paninian System to Process Euphonic Conjunctions for Word Searches |  http://arxiv.org/abs/1409.4364  | author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL published:2014-09-15 summary:Searching for words in Sanskrit E-text is a problem that is accompanied bycomplexities introduced by features of Sanskrit such as euphonic conjunctionsor sandhis. A word could occur in an E-text in a transformed form owing to theoperation of rules of sandhi. Simple word search would not yield thesetransformed forms of the word. Further, there is no search engine in theliterature that can comprehensively search for words in Sanskrit E-texts takingeuphonic conjunctions into account. This work presents an optimal binaryrepresentational schema for letters of the Sanskrit alphabet along withalgorithms to efficiently process the sandhi rules of Sanskrit grammar. Thework further presents an algorithm that uses the sandhi processing algorithm toperform a comprehensive word search on E-text.
arxiv-1409-4354 | A Binary Schema and Computational Algorithms to Process Vowel-based Euphonic Conjunctions for Word Searches |  http://arxiv.org/abs/1409.4354  | author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL published:2014-09-15 summary:Comprehensively searching for words in Sanskrit E-text is a non-trivialproblem because words could change their forms in different contexts. One suchcontext is sandhi or euphonic conjunctions, which cause a word to change owingto the presence of adjacent letters or words. The change wrought by thesepossible conjunctions can be so significant in Sanskrit that a simple searchfor the word in its given form alone can significantly reduce the success levelof the search. This work presents a representational schema that representsletters in a binary format and reduces Paninian rules of euphonic conjunctionsto simple bit set-unset operations. The work presents an efficient algorithm toprocess vowel-based sandhis using this schema. It further presents anotheralgorithm that uses the sandhi processor to generate the possible transformedword forms of a given word to use in a comprehensive word search.
arxiv-1409-4155 | Active Metric Learning from Relative Comparisons |  http://arxiv.org/abs/1409.4155  | author:Sicheng Xiong, Rómer Rosales, Yuanli Pei, Xiaoli Z. Fern category:cs.LG published:2014-09-15 summary:This work focuses on active learning of distance metrics from relativecomparison information. A relative comparison specifies, for a data pointtriplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to$x_k$. Such constraints, when available, have been shown to be useful towarddefining appropriate distance metrics. In real-world applications, acquiringconstraints often require considerable human effort. This motivates us to studyhow to select and query the most useful relative comparisons to achieveeffective metric learning with minimum user effort. Given an underlying classconcept that is employed by the user to provide such constraints, we present aninformation-theoretic criterion that selects the triplet whose answer leads tothe highest expected gain in information about the classes of a set ofexamples. Directly applying the proposed criterion requires examining $O(n^3)$triplets with $n$ instances, which is prohibitive even for datasets of moderatesize. We show that a randomized selection strategy can be used to reduce theselection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-sizeproblems. Experiments show that the proposed method consistently outperformstwo baseline policies.
arxiv-1409-4205 | Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers |  http://arxiv.org/abs/1409.4205  | author:B. Conejo, N. Komodakis, S. Leprince, J. P. Avouac category:cs.CV published:2014-09-15 summary:We propose a general and versatile framework that significantly speeds-upgraphical model optimization while maintaining an excellent solution accuracy.The proposed approach relies on a multi-scale pruning scheme that is able toprogressively reduce the solution space by use of a novel strategy based on acoarse-to-fine cascade of learnt classifiers. We thoroughly experiment withclassic computer vision related MRF problems, where our framework constantlyyields a significant time speed-up (with respect to the most efficientinference methods) and obtains a more accurate solution than directlyoptimizing the MRF.
arxiv-1409-4469 | Convolutional Networks for Image Processing by Coupled Oscillator Arrays |  http://arxiv.org/abs/1409.4469  | author:Dmitri E. Nikonov, Ian A. Young, George I. Bourianoff category:nlin.PS cs.CV published:2014-09-15 summary:A coupled oscillator array is shown to approximate convolutions with Gaborfilters for image processing tasks. Pixelated image fragments and filterfunctions are converted to voltages, differenced, and input into acorresponding array of weakly coupled Voltage Controlled Oscillators (VCOs).This is referred to as Frequency Shift Keying (FSK). Upon synchronization ofthe array, the common node amplitude provides a metric for the degree of matchbetween the image fragment and the filter function. The optimal oscillatorparameters for synchronization are determined and favor a moderate value of theQ-factor.
arxiv-1409-4326 | Computing the Stereo Matching Cost with a Convolutional Neural Network |  http://arxiv.org/abs/1409.4326  | author:Jure Žbontar, Yann LeCun category:cs.CV cs.LG cs.NE published:2014-09-15 summary:We present a method for extracting depth information from a rectified imagepair. We train a convolutional neural network to predict how well two imagepatches match and use it to compute the stereo matching cost. The cost isrefined by cross-based cost aggregation and semiglobal matching, followed by aleft-right consistency check to eliminate errors in the occluded regions. Ourstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset andis currently (August 2014) the top performing method on this dataset.
arxiv-1409-4349 | On the optimality of shape and data representation in the spectral domain |  http://arxiv.org/abs/1409.4349  | author:Yonathan Aflalo, Haim Brezis, Ron Kimmel category:cs.CV published:2014-09-15 summary:A proof of the optimality of the eigenfunctions of the Laplace-Beltramioperator (LBO) in representing smooth functions on surfaces is provided andadapted to the field of applied shape and data analysis. It is based on theCourant-Fischer min-max principle adapted to our case. % The theorem we presentsupports the new trend in geometry processing of treating geometric structuresby using their projection onto the leading eigenfunctions of the decompositionof the LBO. Utilisation of this result can be used for constructing numericallyefficient algorithms to process shapes in their spectrum. We review a couple ofapplications as possible practical usage cases of the proposed optimalitycriteria. % We refer to a scale invariant metric, which is also invariant tobending of the manifold. This novel pseudo-metric allows constructing an LBO bywhich a scale invariant eigenspace on the surface is defined. We demonstratethe efficiency of an intermediate metric, defined as an interpolation betweenthe scale invariant and the regular one, in representing geometric structureswhile capturing both coarse and fine details. Next, we review a numericalacceleration technique for classical scaling, a member of a family offlattening methods known as multidimensional scaling (MDS). There, theoptimality is exploited to efficiently approximate all geodesic distancesbetween pairs of points on a given surface, and thereby match and comparebetween almost isometric surfaces. Finally, we revisit the classical principalcomponent analysis (PCA) definition by coupling its variational form with aDirichlet energy on the data manifold. By pairing the PCA with the LBO we canhandle cases that go beyond the scope defined by the observation set that ishandled by regular PCA.
arxiv-1409-4320 | Self-Dictionary Sparse Regression for Hyperspectral Unmixing: Greedy Pursuit and Pure Pixel Search are Related |  http://arxiv.org/abs/1409.4320  | author:Xiao Fu, Wing-Kin Ma, Tsung-Han Chan, José M. Bioucas-Dias category:stat.ML cs.IT math.IT math.OC published:2014-09-15 summary:This paper considers a recently emerged hyperspectral unmixing formulationbased on sparse regression of a self-dictionary multiple measurement vector(SD-MMV) model, wherein the measured hyperspectral pixels are used as thedictionary. Operating under the pure pixel assumption, this SD-MMV formalism isspecial in that it allows simultaneous identification of the endmember spectralsignatures and the number of endmembers. Previous SD-MMV studies mainly focuson convex relaxations. In this study, we explore the alternative of greedypursuit, which generally provides efficient and simple algorithms. Inparticular, we design a greedy SD-MMV algorithm using simultaneous orthogonalmatching pursuit. Intriguingly, the proposed greedy algorithm is shown to beclosely related to some existing pure pixel search algorithms, especially, thesuccessive projection algorithm (SPA). Thus, a link between SD-MMV and purepixel search is revealed. We then perform exact recovery analyses, and provethat the proposed greedy algorithm is robust to noise---including itsidentification of the (unknown) number of endmembers---under a sufficiently lownoise level. The identification performance of the proposed greedy algorithm isdemonstrated through both synthetic and real-data experiments.
arxiv-1409-4127 | Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network |  http://arxiv.org/abs/1409.4127  | author:Yu-Chuan Su, Tzu-Hsuan Chiu, Chun-Yen Yeh, Hsin-Fu Huang, Winston H. Hsu category:cs.CV cs.LG published:2014-09-15 summary:Unconstrained video recognition and Deep Convolution Network (DCN) are twoactive topics in computer vision recently. In this work, we apply DCNs asframe-based recognizers for video recognition. Our preliminary studies,however, show that video corpora with complete ground truth are usually notlarge and diverse enough to learn a robust model. The networks trained directlyon the video data set suffer from significant overfitting and have poorrecognition rate on the test set. The same lack-of-training-sample problemlimits the usage of deep models on a wide range of computer vision problemswhere obtaining training data are difficult. To overcome the problem, weperform transfer learning from images to videos to utilize the knowledge in theweakly labeled image corpus for video recognition. The image corpus help tolearn important visual patterns for natural images, while these patterns areignored by models trained only on the video corpus. Therefore, the resultantnetworks have better generalizability and better recognition rate. We show thatby means of transfer learning from image to video, we can learn a frame-basedrecognizer with only 4k videos. Because the image corpus is weakly labeled, theentire learning process requires only 4k annotated instances, which is far lessthan the million scale image data sets required by previous works. The sameapproach may be applied to other visual recognition tasks where only scarcetraining data is available, and it improves the applicability of DCNs invarious computer vision problems. Our experiments also reveal the correlationbetween meta-parameters and the performance of DCNs, given the properties ofthe target problem and data. These results lead to a heuristic formeta-parameter selection for future researches, which does not rely on the timeconsuming meta-parameter search.
arxiv-1409-4005 | Sparse Estimation with Strongly Correlated Variables using Ordered Weighted L1 Regularization |  http://arxiv.org/abs/1409.4005  | author:Mario A. T. Figueiredo, Robert D. Nowak category:stat.ML published:2014-09-14 summary:This paper studies ordered weighted L1 (OWL) norm regularization for sparseestimation problems with strongly correlated variables. We prove sufficientconditions for clustering based on the correlation/colinearity of variablesusing the OWL norm, of which the so-called OSCAR is a particular case. Ourresults extend previous ones for OSCAR in several ways: for the squared errorloss, our conditions hold for the more general OWL norm and under weakerassumptions; we also establish clustering conditions for the absolute errorloss, which is, as far as we know, a novel result. Furthermore, we characterizethe statistical performance of OWL norm regularization for generative models inwhich certain clusters of regression variables are strongly (even perfectly)correlated, but variables in different clusters are uncorrelated. We show thatif the true p-dimensional signal generating the data involves only s of theclusters, then O(s log p) samples suffice to accurately estimate the signal,regardless of the number of coefficients within the clusters. The estimation ofs-sparse signals with completely independent variables requires just as manymeasurements. In other words, using the OWL we pay no price (in terms of thenumber of measurements) for the presence of strongly correlated variables.
arxiv-1409-4046 | A New Framework for Retinex based Color Image Enhancement using Particle Swarm Optimization |  http://arxiv.org/abs/1409.4046  | author:M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu, V. N Manjunath Aradhya category:cs.CV 68T45 H.2.0 published:2014-09-14 summary:A new approach for tuning the parameters of MultiScale Retinex (MSR) basedcolor image enhancement algorithm using a popular optimization method, namely,Particle Swarm Optimization (PSO) is presented in this paper. The imageenhancement using MSR scheme heavily depends on parameters such as Gaussiansurround space constant, number of scales, gain and offset etc. Selection ofthese parameters, empirically and its application to MSR scheme to produceinevitable results are the major blemishes. The method presented here resultsin huge savings of computation time as well as improvement in the visualquality of an image, since the PSO exploited maximizes the MSR parameters. Theobjective of PSO is to validate the visual quality of the enhanced imageiteratively using an effective objective criterion based on entropy and edgeinformation of an image. The PSO method of parameter optimization of MSR schemeachieves a very good quality of reconstructed images, far better than thatpossible with the other existing methods. Finally, the quality of the enhancedcolor images obtained by the proposed method are evaluated using novel metric,namely, Wavelet Energy (WE). The experimental results presented show that colorimages enhanced using the proposed scheme are clearer, more vivid andefficient.
arxiv-1409-4018 | EquiNMF: Graph Regularized Multiview Nonnegative Matrix Factorization |  http://arxiv.org/abs/1409.4018  | author:Daniel Hidru, Anna Goldenberg category:cs.LG cs.NA published:2014-09-14 summary:Nonnegative matrix factorization (NMF) methods have proved to be powerfulacross a wide range of real-world clustering applications. Integrating multipletypes of measurements for the same objects/subjects allows us to gain a deeperunderstanding of the data and refine the clustering. We have developed a novelGraph-reguarized multiview NMF-based method for data integration calledEquiNMF. The parameters for our method are set in a completely automateddata-specific unsupervised fashion, a highly desirable property in real-worldapplications. We performed extensive and comprehensive experiments on multiviewimaging data. We show that EquiNMF consistently outperforms other single-viewNMF methods used on concatenated data and multi-view NMF methods with differenttypes of regularizations.
arxiv-1409-4011 | Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces |  http://arxiv.org/abs/1409.4011  | author:Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, Michael A. Osborne category:stat.ML published:2014-09-14 summary:In practical Bayesian optimization, we must often search over structures withdiffering numbers of parameters. For instance, we may wish to search overneural network architectures with an unknown number of layers. To relateperformance data gathered for different architectures, we define a new kernelfor conditional parameter spaces that explicitly includes information aboutwhich parameters are relevant in a given structure. We show that this kernelimproves model quality and Bayesian optimization results over several simplerbaseline kernels.
arxiv-1409-4095 | Cavlectometry: Towards Holistic Reconstruction of Large Mirror Objects |  http://arxiv.org/abs/1409.4095  | author:Jonathan Balzer, Daniel Acevedo-Feliz, Stefano Soatto, Sebastian Höfer, Markus Hadwiger, Jürgen Beyerer category:cs.CV published:2014-09-14 summary:We introduce a method based on the deflectometry principle for thereconstruction of specular objects exhibiting significant size and geometriccomplexity. A key feature of our approach is the deployment of an AutomaticVirtual Environment (CAVE) as pattern generator. To unfold the full power ofthis extraordinary experimental setup, an optical encoding scheme is developedwhich accounts for the distinctive topology of the CAVE. Furthermore, we devisean algorithm for detecting the object of interest in raw deflectometric images.The segmented foreground is used for single-view reconstruction, the backgroundfor estimation of the camera pose, necessary for calibrating the sensor system.Experiments suggest a significant gain of coverage in single measurementscompared to previous methods. To facilitate research on specular surfacereconstruction, we will make our data set publicly available.
arxiv-1409-4014 | Mining Mid-level Features for Action Recognition Based on Effective Skeleton Representation |  http://arxiv.org/abs/1409.4014  | author:Pichao Wang, Wanqing Li, Philip Ogunbona, Zhimin Gao, Hanling Zhang category:cs.CV published:2014-09-14 summary:Recently, mid-level features have shown promising performance in computervision. Mid-level features learned by incorporating class-level information arepotentially more discriminative than traditional low-level local features. Inthis paper, an effective method is proposed to extract mid-level features fromKinect skeletons for 3D human action recognition. Firstly, the orientations oflimbs connected by two skeleton joints are computed and each orientation isencoded into one of the 27 states indicating the spatial relationship of thejoints. Secondly, limbs are combined into parts and the limb's states aremapped into part states. Finally, frequent pattern mining is employed to minethe most frequent and relevant (discriminative, representative andnon-redundant) states of parts in continuous several frames. These parts arereferred to as Frequent Local Parts or FLPs. The FLPs allow us to buildpowerful bag-of-FLP-based action representation. This new representation yieldsstate-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D.
arxiv-1409-4044 | A new approach in machine learning |  http://arxiv.org/abs/1409.4044  | author:Alain Tapp category:stat.ML cs.LG published:2014-09-14 summary:In this technical report we presented a novel approach to machine learning.Once the new framework is presented, we will provide a simple and yet verypowerful learning algorithm which will be benchmark on various dataset. The framework we proposed is based on booleen circuits; more specifically theclassifier produced by our algorithm have that form. Using bits and booleangates instead of real numbers and multiplication enable the the learningalgorithm and classifier to use very efficient boolean vector operations. Thisenable both the learning algorithm and classifier to be extremely efficient.The accuracy of the classifier we obtain with our framework compares veryfavorably those produced by conventional techniques, both in terms ofefficiency and accuracy.
arxiv-1409-4043 | Design of Novel Algorithm and Architecture for Gaussian Based Color Image Enhancement System for Real Time Applications |  http://arxiv.org/abs/1409.4043  | author:M. C. Hanumantharaju, M. Ravishankar, D. R. Rameshbabu category:cs.AR cs.CV published:2014-09-14 summary:This paper presents the development of a new algorithm for Gaussian basedcolor image enhancement system. The algorithm has been designed intoarchitecture suitable for FPGA/ASIC implementation. The color image enhancementis achieved by first convolving an original image with a Gaussian kernel sinceGaussian distribution is a point spread function which smoothen the image.Further, logarithm-domain processing and gain/offset corrections are employedin order to enhance and translate pixels into the display range of 0 to 255.The proposed algorithm not only provides better dynamic range compression andcolor rendition effect but also achieves color constancy in an image. Thedesign exploits high degrees of pipelining and parallel processing to achievereal time performance. The design has been realized by RTL compliant Verilogcoding and fits into a single FPGA with a gate count utilization of 321,804.The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148FPGA device and is capable of processing high resolution color motion picturesof sizes of up to 1600x1200 pixels at the real time video rate of 116 framesper second. This shows that the proposed design would work for not only stillimages but also for high resolution video sequences.
arxiv-1409-3912 | Parallel Distributed Block Coordinate Descent Methods based on Pairwise Comparison Oracle |  http://arxiv.org/abs/1409.3912  | author:Kota Matsui, Wataru Kumagai, Takafumi Kanamori category:stat.ML cs.LG published:2014-09-13 summary:This paper provides a block coordinate descent algorithm to solveunconstrained optimization problems. In our algorithm, computation of functionvalues or gradients is not required. Instead, pairwise comparison of functionvalues is used. Our algorithm consists of two steps; one is the directionestimate step and the other is the search step. Both steps require onlypairwise comparison of function values, which tells us only the order offunction values over two points. In the direction estimate step, a Newton typesearch direction is estimated. A computation method like block coordinatedescent methods is used with the pairwise comparison. In the search step, anumerical solution is updated along the estimated direction. The computation inthe direction estimate step can be easily parallelized, and thus, the algorithmworks efficiently to find the minimizer of the objective function. Also, weshow an upper bound of the convergence rate. In numerical experiments, we showthat our method efficiently finds the optimal solution compared to someexisting methods based on the pairwise comparison.
arxiv-1409-3942 | Polarity detection movie reviews in hindi language |  http://arxiv.org/abs/1409.3942  | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.CL cs.IR published:2014-09-13 summary:Nowadays peoples are actively involved in giving comments and reviews onsocial networking websites and other websites like shopping websites, newswebsites etc. large number of people everyday share their opinion on the web,results is a large number of user data is collected .users also find it trivialtask to read all the reviews and then reached into the decision. It would bebetter if these reviews are classified into some category so that the userfinds it easier to read. Opinion Mining or Sentiment Analysis is a naturallanguage processing task that mines information from various text forms such asreviews, news, and blogs and classify them on the basis of their polarity aspositive, negative or neutral. But, from the last few years, user content inHindi language is also increasing at a rapid rate on the Web. So it is veryimportant to perform opinion mining in Hindi language as well. In this paper aHindi language opinion mining system is proposed. The system classifies thereviews as positive, negative and neutral for Hindi language. Negation is alsohandled in the proposed system. Experimental results using reviews of moviesshow the effectiveness of the system
arxiv-1409-3970 | A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data |  http://arxiv.org/abs/1409.3970  | author:Yin Zheng, Yu-Jin Zhang, Hugo Larochelle category:cs.CV cs.IR cs.LG cs.NE published:2014-09-13 summary:Topic modeling based on latent Dirichlet allocation (LDA) has been aframework of choice to deal with multimodal data, such as in image annotationtasks. Another popular approach to model the multimodal data is through deepneural networks, such as the deep Boltzmann machine (DBM). Recently, a new typeof topic model called the Document Neural Autoregressive Distribution Estimator(DocNADE) was proposed and demonstrated state-of-the-art performance for textdocument modeling. In this work, we show how to successfully apply and extendthis model to multimodal data, such as simultaneous image classification andannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,that increases the discriminative power of the learned hidden topic featuresand show how to employ it to learn a joint representation from image visualwords, annotation words and class label information. We test our model on theLabelMe and UIUC-Sports data sets and show that it compares favorably to othertopic models. Second, we propose a deep extension of our model and provide anefficient way of training the deep model. Experimental results show that ourdeep model outperforms its shallow version and reaches state-of-the-artperformance on the Multimedia Information Retrieval (MIR) Flickr data set.
arxiv-1409-3964 | Self-taught Object Localization with Deep Networks |  http://arxiv.org/abs/1409.3964  | author:Loris Bazzani, Alessandro Bergamo, Dragomir Anguelov, Lorenzo Torresani category:cs.CV published:2014-09-13 summary:This paper introduces self-taught object localization, a novel approach thatleverages deep convolutional networks trained for whole-image recognition tolocalize objects in images without additional human supervision, i.e., withoutusing any ground-truth bounding boxes for training. The key idea is to analyzethe change in the recognition scores when artificially masking out differentregions of the image. The masking out of a region that includes the objecttypically causes a significant drop in recognition score. This idea is embeddedinto an agglomerative clustering technique that generates self-taughtlocalization hypotheses. Our object localization scheme outperforms existingproposal methods in both precision and recall for small number of subwindowproposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over thestate-of-the-art for top-1 hypothesis). Furthermore, our experiments show thatthe annotations automatically-generated by our method can be used to trainobject detectors yielding recognition results remarkably close to thoseobtained by training on manually-annotated bounding boxes.
arxiv-1409-3906 | Structure Preserving Large Imagery Reconstruction |  http://arxiv.org/abs/1409.3906  | author:Ju Shen, Jianjun Yang, Sami Taha-abusneineh, Bryson Payne, Markus Hitz category:cs.CV published:2014-09-13 summary:With the explosive growth of web-based cameras and mobile devices, billionsof photographs are uploaded to the internet. We can trivially collect a hugenumber of photo streams for various goals, such as image clustering, 3D scenereconstruction, and other big data applications. However, such tasks are noteasy due to the fact the retrieved photos can have large variations in theirview perspectives, resolutions, lighting, noises, and distortions.Fur-thermore, with the occlusion of unexpected objects like people, vehicles,it is even more challenging to find feature correspondences and reconstructre-alistic scenes. In this paper, we propose a structure-based image completionalgorithm for object removal that produces visually plausible content withconsistent structure and scene texture. We use an edge matching technique toinfer the potential structure of the unknown region. Driven by the estimatedstructure, texture synthesis is performed automatically along the estimatedcurves. We evaluate the proposed method on different types of images: fromhighly structured indoor environment to natural scenes. Our experimentalresults demonstrate satisfactory performance that can be potentially used forsubsequent big data processing, such as image localization, object retrieval,and scene reconstruction. Our experiments show that this approach achievesfavorable results that outperform existing state-of-the-art techniques.
arxiv-1409-3924 | A study on effectiveness of extreme learning machine |  http://arxiv.org/abs/1409.3924  | author:Yuguang Wang, Feilong Cao, Yubo Yuan category:cs.NE cs.LG published:2014-09-13 summary:Extreme learning machine (ELM), proposed by Huang et al., has been shown apromising learning algorithm for single-hidden layer feedforward neuralnetworks (SLFNs). Nevertheless, because of the random choice of input weightsand biases, the ELM algorithm sometimes makes the hidden layer output matrix Hof SLFN not full column rank, which lowers the effectiveness of ELM. This paperdiscusses the effectiveness of ELM and proposes an improved algorithm calledEELM that makes a proper selection of the input weights and bias beforecalculating the output weights, which ensures the full column rank of H intheory. This improves to some extend the learning rate (testing accuracy,prediction accuracy, learning time) and the robustness property of thenetworks. The experimental results based on both the benchmark functionapproximation and real-world problems including classification and regressionapplications show the good performances of EELM.
arxiv-1409-3913 | Concurrent Tracking of Inliers and Outliers |  http://arxiv.org/abs/1409.3913  | author:Jae-Yeong Lee, Wonpil Yu category:cs.CV published:2014-09-13 summary:In object tracking, outlier is one of primary factors which degradeperformance of image-based tracking algorithms. In this respect, therefore,most of the existing methods simply discard detected outliers and pay little orno attention to employing them as an important source of information for motionestimation. We consider outliers as important as inliers for object trackingand propose a motion estimation algorithm based on concurrent tracking ofinliers and outliers. Our tracker makes use of pyramidal implementation of theLucas-Kanade tracker to estimate motion flows of inliers and outliers and finaltarget motion is estimated robustly based on both of these information.Experimental results from challenging benchmark video sequences confirmenhanced tracking performance, showing highly stable target tracking undersevere occlusion compared with state-of-the-art algorithms. The proposedalgorithm runs at more than 100 frames per second even without using a hardwareaccelerator, which makes the proposed method more practical and portable.
arxiv-1409-3768 | Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection |  http://arxiv.org/abs/1409.3768  | author:Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam category:stat.CO cs.LG stat.ML published:2014-09-12 summary:Sparse high dimensional graphical model selection is a popular topic incontemporary machine learning. To this end, various useful approaches have beenproposed in the context of $\ell_1$-penalized estimation in the Gaussianframework. Though many of these inverse covariance estimation approaches aredemonstrably scalable and have leveraged recent advances in convexoptimization, they still depend on the Gaussian functional form. To addressthis gap, a convex pseudo-likelihood based partial correlation graph estimationmethod (CONCORD) has been recently proposed. This method uses coordinate-wiseminimization of a regression based pseudo-likelihood, and has been shown tohave robust model selection properties in comparison with the Gaussianapproach. In direct contrast to the parallel work in the Gaussian settinghowever, this new convex pseudo-likelihood framework has not leveraged theextensive array of methods that have been proposed in the machine learningliterature for convex optimization. In this paper, we address this crucial gapby proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) forperforming $\ell_1$-regularized inverse covariance matrix estimation in thepseudo-likelihood framework. We present timing comparisons with coordinate-wiseminimization and demonstrate that our approach yields tremendous payoffs for$\ell_1$-penalized partial correlation graph estimation outside the Gaussiansetting, thus yielding the fastest and most scalable approach for suchproblems. We undertake a theoretical analysis of our approach and rigorouslydemonstrate convergence, and also derive rates thereof.
arxiv-1409-3879 | Unsupervised learning of clutter-resistant visual representations from natural videos |  http://arxiv.org/abs/1409.3879  | author:Qianli Liao, Joel Z. Leibo, Tomaso Poggio category:cs.CV cs.LG published:2014-09-12 summary:Populations of neurons in inferotemporal cortex (IT) maintain an explicitcode for object identity that also tolerates transformations of objectappearance e.g., position, scale, viewing angle [1, 2, 3]. Though the learningrules are not known, recent results [4, 5, 6] suggest the operation of anunsupervised temporal-association-based method e.g., Foldiak's trace rule [7].Such methods exploit the temporal continuity of the visual world by assumingthat visual experience over short timescales will tend to have invariantidentity content. Thus, by associating representations of frames from nearbytimes, a representation that tolerates whatever transformations occurred in thevideo may be achieved. Many previous studies verified that such rules can workin simple situations without background clutter, but the presence of visualclutter has remained problematic for this approach. Here we show that temporalassociation based on large class-specific filters (templates) avoids theproblem of clutter. Our system learns in an unsupervised way from naturalvideos gathered from the internet, and is able to perform a difficultunconstrained face recognition task on natural images: Labeled Faces in theWild [8].
arxiv-1409-4244 | An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem |  http://arxiv.org/abs/1409.4244  | author:Enrique Gabriel Baquela, Ana Carolina Olivera category:cs.NE published:2014-09-12 summary:The lane reversal has proven to be a useful method to mitigate trafficcongestion during rush hour or in case of specific events that affect hightraffic volumes. In this work we propose a methodology that is placed withinoptimization via Simulation, by means of which a multi-objective geneticalgorithm and simulations of traffic are used to determine the configuration ofideal lane reversal.
arxiv-1409-3870 | Text mixing shapes the anatomy of rank-frequency distributions: A modern Zipfian mechanics for natural language |  http://arxiv.org/abs/1409.3870  | author:Jake Ryland Williams, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL physics.soc-ph published:2014-09-12 summary:Natural languages are full of rules and exceptions. One of the most famousquantitative rules is Zipf's law which states that the frequency of occurrenceof a word is approximately inversely proportional to its rank. Though this`law' of ranks has been found to hold across disparate texts and forms of data,analyses of increasingly large corpora over the last 15 years have revealed theexistence of two scaling regimes. These regimes have thus far been explained bya hypothesis suggesting a separability of languages into core and non-corelexica. Here, we present and defend an alternative hypothesis, that the twoscaling regimes result from the act of aggregating texts. We observe that textmixing leads to an effective decay of word introduction, which we show providesaccurate predictions of the location and severity of breaks in scaling. Uponexamining large corpora from 10 languages in the Project Gutenberg eBookscollection (eBooks), we find emphatic empirical support for the universality ofour claim.
arxiv-1409-3660 | 10,000+ Times Accelerated Robust Subset Selection (ARSS) |  http://arxiv.org/abs/1409.3660  | author:Feiyun Zhu, Bin Fan, Xinliang Zhu, Ying Wang, Shiming Xiang, Chunhong Pan category:cs.LG cs.CV stat.ML published:2014-09-12 summary:Subset selection from massive data with noised information is increasinglypopular for various applications. This problem is still highly challenging ascurrent methods are generally slow in speed and sensitive to outliers. Toaddress the above two issues, we propose an accelerated robust subset selection(ARSS) method. Specifically in the subset selection area, this is the firstattempt to employ the $\ell_{p}(0<p\leq1)$-norm based measure for therepresentation loss, preventing large errors from dominating our objective. Asa result, the robustness against outlier elements is greatly enhanced.Actually, data size is generally much larger than feature length, i.e. $N\ggL$. Based on this observation, we propose a speedup solver (via ALM andequivalent derivations) to highly reduce the computational cost, theoreticallyfrom $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmarkdatasets verify that our method not only outperforms state of the art methods,but also runs 10,000+ times faster than the most related method.
arxiv-1409-4276 | A Fast Quartet Tree Heuristic for Hierarchical Clustering |  http://arxiv.org/abs/1409.4276  | author:Rudi L. Cilibrasi, Paul M. B. Vitanyi category:cs.LG cs.CE cs.DS published:2014-09-12 summary:The Minimum Quartet Tree Cost problem is to construct an optimal weight treefrom the $3{n \choose 4}$ weighted quartet topologies on $n$ objects, whereoptimality means that the summed weight of the embedded quartet topologies isoptimal (so it can be the case that the optimal tree embeds all quartets asnonoptimal topologies). We present a Monte Carlo heuristic, based on randomizedhill climbing, for approximating the optimal weight tree, given the quartettopology weights. The method repeatedly transforms a dendrogram, with allobjects involved as leaves, achieving a monotonic approximation to the exactsingle globally optimal tree. The problem and the solution heuristic has beenextensively used for general hierarchical clustering of nontree-like(non-phylogeny) data in various domains and across domains with heterogeneousdata. We also present a greatly improved heuristic, reducing the running timeby a factor of order a thousand to ten thousand. All this is implemented andavailable, as part of the CompLearn package. We compare performance and runningtime of the original and improved versions with those of UPGMA, BioNJ, and NJ,as implemented in the SplitsTree package on genomic data for which the latterare optimized. Keywords: Data and knowledge visualization, Patternmatching--Clustering--Algorithms/Similarity measures, Hierarchical clustering,Global optimization, Quartet tree, Randomized hill-climbing,
arxiv-1409-3813 | Incorporating Semi-supervised Features into Discontinuous Easy-First Constituent Parsing |  http://arxiv.org/abs/1409.3813  | author:Yannick Versley category:cs.CL published:2014-09-12 summary:This paper describes adaptations for EaFi, a parser for easy-first parsing ofdiscontinuous constituents, to adapt it to multiple languages as well as makeuse of the unlabeled data that was provided as part of the SPMRL shared task2014.
arxiv-1409-3854 | Linear, Deterministic, and Order-Invariant Initialization Methods for the K-Means Clustering Algorithm |  http://arxiv.org/abs/1409.3854  | author:M. Emre Celebi, Hassan A. Kingravi category:cs.LG cs.CV I.5.3; H.2.8 published:2014-09-12 summary:Over the past five decades, k-means has become the clustering algorithm ofchoice in many application domains primarily due to its simplicity, time/spaceefficiency, and invariance to the ordering of the data points. Unfortunately,the algorithm's sensitivity to the initial selection of the cluster centersremains to be its most serious drawback. Numerous initialization methods havebeen proposed to address this drawback. Many of these methods, however, havetime complexity superlinear in the number of data points, which makes themimpractical for large data sets. On the other hand, linear methods are oftenrandom and/or sensitive to the ordering of the data points. These methods aregenerally unreliable in that the quality of their results is unpredictable.Therefore, it is common practice to perform multiple runs of such methods andtake the output of the run that produces the best results. Such a practice,however, greatly increases the computational requirements of the otherwisehighly efficient k-means algorithm. In this chapter, we investigate theempirical performance of six linear, deterministic (non-random), andorder-invariant k-means initialization methods on a large and diversecollection of data sets from the UCI Machine Learning Repository. The resultsdemonstrate that two relatively unknown hierarchical initialization methods dueto Su and Dy outperform the remaining four methods with respect to twoobjective effectiveness criteria. In addition, a recent method due to Erisogluet al. performs surprisingly poorly.
arxiv-1409-3821 | Computational Implications of Reducing Data to Sufficient Statistics |  http://arxiv.org/abs/1409.3821  | author:Andrea Montanari category:stat.CO cs.IT cs.LG math.IT published:2014-09-12 summary:Given a large dataset and an estimation task, it is common to pre-process thedata by reducing them to a set of sufficient statistics. This step is oftenregarded as straightforward and advantageous (in that it simplifies statisticalanalysis). I show that -on the contrary- reducing data to sufficient statisticscan change a computationally tractable estimation problem into an intractableone. I discuss connections with recent work in theoretical computer science,and implications for some techniques to estimate graphical models.
arxiv-1409-3881 | An Approach to Reducing Annotation Costs for BioNLP |  http://arxiv.org/abs/1409.3881  | author:Michael Bloodgood, K. Vijay-Shanker category:cs.CL cs.LG stat.ML published:2014-09-12 summary:There is a broad range of BioNLP tasks for which active learning (AL) cansignificantly reduce annotation costs and a specific AL algorithm we havedeveloped is particularly effective in reducing annotation costs for thesetasks. We have previously developed an AL algorithm called ClosestInitPA thatworks best with tasks that have the following characteristics: redundancy intraining material, burdensome annotation costs, Support Vector Machines (SVMs)work well for the task, and imbalanced datasets (i.e. when set up as a binaryclassification problem, one class is substantially rarer than the other). ManyBioNLP tasks have these characteristics and thus our AL algorithm is a naturalapproach to apply to BioNLP tasks.
arxiv-1409-3714 | Time-domain multiscale shape identification in electro-sensing |  http://arxiv.org/abs/1409.3714  | author:Habib Ammari, Han Wang category:math.NA cs.CV published:2014-09-12 summary:This paper presents premier and innovative time-domain multi-scale method forshape identification in electro-sensing using pulse-type signals. The method isbased on transform-invariant shape descriptors computed from filteredpolarization tensors at multi-scales. The proposed algorithm enjoys aremarkable noise robustness even with far-field measurements at very limitedangle of view. It opens a door for pulsed imaging using echolocation andinduction data.
arxiv-1409-5671 | A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems |  http://arxiv.org/abs/1409.5671  | author:Ebru Aydin Gol, Ezio Bartocci, Calin Belta category:cs.AI cs.CE cs.LG cs.LO cs.SY published:2014-09-12 summary:We propose a technique to detect and generate patterns in a network oflocally interacting dynamical systems. Central to our approach is a novelspatial superposition logic, whose semantics is defined over the quad-tree of apartitioned image. We show that formulas in this logic can be efficientlylearned from positive and negative examples of several types of patterns. Wealso demonstrate that pattern detection, which is implemented as a modelchecking algorithm, performs very well for test data sets different from thelearning sets. We define a quantitative semantics for the logic and integratethe model checking algorithm with particle swarm optimization in acomputational framework for synthesis of parameters leading to desired patternsin reaction-diffusion systems.
arxiv-1409-3358 | Building Program Vector Representations for Deep Learning |  http://arxiv.org/abs/1409.3358  | author:Lili Mou, Ge Li, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang category:cs.SE cs.LG cs.NE published:2014-09-11 summary:Deep learning has made significant breakthroughs in various fields ofartificial intelligence. Advantages of deep learning include the ability tocapture highly complicated features, weak involvement of human engineering,etc. However, it is still virtually impossible to use deep learning to analyzeprograms since deep architectures cannot be trained effectively with pure backpropagation. In this pioneering paper, we propose the "coding criterion" tobuild program vector representations, which are the premise of deep learningfor program analysis. Our representation learning approach directly makes deeplearning a reality in this new field. We evaluate the learned vectorrepresentations both qualitatively and quantitatively. We conclude, based onthe experiments, the coding criterion is successful in building programrepresentations. To evaluate whether deep learning is beneficial for programanalysis, we feed the representations to deep neural networks, and achievehigher accuracy in the program classification task than "shallow" methods, suchas logistic regression and the support vector machine. This result confirms thefeasibility of deep learning to analyze programs. It also gives primaryevidence of its success in this new field. We believe deep learning will becomean outstanding technique for program analysis in the near future.
arxiv-1409-3518 | Topic Modeling of Hierarchical Corpora |  http://arxiv.org/abs/1409.3518  | author:Do-kyum Kim, Geoffrey M. Voelker, Lawrence K. Saul category:stat.ML cs.IR cs.LG published:2014-09-11 summary:We study the problem of topic modeling in corpora whose documents areorganized in a multi-level hierarchy. We explore a parametric approach to thisproblem, assuming that the number of topics is known or can be estimated bycross-validation. The models we consider can be viewed as special(finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). Forthese models we show that there exists a simple variational approximation forprobabilistic inference. The approximation relies on a previously unexploitedinequality that handles the conditional dependence between Dirichlet latentvariables in adjacent levels of the model's hierarchy. We compare our approachto existing implementations of nonparametric HDPs. On several benchmarks wefind that our approach is faster than Gibbs sampling and able to learn morepredictive models than existing variational methods. Finally, we demonstratethe large-scale viability of our approach on two newly available corpora fromresearchers in computer security---one with 350,000 documents and over 6,000internal subcategories, the other with a five-level deep hierarchy.
arxiv-1409-3505 | DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection |  http://arxiv.org/abs/1409.3505  | author:Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-09-11 summary:In this paper, we propose multi-stage and deformable deep convolutionalneural networks for object detection. This new deep learning object detectiondiagram has innovations in multiple aspects. In the proposed new deeparchitecture, a new deformation constrained pooling (def-pooling) layer modelsthe deformation of object parts with geometric constraint and penalty. With theproposed multi-stage training strategy, multiple classifiers are jointlyoptimized to process samples at different difficulty levels. A new pre-trainingstrategy is proposed to learn feature representations more suitable for theobject detection task and with good generalization capability. By changing thenet structures, training strategies, adding and removing some key components inthe detection pipeline, a set of models with large diversity are obtained,which significantly improves the effectiveness of modeling averaging. Theproposed approach ranked \#2 in ILSVRC 2014. It improves the mean averagedprecision obtained by RCNN, which is the state-of-the-art of object detection,from $31\%$ to $45\%$. Detailed component-wise analysis is also providedthrough extensive experimental evaluation.
arxiv-1409-4256 | Machine learning for ultrafast X-ray diffraction patterns on large-scale GPU clusters |  http://arxiv.org/abs/1409.4256  | author:Tomas Ekeberg, Stefan Engblom, Jing Liu category:q-bio.BM cs.DC cs.LG physics.bio-ph q-bio.QM published:2014-09-11 summary:The classical method of determining the atomic structure of complex moleculesby analyzing diffraction patterns is currently undergoing drastic developments.Modern techniques for producing extremely bright and coherent X-ray lasersallow a beam of streaming particles to be intercepted and hit by an ultrashorthigh energy X-ray beam. Through machine learning methods the data thuscollected can be transformed into a three-dimensional volumetric intensity mapof the particle itself. The computational complexity associated with thisproblem is very high such that clusters of data parallel accelerators arerequired. We have implemented a distributed and highly efficient algorithm forinversion of large collections of diffraction patterns targeting clusters ofhundreds of GPUs. With the expected enormous amount of diffraction data to beproduced in the foreseeable future, this is the required scale to approach realtime processing of data at the beam site. Using both real and synthetic data welook at the scaling properties of the application and discuss the overallcomputational viability of this exciting and novel imaging technique.
arxiv-1409-4727 | Selection of Most Appropriate Backpropagation Training Algorithm in Data Pattern Recognition |  http://arxiv.org/abs/1409.4727  | author:Hindayati Mustafidah, Sri Hartati, Retantyo Wardoyo, Agus Harjoko category:cs.NE published:2014-09-11 summary:There are several training algorithms for backpropagation method in neuralnetwork. Not all of these algorithms have the same accuracy level demonstratedthrough the percentage level of suitability in recognizing patterns in thedata. In this research tested 12 training algorithms specifically in recognizedata patterns of test validity. The basic network parameters used are themaximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05.Of the twelve training algorithms each performed 20 times looping. The testresults obtained that the percentage rate of the great match is trainlmalgorithm with alpha 5% have adequate levels of suitability of 87.5% at thelevel of significance of 0.000. This means the most appropriate trainingalgorithm in recognizing the the data pattern of test validity is the trainlmalgorithm.
arxiv-1409-3446 | Consensus-Based Modelling using Distributed Feature Construction |  http://arxiv.org/abs/1409.3446  | author:Haimonti Dutta, Ashwin Srinivasan category:cs.LG published:2014-09-11 summary:A particularly successful role for Inductive Logic Programming (ILP) is as atool for discovering useful relational features for subsequent use in apredictive model. Conceptually, the case for using ILP to construct relationalfeatures rests on treating these features as functions, the automated discoveryof which necessarily requires some form of first-order learning. Practically,there are now several reports in the literature that suggest that augmentingany existing features with ILP-discovered relational features can substantiallyimprove the predictive power of a model. While the approach is straightforwardenough, much still needs to be done to scale it up to explore more fully thespace of possible features that can be constructed by an ILP system. This is inprinciple, infinite and in practice, extremely large. Applications have beenconfined to heuristic or random selections from this space. In this paper, weaddress this computational difficulty by allowing features to be constructed ina distributed manner. That is, there is a network of computational units, eachof which employs an ILP engine to construct some small number of features andthen builds a (local) model. We then employ a consensus-based algorithm, inwhich neighboring nodes share information to update local models. For acategory of models (those with convex loss functions), it can be shown that thealgorithm will result in all nodes converging to a consensus model. Inpractice, it may be slow to achieve this convergence. Nevertheless, our resultson synthetic and real datasets that suggests that in relatively short time the"best" node in the network reaches a model whose predictive accuracy iscomparable to that obtained using more computational effort in anon-distributed setting (the best node is identified as the one whose weightsconverge first).
arxiv-1409-3215 | Sequence to Sequence Learning with Neural Networks |  http://arxiv.org/abs/1409.3215  | author:Ilya Sutskever, Oriol Vinyals, Quoc V. Le category:cs.CL cs.LG published:2014-09-10 summary:Deep Neural Networks (DNNs) are powerful models that have achieved excellentperformance on difficult learning tasks. Although DNNs work well whenever largelabeled training sets are available, they cannot be used to map sequences tosequences. In this paper, we present a general end-to-end approach to sequencelearning that makes minimal assumptions on the sequence structure. Our methoduses a multilayered Long Short-Term Memory (LSTM) to map the input sequence toa vector of a fixed dimensionality, and then another deep LSTM to decode thetarget sequence from the vector. Our main result is that on an English toFrench translation task from the WMT'14 dataset, the translations produced bythe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM'sBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM didnot have difficulty on long sentences. For comparison, a phrase-based SMTsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTMto rerank the 1000 hypotheses produced by the aforementioned SMT system, itsBLEU score increases to 36.5, which is close to the previous best result onthis task. The LSTM also learned sensible phrase and sentence representationsthat are sensitive to word order and are relatively invariant to the active andthe passive voice. Finally, we found that reversing the order of the words inall source sentences (but not target sentences) improved the LSTM's performancemarkedly, because doing so introduced many short term dependencies between thesource and the target sentence which made the optimization problem easier.
arxiv-1409-2944 | Collaborative Deep Learning for Recommender Systems |  http://arxiv.org/abs/1409.2944  | author:Hao Wang, Naiyan Wang, Dit-Yan Yeung category:cs.LG cs.CL cs.IR cs.NE stat.ML published:2014-09-10 summary:Collaborative filtering (CF) is a successful approach commonly used by manyrecommender systems. Conventional CF-based methods use the ratings given toitems by users as the sole source of information for learning to makerecommendation. However, the ratings are often very sparse in manyapplications, causing CF-based methods to degrade significantly in theirrecommendation performance. To address this sparsity problem, auxiliaryinformation such as item content information may be utilized. Collaborativetopic regression (CTR) is an appealing recent method taking this approach whichtightly couples the two components that learn from two different sources ofinformation. Nevertheless, the latent representation learned by CTR may not bevery effective when the auxiliary information is very sparse. To address thisproblem, we generalize recent advances in deep learning from i.i.d. input tonon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesianmodel called collaborative deep learning (CDL), which jointly performs deeprepresentation learning for the content information and collaborative filteringfor the ratings (feedback) matrix. Extensive experiments on three real-worlddatasets from different domains show that CDL can significantly advance thestate of the art.
arxiv-1412-6153 | Intelligent Indoor Mobile Robot Navigation Using Stereo Vision |  http://arxiv.org/abs/1412.6153  | author:Arjun B. Krishnan, Jayaram Kollipara category:cs.RO cs.AI cs.CV published:2014-09-10 summary:Majority of the existing robot navigation systems, which facilitate the useof laser range finders, sonar sensors or artificial landmarks, has the abilityto locate itself in an unknown environment and then build a map of thecorresponding environment. Stereo vision, while still being a rapidlydeveloping technique in the field of autonomous mobile robots, are currentlyless preferable due to its high implementation cost. This paper aims atdescribing an experimental approach for the building of a stereo vision systemthat helps the robots to avoid obstacles and navigate through indoorenvironments and at the same time remaining very much cost effective. Thispaper discusses the fusion techniques of stereo vision and ultrasound sensorswhich helps in the successful navigation through different types of complexenvironments. The data from the sensor enables the robot to create the twodimensional topological map of unknown environments and stereo vision systemsmodels the three dimension model of the same environment.
arxiv-1409-3512 | Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words |  http://arxiv.org/abs/1409.3512  | author:Udaya Raj Dhungana, Subarna Shakya, Kabita Baral, Bharat Sharma category:cs.CL published:2014-09-10 summary:This paper presents a new model of WordNet that is used to disambiguate thecorrect sense of polysemy word based on the clue words. The related words foreach sense of a polysemy word as well as single sense word are referred to asthe clue words. The conventional WordNet organizes nouns, verbs, adjectives andadverbs together into sets of synonyms called synsets each expressing adifferent concept. In contrast to the structure of WordNet, we developed a newmodel of WordNet that organizes the different senses of polysemy words as wellas the single sense words based on the clue words. These clue words for eachsense of a polysemy word as well as for single sense word are used todisambiguate the correct meaning of the polysemy word in the given contextusing knowledge based Word Sense Disambiguation (WSD) algorithms. The clue wordcan be a noun, verb, adjective or adverb.
arxiv-1409-3136 | Metric Learning for Temporal Sequence Alignment |  http://arxiv.org/abs/1409.3136  | author:Damien Garreau, Rémi Lajugie, Sylvain Arlot, Francis Bach category:cs.LG published:2014-09-10 summary:In this paper, we propose to learn a Mahalanobis distance to performalignment of multivariate time series. The learning examples for this task aretime series for which the true alignment is known. We cast the alignmentproblem as a structured prediction task, and propose realistic losses betweenalignments for which the optimization is tractable. We provide experiments onreal data in the audio to audio context, where we show that the learning of asimilarity measure leads to improvements in the performance of the alignmenttask. We also propose to use this metric learning framework to perform featureselection and, from basic audio features, build a combination of these withbetter performance for the alignment.
arxiv-1409-3078 | An improved genetic algorithm with a local optimization strategy and an extra mutation level for solving traveling salesman problem |  http://arxiv.org/abs/1409.3078  | author:Keivan Borna, Vahid Haji Hashemi category:cs.NE published:2014-09-10 summary:The Traveling salesman problem (TSP) is proved to be NP-complete in mostcases. The genetic algorithm (GA) is one of the most useful algorithms forsolving this problem. In this paper a conventional GA is compared with animproved hybrid GA in solving TSP. The improved or hybrid GA consist ofconventional GA and two local optimization strategies. The first strategy isextracting all sequential groups including four cities of samples and changingthe two central cities with each other. The second local optimization strategyis similar to an extra mutation process. In this step with a low probability asample is selected. In this sample two random cities are defined and the pathbetween these cities is reversed. The computation results show that theproposed method also finds better paths than the conventional GA within anacceptable computation time.
arxiv-1409-2993 | "Look Ma, No Hands!" A Parameter-Free Topic Model |  http://arxiv.org/abs/1409.2993  | author:Jian Tang, Ming Zhang, Qiaozhu Mei category:cs.LG cs.CL cs.IR published:2014-09-10 summary:It has always been a burden to the users of statistical topic models topredetermine the right number of topics, which is a key parameter of most topicmodels. Conventionally, automatic selection of this parameter is done througheither statistical model selection (e.g., cross-validation, AIC, or BIC) orBayesian nonparametric models (e.g., hierarchical Dirichlet process). Thesemethods either rely on repeated runs of the inference algorithm to searchthrough a large range of parameter values which does not suit the mining of bigdata, or replace this parameter with alternative parameters that are lessintuitive and still hard to be determined. In this paper, we explore to"eliminate" this parameter from a new perspective. We first present anonparametric treatment of the PLSA model named nonparametric probabilisticlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows forthe exploration and comparison of different numbers of topics within a singleexecution, yet remains as simple as that of PLSA. This is achieved bysubstituting the parameter of the number of topics with an alternativeparameter that is the minimal goodness of fit of a document. We show that thenew parameter can be further eliminated by two parameter-free treatments:either by monitoring the diversity among the discovered topics or by a weaksupervision from users in the form of an exemplar topic. The parameter-freetopic model finds the appropriate number of topics when the diversity among thediscovered topics is maximized, or when the granularity of the discoveredtopics matches the exemplar topic. Experiments on both synthetic and real dataprove that the parameter-free topic model extracts topics with a comparablequality comparing to classical topic models with "manual transmission". Thequality of the topics outperforms those extracted through classical Bayesiannonparametric models.
arxiv-1410-2175 | Image Denoising using New Adaptive Based Median Filters |  http://arxiv.org/abs/1410.2175  | author:Suman Shrestha category:cs.CV published:2014-09-10 summary:Noise is a major issue while transferring images through all kinds ofelectronic communication. One of the most common noise in electroniccommunication is an impulse noise which is caused by unstable voltage. In thispaper, the comparison of known image denoising techniques is discussed and anew technique using the decision based approach has been used for the removalof impulse noise. All these methods can primarily preserve image details whilesuppressing impulsive noise. The principle of these techniques is at firstintroduced and then analysed with various simulation results using MATLAB. Mostof the previously known techniques are applicable for the denoising of imagescorrupted with less noise density. Here a new decision based technique has beenpresented which shows better performances than those already being used. Thecomparisons are made based on visual appreciation and further quantitatively byMean Square error (MSE) and Peak Signal to Noise Ratio (PSNR) of differentfiltered images..
arxiv-1409-3005 | A Study of Association Measures and their Combination for Arabic MWT Extraction |  http://arxiv.org/abs/1409.3005  | author:Abdelkader El Mahdaouy, Saïd EL Alaoui Ouatik, Eric Gaussier category:cs.CL published:2014-09-10 summary:Automatic Multi-Word Term (MWT) extraction is a very important issue to manyapplications, such as information retrieval, question answering, and textcategorization. Although many methods have been used for MWT extraction inEnglish and other European languages, few studies have been applied to Arabic.In this paper, we propose a novel, hybrid method which combines linguistic andstatistical approaches for Arabic Multi-Word Term extraction. The maincontribution of our method is to consider contextual information and bothtermhood and unithood for association measures at the statistical filteringstep. In addition, our technique takes into account the problem of MWTvariation in the linguistic filtering step. The performance of the proposedstatistical measure (NLC-value) is evaluated using an Arabic environment corpusby comparing it with some existing competitors. Experimental results show thatour NLC-value measure outperforms the other ones in term of precision for bothbi-grams and tri-grams.
arxiv-1409-3024 | One-Dimensional Vector based Pattern Matching |  http://arxiv.org/abs/1409.3024  | author:Y. M. Fouda category:cs.CV published:2014-09-10 summary:Template matching is a basic method in image analysis to extract usefulinformation from images. In this paper, we suggest a new method for patternmatching. Our method transform the template image from two dimensional imageinto one dimensional vector. Also all sub-windows (same size of template) inthe reference image will transform into one dimensional vectors. The threesimilarity measures SAD, SSD, and Euclidean are used to compute the likenessbetween template and all sub-windows in the reference image to find the bestmatch. The experimental results show the superior performance of the proposedmethod over the conventional methods on various template of different sizes.
arxiv-1409-3257 | Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization |  http://arxiv.org/abs/1409.3257  | author:Yuchen Zhang, Lin Xiao category:math.OC stat.ML published:2014-09-10 summary:We consider a generic convex optimization problem associated with regularizedempirical risk minimization of linear predictors. The problem structure allowsus to reformulate it as a convex-concave saddle point problem. We propose astochastic primal-dual coordinate (SPDC) method, which alternates betweenmaximizing over a randomly chosen dual variable and minimizing over the primalvariable. An extrapolation step on the primal variable is performed to obtainaccelerated convergence rate. We also develop a mini-batch version of the SPDCmethod which facilitates parallel computing, and an extension with weightedsampling probabilities on the dual variables, which has a better complexitythan uniform sampling on unnormalized data. Both theoretically and empirically,we show that the SPDC method has comparable or better performance than severalstate-of-the-art optimization methods.
arxiv-1409-3040 | Towards Optimal Algorithms for Prediction with Expert Advice |  http://arxiv.org/abs/1409.3040  | author:Nick Gravin, Yuval Peres, Balasubramanian Sivan category:cs.LG cs.GT math.PR published:2014-09-10 summary:We study the classical problem of prediction with expert advice in theadversarial setting with a geometric stopping time. In 1965, Cover gave theoptimal algorithm for the case of $2$ experts. In this paper, we design theoptimal algorithm, adversary and regret for the case of $3$ experts. Further,we show that the optimal algorithm for $2$ and $3$ experts is a probabilitymatching algorithm (analogous to Thompson sampling) against a particularrandomized adversary. Remarkably, it turns out that this algorithm is not onlyoptimal against this adversary, but also minimax optimal against all possibleadversaries. We establish a constant factor separation between the regrets achieved by theoptimal algorithm and the widely used multiplicative weights algorithm. Alongthe way, we improve the regret lower bounds for the multiplicative weightsalgorithm for an arbitrary number of experts and show that this is tight for$2$ experts. A novel aspect of our analysis is that we develop upper and lower boundssimultaneously, analogous to the primal-dual method. The analysis of theoptimal adversary relies on delicate random walk estimates. We further use thisconnection to develop an improved regret bound for the case of $4$ experts, andprovide a general framework for designing the optimal algorithm for anarbitrary number of experts.
arxiv-1409-2617 | Large-scale randomized-coordinate descent methods with non-separable linear constraints |  http://arxiv.org/abs/1409.2617  | author:Sashank Reddi, Ahmed Hefny, Carlton Downey, Avinava Dubey, Suvrit Sra category:math.OC stat.ML published:2014-09-09 summary:We develop randomized (block) coordinate descent (CD) methods for linearlyconstrained convex optimization. Unlike most CD methods, we do not assume theconstraints to be separable, but let them be coupled linearly. To ourknowledge, ours is the first CD method that allows linear coupling constraints,without making the global iteration complexity have an exponential dependenceon the number of constraints. We present algorithms and analysis for four keyproblem scenarios: (i) smooth; (ii) smooth + nonsmooth separable; (iii)asynchronous parallel; and (iv) stochastic. We illustrate empirical behavior ofour algorithms by simulation experiments.
arxiv-1409-2558 | Penalty methods for a class of non-Lipschitz optimization problems |  http://arxiv.org/abs/1409.2558  | author:Xiaojun Chen, Zhaosong Lu, Ting Kei Pong category:math.OC stat.ML published:2014-09-09 summary:We consider a class of constrained optimization problems with a possiblynonconvex non-Lipschitz objective and a convex feasible set being theintersection of a polyhedron and a possibly degenerate ellipsoid. Such problemshave a wide range of applications in data science, where the objective is usedfor inducing sparsity in the solutions while the constraint set models thenoise tolerance and incorporates other prior information for data fitting. Tosolve this class of constrained optimization problems, a common approach is thepenalty method. However, there is little theory on exact penalization forproblems with nonconvex and non-Lipschitz objective functions. In this paper,we study the existence of exact penalty parameters regarding local minimizers,stationary points and $\epsilon$-minimizers under suitable assumptions.Moreover, we discuss a penalty method whose subproblems are solved via anonmonotone proximal gradient method with a suitable update scheme for thepenalty parameters, and prove the convergence of the algorithm to a KKT pointof the constrained problem. Preliminary numerical results demonstrate theefficiency of the penalty method for finding sparse solutions ofunderdetermined linear systems.
arxiv-1409-2752 | Winner-Take-All Autoencoders |  http://arxiv.org/abs/1409.2752  | author:Alireza Makhzani, Brendan Frey category:cs.LG cs.NE published:2014-09-09 summary:In this paper, we propose a winner-take-all method for learning hierarchicalsparse representations in an unsupervised fashion. We first introducefully-connected winner-take-all autoencoders which use mini-batch statistics todirectly enforce a lifetime sparsity in the activations of the hidden units. Wethen propose the convolutional winner-take-all autoencoder which combines thebenefits of convolutional architectures and autoencoders for learningshift-invariant sparse representations. We describe a way to trainconvolutional autoencoders layer by layer, where in addition to lifetimesparsity, a spatial sparsity within each feature map is achieved usingwinner-take-all activation functions. We will show that winner-take-allautoencoders can be used to to learn deep sparse representations from theMNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,and achieve competitive classification performance.
arxiv-1409-2802 | Far-Field Compression for Fast Kernel Summation Methods in High Dimensions |  http://arxiv.org/abs/1409.2802  | author:William B. March, George Biros category:cs.LG stat.ML published:2014-09-09 summary:We consider fast kernel summations in high dimensions: given a large set ofpoints in $d$ dimensions (with $d \gg 3$) and a pair-potential function (the{\em kernel} function), we compute a weighted sum of all pairwise kernelinteractions for each point in the set. Direct summation is equivalent to a(dense) matrix-vector multiplication and scales quadratically with the numberof points. Fast kernel summation algorithms reduce this cost to log-linear orlinear complexity. Treecodes and Fast Multipole Methods (FMMs) deliver tremendous speedups byconstructing approximate representations of interactions of points that are farfrom each other. In algebraic terms, these representations correspond tolow-rank approximations of blocks of the overall interaction matrix. Existingapproaches require an excessive number of kernel evaluations with increasing$d$ and number of points in the dataset. To address this issue, we use a randomized algebraic approach in which wefirst sample the rows of a block and then construct its approximate, low-rankinterpolative decomposition. We examine the feasibility of this approachtheoretically and experimentally. We provide a new theoretical result showing atighter bound on the reconstruction error from uniformly sampling rows than theexisting state-of-the-art. We demonstrate that our sampling approach iscompetitive with existing (but prohibitively expensive) methods from theliterature. We also construct kernel matrices for the Laplacian, Gaussian, andpolynomial kernels -- all commonly used in physics and data analysis. Weexplore the numerical properties of blocks of these matrices, and show thatthey are amenable to our approach. Depending on the data set, our randomizedalgorithm can successfully compute low rank approximations in high dimensions.We report results for data sets with ambient dimensions from four to 1,000.
arxiv-1409-2848 | A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate |  http://arxiv.org/abs/1409.2848  | author:Ohad Shamir category:cs.LG cs.NA math.OC stat.ML published:2014-09-09 summary:We describe and analyze a simple algorithm for principal component analysisand singular value decomposition, VR-PCA, which uses computationally cheapstochastic iterations, yet converges exponentially fast to the optimalsolution. In contrast, existing algorithms suffer either from slow convergence,or computationally intensive iterations whose runtime scales with the datasize. The algorithm builds on a recent variance-reduced stochastic gradienttechnique, which was previously analyzed for strongly convex optimization,whereas here we apply it to an inherently non-convex problem, using a verydifferent analysis.
arxiv-1409-2824 | Scalable Bayesian Modelling of Paired Symbols |  http://arxiv.org/abs/1409.2824  | author:Ulrich Paquet, Noam Koenigstein, Ole Winther category:stat.ML published:2014-09-09 summary:We present a novel, scalable and Bayesian approach to modelling theoccurrence of pairs of symbols (i,j) drawn from a large vocabulary. Observedpairs are assumed to be generated by a simple popularity based selectionprocess followed by censoring using a preference function. By basing inferenceon the well-founded principle of variational bounding, and using newsite-independent bounds, we show how a scalable inference procedure can beobtained for large data sets. State of the art results are presented onreal-world movie viewing data.
arxiv-1409-2918 | Quantum Edge Detection for Image Segmentation in Optical Environments |  http://arxiv.org/abs/1409.2918  | author:Mario Mastriani category:cs.CV published:2014-09-09 summary:A quantum edge detector for image segmentation in optical environments ispresented in this work. A Boolean version of the same detector is presentedtoo. The quantum version of the new edge detector works with computationalbasis states, exclusively. This way, we can easily avoid the problem of quantummeasurement retrieving the result of applying the new detector on the image.Besides, a new criterion and logic based on projections onto vertical axis ofBloch's Sphere exclusively are presented too. This approach will allow us: 1) asimpler development of logic quantum operations, where they will closer tothose used in the classical logic operations, 2) building simple and robustclassical-to-quantum and quantum-to-classical interfaces. Said so far isextended to quantum algorithms outside image processing too. In a specialsection on metric and simulations, a new metric based on the comparison betweenthe classical and quantum versions algorithms for edge detection of images ispresented. Notable differences between the results of classical and quantumversions of such algorithms (outside and inside of quantum computer,respectively) show the existence of implementation problems involved in theexperiment, and that they have not been properly modeled for opticalenvironments. However, although they are different, the quantum results areequally valid. The latter is clearly seen in the computer simulations
arxiv-1409-2905 | Non-Convex Boosting Overcomes Random Label Noise |  http://arxiv.org/abs/1409.2905  | author:Sunsern Cheamanunkul, Evan Ettinger, Yoav Freund category:cs.LG published:2014-09-09 summary:The sensitivity of Adaboost to random label noise is a well-studied problem.LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to beless sensitive to noise than AdaBoost. We present the results of experimentsevaluating these algorithms on both synthetic and real datasets. We compare theperformance on each of datasets when the labels are corrupted by differentlevels of independent label noise. In presence of random label noise, we foundthat BrownBoost and RobustBoost perform significantly better than AdaBoost andLogitBoost, while the difference between each pair of algorithms isinsignificant. We provide an explanation for the difference based on the margindistributions of the algorithms.
arxiv-1409-2821 | Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain Clustered Records |  http://arxiv.org/abs/1409.2821  | author:Meysam Ghaffari, Nasser Ghadiri category:cs.AI cs.CV published:2014-09-09 summary:As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each inputsample to belong to more than one cluster, providing more flexibility thannon-fuzzy clustering methods. However, the accuracy of FCM is subject to falsedetections caused by noisy records, weak feature selection and low certainty ofthe algorithm in some cases. The false detections are very important in somedecision-making application domains like network security and medicaldiagnosis, where weak decisions based on such false detections may lead tocatastrophic outcomes. They are mainly emerged from making decisions about asubset of records that do not provide enough evidence to make a good decision.In this paper, we propose a method for detecting such ambiguous records in FCMby introducing a certainty factor to decrease invalid detections. This approachenables us to send the detected ambiguous records to another discriminationmethod for a deeper investigation, thus increasing the accuracy by lowering theerror rate. Most of the records are still processed quickly and with low errorrate which prevents performance loss compared to similar hybrid methods.Experimental results of applying the proposed method on several datasets fromdifferent domains show a significant decrease in error rate as well as improvedsensitivity of the algorithm.
arxiv-1409-2800 | Enforcing Label and Intensity Consistency for IR Target Detection |  http://arxiv.org/abs/1409.2800  | author:Toufiq Parag category:cs.CV published:2014-09-09 summary:This study formulates the IR target detection as a binary classificationproblem of each pixel. Each pixel is associated with a label which indicateswhether it is a target or background pixel. The optimal label set for all thepixels of an image maximizes aposteriori distribution of label configurationgiven the pixel intensities. The posterior probability is factored into (orproportional to) a conditional likelihood of the intensity values and a priorprobability of label configuration. Each of these two probabilities arecomputed assuming a Markov Random Field (MRF) on both pixel intensities andtheir labels. In particular, this study enforces neighborhood dependency onboth intensity values, by a Simultaneous Auto Regressive (SAR) model, and onlabels, by an Auto-Logistic model. The parameters of these MRF models arelearned from labeled examples. During testing, an MRF inference technique,namely Iterated Conditional Mode (ICM), produces the optimal label for eachpixel. The detection performance is further improved by incorporating temporalinformation through background subtraction. High performances on benchmarkdatasets demonstrate effectiveness of this method for IR target detection.
arxiv-1409-2713 | Context-specific independence in graphical log-linear models |  http://arxiv.org/abs/1409.2713  | author:Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander category:stat.ML published:2014-09-09 summary:Log-linear models are the popular workhorses of analyzing contingency tables.A log-linear parameterization of an interaction model can be more expressivethan a direct parameterization based on probabilities, leading to a powerfulway of defining restrictions derived from marginal, conditional andcontext-specific independence. However, parameter estimation is often simplerunder a direct parameterization, provided that the model enjoys certaindecomposability properties. Here we introduce a cyclical projection algorithmfor obtaining maximum likelihood estimates of log-linear parameters under anarbitrary context-specific graphical log-linear model, which needs not satisfycriteria of decomposability. We illustrate that lifting the restriction ofdecomposability makes the models more expressive, such that additionalcontext-specific independencies embedded in real data can be identified. It isalso shown how a context-specific graphical model can correspond to anon-hierarchical log-linear parameterization with a concise interpretation.This observation can pave way to further development of non-hierarchicallog-linear models, which have been largely neglected due to their believed lackof interpretability.
arxiv-1409-2710 | eAnt-Miner : An Ensemble Ant-Miner to Improve the ACO Classification |  http://arxiv.org/abs/1409.2710  | author:Gopinath Chennupati category:cs.NE published:2014-09-09 summary:Ant Colony Optimization (ACO) has been applied in supervised learning inorder to induce classification rules as well as decision trees, namedAnt-Miners. Although these are competitive classifiers, the stability of theseclassifiers is an important concern that owes to their stochastic nature. Inthis paper, to address this issue, an acclaimed machine learning techniquenamed, ensemble of classifiers is applied, where an ACO classifier is used as abase classifier to prepare the ensemble. The main trade-off is, the predictionsin the new approach are determined by discovering a group of models as opposedto the single model classification. In essence, we prepare multiple models fromthe randomly replaced samples of training data from which, a unique model isprepared by aggregating the models to test the unseen data points. The mainobjective of this new approach is to increase the stability of the Ant-Minerresults there by improving the performance of ACO classification. We found thatthe ensemble Ant-Miners significantly improved the stability by reducing theclassification error on unseen data.
arxiv-1409-2702 | F-formation Detection: Individuating Free-standing Conversational Groups in Images |  http://arxiv.org/abs/1409.2702  | author:Francesco Setti, Chris Russell, Chiara Bassetti, Marco Cristani category:cs.CV published:2014-09-09 summary:Detection of groups of interacting people is a very interesting and usefultask in many modern technologies, with application fields spanning fromvideo-surveillance to social robotics. In this paper we first furnish arigorous definition of group considering the background of the social sciences:this allows us to specify many kinds of group, so far neglected in the ComputerVision literature. On top of this taxonomy, we present a detailed state of theart on the group detection algorithms. Then, as a main contribution, we presenta brand new method for the automatic detection of groups in still images, whichis based on a graph-cuts framework for clustering individuals; in particular weare able to codify in a computational sense the sociological definition ofF-formation, that is very useful to encode a group having only proxemicinformation: position and orientation of people. We call the proposed methodGraph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms allthe state of the art methods in terms of different accuracy measures (some ofthem are brand new), demonstrating also a strong robustness to noise andversatility in recognizing groups of various cardinality.
arxiv-1409-2579 | A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices |  http://arxiv.org/abs/1409.2579  | author:Ting-ting Feng, Gang Wu category:cs.NA cs.CV cs.LG published:2014-09-09 summary:The null linear discriminant analysis method is a competitive approach fordimensionality reduction. The implementation of this method, however, iscomputationally expensive. Recently, a fast implementation of null lineardiscriminant analysis method using random matrix multiplication with scattermatrices was proposed. However, if the random matrix is chosen arbitrarily, theorientation matrix may be rank deficient, and some useful discriminantinformation will be lost. In this paper, we investigate how to choose therandom matrix properly, such that the two criteria of the null LDA method aresatisfied theoretically. We give a necessary and sufficient condition toguarantee full column rank of the orientation matrix. Moreover, the geometriccharacterization of the condition is also described.
arxiv-1409-2650 | Combining the analytical hierarchy process and the genetic algorithm to solve the timetable problem |  http://arxiv.org/abs/1409.2650  | author:Ihab Sbeity, Mohamed Dbouk, Habib Kobeissi category:cs.AI cs.NE published:2014-09-09 summary:The main problems of school course timetabling are time, curriculum, andclassrooms. In addition there are other problems that vary from one institutionto another. This paper is intended to solve the problem of satisfying theteachers preferred schedule in a way that regards the importance of the teacherto the supervising institute, i.e. his score according to some criteria.Genetic algorithm (GA) has been presented as an elegant method in solvingtimetable problem (TTP) in order to produce solutions with no conflict. In thispaper, we consider the analytic hierarchy process (AHP) to efficiently obtain ascore for each teacher, and consequently produce a GA-based TTP solution thatsatisfies most of the teachers preferences.
arxiv-1409-2620 | Learning Machines Implemented on Non-Deterministic Hardware |  http://arxiv.org/abs/1409.2620  | author:Suyog Gupta, Vikas Sindhwani, Kailash Gopalakrishnan category:cs.LG stat.ML published:2014-09-09 summary:This paper highlights new opportunities for designing large-scale machinelearning systems as a consequence of blurring traditional boundaries that haveallowed algorithm designers and application-level practitioners to stay -- forthe most part -- oblivious to the details of the underlying hardware-levelimplementations. The hardware/software co-design methodology advocated herehinges on the deployment of compute-intensive machine learning kernels ontocompute platforms that trade-off determinism in the computation for improvementin speed and/or energy efficiency. To achieve this, we revisit digitalstochastic circuits for approximating matrix computations that are ubiquitousin machine learning algorithms. Theoretical and empirical evaluation isundertaken to assess the impact of the hardware-induced computational noise onalgorithm performance. As a proof-of-concept, a stochastic hardware simulatoris employed for training deep neural networks for image recognition problems.
arxiv-1409-2574 | Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures |  http://arxiv.org/abs/1409.2574  | author:John R. Hershey, Jonathan Le Roux, Felix Weninger category:cs.LG cs.NE stat.ML published:2014-09-09 summary:Model-based methods and deep neural networks have both been tremendouslysuccessful paradigms in machine learning. In model-based methods, problemdomain knowledge can be built into the constraints of the model, typically atthe expense of difficulties during inference. In contrast, deterministic deepneural networks are constructed in such a way that inference isstraightforward, but their architectures are generic and it is unclear how toincorporate knowledge. This work aims to obtain the advantages of bothapproaches. To do so, we start with a model-based approach and an associatedinference algorithm, and \emph{unfold} the inference iterations as layers in adeep network. Rather than optimizing the original model, we \emph{untie} themodel parameters across layers, in order to create a more powerful network. Theresulting architecture can be trained discriminatively to perform accurateinference within a fixed network size. We show how this framework allows us tointerpret conventional networks as mean-field inference in Markov randomfields, and to obtain new architectures by instead using belief propagation asthe inference algorithm. We then show its application to a non-negative matrixfactorization model that incorporates the problem-domain knowledge that soundsources are additive. Deep unfolding of this model yields a new kind ofnon-negative deep neural network, that can be trained using a multiplicativebackpropagation-style update algorithm. We present speech enhancementexperiments showing that our approach is competitive with conventional neuralnetworks despite using far fewer parameters.
arxiv-1409-2655 | Weighted Classification Cascades for Optimizing Discovery Significance in the HiggsML Challenge |  http://arxiv.org/abs/1409.2655  | author:Lester Mackey, Jordan Bryan, Man Yue Mo category:stat.ML cs.LG published:2014-09-09 summary:We introduce a minorization-maximization approach to optimizing commonmeasures of discovery significance in high energy physics. The approachalternates between solving a weighted binary classification problem andupdating class weights in a simple, closed-form manner. Moreover, an argumentbased on convex duality shows that an improvement in weighted classificationerror on any round yields a commensurate improvement in discovery significance.We complement our derivation with experimental results from the 2014 Higgsboson machine learning challenge.
arxiv-1409-2287 | Variational Inference for Uncertainty on the Inputs of Gaussian Process Models |  http://arxiv.org/abs/1409.2287  | author:Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence category:stat.ML cs.AI cs.CV cs.LG published:2014-09-08 summary:The Gaussian process latent variable model (GP-LVM) provides a flexibleapproach for non-linear dimensionality reduction that has been widely applied.However, the current approach for training GP-LVMs is based on maximumlikelihood, where the latent projection variables are maximized over ratherthan integrated out. In this paper we present a Bayesian method for trainingGP-LVMs by introducing a non-standard variational inference framework thatallows to approximately integrate out the latent variables and subsequentlytrain a GP-LVM by maximizing an analytic lower bound on the exact marginallikelihood. We apply this method for learning a GP-LVM from iid observationsand for learning non-linear dynamical systems where the observations aretemporally correlated. We show that a benefit of the variational Bayesianprocedure is its robustness to overfitting and its ability to automaticallyselect the dimensionality of the nonlinear latent space. The resultingframework is generic, flexible and easy to extend for other purposes, such asGaussian process regression with uncertain inputs and semi-supervised Gaussianprocesses. We demonstrate our method on synthetic data and standard machinelearning benchmarks, as well as challenging real world datasets, including highresolution video data.
arxiv-1409-2390 | Symbolic regression of generative network models |  http://arxiv.org/abs/1409.2390  | author:Telmo Menezes, Camille Roth category:cs.NE cs.SI physics.soc-ph published:2014-09-08 summary:Networks are a powerful abstraction with applicability to a variety ofscientific fields. Models explaining their morphology and growth processespermit a wide range of phenomena to be more systematically analysed andunderstood. At the same time, creating such models is often challenging andrequires insights that may be counter-intuitive. Yet there currently exists nogeneral method to arrive at better models. We have developed an approach toautomatically detect realistic decentralised network growth models fromempirical data, employing a machine learning technique inspired by naturalselection and defining a unified formalism to describe such models as computerprograms. As the proposed method is completely general and does not assume anypre-existing models, it can be applied "out of the box" to any given network.To validate our approach empirically, we systematically rediscover pre-definedgrowth laws underlying several canonical network generation models and crediblelaws for diverse real-world networks. We were able to find programs that aresimple enough to lead to an actual understanding of the mechanisms proposed,namely for a simple brain and a social network.
arxiv-1409-2433 | Approximating solution structure of the Weighted Sentence Alignment problem |  http://arxiv.org/abs/1409.2433  | author:Antonina Kolokolova, Renesa Nizamee category:cs.CL cs.CC cs.DS published:2014-09-08 summary:We study the complexity of approximating solution structure of the bijectiveweighted sentence alignment problem of DeNero and Klein (2008). In particular,we consider the complexity of finding an alignment that has a significantoverlap with an optimal alignment. We discuss ways of representing the solutionfor the general weighted sentence alignment as well as phrases-to-wordsalignment problem, and show that computing a string which agrees with theoptimal sentence partition on more than half (plus an arbitrarily smallpolynomial fraction) positions for the phrases-to-words alignment is NP-hard.For the general weighted sentence alignment we obtain such bound from theagreement on a little over 2/3 of the bits. Additionally, we generalize theHamming distance approximation of a solution structure to approximating it withrespect to the edit distance metric, obtaining similar lower bounds.
arxiv-1409-2450 | Exploiting Social Network Structure for Person-to-Person Sentiment Analysis |  http://arxiv.org/abs/1409.2450  | author:Robert West, Hristo S. Paskov, Jure Leskovec, Christopher Potts category:cs.SI cs.CL physics.soc-ph published:2014-09-08 summary:Person-to-person evaluations are prevalent in all kinds of discourse andimportant for establishing reputations, building social bonds, and shapingpublic opinion. Such evaluations can be analyzed separately using signed socialnetworks and textual sentiment analysis, but this misses the rich interactionsbetween language and social context. To capture such interactions, we develop amodel that predicts individual A's opinion of individual B by synthesizinginformation from the signed social network in which A and B are embedded withsentiment analysis of the evaluative texts relating A to B. We prove that thisproblem is NP-hard but can be relaxed to an efficiently solvable hinge-lossMarkov random field, and we show that this implementation outperforms text-onlyand network-only versions in two very different datasets involvingcommunity-level decision-making: the Wikipedia Requests for Adminship corpusand the Convote U.S. Congressional speech corpus.
arxiv-1409-2329 | Recurrent Neural Network Regularization |  http://arxiv.org/abs/1409.2329  | author:Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals category:cs.NE published:2014-09-08 summary:We present a simple regularization technique for Recurrent Neural Networks(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successfultechnique for regularizing neural networks, does not work well with RNNs andLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and showthat it substantially reduces overfitting on a variety of tasks. These tasksinclude language modeling, speech recognition, image caption generation, andmachine translation.
arxiv-1409-2465 | Comparing Feature Detectors: A bias in the repeatability criteria, and how to correct it |  http://arxiv.org/abs/1409.2465  | author:Ives Rey-Otero, Mauricio Delbracio, Jean-Michel Morel category:cs.CV published:2014-09-08 summary:Most computer vision application rely on algorithms finding localcorrespondences between different images. These algorithms detect and comparestable local invariant descriptors centered at scale-invariant keypoints.Because of the importance of the problem, new keypoint detectors anddescriptors are constantly being proposed, each one claiming to perform better(or to be complementary) to the preceding ones. This raises the question of afair comparison between very diverse methods. This evaluation has been mainlybased on a repeatability criterion of the keypoints under a series of imageperturbations (blur, illumination, noise, rotations, homotheties, homographies,etc). In this paper, we argue that the classic repeatability criterion isbiased towards algorithms producing redundant overlapped detections. Tocompensate this bias, we propose a variant of the repeatability rate takinginto account the descriptors overlap. We apply this variant to revisit thepopular benchmark by Mikolajczyk et al., on classic and new feature detectors.Experimental evidence shows that the hierarchy of these feature detectors isseverely disrupted by the amended comparator.
arxiv-1409-2552 | Sparse Additive Model using Symmetric Nonnegative Definite Smoothers |  http://arxiv.org/abs/1409.2552  | author:Yan Li category:stat.ML cs.LG published:2014-09-08 summary:We introduce a new algorithm, called adaptive sparse backfitting algorithm,for solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric,non-negative definite smoothers. Unlike the previous sparse backfittingalgorithm, our method is essentially a block coordinate descent algorithm thatguarantees to converge to the optimal solution. It bridges the gap between thepopulation backfitting algorithm and that of the data version. We also provevariable selection consistency under suitable conditions. Numerical studies onboth synthesis and real data are conducted to show that adaptive sparsebackfitting algorithm outperforms previous sparse backfitting algorithm infitting and predicting high dimensional nonparametric models.
arxiv-1409-2195 | Analyzing the Language of Food on Social Media |  http://arxiv.org/abs/1409.2195  | author:Daniel Fried, Mihai Surdeanu, Stephen Kobourov, Melanie Hingle, Dane Bell category:cs.CL cs.CY cs.SI published:2014-09-08 summary:We investigate the predictive power behind the language of food on socialmedia. We collect a corpus of over three million food-related posts fromTwitter and demonstrate that many latent population characteristics can bedirectly predicted from this data: overweight rate, diabetes rate, politicalleaning, and home geographical location of authors. For all tasks, ourlanguage-based models significantly outperform the majority-class baselines.Performance is further improved with more complex natural language processing,such as topic modeling. We analyze which textual features have most predictivepower for these datasets, providing insight into the connections between thelanguage of food, geographic locale, and community characteristics. Lastly, wedesign and implement an online system for real-time query and visualization ofthe dataset. Visualization tools, such as geo-referenced heatmaps,semantics-preserving wordclouds and temporal histograms, allow us to discovermore complex, global patterns mirrored in the language of food.
arxiv-1409-2232 | When coding meets ranking: A joint framework based on local learning |  http://arxiv.org/abs/1409.2232  | author:Jim Jing-Yan Wang category:cs.CV cs.LG stat.ML published:2014-09-08 summary:Sparse coding, which represents a data point as a s- parse reconstructioncode with regard to a dictionary, has been a popular data representationmethod. Meanwhile, in database retrieval problems, learn the ranking scoresfrom data points plays an important role. Up to new, these two methods havealways been used individually, assuming that data coding and ranking are twoindependent and irrele- vant problems. However, is there any internalrelationship between sparse coding and ranking score learning? If yes, how toexplore this internal relationship? In this paper, we try to answer thesequestions by developing the first join- t sparse coding and ranking scorelearning algorithm. To explore the local distribution in the sparse code space,and also to bridgecoding and rankingproblems, we assume that in theneighborhood of each data points, the ranking scores can be approximated fromthe corresponding sparse codes by a local linear function. By considering thelocal approx- imation error of ranking scores, reconstruction error andsparsity of sparse coding, and the query information pro- vided by the user, weconstruct an unified objective func- tion for learning of sparse codes,dictionary and rankings scores. An iterative algorithm is developed to optimizethe objective function to jointly learn the sparse codes, dictio- nary andrankings scores.
arxiv-1410-0371 | Real Time Fabric Defect Detection System on an Embedded DSP Platform |  http://arxiv.org/abs/1410.0371  | author:J. L. Raheja, B. Ajay, Ankit Chaudhary category:cs.CV published:2014-09-08 summary:In industrial fabric productions, automated real time systems are needed tofind out the minor defects. It will save the cost by not transporting defectedproducts and also would help in making compmay image of quality fabrics bysending out only undefected products. A real time fabric defect detectionsystem (FDDS), implementd on an embedded DSP platform is presented here.Textural features of fabric image are extracted based on gray levelco-occurrence matrix (GLCM). A sliding window technique is used for defectdetection where window moves over the whole image computing a textural energyfrom the GLCM of the fabric image. The energy values are compared to areference and the deviations beyond a threshold are reported as defects andalso visually represented by a window. The implementation is carried out on aTI TMS320DM642 platform and programmed using code composer studio software. Thereal time output of this implementation was shown on a monitor.
arxiv-1409-2177 | The Large Margin Mechanism for Differentially Private Maximization |  http://arxiv.org/abs/1409.2177  | author:Kamalika Chaudhuri, Daniel Hsu, Shuang Song category:cs.LG cs.DS cs.IT math.IT math.ST stat.TH published:2014-09-07 summary:A basic problem in the design of privacy-preserving algorithms is the privatemaximization problem: the goal is to pick an item from a universe that(approximately) maximizes a data-dependent function, all under the constraintof differential privacy. This problem has been used as a sub-routine in manyprivacy-preserving algorithms for statistics and machine-learning. Previous algorithms for this problem are either range-dependent---i.e., theirutility diminishes with the size of the universe---or only apply to veryrestricted function classes. This work provides the first general-purpose,range-independent algorithm for private maximization that guaranteesapproximate differential privacy. Its applicability is demonstrated on twofundamental tasks in data mining and machine learning.
arxiv-1409-2080 | Multiscale statistical testing for connectome-wide association studies in fMRI |  http://arxiv.org/abs/1409.2080  | author:P. Bellec, Y. Benhajali, F. Carbonell, C. Dansereau, G. Albouy, M. Pelland, C. Craddock, O. Collignon, J. Doyon, E. Stip, P. Orban category:q-bio.QM cs.CV stat.AP published:2014-09-07 summary:Alterations in brain connectivity have been associated with a variety ofclinical disorders using functional magnetic resonance imaging (fMRI). Weinvestigated empirically how the number of brain parcels (or scale) impactedthe results of a mass univariate general linear model (GLM) on connectomes. Thebrain parcels used as nodes in the connectome analysis were functionnallydefined by a group cluster analysis. We first validated that a classicBenjamini-Hochberg procedure with parametric GLM tests did controlappropriately the false-discovery rate (FDR) at a given scale. We then observedon realistic simulations that there was no substantial inflation of the FDRacross scales, as long as the FDR was controlled independently within eachscale, and the presence of true associations could be established using anomnibus permutation test combining all scales. Second, we observed both onsimulations and on three real resting-state fMRI datasets (schizophrenia,congenital blindness, motor practice) that the rate of discovery variedmarkedly as a function of scales, and was relatively higher for low scales,below 25. Despite the differences in discovery rate, the statistical mapsderived at different scales were generally very consistent in the three realdatasets. Some seeds still showed effects better observed around 50,illustrating the potential benefits of multiscale analysis. On real data, thestatistical maps agreed well with the existing literature. Overall, our resultssupport that the multiscale GLM connectome analysis with FDR is statisticallyvalid and can capture biologically meaningful effects in a variety ofexperimental conditions.
arxiv-1409-2104 | A Computational Model of the Short-Cut Rule for 2D Shape Decomposition |  http://arxiv.org/abs/1409.2104  | author:Lei Luo, Chunhua Shen, Xinwang Liu, Chunyuan Zhang category:cs.CV published:2014-09-07 summary:We propose a new 2D shape decomposition method based on the short-cut rule.The short-cut rule originates from cognition research, and states that thehuman visual system prefers to partition an object into parts using theshortest possible cuts. We propose and implement a computational model for theshort-cut rule and apply it to the problem of shape decomposition. The model weproposed generates a set of cut hypotheses passing through the points on thesilhouette which represent the negative minima of curvature. We then show thatmost part-cut hypotheses can be eliminated by analysis of local properties ofeach. Finally, the remaining hypotheses are evaluated in ascending lengthorder, which guarantees that of any pair of conflicting cuts only the shortestwill be accepted. We demonstrate that, compared with state-of-the-art shapedecomposition methods, the proposed approach achieves decomposition resultswhich better correspond to human intuition as revealed in psychologicalexperiments.
arxiv-1409-2073 | An NLP Assistant for Clide |  http://arxiv.org/abs/1409.2073  | author:Tobias Kortkamp category:cs.CL published:2014-09-07 summary:This report describes an NLP assistant for the collaborative developmentenvironment Clide, that supports the development of NLP applications byproviding easy access to some common NLP data structures. The assistantvisualizes text fragments and their dependencies by displaying the semanticgraph of a sentence, the coreference chain of a paragraph and mined triplesthat are extracted from a paragraph's semantic graphs and linked using itscoreference chain. Using this information and a logic programming library, wecreate an NLP database which is used by a series of queries to mine thetriples. The algorithm is tested by translating a natural language textdescribing a graph to an actual graph that is shown as an annotation in thetext editor.
arxiv-1409-2050 | Depth image hand tracking from an overhead perspective using partially labeled, unbalanced data: Development and real-world testing |  http://arxiv.org/abs/1409.2050  | author:Stephen Czarnuch, Alex Mihailidis category:cs.CV published:2014-09-06 summary:We present the development and evaluation of a hand tracking algorithm basedon single depth images captured from an overhead perspective for use in theCOACH prompting system. We train a random decision forest body part classifierusing approximately 5,000 manually labeled, unbalanced, partially labeledtraining images. The classifier represents a random subset of pixels in eachdepth image with a learned probability density function across all trained bodyparts. A local mode-find approach is used to search for clusters present in theunderlying feature space sampled by the classified pixels. In each frame, bodypart positions are chosen as the mode with the highest confidence. User handpositions are translated into hand washing task actions based on proximity toenvironmental objects. We validate the performance of the classifier and taskaction proposals on a large set of approximately 24,000 manually labeledimages.
arxiv-1409-1976 | A Reduction of the Elastic Net to Support Vector Machines with an Application to GPU Computing |  http://arxiv.org/abs/1409.1976  | author:Quan Zhou, Wenlin Chen, Shiji Song, Jacob R. Gardner, Kilian Q. Weinberger, Yixin Chen category:stat.ML cs.LG published:2014-09-06 summary:The past years have witnessed many dedicated open-source projects that builtand maintain implementations of Support Vector Machines (SVM), parallelized forGPU, multi-core CPUs and distributed systems. Up to this point, no comparableeffort has been made to parallelize the Elastic Net, despite its popularity inmany high impact applications, including genetics, neuroscience and systemsbiology. The first contribution in this paper is of theoretical nature. Weestablish a tight link between two seemingly different algorithms and provethat Elastic Net regression can be reduced to SVM with squared hinge lossclassification. Our second contribution is to derive a practical algorithmbased on this reduction. The reduction enables us to utilize prior efforts inspeeding up and parallelizing SVMs to obtain a highly optimized and parallelsolver for the Elastic Net and Lasso. With a simple wrapper, consisting of only11 lines of MATLAB code, we obtain an Elastic Net implementation that naturallyutilizes GPU and multi-core CPUs. We demonstrate on twelve real world datasets, that our algorithm yields identical results as the popular (and highlyoptimized) glmnet implementation but is one or several orders of magnitudefaster.
arxiv-1409-2045 | Global Convergence of Online Limited Memory BFGS |  http://arxiv.org/abs/1409.2045  | author:Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.LG stat.ML published:2014-09-06 summary:Global convergence of an online (stochastic) limited memory version of theBroyden-Fletcher- Goldfarb-Shanno (BFGS) quasi-Newton method for solvingoptimization problems with stochastic objectives that arise in large scalemachine learning is established. Lower and upper bounds on the Hessianeigenvalues of the sample functions are shown to suffice to guarantee that thecurvature approximation matrices have bounded determinants and traces, which,in turn, permits establishing convergence to optimal arguments with probability1. Numerical experiments on support vector machines with synthetic datashowcase reductions in convergence time relative to stochastic gradient descentalgorithms as well as reductions in storage and computation relative to otheronline quasi-Newton methods. Experimental evaluation on a search engineadvertising problem corroborates that these advantages also manifest inpractical applications.
arxiv-1409-1789 | Identifying Synapses Using Deep and Wide Multiscale Recursive Networks |  http://arxiv.org/abs/1409.1789  | author:Gary B. Huang, Stephen Plaza category:cs.CV published:2014-09-05 summary:In this work, we propose a learning framework for identifying synapses usinga deep and wide multi-scale recursive (DAWMR) network, previously considered inimage segmentation applications. We apply this approach on electron microscopydata from invertebrate fly brain tissue. By learning features directly from thedata, we are able to achieve considerable improvements over existing techniquesthat rely on a small set of hand-designed features. We show that this systemcan reduce the amount of manual annotation required, in both acquisition oftraining data as well as verification of inferred detections.
arxiv-1409-1892 | Automatic Neuron Type Identification by Neurite Localization in the Drosophila Medulla |  http://arxiv.org/abs/1409.1892  | author:Ting Zhao, Stephen M Plaza category:q-bio.NC cs.CV published:2014-09-05 summary:Mapping the connectivity of neurons in the brain (i.e., connectomics) is achallenging problem due to both the number of connections in even the smallestorganisms and the nanometer resolution required to resolve them. Because ofthis, previous connectomes contain only hundreds of neurons, such as in theC.elegans connectome. Recent technological advances will unlock the mysteriesof increasingly large connectomes (or partial connectomes). However, the valueof these maps is limited by our ability to reason with this data and understandany underlying motifs. To aid connectome analysis, we introduce algorithms tocluster similarly-shaped neurons, where 3D neuronal shapes are represented asskeletons. In particular, we propose a novel location-sensitive clusteringalgorithm. We show clustering results on neurons reconstructed from theDrosophila medulla that show high-accuracy.
arxiv-1409-2697 | Particle Swarm Optimized Fuzzy Controller for Indirect Vector Control of Multilevel Inverter Fed Induction Motor |  http://arxiv.org/abs/1409.2697  | author:Sanjaya Kumar Sahu, T. V. Dixit, D. D. Neema category:cs.NE published:2014-09-05 summary:The Particle Swarm Optimized (PSO) fuzzy controller has been proposed forindirect vector control of induction motor. In this proposed scheme a NeutralPoint Clamped (NPC) multilevel inverter is used and hysteresis current controltechnique has been adopted for switching the IGBTs. A Mamdani type fuzzycontroller is used in place of conventional PI controller. To ensure betterperformance of fuzzy controller all parameters such as membership functions,normalizing and de-normalizing parameters are optimized using PSO. Theperformance of proposed controller is investigated under various load and speedconditions. The simulation results show its stability and robustness for highperformance derives applications.
arxiv-1409-1744 | Structure of an elite co-occurrence network |  http://arxiv.org/abs/1409.1744  | author:V. A. Traag, R. Reinanda, G. van Klinken category:physics.soc-ph cs.CL cs.SI published:2014-09-05 summary:The rise of social media allowed for rich analyses of their content and theirnetwork structure. As traditional media (i.e. newspapers and magazines) arebeing digitized, similar analyses can be undertaken. This provides a glimpse ofthe elite, as the news mostly revolves around the more influential members ofsociety. We here focus on a network structure derived from co-occurrences ofpeople in the media. This network has a strong core with peripheral clustersbeing connected to the core. Nonetheless, these characteristics seem to bemainly a result from the bipartite structure of the data. We employ a simplegrowing bipartite model that can qualitatively reproduce such a core-peripherystructure. Two self-reinforcing processes are vital: (1) more frequentlyoccurring persons are more likely to occur again; and (2) if two peopleco-occur frequently, they are more likely to co-occur again. This suggests thatthe core-periphery structure is not necessarily reflective of the elite networkin society, but might be an artefact of how they are portrayed in the media.
arxiv-1409-1801 | Annotating Synapses in Large EM Datasets |  http://arxiv.org/abs/1409.1801  | author:Stephen M. Plaza, Toufiq Parag, Gary B. Huang, Donald J. Olbris, Mathew A. Saunders, Patricia K. Rivlin category:q-bio.QM cs.CV q-bio.NC published:2014-09-05 summary:Reconstructing neuronal circuits at the level of synapses is a centralproblem in neuroscience and becoming a focus of the emerging field ofconnectomics. To date, electron microscopy (EM) is the most proven techniquefor identifying and quantifying synaptic connections. As advances in EM makeacquiring larger datasets possible, subsequent manual synapse identification({\em i.e.}, proofreading) for deciphering a connectome becomes a major timebottleneck. Here we introduce a large-scale, high-throughput, andsemi-automated methodology to efficiently identify synapses. We successfullyapplied our methodology to the Drosophila medulla optic lobe, annotating manymore synapses than previous connectome efforts. Our approaches are extensibleand will make the often complicated process of synapse identificationaccessible to a wider-community of potential proofreaders.
arxiv-1409-1715 | An Experimental Study of Adaptive Control for Evolutionary Algorithms |  http://arxiv.org/abs/1409.1715  | author:Giacomo di Tollo, Frédéric Lardeux, Jorge Maturana, Frédéric Saubion category:cs.NE published:2014-09-05 summary:The balance of exploration versus exploitation (EvE) is a key issue onevolutionary computation. In this paper we will investigate how an adaptivecontroller aimed to perform Operator Selection can be used to dynamicallymanage the EvE balance required by the search, showing that the searchstrategies determined by this control paradigm lead to an improvement ofsolution quality found by the evolutionary algorithm.
arxiv-1409-1917 | Novel Methods for Activity Classification and Occupany Prediction Enabling Fine-grained HVAC Control |  http://arxiv.org/abs/1409.1917  | author:Rajib Rana, Brano Kusy, Josh Wall, Wen Hu category:cs.LG published:2014-09-05 summary:Much of the energy consumption in buildings is due to HVAC systems, which hasmotivated several recent studies on making these systems more energy-efficient. Occupancy and activity are two important aspects, which need to becorrectly estimated for optimal HVAC control. However, state-of-the-art methodsto estimate occupancy and classify activity require infrastructure and/orwearable sensors which suffers from lower acceptability due to higher cost.Encouragingly, with the advancement of the smartphones, these are becoming moreachievable. Most of the existing occupancy estimation tech- niques have theunderlying assumption that the phone is always carried by its user. However,phones are often left at desk while attending meeting or other events, whichgenerates estimation error for the existing phone based occupancy algorithms.Similarly, in the recent days the emerging theory of Sparse Random Classifier(SRC) has been applied for activity classification on smartphone, however,there are rooms to improve the on-phone process- ing. We propose a novel sensorfusion method which offers almost 100% accuracy for occupancy estimation. Wealso propose an activity classifica- tion algorithm, which offers similaraccuracy as of the state-of-the-art SRC algorithms while offering 50% reductionin processing.
arxiv-1409-1612 | Semantic clustering of Russian web search results: possibilities and problems |  http://arxiv.org/abs/1409.1612  | author:Andrey Kutuzov category:cs.CL cs.IR published:2014-09-04 summary:The paper deals with word sense induction from lexical co-occurrence graphs.We construct such graphs on large Russian corpora and then apply this data tocluster Mail.ru Search results according to meanings of the query. We comparedifferent methods of performing such clustering and different source corpora.Models of applying distributional semantics to big linguistic data aredescribed.
arxiv-1409-1458 | Communication-Efficient Distributed Dual Coordinate Ascent |  http://arxiv.org/abs/1409.1458  | author:Martin Jaggi, Virginia Smith, Martin Takáč, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, Michael I. Jordan category:cs.LG math.OC stat.ML 90C25, 68W15 G.1.6; C.1.4 published:2014-09-04 summary:Communication remains the most significant bottleneck in the performance ofdistributed optimization algorithms for large-scale machine learning. In thispaper, we propose a communication-efficient framework, CoCoA, that uses localcomputation in a primal-dual setting to dramatically reduce the amount ofnecessary communication. We provide a strong convergence rate analysis for thisclass of algorithms, as well as experiments on real-world distributed datasetswith implementations in Spark. In our experiments, we find that as compared tostate-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoAconverges to the same .001-accurate solution quality on average 25x as quickly.
arxiv-1409-1484 | The Evolution of First Person Vision Methods: A Survey |  http://arxiv.org/abs/1409.1484  | author:Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias Rauterberg category:cs.CV published:2014-09-04 summary:The emergence of new wearable technologies such as action cameras andsmart-glasses has increased the interest of computer vision scientists in theFirst Person perspective. Nowadays, this field is attracting attention andinvestments of companies aiming to develop commercial devices with First PersonVision recording capabilities. Due to this interest, an increasing demand ofmethods to process these videos, possibly in real-time, is expected. Currentapproaches present a particular combinations of different image features andquantitative methods to accomplish specific objectives like object detection,activity recognition, user machine interaction and so on. This paper summarizesthe evolution of the state of the art in First Person Vision video analysisbetween 1997 and 2014, highlighting, among others, most commonly used features,methods, challenges and opportunities within the field.
arxiv-1409-1556 | Very Deep Convolutional Networks for Large-Scale Image Recognition |  http://arxiv.org/abs/1409.1556  | author:Karen Simonyan, Andrew Zisserman category:cs.CV published:2014-09-04 summary:In this work we investigate the effect of the convolutional network depth onits accuracy in the large-scale image recognition setting. Our maincontribution is a thorough evaluation of networks of increasing depth using anarchitecture with very small (3x3) convolution filters, which shows that asignificant improvement on the prior-art configurations can be achieved bypushing the depth to 16-19 weight layers. These findings were the basis of ourImageNet Challenge 2014 submission, where our team secured the first and thesecond places in the localisation and classification tracks respectively. Wealso show that our representations generalise well to other datasets, wherethey achieve state-of-the-art results. We have made our two best-performingConvNet models publicly available to facilitate further research on the use ofdeep visual representations in computer vision.
arxiv-1409-1576 | Machine Learning Etudes in Astrophysics: Selection Functions for Mock Cluster Catalogs |  http://arxiv.org/abs/1409.1576  | author:Amir Hajian, Marcelo Alvarez, J. Richard Bond category:astro-ph.CO astro-ph.IM cs.LG stat.ML published:2014-09-04 summary:Making mock simulated catalogs is an important component of astrophysicaldata analysis. Selection criteria for observed astronomical objects are oftentoo complicated to be derived from first principles. However the existence ofan observed group of objects is a well-suited problem for machine learningclassification. In this paper we use one-class classifiers to learn theproperties of an observed catalog of clusters of galaxies from ROSAT and topick clusters from mock simulations that resemble the observed ROSAT catalog.We show how this method can be used to study the cross-correlations of thermalSunya'ev-Zeldovich signals with number density maps of X-ray selected clustercatalogs. The method reduces the bias due to hand-tuning the selection functionand is readily scalable to large catalogs with a high-dimensional space ofastrophysical features.
arxiv-1409-1403 | Nonlinear tensor product approximation of functions |  http://arxiv.org/abs/1409.1403  | author:D. Bazarkhanov, V. Temlyakov category:stat.ML math.NA 41A65 published:2014-09-04 summary:We are interested in approximation of a multivariate function$f(x_1,\dots,x_d)$ by linear combinations of products $u^1(x_1)\cdots u^d(x_d)$of univariate functions $u^i(x_i)$, $i=1,\dots,d$. In the case $d=2$ it is aclassical problem of bilinear approximation. In the case of approximation inthe $L_2$ space the bilinear approximation problem is closely related to theproblem of singular value decomposition (also called Schmidt expansion) of thecorresponding integral operator with the kernel $f(x_1,x_2)$. There are knownresults on the rate of decay of errors of best bilinear approximation in $L_p$under different smoothness assumptions on $f$. The problem of multilinearapproximation (nonlinear tensor product approximation) in the case $d\ge 3$ ismore difficult and much less studied than the bilinear approximation problem.We will present results on best multilinear approximation in $L_p$ under mixedsmoothness assumption on $f$.
arxiv-1409-1320 | Marginal Structured SVM with Hidden Variables |  http://arxiv.org/abs/1409.1320  | author:Wei Ping, Qiang Liu, Alexander Ihler category:stat.ML cs.LG published:2014-09-04 summary:In this work, we propose the marginal structured SVM (MSSVM) for structuredprediction with hidden variables. MSSVM properly accounts for the uncertaintyof hidden variables, and can significantly outperform the previously proposedlatent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-artmethods, especially when that uncertainty is large. Our method also results ina smoother objective function, making gradient-based optimization of MSSVMsconverge significantly faster than for LSSVMs. We also show that our methodconsistently outperforms hidden conditional random fields (HCRFs; Quattoni etal. (2007)) on both simulated and real-world datasets. Furthermore, we proposea unified framework that includes both our and several other existing methodsas special cases, and provides insights into the comparison of different modelsin practice.
arxiv-1409-1057 | Augmented Neural Networks for Modelling Consumer Indebtness |  http://arxiv.org/abs/1409.1057  | author:Alexandros Ladas, Jonathan M. Garibaldi, Rodrigo Scarpel, Uwe Aickelin category:cs.CE cs.LG cs.NE published:2014-09-03 summary:Consumer Debt has risen to be an important problem of modern societies,generating a lot of research in order to understand the nature of consumerindebtness, which so far its modelling has been carried out by statisticalmodels. In this work we show that Computational Intelligence can offer a moreholistic approach that is more suitable for the complex relationships anindebtness dataset has and Linear Regression cannot uncover. In particular, asour results show, Neural Networks achieve the best performance in modellingconsumer indebtness, especially when they manage to incorporate the significantand experimentally verified results of the Data Mining process in the model,exploiting the flexibility Neural Networks offer in designing their topology.This novel method forms an elaborate framework to model Consumer indebtnessthat can be extended to any other real world application.
arxiv-1409-1053 | Tuning a Multiple Classifier System for Side Effect Discovery using Genetic Algorithms |  http://arxiv.org/abs/1409.1053  | author:Jenna M. Reps, Uwe Aickelin, Jonathan M. Garibaldi category:cs.LG cs.CE published:2014-09-03 summary:In previous work, a novel supervised framework implementing a binaryclassifier was presented that obtained excellent results for side effectdiscovery. Interestingly, unique side effects were identified when differentbinary classifiers were used within the framework, prompting the investigationof applying a multiple classifier system. In this paper we investigate tuning aside effect multiple classifying system using genetic algorithms. The resultsof this research show that the novel framework implementing a multipleclassifying system trained using genetic algorithms can obtain a higher partialarea under the receiver operating characteristic curve than implementing asingle classifier. Furthermore, the framework is able to detect side effectsefficiently and obtains a low false positive rate.
arxiv-1409-1043 | Variability of Behaviour in Electricity Load Profile Clustering; Who Does Things at the Same Time Each Day? |  http://arxiv.org/abs/1409.1043  | author:Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden category:cs.LG cs.CE published:2014-09-03 summary:UK electricity market changes provide opportunities to alter households'electricity usage patterns for the benefit of the overall electricity network.Work on clustering similar households has concentrated on daily load profilesand the variability in regular household behaviours has not been considered.Those households with most variability in regular activities may be the mostreceptive to incentives to change timing. Whether using the variability of regular behaviour allows the creation ofmore consistent groupings of households is investigated and compared with dailyload profile clustering. 204 UK households are analysed to find repeatingpatterns (motifs). Variability in the time of the motif is used as the basisfor clustering households. Different clustering algorithms are assessed by theconsistency of the results. Findings show that variability of behaviour, using motifs, provides moreconsistent groupings of households across different clustering algorithms andallows for more efficient targeting of behaviour change interventions.
arxiv-1409-5774 | Attributes for Causal Inference in Longitudinal Observational Databases |  http://arxiv.org/abs/1409.5774  | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.CE cs.LG published:2014-09-03 summary:The pharmaceutical industry is plagued by the problem of side effects thatcan occur anytime a prescribed medication is ingested. There has been a recentinterest in using the vast quantities of medical data available in longitudinalobservational databases to identify causal relationships between drugs andmedical events. Unfortunately the majority of existing post marketingsurveillance algorithms measure how dependant or associated an event is on thepresence of a drug rather than measuring causality. In this paper weinvestigate potential attributes that can be used in causal inference toidentify side effects based on the Bradford-Hill causality criteria. Potentialattributes are developed by considering five of the causality criteria andfeature selection is applied to identify the most suitable of these attributesfor detecting side effects. We found that attributes based on the specificitycriterion may improve side effect signalling algorithms but the experiment anddosage criteria attributes investigated in this paper did not offer sufficientadditional information.
arxiv-1409-0964 | Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive Features |  http://arxiv.org/abs/1409.0964  | author:Liansheng Zhuang, Shenghua Gao, Jinhui Tang, Jingjing Wang, Zhouchen Lin, Yi Ma category:cs.CV cs.LG published:2014-09-03 summary:This paper aims at constructing a good graph for discovering intrinsic datastructures in a semi-supervised learning setting. Firstly, we propose to builda non-negative low-rank and sparse (referred to as NNLRS) graph for the givendata representation. Specifically, the weights of edges in the graph areobtained by seeking a nonnegative low-rank and sparse matrix that representseach data sample as a linear combination of others. The so-obtained NNLRS-graphcan capture both the global mixture of subspaces structure (by the lowrankness) and the locally linear structure (by the sparseness) of the data,hence is both generative and discriminative. Secondly, as good features areextremely important for constructing a good graph, we propose to learn the dataembedding matrix and construct the graph jointly within one framework, which istermed as NNLRS with embedded features (referred to as NNLRS-EF). Extensiveexperiments on three publicly available datasets demonstrate that the proposedmethod outperforms the state-of-the-art graph construction method by a largemargin for both semi-supervised classification and discriminative analysis,which verifies the effectiveness of our proposed method.
arxiv-1409-1259 | On the Properties of Neural Machine Translation: Encoder-Decoder Approaches |  http://arxiv.org/abs/1409.1259  | author:Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio category:cs.CL stat.ML published:2014-09-03 summary:Neural machine translation is a relatively new approach to statisticalmachine translation based purely on neural networks. The neural machinetranslation models often consist of an encoder and a decoder. The encoderextracts a fixed-length representation from a variable-length input sentence,and the decoder generates a correct translation from this representation. Inthis paper, we focus on analyzing the properties of the neural machinetranslation using two models; RNN Encoder--Decoder and a newly proposed gatedrecursive convolutional neural network. We show that the neural machinetranslation performs relatively well on short sentences without unknown words,but its performance degrades rapidly as the length of the sentence and thenumber of unknown words increase. Furthermore, we find that the proposed gatedrecursive convolutional network learns a grammatical structure of a sentenceautomatically.
arxiv-1409-1143 | Tunably Rugged Landscapes with Known Maximum and Minimum |  http://arxiv.org/abs/1409.1143  | author:Narine Manukyan, Margaret J. Eppstein, Jeffrey S. Buzas category:cs.NE published:2014-09-03 summary:We propose NM landscapes as a new class of tunably rugged benchmark problems.NM landscapes are well-defined on alphabets of any arity, including bothdiscrete and real-valued alphabets, include epistasis in a natural andtransparent manner, are proven to have known value and location of the globalmaximum and, with some additional constraints, are proven to also have a knownglobal minimum. Empirical studies are used to illustrate that, whencoefficients are selected from a recommended distribution, the ruggedness of NMlandscapes is smoothly tunable and correlates with several measures of searchdifficulty. We discuss why these properties make NM landscapes preferable toboth NK landscapes and Walsh polynomials as benchmark landscape models withtunable epistasis.
arxiv-1409-0934 | Breakdown Point of Robust Support Vector Machine |  http://arxiv.org/abs/1409.0934  | author:Takafumi Kanamori, Shuhei Fujiwara, Akiko Takeda category:stat.ML cs.LG published:2014-09-03 summary:The support vector machine (SVM) is one of the most successful learningmethods for solving classification problems. Despite its popularity, SVM has aserious drawback, that is sensitivity to outliers in training samples. Thepenalty on misclassification is defined by a convex loss called the hinge loss,and the unboundedness of the convex loss causes the sensitivity to outliers. Todeal with outliers, robust variants of SVM have been proposed, such as therobust outlier detection algorithm and an SVM with a bounded loss called theramp loss. In this paper, we propose a robust variant of SVM and investigateits robustness in terms of the breakdown point. The breakdown point is arobustness measure that is the largest amount of contamination such that theestimated classifier still gives information about the non-contaminated data.The main contribution of this paper is to show an exact evaluation of thebreakdown point for the robust SVM. For learning parameters such as theregularization parameter in our algorithm, we derive a simple formula thatguarantees the robustness of the classifier. When the learning parameters aredetermined with a grid search using cross validation, our formula works toreduce the number of candidate search points. The robustness of the proposedmethod is confirmed in numerical experiments. We show that the statisticalproperties of the robust SVM are well explained by a theoretical analysis ofthe breakdown point.
arxiv-1409-1411 | Visual Speech Recognition |  http://arxiv.org/abs/1409.1411  | author:Ahmad B. A. Hassanat category:cs.CV published:2014-09-03 summary:Lip reading is used to understand or interpret speech without hearing it, atechnique especially mastered by people with hearing difficulties. The abilityto lip read enables a person with a hearing impairment to communicate withothers and to engage in social activities, which otherwise would be difficult.Recent advances in the fields of computer vision, pattern recognition, andsignal processing has led to a growing interest in automating this challengingtask of lip reading. Indeed, automating the human ability to lip read, aprocess referred to as visual speech recognition (VSR) (or sometimes speechreading), could open the door for other novel related applications. VSR hasreceived a great deal of attention in the last decade for its potential use inapplications such as human-computer interaction (HCI), audio-visual speechrecognition (AVSR), speaker recognition, talking heads, sign languagerecognition and video surveillance. Its main aim is to recognise spoken word(s)by using only the visual signal that is produced during speech. Hence, VSRdeals with the visual domain of speech and involves image processing,artificial intelligence, object detection, pattern recognition, statisticalmodelling, etc.
arxiv-1409-0925 | Bypassing Captcha By Machine A Proof For Passing The Turing Test |  http://arxiv.org/abs/1409.0925  | author:Ahmad B. A. Hassanat category:cs.CV cs.AI cs.HC published:2014-09-03 summary:For the last ten years, CAPTCHAs have been widely used by websites to preventtheir data being automatically updated by machines. By supposedly allowing onlyhumans to do so, CAPTCHAs take advantage of the reverse Turing test (TT),knowing that humans are more intelligent than machines. Generally, CAPTCHAshave defeated machines, but things are changing rapidly as technology improves.Hence, advanced research into optical character recognition (OCR) is overtakingattempts to strengthen CAPTCHAs against machine-based attacks. This paperinvestigates the immunity of CAPTCHA, which was built on the failure of the TT.We show that some CAPTCHAs are easily broken using a simple OCR machine builtfor the purpose of this study. By reviewing other techniques, we show that evenmore difficult CAPTCHAs can be broken using advanced OCR machines. Currentadvances in OCR should enable machines to pass the TT in the image recognitiondomain, which is exactly where machines are seeking to overcome CAPTCHAs. Weenhance traditional CAPTCHAs by employing not only characters, but also naturallanguage and multiple objects within the same CAPTCHA. The proposed CAPTCHAsmight be able to hold out against machines, at least until the advent of amachine that passes the TT completely.
arxiv-1409-1199 | Focused Proofreading: Efficiently Extracting Connectomes from Segmented EM Images |  http://arxiv.org/abs/1409.1199  | author:Stephen M. Plaza category:q-bio.QM cs.CV published:2014-09-03 summary:Identifying complex neural circuitry from electron microscopic (EM) imagesmay help unlock the mysteries of the brain. However, identifying this circuitryrequires time-consuming, manual tracing (proofreading) due to the size andintricacy of these image datasets, thus limiting state-of-the-art analysis tovery small brain regions. Potential avenues to improve scalability includeautomatic image segmentation and crowd sourcing, but current efforts have hadlimited success. In this paper, we propose a new strategy, focusedproofreading, that works with automatic segmentation and aims to limitproofreading to the regions of a dataset that are most impactful to theresulting circuit. We then introduce a novel workflow, which exploitsbiological information such as synapses, and apply it to a large dataset in thefly optic lobe. With our techniques, we achieve significant tracing speedups of3-5x without sacrificing the quality of the resulting circuit. Furthermore, ourmethodology makes the task of proofreading much more accessible and hencepotentially enhances the effectiveness of crowd sourcing.
arxiv-1409-1200 | Domain Transfer Structured Output Learning |  http://arxiv.org/abs/1409.1200  | author:Jim Jing-Yan Wang category:cs.LG published:2014-09-03 summary:In this paper, we propose the problem of domain transfer structured outputlearn- ing and the first solution to solve it. The problem is defined on twodifferent data domains sharing the same input and output spaces, named assource domain and target domain. The outputs are structured, and for the datasamples of the source domain, the corresponding outputs are available, whilefor most data samples of the target domain, the corresponding outputs aremissing. The input distributions of the two domains are significantlydifferent. The problem is to learn a predictor for the target domain to predictthe structured outputs from the input. Due to the limited number of outputsavailable for the samples form the target domain, it is difficult to directlylearn the predictor from the target domain, thus it is necessary to use theoutput information available in source domain. We propose to learn the targetdomain predictor by adapting a auxiliary predictor trained by using sourcedomain data to the target domain. The adaptation is implemented by adding adelta function on the basis of the auxiliary predictor. An algorithm isdeveloped to learn the parameter of the delta function to minimize lossfunctions associat- ed with the predicted outputs against the true outputs ofthe data samples with available outputs of the target domain.
arxiv-1409-0940 | High-performance Kernel Machines with Implicit Distributed Optimization and Randomization |  http://arxiv.org/abs/1409.0940  | author:Vikas Sindhwani, Haim Avron category:stat.ML cs.DC cs.LG published:2014-09-03 summary:In order to fully utilize "big data", it is often required to use "bigmodels". Such models tend to grow with the complexity and size of the trainingdata, and do not make strong parametric assumptions upfront on the nature ofthe underlying statistical dependencies. Kernel methods fit this need well, asthey constitute a versatile and principled statistical methodology for solvinga wide range of non-parametric modelling problems. However, their highcomputational costs (in storage and time) pose a significant barrier to theirwidespread adoption in big data applications. We propose an algorithmic framework and high-performance implementation formassive-scale training of kernel-based statistical models, based on combiningtwo key technical ingredients: (i) distributed general purpose convexoptimization, and (ii) the use of randomization to improve the scalability ofkernel methods. Our approach is based on a block-splitting variant of theAlternating Directions Method of Multipliers, carefully reconfigured to handlevery large random feature matrices, while exploiting hybrid parallelismtypically found in modern clusters of multicore machines. Our implementationsupports a variety of statistical learning tasks by enabling several lossfunctions, regularization schemes, kernels, and layers of randomizedapproximations for both dense and sparse datasets, in a highly extensibleframework. We evaluate the ability of our framework to learn models on datafrom applications, and provide a comparison against existing sequential andparallel libraries.
arxiv-1409-1257 | Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation |  http://arxiv.org/abs/1409.1257  | author:Jean Pouget-Abadie, Dzmitry Bahdanau, Bart van Merrienboer, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML published:2014-09-03 summary:The authors of (Cho et al., 2014a) have shown that the recently introducedneural network translation systems suffer from a significant drop intranslation quality when translating long sentences, unlike existingphrase-based translation systems. In this paper, we propose a way to addressthis issue by automatically segmenting an input sentence into phrases that canbe easily translated by the neural network translation model. Once each segmenthas been independently translated by the neural machine translation model, thetranslated clauses are concatenated to form a final translation. Empiricalresults show a significant improvement in translation quality for longsentences.
arxiv-1409-1062 | Structured Low-Rank Matrix Factorization with Missing and Grossly Corrupted Observations |  http://arxiv.org/abs/1409.1062  | author:Fanhua Shang, Yuanyuan Liu, Hanghang Tong, James Cheng, Hong Cheng category:cs.LG cs.CV stat.ML published:2014-09-03 summary:Recovering low-rank and sparse matrices from incomplete or corruptedobservations is an important problem in machine learning, statistics,bioinformatics, computer vision, as well as signal and image processing. Intheory, this problem can be solved by the natural convex joint/mixedrelaxations (i.e., l_{1}-norm and trace norm) under certain conditions.However, all current provable algorithms suffer from superlinear per-iterationcost, which severely limits their applicability to large-scale problems. Inthis paper, we propose a scalable, provable structured low-rank matrixfactorization method to recover low-rank and sparse matrices from missing andgrossly corrupted data, i.e., robust matrix completion (RMC) problems, orincomplete and grossly corrupted measurements, i.e., compressive principalcomponent pursuit (CPCP) problems. Specifically, we first present twosmall-scale matrix trace norm regularized bilinear structured factorizationmodels for RMC and CPCP problems, in which repetitively calculating SVD of alarge-scale matrix is replaced by updating two much smaller factor matrices.Then, we apply the alternating direction method of multipliers (ADMM) toefficiently solve the RMC problems. Finally, we provide the convergenceanalysis of our algorithm, and extend it to address general CPCP problems.Experimental results verified both the efficiency and effectiveness of ourmethod compared with the state-of-the-art methods.
arxiv-1409-1073 | Performance Analysis on Evolutionary Algorithms for the Minimum Label Spanning Tree Problem |  http://arxiv.org/abs/1409.1073  | author:Xinsheng Lai, Yuren Zhou, Jun He, Jun Zhang category:cs.NE cs.DS published:2014-09-03 summary:Some experimental investigations have shown that evolutionary algorithms(EAs) are efficient for the minimum label spanning tree (MLST) problem.However, we know little about that in theory. As one step towards this issue,we theoretically analyze the performances of the (1+1) EA, a simple version ofEAs, and a multi-objective evolutionary algorithm called GSEMO on the MLSTproblem. We reveal that for the MLST$_{b}$ problem the (1+1) EA and GSEMOachieve a $\frac{b+1}{2}$-approximation ratio in expected polynomial times of$n$ the number of nodes and $k$ the number of labels. We also show that GSEMOachieves a $(2ln(n))$-approximation ratio for the MLST problem in expectedpolynomial time of $n$ and $k$. At the same time, we show that the (1+1) EA andGSEMO outperform local search algorithms on three instances of the MLSTproblem. We also construct an instance on which GSEMO outperforms the (1+1) EA.
arxiv-1409-0924 | Visual Passwords Using Automatic Lip Reading |  http://arxiv.org/abs/1409.0924  | author:Ahmad Basheer Hassanat category:cs.CV cs.CR published:2014-09-02 summary:This paper presents a visual passwords system to increase security. Thesystem depends mainly on recognizing the speaker using the visual speech signalalone. The proposed scheme works in two stages: setting the visual passwordstage and the verification stage. At the setting stage the visual passwordssystem request the user to utter a selected password, a video recording of theuser face is captured, and processed by a special words-based VSR system whichextracts a sequence of feature vectors. In the verification stage, the sameprocedure is executed, the features will be sent to be compared with the storedvisual password. The proposed scheme has been evaluated using a video databaseof 20 different speakers (10 females and 10 males), and 15 more males inanother video database with different experiment sets. The evaluation hasproved the system feasibility, with average error rate in the range of 7.63% to20.51% at the worst tested scenario, and therefore, has potential to be apractical approach with the support of other conventional authenticationmethods such as the use of usernames and passwords.
arxiv-1409-0814 | CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein Tertiary Structure Retrieval |  http://arxiv.org/abs/1409.0814  | author:Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman, Md. Abul Kashem Mia, Farhana Zaman, Salman Rakin category:cs.CV cs.CE cs.IR published:2014-09-02 summary:Due to the advancements in technology number of entries in the structuraldatabase of proteins are increasing day by day. Methods for retrieving proteintertiary structures from this large database is the key to comparative analysisof structures which plays an important role to understand proteins and theirfunction. In this paper, we present fast and accurate methods for the retrievalof proteins from a large database with tertiary structures similar to a queryprotein. Our proposed methods borrow ideas from the field of computer vision.The speed and accuracy of our methods comes from the two newly introducedfeatures, the co-occurrence matrix of the oriented gradient and pyramidhistogram of oriented gradient and from the use of Euclidean distance as thedistance measure. Experimental results clearly indicate the superiority of ourapproach in both running time and accuracy. Our method is readily available foruse from this website: http://research.buet.ac.bd:8080/Comograd/.
arxiv-1409-0797 | Feature Engineering for Map Matching of Low-Sampling-Rate GPS Trajectories in Road Network |  http://arxiv.org/abs/1409.0797  | author:Jian Yang, Liqiu Meng category:stat.ML cs.LG published:2014-09-02 summary:Map matching of GPS trajectories from a sequence of noisy observations servesthe purpose of recovering the original routes in a road network. In this workin progress, we attempt to share our experience of feature construction in aspatial database by reporting our ongoing experiment of feature extrac-tion inConditional Random Fields (CRFs) for map matching. Our preliminary results areobtained from real-world taxi GPS trajectories.
arxiv-1409-0791 | Feature Selection in Conditional Random Fields for Map Matching of GPS Trajectories |  http://arxiv.org/abs/1409.0791  | author:Jian Yang, Liqiu Meng category:stat.ML cs.AI cs.LG published:2014-09-02 summary:Map matching of the GPS trajectory serves the purpose of recovering theoriginal route on a road network from a sequence of noisy GPS observations. Itis a fundamental technique to many Location Based Services. However, mapmatching of a low sampling rate on urban road network is still a challengingtask. In this paper, the characteristics of Conditional Random Fields withregard to inducing many contextual features and feature selection are exploredfor the map matching of the GPS trajectories at a low sampling rate.Experiments on a taxi trajectory dataset show that our method may achievecompetitive results along with the success of reducing model complexity forcomputation-limited applications.
arxiv-1409-0788 | Ensemble Learning of Colorectal Cancer Survival Rates |  http://arxiv.org/abs/1409.0788  | author:Chris Roadknight, Uwe Aickelin, John Scholefield, Lindy Durrant category:cs.LG cs.CE published:2014-09-02 summary:In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. We build onexisting research on clustering and machine learning facets of this data todemonstrate a role for an ensemble approach to highlighting patients withclearer prognosis parameters. Results for survival prediction using 3 differentapproaches are shown for a subset of the data which is most difficult to model.The performance of each model individually is compared with subsets of the datawhere some agreement is reached for multiple models. Significant improvementsin model accuracy on an unseen test set can be achieved for patients whereagreement between models is achieved.
arxiv-1409-0775 | Feature selection in detection of adverse drug reactions from the Health Improvement Network (THIN) database |  http://arxiv.org/abs/1409.0775  | author:Yihui Liu, Uwe Aickelin category:cs.LG cs.CE published:2014-09-02 summary:Adverse drug reaction (ADR) is widely concerned for public health issue. ADRsare one of most common causes to withdraw some drugs from market. Prescriptionevent monitoring (PEM) is an important approach to detect the adverse drugreactions. The main problem to deal with this method is how to automaticallyextract the medical events or side effects from high-throughput medical events,which are collected from day to day clinical practice. In this study we proposea novel concept of feature matrix to detect the ADRs. Feature matrix, which isextracted from big medical data from The Health Improvement Network (THIN)database, is created to characterize the medical events for the patients whotake drugs. Feature matrix builds the foundation for the irregular and bigmedical data. Then feature selection methods are performed on feature matrix todetect the significant features. Finally the ADRs can be located based on thesignificant features. The experiments are carried out on three drugs:Atorvastatin, Alendronate, and Metoclopramide. Major side effects for each drugare detected and better performance is achieved compared to other computerizedmethods. The detected ADRs are based on computerized methods, furtherinvestigation is needed.
arxiv-1409-0772 | Signalling Paediatric Side Effects using an Ensemble of Simple Study Designs |  http://arxiv.org/abs/1409.0772  | author:Jenna M. Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE published:2014-09-02 summary:Background: Children are frequently prescribed medication off-label, meaningthere has not been sufficient testing of the medication to determine its safetyor effectiveness. The main reason this safety knowledge is lacking is due toethical restrictions that prevent children from being included in the majorityof clinical trials. Objective: The objective of this paper is to investigatewhether an ensemble of simple study designs can be implemented to signalacutely occurring side effects effectively within the paediatric population byusing historical longitudinal data. The majority of pharmacovigilancetechniques are unsupervised, but this research presents a supervised framework.Methods: Multiple measures of association are calculated for each drug andmedical event pair and these are used as features that are fed into aclassiffier to determine the likelihood of the drug and medical event paircorresponding to an adverse drug reaction. The classiffier is trained usingknown adverse drug reactions or known non-adverse drug reaction relationships.Results: The novel ensemble framework obtained a false positive rate of 0:149,a sensitivity of 0:547 and a specificity of 0:851 when implemented on areference set of drug and medical event pairs. The novel framework consistentlyoutperformed each individual simple study design. Conclusion: This researchshows that it is possible to exploit the mechanism of causality and presents aframework for signalling adverse drug reactions effectively.
arxiv-1409-0768 | A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect Discovery |  http://arxiv.org/abs/1409.0768  | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE published:2014-09-02 summary:Drugs are frequently prescribed to patients with the aim of improving eachpatient's medical state, but an unfortunate consequence of most prescriptiondrugs is the occurrence of undesirable side effects. Side effects that occur inmore than one in a thousand patients are likely to be signalled efficiently bycurrent drug surveillance methods, however, these same methods may take decadesbefore generating signals for rarer side effects, risking medical morbidity ormortality in patients prescribed the drug while the rare side effect isundiscovered. In this paper we propose a novel computational meta-analysisframework for signalling rare side effects that integrates existing methods,knowledge from the web, metric learning and semi-supervised clustering. Thenovel framework was able to signal many known rare and serious side effects forthe selection of drugs investigated, such as tendon rupture when prescribedCiprofloxacin or Levofloxacin, renal failure with Naproxen and depressionassociated with Rimonabant. Furthermore, for the majority of the druginvestigated it generated signals for rare side effects at a more stringentsignalling threshold than existing methods and shows the potential to become afundamental part of post marketing surveillance to detect rare side effects.
arxiv-1409-0763 | Data classification using the Dempster-Shafer method |  http://arxiv.org/abs/1409.0763  | author:Qi Chen, Amanda Whitbrook, Uwe Aickelin, Chris Roadknight category:cs.LG published:2014-09-02 summary:In this paper, the Dempster-Shafer method is employed as the theoreticalbasis for creating data classification systems. Testing is carried out usingthree popular (multiple attribute) benchmark datasets that have two, three andfour classes. In each case, a subset of the available data is used for trainingto establish thresholds, limits or likelihoods of class membership for eachattribute, and hence create mass functions that establish probability of classmembership for each attribute of the test data. Classification of each dataitem is achieved by combination of these probabilities via Dempster's Rule ofCombination. Results for the first two datasets show extremely highclassification accuracy that is competitive with other popular methods. Thethird dataset is non-numerical and difficult to classify, but good results canbe achieved provided the system and mass functions are designed carefully andthe right attributes are chosen for combination. In all cases theDempster-Shafer method provides comparable performance to other more popularalgorithms, but the overhead of generating accurate mass functions increasesthe complexity with the addition of new attributes. Overall, the resultssuggest that the D-S approach provides a suitable framework for the design ofclassification systems and that automating the mass function design andcalculation would increase the viability of the algorithm for complexclassification problems.
arxiv-1409-0749 | Image Retrieval And Classification Using Local Feature Vectors |  http://arxiv.org/abs/1409.0749  | author:Vikas Verma category:cs.IR cs.CV cs.MM published:2014-09-02 summary:Content Based Image Retrieval(CBIR) is one of the important subfield in thefield of Information Retrieval. The goal of a CBIR algorithm is to retrievesemantically similar images in response to a query image submitted by the enduser. CBIR is a hard problem because of the phenomenon known as $\textit{semantic gap}$. In this thesis, we aim at analyzing the performance of a CBIR system buildusing local feature vectors and Intermediate Matching Kernel. We also propose aTwo-Step Matching process for reducing the response time of the CBIR systems.Further, we develop a Meta-Learning framework for improving the retrievalperformance of these systems. Our results show that the Two-Step Matchingprocess significantly reduces response time and the Meta-Learning Frameworkimproves the retrieval performance by more than two fold. We also analyze theperformance of various image classification systems that use different imagerepresentations constructed from the local feature vectors.
arxiv-1409-0748 | Comparison of algorithms that detect drug side effects using electronic healthcare databases |  http://arxiv.org/abs/1409.0748  | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack Gibson, Richard Hubbard category:cs.LG cs.CE published:2014-09-02 summary:The electronic healthcare databases are starting to become more readilyavailable and are thought to have excellent potential for generating adversedrug reaction signals. The Health Improvement Network (THIN) database is anelectronic healthcare database containing medical information on over 11million patients that has excellent potential for detecting ADRs. In this paperwe apply four existing electronic healthcare database signal detectingalgorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on theTHIN database for a selection of drugs from six chosen drug families. This isthe first comparison of ADR signalling algorithms that includes MUTARA and HUNTand enabled us to set a benchmark for the adverse drug reaction signallingability of the THIN database. The drugs were selectively chosen to enable acomparison with previous work and for variety. It was found that no algorithmwas generally superior and the algorithms' natural thresholds act at variablestringencies. Furthermore, none of the algorithms perform well at detectingrare ADRs.
arxiv-1409-0745 | A natural framework for sparse hierarchical clustering |  http://arxiv.org/abs/1409.0745  | author:Hongyang Zhang, Ruben H. Zamar category:stat.ML cs.LG 62H30 published:2014-09-02 summary:There has been a surge in the number of large and flat data sets - data setscontaining a large number of features and a relatively small number ofobservations - due to the growing ability to collect and store information inmedical research and other fi?elds. Hierarchical clustering is a widely usedclustering tool. In hierarchical clustering, large and flat data sets may allowfor a better coverage of clustering features (features that help explain thetrue underlying clusters) but, such data sets usually include a large fractionof noise features (non-clustering features) that may hide the underlyingclusters. Witten and Tibshirani (2010) proposed a sparse hierarchicalclustering framework to cluster the observations using an adaptively chosensubset of the features, however, we show that this framework has somelimitations when the data sets contain clustering features with complexstructure. In this paper, another sparse hierarchical clustering (SHC)framework is proposed. We show that, using simulation studies and real dataexamples, the proposed framework produces superior feature selection andclustering performance comparing to the classical (of-the-shelf) hierarchicalclustering and the existing sparse hierarchical clustering framework.
arxiv-1409-0602 | Transferring Landmark Annotations for Cross-Dataset Face Alignment |  http://arxiv.org/abs/1409.0602  | author:Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang category:cs.CV published:2014-09-02 summary:Dataset bias is a well known problem in object recognition domain. Thisissue, nonetheless, is rarely explored in face alignment research. In thisstudy, we show that dataset plays an integral part of face alignmentperformance. Specifically, owing to face alignment dataset bias, training onone database and testing on another or unseen domain would lead to poorperformance. Creating an unbiased dataset through combining various existingdatabases, however, is non-trivial as one has to exhaustively re-label thelandmarks for standardisation. In this work, we propose a simple and yeteffective method to bridge the disparate annotation spaces between databases,making datasets fusion possible. We show extensive results on combining variouspopular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset andunseen data alignment.
arxiv-1409-0585 | On the Equivalence Between Deep NADE and Generative Stochastic Networks |  http://arxiv.org/abs/1409.0585  | author:Li Yao, Sherjil Ozair, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG published:2014-09-02 summary:Neural Autoregressive Distribution Estimators (NADEs) have recently beenshown as successful alternatives for modeling high dimensional multimodaldistributions. One issue associated with NADEs is that they rely on aparticular order of factorization for $P(\mathbf{x})$. This issue has beenrecently addressed by a variant of NADE called Orderless NADEs and its deeperversion, Deep Orderless NADE. Orderless NADEs are trained based on a criterionthat stochastically maximizes $P(\mathbf{x})$ with all possible orders offactorizations. Unfortunately, ancestral sampling from deep NADE is veryexpensive, corresponding to running through a neural net separately predictingeach of the visible variables given some others. This work makes a connectionbetween this criterion and the training criterion for Generative StochasticNetworks (GSNs). It shows that training NADEs in this way also trains a GSN,which defines a Markov chain associated with the NADE model. Based on thisconnection, we show an alternative way to sample from a trained Orderless NADEthat allows to trade-off computing time and quality of the samples: a 3 to10-fold speedup (taking into account the waste due to correlations betweenconsecutive samples of the chain) can be obtained without noticeably reducingthe quality of the samples. This is achieved using a novel sampling procedurefor GSNs called annealed GSN sampling, similar to tempering methods thatcombines fast mixing (obtained thanks to steps at high noise levels) withaccurate samples (obtained thanks to steps at low noise levels).
arxiv-1409-0915 | An Approach for Text Steganography Based on Markov Chains |  http://arxiv.org/abs/1409.0915  | author:H. Hernan Moraldo category:cs.MM cs.CL 68P25, 94A60 D.4.6 published:2014-09-02 summary:A text steganography method based on Markov chains is introduced, togetherwith a reference implementation. This method allows for information hiding intexts that are automatically generated following a given Markov model. OtherMarkov - based systems of this kind rely on big simplifications of the languagemodel to work, which produces less natural looking and more easily detectabletexts. The method described here is designed to generate texts within a goodapproximation of the original language model provided.
arxiv-1409-0919 | Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach |  http://arxiv.org/abs/1409.0919  | author:Ahmad Basheer Hassanat, Mohammad Ali Abbadi, Ghada Awad Altarawneh, Ahmad Ali Alhasanat category:cs.LG published:2014-09-02 summary:This paper presents a new solution for choosing the K parameter in thek-nearest neighbor (KNN) algorithm, the solution depending on the idea ofensemble learning, in which a weak KNN classifier is used each time with adifferent K, starting from one to the square root of the size of the trainingset. The results of the weak classifiers are combined using the weighted sumrule. The proposed solution was tested and compared to other solutions using agroup of experiments in real life problems. The experimental results show thatthe proposed classifier outperforms the traditional KNN classifier that uses adifferent number of neighbors, is competitive with other classifiers, and is apromising classifier with strong potential for a wide range of applications.
arxiv-1409-0685 | Effective Spectral Unmixing via Robust Representation and Learning-based Sparsity |  http://arxiv.org/abs/1409.0685  | author:Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Chunhong Pan category:cs.CV published:2014-09-02 summary:Hyperspectral unmixing (HU) plays a fundamental role in a wide range ofhyperspectral applications. It is still challenging due to the common presenceof outlier channels and the large solution space. To address the above twoissues, we propose a novel model by emphasizing both robust representation andlearning-based sparsity. Specifically, we apply the $\ell_{2,1}$-norm tomeasure the representation error, preventing outlier channels from dominatingour objective. In this way, the side effects of outlier channels are greatlyrelieved. Besides, we observe that the mixed level of each pixel varies overimage grids. Based on this observation, we exploit a learning-based sparsitymethod to simultaneously learn the HU results and a sparse guidance map. Viathis guidance map, the sparsity constraint in the $\ell_{p}\!\left(\!0\!<\!p\!\leq\!1\right)$-norm is adaptively imposed according to the learnt mixedlevel of each pixel. Compared with state-of-the-art methods, our model isbetter suited to the real situation, thus expected to achieve better HUresults. The resulted objective is highly non-convex and non-smooth, and so itis hard to optimize. As a profound theoretical contribution, we propose anefficient algorithm to solve it. Meanwhile, the convergence proof and thecomputational complexity analysis are systematically provided. Extensiveevaluations verify that our method is highly promising for the HU task---itachieves very accurate guidance maps and much better HU results compared withstate-of-the-art methods.
arxiv-1409-0923 | Dimensionality Invariant Similarity Measure |  http://arxiv.org/abs/1409.0923  | author:Ahmad Basheer Hassanat category:cs.LG published:2014-09-02 summary:This paper presents a new similarity measure to be used for general tasksincluding supervised learning, which is represented by the K-nearest neighborclassifier (KNN). The proposed similarity measure is invariant to largedifferences in some dimensions in the feature space. The proposed metric isproved mathematically to be a metric. To test its viability for differentapplications, the KNN used the proposed metric for classifying test exampleschosen from a number of real datasets. Compared to some other well knownmetrics, the experimental results show that the proposed metric is a promisingdistance measure for the KNN classifier with strong potential for a wide rangeof applications.
arxiv-1409-0908 | Action Recognition in the Frequency Domain |  http://arxiv.org/abs/1409.0908  | author:Anh Tran, Jinyan Guan, Thanima Pilantanakitti, Paul Cohen category:cs.CV published:2014-09-02 summary:In this paper, we describe a simple strategy for mitigating variability intemporal data series by shifting focus onto long-term, frequency domainfeatures that are less susceptible to variability. We apply this method to thehuman action recognition task and demonstrate how working in the frequencydomain can yield good recognition features for commonly used optical flow andarticulated pose features, which are highly sensitive to small differences inmotion, viewpoint, dynamic backgrounds, occlusion and other sources ofvariability. We show how these frequency-based features can be used incombination with a simple forest classifier to achieve good and robust resultson the popular KTH Actions dataset.
arxiv-1409-0280 | Towards a Calculus of Echo State Networks |  http://arxiv.org/abs/1409.0280  | author:Alireza Goudarzi, Darko Stefanovic category:cs.NE published:2014-09-01 summary:Reservoir computing is a recent trend in neural networks which uses thedynamical perturbations on the phase space of a system to compute a desiredtarget function. We present how one can formulate an expectation of systemperformance in a simple class of reservoir computing called echo statenetworks. In contrast with previous theoretical frameworks, which only revealan upper bound on the total memory in the system, we analytically calculate theentire memory curve as a function of the structure of the system and theproperties of the input and the target function. We demonstrate the precisionof our framework by validating its result for a wide range of system sizes andspectral radii. Our analytical calculation agrees with numerical simulations.To the best of our knowledge this work presents the first exact analyticalcharacterization of the memory curve in echo state networks.
arxiv-1409-0272 | Multi-task Sparse Structure Learning |  http://arxiv.org/abs/1409.0272  | author:Andre R. Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar, Fernando J. Von Zuben, Arindam Banerjee category:cs.LG stat.ML I.5.1, J.2 published:2014-09-01 summary:Multi-task learning (MTL) aims to improve generalization performance bylearning multiple related tasks simultaneously. While sometimes the underlyingtask relationship structure is known, often the structure needs to be estimatedfrom data at hand. In this paper, we present a novel family of models for MTL,applicable to regression and classification problems, capable of learning thestructure of task relationships. In particular, we consider a joint estimationproblem of the task relationship structure and the individual task parameters,which is solved using alternating minimization. The task relationship structurelearning component builds on recent advances in structure learning of Gaussiangraphical models based on sparse estimators of the precision (inversecovariance) matrix. We illustrate the effectiveness of the proposed model on avariety of synthetic and benchmark datasets for regression and classification.We also consider the problem of combining climate model outputs for betterprojections of future climate, with focus on temperature in South America, andshow that the proposed model outperforms several existing methods for theproblem.
arxiv-1409-0314 | Empirical Evaluation of Tree distances for Parser Evaluation |  http://arxiv.org/abs/1409.0314  | author:Taraka Rama category:cs.CL published:2014-09-01 summary:In this empirical study, I compare various tree distance measures --originally developed in computational biology for the purpose of treecomparison -- for the purpose of parser evaluation. I will control for theparser setting by comparing the automatically generated parse trees from thestate-of-the-art parser Charniak, 2000) with the gold-standard parse trees. Thearticle describes two different tree distance measures (RF and QD) along withits variants (GRF and GQD) for the purpose of parser evaluation. The articlewill argue that RF measure captures similar information as the standard EvalBmetric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,1989) applied by Tsarfaty et al. (2011). Finally, the article also providesempirical evidence by reporting high correlations between the different treedistances and EvalB metric's scores.
arxiv-1409-0473 | Neural Machine Translation by Jointly Learning to Align and Translate |  http://arxiv.org/abs/1409.0473  | author:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML published:2014-09-01 summary:Neural machine translation is a recently proposed approach to machinetranslation. Unlike the traditional statistical machine translation, the neuralmachine translation aims at building a single neural network that can bejointly tuned to maximize the translation performance. The models proposedrecently for neural machine translation often belong to a family ofencoder-decoders and consists of an encoder that encodes a source sentence intoa fixed-length vector from which a decoder generates a translation. In thispaper, we conjecture that the use of a fixed-length vector is a bottleneck inimproving the performance of this basic encoder-decoder architecture, andpropose to extend this by allowing a model to automatically (soft-)search forparts of a source sentence that are relevant to predicting a target word,without having to form these parts as a hard segment explicitly. With this newapproach, we achieve a translation performance comparable to the existingstate-of-the-art phrase-based system on the task of English-to-Frenchtranslation. Furthermore, qualitative analysis reveals that the(soft-)alignments found by the model agree well with our intuition.
arxiv-1409-0347 | Multi-tensor Completion for Estimating Missing Values in Video Data |  http://arxiv.org/abs/1409.0347  | author:Chao Li, Lili Guo, Andrzej Cichocki category:cs.CV published:2014-09-01 summary:Many tensor-based data completion methods aim to solve image and videoin-painting problems. But, all methods were only developed for a singledataset. In most of real applications, we can usually obtain more than onedataset to reflect one phenomenon, and all the datasets are mutually related insome sense. Thus one question raised whether such the relationship can improvethe performance of data completion or not? In the paper, we proposed a noveland efficient method by exploiting the relationship among datasets formulti-video data completion. Numerical results show that the proposed methodsignificantly improve the performance of video in-painting, particularly in thecase of very high missing percentage.
arxiv-1409-0553 | Sampling-based Approximations with Quantitative Performance for the Probabilistic Reach-Avoid Problem over General Markov Processes |  http://arxiv.org/abs/1409.0553  | author:Sofie Haesaert, Robert Babuska, Alessandro Abate category:cs.SY cs.LG published:2014-09-01 summary:This article deals with stochastic processes endowed with the Markov(memoryless) property and evolving over general (uncountable) state spaces. Themodels further depend on a non-deterministic quantity in the form of a controlinput, which can be selected to affect the probabilistic dynamics. We addressthe computation of maximal reach-avoid specifications, together with thesynthesis of the corresponding optimal controllers. The reach-avoidspecification deals with assessing the likelihood that any finite-horizontrajectory of the model enters a given goal set, while avoiding a given set ofundesired states. This article newly provides an approximate computationalscheme for the reach-avoid specification based on the Fitted Value Iterationalgorithm, which hinges on random sample extractions, and gives a-prioricomputable formal probabilistic bounds on the error made by the approximationalgorithm: as such, the output of the numerical scheme is quantitativelyassessed and thus meaningful for safety-critical applications. Furthermore, weprovide tighter probabilistic error bounds that are sample-based. The overallcomputational scheme is put in relationship with alternative approximationalgorithms in the literature, and finally its performance is practicallyassessed over a benchmark case study.
arxiv-1409-0575 | ImageNet Large Scale Visual Recognition Challenge |  http://arxiv.org/abs/1409.0575  | author:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei category:cs.CV I.4.8; I.5.2 published:2014-09-01 summary:The ImageNet Large Scale Visual Recognition Challenge is a benchmark inobject category classification and detection on hundreds of object categoriesand millions of images. The challenge has been run annually from 2010 topresent, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advancesin object recognition that have been possible as a result. We discuss thechallenges of collecting large-scale ground truth annotation, highlight keybreakthroughs in categorical object recognition, provide a detailed analysis ofthe current state of the field of large-scale image classification and objectdetection, and compare the state-of-the-art computer vision accuracy with humanaccuracy. We conclude with lessons learned in the five years of the challenge,and propose future directions and improvements.
arxiv-1409-0578 | Consistency and fluctuations for stochastic gradient Langevin dynamics |  http://arxiv.org/abs/1409.0578  | author:Yee Whye Teh, Alexandre Thiéry, Sebastian Vollmer category:stat.ML 60J22, 65C40 published:2014-09-01 summary:Applying standard Markov chain Monte Carlo (MCMC) algorithms to large datasets is computationally expensive. Both the calculation of the acceptanceprobability and the creation of informed proposals usually require an iterationthrough the whole data set. The recently proposed stochastic gradient Langevindynamics (SGLD) method circumvents this problem by generating proposals whichare only based on a subset of the data, by skipping the accept-reject step andby using decreasing step-sizes sequence $(\delta_m)_{m \geq 0}$. %Under appropriate Lyapunov conditions, We provide in this article a rigorousmathematical framework for analysing this algorithm. We prove that, underverifiable assumptions, the algorithm is consistent, satisfies a central limittheorem (CLT) and its asymptotic bias-variance decomposition can becharacterized by an explicit functional of the step-sizes sequence$(\delta_m)_{m \geq 0}$. We leverage this analysis to give practicalrecommendations for the notoriously difficult tuning of this algorithm: it isasymptotically optimal to use a step-size sequence of the type $\delta_m \asympm^{-1/3}$, leading to an algorithm whose mean squared error (MSE) decreases atrate $\mathcal{O}(m^{-1/3})$
arxiv-1409-0470 | Neural coordination can be enhanced by occasional interruption of normal firing patterns: A self-optimizing spiking neural network model |  http://arxiv.org/abs/1409.0470  | author:Alexander Woodward, Tom Froese, Takashi Ikegami category:nlin.AO cs.NE q-bio.NC 92B20 published:2014-09-01 summary:The state space of a conventional Hopfield network typically exhibits manydifferent attractors of which only a small subset satisfy constraints betweenneurons in a globally optimal fashion. It has recently been demonstrated thatcombining Hebbian learning with occasional alterations of normal neural statesavoids this problem by means of self-organized enlargement of the best basinsof attraction. However, so far it is not clear to what extent this process ofself-optimization is also operative in real brains. Here we demonstrate that itcan be transferred to more biologically plausible neural networks byimplementing a self-optimizing spiking neural network model. In addition, byusing this spiking neural network to emulate a Hopfield network with Hebbianlearning, we attempt to make a connection between rate-based and temporalcoding based neural systems. Although further work is required to make thismodel more realistic, it already suggests that the efficacy of theself-optimizing process is independent from the simplifying assumptions of aconventional Hopfield network. We also discuss natural and cultural processesthat could be responsible for occasional alteration of neural firing patternsin actual brains
arxiv-1409-0334 | Storing sequences in binary tournament-based neural networks |  http://arxiv.org/abs/1409.0334  | author:Xiaoran Jiang, Vincent Gripon, Claude Berrou, Michael Rabbat category:cs.NE published:2014-09-01 summary:An extension to a recently introduced architecture of clique-based neuralnetworks is presented. This extension makes it possible to store sequences withhigh efficiency. To obtain this property, network connections are provided withorientation and with flexible redundancy carried by both spatial and temporalredundancy, a mechanism of anticipation being introduced in the model. Inaddition to the sequence storage with high efficiency, this new scheme alsooffers biological plausibility. In order to achieve accurate sequenceretrieval, a double layered structure combining hetero-association andauto-association is also proposed.
arxiv-1409-0177 | Persistent Homology in Sparse Regression and Its Application to Brain Morphometry |  http://arxiv.org/abs/1409.0177  | author:Moo K. Chung, Jamie L. Hanson, Jieping Ye, Richard J. Davidson, Seth D. Pollak category:stat.ME cs.CV published:2014-08-31 summary:Sparse systems are usually parameterized by a tuning parameter thatdetermines the sparsity of the system. How to choose the right tuning parameteris a fundamental and difficult problem in learning the sparse system. In thispaper, by treating the the tuning parameter as an additional dimension,persistent homological structures over the parameter space is introduced andexplored. The structures are then further exploited in speeding up thecomputation using the proposed soft-thresholding technique. The topologicalstructures are further used as multivariate features in the tensor-basedmorphometry (TBM) in characterizing white matter alterations in children whohave experienced severe early life stress and maltreatment. These analysesreveal that stress-exposed children exhibit more diffuse anatomicalorganization across the whole white matter region.
arxiv-1409-0203 | Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix Completion Algorithm and Theoretical Guarantees |  http://arxiv.org/abs/1409.0203  | author:Mohammad J. Taghizadeh, Reza Parhizkar, Philip N. Garner, Herve Bourlard, Afsaneh Asaei category:cs.SD cs.LG published:2014-08-31 summary:This paper addresses the problem of ad hoc microphone array calibration whereonly partial information about the distances between microphones is available.We construct a matrix consisting of the pairwise distances and propose toestimate the missing entries based on a novel Euclidean distance matrixcompletion algorithm by alternative low-rank matrix completion and projectiononto the Euclidean distance space. This approach confines the recovered matrixto the EDM cone at each iteration of the matrix completion algorithm. Thetheoretical guarantees of the calibration performance are obtained consideringthe random and locally structured missing entries as well as the measurementnoise on the known distances. This study elucidates the links between thecalibration error and the number of microphones along with the noise level andthe ratio of missing distances. Thorough experiments on real data recordingsand simulated setups are conducted to demonstrate these theoretical insights. Asignificant improvement is achieved by the proposed Euclidean distance matrixcompletion algorithm over the state-of-the-art techniques for ad hoc microphonearray calibration.
arxiv-1409-0084 | Kernel Coding: General Formulation and Special Cases |  http://arxiv.org/abs/1409.0084  | author:Mehrtash Harandi, Mathieu Salzmann category:cs.CV published:2014-08-30 summary:Representing images by compact codes has proven beneficial for many visualrecognition tasks. Most existing techniques, however, perform this coding stepdirectly in image feature space, where the distributions of the differentclasses are typically entangled. In contrast, here, we study the problem ofperforming coding in a high-dimensional Hilbert space, where the classes areexpected to be more easily separable. To this end, we introduce a generalcoding formulation that englobes the most popular techniques, such as bag ofwords, sparse coding and locality-based coding, and show how this formulationand its special cases can be kernelized. Importantly, we address severalaspects of learning in our general formulation, such as kernel learning,dictionary learning and supervised kernel coding. Our experimental evaluationon several visual recognition tasks demonstrates the benefits of performingcoding in Hilbert space, and in particular of jointly learning the kernel, thedictionary and the classifier.
arxiv-1409-0107 | A Plug&Play P300 BCI Using Information Geometry |  http://arxiv.org/abs/1409.0107  | author:Alexandre Barachant, Marco Congedo category:cs.LG cs.HC stat.ML published:2014-08-30 summary:This paper presents a new classification methods for Event Related Potentials(ERP) based on an Information geometry framework. Through a new estimation ofcovariance matrices, this work extend the use of Riemannian geometry, which waspreviously limited to SMR-based BCI, to the problem of classification of ERPs.As compared to the state-of-the-art, this new method increases performance,reduces the number of data needed for the calibration and features goodgeneralisation across sessions and subjects. This method is illustrated on datarecorded with the P300-based game brain invaders. Finally, an online andadaptive implementation is described, where the BCI is initialized with genericparameters derived from a database and continuously adapt to the individual,allowing the user to play the game without any calibration while keeping a highaccuracy.
arxiv-1409-0083 | Sparse Coding on Symmetric Positive Definite Manifolds using Bregman Divergences |  http://arxiv.org/abs/1409.0083  | author:Mehrtash Harandi, Richard Hartley, Brian Lovell, Conrad Sanderson category:cs.CV published:2014-08-30 summary:This paper introduces sparse coding and dictionary learning for SymmetricPositive Definite (SPD) matrices, which are often used in machine learning,computer vision and related areas. Unlike traditional sparse coding schemesthat work in vector spaces, in this paper we discuss how SPD matrices can bedescribed by sparse combination of dictionary atoms, where the atoms are alsoSPD matrices. We propose to seek sparse coding by embedding the space of SPDmatrices into Hilbert spaces through two types of Bregman matrix divergences.This not only leads to an efficient way of performing sparse coding, but alsoan online and iterative scheme for dictionary learning. We apply the proposedmethods to several computer vision tasks where images are represented by regioncovariance matrices. Our proposed algorithms outperform state-of-the-artmethods on a wide range of classification tasks, including face recognition,action recognition, material classification and texture categorization.
arxiv-1408-6911 | Text Line Identification in Tagore's Manuscript |  http://arxiv.org/abs/1408.6911  | author:Chandranath Adak, Bidyut B. Chaudhuri category:cs.CV published:2014-08-29 summary:In this paper, a text line identification method is proposed. The text linesof printed document are easy to segment due to uniform straightness of thelines and sufficient gap between the lines. But in handwritten documents, theline is non-uniform and interline gaps are variable. We take RabindranathTagore's manuscript as it is one of the most difficult manuscripts that containdoodles. Our method consists of a pre-processing stage to clean the documentimage. Then we separate doodles from the manuscript to get the textual region.After that we identify the text lines on the manuscript. For text lineidentification, we use window examination, black run-length smearing,horizontal histogram and connected component analysis.
arxiv-1408-7071 | Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for Action Recognition |  http://arxiv.org/abs/1408.7071  | author:Zhenzhong Lan, Xuanchong Li, Alexandar G. Hauptmann category:cs.CV published:2014-08-29 summary:Historically, researchers in the field have spent a great deal of effort tocreate image representations that have scale invariance and retain spatiallocation information. This paper proposes to encode equivalent temporalcharacteristics in video representations for action recognition. To achievetemporal scale invariance, we develop a method called temporal scale pyramid(TSP). To encode temporal information, we present and compare two methodscalled temporal extension descriptor (TED) and temporal division pyramid (TDP). Our purpose is to suggest solutions for matching complex actions that havelarge variation in velocity and appearance, which is missing from most currentaction representations. The experimental results on four benchmark datasets,UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach andsignificantly outperform state-of-the-art methods. Most noticeably, we achieve65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51and Hollywood2 datasets which constitutes an absolute improvement over thestate-of-the-art by 7.8% and 3.9%, respectively.
arxiv-1408-6963 | Comment on "Ensemble Projection for Semi-supervised Image Classification" |  http://arxiv.org/abs/1408.6963  | author:Xavier Boix, Gemma Roig, Luc Van Gool category:cs.CV published:2014-08-29 summary:In a series of papers by Dai and colleagues [1,2], a feature map (or kernel)was introduced for semi- and unsupervised learning. This feature map is buildfrom the output of an ensemble of classifiers trained without using theground-truth class labels. In this critique, we analyze the latest version ofthis series of papers, which is called Ensemble Projections [2]. We show thatthe results reported in [2] were not well conducted, and that EnsembleProjections performs poorly for semi-supervised learning.
arxiv-1408-6988 | An Information Retrieval Approach to Short Text Conversation |  http://arxiv.org/abs/1408.6988  | author:Zongcheng Ji, Zhengdong Lu, Hang Li category:cs.IR cs.CL published:2014-08-29 summary:Human computer conversation is regarded as one of the most difficult problemsin artificial intelligence. In this paper, we address one of its keysub-problems, referred to as short text conversation, in which given a messagefrom human, the computer returns a reasonable response to the message. Weleverage the vast amount of short conversation data available on social mediato study the issue. We propose formalizing short text conversation as a searchproblem at the first step, and employing state-of-the-art information retrieval(IR) techniques to carry out the task. We investigate the significance as wellas the limitation of the IR approach. Our experiments demonstrate that theretrieval-based model can make the system behave rather "intelligently", whencombined with a huge repository of conversation data from social media.
arxiv-1408-6980 | Augmentation Schemes for Particle MCMC |  http://arxiv.org/abs/1408.6980  | author:Paul Fearnhead, Loukia Meligkotsidou category:stat.CO stat.ML published:2014-08-29 summary:Particle MCMC involves using a particle filter within an MCMC algorithm. Forinference of a model which involves an unobserved stochastic process, thestandard implementation uses the particle filter to propose new values for thestochastic process, and MCMC moves to propose new values for the parameters. Weshow how particle MCMC can be generalised beyond this. Our key idea is tointroduce new latent variables. We then use the MCMC moves to update the latentvariables, and the particle filter to propose new values for the parameters andstochastic process given the latent variables. A generic way of defining theselatent variables is to model them as pseudo-observations of the parameters orof the stochastic process. By choosing the amount of information these latentvariables have about the parameters and the stochastic process we can oftenimprove the mixing of the particle MCMC algorithm by trading off the MonteCarlo error of the particle filter and the mixing of the MCMC moves. We showthat using pseudo-observations within particle MCMC can improve its efficiencyin certain scenarios: dealing with initialisation problems of the particlefilter; speeding up the mixing of particle Gibbs when there is strongdependence between the parameters and the stochastic process; and enablingfurther MCMC steps to be used within the particle filter.
arxiv-1408-6974 | Fast Disk Conformal Parameterization of Simply-connected Open Surfaces |  http://arxiv.org/abs/1408.6974  | author:Pui Tung Choi, Lok Ming Lui category:cs.CG cs.CV cs.GR cs.MM math.DG published:2014-08-29 summary:Surface parameterizations have been widely used in computer graphics andgeometry processing. In particular, as simply-connected open surfaces areconformally equivalent to the unit disk, it is desirable to compute the diskconformal parameterizations of the surfaces. In this paper, we propose a novelalgorithm for the conformal parameterization of a simply-connected open surfaceonto the unit disk, which significantly speeds up the computation, enhances theconformality and stability, and guarantees the bijectivity. The conformalitydistortions at the inner region and on the boundary are corrected by two steps,with the aid of an iterative scheme using quasi-conformal theories.Experimental results demonstrate the effectiveness of our proposed method.
arxiv-1408-6915 | Binary matrices of optimal autocorrelations as alignment marks |  http://arxiv.org/abs/1408.6915  | author:Scott A. Skirlo, Ling Lu, Marin Soljačić category:cs.CV cs.IT math.IT published:2014-08-29 summary:We define a new class of binary matrices by maximizing the peak-sidelobedistances in the aperiodic autocorrelations. These matrices can be used asrobust position marks for in-plane spatial alignment. The optimal squarematrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matricesof 8 by 8 and 9 by 9 were found by exhaustive searches.
arxiv-1409-0031 | Tracking Dynamic Point Processes on Networks |  http://arxiv.org/abs/1409.0031  | author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.IT cs.SI math.IT published:2014-08-29 summary:Cascading chains of events are a salient feature of many real-world social,biological, and financial networks. In social networks, social reciprocityaccounts for retaliations in gang interactions, proxy wars in nation-stateconflicts, or Internet memes shared via social media. Neuron spikes stimulateor inhibit spike activity in other neurons. Stock market shocks can trigger acontagion of volatility throughout a financial network. In these and otherexamples, only individual events associated with network nodes are observed,usually without knowledge of the underlying dynamic relationships betweennodes. This paper addresses the challenge of tracking how events within suchnetworks stimulate or influence future events. The proposed approach is anonline learning framework well-suited to streaming data, using a multivariateHawkes point process model to encapsulate autoregressive features of observedevents within the social network. Recent work on online learning in dynamicenvironments is leveraged not only to exploit the dynamics within theunderlying network, but also to track that network structure as it evolves.Regret bounds and experimental results demonstrate that the proposed methodperforms nearly as well as an oracle or batch algorithm.
arxiv-1408-6804 | A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training Structural SVMs with a Costly max-Oracle |  http://arxiv.org/abs/1408.6804  | author:Neel Shah, Vladimir Kolmogorov, Christoph H. Lampert category:cs.LG published:2014-08-28 summary:Structural support vector machines (SSVMs) are amongst the best performingmodels for structured computer vision tasks, such as semantic imagesegmentation or human pose estimation. Training SSVMs, however, iscomputationally costly, because it requires repeated calls to a structuredprediction subroutine (called \emph{max-oracle}), which has to solve anoptimization problem itself, e.g. a graph cut. In this work, we introduce a new algorithm for SSVM training that is moreefficient than earlier techniques when the max-oracle is computationallyexpensive, as it is frequently the case in computer vision tasks. The main ideais to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithmwith efficient hyperplane caching, and (ii) use an automatic selection rule fordeciding whether to call the exact max-oracle or to rely on an approximate onebased on the cached hyperplanes. We show experimentally that this strategy leads to faster convergence to theoptimum with respect to the number of requires oracle calls, and that thistranslates into faster convergence with respect to the total runtime when themax-oracle is slow compared to the other steps of the algorithm. A publicly available C++ implementation is provided athttp://pub.ist.ac.at/~vnk/papers/SVM.html .
arxiv-1408-6746 | Non-Standard Words as Features for Text Categorization |  http://arxiv.org/abs/1408.6746  | author:Slobodan Beliga, Sanda Martinčić-Ipšić category:cs.CL cs.LG published:2014-08-28 summary:This paper presents categorization of Croatian texts using Non-Standard Words(NSW) as features. Non-Standard Words are: numbers, dates, acronyms,abbreviations, currency, etc. NSWs in Croatian language are determinedaccording to Croatian NSW taxonomy. For the purpose of this research, 390 textdocuments were collected and formed the SKIPEZ collection with 6 classes:official, literary, informative, popular, educational and scientific. Textcategorization experiment was conducted on three different representations ofthe SKIPEZ collection: in the first representation, the frequencies of NSWs areused as features; in the second representation, the statistic measures of NSWs(variance, coefficient of variation, standard deviation, etc.) are used asfeatures; while the third representation combines the first two feature sets.Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithmswere used in text categorization experiments. The best categorization resultsare achieved using the first feature set (NSW frequencies) with thecategorization accuracy of 87%. This suggests that the NSWs should beconsidered as features in highly inflectional languages, such as Croatian. NSWbased features reduce the dimensionality of the feature space without standardlemmatization procedures, and therefore the bag-of-NSWs should be consideredfor further Croatian texts categorization experiments.
arxiv-1408-6615 | Multispectral Palmprint Recognition Using Textural Features |  http://arxiv.org/abs/1408.6615  | author:Shervin Minaee, AmirAli Abdolrashidi category:cs.CV published:2014-08-28 summary:In order to utilize identification to the best extent, we need robust andfast algorithms and systems to process the data. Having palmprint as a reliableand unique characteristic of every person, we extract and use its featuresbased on its geometry, lines and angles. There are countless ways to definemeasures for the recognition task. To analyze a new point of view, we extractedtextural features and used them for palmprint recognition. Co-occurrence matrixcan be used for textural feature extraction. As classifiers, we have used theminimum distance classifier (MDC) and the weighted majority voting system(WMV). The proposed method is tested on a well-known multispectral palmprintdataset of 6000 samples and an accuracy rate of 99.96-100% is obtained for mostscenarios which outperforms all previous works in multispectral palmprintrecognition.
arxiv-1408-6762 | Chatbot for admissions |  http://arxiv.org/abs/1408.6762  | author:Nikolaos Polatidis category:cs.CY cs.CL published:2014-08-28 summary:The communication of potential students with a university department isperformed manually and it is a very time consuming procedure. The opportunityto communicate with on a one-to-one basis is highly valued. However with manyhundreds of applications each year, one-to-one conversations are not feasiblein most cases. The communication will require a member of academic staff toexpend several hours to find suitable answers and contact each student. Itwould be useful to reduce his costs and time. The project aims to reduce the burden on the head of admissions, andpotentially other users, by developing a convincing chatbot. A suitablealgorithm must be devised to search through the set of data and find apotential answer. The program then replies to the user and provides a relevantweb link if the user is not satisfied by the answer. Furthermore a webinterface is provided for both users and an administrator. The achievements of the project can be summarised as follows. To prepare thebackground of the project a literature review was undertaken, together with aninvestigation of existing tools, and consultation with the head of admissions.The requirements of the system were established and a range of algorithms andtools were investigated, including keyword and template matching. An algorithmthat combines keyword matching with string similarity has been developed. Ausable system using the proposed algorithm has been implemented. The system wasevaluated by keeping logs of questions and answers and by feedback received bypotential students that used it.
arxiv-1408-6741 | Memcomputing and Swarm Intelligence |  http://arxiv.org/abs/1408.6741  | author:Y. V. Pershin, M. Di Ventra category:cs.NE cs.ET published:2014-08-28 summary:We explore the relation between memcomputing, namely computing with and inmemory, and swarm intelligence algorithms. In particular, we show that one candesign memristive networks to solve short-path optimization problems that canalso be solved by ant-colony algorithms. By employing appropriate memristiveelements one can demonstrate an almost one-to-one correspondence betweenmemcomputing and ant colony optimization approaches. However, the memristivenetwork has the capability of finding the solution in one deterministic step,compared to the stochastic multi-step ant colony optimization. This resultpaves the way for nanoscale hardware implementations of several swarmintelligence algorithms that are presently explored, from scheduling problemsto robotics.
arxiv-1408-6693 | A study of the fixed points and spurious solutions of the FastICA algorithm |  http://arxiv.org/abs/1408.6693  | author:Tianwen Wei category:stat.ML published:2014-08-28 summary:The FastICA algorithm is one of the most popular iterative algorithms in thedomain of linear independent component analysis. Despite its success, it isobserved that FastICA occasionally yields outcomes that do not correspond toany true solutions (known as demixing vectors) of the ICA problem. Theseoutcomes are commonly referred to as spurious solutions. Although FastICA isamong the most extensively studied ICA algorithms, the occurrence of spurioussolutions are not yet completely understood by the community. In thiscontribution, we aim at addressing this issue. In the first part of this work,we are interested in the relationship between demixing vectors, localoptimizers of the contrast function and (attractive or unattractive) fixedpoints of FastICA algorithm. Characterizations of these sets are given, and aninclusion relationship is discovered. In the second part, we investigate thepossible scenarios where spurious solutions occur. We show that when certainbimodal Gaussian mixtures distributions are involved, there may exist spurioussolutions that are attractive fixed points of FastICA. In this case, popularnonlinearities such as "gauss" or "tanh" tend to yield spurious solutions,whereas only "kurtosis" may give reliable results. Some advices are given forthe practical choice of nonlinearity function.
arxiv-1408-6617 | Task-group Relatedness and Generalization Bounds for Regularized Multi-task Learning |  http://arxiv.org/abs/1408.6617  | author:Chao Zhang, Dacheng Tao, Tao Hu, Xiang Li category:cs.LG published:2014-08-28 summary:In this paper, we study the generalization performance of regularizedmulti-task learning (RMTL) in a vector-valued framework, where MTL isconsidered as a learning process for vector-valued functions. We are mainlyconcerned with two theoretical questions: 1) under what conditions does RMTLperform better with a smaller task sample size than STL? 2) under whatconditions is RMTL generalizable and can guarantee the consistency of each taskduring simultaneous learning? In particular, we investigate two types of task-group relatedness: theobserved discrepancy-dependence measure (ODDM) and the empiricaldiscrepancy-dependence measure (EDDM), both of which detect the dependencebetween two groups of multiple related tasks (MRTs). We then introduce theCartesian product-based uniform entropy number (CPUEN) to measure thecomplexities of vector-valued function classes. By applying the specificdeviation and the symmetrization inequalities to the vector-valued framework,we obtain the generalization bound for RMTL, which is the upper bound of thejoint probability of the event that there is at least one task with a largeempirical discrepancy between the expected and empirical risks. Finally, wepresent a sufficient condition to guarantee the consistency of each task in thesimultaneous learning process, and we discuss how task relatedness affects thegeneralization performance of RMTL. Our theoretical findings answer theaforementioned two questions.
arxiv-1408-6618 | Falsifiable implies Learnable |  http://arxiv.org/abs/1408.6618  | author:David Balduzzi category:cs.LG math.ST stat.ML stat.TH published:2014-08-28 summary:The paper demonstrates that falsifiability is fundamental to learning. Weprove the following theorem for statistical learning and sequential prediction:If a theory is falsifiable then it is learnable -- i.e. admits a strategy thatpredicts optimally. An analogous result is shown for universal induction.
arxiv-1408-6788 | Strongly Incremental Repair Detection |  http://arxiv.org/abs/1408.6788  | author:Julian Hough, Matthew Purver category:cs.CL published:2014-08-28 summary:We present STIR (STrongly Incremental Repair detection), a system thatdetects speech repairs and edit terms on transcripts incrementally with minimallatency. STIR uses information-theoretic measures from n-gram models as itsprincipal decision features in a pipeline of classifiers detecting thedifferent stages of repairs. Results on the Switchboard disfluency taggedcorpus show utterance-final accuracy on a par with state-of-the-art incrementalrepair detection methods, but with better incremental accuracy, fastertime-to-detection and less computational overhead. We evaluate its performanceusing incremental metrics and propose new repair processing evaluationstandards.
arxiv-1408-6686 | Sparse Generalized Eigenvalue Problem via Smooth Optimization |  http://arxiv.org/abs/1408.6686  | author:Junxiao Song, Prabhu Babu, Daniel P. Palomar category:stat.ML cs.LG published:2014-08-28 summary:In this paper, we consider an $\ell_{0}$-norm penalized formulation of thegeneralized eigenvalue problem (GEP), aimed at extracting the leading sparsegeneralized eigenvector of a matrix pair. The formulation involves maximizationof a discontinuous nonconcave objective function over a nonconvex constraintset, and is therefore computationally intractable. To tackle the problem, wefirst approximate the $\ell_{0}$-norm by a continuous surrogate function. Thenan algorithm is developed via iteratively majorizing the surrogate function bya quadratic separable function, which at each iteration reduces to a regulargeneralized eigenvalue problem. A preconditioned steepest ascent algorithm forfinding the leading generalized eigenvector is provided. A systematic way basedon smoothing is proposed to deal with the "singularity issue" that arises whena quadratic function is used to majorize the nondifferentiable surrogatefunction. For sparse GEPs with special structure, algorithms that admit aclosed-form solution at every iteration are derived. Numerical experiments showthat the proposed algorithms match or outperform existing algorithms in termsof computational complexity and support recovery.
arxiv-1408-6335 | Compression, Restoration, Re-sampling, Compressive Sensing: Fast Transforms in Digital Imaging |  http://arxiv.org/abs/1408.6335  | author:Leonid Yaroslavsky category:cs.CV physics.optics published:2014-08-27 summary:Transform image processing methods are methods that work in domains of imagetransforms, such as Discrete Fourier, Discrete Cosine, Wavelet and alike. Theyare the basic tool in image compression, in image restoration, in imagere-sampling and geometrical transformations and can be traced back to early1970-ths. The paper presents a review of these methods with emphasis on theircomparison and relationships, from the very first steps of transform imagecompression methods to adaptive and local adaptive transform domain filters forimage restoration, to methods of precise image re-sampling and imagereconstruction from sparse samples and up to "compressive sensing" approachthat has gained popularity in last few years. The review has a tutorialcharacter and purpose.
arxiv-1408-6299 | An inexact Newton-Krylov algorithm for constrained diffeomorphic image registration |  http://arxiv.org/abs/1408.6299  | author:Andreas Mang, George Biros category:math.NA cs.CV cs.NA math.OC published:2014-08-27 summary:We propose numerical algorithms for solving large deformation diffeomorphicimage registration problems. We formulate the nonrigid image registrationproblem as a problem of optimal control. This leads to an infinite-dimensionalpartial differential equation (PDE) constrained optimization problem. The PDE constraint consists, in its simplest form, of a hyperbolic transportequation for the evolution of the image intensity. The control variable is thevelocity field. Tikhonov regularization on the control ensures well-posedness.We consider standard smoothness regularization based on $H^1$- or$H^2$-seminorms. We augment this regularization scheme with a constraint on thedivergence of the velocity field rendering the deformation incompressible andthus ensuring that the determinant of the deformation gradient is equal to one,up to the numerical error. We use a Fourier pseudospectral discretization in space and a Chebyshevpseudospectral discretization in time. We use a preconditioned, globalized,matrix-free, inexact Newton-Krylov method for numerical optimization. Aparameter continuation is designed to estimate an optimal regularizationparameter. Regularity is ensured by controlling the geometric properties of thedeformation field. Overall, we arrive at a black-box solver. We study spectralproperties of the Hessian, grid convergence, numerical accuracy, computationalefficiency, and deformation regularity of our scheme. We compare the designedNewton-Krylov methods with a globalized preconditioned gradient descent. Westudy the influence of a varying number of unknowns in time. The reported results demonstrate excellent numerical accuracy, guaranteedlocal deformation regularity, and computational efficiency with an optionalcontrol on local mass conservation. The Newton-Krylov methods clearlyoutperform the Picard method if high accuracy of the inversion is required.
arxiv-1410-0969 | A Model of Plant Identification System Using GLCM, Lacunarity And Shen Features |  http://arxiv.org/abs/1410.0969  | author:Abdul Kadir category:cs.CV published:2014-08-27 summary:Recently, many approaches have been introduced by several researchers toidentify plants. Now, applications of texture, shape, color and vein featuresare common practices. However, there are many possibilities of methods can bedeveloped to improve the performance of such identification systems. Therefore,several experiments had been conducted in this research. As a result, a newnovel approach by using combination of Gray-Level Co-occurrence Matrix,lacunarity and Shen features and a Bayesian classifier gives a better resultcompared to other plant identification systems. For comparison, this researchused two kinds of several datasets that were usually used for testing theperformance of each plant identification system. The results show that thesystem gives an accuracy rate of 97.19% when using the Flavia dataset and95.00% when using the Foliage dataset and outperforms other approaches.
arxiv-1408-6515 | Large Scale Purchase Prediction with Historical User Actions on B2C Online Retail Platform |  http://arxiv.org/abs/1408.6515  | author:Yuyu Zhang, Liang Pang, Lei Shi, Bin Wang category:cs.LG published:2014-08-27 summary:This paper describes the solution of Bazinga Team for Tmall RecommendationPrize 2014. With real-world user action data provided by Tmall, one of thelargest B2C online retail platforms in China, this competition requires topredict future user purchases on Tmall website. Predictions are judged onF1Score, which considers both precision and recall for fair evaluation. Thedata set provided by Tmall contains more than half billion action records fromover ten million distinct users. Such massive data volume poses a bigchallenge, and drives competitors to write every single program in MapReducefashion and run it on distributed cluster. We model the purchase predictionproblem as standard machine learning problem, and mainly employ regression andclassification methods as single models. Individual models are then aggregatedin a two-stage approach, using linear regression for blending, and finally alinear ensemble of blended models. The competition is approaching the end butstill in running during writing this paper. In the end, our team achievesF1Score 6.11 and ranks 7th (out of 7,276 teams in total).
arxiv-1410-0311 | $\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous Update |  http://arxiv.org/abs/1410.0311  | author:Subhadip Mukherjee, Rupam Basu, Chandra Sekhar Seelamantula category:cs.CV cs.LG published:2014-08-26 summary:We develop a dictionary learning algorithm by minimizing the $\ell_1$distortion metric on the data term, which is known to be robust fornon-Gaussian noise contamination. The proposed algorithm exploits the idea ofiterative minimization of weighted $\ell_2$ error. We refer to this algorithmas $\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparsecoefficients are simultaneously updated to minimize the $\ell_1$ objective,resulting in noise-robustness. We demonstrate through experiments that the$\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with theK-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.,both in Gaussian and non-Gaussian noise conditions. We also show that, forfixed values of sparsity, number of dictionary atoms, and data-dimension, the$\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when thetraining set available is small. We apply the proposed algorithm for denoisingnatural images corrupted by additive Gaussian and Laplacian noise. The imagesdenoised using $\ell_1$-K-SVD are observed to have slightly higher peaksignal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but theimprovement in structural similarity index (SSIM) is significant (approximately$0.1$) for lower values of input PSNR, indicating the efficacy of the $\ell_1$metric.
arxiv-1408-6179 | Evaluating Neural Word Representations in Tensor-Based Compositional Settings |  http://arxiv.org/abs/1408.6179  | author:Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Matthew Purver category:cs.CL published:2014-08-26 summary:We provide a comparative study between neural word representations andtraditional vector spaces based on co-occurrence counts, in a number ofcompositional tasks. We use three different semantic spaces and implement seventensor-based compositional models, which we then test (together with simpleradditive and multiplicative approaches) in tasks involving verb disambiguationand sentence similarity. To check their scalability, we additionally evaluatethe spaces using simple compositional methods on larger-scale tasks with lessconstrained language: paraphrase detection and dialogue act tagging. In themore constrained tasks, co-occurrence vectors are competitive, although choiceof compositional method is important; on the larger-scale tasks, they areoutperformed by neural word embeddings, which show robust, stable performanceacross the tasks.
arxiv-1408-6218 | Adaptive Multinomial Matrix Completion |  http://arxiv.org/abs/1408.6218  | author:Olga Klopp, Jean Lafond, Eric Moulines, Joseph Salmon category:math.ST stat.ML stat.TH published:2014-08-26 summary:The task of estimating a matrix given a sample of observed entries is knownas the \emph{matrix completion problem}. Most works on matrix completion havefocused on recovering an unknown real-valued low-rank matrix from a randomsample of its entries. Here, we investigate the case of highly quantizedobservations when the measurements can take only a small number of values.These quantized outputs are generated according to a probability distributionparametrized by the unknown matrix of interest. This model corresponds, forexample, to ratings in recommender systems or labels in multi-classclassification. We consider a general, non-uniform, sampling scheme and givetheoretical guarantees on the performance of a constrained, nuclear normpenalized maximum likelihood estimator. One important advantage of thisestimator is that it does not require knowledge of the rank or an upper boundon the nuclear norm of the unknown matrix and, thus, it is adaptive. We providelower bounds showing that our estimator is minimax optimal. An efficientalgorithm based on lifted coordinate gradient descent is proposed to computethe estimator. A limited Monte-Carlo experiment, using both simulated and realdata is provided to support our claims.
arxiv-1408-6214 | A Methodology for the Diagnostic of Aircraft Engine Based on Indicators Aggregation |  http://arxiv.org/abs/1408.6214  | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2014-08-26 summary:Aircraft engine manufacturers collect large amount of engine related dataduring flights. These data are used to detect anomalies in the engines in orderto help companies optimize their maintenance costs. This article introduces andstudies a generic methodology that allows one to build automatic early signs ofanomaly detection in a way that is understandable by human operators who makethe final maintenance decision. The main idea of the method is to generate avery large number of binary indicators based on parametric anomaly scoresdesigned by experts, complemented by simple aggregations of those scores. Thebest indicators are selected via a classical forward scheme, leading to a muchreduced number of indicators that are tuned to a data set. We illustrate theinterest of the method on simulated data which contain realistic early signs ofanomalies.
arxiv-1408-6027 | Label Distribution Learning |  http://arxiv.org/abs/1408.6027  | author:Xin Geng category:cs.LG published:2014-08-26 summary:Although multi-label learning can deal with many problems with labelambiguity, it does not fit some real applications well where the overalldistribution of the importance of the labels matters. This paper proposes anovel learning paradigm named \emph{label distribution learning} (LDL) for suchkind of applications. The label distribution covers a certain number of labels,representing the degree to which each label describes the instance. LDL is amore general learning framework which includes both single-label andmulti-label learning as its special cases. This paper proposes six working LDLalgorithms in three ways: problem transformation, algorithm adaptation, andspecialized algorithm design. In order to compare the performance of the LDLalgorithms, six representative and diverse evaluation measures are selected viaa clustering analysis, and the first batch of label distribution datasets arecollected and made publicly available. Experimental results on one artificialand fifteen real-world datasets show clear advantages of the specializedalgorithms, which indicates the importance of special design for thecharacteristics of the LDL problem.
arxiv-1408-6181 | Resolving Lexical Ambiguity in Tensor Regression Models of Meaning |  http://arxiv.org/abs/1408.6181  | author:Dimitri Kartsaklis, Nal Kalchbrenner, Mehrnoosh Sadrzadeh category:cs.CL published:2014-08-26 summary:This paper provides a method for improving tensor-based compositionaldistributional models of meaning by the addition of an explicit disambiguationstep prior to composition. In contrast with previous research where thishypothesis has been successfully tested against relatively simple compositionalmodels, in our work we use a robust model trained with linear regression. Theresults we get in two experiments show the superiority of the priordisambiguation method and suggest that the effectiveness of this approach ismodel-independent.
arxiv-1408-6032 | Inference of Cancer Progression Models with Biological Noise |  http://arxiv.org/abs/1408.6032  | author:Ilya Korsunsky, Daniele Ramazzotti, Giulio Caravagna, Bud Mishra category:stat.ML cs.LG q-bio.QM published:2014-08-26 summary:Many applications in translational medicine require the understanding of howdiseases progress through the accumulation of persistent events. SpecializedBayesian networks called monotonic progression networks offer a statisticalframework for modeling this sort of phenomenon. Current machine learning toolsto reconstruct Bayesian networks from data are powerful but not suited toprogression models. We combine the technological advances in machine learningwith a rigorous philosophical theory of causation to produce Polaris, ascalable algorithm for learning progression networks that accounts for causalor biological noise as well as logical relations among genetic events, makingthe resulting models easy to interpret qualitatively. We tested Polaris onsynthetically generated data and showed that it outperforms a widely usedmachine learning algorithm and approaches the performance of the competingspecial-purpose, albeit clairvoyant algorithm that is given a prioriinformation about the model parameters. We also prove that under certain rathermild conditions, Polaris is guaranteed to converge for sufficiently largesample sizes. Finally, we applied Polaris to point mutation and copy numbervariation data in Prostate cancer from The Cancer Genome Atlas (TCGA) and foundthat there are likely three distinct progressions, one major androgen drivenprogression, one major non-androgen driven progression, and one novel minorandrogen driven progression.
arxiv-1408-6257 | Sparse Graph-based Transduction for Image Classification |  http://arxiv.org/abs/1408.6257  | author:Sheng Huang, Dan Yang, Jia Zhou, Luwen Huangfu, Xiaohong Zhang category:cs.CV published:2014-08-26 summary:Motivated by the remarkable successes of Graph-based Transduction (GT) andSparse Representation (SR), we present a novel Classifier named SparseGraph-based Classifier (SGC) for image classification. In SGC, SR is leveragedto measure the correlation (similarity) of each two samples and a graph isconstructed for encoding these correlations. Then the Laplacian eigenmapping isadopted for deriving the graph Laplacian of the graph. Finally, SGC can beobtained by plugging the graph Laplacian into the conventional GT framework. Inthe image classification procedure, SGC utilizes the correlations, which areencoded in the learned graph Laplacian, to infer the labels of unlabeledimages. SGC inherits the merits of both GT and SR. Compared to SR, SGC improvesthe robustness and the discriminating power of GT. Compared to GT, SGCsufficiently exploits the whole data. Therefore it alleviates the undercompletedictionary issue suffered by SR. Four popular image databases are employed forevaluation. The results demonstrate that SGC can achieve a promisingperformance in comparison with the state-of-the-art classifiers, particularlyin the small training sample size case and the noisy sample case.
arxiv-1409-2413 | Image processing |  http://arxiv.org/abs/1409.2413  | author:Franco Rino category:cs.CV published:2014-08-25 summary:Gabor filters can extract multi-orientation and multiscale features from faceimages. Researchers have designed different ways to use the magnitude of thefiltered results for face recognition: Gabor Fisher classifier exploited onlythe magnitude information of Gabor magnitude pictures (GMPs); Local GaborBinary Pattern uses only the gradient information. In this paper, we regardGMPs as smooth surfaces. By completely describing the shape of GMPs, we get aface representation method called Gabor Surface Feature (GSF). First, wecompute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them andtransform them into decimal values. Finally we construct joint histograms anduse subspace methods for classification. Experiments on FERET, ORL and FRGC1.0.4 database show the effectiveness of GSF.
arxiv-1408-5801 | A General Framework for Fast Stagewise Algorithms |  http://arxiv.org/abs/1408.5801  | author:Ryan J. Tibshirani category:stat.ML stat.CO published:2014-08-25 summary:Forward stagewise regression follows a very simple strategy for constructinga sequence of sparse regression estimates: it starts with all coefficientsequal to zero, and iteratively updates the coefficient (by a small amount$\epsilon$) of the variable that achieves the maximal absolute inner productwith the current residual. This procedure has an interesting connection to thelasso: under some conditions, it is known that the sequence of forwardstagewise estimates exactly coincides with the lasso path, as the step size$\epsilon$ goes to zero. Furthermore, essentially the same equivalence holdsoutside of least squares regression, with the minimization of a differentiableconvex loss function subject to an $\ell_1$ norm constraint (the stagewisealgorithm now updates the coefficient corresponding to the maximal absolutecomponent of the gradient). Even when they do not match their $\ell_1$-constrained analogues, stagewiseestimates provide a useful approximation, and are computationally appealing.Their success in sparse modeling motivates the question: can a simple,effective strategy like forward stagewise be applied more broadly in otherregularization settings, beyond the $\ell_1$ norm and sparsity? The currentpaper is an attempt to do just this. We present a general framework forstagewise estimation, which yields fast algorithms for problems such asgroup-structured learning, matrix completion, image denoising, and more.
arxiv-1408-5823 | Improved Distributed Principal Component Analysis |  http://arxiv.org/abs/1408.5823  | author:Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, David Woodruff category:cs.LG published:2014-08-25 summary:We study the distributed computing setting in which there are multipleservers, each holding a set of points, who wish to compute functions on theunion of their point sets. A key task in this setting is Principal ComponentAnalysis (PCA), in which the servers would like to compute a low dimensionalsubspace capturing as much of the variance of the union of their point sets aspossible. Given a procedure for approximate PCA, one can use it toapproximately solve $\ell_2$-error fitting problems such as $k$-meansclustering and subspace clustering. The essential properties of an approximatedistributed PCA algorithm are its communication cost and computationalefficiency for a given desired accuracy in downstream applications. We give newalgorithms and analyses for distributed PCA which lead to improvedcommunication and computational costs for $k$-means clustering and relatedproblems. Our empirical study on real world data shows a speedup of orders ofmagnitude, preserving communication with only a negligible degradation insolution quality. Some of these techniques we develop, such as a generaltransformation from a constant success probability subspace embedding to a highsuccess probability subspace embedding with a dimension and sparsityindependent of the success probability, may be of independent interest.
arxiv-1408-5661 | Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable |  http://arxiv.org/abs/1408.5661  | author:Keisuke Yamazaki category:stat.ML cs.LG published:2014-08-25 summary:In data science and machine learning, hierarchical parametric models, such asmixture models, are often used. They contain two kinds of variables: observablevariables, which represent the parts of the data that can be directly measured,and latent variables, which represent the underlying processes that generatethe data. Although there has been an increase in research on the estimationaccuracy for observable variables, the theoretical analysis of estimatinglatent variables has not been thoroughly investigated. In a previous study, wedetermined the accuracy of a Bayes estimation for the joint probability of thelatent variables in a dataset, and we proved that the Bayes method isasymptotically more accurate than the maximum-likelihood method. However, theaccuracy of the Bayes estimation for a single latent variable remains unknown.In the present paper, we derive the asymptotic expansions of the errorfunctions, which are defined by the Kullback-Leibler divergence, for two typesof single-variable estimations when the statistical regularity is satisfied.Our results indicate that the accuracies of the Bayes and maximum-likelihoodmethods are asymptotically equivalent and clarify that the Bayes method is onlyadvantageous for multivariable estimations.
arxiv-1408-5845 | Analysis of a Reduced-Communication Diffusion LMS Algorithm |  http://arxiv.org/abs/1408.5845  | author:Reza Arablouei, Stefan Werner, Kutluyıl Doğançay, Yih-Fang Huang category:cs.DC cs.LG cs.SY math.OC published:2014-08-25 summary:In diffusion-based algorithms for adaptive distributed estimation, each nodeof an adaptive network estimates a target parameter vector by creating anintermediate estimate and then combining the intermediate estimates availablewithin its closed neighborhood. We analyze the performance of areduced-communication diffusion least mean-square (RC-DLMS) algorithm, whichallows each node to receive the intermediate estimates of only a subset of itsneighbors at each iteration. This algorithm eases the usage of networkcommunication resources and delivers a trade-off between estimation performanceand communication cost. We show analytically that the RC-DLMS algorithm isstable and convergent in both mean and mean-square senses. We also calculateits theoretical steady-state mean-square deviation. Simulation resultsdemonstrate a good match between theory and experiment.
arxiv-1408-5810 | Kernel-based Information Criterion |  http://arxiv.org/abs/1408.5810  | author:Somayeh Danafar, Kenji Fukumizu, Faustino Gomez category:stat.ML published:2014-08-25 summary:This paper introduces Kernel-based Information Criterion (KIC) for modelselection in regression analysis. The novel kernel-based complexity measure inKIC efficiently computes the interdependency between parameters of the modelusing a variable-wise variance and yields selection of better, more robustregressors. Experimental results show superior performance on both simulatedand real data sets compared to Leave-One-Out Cross-Validation (LOOCV),kernel-based Information Complexity (ICOMP), and maximum log of marginallikelihood in Gaussian Process Regression (GPR).
arxiv-1408-5667 | Dependent Nonparametric Bayesian Group Dictionary Learning for online reconstruction of Dynamic MR images |  http://arxiv.org/abs/1408.5667  | author:Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim category:cs.CV published:2014-08-25 summary:In this paper, we introduce a dictionary learning based approach applied tothe problem of real-time reconstruction of MR image sequences that are highlyundersampled in k-space. Unlike traditional dictionary learning, our methodintegrates both global and patch-wise (local) sparsity information andincorporates some priori information into the reconstruction process. Moreover,we use a Dependent Hierarchical Beta-process as the prior for the group-baseddictionary learning, which adaptively infers the dictionary size and thesparsity of each patch; and also ensures that similar patches are manifested interms of similar dictionary atoms. An efficient numerical algorithm based onthe alternating direction method of multipliers (ADMM) is also presented.Through extensive experimental results we show that our proposed methodachieves superior reconstruction quality, compared to the other state-of-the-art DL-based methods.
arxiv-1408-6141 | Recursive Total Least-Squares Algorithm Based on Inverse Power Method and Dichotomous Coordinate-Descent Iterations |  http://arxiv.org/abs/1408.6141  | author:Reza Arablouei, Kutluyıl Doğançay, Stefan Werner category:cs.SY cs.LG published:2014-08-25 summary:We develop a recursive total least-squares (RTLS) algorithm forerrors-in-variables system identification utilizing the inverse power methodand the dichotomous coordinate-descent (DCD) iterations. The proposedalgorithm, called DCD-RTLS, outperforms the previously-proposed RTLSalgorithms, which are based on the line-search method, with reducedcomputational complexity. We perform a comprehensive analysis of the DCD-RTLSalgorithm and show that it is asymptotically unbiased as well as being stablein the mean. We also find a lower bound for the forgetting factor that ensuresmean-square stability of the algorithm and calculate the theoreticalsteady-state mean-square deviation (MSD). We verify the effectiveness of theproposed algorithm and the accuracy of the predicted steady-state MSD viasimulations.
arxiv-1408-5882 | Convolutional Neural Networks for Sentence Classification |  http://arxiv.org/abs/1408.5882  | author:Yoon Kim category:cs.CL cs.NE published:2014-08-25 summary:We report on a series of experiments with convolutional neural networks (CNN)trained on top of pre-trained word vectors for sentence-level classificationtasks. We show that a simple CNN with little hyperparameter tuning and staticvectors achieves excellent results on multiple benchmarks. Learningtask-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow forthe use of both task-specific and static vectors. The CNN models discussedherein improve upon the state of the art on 4 out of 7 tasks, which includesentiment analysis and question classification.
arxiv-1408-5544 | To lie or not to lie in a subspace |  http://arxiv.org/abs/1408.5544  | author:Daniel L. Pimentel-Alarcón category:stat.ML cs.LG published:2014-08-24 summary:Give deterministic necessary and sufficient conditions to guarantee that if asubspace fits certain partially observed data from a union of subspaces, it isbecause such data really lies in a subspace. Furthermore, Give deterministic necessary and sufficient conditions toguarantee that if a subspace fits certain partially observed data, suchsubspace is unique. Do this by characterizing when and only when a set of incomplete vectorsbehaves as a single but complete one.
arxiv-1408-5552 | Fuzzy and entropy facial recognition |  http://arxiv.org/abs/1408.5552  | author:Jaejun Lee, Taeseon Yun category:cs.CV 68T10 published:2014-08-24 summary:This paper suggests an effective method for facial recognition using fuzzytheory and Shannon entropy. Combination of fuzzy theory and Shannon entropyeliminates the complication of other methods. Shannon entropy calculates theratio of an element between faces, and fuzzy theory calculates the member shipof the entropy with 1. More details will be mentioned in Section 3. Thelearning performance is better than others as it is very simple, and only needtwo data per learning. By using factors that don't usually change during thelife, the method will have a high accuracy.
arxiv-1408-5634 | An application of topological graph clustering to protein function prediction |  http://arxiv.org/abs/1408.5634  | author:R. Sean Bowman, Douglas Heisterkamp, Jesse Johnson, Danielle O'Donnol category:cs.CE cs.LG q-bio.QM stat.ML published:2014-08-24 summary:We use a semisupervised learning algorithm based on a topological dataanalysis approach to assign functional categories to yeast proteins usingsimilarity graphs. This new approach to analyzing biological networks yieldsresults that are as good as or better than state of the art existingapproaches.
arxiv-1408-5601 | Learn Convolutional Neural Network for Face Anti-Spoofing |  http://arxiv.org/abs/1408.5601  | author:Jianwei Yang, Zhen Lei, Stan Z. Li category:cs.CV published:2014-08-24 summary:Though having achieved some progresses, the hand-crafted texture features,e.g., LBP [23], LBP-TOP [11] are still unable to capture the mostdiscriminative cues between genuine and fake faces. In this paper, instead ofdesigning feature by ourselves, we rely on the deep convolutional neuralnetwork (CNN) to learn features of high discriminative ability in a supervisedmanner. Combined with some data pre-processing, the face anti-spoofingperformance improves drastically. In the experiments, over 70% relativedecrease of Half Total Error Rate (HTER) is achieved on two challengingdatasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art.Meanwhile, the experimental results from inter-tests between two datasetsindicates CNN can obtain features with better generalization ability. Moreover,the nets trained using combined data from two datasets have less biases betweentwo datasets.
arxiv-1408-5574 | Supervised Hashing Using Graph Cuts and Boosted Decision Trees |  http://arxiv.org/abs/1408.5574  | author:Guosheng Lin, Chunhua Shen, Anton van den Hengel category:cs.LG cs.CV published:2014-08-24 summary:Embedding image features into a binary Hamming space can improve both thespeed and accuracy of large-scale query-by-example image retrieval systems.Supervised hashing aims to map the original features to compact binary codes ina manner which preserves the label-based similarities of the original data.Most existing approaches apply a single form of hash function, and anoptimization process which is typically deeply coupled to this specific form.This tight coupling restricts the flexibility of those methods, and can resultin complex optimization problems that are difficult to solve. In this work weproffer a flexible yet simple framework that is able to accommodate differenttypes of loss functions and hash functions. The proposed framework allows anumber of existing approaches to hashing to be placed in context, andsimplifies the development of new problem-specific hashing methods. Ourframework decomposes the into two steps: binary code (hash bits) learning, andhash function learning. The first step can typically be formulated as a binaryquadratic problem, and the second step can be accomplished by training standardbinary classifiers. For solving large-scale binary code inference, we show howto ensure that the binary quadratic problems are submodular such that anefficient graph cut approach can be used. To achieve efficiency as well asefficacy on large-scale high-dimensional data, we propose to use boosteddecision trees as the hash functions, which are nonlinear, highly descriptive,and very fast to train and evaluate. Experiments demonstrate that our proposedmethod significantly outperforms most state-of-the-art methods, especially onhigh-dimensional data.
arxiv-1408-5449 | Stretchy Polynomial Regression |  http://arxiv.org/abs/1408.5449  | author:Kar-Ann Toh category:cs.LG stat.ML published:2014-08-23 summary:This article proposes a novel solution for stretchy polynomial regressionlearning. The solution comes in primal and dual closed-forms similar to that ofridge regression. Essentially, the proposed solution stretches the covariancecomputation via a power term thereby compresses or amplifies the estimation.Our experiments on both synthetic data and real-world data show effectivenessof the proposed method for compressive learning.
arxiv-1408-5404 | A Wild Bootstrap for Degenerate Kernel Tests |  http://arxiv.org/abs/1408.5404  | author:Kacper Chwialkowski, Dino Sejdinovic, Arthur Gretton category:stat.ML 62G10 published:2014-08-23 summary:A wild bootstrap method for nonparametric hypothesis tests based on kerneldistribution embeddings is proposed. This bootstrap method is used to constructprovably consistent tests that apply to random processes, for which the naivepermutation-based bootstrap fails. It applies to a large group of kernel testsbased on V-statistics, which are degenerate under the null hypothesis, andnon-degenerate elsewhere. To illustrate this approach, we construct atwo-sample test, an instantaneous independence test and a multiple lagindependence test for time series. In experiments, the wild bootstrap givesstrong performance on synthetic examples, on audio data, and in performancebenchmarking for the Gibbs sampler.
arxiv-1408-5516 | Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation |  http://arxiv.org/abs/1408.5516  | author:Sanja Fidler, Marko Boben, Ales Leonardis category:cs.CV published:2014-08-23 summary:Hierarchies allow feature sharing between objects at multiple levels ofrepresentation, can code exponential variability in a very compact way andenable fast inference. This makes them potentially suitable for learning andrecognizing a higher number of object classes. However, the success of thehierarchical approaches so far has been hindered by the use of hand-craftedfeatures or predetermined grouping rules. This paper presents a novel frameworkfor learning a hierarchical compositional shape vocabulary for representingmultiple object classes. The approach takes simple contour fragments and learnstheir frequent spatial configurations. These are recursively combined intoincreasingly more complex and class-specific shape compositions, each exertinga high degree of shape variability. At the top-level of the vocabulary, thecompositions are sufficiently large and complex to represent the whole shapesof the objects. We learn the vocabulary layer after layer, by graduallyincreasing the size of the window of analysis and reducing the spatialresolution at which the shape configurations are learned. The lower layers arelearned jointly on images of all classes, whereas the higher layers of thevocabulary are learned incrementally, by presenting the algorithm with oneobject class after another. The experimental results show that the learnedmulti-class object representation scales favorably with the number of objectclasses and achieves a state-of-the-art detection performance at both, fasterinference as well as shorter training times.
arxiv-1408-5456 | Interpreting Tree Ensembles with inTrees |  http://arxiv.org/abs/1408.5456  | author:Houtao Deng category:cs.LG stat.ML published:2014-08-23 summary:Tree ensembles such as random forests and boosted trees are accurate butdifficult to understand, debug and deploy. In this work, we provide the inTrees(interpretable trees) framework that extracts, measures, prunes and selectsrules from a tree ensemble, and calculates frequent variable interactions. Anrule-based learner, referred to as the simplified tree ensemble learner (STEL),can also be formed and used for future prediction. The inTrees framework canapplied to both classification and regression problems, and is applicable tomany types of tree ensembles, e.g., random forests, regularized random forests,and boosted trees. We implemented the inTrees algorithms in the "inTrees" Rpackage.
arxiv-1408-5389 | Computing Multi-Relational Sufficient Statistics for Large Databases |  http://arxiv.org/abs/1408.5389  | author:Zhensong Qian, Oliver Schulte, Yan Sun category:cs.LG cs.DB H.2.8; H.2.4 published:2014-08-22 summary:Databases contain information about which relationships do and do not holdamong entities. To make this information accessible for statistical analysisrequires computing sufficient statistics that combine information fromdifferent database tables. Such statistics may involve any number of {\empositive and negative} relationships. With a naive enumeration approach,computing sufficient statistics for negative relationships is feasible only forsmall databases. We solve this problem with a new dynamic programming algorithmthat performs a virtual join, where the requisite counts are computed withoutmaterializing join tables. Contingency table algebra is a new extension ofrelational algebra, that facilitates the efficient implementation of thisM\"obius virtual join operation. The M\"obius Join scales to large datasets(over 1M tuples) with complex schemas. Empirical evaluation with sevenbenchmark datasets showed that information about the presence and absence oflinks can be exploited in feature selection, association rule mining, andBayesian network learning.
arxiv-1408-5369 | Statistical and computational trade-offs in estimation of sparse principal components |  http://arxiv.org/abs/1408.5369  | author:Tengyao Wang, Quentin Berthet, Richard J. Samworth category:math.ST stat.ML stat.TH 62H25, 68Q17 published:2014-08-22 summary:In recent years, Sparse Principal Component Analysis has emerged as anextremely popular dimension reduction technique for high-dimensional data. Thetheoretical challenge, in the simplest case, is to estimate the leadingeigenvector of a population covariance matrix under the assumption that thiseigenvector is sparse. An impressive range of estimators have been proposed;some of these are fast to compute, while others are known to achieve theminimax optimal rate over certain Gaussian or subgaussian classes. In thispaper we show that, under a widely-believed assumption from computationalcomplexity theory, there is a fundamental trade-off between statistical andcomputational performance in this problem. More precisely, working with new,larger classes satisfying a Restricted Covariance Concentration condition, weshow that no randomised polynomial time algorithm can achieve the minimaxoptimal rate. On the other hand, we also study a (polynomial time) variant ofthe well-known semidefinite relaxation estimator, and show that it attainsessentially the optimal rate among all randomised polynomial time algorithms.
arxiv-1408-5241 | A two-stage architecture for stock price forecasting by combining SOM and fuzzy-SVM |  http://arxiv.org/abs/1408.5241  | author:Duc-Hien Nguyen, Manh-Thanh Le category:cs.AI cs.LG 68U35 I.2.3 published:2014-08-22 summary:This paper proposed a model to predict the stock price based on combiningSelf-Organizing Map (SOM) and fuzzy-Support Vector Machines (f-SVM). Extractionof fuzzy rules from raw data based on the combining of statistical machinelearning models is base of this proposed approach. In the proposed model, SOMis used as a clustering algorithm to partition the whole input space into theseveral disjoint regions. For each partition, a set of fuzzy rules is extractedbased on a f-SVM combining model. Then fuzzy rules sets are used to predict thetest data using fuzzy inference algorithms. The performance of the proposedapproach is compared with other models using four data sets
arxiv-1408-5246 | Improving the Interpretability of Support Vector Machines-based Fuzzy Rules |  http://arxiv.org/abs/1408.5246  | author:Duc-Hien Nguyen, Manh-Thanh Le category:cs.LG cs.AI 68U35 I.2.3 published:2014-08-22 summary:Support vector machines (SVMs) and fuzzy rule systems are functionallyequivalent under some conditions. Therefore, the learning algorithms developedin the field of support vector machines can be used to adapt the parameters offuzzy systems. Extracting fuzzy models from support vector machines has theinherent advantage that the model does not need to determine the number ofrules in advance. However, after the support vector machine learning, thecomplexity is usually high, and interpretability is also impaired. This papernot only proposes a complete framework for extracting interpretable SVM-basedfuzzy modeling, but also provides optimization issues of the models.Simulations examples are given to embody the idea of this paper.
arxiv-1408-5275 | Unsupervised Spike Sorting Based on Discriminative Subspace Learning |  http://arxiv.org/abs/1408.5275  | author:Mohammad Reza Keshtkaran, Zhi Yang category:cs.CV physics.med-ph published:2014-08-22 summary:Spike sorting is a fundamental preprocessing step for many neurosciencestudies which rely on the analysis of spike trains. In this paper, we presenttwo unsupervised spike sorting algorithms based on discriminative subspacelearning. The first algorithm simultaneously learns the discriminative featuresubspace and performs clustering. It uses histogram of features in the mostdiscriminative projection to detect the number of neurons. The second algorithmperforms hierarchical divisive clustering that learns a discriminative1-dimensional subspace for clustering in each level of the hierarchy untilachieving almost unimodal distribution in the subspace. The algorithms aretested on synthetic and in-vivo data, and are compared against two widely usedspike sorting methods. The comparative results demonstrate that our spikesorting methods can achieve substantially higher accuracy in lower dimensionalfeature space, and they are highly robust to noise. Moreover, they providesignificantly better cluster separability in the learned subspace than in thesubspace obtained by principal component analysis or wavelet transform.
arxiv-1408-5403 | Neural Mechanism of Language |  http://arxiv.org/abs/1408.5403  | author:Peilei Liu, Ting Wang category:cs.NE cs.CL q-bio.NC published:2014-08-22 summary:This paper is based on our previous work on neural coding. It is aself-organized model supported by existing evidences. Firstly, we brieflyintroduce this model in this paper, and then we explain the neural mechanism oflanguage and reasoning with it. Moreover, we find that the position of an areadetermines its importance. Specifically, language relevant areas are in thecapital position of the cortical kingdom. Therefore they are closely relatedwith autonomous consciousness and working memories. In essence, language is aminiature of the real world. Briefly, this paper would like to bridge the gapbetween molecule mechanism of neurons and advanced functions such as languageand reasoning.
arxiv-1408-5352 | Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in Polynomial Time |  http://arxiv.org/abs/1408.5352  | author:Zhaoran Wang, Huanran Lu, Han Liu category:stat.ML cs.LG published:2014-08-22 summary:Sparse principal component analysis (PCA) involves nonconvex optimization forwhich the global solution is hard to obtain. To address this issue, one popularapproach is convex relaxation. However, such an approach may produce suboptimalestimators due to the relaxation effect. To optimally estimate sparse principalsubspaces, we propose a two-stage computational framework named "tighten afterrelax": Within the 'relax' stage, we approximately solve a convex relaxation ofsparse PCA with early stopping to obtain a desired initial estimator; For the'tighten' stage, we propose a novel algorithm called sparse orthogonaliteration pursuit (SOAP), which iteratively refines the initial estimator bydirectly solving the underlying nonconvex problem. A key concept of thistwo-stage framework is the basin of attraction. It represents a local regionwithin which the `tighten' stage has desired computational and statisticalguarantees. We prove that, the initial estimator obtained from the 'relax'stage falls into such a region, and hence SOAP geometrically converges to aprincipal subspace estimator which is minimax-optimal within a certain modelclass. Unlike most existing sparse PCA estimators, our approach applies to thenon-spiked covariance models, and adapts to non-Gaussianity as well asdependent data settings. Moreover, through analyzing the computationalcomplexity of the two stages, we illustrate an interesting phenomenon thatlarger sample size can reduce the total iteration complexity. Our frameworkmotivates a general paradigm for solving many complex statistical problemswhich involve nonconvex optimization with provable guarantees.
arxiv-1408-5350 | Structural bias in population-based algorithms |  http://arxiv.org/abs/1408.5350  | author:Anna V. Kononova, David W. Corne, Philippe De Wilde, Vsevolod Shneer, Fabio Caraffini category:cs.NE published:2014-08-22 summary:Challenging optimisation problems are abundant in all areas of science. Sincethe 1950s, scientists have developed ever-diversifying families of black boxoptimisation algorithms designed to address any optimisation problem, requiringonly that quality of a candidate solution is calculated via a fitness functionspecific to the problem. For such algorithms to be successful, at least threeproperties are required: an effective informed sampling strategy, that guidesgeneration of new candidates on the basis of fitnesses and locations ofpreviously visited candidates; mechanisms to ensure efficiency, so that samecandidates are not repeatedly visited; absence of structural bias, which, ifpresent, would predispose the algorithm towards limiting its search to someregions of solution space. The first two of these properties have beenextensively investigated, however the third is little understood. In thisarticle we provide theoretical and empirical analyses that contribute to theunderstanding of structural bias. We prove a theorem concerning dynamics ofpopulation variance in the case of real-valued search spaces. This reveals howstructural bias can manifest as non-uniform clustering of population over time.Theory predicts that structural bias is exacerbated with increasing populationsize and problem difficulty. These predictions reveal two previouslyunrecognised aspects of structural bias. Respectively, increasing populationsize, though ostensibly promoting diversity, will magnify any inherentstructural bias, and effects of structural bias are more apparent when facedwith difficult problems. Our theoretical result also suggests that two commonlyused approaches to enhancing exploration, increasing population size andincreasing disruptiveness of search operators, have quite distinct implicationsin terms of structural bias.
arxiv-1408-5316 | Cuckoo Search: Recent Advances and Applications |  http://arxiv.org/abs/1408.5316  | author:Xin-She Yang, Suash Deb category:math.OC cs.NE nlin.AO 90-XX published:2014-08-22 summary:Cuckoo search (CS) is a relatively new algorithm, developed by Yang and Debin 2009, and CS is efficient in solving global optimization problems. In thispaper, we review the fundamental ideas of cuckoo search and the latestdevelopments as well as its applications. We analyze the algorithm and gaininsight into its search mechanisms and find out why it is efficient. We alsodiscuss the essence of algorithms and its link to self-organizing systems, andfinally we propose some important topics for further research.
arxiv-1408-5320 | Applications and Analysis of Bio-Inspired Eagle Strategy for Engineering Optimization |  http://arxiv.org/abs/1408.5320  | author:Xin-She Yang, M. Karamanoglu, T. O. Ting, Y. X. Zhao category:math.OC cs.NE nlin.AO 90C26 published:2014-08-22 summary:All swarm-intelligence-based optimization algorithms use some stochasticcomponents to increase the diversity of solutions during the search process.Such randomization is often represented in terms of random walks. However, itis not yet clear why some randomization techniques (and thus why somealgorithms) may perform better than others for a given set of problems. In thiswork, we analyze these randomization methods in the context of nature-inspiredalgorithms. We also use eagle strategy to provide basic observations and relatestep sizes and search efficiency using Markov theory. Then, we apply ouranalysis and observations to solve four design benchmarks, including thedesigns of a pressure vessel, a speed reducer, a PID controller and a heatexchanger. Our results demonstrate that eagle strategy with L\'evy flights canperform extremely well in reducing the overall computational efforts.
arxiv-1408-5332 | Flower Pollination Algorithm: A Novel Approach for Multiobjective Optimization |  http://arxiv.org/abs/1408.5332  | author:Xin-She Yang, M. Karamanoglu, X. S. He category:math.OC cs.NE nlin.AO 90C26 published:2014-08-22 summary:Multiobjective design optimization problems require multiobjectiveoptimization techniques to solve, and it is often very challenging to obtainhigh-quality Pareto fronts accurately. In this paper, the recently developedflower pollination algorithm (FPA) is extended to solve multiobjectiveoptimization problems. The proposed method is used to solve a set ofmultobjective test functions and two bi-objective design benchmarks, and acomparison of the proposed algorithm with other algorithms has been made, whichshows that FPA is efficient with a good convergence rate. Finally, theimportance for further parametric studies and theoretical analysis arehighlighted and discussed.
arxiv-1408-5286 | Designing labeled graph classifiers by exploiting the Rényi entropy of the dissimilarity representation |  http://arxiv.org/abs/1408.5286  | author:Lorenzo Livi category:cs.CV cs.IT math.IT stat.ML published:2014-08-22 summary:Representing patterns by complex relational structures, such as labeledgraphs, is becoming an increasingly common practice in the broad field ofcomputational intelligence. Accordingly, a wide repertoire of patternrecognition tools, such as classifiers and knowledge discovery procedures, arenowadays available and tested for various labeled graph data types. However,the design of effective learning and mining procedures operating in the spaceof labeled graphs is still a challenging problem, especially from thecomputational complexity viewpoint. In this paper, we present a majorimprovement of a general-purpose graph classification system, which isconceived on an interplay among dissimilarity representation, clustering,information-theoretic techniques, and evolutionary optimization. Theimprovement focuses on a specific key subroutine of the system that performsthe compression of the input data. We prove different theorems which arefundamental to the setting of such a compression operation. We demonstrate theeffectiveness of the resulting classifier by benchmarking the developedvariants on well-known datasets of labeled graphs, considering as distinctperformance indicators the classification accuracy, the computing time, and theparsimony in terms of structural complexity of the synthesized classificationmodel. Overall, the results show state-of-the-art standards in terms of testset accuracy, while achieving considerable reductions for what concerns boththe effective computing time and model complexity.
arxiv-1408-5343 | Cuckoo Search: A Brief Literature Review |  http://arxiv.org/abs/1408.5343  | author:I. Fister Jr., X. S. Yang, D. Fister, I. Fister category:math.OC cs.NE nlin.AO 90C26 published:2014-08-22 summary:Cuckoo search (CS) was introduced in 2009, and it has attracted greatattention due to its promising efficiency in solving many optimization problemsand real-world applications. In the last few years, many papers have beenpublished regarding cuckoo search, and the relevant literature has expandedsignificantly. This chapter summarizes briefly the majority of the literatureabout cuckoo search in peer-reviewed journals and conferences found so far.These references can be systematically classified into appropriate categories,which can be used as a basis for further research.
arxiv-1408-5348 | Bat Algorithm is Better Than Intermittent Search Strategy |  http://arxiv.org/abs/1408.5348  | author:Xin-She Yang, Suash Deb, Simon Fong category:math.OC cs.NE published:2014-08-22 summary:The efficiency of any metaheuristic algorithm largely depends on the way ofbalancing local intensive exploitation and global diverse exploration. Studiesshow that bat algorithm can provide a good balance between these two keycomponents with superior efficiency. In this paper, we first review somecommonly used metaheuristic algorithms, and then compare the performance of batalgorithm with the so-called intermittent search strategy. From simulations, wefound that bat algorithm is better than the optimal intermittent searchstrategy. We also analyse the comparison results and their implications forhigher dimensional optimization problems. In addition, we also apply batalgorithm in solving business optimization and engineering design problems.
arxiv-1408-5400 | Hierarchical Adaptive Structural SVM for Domain Adaptation |  http://arxiv.org/abs/1408.5400  | author:Jiaolong Xu, Sebastian Ramos, David Vazquez, Antonio M. Lopez category:cs.CV cs.LG published:2014-08-22 summary:A key topic in classification is the accuracy loss produced when the datadistribution in the training (source) domain differs from that in the testing(target) domain. This is being recognized as a very relevant problem for manycomputer vision tasks such as image classification, object detection, andobject category recognition. In this paper, we present a novel domainadaptation method that leverages multiple target domains (or sub-domains) in ahierarchical adaptation tree. The core idea is to exploit the commonalities anddifferences of the jointly considered target domains. Given the relevance of structural SVM (SSVM) classifiers, we apply our ideato the adaptive SSVM (A-SSVM), which only requires the target domain samplestogether with the existing source-domain classifier for performing the desiredadaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM). As proof of concept we use HA-SSVM for pedestrian detection and objectcategory recognition. In the former we apply HA-SSVM to the deformablepart-based model (DPM) while in the latter HA-SSVM is applied to multi-categoryclassifiers. In both cases, we show how HA-SSVM is effective in increasing thedetection/recognition accuracy with respect to adaptation strategies thatignore the structure of the target data. Since, the sub-domains of the targetdata are not always known a priori, we shown how HA-SSVM can incorporatesub-domain structure discovery for object category recognition.
arxiv-1408-5405 | Recurrent Neural Network Based Hybrid Model of Gene Regulatory Network |  http://arxiv.org/abs/1408.5405  | author:Khalid Raza, Mansaf Alam category:cs.NE cs.CE q-bio.MN published:2014-08-22 summary:Systems biology is an emerging interdisciplinary area of research thatfocuses on study of complex interactions in a biological system, such as generegulatory networks. The discovery of gene regulatory networks leads to a widerange of applications, such as pathways related to a disease that can unveil inwhat way the disease acts and provide novel tentative drug targets. Inaddition, the development of biological models from discovered networks orpathways can help to predict the responses to disease and can be much usefulfor the novel drug development and treatments. The inference of regulatorynetworks from biological data is still in its infancy stage. This paperproposes a recurrent neural network (RNN) based gene regulatory network (GRN)model hybridized with generalized extended Kalman filter for weight update inbackpropagation through time training algorithm. The RNN is a complex neuralnetwork that gives a better settlement between the biological closeness andmathematical flexibility to model GRN. The RNN is able to capture complex,non-linear and dynamic relationship among variables. Gene expression data areinherently noisy and Kalman filter performs well for estimation even in noisydata. Hence, non-linear version of Kalman filter, i.e., generalized extendedKalman filter has been applied for weight update during network training. Thedeveloped model has been applied on DNA SOS repair network, IRMA network, andtwo synthetic networks from DREAM Challenge. We compared our results with otherstate-of-the-art techniques that show superiority of our model. Further, 5%Gaussian noise has been added in the dataset and result of the proposed modelshows negligible effect of noise on the results.
arxiv-1408-5099 | Uniform Sampling for Matrix Approximation |  http://arxiv.org/abs/1408.5099  | author:Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, Aaron Sidford category:cs.DS cs.LG stat.ML published:2014-08-21 summary:Random sampling has become a critical tool in solving massive matrixproblems. For linear regression, a small, manageable set of data rows can berandomly selected to approximate a tall, skinny data matrix, improvingprocessing time significantly. For theoretical performance guarantees, each rowmust be sampled with probability proportional to its statistical leveragescore. Unfortunately, leverage scores are difficult to compute. A simple alternative is to sample rows uniformly at random. While this oftenworks, uniform sampling will eliminate critical row information for manynatural instances. We take a fresh look at uniform sampling by examining whatinformation it does preserve. Specifically, we show that uniform samplingyields a matrix that, in some sense, well approximates a large fraction of theoriginal. While this weak form of approximation is not enough for solvinglinear regression directly, it is enough to compute a better approximation. This observation leads to simple iterative row sampling algorithms for matrixapproximation that run in input-sparsity time and preserve row structure andsparsity at all intermediate steps. In addition to an improved understanding ofuniform sampling, our main proof introduces a structural result of independentinterest: we show that every matrix can be made to have low coherence byreweighting a small subset of its rows.
arxiv-1408-5427 | A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets |  http://arxiv.org/abs/1408.5427  | author:Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek category:stat.ML cs.CL cs.IR cs.LG published:2014-08-21 summary:Cluster analysis is a field of data analysis that extracts underlyingpatterns in data. One application of cluster analysis is in text-mining, theanalysis of large collections of text to find similarities between documents.We used a collection of about 30,000 tweets extracted from Twitter just beforethe World Cup started. A common problem with real world text data is thepresence of linguistic noise. In our case it would be extraneous tweets thatare unrelated to dominant themes. To combat this problem, we created analgorithm that combined the DBSCAN algorithm and a consensus matrix. This waywe are left with the tweets that are related to those dominant themes. We thenused cluster analysis to find those topics that the tweets describe. Weclustered the tweets using k-means, a commonly used clustering algorithm, andNon-Negative Matrix Factorization (NMF) and compared the results. The twoalgorithms gave similar results, but NMF proved to be faster and provided moreeasily interpreted results. We explored our results using two visualizationtools, Gephi and Wordle.
arxiv-1408-4792 | Enhanced Estimation of Autoregressive Wind Power Prediction Model Using Constriction Factor Particle Swarm Optimization |  http://arxiv.org/abs/1408.4792  | author:Adnan Anwar, Abdun Naser Mahmood category:cs.CE cs.NE published:2014-08-21 summary:Accurate forecasting is important for cost-effective and efficient monitoringand control of the renewable energy based power generation. Wind based power isone of the most difficult energy to predict accurately, due to the widelyvarying and unpredictable nature of wind energy. Although Autoregressive (AR)techniques have been widely used to create wind power models, they have shownlimited accuracy in forecasting, as well as difficulty in determining thecorrect parameters for an optimized AR model. In this paper, ConstrictionFactor Particle Swarm Optimization (CF-PSO) is employed to optimally determinethe parameters of an Autoregressive (AR) model for accurate prediction of thewind power output behaviour. Appropriate lag order of the proposed model isselected based on Akaike information criterion. The performance of the proposedPSO based AR model is compared with four well-established approaches;Forward-backward approach, Geometric lattice approach, Least-squares approachand Yule-Walker approach, that are widely used for error minimization of the ARmodel. To validate the proposed approach, real-life wind power data of\textit{Capital Wind Farm} was obtained from Australian Energy Market Operator.Experimental evaluation based on a number of different datasets demonstratethat the performance of the AR model is significantly improved compared withbenchmark methods.
arxiv-1408-5032 | On the Sample Complexity of Subspace Learning |  http://arxiv.org/abs/1408.5032  | author:Alessandro Rudi, Guille D. Canas, Lorenzo Rosasco category:stat.ML published:2014-08-21 summary:A large number of algorithms in machine learning, from principal componentanalysis (PCA), and its non-linear (kernel) extensions, to more recent spectralembedding and support estimation methods, rely on estimating a linear subspacefrom samples. In this paper we introduce a general formulation of this problemand derive novel learning error estimates. Our results rely on naturalassumptions on the spectral properties of the covariance operator associated tothe data distribu- tion, and hold for a wide class of metrics betweensubspaces. As special cases, we discuss sharp error estimates for thereconstruction properties of PCA and spectral support estimation. Key to ouranalysis is an operator theoretic approach that has broad applicability tospectral learning methods.
arxiv-1408-4966 | Diffusion Fingerprints |  http://arxiv.org/abs/1408.4966  | author:Jimmy Dubuisson, Jean-Pierre Eckmann, Andrea Agazzi category:stat.ML cs.IR cs.LG published:2014-08-21 summary:We introduce, test and discuss a method for classifying and clustering datamodeled as directed graphs. The idea is to start diffusion processes from anysubset of a data collection, generating corresponding distributions forreaching points in the network. These distributions take the form ofhigh-dimensional numerical vectors and capture essential topological propertiesof the original dataset. We show how these diffusion vectors can besuccessfully applied for getting state-of-the-art accuracies in the problem ofextracting pathways from metabolic networks. We also provide a guideline toillustrate how to use our method for classification problems, and discussimportant details of its implementation. In particular, we present a simpledimensionality reduction technique that lowers the computational cost ofclassifying diffusion vectors, while leaving the predictive power of theclassification process substantially unaltered. Although the method has veryfew parameters, the results we obtain show its flexibility and power. Thisshould make it helpful in many other contexts.
arxiv-1408-4908 | Theoretical Foundations of Equitability and the Maximal Information Coefficient |  http://arxiv.org/abs/1408.4908  | author:Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael Mitzenmacher category:stat.ME cs.IT math.IT math.ST q-bio.QM stat.ML stat.TH published:2014-08-21 summary:The maximal information coefficient (MIC) is a tool for finding the strongestpairwise relationships in a data set with many variables (Reshef et al., 2011).MIC is useful because it gives similar scores to equally noisy relationships ofdifferent types. This property, called {\em equitability}, is important foranalyzing high-dimensional data sets. Here we formalize the theory behind both equitability and MIC in the languageof estimation theory. This formalization has a number of advantages. First, itallows us to show that equitability is a generalization of power againststatistical independence. Second, it allows us to compute and discuss thepopulation value of MIC, which we call MIC_*. In doing so we generalize andstrengthen the mathematical results proven in Reshef et al. (2011) and clarifythe relationship between MIC and mutual information. Introducing MIC_* alsoenables us to reason about the properties of MIC more abstractly: for instance,we show that MIC_* is continuous and that there is a sense in which it is acanonical "smoothing" of mutual information. We also prove an alternate,equivalent characterization of MIC_* that we use to state new estimators of itas well as an algorithm for explicitly computing it when the joint probabilitydensity function of a pair of random variables is known. Our hope is that thispaper provides a richer theoretical foundation for MIC and equitability goingforward. This paper will be accompanied by a forthcoming companion paper that performsextensive empirical analysis and comparison to other methods and discusses thepractical aspects of both equitability and the use of MIC and its relatedstatistics.
arxiv-1408-4849 | Swarm Intelligence Based Multi-phase OPF For Peak Power Loss Reduction In A Smart Grid |  http://arxiv.org/abs/1408.4849  | author:Adnan Anwar, A. N. Mahmood category:cs.CE cs.NE published:2014-08-21 summary:Recently there has been increasing interest in improving smart gridsefficiency using computational intelligence. A key challenge in future smartgrid is designing Optimal Power Flow tool to solve important planning problemsincluding optimal DG capacities. Although, a number of OPF tools exists forbalanced networks there is a lack of research for unbalanced multi-phasedistribution networks. In this paper, a new OPF technique has been proposed forthe DG capacity planning of a smart grid. During the formulation of theproposed algorithm, multi-phase power distribution system is considered whichhas unbalanced loadings, voltage control and reactive power compensationdevices. The proposed algorithm is built upon a co-simulation framework thatoptimizes the objective by adapting a constriction factor Particle Swarmoptimization. The proposed multi-phase OPF technique is validated using IEEE8500-node benchmark distribution system.
arxiv-1408-4660 | Joint Hierarchical Gaussian Process Model with Application to Forecast in Medical Monitoring |  http://arxiv.org/abs/1408.4660  | author:Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak category:stat.ME stat.ML published:2014-08-20 summary:A novel extrapolation method is proposed for longitudinal forecasting. Ahierarchical Gaussian process model is used to combine nonlinear populationchange and individual memory of the past to make prediction. The predictionerror is minimized through the hierarchical design. The method is furtherextended to joint modeling of continuous measurements and survival events. Thebaseline hazard, covariate and joint effects are conveniently modeled in thishierarchical structure. The estimation and inference are implemented in fullyBayesian framework using the objective and shrinkage priors. In simulationstudies, this model shows robustness in latent estimation, correlationdetection and high accuracy in forecasting. The model is illustrated withmedical monitoring data from cystic fibrosis (CF) patients. Estimation andforecasts are obtained in the measurement of lung function and records of acuterespiratory events. Keyword: Extrapolation, Joint Model, Longitudinal Model, HierarchicalGaussian Process, Cystic Fibrosis, Medical Monitoring
arxiv-1408-4673 | Horn functions and the AFP Algorithm |  http://arxiv.org/abs/1408.4673  | author:Rooholah Majdodin category:cs.LG 68Q32, 68T27 published:2014-08-20 summary:It is described why multiple refinements with each negative counterexampledoes not improve the complexity of the AFP Algorithm. Also Canonical normalformulas for Horn functions are discussed.
arxiv-1408-4753 | Be Careful When Assuming the Obvious: Commentary on "The placement of the head that minimizes online memory: a complex systems approach" |  http://arxiv.org/abs/1408.4753  | author:Phillip M. Alday category:cs.CL published:2014-08-20 summary:Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronicand diachronic nature of word order based on the assumption that memory costsare a never decreasing function of distance and a few very general linguisticassumptions. However, even these minimal and seemingly obvious assumptions arenot as safe as they appear in light of recent typological and psycholinguisticevidence. The interaction of word order and memory has further depths to beexplored.
arxiv-1408-4721 | Code Generation for High-Level Synthesis of Multiresolution Applications on FPGAs |  http://arxiv.org/abs/1408.4721  | author:Moritz Schmid, Oliver Reiche, Christian Schmitt, Frank Hannig, Jürgen Teich category:cs.CV cs.DC cs.PL published:2014-08-20 summary:Multiresolution Analysis (MRA) is a mathematical method that is based onworking on a problem at different scales. One of its applications is medicalimaging where processing at multiple scales, based on the concept of Gaussianand Laplacian image pyramids, is a well-known technique. It is often applied toreduce noise while preserving image detail on different levels of granularitywithout modifying the filter kernel. In scientific computing, multigrid methodsare a popular choice, as they are asymptotically optimal solvers for ellipticPartial Differential Equations (PDEs). As such algorithms have a very highcomputational complexity that would overwhelm CPUs in the presence of real-timeconstraints, application-specific processors come into consideration forimplementation. Despite of huge advancements in leveraging productivity in therespective fields, designers are still required to have detailed knowledgeabout coding techniques and the targeted architecture to achieve efficientsolutions. Recently, the HIPAcc framework was proposed as a means for automaticcode generation of image processing algorithms, based on a Domain-SpecificLanguage (DSL). From the same code base, it is possible to generate code forefficient implementations on several accelerator technologies includingdifferent types of Graphics Processing Units (GPUs) as well as reconfigurablelogic (FPGAs). In this work, we demonstrate the ability of HIPAcc to generatecode for the implementation of multiresolution applications on FPGAs andembedded GPUs.
arxiv-1408-4712 | Bi-l0-l2-Norm Regularization for Blind Motion Deblurring |  http://arxiv.org/abs/1408.4712  | author:Wen-Ze Shao, Hai-Bo Li, Michael Elad category:cs.CV published:2014-08-20 summary:In blind motion deblurring, leading methods today tend towards highlynon-convex approximations of the l0-norm, especially in the imageregularization term. In this paper, we propose a simple, effective and fastapproach for the estimation of the motion blur-kernel, through a bi-l0-l2-normregularization imposed on both the intermediate sharp image and theblur-kernel. Compared with existing methods, the proposed regularization isshown to be more effective and robust, leading to a more accurate motionblur-kernel and a better final restored image. A fast numerical scheme isdeployed for alternatingly computing the sharp image and the blur-kernel, bycoupling the operator splitting and augmented Lagrangian methods. Experimentalresults on both a benchmark image dataset and real-world motion blurred imagesshow that the proposed approach is highly competitive with state-of-the- artmethods in both deblurring effectiveness and computational efficiency.
arxiv-1408-4714 | Conic Multi-Task Classification |  http://arxiv.org/abs/1408.4714  | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2014-08-20 summary:Traditionally, Multi-task Learning (MTL) models optimize the average oftask-related objective functions, which is an intuitive approach and which wewill be referring to as Average MTL. However, a more general framework,referred to as Conic MTL, can be formulated by considering conic combinationsof the objective functions instead; in this framework, Average MTL arises as aspecial case, when all combination coefficients equal 1. Although the advantageof Conic MTL over Average MTL has been shown experimentally in previous works,no theoretical justification has been provided to date. In this paper, wederive a generalization bound for the Conic MTL method, and demonstrate thatthe tightest bound is not necessarily achieved, when all combinationcoefficients equal 1; hence, Average MTL may not always be the optimal choice,and it is important to consider Conic MTL. As a byproduct of the generalizationbound, it also theoretically explains the good experimental results of previousrelevant works. Finally, we propose a new Conic MTL model, whose coniccombination coefficients minimize the generalization bound, instead of choosingthem heuristically as has been done in previous methods. The rationale andadvantage of our model is demonstrated and verified via a series of experimentsby comparing with several other methods.
arxiv-1408-4703 | GIMP and Wavelets for Medical Image Processing: Enhancing Images of the Fundus of the Eye |  http://arxiv.org/abs/1408.4703  | author:Amelia Carolina Sparavigna category:cs.CV published:2014-08-20 summary:The visual analysis of retina and of its vascular characteristics isimportant in the diagnosis and monitoring of diseases of visual perception. Inthe related medical diagnoses, the digital processing of the fundus images isused to obtain the segmentation of retinal vessels. However, an imagesegmentation is often requiring methods based on peculiar or complexalgorithms: in this paper we will show some alternative approaches obtained byapplying freely available tools to enhance, without a specific segmentation,the images of the fundus of the eye. We will see in particular, that combiningthe use of GIMP, the GNU Image Manipulation Program, with the wavelet filter ofIris, a program well-known for processing astronomical images, the result isgiving images which can be alternative of those obtained from segmentation.
arxiv-1408-4587 | EURETILE D7.3 - Dynamic DAL benchmark coding, measurements on MPI version of DPSNN-STDP (distributed plastic spiking neural net) and improvements to other DAL codes |  http://arxiv.org/abs/1408.4587  | author:Pier Stanislao Paolucci, Iuliana Bacivarov, Devendra Rai, Lars Schor, Lothar Thiele, Hoeseok Yang, Elena Pastorelli, Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Francesca Lo Cicero, Alessandro Lonardo, Francesco Simula, Laura Tosoratto, Piero Vicini category:cs.DC cs.CE cs.MS cs.NE q-bio.NC published:2014-08-20 summary:The EURETILE project required the selection and coding of a set of dedicatedbenchmarks. The project is about the software and hardware architecture offuture many-tile distributed fault-tolerant systems. We focus on dynamicworkloads characterised by heavy numerical processing requirements. Theambition is to identify common techniques that could be applied to both theEmbedded Systems and HPC domains. This document is the first public deliverableof Work Package 7: Challenging Tiled Applications.
arxiv-1408-4692 | Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods |  http://arxiv.org/abs/1408.4692  | author:Alexander Freytag, Johannes Rühle, Paul Bodesheim, Erik Rodner, Joachim Denzler category:cs.CV published:2014-08-20 summary:Vector-quantized local features frequently used in bag-of-visual-wordsapproaches are the backbone of popular visual recognition systems due to boththeir simplicity and their performance. Despite their success,bag-of-words-histograms basically contain low-level image statistics (e.g.,number of edges of different orientations). The question remains how muchvisual information is "lost in quantization" when mapping visual features tocode words? To answer this question, we present an in-depth analysis of theeffect of local feature quantization on human recognition performance. Ouranalysis is based on recovering the visual information by inverting quantizedlocal features and presenting these visualizations with different codebooksizes to human observers. Although feature inversion techniques are around forquite a while, to the best of our knowledge, our technique is the firstvisualizing especially the effect of feature quantization. Thereby, we are nowable to compare single steps in common image classification pipelines to humancounterparts.
arxiv-1408-4504 | Unsupervised Parallel Extraction based Texture for Efficient Image Representation |  http://arxiv.org/abs/1408.4504  | author:Mohammed M. Abdelsamea category:cs.CV published:2014-08-20 summary:SOM is a type of unsupervised learning where the goal is to discover someunderlying structure of the data. In this paper, a new extraction method basedon the main idea of Concurrent Self-Organizing Maps (CSOM), representing awinner-takes-all collection of small SOM networks is proposed. Each SOM of thesystem is trained individually to provide best results for one class only. Theexperiments confirm that the proposed features based CSOM is capable torepresent image content better than extracted features based on a single bigSOM and these proposed features improve the final decision of the CAD.Experiments held on Mammographic Image Analysis Society (MIAS) dataset.
arxiv-1408-4576 | Introduction to Clustering Algorithms and Applications |  http://arxiv.org/abs/1408.4576  | author:Sibei Yang, Liangde Tao, Bingchen Gong category:cs.LG cs.CV published:2014-08-20 summary:Data clustering is the process of identifying natural groupings or clusterswithin multidimensional data based on some similarity measure. Clustering is afundamental process in many different disciplines. Hence, researchers fromdifferent fields are actively working on the clustering problem. This paperprovides an overview of the different representative clustering methods. Inaddition, application of clustering in different field is briefly introduced.
arxiv-1408-4622 | A new integral loss function for Bayesian optimization |  http://arxiv.org/abs/1408.4622  | author:Emmanuel Vazquez, Julien Bect category:stat.CO cs.LG math.OC stat.ML published:2014-08-20 summary:We consider the problem of maximizing a real-valued continuous function $f$using a Bayesian approach. Since the early work of Jonas Mockus and Antanas\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated byconsidering the loss function $\max f - M_n$ (where $M_n$ denotes the bestfunction value observed after $n$ evaluations of $f$). This loss function putsemphasis on the value of the maximum, at the expense of the location of themaximizer. In the special case of a one-step Bayes-optimal strategy, it leadsto the classical Expected Improvement (EI) sampling criterion. This is aspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the riskassociated to a certain uncertainty measure (here, the expected loss) on thequantity of interest is minimized at each step of the algorithm. In thisarticle, assuming that $f$ is defined over a measure space $(\mathbb{X},\lambda)$, we propose to consider instead the integral loss function$\int_{\mathbb{X}} (f - M_n)_{+}\, d\lambda$, and we show that this leads, inthe case of a Gaussian process prior, to a new numerically tractable samplingcriterion that we call $\rm EI^2$ (for Expected Integrated ExpectedImprovement). A numerical experiment illustrates that a SUR strategy based onthis new sampling criterion reduces the error on both the value and thelocation of the maximizer faster than the EI-based strategy.
arxiv-1408-4551 | Dimensionality Reduction of Affine Variational Inequalities Using Random Projections |  http://arxiv.org/abs/1408.4551  | author:Bharat Prabhakar, Ankur A. Kulkarni category:math.OC cs.LG cs.SY published:2014-08-20 summary:We present a method for dimensionality reduction of an affine variationalinequality (AVI) defined over a compact feasible region. Centered around theJohnson Lindenstrauss lemma, our method is a randomized algorithm that produceswith high probability an approximate solution for the given AVI by solving alower-dimensional AVI. The algorithm allows the lower dimension to be chosenbased on the quality of approximation desired. The algorithm can also be usedas a subroutine in an exact algorithm for generating an initial point close tothe solution. The lower-dimensional AVI is obtained by appropriately projectingthe original AVI on a randomly chosen subspace. The lower-dimensional AVI issolved using standard solvers and from this solution an approximate solution tothe original AVI is recovered through an inexpensive process. Our numericalexperiments corroborate the theoretical results and validate that the algorithmprovides a good approximation at low dimensions and substantial savings in timefor an exact solution.
arxiv-1408-4325 | What makes an Image Iconic? A Fine-Grained Case Study |  http://arxiv.org/abs/1408.4325  | author:Yangmuzi Zhang, Diane Larlus, Florent Perronnin category:cs.CV published:2014-08-19 summary:A natural approach to teaching a visual concept, e.g. a bird species, is toshow relevant images. However, not all relevant images represent a conceptequally well. In other words, they are not necessarily iconic. This observationraises three questions. Is iconicity a subjective property? If not, can wepredict iconicity? And what exactly makes an image iconic? We provide answersto these questions through an extensive experimental study on a challengingfine-grained dataset of birds. We first show that iconicity ratings areconsistent across individuals, even when they are not domain experts, thusdemonstrating that iconicity is not purely subjective. We then consider anexhaustive list of properties that are intuitively related to iconicity andmeasure their correlation with these iconicity ratings. We combine them topredict iconicity of new unseen images. We also propose a direct iconicitypredictor that is discriminatively trained with iconicity ratings. By combiningboth systems, we get an iconicity prediction that approaches human performance.
arxiv-1408-4245 | Towards crowdsourcing and cooperation in linguistic resources |  http://arxiv.org/abs/1408.4245  | author:Dmitry Ustalov category:cs.SI cs.CL K.4.3 published:2014-08-19 summary:Linguistic resources can be populated with data through the use of suchapproaches as crowdsourcing and gamification when motivated people areinvolved. However, current crowdsourcing genre taxonomies lack the concept ofcooperation, which is the principal element of modern video games and maypotentially drive the annotators' interest. This survey on crowdsourcingtaxonomies and cooperation in linguistic resources provides recommendations onusing cooperation in existent genres of crowdsourcing and an evidence of theefficiency of cooperation using a popular Russian linguistic resource createdthrough crowdsourcing as an example.
arxiv-1408-4222 | Can Artificial Neural Networks be Applied in Seismic Predicition? Preliminary Analysis Applying Radial Topology. Case: Mexico |  http://arxiv.org/abs/1408.4222  | author:Cinthya Mota-Hernandez, Luis Esquivel-Rodriguez, Rafael Alvarado-Corona category:cs.NE physics.geo-ph published:2014-08-19 summary:Tectonic earthquakes of high magnitude can cause considerable losses in termsof human lives, economic and infrastructure, among others. According to anevaluation published by the U.S. Geological Survey, 30 is the number ofearthquakes which have greatly impacted Mexico from the end of the XIX centuryto this one. Based upon data from the National Seismological Service, on theperiod between January 1, 2006 and May 1, 2013 there have occurred 5,826earthquakes which magnitude has been greater than 4.0 degrees on the Richtermagnitude scale (25.54% of the total of earthquakes registered on the nationalterritory), being the Pacific Plate and the Cocos Plate the most importantones. This document describes the development of an Artificial Neural Network(ANN) based on the radial topology which seeks to generate a prediction with anerror margin lower than 20% which can inform about the probability of a futureearthquake one of the main questions is: can artificial neural networks beapplied in seismic forecasting? It can be argued that research has thepotential to bring in the forecast seismic, more research is needed toconsolidate data and help mitigate the impact caused by such events linked withsociety. Keywords--- Analysis, Mexico, Neural Artificial Networks, Seismicity.
arxiv-1408-4363 | Object Segmentation in Images using EEG Signals |  http://arxiv.org/abs/1408.4363  | author:Eva Mohedano, Graham Healy, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor, Alan F. Smeaton category:cs.CV cs.MM published:2014-08-19 summary:This paper explores the potential of brain-computer interfaces in segmentingobjects from images. Our approach is centered around designing an effectivemethod for displaying the image parts to the users such that they generatemeasurable brain reactions. When an image region, specifically a block ofpixels, is displayed we estimate the probability of the block containing theobject of interest using a score based on EEG activity. After several suchblocks are displayed, the resulting probability map is binarized and combinedwith the GrabCut algorithm to segment the image into object and backgroundregions. This study shows that BCI and simple EEG analysis are useful inlocating object boundaries in images.
arxiv-1408-4487 | On Optimal Decision-Making in Ant Colonies |  http://arxiv.org/abs/1408.4487  | author:Mahnush Movahedi, Mahdi Zamani category:cs.DC cs.NE published:2014-08-19 summary:Colonies of ants can collectively choose the best of several nests, even whenmany of the active ants who organize the move visit only one site.Understanding such a behavior can help us design efficient distributed decisionmaking algorithms. Marshall et al. propose a model for house-hunting incolonies of ant Temnothorax albipennis. Unfortunately, their model does notachieve optimal decision-making while laboratory experiments show that, infact, colonies usually achieve optimality during the house-hunting process. Inthis paper, we argue that the model of Marshall et al. can achieve optimalityby including nest size information in their mathematical model. We use labresults of Pratt et al. to re-define the differential equations of Marshall etal. Finally, we sketch our strategy for testing the optimality of the newmodel.
arxiv-1408-4002 | The Filament Sensor for Near Real-Time Detection of Cytoskeletal Fiber Structures |  http://arxiv.org/abs/1408.4002  | author:Benjamin Eltzner, Carina Wollnik, Carsten Gottschlich, Stephan Huckemann, Florian Rehfeldt category:cs.CV I.4.3; I.4.6 published:2014-08-18 summary:A reliable extraction of filament data from microscopic images is of highinterest in the analysis of acto-myosin structures as early morphologicalmarkers in mechanically guided differentiation of human mesenchymal stem cellsand the understanding of the underlying fiber arrangement processes. In thispaper, we propose the filament sensor (FS), a fast and robust processingsequence which detects and records location, orientation, length and width foreach single filament of an image, and thus allows for the above describedanalysis. The extraction of these features has previously not been possiblewith existing methods. We evaluate the performance of the proposed FS in termsof accuracy and speed in comparison to three existing methods with respect totheir limited output. Further, we provide a benchmark dataset of real cellimages along with filaments manually marked by a human expert as well assimulated benchmark images. The FS clearly outperforms existing methods interms of computational runtime and filament extraction accuracy. Theimplementation of the FS and the benchmark database are available as opensource.
arxiv-1408-4140 | BET: Bayesian Ensemble Trees for Clustering and Prediction in Heterogeneous Data |  http://arxiv.org/abs/1408.4140  | author:Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak category:stat.ML stat.CO published:2014-08-18 summary:We propose a novel "tree-averaging" model that utilizes the ensemble ofclassification and regression trees (CART). Each constituent tree is estimatedwith a subset of similar data. We treat this grouping of subsets as Bayesianensemble trees (BET) and model them as an infinite mixture Dirichlet process.We show that BET adapts to data heterogeneity and accurately estimates eachcomponent. Compared with the bootstrap-aggregating approach, BET shows improvedprediction performance with fewer trees. We develop an efficient estimatingprocedure with improved sampling strategies in both CART and mixture models. Wedemonstrate these advantages of BET with simulations, classification of breastcancer and regression of lung function measurement of cystic fibrosis patients. Keywords: Bayesian CART; Dirichlet Process; Ensemble Approach; Heterogeneity;Mixture of Trees.
arxiv-1408-3967 | Learning Deep Representation for Face Alignment with Auxiliary Attributes |  http://arxiv.org/abs/1408.3967  | author:Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.LG published:2014-08-18 summary:In this study, we show that landmark detection or face alignment task is nota single and independent problem. Instead, its robustness can be greatlyimproved with auxiliary information. Specifically, we jointly optimize landmarkdetection together with the recognition of heterogeneous but subtly correlatedfacial attributes, such as gender, expression, and appearance attributes. Thisis non-trivial since different attribute inference tasks have differentlearning difficulties and convergence rates. To address this problem, weformulate a novel tasks-constrained deep model, which not only learns theinter-task correlation but also employs dynamic task coefficients to facilitatethe optimization convergence when learning multiple complex tasks. Extensiveevaluations show that the proposed task-constrained learning (i) outperformsexisting face alignment methods, especially in dealing with faces with severeocclusion and pose variation, and (ii) reduces model complexity drasticallycompared to the state-of-the-art methods based on cascaded deep model.
arxiv-1408-4077 | Brain: Biological noise-based logic |  http://arxiv.org/abs/1408.4077  | author:Laszlo B. Kish, Claes-Goran Granqvist, Sergey M. Bezrukov, Tamas Horvath category:cs.NE cs.ET published:2014-08-18 summary:Neural spikes in the brain form stochastic sequences, i.e., belong to theclass of pulse noises. This stochasticity is a counterintuitive feature becauseextracting information - such as the commonly supposed neural information ofmean spike frequency - requires long times for reasonably low errorprobability. The mystery could be solved by noise-based logic, whereinrandomness has an important function and allows large speed enhancements forspecial-purpose tasks, and the same mechanism is at work for the brain logicversion of this concept.
arxiv-1408-3985 | Offline Signature-Based Fuzzy Vault (OSFV: Review and New Results |  http://arxiv.org/abs/1408.3985  | author:George S. Eskander, Robert Sabourin, Eric Granger category:cs.CV cs.CR published:2014-08-18 summary:An offline signature-based fuzzy vault (OSFV) is a bio-cryptographicimplementation that uses handwritten signature images as biometrics instead oftraditional passwords to secure private cryptographic keys. Having a reliableOSFV implementation is the first step towards automating financial and legalauthentication processes, as it provides greater security of confidentialdocuments by means of the embedded handwritten signatures. The authors haverecently proposed the first OSFV implementation which is reviewed in thispaper. In this system, a machine learning approach based on the dissimilarityrepresentation concept is employed to select a reliable feature representationadapted for the fuzzy vault scheme. Some variants of this system are proposedfor enhanced accuracy and security. In particular, a new method that adaptsuser key size is presented. Performance of proposed methods are compared usingthe Brazilian PUCPR and GPDS signature databases and results indicate that thekey-size adaptation method achieves a good compromise between security andaccuracy. While average system entropy is increased from 45-bits to about51-bits, the AER (average error rate) is decreased by about 21%.
arxiv-1408-3934 | On Detecting Messaging Abuse in Short Text Messages using Linguistic and Behavioral patterns |  http://arxiv.org/abs/1408.3934  | author:Alejandro Mosquera, Lamine Aouad, Slawomir Grzonkowski, Dylan Morss category:cs.CL cs.AI cs.SI published:2014-08-18 summary:The use of short text messages in social media and instant messaging hasbecome a popular communication channel during the last years. This risingpopularity has caused an increment in messaging threats such as spam, phishingor malware as well as other threats. The processing of these short text messagethreats could pose additional challenges such as the presence of lexicalvariants, SMS-like contractions or advanced obfuscations which can degrade theperformance of traditional filtering solutions. By using a real-world SMS dataset from a large telecommunications operator from the US and a social mediacorpus, in this paper we analyze the effectiveness of machine learning filtersbased on linguistic and behavioral patterns in order to detect short text spamand abusive users in the network. We have also explored different ways to dealwith short text message challenges such as tokenization and entity detection byusing text normalization and substring clustering techniques. The obtainedresults show the validity of the proposed solution by enhancing baselineapproaches.
arxiv-1408-3944 | Down-Sampling coupled to Elastic Kernel Machines for Efficient Recognition of Isolated Gestures |  http://arxiv.org/abs/1408.3944  | author:Pierre-François Marteau, Sylvie Gibet, Clement Reverdy category:cs.LG cs.HC published:2014-08-18 summary:In the field of gestural action recognition, many studies have focused ondimensionality reduction along the spatial axis, to reduce both the variabilityof gestural sequences expressed in the reduced space, and the computationalcomplexity of their processing. It is noticeable that very few of these methodshave explicitly addressed the dimensionality reduction along the time axis.This is however a major issue with regard to the use of elastic distancescharacterized by a quadratic complexity. To partially fill this apparent gap,we present in this paper an approach based on temporal down-sampling associatedto elastic kernel machine learning. We experimentally show, on two data setsthat are widely referenced in the domain of human gesture recognition, and verydifferent in terms of quality of motion capture, that it is possible tosignificantly reduce the number of skeleton frames while maintaining a goodrecognition rate. The method proves to give satisfactory results at a levelcurrently reached by state-of-the-art methods on these data sets. Thecomputational complexity reduction makes this approach eligible for real-timeapplications.
arxiv-1408-4045 | Relax, no need to round: integrality of clustering formulations |  http://arxiv.org/abs/1408.4045  | author:Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, Rachel Ward category:stat.ML cs.DS cs.LG math.ST stat.TH published:2014-08-18 summary:We study exact recovery conditions for convex relaxations of point cloudclustering problems, focusing on two of the most common optimization problemsfor unsupervised clustering: $k$-means and $k$-median clustering. Motivationsfor focusing on convex relaxations are: (a) they come with a certificate ofoptimality, and (b) they are generic tools which are relatively parameter-free,not tailored to specific assumptions over the input. More precisely, weconsider the distributional setting where there are $k$ clusters in$\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from asymmetric distribution within a ball of unit radius. We ask: what is theminimal separation distance between cluster centers needed for convexrelaxations to exactly recover these $k$ clusters as the optimal integralsolution? For the $k$-median linear programming relaxation we show a tightbound: exact recovery is obtained given arbitrarily small pairwise separation$\epsilon > 0$ between the balls. In other words, the pairwise centerseparation is $\Delta > 2+\epsilon$. Under the same distributional model, the$k$-means LP relaxation fails to recover such clusters at separation as largeas $\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we getexact cluster recovery at center separation $\Delta > 2\sqrt2(1+\sqrt{1/m})$.In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-meansalgorithm) can fail to recover clusters in this setting; even with arbitrarilylarge cluster separation, k-means++ with overseeding by any constant factorfails with high probability at exact cluster recovery. To complement thetheoretical analysis, we provide an experimental study of the recoveryguarantees for these various methods, and discuss several open problems whichthese experiments suggest.
arxiv-1408-3814 | Robust Statistical Approach for Extraction of Moving Human Silhouettes from Videos |  http://arxiv.org/abs/1408.3814  | author:Oinam Binarani Devi, Nissi S. Paul, Y. Jayanta Singh category:cs.CV published:2014-08-17 summary:Human pose estimation is one of the key problems in computer vision that hasbeen studied in the recent years. The significance of human pose estimation isin the higher level tasks of understanding human actions applications such asrecognition of anomalous actions present in videos and many other relatedapplications. The human poses can be estimated by extracting silhouettes ofhumans as silhouettes are robust to variations and it gives the shapeinformation of the human body. Some common challenges include illuminationchanges, variation in environments, and variation in human appearances. Thusthere is a need for a robust method for human pose estimation. This paperpresents a study and analysis of approaches existing for silhouette extractionand proposes a robust technique for extracting human silhouettes in videosequences. Gaussian Mixture Model (GMM) A statistical approach is combined withHSV (Hue, Saturation and Value) color space model for a robust background modelthat is used for background subtraction to produce foreground blobs, calledhuman silhouettes. Morphological operations are then performed on foregroundblobs from background subtraction. The silhouettes obtained from this work canbe used in further tasks associated with human action interpretation andactivity processes like human action classification, human pose estimation andaction recognition or action interpretation.
arxiv-1408-3807 | On solving Ordinary Differential Equations using Gaussian Processes |  http://arxiv.org/abs/1408.3807  | author:David Barber category:stat.ME cs.NA math.NA stat.CO stat.ML published:2014-08-17 summary:We describe a set of Gaussian Process based approaches that can be used tosolve non-linear Ordinary Differential Equations. We suggest an explicitprobabilistic solver and two implicit methods, one analogous to Picarditeration and the other to gradient matching. All methods have greater accuracythan previously suggested Gaussian Process approaches. We also suggest ageneral approach that can yield error estimates from any standard ODE solver.
arxiv-1408-3873 | Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome |  http://arxiv.org/abs/1408.3873  | author:Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.CV cs.AI physics.bio-ph q-bio.BM I.5 published:2014-08-17 summary:We evaluate a version of the recently-proposed classification system namedOptimized Dissimilarity Space Embedding (ODSE) that operates in the input spaceof sequences of generic objects. The ODSE system has been originally presentedas a classification system for patterns represented as labeled graphs. However,since ODSE is founded on the dissimilarity space representation of the inputdata, the classifier can be easily adapted to any input domain where it ispossible to define a meaningful dissimilarity measure. Here we demonstrate theeffectiveness of the ODSE classifier for sequences by considering anapplication dealing with the recognition of the solubility degree of theEscherichia coli proteome. Solubility, or analogously aggregation propensity,is an important property of protein molecules, which is intimately related tothe mechanisms underlying the chemico-physical process of folding. Each proteinof our dataset is initially associated with a solubility degree and it isrepresented as a sequence of symbols, denoting the 20 amino acid residues. Theherein obtained computational results, which we stress that have been achievedwith no context-dependent tuning of the ODSE system, confirm the validity andgenerality of the ODSE-based approach for structured data classification.
arxiv-1408-3809 | HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition |  http://arxiv.org/abs/1408.3809  | author:Hossein Rahmani, Arif Mahmood, Du Q. Huynh, Ajmal Mian category:cs.CV published:2014-08-17 summary:Existing techniques for 3D action recognition are sensitive to viewpointvariations because they extract features from depth images which changesignificantly with viewpoint. In contrast, we directly process the pointcloudsand propose a new technique for action recognition which is more robust tonoise, action speed and viewpoint variations. Our technique consists of a noveldescriptor and keypoint detection algorithm. The proposed descriptor isextracted at a point by encoding the Histogram of Oriented Principal Components(HOPC) within an adaptive spatio-temporal support volume around that point.Based on this descriptor, we present a novel method to detect Spatio-TemporalKey-Points (STKPs) in 3D pointcloud sequences. Experimental results show thatthe proposed descriptor and STKP detector outperform state-of-the-artalgorithms on three benchmark human activity datasets. We also introduce a newmultiview public dataset and show the robustness of our proposed method toviewpoint variations.
arxiv-1408-3810 | Action Classification with Locality-constrained Linear Coding |  http://arxiv.org/abs/1408.3810  | author:Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian category:cs.CV published:2014-08-17 summary:We propose an action classification algorithm which uses Locality-constrainedLinear Coding (LLC) to capture discriminative information of human bodyvariations in each spatiotemporal subsequence of a video sequence. Our proposedmethod divides the input video into equally spaced overlapping spatiotemporalsubsequences, each of which is decomposed into blocks and then cells. We usethe Histogram of Oriented Gradient (HOG3D) feature to encode the information ineach cell. We justify the use of LLC for encoding the block descriptor bydemonstrating its superiority over Sparse Coding (SC). Our sequence descriptoris obtained via a logistic regression classifier with L2 regularization. Weevaluate and compare our algorithm with ten state-of-the-art algorithms on fivebenchmark datasets. Experimental results show that, on average, our algorithmgives better accuracy than these ten algorithms.
arxiv-1408-3818 | Unsupervised learning segmentation for dynamic speckle activity images |  http://arxiv.org/abs/1408.3818  | author:Lucia I. Passoni, Ana I. Dai Pra, Gustavo J. Meschino, MArcelo Guzman, Chistian Weber, Héctor Rabal, Marcelo Trivi category:physics.optics cs.CV published:2014-08-17 summary:This paper proposes the design of decision models based on ComputationalIntelligence techniques applied to image sequences of dynamic laser speckle.These models aim to identify image regions of biological specimens illuminatedby a coherent beam coming from a laser. The field image is pseudo colored usinga Self Organizing Map projection. This process is carried out using a set ofdescriptors applied to the intensity variations along time in every pixel of animage sequence. The models use descriptors selected to improve effectiveness,depending on the specific application. We present two examples of theapplication of the proposed techniques to assess biological tissues. Theresults obtained are encouraging and significantly improve those obtained usinga single descriptor.
arxiv-1408-3829 | Opinion mining of movie reviews at document level |  http://arxiv.org/abs/1408.3829  | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.IR cs.CL published:2014-08-17 summary:The whole world is changed rapidly and using the current technologiesInternet becomes an essential need for everyone. Web is used in every field.Most of the people use web for a common purpose like online shopping, chattingetc. During an online shopping large number of reviews/opinions are given bythe users that reflect whether the product is good or bad. These reviews needto be explored, analyse and organized for better decision making. OpinionMining is a natural language processing task that deals with findingorientation of opinion in a piece of text with respect to a topic. In thispaper a document based opinion mining system is proposed that classify thedocuments as positive, negative and neutral. Negation is also handled in theproposed system. Experimental results using reviews of movies show theeffectiveness of the system.
arxiv-1408-3740 | A fast patch-dictionary method for whole image recovery |  http://arxiv.org/abs/1408.3740  | author:Yangyang Xu, Wotao Yin category:cs.CV math.OC 94A08, 94A12 published:2014-08-16 summary:Various algorithms have been proposed for dictionary learning. Among thosefor image processing, many use image patches to form dictionaries. This paperfocuses on whole-image recovery from corrupted linear measurements. We addressthe open issue of representing an image by overlapping patches: the overlappingleads to an excessive number of dictionary coefficients to determine. With veryfew exceptions, this issue has limited the applications of image-patch methodsto the local kind of tasks such as denoising, inpainting, cartoon-texturedecomposition, super-resolution, and image deblurring, for which one canprocess a few patches at a time. Our focus is global imaging tasks such ascompressive sensing and medical image recovery, where the whole image isencoded together, making it either impossible or very ineffective to update afew patches at a time. Our strategy is to divide the sparse recovery into multiple subproblems, eachof which handles a subset of non-overlapping patches, and then the results ofthe subproblems are averaged to yield the final recovery. This simple strategyis surprisingly effective in terms of both quality and speed. In addition, weaccelerate computation of the learned dictionary by applying a recent blockproximal-gradient method, which not only has a lower per-iteration complexitybut also takes fewer iterations to converge, compared to the currentstate-of-the-art. We also establish that our algorithm globally converges to astationary point. Numerical results on synthetic data demonstrate that ouralgorithm can recover a more faithful dictionary than two state-of-the-artmethods. Combining our whole-image recovery and dictionary-learning methods, wenumerically simulate image inpainting, compressive sensing recovery, anddeblurring. Our recovery is more faithful than those of a total variationmethod and a method based on overlapping patches.
