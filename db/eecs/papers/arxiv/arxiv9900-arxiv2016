arxiv-9900-1 | Low Rank Matrix Completion with Exponential Family Noise | http://arxiv.org/pdf/1502.06919v2.pdf | author:Jean Lafond category:math.ST stat.ML stat.TH published:2015-02-24 summary:The matrix completion problem consists in reconstructing a matrix from asample of entries, possibly observed with noise. A popular class of estimator,known as nuclear norm penalized estimators, are based on minimizing the sum ofa data fitting term and a nuclear norm penalization. Here, we investigate thecase where the noise distribution belongs to the exponential family and issub-exponential. Our framework alllows for a general sampling scheme. We firstconsider an estimator defined as the minimizer of the sum of a log-likelihoodterm and a nuclear norm penalization and prove an upper bound on the Frobeniusprediction risk. The rate obtained improves on previous works on matrixcompletion for exponential family. When the sampling distribution is known, wepropose another estimator and prove an oracle inequality w.r.t. theKullback-Leibler prediction risk, which translates immediatly into an upperbound on the Frobenius prediction risk. Finally, we show that all the ratesobtained are minimax optimal up to a logarithmic factor.
arxiv-9900-2 | Weakly Supervised Fine-Grained Image Categorization | http://arxiv.org/pdf/1504.04943v1.pdf | author:Yu Zhang, Xiu-shen Wei, Jianxin Wu, Jianfei Cai, Jiangbo Lu, Viet-Anh Nguyen, Minh N. Do category:cs.CV published:2015-04-20 summary:In this paper, we categorize fine-grained images without using any object /part annotation neither in the training nor in the testing stage, a steptowards making it suitable for deployments. Fine-grained image categorizationaims to classify objects with subtle distinctions. Most existing works heavilyrely on object / part detectors to build the correspondence between objectparts by using object or object part annotations inside training images. Theneed for expensive object annotations prevents the wide usage of these methods.Instead, we propose to select useful parts from multi-scale part proposals inobjects, and use them to compute a global image representation forcategorization. This is specially designed for the annotation-free fine-grainedcategorization task, because useful parts have shown to play an important rolein existing annotation-dependent works but accurate part detectors can behardly acquired. With the proposed image representation, we can further detectand visualize the key (most discriminative) parts in objects of differentclasses. In the experiment, the proposed annotation-free method achieves betteraccuracy than that of state-of-the-art annotation-free and most existingannotation-dependent methods on two challenging datasets, which shows that itis not always necessary to use accurate object / part annotations infine-grained image categorization.
arxiv-9900-3 | Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition | http://arxiv.org/pdf/1504.04923v1.pdf | author:Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton von den Hengel category:cs.CV published:2015-04-20 summary:The introduction of low-cost RGB-D sensors has promoted the research inskeleton-based human action recognition. Devising a representation suitable forcharacterising actions on the basis of noisy skeleton sequences remains achallenge, however. We here provide two insights into this challenge. First, weshow that the discriminative information of a skeleton sequence usually residesin a short temporal interval and we propose a simple-but-effective localdescriptor called trajectorylet to capture the static and kinematic informationwithin this interval. Second, we further propose to encode each trajectoryletwith a discriminative trajectorylet detector set which is selected from a largenumber of candidate detectors trained through exemplar-SVMs. The action-levelrepresentation is obtained by pooling trajectorylet encodings. Evaluating onstandard datasets acquired from the Kinect sensor, it is demonstrated that ourmethod obtains superior results over existing approaches under variousexperimental setups.
arxiv-9900-4 | Illuminating search spaces by mapping elites | http://arxiv.org/pdf/1504.04909v1.pdf | author:Jean-Baptiste Mouret, Jeff Clune category:cs.AI cs.NE cs.RO q-bio.PE published:2015-04-20 summary:Many fields use search algorithms, which automatically explore a search spaceto find high-performing solutions: chemists search through the space ofmolecules to discover new drugs; engineers search for stronger, cheaper, saferdesigns, scientists search for models that best explain data, etc. The goal ofsearch algorithms has traditionally been to return the singlehighest-performing solution in a search space. Here we describe a new,fundamentally different type of algorithm that is more useful because itprovides a holistic view of how high-performing solutions are distributedthroughout a search space. It creates a map of high-performing solutions ateach point in a space defined by dimensions of variation that a user gets tochoose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)algorithm illuminates search spaces, allowing researchers to understand howinteresting attributes of solutions combine to affect performance, eitherpositively or, equally of interest, negatively. For example, a drug company maywish to understand how performance changes as the size of molecules and theircost-to-produce vary. MAP-Elites produces a large diversity of high-performing,yet qualitatively different solutions, which can be more helpful than a single,high-performing solution. Interestingly, because MAP-Elites explores more ofthe search space, it also tends to find a better overall solution thanstate-of-the-art search algorithms. We demonstrate the benefits of this newalgorithm in three different problem domains ranging from producing modularneural networks to designing simulated and real soft robots. Because MAP-Elites (1) illuminates the relationship between performance and dimensions ofinterest in solutions, (2) returns a set of high-performing, yet diversesolutions, and (3) improves finding a single, best solution, it will advancescience and engineering.
arxiv-9900-5 | Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition | http://arxiv.org/pdf/1411.6660v4.pdf | author:Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander G. Hauptmann, Bhiksha Raj category:cs.CV published:2014-11-24 summary:Most state-of-the-art action feature extractors involve differentialoperators, which act as highpass filters and tend to attenuate low frequencyaction information. This attenuation introduces bias to the resulting featuresand generates ill-conditioned feature matrices. The Gaussian Pyramid has beenused as a feature enhancing technique that encodes scale-invariantcharacteristics into the feature space in an attempt to deal with thisattenuation. However, at the core of the Gaussian Pyramid is a convolutionalsmoothing operation, which makes it incapable of generating new features atcoarse scales. In order to address this problem, we propose a novel featureenhancing technique called Multi-skIp Feature Stacking (MIFS), which stacksfeatures extracted using a family of differential filters parameterized withmultiple time skips and encodes shift-invariance into the frequency space. MIFScompensates for information lost from using differential operators byrecapturing information at coarse scales. This recaptured information allows usto match actions at different speeds and ranges of motion. We prove that MIFSenhances the learnability of differential-based features exponentially. Theresulting feature matrices from MIFS have much smaller conditional numbers andvariances than those from conventional methods. Experimental results showsignificantly improved performance on challenging action recognition and eventdetection tasks. Specifically, our method exceeds the state-of-the-arts onHollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts onHMDB51 and Olympics Sports datasets. MIFS can also be used as a speedupstrategy for feature extraction with minimal or no accuracy cost.
arxiv-9900-6 | DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets | http://arxiv.org/pdf/1504.04871v1.pdf | author:Sukrit Shankar, Vikas K. Garg, Roberto Cipolla category:cs.CV published:2015-04-19 summary:Most of the approaches for discovering visual attributes in images demandsignificant supervision, which is cumbersome to obtain. In this paper, we aimto discover visual attributes in a weakly supervised setting that is commonlyencountered with contemporary image search engines. Deep Convolutional NeuralNetworks (CNNs) have enjoyed remarkable success in vision applicationsrecently. However, in a weakly supervised scenario, widely used CNN trainingprocedures do not learn a robust model for predicting multiple attribute labelssimultaneously. The primary reason is that the attributes highly co-occurwithin the training data. To ameliorate this limitation, we proposeDeep-Carving, a novel training procedure with CNNs, that helps the netefficiently carve itself for the task of multiple attribute prediction. Duringtraining, the responses of the feature maps are exploited in an ingenious wayto provide the net with multiple pseudo-labels (for training images) forsubsequent iterations. The process is repeated periodically after a fixednumber of iterations, and enables the net carve itself iteratively forefficiently disentangling features. Additionally, we contribute anoun-adjective pairing inspired Natural Scenes Attributes Dataset to theresearch community, CAMIT - NSAD, containing a number of co-occurringattributes within a noun category. We describe, in detail, salient aspects ofthis dataset. Our experiments on CAMIT-NSAD and the SUN Attributes Dataset,with weak supervision, clearly demonstrate that the Deep-Carved CNNsconsistently achieve considerable improvement in the precision of attributeprediction over popular baseline methods.
arxiv-9900-7 | Score Function Features for Discriminative Learning | http://arxiv.org/pdf/1412.6514v2.pdf | author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG stat.ML published:2014-12-19 summary:Feature learning forms the cornerstone for tackling challenging learningproblems in domains such as speech, computer vision and natural languageprocessing. In this paper, we consider a novel class of matrix andtensor-valued features, which can be pre-trained using unlabeled samples. Wepresent efficient algorithms for extracting discriminative information, giventhese pre-trained features and labeled samples for any related task. Our classof features are based on higher-order score functions, which capture localvariations in the probability density function of the input. We establish atheoretical framework to characterize the nature of discriminative informationthat can be extracted from score-function features, when used in conjunctionwith labeled samples. We employ efficient spectral decomposition algorithms (onmatrices and tensors) for extracting discriminative components. The advantageof employing tensor-valued features is that we can extract richerdiscriminative information in the form of an overcomplete representations.Thus, we present a novel framework for employing generative models of the inputfor discriminative learning.
arxiv-9900-8 | Automatic differentiation in machine learning: a survey | http://arxiv.org/pdf/1502.05767v2.pdf | author:Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind category:cs.SC cs.LG G.1.4; I.2.6 published:2015-02-20 summary:Derivatives, mostly in the form of gradients and Hessians, are ubiquitous inmachine learning. Automatic differentiation (AD) is a technique for calculatingderivatives of numeric functions expressed as computer programs efficiently andaccurately, used in fields such as computational fluid dynamics, nuclearengineering, and atmospheric sciences. Despite its advantages and use in otherfields, machine learning practitioners have been little influenced by AD andmake scant use of available tools. We survey the intersection of AD and machinelearning, cover applications where AD has the potential to make a big impact,and report on some recent developments in the adoption of this technique. Weaim to dispel some misconceptions that we contend have impeded the use of ADwithin the machine learning community.
arxiv-9900-9 | Exploring Bayesian Models for Multi-level Clustering of Hierarchically Grouped Sequential Data | http://arxiv.org/pdf/1504.04850v1.pdf | author:Adway Mitra category:cs.LG cs.AI published:2015-04-19 summary:A wide range of Bayesian models have been proposed for data that is dividedhierarchically into groups. These models aim to cluster the data at differentlevels of grouping, by assigning a mixture component to each datapoint, and amixture distribution to each group. Multi-level clustering is facilitated bythe sharing of these components and distributions by the groups. In this paper,we introduce the concept of Degree of Sharing (DoS) for the mixture componentsand distributions, with an aim to analyze and classify various existing models.Next we introduce a generalized hierarchical Bayesian model, of which theexisting models can be shown to be special cases. Unlike most of these models,our model takes into account the sequential nature of the data, and variousother temporal structures at different levels while assigning mixturecomponents and distributions. We show one specialization of this model aimed athierarchical segmentation of news transcripts, and present a Gibbs Samplingbased inference algorithm for it. We also show experimentally that the proposedmodel outperforms existing models for the same task.
arxiv-9900-10 | Gradual Classical Logic for Attributed Objects - Extended in Re-Presentation | http://arxiv.org/pdf/1504.04802v1.pdf | author:Ryuta Arisaka category:cs.AI cs.CL cs.LO published:2015-04-19 summary:Our understanding about things is conceptual. By stating that we reason aboutobjects, it is in fact not the objects but concepts referring to them that wemanipulate. Now, so long just as we acknowledge infinitely extending notionssuch as space, time, size, colour, etc, - in short, any reasonable quality -into which an object is subjected, it becomes infeasible to affirm atomicity inthe concept referring to the object. However, formal/symbolic logics typicallypresume atomic entities upon which other expressions are built. Can we reflectour intuition about the concept onto formal/symbolic logics at all? I assurethat we can, but the usual perspective about the atomicity needs inspected. Inthis work, I present gradual logic which materialises the observation that wecannot tell apart whether a so-regarded atomic entity is atomic or is justatomic enough not to be considered non-atomic. The motivation is to capturecertain phenomena that naturally occur around concepts with attributes,including presupposition and contraries. I present logical particulars of thelogic, which is then mapped onto formal semantics. Two linguisticallyinteresting semantics will be considered. Decidability is shown.
arxiv-9900-11 | Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval | http://arxiv.org/pdf/1501.06272v2.pdf | author:Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan category:cs.CV cs.LG published:2015-01-26 summary:With the rapid growth of web images, hashing has received increasinginterests in large scale image retrieval. Research efforts have been devoted tolearning compact binary codes that preserve semantic similarity based onlabels. However, most of these hashing methods are designed to handle simplebinary similarity. The complex multilevel semantic structure of imagesassociated with multiple labels have not yet been well explored. Here wepropose a deep semantic ranking based method for learning hash functions thatpreserve multilevel semantic similarity between multi-label images. In ourapproach, deep convolutional neural network is incorporated into hash functionsto jointly learn feature representations and mappings from them to hash codes,which avoids the limitation of semantic representation power of hand-craftedfeatures. Meanwhile, a ranking list that encodes the multilevel similarityinformation is employed to guide the learning of such deep hash functions. Aneffective scheme based on surrogate loss is used to solve the intractableoptimization problem of nonsmooth and multivariate ranking measures involved inthe learning procedure. Experimental results show the superiority of ourproposed approach over several state-of-the-art hashing methods in term ofranking evaluation metrics when tested on multi-label image datasets.
arxiv-9900-12 | Compressing Neural Networks with the Hashing Trick | http://arxiv.org/pdf/1504.04788v1.pdf | author:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen category:cs.LG cs.NE published:2015-04-19 summary:As deep nets are increasingly used in applications suited for mobile devices,a fundamental dilemma becomes apparent: the trend in deep learning is to growmodels to absorb ever-increasing data set sizes; however mobile devices aredesigned with very little memory and cannot store such large models. We presenta novel network architecture, HashedNets, that exploits inherent redundancy inneural networks to achieve drastic reductions in model sizes. HashedNets uses alow-cost hash function to randomly group connection weights into hash buckets,and all connections within the same hash bucket share a single parameter value.These parameters are tuned to adjust to the HashedNets weight sharingarchitecture with standard backprop during training. Our hashing procedureintroduces no additional memory overhead, and we demonstrate on severalbenchmark data sets that HashedNets shrink the storage requirements of neuralnetworks substantially while mostly preserving generalization performance.
arxiv-9900-13 | Supervised Discrete Hashing | http://arxiv.org/pdf/1503.01557v3.pdf | author:Fumin Shen, Chunhua Shen, Wei Liu, Heng Tao Shen category:cs.CV published:2015-03-05 summary:This paper has been withdrawn by the authour.
arxiv-9900-14 | A Chasm Between Identity and Equivalence Testing with Conditional Queries | http://arxiv.org/pdf/1411.7346v2.pdf | author:Jayadev Acharya, Clément L. Canonne, Gautam Kamath category:cs.DS cs.CC cs.LG math.PR math.ST stat.TH published:2014-11-26 summary:A recent model for property testing of probability distributions enablestremendous savings in the sample complexity of testing algorithms, by allowingthem to condition the sampling on subsets of the domain. In particular, Canonne, Ron, and Servedio showed that, in this setting,testing identity of an unknown distribution $D$ (i.e., whether $D=D^*$ for anexplicitly known $D^*$) can be done with a constant number of samples,independent of the support size $n$ -- in contrast to the required $\sqrt{n}$in the standard sampling model. However, it was unclear whether the same heldfor the case of testing equivalence, where both distributions are unknown.Indeed, while Canonne, Ron, and Servedio established a$\mathrm{poly}\log(n)$-query upper bound for equivalence testing, very recentlybrought down to $\tilde O(\log\log n)$ by Falahatgar et al., whether adependence on the domain size $n$ is necessary was still open, and explicitlyposed by Fischer at the Bertinoro Workshop on Sublinear Algorithms. In thiswork, we answer the question in the positive, showing that any testingalgorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in theconditional sampling model. Interestingly, this demonstrates an intrinsicqualitative gap between identity and equivalence testing, absent in thestandard sampling model (where both problems have sampling complexity$n^{\Theta(1)}$). Turning to another question, we investigate the complexity of support sizeestimation. We provide a doubly-logarithmic upper bound for the adaptiveversion of this problem, generalizing work of Ron and Tsur to our weaker model.We also establish a logarithmic lower bound for the non-adaptive version ofthis problem. This latter result carries on to the related problem ofnon-adaptive uniformity testing, an exponential improvement over previousresults that resolves an open question of Chakraborty et al.
arxiv-9900-15 | Online Inference for Relation Extraction with a Reduced Feature Set | http://arxiv.org/pdf/1504.04770v1.pdf | author:Maxim Rabinovich, Cédric Archambeau category:cs.CL cs.LG published:2015-04-18 summary:Access to web-scale corpora is gradually bringing robust automatic knowledgebase creation and extension within reach. To exploit these largeunannotated---and extremely difficult to annotate---corpora, unsupervisedmachine learning methods are required. Probabilistic models of text haverecently found some success as such a tool, but scalability remains an obstaclein their application, with standard approaches relying on sampling schemes thatare known to be difficult to scale. In this report, we therefore present anempirical assessment of the sublinear time sparse stochastic variationalinference (SSVI) scheme applied to RelLDA. We demonstrate that online inferenceleads to relatively strong qualitative results but also identify some of itspathologies---and those of the model---which will need to be overcome if SSVIis to be used for large-scale relation extraction.
arxiv-9900-16 | Understanding the Fisher Vector: a multimodal part model | http://arxiv.org/pdf/1504.04763v1.pdf | author:David Novotný, Diane Larlus, Florent Perronnin, Andrea Vedaldi category:cs.CV published:2015-04-18 summary:Fisher Vectors and related orderless visual statistics have demonstratedexcellent performance in object detection, sometimes superior to establishedapproaches such as the Deformable Part Models. However, it remains unclear howthese models can capture complex appearance variations using visual codebooksof limited sizes and coarse geometric information. In this work, we propose tointerpret Fisher-Vector-based object detectors as part-based models. Throughthe use of several visualizations and experiments, we show that this is auseful insight to explain the good performance of the model. Furthermore, wereveal for the first time several interesting properties of the FV, includingits ability to work well using only a small subset of input patches and visualwords. Finally, we discuss the relation of the FV and DPM detectors, pointingout differences and commonalities between them.
arxiv-9900-17 | Time Resolution Dependence of Information Measures for Spiking Neurons: Atoms, Scaling, and Universality | http://arxiv.org/pdf/1504.04756v1.pdf | author:Sarah E. Marzen, Michael R. DeWeese, James P. Crutchfield category:q-bio.NC cs.NE math.PR nlin.CD published:2015-04-18 summary:The mutual information between stimulus and spike-train response is commonlyused to monitor neural coding efficiency, but neuronal computation broadlyconceived requires more refined and targeted information measures ofinput-output joint processes. A first step towards that larger goal is todevelop information measures for individual output processes, includinginformation generation (entropy rate), stored information (statisticalcomplexity), predictable information (excess entropy), and active informationaccumulation (bound information rate). We calculate these for spike trainsgenerated by a variety of noise-driven integrate-and-fire neurons as a functionof time resolution and for alternating renewal processes. We show that theirtime-resolution dependence reveals coarse-grained structural properties ofinterspike interval statistics; e.g., $\tau$-entropy rates that diverge lessquickly than the firing rate indicate interspike interval correlations. We alsofind evidence that the excess entropy and regularized statistical complexity ofdifferent types of integrate-and-fire neurons are universal in thecontinuous-time limit in the sense that they do not depend on mechanismdetails. This suggests a surprising simplicity in the spike trains generated bythese model neurons. Interestingly, neurons with gamma-distributed ISIs andneurons whose spike trains are alternating renewal processes do not fall intothe same universality class. These results lead to two conclusions. First, thedependence of information measures on time resolution reveals mechanisticdetails about spike train generation. Second, information measures can be usedas model selection tools for analyzing spike train processes.
arxiv-9900-18 | A Knowledge-poor Pronoun Resolution System for Turkish | http://arxiv.org/pdf/1504.04751v1.pdf | author:Dilek Küçük, Meltem Turhan Yöndem category:cs.CL published:2015-04-18 summary:A pronoun resolution system which requires limited syntactic knowledge toidentify the antecedents of personal and reflexive pronouns in Turkish ispresented. As in its counterparts for languages like English, Spanish andFrench, the core of the system is the constraints and preferences determinedempirically. In the evaluation phase, it performed considerably better than thebaseline algorithm used for comparison. The system is significant for its beingthe first fully specified knowledge-poor computational framework for pronounresolution in Turkish where Turkish possesses different structural propertiesfrom the languages for which knowledge-poor systems had been developed.
arxiv-9900-19 | On the consistency of Multithreshold Entropy Linear Classifier | http://arxiv.org/pdf/1504.04740v1.pdf | author:Wojciech Marian Czarnecki category:cs.LG stat.ML published:2015-04-18 summary:Multithreshold Entropy Linear Classifier (MELC) is a recent classifier ideawhich employs information theoretic concept in order to create a multithresholdmaximum margin model. In this paper we analyze its consistency overmultithreshold linear models and show that its objective function upper boundsthe amount of misclassified points in a similar manner like hinge loss does insupport vector machines. For further confirmation we also conduct somenumerical experiments on five datasets.
arxiv-9900-20 | Fast optimization of Multithreshold Entropy Linear Classifier | http://arxiv.org/pdf/1504.04739v1.pdf | author:Rafal Jozefowicz, Wojciech Marian Czarnecki category:cs.LG stat.ML published:2015-04-18 summary:Multithreshold Entropy Linear Classifier (MELC) is a density based modelwhich searches for a linear projection maximizing the Cauchy-Schwarz Divergenceof dataset kernel density estimation. Despite its good empirical results, oneof its drawbacks is the optimization speed. In this paper we analyze how onecan speed it up through solving an approximate problem. We analyze two methods,both similar to the approximate solutions of the Kernel Density Estimationquerying and provide adaptive schemes for selecting a crucial parameters basedon user-specified acceptable error. Furthermore we show how one can exploitwell known conjugate gradients and L-BFGS optimizers despite the fact that theoriginal optimization problem should be solved on the sphere. All above methodsand modifications are tested on 10 real life datasets from UCI repository toconfirm their practical usability.
arxiv-9900-21 | Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality | http://arxiv.org/pdf/1504.04716v1.pdf | author:Vishal Shukla category:cs.CL cs.AI published:2015-04-18 summary:Modality is one of the important components of grammar in linguistics. Itlets speaker to express attitude towards, or give assessment or potentiality ofstate of affairs. It implies different senses and thus has differentperceptions as per the context. This paper presents an account showing the gapin the functionality of the current state of art Natural Language Processing(NLP) systems. The contextual nature of linguistic modality is studied. In thispaper, the works and logical approaches employed by Natural Language Processingsystems dealing with modality are reviewed. It sees human cognition andintelligence as multi-layered approach that can be implemented by intelligentsystems for learning. Lastly, current flow of research going on within thisfield is talked providing futurology.
arxiv-9900-22 | Web-Scale Training for Face Identification | http://arxiv.org/pdf/1406.5266v2.pdf | author:Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf category:cs.CV published:2014-06-20 summary:Scaling machine learning methods to very large datasets has attractedconsiderable attention in recent years, thanks to easy access to ubiquitoussensing and data from the web. We study face recognition and show that threedistinct properties have surprising effects on the transferability of deepconvolutional networks (CNN): (1) The bottleneck of the network serves as animportant transfer learning regularizer, and (2) in contrast to the commonwisdom, performance saturation may exist in CNN's (as the number of trainingsamples grows); we propose a solution for alleviating this by replacing thenaive random subsampling of the training set with a bootstrapping process.Moreover, (3) we find a link between the representation norm and the ability todiscriminate in a target domain, which sheds lights on how such networksrepresent faces. Based on these discoveries, we are able to improve facerecognition accuracy on the widely used LFW benchmark, both in the verification(1:1) and identification (1:N) protocols, and directly compare, for the firsttime, with the state of the art Commercially-Off-The-Shelf system and show asizable leap in performance.
arxiv-9900-23 | Adaptive Stochastic Gradient Descent on the Grassmannian for Robust Low-Rank Subspace Recovery and Clustering | http://arxiv.org/pdf/1412.4044v2.pdf | author:Jun He, Yue Zhang category:stat.ML cs.CV cs.NA math.OC published:2014-12-12 summary:In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradientfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm torobustly recover the low-rank subspace from a large matrix. In the presence ofcolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ normminimization with rank constraint problem as a stochastic optimization approachconstrained on Grassmann manifold. For each observed data vector, the low-ranksubspace $\mathcal{S}$ is updated by taking a gradient step along the geodesicof Grassmannian. In order to accelerate the convergence rate of the stochasticgradient method, we choose to adaptively tune the constant step-size byleveraging the consecutive gradients. Furthermore, we demonstrate that withproper initialization, the K-subspaces extension, K-GASG21, can robustlycluster a large number of corrupted data vectors into a union of subspaces.Numerical experiments on synthetic and real data demonstrate the efficiency andaccuracy of the proposed algorithms even with heavy column outliers corruption.
arxiv-9900-24 | Local, Private, Efficient Protocols for Succinct Histograms | http://arxiv.org/pdf/1504.04686v1.pdf | author:Raef Bassily, Adam Smith category:cs.CR cs.DS cs.LG F.2.0 published:2015-04-18 summary:We give efficient protocols and matching accuracy lower bounds for frequencyestimation in the local model for differential privacy. In this model,individual users randomize their data themselves, sending differentiallyprivate reports to an untrusted server that aggregates them. We study protocols that produce a succinct histogram representation of thedata. A succinct histogram is a list of the most frequent items in the data(often called "heavy hitters") along with estimates of their frequencies; thefrequency of all other items is implicitly estimated as 0. If there are $n$ users whose items come from a universe of size $d$, ourprotocols run in time polynomial in $n$ and $\log(d)$. With high probability,they estimate the accuracy of every item up to error$O\left(\sqrt{\log(d)/(\epsilon^2n)}\right)$ where $\epsilon$ is the privacyparameter. Moreover, we show that this much error is necessary, regardless ofcomputational efficiency, and even for the simple setting where only one itemappears with significant frequency in the data set. Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) forthis task either ran in time $\Omega(d)$ or had much worse error (about$\sqrt[6]{\log(d)/(\epsilon^2n)}$), and the only known lower bound on error was$\Omega(1/\sqrt{n})$. We also adapt a result of McGregor et al (2010) to the local setting. In amodel with public coins, we show that each user need only send 1 bit to theserver. For all known local protocols (including ours), the transformationpreserves computational efficiency.
arxiv-9900-25 | Inverse Reinforcement Learning with Multi-Relational Chains for Robot-Centered Smart Home | http://arxiv.org/pdf/1408.3727v5.pdf | author:Kun Li, Max Q. -H. Meng category:cs.RO cs.LG published:2014-08-16 summary:In a robot-centered smart home, the robot observes the home states with itsown sensors, and then it can change certain object states according to anoperator's commands for remote operations, or imitate the operator's behaviorsin the house for autonomous operations. To model the robot's imitation of theoperator's behaviors in a dynamic indoor environment, we use multi-relationalchains to describe the changes of environment states, and apply inversereinforcement learning to encoding the operator's behaviors with a learnedreward function. We implement this approach with a mobile robot, and do fiveexperiments to include increasing training days, object numbers, and actiontypes. Besides, a baseline method by directly recording the operator'sbehaviors is also implemented, and comparison is made on the accuracy of homestate evaluation and the accuracy of robot action selection. The results showthat the proposed approach handles dynamic environment well, and guides therobot's actions in the house more accurately.
arxiv-9900-26 | Unsupervised Dependency Parsing: Let's Use Supervised Parsers | http://arxiv.org/pdf/1504.04666v1.pdf | author:Phong Le, Willem Zuidema category:cs.CL cs.LG published:2015-04-18 summary:We present a self-training approach to unsupervised dependency parsing thatreuses existing supervised and unsupervised parsing algorithms. Our approach,called `iterated reranking' (IR), starts with dependency trees generated by anunsupervised parser, and iteratively improves these trees using the richerprobability models used in supervised parsing that are in turn trained on thesetrees. Our system achieves 1.8% accuracy higher than the state-of-the-partparser of Spitkovsky et al. (2013) on the WSJ corpus.
arxiv-9900-27 | Compositional Distributional Semantics with Long Short Term Memory | http://arxiv.org/pdf/1503.02510v2.pdf | author:Phong Le, Willem Zuidema category:cs.CL cs.AI cs.LG published:2015-03-09 summary:We are proposing an extension of the recursive neural network that makes useof a variant of the long short-term memory architecture. The extension allowsinformation low in parse trees to be stored in a memory register (the `memorycell') and used much later higher up in the parse tree. This provides asolution to the vanishing gradient problem and allows the network to capturelong range dependencies. Experimental results show that our compositionoutperformed the traditional neural-network composition on the StanfordSentiment Treebank.
arxiv-9900-28 | A spectral optical flow method for determining velocities from digital imagery | http://arxiv.org/pdf/1504.04660v1.pdf | author:Neal Hurlburt, Steve Jaffey category:cs.CV astro-ph.IM published:2015-04-17 summary:We present a method for determining surface flows from solar images basedupon optical flow techniques. We apply the method to sets of images obtained bya variety of solar imagers to assess its performance. The {\tt opflow3d}procedure is shown to extract accurate velocity estimates when provided perfecttest data and quickly generates results consistent with completely distinctmethods when applied on global scales. We also validate it in detail bycomparing it to an established method when applied to high-resolution datasetsand find that it provides comparable results without the need to tune, filteror otherwise preprocess the images before its application.
arxiv-9900-29 | Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network | http://arxiv.org/pdf/1504.04658v1.pdf | author:Andrew J. R. Simpson, Gerard Roma, Mark D. Plumbley category:cs.SD cs.LG cs.NE 68Txx published:2015-04-17 summary:Identification and extraction of singing voice from within musical mixturesis a key challenge in source separation and machine audition. Recently, deepneural networks (DNN) have been used to estimate 'ideal' binary masks forcarefully controlled cocktail party speech separation problems. However, it isnot yet known whether these methods are capable of generalizing to thediscrimination of voice and non-voice in the context of musical mixtures. Here,we trained a convolutional DNN (of around a billion parameters) to provideprobabilistic estimates of the ideal binary mask for separation of vocal soundsfrom real-world musical mixtures. We contrast our DNN results with moretraditional linear methods. Our approach may be useful for automatic removal ofvocal sounds from musical mixtures for 'karaoke' type applications.
arxiv-9900-30 | Biometrics for Child Vaccination and Welfare: Persistence of Fingerprint Recognition for Infants and Toddlers | http://arxiv.org/pdf/1504.04651v1.pdf | author:Anil K. Jain, Sunpreet S. Arora, Lacey Best-Rowden, Kai Cao, Prem Sewak Sudhish, Anjoo Bhatnagar category:cs.CV published:2015-04-17 summary:With a number of emerging applications requiring biometric recognition ofchildren (e.g., tracking child vaccination schedules, identifying missingchildren and preventing newborn baby swaps in hospitals), investigating thetemporal stability of biometric recognition accuracy for children is important.The persistence of recognition accuracy of three of the most commonly usedbiometric traits (fingerprints, face and iris) has been investigated foradults. However, persistence of biometric recognition accuracy has not beenstudied systematically for children in the age group of 0-4 years. Given thatvery young children are often uncooperative and do not comprehend or followinstructions, in our opinion, among all biometric modalities, fingerprints arethe most viable for recognizing children. This is primarily because it iseasier to capture fingerprints of young children compared to other biometrictraits, e.g., iris, where a child needs to stare directly towards the camera toinitiate iris capture. In this report, we detail our initiative to investigatethe persistence of fingerprint recognition for children in the age group of 0-4years. Based on preliminary results obtained for the data collected in thefirst phase of our study, use of fingerprints for recognition of 0-4 year-oldchildren appears promising.
arxiv-9900-31 | Performance Evaluation of Machine Learning Algorithms in Post-operative Life Expectancy in the Lung Cancer Patients | http://arxiv.org/pdf/1504.04646v1.pdf | author:Kwetishe Joro Danjuma category:cs.LG published:2015-04-17 summary:The nature of clinical data makes it difficult to quickly select, tune andapply machine learning algorithms to clinical prognosis. As a result, a lot oftime is spent searching for the most appropriate machine learning algorithmsapplicable in clinical prognosis that contains either binary-valued ormulti-valued attributes. The study set out to identify and evaluate theperformance of machine learning classification schemes applied in clinicalprognosis of post-operative life expectancy in the lung cancer patients.Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to trainand test models on Thoracic Surgery datasets obtained from the University ofCalifornia Irvine machine learning repository. Stratified 10-foldcross-validation was used to evaluate baseline performance accuracy of theclassifiers. The comparative analysis shows that multilayer perceptronperformed best with classification accuracy of 82.3%, J48 came out second withclassification accuracy of 81.8%, and Naive Bayes came out the worst withclassification accuracy of 74.4%. The quality and outcome of the chosen machinelearning algorithms depends on the ingenuity of the clinical miner.
arxiv-9900-32 | Convex Learning of Multiple Tasks and their Structure | http://arxiv.org/pdf/1504.03101v2.pdf | author:Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco category:cs.LG published:2015-04-13 summary:Reducing the amount of human supervision is a key problem in machine learningand a natural approach is that of exploiting the relations (structure) amongdifferent tasks. This is the idea at the core of multi-task learning. In thiscontext a fundamental question is how to incorporate the tasks structure in thelearning problem.We tackle this question by studying a general computationalframework that allows to encode a-priori knowledge of the tasks structure inthe form of a convex penalty; in this setting a variety of previously proposedmethods can be recovered as special cases, including linear and non-linearapproaches. Within this framework, we show that tasks and their structure canbe efficiently learned considering a convex optimization problem that can beapproached by means of block coordinate methods such as alternatingminimization and for which we prove convergence to the global minimum.
arxiv-9900-33 | Visual Scene Representations: Contrast, Scaling and Occlusion | http://arxiv.org/pdf/1412.6607v5.pdf | author:Stefano Soatto, Jingming Dong, Nikolaos Karianakis category:cs.CV published:2014-12-20 summary:We study the structure of representations, defined as approximations ofminimal sufficient statistics that are maximal invariants to nuisance factors,for visual data subject to scaling and occlusion of line-of-sight. We deriveanalytical expressions for such representations and show that, under certainrestrictive assumptions, they are related to features commonly in use in thecomputer vision community. This link highlights the condition tacitly assumedby these descriptors, and also suggests ways to improve and generalize them.This new interpretation draws connections to the classical theories ofsampling, hypothesis testing and group invariance.
arxiv-9900-34 | A comparison of dense region detectors for image search and fine-grained classification | http://arxiv.org/pdf/1410.8151v3.pdf | author:Ahmet Iscen, Giorgos Tolias, Philippe-Henri Gosselin, Hervé Jégou category:cs.CV published:2014-10-29 summary:We consider a pipeline for image classification or search based on codingapproaches like Bag of Words or Fisher vectors. In this context, the mostcommon approach is to extract the image patches regularly in a dense manner onseveral scales. This paper proposes and evaluates alternative choices toextract patches densely. Beyond simple strategies derived from regular interestregion detectors, we propose approaches based on super-pixels, edges, and abank of Zernike filters used as detectors. The different approaches areevaluated on recent image retrieval and fine-grain classification benchmarks.Our results show that the regular dense detector is outperformed by othermethods in most situations, leading us to improve the state of the art incomparable setups on standard retrieval and fined-grain benchmarks. As abyproduct of our study, we show that existing methods for blob and super-pixelextraction achieve high accuracy if the patches are extracted along the edgesand not around the detected regions.
arxiv-9900-35 | Testing Closeness With Unequal Sized Samples | http://arxiv.org/pdf/1504.04599v1.pdf | author:Bhaswar B. Bhattacharya, Gregory Valiant category:cs.LG cs.IT math.IT math.ST stat.ML stat.TH published:2015-04-17 summary:We consider the problem of closeness testing for two discrete distributionsin the practically relevant setting of \emph{unequal} sized samples drawn fromeach of them. Specifically, given a target error parameter $\varepsilon > 0$,$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws froman unknown distribution $q$, we describe a test for distinguishing the casethat $p=q$ from the case that $p-q_1 \geq \varepsilon$. If $p$ and $q$ aresupported on at most $n$ elements, then our test is successful with highprobability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 =\Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrtn}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout thisrange, to constant factors. These results extend the recent work of Chan et al.who established the sample complexity when the two samples have equal sizes,and tightens the results of Acharya et al. by polynomials factors in both $n$and $\varepsilon$. As a consequence, we obtain an algorithm for estimating themixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses$\tilde{O}(n^{3/2} \tau_{mix})$ queries to a "next node" oracle, improving uponthe $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, wenote that the core of our testing algorithm is a relatively simple statisticthat seems to perform well in practice, both on synthetic data and on naturallanguage data.
arxiv-9900-36 | The Nataf-Beta Random Field Classifier: An Extension of the Beta Conjugate Prior to Classification Problems | http://arxiv.org/pdf/1504.04588v1.pdf | author:James-A. Goulet category:cs.LG I.5.2 published:2015-04-17 summary:This paper presents the Nataf-Beta Random Field Classifier, a discriminativeapproach that extends the applicability of the Beta conjugate prior toclassification problems. The approach's key feature is to model the probabilityof a class conditional on attribute values as a random field whose marginalsare Beta distributed, and where the parameters of marginals are themselvesdescribed by random fields. Although the classification accuracy of theapproach proposed does not statistically outperform the best accuraciesreported in the literature, it ranks among the top tier for the six benchmarkdatasets tested. The Nataf-Beta Random Field Classifier is suited as a generalpurpose classification approach for real-continuous and real-integer attributevalue problems.
arxiv-9900-37 | GReTA - a novel Global and Recursive Tracking Algorithm in three dimensions | http://arxiv.org/pdf/1305.1495v3.pdf | author:Alessandro Attanasi, Andrea Cavagna, Lorenzo Del Castello, Irene Giardina, Asja Jelic, Stefania Melillo, Leonardo Parisi, Fabio Pellacini, Edward Shen, Edmondo Silvestri, Massimiliano Viale category:q-bio.QM cs.CV published:2013-05-07 summary:Tracking multiple moving targets allows quantitative measure of the dynamicbehavior in systems as diverse as animal groups in biology, turbulence in fluiddynamics and crowd and traffic control. In three dimensions, tracking severaltargets becomes increasingly hard since optical occlusions are very likely,i.e. two featureless targets frequently overlap for several frames. Occlusionsare particularly frequent in biological groups such as bird flocks, fishschools, and insect swarms, a fact that has severely limited collective animalbehavior field studies in the past. This paper presents a 3D tracking methodthat is robust in the case of severe occlusions. To ensure robustness, we adopta global optimization approach that works on all objects and frames at once. Toachieve practicality and scalability, we employ a divide and conquerformulation, thanks to which the computational complexity of the problem isreduced by orders of magnitude. We tested our algorithm with synthetic data,with experimental data of bird flocks and insect swarms and with publicbenchmark datasets, and show that our system yields high quality trajectoriesfor hundreds of moving targets with severe overlap. The results obtained onvery heterogeneous data show the potential applicability of our method to themost diverse experimental situations.
arxiv-9900-38 | Color Constancy Using CNNs | http://arxiv.org/pdf/1504.04548v1.pdf | author:Simone Bianco, Claudio Cusano, Raimondo Schettini category:cs.CV published:2015-04-17 summary:In this work we describe a Convolutional Neural Network (CNN) to accuratelypredict the scene illumination. Taking image patches as input, the CNN works inthe spatial domain without using hand-crafted features that are employed bymost previous methods. The network consists of one convolutional layer with maxpooling, one fully connected layer and three output nodes. Within the networkstructure, feature learning and regression are integrated into one optimizationprocess, which leads to a more effective model for estimating sceneillumination. This approach achieves state-of-the-art performance on a standarddataset of RAW images. Preliminary experiments on images with spatially varyingillumination demonstrate the stability of the local illuminant estimationability of our CNN.
arxiv-9900-39 | Hyperspectral pansharpening: a review | http://arxiv.org/pdf/1504.04531v1.pdf | author:Laetitia Loncan, Luis B. Almeida, José M. Bioucas-Dias, Xavier Briottet, Jocelyn Chanussot, Nicolas Dobigeon, Sophie Fabre, Wenzhi Liao, Giorgio A. Licciardi, Miguel Simões, Jean-Yves Tourneret, Miguel A. Veganzones, Gemine Vivone, Qi Wei, Naoto Yokoya category:cs.CV stat.AP published:2015-04-17 summary:Pansharpening aims at fusing a panchromatic image with a multispectral one,to generate an image with the high spatial resolution of the former and thehigh spectral resolution of the latter. In the last decade, many algorithmshave been presented in the literature for pansharpening using multispectraldata. With the increasing availability of hyperspectral systems, these methodsare now being adapted to hyperspectral images. In this work, we compare newpansharpening techniques designed for hyperspectral data with some of the stateof the art methods for multispectral pansharpening, which have been adapted forhyperspectral data. Eleven methods from different classes (componentsubstitution, multiresolution analysis, hybrid, Bayesian and matrixfactorization) are analyzed. These methods are applied to three datasets andtheir effectiveness and robustness are evaluated with widely used performanceindicators. In addition, all the pansharpening techniques considered in thispaper have been implemented in a MATLAB toolbox that is made available to thecommunity.
arxiv-9900-40 | Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings | http://arxiv.org/pdf/1503.08195v2.pdf | author:Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Krüger category:math.ST stat.ME stat.ML stat.TH published:2015-03-27 summary:In the practice of point prediction, it is desirable that forecasters receivea directive in the form of a statistical functional, such as the mean or aquantile of the predictive distribution. When evaluating and comparingcompeting forecasts, it is then critical that the scoring function used forthese purposes be consistent for the functional at hand, in the sense that theexpected score is minimized when following the directive. We show that any scoring function that is consistent for a quantile or anexpectile functional, respectively, can be represented as a mixture of extremalscoring functions that form a linearly parameterized family. Scoring functionsfor the mean value and probability forecasts of binary events constituteimportant examples. The quantile and expectile functionals along with therespective extremal scoring functions admit appealing economic interpretationsin terms of thresholds in decision making. The Choquet type mixture representations give rise to simple checks ofwhether a forecast dominates another in the sense that it is preferable underany consistent scoring function. In empirical settings it suffices to comparethe average scores for only a finite number of extremal elements. Plots of theaverage scores with respect to the extremal scoring functions, which we callMurphy diagrams, permit detailed comparisons of the relative merits ofcompeting forecasts.
arxiv-9900-41 | Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable | http://arxiv.org/pdf/1408.5661v3.pdf | author:Keisuke Yamazaki category:stat.ML cs.LG published:2014-08-25 summary:In data science and machine learning, hierarchical parametric models, such asmixture models, are often used. They contain two kinds of variables: observablevariables, which represent the parts of the data that can be directly measured,and latent variables, which represent the underlying processes that generatethe data. Although there has been an increase in research on the estimationaccuracy for observable variables, the theoretical analysis of estimatinglatent variables has not been thoroughly investigated. In a previous study, wedetermined the accuracy of a Bayes estimation for the joint probability of thelatent variables in a dataset, and we proved that the Bayes method isasymptotically more accurate than the maximum-likelihood method. However, theaccuracy of the Bayes estimation for a single latent variable remains unknown.In the present paper, we derive the asymptotic expansions of the errorfunctions, which are defined by the Kullback-Leibler divergence, for two typesof single-variable estimations when the statistical regularity is satisfied.Our results indicate that the accuracies of the Bayes and maximum-likelihoodmethods are asymptotically equivalent and clarify that the Bayes method is onlyadvantageous for multivariable estimations.
arxiv-9900-42 | VIP: Finding Important People in Images | http://arxiv.org/pdf/1502.05678v2.pdf | author:Clint Solomon Mathialagan, Andrew C. Gallagher, Dhruv Batra category:cs.CV published:2015-02-19 summary:People preserve memories of events such as birthdays, weddings, or vacationsby capturing photos, often depicting groups of people. Invariably, someindividuals in the image are more important than others given the context ofthe event. This paper analyzes the concept of the importance of individuals ingroup photographs. We address two specific questions -- Given an image, who arethe most important individuals in it? Given multiple images of a person, whichimage depicts the person in the most important role? We introduce a measure ofimportance of people in images and investigate the correlation betweenimportance and visual saliency. We find that not only can we automaticallypredict the importance of people from purely visual cues, incorporating thispredicted importance results in significant improvement in applications such asim2text (generating sentences that describe images of groups of people).
arxiv-9900-43 | Feasibility Preserving Constraint-Handling Strategies for Real Parameter Evolutionary Optimization | http://arxiv.org/pdf/1504.04421v1.pdf | author:Nikhil Padhye, Pulkit Mittal, Kalyanmoy Deb category:cs.NE published:2015-04-17 summary:Evolutionary Algorithms (EAs) are being routinely applied for a variety ofoptimization tasks, and real-parameter optimization in the presence ofconstraints is one such important area. During constrained optimization EAsoften create solutions that fall outside the feasible region; hence a viableconstraint- handling strategy is needed. This paper focuses on the class ofconstraint-handling strategies that repair infeasible solutions by bringingthem back into the search space and explicitly preserve feasibility of thesolutions. Several existing constraint-handling strategies are studied, and twonew single parameter constraint-handling methodologies based on parent-centricand inverse parabolic probability (IP) distribution are proposed. The existingand newly proposed constraint-handling methods are first studied with PSO, DE,GAs, and simulation results on four scalable test-problems under differentlocation settings of the optimum are presented. The newly proposedconstraint-handling methods exhibit robustness in terms of performance and alsosucceed on search spaces comprising up-to 500 variables while locating theoptimum within an error of 10$^{-10}$. The working principle of the IP basedmethods is also demonstrated on (i) some generic constrained optimizationproblems, and (ii) a classic `Weld' problem from structural design andmechanics. The successful performance of the proposed methods clearly exhibitstheir efficacy as a generic constrained-handling strategy for a wide range ofapplications.
arxiv-9900-44 | An Empirical Evaluation of Deep Learning on Highway Driving | http://arxiv.org/pdf/1504.01716v3.pdf | author:Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Mujica, Adam Coates, Andrew Y. Ng category:cs.RO cs.CV published:2015-04-07 summary:Numerous groups have applied a variety of deep learning techniques tocomputer vision problems in highway perception scenarios. In this paper, wepresented a number of empirical evaluations of recent deep learning advances.Computer vision, combined with deep learning, has the potential to bring abouta relatively inexpensive, robust solution to autonomous driving. To preparedeep learning for industry uptake and practical applications, neural networkswill require large data sets that represent all possible driving environmentsand scenarios. We collect a large data set of highway data and apply deeplearning and computer vision algorithms to problems such as car and lanedetection. We show how existing convolutional neural networks (CNNs) can beused to perform lane and vehicle detection while running at frame ratesrequired for a real-time system. Our results lend credence to the hypothesisthat deep learning holds promise for autonomous driving.
arxiv-9900-45 | Random Forests Can Hash | http://arxiv.org/pdf/1412.5083v3.pdf | author:Qiang Qiu, Guillermo Sapiro, Alex Bronstein category:cs.CV cs.IR cs.LG stat.ML published:2014-12-16 summary:Hash codes are a very efficient data representation needed to be able to copewith the ever growing amounts of data. We introduce a random forest semantichashing scheme with information-theoretic code aggregation, showing for thefirst time how random forest, a technique that together with deep learning haveshown spectacular results in classification, can also be extended tolarge-scale retrieval. Traditional random forest fails to enforce theconsistency of hashes generated from each tree for the same class data, i.e.,to preserve the underlying similarity, and it also lacks a principled way forcode aggregation across trees. We start with a simple hashing scheme, whereindependently trained random trees in a forest are acting as hashing functions.We the propose a subspace model as the splitting function, and show that itenforces the hash consistency in a tree for data from the same class. We alsointroduce an information-theoretic approach for aggregating codes of individualtrees into a single hash code, producing a near-optimal unique hash for eachclass. Experiments on large-scale public datasets are presented, showing thatthe proposed approach significantly outperforms state-of-the-art hashingmethods for retrieval tasks.
arxiv-9900-46 | Learning Longer Memory in Recurrent Neural Networks | http://arxiv.org/pdf/1412.7753v2.pdf | author:Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato category:cs.NE cs.LG published:2014-12-24 summary:Recurrent neural network is a powerful model that learns temporal patterns insequential data. For a long time, it was believed that recurrent networks aredifficult to train using simple optimizers, such as stochastic gradientdescent, due to the so-called vanishing gradient problem. In this paper, weshow that learning longer term patterns in real data, such as in naturallanguage, is perfectly possible using gradient descent. This is achieved byusing a slight structural modification of the simple recurrent neural networkarchitecture. We encourage some of the hidden units to change their stateslowly by making part of the recurrent weight matrix close to identity, thusforming kind of a longer term memory. We evaluate our model in languagemodeling experiments, where we obtain similar performance to the much morecomplex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber,1997).
arxiv-9900-47 | Non-Uniform Stochastic Average Gradient Method for Training Conditional Random Fields | http://arxiv.org/pdf/1504.04406v1.pdf | author:Mark Schmidt, Reza Babanezhad, Mohamed Osama Ahmed, Aaron Defazio, Ann Clifton, Anoop Sarkar category:stat.ML cs.LG math.OC stat.CO published:2015-04-16 summary:We apply stochastic average gradient (SAG) algorithms for trainingconditional random fields (CRFs). We describe a practical implementation thatuses structure in the CRF gradient to reduce the memory requirement of thislinearly-convergent stochastic gradient method, propose a non-uniform samplingscheme that substantially improves practical performance, and analyze the rateof convergence of the SAGA variant under non-uniform sampling. Our experimentalresults reveal that our method often significantly outperforms existing methodsin terms of the training objective, and performs as well or better thanoptimally-tuned stochastic gradient methods in terms of test error.
arxiv-9900-48 | Breaking the News: First Impressions Matter on Online News | http://arxiv.org/pdf/1503.07921v2.pdf | author:Julio Reis, Fabrıcio Benevenuto, Pedro O. S. Vaz de Melo, Raquel Prates, Haewoon Kwak, Jisun An category:cs.CY cs.CL published:2015-03-26 summary:A growing number of people are changing the way they consume news, replacingthe traditional physical newspapers and magazines by their virtual onlineversions or/and weblogs. The interactivity and immediacy present in online newsare changing the way news are being produced and exposed by media corporations.News websites have to create effective strategies to catch people's attentionand attract their clicks. In this paper we investigate possible strategies usedby online news corporations in the design of their news headlines. We analyzethe content of 69,907 headlines produced by four major global mediacorporations during a minimum of eight consecutive months in 2014. In order todiscover strategies that could be used to attract clicks, we extracted featuresfrom the text of the news headlines related to the sentiment polarity of theheadline. We discovered that the sentiment of the headline is strongly relatedto the popularity of the news and also with the dynamics of the posted commentson that particular news.
arxiv-9900-49 | On Learning Vector Representations in Hierarchical Label Spaces | http://arxiv.org/pdf/1412.6881v3.pdf | author:Jinseok Nam, Johannes Fürnkranz category:cs.LG cs.CL stat.ML published:2014-12-22 summary:An important problem in multi-label classification is to capture labelpatterns or underlying structures that have an impact on such patterns. Thispaper addresses one such problem, namely how to exploit hierarchical structuresover labels. We present a novel method to learn vector representations of alabel space given a hierarchy of labels and label co-occurrence patterns. Ourexperimental results demonstrate qualitatively that the proposed method is ableto learn regularities among labels by exploiting a label hierarchy as well aslabel co-occurrences. It highlights the importance of the hierarchicalinformation in order to obtain regularities which facilitate analogicalreasoning over a label space. We also experimentally illustrate the dependencyof the learned representations on the label hierarchy.
arxiv-9900-50 | In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning | http://arxiv.org/pdf/1412.6614v4.pdf | author:Behnam Neyshabur, Ryota Tomioka, Nathan Srebro category:cs.LG cs.AI cs.CV stat.ML published:2014-12-20 summary:We present experiments demonstrating that some other form of capacitycontrol, different from network size, plays a central role in learningmultilayer feed-forward networks. We argue, partially through analogy to matrixfactorization, that this is an inductive bias that can help shed light on deeplearning.
arxiv-9900-51 | High-performance Kernel Machines with Implicit Distributed Optimization and Randomization | http://arxiv.org/pdf/1409.0940v3.pdf | author:Vikas Sindhwani, Haim Avron category:stat.ML cs.DC cs.LG published:2014-09-03 summary:In order to fully utilize "big data", it is often required to use "bigmodels". Such models tend to grow with the complexity and size of the trainingdata, and do not make strong parametric assumptions upfront on the nature ofthe underlying statistical dependencies. Kernel methods fit this need well, asthey constitute a versatile and principled statistical methodology for solvinga wide range of non-parametric modelling problems. However, their highcomputational costs (in storage and time) pose a significant barrier to theirwidespread adoption in big data applications. We propose an algorithmic framework and high-performance implementation formassive-scale training of kernel-based statistical models, based on combiningtwo key technical ingredients: (i) distributed general purpose convexoptimization, and (ii) the use of randomization to improve the scalability ofkernel methods. Our approach is based on a block-splitting variant of theAlternating Directions Method of Multipliers, carefully reconfigured to handlevery large random feature matrices, while exploiting hybrid parallelismtypically found in modern clusters of multicore machines. Our implementationsupports a variety of statistical learning tasks by enabling several lossfunctions, regularization schemes, kernels, and layers of randomizedapproximations for both dense and sparse datasets, in a highly extensibleframework. We evaluate the ability of our framework to learn models on datafrom applications, and provide a comparison against existing sequential andparallel libraries.
arxiv-9900-52 | Towards a relation extraction framework for cyber-security concepts | http://arxiv.org/pdf/1504.04317v1.pdf | author:Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall category:cs.IR cs.CL cs.CR H.3.3 published:2015-04-16 summary:In order to assist security analysts in obtaining information pertaining totheir network, such as novel vulnerabilities, exploits, or patches, informationretrieval methods tailored to the security domain are needed. As labeled textdata is scarce and expensive, we follow developments in semi-supervised NaturalLanguage Processing and implement a bootstrapping algorithm for extractingsecurity entities and their relationships from text. The algorithm requireslittle input data, specifically, a few relations or patterns (heuristics foridentifying relations), and incorporates an active learning component whichqueries the user on the most important decisions to prevent drifting from thedesired relations. Preliminary testing on a small corpus shows promisingresults, obtaining precision of .82.
arxiv-9900-53 | Reweighted Wake-Sleep | http://arxiv.org/pdf/1406.2751v4.pdf | author:Jörg Bornschein, Yoshua Bengio category:cs.LG published:2014-06-11 summary:Training deep directed graphical models with many hidden variables andperforming inference remains a major challenge. Helmholtz machines and deepbelief networks are such models, and the wake-sleep algorithm has been proposedto train them. The wake-sleep algorithm relies on training not just thedirected generative model but also a conditional generative model (theinference network) that runs backward from visible to latent, estimating theposterior distribution of latent given visible. We propose a novelinterpretation of the wake-sleep algorithm which suggests that betterestimators of the gradient can be obtained by sampling latent variablesmultiple times from the inference network. This view is based on importancesampling as an estimator of the likelihood, with the approximate inferencenetwork as a proposal distribution. This interpretation is confirmedexperimentally, showing that better likelihood can be achieved with thisreweighted wake-sleep procedure. Based on this interpretation, we propose thata sigmoidal belief network is not sufficiently powerful for the layers of theinference network in order to recover a good estimator of the posteriordistribution of latent variables. Our experiments show that using a morepowerful layer model, such as NADE, yields substantially better generativemodels.
arxiv-9900-54 | Metric Localization using Google Street View | http://arxiv.org/pdf/1503.04287v2.pdf | author:Pratik Agarwal, Wolfram Burgard, Luciano Spinello category:cs.RO cs.CV published:2015-03-14 summary:Accurate metrical localization is one of the central challenges in mobilerobotics. Many existing methods aim at localizing after building a map with therobot. In this paper, we present a novel approach that instead uses geotaggedpanoramas from the Google Street View as a source of global positioning. Wemodel the problem of localization as a non-linear least squares estimation intwo phases. The first estimates the 3D position of tracked feature points fromshort monocular camera sequences. The second computes the rigid bodytransformation between the Street View panoramas and the estimated points. Theonly input of this approach is a stream of monocular camera images and odometryestimates. We quantified the accuracy of the method by running the approach ona robotic platform in a parking lot by using visual fiducials as ground truth.Additionally, we applied the approach in the context of personal localizationin a real urban scenario by using data from a Google Tango tablet.
arxiv-9900-55 | Predictive Encoding of Contextual Relationships for Perceptual Inference, Interpolation and Prediction | http://arxiv.org/pdf/1411.3815v6.pdf | author:Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee category:cs.LG cs.CV cs.NE published:2014-11-14 summary:We propose a new neurally-inspired model that can learn to encode the globalrelationship context of visual events across time and space and to use thecontextual information to modulate the analysis by synthesis process in apredictive coding framework. The model learns latent contextual representationsby maximizing the predictability of visual events based on local and globalcontextual information through both top-down and bottom-up processes. Incontrast to standard predictive coding models, the prediction error in thismodel is used to update the contextual representation but does not alter thefeedforward input for the next layer, and is thus more consistent withneurophysiological observations. We establish the computational feasibility ofthis model by demonstrating its ability in several aspects. We show that ourmodel can outperform state-of-art performances of gated Boltzmann machines(GBM) in estimation of contextual information. Our model can also interpolatemissing events or predict future events in image sequences while simultaneouslyestimating contextual information. We show it achieves state-of-artperformances in terms of prediction accuracy in a variety of tasks andpossesses the ability to interpolate missing frames, a function that is lackingin GBM.
arxiv-9900-56 | Image Specificity | http://arxiv.org/pdf/1502.04569v2.pdf | author:Mainak Jas, Devi Parikh category:cs.CV published:2015-02-16 summary:For some images, descriptions written by multiple people are consistent witheach other. But for other images, descriptions across people vary considerably.In other words, some images are specific $-$ they elicit consistentdescriptions from different people $-$ while other images are ambiguous.Applications involving images and text can benefit from an understanding ofwhich images are specific and which ones are ambiguous. For instance, considertext-based image retrieval. If a query description is moderately similar to thecaption (or reference description) of an ambiguous image, that query may beconsidered a decent match to the image. But if the image is very specific, amoderate similarity between the query and the reference description may not besufficient to retrieve the image. In this paper, we introduce the notion of image specificity. We present twomechanisms to measure specificity given multiple descriptions of an image: anautomated measure and a measure that relies on human judgement. We analyzeimage specificity with respect to image content and properties to betterunderstand what makes an image specific. We then train models to automaticallypredict the specificity of an image from image features alone without requiringtextual descriptions of the image. Finally, we show that modeling imagespecificity leads to improvements in a text-based image retrieval application.
arxiv-9900-57 | Purine: A bi-graph based deep learning framework | http://arxiv.org/pdf/1412.6249v5.pdf | author:Min Lin, Shuo Li, Xuan Luo, Shuicheng Yan category:cs.NE cs.LG published:2014-12-19 summary:In this paper, we introduce a novel deep learning framework, termed Purine.In Purine, a deep network is expressed as a bipartite graph (bi-graph), whichis composed of interconnected operators and data tensors. With the bi-graphabstraction, networks are easily solvable with event-driven task dispatcher. Wethen demonstrate that different parallelism schemes over GPUs and/or CPUs onsingle or multiple PCs can be universally implemented by graph composition.This eases researchers from coding for various parallelization schemes, and thesame dispatcher can be used for solving variant graphs. Scheduled by the taskdispatcher, memory transfers are fully overlapped with other computations,which greatly reduce the communication overhead and help us achieve approximatelinear acceleration.
arxiv-9900-58 | Genetic algorithm implementation for effective document subject search | http://arxiv.org/pdf/1504.04216v1.pdf | author:V. K. Ivanov, P. I. Meskin category:cs.IR cs.NE published:2015-04-16 summary:This paper describes the software implementation of genetic algorithm foridentifying and selecting most relevant results received during sequentiallyexecuted subject search operations. Simulated evolutionary process generatessustainable and effective population of search queries, forms search pattern ofdocuments or semantic core, creates relevant sets of required documents, allowsautomatic classification of search results. The paper discusses the features ofsubject search, justifies the use of a genetic algorithm, describes argumentsof the fitness function and describes basic steps and parameters of thealgorithm.
arxiv-9900-59 | Face Prediction Model for an Automatic Age-invariant Face Recognition System | http://arxiv.org/pdf/1506.06046v1.pdf | author:Poonam Yadav category:cs.CV cs.NE published:2015-04-16 summary:Automated face recognition and identification softwares are becoming part ofour daily life; it finds its abode not only with Facebook's auto photo tagging,Apple's iPhoto, Google's Picasa, Microsoft's Kinect, but also in HomelandSecurity Department's dedicated biometric face detection systems. Most of theseautomatic face identification systems fail where the effects of aging come intothe picture. Little work exists in the literature on the subject of faceprediction that accounts for aging, which is a vital part of the computer facerecognition systems. In recent years, individual face components' (e.g. eyes,nose, mouth) features based matching algorithms have emerged, but theseapproaches are still not efficient. Therefore, in this work we describe a FacePrediction Model (FPM), which predicts human face aging or growth related imagevariation using Principle Component Analysis (PCA) and Artificial NeuralNetwork (ANN) learning techniques. The FPM captures the facial changes, whichoccur with human aging and predicts the facial image with a few years of gapwith an acceptable accuracy of face matching from 76 to 86%.
arxiv-9900-60 | Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition | http://arxiv.org/pdf/1503.01224v2.pdf | author:Peng Wang, Yuanzhouhan Cao, Chunhua Shen, Lingqiao Liu, Heng Tao Shen category:cs.CV published:2015-03-04 summary:Encouraged by the success of Convolutional Neural Networks (CNNs) in imageclassification, recently much effort is spent on applying CNNs to video basedaction recognition problems. One challenge is that video contains a varyingnumber of frames which is incompatible to the standard input format of CNNs.Existing methods handle this issue either by directly sampling a fixed numberof frames or bypassing this issue by introducing a 3D convolutional layer whichconducts convolution in spatial-temporal domain. To solve this issue, here we propose a novel network structure which allowsan arbitrary number of frames as the network input. The key of our solution isto introduce a module consisting of an encoding layer and a temporal pyramidpooling layer. The encoding layer maps the activation from previous layers to afeature vector suitable for pooling while the temporal pyramid pooling layerconverts multiple frame-level activations into a fixed-length video-levelrepresentation. In addition, we adopt a feature concatenation layer whichcombines appearance information and motion information. Compared with the framesampling strategy, our method avoids the risk of missing any important frames.Compared with the 3D convolutional method which requires a huge video datasetfor network training, our model can be learned on a small target datasetbecause we can leverage the off-the-shelf image-level CNN for model parameterinitialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,demonstrate that our method achieves superior performance over state-of-the-artmethods while requiring much fewer training data.
arxiv-9900-61 | Multichannel sparse recovery of complex-valued signals using Huber's criterion | http://arxiv.org/pdf/1504.04184v1.pdf | author:Esa Ollila category:cs.IT math.IT stat.CO stat.ML published:2015-04-16 summary:In this paper, we generalize Huber's criterion to multichannel sparserecovery problem of complex-valued measurements where the objective is to findgood recovery of jointly sparse unknown signal vectors from the given multiplemeasurement vectors which are different linear combinations of the same knownelementary vectors. This requires careful characterization of robustcomplex-valued loss functions as well as Huber's criterion function for themultivariate sparse regression problem. We devise a greedy algorithm based onsimultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlikethe conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, isrobust under heavy-tailed non-Gaussian noise conditions, yet has a negligibleperformance loss compared to SNIHT under Gaussian noise. Usefulness of themethod is illustrated in source localization application with sensor arrays.
arxiv-9900-62 | Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations | http://arxiv.org/pdf/1412.6418v3.pdf | author:Ivan Titov, Ehsan Khoddam category:cs.CL cs.LG stat.ML published:2014-12-19 summary:In this work, we propose a new method to integrate two recent lines of work:unsupervised induction of shallow semantics (e.g., semantic roles) andfactorization of relations in text and knowledge bases. Our model consists oftwo components: (1) an encoding component: a semantic role labeling model whichpredicts roles given a rich set of syntactic and lexical features; (2) areconstruction component: a tensor factorization model which relies on roles topredict argument fillers. When the components are estimated jointly to minimizeerrors in argument reconstruction, the induced roles largely correspond toroles defined in annotated resources. Our method performs on par with mostaccurate role induction methods on English, even though, unlike these previousapproaches, we do not incorporate any prior linguistic knowledge about thelanguage.
arxiv-9900-63 | Learning linearly separable features for speech recognition using convolutional neural networks | http://arxiv.org/pdf/1412.7110v6.pdf | author:Dimitri Palaz, Mathew Magimai Doss, Ronan Collobert category:cs.LG cs.CL cs.NE published:2014-12-22 summary:Automatic speech recognition systems usually rely on spectral-based features,such as MFCC of PLP. These features are extracted based on prior knowledge suchas, speech perception or/and speech production. Recently, convolutional neuralnetworks have been shown to be able to estimate phoneme conditionalprobabilities in a completely data-driven manner, i.e. using directly temporalraw speech signal as input. This system was shown to yield similar or betterperformance than HMM/ANN based system on phoneme recognition task and on largescale continuous speech recognition task, using less parameters. Motivated bythese studies, we investigate the use of simple linear classifier in theCNN-based framework. Thus, the network learns linearly separable features fromraw speech. We show that such system yields similar or better performance thanMLP based system using cepstral-based features as input.
arxiv-9900-64 | Actively Learning to Attract Followers on Twitter | http://arxiv.org/pdf/1504.04114v1.pdf | author:Nir Levine, Timothy A. Mann, Shie Mannor category:stat.ML cs.LG cs.SI published:2015-04-16 summary:Twitter, a popular social network, presents great opportunities for on-linemachine learning research. However, previous research has focused almostentirely on learning from passively collected data. We study the problem oflearning to acquire followers through normative user behavior, as opposed tothe mass following policies applied by many bots. We formalize the problem as acontextual bandit problem, in which we consider retweeting content to be theaction chosen and each tweet (content) is accompanied by context. We designreward signals based on the change in followers. The result of our month longexperiment with 60 agents suggests that (1) aggregating experience acrossagents can adversely impact prediction accuracy and (2) the Twitter community'sresponse to different actions is non-stationary. Our findings suggest thatactively learning on-line can provide deeper insights about how to attractfollowers than machine learning over passively collected data alone.
arxiv-9900-65 | Faster Algorithms for Testing under Conditional Sampling | http://arxiv.org/pdf/1504.04103v1.pdf | author:Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, Venkatadheeraj Pichapathi, Ananda Theertha Suresh category:cs.DS cs.CC cs.LG math.ST stat.TH published:2015-04-16 summary:There has been considerable recent interest in distribution-tests whoserun-time and sample requirements are sublinear in the domain-size $k$. We studytwo of the most important tests under the conditional-sampling model where eachquery specifies a subset $S$ of the domain, and the response is a sample drawnfrom $S$ according to the underlying distribution. For identity testing, which asks whether the underlying distribution equals aspecific given distribution or $\epsilon$-differs from it, we reduce the knowntime and sample complexities from $\tilde{\mathcal{O}}(\epsilon^{-4})$ to$\tilde{\mathcal{O}}(\epsilon^{-2})$, thereby matching the informationtheoretic lower bound. For closeness testing, which asks whether twodistributions underlying observed data sets are equal or different, we reduceexisting complexity from $\tilde{\mathcal{O}}(\epsilon^{-4} \log^5 k)$ to aneven sub-logarithmic $\tilde{\mathcal{O}}(\epsilon^{-5} \log \log k)$ thusproviding a better bound to an open problem in Bertinoro Workshop on SublinearAlgorithms [Fisher, 2004].
arxiv-9900-66 | Bayesian Robust Tensor Factorization for Incomplete Multiway Data | http://arxiv.org/pdf/1410.2386v2.pdf | author:Qibin Zhao, Guoxu Zhou, Liqing Zhang, Andrzej Cichocki, Shun-ichi Amari category:cs.CV cs.LG published:2014-10-09 summary:We propose a generative model for robust tensor factorization in the presenceof both missing data and outliers. The objective is to explicitly infer theunderlying low-CP-rank tensor capturing the global information and a sparsetensor capturing the local information (also considered as outliers), thusproviding the robust predictive distribution over missing entries. Thelow-CP-rank tensor is modeled by multilinear interactions between multiplelatent factors on which the column sparsity is enforced by a hierarchicalprior, while the sparse tensor is modeled by a hierarchical view of Student-$t$distribution that associates an individual hyperparameter with each elementindependently. For model learning, we develop an efficient closed-formvariational inference under a fully Bayesian treatment, which can effectivelyprevent the overfitting problem and scales linearly with data size. In contrastto existing related works, our method can perform model selection automaticallyand implicitly without need of tuning parameters. More specifically, it candiscover the groundtruth of CP rank and automatically adapt the sparsityinducing priors to various types of outliers. In addition, the tradeoff betweenthe low-rank approximation and the sparse representation can be optimized inthe sense of maximum model evidence. The extensive experiments and comparisonswith many state-of-the-art algorithms on both synthetic and real-world datasetsdemonstrate the superiorities of our method from several perspectives.
arxiv-9900-67 | Comparisons of wavelet functions in QRS signal to noise ratio enhancement and detection accuracy | http://arxiv.org/pdf/1504.03834v2.pdf | author:Pornchai Phukpattaranont category:cs.CV cs.CE published:2015-04-15 summary:We compare the capability of wavelet functions used for noise removal inpreprocessing step of a QRS detection algorithm in the electrocardiogram (ECG)signal. The QRS signal to noise ratio enhancement and the detection accuracy ofeach wavelet function are evaluated using three measures: (1) the ratio of themaximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean ofabsolute of time error (MATE), and (3) the figure of merit (FOM). Three waveletfunctions from previous well-known publications are explored, i.e., Bior1.3,Db10, and Mexican hat wavelet functions. Results evaluated with the ECG signalfrom MIT-BIH arrhythmia database show that the Mexican hat wavelet function isbetter than the others. While the scale 8 of Mexican hat wavelet function canprovide the best enhancement in QRS signal to noise ratio, the scale 4 ofMexican hat wavelet function can provide the best detection accuracy. Theseresults may be combined and may enable the use of a single fixed threshold forall ECG records leading to the reduction in computational complexity of the QRSdetection algorithm.
arxiv-9900-68 | Segmentation of Subspaces in Sequential Data | http://arxiv.org/pdf/1504.04090v1.pdf | author:Stephen Tierney, Yi Guo, Junbin Gao category:cs.CV published:2015-04-16 summary:We propose Ordered Subspace Clustering (OSC) to segment data drawn from asequentially ordered union of subspaces. Similar to Sparse Subspace Clustering(SSC) we formulate the problem as one of finding a sparse representation butinclude an additional penalty term to take care of sequential data. We test ourmethod on data drawn from infrared hyper spectral, video and motion capturedata. Experiments show that our method, OSC, outperforms the state of the artmethods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR)and SSC.
arxiv-9900-69 | FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave Infrared | http://arxiv.org/pdf/1504.04085v1.pdf | author:Huaijin Chen, M. Salman Asif, Aswin C. Sankaranarayanan, Ashok Veeraraghavan category:cs.CV published:2015-04-16 summary:Cameras for imaging in short and mid-wave infrared spectra are significantlymore expensive than their counterparts in visible imaging. As a result,high-resolution imaging in those spectrum remains beyond the reach of mostconsumers. Over the last decade, compressive sensing (CS) has emerged as apotential means to realize inexpensive short-wave infrared cameras. Oneapproach for doing this is the single-pixel camera (SPC) where a singledetector acquires coded measurements of a high-resolution image. Acomputational reconstruction algorithm is then used to recover the image fromthese coded measurements. Unfortunately, the measurement rate of a SPC isinsufficient to enable imaging at high spatial and temporal resolutions. We present a focal plane array-based compressive sensing (FPA-CS)architecture that achieves high spatial and temporal resolutions. The idea isto use an array of SPCs that sense in parallel to increase the measurementrate, and consequently, the achievable spatio-temporal resolution of thecamera. We develop a proof-of-concept prototype in the short-wave infraredusing a sensor with 64$\times$ 64 pixels; the prototype provides a 4096$\times$increase in the measurement rate compared to the SPC and achieves a megapixelresolution at video rate using CS techniques.
arxiv-9900-70 | Unsupervised Domain Adaptation with Feature Embeddings | http://arxiv.org/pdf/1412.4385v3.pdf | author:Yi Yang, Jacob Eisenstein category:cs.CL cs.LG published:2014-12-14 summary:Representation learning is the dominant technique for unsupervised domainadaptation, but existing approaches often require the specification of "pivotfeatures" that generalize across domains, which are selected by task-specificheuristics. We show that a novel but simple feature embedding approach providesbetter performance, by exploiting the feature template structure common in NLPproblems.
arxiv-9900-71 | Unsupervised Feature Learning from Temporal Data | http://arxiv.org/pdf/1504.02518v2.pdf | author:Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun category:cs.CV cs.LG published:2015-04-09 summary:Current state-of-the-art classification and detection algorithms rely onsupervised training. In this work we study unsupervised feature learning in thecontext of temporally coherent video data. We focus on feature learning fromunlabeled video data, using the assumption that adjacent video frames containsemantically similar information. This assumption is exploited to train aconvolutional pooling auto-encoder regularized by slowness and sparsity. Weestablish a connection between slow feature learning to metric learning andshow that the trained encoder can be used to define a more temporally andsemantically coherent metric.
arxiv-9900-72 | A Generative Model for Deep Convolutional Learning | http://arxiv.org/pdf/1504.04054v1.pdf | author:Yunchen Pu, Xin Yuan, Lawrence Carin category:stat.ML cs.LG cs.NE published:2015-04-15 summary:A generative model is developed for deep (multi-layered) convolutionaldictionary learning. A novel probabilistic pooling operation is integrated intothe deep model, yielding efficient bottom-up (pretraining) and top-down(refinement) probabilistic learning. Experimental results demonstrate powerfulcapabilities of the model to learn multi-layer features from images, andexcellent classification results are obtained on the MNIST and Caltech 101datasets.
arxiv-9900-73 | Diverse Embedding Neural Network Language Models | http://arxiv.org/pdf/1412.7063v5.pdf | author:Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran category:cs.CL cs.LG cs.NE published:2014-12-22 summary:We propose Diverse Embedding Neural Network (DENN), a novel architecture forlanguage models (LMs). A DENNLM projects the input word history vector ontomultiple diverse low-dimensional sub-spaces instead of a singlehigher-dimensional sub-space as in conventional feed-forward neural networkLMs. We encourage these sub-spaces to be diverse during network trainingthrough an augmented loss function. Our language modeling experiments on thePenn Treebank data set show the performance benefit of using a DENNLM.
arxiv-9900-74 | Anatomy-specific classification of medical images using deep convolutional nets | http://arxiv.org/pdf/1504.04003v1.pdf | author:Holger R. Roth, Christopher T. Lee, Hoo-Chang Shin, Ari Seff, Lauren Kim, Jianhua Yao, Le Lu, Ronald M. Summers category:cs.CV published:2015-04-15 summary:Automated classification of human anatomy is an important prerequisite formany computer-aided diagnosis systems. The spatial complexity and variabilityof anatomy throughout the human body makes classification difficult. "Deeplearning" methods such as convolutional networks (ConvNets) outperform otherstate-of-the-art methods in image classification tasks. In this work, wepresent a method for organ- or body-part-specific anatomical classification ofmedical images acquired using computed tomography (CT) with ConvNets. We traina ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomicalclasses. Key-images were mined from a hospital PACS archive, using a set of1,675 patients. We show that a data augmentation approach can help to enrichthe data set and improve classification performance. Using ConvNets and dataaugmentation, we achieve anatomy-specific classification error of 5.9 % andarea-under-the-curve (AUC) values of an average of 0.998 in testing. Wedemonstrate that deep learning can be used to train very reliable and accurateclassifiers that could initialize further computer-aided diagnosis.
arxiv-9900-75 | Training Deep Neural Networks on Noisy Labels with Bootstrapping | http://arxiv.org/pdf/1412.6596v3.pdf | author:Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, Andrew Rabinovich category:cs.CV cs.LG cs.NE published:2014-12-20 summary:Current state-of-the-art deep learning systems for visual object recognitionand detection use purely supervised training with regularization such asdropout to avoid overfitting. The performance depends critically on the amountof labeled examples, and in current practice the labels are assumed to beunambiguous and accurate. However, this assumption often does not hold; e.g. inrecognition, class labels may be missing; in detection, objects in the imagemay not be localized; and in general, the labeling may be subjective. In thiswork we propose a generic way to handle noisy and incomplete labeling byaugmenting the prediction objective with a notion of consistency. We consider aprediction consistent if the same prediction is made given similar percepts,where the notion of similarity is between deep network features computed fromthe input data. In experiments we demonstrate that our approach yieldssubstantial robustness to label noise on several datasets. On MNIST handwrittendigits, we show that our model is robust to label corruption. On the TorontoFace Database, we show that our model handles well the case of subjectivelabels in emotion recognition, achieving state-of-the- art results, and canalso benefit from unlabeled face images with no modification to our method. Onthe ILSVRC2014 detection challenge data, we show that our approach extends tovery deep networks, high resolution images and structured outputs, and resultsin improved scalable detection.
arxiv-9900-76 | Object Detectors Emerge in Deep Scene CNNs | http://arxiv.org/pdf/1412.6856v2.pdf | author:Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba category:cs.CV cs.NE published:2014-12-22 summary:With the success of new computational architectures for visual processing,such as convolutional neural networks (CNN) and access to image databases withmillions of labeled examples (e.g., ImageNet, Places), the state of the art incomputer vision is advancing rapidly. One important factor for continuedprogress is to understand the representations that are learned by the innerlayers of these deep architectures. Here we show that object detectors emergefrom training CNNs to perform scene classification. As scenes are composed ofobjects, the CNN for scene classification automatically discovers meaningfulobjects detectors, representative of the learned scene categories. With objectdetectors emerging as a result of learning to recognize scenes, our workdemonstrates that the same network can perform both scene recognition andobject localization in a single forward-pass, without ever having beenexplicitly taught the notion of objects.
arxiv-9900-77 | Deep convolutional networks for pancreas segmentation in CT imaging | http://arxiv.org/pdf/1504.03967v1.pdf | author:Holger R. Roth, Amal Farag, Le Lu, Evrim B. Turkbey, Ronald M. Summers category:cs.CV published:2015-04-15 summary:Automatic organ segmentation is an important prerequisite for manycomputer-aided diagnosis systems. The high anatomical variability of organs inthe abdomen, such as the pancreas, prevents many segmentation methods fromachieving high accuracies when compared to other segmentation of organs likethe liver, heart or kidneys. Recently, the availability of large annotatedtraining sets and the accessibility of affordable parallel computing resourcesvia GPUs have made it feasible for "deep learning" methods such asconvolutional networks (ConvNets) to succeed in image classification tasks.These methods have the advantage that used classification features are traineddirectly from the imaging data. We present a fully-automated bottom-up methodfor pancreas segmentation in computed tomography (CT) images of the abdomen.The method is based on hierarchical coarse-to-fine classification of localimage regions (superpixels). Superpixels are extracted from the abdominalregion using Simple Linear Iterative Clustering (SLIC). An initial probabilityresponse map is generated, using patch-level confidences and a two-levelcascade of random forest classifiers, from which superpixel regions withprobabilities larger 0.5 are retained. These retained superpixels serve as ahighly sensitive initial input of the pancreas and its surroundings to aConvNet that samples a bounding box around each superpixel at different scales(and random non-rigid deformations at training time) in order to assign a moredistinct probability of each superpixel region being pancreas or not. Weevaluate our method on CT images of 82 patients (60 for training, 2 forvalidation, and 20 for testing). Using ConvNets we achieve average Dice scoresof 68%+-10% (range, 43-80%) in testing. This shows promise for accuratepancreas segmentation, using a deep learning approach and compares favorably tostate-of-the-art methods.
arxiv-9900-78 | 3D ShapeNets: A Deep Representation for Volumetric Shapes | http://arxiv.org/pdf/1406.5670v3.pdf | author:Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao category:cs.CV published:2014-06-22 summary:3D shape is a crucial but heavily underutilized cue in today's computervision systems, mostly due to the lack of a good generic shape representation.With the recent availability of inexpensive 2.5D depth sensors (e.g. MicrosoftKinect), it is becoming increasingly important to have a powerful 3D shaperepresentation in the loop. Apart from category recognition, recovering full 3Dshapes from view-based 2.5D depth maps is also a critical part of visualunderstanding. To this end, we propose to represent a geometric 3D shape as aprobability distribution of binary variables on a 3D voxel grid, using aConvolutional Deep Belief Network. Our model, 3D ShapeNets, learns thedistribution of complex 3D shapes across different object categories andarbitrary poses from raw CAD data, and discovers hierarchical compositionalpart representations automatically. It naturally supports joint objectrecognition and shape completion from 2.5D depth maps, and it enables activeobject recognition through view planning. To train our 3D deep learning model,we construct ModelNet -- a large-scale 3D CAD model dataset. Extensiveexperiments show that our 3D deep representation enables significantperformance improvement over the-state-of-the-arts in a variety of tasks.
arxiv-9900-79 | Optimizing Text Quantifiers for Multivariate Loss Functions | http://arxiv.org/pdf/1502.05491v2.pdf | author:Andrea Esuli, Fabrizio Sebastiani category:cs.LG cs.IR published:2015-02-19 summary:We address the problem of \emph{quantification}, a supervised learning taskwhose goal is, given a class, to estimate the relative frequency (or\emph{prevalence}) of the class in a dataset of unlabelled items.Quantification has several applications in data and text mining, such asestimating the prevalence of positive reviews in a set of reviews of a givenproduct, or estimating the prevalence of a given support issue in a dataset oftranscripts of phone calls to tech support. So far, quantification has beenaddressed by learning a general-purpose classifier, counting the unlabelleditems which have been assigned the class, and tuning the obtained countsaccording to some heuristics. In this paper we depart from the tradition ofusing general-purpose classifiers, and use instead a supervised learning modelfor \emph{structured prediction}, capable of generating classifiers directlyoptimized for the (multivariate and non-linear) function used for evaluatingquantification accuracy. The experiments that we have run on 5500 binaryhigh-dimensional datasets (averaging more than 14,000 documents each) show thatthis method is more accurate, more stable, and more efficient than existing,state-of-the-art quantification methods.
arxiv-9900-80 | Application of Enhanced-2D-CWT in Topographic Images for Mapping Landslide Risk Areas | http://arxiv.org/pdf/1504.05137v1.pdf | author:V. V. Vermehren Valenzuela, R. D. Lins, H. M. de Oliveira category:cs.CV physics.geo-ph published:2015-04-15 summary:There has been lately a number of catastrophic events of landslides andmudslides in the mountainous region of Rio de Janeiro, Brazil. Those werecaused by intense rain in localities where there was unplanned occupation ofslopes of hills and mountains. Thus, it became imperative creating an inventoryof landslide risk areas in densely populated cities. This work presents a wayof demarcating risk areas by using the bidimensional Continuous WaveletTransform (2D-CWT) applied to high resolution topographic images of themountainous region of Rio de Janeiro.
arxiv-9900-81 | Improving zero-shot learning by mitigating the hubness problem | http://arxiv.org/pdf/1412.6568v3.pdf | author:Georgiana Dinu, Angeliki Lazaridou, Marco Baroni category:cs.CL cs.LG published:2014-12-20 summary:The zero-shot paradigm exploits vector-based word representations extractedfrom text corpora with unsupervised methods to learn general mapping functionsfrom other feature spaces onto word space, where the words associated to thenearest neighbours of the mapped vectors are used as their linguistic labels.We show that the neighbourhoods of the mapped elements are strongly polluted byhubs, vectors that tend to be near a high proportion of items, pushing theircorrect labels down the neighbour list. After illustrating the problemempirically, we propose a simple method to correct it by taking the proximitydistribution of potential neighbours across many mapped vectors into account.We show that this correction leads to consistent improvements in realisticzero-shot experiments in the cross-lingual, image labeling and image retrievaldomains.
arxiv-9900-82 | Linear Maximum Margin Classifier for Learning from Uncertain Data | http://arxiv.org/pdf/1504.03892v1.pdf | author:Christos Tzelepis, Vasileios Mezaris, Ioannis Patras category:cs.LG published:2015-04-15 summary:In this paper, we propose a maximum margin classifier that deals withuncertainty in data input. Specifically, we reformulate the SVM framework suchthat each input training entity is not solely a feature vector representation,but a multi-dimensional Gaussian distribution with given probability density,i.e., with a given mean and covariance matrix. The latter expresses theuncertainty. We arrive at a convex optimization problem, which is solved in theprimal form using a gradient descent approach. The resulting classifier, whichwe name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on syntheticdata, as well as on the problem of event detection in video using thelarge-scale TRECVID MED 2014 dataset, and the problem of image classificationusing the MNIST dataset of handwritten digits. Experimental results verify theeffectiveness of the proposed classifier.
arxiv-9900-83 | Bridging belief function theory to modern machine learning | http://arxiv.org/pdf/1504.03874v1.pdf | author:Thomas Burger category:cs.AI cs.LG published:2015-04-15 summary:Machine learning is a quickly evolving field which now looks really differentfrom what it was 15 years ago, when classification and clustering were majorissues. This document proposes several trends to explore the new questions ofmodern machine learning, with the strong afterthought that the belief functionframework has a major role to play.
arxiv-9900-84 | Tracking Live Fish from Low-Contrast and Low-Frame-Rate Stereo Videos | http://arxiv.org/pdf/1504.03811v1.pdf | author:Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams, Richard Towler category:cs.CV published:2015-04-15 summary:Non-extractive fish abundance estimation with the aid of visual analysis hasdrawn increasing attention. Unstable illumination, ubiquitous noise and lowframe rate video capturing in the underwater environment, however, makeconventional tracking methods unreliable. In this paper, we present a multiplefish tracking system for low-contrast and low-frame-rate stereo videos with theuse of a trawl-based underwater camera system. An automatic fish segmentationalgorithm overcomes the low-contrast issues by adopting a histogrambackprojection approach on double local-thresholded images to ensure anaccurate segmentation on the fish shape boundaries. Built upon a reliablefeature-based object matching method, a multiple-target tracking algorithm viaa modified Viterbi data association is proposed to overcome the poor motioncontinuity and frequent entrance/exit of fish targets under low-frame-ratescenarios. In addition, a computationally efficient block-matching approachperforms successful stereo matching, which enables an automatic fish-body tailcompensation to greatly reduce segmentation error and allows for an accuratefish length measurement. Experimental results show that an effective andreliable tracking performance for multiple live fish with underwater stereocameras is achieved.
arxiv-9900-85 | Text Localization in Video Using Multiscale Weber's Local Descriptor | http://arxiv.org/pdf/1504.03810v1.pdf | author:B. H. Shekar, Smitha M. L. category:cs.CV published:2015-04-15 summary:In this paper, we propose a novel approach for detecting the text present invideos and scene images based on the Multiscale Weber's Local Descriptor(MWLD). Given an input video, the shots are identified and the key frames areextracted based on their spatio-temporal relationship. From each key frame, wedetect the local region information using WLD with different radius andneighborhood relationship of pixel values and hence obtained intensity enhancedkey frames at multiple scales. These multiscale WLD key frames are mergedtogether and then the horizontal gradients are computed using morphologicaloperations. The obtained results are then binarized and the false positives areeliminated based on geometrical properties. Finally, we employ connectedcomponent analysis and morphological dilation operation to determine the textregions that aids in text localization. The experimental results obtained onpublicly available standard Hua, Horizontal-1 and Horizontal-2 video datasetillustrate that the proposed method can accurately detect and localize texts ofvarious sizes, fonts and colors in videos.
arxiv-9900-86 | Fully Convolutional Multi-Class Multiple Instance Learning | http://arxiv.org/pdf/1412.7144v4.pdf | author:Deepak Pathak, Evan Shelhamer, Jonathan Long, Trevor Darrell category:cs.CV cs.LG cs.NE published:2014-12-22 summary:Multiple instance learning (MIL) can reduce the need for costly annotation intasks such as semantic segmentation by weakening the required degree ofsupervision. We propose a novel MIL formulation of multi-class semanticsegmentation learning by a fully convolutional network. In this setting, weseek to learn a semantic segmentation model from just weak image-level labels.The model is trained end-to-end to jointly optimize the representation whiledisambiguating the pixel-image label assignment. Fully convolutional trainingaccepts inputs of any size, does not need object proposal pre-processing, andoffers a pixelwise loss map for selecting latent instances. Our multi-class MILloss exploits the further supervision given by images with multiple labels. Weevaluate this approach through preliminary experiments on the PASCAL VOCsegmentation challenge.
arxiv-9900-87 | Posterior contraction of the population polytope in finite admixture models | http://arxiv.org/pdf/1206.0068v3.pdf | author:XuanLong Nguyen category:math.ST cs.LG stat.TH published:2012-06-01 summary:We study the posterior contraction behavior of the latent populationstructure that arises in admixture models as the amount of data increases. Weadopt the geometric view of admixture models - alternatively known as topicmodels - as a data generating mechanism for points randomly sampled from theinterior of a (convex) population polytope, whose extreme points correspond tothe population structure variables of interest. Rates of posterior contractionare established with respect to Hausdorff metric and a minimum matchingEuclidean metric defined on polytopes. Tools developed include posteriorasymptotics of hierarchical models and arguments from convex geometry.
arxiv-9900-88 | Relax, no need to round: integrality of clustering formulations | http://arxiv.org/pdf/1408.4045v5.pdf | author:Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, Rachel Ward category:stat.ML cs.DS cs.LG math.ST stat.TH published:2014-08-18 summary:We study exact recovery conditions for convex relaxations of point cloudclustering problems, focusing on two of the most common optimization problemsfor unsupervised clustering: $k$-means and $k$-median clustering. Motivationsfor focusing on convex relaxations are: (a) they come with a certificate ofoptimality, and (b) they are generic tools which are relatively parameter-free,not tailored to specific assumptions over the input. More precisely, weconsider the distributional setting where there are $k$ clusters in$\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from asymmetric distribution within a ball of unit radius. We ask: what is theminimal separation distance between cluster centers needed for convexrelaxations to exactly recover these $k$ clusters as the optimal integralsolution? For the $k$-median linear programming relaxation we show a tightbound: exact recovery is obtained given arbitrarily small pairwise separation$\epsilon > 0$ between the balls. In other words, the pairwise centerseparation is $\Delta > 2+\epsilon$. Under the same distributional model, the$k$-means LP relaxation fails to recover such clusters at separation as largeas $\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we getexact cluster recovery at center separation $\Delta > 2\sqrt2(1+\sqrt{1/m})$.In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-meansalgorithm) can fail to recover clusters in this setting; even with arbitrarilylarge cluster separation, k-means++ with overseeding by any constant factorfails with high probability at exact cluster recovery. To complement thetheoretical analysis, we provide an experimental study of the recoveryguarantees for these various methods, and discuss several open problems whichthese experiments suggest.
arxiv-9900-89 | Norm-Based Capacity Control in Neural Networks | http://arxiv.org/pdf/1503.00036v2.pdf | author:Behnam Neyshabur, Ryota Tomioka, Nathan Srebro category:cs.LG cs.AI cs.NE stat.ML published:2015-02-27 summary:We investigate the capacity, convexity and characterization of a generalfamily of norm-constrained feed-forward networks.
arxiv-9900-90 | Background Subtraction via Generalized Fused Lasso Foreground Modeling | http://arxiv.org/pdf/1504.03707v1.pdf | author:Bo Xin, Yuan Tian, Yizhou Wang, Wen Gao category:cs.CV published:2015-04-14 summary:Background Subtraction (BS) is one of the key steps in video analysis. Manybackground models have been proposed and achieved promising performance onpublic data sets. However, due to challenges such as illumination change,dynamic background etc. the resulted foreground segmentation often consists ofholes as well as background noise. In this regard, we consider generalizedfused lasso regularization to quest for intact structured foregrounds. Togetherwith certain assumptions about the background, such as the low-rank assumptionor the sparse-composition assumption (depending on whether pure backgroundframes are provided), we formulate BS as a matrix decomposition problem usingregularization terms for both the foreground and background matrices. Moreover,under the proposed formulation, the two generally distinctive backgroundassumptions can be solved in a unified manner. The optimization was carried outvia applying the augmented Lagrange multiplier (ALM) method in such a way thata fast parametric-flow algorithm is used for updating the foreground matrix.Experimental results on several popular BS data sets demonstrate the advantageof the proposed model compared to state-of-the-arts.
arxiv-9900-91 | Probabilistic Clustering of Time-Evolving Distance Data | http://arxiv.org/pdf/1504.03701v1.pdf | author:Julia E. Vogt, Marius Kloft, Stefan Stark, Sudhir S. Raman, Sandhya Prabhakaran, Volker Roth, Gunnar Rätsch category:cs.LG stat.ML published:2015-04-14 summary:We present a novel probabilistic clustering model for objects that arerepresented via pairwise distances and observed at different time points. Theproposed method utilizes the information given by adjacent time points to findthe underlying cluster structure and obtain a smooth cluster evolution. Thisapproach allows the number of objects and clusters to differ at every timepoint, and no identification on the identities of the objects is needed.Further, the model does not require the number of clusters being specified inadvance -- they are instead determined automatically using a Dirichlet processprior. We validate our model on synthetic data showing that the proposed methodis more accurate than state-of-the-art clustering methods. Finally, we use ourdynamic clustering model to analyze and illustrate the evolution of braincancer patients over time.
arxiv-9900-92 | Temporal ordering of clinical events | http://arxiv.org/pdf/1504.03659v1.pdf | author:Azad Dehghan category:cs.CL cs.AI published:2015-04-14 summary:This report describes a minimalistic set of methods engineered to anchorclinical events onto a temporal space. Specifically, we describe methods toextract clinical events (e.g., Problems, Treatments and Tests), temporalexpressions (i.e., time, date, duration, and frequency), and temporal links(e.g., Before, After, Overlap) between events and temporal entities. Thesemethods are developed and validated using high quality datasets.
arxiv-9900-93 | From Captions to Visual Concepts and Back | http://arxiv.org/pdf/1411.4952v3.pdf | author:Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig category:cs.CV cs.CL published:2014-11-18 summary:This paper presents a novel approach for automatically generating imagedescriptions: visual detectors, language models, and multimodal similaritymodels learnt directly from a dataset of image captions. We use multipleinstance learning to train visual detectors for words that commonly occur incaptions, including many different parts of speech such as nouns, verbs, andadjectives. The word detector outputs serve as conditional inputs to amaximum-entropy language model. The language model learns from a set of over400,000 image descriptions to capture the statistics of word usage. We captureglobal semantics by re-ranking caption candidates using sentence-level featuresand a deep multimodal similarity model. Our system is state-of-the-art on theofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. Whenhuman judges compare the system captions to ones written by other people on ourheld-out test set, the system captions have equal or better quality 34% of thetime.
arxiv-9900-94 | Learning to Compare Image Patches via Convolutional Neural Networks | http://arxiv.org/pdf/1504.03641v1.pdf | author:Sergey Zagoruyko, Nikos Komodakis category:cs.CV cs.LG cs.NE published:2015-04-14 summary:In this paper we show how to learn directly from image data (i.e., withoutresorting to manually-designed features) a general similarity function forcomparing image patches, which is a task of fundamental importance for manycomputer vision problems. To encode such a function, we opt for a CNN-basedmodel that is trained to account for a wide variety of changes in imageappearance. To that end, we explore and study multiple neural networkarchitectures, which are specifically adapted to this task. We show that suchan approach can significantly outperform the state-of-the-art on severalproblems and benchmark datasets.
arxiv-9900-95 | A Canonical Semi-Deterministic Transducer | http://arxiv.org/pdf/1405.2476v3.pdf | author:Achilles Beros, Colin de la Higuera category:cs.LG published:2014-05-10 summary:We prove the existence of a canonical form for semi-deterministic transducerswith incomparable sets of output strings. Based on this, we develop analgorithm which learns semi-deterministic transducers given access totranslation queries. We also prove that there is no learning algorithm forsemi-deterministic transducers that uses only domain knowledge.
arxiv-9900-96 | A data-based classification of Slavic languages: Indices of qualitative variation applied to grapheme frequencies | http://arxiv.org/pdf/1504.03608v1.pdf | author:Michaela Koscová, Ján Macutek, Emmerich Kelih category:stat.AP cs.CL published:2015-04-14 summary:The Ord's graph is a simple graphical method for displaying frequencydistributions of data or theoretical distributions in the two-dimensionalplane. Its coordinates are proportions of the first three moments, eitherempirical or theoretical ones. A modification of the Ord's graph based onproportions of indices of qualitative variation is presented. Such amodification makes the graph applicable also to data of categorical character.In addition, the indices are normalized with values between 0 and 1, whichenables comparing data files divided into different numbers of categories. Boththe original and the new graph are used to display grapheme frequencies ineleven Slavic languages. As the original Ord's graph requires an assignment ofnumbers to the categories, graphemes were ordered decreasingly according totheir frequencies. Data were taken from parallel corpora, i.e., we work withgrapheme frequencies from a Russian novel and its translations to ten otherSlavic languages. Then, cluster analysis is applied to the graph coordinates.While the original graph yields results which are not linguisticallyinterpretable, the modification reveals meaningful relations among thelanguages.
arxiv-9900-97 | A Solution for Multi-Alignment by Transformation Synchronisation | http://arxiv.org/pdf/1410.8546v2.pdf | author:Florian Bernard, Johan Thunberg, Peter Gemmar, Frank Hertel, Andreas Husch, Jorge Goncalves category:cs.CV stat.ML published:2014-10-30 summary:The alignment of a set of objects by means of transformations plays animportant role in computer vision. Whilst the case for only two objects can besolved globally, when multiple objects are considered usually iterative methodsare used. In practice the iterative methods perform well if the relativetransformations between any pair of objects are free of noise. However, if onlynoisy relative transformations are available (e.g. due to missing data or wrongcorrespondences) the iterative methods may fail. Based on the observation that the underlying noise-free transformations canbe retrieved from the null space of a matrix that can directly be obtained frompairwise alignments, this paper presents a novel method for the synchronisationof pairwise transformations such that they are transitively consistent. Simulations demonstrate that for noisy transformations, a large proportion ofmissing data and even for wrong correspondence assignments the method deliversencouraging results.
arxiv-9900-98 | Low-level Vision by Consensus in a Spatial Hierarchy of Regions | http://arxiv.org/pdf/1411.4894v2.pdf | author:Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, Todd Zickler category:cs.CV published:2014-11-18 summary:We introduce a multi-scale framework for low-level vision, where the goal isestimating physical scene values from image data---such as depth from stereoimage pairs. The framework uses a dense, overlapping set of image regions atmultiple scales and a "local model," such as a slanted-plane model for stereodisparity, that is expected to be valid piecewise across the visual field.Estimation is cast as optimization over a dichotomous mixture of variables,simultaneously determining which regions are inliers with respect to the localmodel (binary variables) and the correct co-ordinates in the local model spacefor each inlying region (continuous variables). When the regions are organizedinto a multi-scale hierarchy, optimization can occur in an efficient andparallel architecture, where distributed computational units iterativelyperform calculations and share information through sparse connections betweenparents and children. The framework performs well on a standard benchmark forbinocular stereo, and it produces a distributional scene representation that isappropriate for combining with higher-level reasoning and other low-level cues.
arxiv-9900-99 | Building Proteins in a Day: Efficient 3D Molecular Reconstruction | http://arxiv.org/pdf/1504.03573v1.pdf | author:Marcus A. Brubaker, Ali Punjani, David J. Fleet category:cs.CV q-bio.QM published:2015-04-14 summary:Discovering the 3D atomic structure of molecules such as proteins and virusesis a fundamental research problem in biology and medicine. ElectronCryomicroscopy (Cryo-EM) is a promising vision-based technique for structureestimation which attempts to reconstruct 3D structures from 2D images. Thispaper addresses the challenging problem of 3D reconstruction from 2D Cryo-EMimages. A new framework for estimation is introduced which relies on modernstochastic optimization techniques to scale to large datasets. We alsointroduce a novel technique which reduces the cost of evaluating the objectivefunction during optimization by over five orders or magnitude. The net resultis an approach capable of estimating 3D molecular structure from large scaledatasets in about a day on a single workstation.
arxiv-9900-100 | Efficient Scene Text Localization and Recognition with Local Character Refinement | http://arxiv.org/pdf/1504.03522v1.pdf | author:Lukáš Neumann, Jiří Matas category:cs.CV published:2015-04-14 summary:An unconstrained end-to-end text localization and recognition method ispresented. The method detects initial text hypothesis in a single pass by anefficient region-based method and subsequently refines the text hypothesisusing a more robust local text model, which deviates from the common assumptionof region-based methods that all characters are detected as connectedcomponents. Additionally, a novel feature based on character stroke area estimation isintroduced. The feature is efficiently computed from a region distance map, itis invariant to scaling and rotations and allows to efficiently detect textregions regardless of what portion of text they capture. The method runs in real time and achieves state-of-the-art text localizationand recognition results on the ICDAR 2013 Robust Reading dataset.
arxiv-9900-101 | A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor Images | http://arxiv.org/pdf/1504.01800v2.pdf | author:Mohammed Khader, A. Ben Hamza category:cs.CV published:2015-04-08 summary:We propose a nonrigid registration approach for diffusion tensor images usinga multicomponent information-theoretic measure. Explicit orientationoptimization is enabled by incorporating tensor reorientation, which isnecessary for wrapping diffusion tensor images. Experimental results ondiffusion tensor images indicate the feasibility of the proposed approach and amuch better performance compared to the affine registration method based onmutual information in terms of registration accuracy in the presence ofgeometric distortion.
arxiv-9900-102 | Sample compression schemes for VC classes | http://arxiv.org/pdf/1503.06960v2.pdf | author:Shay Moran, Amir Yehudayoff category:cs.LG published:2015-03-24 summary:Sample compression schemes were defined by Littlestone and Warmuth (1986) asan abstraction of the structure underlying many learning algorithms. Roughlyspeaking, a sample compression scheme of size $k$ means that given an arbitrarylist of labeled examples, one can retain only $k$ of them in a way that allowsto recover the labels of all other examples in the list. They showed thatcompression implies PAC learnability for binary-labeled classes, and askedwhether the other direction holds. We answer their question and show that everyconcept class $C$ with VC dimension $d$ has a sample compression scheme of sizeexponential in $d$. The proof uses an approximate minimax phenomenon for binarymatrices of low VC dimension, which may be of interest in the context of gametheory.
arxiv-9900-103 | Sketch-based 3D Shape Retrieval using Convolutional Neural Networks | http://arxiv.org/pdf/1504.03504v1.pdf | author:Fang Wang, Le Kang, Yi Li category:cs.CV published:2015-04-14 summary:Retrieving 3D models from 2D human sketches has received considerableattention in the areas of graphics, image retrieval, and computer vision.Almost always in state of the art approaches a large amount of "best views" arecomputed for 3D models, with the hope that the query sketch matches one ofthese 2D projections of 3D models using predefined features. We argue that this two stage approach (view selection -- matching) ispragmatic but also problematic because the "best views" are subjective andambiguous, which makes the matching inputs obscure. This imprecise nature ofmatching further makes it challenging to choose features manually. Instead ofrelying on the elusive concept of "best views" and the hand-crafted features,we propose to define our views using a minimalism approach and learn featuresfor both sketches and views. Specifically, we drastically reduce the number ofviews to only two predefined directions for the whole dataset. Then, we learntwo Siamese Convolutional Neural Networks (CNNs), one for the views and one forthe sketches. The loss function is defined on the within-domain as well as thecross-domain similarities. Our experiments on three benchmark datasetsdemonstrate that our method is significantly better than state of the artapproaches, and outperforms them in all conventional metrics.
arxiv-9900-104 | An active search strategy for efficient object class detection | http://arxiv.org/pdf/1412.3709v2.pdf | author:Abel Gonzalez-Garcia, Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2014-12-11 summary:Object class detectors typically apply a window classifier to all the windowsin a large set, either in a sliding window manner or using object proposals. Inthis paper, we develop an active search strategy that sequentially chooses thenext window to evaluate based on all the information gathered before. Thisresults in a substantial reduction in the number of classifier evaluations andin a more elegant approach in general. Our search strategy is guided by twoforces. First, we exploit context as the statistical relation between theappearance of a window and its location relative to the object, as observed inthe training set. This enables to jump across distant regions in the image(e.g. observing a sky region suggests that cars might be far below) and is doneefficiently in a Random Forest framework. Second, we exploit the score of theclassifier to attract the search to promising areas surrounding a highly scoredwindow, and to keep away from areas near low scored ones. Our search strategycan be applied on top of any classifier as it treats it as a black-box. Inexperiments with R-CNN on the challenging SUN2012 dataset, our method matchesthe detection accuracy of evaluating all windows independently, whileevaluating 9x fewer windows.
arxiv-9900-105 | The jump set under geometric regularisation. Part 1: Basic technique and first-order denoising | http://arxiv.org/pdf/1407.1531v2.pdf | author:Tuomo Valkonen category:math.FA cs.CV published:2014-07-06 summary:Let $u \in \mbox{BV}(\Omega)$ solve the total variation denoising problemwith $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model.Simul. 6 (2008), 879--894] have shown the containment $\mathcal{H}^{m-1}(J_u\setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proofunfortunately depends heavily on the co-area formula, as do many results inthis area, and as such is not directly extensible to higher-order,curvature-based, and other advanced geometric regularisers, such as totalgeneralised variation (TGV) and Euler's elastica. These have received increasedattention in recent times due to their better practical regularisationproperties compared to conventional total variation or wavelets. We proveanalogous jump set containment properties for a general class of regularisers.We do this with novel Lipschitz transformation techniques, and do not requirethe co-area formula. In the present Part 1 we demonstrate the general techniqueon first-order regularisers, while in Part 2 we will extend it to higher-orderregularisers. In particular, we concentrate in this part on TV and, as anovelty, Huber-regularised TV. We also demonstrate that the technique wouldapply to non-convex TV models as well as the Perona-Malik anisotropicdiffusion, if these approaches were well-posed to begin with.
arxiv-9900-106 | Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How Many Objects can iCub Learn? | http://arxiv.org/pdf/1504.03154v2.pdf | author:Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco, Lorenzo Natale category:cs.RO cs.CV cs.LG published:2015-04-13 summary:The ability to visually recognize objects is a fundamental skill for roboticssystems. Indeed, a large variety of tasks involving manipulation, navigation orinteraction with other agents, deeply depends on the accurate understanding ofthe visual scene. Yet, at the time being, robots are lacking good visualperceptual systems, which often become the main bottleneck preventing the useof autonomous agents for real-world applications. Lately in computer vision, systems that learn suitable visual representationsand based on multi-layer deep convolutional networks are showing remarkableperformance in tasks such as large-scale visual recognition and imageretrieval. To this regard, it is natural to ask whether such remarkableperformance would generalize also to the robotic setting. In this paper we investigate such possibility, while taking further steps indeveloping a computational vision system to be embedded on a robotic platform,the iCub humanoid robot. In particular, we release a new dataset ({\sciCubWorld28}) that we use as a benchmark to address the question: {\it how manyobjects can iCub recognize?} Our study is developed in a learning frameworkwhich reflects the typical visual experience of a humanoid robot like the iCub.Experiments shed interesting insights on the strength and weaknesses of currentcomputer vision approaches applied in real robotic settings.
arxiv-9900-107 | Automated Analysis and Prediction of Job Interview Performance | http://arxiv.org/pdf/1504.03425v1.pdf | author:Iftekhar Naim, M. Iftekhar Tanveer, Daniel Gildea, Mohammed, Hoque category:cs.HC cs.AI cs.CL published:2015-04-14 summary:We present a computational framework for automatically quantifying verbal andnonverbal behaviors in the context of job interviews. The proposed framework istrained by analyzing the videos of 138 interview sessions with 69internship-seeking undergraduates at the Massachusetts Institute of Technology(MIT). Our automated analysis includes facial expressions (e.g., smiles, headgestures, facial tracking points), language (e.g., word counts, topicmodeling), and prosodic information (e.g., pitch, intonation, and pauses) ofthe interviewees. The ground truth labels are derived by taking a weightedaverage over the ratings of 9 independent judges. Our framework canautomatically predict the ratings for interview traits such as excitement,friendliness, and engagement with correlation coefficients of 0.75 or higher,and can quantify the relative importance of prosody, language, and facialexpressions. By analyzing the relative feature weights learned by theregression models, our framework recommends to speak more fluently, use lessfiller words, speak as "we" (vs. "I"), use more unique words, and smile more.We also find that the students who were rated highly while answering the firstinterview question were also rated highly overall (i.e., first impressionmatters). Finally, our MIT Interview dataset will be made available to otherresearchers to further validate and expand our findings.
arxiv-9900-108 | Material Recognition in the Wild with the Materials in Context Database | http://arxiv.org/pdf/1412.0623v2.pdf | author:Sean Bell, Paul Upchurch, Noah Snavely, Kavita Bala category:cs.CV published:2014-12-01 summary:Recognizing materials in real-world images is a challenging task. Real-worldmaterials have rich surface texture, geometry, lighting conditions, andclutter, which combine to make the problem particularly difficult. In thispaper, we introduce a new, large-scale, open dataset of materials in the wild,the Materials in Context Database (MINC), and combine this dataset with deeplearning to achieve material recognition and segmentation of images in thewild. MINC is an order of magnitude larger than previous material databases, whilebeing more diverse and well-sampled across its 23 categories. Using MINC, wetrain convolutional neural networks (CNNs) for two tasks: classifying materialsfrom patches, and simultaneous material recognition and segmentation in fullimages. For patch-based classification on MINC we found that the bestperforming CNN architectures can achieve 85.2% mean class accuracy. We convertthese trained CNN classifiers into an efficient fully convolutional frameworkcombined with a fully connected conditional random field (CRF) to predict thematerial at every pixel in an image, achieving 73.1% mean class accuracy. Ourexperiments demonstrate that having a large, well-sampled dataset such as MINCis crucial for real-world material recognition and segmentation.
arxiv-9900-109 | Deep Visual-Semantic Alignments for Generating Image Descriptions | http://arxiv.org/pdf/1412.2306v2.pdf | author:Andrej Karpathy, Li Fei-Fei category:cs.CV published:2014-12-07 summary:We present a model that generates natural language descriptions of images andtheir regions. Our approach leverages datasets of images and their sentencedescriptions to learn about the inter-modal correspondences between languageand visual data. Our alignment model is based on a novel combination ofConvolutional Neural Networks over image regions, bidirectional RecurrentNeural Networks over sentences, and a structured objective that aligns the twomodalities through a multimodal embedding. We then describe a MultimodalRecurrent Neural Network architecture that uses the inferred alignments tolearn to generate novel descriptions of image regions. We demonstrate that ouralignment model produces state of the art results in retrieval experiments onFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generateddescriptions significantly outperform retrieval baselines on both full imagesand on a new dataset of region-level annotations.
arxiv-9900-110 | HHCART: An Oblique Decision Tree | http://arxiv.org/pdf/1504.03415v1.pdf | author:D. C. Wickramarachchi, B. L. Robertson, M. Reale, C. J. Price, J. Brown category:stat.ML cs.LG published:2015-04-14 summary:Decision trees are a popular technique in statistical data classification.They recursively partition the feature space into disjoint sub-regions untileach sub-region becomes homogeneous with respect to a particular class. Thebasic Classification and Regression Tree (CART) algorithm partitions thefeature space using axis parallel splits. When the true decision boundaries arenot aligned with the feature axes, this approach can produce a complicatedboundary structure. Oblique decision trees use oblique decision boundaries topotentially simplify the boundary structure. The major limitation of thisapproach is that the tree induction algorithm is computationally expensive. Inthis article we present a new decision tree algorithm, called HHCART. Themethod utilizes a series of Householder matrices to reflect the training dataat each node during the tree construction. Each reflection is based on thedirections of the eigenvectors from each classes' covariance matrix.Considering axis parallel splits in the reflected training data provides anefficient way of finding oblique splits in the unreflected training data.Experimental results show that the accuracy and size of the HHCART trees arecomparable with some benchmark methods in the literature. The appealing featureof HHCART is that it can handle both qualitative and quantitative features inthe same oblique split.
arxiv-9900-111 | Consensus based Detection in the Presence of Data Falsification Attacks | http://arxiv.org/pdf/1504.03413v1.pdf | author:Bhavya Kailkhura, Swastik Brahma, Pramod K. Varshney category:cs.SY cs.DC stat.AP stat.ML published:2015-04-14 summary:This paper considers the problem of detection in distributed networks in thepresence of data falsification (Byzantine) attacks. Detection approachesconsidered in the paper are based on fully distributed consensus algorithms,where all of the nodes exchange information only with their neighbors in theabsence of a fusion center. In such networks, we characterize the negativeeffect of Byzantines on the steady-state and transient detection performance ofthe conventional consensus based detection algorithms. To address this issue,we study the problem from the network designer's perspective. Morespecifically, we first propose a distributed weighted average consensusalgorithm that is robust to Byzantine attacks. We show that, under reasonableassumptions, the global test statistic for detection can be computed locally ateach node using our proposed consensus algorithm. We exploit the statisticaldistribution of the nodes' data to devise techniques for mitigating theinfluence of data falsifying Byzantines on the distributed detection system.Since some parameters of the statistical distribution of the nodes' data mightnot be known a priori, we propose learning based techniques to enable anadaptive design of the local fusion or update rules.
arxiv-9900-112 | Simultaneous Feature Learning and Hash Coding with Deep Neural Networks | http://arxiv.org/pdf/1504.03410v1.pdf | author:Hanjiang Lai, Yan Pan, Ye Liu, Shuicheng Yan category:cs.CV published:2015-04-14 summary:Similarity-preserving hashing is a widely-used method for nearest neighboursearch in large-scale image retrieval tasks. For most existing hashing methods,an image is first encoded as a vector of hand-engineering visual features,followed by another separate projection or quantization step that generatesbinary codes. However, such visual feature vectors may not be optimallycompatible with the coding process, thus producing sub-optimal hashing codes.In this paper, we propose a deep architecture for supervised hashing, in whichimages are mapped into binary codes via carefully designed deep neuralnetworks. The pipeline of the proposed deep architecture consists of threebuilding blocks: 1) a sub-network with a stack of convolution layers to producethe effective intermediate image features; 2) a divide-and-encode module todivide the intermediate image features into multiple branches, each encodedinto one hash bit; and 3) a triplet ranking loss designed to characterize thatone image is more similar to the second image than to the third one. Extensiveevaluations on several benchmark image datasets show that the proposedsimultaneous feature learning and hash coding pipeline brings substantialimprovements over other state-of-the-art supervised or unsupervised hashingmethods.
arxiv-9900-113 | Clustering Assisted Fundamental Matrix Estimation | http://arxiv.org/pdf/1504.03409v1.pdf | author:Hao Wu, Yi Wan category:cs.CV published:2015-04-14 summary:In computer vision, the estimation of the fundamental matrix is a basicproblem that has been extensively studied. The accuracy of the estimationimposes a significant influence on subsequent tasks such as the cameratrajectory determination and 3D reconstruction. In this paper we propose a newmethod for fundamental matrix estimation that makes use of clustering a groupof 4D vectors. The key insight is the observation that among the 4D vectorsconstructed from matching pairs of points obtained from the SIFT algorithm,well-defined cluster points tend to be reliable inliers suitable forfundamental matrix estimation. Based on this, we utilizes a recently proposedefficient clustering method through density peaks seeking and propose a newclustering assisted method. Experimental results show that the proposedalgorithm is faster and more accurate than currently commonly used methods.
arxiv-9900-114 | Hot Swapping for Online Adaptation of Optimization Hyperparameters | http://arxiv.org/pdf/1412.6599v3.pdf | author:Kevin Bache, Dennis DeCoste, Padhraic Smyth category:cs.LG 62L20 G.1.6; I.2.6 published:2014-12-20 summary:We describe a general framework for online adaptation of optimizationhyperparameters by `hot swapping' their values during learning. We investigatethis approach in the context of adaptive learning rate selection using anexplore-exploit strategy from the multi-armed bandit literature. Experiments ona benchmark neural network show that the hot swapping approach leads toconsistently better solutions compared to well-known alternatives such asAdaDelta and stochastic gradient with exhaustive hyperparameter search.
arxiv-9900-115 | Self-informed neural network structure learning | http://arxiv.org/pdf/1412.6563v2.pdf | author:David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov category:stat.ML cs.CV cs.LG cs.NE published:2014-12-20 summary:We study the problem of large scale, multi-label visual recognition with alarge number of possible classes. We propose a method for augmenting a trainedneural network classifier with auxiliary capacity in a manner designed tosignificantly improve upon an already well-performing model, while minimallyimpacting its computational footprint. Using the predictions of the networkitself as a descriptor for assessing visual similarity, we define apartitioning of the label space into groups of visually similar entities. Wethen augment the network with auxilliary hidden layer pathways withconnectivity only to these groups of label units. We report a significantimprovement in mean average precision on a large-scale object recognition taskwith the augmented model, while increasing the number of multiply-adds by lessthan 3%.
arxiv-9900-116 | Beyond Short Snippets: Deep Networks for Video Classification | http://arxiv.org/pdf/1503.08909v2.pdf | author:Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici category:cs.CV published:2015-03-31 summary:Convolutional neural networks (CNNs) have been extensively applied for imagerecognition problems giving state-of-the-art results on recognition, detection,segmentation and retrieval. In this work we propose and evaluate several deepneural network architectures to combine image information across a video overlonger time periods than previously attempted. We propose two methods capableof handling full length videos. The first method explores various convolutionaltemporal feature pooling architectures, examining the various design choiceswhich need to be made when adapting a CNN for this task. The second proposedmethod explicitly models the video as an ordered sequence of frames. For thispurpose we employ a recurrent neural network that uses Long Short-Term Memory(LSTM) cells which are connected to the output of the underlying CNN. Our bestnetworks exhibit significant performance improvements over previously publishedresults on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101datasets with (88.6% vs. 88.0%) and without additional optical flow information(82.6% vs. 72.8%).
arxiv-9900-117 | Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors - Extended Version | http://arxiv.org/pdf/1504.03285v1.pdf | author:Filip Radenovic, Herve Jegou, Ondrej Chum category:cs.CV published:2015-04-13 summary:This paper addresses the construction of a short-vector (128D) imagerepresentation for large-scale image and particular object retrieval. Inparticular, the method of joint dimensionality reduction of multiplevocabularies is considered. We study a variety of vocabulary generationtechniques: different k-means initializations, different descriptortransformations, different measurement regions for descriptor extraction. Ourextensive evaluation shows that different combinations of vocabularies, eachpartitioning the descriptor space in a different yet complementary manner,results in a significant performance improvement, which exceeds thestate-of-the-art.
arxiv-9900-118 | Egyptian Dialect Stopword List Generation from Social Network Data | http://arxiv.org/pdf/1508.02060v1.pdf | author:Walaa Medhat, Ahmed H. Yousef, Hoda Korashy category:cs.CL published:2015-04-13 summary:This paper proposes a methodology for generating a stopword list from onlinesocial network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper isto investigate the effect of removingED stopwords on the Sentiment Analysis(SA) task. The stopwords lists generated before were on Modern Standard Arabic(MSA) which is not the common language used in OSN. We have generated astopword list of Egyptian dialect to be used with the OSN corpora. We comparethe efficiency of text classification when using the generated list along withpreviously generated lists of MSA and combining the Egyptian dialect list withthe MSA list. The text classification was performed using Na\"ive Bayes andDecision Tree classifiers and two feature selection approaches, unigram andbigram. The experiments show that removing ED stopwords give better performancethan using lists of MSA stopwords only.
arxiv-9900-119 | Optimal Parameter Choices Through Self-Adjustment: Applying the 1/5-th Rule in Discrete Settings | http://arxiv.org/pdf/1504.03212v1.pdf | author:Benjamin Doerr, Carola Doerr category:cs.NE published:2015-04-13 summary:While evolutionary algorithms are known to be very successful for a broadrange of applications, the algorithm designer is often left with manyalgorithmic choices, for example, the size of the population, the mutationrates, and the crossover rates of the algorithm. These parameters are known tohave a crucial influence on the optimization time, and thus need to be chosencarefully, a task that often requires substantial efforts. Moreover, theoptimal parameters can change during the optimization process. It is thereforeof great interest to design mechanisms that dynamically choose best-possibleparameters. An example for such an update mechanism is the one-fifth successrule for step-size adaption in evolutionary strategies. While in continuousdomains this principle is well understood also from a mathematical point ofview, no comparable theory is available for problems in discrete domains. In this work we show that the one-fifth success rule can be effective also indiscrete settings. We regard the $(1+(\lambda,\lambda))$~GA proposed in[Doerr/Doerr/Ebel: From black-box complexity to designing new geneticalgorithms, TCS 2015]. We prove that if its population size is chosen accordingto the one-fifth success rule then the expected optimization time on\textsc{OneMax} is linear. This is better than what \emph{any} staticpopulation size $\lambda$ can achieve and is asymptotically optimal also amongall adaptive parameter choices.
arxiv-9900-120 | Sensitivity Analysis for additive STDP rule | http://arxiv.org/pdf/1503.07490v2.pdf | author:Subhajit Sengupta, Karthik S. Gurumoorthy, Arunava Banerjee category:q-bio.NC cs.NE published:2015-02-28 summary:Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learningrule. The basis of STDP has strong experimental evidences and it depends onprecise input and output spike timings. In this paper we show that underbiologically plausible spiking regime, slight variability in the spike timingleads to drastically different evolution of synaptic weights when its dynamicsare governed by the additive STDP rule.
arxiv-9900-121 | Learning Descriptors for Object Recognition and 3D Pose Estimation | http://arxiv.org/pdf/1502.05908v2.pdf | author:Paul Wohlhart, Vincent Lepetit category:cs.CV published:2015-02-20 summary:Detecting poorly textured objects and estimating their 3D pose reliably isstill a very challenging problem. We introduce a simple but powerful approachto computing descriptors for object views that efficiently capture both theobject identity and 3D pose. By contrast with previous manifold-basedapproaches, we can rely on the Euclidean distance to evaluate the similaritybetween descriptors, and therefore use scalable Nearest Neighbor search methodsto efficiently handle a large number of objects under a large range of poses.To achieve this, we train a Convolutional Neural Network to compute thesedescriptors by enforcing simple similarity and dissimilarity constraintsbetween the descriptors. We show that our constraints nicely untangle theimages from different objects and different views into clusters that are notonly well-separated but also structured as the corresponding sets of poses: TheEuclidean distance between descriptors is large when the descriptors are fromdifferent objects, and directly related to the distance between the poses whenthe descriptors are from the same object. These important properties allow usto outperform state-of-the-art object views representations on challenging RGBand RGB-D data.
arxiv-9900-122 | Adaptive Randomized Dimension Reduction on Massive Data | http://arxiv.org/pdf/1504.03183v1.pdf | author:Gregory Darnell, Stoyan Georgiev, Sayan Mukherjee, Barbara E Engelhardt category:stat.ML q-bio.QM published:2015-04-13 summary:The scalability of statistical estimators is of increasing importance inmodern applications. One approach to implementing scalable algorithms is tocompress data into a low dimensional latent space using dimension reductionmethods. In this paper we develop an approach for dimension reduction thatexploits the assumption of low rank structure in high dimensional data to gainboth computational and statistical advantages. We adapt recent randomizedlow-rank approximation algorithms to provide an efficient solution to principalcomponent analysis (PCA), and we use this efficient solver to improve parameterestimation in large-scale linear mixed models (LMM) for association mapping instatistical and quantitative genomics. A key observation in this paper is thatrandomization serves a dual role, improving both computational and statisticalperformance by implicitly regularizing the covariance matrix estimate of therandom effect in a LMM. These statistical and computational advantages arehighlighted in our experiments on simulated data and large-scale genomicstudies.
arxiv-9900-123 | Streaming, Memory Limited Matrix Completion with Noise | http://arxiv.org/pdf/1504.03156v1.pdf | author:Se-Young Yun, Marc Lelarge, Alexandre Proutiere category:math.SP stat.ML published:2015-04-13 summary:In this paper, we consider the streaming memory-limited matrix completionproblem when the observed entries are noisy versions of a small random fractionof the original entries. We are interested in scenarios where the matrix sizeis very large so the matrix is very hard to store and manipulate. Here, columnsof the observed matrix are presented sequentially and the goal is to completethe missing entries after one pass on the data with limited memory space andlimited computational complexity. We propose a streaming algorithm whichproduces an estimate of the original matrix with a vanishing mean square error,uses memory space scaling linearly with the ambient dimension of the matrix,i.e. the memory required to store the output alone, and spends computations asmuch as the number of non-zero entries of the input matrix.
arxiv-9900-124 | A Regularization Approach to Blind Deblurring and Denoising of QR Barcodes | http://arxiv.org/pdf/1410.6333v2.pdf | author:Yves van Gennip, Prashant Athavale, Jérôme Gilles, Rustum Choksi category:cs.CV math.NA 68U10, 65K10 published:2014-10-23 summary:QR bar codes are prototypical images for which part of the image is a prioriknown (required patterns). Open source bar code readers, such as ZBar, arereadily available. We exploit both these facts to provide and assess purelyregularization-based methods for blind deblurring of QR bar codes in thepresence of noise.
arxiv-9900-125 | Learning Multiple Visual Tasks while Discovering their Structure | http://arxiv.org/pdf/1504.03106v1.pdf | author:Carlo Ciliberto, Lorenzo Rosasco, Silvia Villa category:cs.LG cs.CV published:2015-04-13 summary:Multi-task learning is a natural approach for computer vision applicationsthat require the simultaneous solution of several distinct but relatedproblems, e.g. object detection, classification, tracking of multiple agents,or denoising, to name a few. The key idea is that exploring task relatedness(structure) can lead to improved performances. In this paper, we propose and study a novel sparse, non-parametric approachexploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valuedfunctions. We develop a suitable regularization framework which can beformulated as a convex optimization problem, and is provably solvable using analternating minimization approach. Empirical tests show that the proposedmethod compares favorably to state of the art techniques and further allows torecover interpretable structures, a problem of interest in its own right.
arxiv-9900-126 | Striving for Simplicity: The All Convolutional Net | http://arxiv.org/pdf/1412.6806v3.pdf | author:Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller category:cs.LG cs.CV cs.NE published:2014-12-21 summary:Most modern convolutional neural networks (CNNs) used for object recognitionare built using the same principles: Alternating convolution and max-poolinglayers followed by a small number of fully connected layers. We re-evaluate thestate of the art for object recognition from small images with convolutionalnetworks, questioning the necessity of different components in the pipeline. Wefind that max-pooling can simply be replaced by a convolutional layer withincreased stride without loss in accuracy on several image recognitionbenchmarks. Following this finding -- and building on other recent work forfinding simple network structures -- we propose a new architecture thatconsists solely of convolutional layers and yields competitive or state of theart performance on several object recognition datasets (CIFAR-10, CIFAR-100,ImageNet). To analyze the network we introduce a new variant of the"deconvolution approach" for visualizing features learned by CNNs, which can beapplied to a broader range of network structures than existing approaches.
arxiv-9900-127 | Topic Modeling of Hierarchical Corpora | http://arxiv.org/pdf/1409.3518v2.pdf | author:Do-kyum Kim, Geoffrey M. Voelker, Lawrence K. Saul category:stat.ML cs.IR cs.LG published:2014-09-11 summary:We study the problem of topic modeling in corpora whose documents areorganized in a multi-level hierarchy. We explore a parametric approach to thisproblem, assuming that the number of topics is known or can be estimated bycross-validation. The models we consider can be viewed as special(finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). Forthese models we show that there exists a simple variational approximation forprobabilistic inference. The approximation relies on a previously unexploitedinequality that handles the conditional dependence between Dirichlet latentvariables in adjacent levels of the model's hierarchy. We compare our approachto existing implementations of nonparametric HDPs. On several benchmarks wefind that our approach is faster than Gibbs sampling and able to learn morepredictive models than existing variational methods. Finally, we demonstratethe large-scale viability of our approach on two newly available corpora fromresearchers in computer security---one with 350,000 documents and over 6,000internal subcategories, the other with a five-level deep hierarchy.
arxiv-9900-128 | Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation | http://arxiv.org/pdf/1503.06813v2.pdf | author:Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang category:cs.CV published:2015-03-23 summary:Due to large variations in shape, appearance, and viewing conditions, objectrecognition is a key precursory challenge in the fields of object manipulationand robotic/AI visual reasoning in general. Recognizing object categories,particular instances of objects and viewpoints/poses of objects are threecritical subproblems robots must solve in order to accurately grasp/manipulateobjects and reason about their environments. Multi-view images of the sameobject lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g.visual/depth descriptor spaces). These object manifolds share the same topologydespite being geometrically different. Each object manifold can be representedas a deformed version of a unified manifold. The object manifolds can thus beparameterized by its homeomorphic mapping/reconstruction from the unifiedmanifold. In this work, we develop a novel framework to jointly solve the threechallenging recognition sub-problems, by explicitly modeling the deformationsof object manifolds and factorizing it in a view-invariant space forrecognition. We perform extensive experiments on several challenging datasetsand achieve state-of-the-art results.
arxiv-9900-129 | Robust, scalable and fast bootstrap method for analyzing large scale data | http://arxiv.org/pdf/1504.02382v2.pdf | author:Shahab Basiri, Esa Ollila, Visa Koivunen category:stat.ME cs.IR cs.IT math.IT stat.CO stat.ML published:2015-04-09 summary:In this paper we address the problem of performing statistical inference forlarge scale data sets i.e., Big Data. The volume and dimensionality of the datamay be so high that it cannot be processed or stored in a single computingnode. We propose a scalable, statistically robust and computationally efficientbootstrap method, compatible with distributed processing and storage systems.Bootstrap resamples are constructed with smaller number of distinct data pointson multiple disjoint subsets of data, similarly to the bag of little bootstrapmethod (BLB) [1]. Then significant savings in computation is achieved byavoiding the re-computation of the estimator for each bootstrap sample.Instead, a computationally efficient fixed-point estimation equation isanalytically solved via a smart approximation following the Fast and RobustBootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the useof highly robust statistical methods in analyzing large scale data sets. Thefavorable statistical properties of the method are established analytically.Numerical examples demonstrate scalability, low complexity and robuststatistical performance of the method in analyzing large data sets.
arxiv-9900-130 | Geometric Representations of Random Hypergraphs | http://arxiv.org/pdf/0912.3648v3.pdf | author:Simón Lunagómez, Sayan Mukherjee, Robert L. Wolpert, Edoardo M. Airoldi category:math.ST math.PR stat.ML stat.TH 60K35 published:2009-12-18 summary:A parametrization of hypergraphs based on the geometry of points in$\mathbf{R}^d$ is developed. Informative prior distributions on hypergraphs areinduced through this parametrization by priors on point configurations viaspatial processes. This prior specification is used to infer conditionalindependence models or Markov structure of multivariate distributions.Specifically, we can recover both the junction tree factorization as well asthe hyper Markov law. This approach offers greater control on the distributionof graph features than Erd\"os-R\'enyi random graphs, supports inference offactorizations that cannot be retrieved by a graph alone, and leads to newMetropolis\slash Hastings Markov chain Monte Carlo algorithms with both localand global moves in graph space. We illustrate the utility of thisparametrization and prior specification using simulations.
arxiv-9900-131 | Classification with Extreme Learning Machine and Ensemble Algorithms Over Randomly Partitioned Data | http://arxiv.org/pdf/1504.02975v1.pdf | author:Ferhat Özgür Çatak category:cs.LG published:2015-04-12 summary:In this age of Big Data, machine learning based data mining methods areextensively used to inspect large scale data sets. Deriving applicablepredictive modeling from these type of data sets is a challenging obstaclebecause of their high complexity. Opportunity with high data availabilitylevels, automated classification of data sets has become a critical andcomplicated function. In this paper, the power of applying MapReduce basedDistributed AdaBoosting of Extreme Learning Machine (ELM) are explored to buildreliable predictive bag of classification models. Thus, (i) dataset ensemblesare build; (ii) ELM algorithm is used to build weak classification models; and(iii) build a strong classification model from a set of weak classificationmodels. This training model is applied to the publicly available knowledgediscovery and data mining datasets.
arxiv-9900-132 | Computing trading strategies based on financial sentiment data using evolutionary optimization | http://arxiv.org/pdf/1504.02972v1.pdf | author:Ronald Hochreiter category:q-fin.PM cs.NE published:2015-04-12 summary:In this paper we apply evolutionary optimization techniques to computeoptimal rule-based trading strategies based on financial sentiment data. Thesentiment data was extracted from the social media service StockTwits toaccommodate the level of bullishness or bearishness of the online tradingcommunity towards certain stocks. Numerical results for all stocks from the DowJones Industrial Average (DJIA) index are presented and a comparison toclassical risk-return portfolio selection is provided.
arxiv-9900-133 | Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal | http://arxiv.org/pdf/1503.00593v3.pdf | author:Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce category:cs.CV I.4 published:2015-03-02 summary:In this paper, we address the problem of estimating and removing non-uniformmotion blur from a single blurry image. We propose a deep learning approach topredicting the probabilistic distribution of motion blur at the patch levelusing a convolutional neural network (CNN). We further extend the candidate setof motion kernels predicted by the CNN using carefully designed imagerotations. A Markov random field model is then used to infer a densenon-uniform motion blur field enforcing motion smoothness. Finally, motion bluris removed by a non-uniform deblurring model using patch-level image prior.Experimental evaluations show that our approach can effectively estimate andremove complex non-uniform motion blur that is not handled well by previousapproaches.
arxiv-9900-134 | Deep Transform: Cocktail Party Source Separation via Complex Convolution in a Deep Neural Network | http://arxiv.org/pdf/1504.02945v1.pdf | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-04-12 summary:Convolutional deep neural networks (DNN) are state of the art in manyengineering problems but have not yet addressed the issue of how to deal withcomplex spectrograms. Here, we use circular statistics to provide a convenientprobabilistic estimate of spectrogram phase in a complex convolutional DNN. Ina typical cocktail party source separation scenario, we trained a convolutionalDNN to re-synthesize the complex spectrograms of two source speech signalsgiven a complex spectrogram of the monaural mixture - a discriminative deeptransform (DT). We then used this complex convolutional DT to obtainprobabilistic estimates of the magnitude and phase components of the sourcespectrograms. Our separation results are on a par with equivalent binary-maskbased non-complex separation approaches.
arxiv-9900-135 | Capacity-achieving Sparse Superposition Codes via Approximate Message Passing Decoding | http://arxiv.org/pdf/1501.05892v2.pdf | author:Cynthia Rush, Adam Greig, Ramji Venkataramanan category:cs.IT math.IT stat.ML published:2015-01-23 summary:Sparse superposition codes were recently introduced by Barron and Joseph forreliable communication over the AWGN channel at rates approaching the channelcapacity. The codebook is defined in terms of a Gaussian design matrix, andcodewords are sparse linear combinations of columns of the matrix. In thispaper, we propose an approximate message passing decoder for sparsesuperposition codes, whose decoding complexity scales linearly with the size ofthe design matrix. The performance of the decoder is rigorously analyzed and itis shown to asymptotically achieve the AWGN capacity with an appropriate powerallocation. Simulation results are provided to demonstrate the performance ofthe decoder at finite blocklengths. We introduce a power allocation scheme toimprove the empirical performance, and demonstrate how the decoding complexitycan be significantly reduced by using Hadamard design matrices.
arxiv-9900-136 | Generalized Correntropy for Robust Adaptive Filtering | http://arxiv.org/pdf/1504.02931v1.pdf | author:Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, José C. Príncipe category:stat.ML cs.IT math.IT published:2015-04-12 summary:As a robust nonlinear similarity measure in kernel space, correntropy hasreceived increasing attention in domains of machine learning and signalprocessing. In particular, the maximum correntropy criterion (MCC) has recentlybeen successfully applied in robust regression and filtering. The defaultkernel function in correntropy is the Gaussian kernel, which is, of course, notalways the best choice. In this work, we propose a generalized correntropy thatadopts the generalized Gaussian density (GGD) function as the kernel (notnecessarily a Mercer kernel), and present some important properties. We furtherpropose the generalized maximum correntropy criterion (GMCC), and apply it toadaptive filtering. An adaptive algorithm, called the GMCC algorithm, isderived, and the mean square convergence performance is studied. We show thatthe proposed algorithm is very stable and can achieve zero probability ofdivergence (POD). Simulation results confirm the theoretical expectations anddemonstrate the desirable performance of the new algorithm.
arxiv-9900-137 | Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo | http://arxiv.org/pdf/1502.07645v2.pdf | author:Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola category:stat.ML cs.LG published:2015-02-26 summary:We consider the problem of Bayesian learning on sensitive datasets andpresent two simple but somewhat surprising results that connect Bayesianlearning to "differential privacy:, a cryptographic approach to protectindividual-level privacy while permiting database-level utility. Specifically,we show that that under standard assumptions, getting one single sample from aposterior distribution is differentially private "for free". We will see thatestimator is statistically consistent, near optimal and computationallytractable whenever the Bayesian model of interest is consistent, optimal andtractable. Similarly but separately, we show that a recent line of works thatuse stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preservedifferentially privacy with minor or no modifications of the algorithmicprocedure at all, these observations lead to an "anytime" algorithm forBayesian learning under privacy constraint. We demonstrate that it performsmuch better than the state-of-the-art differential private methods on syntheticand real datasets.
arxiv-9900-138 | The entropic barrier: a simple and optimal universal self-concordant barrier | http://arxiv.org/pdf/1412.1587v3.pdf | author:Sébastien Bubeck, Ronen Eldan category:math.OC cs.IT cs.LG math.IT published:2014-12-04 summary:We prove that the Cram\'er transform of the uniform measure on a convex bodyin $\mathbb{R}^n$ is a $(1+o(1)) n$-self-concordant barrier, improving aseminal result of Nesterov and Nemirovski. This gives the first explicitconstruction of a universal barrier for convex bodies with optimalself-concordance parameter. The proof is based on basic geometry of log-concavedistributions, and elementary duality in exponential families.
arxiv-9900-139 | Automatic Discovery and Optimization of Parts for Image Classification | http://arxiv.org/pdf/1412.6598v2.pdf | author:Sobhan Naderi Parizi, Andrea Vedaldi, Andrew Zisserman, Pedro Felzenszwalb category:cs.CV cs.LG published:2014-12-20 summary:Part-based representations have been shown to be very useful for imageclassification. Learning part-based models is often viewed as a two-stageproblem. First, a collection of informative parts is discovered, usingheuristics that promote part distinctiveness and diversity, and thenclassifiers are trained on the vector of part responses. In this paper we unifythe two stages and learn the image classifiers and a set of shared partsjointly. We generate an initial pool of parts by randomly sampling partcandidates and selecting a good subset using L1/L2 regularization. All stepsare driven "directly" by the same objective namely the classification loss on atraining set. This lets us do away with engineered heuristics. We alsointroduce the notion of "negative parts", intended as parts that are negativelycorrelated with one or more classes. Negative parts are complementary to theparts discovered by other methods, which look only for positive correlations.
arxiv-9900-140 | Gradual Training Method for Denoising Auto Encoders | http://arxiv.org/pdf/1504.02902v1.pdf | author:Alexander Kalmanovich, Gal Chechik category:cs.LG cs.NE published:2015-04-11 summary:Stacked denoising auto encoders (DAEs) are well known to learn useful deeprepresentations, which can be used to improve supervised training byinitializing a deep network. We investigate a training scheme of a deep DAE,where DAE layers are gradually added and keep adapting as additional layers areadded. We show that in the regime of mid-sized datasets, this gradual trainingprovides a small but consistent improvement over stacked training in bothreconstruction quality and classification error over stacked training on MNISTand CIFAR datasets.
arxiv-9900-141 | On the Convergence Properties of Optimal AdaBoost | http://arxiv.org/pdf/1212.1108v2.pdf | author:Joshua Belanich, Luis E. Ortiz category:cs.LG cs.AI stat.ML I.2.6 published:2012-12-05 summary:AdaBoost is one of the most popular machine-learning algorithms. It is simpleto implement and often found very effective by practitioners, while still beingmathematically elegant and theoretically sound. AdaBoost's behavior inpractice, and in particular the test-error behavior, has puzzled many eminentresearchers for over a decade: It seems to defy our general intuition inmachine learning regarding the fundamental trade-off between model complexityand generalization performance. In this paper, we establish the convergence of"Optimal AdaBoost," a term coined by Rudin, Daubechies, and Schapire in 2004.We prove the convergence, with the number of rounds, of the classifier itself,its generalization error, and its resulting margins for fixed data sets, undercertain reasonable conditions. More generally, we prove that the time/per-roundaverage of almost any function of the example weights converges. Our approachis to frame AdaBoost as a dynamical system, to provide sufficient conditionsfor the existence of an invariant measure, and to employ tools from ergodictheory. Unlike previous work, we do not assume AdaBoost cycles; actually, wepresent empirical evidence against it on real-world datasets. Our maintheoretical results hold under a weaker condition. We show sufficient empiricalevidence that Optimal AdaBoost always met the condition on every real-worlddataset we tried. Our results formally ground future convergence-rate analyses,and may even provide opportunities for slight algorithmic modifications tooptimize the generalization ability of AdaBoost classifiers, thus reducing apractitioner's burden of deciding how long to run the algorithm.
arxiv-9900-142 | Prediction of Search Targets From Fixations in Open-World Settings | http://arxiv.org/pdf/1502.05137v3.pdf | author:Hosnieh Sattar, Sabine Müller, Mario Fritz, Andreas Bulling category:cs.CV published:2015-02-18 summary:Previous work on predicting the target of visual search from human fixationsonly considered closed-world settings in which training labels are availableand predictions are performed for a known set of potential targets. In thiswork we go beyond the state of the art by studying search target prediction inan open-world setting in which we no longer assume that we have fixation datato train for the search targets. We present a dataset containing fixation dataof 18 users searching for natural images from three image categories withinsynthesised image collages of about 80 images. In a closed-world baselineexperiment we show that we can predict the correct target image out of acandidate set of five images. We then present a new problem formulation forsearch target prediction in the open-world setting that is based on learningcompatibilities between fixations and potential targets.
arxiv-9900-143 | End-to-End Photo-Sketch Generation via Fully Convolutional Representation Learning | http://arxiv.org/pdf/1501.07180v2.pdf | author:Liliang Zhang, Liang Lin, Xian Wu, Shengyong Ding, Lei Zhang category:cs.CV I.2.10 published:2015-01-28 summary:Sketch-based face recognition is an interesting task in vision and multimediaresearch, yet it is quite challenging due to the great difference between facephotos and sketches. In this paper, we propose a novel approach forphoto-sketch generation, aiming to automatically transform face photos intodetail-preserving personal sketches. Unlike the traditional models synthesizingsketches based on a dictionary of exemplars, we develop a fully convolutionalnetwork to learn the end-to-end photo-sketch mapping. Our approach takes wholeface photos as inputs and directly generates the corresponding sketch imageswith efficient inference and learning, in which the architecture are stacked byonly convolutional kernels of very small sizes. To well capture the personidentity during the photo-sketch transformation, we define our optimizationobjective in the form of joint generative-discriminative minimization. Inparticular, a discriminative regularization term is incorporated into thephoto-sketch generation, enhancing the discriminability of the generated personsketches against other individuals. Extensive experiments on several standardbenchmarks suggest that our approach outperforms other state-of-the-art methodsin both photo-sketch generation and face sketch verification.
arxiv-9900-144 | Towards radio astronomical imaging using an arbitrary basis | http://arxiv.org/pdf/1503.04338v2.pdf | author:Matthias Petschow category:astro-ph.IM cs.CV published:2015-03-14 summary:The new generation of radio telescopes, such as the Square Kilometer Array(SKA), requires dramatic advances in computer hardware and software, in orderto process the large amounts of produced data efficiently. In this document, weexplore a new approach to wide-field imaging. By generalizing the imagereconstruction, which is performed by an inverse Fourier transform, toarbitrary transformations, we gain enormous new possibilities. In particular,we outline an approach that might allow to obtain a sky image of size P times Qin (optimal) O(PQ) time. This could be a step in the direction of real-time,wide-field sky imaging for future telescopes.
arxiv-9900-145 | Quick sensitivity analysis for incremental data modification and its application to leave-one-out CV in linear classification problems | http://arxiv.org/pdf/1504.02870v1.pdf | author:Shota Okumura, Yoshiki Suzuki, Ichiro Takeuchi category:stat.ML cs.LG published:2015-04-11 summary:We introduce a novel sensitivity analysis framework for large scaleclassification problems that can be used when a small number of instances areincrementally added or removed. For quickly updating the classifier in such asituation, incremental learning algorithms have been intensively studied in theliterature. Although they are much more efficient than solving the optimizationproblem from scratch, their computational complexity yet depends on the entiretraining set size. It means that, if the original training set is large,completely solving an incremental learning problem might be still ratherexpensive. To circumvent this computational issue, we propose a novel frameworkthat allows us to make an inference about the updated classifier withoutactually re-optimizing it. Specifically, the proposed framework can quicklyprovide a lower and an upper bounds of a quantity on the unknown updatedclassifier. The main advantage of the proposed framework is that thecomputational cost of computing these bounds depends only on the number ofupdated instances. This property is quite advantageous in a typical sensitivityanalysis task where only a small number of instances are updated. In this paperwe demonstrate that the proposed framework is applicable to various practicalsensitivity analysis tasks, and the bounds provided by the framework are oftensufficiently tight for making desired inferences.
arxiv-9900-146 | Appearance-Based Gaze Estimation in the Wild | http://arxiv.org/pdf/1504.02863v1.pdf | author:Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling category:cs.CV published:2015-04-11 summary:Appearance-based gaze estimation is believed to work well in real-worldsettings, but existing datasets have been collected under controlled laboratoryconditions and methods have been not evaluated across multiple datasets. Inthis work we study appearance-based gaze estimation in the wild. We present theMPIIGaze dataset that contains 213,659 images we collected from 15 participantsduring natural everyday laptop use over more than three months. Our dataset issignificantly more variable than existing ones with respect to appearance andillumination. We also present a method for in-the-wild appearance-based gazeestimation using multimodal convolutional neural networks that significantlyoutperforms state-of-the art methods in the most challenging cross-datasetevaluation. We present an extensive evaluation of several state-of-the-artimage-based gaze estimation algorithms on three current datasets, including ourown. This evaluation provides clear insights and allows us to identify keyresearch challenges of gaze estimation in the wild.
arxiv-9900-147 | High Density Noise Removal by Cascading Algorithms | http://arxiv.org/pdf/1504.02856v1.pdf | author:Arabinda Dash, Sujaya Kumar Sathua category:cs.CV published:2015-04-11 summary:An advanced non-linear cascading filter algorithm for the removal of highdensity salt and pepper noise from the digital images is proposed. The proposedmethod consists of two stages. The first stage Decision base Median Filter(DMF) acts as the preliminary noise removal algorithm. The second stage iseither Modified Decision Base Partial Trimmed Global Mean Filter (MDBPTGMF) orModified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) which isused to remove the remaining noise and enhance the image quality. The DMFalgorithm performs well at low noise density but it fails to remove the noiseat medium and high level. The MDBPTGMF and MDUTMF have excellent performance atlow, medium and high noise density but these reduce the image quality and blurthe image at high noise level. So the basic idea behind this paper is tocombine the advantages of the filters used in both the stages to remove theSalt and Pepper noise and enhance the image quality at all the noise densitylevel. The proposed method is tested against different gray scale images and itgives better Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR) andImage Enhancement Factor (IEF) than the Adaptive Median Filter (AMF), DecisionBase Unsymmetric Trimmed Median Filter (DBUTMF), Modified Decision BaseUnsymmetric Trimmed Median Filter (MDBUTMF) and Decision Base Partial TrimmedGlobal Mean Filter (DBPTGMF).
arxiv-9900-148 | siftservice.com - Turning a Computer Vision algorithm into a World Wide Web Service | http://arxiv.org/pdf/1504.02840v1.pdf | author:Ahmad Pahlavan Tafti, Hamid Hassannia, Zeyun Yu category:cs.CV published:2015-04-11 summary:Image features detection and description is a longstanding topic in computervision and pattern recognition areas. The Scale Invariant Feature Transform(SIFT) is probably the most popular and widely demanded feature descriptorwhich facilitates a variety of computer vision applications such as imageregistration, object tracking, image forgery detection, and 3D surfacereconstruction. This work introduces a Software as a Service (SaaS) basedimplementation of the SIFT algorithm which is freely available athttp://siftservice.com for any academic, educational and research purposes. Theservice provides application-to-application interaction and aims RapidApplication Development (RAD) and also fast prototyping for computer visionstudents and researchers all around the world. An Internet connection is allthey need!
arxiv-9900-149 | Simple Image Description Generator via a Linear Phrase-Based Approach | http://arxiv.org/pdf/1412.8419v3.pdf | author:Remi Lebret, Pedro O. Pinheiro, Ronan Collobert category:cs.CL cs.CV cs.NE published:2014-12-29 summary:Generating a novel textual description of an image is an interesting problemthat connects computer vision and natural language processing. In this paper,we present a simple model that is able to generate descriptive sentences givena sample image. This model has a strong focus on the syntax of thedescriptions. We train a purely bilinear model that learns a metric between animage representation (generated from a previously trained Convolutional NeuralNetwork) and phrases that are used to described them. The system is then ableto infer phrases from a given image sample. Based on caption syntax statistics,we propose a simple language model that can produce relevant descriptions for agiven test image using the phrases inferred. Our approach, which isconsiderably simpler than state-of-the-art models, achieves comparable resultson the recently release Microsoft COCO dataset.
arxiv-9900-150 | Attention for Fine-Grained Categorization | http://arxiv.org/pdf/1412.7054v3.pdf | author:Pierre Sermanet, Andrea Frome, Esteban Real category:cs.CV cs.LG cs.NE published:2014-12-22 summary:This paper presents experiments extending the work of Ba et al. (2014) onrecurrent neural models for attention into less constrained visualenvironments, specifically fine-grained categorization on the Stanford Dogsdata set. In this work we use an RNN of the same structure but substitute amore powerful visual network and perform large-scale pre-training of the visualnetwork outside of the attention RNN. Most work in attention models to datefocuses on tasks with toy or more constrained visual environments, whereas wepresent results for fine-grained categorization better than thestate-of-the-art GoogLeNet classification model. We show that our model learnsto direct high resolution attention to the most discriminative regions withoutany spatial supervision such as bounding boxes, and it is able to discriminatefine-grained dog breeds moderately well even when given only an initiallow-resolution context image and narrow, inexpensive glimpses at faces and furpatterns. This and similar attention models have the major advantage of beingtrained end-to-end, as opposed to other current detection and recognitionpipelines with hand-engineered components where information is lost. While ourmodel is state-of-the-art, further work is needed to fully leverage thesequential input.
arxiv-9900-151 | Modelling Multi-level Power Usage with Latent States and Smooth Functions | http://arxiv.org/pdf/1504.02813v1.pdf | author:Camila P. E. de Souza, Nancy E. Heckman category:stat.ME stat.AP stat.ML published:2015-04-10 summary:We develop and apply a new approach for analyzing a building's business daypower usage. We treat each business day as a replicate and model power usage asarising from two smooth functions, one function giving power usage when thecooling system is off, the other function giving power usage when the coolingsystem is on. The condition "chiller on"/"chiller off" at any particular timecannot be observed directly, thus forming a latent process. In general, ourmethod can be applied to multi-curve data where each curve is driven by alatent state process. The state at any particular point determines a smoothfunction. Thus each curve follows what we call a switching nonparametricregression model. We develop an EM algorithm to estimate the parameters of thelatent process and the function corresponding to each state. We also obtainstandard errors for the parameter estimates of the state process. Simulationsstudies show the frequentist properties of our estimates.
arxiv-9900-152 | Joint RNN-Based Greedy Parsing and Word Composition | http://arxiv.org/pdf/1412.7028v4.pdf | author:Joël Legrand, Ronan Collobert category:cs.LG cs.CL cs.NE published:2014-12-22 summary:This paper introduces a greedy parser based on neural networks, whichleverages a new compositional sub-tree representation. The greedy parser andthe compositional procedure are jointly trained, and tightly depends oneach-other. The composition procedure outputs a vector representation whichsummarizes syntactically (parsing tags) and semantically (words) sub-trees.Composition and tagging is achieved over continuous (word or tag)representations, and recurrent neural networks. We reach F1 performance on parwith well-known existing parsers, while having the advantage of speed, thanksto the greedy nature of the parser. We provide a fully functionalimplementation of the method described in this paper.
arxiv-9900-153 | High-Dimensional Classification for Brain Decoding | http://arxiv.org/pdf/1504.02800v1.pdf | author:Nicole Croteau, Farouk S. Nathoo, Jiguo Cao, Ryan Budney category:stat.ML published:2015-04-10 summary:Brain decoding involves the determination of a subject's cognitive state oran associated stimulus from functional neuroimaging data measuring brainactivity. In this setting the cognitive state is typically characterized by anelement of a finite set, and the neuroimaging data comprise voluminous amountsof spatiotemporal data measuring some aspect of the neural signal. Theassociated statistical problem is one of classification from high-dimensionaldata. We explore the use of functional principal component analysis, mutualinformation networks, and persistent homology for examining the data throughexploratory analysis and for constructing features characterizing the neuralsignal for brain decoding. We review each approach from this perspective, andwe incorporate the features into a classifier based on symmetric multinomiallogistic regression with elastic net regularization. The approaches areillustrated in an application where the task is to infer, from brain activitymeasured with magnetoencephalography (MEG), the type of video stimulus shown toa subject.
arxiv-9900-154 | An Analysis of Unsupervised Pre-training in Light of Recent Advances | http://arxiv.org/pdf/1412.6597v4.pdf | author:Tom Le Paine, Pooya Khorrami, Wei Han, Thomas S. Huang category:cs.CV cs.LG cs.NE published:2014-12-20 summary:Convolutional neural networks perform well on object recognition because of anumber of recent advances: rectified linear units (ReLUs), data augmentation,dropout, and large labelled datasets. Unsupervised data has been proposed asanother way to improve performance. Unfortunately, unsupervised pre-training isnot used by state-of-the-art methods leading to the following question: Isunsupervised pre-training still useful given recent advances? If so, when? Weanswer this in three parts: we 1) develop an unsupervised method thatincorporates ReLUs and recent unsupervised regularization techniques, 2)analyze the benefits of unsupervised pre-training compared to data augmentationand dropout on CIFAR-10 while varying the ratio of unsupervised to supervisedsamples, 3) verify our findings on STL-10. We discover unsupervisedpre-training, as expected, helps when the ratio of unsupervised to supervisedsamples is high, and surprisingly, hurts when the ratio is low. We also useunsupervised pre-training with additional color augmentation to achieve nearstate-of-the-art performance on STL-10.
arxiv-9900-155 | Scheduled denoising autoencoders | http://arxiv.org/pdf/1406.3269v3.pdf | author:Krzysztof J. Geras, Charles Sutton category:cs.LG stat.ML published:2014-06-12 summary:We present a representation learning method that learns features at multipledifferent levels of scale. Working within the unsupervised framework ofdenoising autoencoders, we observe that when the input is heavily corruptedduring training, the network tends to learn coarse-grained features, whereaswhen the input is only slightly corrupted, the network tends to learnfine-grained features. This motivates the scheduled denoising autoencoder,which starts with a high level of noise that lowers as training progresses. Wefind that the resulting representation yields a significant boost on a latersupervised task compared to the original input, or to a standard denoisingautoencoder trained at a single noise level. After supervised fine-tuning ourbest model achieves the lowest ever reported error on the CIFAR-10 data setamong permutation-invariant methods.
arxiv-9900-156 | Phase Transitions in Spectral Community Detection of Large Noisy Networks | http://arxiv.org/pdf/1504.02412v2.pdf | author:Pin-Yu Chen, Alfred O. Hero III category:cs.SI physics.soc-ph stat.ML published:2015-04-09 summary:In this paper, we study the sensitivity of the spectral clustering basedcommunity detection algorithm subject to a Erdos-Renyi type random noise model.We prove phase transitions in community detectability as a function of theexternal edge connection probability and the noisy edge presence probabilityunder a general network model where two arbitrarily connected communities areinterconnected by random external edges. Specifically, the community detectionperformance transitions from almost perfect detectability to low detectabilityas the inter-community edge connection probability exceeds some critical value.We derive upper and lower bounds on the critical value and show that the boundsare identical when the two communities have the same size. The phase transitionresults are validated using network simulations. Using the derived expressionsfor the phase transition threshold we propose a method for estimating thisthreshold from observed data.
arxiv-9900-157 | Fast Convolutional Nets With fbfft: A GPU Performance Evaluation | http://arxiv.org/pdf/1412.7580v3.pdf | author:Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan Piantino, Yann LeCun category:cs.LG cs.DC cs.NE published:2014-12-24 summary:We examine the performance profile of Convolutional Neural Network trainingon the current generation of NVIDIA Graphics Processing Units. We introduce twonew Fast Fourier Transform convolution implementations: one based on NVIDIA'scuFFT library, and another based on a Facebook authored FFT implementation,fbfft, that provides significant speedups over cuFFT (over 1.5x) for wholeCNNs. Both of these convolution implementations are available in open source,and are faster than NVIDIA's cuDNN implementation for many common convolutionallayers (up to 23.5x for some synthetic kernel configurations). We discussdifferent performance regimes of convolutions, comparing areas wherestraightforward time domain convolutions outperform Fourier frequency domainconvolutions. Details on algorithmic applications of NVIDIA GPU hardwarespecifics in the implementation of fbfft are also provided.
arxiv-9900-158 | Deep Networks With Large Output Spaces | http://arxiv.org/pdf/1412.7479v4.pdf | author:Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, Jay Yagnik category:cs.NE cs.LG published:2014-12-23 summary:Deep neural networks have been extremely successful at various image, speech,video recognition tasks because of their ability to model deep structureswithin the data. However, they are still prohibitively expensive to train andapply for problems containing millions of classes in the output layer. Based onthe observation that the key computation common to most neural network layersis a vector/matrix product, we propose a fast locality-sensitive hashingtechnique to approximate the actual dot product enabling us to scale up thetraining and inference to millions of output classes. We evaluate our techniqueon three diverse large-scale recognition tasks and show that our approach cantrain large-scale models at a faster rate (in terms of steps/total time)compared to baseline methods.
arxiv-9900-159 | A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition | http://arxiv.org/pdf/1504.02764v1.pdf | author:Roozbeh Mottaghi, Yu Xiang, Silvio Savarese category:cs.CV published:2015-04-10 summary:Despite the fact that object detection, 3D pose estimation, and sub-categoryrecognition are highly correlated tasks, they are usually addressedindependently from each other because of the huge space of parameters. Tojointly model all of these tasks, we propose a coarse-to-fine hierarchicalrepresentation, where each level of the hierarchy represents objects at adifferent level of granularity. The hierarchical representation preventsperformance loss, which is often caused by the increase in the number ofparameters (as we consider more tasks to model), and the joint modellingenables resolving ambiguities that exist in independent modelling of thesetasks. We augment PASCAL3D+ dataset with annotations for these tasks and showthat our hierarchical model is effective in joint modelling of objectdetection, 3D pose estimation, and sub-category recognition.
arxiv-9900-160 | Move Evaluation in Go Using Deep Convolutional Neural Networks | http://arxiv.org/pdf/1412.6564v2.pdf | author:Chris J. Maddison, Aja Huang, Ilya Sutskever, David Silver category:cs.LG cs.NE published:2014-12-20 summary:The game of Go is more challenging than other board games, due to thedifficulty of constructing a position or move evaluation function. In thispaper we investigate whether deep convolutional networks can be used todirectly represent and learn this knowledge. We train a large 12-layerconvolutional neural network by supervised learning from a database of humanprofessional games. The network correctly predicts the expert move in 55% ofpositions, equalling the accuracy of a 6 dan human player. When the trainedconvolutional network was used directly to play games of Go, without anysearch, it beat the traditional search program GnuGo in 97% of games, andmatched the performance of a state-of-the-art Monte-Carlo tree search thatsimulates a million positions per move.
arxiv-9900-161 | Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison | http://arxiv.org/pdf/1412.7004v2.pdf | author:Pranava Swaroop Madhyastha, Xavier Carreras, Ariadna Quattoni category:cs.CL cs.LG published:2014-12-22 summary:We investigate the problem of inducing word embeddings that are tailored fora particular bilexical relation. Our learning algorithm takes an existinglexical vector space and compresses it such that the resulting word embeddingsare good predictors for a target bilexical relation. In experiments we showthat task-specific embeddings can benefit both the quality and efficiency inlexical prediction tasks.
arxiv-9900-162 | Training Convolutional Networks with Noisy Labels | http://arxiv.org/pdf/1406.2080v4.pdf | author:Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, Rob Fergus category:cs.CV cs.LG cs.NE published:2014-06-09 summary:The availability of large labeled datasets has allowed Convolutional Networkmodels to achieve impressive recognition results. However, in many settingsmanual annotation of the data is impractical; instead our data has noisylabels, i.e. there is some freely available label for each image which may ormay not be accurate. In this paper, we explore the performance ofdiscriminatively-trained Convnets when trained on such noisy data. We introducean extra noise layer into the network which adapts the network outputs to matchthe noisy label distribution. The parameters of this noise layer can beestimated as part of the training process and involve simple modifications tocurrent training infrastructures for deep networks. We demonstrate theapproaches on several datasets, including large scale experiments on theImageNet classification benchmark.
arxiv-9900-163 | Very Deep Convolutional Networks for Large-Scale Image Recognition | http://arxiv.org/pdf/1409.1556v6.pdf | author:Karen Simonyan, Andrew Zisserman category:cs.CV published:2014-09-04 summary:In this work we investigate the effect of the convolutional network depth onits accuracy in the large-scale image recognition setting. Our maincontribution is a thorough evaluation of networks of increasing depth using anarchitecture with very small (3x3) convolution filters, which shows that asignificant improvement on the prior-art configurations can be achieved bypushing the depth to 16-19 weight layers. These findings were the basis of ourImageNet Challenge 2014 submission, where our team secured the first and thesecond places in the localisation and classification tracks respectively. Wealso show that our representations generalise well to other datasets, wherethey achieve state-of-the-art results. We have made our two best-performingConvNet models publicly available to facilitate further research on the use ofdeep visual representations in computer vision.
arxiv-9900-164 | Diffusion Component Analysis: Unraveling Functional Topology in Biological Networks | http://arxiv.org/pdf/1504.02719v1.pdf | author:Hyunghoon Cho, Bonnie Berger, Jian Peng category:q-bio.MN cs.LG cs.SI stat.ML published:2015-04-10 summary:Complex biological systems have been successfully modeled by biochemical andgenetic interaction networks, typically gathered from high-throughput (HTP)data. These networks can be used to infer functional relationships betweengenes or proteins. Using the intuition that the topological role of a gene in anetwork relates to its biological function, local or diffusion based"guilt-by-association" and graph-theoretic methods have had success ininferring gene functions. Here we seek to improve function prediction byintegrating diffusion-based methods with a novel dimensionality reductiontechnique to overcome the incomplete and noisy nature of network data. In thispaper, we introduce diffusion component analysis (DCA), a framework that plugsin a diffusion model and learns a low-dimensional vector representation of eachnode to encode the topological properties of a network. As a proof of concept,we demonstrate DCA's substantial improvement over state-of-the-artdiffusion-based approaches in predicting protein function from molecularinteraction networks. Moreover, our DCA framework can integrate multiplenetworks from heterogeneous sources, consisting of genomic information,biochemical experiments and other resources, to even further improve functionprediction. Yet another layer of performance gain is achieved by integratingthe DCA framework with support vector machines that take our node vectorrepresentations as features. Overall, our DCA framework provides a novelrepresentation of nodes in a network that can be used as a plug-in architectureto other machine learning algorithms to decipher topological properties of andobtain novel insights into interactomes.
arxiv-9900-165 | Deep Structured Output Learning for Unconstrained Text Recognition | http://arxiv.org/pdf/1412.5903v5.pdf | author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-12-18 summary:We develop a representation suitable for the unconstrained recognition ofwords in natural images: the general case of no fixed lexicon and unknownlength. To this end we propose a convolutional neural network (CNN) basedarchitecture which incorporates a Conditional Random Field (CRF) graphicalmodel, taking the whole word image as a single input. The unaries of the CRFare provided by a CNN that predicts characters at each position of the output,while higher order terms are provided by another CNN that detects the presenceof N-grams. We show that this entire model (CRF, character predictor, N-grampredictor) can be jointly optimised by back-propagating the structured outputloss, essentially requiring the system to perform multi-task learning, andtraining uses purely synthetically generated data. The resulting model is amore accurate system on standard real-world text recognition benchmarks thancharacter prediction alone, setting a benchmark for systems that have not beentrained on a particular lexicon. In addition, our model achievesstate-of-the-art accuracy in lexicon-constrained scenarios, without beingspecifically modelled for constrained recognition. To test the generalisationof our model, we also perform experiments with random alpha-numeric strings toevaluate the method when no visual language model is applicable.
arxiv-9900-166 | Gradient of Probability Density Functions based Contrasts for Blind Source Separation (BSS) | http://arxiv.org/pdf/1504.02712v1.pdf | author:Dharmani Bhaveshkumar C category:cs.LG cs.IT math.IT stat.ML 94A17 published:2015-04-10 summary:The article derives some novel independence measures and contrast functionsfor Blind Source Separation (BSS) application. For the $k^{th}$ orderdifferentiable multivariate functions with equal hyper-volumes (region boundedby hyper-surfaces) and with a constraint of bounded support for $k>1$, itproves that equality of any $k^{th}$ order derivatives implies equality of thefunctions. The difference between product of marginal Probability DensityFunctions (PDFs) and joint PDF of a random vector is defined as FunctionDifference (FD) of a random vector. Assuming the PDFs are $k^{th}$ orderdifferentiable, the results on generalized functions are applied to theindependence condition. This brings new sets of independence measures and BSScontrasts based on the $L^p$-Norm, $ p \geq 1$ of - FD, gradient of FD (GFD)and Hessian of FD (HFD). Instead of a conventional two stage indirectestimation method for joint PDF based BSS contrast estimation, a single stagedirect estimation of the contrasts is desired. The article targets both theefficient estimation of the proposed contrasts and extension of the potentialtheory for an information field. The potential theory has a concept ofreference potential and it is used to derive closed form expression for therelative analysis of potential field. Analogous to it, there are introducedconcepts of Reference Information Potential (RIP) and Cross ReferenceInformation Potential (CRIP) based on the potential due to kernel functionsplaced at selected sample points as basis in kernel methods. The quantities areused to derive closed form expressions for information field analysis usingleast squares. The expressions are used to estimate $L^2$-Norm of FD and$L^2$-Norm of GFD based contrasts.
arxiv-9900-167 | N-gram-Based Low-Dimensional Representation for Document Classification | http://arxiv.org/pdf/1412.6277v2.pdf | author:Rémi Lebret, Ronan Collobert category:cs.CL published:2014-12-19 summary:The bag-of-words (BOW) model is the common approach for classifyingdocuments, where words are used as feature for training a classifier. Thisgenerally involves a huge number of features. Some techniques, such as LatentSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have beendesigned to summarize documents in a lower dimension with the least semanticinformation loss. Some semantic information is nevertheless always lost, sinceonly words are considered. Instead, we aim at using information coming fromn-grams to overcome this limitation, while remaining in a low-dimension space.Many approaches, such as the Skip-gram model, provide good word vectorrepresentations very quickly. We propose to average these representations toobtain representations of n-grams. All n-grams are thus embedded in a samesemantic space. A K-means clustering can then group them into semanticconcepts. The number of features is therefore dramatically reduced anddocuments can be represented as bag of semantic concepts. We show that thismodel outperforms LSA and LDA on a sentiment classification task, and yieldssimilar results than a traditional BOW-model with far less features.
arxiv-9900-168 | The Ordered Weighted $\ell_1$ Norm: Atomic Formulation, Projections, and Algorithms | http://arxiv.org/pdf/1409.4271v5.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.DS cs.CV cs.IT cs.LG math.IT published:2014-09-15 summary:The ordered weighted $\ell_1$ norm (OWL) was recently proposed, with twodifferent motivations: its good statistical properties as a sparsity promotingregularizer; the fact that it generalizes the so-called {\it octagonalshrinkage and clustering algorithm for regression} (OSCAR), which has theability to cluster/group regression variables that are highly correlated. Thispaper contains several contributions to the study and application of OWLregularization: the derivation of the atomic formulation of the OWL norm; thederivation of the dual of the OWL norm, based on its atomic formulation; a newand simpler derivation of the proximity operator of the OWL norm; an efficientscheme to compute the Euclidean projection onto an OWL ball; the instantiationof the conditional gradient (CG, also known as Frank-Wolfe) algorithm forlinear regression problems under OWL regularization; the instantiation ofaccelerated projected gradient algorithms for the same class of problems.Finally, a set of experiments give evidence that accelerated projected gradientalgorithms are considerably faster than CG, for the class of problemsconsidered.
arxiv-9900-169 | Unsupervised Neural Architecture for Saliency Detection: Extended Version | http://arxiv.org/pdf/1412.3717v2.pdf | author:Natalia Efremova, Sergey Tarasenko category:cs.CV cs.NE published:2014-11-18 summary:We propose a novel neural network architecture for visual saliencydetections, which utilizes neurophysiologically plausible mechanisms forextraction of salient regions. The model has been significantly inspired byrecent findings from neurophysiology and aimed to simulate the bottom-upprocesses of human selective attention. Two types of features were analyzed:color and direction of maximum variance. The mechanism we employ for processingthose features is PCA, implemented by means of normalized Hebbian learning andthe waves of spikes. To evaluate performance of our model we have conductedpsychological experiment. Comparison of simulation results with those ofexperiment indicates good performance of our model.
arxiv-9900-170 | NICE: Non-linear Independent Components Estimation | http://arxiv.org/pdf/1410.8516v6.pdf | author:Laurent Dinh, David Krueger, Yoshua Bengio category:cs.LG published:2014-10-30 summary:We propose a deep learning framework for modeling complex high-dimensionaldensities called Non-linear Independent Component Estimation (NICE). It isbased on the idea that a good representation is one in which the data has adistribution that is easy to model. For this purpose, a non-lineardeterministic transformation of the data is learned that maps it to a latentspace so as to make the transformed data conform to a factorized distribution,i.e., resulting in independent latent variables. We parametrize thistransformation so that computing the Jacobian determinant and inverse transformis trivial, yet we maintain the ability to learn complex non-lineartransformations, via a composition of simple building blocks, each based on adeep neural network. The training criterion is simply the exact log-likelihood,which is tractable. Unbiased ancestral sampling is also easy. We show that thisapproach yields good generative models on four image datasets and can be usedfor inpainting.
arxiv-9900-171 | Deep Narrow Boltzmann Machines are Universal Approximators | http://arxiv.org/pdf/1411.3784v3.pdf | author:Guido Montufar category:stat.ML cs.LG math.PR published:2014-11-14 summary:We show that deep narrow Boltzmann machines are universal approximators ofprobability distributions on the activities of their visible units, providedthey have sufficiently many hidden layers, each containing the same number ofunits as the visible layer. We show that, within certain parameter domains,deep Boltzmann machines can be studied as feedforward networks. We provideupper and lower bounds on the sufficient depth and width of universalapproximators. These results settle various intuitions regarding undirectednetworks and, in particular, they show that deep narrow Boltzmann machines areat least as compact universal approximators as narrow sigmoid belief networksand restricted Boltzmann machines, with respect to the currently availablebounds for those models.
arxiv-9900-172 | Maximum Entropy Linear Manifold for Learning Discriminative Low-dimensional Representation | http://arxiv.org/pdf/1504.02622v1.pdf | author:Wojciech Marian Czarnecki, Rafał Józefowicz, Jacek Tabor category:cs.LG published:2015-04-10 summary:Representation learning is currently a very hot topic in modern machinelearning, mostly due to the great success of the deep learning methods. Inparticular low-dimensional representation which discriminates classes can notonly enhance the classification procedure, but also make it faster, whilecontrary to the high-dimensional embeddings can be efficiently used for visualbased exploratory data analysis. In this paper we propose Maximum Entropy Linear Manifold (MELM), amultidimensional generalization of Multithreshold Entropy Linear Classifiermodel which is able to find a low-dimensional linear data projection maximizingdiscriminativeness of projected classes. As a result we obtain a linearembedding which can be used for classification, class aware dimensionalityreduction and data visualization. MELM provides highly discriminative 2Dprojections of the data which can be used as a method for constructing robustclassifiers. We provide both empirical evaluation as well as some interesting theoreticalproperties of our objective function such us scale and affine transformationinvariance, connections with PCA and bounding of the expected balanced accuracyerror.
arxiv-9900-173 | Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic Algorithm, Using Symmetric Travelling Salesman Problem | http://arxiv.org/pdf/1504.02590v1.pdf | author:Hassan Ismkhan, Kamran Zamanifar category:cs.NE published:2015-04-10 summary:The Travelling Salesman Problem (TSP) is one of the most famous optimizationproblems. The Genetic Algorithm (GA) is one of metaheuristics that have beenapplied to TSP. The Crossover and mutation operators are two important elementsof GA. There are many TSP solver crossover operators. In this paper, we stateimplementation of some recent TSP solver crossovers at first and then we useeach of them in GA to solve some Symmetric TSP (STSP) instances and finallycompare their effects on speed and accuracy of presented GA.
arxiv-9900-174 | Visual Saliency Based on Multiscale Deep Features | http://arxiv.org/pdf/1503.08663v3.pdf | author:Guanbin Li, Yizhou Yu category:cs.CV published:2015-03-30 summary:Visual saliency is a fundamental problem in both cognitive and computationalsciences, including computer vision. In this CVPR 2015 paper, we discover thata high-quality visual saliency model can be trained with multiscale featuresextracted using a popular deep learning architecture, convolutional neuralnetworks (CNNs), which have had many successes in visual recognition tasks. Forlearning such saliency models, we introduce a neural network architecture,which has fully connected layers on top of CNNs responsible for extractingfeatures at three different scales. We then propose a refinement method toenhance the spatial coherence of our saliency results. Finally, aggregatingmultiple saliency maps computed for different levels of image segmentation canfurther boost the performance, yielding saliency maps better than thosegenerated from a single segmentation. To promote further research andevaluation of visual saliency models, we also construct a new large database of4447 challenging images and their pixelwise saliency annotation. Experimentalresults demonstrate that our proposed method is capable of achievingstate-of-the-art performance on all public benchmarks, improving the F-Measureby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectivelyon these two datasets.
arxiv-9900-175 | Learning Compact Convolutional Neural Networks with Nested Dropout | http://arxiv.org/pdf/1412.7155v4.pdf | author:Chelsea Finn, Lisa Anne Hendricks, Trevor Darrell category:cs.CV cs.LG cs.NE published:2014-12-22 summary:Recently, nested dropout was proposed as a method for ordering representationunits in autoencoders by their information content, without diminishingreconstruction cost. However, it has only been applied to trainingfully-connected autoencoders in an unsupervised setting. We explore the impactof nested dropout on the convolutional layers in a CNN trained bybackpropagation, investigating whether nested dropout can provide a simple andsystematic way to determine the optimal representation size with respect to thedesired accuracy and desired task and data complexity.
arxiv-9900-176 | Fast Optimal Transport Averaging of Neuroimaging Data | http://arxiv.org/pdf/1503.08596v2.pdf | author:Alexandre Gramfort, Gabriel Peyré, Marco Cuturi category:cs.CV published:2015-03-30 summary:Knowing how the Human brain is anatomically and functionally organized at thelevel of a group of healthy individuals or patients is the primary goal ofneuroimaging research. Yet computing an average of brain imaging data definedover a voxel grid or a triangulation remains a challenge. Data are large, thegeometry of the brain is complex and the between subjects variability leads tospatially or temporally non-overlapping effects of interest. To address theproblem of variability, data are commonly smoothed before group linearaveraging. In this work we build on ideas originally introduced by Kantorovichto propose a new algorithm that can average efficiently non-normalized datadefined over arbitrary discrete domains using transportation metrics. We showhow Kantorovich means can be linked to Wasserstein barycenters in order to takeadvantage of an entropic smoothing approach. It leads to a smooth convexoptimization problem and an algorithm with strong convergence guarantees. Weillustrate the versatility of this tool and its empirical behavior onfunctional neuroimaging data, functional MRI and magnetoencephalography (MEG)source estimates, defined on voxel grids and triangulations of the foldedcortical surface.
arxiv-9900-177 | Learning Arbitrary Statistical Mixtures of Discrete Distributions | http://arxiv.org/pdf/1504.02526v1.pdf | author:Jian Li, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy category:cs.LG cs.DS published:2015-04-10 summary:We study the problem of learning from unlabeled samples very generalstatistical mixture models on large finite sets. Specifically, the model to belearned, $\vartheta$, is a probability distribution over probabilitydistributions $p$, where each such $p$ is a probability distribution over $[n]= \{1,2,\dots,n\}$. When we sample from $\vartheta$, we do not observe $p$directly, but only indirectly and in very noisy fashion, by sampling from $[n]$repeatedly, independently $K$ times from the distribution $p$. The problem isto infer $\vartheta$ to high accuracy in transportation (earthmover) distance. We give the first efficient algorithms for learning this mixture modelwithout making any restricting assumptions on the structure of the distribution$\vartheta$. We bound the quality of the solution as a function of the size ofthe samples $K$ and the number of samples used. Our model and results haveapplications to a variety of unsupervised learning scenarios, includinglearning topic models and collaborative filtering.
arxiv-9900-178 | Linearly Supporting Feature Extraction For Automated Estimation Of Stellar Atmospheric Parameters | http://arxiv.org/pdf/1504.02164v2.pdf | author:Xiangru Li, Yu Lu, Georges Comte, Ali Luo, Yongheng Zhao, Yongjun Wang category:astro-ph.SR astro-ph.IM cs.CV published:2015-04-09 summary:We describe a scheme to extract linearly supporting (LSU) features fromstellar spectra to automatically estimate the atmospheric parameters $T_{eff}$,log$~g$, and [Fe/H]. "Linearly supporting" means that the atmosphericparameters can be accurately estimated from the extracted features through alinear model. The successive steps of the process are as follow: first,decompose the spectrum using a wavelet packet (WP) and represent it by thederived decomposition coefficients; second, detect representative spectralfeatures from the decomposition coefficients using the proposed method LeastAbsolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate theatmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detectedfeatures using a linear regression method. One prominent characteristic of thisscheme is its ability to evaluate quantitatively the contribution of eachdetected feature to the atmospheric parameter estimate and also to trace backthe physical significance of that feature. This work also shows that theusefulness of a component depends on both wavelength and frequency. Theproposed scheme has been evaluated on both real spectra from the Sloan DigitalSky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODFmodels. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62features for log$~g$, and 68 features for [Fe/H]. Test consistencies betweenour estimates and those provided by the Spectroscopic Sarameter Pipeline ofSDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$(83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. Forthe synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$(32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H].
arxiv-9900-179 | Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel | http://arxiv.org/pdf/1502.02063v2.pdf | author:Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, Greg Mori category:cs.CV published:2015-02-06 summary:Many visual recognition problems can be approached by counting instances. Todetermine whether an event is present in a long internet video, one could counthow many frames seem to contain the activity. Classifying the activity of agroup of people can be done by counting the actions of individual people.Encoding these cardinality relationships can reduce sensitivity to clutter, inthe form of irrelevant frames or individuals not involved in a group activity.Learned parameters can encode how many instances tend to occur in a class ofinterest. To this end, this paper develops a powerful and flexible framework toinfer any cardinality relation between latent labels in a multi-instance model.Hard or soft cardinality relations can be encoded to tackle diverse levels ofambiguity. Experiments on tasks such as human activity recognition, video eventdetection, and video summarization demonstrate the effectiveness of usingcardinality relations for improving recognition results.
arxiv-9900-180 | Towards Deep Neural Network Architectures Robust to Adversarial Examples | http://arxiv.org/pdf/1412.5068v4.pdf | author:Shixiang Gu, Luca Rigazio category:cs.LG cs.CV cs.NE published:2014-12-11 summary:Recent work has shown deep neural networks (DNNs) to be highly susceptible towell-designed, small perturbations at the input layer, or so-called adversarialexamples. Taking images as an example, such distortions are oftenimperceptible, but can result in 100% mis-classification for a state of the artDNN. We study the structure of adversarial examples and explore networktopology, pre-processing and training strategies to improve the robustness ofDNNs. We perform various experiments to assess the removability of adversarialexamples by corrupting with additional noise and pre-processing with denoisingautoencoders (DAEs). We find that DAEs can remove substantial amounts of theadversarial noise. How- ever, when stacking the DAE with the original DNN, theresulting network can again be attacked by new adversarial examples with evensmaller distortion. As a solution, we propose Deep Contractive Network, a modelwith a new end-to-end training procedure that includes a smoothness penaltyinspired by the contractive autoencoder (CAE). This increases the networkrobustness to adversarial examples, without a significant performance penalty.
arxiv-9900-181 | Memory Networks | http://arxiv.org/pdf/1410.3916v11.pdf | author:Jason Weston, Sumit Chopra, Antoine Bordes category:cs.AI cs.CL stat.ML published:2014-10-15 summary:We describe a new class of learning models called memory networks. Memorynetworks reason with inference components combined with a long-term memorycomponent; they learn how to use these jointly. The long-term memory can beread and written to, with the goal of using it for prediction. We investigatethese models in the context of question answering (QA) where the long-termmemory effectively acts as a (dynamic) knowledge base, and the output is atextual response. We evaluate them on a large-scale QA task, and a smaller, butmore complex, toy task generated from a simulated world. In the latter, we showthe reasoning power of such models by chaining multiple supporting sentences toanswer questions that require understanding the intension of verbs.
arxiv-9900-182 | Leveraging Twitter for Low-Resource Conversational Speech Language Modeling | http://arxiv.org/pdf/1504.02490v1.pdf | author:Aaron Jaech, Mari Ostendorf category:cs.CL published:2015-04-09 summary:In applications involving conversational speech, data sparsity is a limitingfactor in building a better language model. We propose a simple,language-independent method to quickly harvest large amounts of data fromTwitter to supplement a smaller training set that is more closely matched tothe domain. The techniques lead to a significant reduction in perplexity onfour low-resource languages even though the presence on Twitter of theselanguages is relatively small. We also find that the Twitter text is moreuseful for learning word classes than the in-domain text and that use of theseword classes leads to further reductions in perplexity. Additionally, weintroduce a method of using social and textual information to prioritize thedownload queue during the Twitter crawling. This maximizes the amount of usefuldata that can be collected, impacting both perplexity and vocabulary coverage.
arxiv-9900-183 | Video-Based Action Recognition Using Rate-Invariant Analysis of Covariance Trajectories | http://arxiv.org/pdf/1503.06699v2.pdf | author:Zhengwu Zhang, Jingyong Su, Eric Klassen, Huiling Le, Anuj Srivastava category:cs.CV published:2015-03-23 summary:Statistical classification of actions in videos is mostly performed byextracting relevant features, particularly covariance features, from imageframes and studying time series associated with temporal evolutions of thesefeatures. A natural mathematical representation of activity videos is in formof parameterized trajectories on the covariance manifold, i.e. the set ofsymmetric, positive-definite matrices (SPDMs). The variable execution-rates ofactions implies variable parameterizations of the resulting trajectories, andcomplicates their classification. Since action classes are invariant toexecution rates, one requires rate-invariant metrics for comparingtrajectories. A recent paper represented trajectories using their transportedsquare-root vector fields (TSRVFs), defined by parallel translatingscaled-velocity vectors of trajectories to a reference tangent space on themanifold. To avoid arbitrariness of selecting the reference and to reducedistortion introduced during this mapping, we develop a purely intrinsicapproach where SPDM trajectories are represented by redefining their TSRVFs atthe starting points of the trajectories, and analyzed as elements of a vectorbundle on the manifold. Using a natural Riemannain metric on vector bundles ofSPDMs, we compute geodesic paths and geodesic distances between trajectories inthe quotient space of this vector bundle, with respect to there-parameterization group. This makes the resulting comparison of trajectoriesinvariant to their re-parameterization. We demonstrate this framework on twoapplications involving video classification: visual speech recognition orlip-reading and hand-gesture recognition. In both cases we achieve resultseither comparable to or better than the current literature.
arxiv-9900-184 | What Do Deep CNNs Learn About Objects? | http://arxiv.org/pdf/1504.02485v1.pdf | author:Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko category:cs.CV published:2015-04-09 summary:Deep convolutional neural networks learn extremely powerful imagerepresentations, yet most of that power is hidden in the millions of deep-layerparameters. What exactly do these parameters represent? Recent work has startedto analyse CNN representations, finding that, e.g., they are invariant to some2D transformations Fischer et al. (2014), but are confused by particular typesof image noise Nguyen et al. (2014). In this work, we delve deeper and ask: howinvariant are CNNs to object-class variations caused by 3D shape, pose, andphotorealism?
arxiv-9900-185 | Predicting Complete 3D Models of Indoor Scenes | http://arxiv.org/pdf/1504.02437v1.pdf | author:Ruiqi Guo, Chuhang Zou, Derek Hoiem category:cs.CV published:2015-04-09 summary:One major goal of vision is to infer physical models of objects, surfaces,and their layout from sensors. In this paper, we aim to interpret indoor scenesfrom one RGBD image. Our representation encodes the layout of walls, which mustconform to a Manhattan structure but is otherwise flexible, and the layout andextent of objects, modeled with CAD-like 3D shapes. We represent both thevisible and occluded portions of the scene, producing a $complete$ 3D parse.Such a scene interpretation is useful for robotics and visual reasoning, butdifficult to produce due to the well-known challenge of segmentation, the highdegree of occlusion, and the diversity of objects in indoor scene. We take adata-driven approach, generating sets of potential object regions, matching toregions in training images, and transferring and aligning associated 3D modelswhile encouraging fit to observations and overall consistency. We demonstrateencouraging results on the NYU v2 dataset and highlight a variety ofinteresting directions for future work.
arxiv-9900-186 | Collaborative Feature Learning from Social Media | http://arxiv.org/pdf/1502.01423v3.pdf | author:Chen Fang, Hailin Jin, Jianchao Yang, Zhe Lin category:cs.CV published:2015-02-05 summary:Image feature representation plays an essential role in image recognition andrelated tasks. The current state-of-the-art feature learning paradigm issupervised learning from labeled data. However, this paradigm requireslarge-scale category labels, which limits its applicability to domains wherelabels are hard to obtain. In this paper, we propose a new data-driven featurelearning paradigm which does not rely on category labels. Instead, we learnfrom user behavior data collected on social media. Concretely, we use the imagerelationship discovered in the latent space from the user behavior data toguide the image feature learning. We collect a large-scale image and userbehavior dataset from Behance.net. The dataset consists of 1.9 million imagesand over 300 million view records from 1.9 million users. We validate ourfeature learning paradigm on this dataset and find that the learned featuresignificantly outperforms the state-of-the-art image features in learningbetter image similarities. We also show that the learned feature performscompetitively on various recognition benchmarks.
arxiv-9900-187 | Deciding when to stop: Efficient stopping of active learning guided drug-target prediction | http://arxiv.org/pdf/1504.02406v1.pdf | author:Maja Temerinac-Ott, Armaghan W. Naik, Robert F. Murphy category:q-bio.QM cs.LG stat.ML published:2015-04-09 summary:Active learning has shown to reduce the number of experiments needed toobtain high-confidence drug-target predictions. However, in order to actuallysave experiments using active learning, it is crucial to have a method toevaluate the quality of the current prediction and decide when to stop theexperimentation process. Only by applying reliable stoping criteria to activelearning, time and costs in the experimental process can be actually saved. Wecompute active learning traces on simulated drug-target matrices in order tolearn a regression model for the accuracy of the active learner. By analyzingthe performance of the regression model on simulated data, we design stoppingcriteria for previously unseen experimental matrices. We demonstrate on fourpreviously characterized drug effect data sets that applying the stoppingcriteria can result in upto 40% savings of the total experiments for highlyaccurate predictions.
arxiv-9900-188 | Real-time Monocular Object SLAM | http://arxiv.org/pdf/1504.02398v1.pdf | author:Dorian Gálvez-López, Marta Salas, Juan D. Tardós, J. M. M. Montiel category:cs.RO cs.CV published:2015-04-09 summary:We present a real-time object-based SLAM system that leverages the largestobject database to date. Our approach comprises two main components: 1) amonocular SLAM algorithm that exploits object rigidity constraints to improvethe map and find its real scale, and 2) a novel object recognition algorithmbased on bags of binary words, which provides live detections with a databaseof 500 3D objects. The two components work together and benefit each other: theSLAM algorithm accumulates information from the observations of the objects,anchors object features to especial map landmarks and sets constrains on theoptimization. At the same time, objects partially or fully located within themap are used as a prior to guide the recognition algorithm, achieving higherrecall. We evaluate our proposal on five real environments showing improvementson the accuracy of the map and efficiency with respect to otherstate-of-the-art techniques.
arxiv-9900-189 | Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs | http://arxiv.org/pdf/1412.7062v3.pdf | author:Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille category:cs.CV cs.LG cs.NE published:2014-12-22 summary:Deep Convolutional Neural Networks (DCNNs) have recently shown state of theart performance in high level vision tasks, such as image classification andobject detection. This work brings together methods from DCNNs andprobabilistic graphical models for addressing the task of pixel-levelclassification (also called "semantic image segmentation"). We show thatresponses at the final layer of DCNNs are not sufficiently localized foraccurate object segmentation. This is due to the very invariance propertiesthat make DCNNs good for high level tasks. We overcome this poor localizationproperty of deep networks by combining the responses at the final DCNN layerwith a fully connected Conditional Random Field (CRF). Qualitatively, our"DeepLab" system is able to localize segment boundaries at a level of accuracywhich is beyond previous methods. Quantitatively, our method sets the newstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching71.6% IOU accuracy in the test set. We show how these results can be obtainedefficiently: Careful network re-purposing and a novel application of the 'hole'algorithm from the wavelet community allow dense computation of neural netresponses at 8 frames per second on a modern GPU.
arxiv-9900-190 | A Collection of Challenging Optimization Problems in Science, Engineering and Economics | http://arxiv.org/pdf/1504.02366v1.pdf | author:Dhagash Mehta, Crina Grosan category:cs.NA cs.MS cs.NE math.AG math.NA math.OC published:2015-04-09 summary:Function optimization and finding simultaneous solutions of a system ofnonlinear equations (SNE) are two closely related and important optimizationproblems. However, unlike in the case of function optimization in which one isrequired to find the global minimum and sometimes local minima, a database ofchallenging SNEs where one is required to find stationary points (extrama andsaddle points) is not readily available. In this article, we initiate buildingsuch a database of important SNE (which also includes related functionoptimization problems), arising from Science, Engineering and Economics. Afterproviding a short review of the most commonly used mathematical andcomputational approaches to find solutions of such systems, we provide apreliminary list of challenging problems by writing the Mathematicalformulation down, briefly explaning the origin and importance of the problemand giving a short account on the currently known results, for each of theproblems. We anticipate that this database will not only help benchmarkingnovel numerical methods for solving SNEs and function optimization problems butalso will help advancing the corresponding research areas.
arxiv-9900-191 | Exploring EEG for Object Detection and Retrieval | http://arxiv.org/pdf/1504.02356v1.pdf | author:Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Giró-i-Nieto, Graham Healy, Kevin McGuinness, Noel O'Connor, Alan F. Smeaton category:cs.HC cs.CV cs.IR H.1.2; H.3.3 published:2015-04-09 summary:This paper explores the potential for using Brain Computer Interfaces (BCI)as a relevance feedback mechanism in content-based image retrieval. Weinvestigate if it is possible to capture useful EEG signals to detect ifrelevant objects are present in a dataset of realistic and complex images. Weperform several experiments using a rapid serial visual presentation (RSVP) ofimages at different rates (5Hz and 10Hz) on 8 users with different degrees offamiliarization with BCI and the dataset. We then use the feedback from the BCIand mouse-based interfaces to retrieve localized objects in a subset of TRECVidimages. We show that it is indeed possible to detect such objects in compleximages and, also, that users with previous knowledge on the dataset orexperience with the RSVP outperform others. When the users have limited time toannotate the images (100 seconds in our experiments) both interfaces arecomparable in performance. Comparing our best users in a retrieval task, wefound that EEG-based relevance feedback outperforms mouse-based feedback. Therealistic and complex image dataset differentiates our work from previousstudies on EEG for image retrieval.
arxiv-9900-192 | When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition | http://arxiv.org/pdf/1504.02351v1.pdf | author:Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas, Stan Z. Li, Timothy Hospedales category:cs.CV cs.LG cs.NE published:2015-04-09 summary:Deep learning, in particular Convolutional Neural Network (CNN), has achievedpromising results in face recognition recently. However, it remains an openquestion: why CNNs work well and how to design a 'good' architecture. Theexisting works tend to focus on reporting CNN architectures that work well forface recognition rather than investigate the reason. In this work, we conductan extensive evaluation of CNN-based face recognition systems (CNN-FRS) on acommon ground to make our work easily reproducible. Specifically, we use publicdatabase LFW (Labeled Faces in the Wild) to train CNNs, unlike most existingCNNs trained on private databases. We propose three CNN architectures which arethe first reported architectures trained using LFW data. This paperquantitatively compares the architectures of CNNs and evaluate the effect ofdifferent implementation choices. We identify several useful properties ofCNN-FRS. For instance, the dimensionality of the learned features can besignificantly reduced without adverse effect on face recognition accuracy. Inaddition, traditional metric learning method exploiting CNN-learned features isevaluated. Experiments show two crucial factors to good CNN-FRS performance arethe fusion of multiple CNNs and metric learning. To make our work reproducible,source code and models will be made publicly available.
arxiv-9900-193 | Generative Modeling of Convolutional Neural Networks | http://arxiv.org/pdf/1412.6296v2.pdf | author:Jifeng Dai, Yang Lu, Ying-Nian Wu category:cs.CV cs.LG cs.NE published:2014-12-19 summary:The convolutional neural networks (CNNs) have proven to be a powerful toolfor discriminative learning. Recently researchers have also started to showinterest in the generative aspects of CNNs in order to gain a deeperunderstanding of what they have learned and how to further improve them. Thispaper investigates generative modeling of CNNs. The main contributions include:(1) We construct a generative model for the CNN in the form of exponentialtilting of a reference distribution. (2) We propose a generative gradient forpre-training CNNs by a non-parametric importance sampling scheme, which isfundamentally different from the commonly used discriminative gradient, and yethas the same computational architecture and cost as the latter. (3) We proposea generative visualization method for the CNNs by sampling from an explicitparametric image distribution. The proposed visualization method can directlydraw synthetic samples for any given node in a trained CNN by the HamiltonianMonte Carlo (HMC) algorithm, without resorting to any extra hold-out images.Experiments on the challenging ImageNet benchmark show that the proposedgenerative gradient pre-training consistently helps improve the performances ofCNNs, and the proposed generative visualization method generates meaningful andvaried samples of synthetic images from a large-scale deep CNN.
arxiv-9900-194 | Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor | http://arxiv.org/pdf/1504.02340v1.pdf | author:Wongun Choi category:cs.CV published:2015-04-09 summary:In this paper, we focus on the two key aspects of multiple target trackingproblem: 1) designing an accurate affinity measure to associate detections and2) implementing an efficient and accurate (near) online multiple targettracking algorithm. As the first contribution, we introduce a novel AggregatedLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between apair of temporally distant detections using long term interest pointtrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robustaffinity measure for estimating the likelihood of matching detectionsregardless of the application scenarios. As another contribution, we present aNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem isformulated as a data-association between targets and detections in a temporalwindow, that is performed repeatedly at every frame. While being efficient,NOMT achieves robustness via integrating multiple cues including ALFD metric,target dynamics, appearance similarity, and long term trajectory regularizationinto the model. Our ablative analysis verifies the superiority of the ALFDmetric over the other conventional affinity metrics. We run a comprehensiveexperimental evaluation on two challenging tracking datasets, KITTI and MOTdatasets. The NOMT method combined with ALFD metric achieves the best accuracyin both datasets with significant margins (about 10% higher MOTA) over thestate-of-the-arts.
arxiv-9900-195 | Techniques for Learning Binary Stochastic Feedforward Neural Networks | http://arxiv.org/pdf/1406.2989v3.pdf | author:Tapani Raiko, Mathias Berglund, Guillaume Alain, Laurent Dinh category:stat.ML cs.LG cs.NE published:2014-06-11 summary:Stochastic binary hidden units in a multi-layer perceptron (MLP) network giveat least three potential benefits when compared to deterministic MLP networks.(1) They allow to learn one-to-many type of mappings. (2) They can be used instructured prediction problems, where modeling the internal structure of theoutput is important. (3) Stochasticity has been shown to be an excellentregularizer, which makes generalization performance potentially better ingeneral. However, training stochastic networks is considerably more difficult.We study training using M samples of hidden activations per input. We show thatthe case M=1 leads to a fundamentally different behavior where the networktries to avoid stochasticity. We propose two new estimators for the traininggradient and propose benchmark tests for comparing training algorithms. Ourexperiments confirm that training stochastic networks is difficult and showthat the proposed two estimators perform favorably among all the five knownestimators.
arxiv-9900-196 | Projective simulation with generalization | http://arxiv.org/pdf/1504.02247v1.pdf | author:Alexey A. Melnikov, Adi Makmal, Vedran Dunjko, Hans J. Briegel category:cs.AI cs.LG stat.ML published:2015-04-09 summary:The ability to generalize is an important feature of any intelligent agent.Not only because it may allow the agent to cope with large amounts of data, butalso because in some environments, an agent with no generalization ability issimply doomed to fail. In this work we outline several criteria forgeneralization, and present a dynamic and autonomous machinery that enablesprojective simulation agents to meaningfully generalize. Projective simulation,a novel, physical, approach to artificial intelligence, was recently shown toperform well, in comparison with standard models, on both simple reinforcementlearning problems, as well as on more complicated canonical tasks, such as the"grid world" and the "mountain car problem". Both the basic projectivesimulation model and the presented generalization machinery are based on verysimple principles. This simplicity allows us to provide a full analyticalanalysis of the agent's performance and to illustrate the benefit the agentgains by generalizing. Specifically, we show how such an ability allows theagent to learn in rather extreme environments, in which learning is otherwiseimpossible.
arxiv-9900-197 | Extraction of Protein Sequence Motif Information using PSO K-Means | http://arxiv.org/pdf/1504.02235v1.pdf | author:R. Gowri, R. Rathipriya category:cs.CV published:2015-04-09 summary:The main objective of the paper is to find the motif information.Thefunctionalities of the proteins are ideally found from their motif informationwhich is extracted using various techniques like clustering with k-means,hybrid k-means, self-organising maps, etc., in the literature. In this workprotein sequence information is extracted using optimised k-means algorithm.The particle swarm optimisation technique is one of the frequently usedoptimisation method. In the current work the PSO k-means is used for motifinformation extraction. This paper also deals with the comparison between themotif information obtained from clusters and biclustersusing PSO k-meansalgorithm. The motif information acquired is based on the structure homogeneityof the protein sequence.
arxiv-9900-198 | Phrase-based Image Captioning | http://arxiv.org/pdf/1502.03671v2.pdf | author:Rémi Lebret, Pedro O. Pinheiro, Ronan Collobert category:cs.CL published:2015-02-12 summary:Generating a novel textual description of an image is an interesting problemthat connects computer vision and natural language processing. In this paper,we present a simple model that is able to generate descriptive sentences givena sample image. This model has a strong focus on the syntax of thedescriptions. We train a purely bilinear model that learns a metric between animage representation (generated from a previously trained Convolutional NeuralNetwork) and phrases that are used to described them. The system is then ableto infer phrases from a given image sample. Based on caption syntax statistics,we propose a simple language model that can produce relevant descriptions for agiven test image using the phrases inferred. Our approach, which isconsiderably simpler than state-of-the-art models, achieves comparable resultsin two popular datasets for the task: Flickr30k and the recently proposedMicrosoft COCO.
arxiv-9900-199 | Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet | http://arxiv.org/pdf/1411.1045v4.pdf | author:Matthias Kümmerer, Lucas Theis, Matthias Bethge category:cs.CV q-bio.NC stat.AP published:2014-11-04 summary:Recent results suggest that state-of-the-art saliency models perform far fromoptimal in predicting fixations. This lack in performance has been attributedto an inability to model the influence of high-level image features such asobjects. Recent seminal advances in applying deep neural networks to tasks likeobject recognition suggests that they are able to capture this kind ofstructure. However, the enormous amount of training data necessary to trainthese networks makes them difficult to apply directly to saliency prediction.We present a novel way of reusing existing neural networks that have beenpretrained on the task of object recognition in models of fixation prediction.Using the well-known network of Krizhevsky et al. (2012), we come up with a newsaliency model that significantly outperforms all state-of-the-art models onthe MIT Saliency Benchmark. We show that the structure of this network allowsnew insights in the psychophysics of fixation selection and potentially theirneural implementation. To train our network, we build on recent work on themodeling of saliency as point processes.
arxiv-9900-200 | Bayesian estimation of the multifractality parameter for image texture using a Whittle approximation | http://arxiv.org/pdf/1410.4871v2.pdf | author:Sébastien Combrexelle, Herwig Wendt, Nicolas Dobigeon, Jean-Yves Tourneret, Steve McLaughlin, Patrice Abry category:cs.CV stat.ME published:2014-10-17 summary:Texture characterization is a central element in many image processingapplications. Multifractal analysis is a useful signal and image processingtool, yet, the accurate estimation of multifractal parameters for image textureremains a challenge. This is due in the main to the fact that currentestimation procedures consist of performing linear regressions across frequencyscales of the two-dimensional (2D) dyadic wavelet transform, for which only afew such scales are computable for images. The strongly non-Gaussian nature ofmultifractal processes, combined with their complicated dependence structure,makes it difficult to develop suitable models for parameter estimation. Here,we propose a Bayesian procedure that addresses the difficulties in theestimation of the multifractality parameter. The originality of the procedureis threefold: The construction of a generic semi-parametric statistical modelfor the logarithm of wavelet leaders; the formulation of Bayesian estimatorsthat are associated with this model and the set of parameter values admitted bymultifractal theory; the exploitation of a suitable Whittle approximationwithin the Bayesian model which enables the otherwise infeasible evaluation ofthe posterior distribution associated with the model. Performance is assessednumerically for several 2D multifractal processes, for several image sizes anda large range of process parameters. The procedure yields significant benefitsover current benchmark estimators in terms of estimation performance andability to discriminate between the two most commonly used classes ofmultifractal process models. The gains in performance are particularlypronounced for small image sizes, notably enabling for the first time theanalysis of image patches as small as 64x64 pixels.
arxiv-9900-201 | Sample Complexity of Dictionary Learning and other Matrix Factorizations | http://arxiv.org/pdf/1312.3790v3.pdf | author:Rémi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, Matthias Seibert category:stat.ML cs.IT math.IT published:2013-12-13 summary:Many modern tools in machine learning and signal processing, such as sparsedictionary learning, principal component analysis (PCA), non-negative matrixfactorization (NMF), $K$-means clustering, etc., rely on the factorization of amatrix obtained by concatenating high-dimensional vectors from a trainingcollection. While the idealized task would be to optimize the expected qualityof the factors over the underlying distribution of training vectors, it isachieved in practice by minimizing an empirical average over the consideredcollection. The focus of this paper is to provide sample complexity estimatesto uniformly control how much the empirical average deviates from the expectedcost function. Standard arguments imply that the performance of the empiricalpredictor also exhibit such guarantees. The level of genericity of the approachencompasses several possible constraints on the factors (tensor productstructure, shift-invariance, sparsity \ldots), thus providing a unifiedperspective on the sample complexity of several widely used matrixfactorization schemes. The derived generalization bounds behave proportional to$\sqrt{\log(n)/n}$ w.r.t.\ the number of samples $n$ for the considered matrixfactorization techniques.
arxiv-9900-202 | `local' vs. `global' parameters -- breaking the gaussian complexity barrier | http://arxiv.org/pdf/1504.02191v1.pdf | author:Shahar Mendelson category:stat.ML math.ST stat.TH published:2015-04-09 summary:We show that if $F$ is a convex class of functions that is $L$-subgaussian,the error rate of learning problems generated by independent noise isequivalent to a fixed point determined by `local' covering estimates of theclass, rather than by the gaussian averages. To that end, we establish newsharp upper and lower estimates on the error rate for such problems.
arxiv-9900-203 | Mid-level Deep Pattern Mining | http://arxiv.org/pdf/1411.6382v3.pdf | author:Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-11-24 summary:Mid-level visual element discovery aims to find clusters of image patchesthat are both representative and discriminative. In this work, we study thisproblem from the prospective of pattern mining while relying on the recentlypopularized Convolutional Neural Networks (CNNs). Specifically, we find thatfor an image patch, activations extracted from the first fully-connected layerof CNNs have two appealing properties which enable its seamless integrationwith pattern mining. Patterns are then discovered from a large number of CNNactivations of image patches through the well-known association rule mining.When we retrieve and visualize image patches with the same pattern,surprisingly, they are not only visually similar but also semanticallyconsistent. We apply our approach to scene and object classification tasks, anddemonstrate that our approach outperforms all previous works on mid-levelvisual element discovery by a sizeable margin with far fewer elements beingused. Our approach also outperforms or matches recent works using CNN for thesetasks. Source code of the complete system is available online.
arxiv-9900-204 | Generative Class-conditional Autoencoders | http://arxiv.org/pdf/1412.7009v3.pdf | author:Jan Rudy, Graham Taylor category:cs.NE cs.LG published:2014-12-22 summary:Recent work by Bengio et al. (2013) proposes a sampling procedure fordenoising autoencoders which involves learning the transition operator of aMarkov chain. The transition operator is typically unimodal, which limits itscapacity to model complex data. In order to perform efficient sampling fromconditional distributions, we extend this work, both theoretically andalgorithmically, to gated autoencoders (Memisevic, 2013), The proposed model isable to generate convincing class-conditional samples when trained on both theMNIST and TFD datasets.
arxiv-9900-205 | Understanding Locally Competitive Networks | http://arxiv.org/pdf/1410.1165v3.pdf | author:Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez, Jürgen Schmidhuber category:cs.NE cs.LG 68T30, 68T10 I.2.6 published:2014-10-05 summary:Recently proposed neural network activation functions such as rectifiedlinear, maxout, and local winner-take-all have allowed for faster and moreeffective training of deep neural architectures on large and complex datasets.The common trait among these functions is that they implement local competitionbetween small groups of computational units within a layer, so that only partof the network is activated for any given input pattern. In this paper, weattempt to visualize and understand this self-modularization, and suggest aunified explanation for the beneficial properties of such networks. We alsoshow how our insights can be directly useful for efficiently performingretrieval over large datasets using neural networks.
arxiv-9900-206 | Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual Entailment in NTCIR RITE Evaluation Tasks | http://arxiv.org/pdf/1504.02150v1.pdf | author:Wei-Jie Huang, Chao-Lin Liu category:cs.CL cs.AI cs.DL I.2.7 published:2015-04-08 summary:We computed linguistic information at the lexical, syntactic, and semanticlevels for Recognizing Inference in Text (RITE) tasks for both traditional andsimplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,named-entity recognition, and near synonym recognition were employed, andfeatures like counts of common words, statement lengths, negation words, andantonyms were considered to judge the entailment relationships of twostatements, while we explored both heuristics-based functions andmachine-learning approaches. The reported systems showed robustness bysimultaneously achieving second positions in the binary-classification subtasksfor both simplified and traditional Chinese in NTCIR-10 RITE-2. We conductedmore experiments with the test data of NTCIR-9 RITE, with good results. We alsoextended our work to search for better configurations of our classifiers andinvestigated contributions of individual features. This extended work showedinteresting results and should encourage further discussion.
arxiv-9900-207 | Mining and discovering biographical information in Difangzhi with a language-model-based approach | http://arxiv.org/pdf/1504.02148v1.pdf | author:Peter K. Bol, Chao-Lin Liu, Hongsu Wang category:cs.CL cs.CY cs.DL I.2.7 published:2015-04-08 summary:We present results of expanding the contents of the China BiographicalDatabase by text mining historical local gazetteers, difangzhi. The goal of thedatabase is to see how people are connected together, through kinship, socialconnections, and the places and offices in which they served. The gazetteersare the single most important collection of names and offices covering the Songthrough Qing periods. Although we begin with local officials we shalleventually include lists of local examination candidates, people from thelocality who served in government, and notable local figures with biographies.The more data we collect the more connections emerge. The value of doingsystematic text mining work is that we can identify relevant connections thatare either directly informative or can become useful without deep historicalresearch. Academia Sinica is developing a name database for officials in thecentral governments of the Ming and Qing dynasties.
arxiv-9900-208 | Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction | http://arxiv.org/pdf/1504.02147v1.pdf | author:Tom Goldstein, Gavin Taylor, Kawika Barabin, Kent Sayre category:cs.DC cs.LG published:2015-04-08 summary:Recent approaches to distributed model fitting rely heavily on consensusADMM, where each node solves small sub-problems using only local data. Wepropose iterative methods that solve {\em global} sub-problems over an entiredistributed dataset. This is possible using transpose reduction strategies thatallow a single node to solve least-squares over massive datasets withoutputting all the data in one place. This results in simple iterative methodsthat avoid the expensive inner loops required for consensus methods. Todemonstrate the efficiency of this approach, we fit linear classifiers andsparse linear models to datasets over 5 Tb in size using a distributedimplementation with over 7000 cores in far less time than previous approaches.
arxiv-9900-209 | Compact Part-Based Image Representations | http://arxiv.org/pdf/1412.3708v3.pdf | author:Marc Goessling, Yali Amit category:cs.CV cs.LG stat.ML published:2014-12-11 summary:Learning compact, interpretable image representations is a very natural taskwhich has not been solved satisfactorily even for simple classes of binaryimages. In this paper, we review various ways of composing parts (or experts)for binary data and argue that competitive forms of interaction are best suitedto learn low-dimensional representations. We propose a new composition rulewhich discourages parts from focusing on similar structures and which penalizesopposing votes strongly so that abstaining from voting becomes more attractive.We also introduce a novel sequential initialization procedure based on aprocess of oversimplification and correction. Experiments show that with ourapproach very intuitive models can be learned.
arxiv-9900-210 | Residential Demand Response Applications Using Batch Reinforcement Learning | http://arxiv.org/pdf/1504.02125v1.pdf | author:Frederik Ruelens, Bert Claessens, Stijn Vandael, Bart De Schutter, Robert Babuska, Ronnie Belmans category:cs.SY cs.LG published:2015-04-08 summary:Driven by recent advances in batch Reinforcement Learning (RL), this papercontributes to the application of batch RL to demand response. In contrast toconventional model-based approaches, batch RL techniques do not require asystem identification step, which makes them more suitable for a large-scaleimplementation. This paper extends fitted Q-iteration, a standard batch RLtechnique, to the situation where a forecast of the exogenous data is provided.In general, batch RL techniques do not rely on expert knowledge on the systemdynamics or the solution. However, if some expert knowledge is provided, it canbe incorporated by using our novel policy adjustment method. Finally, we tacklethe challenge of finding an open-loop schedule required to participate in theday-ahead market. We propose a model-free Monte-Carlo estimator method thatuses a metric to construct artificial trajectories and we illustrate thismethod by finding the day-ahead schedule of a heat-pump thermostat. Ourexperiments show that batch RL techniques provide a valuable alternative tomodel-based controllers and that they can be used to construct both closed-loopand open-loop policies.
arxiv-9900-211 | Rehabilitation of Count-based Models for Word Vector Representations | http://arxiv.org/pdf/1412.4930v2.pdf | author:Rémi Lebret, Ronan Collobert category:cs.CL published:2014-12-16 summary:Recent works on word representations mostly rely on predictive models.Distributed word representations (aka word embeddings) are trained to optimallypredict the contexts in which the corresponding words tend to appear. Suchmodels have succeeded in capturing word similarties as well as semantic andsyntactic regularities. Instead, we aim at reviving interest in a model basedon counts. We present a systematic study of the use of the Hellinger distanceto extract semantic representations from the word co-occurence statistics oflarge text corpora. We show that this distance gives good performance on wordsimilarity and analogy tasks, with a proper type and size of context, and adimensionality reduction based on a stochastic low-rank approximation. Besidesbeing both simple and intuitive, this method also provides an encoding functionwhich can be used to infer unseen words or phrases. This becomes a clearadvantage compared to predictive models which must train these new words.
arxiv-9900-212 | Supporting Language Learners with the Meanings Of Closed Class Items | http://arxiv.org/pdf/1504.02059v1.pdf | author:Hayat Alrefaie, Allan Ramsay category:cs.AI cs.CL published:2015-04-08 summary:The process of language learning involves the mastery of countless tasks:making the constituent sounds of the language being learned, learning thegrammatical patterns, and acquiring the requisite vocabulary for reception andproduction. While a plethora of computational tools exist to facilitate thefirst and second of these tasks, a number of challenges arise with respect toenabling the third. This paper describes a tool that has been designed tosupport language learners with the challenge of understanding the use ofclosed-class lexical items. The process of learning the Arabic for office is(mktb) is relatively simple and should be possible by means of simplerepetition of the word. However, it is much more difficult to learn andcorrectly use the Arabic equivalent of the word on. The current paper describesa mechanism for the delivery of diagnostic information regarding specificlexical examples, with the aim of clearly demonstrating why a particulartranslation of a given closed-class item may be appropriate in certainsituations but not others, thereby helping learners to understand and use theterm correctly.
arxiv-9900-213 | A Chaotic Dynamical System that Paints | http://arxiv.org/pdf/1504.02010v1.pdf | author:Tuhin Sahai, George Mathew, Amit Surana category:nlin.CD cs.LG published:2015-04-08 summary:Can a dynamical system paint masterpieces such as Da Vinci's Mona Lisa orMonet's Water Lilies? Moreover, can this dynamical system be chaotic in thesense that although the trajectories are sensitive to initial conditions, thesame painting is created every time? Setting aside the creative aspect ofpainting a picture, in this work, we develop a novel algorithm to reproducepaintings and photographs. Combining ideas from ergodic theory and controltheory, we construct a chaotic dynamical system with predetermined statisticalproperties. If one makes the spatial distribution of colors in the picture thetarget distribution, akin to a human, the algorithm first captures large scalefeatures and then goes on to refine small scale features. Beyond reproducingpaintings, this approach is expected to have a wide variety of applicationssuch as uncertainty quantification, sampling for efficient inference inscalable machine learning for big data, and developing effective strategies forsearch and rescue. In particular, our preliminary studies demonstrate that thisalgorithm provides significant acceleration and higher accuracy than competingmethods for Markov Chain Monte Carlo (MCMC).
arxiv-9900-214 | Zero-bias autoencoders and the benefits of co-adapting features | http://arxiv.org/pdf/1402.3337v5.pdf | author:Kishore Konda, Roland Memisevic, David Krueger category:stat.ML cs.CV cs.LG cs.NE published:2014-02-13 summary:Regularized training of an autoencoder typically results in hidden unitbiases that take on large negative values. We show that negative biases are anatural result of using a hidden layer whose responsibility is to bothrepresent the input data and act as a selection mechanism that ensures sparsityof the representation. We then show that negative biases impede the learning ofdata distributions whose intrinsic dimensionality is high. We also propose anew activation function that decouples the two roles of the hidden layer andthat allows us to learn representations on data with very high intrinsicdimensionality, where standard autoencoders typically fail. Since the decoupledactivation function acts like an implicit regularizer, the model can be trainedby minimizing the reconstruction error of training data, without requiring anyadditional regularization.
arxiv-9900-215 | Pixel-wise Deep Learning for Contour Detection | http://arxiv.org/pdf/1504.01989v1.pdf | author:Jyh-Jing Hwang, Tyng-Luh Liu category:cs.CV cs.LG cs.NE published:2015-04-08 summary:We address the problem of contour detection via per-pixel classifications ofedge point. To facilitate the process, the proposed approach leverages withDenseNet, an efficient implementation of multiscale convolutional neuralnetworks (CNNs), to extract an informative feature vector for each pixel anduses an SVM classifier to accomplish contour detection. In the experiment ofcontour detection, we look into the effectiveness of combining per-pixelfeatures from different CNN layers and verify their performance on BSDS500.
arxiv-9900-216 | Decoupled Adapt-then-Combine diffusion networks with adaptive combiners | http://arxiv.org/pdf/1504.01982v1.pdf | author:Jesus Fernandez-Bes, Jerónimo Arenas-García, Magno T. M. Silva, Luis A. Azpicueta-Ruiz category:cs.SY cs.LG published:2015-04-08 summary:In this paper we analyze a novel diffusion strategy for adaptive networkscalled Decoupled Adapt-then-Combine, which keeps a fully local estimate of thesolution for the adaptation step. Our strategy, which is specially convenientfor heterogeneous networks, is compared with the standard Adapt-then-Combinescheme and theoretically analyzed using energy conservation arguments. Suchcomparison shows the need of implementing adaptive combiners for both schemesto obtain a good performance in case of heterogeneous networks. Therefore, wepropose two adaptive rules to learn the combination coefficients that areuseful for our diffusion strategy. Several experiments simulating bothstationary estimation and tracking problems show that our method outperformsstate-of-the-art techniques, becoming a competitive approach in differentscenarios.
arxiv-9900-217 | Protein Contact Prediction by Integrating Joint Evolutionary Coupling Analysis and Supervised Learning | http://arxiv.org/pdf/1312.2988v5.pdf | author:Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.LG math.OC q-bio.BM stat.ML published:2013-12-10 summary:Protein contacts contain important information for protein structure andfunctional study, but contact prediction from sequence remains verychallenging. Both evolutionary coupling (EC) analysis and supervised machinelearning methods are developed to predict contacts, making use of differenttypes of information, respectively. This paper presents a group graphical lasso(GGL) method for contact prediction that integrates joint multi-family ECanalysis and supervised learning. Different from existing single-family ECanalysis that uses residue co-evolution information in only the target proteinfamily, our joint EC analysis uses residue co-evolution in both the targetfamily and its related families, which may have divergent sequences but similarfolds. To implement joint EC analysis, we model a set of related proteinfamilies using Gaussian graphical models (GGM) and then co-estimate theirprecision matrices by maximum-likelihood, subject to the constraint that theprecision matrices shall share similar residue co-evolution patterns. Tofurther improve the accuracy of the estimated precision matrices, we employ asupervised learning method to predict contact probability from a variety ofevolutionary and non-evolutionary information and then incorporate thepredicted probability as prior into our GGL framework. Experiments show thatour method can predict contacts much more accurately than existing methods, andthat our method performs better on both conserved and family-specific contacts.
arxiv-9900-218 | Image Subset Selection Using Gabor Filters and Neural Networks | http://arxiv.org/pdf/1504.01954v1.pdf | author:Heider K. Ali, Anthony Whitehead category:cs.CV published:2015-04-08 summary:An automatic method for the selection of subsets of images, both modern andhistoric, out of a set of landmark large images collected from the Internet ispresented in this paper. This selection depends on the extraction of dominantfeatures using Gabor filtering. Features are selected carefully from apreliminary image set and fed into a neural network as a training data. Themethod collects a large set of raw landmark images containing modern andhistoric landmark images and non-landmark images. The method then processesthese images to classify them as landmark and non-landmark images. Theclassification performance highly depends on the number of candidate featuresof the landmark.
arxiv-9900-219 | MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking | http://arxiv.org/pdf/1504.01942v1.pdf | author:Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, Konrad Schindler category:cs.CV published:2015-04-08 summary:In the recent past, the computer vision community has developed centralizedbenchmarks for the performance evaluation of a variety of tasks, includinggeneric object and pedestrian detection, 3D reconstruction, optical flow,single-object short-term tracking, and stereo estimation. Despite potentialpitfalls of such benchmarks, they have proved to be extremely helpful toadvance the state of the art in the respective area. Interestingly, there hasbeen rather limited work on the standardization of quantitative benchmarks formultiple target tracking. One of the few exceptions is the well-known PETSdataset, targeted primarily at surveillance applications. Despite being widelyused, it is often applied inconsistently, for example involving using differentsubsets of the available data, different ways of training the models, ordiffering evaluation scripts. This paper describes our work toward a novelmultiple object tracking benchmark aimed to address such issues. We discuss thechallenges of creating such a framework, collecting existing and new data,gathering state-of-the-art methods to be tested on the datasets, and finallycreating a unified evaluation system. With MOTChallenge we aim to pave the waytoward a unified evaluation framework for a more meaningful quantification ofmulti-target tracking.
arxiv-9900-220 | Data Mining for Prediction of Human Performance Capability in the Software-Industry | http://arxiv.org/pdf/1504.01934v1.pdf | author:Gaurav Singh Thakur, Anubhav Gupta, Sangita Gupta category:cs.LG published:2015-04-08 summary:The recruitment of new personnel is one of the most essential businessprocesses which affect the quality of human capital within any company. It ishighly essential for the companies to ensure the recruitment of right talent tomaintain a competitive edge over the others in the market. However IT companiesoften face a problem while recruiting new people for their ongoing projects dueto lack of a proper framework that defines a criteria for the selectionprocess. In this paper we aim to develop a framework that would allow anyproject manager to take the right decision for selecting new talent bycorrelating performance parameters with the other domain-specific attributes ofthe candidates. Also, another important motivation behind this project is tocheck the validity of the selection procedure often followed by various bigcompanies in both public and private sectors which focus only on academicscores, GPA/grades of students from colleges and other academic backgrounds. Wetest if such a decision will produce optimal results in the industry or isthere a need for change that offers a more holistic approach to recruitment ofnew talent in the software companies. The scope of this work extends beyond theIT domain and a similar procedure can be adopted to develop a recruitmentframework in other fields as well. Data-mining techniques provide usefulinformation from the historical projects depending on which the hiring-managercan make decisions for recruiting high-quality workforce. This study aims tobridge this hiatus by developing a data-mining framework based on anensemble-learning technique to refocus on the criteria for personnel selection.The results from this research clearly demonstrated that there is a need torefocus on the selection-criteria for quality objectives.
arxiv-9900-221 | Evaluating Two-Stream CNN for Video Classification | http://arxiv.org/pdf/1504.01920v1.pdf | author:Hao Ye, Zuxuan Wu, Rui-Wei Zhao, Xi Wang, Yu-Gang Jiang, Xiangyang Xue category:cs.CV published:2015-04-08 summary:Videos contain very rich semantic information. Traditional hand-craftedfeatures are known to be inadequate in analyzing complex video semantics.Inspired by the huge success of the deep learning methods in analyzing image,audio and text data, significant efforts are recently being devoted to thedesign of deep nets for video analytics. Among the many practical needs,classifying videos (or video clips) based on their major semantic categories(e.g., "skiing") is useful in many applications. In this paper, we conduct anin-depth study to investigate important implementation options that may affectthe performance of deep nets on video classification. Our evaluations areconducted on top of a recent two-stream convolutional neural network (CNN)pipeline, which uses both static frames and motion optical flows, and hasdemonstrated competitive performance against the state-of-the-art methods. Inorder to gain insights and to arrive at a practical guideline, many importantoptions are studied, including network architectures, model fusion, learningparameters and the final prediction methods. Based on the evaluations, verycompetitive results are attained on two popular video classificationbenchmarks. We hope that the discussions and conclusions from this work canhelp researchers in related fields to quickly set up a good basis for furtherinvestigations along this very promising direction.
arxiv-9900-222 | Robust real time face recognition and tracking on gpu using fusion of rgb and depth image | http://arxiv.org/pdf/1504.01883v1.pdf | author:Narmada Naik, G. N Rathna category:cs.CV published:2015-04-08 summary:This paper presents a real-time face recognition system using kinect sensor.The algorithm is implemented on GPU using opencl and significant speedimprovements are observed. We use kinect depth image to increase the robustnessand reduce computational cost of conventional LBP based face recognition. Themain objective of this paper was to perform robust, high speed fusion basedface recognition and tracking. The algorithm is mainly composed of three steps.First step is to detect all faces in the video using viola jones algorithm. Thesecond step is online database generation using a tracking window on the face.A modified LBP feature vector is calculated using fusion information from depthand greyscale image on GPU. This feature vector is used to train a svmclassifier. Third step involves recognition of multiple faces based on ourmodified feature vector.
arxiv-9900-223 | Modeling Brain Circuitry over a Wide Range of Scales | http://arxiv.org/pdf/1502.04110v2.pdf | author:Pascal Fua, Graham Knott category:cs.CV published:2015-02-13 summary:If we are ever to unravel the mysteries of brain function at its mostfundamental level, we will need a precise understanding of how its componentneurons connect to each other. Electron Microscopes (EM) can now provide thenanometer resolution that is needed to image synapses, and thereforeconnections, while Light Microscopes (LM) see at the micrometer resolutionrequired to model the 3D structure of the dendritic network. Since both thetopology and the connection strength are integral parts of the brain's wiringdiagram, being able to combine these two modalities is critically important. In fact, these microscopes now routinely produce high-resolution imagery insuch large quantities that the bottleneck becomes automated processing andinterpretation, which is needed for such data to be exploited to its fullpotential. In this paper, we briefly review the Computer Vision techniques wehave developed at EPFL to address this need. They include delineating dendriticarbors from LM imagery, segmenting organelles from EM, and combining the twointo a consistent representation.
arxiv-9900-224 | Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space | http://arxiv.org/pdf/1504.01840v1.pdf | author:Yegor Tkachenko category:cs.LG published:2015-04-08 summary:The paper outlines a framework for autonomous control of a CRM (customerrelationship management) system. First, it explores how a modified version ofthe widely accepted Recency-Frequency-Monetary Value system of metrics can beused to define the state space of clients or donors. Second, it describes aprocedure to determine the optimal direct marketing action in discrete andcontinuous action space for the given individual, based on his position in thestate space. The procedure involves the use of model-free Q-learning to train adeep neural network that relates a client's position in the state space torewards associated with possible marketing actions. The estimated valuefunction over the client state space can be interpreted as customer lifetimevalue, and thus allows for a quick plug-in estimation of CLV for a givenclient. Experimental results are presented, based on KDD Cup 1998 mailingdataset of donation solicitations.
arxiv-9900-225 | Structured Matrix Completion with Applications to Genomic Data Integration | http://arxiv.org/pdf/1504.01823v1.pdf | author:Tianxi Cai, T. Tony Cai, Anru Zhang category:stat.ME math.ST stat.ML stat.TH published:2015-04-08 summary:Matrix completion has attracted significant recent attention in many fieldsincluding statistics, applied mathematics and electrical engineering. Currentliterature on matrix completion focuses primarily on independent samplingmodels under which the individual observed entries are sampled independently.Motivated by applications in genomic data integration, we propose a newframework of structured matrix completion (SMC) to treat structured missingnessby design. Specifically, our proposed method aims at efficient matrix recoverywhen a subset of the rows and columns of an approximately low-rank matrix areobserved. We provide theoretical justification for the proposed SMC method andderive lower bound for the estimation errors, which together establish theoptimal rate of recovery over certain classes of approximately low-rankmatrices. Simulation studies show that the method performs well in finitesample under a variety of configurations. The method is applied to integrateseveral ovarian cancer genomic studies with different extent of genomicmeasurements, which enables us to construct more accurate prediction rules forovarian cancer survival.
arxiv-9900-226 | Low Rank Representation on Grassmann Manifolds: An Extrinsic Perspective | http://arxiv.org/pdf/1504.01807v1.pdf | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2015-04-08 summary:Many computer vision algorithms employ subspace models to represent data. TheLow-rank representation (LRR) has been successfully applied in subspaceclustering for which data are clustered according to their subspace structures.The possibility of extending LRR on Grassmann manifold is explored in thispaper. Rather than directly embedding Grassmann manifold into a symmetricmatrix space, an extrinsic view is taken by building the self-representation ofLRR over the tangent space of each Grassmannian point. A new algorithm forsolving the proposed Grassmannian LRR model is designed and implemented.Several clustering experiments are conducted on handwritten digits dataset,dynamic texture video clips and YouTube celebrity face video data. Theexperimental results show our method outperforms a number of existing methods.
arxiv-9900-227 | Kernelized Low Rank Representation on Grassmann Manifolds | http://arxiv.org/pdf/1504.01806v1.pdf | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2015-04-08 summary:Low rank representation (LRR) has recently attracted great interest due toits pleasing efficacy in exploring low-dimensional subspace structures embeddedin data. One of its successful applications is subspace clustering which meansdata are clustered according to the subspaces they belong to. In this paper, ata higher level, we intend to cluster subspaces into classes of subspaces. Thisis naturally described as a clustering problem on Grassmann manifold. Thenovelty of this paper is to generalize LRR on Euclidean space onto an LRR modelon Grassmann manifold in a uniform kernelized framework. The new methods havemany applications in computer vision tasks. Several clustering experiments areconducted on handwritten digit images, dynamic textures, human face clips andtraffic scene sequences. The experimental results show that the proposedmethods outperform a number of state-of-the-art subspace clustering methods.
arxiv-9900-228 | A Simple Way to Initialize Recurrent Networks of Rectified Linear Units | http://arxiv.org/pdf/1504.00941v2.pdf | author:Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton category:cs.NE cs.LG published:2015-04-03 summary:Learning long term dependencies in recurrent networks is difficult due tovanishing and exploding gradients. To overcome this difficulty, researchershave developed sophisticated optimization techniques and network architectures.In this paper, we propose a simpler solution that use recurrent neural networkscomposed of rectified linear units. Key to our solution is the use of theidentity matrix or its scaled version to initialize the recurrent weightmatrix. We find that our solution is comparable to LSTM on our four benchmarks:two toy problems involving long-range temporal structures, a large languagemodeling problem and a benchmark speech recognition problem.
arxiv-9900-229 | Transformation Properties of Learned Visual Representations | http://arxiv.org/pdf/1412.7659v3.pdf | author:Taco S. Cohen, Max Welling category:cs.LG cs.CV cs.NE published:2014-12-24 summary:When a three-dimensional object moves relative to an observer, a changeoccurs on the observer's image plane and in the visual representation computedby a learned model. Starting with the idea that a good visual representation isone that transforms linearly under scene motions, we show, using the theory ofgroup representations, that any such representation is equivalent to acombination of the elementary irreducible representations. We derive a strikingrelationship between irreducibility and the statistical dependency structure ofthe representation, by showing that under restricted conditions, irreduciblerepresentations are decorrelated. Under partial observability, as induced bythe perspective projection of a scene onto the image plane, the motion groupdoes not have a linear action on the space of images, so that it becomesnecessary to perform inference over a latent representation that does transformlinearly. This idea is demonstrated in a model of rotating NORB objects thatemploys a latent representation of the non-commutative 3D rotation group SO(3).
arxiv-9900-230 | Consistent Collective Matrix Completion under Joint Low Rank Structure | http://arxiv.org/pdf/1412.2113v3.pdf | author:Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang category:stat.ML cs.LG published:2014-12-05 summary:We address the collective matrix completion problem of jointly recovering acollection of matrices with shared structure from partial (and potentiallynoisy) observations. To ensure well--posedness of the problem, we impose ajoint low rank structure, wherein each component matrix is low rank and thelatent space of the low rank factors corresponding to each entity is sharedacross the entire collection. We first develop a rigorous algebra forrepresenting and manipulating collective--matrix structure, and identifysufficient conditions for consistent estimation of collective matrices. We thenpropose a tractable convex estimator for solving the collective matrixcompletion problem, and provide the first non--trivial theoretical guaranteesfor consistency of collective matrix completion. We show that under reasonableassumptions stated in Section 3.1, with high probability, the proposedestimator exactly recovers the true matrices whenever sample complexityrequirements dictated by Theorem 1 are met. The sample complexity requirementderived in the paper are optimum up to logarithmic factors, and significantlyimprove upon the requirements obtained by trivial extensions of standard matrixcompletion. Finally, we propose a scalable approximate algorithm to solve theproposed convex program, and corroborate our results through simulatedexperiments.
arxiv-9900-231 | Design and Implementation of a 3D Undersea Camera System | http://arxiv.org/pdf/1504.01753v1.pdf | author:Xida Chen, Steve Sutphen, Paul Macoun, Yee-Hong Yang category:cs.CV published:2015-04-07 summary:In this paper, we present the design and development of an undersea camerasystem. The goal of our system is to provide a 3D model of the undersea habitatin a long-term continuous manner. The most important feature of our system isthe use of multiple cameras and multiple projectors, which is able to provideaccurate 3D models with an accuracy of a millimeter. By introducing projectorsin our system, we can use many different structured light methods for differenttasks. There are two main advantages comparing our system with using ROVs orAUVs. First, our system can provide continuous monitoring of the underseahabitat. Second, our system has a low hardware cost. Comparing to existingdeployed camera systems, the advantage of our system is that it can provideaccurate 3D models and provides opportunities for future development ofinnovative algorithms for undersea research.
arxiv-9900-232 | Understanding Minimum Probability Flow for RBMs Under Various Kinds of Dynamics | http://arxiv.org/pdf/1412.6617v6.pdf | author:Daniel Jiwoong Im, Ethan Buchman, Graham W. Taylor category:cs.LG published:2014-12-20 summary:Energy-based models are popular in machine learning due to the elegance oftheir formulation and their relationship to statistical physics. Among these,the Restricted Boltzmann Machine (RBM), and its staple training algorithmcontrastive divergence (CD), have been the prototype for some recentadvancements in the unsupervised training of deep neural networks. However, CDhas limited theoretical motivation, and can in some cases produce undesirablebehavior. Here, we investigate the performance of Minimum Probability Flow(MPF) learning for training RBMs. Unlike CD, with its focus on approximating anintractable partition function via Gibbs sampling, MPF proposes a tractable,consistent, objective function defined in terms of a Taylor expansion of the KLdivergence with respect to sampling dynamics. Here we propose a more generalform for the sampling dynamics in MPF, and explore the consequences ofdifferent choices for these dynamics for training RBMs. Experimental resultsshow MPF outperforming CD for various RBM configurations.
arxiv-9900-233 | Tensor machines for learning target-specific polynomial features | http://arxiv.org/pdf/1504.01697v1.pdf | author:Jiyan Yang, Alex Gittens category:cs.LG stat.ML published:2015-04-07 summary:Recent years have demonstrated that using random feature maps cansignificantly decrease the training and testing times of kernel-basedalgorithms without significantly lowering their accuracy. Regrettably, becauserandom features are target-agnostic, typically thousands of such features arenecessary to achieve acceptable accuracies. In this work, we consider theproblem of learning a small number of explicit polynomial features. Ourapproach, named Tensor Machines, finds a parsimonious set of features byoptimizing over the hypothesis class introduced by Kar and Karnick for randomfeature maps in a target-specific manner. Exploiting a natural connectionbetween polynomials and tensors, we provide bounds on the generalization errorof Tensor Machines. Empirically, Tensor Machines behave favorably on severalreal-world datasets compared to other state-of-the-art techniques for learningpolynomial features, and deliver significantly more parsimonious models.
arxiv-9900-234 | Large Margin Nearest Neighbor Embedding for Knowledge Representation | http://arxiv.org/pdf/1504.01684v1.pdf | author:Miao Fan, Qiang Zhou, Thomas Fang Zheng, Ralph Grishman category:cs.AI cs.CL published:2015-04-07 summary:Traditional way of storing facts in triplets ({\it head\_entity, relation,tail\_entity}), abbreviated as ({\it h, r, t}), makes the knowledge intuitivelydisplayed and easily acquired by mankind, but hardly computed or even reasonedby AI machines. Inspired by the success in applying {\it DistributedRepresentations} to AI-related fields, recent studies expect to represent eachentity and relation with a unique low-dimensional embedding, which is differentfrom the symbolic and atomic framework of displaying knowledge in triplets. Inthis way, the knowledge computing and reasoning can be essentially facilitatedby means of a simple {\it vector calculation}, i.e. ${\bf h} + {\bf r} \approx{\bf t}$. We thus contribute an effective model to learn better embeddingssatisfying the formula by pulling the positive tail entities ${\bf t^{+}}$ toget together and close to {\bf h} + {\bf r} ({\it Nearest Neighbor}), andsimultaneously pushing the negatives ${\bf t^{-}}$ away from the positives${\bf t^{+}}$ via keeping a {\it Large Margin}. We also design a correspondinglearning algorithm to efficiently find the optimal solution based on {\itStochastic Gradient Descent} in iterative fashion. Quantitative experimentsillustrate that our approach can achieve the state-of-the-art performance,compared with several latest methods on some benchmark datasets for twoclassical applications, i.e. {\it Link prediction} and {\it Tripletclassification}. Moreover, we analyze the parameter complexities among all theevaluated models, and analytical results indicate that our model needs fewercomputational resources on outperforming the other methods.
arxiv-9900-235 | From Averaging to Acceleration, There is Only a Step-size | http://arxiv.org/pdf/1504.01577v1.pdf | author:Nicolas Flammarion, Francis Bach category:stat.ML math.OC published:2015-04-07 summary:We show that accelerated gradient descent, averaged gradient descent and theheavy-ball method for non-strongly-convex problems may be reformulated asconstant parameter second-order difference equation algorithms, where stabilityof the system is equivalent to convergence at rate O(1/n 2), where n is thenumber of iterations. We provide a detailed analysis of the eigenvalues of thecorresponding linear dynamical system , showing various oscillatory andnon-oscillatory behaviors, together with a sharp stability result with explicitconstants. We also consider the situation where noisy gradients are available,where we extend our general convergence result, which suggests an alternativealgorithm (i.e., with different step sizes) that exhibits the good aspects ofboth averaging and acceleration.
arxiv-9900-236 | Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification | http://arxiv.org/pdf/1504.01561v1.pdf | author:Zuxuan Wu, Xi Wang, Yu-Gang Jiang, Hao Ye, Xiangyang Xue category:cs.CV cs.MM published:2015-04-07 summary:Classifying videos according to content semantics is an important problemwith a wide range of applications. In this paper, we propose a hybrid deeplearning framework for video classification, which is able to model staticspatial information, short-term motion, as well as long-term temporal clues inthe videos. Specifically, the spatial and the short-term motion features areextracted separately by two Convolutional Neural Networks (CNN). These twotypes of CNN-based features are then combined in a regularized feature fusionnetwork for classification, which is able to learn and utilize featurerelationships for improved performance. In addition, Long Short Term Memory(LSTM) networks are applied on top of the two features to further modellonger-term temporal clues. The main contribution of this work is the hybridlearning framework that can model several important aspects of the video data.We also show that (1) combining the spatial and the short-term motion featuresin the regularized fusion network is better than direct classification andfusion using the CNN with a softmax layer, and (2) the sequence-based LSTM ishighly complementary to the traditional classification strategy withoutconsidering the temporal frame orders. Extensive experiments are conducted ontwo popular and challenging benchmarks, the UCF-101 Human Actions and theColumbia Consumer Videos (CCV). On both benchmarks, our framework achievesto-date the best reported performance: $91.3\%$ on the UCF-101 and $83.5\%$ onthe CCV.
arxiv-9900-237 | Separable time-causal and time-recursive spatio-temporal receptive fields | http://arxiv.org/pdf/1504.01502v1.pdf | author:Tony Lindeberg category:cs.CV q-bio.NC published:2015-04-07 summary:We present an improved model and theory for time-causal and time-recursivespatio-temporal receptive fields, obtained by a combination of Gaussianreceptive fields over the spatial domain and first-order integrators orequivalently truncated exponential filters coupled in cascade over the temporaldomain. Compared to previous spatio-temporal scale-space formulations in termsof non-enhancement of local extrema or scale invariance, these receptive fieldsare based on different scale-space axiomatics over time by ensuringnon-creation of new local extrema or zero-crossings with increasing temporalscale. Specifically, extensions are presented about parameterizing theintermediate temporal scale levels, analysing the resulting temporal dynamicsand transferring the theory to a discrete implementation in terms of recursivefilters over time.
arxiv-9900-238 | Fractional differentiation based image processing | http://arxiv.org/pdf/0910.2381v4.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2009-10-13 summary:There are many resources useful for processing images, most of them freelyavailable and quite friendly to use. In spite of this abundance of tools, astudy of the processing methods is still worthy of efforts. Here, we want todiscuss the possibilities arising from the use of fractional differentialcalculus. This calculus evolved in the research field of pure mathematics until1920, when applied science started to use it. Only recently, fractionalcalculus was involved in image processing methods. As we shall see, thefractional calculation is able to enhance the quality of images, withinteresting possibilities in edge detection and image restoration. We suggestalso the fractional differentiation as a tool to reveal faint objects inastronomical images.
arxiv-9900-239 | Voice based self help System: User Experience Vs Accuracy | http://arxiv.org/pdf/1504.01496v1.pdf | author:Sunil Kumar Kopparapu category:cs.CL published:2015-04-07 summary:In general, self help systems are being increasingly deployed by servicebased industries because they are capable of delivering better customer serviceand increasingly the switch is to voice based self help systems because theyprovide a natural interface for a human to interact with a machine. A speechbased self help system ideally needs a speech recognition engine to convertspoken speech to text and in addition a language processing engine to take careof any misrecognitions by the speech recognition engine. Any off-the-shelfspeech recognition engine is generally a combination of acoustic processing andspeech grammar. While this is the norm, we believe that ideally a speechrecognition application should have in addition to a speech recognition enginea separate language processing engine to give the system better performance. Inthis paper, we discuss ways in which the speech recognition engine and thelanguage processing engine can be combined to give a better user experience.
arxiv-9900-240 | Efficient SDP Inference for Fully-connected CRFs Based on Low-rank Decomposition | http://arxiv.org/pdf/1504.01492v1.pdf | author:Peng Wang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG stat.ML published:2015-04-07 summary:Conditional Random Fields (CRF) have been widely used in a variety ofcomputer vision tasks. Conventional CRFs typically define edges on neighboringimage pixels, resulting in a sparse graph such that efficient inference can beperformed. However, these CRFs fail to model long-range contextualrelationships. Fully-connected CRFs have thus been proposed. While there areefficient approximate inference methods for such CRFs, usually they aresensitive to initialization and make strong assumptions. In this work, wedevelop an efficient, yet general algorithm for inference on fully-connectedCRFs. The algorithm is based on a scalable SDP algorithm and the low- rankapproximation of the similarity/kernel matrix. The core of the proposedalgorithm is a tailored quasi-Newton method that takes advantage of thelow-rank matrix approximation when solving the specialized SDP dual problem.Experiments demonstrate that our method can be applied on fully-connected CRFsthat cannot be solved previously, such as pixel-level image co-segmentation.
arxiv-9900-241 | On-line Handwritten Devanagari Character Recognition using Fuzzy Directional Features | http://arxiv.org/pdf/1504.01488v1.pdf | author:Sunil Kumar Kopparapu, Lajish VL category:cs.CV published:2015-04-07 summary:This paper describes a new feature set for use in the recognition of on-linehandwritten Devanagari script based on Fuzzy Directional Features. Experimentsare conducted for the automatic recognition of isolated handwritten characterprimitives (sub-character units). Initially we describe the proposed featureset, called the Fuzzy Directional Features (FDF) and then show how thesefeatures can be effectively utilized for writer independent characterrecognition. Experimental results show that FDF set perform well for writerindependent data set at stroke level recognition. The main contribution of thispaper is the introduction of a novel feature set and establish experimentallyits ability in recognition of handwritten Devanagari script.
arxiv-9900-242 | Transferring Knowledge from a RNN to a DNN | http://arxiv.org/pdf/1504.01483v1.pdf | author:William Chan, Nan Rosemary Ke, Ian Lane category:cs.LG cs.CL cs.NE stat.ML published:2015-04-07 summary:Deep Neural Network (DNN) acoustic models have yielded many state-of-the-artresults in Automatic Speech Recognition (ASR) tasks. More recently, RecurrentNeural Network (RNN) models have been shown to outperform DNNs counterparts.However, state-of-the-art DNN and RNN models tend to be impractical to deployon embedded systems with limited computational capacity. Traditionally, theapproach for embedded platforms is to either train a small DNN directly, or totrain a small DNN that learns the output distribution of a large DNN. In thispaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. Weuse the RNN model to generate soft alignments and minimize the Kullback-Leiblerdivergence against the small DNN. The small DNN trained on the soft RNNalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 taskcompared to a baseline 4.54 WER or more than 13% relative improvement.
arxiv-9900-243 | Deep Recurrent Neural Networks for Acoustic Modelling | http://arxiv.org/pdf/1504.01482v1.pdf | author:William Chan, Ian Lane category:cs.LG cs.CL cs.NE stat.ML published:2015-04-07 summary:We present a novel deep Recurrent Neural Network (RNN) model for acousticmodelling in Automatic Speech Recognition (ASR). We term our contribution as aTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) withTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory(BLSTM), and a final DNN. The first DNN acts as a feature processor to ourmodel, the BLSTM then generates a context from the sequence acoustic signal,and the final DNN takes the context and models the posterior probabilities ofthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)eval92 task or more than 8% relative improvement over the baseline DNN models.
arxiv-9900-244 | Mobile Phone Based Vehicle License Plate Recognition for Road Policing | http://arxiv.org/pdf/1504.01476v1.pdf | author:Lajish V. L., Sunil Kumar Kopparapu category:cs.CV published:2015-04-07 summary:Identity of a vehicle is done through the vehicle license plate by trafficpolice in general. Au- tomatic vehicle license plate recognition has severalapplications in intelligent traffic management systems. The security situationacross the globe and particularly in India demands a need to equip the trafficpolice with a system that enables them to get instant details of a vehicle. Thesystem should be easy to use, should be mobile, and work 24 x 7. In this paper,we describe a mobile phone based, client-server architected, license platerecognition system. While we use the state of the art image processing andpattern recognition algorithms tuned for Indian conditions to automaticallyrecognize non-uniform license plates, the main contribution is in creating anend to end usable solution. The client application runs on a mobile device anda server application, with access to vehicle information database, is hostedcentrally. The solution enables capture of license plate image captured by thephone camera and passes to the server; on the server the license plate numberis recognized; the data associated with the number plate is then sent back tothe mobile device, instantaneously. We describe the end to end systemarchitecture in detail. A working prototype of the proposed system has beenimplemented in the lab environment.
arxiv-9900-245 | The geometry of kernelized spectral clustering | http://arxiv.org/pdf/1404.7552v3.pdf | author:Geoffrey Schiebinger, Martin J. Wainwright, Bin Yu category:math.ST stat.ML stat.TH published:2014-04-29 summary:Clustering of data sets is a standard problem in many areas of science andengineering. The method of spectral clustering is based on embedding the dataset using a kernel function, and using the top eigenvectors of the normalizedLaplacian to recover the connected components. We study the performance ofspectral clustering in recovering the latent labels of i.i.d. samples from afinite mixture of nonparametric distributions. The difficulty of this labelrecovery problem depends on the overlap between mixture components and howeasily a mixture component is divided into two nonoverlapping components. Whenthe overlap is small compared to the indivisibility of the mixture components,the principal eigenspace of the population-level normalized Laplacian operatoris approximately spanned by the square-root kernelized component densities. Inthe finite sample setting, and under the same assumption, embedded samples fromdifferent components are approximately orthogonal with high probability whenthe sample size is large. As a corollary we control the fraction of samplesmislabeled by spectral clustering under finite mixtures with nonparametriccomponents.
arxiv-9900-246 | A comparative study between proposed Hyper Kurtosis based Modified Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human Brain CT scan images | http://arxiv.org/pdf/1505.06219v1.pdf | author:Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Satyasaran Changdar, Ritwik Burman, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV published:2015-04-07 summary:In this paper, a comparative study between proposed hyper kurtosis basedmodified duo-histogram equalization (HKMDHE) algorithm and contrast limitedadaptive histogram enhancement (CLAHE) has been presented for theimplementation of contrast enhancement and brightness preservation of lowcontrast human brain CT scan images. In HKMDHE algorithm, contrast enhancementis done on the hyper-kurtosis based application. The results are very promisingof proposed HKMDHE technique with improved PSNR values and lesser AMMBE valuesthan CLAHE technique.
arxiv-9900-247 | Totally Corrective Boosting with Cardinality Penalization | http://arxiv.org/pdf/1504.01446v1.pdf | author:Vasil S. Denchev, Nan Ding, Shin Matsushima, S. V. N. Vishwanathan, Hartmut Neven category:cs.LG quant-ph published:2015-04-07 summary:We propose a totally corrective boosting algorithm with explicit cardinalityregularization. The resulting combinatorial optimization problems are not knownto be efficiently solvable with existing classical methods, but emergingquantum optimization technology gives hope for achieving sparser models inpractice. In order to demonstrate the utility of our algorithm, we use adistributed classical heuristic optimizer as a stand-in for quantum hardware.Even though this evaluation methodology incurs large time and resource costs onclassical computing machinery, it allows us to gauge the potential gains ingeneralization performance and sparsity of the resulting boosted ensembles. Ourexperimental results on public data sets commonly used for benchmarking ofboosting algorithms decidedly demonstrate the existence of such advantages. Ifactual quantum optimization were to be used with this algorithm in the future,we would expect equivalent or superior results at much smaller time and energycosts during training. Moreover, studying cardinality-penalized boosting alsosheds light on why unregularized boosting algorithms with early stopping oftenyield better results than their counterparts with explicit convexregularization: Early stopping performs suboptimal cardinality regularization.The results that we present here indicate it is beneficial to explicitly solvethe combinatorial problem still left open at early termination.
arxiv-9900-248 | A Metric to Classify Style of Spoken Speech | http://arxiv.org/pdf/1504.01427v1.pdf | author:Sunil Kopparapu, Saurabh Bhatnagar, K. Sahana, Sathyanarayana, Akhilesh Srivastava, P. V. S. Rao category:cs.CL published:2015-04-06 summary:The ability to classify spoken speech based on the style of speaking is animportant problem. With the advent of BPO's in recent times, specifically thosethat cater to a population other than the local population, it has becomenecessary for BPO's to identify people with certain style of speaking(American, British etc). Today BPO's employ accent analysts to identify peoplehaving the required style of speaking. This process while involving human bias,it is becoming increasingly infeasible because of the high attrition rate inthe BPO industry. In this paper, we propose a new metric, which robustly andaccurately helps classify spoken speech based on the style of speaking. Therole of the proposed metric is substantiated by using it to classify realspeech data collected from over seventy different people working in a BPO. Wecompare the performance of the metric against human experts who independentlycarried out the classification process. Experimental results show that theperformance of the system using the novel metric performs better than twodifferent human expert.
arxiv-9900-249 | Explorations on high dimensional landscapes | http://arxiv.org/pdf/1412.6615v4.pdf | author:Levent Sagun, V. Ugur Guney, Gerard Ben Arous, Yann LeCun category:stat.ML cs.LG published:2014-12-20 summary:Finding minima of a real valued non-convex function over a high dimensionalspace is a major challenge in science. We provide evidence that some suchfunctions that are defined on high dimensional domains have a narrow band ofvalues whose pre-image contains the bulk of its critical points. This is incontrast with the low dimensional picture in which this band is wide. Oursimulations agree with the previous theoretical work on spin glasses thatproves the existence of such a band when the dimension of the domain tends toinfinity. Furthermore our experiments on teacher-student networks with theMNIST dataset establish a similar phenomenon in deep networks. We finallyobserve that both the gradient descent and the stochastic gradient descentmethods can reach this level within the same number of steps.
arxiv-9900-250 | Knowledge driven Offline to Online Script Conversion | http://arxiv.org/pdf/1504.01420v1.pdf | author:Sunil Kopparapu, Devanuj, Akhilesh Srivastava, P. V. S. Rao category:cs.CV published:2015-04-06 summary:The problem of offline to online script conversion is a challenging and anill-posed problem. The interest in offline to online conversion exists becausethere are a plethora of robust algorithms in online script literature which cannot be used on offline scripts. In this paper, we propose a method, based onheuristics, to extract online script information from offline bitmap image. Weshow the performance of the proposed method on a real sample signature offlineimage, whose online information is known.
arxiv-9900-251 | Theano-based Large-Scale Visual Recognition with Multiple GPUs | http://arxiv.org/pdf/1412.2302v4.pdf | author:Weiguang Ding, Ruoyan Wang, Fei Mao, Graham Taylor category:cs.LG published:2014-12-07 summary:In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)implementation and its naive data parallelism on multiple GPUs. Our performanceon 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)run on 1 GPU. To the best of our knowledge, this is the first open-sourcePython-based AlexNet implementation to-date.
arxiv-9900-252 | QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting Patterns | http://arxiv.org/pdf/1504.01383v1.pdf | author:Vlad Niculae, Caroline Suen, Justine Zhang, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.CL cs.SI physics.soc-ph published:2015-04-06 summary:Given the extremely large pool of events and stories available, media outletsneed to focus on a subset of issues and aspects to convey to their audience.Outlets are often accused of exhibiting a systematic bias in this selectionprocess, with different outlets portraying different versions of reality.However, in the absence of objective measures and empirical evidence, thedirection and extent of systematicity remains widely disputed. In this paper we propose a framework based on quoting patterns forquantifying and characterizing the degree to which media outlets exhibitsystematic bias. We apply this framework to a massive dataset of news articlesspanning the six years of Obama's presidency and all of his speeches, andreveal that a systematic pattern does indeed emerge from the outlet's quotingbehavior. Moreover, we show that this pattern can be successfully exploited inan unsupervised prediction setting, to determine which new quotes an outletwill select to broadcast. By encoding bias patterns in a low-rank space weprovide an analysis of the structure of political media coverage. This revealsa latent media bias space that aligns surprisingly well with political ideologyand outlet type. A linguistic analysis exposes striking differences acrossthese latent dimensions, showing how the different types of media outletsportray different realities even when reporting on the same events. Forexample, outlets mapped to the mainstream conservative side of the latent spacefocus on quotes that portray a presidential persona disproportionatelycharacterized by negativity.
arxiv-9900-253 | PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent | http://arxiv.org/pdf/1504.01365v1.pdf | author:Cho-Jui Hsieh, Hsiang-Fu Yu, Inderjit S. Dhillon category:cs.LG published:2015-04-06 summary:Stochastic Dual Coordinate Descent (SDCD) has become one of the mostefficient ways to solve the family of $\ell_2$-regularized empirical riskminimization problems, including linear SVM, logistic regression, and manyothers. The vanilla implementation of DCD is quite slow; however, bymaintaining primal variables while updating dual variables, the time complexityof SDCD can be significantly reduced. Such a strategy forms the core algorithmin the widely-used LIBLINEAR package. In this paper, we parallelize the SDCDalgorithms in LIBLINEAR. In recent research, several synchronized parallel SDCDalgorithms have been proposed, however, they fail to achieve good speedup inthe shared memory multi-core setting. In this paper, we propose a family ofasynchronous stochastic dual coordinate descent algorithms (ASDCD). Each threadrepeatedly selects a random dual variable and conducts coordinate updates usingthe primal variables that are stored in the shared memory. We analyze theconvergence properties when different locking/atomic mechanisms are applied.For implementation with atomic operations, we show linear convergence undermild conditions. For implementation without any atomic operations or locking,we present the first {\it backward error analysis} for ASDCD under themulti-core environment, showing that the converged solution is the exactsolution for a primal problem with perturbed regularizer. Experimental resultsshow that our methods are much faster than previous parallel coordinate descentsolvers.
arxiv-9900-254 | Early Stopping is Nonparametric Variational Inference | http://arxiv.org/pdf/1504.01344v1.pdf | author:Dougal Maclaurin, David Duvenaud, Ryan P. Adams category:stat.ML cs.LG published:2015-04-06 summary:We show that unconverged stochastic gradient descent can be interpreted as aprocedure that samples from a nonparametric variational approximate posteriordistribution. This distribution is implicitly defined as the transformation ofan initial distribution by a sequence of optimization updates. By tracking thechange in entropy over this sequence of transformations during optimization, weform a scalable, unbiased estimate of the variational lower bound on the logmarginal likelihood. We can use this bound to optimize hyperparameters insteadof using cross-validation. This Bayesian interpretation of SGD suggestsimproved, overfitting-resistant optimization procedures, and gives atheoretical foundation for popular tricks such as early stopping andensembling. We investigate the properties of this marginal likelihood estimatoron neural network models.
arxiv-9900-255 | Hyperparameter Search in Machine Learning | http://arxiv.org/pdf/1502.02127v2.pdf | author:Marc Claesen, Bart De Moor category:cs.LG stat.ML published:2015-02-07 summary:We introduce the hyperparameter search problem in the field of machinelearning and discuss its main challenges from an optimization perspective.Machine learning methods attempt to build models that capture some element ofinterest based on given data. Most common learning algorithms feature a set ofhyperparameters that must be determined before training commences. The choiceof hyperparameters can significantly affect the resulting model's performance,but determining good values can be complex; hence a disciplined, theoreticallysound search strategy is essential.
arxiv-9900-256 | Matching-CNN Meets KNN: Quasi-Parametric Human Parsing | http://arxiv.org/pdf/1504.01220v1.pdf | author:Si Liu, Xiaodan Liang, Luoqi Liu, Xiaohui Shen, Jianchao Yang, Changsheng Xu, Liang Lin, Xiaochun Cao, Shuicheng Yan category:cs.CV published:2015-04-06 summary:Both parametric and non-parametric approaches have demonstrated encouragingperformances in the human parsing task, namely segmenting a human image intoseveral semantic regions (e.g., hat, bag, left arm, face). In this work, we aimto develop a new solution with the advantages of both methodologies, namelysupervision from annotated data and the flexibility to use newly annotated(possibly uncommon) images, and present a quasi-parametric human parsing model.Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, theparametric Matching Convolutional Neural Network (M-CNN) is proposed to predictthe matching confidence and displacements of the best matched region in thetesting image for a particular semantic region in one KNN image. Given atesting image, we first retrieve its KNN images from theannotated/manually-parsed human image corpus. Then each semantic region in eachKNN image is matched with confidence to the testing image using M-CNN, and thematched regions from all KNN images are further fused, followed by a superpixelsmoothing procedure to obtain the ultimate human parsing result. The M-CNNdiffers from the classic CNN in that the tailored cross image matching filtersare introduced to characterize the matching between the testing image and thesemantic region of a KNN image. The cross image matching filters are defined atdifferent convolutional layers, each aiming to capture a particular range ofdisplacements. Comprehensive evaluations over a large dataset with 7,700annotated human images well demonstrate the significant performance gain fromthe quasi-parametric model over the state-of-the-arts, for the human parsingtask.
arxiv-9900-257 | Bengali to Assamese Statistical Machine Translation using Moses (Corpus Based) | http://arxiv.org/pdf/1504.01182v1.pdf | author:Nayan Jyoti Kalita, Baharul Islam category:cs.CL published:2015-04-06 summary:Machine dialect interpretation assumes a real part in encouraging man-machinecorrespondence and in addition men-men correspondence in Natural LanguageProcessing (NLP). Machine Translation (MT) alludes to utilizing machine tochange one dialect to an alternate. Statistical Machine Translation is a typeof MT consisting of Language Model (LM), Translation Model (TM) and decoder. Inthis paper, Bengali to Assamese Statistical Machine Translation Model has beencreated by utilizing Moses. Other translation tools like IRSTLM for LanguageModel and GIZA-PP-V1.0.7 for Translation model are utilized within thisframework which is accessible in Linux situations. The purpose of the LM is toencourage fluent output and the purpose of TM is to encourage similaritybetween input and output, the decoder increases the probability of translatedtext in target language. A parallel corpus of 17100 sentences in Bengali andAssamese has been utilized for preparing within this framework. Measurable MTprocedures have not so far been generally investigated for Indian dialects. Itmight be intriguing to discover to what degree these models can help theimmense continuous MT deliberations in the nation.
arxiv-9900-258 | Efficient Dictionary Learning via Very Sparse Random Projections | http://arxiv.org/pdf/1504.01169v1.pdf | author:Farhad Pourkamali-Anaraki, Stephen Becker, Shannon M. Hughes category:stat.ML cs.LG published:2015-04-05 summary:Performing signal processing tasks on compressive measurements of data hasreceived great attention in recent years. In this paper, we extend previouswork on compressive dictionary learning by showing that more general randomprojections may be used, including sparse ones. More precisely, we examinecompressive K-means clustering as a special case of compressive dictionarylearning and give theoretical guarantees for its performance for a very generalclass of random projections. We then propose a memory and computation efficientdictionary learning algorithm, specifically designed for analyzing largevolumes of high-dimensional data, which learns the dictionary from very sparserandom projections. Experimental results demonstrate that our approach allowsfor reduction of computational complexity and memory/data access, withcontrollable loss in accuracy.
arxiv-9900-259 | Heuristic algorithms for obtaining Polynomial Threshold Functions with low densities | http://arxiv.org/pdf/1504.01167v1.pdf | author:Can Eren Sezener, Erhan Oztop category:cs.CC cs.NE published:2015-04-05 summary:In this paper we present several heuristic algorithms, including a GeneticAlgorithm (GA), for obtaining polynomial threshold function (PTF)representations of Boolean functions (BFs) with small number of monomials. Wecompare these among each other and against the algorithm of Oztop viacomputational experiments. The results indicate that our heuristic algorithmsfind more parsimonious representations compared to the those of non-heuristicand GA-based algorithms.
arxiv-9900-260 | Ultra-large alignments using Phylogeny-aware Profiles | http://arxiv.org/pdf/1504.01142v1.pdf | author:Nam-phuong Nguyen, Siavash Mirarab, Keerthana Kumar, Tandy Warnow category:q-bio.GN cs.CE cs.LG published:2015-04-05 summary:Many biological questions, including the estimation of deep evolutionaryhistories and the detection of remote homology between protein sequences, relyupon multiple sequence alignments (MSAs) and phylogenetic trees of largedatasets. However, accurate large-scale multiple sequence alignment is verydifficult, especially when the dataset contains fragmentary sequences. Wepresent UPP, an MSA method that uses a new machine learning technique - theEnsemble of Hidden Markov Models - that we propose here. UPP produces highlyaccurate alignments for both nucleotide and amino acid sequences, even onultra-large datasets or datasets containing fragmentary sequences. UPP isavailable at https://github.com/smirarab/sepp.
arxiv-9900-261 | EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by Noise and Interference | http://arxiv.org/pdf/1504.01072v1.pdf | author:Silvija Kokalj-Filipovic, Larry Greenstein category:cs.LG published:2015-04-05 summary:We propose a method for estimating channel parameters from RSSI measurementsand the lost packet count, which can work in the presence of losses due to bothinterference and signal attenuation below the noise floor. This is especiallyimportant in the wireless networks, such as vehicular, where propagation modelchanges with the density of nodes. The method is based on StochasticExpectation Maximization, where the received data is modeled as a mixture ofdistributions (no/low interference and strong interference), incomplete(censored) due to packet losses. The PDFs in the mixture are Gamma, accordingto the commonly accepted model for wireless signal and interference power. Thisapproach leverages the loss count as additional information, henceoutperforming maximum likelihood estimation, which does not use thisinformation (ML-), for a small number of received RSSI samples. Hence, itallows inexpensive on-line channel estimation from ad-hoc collected data. Themethod also outperforms ML- on uncensored data mixtures, as ML- assumes thatsamples are from a single-mode PDF.
arxiv-9900-262 | Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via Eigenvector and Semidefinite Programming Synchronization | http://arxiv.org/pdf/1504.01070v1.pdf | author:Mihai Cucuringu category:cs.LG cs.SI math.OC stat.ML published:2015-04-05 summary:We consider the classic problem of establishing a statistical ranking of aset of n items given a set of inconsistent and incomplete pairwise comparisonsbetween such items. Instantiations of this problem occur in numerousapplications in data analysis (e.g., ranking teams in sports data), computervision, and machine learning. We formulate the above problem of ranking withincomplete noisy information as an instance of the group synchronizationproblem over the group SO(2) of planar rotations, whose usefulness has beendemonstrated in numerous applications in recent years. Its least squaressolution can be approximated by either a spectral or a semidefinite programming(SDP) relaxation, followed by a rounding procedure. We perform extensivenumerical simulations on both synthetic and real-world data sets, showing thatour proposed method compares favorably to other algorithms from the recentliterature. Existing theoretical guarantees on the group synchronizationproblem imply lower bounds on the largest amount of noise permissible in theranking data while still achieving exact recovery. We propose a similarsynchronization-based algorithm for the rank-aggregation problem, whichintegrates in a globally consistent ranking pairwise comparisons given bydifferent rating systems on the same set of items. We also discuss the problemof semi-supervised ranking when there is available information on the groundtruth rank of a subset of players, and propose an algorithm based on SDP whichrecovers the ranks of the remaining players. Finally, synchronization-basedranking, combined with a spectral technique for the densest subgraph problem,allows one to extract locally-consistent partial rankings, in other words, toidentify the rank of a small subset of players whose pairwise comparisons areless noisy than the rest of the data, which other methods are not able toidentify.
arxiv-9900-263 | Fast algorithms for morphological operations using run-length encoded binary images | http://arxiv.org/pdf/1504.01052v1.pdf | author:Gregor Ehrensperger, Alexander Ostermann, Felix Schwitzer category:cs.CV cs.GR cs.IT math.IT published:2015-04-04 summary:This paper presents innovative algorithms to efficiently compute erosions anddilations of run-length encoded (RLE) binary images with arbitrary shapedstructuring elements. An RLE image is given by a set of runs, where a run is ahorizontal concatenation of foreground pixels. The proposed algorithms extractthe skeleton of the structuring element and build distance tables of the inputimage, which are storing the distance to the next background pixel on the leftand right hand sides. This information is then used to speed up thecalculations of the erosion and dilation operator by enabling the use oftechniques which allow to skip the analysis of certain pixels whenever a hit ormiss occurs. Additionally the input image gets trimmed during the preprocessingsteps on the base of two primitive criteria. Experimental results show theadvantages over other algorithms. The source code of our algorithms isavailable in C++.
arxiv-9900-264 | An Online Approach to Dynamic Channel Access and Transmission Scheduling | http://arxiv.org/pdf/1504.01050v1.pdf | author:Yang Liu, Mingyan Liu category:cs.LG cs.SY published:2015-04-04 summary:Making judicious channel access and transmission scheduling decisions isessential for improving performance as well as energy and spectral efficiencyin multichannel wireless systems. This problem has been a subject of extensivestudy in the past decade, and the resulting dynamic and opportunistic channelaccess schemes can bring potentially significant improvement over traditionalschemes. However, a common and severe limitation of these dynamic schemes isthat they almost always require some form of a priori knowledge of the channelstatistics. A natural remedy is a learning framework, which has also beenextensively studied in the same context, but a typical learning algorithm inthis literature seeks only the best static policy, with performance measured byweak regret, rather than learning a good dynamic channel access policy. Thereis thus a clear disconnect between what an optimal channel access policy canachieve with known channel statistics that actively exploits temporal, spatialand spectral diversity, and what a typical existing learning algorithm aimsfor, which is the static use of a single channel devoid of diversity gain. Inthis paper we bridge this gap by designing learning algorithms that track knownoptimal or sub-optimal dynamic channel access and transmission schedulingpolicies, thereby yielding performance measured by a form of strong regret, theaccumulated difference between the reward returned by an optimal solution whena priori information is available and that by our online algorithm. We do so inthe context of two specific algorithms that appeared in [1] and [2],respectively, the former for a multiuser single-channel setting and the latterfor a single-user multichannel setting. In both cases we show that ouralgorithms achieve sub-linear regret uniform in time and outperforms thestandard weak-regret learning algorithms.
arxiv-9900-265 | Microsoft COCO Captions: Data Collection and Evaluation Server | http://arxiv.org/pdf/1504.00325v2.pdf | author:Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, C. Lawrence Zitnick category:cs.CV cs.CL published:2015-04-01 summary:In this paper we describe the Microsoft COCO Caption dataset and evaluationserver. When completed, the dataset will contain over one and a half millioncaptions describing over 330,000 images. For the training and validationimages, five independent human generated captions will be provided. To ensureconsistency in evaluation of automatic caption generation algorithms, anevaluation server is used. The evaluation server receives candidate captionsand scores them using several popular metrics, including BLEU, METEOR, ROUGEand CIDEr. Instructions for using the evaluation server are provided.
arxiv-9900-266 | A Unified Deep Neural Network for Speaker and Language Recognition | http://arxiv.org/pdf/1504.00923v1.pdf | author:Fred Richardson, Douglas Reynolds, Najim Dehak category:cs.CL cs.CV cs.LG cs.NE stat.ML published:2015-04-03 summary:Learned feature representations and sub-phoneme posteriors from Deep NeuralNetworks (DNNs) have been used separately to produce significant performancegains for speaker and language recognition tasks. In this work we show howthese gains are possible using a single DNN for both speaker and languagerecognition. The unified DNN approach is shown to yield substantial performanceimprovements on the the 2013 Domain Adaptation Challenge speaker recognitiontask (55% reduction in EER for the out-of-domain condition) and on the NIST2011 Language Recognition Evaluation (48% reduction in EER for the 30s testcondition).
arxiv-9900-267 | On model misspecification and KL separation for Gaussian graphical models | http://arxiv.org/pdf/1501.02320v2.pdf | author:Varun Jog, Po-Ling Loh category:cs.IT math.IT math.ST stat.ML stat.TH 62B10 published:2015-01-10 summary:We establish bounds on the KL divergence between two multivariate Gaussiandistributions in terms of the Hamming distance between the edge sets of thecorresponding graphical models. We show that the KL divergence is bounded belowby a constant when the graphs differ by at least one edge; this is essentiallythe tightest possible bound, since classes of graphs exist for which the edgediscrepancy increases but the KL divergence remains bounded above by aconstant. As a natural corollary to our KL lower bound, we also establish asample size requirement for correct model selection via maximum likelihoodestimation. Our results rigorize the notion that it is essential to estimatethe edge structure of a Gaussian graphical model accurately in order toapproximate the true distribution to close precision.
arxiv-9900-268 | Embedding Word Similarity with Neural Machine Translation | http://arxiv.org/pdf/1412.6448v4.pdf | author:Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio category:cs.CL published:2014-12-19 summary:Neural language models learn word representations, or embeddings, thatcapture rich linguistic and conceptual information. Here we investigate theembeddings learned by neural machine translation models, a recently-developedclass of neural language model. We show that embeddings from translation modelsoutperform those learned by monolingual models at tasks that require knowledgeof both conceptual similarity and lexical-syntactic role. We further show thatthese effects hold when translating from both English to French and English toGerman, and argue that the desirable properties of translation embeddingsshould emerge largely independently of the source and target languages.Finally, we apply a new method for training neural translation models with verylarge vocabularies, and show that this vocabulary expansion algorithm resultsin minimal degradation of embedding quality. Our embedding spaces can bequeried in an online demo and downloaded from our web page. Overall, ouranalyses indicate that translation-based embeddings should be used inapplications that require concepts to be organised according to similarityand/or lexical function, while monolingual embeddings are better suited tomodelling (nonspecific) inter-word relatedness.
arxiv-9900-269 | Beta diffusion trees and hierarchical feature allocations | http://arxiv.org/pdf/1408.3378v2.pdf | author:Creighton Heaukulani, David A. Knowles, Zoubin Ghahramani category:stat.ML published:2014-08-14 summary:We define the beta diffusion tree, a random tree structure with a set ofleaves that defines a collection of overlapping subsets of objects, known as afeature allocation. A generative process for the tree structure is defined interms of particles (representing the objects) diffusing in some continuousspace, analogously to the Dirichlet diffusion tree (Neal, 2003), which definesa tree structure over partitions (i.e., non-overlapping subsets) of theobjects. Unlike in the Dirichlet diffusion tree, multiple copies of a particlemay exist and diffuse along multiple branches in the beta diffusion tree, andan object may therefore belong to multiple subsets of particles. We demonstratehow to build a hierarchically-clustered factor analysis model with the betadiffusion tree and how to perform inference over the random tree structureswith a Markov chain Monte Carlo algorithm. We conclude with several numericalexperiments on missing data problems with data sets of gene expressionmicroarrays, international development statistics, and intranationalsocioeconomic measurements.
arxiv-9900-270 | Evaluation Evaluation a Monte Carlo study | http://arxiv.org/pdf/1504.00854v1.pdf | author:David M. W. Powers category:cs.AI cs.CL stat.ML published:2015-04-03 summary:Over the last decade there has been increasing concern about the biasesembodied in traditional evaluation methods for Natural LanguageProcessing/Learning, particularly methods borrowed from Information Retrieval.Without knowledge of the Bias and Prevalence of the contingency being tested,or equivalently the expectation due to chance, the simple conditionalprobabilities Recall, Precision and Accuracy are not meaningful as evaluationmeasures, either individually or in combinations such as F-factor. Theexistence of bias in NLP measures leads to the 'improvement' of systems byincreasing their bias, such as the practice of improving tagging and parsingscores by using most common value (e.g. water is always a Noun) rather than theattempting to discover the correct one. The measures Cohen Kappa and PowersInformedness are discussed as unbiased alternative to Recall and related to thepsychologically significant measure DeltaP. In this paper we will analyze bothbiased and unbiased measures theoretically, characterizing the preciserelationship between all these measures as well as evaluating the evaluationmeasures themselves empirically using a Monte Carlo simulation.
arxiv-9900-271 | The Evolution of First Person Vision Methods: A Survey | http://arxiv.org/pdf/1409.1484v3.pdf | author:Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias Rauterberg category:cs.CV published:2014-09-04 summary:The emergence of new wearable technologies such as action cameras andsmart-glasses has increased the interest of computer vision scientists in theFirst Person perspective. Nowadays, this field is attracting attention andinvestments of companies aiming to develop commercial devices with First PersonVision recording capabilities. Due to this interest, an increasing demand ofmethods to process these videos, possibly in real-time, is expected. Currentapproaches present a particular combinations of different image features andquantitative methods to accomplish specific objectives like object detection,activity recognition, user machine interaction and so on. This paper summarizesthe evolution of the state of the art in First Person Vision video analysisbetween 1997 and 2014, highlighting, among others, most commonly used features,methods, challenges and opportunities within the field.
arxiv-9900-272 | Properties of the Least Squares Temporal Difference learning algorithm | http://arxiv.org/pdf/1301.5220v2.pdf | author:Kamil Ciosek category:stat.ML cs.LG published:2013-01-22 summary:This paper presents four different ways of looking at the well-known LeastSquares Temporal Differences (LSTD) algorithm for computing the value functionof a Markov Reward Process, each of them leading to different insights: theoperator-theory approach via the Galerkin method, the statistical approach viainstrumental variables, the linear dynamical system view as well as the limitof the TD iteration. We also give a geometric view of the algorithm as anoblique projection. Furthermore, there is an extensive comparison of theoptimization problem solved by LSTD as compared to Bellman ResidualMinimization (BRM). We then review several schemes for the regularization ofthe LSTD solution. We then proceed to treat the modification of LSTD for thecase of episodic Markov Reward Processes.
arxiv-9900-273 | The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth Selection in Univariate and Multivariate Kernel Density Estimations | http://arxiv.org/pdf/1504.00781v1.pdf | author:Dharmani Bhaveshkumar C category:cs.LG stat.CO stat.ME stat.ML 11Kxx I.5.0 published:2015-04-03 summary:The article derives a novel Gram-Charlier A (GCA) Series based ExtendedRule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation(KDE). There are existing various bandwidth selection rules achievingminimization of the Asymptotic Mean Integrated Square Error (AMISE) between theestimated probability density function (PDF) and the actual PDF. The rulesdiffer in a way to estimate the integration of the squared second orderderivative of an unknown PDF $(f(\cdot))$, identified as the roughness$R(f''(\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\cdot))$with an assumption that the density being estimated is Gaussian. Intuitively,better estimation of $R(f''(\cdot))$ and consequently better bandwidthselection rules can be derived, if the unknown PDF is approximated through aninfinite series expansion based on a more generalized density assumption. As ademonstration and verification to this concept, the ExROT derived in thearticle uses an extended assumption that the density being estimated is nearGaussian. This helps use of the GCA expansion as an approximation to theunknown near Gaussian PDF. The ExROT for univariate KDE is extended to that formultivariate KDE. The required multivariate AMISE criteria is re-derived usingelementary calculus of several variables, instead of Tensor calculus. Thederivation uses the Kronecker product and the vector differential operator toachieve the AMISE expression in vector notations. There is also derived ExROTfor kernel based density derivative estimator.
arxiv-9900-274 | Learning Mixed Membership Mallows Models from Pairwise Comparisons | http://arxiv.org/pdf/1504.00757v1.pdf | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG stat.ML published:2015-04-03 summary:We propose a novel parameterized family of Mixed Membership Mallows Models(M4) to account for variability in pairwise comparisons generated by aheterogeneous population of noisy and inconsistent users. M4 models individualpreferences as a user-specific probabilistic mixture of shared latent Mallowscomponents. Our key algorithmic insight for estimation is to establish astatistical connection between M4 and topic models by viewing pairwisecomparisons as words, and users as documents. This key insight leads us toexplore Mallows components with a separable structure and leverage recentadvances in separable topic discovery. While separability appears to be overlyrestrictive, we nevertheless show that it is an inevitable outcome of arelatively small number of latent Mallows components in a world of large numberof items. We then develop an algorithm based on robust extreme-pointidentification of convex polygons to learn the reference rankings, and isprovably consistent with polynomial sample complexity guarantees. Wedemonstrate that our new model is empirically competitive with the currentstate-of-the-art approaches in predicting real-world preferences.
arxiv-9900-275 | Unsupervised Feature Selection with Adaptive Structure Learning | http://arxiv.org/pdf/1504.00736v1.pdf | author:Liang Du, Yi-Dong Shen category:cs.LG published:2015-04-03 summary:The problem of feature selection has raised considerable interests in thepast decade. Traditional unsupervised methods select the features which canfaithfully preserve the intrinsic structures of data, where the intrinsicstructures are estimated using all the input features of data. However, theestimated intrinsic structures are unreliable/inaccurate when the redundant andnoisy features are not removed. Therefore, we face a dilemma here: one need thetrue structures of data to identify the informative features, and one need theinformative features to accurately estimate the true structures of data. Toaddress this, we propose a unified learning framework which performs structurelearning and feature selection simultaneously. The structures are adaptivelylearned from the results of feature selection, and the informative features arereselected to preserve the refined structures of data. By leveraging theinteractions between these two essential tasks, we are able to capture accuratestructures and select more informative features. Experimental results on manybenchmark data sets demonstrate that the proposed method outperforms many stateof the art unsupervised feature selection methods.
arxiv-9900-276 | Dimensionality Reduction for k-Means Clustering and Low Rank Approximation | http://arxiv.org/pdf/1410.6801v3.pdf | author:Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, Madalina Persu category:cs.DS cs.LG published:2014-10-24 summary:We show how to approximate a data matrix $\mathbf{A}$ with a much smallersketch $\mathbf{\tilde A}$ that can be used to solve a general class ofconstrained k-rank approximation problems to within $(1+\epsilon)$ error.Importantly, this class of problems includes $k$-means clustering andunconstrained low rank approximation (i.e. principal component analysis). Byreducing data points to just $O(k)$ dimensions, our methods genericallyaccelerate any exact, approximate, or heuristic algorithm for these ubiquitousproblems. For $k$-means dimensionality reduction, we provide $(1+\epsilon)$ relativeerror results for many common sketching techniques, including random rowprojection, column selection, and approximate SVD. For approximate principalcomponent analysis, we give a simple alternative to known algorithms that hasapplications in the streaming setting. Additionally, we extend recent work oncolumn-based matrix reconstruction, giving column subsets that not only `cover'a good subspace for $\bv{A}$, but can be used directly to compute thissubspace. Finally, for $k$-means clustering, we show how to achieve a $(9+\epsilon)$approximation by Johnson-Lindenstrauss projecting data points to just $O(\logk/\epsilon^2)$ dimensions. This gives the first result that leverages thespecific structure of $k$-means to achieve dimension independent of input sizeand sublinear in $k$.
arxiv-9900-277 | Classifier with Hierarchical Topographical Maps as Internal Representation | http://arxiv.org/pdf/1412.6567v4.pdf | author:Thomas Trappenberg, Paul Hollensen, Pitoyo Hartono category:cs.NE published:2014-12-20 summary:In this study we want to connect our previously proposed context-relevanttopographical maps with the deep learning community. Our architecture is aclassifier with hidden layers that are hierarchical two-dimensionaltopographical maps. These maps differ from the conventional self-organizingmaps in that their organizations are influenced by the context of the datalabels in a top-down manner. In this way bottom-up and top-down learning arecombined in a biologically relevant representational learning setting. Comparedto our previous work, we are here specifically elaborating the model in a morechallenging setting compared to our previous experiments and to advance morehidden representation layers to bring our discussions into the context of deeprepresentational learning.
arxiv-9900-278 | Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images | http://arxiv.org/pdf/1412.1897v4.pdf | author:Anh Nguyen, Jason Yosinski, Jeff Clune category:cs.CV cs.AI cs.NE published:2014-12-05 summary:Deep neural networks (DNNs) have recently been achieving state-of-the-artperformance on a variety of pattern-recognition tasks, most notably visualclassification problems. Given that DNNs are now able to classify objects inimages with near-human-level performance, questions naturally arise as to whatdifferences remain between computer and human vision. A recent study revealedthat changing an image (e.g. of a lion) in a way imperceptible to humans cancause a DNN to label the image as something else entirely (e.g. mislabeling alion a library). Here we show a related result: it is easy to produce imagesthat are completely unrecognizable to humans, but that state-of-the-art DNNsbelieve to be recognizable objects with 99.99% confidence (e.g. labeling withcertainty that white noise static is a lion). Specifically, we takeconvolutional neural networks trained to perform well on either the ImageNet orMNIST datasets and then find images with evolutionary algorithms or gradientascent that DNNs label with high confidence as belonging to each dataset class.It is possible to produce images totally unrecognizable to human eyes that DNNsbelieve with near certainty are familiar objects, which we call "foolingimages" (more generally, fooling examples). Our results shed light oninteresting differences between human vision and current DNNs, and raisequestions about the generality of DNN computer vision.
arxiv-9900-279 | A Probabilistic Theory of Deep Learning | http://arxiv.org/pdf/1504.00641v1.pdf | author:Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk category:stat.ML cs.CV cs.LG cs.NE published:2015-04-02 summary:A grand challenge in machine learning is the development of computationalalgorithms that match or outperform humans in perceptual inference tasks thatare complicated by nuisance variation. For instance, visual object recognitioninvolves the unknown object position, orientation, and scale in objectrecognition while speech recognition involves the unknown voice pronunciation,pitch, and speed. Recently, a new breed of deep learning algorithms haveemerged for high-nuisance inference tasks that routinely yield patternrecognition systems with near- or super-human capabilities. But a fundamentalquestion remains: Why do they work? Intuitions abound, but a coherent frameworkfor understanding, analyzing, and synthesizing deep learning architectures hasremained elusive. We answer this question by developing a new probabilisticframework for deep learning based on the Deep Rendering Model: a generativeprobabilistic model that explicitly captures latent nuisance variation. Byrelaxing the generative model to a discriminative one, we can recover two ofthe current leading deep learning systems, deep convolutional neural networksand random decision forests, providing insights into their successes andshortcomings, as well as a principled route to their improvement.
arxiv-9900-280 | Gradient-based Hyperparameter Optimization through Reversible Learning | http://arxiv.org/pdf/1502.03492v3.pdf | author:Dougal Maclaurin, David Duvenaud, Ryan P. Adams category:stat.ML cs.LG published:2015-02-11 summary:Tuning hyperparameters of learning algorithms is hard because gradients areusually unavailable. We compute exact gradients of cross-validation performancewith respect to all hyperparameters by chaining derivatives backwards throughthe entire training procedure. These gradients allow us to optimize thousandsof hyperparameters, including step-size and momentum schedules, weightinitialization distributions, richly parameterized regularization schemes, andneural network architectures. We compute hyperparameter gradients by exactlyreversing the dynamics of stochastic gradient descent with momentum.
arxiv-9900-281 | The Approximation of the Dissimilarity Projection | http://arxiv.org/pdf/1504.00593v1.pdf | author:Emanuele Olivetti, Thien Bao Nguyen, Paolo Avesani category:stat.ML cs.CV published:2015-04-02 summary:Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3Dpathways of axons within the white matter of the brain as a tractography. Theanalysis of tractographies has drawn attention from the machine learning andpattern recognition communities providing novel challenges such as finding anappropriate representation space for the data. Many of the current learningalgorithms require the input to be from a vectorial space. This requirementcontrasts with the intrinsic nature of the tractography because its basicelements, called streamlines or tracks, have different lengths and differentnumber of points and for this reason they cannot be directly represented in acommon vectorial space. In this work we propose the adoption of thedissimilarity representation which is an Euclidean embedding technique definedby selecting a set of streamlines called prototypes and then mapping any newstreamline to the vector of distances from prototypes. We investigate thedegree of approximation of this projection under different prototype selectionpolicies and prototype set sizes in order to characterise its use ontractography data. Additionally we propose the use of a scalable approximationof the most effective prototype selection policy that provides fast andaccurate dissimilarity approximations of complete tractographies.
arxiv-9900-282 | Quantum image classification using principal component analysis | http://arxiv.org/pdf/1504.00580v1.pdf | author:Mateusz Ostaszewski, Przemysław Sadowski, Piotr Gawron category:quant-ph cs.CV cs.LG published:2015-04-02 summary:We present a novel quantum algorithm for classification of images. Thealgorithm is constructed using principal component analysis and von Neumanquantum measurements. In order to apply the algorithm we present a new quantumrepresentation of grayscale images.
arxiv-9900-283 | Local Identification of Overcomplete Dictionaries | http://arxiv.org/pdf/1401.6354v2.pdf | author:Karin Schnass category:cs.IT math.IT stat.ML published:2014-01-24 summary:This paper presents the first theoretical results showing that stableidentification of overcomplete $\mu$-coherent dictionaries $\Phi \in\mathbb{R}^{d\times K}$ is locally possible from training signals with sparsitylevels $S$ up to the order $O(\mu^{-2})$ and signal to noise ratios up to$O(\sqrt{d})$. In particular the dictionary is recoverable as the local maximumof a new maximisation criterion that generalises the K-means criterion. Forthis maximisation criterion results for asymptotic exact recovery for sparsitylevels up to $O(\mu^{-1})$ and stable recovery for sparsity levels up to$O(\mu^{-2})$ as well as signal to noise ratios up to $O(\sqrt{d})$ areprovided. These asymptotic results translate to finite sample size recoveryresults with high probability as long as the sample size $N$ scales as $O(K^3dS\tilde \varepsilon^{-2})$, where the recovery precision $\tilde \varepsilon$can go down to the asymptotically achievable precision. Further, to actuallyfind the local maxima of the new criterion, a very simple IterativeThresholding and K (signed) Means algorithm (ITKM), which has complexity$O(dKN)$ in each iteration, is presented and its local efficiency isdemonstrated in several experiments.
arxiv-9900-284 | Strong oracle optimality of folded concave penalized estimation | http://arxiv.org/pdf/1210.5992v4.pdf | author:Jianqing Fan, Lingzhou Xue, Hui Zou category:math.ST stat.CO stat.ML stat.TH published:2012-10-22 summary:Folded concave penalization methods have been shown to enjoy the strongoracle property for high-dimensional sparse estimation. However, a foldedconcave penalization problem usually has multiple local solutions and theoracle property is established only for one of the unknown local solutions. Achallenging fundamental issue still remains that it is not clear whether thelocal optimum computed by a given optimization algorithm possesses those nicetheoretical properties. To close this important theoretical gap in over adecade, we provide a unified theory to show explicitly how to obtain the oraclesolution via the local linear approximation algorithm. For a folded concavepenalized estimation problem, we show that as long as the problem islocalizable and the oracle estimator is well behaved, we can obtain the oracleestimator by using the one-step local linear approximation. In addition, oncethe oracle estimator is obtained, the local linear approximation algorithmconverges, namely it produces the same estimator in the next iteration. Thegeneral theory is demonstrated by using four classical sparse estimationproblems, that is, sparse linear regression, sparse logistic regression, sparseprecision matrix estimation and sparse quantile regression.
arxiv-9900-285 | Convolutional Feature Masking for Joint Object and Stuff Segmentation | http://arxiv.org/pdf/1412.1283v4.pdf | author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV published:2014-12-03 summary:The topic of semantic segmentation has witnessed considerable progress due tothe powerful features learned by convolutional neural networks (CNNs). Thecurrent leading approaches for semantic segmentation exploit shape informationby extracting CNN features from masked image regions. This strategy introducesartificial boundaries on the images and may impact the quality of the extractedfeatures. Besides, the operations on the raw image domain require to computethousands of networks on a single image, which is time-consuming. In thispaper, we propose to exploit shape information via masking convolutionalfeatures. The proposal segments (e.g., super-pixels) are treated as masks onthe convolutional feature maps. The CNN features of segments are directlymasked out from these maps and used to train classifiers for recognition. Wefurther propose a joint method to handle objects and "stuff" (e.g., grass, sky,water) in the same framework. State-of-the-art results are demonstrated onbenchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compellingcomputational speed.
arxiv-9900-286 | Rademacher Observations, Private Data, and Boosting | http://arxiv.org/pdf/1502.02322v2.pdf | author:Richard Nock, Giorgio Patrini, Arik Friedman category:cs.LG 68Q32 E.4; I.2.6 published:2015-02-09 summary:The minimization of the logistic loss is a popular approach to batchsupervised learning. Our paper starts from the surprising observation that,when fitting linear (or kernelized) classifiers, the minimization of thelogistic loss is \textit{equivalent} to the minimization of an exponential\textit{rado}-loss computed (i) over transformed data that we call Rademacherobservations (rados), and (ii) over the \textit{same} classifier as the one ofthe logistic loss. Thus, a classifier learnt from rados can be\textit{directly} used to classify \textit{observations}. We provide a learningalgorithm over rados with boosting-compliant convergence rates on the\textit{logistic loss} (computed over examples). Experiments on domains with upto millions of examples, backed up by theoretical arguments, display thatlearning over a small set of random rados can challenge the state of the artthat learns over the \textit{complete} set of examples. We show that radoscomply with various privacy requirements that make them good candidates formachine learning in a privacy framework. We give several algebraic, geometricand computational hardness results on reconstructing examples from rados. Wealso show how it is possible to craft, and efficiently learn from, rados in adifferential privacy framework. Tests reveal that learning from differentiallyprivate rados can compete with learning from random rados, and hence with batchlearning from examples, achieving non-trivial privacy vs accuracy tradeoffs.
arxiv-9900-287 | Learning Parameters for Weighted Matrix Completion via Empirical Estimation | http://arxiv.org/pdf/1501.00192v4.pdf | author:Jason Jo category:stat.ML published:2014-12-31 summary:Recently theoretical guarantees have been obtained for matrix completion inthe non-uniform sampling regime. In particular, if the sampling distributionaligns with the underlying matrix's leverage scores, then with high probabilitynuclear norm minimization will exactly recover the low rank matrix. In thisarticle, we analyze the scenario in which the non-uniform sampling distributionmay or may not not align with the underlying matrix's leverage scores. Here weexplore learning the parameters for weighted nuclear norm minimization in termsof the empirical sampling distribution. We provide a sufficiency condition forthese learned weights which provide an exact recovery guarantee for weightednuclear norm minimization. It has been established that a specific choice ofweights in terms of the true sampling distribution not only allows for weightednuclear norm minimization to exactly recover the low rank matrix, but alsoallows for a quantifiable relaxation in the exact recovery conditions. In thisarticle we extend this quantifiable relaxation in exact recovery conditions fora specific choice of weights defined analogously in terms of the empiricaldistribution as opposed to the true sampling distribution. To accomplish thiswe employ a concentration of measure bound and a large deviation bound. We alsopresent numerical evidence for the healthy robustness of the weighted nuclearnorm minimization algorithm to the choice of empirically learned weights. Thesenumerical experiments show that for a variety of easily computable empiricalweights, weighted nuclear norm minimization outperforms unweighted nuclear normminimization in the non-uniform sampling regime.
arxiv-9900-288 | Direct l_(2,p)-Norm Learning for Feature Selection | http://arxiv.org/pdf/1504.00430v1.pdf | author:Hanyang Peng, Yong Fan category:cs.LG cs.CV published:2015-04-02 summary:In this paper, we propose a novel sparse learning based feature selectionmethod that directly optimizes a large margin linear classification modelsparsity with l_(2,p)-norm (0 < p < 1)subject to data-fitting constraints,rather than using the sparsity as a regularization term. To solve the directsparsity optimization problem that is non-smooth and non-convex when 0<p<1, weprovide an efficient iterative algorithm with proved convergence by convertingit to a convex and smooth optimization problem at every iteration step. Theproposed algorithm has been evaluated based on publicly available datasets, andextensive comparison experiments have demonstrated that our algorithm couldachieve feature selection performance competitive to state-of-the-artalgorithms.
arxiv-9900-289 | Decomposition-Based Domain Adaptation for Real-World Font Recognition | http://arxiv.org/pdf/1412.5758v4.pdf | author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV published:2014-12-18 summary:We present a domain adaption framework to address a domain mismatch betweensynthetic training and real-world testing data. We demonstrate our method on achallenging fine-grain classification problem: recognizing a font style from animage of text. In this task, it is very easy to generate lots of rendered fontexamples but very hard to obtain real-world labeled images. Thisreal-to-synthetic domain gap caused poor generalization to new real data inprevious font recognition methods (Chen et al. (2014)). In this paper, weintroduce a Convolutional Neural Network decomposition approach, leveraging alarge training corpus of synthetic data to obtain effective features forclassification. This is done using an adaptation technique based on a StackedConvolutional Auto-Encoder that exploits a large collection of unlabeledreal-world text images combined with synthetic data preprocessed in a specificway. The proposed DeepFont method achieves an accuracy of higher than 80%(top-5) on a new large labeled real-world dataset we collected.
arxiv-9900-290 | Signatures of Infinity: Nonergodicity and Resource Scaling in Prediction, Complexity, and Learning | http://arxiv.org/pdf/1504.00386v1.pdf | author:James P. Crutchfield, Sarah Marzen category:cs.IT cs.LG math.IT stat.ML published:2015-04-01 summary:We introduce a simple analysis of the structural complexity ofinfinite-memory processes built from random samples of stationary, ergodicfinite-memory component processes. Such processes are familiar from the wellknown multi-arm Bandit problem. We contrast our analysis withcomputation-theoretic and statistical inference approaches to understandingtheir complexity. The result is an alternative view of the relationship betweenpredictability, complexity, and learning that highlights the distinct ways inwhich informational and correlational divergences arise in complex ergodic andnonergodic processes. We draw out consequences for the resource divergencesthat delineate the structural hierarchy of ergodic processes and for processesthat are themselves hierarchical.
arxiv-9900-291 | Bayesian Clustering of Shapes of Curves | http://arxiv.org/pdf/1504.00377v1.pdf | author:Zhengwu Zhang, Debdeep Pati, Anuj Srivastava category:stat.ML cs.LG published:2015-04-01 summary:Unsupervised clustering of curves according to their shapes is an importantproblem with broad scientific applications. The existing model-based clusteringtechniques either rely on simple probability models (e.g., Gaussian) that arenot generally valid for shape analysis or assume the number of clusters. Wedevelop an efficient Bayesian method to cluster curve data using an elasticshape metric that is based on joint registration and comparison of shapes ofcurves. The elastic-inner product matrix obtained from the data is modeledusing a Wishart distribution whose parameters are assigned carefully chosenprior distributions to allow for automatic inference on the number of clusters.Posterior is sampled through an efficient Markov chain Monte Carlo procedurebased on the Chinese restaurant process to infer (1) the posterior distributionon the number of clusters, and (2) clustering configuration of shapes. Thismethod is demonstrated on a variety of synthetic data and real data examples onprotein structure analysis, cell shape analysis in microscopy images, andclustering of shaped from MPEG7 database.
arxiv-9900-292 | Hierarchical Sparse and Collaborative Low-Rank Representation for Emotion Recognition | http://arxiv.org/pdf/1410.1606v2.pdf | author:Xiang Xiang, Minh Dao, Gregory D. Hager, Trac D. Tran category:cs.CV published:2014-10-07 summary:In this paper, we design a Collaborative-Hierarchical Sparse and Low-Rank(C-HiSLR) model that is natural for recognizing human emotion in visual data.Previous attempts require explicit expression components, which are oftenunavailable and difficult to recover. Instead, our model exploits the lowrankproperty over expressive facial frames and rescue inexact sparserepresentations by incorporating group sparsity. For the CK+ dataset, C-HiSLRon raw expressive faces performs as competitive as the Sparse Representationbased Classification (SRC) applied on manually prepared emotions. C-HiSLRperforms even better than SRC in terms of true positive rate.
arxiv-9900-293 | Representation Learning with Deep Extreme Learning Machines for Efficient Image Set Classification | http://arxiv.org/pdf/1503.02445v3.pdf | author:Muhammad Uzair, Faisal Shafait, Bernard Ghanem, Ajmal Mian category:cs.CV published:2015-03-09 summary:Efficient and accurate joint representation of a collection of images, thatbelong to the same class, is a major research challenge for practical image setclassification. Existing methods either make prior assumptions about the datastructure, or perform heavy computations to learn structure from the dataitself. In this paper, we propose an efficient image set representation thatdoes not make any prior assumptions about the structure of the underlying data.We learn the non-linear structure of image sets with Deep Extreme LearningMachines (DELM) that are very efficient and generalize well even on a limitednumber of training samples. Extensive experiments on a broad range of publicdatasets for image set classification (Honda/UCSD, CMU Mobo, YouTubeCelebrities, Celebrity-1000, ETH-80) show that the proposed algorithmconsistently outperforms state-of-the-art image set classification methods bothin terms of speed and accuracy.
arxiv-9900-294 | A New Repair Operator for Multi-objective Evolutionary Algorithm in Constrained Optimization Problems | http://arxiv.org/pdf/1504.00154v1.pdf | author:Zhun Fan, Wenji Li, Xinye Cai, Huibiao Lin, Shuxiang Xie, Erik Goodman category:cs.NE 68Q01 G.1.6 published:2015-04-01 summary:In this paper, we design a set of multi-objective constrained optimizationproblems (MCOPs) and propose a new repair operator to address them. Theproposed repair operator is used to fix the solutions that violate the boxconstraints. More specifically, it employs a reversed correction strategy thatcan effectively avoid the population falling into local optimum. In addition,we integrate the proposed repair operator into two classical multi-objectiveevolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator iscompared with other two kinds of commonly used repair operators on benchmarkproblems CTPs and MCOPs. The experiment results demonstrate that our proposedapproach is very effective in terms of convergence and diversity.
arxiv-9900-295 | Iterative Regularization for Learning with Convex Loss Functions | http://arxiv.org/pdf/1503.08985v2.pdf | author:Junhong Lin, Lorenzo Rosasco, Ding-Xuan Zhou category:stat.ML math.OC published:2015-03-31 summary:We consider the problem of supervised learning with convex loss functions andpropose a new form of iterative regularization based on the subgradient method.Unlike other regularization approaches, in iterative regularization noconstraint or penalization is considered, and generalization is achieved by(early) stopping an empirical iteration. We consider a nonparametric setting,in the framework of reproducing kernel Hilbert spaces, and prove finite samplebounds on the excess risk under general regularity conditions. Our studyprovides a new class of efficient regularized learning algorithms and givesinsights on the interplay between statistics and optimization in machinelearning.
arxiv-9900-296 | Multidimensional Digital Smoothing Filters for Target Detection | http://arxiv.org/pdf/1410.0582v5.pdf | author:Hugh L. Kennedy category:cs.CV published:2014-10-02 summary:Recursive, causal and non-causal, multidimensional digital filters, withinfinite impulse responses and maximally flat magnitude and delay responses inthe low-frequency region, are designed to negate correlated clutter andinterference in the background and to accumulate power due to dim targets inthe foreground of a surveillance sensor. Expressions relating meanimpulse-response duration, frequency selectivity and group delay, to low-orderlinear-difference-equation coefficients are derived using discrete Laguerrepolynomials and discounted least-squares regression, then verified throughsimulation.
arxiv-9900-297 | The Libra Toolkit for Probabilistic Models | http://arxiv.org/pdf/1504.00110v1.pdf | author:Daniel Lowd, Amirmohammad Rooshenas category:cs.LG cs.AI published:2015-04-01 summary:The Libra Toolkit is a collection of algorithms for learning and inferencewith discrete probabilistic models, including Bayesian networks, Markovnetworks, dependency networks, and sum-product networks. Compared to othertoolkits, Libra places a greater emphasis on learning the structure oftractable models in which exact inference is efficient. It also includes avariety of algorithms for learning graphical models in which inference ispotentially intractable, and for performing exact and approximate inference.Libra is released under a 2-clause BSD license to encourage broad use inacademia and industry.
arxiv-9900-298 | A Weight-coded Evolutionary Algorithm for the Multidimensional Knapsack Problem | http://arxiv.org/pdf/1302.5374v4.pdf | author:Quan Yuan, Zhixin Yang category:cs.NE math.OC 90B50 published:2013-02-21 summary:A revised weight-coded evolutionary algorithm (RWCEA) is proposed for solvingmultidimensional knapsack problems. This RWCEA uses a new decoding method andincorporates a heuristic method in initialization. Computational results showthat the RWCEA performs better than a weight-coded evolutionary algorithmproposed by Raidl (1999) and to some existing benchmarks, it can yield betterresults than the ones reported in the OR-library.
arxiv-9900-299 | A Theory of Feature Learning | http://arxiv.org/pdf/1504.00083v1.pdf | author:Brendan van Rooyen, Robert C. Williamson category:stat.ML cs.LG published:2015-04-01 summary:Feature Learning aims to extract relevant information contained in data setsin an automated fashion. It is driving force behind the current deep learningtrend, a set of methods that have had widespread empirical success. What islacking is a theoretical understanding of different feature learning schemes.This work provides a theoretical framework for feature learning and thencharacterizes when features can be learnt in an unsupervised fashion. We alsoprovide means to judge the quality of features via rate-distortion theory andits generalizations.
arxiv-9900-300 | Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons | http://arxiv.org/pdf/1504.00064v1.pdf | author:James Y. Zou, Kamalika Chaudhuri, Adam Tauman Kalai category:stat.ML cs.LG published:2015-03-31 summary:We introduce an unsupervised approach to efficiently discover the underlyingfeatures in a data set via crowdsourcing. Our queries ask crowd members toarticulate a feature common to two out of three displayed examples. In additionwe also ask the crowd to provide binary labels to the remaining examples basedon the discovered features. The triples are chosen adaptively based on thelabels of the previously discovered features on the data set. In two naturalmodels of features, hierarchical and independent, we show that a simpleadaptive algorithm, using "two-out-of-three" similarity queries, recovers allfeatures with less labor than any nonadaptive algorithm. Experimental resultsvalidate the theoretical findings.
