arxiv-1502-01753 | Monitoring Term Drift Based on Semantic Consistency in an Evolving Vector Field |  http://arxiv.org/abs/1502.01753  | author:Peter Wittek, Sándor Darányi, Efstratios Kontopoulos, Theodoros Moysiadis, Ioannis Kompatsiaris category:cs.CL cs.LG cs.NE stat.ML published:2015-02-05 summary:Based on the Aristotelian concept of potentiality vs. actuality allowing forthe study of energy and dynamics in language, we propose a field approach tolexical analysis. Falling back on the distributional hypothesis tostatistically model word meaning, we used evolving fields as a metaphor toexpress time-dependent changes in a vector space model by a combination ofrandom indexing and evolving self-organizing maps (ESOM). To monitor semanticdrifts within the observation period, an experiment was carried out on the termspace of a collection of 12.8 million Amazon book reviews. For evaluation, thesemantic consistency of ESOM term clusters was compared with their respectiveneighbourhoods in WordNet, and contrasted with distances among term vectors byrandom indexing. We found that at 0.05 level of significance, the terms in theclusters showed a high level of semantic consistency. Tracking the drift ofdistributional patterns in the term space across time periods, we found thatconsistency decreased, but not at a statistically significant level. Our methodis highly scalable, with interpretations in philosophy.
arxiv-1502-01761 | A Framework for Symmetric Part Detection in Cluttered Scenes |  http://arxiv.org/abs/1502.01761  | author:Tom Lee, Sanja Fidler, Alex Levinshtein, Cristian Sminchisescu, Sven Dickinson category:cs.CV published:2015-02-05 summary:The role of symmetry in computer vision has waxed and waned in importanceduring the evolution of the field from its earliest days. At first figuringprominently in support of bottom-up indexing, it fell out of favor as shapegave way to appearance and recognition gave way to detection. With a strongprior in the form of a target object, the role of the weaker priors offered byperceptual grouping was greatly diminished. However, as the field returns tothe problem of recognition from a large database, the bottom-up recovery of theparts that make up the objects in a cluttered scene is critical for theirrecognition. The medial axis community has long exploited the ubiquitousregularity of symmetry as a basis for the decomposition of a closed contourinto medial parts. However, today's recognition systems are faced withcluttered scenes, and the assumption that a closed contour exists, i.e. thatfigure-ground segmentation has been solved, renders much of the medial axiscommunity's work inapplicable. In this article, we review a computationalframework, previously reported in Lee et al. (2013), Levinshtein et al. (2009,2013), that bridges the representation power of the medial axis and the need torecover and group an object's parts in a cluttered scene. Our framework isrooted in the idea that a maximally inscribed disc, the building block of amedial axis, can be modeled as a compact superpixel in the image. We evaluatethe method on images of cluttered scenes.
arxiv-1502-01632 | A Simple Expression for Mill's Ratio of the Student's $t$-Distribution |  http://arxiv.org/abs/1502.01632  | author:Francesco Orabona category:cs.LG math.PR published:2015-02-05 summary:I show a simple expression of the Mill's ratio of the Student'st-Distribution. I use it to prove Conjecture 1 in P. Auer, N. Cesa-Bianchi, andP. Fischer. Finite-time analysis of the multiarmed bandit problem. Mach.Learn., 47(2-3):235--256, May 2002.
arxiv-1502-01403 | Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds |  http://arxiv.org/abs/1502.01403  | author:Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan category:cs.DS cs.CC stat.ML published:2015-02-05 summary:We study the following generalized matrix rank estimation problem: given an$n \times n$ matrix and a constant $c \geq 0$, estimate the number ofeigenvalues that are greater than $c$. In the distributed setting, the matrixof interest is the sum of $m$ matrices held by separate machines. We show thatany deterministic algorithm solving this problem must communicate $\Omega(n^2)$bits, which is order-equivalent to transmitting the whole matrix. In contrast,we propose a randomized algorithm that communicates only $\widetilde O(n)$bits. The upper bound is matched by an $\Omega(n)$ lower bound on therandomized communication complexity. We demonstrate the practical effectivenessof the proposed algorithm with some numerical experiments.
arxiv-1502-01705 | A Confident Information First Principle for Parametric Reduction and Model Selection of Boltzmann Machines |  http://arxiv.org/abs/1502.01705  | author:Xiaozhao Zhao, Yuexian Hou, Dawei Song, Wenjie Li category:cs.LG stat.ML published:2015-02-05 summary:Typical dimensionality reduction (DR) methods are often data-oriented,focusing on directly reducing the number of random variables (features) whileretaining the maximal variations in the high-dimensional data. In unsupervisedsituations, one of the main limitations of these methods lies in theirdependency on the scale of data features. This paper aims to address theproblem from a new perspective and considers model-oriented dimensionalityreduction in parameter spaces of binary multivariate distributions. Specifically, we propose a general parameter reduction criterion, calledConfident-Information-First (CIF) principle, to maximally preserve confidentparameters and rule out less confident parameters. Formally, the confidence ofeach parameter can be assessed by its contribution to the expected Fisherinformation distance within the geometric manifold over the neighbourhood ofthe underlying real distribution. We then revisit Boltzmann machines (BM) from a model selection perspectiveand theoretically show that both the fully visible BM (VBM) and the BM withhidden units can be derived from the general binary multivariate distributionusing the CIF principle. This can help us uncover and formalize the essentialparts of the target density that BM aims to capture and the non-essential partsthat BM should discard. Guided by the theoretical analysis, we develop asample-specific CIF for model selection of BM that is adaptive to the observedsamples. The method is studied in a series of density estimation experimentsand has been shown effective in terms of the estimate accuracy.
arxiv-1502-01733 | Arrhythmia Detection using Mutual Information-Based Integration Method |  http://arxiv.org/abs/1502.01733  | author:Othman Soufan, Samer Arafat category:cs.CE cs.LG published:2015-02-05 summary:The aim of this paper is to propose an application of mutualinformation-based ensemble methods to the analysis and classification of heartbeats associated with different types of Arrhythmia. Models of multilayerperceptrons, support vector machines, and radial basis function neural networkswere trained and tested using the MIT-BIH arrhythmia database. This researchbrings a focus to an ensemble method that, to our knowledge, is a novelapplication in the area of ECG Arrhythmia detection. The proposed classifierensemble method showed improved performance, relative to either majority votingclassifier integration or to individual classifier performance. The overallensemble accuracy was 98.25%.
arxiv-1502-01423 | Collaborative Feature Learning from Social Media |  http://arxiv.org/abs/1502.01423  | author:Chen Fang, Hailin Jin, Jianchao Yang, Zhe Lin category:cs.CV published:2015-02-05 summary:Image feature representation plays an essential role in image recognition andrelated tasks. The current state-of-the-art feature learning paradigm issupervised learning from labeled data. However, this paradigm requireslarge-scale category labels, which limits its applicability to domains wherelabels are hard to obtain. In this paper, we propose a new data-driven featurelearning paradigm which does not rely on category labels. Instead, we learnfrom user behavior data collected on social media. Concretely, we use the imagerelationship discovered in the latent space from the user behavior data toguide the image feature learning. We collect a large-scale image and userbehavior dataset from Behance.net. The dataset consists of 1.9 million imagesand over 300 million view records from 1.9 million users. We validate ourfeature learning paradigm on this dataset and find that the learned featuresignificantly outperforms the state-of-the-art image features in learningbetter image similarities. We also show that the learned feature performscompetitively on various recognition benchmarks.
arxiv-1502-01643 | Performance Analysis of Cone Detection Algorithms |  http://arxiv.org/abs/1502.01643  | author:Letizia Mariotti, Nicholas Devaney category:physics.med-ph cs.CV published:2015-02-05 summary:Many algorithms have been proposed to help clinicians evaluate cone densityand spacing, as these may be related to the onset of retinal diseases. However,there has been no rigorous comparison of the performance of these algorithms.In addition, the performance of such algorithms is typically determined bycomparison with human observers. Here we propose a technique to simulaterealistic images of the cone mosaic. We use the simulated images to test theperformance of two popular cone detection algorithms and we introduce analgorithm which is used by astronomers to detect stars in astronomical images.We use Free Response Operating Characteristic (FROC) curves to evaluate andcompare the performance of the three algorithms. This allows us to optimize theperformance of each algorithm. We observe that performance is significantlyenhanced by up-sampling the images. We investigate the effect of noise andimage quality on cone mosaic parameters estimated using the differentalgorithms, finding that the estimated regularity is the most sensitiveparameter. This paper was published in JOSA A and is made available as an electronicreprint with the permission of OSA. The paper can be found at the following URLon the OSA website: http://www.opticsinfobase.org/abstract.cfm?msid=224577.Systematic or multiple reproduction or distribution to multiple locations viaelectronic or other means is prohibited and is subject to penalties under law.
arxiv-1502-01710 | Text Understanding from Scratch |  http://arxiv.org/abs/1502.01710  | author:Xiang Zhang, Yann LeCun category:cs.LG cs.CL published:2015-02-05 summary:This article demontrates that we can apply deep learning to textunderstanding from character-level inputs all the way up to abstract textconcepts, using temporal convolutional networks (ConvNets). We apply ConvNetsto various large-scale datasets, including ontology classification, sentimentanalysis, and text categorization. We show that temporal ConvNets can achieveastonishing performance without the knowledge of words, phrases, sentences andany other syntactic or semantic structures with regards to a human language.Evidence shows that our models can work for both English and Chinese.
arxiv-1502-01475 | Fast Constraint Propagation for Image Segmentation |  http://arxiv.org/abs/1502.01475  | author:Peng Han category:cs.CV published:2015-02-05 summary:This paper presents a novel selective constraint propagation method forconstrained image segmentation. In the literature, many pairwise constraintpropagation methods have been developed to exploit pairwise constraints forcluster analysis. However, since most of these methods have a polynomial timecomplexity, they are not much suitable for segmentation of images even with amoderate size, which is actually equivalent to cluster analysis with a largedata size. Considering the local homogeneousness of a natural image, we chooseto perform pairwise constraint propagation only over a selected subset ofpixels, but not over the whole image. Such a selective constraint propagationproblem is then solved by an efficient graph-based learning algorithm. Tofurther speed up our selective constraint propagation, we also discard thoseless important propagated constraints during graph-based learning. Finally, theselectively propagated constraints are exploited based on $L_1$-minimizationfor normalized cuts over the whole image. The experimental results demonstratethe promising performance of the proposed method for segmentation withselectively propagated constraints.
arxiv-1502-01563 | A PARTAN-Accelerated Frank-Wolfe Algorithm for Large-Scale SVM Classification |  http://arxiv.org/abs/1502.01563  | author:Emanuele Frandi, Ricardo Nanculef, Johan A. K. Suykens category:stat.ML cs.LG math.OC published:2015-02-05 summary:Frank-Wolfe algorithms have recently regained the attention of the MachineLearning community. Their solid theoretical properties and sparsity guaranteesmake them a suitable choice for a wide range of problems in this field. Inaddition, several variants of the basic procedure exist that improve itstheoretical properties and practical performance. In this paper, we investigatethe application of some of these techniques to Machine Learning, focusing inparticular on a Parallel Tangent (PARTAN) variant of the FW algorithm that hasnot been previously suggested or studied for this type of problems. We provideexperiments both in a standard setting and using a stochastic speed-uptechnique, showing that the considered algorithms obtain promising results onseveral medium and large-scale benchmark datasets for SVM classification.
arxiv-1502-01540 | Semantic Embedding Space for Zero-Shot Action Recognition |  http://arxiv.org/abs/1502.01540  | author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV published:2015-02-05 summary:The number of categories for action recognition is growing rapidly. It isthus becoming increasingly hard to collect sufficient training data to learnconventional models for each category. This issue may be ameliorated by theincreasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework amapping is constructed between visual features and a human interpretablesemantic description of each category, allowing categories to be recognised inthe absence of any training data. Existing ZSL studies focus primarily on imagedata, and attribute-based semantic representations. In this paper, we addresszero-shot recognition in contemporary video action recognition tasks, usingsemantic word vector space as the common space to embed videos and categorylabels. This is more challenging because the mapping between the semantic spaceand space-time features of videos containing complex actions is more complexand harder to learn. We demonstrate that a simple self-training and dataaugmentation strategy can significantly improve the efficacy of this mapping.Experiments on human action datasets including HMDB51 and UCF101 demonstratethat our approach achieves the state-of-the-art zero-shot action recognitionperformance.
arxiv-1502-01526 | Object Proposal via Partial Re-ranking |  http://arxiv.org/abs/1502.01526  | author:Jing Wang, Jie Shen category:cs.CV published:2015-02-05 summary:Object proposals are an ensemble of bounding boxes with high potential tocontain objects. Usually, the ranking models are utilized in order to provide amanageable number of candidate boxes. To obtain the rank for each candidate,prior ranking models generally compare each pair of candidates. However, onemay be interested in only the top-$k$ candidates rather than all ones. Thus, inthis paper, we propose a new ranking model for object proposals, which aims toproduce a reliable estimation for only the top-$k$ candidates. To this end, wecompute the IoU for each candidate and split the candidates into two subsetsconsisting of the top-$k$ candidates and the others respectively. Partialranking constraints are imposed on the two subsets: any candidate from thefirst subset is better than that from the second one. In this way, theconstraints are reduced dramatically compared to the full ranking model, whichfurther facilitates an efficient learning procedure. Moreover, we show that ourpartial ranking model can be reduced into the large margin based framework.Extensive experiments demonstrate that after a re-ranking step of our model,the top-$k$ detection rate can be significantly improved.
arxiv-1502-01493 | A mixture Cox-Logistic model for feature selection from survival and classification data |  http://arxiv.org/abs/1502.01493  | author:Samuel Branders, Roberto D'Ambrosio, Pierre Dupont category:stat.ML cs.LG stat.ME published:2015-02-05 summary:This paper presents an original approach for jointly fitting survival timesand classifying samples into subgroups. The Coxlogit model is a generalizedlinear model with a common set of selected features for both tasks. Survivaltimes and class labels are here assumed to be conditioned by a common riskscore which depends on those features. Learning is then naturally expressed asmaximizing the joint probability of subgroup labels and the ordering ofsurvival events, conditioned to a common weight vector. The model is estimatedby minimizing a regularized log-likelihood through a coordinate descentalgorithm. Validation on synthetic and breast cancer data shows that the proposedapproach outperforms a standard Cox model or logistic regression when bothpredicting the survival times and classifying new samples into subgroups. It isalso better at selecting informative features for both tasks.
arxiv-1502-01400 | Fast unsupervised Bayesian image segmentation with adaptive spatial regularisation |  http://arxiv.org/abs/1502.01400  | author:Marcelo Pereyra, Steve McLaughlin category:stat.CO cs.CV published:2015-02-05 summary:This paper presents a new Bayesian estimation technique for hiddenPotts-Markov random fields with unknown regularisation parameters, withapplication to fast unsupervised K-class image segmentation. The technique isderived by first removing the regularisation parameter from the Bayesian modelby marginalisation, followed by a small-variance-asymptotic (SVA) analysis inwhich the spatial regularisation and the integer-constrained terms of the Pottsmodel are decoupled. The evaluation of this SVA Bayesian estimator is thenrelaxed into a problem that can be computed efficiently by iteratively solvinga convex total-variation denoising problem and a least-squares clustering(K-means) problem, both of which can be solved straightforwardly, even inhigh-dimensions, and with parallel computing techniques. This leads to a fastfully unsupervised Bayesian image segmentation methodology in which thestrength of the spatial regularisation is adapted automatically to the observedimage during the inference procedure, and that can be easily applied in large2D and 3D scenarios or in applications requiring low computing times.Experimental results on real images, as well as extensive comparisons withstate-of-the-art algorithms, confirm that the proposed methodology offerextremely fast convergence and produces accurate segmentation results, with theimportant additional advantage of self-adjusting regularisation parameters.
arxiv-1502-01480 | Ring artifacts correction in compressed sensing tomographic reconstruction |  http://arxiv.org/abs/1502.01480  | author:Pierre Paleo, Alessandro Mirone category:cs.CV published:2015-02-05 summary:We present a novel approach to handle ring artifacts correction in compressedsensing tomographic reconstruction. The correction is part of thereconstruction process, which differs from classical sinogram pre-processingand image post-processing techniques. The principle of compressed sensingtomographic reconstruction is presented. Then, we show that the ring artifactscorrection can be integrated in the reconstruction problem formalism. Weprovide numerical results for both simulated and real data. This technique isincluded in the PyHST2 code which is used at the European Synchrotron RadiationFacility for tomographic reconstruction.
arxiv-1502-01380 | Artificial neural networks in calibration of nonlinear mechanical models |  http://arxiv.org/abs/1502.01380  | author:Tomáš Mareš, Eliška Janouchová, Anna Kučerová category:cs.NE cs.CE published:2015-02-04 summary:Rapid development in numerical modelling of materials and the complexity ofnew models increases quickly together with their computational demands. Despitethe growing performance of modern computers and clusters, calibration of suchmodels from noisy experimental data remains a nontrivial and oftencomputationally exhaustive task. The layered neural networks thus represent arobust and efficient technique to overcome the time-consuming simulations of acalibrated model. The potential of neural networks consists in simpleimplementation and high versatility in approximating nonlinear relationships.Therefore, there were several approaches proposed to accelerate the calibrationof nonlinear models by neural networks. This contribution reviews and comparesthree possible strategies based on approximating (i) model response, (ii)inverse relationship between the model response and its parameters and (iii)error function quantifying how well the model fits the data. The advantages anddrawbacks of particular strategies are demonstrated on the calibration of fourparameters of the affinity hydration model from simulated data as well as fromexperimental measurements. This model is highly nonlinear, but computationallycheap thus allowing its calibration without any approximation and betterquantification of results obtained by the examined calibration strategies. Thepaper can be thus viewed as a guide intended for the engineers to help themselect an appropriate strategy in their particular calibration problems.
arxiv-1502-01368 | Sparse Representation Classification Beyond L1 Minimization and the Subspace Assumption |  http://arxiv.org/abs/1502.01368  | author:Cencheng Shen, Li Chen, Carey E. Priebe category:stat.ML published:2015-02-04 summary:The sparse representation classifier (SRC) proposed in Wright et al. (2009)has recently gained much attention from the machine learning community. Itmakes use of L1 minimization, and is known to work well for data satisfying asubspace assumption. In this paper, we use the notion of class dominance aswell as a principal angle condition to investigate and validate theclassification performance of SRC, without relying on L1 minimization and thesubspace assumption. We prove that SRC can still work well using faster subsetregression methods such as orthogonal matching pursuit and marginal regression,and its applicability is not limited to data satisfying the subspaceassumption. We illustrate our theorems via various real data sets includingface images, text features, and network data.
arxiv-1502-01271 | INRIASAC: Simple Hypernym Extraction Methods |  http://arxiv.org/abs/1502.01271  | author:Gregory Grefenstette category:cs.CL published:2015-02-04 summary:Given a set of terms from a given domain, how can we structure them into ataxonomy without manual intervention? This is the task 17 of SemEval 2015. Herewe present our simple taxonomy structuring techniques which, despite theirsimplicity, ranked first in this 2015 benchmark. We use large quantities oftext (English Wikipedia) and simple heuristics such as term overlap anddocument and sentence co-occurrence to produce hypernym lists. We describethese techniques and pre-sent an initial evaluation of results.
arxiv-1502-01097 | Dense v.s. Sparse: A Comparative Study of Sampling Analysis in Scene Classification of High-Resolution Remote Sensing Imagery |  http://arxiv.org/abs/1502.01097  | author:Jingwen Hu, Gui-Song Xia, Fan Hu, Liangpei Zhang category:cs.CV published:2015-02-04 summary:Scene classification is a key problem in the interpretation ofhigh-resolution remote sensing imagery. Many state-of-the-art methods, e.g.bag-of-visual-words model and its variants, the topic models as well as deeplearning-based approaches, share similar procedures: patch sampling, featuredescription/learning and classification. Patch sampling is the first and a keyprocedure which has a great influence on the results. In the literature, manydifferent sampling strategies have been used, {e.g. dense sampling, randomsampling, keypoint-based sampling and saliency-based sampling, etc. However, itis still not clear which sampling strategy is suitable for the sceneclassification of high-resolution remote sensing images. In this paper, wecomparatively study the effects of different sampling strategies under thescenario of scene classification of high-resolution remote sensing images. Wedivide the existing sampling methods into two types: dense sampling and sparsesampling, the later of which includes random sampling, keypoint-based samplingand various saliency-based sampling proposed recently. In order to comparetheir performances, we rely on a standard bag-of-visual-words model toconstruct our testing scheme, owing to their simplicity, robustness andefficiency. The experimental results on two commonly used datasets show thatdense sampling has the best performance among all the strategies but with highspatial and computational complexity, random sampling gives better orcomparable results than other sparse sampling methods, like the sophisticatedmulti-scale key-point operators and the saliency-based methods which areintensively studied and commonly used recently.
arxiv-1502-01228 | Linear-time Online Action Detection From 3D Skeletal Data Using Bags of Gesturelets |  http://arxiv.org/abs/1502.01228  | author:Moustafa Meshry, Mohamed E. Hussein, Marwan Torki category:cs.CV published:2015-02-04 summary:Sliding window is one direct way to extend a successful recognition system tohandle the more challenging detection problem. While action recognition decidesonly whether or not an action is present in a pre-segmented video sequence,action detection identifies the time interval where the action occurred in anunsegmented video stream. Sliding window approaches for action detection canhowever be slow as they maximize a classifier score over all possiblesub-intervals. Even though new schemes utilize dynamic programming to speed upthe search for the optimal sub-interval, they require offline processing on thewhole video sequence. In this paper, we propose a novel approach for onlineaction detection based on 3D skeleton sequences extracted from depth data. Itidentifies the sub-interval with the maximum classifier score in linear time.Furthermore, it is invariant to temporal scale variations and is suitable forreal-time applications with low latency.
arxiv-1502-01176 | Learning Local Invariant Mahalanobis Distances |  http://arxiv.org/abs/1502.01176  | author:Ethan Fetaya, Shimon Ullman category:cs.LG stat.ML published:2015-02-04 summary:For many tasks and data types, there are natural transformations to which thedata should be invariant or insensitive. For instance, in visual recognition,natural images should be insensitive to rotation and translation. Thisrequirement and its implications have been important in many machine learningapplications, and tolerance for image transformations was primarily achieved byusing robust feature vectors. In this paper we propose a novel andcomputationally efficient way to learn a local Mahalanobis metric per datum,and show how we can learn a local invariant metric to any transformation inorder to improve performance.
arxiv-1502-01068 | Composite convex minimization involving self-concordant-like cost functions |  http://arxiv.org/abs/1502.01068  | author:Quoc Tran-Dinh, Yen-Huan Li, Volkan Cevher category:math.OC stat.ML published:2015-02-04 summary:The self-concordant-like property of a smooth convex function is a newanalytical structure that generalizes the self-concordant notion. While a widevariety of important applications feature the self-concordant-like property,this concept has heretofore remained unexploited in convex optimization. Tothis end, we develop a variable metric framework of minimizing the sum of a"simple" convex function and a self-concordant-like function. We introduce anew analytic step-size selection procedure and prove that the basic gradientalgorithm has improved convergence guarantees as compared to "fast" algorithmsthat rely on the Lipschitz gradient property. Our numerical tests withreal-data sets shows that the practice indeed follows the theory.
arxiv-1502-01241 | A specialized face-processing network consistent with the representational geometry of monkey face patches |  http://arxiv.org/abs/1502.01241  | author:Amirhossein Farzmahdi, Karim Rajaei, Masoud Ghodrati, Reza Ebrahimpour, Seyed-Mahdi Khaligh-Razavi category:q-bio.NC cs.CV published:2015-02-04 summary:Ample evidence suggests that face processing in human and non-human primatesis performed differently compared with other objects. Converging reports, bothphysiologically and psychophysically, indicate that faces are processed inspecialized neural networks in the brain -i.e. face patches in monkeys and thefusiform face area (FFA) in humans. We are all expert face-processing agents,and able to identify very subtle differences within the category of faces,despite substantial visual and featural similarities. Identification isperformed rapidly and accurately after viewing a whole face, whilesignificantly drops if some of the face configurations (e.g. inversion,misalignment) are manipulated or if partial views of faces are shown due toocclusion. This refers to a hotly-debated, yet highly-supported concept, knownas holistic face processing. We built a hierarchical computational model offace-processing based on evidence from recent neuronal and behavioural studieson faces processing in primates. Representational geometries of the last threelayers of the model have characteristics similar to those observed in monkeyface patches (posterior, middle and anterior patches). Furthermore, severalface-processing-related phenomena reported in the literature automaticallyemerge as properties of this model. The representations are evolved throughseveral computational layers, using biologically plausible learning rules. Themodel satisfies face inversion effect, composite face effect, other raceeffect, view and identity selectivity, and canonical face views. To ourknowledge, no models have so far been proposed with this performance andagreement with biological data.
arxiv-1502-01094 | Multimodal Task-Driven Dictionary Learning for Image Classification |  http://arxiv.org/abs/1502.01094  | author:Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins category:stat.ML cs.CV cs.LG published:2015-02-04 summary:Dictionary learning algorithms have been successfully used for bothreconstructive and discriminative tasks, where an input signal is representedwith a sparse linear combination of dictionary atoms. While these methods aremostly developed for single-modality scenarios, recent studies havedemonstrated the advantages of feature-level fusion based on the joint sparserepresentation of the multimodal inputs. In this paper, we propose a multimodaltask-driven dictionary learning algorithm under the joint sparsity constraint(prior) to enforce collaborations among multiple homogeneous/heterogeneoussources of information. In this task-driven formulation, the multimodaldictionaries are learned simultaneously with their corresponding classifiers.The resulting multimodal dictionaries can generate discriminative latentfeatures (sparse codes) from the data that are optimized for a given task suchas binary or multiclass classification. Moreover, we present an extension ofthe proposed formulation using a mixed joint and independent sparsity priorwhich facilitates more flexible fusion of the modalities at feature level. Theefficacy of the proposed algorithms for multimodal classification isillustrated on four different applications -- multimodal face recognition,multi-view face recognition, multi-view action recognition, and multimodalbiometric recognition. It is also shown that, compared to the counterpartreconstructive-based dictionary learning algorithms, the task-drivenformulations are more computationally efficient in the sense that they can beequipped with more compact dictionaries and still achieve superior performance.
arxiv-1502-01245 | Authorship recognition via fluctuation analysis of network topology and word intermittency |  http://arxiv.org/abs/1502.01245  | author:Diego R. Amancio category:cs.CL published:2015-02-04 summary:Statistical methods have been widely employed in many practical naturallanguage processing applications. More specifically, complex networks conceptsand methods from dynamical systems theory have been successfully applied torecognize stylistic patterns in written texts. Despite the large amount ofstudies devoted to represent texts with physical models, only a few studieshave assessed the relevance of attributes derived from the analysis ofstylistic fluctuations. Because fluctuations represent a pivotal factor forcharacterizing a myriad of real systems, this study focused on the analysis ofthe properties of stylistic fluctuations in texts via topological analysis ofcomplex networks and intermittency measurements. The results showed thatdifferent authors display distinct fluctuation patterns. In particular, it wasfound that it is possible to identify the authorship of books using theintermittency of specific words. Taken together, the results described heresuggest that the patterns found in stylistic fluctuations could be used toanalyze other related complex systems. Furthermore, the discovery of novelpatterns related to textual stylistic fluctuations indicates that thesepatterns could be useful to improve the state of the art of manystylistic-based natural language processing tasks.
arxiv-1502-01199 | A Multiple-Expert Binarization Framework for Multispectral Images |  http://arxiv.org/abs/1502.01199  | author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2015-02-04 summary:In this work, a multiple-expert binarization framework for multispectralimages is proposed. The framework is based on a constrained subspace selectionlimited to the spectral bands combined with state-of-the-art gray-levelbinarization methods. The framework uses a binarization wrapper to enhance theperformance of the gray-level binarization. Nonlinear preprocessing of theindividual spectral bands is used to enhance the textual information. Anevolutionary optimizer is considered to obtain the optimal and some suboptimal3-band subspaces from which an ensemble of experts is then formed. Theframework is applied to a ground truth multispectral dataset with promisingresults. In addition, a generalization to the cross-validation approach isdeveloped that not only evaluates generalizability of the framework, it alsoprovides a practical instance of the selected experts that could be thenapplied to unseen inputs despite the small size of the given ground truthdataset.
arxiv-1502-00705 | Recovery of Piecewise Smooth Images from Few Fourier Samples |  http://arxiv.org/abs/1502.00705  | author:Greg Ongie, Mathews Jacob category:cs.CV published:2015-02-03 summary:We introduce a Prony-like method to recover a continuous domain 2-D piecewisesmooth image from few of its Fourier samples. Assuming the discontinuity set ofthe image is localized to the zero level-set of a trigonometric polynomial, weshow the Fourier transform coefficients of partial derivatives of the signalsatisfy an annihilation relation. We present necessary and sufficientconditions for unique recovery of piecewise constant images using the aboveannihilation relation. We pose the recovery of the Fourier coefficients of thesignal from the measurements as a convex matrix completion algorithm, whichrelies on the lifting of the Fourier data to a structured low-rank matrix; thisapproach jointly estimates the signal and the annihilating filter. Finally, wedemonstrate our algorithm on the recovery of MRI phantoms from fewlow-resolution Fourier samples.
arxiv-1502-00741 | Dynamical And-Or Graph Learning for Object Shape Modeling and Detection |  http://arxiv.org/abs/1502.00741  | author:Xiaolong Wang, Liang Lin category:cs.CV 68U01 published:2015-02-03 summary:This paper studies a novel discriminative part-based model to represent andrecognize object shapes with an "And-Or graph". We define this model consistingof three layers: the leaf-nodes with collaborative edges for localizing localparts, the or-nodes specifying the switch of leaf-nodes, and the root-nodeencoding the global verification. A discriminative learning algorithm, extendedfrom the CCCP [23], is proposed to train the model in a dynamical manner: themodel structure (e.g., the configuration of the leaf-nodes associated with theor-nodes) is automatically determined with optimizing the multi-layerparameters during the iteration. The advantages of our method are two-fold. (i)The And-Or graph model enables us to handle well large intra-class variance andbackground clutters for object shape detection from images. (ii) The proposedlearning algorithm is able to obtain the And-Or graph representation withoutrequiring elaborate supervision and initialization. We validate the proposedmethod on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, andUIUC-People), and it outperforms the state-of-the-arts approaches.
arxiv-1502-00702 | Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks |  http://arxiv.org/abs/1502.00702  | author:Shiliang Zhang, Hui Jiang category:cs.LG cs.NE published:2015-02-03 summary:In this paper, we propose a novel model for high-dimensional data, called theHybrid Orthogonal Projection and Estimation (HOPE) model, which combines alinear orthogonal projection and a finite mixture model under a unifiedgenerative modeling framework. The HOPE model itself can be learnedunsupervised from unlabelled data based on the maximum likelihood estimation aswell as discriminatively from labelled data. More interestingly, we have shownthe proposed HOPE models are closely related to neural networks (NNs) in asense that each hidden layer can be reformulated as a HOPE model. As a result,the HOPE framework can be used as a novel tool to probe why and how NNs work,more importantly, to learn NNs in either supervised or unsupervised ways. Inthis work, we have investigated the HOPE framework to learn NNs for severalstandard tasks, including image recognition on MNIST and speech recognition onTIMIT. Experimental results have shown that the HOPE framework yieldssignificant performance gains over the current state-of-the-art methods invarious types of NN learning problems, including unsupervised feature learning,supervised or semi-supervised learning.
arxiv-1502-00739 | Clothing Co-Parsing by Joint Image Segmentation and Labeling |  http://arxiv.org/abs/1502.00739  | author:Wei Yang, Ping Luo, Liang Lin category:cs.CV 68U01 published:2015-02-03 summary:This paper aims at developing an integrated system of clothing co-parsing, inorder to jointly parse a set of clothing images (unsegmented but annotated withtags) into semantic configurations. We propose a data-driven frameworkconsisting of two phases of inference. The first phase, referred as "imageco-segmentation", iterates to extract consistent regions on images and jointlyrefines the regions over all images by employing the exemplar-SVM (E-SVM)technique [23]. In the second phase (i.e. "region co-labeling"), we construct amulti-image graphical model by taking the segmented regions as vertices, andincorporate several contexts of clothing configuration (e.g., item location andmutual interactions). The joint label assignment can be solved using theefficient Graph Cuts algorithm. In addition to evaluate our framework on theFashionista dataset [30], we construct a dataset called CCP consisting of 2098high-resolution street fashion photos to demonstrate the performance of oursystem. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89%recognition rate on the Fashionista and the CCP datasets, respectively, whichare superior compared with state-of-the-art methods.
arxiv-1502-00727 | Macrostate mixture models for probabilistic multiscale nonparametric kernelized spectral clustering |  http://arxiv.org/abs/1502.00727  | author:Daniel Korenblum category:stat.ML published:2015-02-03 summary:Automating the discovery of meaningful structures in large complex datasetsis an important problem in many application areas including machine learning,source separation, and dimensionality reduction. Mixture models are onecategory of methods for discovering structure using convex sums of probabilitydistributions to represent structures or clusters in data. Spectral clusteringis another category of methods where eigenspaces of Laplacian matrices are usedprior to or as part of the clustering process. Macrostate theory definesnonparametric mixture models directly from Laplacian eigensystems, providing aconnection between nonhierarchical spectral clustering and nonparametricmixture modeling. Unlike other spectral clustering methods, macrostates areself-contained and predict both the appropriate number of mixture componentsand the cluster assignment distributions directly from Laplacian eigensystems.Macrostates reduce the number of input parameters and steps required comparedto other spectral clustering methods and avoid issues of explicit densityestimation in higher dimensional input data spaces. Previous formulations usedcustomized algorithms to compute macrostate clustering solutions, limitingtheir practical accessibility. The new formulation presented here depends onlyon standardized linear programming solvers and is very easily parallelized,improving the practicality and performance compared to previous formulations.Numerical examples compare the performance of other finite mixture modeling andspectral clustering methods to macrostate clustering.
arxiv-1502-00749 | Data-Driven Scene Understanding with Adaptively Retrieved Exemplars |  http://arxiv.org/abs/1502.00749  | author:Xionghao Liu, Wei Yang, Liang Lin, Qing Wang, Zhaoquan Cai, Jianhuang Lai category:cs.CV 68U01 published:2015-02-03 summary:This article investigates a data-driven approach for semantically sceneunderstanding, without pixelwise annotation and classifier training. Ourframework parses a target image with two steps: (i) retrieving its exemplars(i.e. references) from an image database, where all images are unsegmented butannotated with tags; (ii) recovering its pixel labels by propagating semanticsfrom the references. We present a novel framework making the two steps mutuallyconditional and bootstrapped under the probabilistic Expectation-Maximization(EM) formulation. In the first step, the references are selected by jointlymatching their appearances with the target as well as the semantics (i.e. theassigned labels of the target and the references). We process the second stepvia a combinatorial graphical representation, in which the vertices aresuperpixels extracted from the target and its selected references. Then wederive the potentials of assigning labels to one vertex of the target, whichdepend upon the graph edges that connect the vertex to its spatial neighbors ofthe target and to its similar vertices of the references. Besides, the proposedframework can be naturally applied to perform image annotation on new testimages. In the experiments, we validate our approach on two public databases,and demonstrate superior performances over the state-of-the-art methods in bothsemantic segmentation and image annotation tasks.
arxiv-1502-00750 | Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with Discriminatively Trained Spatio-Temporal Model |  http://arxiv.org/abs/1502.00750  | author:Xiaodan Liang, Qingxing Cao, Rui Huang, Liang Lin category:cs.CV 68U01 published:2015-02-03 summary:The aim of this study is to provide an automatic computational framework toassist clinicians in diagnosing Focal Liver Lesions (FLLs) inContrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clipas an ensemble of Region-of-Interests (ROIs), whose locations are modeled aslatent variables in a discriminative model. Different types of FLLs arecharacterized by both spatial and temporal enhancement patterns of the ROIs.The model is learned by iteratively inferring the optimal ROI locations andoptimizing the model parameters. To efficiently search the optimal spatial andtemporal locations of the ROIs, we propose a data-driven inference algorithm bycombining effective spatial and temporal pruning. The experiments show that ourmethod achieves promising results on the largest dataset in the literature (tothe best of our knowledge), which we have made publicly available.
arxiv-1502-00725 | Cheaper and Better: Selecting Good Workers for Crowdsourcing |  http://arxiv.org/abs/1502.00725  | author:Hongwei Li, Qiang Liu category:stat.ML cs.AI cs.LG stat.AP published:2015-02-03 summary:Crowdsourcing provides a popular paradigm for data collection at scale. Westudy the problem of selecting subsets of workers from a given worker pool tomaximize the accuracy under a budget constraint. One natural question iswhether we should hire as many workers as the budget allows, or restrict on asmall number of top-quality workers. By theoretically analyzing the error rateof a typical setting in crowdsourcing, we frame the worker selection probleminto a combinatorial optimization problem and propose an algorithm to solve itefficiently. Empirical results on both simulated and real-world datasets showthat our algorithm is able to select a small number of high-quality workers,and performs as good as, sometimes even better than, the much larger crowds asthe budget allows.
arxiv-1502-00723 | Learning Contour-Fragment-based Shape Model with And-Or Tree Representation |  http://arxiv.org/abs/1502.00723  | author:Liang Lin, Xiaolong Wang, Wei Yang, Jianhuang Lai category:cs.CV 68U01 published:2015-02-03 summary:This paper proposes a simple yet effective method to learn the hierarchicalobject shape model consisting of local contour fragments, which represents acategory of shapes in the form of an And-Or tree. This model extends thetraditional hierarchical tree structures by introducing the "switch" variables(i.e. the or-nodes) that explicitly specify production rules to capture shapevariations. We thus define the model with three layers: the leaf-nodes fordetecting local contour fragments, the or-nodes specifying selection ofleaf-nodes, and the root-node encoding the holistic distortion. In the trainingstage, for optimization of the And-Or tree learning, we extend theconcave-convex procedure (CCCP) by embedding the structural clustering duringthe iterative learning steps. The inference of shape detection is consistentwith the model optimization, which integrates the local testings via theleaf-nodes and or-nodes with the global verification via the root-node. Theadvantages of our approach are validated on the challenging shape databases(i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed methodis able to accurately localize shape contours against unreliable edge detectionand edge tracing. (2) The And-Or tree model enables us to well capture theintraclass variance.
arxiv-1502-07666 | Landmark-Guided Elastic Shape Analysis of Human Character Motions |  http://arxiv.org/abs/1502.07666  | author:Martin Bauer, Markus Eslitzbichler, Markus Grasmair category:cs.CV cs.GR published:2015-02-03 summary:Motions of virtual characters in movies or video games are typicallygenerated by recording actors using motion capturing methods. Animationsgenerated this way often need postprocessing, such as improving the periodicityof cyclic animations or generating entirely new motions by interpolation ofexisting ones. Furthermore, search and classification of recorded motionsbecomes more and more important as the amount of recorded motion data grows. In this paper, we will apply methods from shape analysis to the processing ofanimations. More precisely, we will use the by now classical elastic metricmodel used in shape matching, and extend it by incorporating additional inexactfeature point information, which leads to an improved temporal alignment ofdifferent animations.
arxiv-1502-00717 | Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image Segmentation and Cosegmentation |  http://arxiv.org/abs/1502.00717  | author:Hongyuan Zhu, Fanman Meng, Jianfei Cai, Shijian Lu category:cs.CV published:2015-02-03 summary:Image segmentation refers to the process to divide an image intononoverlapping meaningful regions according to human perception, which hasbecome a classic topic since the early ages of computer vision. A lot ofresearch has been conducted and has resulted in many applications. However,while many segmentation algorithms exist, yet there are only a few sparse andoutdated summarizations available, an overview of the recent achievements andissues is lacking. We aim to provide a comprehensive review of the recentprogress in this field. Covering 180 publications, we give an overview of broadareas of segmentation topics including not only the classic bottom-upapproaches, but also the recent development in superpixel, interactive methods,object proposals, semantic image parsing and image cosegmentation. In addition,we also review the existing influential datasets and evaluation metrics.Finally, we suggest some design flavors and research directions for futureresearch in image segmentation.
arxiv-1502-00731 | Incremental Knowledge Base Construction Using DeepDive |  http://arxiv.org/abs/1502.00731  | author:Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, Christopher Ré category:cs.DB cs.CL cs.LG published:2015-02-03 summary:Populating a database with unstructured information is a long-standingproblem in industry and research that encompasses problems of extraction,cleaning, and integration. Recent names used for this problem include dealingwith dark data and knowledge base construction (KBC). In this work, we describeDeepDive, a system that combines database and machine learning ideas to helpdevelop KBC systems, and we present techniques to make the KBC process moreefficient. We observe that the KBC process is iterative, and we developtechniques to incrementally produce inference results for KBC systems. Wepropose two methods for incremental inference, based respectively on samplingand variational techniques. We also study the tradeoff space of these methodsand develop a simple rule-based optimizer. DeepDive includes all of thesecontributions, and we evaluate DeepDive on five KBC systems, showing that itcan speed up KBC inference tasks by up to two orders of magnitude withnegligible impact on quality.
arxiv-1502-00836 | Task-Driven Dictionary Learning for Hyperspectral Image Classification with Structured Sparsity Constraints |  http://arxiv.org/abs/1502.00836  | author:Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV published:2015-02-03 summary:Sparse representation models a signal as a linear combination of a smallnumber of dictionary atoms. As a generative model, it requires the dictionaryto be highly redundant in order to ensure both a stable high sparsity level anda low reconstruction error for the signal. However, in practice, thisrequirement is usually impaired by the lack of labelled training samples.Fortunately, previous research has shown that the requirement for a redundantdictionary can be less rigorous if simultaneous sparse approximation isemployed, which can be carried out by enforcing various structured sparsityconstraints on the sparse codes of the neighboring pixels. In addition,numerous works have shown that applying a variety of dictionary learningmethods for the sparse representation model can also improve the classificationperformance. In this paper, we highlight the task-driven dictionary learningalgorithm, which is a general framework for the supervised dictionary learningmethod. We propose to enforce structured sparsity priors on the task-drivendictionary learning method in order to improve the performance of thehyperspectral classification. Our approach is able to benefit from both theadvantages of the simultaneous sparse representation and those of thesupervised dictionary learning. We enforce two different structured sparsitypriors, the joint and Laplacian sparsity, on the task-driven dictionarylearning method and provide the details of the corresponding optimizationalgorithms. Experiments on numerous popular hyperspectral images demonstratethat the classification performance of our approach is superior to sparserepresentation classifier with structured priors or the task-driven dictionarylearning method.
arxiv-1502-00839 | A multiset model of multi-species evolution to solve big deceptive problems |  http://arxiv.org/abs/1502.00839  | author:Luis Correia, Antonio Manso category:cs.NE q-bio.PE published:2015-02-03 summary:This chapter presents SMuGA, an integration of symbiogenesis with theMultiset Genetic Algorithm (MuGA). The symbiogenetic approach used here isbased on the host-parasite model with the novelty of varying the length ofparasites along the evolutionary process. Additionally, it modelscollaborations between multiple parasites and a single host. To improveefficiency, we introduced proxy evaluation of parasites, which saves fitnessfunction calls and exponentially reduces the symbiotic collaborations produced.Another novel feature consists of breaking the evolutionary cycle into twophases: a symbiotic phase and a phase of independent evolution of both hostsand parasites. SMuGA was tested in optimization of a variety of deceptivefunctions, with results one order of magnitude better than state of the artsymbiotic algorithms. This allowed to optimize deceptive problems with largesizes, and showed a linear scaling in the number of iterations to attain theoptimum.
arxiv-1502-00743 | Deep Joint Task Learning for Generic Object Extraction |  http://arxiv.org/abs/1502.00743  | author:Xiaolong Wang, Liliang Zhang, Liang Lin, Zhujin Liang, Wangmeng Zuo category:cs.CV 68U01 published:2015-02-03 summary:This paper investigates how to extract objects-of-interest without relying onhand-craft features and sliding windows approaches, that aims to jointly solvetwo sub-tasks: (i) rapidly localizing salient objects from images, and (ii)accurately segmenting the objects based on the localizations. We present ageneral joint task learning framework, in which each task (either objectlocalization or object segmentation) is tackled via a multi-layer convolutionalneural network, and the two networks work collaboratively to boost performance.In particular, we propose to incorporate latent variables bridging the twonetworks in a joint optimization manner. The first network directly predictsthe positions and scales of salient objects from raw images, and the latentvariables adjust the object localizations to feed the second network thatproduces pixelwise object masks. An EM-type method is presented for theoptimization, iterating with two steps: (i) by using the two networks, itestimates the latent variables by employing an MCMC-based sampling method; (ii)it optimizes the parameters of the two networks unitedly via back propagation,with the fixed latent variables. Extensive experiments suggest that ourframework significantly outperforms other state-of-the-art approaches in bothaccuracy and efficiency (e.g. 1000 times faster than competing approaches).
arxiv-1502-00956 | ORB-SLAM: a Versatile and Accurate Monocular SLAM System |  http://arxiv.org/abs/1502.00956  | author:Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos category:cs.RO cs.CV published:2015-02-03 summary:This paper presents ORB-SLAM, a feature-based monocular SLAM system thatoperates in real time, in small and large, indoor and outdoor environments. Thesystem is robust to severe motion clutter, allows wide baseline loop closingand relocalization, and includes full automatic initialization. Building onexcellent algorithms of recent years, we designed from scratch a novel systemthat uses the same features for all SLAM tasks: tracking, mapping,relocalization, and loop closing. A survival of the fittest strategy thatselects the points and keyframes of the reconstruction leads to excellentrobustness and generates a compact and trackable map that only grows if thescene content changes, allowing lifelong operation. We present an exhaustiveevaluation in 27 sequences from the most popular datasets. ORB-SLAM achievesunprecedented performance with respect to other state-of-the-art monocular SLAMapproaches. For the benefit of the community, we make the source code public.
arxiv-1502-00852 | Face frontalization for Alignment and Recognition |  http://arxiv.org/abs/1502.00852  | author:Christos Sagonas, Yannis Panagakis, Stefanos Zafeiriou, Maja Pantic category:cs.CV published:2015-02-03 summary:Recently, it was shown that excellent results can be achieved in both facelandmark localization and pose-invariant face recognition. These breakthroughsare attributed to the efforts of the community to manually annotate facialimages in many different poses and to collect 3D faces data. In this paper, wepropose a novel method for joint face landmark localization and frontal facereconstruction (pose correction) using a small set of frontal images only. Byobserving that the frontal facial image is the one with the minimum rank fromall different poses we formulate an appropriate model which is able to jointlyrecover the facial landmarks as well as the frontalized version of the face. Tothis end, a suitable optimization problem, involving the minimization of thenuclear norm and the matrix $\ell_1$ norm, is solved. The proposed method isassessed in frontal face reconstruction (pose correction), face landmarklocalization, and pose-invariant face recognition and verification byconducting experiments on $6$ facial images databases. The experimental resultsdemonstrate the effectiveness of the proposed method.
arxiv-1502-00873 | DeepID3: Face Recognition with Very Deep Neural Networks |  http://arxiv.org/abs/1502.00873  | author:Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2015-02-03 summary:The state-of-the-art of face recognition has been significantly advanced bythe emergence of deep learning. Very deep neural networks recently achievedgreat success on general object recognition because of their superb learningcapacity. This motivates us to investigate their effectiveness on facerecognition. This paper proposes two very deep neural network architectures,referred to as DeepID3, for face recognition. These two architectures arerebuilt from stacked convolution and inception layers proposed in VGG net andGoogLeNet to make them suitable to face recognition. Joint faceidentification-verification supervisory signals are added to both intermediateand final feature extraction layers during training. An ensemble of theproposed two architectures achieves 99.53% LFW face verification accuracy and96.0% LFW rank-1 face identification accuracy, respectively. A furtherdiscussion of LFW face verification result is given in the end.
arxiv-1502-00756 | Design of a Mobile Face Recognition System for Visually Impaired Persons |  http://arxiv.org/abs/1502.00756  | author:Shonal Chaudhry, Rohitash Chandra category:cs.CY cs.CV cs.HC published:2015-02-03 summary:It is estimated that 285 million people globally are visually impaired. Amajority of these people live in developing countries and are among the elderlypopulation. One of the most difficult tasks faced by the visually impaired isidentification of people. While naturally, voice recognition is a common methodof identification, it is an intuitive and difficult process. The rise ofcomputation capability of mobile devices gives motivation to developapplications that can assist visually impaired persons. With the availabilityof mobile devices, these people can be assisted by an additional method ofidentification through intelligent software based on computer visiontechniques. In this paper, we present the design and implementation of a facedetection and recognition system for the visually impaired through the use ofmobile computing. This mobile system is assisted by a server-based supportsystem. The system was tested on a custom video database. Experiment resultsshow high face detection accuracy and promising face recognition accuracy insuitable conditions. The challenges of the system lie in better recognitiontechniques for difficult situations in terms of lighting and weather.
arxiv-1502-00744 | Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection |  http://arxiv.org/abs/1502.00744  | author:Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan category:cs.CV 68U01 published:2015-02-03 summary:This paper proposes a reconfigurable model to recognize and detect multiclass(or multiview) objects with large variation in appearance. Compared with wellacknowledged hierarchical models, we study two advanced capabilities inhierarchy for object modeling: (i) "switch" variables(i.e. or-nodes) forspecifying alternative compositions, and (ii) making local classifiers (i.e.leaf-nodes) shared among different classes. These capabilities enable us toaccount well for structural variabilities while preserving the model compact.Our model, in the form of an And-Or Graph, comprises four layers: a batch ofleaf-nodes with collaborative edges in bottom for localizing object parts; theor-nodes over bottom to activate their children leaf-nodes; the and-nodes toclassify objects as a whole; one root-node on the top for switching multiclassclassification, which is also an or-node. For model training, we present anEM-type algorithm, namely dynamical structural optimization (DSO), toiteratively determine the structural configuration, (e.g., leaf-node generationassociated with their parent or-nodes and shared across other classes), alongwith optimizing multi-layer parameters. The proposed method is valid onchallenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achievesstate-of-the-arts performance.
arxiv-1502-00916 | Learning Planar Ising Models |  http://arxiv.org/abs/1502.00916  | author:Jason K. Johnson, Diane Oyen, Michael Chertkov, Praneeth Netrapalli category:stat.ML published:2015-02-03 summary:Inference and learning of graphical models are both well-studied problems instatistics and machine learning that have found many applications in scienceand engineering. However, exact inference is intractable in general graphicalmodels, which suggests the problem of seeking the best approximation to acollection of random variables within some tractable family of graphicalmodels. In this paper, we focus on the class of planar Ising models, for whichexact inference is tractable using techniques of statistical physics. Based onthese techniques and recent methods for planarity testing and planar embedding,we propose a simple greedy algorithm for learning the best planar Ising modelto approximate an arbitrary collection of binary random variables (possiblyfrom sample data). Given the set of all pairwise correlations among variables,we select a planar graph and optimal planar Ising model defined on this graphto best approximate that set of correlations. We demonstrate our method insimulations and for the application of modeling senate voting records.
arxiv-1502-00831 | Open System Categorical Quantum Semantics in Natural Language Processing |  http://arxiv.org/abs/1502.00831  | author:Robin Piedeleu, Dimitri Kartsaklis, Bob Coecke, Mehrnoosh Sadrzadeh category:cs.CL cs.LO math.CT math.QA published:2015-02-03 summary:Originally inspired by categorical quantum mechanics (Abramsky and Coecke,LiCS'04), the categorical compositional distributional model of naturallanguage meaning of Coecke, Sadrzadeh and Clark provides a conceptuallymotivated procedure to compute the meaning of a sentence, given its grammaticalstructure within a Lambek pregroup and a vectorial representation of themeaning of its parts. The predictions of this first model have outperformedthat of other models in mainstream empirical language processing tasks on largescale data. Moreover, just like CQM allows for varying the model in which weinterpret quantum axioms, one can also vary the model in which we interpretword meaning. In this paper we show that further developments in categorical quantummechanics are relevant to natural language processing too. Firstly, Selinger'sCPM-construction allows for explicitly taking into account lexical ambiguityand distinguishing between the two inherently different notions of homonymy andpolysemy. In terms of the model in which we interpret word meaning, this meansa passage from the vector space model to density matrices. Despite this changeof model, standard empirical methods for comparing meanings can be easilyadopted, which we demonstrate by a small-scale experiment on real-world data.This experiment moreover provides preliminary evidence of the validity of ourproposed new model for word meaning. Secondly, commutative classical structures as well as their non-commutativecounterparts that arise in the image of the CPM-construction allow for encodingrelative pronouns, verbs and adjectives, and finally, iteration of theCPM-construction, something that has no counterpart in the quantum realm,enables one to accommodate both entailment and ambiguity.
arxiv-1502-00718 | Product Reservoir Computing: Time-Series Computation with Multiplicative Neurons |  http://arxiv.org/abs/1502.00718  | author:Alireza Goudarzi, Alireza Shabani, Darko Stefanovic category:cs.NE published:2015-02-03 summary:Echo state networks (ESN), a type of reservoir computing (RC) architecture,are efficient and accurate artificial neural systems for time series processingand learning. An ESN consists of a core of recurrent neural networks, called areservoir, with a small number of tunable parameters to generate ahigh-dimensional representation of an input, and a readout layer which iseasily trained using regression to produce a desired output from the reservoirstates. Certain computational tasks involve real-time calculation of high-ordertime correlations, which requires nonlinear transformation either in thereservoir or the readout layer. Traditional ESN employs a reservoir withsigmoid or tanh function neurons. In contrast, some types of biological neuronsobey response curves that can be described as a product unit rather than a sumand threshold. Inspired by this class of neurons, we introduce a RCarchitecture with a reservoir of product nodes for time series computation. Wefind that the product RC shows many properties of standard ESN such asshort-term memory and nonlinear capacity. On standard benchmarks for chaoticprediction tasks, the product RC maintains the performance of a standardnonlinear ESN while being more amenable to mathematical analysis. Our studyprovides evidence that such networks are powerful in highly nonlinear tasksowing to high-order statistics generated by the recurrent product nodereservoir.
arxiv-1502-00946 | Classification of Hyperspectral Imagery on Embedded Grassmannians |  http://arxiv.org/abs/1502.00946  | author:Sofya Chepushtanova, Michael Kirby category:cs.CV published:2015-02-03 summary:We propose an approach for capturing the signal variability in hyperspectralimagery using the framework of the Grassmann manifold. Labeled points from eachclass are sampled and used to form abstract points on the Grassmannian. Theresulting points on the Grassmannian have representations as orthonormalmatrices and as such do not reside in Euclidean space in the usual sense. Thereare a variety of metrics which allow us to determine a distance matrices thatcan be used to realize the Grassmannian as an embedding in Euclidean space. Weillustrate that we can achieve an approximately isometric embedding of theGrassmann manifold using the chordal metric while this is not the case withgeodesic distances. However, non-isometric embeddings generated by using apseudometric on the Grassmannian lead to the best classification results. Weobserve that as the dimension of the Grassmannian grows, the accuracy of theclassification grows to 100% on two illustrative examples. We also observe adecrease in classification rates if the dimension of the points on theGrassmannian is too large for the dimension of the Euclidean space. We usesparse support vector machines to perform additional model reduction. Theresulting classifier selects a subset of dimensions of the embedding withoutloss in classification performance.
arxiv-1502-01032 | DFDL: Discriminative Feature-oriented Dictionary Learning for Histopathological Image Classification |  http://arxiv.org/abs/1502.01032  | author:Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga, UK Arvind Rao, Ganesh Rao category:cs.CV published:2015-02-03 summary:In histopathological image analysis, feature extraction for classification isa challenging task due to the diversity of histology features suitable for eachproblem as well as presence of rich geometrical structure. In this paper, wepropose an automatic feature discovery framework for extracting discriminativeclass-specific features and present a low-complexity method for classificationand disease grading in histopathology. Essentially, our DiscriminativeFeature-oriented Dictionary Learning (DFDL) method learns class-specificfeatures which are suitable for representing samples from the same class whileare poorly capable of representing samples from other classes. Experiments onthree challenging real-world image databases: 1) histopathological images ofintraductal breast lesions, 2) mammalian lung images provided by the AnimalDiagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumorimages from The Cancer Genome Atlas (TCGA) database, show the significance ofDFDL model in a variety problems over state-of-the-art methods
arxiv-1502-01057 | Personalized Web Search |  http://arxiv.org/abs/1502.01057  | author:Li Zhou category:cs.IR cs.LG published:2015-02-03 summary:Personalization is important for search engines to improve user experience.Most of the existing work do pure feature engineering and extract a lot ofsession-style features and then train a ranking model. Here we proposed a novelway to model both long term and short term user behavior using Multi-armedbandit algorithm. Our algorithm can generalize session information across userswell, and as an Explore-Exploit style algorithm, it can generalize to new urlsand new users well. Experiments show that our algorithm can improve performanceover the default ranking and outperforms several popular Multi-armed banditalgorithms.
arxiv-1502-00712 | Deep Boosting: Layered Feature Mining for General Image Classification |  http://arxiv.org/abs/1502.00712  | author:Zhanglin Peng, Liang Lin, Ruimao Zhang, Jing Xu category:cs.CV 68U01 published:2015-02-03 summary:Constructing effective representations is a critical but challenging problemin multimedia understanding. The traditional handcraft features often rely ondomain knowledge, limiting the performances of exiting methods. This paperdiscusses a novel computational architecture for general image feature mining,which assembles the primitive filters (i.e. Gabor wavelets) into compositionalfeatures in a layer-wise manner. In each layer, we produce a number of baseclassifiers (i.e. regression stumps) associated with the generated features,and discover informative compositions by using the boosting algorithm. Theoutput compositional features of each layer are treated as the base componentsto build up the next layer. Our framework is able to generate expressive imagerepresentations while inducing very discriminate functions for imageclassification. The experiments are conducted on several public datasets, andwe demonstrate superior performances over state-of-the-art approaches.
arxiv-1502-00524 | Unsupervised Incremental Learning and Prediction of Music Signals |  http://arxiv.org/abs/1502.00524  | author:Ricard Marxer, Hendrik Purwins category:cs.SD cs.IR cs.LG stat.ML 68T05 I.2.6; H.5.5 published:2015-02-02 summary:A system is presented that segments, clusters and predicts musical audio inan unsupervised manner, adjusting the number of (timbre) clustersinstantaneously to the audio input. A sequence learning algorithm adapts itsstructure to a dynamically changing clustering tree. The flow of the system isas follows: 1) segmentation by onset detection, 2) timbre representation ofeach segment by Mel frequency cepstrum coefficients, 3) discretization byincremental clustering, yielding a tree of different sound classes (e.g.instruments) that can grow or shrink on the fly driven by the instantaneoussound events, resulting in a discrete symbol sequence, 4) extraction ofstatistical regularities of the symbol sequence, using hierarchical N-grams andthe newly introduced conceptual Boltzmann machine, and 5) prediction of thenext sound event in the sequence. The system's robustness is assessed withrespect to complexity and noisiness of the signal. Clustering in isolationyields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singingvoice and drums. Onset detection jointly with clustering achieve an ARI of81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /39.2%.
arxiv-1502-00478 | Structured Occlusion Coding for Robust Face Recognition |  http://arxiv.org/abs/1502.00478  | author:Yandong Wen, Weiyang Liu, Meng Yang, Yuli Fu, Youjun Xiang, Rui Hu category:cs.CV published:2015-02-02 summary:Occlusion in face recognition is a common yet challenging problem. Whilesparse representation based classification (SRC) has been shown promisingperformance in laboratory conditions (i.e. noiseless or random pixelcorrupted), it performs much worse in practical scenarios. In this paper, weconsider the practical face recognition problem, where the occlusions arepredictable and available for sampling. We propose the structured occlusioncoding (SOC) to address occlusion problems. The structured coding here lies intwo folds. On one hand, we employ a structured dictionary for recognition. Onthe other hand, we propose to use the structured sparsity in this formulation.Specifically, SOC simultaneously separates the occlusion and classifies theimage. In this way, the problem of recognizing an occluded image is turned intoseeking a structured sparse solution on occlusion-appended dictionary. In orderto construct a well-performing occlusion dictionary, we propose an occlusionmask estimating technique via locality constrained dictionary (LCD), showingstriking improvement in occlusion sample. On a category-specific occlusiondictionary, we replace norm sparsity with the structured sparsity which isshown more robust, further enhancing the robustness of our approach. Moreover,SOC achieves significant improvement in handling large occlusion in real world.Extensive experiments are conducted on public data sets to validate thesuperiority of the proposed algorithm.
arxiv-1502-00558 | Complex-Valued Hough Transforms for Circles |  http://arxiv.org/abs/1502.00558  | author:Marcelo Cicconet, Davi Geiger, Michael Werman category:cs.CV published:2015-02-02 summary:This paper advocates the use of complex variables to represent votes in theHough transform for circle detection. Replacing the positive numbersclassically used in the parameter space of the Hough transforms by complexnumbers allows cancellation effects when adding up the votes. Cancellation andthe computation of shape likelihood via a complex number's magnitude squarelead to more robust solutions than the "classic" algorithms, as shown bycomputational experiments on synthetic and real datasets.
arxiv-1502-00561 | Quantum Pairwise Symmetry: Applications in 2D Shape Analysis |  http://arxiv.org/abs/1502.00561  | author:Marcelo Cicconet, Davi Geiger, Michael Werman category:cs.CV published:2015-02-02 summary:A pair of rooted tangents -- defining a quantum triangle -- with anassociated quantum wave of spin 1/2 is proposed as the primitive to representand compute symmetry. Measures of the spin characterize how "isosceles" or how"degenerate" these triangles are -- which corresponds to their mirror orparallel symmetry. We also introduce a complex-valued kernel to modelprobability errors in the parameter space, which is more robust to noise andclutter than the classical model.
arxiv-1502-00374 | Adaptive Scene Category Discovery with Generative Learning and Compositional Sampling |  http://arxiv.org/abs/1502.00374  | author:Liang Lin, Ruimao Zhang, Xiaohua Duan category:cs.CV 68U01 published:2015-02-02 summary:This paper investigates a general framework to discover categories ofunlabeled scene images according to their appearances (i.e., textures andstructures). We jointly solve the two coupled tasks in an unsupervised manner:(i) classifying images without pre-determining the number of categories, and(ii) pursuing generative model for each category. In our method, each image isrepresented by two types of image descriptors that are effective to captureimage appearances from different aspects. By treating each image as a graphvertex, we build up an graph, and pose the image categorization as a graphpartition process. Specifically, a partitioned sub-graph can be regarded as acategory of scenes, and we define the probabilistic model of graph partition byaccumulating the generative models of all separated categories. For efficientinference with the graph, we employ a stochastic cluster sampling algorithm,which is designed based on the Metropolis-Hasting mechanism. During theiterations of inference, the model of each category is analytically updated bya generative learning algorithm. In the experiments, our approach is validatedon several challenging databases, and it outperforms other popularstate-of-the-art methods. The implementation details and empirical analysis arepresented as well.
arxiv-1502-00354 | A Web-based Interactive Visual Graph Analytics Platform |  http://arxiv.org/abs/1502.00354  | author:Nesreen K. Ahmed, Ryan A. Rossi category:cs.SI cs.HC stat.ML published:2015-02-02 summary:This paper proposes a web-based visual graph analytics platform forinteractive graph mining, visualization, and real-time exploration of networks.GraphVis is fast, intuitive, and flexible, combining interactive visualizationswith analytic techniques to reveal important patterns and insights for sensemaking, reasoning, and decision making. Networks can be visualized and exploredwithin seconds by simply drag-and-dropping a graph file into the web browser.The structure, properties, and patterns of the network are computedautomatically and can be instantly explored in real-time. At the heart ofGraphVis lies a multi-level interactive network visualization and analyticsengine that allows for real-time graph mining and exploration across multiplelevels of granularity simultaneously. Both the graph analytic and visualizationtechniques (at each level of granularity) are dynamic and interactive, withimmediate and continuous visual feedback upon every user interaction (e.g.,change of a slider for filtering). Furthermore, nodes, edges, and subgraphs areeasily inserted, deleted or exported via a number of novel techniques and toolsthat make it extremely easy and flexible for exploring, testing hypothesis, andunderstanding networks in real-time over the web. A number of interactivevisual graph analytic techniques are also proposed including interactive rolediscovery methods, community detection, as well as a number of novel blockmodels for generating graphs with community structure. Finally, we alsohighlight other key aspects including filtering, querying, ranking,manipulating, exporting, partitioning, as well as tools for dynamic networkanalysis and visualization, interactive graph generators, and a variety ofmulti-level network analysis, summarization, and statistical techniques.
arxiv-1502-00501 | An Expressive Deep Model for Human Action Parsing from A Single Image |  http://arxiv.org/abs/1502.00501  | author:Zhujin Liang, Xiaolong Wang, Rui Huang, Liang Lin category:cs.CV 68U01 published:2015-02-02 summary:This paper aims at one newly raising task in vision and multimedia research:recognizing human actions from still images. Its main challenges lie in thelarge variations in human poses and appearances, as well as the lack oftemporal motion information. Addressing these problems, we propose to developan expressive deep model to naturally integrate human layout and surroundingcontexts for higher level action understanding from still images. Inparticular, a Deep Belief Net is trained to fuse information from differentnoisy sources such as body part detection and object detection. To bridge thesemantic gap, we used manually labeled data to greatly improve theeffectiveness and efficiency of the pre-training and fine-tuning stages of theDBN training. The resulting framework is shown to be robust to sometimesunreliable inputs (e.g., imprecise detections of human parts and objects), andoutperforms the state-of-the-art approaches.
arxiv-1502-00512 | Scaling Recurrent Neural Network Language Models |  http://arxiv.org/abs/1502.00512  | author:Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson category:cs.CL cs.LG published:2015-02-02 summary:This paper investigates the scaling properties of Recurrent Neural NetworkLanguage Models (RNNLMs). We discuss how to train very large RNNs on GPUs andaddress the questions of how RNNLMs scale with respect to model size,training-set size, computational costs and memory. Our analysis shows thatdespite being more costly to train, RNNLMs obtain much lower perplexities onstandard benchmarks than n-gram models. We train the largest known RNNs andpresent relative word error rates gains of 18% on an ASR task. We also presentthe new lowest perplexities on the recently released billion word languagemodelling benchmark, 1 BLEU point gain on machine translation and a 17%relative hit rate gain in word prediction.
arxiv-1502-00377 | Integrating Graph Partitioning and Matching for Trajectory Analysis in Video Surveillance |  http://arxiv.org/abs/1502.00377  | author:Liang Lin, Yongyi Lu, Yan Pan, Xiaowu Chen category:cs.CV 68U01 published:2015-02-02 summary:In order to track the moving objects in long range against occlusion,interruption, and background clutter, this paper proposes a unified approachfor global trajectory analysis. Instead of the traditional frame-by-frametracking, our method recovers target trajectories based on a short sequence ofvideo frames, e.g. $15$ frames. We initially calculate a foreground map at eachframe, as obtained from a state-of-the-art background model. An attribute graphis then extracted from the foreground map, where the graph vertices are imageprimitives represented by the composite features. With this graphrepresentation, we pose trajectory analysis as a joint task of spatial graphpartitioning and temporal graph matching. The task can be formulated bymaximizing a posteriori under the Bayesian framework, in which we integrate thespatio-temporal contexts and the appearance models. The probabilistic inferenceis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given aperoid of observed frames, the algorithm simulates a ergodic and aperiodicMarkov Chain, and it visits a sequence of solution states in the joint space ofspatial graph partitioning and temporal graph matching. In the experiments, ourmethod is tested on several challenging videos from the public datasets ofvisual surveillance, and it outperforms the state-of-the-art methods.
arxiv-1502-00500 | Fast and Robust Feature Matching for RGB-D Based Localization |  http://arxiv.org/abs/1502.00500  | author:Miguel Heredia, Felix Endres, Wolfram Burgard, Rafael Sanz category:cs.CV cs.RO published:2015-02-02 summary:In this paper we present a novel approach to global localization using anRGB-D camera in maps of visual features. For large maps, the performance ofpure image matching techniques decays in terms of robustness and computationalcost. Particularly, repeated occurrences of similar features due to repeatingstructure in the world (e.g., doorways, chairs, etc.) or missing associationsbetween observations pose critical challenges to visual localization. Weaddress these challenges using a two-step approach. We first estimate acandidate pose using few correspondences between features of the current cameraframe and the feature map. The initial set of correspondences is established byproximity in feature space. The initial pose estimate is used in the secondstep to guide spatial matching of features in 3D, i.e., searching forassociations where the image features are expected to be found in the map. ARANSAC algorithm is used to compute a fine estimation of the pose from thecorrespondences. Our approach clearly outperforms localization based on featurematching exclusively in feature space, both in terms of estimation accuracy androbustness to failure and allows for global localization in real time (30Hz).
arxiv-1502-00363 | Iterated Support Vector Machines for Distance Metric Learning |  http://arxiv.org/abs/1502.00363  | author:Wangmeng Zuo, Faqiang Wang, David Zhang, Liang Lin, Yuchi Huang, Deyu Meng, Lei Zhang category:cs.LG cs.CV published:2015-02-02 summary:Distance metric learning aims to learn from the given training data a validdistance metric, with which the similarity between data samples can be moreeffectively evaluated for classification. Metric learning is often formulatedas a convex or nonconvex optimization problem, while many existing metriclearning algorithms become inefficient for large scale problems. In this paper,we formulate metric learning as a kernel classification problem, and solve itby iterated training of support vector machines (SVM). The new formulation iseasy to implement, efficient in training, and tractable for large-scaleproblems. Two novel metric learning models, namely Positive-semidefiniteConstrained Metric Learning (PCML) and Nonnegative-coefficient ConstrainedMetric Learning (NCML), are developed. Both PCML and NCML can guarantee theglobal optimality of their solutions. Experimental results on UCI datasetclassification, handwritten digit recognition, face verification and personre-identification demonstrate that the proposed metric learning methods achievehigher classification accuracy than state-of-the-art methods and they aresignificantly more efficient in training.
arxiv-1502-00592 | A Class of DCT Approximations Based on the Feig-Winograd Algorithm |  http://arxiv.org/abs/1502.00592  | author:C. J. Tablada, F. M. Bayer, R. J. Cintra category:stat.ME cs.CV cs.MM cs.NA stat.AP published:2015-02-02 summary:A new class of matrices based on a parametrization of the Feig-Winogradfactorization of 8-point DCT is proposed. Such parametrization induces a matrixsubspace, which unifies a number of existing methods for DCT approximation. Bysolving a comprehensive multicriteria optimization problem, we identifiedseveral new DCT approximations. Obtained solutions were sought to possess thefollowing properties: (i) low multiplierless computational complexity, (ii)orthogonality or near orthogonality, (iii) low complexity invertibility, and(iv) close proximity and performance to the exact DCT. Proposed approximationswere submitted to assessment in terms of proximity to the DCT, codingperformance, and suitability for image compression. Considering Paretoefficiency, particular new proposed approximations could outperform variousexisting methods archived in literature.
arxiv-1502-00652 | Learning the Matching Function |  http://arxiv.org/abs/1502.00652  | author:Ľubor Ladický, Christian Häne, Marc Pollefeys category:cs.CV published:2015-02-02 summary:The matching function for the problem of stereo reconstruction or opticalflow has been traditionally designed as a function of the distance between thefeatures describing matched pixels. This approach works under assumption, thatthe appearance of pixels in two stereo cameras or in two consecutive videoframes does not change dramatically. However, this might not be the case, if wetry to match pixels over a large interval of time. In this paper we propose a method, which learns the matching function, thatautomatically finds the space of allowed changes in visual appearance, such asdue to the motion blur, chromatic distortions, different colour calibration orseasonal changes. Furthermore, it automatically learns the importance ofmatching scores of contextual features at different relative locations andscales. Proposed classifier gives reliable estimations of pixel disparitiesalready without any form of regularization. We evaluated our method on two standard problems - stereo matching on KITTIoutdoor dataset, optical flow on Sintel data set, and on newly introducedTimeLapse change detection dataset. Our algorithm obtained very promisingresults comparable to the state-of-the-art.
arxiv-1502-00598 | Lock in Feedback in Sequential Experiments |  http://arxiv.org/abs/1502.00598  | author:Maurits Kaptein, Davide Iannuzzi category:cs.LG published:2015-02-02 summary:We often encounter situations in which an experimenter wants to find, bysequential experimentation, $x_{max} = \arg\max_{x} f(x)$, where $f(x)$ is a(possibly unknown) function of a well controllable variable $x$. Takinginspiration from physics and engineering, we have designed a new method toaddress this problem. In this paper, we first introduce the method incontinuous time, and then present two algorithms for use in sequentialexperiments. Through a series of simulation studies, we show that the method iseffective for finding maxima of unknown functions by experimentation, even whenthe maximum of the functions drifts or when the signal to noise ratio is low.
arxiv-1502-00344 | Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal Models |  http://arxiv.org/abs/1502.00344  | author:Liang Lin, Yuanlu Xu, Xiaodan Liang, Jianhuang Lai category:cs.CV 68U01 published:2015-02-02 summary:Although it has been widely discussed in video surveillance, backgroundsubtraction is still an open problem in the context of complex scenarios, e.g.,dynamic backgrounds, illumination variations, and indistinct foregroundobjects. To address these challenges, we propose an effective backgroundsubtraction method by learning and maintaining an array of dynamic texturemodels within the spatio-temporal representations. At any location of thescene, we extract a sequence of regular video bricks, i.e. video volumesspanning over both spatial and temporal domain. The background modeling is thusposed as pursuing subspaces within the video bricks while adapting the scenevariations. For each sequence of video bricks, we pursue the subspace byemploying the ARMA (Auto Regressive Moving Average) Model that jointlycharacterizes the appearance consistency and temporal coherence of theobservations. During online processing, we incrementally update the subspacesto cope with disturbances from foreground objects and scene changes. In theexperiments, we validate the proposed method in several complex scenarios, andshow superior performances over other state-of-the-art approaches of backgroundsubtraction. The empirical studies of parameter setting and component analysisare presented as well.
arxiv-1502-00341 | Discriminatively Trained And-Or Graph Models for Object Shape Detection |  http://arxiv.org/abs/1502.00341  | author:Liang Lin, Xiaolong Wang, Wei Yang, Jian-Huang Lai category:cs.CV 68U01 published:2015-02-02 summary:In this paper, we investigate a novel reconfigurable part-based model, namelyAnd-Or graph model, to recognize object shapes in images. Our proposed modelconsists of four layers: leaf-nodes at the bottom are local classifiers fordetecting contour fragments; or-nodes above the leaf-nodes function as theswitches to activate their child leaf-nodes, making the model reconfigurableduring inference; and-nodes in a higher layer capture holistic shapedeformations; one root-node on the top, which is also an or-node, activates oneof its child and-nodes to deal with large global variations (e.g. differentposes and views). We propose a novel structural optimization algorithm todiscriminatively train the And-Or model from weakly annotated data. Thisalgorithm iteratively determines the model structures (e.g. the nodes and theirlayouts) along with the parameter learning. On several challenging datasets,our model demonstrates the effectiveness to perform robust shape-based objectdetection against background clutter and outperforms the other state-of-the-artapproaches. We also release a new shape database with annotations, whichincludes more than 1500 challenging shape instances, for recognition anddetection.
arxiv-1502-00416 | Towards a solid solution of real-time fire and flame detection |  http://arxiv.org/abs/1502.00416  | author:Bo Jiang, Yongyi Lu, Xiying Li, Liang Lin category:cs.CV 68U01 published:2015-02-02 summary:Although the object detection and recognition has received growing attentionfor decades, a robust fire and flame detection method is rarely explored. Thispaper presents an empirical study, towards a general and solid approach to fastdetect fire and flame in videos, with the applications in video surveillanceand event retrieval. Our system consists of three cascaded steps: (1) candidateregions proposing by a background model, (2) fire region classifying withcolor-texture features and a dictionary of visual words, and (3) temporalverifying. The experimental evaluation and analysis are done for each step. Webelieve that it is a useful service to both academic research and real-worldapplication. In addition, we release the software of the proposed system withthe source code, as well as a public benchmark and data set, including 64 videoclips covered both indoor and outdoor scenes under different conditions. Weachieve an 82% Recall with 93% Precision on the data set, and greatly improvethe performance by state-of-the-arts methods.
arxiv-1502-00245 | Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil |  http://arxiv.org/abs/1502.00245  | author:Christian S. Perone category:cs.LG cs.AI published:2015-02-01 summary:This study describes the experimental application of Machine Learningtechniques to build prediction models that can assess the injury riskassociated with traffic accidents. This work uses an freely available data setof traffic accident records that took place in the city of Porto Alegre/RS(Brazil) during the year of 2013. This study also provides an analysis of themost important attributes of a traffic accident that could produce an outcomeof injury to the people involved in the accident.
arxiv-1502-00192 | Pose and Shape Estimation with Discriminatively Learned Parts |  http://arxiv.org/abs/1502.00192  | author:Menglong Zhu, Xiaowei Zhou, Kostas Daniilidis category:cs.CV published:2015-02-01 summary:We introduce a new approach for estimating the 3D pose and the 3D shape of anobject from a single image. Given a training set of view exemplars, we learnand select appearance-based discriminative parts which are mapped onto the 3Dmodel from the training set through a facil- ity location optimization. Thetraining set of 3D models is summarized into a sparse set of shapes from whichwe can generalize by linear combination. Given a test picture, we detecthypotheses for each part. The main challenge is to select from these hypothesesand compute the 3D pose and shape coefficients at the same time. To achievethis, we optimize a function that minimizes simultaneously the geometricreprojection error as well as the appearance matching of the parts. We applythe alternating direction method of multipliers (ADMM) to minimize theresulting convex function. We evaluate our approach on the Fine Grained 3D Cardataset with superior performance in shape and pose errors. Our main and novelcontribution is the simultaneous solution for part localization, 3D pose andshape by maximizing both geometric and appearance compatibility.
arxiv-1502-00193 | Evolutionary Artificial Neural Network Based on Chemical Reaction Optimization |  http://arxiv.org/abs/1502.00193  | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-02-01 summary:Evolutionary algorithms (EAs) are very popular tools to design and evolveartificial neural networks (ANNs), especially to train them. These methods haveadvantages over the conventional backpropagation (BP) method because of theirlow computational requirement when searching in a large solution space. In thispaper, we employ Chemical Reaction Optimization (CRO), a newly developed globaloptimization method, to replace BP in training neural networks. CRO is apopulation-based metaheuristics mimicking the transition of molecules and theirinteractions in a chemical reaction. Simulation results show that CROoutperforms many EA strategies commonly used to train neural networks.
arxiv-1502-00194 | Real-Coded Chemical Reaction Optimization with Different Perturbation Functions |  http://arxiv.org/abs/1502.00194  | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-02-01 summary:Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimicsthe interactions of molecules in chemical reactions to search for the globaloptimum. The perturbation function greatly influences the performance of CRO onsolving different continuous problems. In this paper, we study four differentprobability distributions, namely, the Gaussian distribution, the Cauchydistribution, the exponential distribution, and a modified Rayleighdistribution, for the perturbation function of CRO. Different distributionshave different impacts on the solutions. The distributions are tested by a setof well-known benchmark functions and simulation results show that problemswith different characteristics have different preference on the distributionfunction. Our study gives guidelines to design CRO for different types ofoptimization problems.
arxiv-1502-00195 | Sensor Deployment for Air Pollution Monitoring Using Public Transportation System |  http://arxiv.org/abs/1502.00195  | author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE published:2015-02-01 summary:Air pollution monitoring is a very popular research topic and many monitoringsystems have been developed. In this paper, we formulate the Bus SensorDeployment Problem (BSDP) to select the bus routes on which sensors aredeployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO isa recently proposed metaheuristic designed to solve a wide range ofoptimization problems. Using the real world data, namely Hong Kong Island busroute data, we perform a series of simulations and the results show that CRO iscapable of solving this optimization problem efficiently.
arxiv-1502-00196 | Optimal V2G Scheduling of Electric Vehicles and Unit Commitment using Chemical Reaction Optimization |  http://arxiv.org/abs/1502.00196  | author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE published:2015-02-01 summary:An electric vehicle (EV) may be used as energy storage which allows thebi-directional electricity flow between the vehicle's battery and the electricpower grid. In order to flatten the load profile of the electricity system, EVscheduling has become a hot research topic in recent years. In this paper, wepropose a new formulation of the joint scheduling of EV and Unit Commitment(UC), called EVUC. Our formulation considers the characteristics of EVs whileoptimizing the system total running cost. We employ Chemical ReactionOptimization (CRO), a general-purpose optimization algorithm to solve thisproblem and the simulation results on a widely used set of instances indicatethat CRO can effectively optimize this problem.
arxiv-1502-00197 | An Inter-molecular Adaptive Collision Scheme for Chemical Reaction Optimization |  http://arxiv.org/abs/1502.00197  | author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE published:2015-02-01 summary:Optimization techniques are frequently applied in science and engineeringresearch and development. Evolutionary algorithms, as a kind of general-purposemetaheuristic, have been shown to be very effective in solving a wide range ofoptimization problems. A recently proposed chemical-reaction-inspiredmetaheuristic, Chemical Reaction Optimization (CRO), has been applied to solvemany global optimization problems. However, the functionality of theinter-molecular ineffective collision operator in the canonical CRO designoverlaps that of the on-wall ineffective collision operator, which canpotential impair the overall performance. In this paper we propose a newinter-molecular ineffective collision operator for CRO for global optimization.To fully utilize our newly proposed operator, we also design a scheme to adaptthe algorithm to optimization problems with different search spacecharacteristics. We analyze the performance of our proposed algorithm with anumber of widely used benchmark functions. The simulation results indicate thatthe new algorithm has superior performance over the canonical CRO.
arxiv-1502-00199 | Chemical Reaction Optimization for the Set Covering Problem |  http://arxiv.org/abs/1502.00199  | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-02-01 summary:The set covering problem (SCP) is one of the representative combinatorialoptimization problems, having many practical applications. This paperinvestigates the development of an algorithm to solve SCP by employing chemicalreaction optimization (CRO), a general-purpose metaheuristic. It is tested on awide range of benchmark instances of SCP. The simulation results indicate thatthis algorithm gives outstanding performance compared with other heuristics andmetaheuristics in solving SCP.
arxiv-1502-00324 | Modified Fast Fractal Image Compression Algorithm in spatial domain |  http://arxiv.org/abs/1502.00324  | author:M. Salarian, H. Miar Naimi category:cs.CV published:2015-02-01 summary:In this paper a new fractal image compression algorithm is proposed in whichthe time of encoding process is considerably reduced. The algorithm exploits adomain pool reduction approach, along with using innovative predefined valuesfor contrast scaling factor, S, instead of searching it across [0,1]. Only thedomain blocks with entropy greater than a threshold are considered as domainpool. As a novel point, it is assumed that in each step of the encodingprocess, the domain block with small enough distance shall be found only forthe range blocks with low activity (equivalently low entropy). This novel pointis used to find reasonable estimations of S, and use them in the encodingprocess as predefined values, mentioned above, the remaining range blocks aresplit into four new smaller range blocks and the algorithm must be iterated forthem, considered as the other step of encoding process. The algorithm has beenexamined for some of the well-known images and the results have been comparedwith the state-of-the-art algorithms. The experiments show that our proposedalgorithm has considerably lower encoding time than the other where the encodedimages are approximately the same in quality.
arxiv-1502-00231 | Feature Selection with Redundancy-complementariness Dispersion |  http://arxiv.org/abs/1502.00231  | author:Zhijun Chen, Chaozhong Wu, Yishi Zhang, Zhen Huang, Bin Ran, Ming Zhong, Nengchao Lyu category:cs.LG stat.ML I.5.2; H.1.1 published:2015-02-01 summary:Feature selection has attracted significant attention in data mining andmachine learning in the past decades. Many existing feature selection methodseliminate redundancy by measuring pairwise inter-correlation of features,whereas the complementariness of features and higher inter-correlation amongmore than two features are ignored. In this study, a modification itemconcerning the complementariness of features is introduced in the evaluationcriterion of features. Additionally, in order to identify the interferenceeffect of already-selected False Positives (FPs), theredundancy-complementariness dispersion is also taken into account to adjustthe measurement of pairwise inter-correlation of features. To illustrate theeffectiveness of proposed method, classification experiments are applied withfour frequently used classifiers on ten datasets. Classification results verifythe superiority of proposed method compared with five representative featureselection methods.
arxiv-1502-00186 | Advanced Mean Field Theory of Restricted Boltzmann Machine |  http://arxiv.org/abs/1502.00186  | author:Haiping Huang, Taro Toyoizumi category:cs.LG q-bio.NC stat.ML published:2015-02-01 summary:Learning in restricted Boltzmann machine is typically hard due to thecomputation of gradients of log-likelihood function. To describe the networkstate statistics of the restricted Boltzmann machine, we develop an advancedmean field theory based on the Bethe approximation. Our theory provides anefficient message passing based method that evaluates not only the partitionfunction (free energy) but also its gradients without requiring statisticalsampling. The results are compared with those obtained by the computationallyexpensive sampling based method.
arxiv-1502-00254 | Freehand Sketch Recognition Using Deep Features |  http://arxiv.org/abs/1502.00254  | author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV published:2015-02-01 summary:Freehand sketches often contain sparse visual detail. In spite of thesparsity, they are easily and consistently recognized by humans acrosscultures, languages and age groups. Therefore, analyzing such sparse sketchescan aid our understanding of the neuro-cognitive processes involved in visualrepresentation and recognition. In the recent past, Convolutional NeuralNetworks (CNNs) have emerged as a powerful framework for feature representationand recognition for a variety of image domains. However, the domain of sketchimages has not been explored. This paper introduces a freehand sketchrecognition framework based on "deep" features extracted from CNNs. We use twopopular CNNs for our experiments -- Imagenet CNN and a modified version ofLeNet CNN. We evaluate our recognition framework on a publicly availablebenchmark database containing thousands of freehand sketches depicting everydayobjects. Our results are an improvement over the existing state-of-the-artaccuracies by 3% - 11%. The effectiveness and relative compactness of our deepfeatures also make them an ideal candidate for related problems such assketch-based image retrieval. In addition, we provide a preliminary glimpse ofhow such features can help identify crucial attributes (e.g. object-parts) ofthe sketched objects.
arxiv-1502-00182 | A Subspace Learning Approach to High-Dimensional Matrix Decomposition with Efficient Information Sampling |  http://arxiv.org/abs/1502.00182  | author:Mostafa Rahmani, George Atia category:cs.NA cs.DS cs.LG math.NA stat.ML published:2015-02-01 summary:This paper is concerned with the problem of low-rank plus sparse matrixdecomposition for big data. Conventional algorithms for matrix decompositionuse the entire data to extract the low-rank and sparse components, and arebased on optimization problems that scale with the dimension of the data, whichlimit their scalability. Furthermore, the existing randomized approaches mostlyrely on uniform random sampling, which can be quite inefficient for many realworld data matrices that exhibit additional structures (e.g. clustering). Inthis paper, a scalable subspace-pursuit approach that transforms thedecomposition problem to a subspace learning problem is proposed. Thedecomposition is carried out using a small data sketch formed from sampledcolumns/rows. Even when the data is sampled uniformly at random, it is shownthat the sufficient number of sampled columns/rows is roughly O(r \mu), where\mu is the coherency parameter and r the rank of the low-rank component. Inaddition, efficient sampling algorithms are proposed to address the problem ofcolumn/row sampling from structured data. The proposed sampling algorithms canbe independently used for feature selection from high-dimensional data. Theproposed approach is amenable to online implementation and an online scheme isproposed.
arxiv-1502-00303 | Dynamic texture and scene classification by transferring deep image features |  http://arxiv.org/abs/1502.00303  | author:Xianbiao Qi, Chun-Guang Li, Guoying Zhao, Xiaopeng Hong, Matti Pietikäinen category:cs.CV published:2015-02-01 summary:Dynamic texture and scene classification are two fundamental problems inunderstanding natural video content. Extracting robust and effective featuresis a crucial step towards solving these problems. However the existingapproaches suffer from the sensitivity to either varying illumination, orviewpoint changing, or even camera motion, and/or the lack of spatialinformation. Inspired by the success of deep structures in imageclassification, we attempt to leverage a deep structure to extract feature fordynamic texture and scene classification. To tackle with the challenges intraining a deep structure, we propose to transfer some prior knowledge fromimage domain to video domain. To be specific, we propose to apply awell-trained Convolutional Neural Network (ConvNet) as a mid-level featureextractor to extract features from each frame, and then form a representationof a video by concatenating the first and the second order statistics over themid-level features. We term this two-level feature extraction scheme as aTransferred ConvNet Feature (TCoF). Moreover we explore two differentimplementations of the TCoF scheme, i.e., the \textit{spatial} TCoF and the\textit{temporal} TCoF, in which the mean-removed frames and the differencebetween two adjacent frames are used as the inputs of the ConvNet,respectively. We evaluate systematically the proposed spatial TCoF and thetemporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN,and Maryland, and demonstrate that the proposed approach yields superiorperformance.
arxiv-1502-00258 | Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition |  http://arxiv.org/abs/1502.00258  | author:Xiaodan Liang, Liang Lin, Liangliang Cao category:cs.CV 68U01 I.5; I.4 published:2015-02-01 summary:Action recognition is an important problem in multimedia understanding. Thispaper addresses this problem by building an expressive compositional actionmodel. We model one action instance in the video with an ensemble ofspatio-temporal compositions: a number of discrete temporal anchor frames, eachof which is further decomposed to a layout of deformable parts. In this way,our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent thelatent structure of actions e.g. triple jumping, swinging and high jumping. TheSTAOG model comprises four layers: (i) a batch of leaf-nodes in bottom fordetecting various action parts within video patches; (ii) the or-nodes overbottom, i.e. switch variables to activate their children leaf-nodes forstructural variability; (iii) the and-nodes within an anchor frame forverifying spatial composition; and (iv) the root-node at top for aggregatingscores over temporal anchor frames. Moreover, the contextual interactions aredefined between leaf-nodes in both spatial and temporal domains. For modeltraining, we develop a novel weakly supervised learning algorithm whichiteratively determines the structural configuration (e.g. the production ofleaf-nodes associated with the or-nodes) along with the optimization ofmulti-layer parameters. By fully exploiting spatio-temporal compositions andinteractions, our approach handles well large intra-class action variance (e.g.different views, individual appearances, spatio-temporal structures). Theexperimental results on the challenging databases demonstrate superiorperformance of our approach over other competing methods.
arxiv-1502-00319 | Efficient refinement of GPS-based localization in urban areas using visual information and sensor parameter |  http://arxiv.org/abs/1502.00319  | author:Mahdi Salarian, Rashid Ansari category:cs.CV cs.IR published:2015-02-01 summary:An efficient method is proposed for refining GPS-acquired locationcoordinates in urban areas using camera images, Google Street View (GSV) andsensor parameters. The main goal is to compensate for GPS location imprecisionin dense area of cities due to proximity to walls and buildings. Avail-ablemethods for better localization often use visual information by using queryimages acquired with camera-equipped mobile devices and applying imageretrieval techniques to find the closest match in a GPS-referenced image dataset. The search areas required for reliable search are about 1-2 sq. Km and theaccuracy is typically 25-100 meters. Here we describe a method based on imageretrieval where a reliable search can be confined to areas of 0.01 sq. Km andthe accuracy in our experiments is less than 10 meters. To test our procedurewe created a database by acquiring all Google Street View images close to whatis seen by a pedestrian in a large region of downtown Chicago and saved allcoordinates and orientation data to be used for confining our search region.Prior knowledge from approximate position of query image is leveraged toaddress complexity and accuracy issues of our search in a large scalegeo-tagged data set. One key aspect that differentiates our work is that itutilizes the sensor information of GPS SOS and the camera orientation inimproving localization. Finally we demonstrate retrieval-based technique areless accurate in sparse open areas compared with purely GPS measurement. Theeffectiveness of our approach is discussed in detail and experimental resultsshow improved performance when compared with regular approaches.
arxiv-1502-00256 | Human Re-identification by Matching Compositional Template with Cluster Sampling |  http://arxiv.org/abs/1502.00256  | author:Yuanlu Xu, Liang Lin, Wei-Shi Zheng, Xiaobai Liu category:cs.CV 68U01 published:2015-02-01 summary:This paper aims at a newly raising task in visual surveillance:re-identifying people at a distance by matching body information, given severalreference examples. Most of existing works solve this task by matching areference template with the target individual, but often suffer from largehuman appearance variability (e.g. different poses/views, illumination) andhigh false positives in matching caused by conjunctions, occlusions orsurrounding clutters. Addressing these problems, we construct a simple yetexpressive template from a few reference images of a certain individual, whichrepresents the body as an articulated assembly of compositional and alternativeparts, and propose an effective matching algorithm with cluster sampling. Thisalgorithm is designed within a candidacy graph whose vertices are matchingcandidates (i.e. a pair of source and target body parts), and iterates in twosteps for convergence. (i) It generates possible partial matches based oncompatible and competitive relations among body parts. (ii) It confirms thepartial matches to generate a new matching solution, which is accepted by theMarkov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstratethe superior performance of our approach on three public databases compared toexisting methods.
arxiv-1502-00250 | Driver distraction detection and recognition using RGB-D sensor |  http://arxiv.org/abs/1502.00250  | author:Céline Craye, Fakhri Karray category:cs.CV published:2015-02-01 summary:Driver inattention assessment has become a very active field in intelligenttransportation systems. Based on active sensor Kinect and computer visiontools, we have built an efficient module for detecting driver distraction andrecognizing the type of distraction. Based on color and depth map data from theKinect, our system is composed of four sub-modules. We call them eye behavior(detecting gaze and blinking), arm position (is the right arm up, down, rightof forward), head orientation, and facial expressions. Each module producesrelevant information for assessing driver inattention. They are merged togetherlater on using two different classification strategies: AdaBoost classifier andHidden Markov Model. Evaluation is done using a driving simulator and 8 driversof different gender, age and nationality for a total of more than 8 hours ofrecording. Qualitative and quantitative results show strong and accuratedetection and recognition capacity (85% accuracy for the type of distractionand 90% for distraction detection). Moreover, each module is obtainedindependently and could be used for other types of inference, such as fatiguedetection, and could be implemented for real cars systems.
arxiv-1502-00141 | An evaluation framework for event detection using a morphological model of acoustic scenes |  http://arxiv.org/abs/1502.00141  | author:Mathieu Lagrange, Grégoire Lafay, Mathias Rossignol, Emmanouil Benetos, Axel Roebel category:stat.ML cs.SD published:2015-01-31 summary:This paper introduces a model of environmental acoustic scenes which adopts amorphological approach by ab-stracting temporal structures of acoustic scenes.To demonstrate its potential, this model is employed to evaluate theperformance of a large set of acoustic events detection systems. This modelallows us to explicitly control key morphological aspects of the acoustic sceneand isolate their impact on the performance of the system under evaluation.Thus, more information can be gained on the behavior of evaluated systems,providing guidance for further improvements. The proposed model is validatedusing submitted systems from the IEEE DCASE Challenge; results indicate thatthe proposed scheme is able to successfully build datasets useful forevaluating some aspects the performance of event detection systems, moreparticularly their robustness to new listening conditions and the increasinglevel of background sounds.
arxiv-1502-00046 | Max-Margin Object Detection |  http://arxiv.org/abs/1502.00046  | author:Davis E. King category:cs.CV published:2015-01-31 summary:Most object detection methods operate by applying a binary classifier tosub-windows of an image, followed by a non-maximum suppression step wheredetections on overlapping sub-windows are removed. Since the number of possiblesub-windows in even moderately sized image datasets is extremely large, theclassifier is typically learned from only a subset of the windows. This avoidsthe computational difficulty of dealing with the entire set of sub-windows,however, as we will show in this paper, it leads to sub-optimal detectorperformance. In particular, the main contribution of this paper is the introduction of anew method, Max-Margin Object Detection (MMOD), for learning to detect objectsin images. This method does not perform any sub-sampling, but instead optimizesover all sub-windows. MMOD can be used to improve any object detection methodwhich is linear in the learned parameters, such as HOG or bag-of-visual-wordmodels. Using this approach we show substantial performance gains on threepublicly available datasets. Strikingly, we show that a single rigid HOG filtercan outperform a state-of-the-art deformable part model on the Face DetectionData Set and Benchmark when the HOG filter is learned via MMOD.
arxiv-1502-00094 | Twitter Hash Tag Recommendation |  http://arxiv.org/abs/1502.00094  | author:Roman Dovgopol, Matt Nohelty category:cs.IR cs.LG published:2015-01-31 summary:The rise in popularity of microblogging services like Twitter has led toincreased use of content annotation strategies like the hashtag. Hashtagsprovide users with a tagging mechanism to help organize, group, and createvisibility for their posts. This is a simple idea but can be challenging forthe user in practice which leads to infrequent usage. In this paper, we willinvestigate various methods of recommending hashtags as new posts are createdto encourage more widespread adoption and usage. Hashtag recommendation comeswith numerous challenges including processing huge volumes of streaming dataand content which is small and noisy. We will investigate preprocessing methodsto reduce noise in the data and determine an effective method of hashtagrecommendation based on the popular classification algorithms.
arxiv-1502-00115 | Optimized Projection for Sparse Representation Based Classification |  http://arxiv.org/abs/1502.00115  | author:Can-Yi Lu, De-Shuang Huang category:cs.CV published:2015-01-31 summary:Dimensionality reduction (DR) methods have been commonly used as a principledway to understand the high-dimensional data such as facial images. In thispaper, we propose a new supervised DR method called Optimized Projection forSparse Representation based Classification (OP-SRC), which is based on therecent face recognition method, Sparse Representation based Classification(SRC). SRC seeks a sparse linear combination on all the training data for agiven query image, and make the decision by the minimal reconstructionresidual. OP-SRC is designed on the decision rule of SRC, it aims to reduce thewithin-class reconstruction residual and simultaneously increase thebetween-class reconstruction residual on the training data. The projections areoptimized and match well with the mechanism of SRC. Therefore, SRC performswell in the OP-SRC transformed space. The feasibility and effectiveness of theproposed method is verified on the Yale, ORL and UMIST databases with promisingresults.
arxiv-1502-00093 | Deep learning of fMRI big data: a novel approach to subject-transfer decoding |  http://arxiv.org/abs/1502.00093  | author:Sotetsu Koyamada, Yumi Shikauchi, Ken Nakae, Masanori Koyama, Shin Ishii category:stat.ML cs.LG q-bio.NC published:2015-01-31 summary:As a technology to read brain states from measurable brain activities, braindecoding are widely applied in industries and medical sciences. In spite ofhigh demands in these applications for a universal decoder that can be appliedto all individuals simultaneously, large variation in brain activities acrossindividuals has limited the scope of many studies to the development ofindividual-specific decoders. In this study, we used deep neural network (DNN),a nonlinear hierarchical model, to construct a subject-transfer decoder. Ourdecoder is the first successful DNN-based subject-transfer decoder. Whenapplied to a large-scale functional magnetic resonance imaging (fMRI) database,our DNN-based decoder achieved higher decoding accuracy than other baselinemethods, including support vector machine (SVM). In order to analyze theknowledge acquired by this decoder, we applied principal sensitivity analysis(PSA) to the decoder and visualized the discriminative features that are commonto all subjects in the dataset. Our PSA successfully visualized thesubject-independent features contributing to the subject-transferability of thetrained decoder.
arxiv-1502-00082 | Category-Epitomes : Discriminatively Minimalist Representations for Object Categories |  http://arxiv.org/abs/1502.00082  | author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV published:2015-01-31 summary:Freehand line sketches are an interesting and unique form of visualrepresentation. Typically, such sketches are studied and utilized as an endproduct of the sketching process. However, we have found it instructive tostudy the sketches as sequentially accumulated composition of drawing strokesadded over time. Studying sketches in this manner has enabled us to createnovel sparse yet discriminative sketch-based representations for objectcategories which we term category-epitomes. Our procedure for obtaining theseepitomes concurrently provides a natural measure for quantifying the sparsenessunderlying the original sketch, which we term epitome-score. We construct andanalyze category-epitomes and epitome-scores for freehand sketches belonging tovarious object categories. Our analysis provides a novel viewpoint for studyingthe semantic nature of object categories.
arxiv-1502-00062 | A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue Fever |  http://arxiv.org/abs/1502.00062  | author:Vadrevu Sree Hari Rao, Mallenahalli Naresh Kumar category:stat.ML cs.AI cs.LG published:2015-01-31 summary:Identification of the influential clinical symptoms and laboratory featuresthat help in the diagnosis of dengue fever in early phase of the illness wouldaid in designing effective public health management and virologicalsurveillance strategies. Keeping this as our main objective we develop in thispaper, a new computational intelligence based methodology that predicts thediagnosis in real time, minimizing the number of false positives and falsenegatives. Our methodology consists of three major components (i) a novelmissing value imputation procedure that can be applied on any data setconsisting of categorical (nominal) and/or numeric (real or integer) (ii) awrapper based features selection method with genetic search for extracting asubset of most influential symptoms that can diagnose the illness and (iii) analternating decision tree method that employs boosting for generating highlyaccurate decision rules. The predictive models developed using our methodologyare found to be more accurate than the state-of-the-art methodologies used inthe diagnosis of the dengue fever.
arxiv-1502-00064 | A Batchwise Monotone Algorithm for Dictionary Learning |  http://arxiv.org/abs/1502.00064  | author:Huan Wang, John Wright, Daniel Spielman category:cs.LG published:2015-01-31 summary:We propose a batchwise monotone algorithm for dictionary learning. Unlike thestate-of-the-art dictionary learning algorithms which impose sparsityconstraints on a sample-by-sample basis, we instead treat the samples as abatch, and impose the sparsity constraint on the whole. The benefit ofbatchwise optimization is that the non-zeros can be better allocated across thesamples, leading to a better approximation of the whole. To accomplish this, wepropose procedures to switch non-zeros in both rows and columns in the supportof the coefficient matrix to reduce the reconstruction error. We prove in theproposed support switching procedure the objective of the algorithm, i.e., thereconstruction error, decreases monotonically and converges. Furthermore, weintroduce a block orthogonal matching pursuit algorithm that also operates onsample batches to provide a warm start. Experiments on both natural imagepatches and UCI data sets show that the proposed algorithm produces a betterapproximation with the same sparsity levels compared to the state-of-the-artalgorithms.
arxiv-1502-00130 | The Search for Computational Intelligence |  http://arxiv.org/abs/1502.00130  | author:Joseph Corneli, Ewen Maclean category:cs.NE cs.AI published:2015-01-31 summary:We define and explore in simulation several rules for the local evolution ofgenerative rules for 1D and 2D cellular automata. Our implementation usesstrategies from conceptual blending. We discuss potential applications tomodelling social dynamics.
arxiv-1502-00133 | Sparse Dueling Bandits |  http://arxiv.org/abs/1502.00133  | author:Kevin Jamieson, Sumeet Katariya, Atul Deshpande, Robert Nowak category:stat.ML cs.LG published:2015-01-31 summary:The dueling bandit problem is a variation of the classical multi-armed banditin which the allowable actions are noisy comparisons between pairs of arms.This paper focuses on a new approach for finding the "best" arm according tothe Borda criterion using noisy comparisons. We prove that in the absence ofstructural assumptions, the sample complexity of this problem is proportionalto the sum of the inverse squared gaps between the Borda scores of eachsuboptimal arm and the best arm. We explore this dependence further andconsider structural constraints on the pairwise comparison matrix (a particularform of sparsity natural to this problem) that can significantly reduce thesample complexity. This motivates a new algorithm called Successive Eliminationwith Comparison Sparsity (SECS) that exploits sparsity to find the Borda winnerusing fewer samples than standard algorithms. We also evaluate the newalgorithm experimentally with synthetic and real data. The results show thatthe sparsity model and the new algorithm can provide significant improvementsover standard approaches.
arxiv-1502-00060 | A Random Matrix Theoretical Approach to Early Event Detection in Smart Grid |  http://arxiv.org/abs/1502.00060  | author:Xing He, Robert Caiming Qiu, Qian Ai, Yinshuang Cao, Jie Gu, Zhijian Jin category:stat.ME cs.LG published:2015-01-31 summary:Power systems are developing very fast nowadays, both in size and incomplexity; this situation is a challenge for Early Event Detection (EED). Thispaper proposes a data- driven unsupervised learning method to handle thischallenge. Specifically, the random matrix theories (RMTs) are introduced asthe statistical foundations for random matrix models (RMMs); based on the RMMs,linear eigenvalue statistics (LESs) are defined via the test functions as thesystem indicators. By comparing the values of the LES between the experimentaland the theoretical ones, the anomaly detection is conducted. Furthermore, wedevelop 3D power-map to visualize the LES; it provides a robust auxiliarydecision-making mechanism to the operators. In this sense, the proposed methodconducts EED with a pure statistical procedure, requiring no knowledge ofsystem topologies, unit operation/control models, etc. The LES, as a keyingredient during this procedure, is a high dimensional indictor deriveddirectly from raw data. As an unsupervised learning indicator, the LES is muchmore sensitive than the low dimensional indictors obtained from supervisedlearning. With the statistical procedure, the proposed method is universal andfast; moreover, it is robust against traditional EED challenges (such as erroraccumulations, spurious correlations, and even bad data in core area). Casestudies, with both simulated data and real ones, validate the proposed method.To manage large-scale distributed systems, data fusion is mentioned as anotherdata processing ingredient.
arxiv-1502-00163 | Spectral Detection in the Censored Block Model |  http://arxiv.org/abs/1502.00163  | author:Alaa Saade, Florent Krzakala, Marc Lelarge, Lenka Zdeborová category:cs.SI cs.LG math.PR published:2015-01-31 summary:We consider the problem of partially recovering hidden binary variables fromthe observation of (few) censored edge weights, a problem with applications incommunity detection, correlation clustering and synchronization. We describetwo spectral algorithms for this task based on the non-backtracking and theBethe Hessian operators. These algorithms are shown to be asymptoticallyoptimal for the partial recovery problem, in that they detect the hiddenassignment as soon as it is information theoretically possible to do so.
arxiv-1502-00068 | TuPAQ: An Efficient Planner for Large-scale Predictive Analytic Queries |  http://arxiv.org/abs/1502.00068  | author:Evan R. Sparks, Ameet Talwalkar, Michael J. Franklin, Michael I. Jordan, Tim Kraska category:cs.DB cs.DC cs.LG published:2015-01-31 summary:The proliferation of massive datasets combined with the development ofsophisticated analytical techniques have enabled a wide variety of novelapplications such as improved product recommendations, automatic image tagging,and improved speech-driven interfaces. These and many other applications can besupported by Predictive Analytic Queries (PAQs). A major obstacle to supportingPAQs is the challenging and expensive process of identifying and training anappropriate predictive model. Recent efforts aiming to automate this processhave focused on single node implementations and have assumed that modeltraining itself is a black box, thus limiting the effectiveness of suchapproaches on large-scale problems. In this work, we build upon these recentefforts and propose an integrated PAQ planning architecture that combinesadvanced model search techniques, bandit resource allocation via runtimealgorithm introspection, and physical optimization via batching. The result isTuPAQ, a component of the MLbase system, which solves the PAQ planning problemwith comparable quality to exhaustive strategies but an order of magnitude moreefficiently than the standard baseline approach, and can scale to modelstrained on terabytes of data across hundreds of machines.
arxiv-1501-07719 | Montblanc: GPU accelerated Radio Interferometer Measurement Equations in support of Bayesian Inference for Radio Observations |  http://arxiv.org/abs/1501.07719  | author:Simon Perkins, Patrick Marais, Jonathan Zwart, Iniyan Natarajan, Cyril Tasse, Oleg Smirnov category:cs.DC astro-ph.IM cs.CV published:2015-01-30 summary:We present Montblanc, a GPU implementation of the Radio interferometermeasurement equation (RIME) in support of the Bayesian inference for radioobservations (BIRO) technique. BIRO uses Bayesian inference to select skymodels that best match the visibilities observed by a radio interferometer. Toaccomplish this, BIRO evaluates the RIME multiple times, varying sky modelparameters to produce multiple model visibilities. Chi-squared values computedfrom the model and observed visibilities are used as likelihood values to drivethe Bayesian sampling process and select the best sky model. As most of the elements of the RIME and chi-squared calculation areindependent of one another, they are highly amenable to parallel computation.Additionally, Montblanc caters for iterative RIME evaluation to producemultiple chi-squared values. Modified model parameters are transferred to theGPU between each iteration. We implemented Montblanc as a Python package based upon NVIDIA's CUDAarchitecture. As such, it is easy to extend and implement different pipelines.At present, Montblanc supports point and Gaussian morphologies, but is designedfor easy addition of new source profiles. Montblanc's RIME implementation is performant: On an NVIDIA K40, it isapproximately 250 times faster than MeqTrees on a dual hexacore Intel E5-2620v2CPU. Compared to the OSKAR simulator's GPU-implemented RIME components it is7.7 and 12 times faster on the same K40 for single and double-precisionfloating point respectively. However, OSKAR's RIME implementation is moregeneral than Montblanc's BIRO-tailored RIME. Theoretical analysis of Montblanc's dominant CUDA kernel suggests that it ismemory bound. In practice, profiling shows that is balanced between compute andmemory, as much of the data required by the problem is retained in L1 and L2cache.
arxiv-1501-07680 | Disaggregation of Remotely Sensed Soil Moisture in Heterogeneous Landscapes using Holistic Structure based Models |  http://arxiv.org/abs/1501.07680  | author:Subit Chakrabarti, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV 68 published:2015-01-30 summary:In this study, a novel machine learning algorithm is presented fordisaggregation of satellite soil moisture (SM) based on self-regularizedregressive models (SRRM) using high-resolution correlated information fromauxiliary sources. It includes regularized clustering that assigns softmemberships to each pixel at fine-scale followed by a kernel regression thatcomputes the value of the desired variable at all pixels. Coarse-scale remotelysensed SM were disaggregated from 10km to 1km using land cover, precipitation,land surface temperature, leaf area index, and in-situ observations of SM. Thisalgorithm was evaluated using multi-scale synthetic observations in NC Floridafor heterogeneous agricultural land covers. It was found that the root meansquare error (RMSE) for 96% of the pixels was less than 0.02 $m^3/m^3$. Theclusters generated represented the data well and reduced the RMSE by upto 40%during periods of high heterogeneity in land-cover and meteorologicalconditions. The Kullback Leibler divergence (KLD) between the true SM and thedisaggregated estimates is close to 0, for both vegetated and baresoillandcovers. The disaggregated estimates were compared to those generated by thePrinciple of Relevant Information (PRI) method. The RMSE for the PRIdisaggregated estimates is higher than the RMSE for the SRRM on each day of theseason. The KLD of the disaggregated estimates generated by the SRRM is atleast four orders of magnitude lower than those for the PRI disaggregatedestimates, while the computational time needed was reduced by three times. Theresults indicate that the SRRM can be used for disaggregating SM with complexnon-linear correlations on a grid with high accuracy.
arxiv-1501-07844 | A Proximal Bregman Projection Approach to Continuous Max-Flow Problems Using Entropic Distances |  http://arxiv.org/abs/1501.07844  | author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV published:2015-01-30 summary:One issue limiting the adaption of large-scale multi-region segmentation isthe sometimes prohibitive memory requirements. This is especially troublingconsidering advances in massively parallel computing and commercial graphicsprocessing units because of their already limited memory compared to thecurrent random access memory used in more traditional computation. To addressthis issue in the field of continuous max-flow segmentation, we have developeda \textit{pseudo-flow} framework using the theory of Bregman proximalprojections and entropic distances which implicitly represents flow variablesbetween labels and designated source and sink nodes. This reduces the memoryrequirements for max-flow segmentation by approximately 20\% for Potts modelsand approximately 30\% for hierarchical max-flow (HMF) and directed acyclicgraph max-flow (DAGMF) models. This represents a great improvement in thestate-of-the-art in max-flow segmentation, allowing for much larger problems tobe addressed and accelerated using commercially available graphics processinghardware.
arxiv-1501-07758 | Gibbs-Ringing Artifact Removal Based on Local Subvoxel-shifts |  http://arxiv.org/abs/1501.07758  | author:Elias Kellner, Bibek Dhital, Marco Reisert category:physics.med-ph cs.CV published:2015-01-30 summary:Gibbs-ringing is a well known artifact which manifests itself as spuriousoscillations in the vicinity of sharp image transients, e.g. at tissueboundaries. The origin can be seen in the truncation of k-space during MRIdata-acquisition. Consequently, correction techniques like Gegenbauerreconstruction or extrapolation methods aim at recovering these missing data.Here, we present a simple and robust method which exploits a different view onthe Gibbs-phenomena. The truncation in k-space can be interpreted as aconvolution with a sinc-function in image space. Hence, the severity of theartifacts depends on how the sinc-function is sampled. We propose tore-interpolate the image based on local, subvoxel shifts to sample the ringingpattern at the zero-crossings of the oscillating sinc-function. With this, theartifact can effectively and robustly be removed with a minimal amount ofsmoothing.
arxiv-1503-07460 | RANSAC based three points algorithm for ellipse fitting of spherical object's projection |  http://arxiv.org/abs/1503.07460  | author:Shenghui Xu category:cs.CV published:2015-01-30 summary:As the spherical object can be seen everywhere, we should extract the ellipseimage accurately and fit it by implicit algebraic curve in order to finish the3D reconstruction. In this paper, we propose a new ellipse fitting algorithmwhich only needs three points to fit the projection of spherical object and isdifferent from the traditional algorithms that need at least five point. Thefitting procedure is just similar as the estimation of Fundamental Matrixestimation by seven points, and the RANSAC algorithm has also been used toexclude the interference of noise and scattered points.
arxiv-1501-07768 | Confidence intervals for AB-test |  http://arxiv.org/abs/1501.07768  | author:Cyrille Dubarry category:stat.ML published:2015-01-30 summary:AB-testing is a very popular technique in web companies since it makes itpossible to accurately predict the impact of a modification with the simplicityof a random split across users. One of the critical aspects of an AB-test isits duration and it is important to reliably compute confidence intervalsassociated with the metric of interest to know when to stop the test. In thispaper, we define a clean mathematical framework to model the AB-test process.We then propose three algorithms based on bootstrapping and on the centrallimit theorem to compute reliable confidence intervals which extend to othermetrics than the common probabilities of success. They apply to both absoluteand relative increments of the most used comparison metrics, including thenumber of occurrences of a particular event and a click-through rate implying aratio.
arxiv-1501-07862 | An Analytical Study of different Document Image Binarization Methods |  http://arxiv.org/abs/1501.07862  | author:Mahua Nandy, Satadal Saha category:cs.CV published:2015-01-30 summary:Document image has been the area of research for a couple of decades becauseof its potential application in the area of text recognition, line recognitionor any other shape recognition from the image. For most of these purposesbinarization of image becomes mandatory as far as recognition is concerned.Throughout couple decades standard algorithms have already been developed forthis purpose. Some of these algorithms are applicable to degraded image also.Our objective behind this work is to study the existing techniques, comparethem in view of advantages and disadvantages and modify some of thesealgorithms to optimize time or performance.
arxiv-1501-07681 | Vector Quantization by Minimizing Kullback-Leibler Divergence |  http://arxiv.org/abs/1501.07681  | author:Lan Yang, Jingbin Wang, Yujin Tu, Prarthana Mahapatra, Nelson Cardoso category:cs.CV published:2015-01-30 summary:This paper proposes a new method for vector quantization by minimizing theKullback-Leibler Divergence between the class label distributions over thequantization inputs, which are original vectors, and the output, which is thequantization subsets of the vector set. In this way, the vector quantizationoutput can keep as much information of the class label as possible. Anobjective function is constructed and we also developed an iterative algorithmto minimize it. The new method is evaluated on bag-of-features based imageclassification problem.
arxiv-1501-07873 | Sketch-a-Net that Beats Humans |  http://arxiv.org/abs/1501.07873  | author:Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales category:cs.CV cs.NE published:2015-01-30 summary:We propose a multi-scale multi-channel deep neural network framework that,for the first time, yields sketch recognition performance surpassing that ofhumans. Our superior performance is a result of explicitly embedding the uniquecharacteristics of sketches in our model: (i) a network architecture designedfor sketch rather than natural photo statistics, (ii) a multi-channelgeneralisation that encodes sequential ordering in the sketching process, and(iii) a multi-scale network ensemble with joint Bayesian fusion that accountsfor the different levels of abstraction exhibited in free-hand sketches. Weshow that state-of-the-art deep networks specifically engineered for photos ofnatural objects fail to perform well on sketch recognition, regardless whetherthey are trained using photo or sketch. Our network on the other hand not onlydelivers the best performance on the largest human sketch dataset to date, butalso is small in size making efficient training possible using just CPUs.
arxiv-1501-07645 | Hyper-parameter optimization of Deep Convolutional Networks for object recognition |  http://arxiv.org/abs/1501.07645  | author:Sachin S. Talathi category:cs.CV cs.LG published:2015-01-30 summary:Recently sequential model based optimization (SMBO) has emerged as apromising hyper-parameter optimization strategy in machine learning. In thiswork, we investigate SMBO to identify architecture hyper-parameters of deepconvolution networks (DCNs) object recognition. We propose a simple SMBOstrategy that starts from a set of random initial DCN architectures to generatenew architectures, which on training perform well on a given dataset. Using theproposed SMBO strategy we are able to identify a number of DCN architecturesthat produce results that are comparable to state-of-the-art results on objectrecognition benchmarks.
arxiv-1501-07738 | Co-Regularized Deep Representations for Video Summarization |  http://arxiv.org/abs/1501.07738  | author:Olivier Morère, Hanlin Goh, Antoine Veillard, Vijay Chandrasekhar, Jie Lin category:cs.CV published:2015-01-30 summary:Compact keyframe-based video summaries are a popular way of generatingviewership on video sharing platforms. Yet, creating relevant and compellingsummaries for arbitrarily long videos with a small number of keyframes is achallenging task. We propose a comprehensive keyframe-based summarizationframework combining deep convolutional neural networks and restricted Boltzmannmachines. An original co-regularization scheme is used to discover meaningfulsubject-scene associations. The resulting multimodal representations are thenused to select highly-relevant keyframes. A comprehensive user study isconducted comparing our proposed method to a variety of schemes, including thesummarization currently in use by one of the most popular video sharingwebsites. The results show that our method consistently outperforms thebaseline schemes for any given amount of keyframes both in terms ofattractiveness and informativeness. The lead is even more significant forsmaller summaries.
arxiv-1501-07692 | Blob indentation identification via curvature measurement |  http://arxiv.org/abs/1501.07692  | author:Matthew Sottile category:cs.CV published:2015-01-30 summary:This paper presents a novel method for identifying indentations on theboundary of solid 2D shape. It uses the signed curvature at a set of pointsalong the boundary to identify indentations and provides one parameter fortuning the selection mechanism for discriminating indentations from otherboundary irregularities. An efficient implementation is described based on theFourier transform for calculating curvature from a sequence of points obtainedfrom the boundary of a binary blob.
arxiv-1501-07867 | Multi-task Image Classification via Collaborative, Hierarchical Spike-and-Slab Priors |  http://arxiv.org/abs/1501.07867  | author:Hojjat Seyed Mousavi, Umamahesh Srinivas, Vishal Monga, Yuanming Suo, Minh Dao, Trac. D. Tran category:cs.CV published:2015-01-30 summary:Promising results have been achieved in image classification problems byexploiting the discriminative power of sparse representations forclassification (SRC). Recently, it has been shown that the use of\emph{class-specific} spike-and-slab priors in conjunction with theclass-specific dictionaries from SRC is particularly effective in low trainingscenarios. As a logical extension, we build on this framework for multitaskscenarios, wherein multiple representations of the same physical phenomena areavailable. We experimentally demonstrate the benefits of mining jointinformation from different camera views for multi-view face recognition.
arxiv-1502-00030 | SHOE: Supervised Hashing with Output Embeddings |  http://arxiv.org/abs/1502.00030  | author:Sravanthi Bondugula, Varun Manjunatha, Larry S. Davis, David Doermann category:cs.CV published:2015-01-30 summary:We present a supervised binary encoding scheme for image retrieval thatlearns projections by taking into account similarity between classes obtainedfrom output embeddings. Our motivation is that binary hash codes learned inthis way improve both the visual quality of retrieval results and existingsupervised hashing schemes. We employ a sequential greedy optimization thatlearns relationship aware projections by minimizing the difference betweeninner products of binary codes and output embedding vectors. We develop a jointoptimization framework to learn projections which improve the accuracy ofsupervised hashing over the current state of the art with respect to standardand sibling evaluation metrics. We further boost performance by applying thesupervised dimensionality reduction technique on kernelized input CNN features.Experiments are performed on three datasets: CUB-2011, SUN-Attribute andImageNet ILSVRC 2010. As a by-product of our method, we show that using asimple k-nn pooling classifier with our discriminative codes improves over thecomplex classification models on fine grained datasets like CUB and offer animpressive compression ratio of 1024 on CNN features.
arxiv-1501-07676 | Towards Resolving Software Quality-in-Use Measurement Challenges |  http://arxiv.org/abs/1501.07676  | author:Issa Atoum, Chih How Bong, Narayanan Kulathuramaiyer category:cs.SE cs.CL published:2015-01-30 summary:Software quality-in-use comprehends the quality from user's perspectives. Ithas gained its importance in e-learning applications, mobile service basedapplications and project management tools. User's decisions on softwareacquisitions are often ad hoc or based on preference due to difficulty inquantitatively measure software quality-in-use. However, why quality-in-usemeasurement is difficult? Although there are many software quality models toour knowledge, no works surveys the challenges related to softwarequality-in-use measurement. This paper has two main contributions; 1) presentsmajor issues and challenges in measuring software quality-in-use in the contextof the ISO SQuaRE series and related software quality models, 2) Presents anovel framework that can be used to predict software quality-in-use, and 3)presents preliminary results of quality-in-use topic prediction. Concisely, theissues are related to the complexity of the current standard models and thelimitations and incompleteness of the customized software quality models. Theproposed framework employs sentiment analysis techniques to predict softwarequality-in-use.
arxiv-1501-07683 | Downscaling Microwave Brightness Temperatures Using Self Regularized Regressive Models |  http://arxiv.org/abs/1501.07683  | author:Subit Chakrabarti, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV 68 published:2015-01-30 summary:A novel algorithm is proposed to downscale microwave brightness temperatures($\mathrm{T_B}$), at scales of 10-40 km such as those from the Soil MoistureActive Passive mission to a resolution meaningful for hydrological andagricultural applications. This algorithm, called Self-Regularized RegressiveModels (SRRM), uses auxiliary variables correlated to $\mathrm{T_B}$ along-witha limited set of \textit{in-situ} SM observations, which are converted to highresolution $\mathrm{T_B}$ observations using biophysical models. It includes aninformation-theoretic clustering step based on all auxiliary variables toidentify areas of similarity, followed by a kernel regression step thatproduces downscaled $\mathrm{T_B}$. This was implemented on a multi-scalesynthetic data-set over NC-Florida for one year. An RMSE of 5.76~K withstandard deviation of 2.8~k was achieved during the vegetated season and anRMSE of 1.2~K with a standard deviation of 0.9~K during periods of novegetation.
arxiv-1501-07467 | Regression and Learning to Rank Aggregation for User Engagement Evaluation |  http://arxiv.org/abs/1501.07467  | author:Hamed Zamani, Azadeh Shakery, Pooya Moradi category:cs.IR cs.LG H.2.8; J.4 published:2015-01-29 summary:User engagement refers to the amount of interaction an instance (e.g., tweet,news, and forum post) achieves. Ranking the items in social media websitesbased on the amount of user participation in them, can be used in differentapplications, such as recommender systems. In this paper, we consider a tweetcontaining a rating for a movie as an instance and focus on ranking theinstances of each user based on their engagement, i.e., the total number ofretweets and favorites it will gain. For this task, we define several features which can be extracted from themeta-data of each tweet. The features are partitioned into three categories:user-based, movie-based, and tweet-based. We show that in order to obtain goodresults, features from all categories should be considered. We exploitregression and learning to rank methods to rank the tweets and propose toaggregate the results of regression and learning to rank methods to achievebetter performance. We have run our experiments on an extended version ofMovieTweeting dataset provided by ACM RecSys Challenge 2014. The results showthat learning to rank approach outperforms most of the regression models andthe combination can improve the performance significantly.
arxiv-1501-07430 | Bayesian Hierarchical Clustering with Exponential Family: Small-Variance Asymptotics and Reducibility |  http://arxiv.org/abs/1501.07430  | author:Juho Lee, Seungjin Choi category:stat.ML cs.LG published:2015-01-29 summary:Bayesian hierarchical clustering (BHC) is an agglomerative clustering method,where a probabilistic model is defined and its marginal likelihoods areevaluated to decide which clusters to merge. While BHC provides a fewadvantages over traditional distance-based agglomerative clustering algorithms,successive evaluation of marginal likelihoods and careful hyperparameter tuningare cumbersome and limit the scalability. In this paper we relax BHC into anon-probabilistic formulation, exploring small-variance asymptotics inconjugate-exponential models. We develop a novel clustering algorithm, referredto as relaxed BHC (RBHC), from the asymptotic limit of the BHC model thatexhibits the scalability of distance-based agglomerative clustering algorithmsas well as the flexibility of Bayesian nonparametric models. We alsoinvestigate the reducibility of the dissimilarity measure emerged from theasymptotic limit of the BHC model, allowing us to use scalable algorithms suchas the nearest neighbor chain algorithm. Numerical experiments on bothsynthetic and real-world datasets demonstrate the validity and high performanceof our method.
arxiv-1501-07359 | Learning And-Or Models to Represent Context and Occlusion for Car Detection and Viewpoint Estimation |  http://arxiv.org/abs/1501.07359  | author:Tianfu Wu, Bo Li, Song-Chun Zhu category:cs.CV published:2015-01-29 summary:This paper presents a method for learning And-Or models to represent contextand occlusion for car detection and viewpoint estimation. The learned And-Ormodel represents car-to-car context and occlusion configurations at threelevels: (i) spatially-aligned cars, (ii) single car under different occlusionconfigurations, and (iii) a small number of parts. The And-Or model embeds agrammar for representing large structural and appearance variations in areconfigurable hierarchy. The learning process consists of two stages in aweakly supervised way (i.e., only bounding boxes of single cars are annotated).Firstly, the structure of the And-Or model is learned with three components:(a) mining multi-car contextual patterns based on layouts of annotated singlecar bounding boxes, (b) mining occlusion configurations between single cars,and (c) learning different combinations of part visibility based on car 3D CADsimulation. The And-Or model is organized in a directed and acyclic graph whichcan be inferred by Dynamic Programming. Secondly, the model parameters (forappearance, deformation and bias) are jointly trained using Weak-LabelStructural SVM. In experiments, we test our model on four car detectiondatasets --- the KITTI dataset \cite{Geiger12}, the PASCAL VOC2007 cardataset~\cite{pascal}, and two self-collected car datasets, namely theStreet-Parking car dataset and the Parking-Lot car dataset, and three datasetsfor car viewpoint estimation --- the PASCAL VOC2006 car dataset~\cite{pascal},the 3D car dataset~\cite{savarese}, and the PASCAL3D+ cardataset~\cite{xiang_wacv14}. Compared with state-of-the-art variants ofdeformable part-based models and other methods, our model achieves significantimprovement consistently on the four detection datasets, and comparableperformance on car viewpoint estimation.
arxiv-1501-07320 | Tensor Factorization via Matrix Factorization |  http://arxiv.org/abs/1501.07320  | author:Volodymyr Kuleshov, Arun Tejasvi Chaganty, Percy Liang category:cs.LG stat.ML published:2015-01-29 summary:Tensor factorization arises in many machine learning applications, suchknowledge base modeling and parameter estimation in latent variable models.However, numerical methods for tensor factorization have not reached the levelof maturity of matrix factorization methods. In this paper, we propose a newmethod for CP tensor factorization that uses random projections to reduce theproblem to simultaneous matrix diagonalization. Our method is conceptuallysimple and also applies to non-orthogonal and asymmetric tensors of arbitraryorder. We prove that a small number random projections essentially preservesthe spectral information in the tensor, allowing us to remove the dependence onthe eigengap that plagued earlier tensor-to-matrix reductions. Experimentally,our method outperforms existing tensor factorization methods on both simulateddata and two real datasets.
arxiv-1503-06680 | Structural Similarity Index SSIMplified: Is there really a simpler concept at the heart of image quality measurement? |  http://arxiv.org/abs/1503.06680  | author:Kieran Gerard Larkin category:cs.CV published:2015-01-29 summary:The Structural Similarity Index (SSIM) is generally considered to be amilestone in the recent history of Image Quality Assessment (IQA). Alas, SSIM'saccepted development from the product of three heuristic factors continues toobscure it's real underlying simplicity. Starting instead from asymmetric-antisymmetric reformulation we first show SSIM to be a contrast orvisibility function in the classic sense. Furthermore, the previously enigmaticstructural covariance is revealed to be the difference of variances. The secondstep, eliminating the intrinsic quadratic nature of SSIM, allows a near linearcorrelation with human observer scores, and without invoking the usual, butarbitrary, sigmoid model fitting. We conclude that SSIM can be re-interpretedin terms of perceptual masking: it is essentially equivalent to a normalisederror or noise visibility function (NVF), and, furthermore, the NVF aloneexplains it success in modelling perceptual image quality. We use the termDissimilarity Quotient (DQ) for the specifically anti/symmetric SSIM derivedNVF. It seems that IQA researchers may now have two choices: 1) Continue to usethe complex SSIM formula, but noting that SSIM only works coincidentally sincethe covariance term is actually the mean square error (MSE) in disguise. 2) Usethe simplest of all perceptually-masked image quality metrics, namely NVF orDQ. On this choice Occam is clear: in the absence of differences in predictiveability, the fewer assumptions that are made, the better.
arxiv-1501-07338 | On Vectorization of Deep Convolutional Neural Networks for Vision Tasks |  http://arxiv.org/abs/1501.07338  | author:Jimmy SJ. Ren, Li Xu category:cs.CV published:2015-01-29 summary:We recently have witnessed many ground-breaking results in machine learningand computer vision, generated by using deep convolutional neural networks(CNN). While the success mainly stems from the large volume of training dataand the deep network architectures, the vector processing hardware (e.g. GPU)undisputedly plays a vital role in modern CNN implementations to supportmassive computation. Though much attention was paid in the extent literature tounderstand the algorithmic side of deep CNN, little research was dedicated tothe vectorization for scaling up CNNs. In this paper, we studied thevectorization process of key building blocks in deep CNNs, in order to betterunderstand and facilitate parallel implementation. Key steps in training andtesting deep CNNs are abstracted as matrix and vector operators, upon whichparallelism can be easily achieved. We developed and compared siximplementations with various degrees of vectorization with which we illustratedthe impact of vectorization on the speed of model training and testing.Besides, a unified CNN framework for both high-level and low-level vision tasksis provided, along with a vectorized Matlab implementation withstate-of-the-art speed performance.
arxiv-1501-07340 | Sequential Probability Assignment with Binary Alphabets and Large Classes of Experts |  http://arxiv.org/abs/1501.07340  | author:Alexander Rakhlin, Karthik Sridharan category:cs.IT cs.LG math.IT stat.ML published:2015-01-29 summary:We analyze the problem of sequential probability assignment for binaryoutcomes with side information and logarithmic loss, where regret---or,redundancy---is measured with respect to a (possibly infinite) class ofexperts. We provide upper and lower bounds for minimax regret in terms ofsequential complexities of the class. These complexities were recently shown togive matching (up to logarithmic factors) upper and lower bounds for sequentialprediction with general convex Lipschitz loss functions (Rakhlin and Sridharan,2015). To deal with unbounded gradients of the logarithmic loss, we present anew analysis that employs a sequential chaining technique with a Bernstein-typebound. The introduced complexities are intrinsic to the problem of sequentialprobability assignment, as illustrated by our lower bound. We also consider an example of a large class of experts parametrized byvectors in a high-dimensional Euclidean ball (or a Hilbert ball). The typicaldiscretization approach fails, while our techniques give a non-trivial bound.For this problem we also present an algorithm based on regularization with aself-concordant barrier. This algorithm is of an independent interest, as itrequires a bound on the function values rather than gradients.
arxiv-1501-07440 | Limits on Support Recovery with Probabilistic Models: An Information-Theoretic Framework |  http://arxiv.org/abs/1501.07440  | author:Jonathan Scarlett, Volkan Cevher category:cs.IT math.IT math.ST stat.ML stat.TH published:2015-01-29 summary:The support recovery problem consists of determining a sparse subset of a setof variables that is relevant in generating a set of observations, and arisesin a diverse range of settings such as group testing, compressive sensing, andsubset selection in regression. In this paper, we take a unified approach tosupport recovery problems, considering general probabilistic observation modelsrelating a sparse data vector to an observation vector. We study theinformation-theoretic limits of both exact and partial support recovery, takinga novel approach motivated by thresholding techniques in channel coding. Weprovide general achievability and converse bounds characterizing the trade-offbetween the error probability and number of measurements, and we specializethese bounds to variants of models from group testing, linear regression, and1-bit compressive sensing. In several cases, our bounds not only providematching scaling laws in the necessary and sufficient number of measurements,but also sharp thresholds with matching constant factors. Our approach hasseveral advantages over previous approaches: For the achievability part, weobtain sharp thresholds under broader scalings of the sparsity level and otherparameters (e.g. signal-to-noise ratio) compared to several previous works, andfor the converse part, we not only provide conditions under which the errorprobability fails to vanish, but also conditions under which it tends to one.
arxiv-1501-07492 | Weakly Supervised Learning for Salient Object Detection |  http://arxiv.org/abs/1501.07492  | author:Huaizu Jiang category:cs.CV published:2015-01-29 summary:Recent advances in supervised salient object detection has resulted insignificant performance on benchmark datasets. Training such models, however,requires expensive pixel-wise annotations of salient objects. Moreover, manyexisting salient object detection models assume that at least one salientobject exists in the input image. Such an assumption often leads to lessappealing saliency maps on the background images, which contain no salientobject at all. To avoid the requirement of expensive pixel-wise salient regionannotations, in this paper, we study weakly supervised learning approaches forsalient object detection. Given a set of background images and salient objectimages, we propose a solution toward jointly addressing the salient objectexistence and detection tasks. We adopt the latent SVM framework and formulatethe two problems together in a single integrated objective function: saliencylabels of superpixels are modeled as hidden variables and involved in aclassification term conditioned to the salient object existence variable, whichin turn depends on both global image and regional saliency features andsaliency label assignment. Experimental results on benchmark datasets validatethe effectiveness of our proposed approach.
arxiv-1501-07399 | Particle swarm optimization for time series motif discovery |  http://arxiv.org/abs/1501.07399  | author:Joan Serrà, Josep Lluis Arcos category:cs.LG cs.NE published:2015-01-29 summary:Efficiently finding similar segments or motifs in time series data is afundamental task that, due to the ubiquity of these data, is present in a widerange of domains and situations. Because of this, countless solutions have beendevised but, to date, none of them seems to be fully satisfactory and flexible.In this article, we propose an innovative standpoint and present a solutioncoming from it: an anytime multimodal optimization algorithm for time seriesmotif discovery based on particle swarms. By considering data from a variety ofdomains, we show that this solution is extremely competitive when compared tothe state-of-the-art, obtaining comparable motifs in considerably less timeusing minimal memory. In addition, we show that it is robust to differentimplementation choices and see that it offers an unprecedented degree offlexibility with regard to the task. All these qualities make the presentedsolution stand out as one of the most prominent candidates for motif discoveryin long time series streams. Besides, we believe the proposed standpoint can beexploited in further time series analysis and mining tasks, widening the scopeof research and potentially yielding novel effective solutions.
arxiv-1501-07315 | Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation |  http://arxiv.org/abs/1501.07315  | author:Konstantinos Slavakis, Georgios B. Giannakis category:cs.LG published:2015-01-29 summary:Applications involving dictionary learning, non-negative matrixfactorization, subspace clustering, and parallel factor tensor decompositiontasks motivate well algorithms for per-block-convex and non-smooth optimizationproblems. By leveraging the stochastic approximation paradigm and first-orderacceleration schemes, this paper develops an online and modular learningalgorithm for a large class of non-convex data models, where convexity ismanifested only per-block of variables whenever the rest of them are heldfixed. The advocated algorithm incurs computational complexity that scaleslinearly with the number of unknowns. Under minimal assumptions on the costfunctions of the composite optimization task, without bounding constraints onthe optimization variables, or any explicit information on bounds of Lipschitzcoefficients, the expected cost evaluated online at the resultant iterates isprovably convergent with quadratic rate to an accumulation point of the(per-block) minima, while subgradients of the expected cost asymptoticallyvanish in the mean-squared sense. The merits of the general approach aredemonstrated in two online learning setups: (i) Robust linear regression usinga sparsity-cognizant total least-squares criterion; and (ii) semi-superviseddictionary learning for network-wide link load tracking and imputation withmissing entries. Numerical tests on synthetic and real data highlight thepotential of the proposed framework for streaming data analytics bydemonstrating superior performance over block coordinate descent, and reducedcomplexity relative to the popular alternating-direction method of multipliers.
arxiv-1501-07422 | Pairwise Rotation Hashing for High-dimensional Features |  http://arxiv.org/abs/1501.07422  | author:Kohta Ishikawa, Ikuro Sato, Mitsuru Ambai category:cs.CV stat.ML published:2015-01-29 summary:Binary Hashing is widely used for effective approximate nearest neighborssearch. Even though various binary hashing methods have been proposed, very fewmethods are feasible for extremely high-dimensional features often used invisual tasks today. We propose a novel highly sparse linear hashing methodbased on pairwise rotations. The encoding cost of the proposed algorithm is$\mathrm{O}(n \log n)$ for n-dimensional features, whereas that of the existingstate-of-the-art method is typically $\mathrm{O}(n^2)$. The proposed method isalso remarkably faster in the learning phase. Along with the efficiency, theretrieval accuracy is comparable to or slightly outperforming thestate-of-the-art. Pairwise rotations used in our method are formulated from ananalytical study of the trade-off relationship between quantization error andentropy of binary codes. Although these hashing criteria are widely used inprevious researches, its analytical behavior is rarely studied. All buildingblocks of our algorithm are based on the analytical solution, and it thusprovides a fairly simple and efficient procedure.
arxiv-1501-07627 | Representing Objects, Relations, and Sequences |  http://arxiv.org/abs/1501.07627  | author:Stephen I. Gallant, T. Wendy Okaywe category:cs.LG published:2015-01-29 summary:Vector Symbolic Architectures (VSAs) are high-dimensional vectorrepresentations of objects (eg., words, image parts), relations (eg., sentencestructures), and sequences for use with machine learning algorithms. Theyconsist of a vector addition operator for representing a collection ofunordered objects, a Binding operator for associating groups of objects, and amethodology for encoding complex structures. We first develop Constraints that machine learning imposes upon VSAs: forexample, similar structures must be represented by similar vectors. Theconstraints suggest that current VSAs should represent phrases ("The smartBrazilian girl") by binding sums of terms, in addition to simply binding theterms directly. We show that matrix multiplication can be used as the binding operator for aVSA, and that matrix elements can be chosen at random. A consequence for livingsystems is that binding is mathematically possible without the need to specify,in advance, precise neuron-to-neuron connection properties for large numbers ofsynapses. A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms),is described that satisfies all Constraints. With respect to machine learning, for some types of problems appropriate VSArepresentations permit us to prove learnability, rather than relying onsimulations. We also propose dividing machine (and neural) learning andrepresentation into three Stages, with differing roles for learning in eachstage. For neural modeling, we give "representational reasons" for nervous systemsto have many recurrent connections, as well as for the importance of phrases inlanguage processing. Sizing simulations and analyses suggest that VSAs in general, and MBAT inparticular, are ready for real-world applications.
arxiv-1501-07584 | Efficient Divide-And-Conquer Classification Based on Feature-Space Decomposition |  http://arxiv.org/abs/1501.07584  | author:Qi Guo, Bo-Wei Chen, Feng Jiang, Xiangyang Ji, Sun-Yuan Kung category:cs.LG published:2015-01-29 summary:This study presents a divide-and-conquer (DC) approach based on feature spacedecomposition for classification. When large-scale datasets are present,typical approaches usually employed truncated kernel methods on the featurespace or DC approaches on the sample space. However, this did not guaranteeseparability between classes, owing to overfitting. To overcome such problems,this work proposes a novel DC approach on feature spaces consisting of threesteps. Firstly, we divide the feature space into several subspaces using thedecomposition method proposed in this paper. Subsequently, these featuresubspaces are sent into individual local classifiers for training. Finally, theoutcomes of local classifiers are fused together to generate the finalclassification results. Experiments on large-scale datasets are carried out forperformance evaluation. The results show that the error rates of the proposedDC method decreased comparing with the state-of-the-art fast SVM solvers, e.g.,reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasetsrespectively.
arxiv-1501-07518 | High-Dimensional Longitudinal Classification with the Multinomial Fused Lasso |  http://arxiv.org/abs/1501.07518  | author:Samrachana Adhikari, Fabrizio Lecci, James T. Becker, Brian W. Junker, Lewis H. Kuller, Oscar L. Lopez, Ryan J. Tibshirani category:stat.AP stat.ML published:2015-01-29 summary:We study regularized estimation in high-dimensional longitudinalclassification problems, using the lasso and fused lasso regularizers. Theconstructed coefficient estimates are piecewise constant across the timedimension in the longitudinal problem, with adaptively selected change points(break points). We present an efficient algorithm for computing such estimates,based on proximal gradient descent. We apply our proposed technique to alongitudinal data set on Alzheimer's disease from the Cardiovascular HealthStudy Cognition Study, and use this data set to motivate and demonstrateseveral practical considerations such as the selection of tuning parameters,and the assessment of model stability.
arxiv-1501-07496 | Implementation of an Automatic Syllabic Division Algorithm from Speech Files in Portuguese Language |  http://arxiv.org/abs/1501.07496  | author:E. L. F. Da Silva, H. M. de Oliveira category:cs.SD cs.CL cs.DS published:2015-01-29 summary:A new algorithm for voice automatic syllabic splitting in the Portugueselanguage is proposed, which is based on the envelope of the speech signal ofthe input audio file. A computational implementation in MatlabTM is presentedand made available at the URLhttp://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to itsstraightforwardness, the proposed method is very attractive for embeddedsystems (e.g. i-phones). It can also be used as a screen to assist moresophisticated methods. Voice excerpts containing more than one syllable andidentified by the same envelope are named as super-syllables and they aresubsequently separated. The results indicate which samples corresponds to thebeginning and end of each detected syllable. Preliminary tests were performedto fifty words at an identification rate circa 70% (further improvements may beincorporated to treat particular phonemes). This algorithm is also useful invoice command systems, as a tool in the teaching of Portuguese language or evenfor patients with speech pathology.
arxiv-1502-00555 | A Discrete Tchebichef Transform Approximation for Image and Video Coding |  http://arxiv.org/abs/1502.00555  | author:P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Madanayake category:stat.ME cs.CV cs.MM cs.NA stat.CO published:2015-01-28 summary:In this paper, we introduce a low-complexity approximation for the discreteTchebichef transform (DTT). The proposed forward and inverse transforms aremultiplication-free and require a reduced number of additions and bit-shiftingoperations. Numerical compression simulations demonstrate the efficiency of theproposed transform for image and video coding. Furthermore, Xilinx Virtex-6FPGA based hardware realization shows 44.9% reduction in dynamic powerconsumption and 64.7% lower area when compared to the literature.
arxiv-1501-07227 | A Neural Network Anomaly Detector Using the Random Cluster Model |  http://arxiv.org/abs/1501.07227  | author:Robert A. Murphy category:cs.LG cs.NE stat.ML 60D05 published:2015-01-28 summary:The random cluster model is used to define an upper bound on a distancemeasure as a function of the number of data points to be classified and theexpected value of the number of classes to form in a hybrid K-means andregression classification methodology, with the intent of detecting anomalies.Conditions are given for the identification of classes which contain anomaliesand individual anomalies within identified classes. A neural network modeldescribes the decision region-separating surface for offline storage and recallin any new anomaly detection.
arxiv-1501-07242 | Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions |  http://arxiv.org/abs/1501.07242  | author:Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin category:cs.NA cs.LG math.OC published:2015-01-28 summary:We consider the problem of optimizing an approximately convex function over abounded convex set in $\mathbb{R}^n$ using only function evaluations. Theproblem is reduced to sampling from an \emph{approximately} log-concavedistribution using the Hit-and-Run method, which is shown to have the same$\mathcal{O}^*$ complexity as sampling from log-concave distributions. Inaddition to extend the analysis for log-concave distributions to approximatelog-concave distributions, the implementation of the 1-dimensional sampler ofthe Hit-and-Run walk requires new methods and analysis. The algorithm then isbased on simulated annealing which does not relies on first order conditionswhich makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context ofzeroth order stochastic convex optimization, the proposed method produces an$\epsilon$-minimizer after $\mathcal{O}^*(n^{7.5}\epsilon^{-2})$ noisy functionevaluations by inducing a $\mathcal{O}(\epsilon/n)$-approximately log concavedistribution. We also consider in detail the case when the "amount ofnon-convexity" decays towards the optimum of the function. Other applicationsof the method discussed in this work include private computation of empiricalrisk minimizers, two-stage stochastic programming, and approximate dynamicprogramming for online learning.
arxiv-1501-07093 | Novel Approaches for Predicting Risk Factors of Atherosclerosis |  http://arxiv.org/abs/1501.07093  | author:V. Sree Hari Rao, M. Naresh Kumar category:cs.LG published:2015-01-28 summary:Coronary heart disease (CHD) caused by hardening of artery walls due tocholesterol known as atherosclerosis is responsible for large number of deathsworld-wide. The disease progression is slow, asymptomatic and may lead tosudden cardiac arrest, stroke or myocardial infraction. Presently, imagingtechniques are being employed to understand the molecular and metabolicactivity of atherosclerotic plaques to estimate the risk. Though imagingmethods are able to provide some information on plaque metabolism they lack therequired resolution and sensitivity for detection. In this paper we considerthe clinical observations and habits of individuals for predicting the riskfactors of CHD. The identification of risk factors helps in stratifyingpatients for further intensive tests such as nuclear imaging or coronaryangiography. We present a novel approach for predicting the risk factors ofatherosclerosis with an in-built imputation algorithm and particle swarmoptimization (PSO). We compare the performance of our methodology with othermachine learning techniques on STULONG dataset which is based on longitudinalstudy of middle aged individuals lasting for twenty years. Our methodologypowered by PSO search has identified physical inactivity as one of the riskfactor for the onset of atherosclerosis in addition to other already knownfactors. The decision rules extracted by our methodology are able to predictthe risk factors with an accuracy of $99.73%$ which is higher than theaccuracies obtained by application of the state-of-the-art machine learningtechniques presently being employed in the identification of atherosclerosisrisk studies.
arxiv-1501-07180 | End-to-End Photo-Sketch Generation via Fully Convolutional Representation Learning |  http://arxiv.org/abs/1501.07180  | author:Liliang Zhang, Liang Lin, Xian Wu, Shengyong Ding, Lei Zhang category:cs.CV I.2.10 published:2015-01-28 summary:Sketch-based face recognition is an interesting task in vision and multimediaresearch, yet it is quite challenging due to the great difference between facephotos and sketches. In this paper, we propose a novel approach forphoto-sketch generation, aiming to automatically transform face photos intodetail-preserving personal sketches. Unlike the traditional models synthesizingsketches based on a dictionary of exemplars, we develop a fully convolutionalnetwork to learn the end-to-end photo-sketch mapping. Our approach takes wholeface photos as inputs and directly generates the corresponding sketch imageswith efficient inference and learning, in which the architecture are stacked byonly convolutional kernels of very small sizes. To well capture the personidentity during the photo-sketch transformation, we define our optimizationobjective in the form of joint generative-discriminative minimization. Inparticular, a discriminative regularization term is incorporated into thephoto-sketch generation, enhancing the discriminability of the generated personsketches against other individuals. Extensive experiments on several standardbenchmarks suggest that our approach outperforms other state-of-the-art methodsin both photo-sketch generation and face sketch verification.
arxiv-1501-07005 | Survey:Natural Language Parsing For Indian Languages |  http://arxiv.org/abs/1501.07005  | author:Monika T. Makwana, Deepak C. Vegda category:cs.CL published:2015-01-28 summary:Syntactic parsing is a necessary task which is required for NLP applicationsincluding machine translation. It is a challenging task to develop aqualitative parser for morphological rich and agglutinative languages.Syntactic analysis is used to understand the grammatical structure of a naturallanguage sentence. It outputs all the grammatical information of each word andits constituent. Also issues related to it help us to understand the languagein a more detailed way. This literature survey is groundwork to understand thedifferent parser development for Indian languages and various approaches thatare used to develop such tools and techniques. This paper provides a survey ofresearch papers from well known journals and conferences.
arxiv-1501-06993 | Feature Sampling Strategies for Action Recognition |  http://arxiv.org/abs/1501.06993  | author:Youjie Zhou, Hongkai Yu, Song Wang category:cs.CV published:2015-01-28 summary:Although dense local spatial-temporal features with bag-of-featuresrepresentation achieve state-of-the-art performance for action recognition, thehuge feature number and feature size prevent current methods from scaling up toreal size problems. In this work, we investigate different types of featuresampling strategies for action recognition, namely dense sampling, uniformlyrandom sampling and selective sampling. We propose two effective selectivesampling methods using object proposal techniques. Experiments conducted on alarge video dataset show that we are able to achieve better average recognitionaccuracy using 25% less features, through one of proposed selective samplingmethods, and even remain comparable accuracy while discarding 70% features.
arxiv-1501-07304 | The Beauty of Capturing Faces: Rating the Quality of Digital Portraits |  http://arxiv.org/abs/1501.07304  | author:Miriam Redi, Nikhil Rasiwasia, Gaurav Aggarwal, Alejandro Jaimes category:cs.CV cs.CY cs.MM published:2015-01-28 summary:Digital portrait photographs are everywhere, and while the number of facepictures keeps growing, not much work has been done to on automatic portraitbeauty assessment. In this paper, we design a specific framework toautomatically evaluate the beauty of digital portraits. To this end, we procurea large dataset of face images annotated not only with aesthetic scores butalso with information about the traits of the subject portrayed. We design aset of visual features based on portrait photography literature, andextensively analyze their relation with portrait beauty, exposing interestingfindings about what makes a portrait beautiful. We find that the beauty of aportrait is linked to its artistic value, and independent from age, race andgender of the subject. We also show that a classifier trained with our featuresto separate beautiful portraits from non-beautiful portraits outperformsgeneric aesthetic classifiers.
arxiv-1501-06721 | Massively-concurrent Agent-based Evolutionary Computing |  http://arxiv.org/abs/1501.06721  | author:D. Krzywicki, W. Turek, A. Byrski, M. Kisiel-Dorohinicki category:cs.MA cs.NE published:2015-01-27 summary:The fusion of the multi-agent paradigm with evolutionary computation yieldedpromising results in many optimization problems. Evolutionary multi-agentsystem (EMAS) are more similar to biological evolution than classicalevolutionary algorithms. However, technological limitations prevented the useof fully asynchronous agents in previous EMAS implementations. In this paper wepresent a new algorithm for agent-based evolutionary computations. Theindividuals are represented as fully autonomous and asynchronous agents. Anefficient implementation of this algorithm was possible through the use ofmodern technologies based on functional languages (namely Erlang and Scala),which natively support lightweight processes and asynchronous communication.Our experiments show that such an asynchronous approach is both faster and moreefficient in solving common optimization problems.
arxiv-1501-06727 | Factorization, Inference and Parameter Learning in Discrete AMP Chain Graphs |  http://arxiv.org/abs/1501.06727  | author:Jose M. Peña category:stat.ML cs.AI published:2015-01-27 summary:We address some computational issues that may hinder the use of AMP chaingraphs in practice. Specifically, we show how a discrete probabilitydistribution that satisfies all the independencies represented by an AMP chaingraph factorizes according to it. We show how this factorization makes itpossible to perform inference and parameter learning efficiently, by adaptingexisting algorithms for Markov and Bayesian networks. Finally, we turn ourattention to another issue that may hinder the use of AMP CGs, namely the lackof an intuitive interpretation of their edges. We provide one suchinterpretation.
arxiv-1501-06769 | Particle Gibbs with Ancestor Sampling for Probabilistic Programs |  http://arxiv.org/abs/1501.06769  | author:Jan-Willem van de Meent, Hongseok Yang, Vikash Mansinghka, Frank Wood category:stat.ML cs.AI cs.PL published:2015-01-27 summary:Particle Markov chain Monte Carlo techniques rank among currentstate-of-the-art methods for probabilistic program inference. A drawback ofthese techniques is that they rely on importance resampling, which results indegenerate particle trajectories and a low effective sample size for variablessampled early in a program. We here develop a formalism to adapt ancestorresampling, a technique that mitigates particle degeneracy, to theprobabilistic programming setting. We present empirical results thatdemonstrate nontrivial performance gains.
arxiv-1501-06633 | maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs |  http://arxiv.org/abs/1501.06633  | author:Andrew Lavin category:cs.NE cs.DC cs.LG published:2015-01-27 summary:This paper describes maxDNN, a computationally efficient convolution kernelfor deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%computational efficiency on typical deep learning network architectures. Thedesign combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. Weonly address forward propagation (FPROP) operation of the network, but webelieve that the same techniques used here will be effective for backwardpropagation (BPROP) as well.
arxiv-1501-06929 | A Probabilistic Least-Mean-Squares Filter |  http://arxiv.org/abs/1501.06929  | author:Jesus Fernandez-Bes, Víctor Elvira, Steven Van Vaerenbergh category:stat.ML cs.SY stat.AP published:2015-01-27 summary:We introduce a probabilistic approach to the LMS filter. By means of anefficient approximation, this approach provides an adaptable step-size LMSalgorithm together with a measure of uncertainty about the estimation. Inaddition, the proposed approximation preserves the linear complexity of thestandard LMS. Numerical results show the improved performance of the algorithmwith respect to standard LMS and state-of-the-art algorithms with similarcomplexity. The goal of this work, therefore, is to open the door to bring somemore Bayesian machine learning techniques to adaptive filtering.
arxiv-1501-06794 | Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations |  http://arxiv.org/abs/1501.06794  | author:Bernhard Schölkopf, Krikamol Muandet, Kenji Fukumizu, Jonas Peters category:stat.ML cs.DS cs.LG published:2015-01-27 summary:We describe a method to perform functional operations on probabilitydistributions of random variables. The method uses reproducing kernel Hilbertspace representations of probability distributions, and it is applicable to alloperations which can be applied to points drawn from the respectivedistributions. We refer to our approach as {\em kernel probabilisticprogramming}. We illustrate it on synthetic data, and show how it can be usedfor nonparametric structural equation models, with an application to causalinference.
arxiv-1501-06751 | A Cheap System for Vehicle Speed Detection |  http://arxiv.org/abs/1501.06751  | author:Chaim Ginzburg, Amit Raphael, Daphna Weinshall category:cs.CV published:2015-01-27 summary:The reliable detection of speed of moving vehicles is considered key totraffic law enforcement in most countries, and is seen by many as an importanttool to reduce the number of traffic accidents and fatalities. Many automaticsystems and different methods are employed in different countries, but as arule they tend to be expensive and/or labor intensive, often employing outdatedtechnology due to the long development time. Here we describe a speed detectionsystem that relies on simple everyday equipment - a laptop and a consumer webcamera. Our method is based on tracking the license plates of cars, which givesthe relative movement of the cars in the image. This image displacement istranslated to actual motion by using the method of projection to a referenceplane, where the reference plane is the road itself. However, since licenseplates do not touch the road, we must compensate for the entailed distortion inspeed measurement. We show how to compute the compensation factor usingknowledge of the license plate standard dimensions. Consequently our systemcomputes the true speed of moving vehicles fast and accurately. We showpromising results on videos obtained in a number of scenes and with differentcar models.
arxiv-1501-06722 | Parametric Image Segmentation of Humans with Structural Shape Priors |  http://arxiv.org/abs/1501.06722  | author:Alin-Ionut Popa, Cristian Sminchisescu category:cs.CV published:2015-01-27 summary:The figure-ground segmentation of humans in images captured in naturalenvironments is an outstanding open problem due to the presence of complexbackgrounds, articulation, varying body proportions, partial views andviewpoint changes. In this work we propose class-specific segmentation modelsthat leverage parametric max-flow image segmentation and a large dataset ofhuman shapes. Our contributions are as follows: (1) formulation of asub-modular energy model that combines class-specific structural constraintsand data-driven shape priors, within a parametric max-flow optimizationmethodology that systematically computes all breakpoints of the model inpolynomial time; (2) design of a data-driven class-specific fusion methodology,based on matching against a large training set of exemplar human shapes(100,000 in our experiments), that allows the shape prior to be constructedon-the-fly, for arbitrary viewpoints and partial views. (3) demonstration ofstate of the art results, in two challenging datasets, H3D and MPII (wherefigure-ground segmentation annotations have been added by us), where wesubstantially improve on the first ranked hypothesis estimates of mid-levelsegmentation methods, by 20%, with hypothesis set sizes that are up to oneorder of magnitude smaller.
arxiv-1501-06716 | A General Preprocessing Method for Improved Performance of Epipolar Geometry Estimation Algorithms |  http://arxiv.org/abs/1501.06716  | author:Maria Kushnir, Ilan Shimshoni category:cs.CV published:2015-01-27 summary:In this paper a deterministic preprocessing algorithm is presented, whoseoutput can be given as input to most state-of-the-art epipolar geometryestimation algorithms, improving their results considerably. They are now ableto succeed on hard cases for which they failed before. The algorithm consistsof three steps, whose scope changes from local to global. In the local step itextracts from a pair of images local features (e.g. SIFT). Similar featuresfrom each image are clustered and the clusters are matched yielding a largenumber of putative matches. In the second step pairs of spatially closefeatures (called 2keypoints) are matched and ranked by a classifier. The2keypoint matches with the highest ranks are selected. In the global step, fromeach two 2keypoint matches a fundamental matrix is computed. As quite a few ofthe matrices are generated from correct matches they are used to rank theputative matches found in the first step. For each match the number offundamental matrices, for which it approximately satisfies the epipolarconstraint, is calculated. This set of matches is combined with the putativematches generated by standard methods and their probabilities to be correct areestimated by a classifier. These are then given as input to state-of-the-artepipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yieldingmuch better results than the original algorithms. This was shown in extensivetesting performed on almost 900 image pairs from six publicly availabledata-sets.
arxiv-1501-06598 | Online Nonparametric Regression with General Loss Functions |  http://arxiv.org/abs/1501.06598  | author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.IT cs.LG math.IT published:2015-01-26 summary:This paper establishes minimax rates for online regression with arbitraryclasses of functions and general losses. We show that below a certain thresholdfor the complexity of the function class, the minimax rates depend on both thecurvature of the loss function and the sequential complexities of the class.Above this threshold, the curvature of the loss does not affect the rates.Furthermore, for the case of square loss, our results point to the interestingphenomenon: whenever sequential and i.i.d. empirical entropies match, the ratesfor statistical and online learning are the same. In addition to the study of minimax regret, we derive a generic forecasterthat enjoys the established optimal rates. We also provide a recipe fordesigning online prediction algorithms that can be computationally efficientfor certain problems. We illustrate the techniques by deriving existing and newforecasters for the case of finite experts and for online linear regression.
arxiv-1501-06241 | Sequential Sensing with Model Mismatch |  http://arxiv.org/abs/1501.06241  | author:Ruiyang Song, Yao Xie, Sebastian Pokutta category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2015-01-26 summary:We characterize the performance of sequential information guided sensing,Info-Greedy Sensing, when there is a mismatch between the true signal model andthe assumed model, which may be a sample estimate. In particular, we consider asetup where the signal is low-rank Gaussian and the measurements are taken inthe directions of eigenvectors of the covariance matrix in a decreasing orderof eigenvalues. We establish a set of performance bounds when a mismatchedcovariance matrix is used, in terms of the gap of signal posterior entropy, aswell as the additional amount of power required to achieve the same signalrecovery precision. Based on this, we further study how to choose aninitialization for Info-Greedy Sensing using the sample covariance matrix, orusing an efficient covariance sketching scheme.
arxiv-1501-06450 | IT-map: an Effective Nonlinear Dimensionality Reduction Method for Interactive Clustering |  http://arxiv.org/abs/1501.06450  | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG published:2015-01-26 summary:Scientists in many fields have the common and basic need of dimensionalityreduction: visualizing the underlying structure of the massive multivariatedata in a low-dimensional space. However, many dimensionality reduction methodsconfront the so-called "crowding problem" that clusters tend to overlap witheach other in the embedding. Previously, researchers expect to avoid thatproblem and seek to make clusters maximally separated in the embedding.However, the proposed in-tree (IT) based method, called IT-map, allows clustersin the embedding to be locally overlapped, while seeking to make themdistinguishable by some small yet key parts. IT-map provides a simple,effective and novel solution to cluster-preserving mapping, which makes itpossible to cluster the original data points interactively and thus should beof general meaning in science and engineering.
arxiv-1501-06521 | Noisy Tensor Completion via the Sum-of-Squares Hierarchy |  http://arxiv.org/abs/1501.06521  | author:Boaz Barak, Ankur Moitra category:cs.LG cs.DS stat.ML published:2015-01-26 summary:In the noisy tensor completion problem we observe $m$ entries (whose locationis chosen uniformly at random) from an unknown $n_1 \times n_2 \times n_3$tensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goalis to fill in its missing entries using as few observations as possible. Let $n= \max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is apolynomial time algorithm based on the sixth level of the sum-of-squareshierarchy for completing it. Our estimate agrees with almost all of $T$'sentries almost exactly and works even when our observations are corrupted bynoise. This is also the first algorithm for tensor completion that works in theovercomplete case when $r > n$, and in fact it works all the way up to $r =n^{3/2-\epsilon}$. Our proofs are short and simple and are based on establishing a newconnection between noisy tensor completion (through the language of Rademachercomplexity) and the task of refuting random constant satisfaction problems.This connection seems to have gone unnoticed even in the context of matrixcompletion. Furthermore, we use this connection to show matching lower bounds.Our main technical result is in characterizing the Rademacher complexity of thesequence of norms that arise in the sum-of-squares relaxations to the tensornuclear norm. These results point to an interesting new direction: Can weexplore computational vs. sample complexity tradeoffs through thesum-of-squares hierarchy?
arxiv-1501-06243 | Poisson Matrix Completion |  http://arxiv.org/abs/1501.06243  | author:Yang Cao, Yao Xie category:stat.ML cs.LG published:2015-01-26 summary:We extend the theory of matrix completion to the case where we make Poissonobservations for a subset of entries of a low-rank matrix. We consider the(now) usual matrix recovery formulation through maximum likelihood with properconstraints on the matrix $M$, and establish theoretical upper and lower boundson the recovery error. Our bounds are nearly optimal up to a factor on theorder of $\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by adaptingthe arguments used for one-bit matrix completion \cite{davenport20121}(although these two problems are different in nature) and the adaptationrequires new techniques exploiting properties of the Poisson likelihoodfunction and tackling the difficulties posed by the locally sub-Gaussiancharacteristic of the Poisson distribution. Our results highlight a fewimportant distinctions of Poisson matrix completion compared to the prior workin matrix completion including having to impose a minimum signal-to-noiserequirement on each observed entry. We also develop an efficient iterativealgorithm and demonstrate its good performance in recovering solar flareimages.
arxiv-1501-06237 | Deep Transductive Semi-supervised Maximum Margin Clustering |  http://arxiv.org/abs/1501.06237  | author:Gang Chen category:cs.LG 68T10 I.2.6 published:2015-01-26 summary:Semi-supervised clustering is an very important topic in machine learning andcomputer vision. The key challenge of this problem is how to learn a metric,such that the instances sharing the same label are more likely close to eachother on the embedded space. However, little attention has been paid to learnbetter representations when the data lie on non-linear manifold. Fortunately,deep learning has led to great success on feature learning recently. Inspiredby the advances of deep learning, we propose a deep transductivesemi-supervised maximum margin clustering approach. More specifically, givenpairwise constraints, we exploit both labeled and unlabeled data to learn anon-linear mapping under maximum margin framework for clustering analysis.Thus, our model unifies transductive learning, feature learning and maximummargin techniques in the semi-supervised clustering framework. We pretrain thedeep network structure with restricted Boltzmann machines (RBMs) layer by layergreedily, and optimize our objective function with gradient descent. Bychecking the most violated constraints, our approach updates the modelparameters through error backpropagation, in which deep features are learnedautomatically. The experimental results shows that our model is significantlybetter than the state of the art on semi-supervised clustering.
arxiv-1501-06478 | Compressed Support Vector Machines |  http://arxiv.org/abs/1501.06478  | author:Zhixiang Xu, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger category:cs.LG published:2015-01-26 summary:Support vector machines (SVM) can classify data sets along highly non-lineardecision boundaries because of the kernel-trick. This expressiveness comes at aprice: During test-time, the SVM classifier needs to compute the kernelinner-product between a test sample and all support vectors. With largetraining data sets, the time required for this computation can be substantial.In this paper, we introduce a post-processing algorithm, which compresses thelearned SVM model by reducing and optimizing support vectors. We evaluate ouralgorithm on several medium-scaled real-world data sets, demonstrating that itmaintains high test accuracy while reducing the test-time evaluation cost byseveral orders of magnitude---in some cases from hours to seconds. It is fairto say that most of the work in this paper was previously been invented byBurges and Sch\"olkopf almost 20 years ago. For most of the time during whichwe conducted this research, we were unaware of this prior work. However, in thepast two decades, computing power has increased drastically, and we cantherefore provide empirical insights that were not possible in their originalpaper.
arxiv-1501-06284 | On a Family of Decomposable Kernels on Sequences |  http://arxiv.org/abs/1501.06284  | author:Andrea Baisero, Florian T. Pokorny, Carl Henrik Ek category:cs.LG published:2015-01-26 summary:In many applications data is naturally presented in terms of orderings ofsome basic elements or symbols. Reasoning about such data requires a notion ofsimilarity capable of handling sequences of different lengths. In this paper wedescribe a family of Mercer kernel functions for such sequentially structureddata. The family is characterized by a decomposable structure in terms ofsymbol-level and structure-level similarities, representing a specificcombination of kernels which allows for efficient computation. We provide anexperimental evaluation on sequential classification tasks comparing kernelsfrom our family of kernels to a state of the art sequence kernel called theGlobal Alignment kernel which has been shown to outperform Dynamic Time Warping
arxiv-1501-06297 | Geodesic convolutional neural networks on Riemannian manifolds |  http://arxiv.org/abs/1501.06297  | author:Jonathan Masci, Davide Boscaini, Michael M. Bronstein, Pierre Vandergheynst category:cs.CV published:2015-01-26 summary:Feature descriptors play a crucial role in a wide range of geometry analysisand processing applications, including shape correspondence, retrieval, andsegmentation. In this paper, we introduce Geodesic Convolutional NeuralNetworks (GCNN), a generalization of the convolutional networks (CNN) paradigmto non-Euclidean manifolds. Our construction is based on a local geodesicsystem of polar coordinates to extract "patches", which are then passed througha cascade of filters and linear and non-linear operators. The coefficients ofthe filters and linear combination weights are optimization variables that arelearned to minimize a task-specific cost function. We use GCNN to learninvariant shape features, allowing to achieve state-of-the-art performance inproblems such as shape description, retrieval, and correspondence.
arxiv-1501-06262 | 3D Human Activity Recognition with Reconfigurable Convolutional Neural Networks |  http://arxiv.org/abs/1501.06262  | author:Keze Wang, Xiaolong Wang, Liang Lin, Meng Wang, Wangmeng Zuo category:cs.CV 68U01 I.4 published:2015-01-26 summary:Human activity understanding with 3D/depth sensors has received increasingattention in multimedia processing and interactions. This work targets ondeveloping a novel deep model for automatic activity recognition from RGB-Dvideos. We represent each human activity as an ensemble of cubic-like videosegments, and learn to discover the temporal structures for a category ofactivities, i.e. how the activities to be decomposed in terms ofclassification. Our model can be regarded as a structured deep architecture, asit extends the convolutional neural networks (CNNs) by incorporating structurealternatives. Specifically, we build the network consisting of 3D convolutionsand max-pooling operators over the video segments, and introduce the latentvariables in each convolutional layer manipulating the activation of neurons.Our model thus advances existing approaches in two aspects: (i) it actsdirectly on the raw inputs (grayscale-depth data) to conduct recognitioninstead of relying on hand-crafted features, and (ii) the model structure canbe dynamically adjusted accounting for the temporal variations of humanactivities, i.e. the network configuration is allowed to be partially activatedduring inference. For model training, we propose an EM-type optimization methodthat iteratively (i) discovers the latent structure by determining thedecomposed actions for each training example, and (ii) learns the networkparameters by using the back-propagation algorithm. Our approach is validatedin challenging scenarios, and outperforms state-of-the-art methods. A largehuman activity database of RGB-D videos is presented in addition.
arxiv-1501-06272 | Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval |  http://arxiv.org/abs/1501.06272  | author:Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan category:cs.CV cs.LG published:2015-01-26 summary:With the rapid growth of web images, hashing has received increasinginterests in large scale image retrieval. Research efforts have been devoted tolearning compact binary codes that preserve semantic similarity based onlabels. However, most of these hashing methods are designed to handle simplebinary similarity. The complex multilevel semantic structure of imagesassociated with multiple labels have not yet been well explored. Here wepropose a deep semantic ranking based method for learning hash functions thatpreserve multilevel semantic similarity between multi-label images. In ourapproach, deep convolutional neural network is incorporated into hash functionsto jointly learn feature representations and mappings from them to hash codes,which avoids the limitation of semantic representation power of hand-craftedfeatures. Meanwhile, a ranking list that encodes the multilevel similarityinformation is employed to guide the learning of such deep hash functions. Aneffective scheme based on surrogate loss is used to solve the intractableoptimization problem of nonsmooth and multivariate ranking measures involved inthe learning procedure. Experimental results show the superiority of ourproposed approach over several state-of-the-art hashing methods in term ofranking evaluation metrics when tested on multi-label image datasets.
arxiv-1501-06587 | Measuring academic influence: Not all citations are equal |  http://arxiv.org/abs/1501.06587  | author:Xiaodan Zhu, Peter Turney, Daniel Lemire, André Vellino category:cs.DL cs.CL cs.LG published:2015-01-26 summary:The importance of a research article is routinely measured by counting howmany times it has been cited. However, treating all citations with equal weightignores the wide variety of functions that citations perform. We want toautomatically identify the subset of references in a bibliography that have acentral academic influence on the citing paper. For this purpose, we examinethe effectiveness of a variety of features for determining the academicinfluence of a citation. By asking authors to identify the key references intheir own work, we created a data set in which citations were labeled accordingto their academic influence. Using automatic feature selection with supervisedmachine learning, we found a model for predicting academic influence thatachieves good performance on this data set using only four features. The bestfeatures, among those we evaluated, were those based on the number of times areference is mentioned in the body of a citing paper. The performance of thesefeatures inspired us to design an influence-primed h-index (the hip-index).Unlike the conventional h-index, it weights citations by how many times areference is mentioned. According to our experiments, the hip-index is a betterindicator of researcher performance than the conventional h-index.
arxiv-1501-06225 | Online Optimization : Competing with Dynamic Comparators |  http://arxiv.org/abs/1501.06225  | author:Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, Karthik Sridharan category:cs.LG math.OC stat.ML published:2015-01-26 summary:Recent literature on online learning has focused on developing adaptivealgorithms that take advantage of a regularity of the sequence of observations,yet retain worst-case performance guarantees. A complementary direction is todevelop prediction methods that perform well against complex benchmarks. Inthis paper, we address these two directions together. We present a fullyadaptive method that competes with dynamic benchmarks in which regret guaranteescales with regularity of the sequence of cost functions and comparators.Notably, the regret bound adapts to the smaller complexity measure in theproblem environment. Finally, we apply our results to drifting zero-sum,two-player games where both players achieve no regret guarantees against bestsequences of actions in hindsight.
arxiv-1501-06180 | Exploring Human Vision Driven Features for Pedestrian Detection |  http://arxiv.org/abs/1501.06180  | author:Shanshan Zhang, Christian Bauckhage, Dominik A. Klein, Armin B. Cremers category:cs.CV published:2015-01-25 summary:Motivated by the center-surround mechanism in the human visual attentionsystem, we propose to use average contrast maps for the challenge of pedestriandetection in street scenes due to the observation that pedestrians indeedexhibit discriminative contrast texture. Our main contributions are first todesign a local, statistical multi-channel descriptorin order to incorporateboth color and gradient information. Second, we introduce a multi-direction andmulti-scale contrast scheme based on grid-cells in order to integrateexpressive local variations. Contributing to the issue of selecting mostdiscriminative features for assessing and classification, we perform extensivecomparisons w.r.t. statistical descriptors, contrast measurements, and scalestructures. This way, we obtain reasonable results under variousconfigurations. Empirical findings from applying our optimized detector on theINRIA and Caltech pedestrian datasets show that our features yieldstate-of-the-art performance in pedestrian detection.
arxiv-1501-06114 | Accurate automatic segmentation of retina layers with emphasis on first layer |  http://arxiv.org/abs/1501.06114  | author:Mahdi Salarian category:cs.CV published:2015-01-25 summary:Quantification of intra-retinal boundaries in optical coherence tomography(OCT) is a crucial task for studying and diagnosing neurological and oculardiseases. Since manual segmentation of layers is usually a time consuming taskand relay on user, a lot of attempts done to do it automatically and withoutinterference of user. Although for extracting all layers usually same procedureis applied but finding the first layer is usually more difficult due tovanishing it in some region specially close to Fobia. To have a generalsoftware, beside using common methods like applying shortest path algorithm onglobal gradient of image, some extra steps are used here to confine search areafor Dijstra algorithm especially for the second layer. Results demonstrateshigh accuracy in segmenting all present layers, especially the first one thatis important for diagnosing issue.
arxiv-1501-06115 | Constrained Extreme Learning Machines: A Study on Classification Cases |  http://arxiv.org/abs/1501.06115  | author:Wentao Zhu, Jun Miao, Laiyun Qing category:cs.LG cs.CV cs.NE published:2015-01-25 summary:Extreme learning machine (ELM) is an extremely fast learning method and has apowerful performance for pattern recognition tasks proven by enormousresearches and engineers. However, its good generalization ability is built onlarge numbers of hidden neurons, which is not beneficial to real time responsein the test process. In this paper, we proposed new ways, named "constrainedextreme learning machines" (CELMs), to randomly select hidden neurons based onsample distribution. Compared to completely random selection of hidden nodes inELM, the CELMs randomly select hidden nodes from the constrained vector spacecontaining some basic combinations of original sample vectors. The experimentalresults show that the CELMs have better generalization ability than traditionalELM, SVM and some other related methods. Additionally, the CELMs have a similarfast learning speed as ELM.
arxiv-1501-06209 | Parallel Magnetic Resonance Imaging |  http://arxiv.org/abs/1501.06209  | author:Martin Uecker category:cs.NA cs.CV math.NA math.OC physics.med-ph published:2015-01-25 summary:The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scantimes and, in consequence, its sensitivity to motion. Exploiting thecomplementary information from multiple receive coils, parallel imaging is ableto recover images from under-sampled k-space data and to accelerate themeasurement. Because parallel magnetic resonance imaging can be used toaccelerate basically any imaging sequence it has many important applications.Parallel imaging brought a fundamental shift in image reconstruction: Imagereconstruction changed from a simple direct Fourier transform to the solutionof an ill-conditioned inverse problem. This work gives an overview of imagereconstruction from the perspective of inverse problems. After introducingbasic concepts such as regularization, discretization, and iterativereconstruction, advanced topics are discussed including algorithms forauto-calibration, the connection to approximation theory, and the combinationwith compressed sensing.
arxiv-1501-06129 | An Occlusion Reasoning Scheme for Monocular Pedestrian Tracking in Dynamic Scenes |  http://arxiv.org/abs/1501.06129  | author:Sourav Garg, Swagat Kumar, Rajesh Ratnakaram, Prithwijit Guha category:cs.CV published:2015-01-25 summary:This paper looks into the problem of pedestrian tracking using a monocular,potentially moving, uncalibrated camera. The pedestrians are located in eachframe using a standard human detector, which are then tracked in subsequentframes. This is a challenging problem as one has to deal with complexsituations like changing background, partial or full occlusion and cameramotion. In order to carry out successful tracking, it is necessary to resolveassociations between the detected windows in the current frame with thoseobtained from the previous frame. Compared to methods that use temporal windowsincorporating past as well as future information, we attempt to make decisionon a frame-by-frame basis. An occlusion reasoning scheme is proposed to resolvethe association problem between a pair of consecutive frames by using anaffinity matrix that defines the closeness between a pair of windows and then,uses a binary integer programming to obtain unique association between them. Asecond stage of verification based on SURF matching is used to deal with thosecases where the above optimization scheme might yield wrong associations. Theefficacy of the approach is demonstrated through experiments on severalstandard pedestrian datasets.
arxiv-1501-06202 | Robust Subjective Visual Property Prediction from Crowdsourced Pairwise Labels |  http://arxiv.org/abs/1501.06202  | author:Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, Yuan Yao category:cs.CV cs.LG cs.MM cs.SI math.ST stat.TH published:2015-01-25 summary:The problem of estimating subjective visual properties from image and videohas attracted increasing interest. A subjective visual property is usefuleither on its own (e.g. image and video interestingness) or as an intermediaterepresentation for visual recognition (e.g. a relative attribute). Due to itsambiguous nature, annotating the value of a subjective visual property forlearning a prediction model is challenging. To make the annotation morereliable, recent studies employ crowdsourcing tools to collect pairwisecomparison labels because human annotators are much better at ranking twoimages/videos (e.g. which one is more interesting) than giving an absolutevalue to each of them separately. However, using crowdsourced data alsointroduces outliers. Existing methods rely on majority voting to prune theannotation outliers/errors. They thus require large amount of pairwise labelsto be collected. More importantly as a local outlier detection method, majorityvoting is ineffective in identifying outliers that can cause global rankinginconsistencies. In this paper, we propose a more principled way to identifyannotation outliers by formulating the subjective visual property predictiontask as a unified robust learning to rank problem, tackling both the outlierdetection and learning to rank jointly. Differing from existing methods, theproposed method integrates local pairwise comparison labels together tominimise a cost that corresponds to global inconsistency of ranking order. Thisnot only leads to better detection of annotation outliers but also enableslearning with extremely sparse annotations. Extensive experiments on variousbenchmark datasets demonstrate that our new approach significantly outperformsstate-of-the-arts alternatives.
arxiv-1501-06218 | Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction |  http://arxiv.org/abs/1501.06218  | author:Mingyuan Zhou category:stat.ML cs.SI published:2015-01-25 summary:A hierarchical gamma process infinite edge partition model is proposed tofactorize the binary adjacency matrix of an unweighted undirected relationalnetwork under a Bernoulli-Poisson link. The model describes both homophily andstochastic equivalence, and is scalable to big sparse networks by focusing itscomputation on pairs of linked nodes. It can not only discover overlappingcommunities and inter-community interactions, but also predict missing edges. Asimplified version omitting inter-community interactions is also provided andwe reveal its interesting connections to existing models. The number ofcommunities is automatically inferred in a nonparametric Bayesian manner, andefficient inference via Gibbs sampling is derived using novel data augmentationtechniques. Experimental results on four real networks demonstrate the models'scalability and state-of-the-art performance.
arxiv-1501-06116 | Prediction Error Reduction Function as a Variable Importance Score |  http://arxiv.org/abs/1501.06116  | author:Ernest Fokoué category:stat.ML 62H25, 62H30 published:2015-01-25 summary:This paper introduces and develops a novel variable importance score functionin the context of ensemble learning and demonstrates its appeal boththeoretically and empirically. Our proposed score function is simple and morestraightforward than its counterpart proposed in the context of random forest,and by avoiding permutations, it is by design computationally more efficientthan the random forest variable importance function. Just like the randomforest variable importance function, our score handles both regression andclassification seamlessly. One of the distinct advantage of our proposed scoreis the fact that it offers a natural cut off at zero, with all the positivescores indicating importance and significance, while the negative scores aredeemed indications of insignificance. An extra advantage of our proposed scorelies in the fact it works very well beyond ensemble of trees and can seamlesslybe used with any base learners in the random subspace learning context. Ourexamples, both simulated and real, demonstrate that our proposed score doescompete mostly favorably with the random forest score.
arxiv-1501-06195 | Randomized sketches for kernels: Fast and optimal non-parametric regression |  http://arxiv.org/abs/1501.06195  | author:Yun Yang, Mert Pilanci, Martin J. Wainwright category:stat.ML cs.DS cs.LG stat.CO published:2015-01-25 summary:Kernel ridge regression (KRR) is a standard method for performingnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$samples, the time and space complexity of computing the KRR estimate scale as$\mathcal{O}(n^3)$ and $\mathcal{O}(n^2)$ respectively, and so is prohibitivein many cases. We propose approximations of KRR based on $m$-dimensionalrandomized sketches of the kernel matrix, and study how small the projectiondimension $m$ can be chosen while still preserving minimax optimality of theapproximate KRR estimate. For various classes of randomized sketches, includingthose based on Gaussian and randomized Hadamard matrices, we prove that itsuffices to choose the sketch dimension $m$ proportional to the statisticaldimension (modulo logarithmic factors). Thus, we obtain fast and minimaxoptimal approximations to the KRR estimate for non-parametric regression.
arxiv-1501-06103 | A simpler condition for consistency of a kernel independence test |  http://arxiv.org/abs/1501.06103  | author:Arthur Gretton category:stat.ML published:2015-01-25 summary:A statistical test of independence may be constructed using theHilbert-Schmidt Independence Criterion (HSIC) as a test statistic. The HSIC isdefined as the distance between the embedding of the joint distribution, andthe embedding of the product of the marginals, in a Reproducing Kernel HilbertSpace (RKHS). It has previously been shown that when the kernel used indefining the joint embedding is characteristic (that is, the embedding of thejoint distribution to the feature space is injective), then the HSIC-based testis consistent. In particular, it is sufficient for the product of kernels onthe individual domains to be characteristic on the joint domain. In this note,it is established via a result of Lyons (2013) that HSIC-based independencetests are consistent when kernels on the marginals are characteristic on theirrespective domains, even when the product of kernels is not characteristic onthe joint domain.
arxiv-1501-06170 | Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals |  http://arxiv.org/abs/1501.06170  | author:Minsu Cho, Suha Kwak, Cordelia Schmid, Jean Ponce category:cs.CV published:2015-01-25 summary:This paper addresses unsupervised discovery and localization of dominantobjects from a noisy image collection with multiple object classes. The settingof this problem is fully unsupervised, without even image-level annotations orany assumption of a single dominant class. This is far more general thantypical colocalization, cosegmentation, or weakly-supervised localizationtasks. We tackle the discovery and localization problem using a part-basedregion matching approach: We use off-the-shelf region proposals to form a setof candidate bounding boxes for objects and object parts. These regions areefficiently matched across images using a probabilistic Hough transform thatevaluates the confidence for each candidate correspondence considering bothappearance and spatial consistency. Dominant objects are discovered andlocalized by comparing the scores of candidate regions and selecting those thatstand out over other regions containing them. Extensive experimentalevaluations on standard benchmarks demonstrate that the proposed approachsignificantly outperforms the current state of the art in colocalization, andachieves robust object discovery in challenging mixed-class datasets.
arxiv-1501-06066 | Sparse Distance Weighted Discrimination |  http://arxiv.org/abs/1501.06066  | author:Boxiang Wang, Hui Zou category:stat.ML stat.CO published:2015-01-24 summary:Distance weighted discrimination (DWD) was originally proposed to handle thedata piling issue in the support vector machine. In this paper, we consider thesparse penalized DWD for high-dimensional classification. The state-of-the-artalgorithm for solving the standard DWD is based on second-order coneprogramming, however such an algorithm does not work well for the sparsepenalized DWD with high-dimensional data. In order to overcome the challengingcomputation difficulty, we develop a very efficient algorithm to compute thesolution path of the sparse DWD at a given fine grid of regularizationparameters. We implement the algorithm in a publicly available R package sdwd.We conduct extensive numerical experiments to demonstrate the computationalefficiency and classification performance of our method.
arxiv-1501-06095 | Between Pure and Approximate Differential Privacy |  http://arxiv.org/abs/1501.06095  | author:Thomas Steinke, Jonathan Ullman category:cs.DS cs.CR cs.LG published:2015-01-24 summary:We show a new lower bound on the sample complexity of $(\varepsilon,\delta)$-differentially private algorithms that accurately answer statisticalqueries on high-dimensional databases. The novelty of our bound is that itdepends optimally on the parameter $\delta$, which loosely corresponds to theprobability that the algorithm fails to be private, and is the first tosmoothly interpolate between approximate differential privacy ($\delta > 0$)and pure differential privacy ($\delta = 0$). Specifically, we consider a database $D \in \{\pm1\}^{n \times d}$ and its\emph{one-way marginals}, which are the $d$ queries of the form "What fractionof individual records have the $i$-th bit set to $+1$?" We show that in orderto answer all of these queries to within error $\pm \alpha$ (on average) whilesatisfying $(\varepsilon, \delta)$-differential privacy, it is necessary that$$ n \geq \Omega\left( \frac{\sqrt{d \log(1/\delta)}}{\alpha \varepsilon}\right), $$ which is optimal up to constant factors. To prove our lower bound,we build on the connection between \emph{fingerprinting codes} and lower boundsin differential privacy (Bun, Ullman, and Vadhan, STOC'14). In addition to our lower bound, we give new purely and approximatelydifferentially private algorithms for answering arbitrary statistical queriesthat improve on the sample complexity of the standard Laplace and Gaussianmechanisms for achieving worst-case accuracy guarantees by a logarithmicfactor.
arxiv-1501-06060 | Consistency Analysis of Nearest Subspace Classifier |  http://arxiv.org/abs/1501.06060  | author:Yi Wang category:stat.ML cs.LG published:2015-01-24 summary:The Nearest subspace classifier (NSS) finds an estimation of the underlyingsubspace within each class and assigns data points to the class thatcorresponds to its nearest subspace. This paper mainly studies how well NSS canbe generalized to new samples. It is proved that NSS is strongly consistentunder certain assumptions. For completeness, NSS is evaluated throughexperiments on various simulated and real data sets, in comparison with someother linear model based classifiers. It is also shown that NSS can obtaineffective classification results and is very efficient, especially for largescale data sets.
arxiv-1501-05740 | Bayesian Learning for Low-Rank matrix reconstruction |  http://arxiv.org/abs/1501.05740  | author:Martin Sundin, Cristian R. Rojas, Magnus Jansson, Saikat Chatterjee category:stat.ML cs.LG cs.NA published:2015-01-23 summary:We develop latent variable models for Bayesian learning based low-rank matrixcompletion and reconstruction from linear measurements. For under-determinedsystems, the developed methods are shown to reconstruct low-rank matrices whenneither the rank nor the noise power is known a-priori. We derive relationsbetween the latent variable models and several low-rank promoting penaltyfunctions. The relations justify the use of Kronecker structured covariancematrices in a Gaussian based prior. In the methods, we use evidenceapproximation and expectation-maximization to learn the model parameters. Theperformance of the methods is evaluated through extensive numericalsimulations.
arxiv-1501-05964 | Advances in Human Action Recognition: A Survey |  http://arxiv.org/abs/1501.05964  | author:Guangchun Cheng, Yiwen Wan, Abdullah N. Saudagar, Kamesh Namuduri, Bill P. Buckles category:cs.CV published:2015-01-23 summary:Human action recognition has been an important topic in computer vision dueto its many applications such as video surveillance, human machine interactionand video retrieval. One core problem behind these applications isautomatically recognizing low-level actions and high-level activities ofinterest. The former is usually the basis for the latter. This survey gives anoverview of the most recent advances in human action recognition during thepast several years, following a well-formed taxonomy proposed by a previoussurvey. From this state-of-the-art survey, researchers can view a panorama ofprogress in this area for future research.
arxiv-1501-05703 | Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues |  http://arxiv.org/abs/1501.05703  | author:Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, Lubomir Bourdev category:cs.CV published:2015-01-23 summary:We explore the task of recognizing peoples' identities in photo albums in anunconstrained setting. To facilitate this, we introduce the new People In PhotoAlbums (PIPA) dataset, consisting of over 60000 instances of 2000 individualscollected from public Flickr photo albums. With only about half of the personimages containing a frontal face, the recognition task is very challenging dueto the large variations in pose, clothing, camera viewpoint, image resolutionand illumination. We propose the Pose Invariant PErson Recognition (PIPER)method, which accumulates the cues of poselet-level person recognizers trainedby deep convolutional networks to discount for the pose variations, combinedwith a face recognizer and a global recognizer. Experiments on three differentsettings confirm that in our unconstrained setup PIPER significantly improveson the performance of DeepFace, which is one of the best face recognizers asmeasured on the LFW dataset.
arxiv-1501-05970 | Automatic Objects Removal for Scene Completion |  http://arxiv.org/abs/1501.05970  | author:Jianjun Yang, Yin Wang, Honggang Wang, Kun Hua, Wei Wang, Ju Shen category:cs.CV published:2015-01-23 summary:With the explosive growth of web-based cameras and mobile devices, billionsof photographs are uploaded to the internet. We can trivially collect a hugenumber of photo streams for various goals, such as 3D scene reconstruction andother big data applications. However, this is not an easy task due to the factthe retrieved photos are neither aligned nor calibrated. Furthermore, with theocclusion of unexpected foreground objects like people, vehicles, it is evenmore challenging to find feature correspondences and reconstruct realisticscenes. In this paper, we propose a structure based image completion algorithmfor object removal that produces visually plausible content with consistentstructure and scene texture. We use an edge matching technique to infer thepotential structure of the unknown region. Driven by the estimated structure,texture synthesis is performed automatically along the estimated curves. Weevaluate the proposed method on different types of images: from highlystructured indoor environment to the natural scenes. Our experimental resultsdemonstrate satisfactory performance that can be potentially used forsubsequent big data processing: 3D scene reconstruction and locationrecognition.
arxiv-1501-05854 | Unsupervised Segmentation of Multispectral Images with Cellular Automata |  http://arxiv.org/abs/1501.05854  | author:Wuilian Torres, Antonio Rueda-Toicen category:cs.CV published:2015-01-23 summary:Multispectral images acquired by satellites are used to study phenomena onthe Earth's surface. Unsupervised classification techniques analyzemultispectral image content without considering prior knowledge of the observedterrain; this is done using techniques which group pixels that have similarstatistics of digital level distribution in the various image channels. In thispaper, we propose a methodology for unsupervised classification based on adeterministic cellular automaton. The automaton is initialized in anunsupervised manner by setting seed cells, selected according to two criteria:to be representative of the spatial distribution of the dominant elements inthe image, and to take into account the diversity of spectral signatures in theimage. The automaton's evolution is based on an attack rule that is appliedsimultaneously to all its cells. Among the noteworthy advantages ofdeterministic cellular automata for multispectral processing of satelliteimagery is the consideration of topological information in the image via seedpositioning, and the ability to modify the scale of the study.
arxiv-1501-05892 | Capacity-achieving Sparse Superposition Codes via Approximate Message Passing Decoding |  http://arxiv.org/abs/1501.05892  | author:Cynthia Rush, Adam Greig, Ramji Venkataramanan category:cs.IT math.IT stat.ML published:2015-01-23 summary:Sparse superposition codes were recently introduced by Barron and Joseph forreliable communication over the AWGN channel at rates approaching the channelcapacity. The codebook is defined in terms of a Gaussian design matrix, andcodewords are sparse linear combinations of columns of the matrix. In thispaper, we propose an approximate message passing decoder for sparsesuperposition codes, whose decoding complexity scales linearly with the size ofthe design matrix. The performance of the decoder is rigorously analyzed and itis shown to asymptotically achieve the AWGN capacity with an appropriate powerallocation. Simulation results are provided to demonstrate the performance ofthe decoder at finite blocklengths. We introduce a power allocation scheme toimprove the empirical performance, and demonstrate how the decoding complexitycan be significantly reduced by using Hadamard design matrices.
arxiv-1501-05790 | Taking a Deeper Look at Pedestrians |  http://arxiv.org/abs/1501.05790  | author:Jan Hosang, Mohamed Omran, Rodrigo Benenson, Bernt Schiele category:cs.CV published:2015-01-23 summary:In this paper we study the use of convolutional neural networks (convnets)for the task of pedestrian detection. Despite their recent diverse successes,convnets historically underperform compared to other pedestrian detectors. Wedeliberately omit explicitly modelling the problem into the network (e.g. partsor occlusion modelling) and show that we can reach competitive performancewithout bells and whistles. In a wide range of experiments we analyse small andbig convnets, their architectural choices, parameters, and the influence ofdifferent training data, including pre-training on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI dataset. OnCaltech our convnets reach top performance both for the Caltech1x andCaltech10x training setup. Using additional data at training time our strongestconvnet model is competitive even to detectors that use additional data(optical flow) at test time.
arxiv-1501-05759 | Filtered Channel Features for Pedestrian Detection |  http://arxiv.org/abs/1501.05759  | author:Shanshan Zhang, Rodrigo Benenson, Bernt Schiele category:cs.CV published:2015-01-23 summary:This paper starts from the observation that multiple top performingpedestrian detectors can be modelled by using an intermediate layer filteringlow-level features in combination with a boosted decision forest. Based on thisobservation we propose a unifying framework and experimentally exploredifferent filter families. We report extensive results enabling a systematicanalysis. Using filtered channel features we obtain top performance on the challengingCaltech and KITTI datasets, while using only HOG+LUV as low-level features.When adding optical flow features we further improve detection quality andreport the best known results on the Caltech dataset, reaching 93% recall at 1FPPI.
arxiv-1501-05382 | Enhanced Mixtures of Part Model for Human Pose Estimation |  http://arxiv.org/abs/1501.05382  | author:Wenjuan Gong, Yongzhen Huang, Jordi Gonzalez, and Liang Wang category:cs.CV published:2015-01-22 summary:Mixture of parts model has been successfully applied to 2D human poseestimation problem either as explicitly trained body part model or as latentvariables for the whole human body model. Mixture of parts model usuallyutilize tree structure for representing relations between body parts. Treestructures facilitate training and referencing of the model but could not dealwith double counting problems, which hinder its applications in 3D poseestimation. While most of work targeted to solve these problems tend to modifythe tree models or the optimization target. We incorporate other cues frominput features. For example, in surveillance environments, human silhouettescan be extracted relative easily although not flawlessly. In this condition, wecan combine extracted human blobs with histogram of gradient feature, which iscommonly used in mixture of parts model for training body part templates. Themethod can be easily extend to other candidate features under our generalizedframework. We show 2D body part detection results on a public availabledataset: HumanEva dataset. Furthermore, a 2D to 3D pose estimator is trainedwith Gaussian process regression model and 2D body part detections from theproposed method is fed to the estimator, thus 3D poses are predictable givennew 2D body part detections. We also show results of 3D pose estimation onHumanEva dataset.
arxiv-1501-05677 | Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs |  http://arxiv.org/abs/1501.05677  | author:David Tolpin, Jan Willem van de Meent, Brooks Paige, Frank Wood category:cs.AI stat.ML published:2015-01-22 summary:We introduce an adaptive output-sensitive Metropolis-Hastings algorithm forprobabilistic models expressed as programs, Adaptive LightweightMetropolis-Hastings (AdLMH). The algorithm extends LightweightMetropolis-Hastings (LMH) by adjusting the probabilities of proposing randomvariables for modification to improve convergence of the program output. Weshow that AdLMH converges to the correct equilibrium distribution and compareconvergence of AdLMH to that of LMH on several test problems to highlightdifferent aspects of the adaptation scheme. We observe consistent improvementin convergence on the test problems.
arxiv-1501-05472 | Handwritten Devanagari Script Segmentation: A non-linear Fuzzy Approach |  http://arxiv.org/abs/1501.05472  | author:Ram Sarkar, Bibhash Sen, Nibaran Das, Subhadip Basu category:cs.CV published:2015-01-22 summary:The paper concentrates on improvement of segmentation accuracy by addressingsome of the key challenges of handwritten Devanagari word image segmentationtechnique. In the present work, we have developed a new feature based approachfor identification of Matra pixels from a word image, design of a non-linearfuzzy membership functions for headline estimation and finally design of anon-linear fuzzy functions for identifying segmentation points on the Matra.The segmentation accuracy achieved by the current technique is 94.8%. Thisshows an improvement of performance by 1.8% over the previous technique [1] ona 300-word dataset, used for the current experiment.
arxiv-1501-05494 | Design of a novel convex hull based feature set for recognition of isolated handwritten Roman numerals |  http://arxiv.org/abs/1501.05494  | author:Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu category:cs.CV published:2015-01-22 summary:In this paper, convex hull based features are used for recognition ofisolated Roman numerals using a Multi Layer Perceptron (MLP) based classifier.Experiments of convex hull based features for handwritten character recognitionare few in numbers. Convex hull of a pattern and the centroid of the convexhull both are affine invariant attributes. In this work, 25 features areextracted based on different bays attributes of the convex hull of the digitpatterns. Then these patterns are divided into four sub-images with respect tothe centroid of the convex hull boundary. From each such sub-image 25 baysfeatures are also calculated. In all 125 convex hull based features areextracted for each numeric digit patterns under the current experiment. Theperformance of the designed feature set is tested on the standard MNIST dataset, consisting of 60000 training and 10000 test images of handwritten Romanusing an MLP based classifier a maximum success rate of 97.44% is achieved onthe test data.
arxiv-1501-05495 | A GA Based approach for selection of local features for recognition of handwritten Bangla numerals |  http://arxiv.org/abs/1501.05495  | author:Nibaran Das, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri category:cs.CV published:2015-01-22 summary:Soft computing approaches are mainly designed to address the real worldill-defined, imprecisely formulated problems, combining different kind of novelmodels of computation, such as neural networks, genetic algorithms (GAs.Handwritten digit recognition is a typical example of one such problem. In thecurrent work we have developed a two-pass approach where the first passclassifier performs a coarse classification, based on some global features ofthe input pattern by restricting the possibility of classification decisionswithin a group of classes, smaller than the number of classes consideredinitially. In the second pass, the group specific classifiers concentrate onthe features extracted from the selected local regions, and refine the earlierdecision by combining the local and the global features for selecting the trueclass of the input pattern from the group of candidate classes selected in thefirst pass. To optimize the selection of local regions a GA based approach hasbeen developed here. The maximum recognition performance on Bangla digitsamples as achieved on the test set, during the first pass of the two passapproach is 93.35%. After combining the results of the two stage classifiers,an overall success rate of 95.25% is achieved.
arxiv-1501-05940 | A New Efficient Method for Calculating Similarity Between Web Services |  http://arxiv.org/abs/1501.05940  | author:T. Rachad, J. Boutahar, S. El ghazi category:cs.AI cs.CL cs.IR cs.SE published:2015-01-22 summary:Web services allow communication between heterogeneous systems in adistributed environment. Their enormous success and their increased use led tothe fact that thousands of Web services are present on the Internet. Thissignificant number of Web services which not cease to increase has led toproblems of the difficulty in locating and classifying web services, theseproblems are encountered mainly during the operations of web services discoveryand substitution. Traditional ways of search based on keywords are notsuccessful in this context, their results do not support the structure of Webservices and they consider in their search only the identifiers of the webservice description language (WSDL) interface elements. The methods based onsemantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Webservice with a semantic description allow raising partially this problem, buttheir complexity and difficulty delays their adoption in real cases. Measuringthe similarity between the web services interfaces is the most suitablesolution for this kind of problems, it will classify available web services soas to know those that best match the searched profile and those that do notmatch. Thus, the main goal of this work is to study the degree of similaritybetween any two web services by offering a new method that is more effectivethan existing works.
arxiv-1501-05427 | Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE) |  http://arxiv.org/abs/1501.05427  | author:Maurizio Filippone, Raphael Engler category:stat.ME stat.CO stat.ML published:2015-01-22 summary:In applications of Gaussian processes where quantification of uncertainty isof primary interest, it is necessary to accurately characterize the posteriordistribution over covariance parameters. This paper proposes an adaptation ofthe Stochastic Gradient Langevin Dynamics algorithm to draw samples from theposterior distribution over covariance parameters with negligible bias andwithout the need to compute the marginal likelihood. In Gaussian processregression, this has the enormous advantage that stochastic gradients can becomputed by solving linear systems only. A novel unbiased linear systems solverbased on parallelizable covariance matrix-vector products is developed toaccelerate the unbiased estimation of gradients. The results demonstrate thepossibility to enable scalable and exact (in a Monte Carlo sense)quantification of uncertainty in Gaussian processes without imposing anyspecial structure on the covariance or reducing the number of input vectors.
arxiv-1501-05680 | Active Mean Fields for Probabilistic Image Segmentation: Connections with Chan-Vese and Rudin-Osher-Fatemi Models |  http://arxiv.org/abs/1501.05680  | author:Marc Niethammer, Kilian M. Pohl, Firdaus Janoos, William M. Wells III category:cs.CV published:2015-01-22 summary:Image segmentation is a fundamental task for extracting semanticallymeaningful regions from an image. The goal is to assign object labels to eachimage location. Due to image-noise, shortcomings of algorithms and otherambiguities in the images, there is uncertainty in the assigned labels. Inmultiple application domains, estimates of this uncertainty are important. Forexample, object segmentation and uncertainty quantification is essential formany medical application, including tumor segmentation for radiation treatmentplanning. While a Bayesian characterization of the label posterior providesestimates of segmentation uncertainty, Bayesian approaches can becomputationally prohibitive for practical applications. On the other hand,typical optimization based algorithms are computationally very efficient, butonly provide maximum a-posteriori solutions and hence no estimates of labeluncertainty. In this paper, we propose Active Mean Fields (AMF), a Bayesian technique thatuses a mean-field approximation to derive an efficient segmentation anduncertainty quantification algorithm. This model, which allows combining anylabel-likelihood measure with a boundary length prior, yields a variationalformulation that is convex. A specific implementation of that model is theChan--Vese segmentation model (CV), which formulates the binary segmentationproblem through Gaussian likelihoods combined with a boundary-lengthregularizer. Furthermore, the Euler--Lagrange equations derived from the AMFmodel are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model forimage de-noising. Solutions to the AMF model can thus be implemented bydirectly utilizing highly-efficient ROF solvers on log-likelihood ratio fields.We demonstrate the approach using synthetic data, as well as real medicalimages (for heart and prostate segmentations), and on standard computer visiontest images.
arxiv-1501-05624 | A Collaborative Kalman Filter for Time-Evolving Dyadic Processes |  http://arxiv.org/abs/1501.05624  | author:San Gultekin, John Paisley category:stat.ML cs.LG published:2015-01-22 summary:We present the collaborative Kalman filter (CKF), a dynamic model forcollaborative filtering and related factorization models. Using the matrixfactorization approach to collaborative filtering, the CKF accounts for timeevolution by modeling each low-dimensional latent embedding as amultidimensional Brownian motion. Each observation is a random variable whosedistribution is parameterized by the dot product of the relevant Brownianmotions at that moment in time. This is naturally interpreted as a Kalmanfilter with multiple interacting state space vectors. We also present a methodfor learning a dynamically evolving drift parameter for each location bymodeling it as a geometric Brownian motion. We handle posterior intractabilityvia a mean-field variational approximation, which also preserves tractabilityfor downstream calculations in a manner similar to the Kalman filter. Weevaluate the model on several large datasets, providing quantitative evaluationon the 10 million Movielens and 100 million Netflix datasets and qualitativeevaluation on a set of 39 million stock returns divided across roughly 6,500companies from the years 1962-2014.
arxiv-1501-05499 | Globally Optimal Cell Tracking using Integer Programming |  http://arxiv.org/abs/1501.05499  | author:Engin Türetken, Xinchao Wang, Carlos Becker, Carsten Haubold, Pascal Fua category:cs.CV published:2015-01-22 summary:We propose a novel approach to automatically tracking cell populations intime-lapse images. To account for cell occlusions and overlaps, we introduce arobust method that generates an over-complete set of competing detectionhypotheses. We then perform detection and tracking simultaneously on thesehypotheses by solving to optimality an integer program with only one type offlow variables. This eliminates the need for heuristics to handle misseddetections due to occlusions and complex morphology. We demonstrate theeffectiveness of our approach on a range of challenging sequences consisting ofclumped cells and show that it outperforms state-of-the-art techniques.
arxiv-1501-05497 | An Improved Feature Descriptor for Recognition of Handwritten Bangla Alphabet |  http://arxiv.org/abs/1501.05497  | author:Nibaran Das, Subhadip Basu, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak kumar Basu category:cs.CV published:2015-01-22 summary:Appropriate feature set for representation of pattern classes is one of themost important aspects of handwritten character recognition. The effectivenessof features depends on the discriminating power of the features chosen torepresent patterns of different classes. However, discriminatory features arenot easily measurable. Investigative experimentation is necessary foridentifying discriminatory features. In the present work we have identified anew variation of feature set which significantly outperforms on handwrittenBangla alphabet from the previously used feature set. 132 number of features inall viz. modified shadow features, octant and centroid features, distance basedfeatures, quad tree based longest run features are used here. Using thisfeature set the recognition performance increases sharply from the 75.05%observed in our previous work [7], to 85.40% on 50 character classes with MLPbased classifier on the same dataset.
arxiv-1501-05432 | Point Context: An Effective Shape Descriptor for RST-invariant Trajectory Recognition |  http://arxiv.org/abs/1501.05432  | author:Xingyu Wu, Xia Mao, Lijiang Chen, Yuli Xue, Angelo Compare category:cs.CV 51A05 I.2.10; I.5.4 published:2015-01-22 summary:Motion trajectory recognition is important for characterizing the movingproperty of an object. The speed and accuracy of trajectory recognition rely ona compact and discriminative feature representation, and the situations ofvarying rotation, scaling and translation has to be specially considered. Inthis paper we propose a novel feature extraction method for trajectories.Firstly a trajectory is represented by a proposed point context, which is arotation-scale-translation (RST) invariant shape descriptor with a flexibletradeoff between computational complexity and discrimination, yet we prove thatit is a complete shape descriptor. Secondly, the shape context is nonlinearlymapped to a subspace by kernel nonparametric discriminant analysis (KNDA) toget a compact feature representation, and thus a trajectory is projected to asingle point in a low-dimensional feature space. Experimental results showthat, the proposed trajectory feature shows encouraging improvement thanstate-of-art methods.
arxiv-1501-05617 | Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks |  http://arxiv.org/abs/1501.05617  | author:Mohamed Ali Mahjoub, Mohamed Mhiri category:cs.CV published:2015-01-22 summary:Today Bayesian networks are more used in many areas of decision support andimage processing. In this way, our proposed approach uses Bayesian Network tomodelize the segmented image quality. This quality is calculated on a set ofattributes that represent local evaluation measures. The idea is to have theselocal levels chosen in a way to be intersected into them to keep the overallappearance of segmentation. The approach operates in two phases: the firstphase is to make an over-segmentation which gives superpixels card. In thesecond phase, we model the superpixels by a Bayesian Network. To find thesegmented image with the best overall quality we used two approximate inferencemethods, the first using ICM algorithm which is widely used in Markov Modelsand a second is a recursive method called algorithm of model decompositionbased on max-product algorithm which is very popular in the recent works ofimage segmentation. For our model, we have shown that the composition of thesetwo algorithms leads to good segmentation performance.
arxiv-1501-05552 | Estimating the Intrinsic Dimension of Hyperspectral Images Using an Eigen-Gap Approach |  http://arxiv.org/abs/1501.05552  | author:A. Halimi, P. Honeine, M. Kharouf, C. Richard, J. -Y. Tourneret category:stat.AP cs.CV published:2015-01-22 summary:Linear mixture models are commonly used to represent hyperspectral datacubeas a linear combinations of endmember spectra. However, determining of thenumber of endmembers for images embedded in noise is a crucial task. This paperproposes a fully automatic approach for estimating the number of endmembers inhyperspectral images. The estimation is based on recent results of randommatrix theory related to the so-called spiked population model. More precisely,we study the gap between successive eigenvalues of the sample covariance matrixconstructed from high dimensional noisy samples. The resulting estimationstrategy is unsupervised and robust to correlated noise. This strategy isvalidated on both synthetic and real images. The experimental results are verypromising and show the accuracy of this algorithm with respect tostate-of-the-art algorithms.
arxiv-1501-05396 | Deep Multimodal Learning for Audio-Visual Speech Recognition |  http://arxiv.org/abs/1501.05396  | author:Youssef Mroueh, Etienne Marcheret, Vaibhava Goel category:cs.CL cs.LG published:2015-01-22 summary:In this paper, we present methods in deep multimodal learning for fusingspeech and visual modalities for Audio-Visual Automatic Speech Recognition(AV-ASR). First, we study an approach where uni-modal deep networks are trainedseparately and their final hidden layers fused to obtain a joint feature spacein which another deep network is built. While the audio network alone achievesa phone error rate (PER) of $41\%$ under clean condition on the IBM largevocabulary audio-visual studio dataset, this fusion model achieves a PER of$35.83\%$ demonstrating the tremendous value of the visual channel in phoneclassification even in audio with high signal to noise ratio. Second, wepresent a new deep network architecture that uses a bilinear softmax layer toaccount for class specific correlations between modalities. We show thatcombining the posteriors from the bilinear networks with those from the fusedmodel mentioned above results in a further significant phone error ratereduction, yielding a final PER of $34.03\%$.
arxiv-1501-05590 | Sketch and Validate for Big Data Clustering |  http://arxiv.org/abs/1501.05590  | author:Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis category:stat.ML cs.LG published:2015-01-22 summary:In response to the need for learning tools tuned to big data analytics, thepresent paper introduces a framework for efficient clustering of huge sets of(possibly high-dimensional) data. Building on random sampling and consensus(RANSAC) ideas pursued earlier in a different (computer vision) context forrobust regression, a suite of novel dimensionality and set-reduction algorithmsis developed. The advocated sketch-and-validate (SkeVa) family includes twoalgorithms that rely on K-means clustering per iteration on reduced number ofdimensions and/or feature vectors: The first operates in a batch fashion, whilethe second sequential one offers computational efficiency and suitability withstreaming modes of operation. For clustering even nonlinearly separablevectors, the SkeVa family offers also a member based on user-selected kernelfunctions. Further trading off performance for reduced complexity, a fourthmember of the SkeVa family is based on a divergence criterion for selectingproper minimal subsets of feature variables and vectors, thus bypassing theneed for K-means clustering per iteration. Extensive numerical tests onsynthetic and real data sets highlight the potential of the proposedalgorithms, and demonstrate their competitive performance relative tostate-of-the-art random projection alternatives.
arxiv-1501-05684 | Bi-Objective Nonnegative Matrix Factorization: Linear Versus Kernel-Based Models |  http://arxiv.org/abs/1501.05684  | author:Paul Honeine, Fei Zhu category:stat.ML cs.CV cs.LG math.OC published:2015-01-22 summary:Nonnegative matrix factorization (NMF) is a powerful class of featureextraction techniques that has been successfully applied in many fields, namelyin signal and image processing. Current NMF techniques have been limited to asingle-objective problem in either its linear or nonlinear kernel-basedformulation. In this paper, we propose to revisit the NMF as a multi-objectiveproblem, in particular a bi-objective one, where the objective functionsdefined in both input and feature spaces are taken into account. By taking theadvantage of the sum-weighted method from the literature of multi-objectiveoptimization, the proposed bi-objective NMF determines a set of nondominated,Pareto optimal, solutions instead of a single optimal decomposition. Moreover,the corresponding Pareto front is studied and approximated. Experimentalresults on unmixing real hyperspectral images confirm the efficiency of theproposed bi-objective NMF compared with the state-of-the-art methods.
arxiv-1501-05192 | A Graph Theoretic Approach for Object Shape Representation in Compositional Hierarchies Using a Hybrid Generative-Descriptive Model |  http://arxiv.org/abs/1501.05192  | author:Umit Rusen Aktas, Mete Ozay, Ales Leonardis, Jeremy L. Wyatt category:cs.CV published:2015-01-21 summary:A graph theoretic approach is proposed for object shape representation in ahierarchical compositional architecture called Compositional Hierarchy of Parts(CHOP). In the proposed approach, vocabulary learning is performed using ahybrid generative-descriptive model. First, statistical relationships betweenparts are learned using a Minimum Conditional Entropy Clustering algorithm.Then, selection of descriptive parts is defined as a frequent subgraphdiscovery problem, and solved using a Minimum Description Length (MDL)principle. Finally, part compositions are constructed by compressing theinternal data representation with discovered substructures. Shaperepresentation and computational complexity properties of the proposed approachand algorithms are examined using six benchmark two-dimensional shape imagedatasets. Experiments show that CHOP can employ part shareability and indexingmechanisms for fast inference of part compositions using learned shapevocabularies. Additionally, CHOP provides better shape retrieval performancethan the state-of-the-art shape retrieval methods.
arxiv-1501-05222 | Plug-and-play dual-tree algorithm runtime analysis |  http://arxiv.org/abs/1501.05222  | author:Ryan R. Curtin, Dongryeol Lee, William B. March, Parikshit Ram category:cs.DS cs.LG published:2015-01-21 summary:Numerous machine learning algorithms contain pairwise statistical problems attheir core---that is, tasks that require computations over all pairs of inputpoints if implemented naively. Often, tree structures are used to solve theseproblems efficiently. Dual-tree algorithms can efficiently solve or approximatemany of these problems. Using cover trees, rigorous worst-case runtimeguarantees have been proven for some of these algorithms. In this paper, wepresent a problem-independent runtime guarantee for any dual-tree algorithmusing the cover tree, separating out the problem-dependent and theproblem-independent elements. This allows us to just plug in bounds for theproblem-dependent elements to get runtime guarantees for dual-tree algorithmsfor any pairwise statistical problem without re-deriving the entire proof. Wedemonstrate this plug-and-play procedure for nearest-neighbor search andapproximate kernel density estimation to get improved runtime guarantees. Undermild assumptions, we also present the first linear runtime guarantee fordual-tree based range search.
arxiv-1501-05144 | Lazier ABC |  http://arxiv.org/abs/1501.05144  | author:Dennis Prangle category:stat.CO stat.ML published:2015-01-21 summary:ABC algorithms involve a large number of simulations from the model ofinterest, which can be very computationally costly. This paper summarises thelazy ABC algorithm of Prangle (2015), which reduces the computational demand byabandoning many unpromising simulations before completion. By using a randomstopping decision and reweighting the output sample appropriately, the targetdistribution is the same as for standard ABC. Lazy ABC is also extended here tothe case of non-uniform ABC kernels, which is shown to simplify the process oftuning the algorithm effectively.
arxiv-1501-05203 | Phrase Based Language Model for Statistical Machine Translation: Empirical Study |  http://arxiv.org/abs/1501.05203  | author:Geliang Chen category:cs.CL published:2015-01-21 summary:Reordering is a challenge to machine translation (MT) systems. In MT, thewidely used approach is to apply word based language model (LM) which considersthe constituent units of a sentence as words. In speech recognition (SR), somephrase based LM have been proposed. However, those LMs are not necessarilysuitable or optimal for reordering. We propose two phrase based LMs whichconsiders the constituent units of a sentence as phrases. Experiments show thatour phrase based LMs outperform the word based LM with the respect ofperplexity and n-best list re-ranking.
arxiv-1502-07743 | Tracking an Object with Unknown Accelerations using a Shadowing Filter |  http://arxiv.org/abs/1502.07743  | author:Kevin Judd category:cs.SY cs.CV math.OC published:2015-01-21 summary:A commonly encountered problem is the tracking of a physical object, like amaneuvering ship, aircraft, land vehicle, spacecraft or animate creaturecarrying a wireless device. The sensor data is often limited and inaccurateobservations of range or bearing. This problem is more difficult than trackinga ballistic trajectory, because an operative affects unknown and arbitrarilychanging accelerations. Although stochastic methods of filtering or stateestimation (Kalman filters and particle filters) are widely used, out of voguevariational methods are more appropriate in this tracking context, because theobjects do not typically display any significant random motions at the lengthand time scales of interest. This leads us to propose a rather elegant approachbased on a \emph{shadowing filter}. The resulting filter is efficient (reducesto the solution of linear equations) and robust (uneffected by missing data andsingular correlations that would cause catastrophic failure of Bayesianfilters.) The tracking is so robust, that in some common situations it actuallyperforms better by ignoring error correlations that are so vital to Kalmanfilters.
arxiv-1501-05069 | Convergent Bayesian formulations of blind source separation and electromagnetic source estimation |  http://arxiv.org/abs/1501.05069  | author:Kevin H. Knuth, Herbert G. Vaughan Jr category:physics.med-ph stat.ML published:2015-01-21 summary:We consider two areas of research that have been developing in parallel overthe last decade: blind source separation (BSS) and electromagnetic sourceestimation (ESE). BSS deals with the recovery of source signals when onlymixtures of signals can be obtained from an array of detectors and the onlyprior knowledge consists of some information about the nature of the sourcesignals. On the other hand, ESE utilizes knowledge of the electromagneticforward problem to assign source signals to their respective generators, whileinformation about the signals themselves is typically ignored. We demonstratethat these two techniques can be derived from the same starting point using theBayesian formalism. This suggests a means by which new algorithms can bedeveloped that utilize as much relevant information as possible. We alsobriefly mention some preliminary work that supports the value of integratinginformation used by these two techniques and review the kinds of informationthat may be useful in addressing the ESE problem.
arxiv-1501-05141 | An Algebra to Merge Heterogeneous Classifiers |  http://arxiv.org/abs/1501.05141  | author:Philippe J. Giabbanelli, Joseph G. Peters category:cs.DM cs.LG 97R50, 08Axx published:2015-01-21 summary:In distributed classification, each learner observes its environment anddeduces a classifier. As a learner has only a local view of its environment,classifiers can be exchanged among the learners and integrated, or merged, toimprove accuracy. However, the operation of merging is not defined for mostclassifiers. Furthermore, the classifiers that have to be merged may be ofdifferent types in settings such as ad-hoc networks in which severalgenerations of sensors may be creating classifiers. We introduce decisionspaces as a framework for merging possibly different classifiers. We formallystudy the merging operation as an algebra, and prove that it satisfies adesirable set of properties. The impact of time is discussed for the two maindata mining settings. Firstly, decision spaces can naturally be used withnon-stationary distributions, such as the data collected by sensor networks, asthe impact of a model decays over time. Secondly, we introduce an approach forstationary distributions, such as homogeneous databases partitioned overdifferent learners, which ensures that all models have the same impact. We alsopresent a method that uses storage flexibly to achieve different types of decayfor non-stationary distributions. Finally, we show that the algebraic approachdeveloped for merging can also be used to analyze the behaviour of otheroperators.
arxiv-1501-05200 | Minimax Optimal Sparse Signal Recovery with Poisson Statistics |  http://arxiv.org/abs/1501.05200  | author:Mohammad H. Rohban, Delaram Motamedvaziri, Venkatesh Saligrama category:stat.ML published:2015-01-21 summary:We are motivated by problems that arise in a number of applications such asOnline Marketing and Explosives detection, where the observations are usuallymodeled using Poisson statistics. We model each observation as a Poisson randomvariable whose mean is a sparse linear superposition of known patterns. Unlikemany conventional problems observations here are not identically distributedsince they are associated with different sensing modalities. We analyze theperformance of a Maximum Likelihood (ML) decoder, which for our Poisson settinginvolves a non-linear optimization but yet is computationally tractable. Wederive fundamental sample complexity bounds for sparse recovery when themeasurements are contaminated with Poisson noise. In contrast to theleast-squares linear regression setting with Gaussian noise, we observe that inaddition to sparsity, the scale of the parameters also fundamentally impacts$\ell_2$ error in the Poisson setting. We show tightness of our upper boundsboth theoretically and experimentally. In particular, we derive a minimaxmatching lower bound on the mean-squared error and show that our constrained MLdecoder is minimax optimal for this regime.
arxiv-1501-05349 | Exploiting Big Data in Logistics Risk Assessment via Bayesian Nonparametrics |  http://arxiv.org/abs/1501.05349  | author:Yan Shang, David B. Dunson, Jing-Sheng Song category:stat.AP stat.ML 62-07 published:2015-01-21 summary:In cargo logistics, a key performance measure is transport risk, defined asthe deviation of the actual arrival time from the planned arrival time. Neitherearliness nor tardiness is desirable for customer and freight forwarders. Inthis paper, we investigate ways to assess and forecast transport risks using ahalf-year of air cargo data, provided by a leading forwarder on 1336 routesserved by 20 airlines. Interestingly, our preliminary data analysis shows astrong multimodal feature in the transport risks, driven by unobserved events,such as cargo missing flights. To accommodate this feature, we introduce aBayesian nonparametric model -- the probit stick-breaking process (PSBP)mixture model -- for flexible estimation of the conditional (i.e.,state-dependent) density function of transport risk. We demonstrate that usingsimpler methods, such as OLS linear regression, can lead to misleadinginferences. Our model provides a tool for the forwarder to offer customizedprice and service quotes. It can also generate baseline airline performance toenable fair supplier evaluation. Furthermore, the method allows us to separaterecurrent risks from disruption risks. This is important, because hedgingstrategies for these two kinds of risks are often drastically different.
arxiv-1501-05068 | Difficulties applying recent blind source separation techniques to EEG and MEG |  http://arxiv.org/abs/1501.05068  | author:Kevin H. Knuth category:physics.med-ph stat.ML published:2015-01-21 summary:High temporal resolution measurements of human brain activity can beperformed by recording the electric potentials on the scalp surface(electroencephalography, EEG), or by recording the magnetic fields near thesurface of the head (magnetoencephalography, MEG). The analysis of the data isproblematic due to the fact that multiple neural generators may besimultaneously active and the potentials and magnetic fields from these sourcesare superimposed on the detectors. It is highly desirable to un-mix the datainto signals representing the behaviors of the original individual generators.This general problem is called blind source separation and several recenttechniques utilizing maximum entropy, minimum mutual information, and maximumlikelihood estimation have been applied. These techniques have had much successin separating signals such as natural sounds or speech, but appear to beineffective when applied to EEG or MEG signals. Many of these techniquesimplicitly assume that the source distributions have a large kurtosis, whereasan analysis of EEG/MEG signals reveals that the distributions are multimodal.This suggests that more effective separation techniques could be designed forEEG and MEG signals.
arxiv-1501-05152 | Mirror, mirror on the wall, tell me, is the error small? |  http://arxiv.org/abs/1501.05152  | author:Heng Yang, Ioannis Patras category:cs.CV published:2015-01-21 summary:Do object part localization methods produce bilaterally symmetric results onmirror images? Surprisingly not, even though state of the art methods augmentthe training set with mirrored images. In this paper we take a closer look intothis issue. We first introduce the concept of mirrorability as the ability of amodel to produce symmetric results in mirrored images and introduce acorresponding measure, namely the \textit{mirror error} that is defined as thedifference between the detection result on an image and the mirror of thedetection result on its mirror image. We evaluate the mirrorability of severalstate of the art algorithms in two of the most intensively studied problems,namely human pose estimation and face alignment. Our experiments lead toseveral interesting findings: 1) Surprisingly, most of state of the art methodsstruggle to preserve the mirror symmetry, despite the fact that they do havevery similar overall performance on the original and mirror images; 2) the lowmirrorability is not caused by training or testing sample bias - all algorithmsare trained on both the original images and their mirrored versions; 3) themirror error is strongly correlated to the localization/alignment error (withcorrelation coefficients around 0.7). Since the mirror error is calculatedwithout knowledge of the ground truth, we show two interesting applications -in the first it is used to guide the selection of difficult samples and in thesecond to give feedback in a popular Cascaded Pose Regression method for facealignment.
arxiv-1501-05279 | Extreme Entropy Machines: Robust information theoretic classification |  http://arxiv.org/abs/1501.05279  | author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG published:2015-01-21 summary:Most of the existing classification methods are aimed at minimization ofempirical risk (through some simple point-based error measured with lossfunction) with added regularization. We propose to approach this problem in amore information theoretic way by investigating applicability of entropymeasures as a classification model objective function. We focus on quadraticRenyi's entropy and connected Cauchy-Schwarz Divergence which leads to theconstruction of Extreme Entropy Machines (EEM). The main contribution of this paper is proposing a model based on theinformation theoretic concepts which on the one hand shows new, entropicperspective on known linear classifiers and on the other leads to aconstruction of very robust method competetitive with the state of the artnon-information theoretic ones (including Support Vector Machines and ExtremeLearning Machines). Evaluation on numerous problems spanning from small, simple ones from UCIrepository to the large (hundreads of thousands of samples) extremelyunbalanced (up to 100:1 classes' ratios) datasets shows wide applicability ofthe EEM in real life problems and that it scales well.
arxiv-1501-05194 | A Bayesian alternative to mutual information for the hierarchical clustering of dependent random variables |  http://arxiv.org/abs/1501.05194  | author:Guillaume Marrelec, Arnaud Messé, Pierre Bellec category:stat.ML cs.LG q-bio.QM published:2015-01-21 summary:The use of mutual information as a similarity measure in agglomerativehierarchical clustering (AHC) raises an important issue: some correction needsto be applied for the dimensionality of variables. In this work, we formulatethe decision of merging dependent multivariate normal variables in an AHCprocedure as a Bayesian model comparison. We found that the Bayesianformulation naturally shrinks the empirical covariance matrix towards a matrixset a priori (e.g., the identity), provides an automated stopping rule, andcorrects for dimensionality using a term that scales up the measure as afunction of the dimensionality of the variables. Also, the resulting log Bayesfactor is asymptotically proportional to the plug-in estimate of mutualinformation, with an additive correction for dimensionality in agreement withthe Bayesian information criterion. We investigated the behavior of theseBayesian alternatives (in exact and asymptotic forms) to mutual information onsimulated and real data. An encouraging result was first derived onsimulations: the hierarchical clustering based on the log Bayes factoroutperformed off-the-shelf clustering techniques as well as raw and normalizedmutual information in terms of classification accuracy. On a toy example, wefound that the Bayesian approaches led to results that were similar to those ofmutual information clustering techniques, with the advantage of an automatedthresholding. On real functional magnetic resonance imaging (fMRI) datasetsmeasuring brain activity, it identified clusters consistent with theestablished outcome of standard procedures. On this application, normalizedmutual information had a highly atypical behavior, in the sense that itsystematically favored very large clusters. These initial experiments suggestthat the proposed Bayesian alternatives to mutual information are a useful newtool for hierarchical clustering.
arxiv-1501-05352 | Optimizing affinity-based binary hashing using auxiliary coordinates |  http://arxiv.org/abs/1501.05352  | author:Ramin Raziperchikolaei, Miguel Á. Carreira-Perpiñán category:cs.LG cs.CV math.OC stat.ML published:2015-01-21 summary:In supervised binary hashing, one wants to learn a function that maps ahigh-dimensional feature vector to a vector of binary codes, for application tofast image retrieval. This typically results in a difficult optimizationproblem, nonconvex and nonsmooth, because of the discrete variables involved.Much work has simply relaxed the problem during training, solving a continuousoptimization, and truncating the codes a posteriori. This gives reasonableresults but is quite suboptimal. Recent work has tried to optimize theobjective directly over the binary codes and achieved better results, but thehash function was still learned a posteriori, which remains suboptimal. Wepropose a general framework for learning hash functions using affinity-basedloss functions that uses auxiliary coordinates. This closes the loop andoptimizes jointly over the hash functions and the binary codes so that theygradually match each other. The resulting algorithm can be seen as a corrected,iterated version of the procedure of optimizing first over the codes and thenlearning the hash function. Compared to this, our optimization is guaranteed toobtain better hash functions while being not much slower, as demonstratedexperimentally in various supervised datasets. In addition, our frameworkfacilitates the design of optimization algorithms for arbitrary types of lossand hash functions.
arxiv-1501-05108 | BDgraph: An R Package for Bayesian Structure Learning in Graphical Models |  http://arxiv.org/abs/1501.05108  | author:Abdolreza Mohammadi, Ernst C. Wit category:stat.ML published:2015-01-21 summary:Graphical models provide powerful tools to uncover complicated patterns inmultivariate data and are commonly used in Bayesian statistics and machinelearning. In this paper, we introduce an R package BDgraph which performsBayesian structure learning for general undirected graphical models with eithercontinuous or discrete variables. The package efficiently implements recentimprovements in the Bayesian literature. To speed up computations, thecomputationally intensive tasks have been implemented in C++ and interfacedwith R. In addition, the package contains several functions for simulation andvisualization, as well as two multivariate datasets taken from the literatureand are used to describe the package capabilities. The paper includes a briefoverview of the statistical methods which have been implemented in the package.The main body of the paper explains how to use the package. Furthermore, weillustrate the package's functionality in both real and artificial examples, aswell as in an extensive simulation study.
arxiv-1501-04725 | Learning Invariants using Decision Trees |  http://arxiv.org/abs/1501.04725  | author:Siddharth Krishna, Christian Puhrsch, Thomas Wies category:cs.PL cs.LG published:2015-01-20 summary:The problem of inferring an inductive invariant for verifying program safetycan be formulated in terms of binary classification. This is a standard problemin machine learning: given a sample of good and bad points, one is asked tofind a classifier that generalizes from the sample and separates the two sets.Here, the good points are the reachable states of the program, and the badpoints are those that reach a safety property violation. Thus, a learnedclassifier is a candidate invariant. In this paper, we propose a new algorithmthat uses decision trees to learn candidate invariants in the form of arbitraryBoolean combinations of numerical inequalities. We have used our algorithm toverify C programs taken from the literature. The algorithm is able to infersafe invariants for a range of challenging benchmarks and compares favorably toother ML-based invariant inference techniques. In particular, it scales well tolarge sample sets.
arxiv-1501-04690 | Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not? |  http://arxiv.org/abs/1501.04690  | author:Erjin Zhou, Zhimin Cao, Qi Yin category:cs.CV published:2015-01-20 summary:Face recognition performance improves rapidly with the recent deep learningtechnique developing and underlying large training dataset accumulating. Inthis paper, we report our observations on how big data impacts the recognitionperformance. According to these observations, we build our Megvii FaceRecognition System, which achieves 99.50% accuracy on the LFW benchmark,outperforming the previous state-of-the-art. Furthermore, we report theperformance in a real-world security certification scenario. There still existsa clear gap between machine recognition and human performance. We summarize ourexperiments and present three challenges lying ahead in recent facerecognition. And we indicate several possible solutions towards thesechallenges. We hope our work will stimulate the community's discussion of thedifference between research benchmark and real-world applications.
arxiv-1501-04878 | A Light Transport Model for Mitigating Multipath Interference in TOF Sensors |  http://arxiv.org/abs/1501.04878  | author:Nikhil Naik, Achuta Kadambi, Christoph Rhemann, Shahram Izadi, Ramesh Raskar, Sing Bing Kang category:cs.CV published:2015-01-20 summary:Continuous-wave Time-of-flight (TOF) range imaging has become a commerciallyviable technology with many applications in computer vision and graphics.However, the depth images obtained from TOF cameras contain scene dependenterrors due to multipath interference (MPI). Specifically, MPI occurs whenmultiple optical reflections return to a single spatial location on the imagingsensor. Many prior approaches to rectifying MPI rely on sparsity in opticalreflections, which is an extreme simplification. In this paper, we correct MPIby combining the standard measurements from a TOF camera with information fromdirect and global light transport. We report results on both simulatedexperiments and physical experiments (using the Kinect sensor). Our results,evaluated against ground truth, demonstrate a quantitative improvement in depthaccuracy.
arxiv-1501-04686 | Deep Convolutional Neural Networks for Action Recognition Using Depth Map Sequences |  http://arxiv.org/abs/1501.04686  | author:Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, Philip Ogunbona category:cs.CV published:2015-01-20 summary:Recently, deep learning approach has achieved promising results in variousfields of computer vision. In this paper, a new framework called HierarchicalDepth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks(3ConvNets) is proposed for human action recognition using depth map sequences.Firstly, we rotate the original depth data in 3D pointclouds to mimic therotation of cameras, so that our algorithms can handle view variant cases.Secondly, in order to effectively extract the body shape and motioninformation, we generate weighted depth motion maps (DMM) at several temporalscales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, threechannels of ConvNets are trained on the HDMMs from three projected orthogonalplanes separately. The proposed algorithms are evaluated on MSRAction3D,MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively.We also combine the last three datasets into a larger one (called CombinedDataset) and test the proposed method on it. The results show that our approachcan achieve state-of-the-art results on the individual datasets and withoutdramatical performance degradation on the Combined Dataset.
arxiv-1501-04711 | DeepHash: Getting Regularization, Depth and Fine-Tuning Right |  http://arxiv.org/abs/1501.04711  | author:Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine Veillard, Hanlin Goh category:cs.CV cs.IR published:2015-01-20 summary:This work focuses on representing very high-dimensional global imagedescriptors using very compact 64-1024 bit binary hashes for instanceretrieval. We propose DeepHash: a hashing scheme based on deep networks. Key tomaking DeepHash work at extremely low bitrates are three importantconsiderations -- regularization, depth and fine-tuning -- each requiringsolutions specific to the hashing problem. In-depth evaluation shows that ourscheme consistently outperforms state-of-the-art methods across all data setsfor both Fisher Vectors and Deep Convolutional Neural Network features, by upto 20 percent over other schemes. The retrieval performance with 256-bit hashesis close to that of the uncompressed floating point features -- a remarkable512 times compression.
arxiv-1501-04717 | Robust Face Recognition by Constrained Part-based Alignment |  http://arxiv.org/abs/1501.04717  | author:Yuting Zhang, Kui Jia, Yueming Wang, Gang Pan, Tsung-Han Chan, Yi Ma category:cs.CV cs.LG published:2015-01-20 summary:Developing a reliable and practical face recognition system is along-standing goal in computer vision research. Existing literature suggeststhat pixel-wise face alignment is the key to achieve high-accuracy facerecognition. By assuming a human face as piece-wise planar surfaces, where eachsurface corresponds to a facial part, we develop in this paper a ConstrainedPart-based Alignment (CPA) algorithm for face recognition across pose and/orexpression. Our proposed algorithm is based on a trainable CPA model, whichlearns appearance evidence of individual parts and a tree-structured shapeconfiguration among different parts. Given a probe face, CPA simultaneouslyaligns all its parts by fitting them to the appearance evidence withconsideration of the constraint from the tree-structured shape configuration.This objective is formulated as a norm minimization problem regularized bygraph likelihoods. CPA can be easily integrated with many existing classifiersto perform part-based face recognition. Extensive experiments on benchmark facedatasets show that CPA outperforms or is on par with existing methods forrobust face recognition across pose, expression, and/or illumination changes.
arxiv-1501-04870 | Scalable Multi-Output Label Prediction: From Classifier Chains to Classifier Trellises |  http://arxiv.org/abs/1501.04870  | author:J. Read, L. Martino, P. Olmos, D. Luengo category:stat.ML cs.CV cs.DS cs.LG stat.CO published:2015-01-20 summary:Multi-output inference tasks, such as multi-label classification, have becomeincreasingly important in recent years. A popular method for multi-labelclassification is classifier chains, in which the predictions of individualclassifiers are cascaded along a chain, thus taking into account inter-labeldependencies and improving the overall performance. Several varieties ofclassifier chain methods have been introduced, and many of them perform verycompetitively across a wide range of benchmark datasets. However, scalabilitylimitations become apparent on larger datasets when modeling a fully-cascadedchain. In particular, the methods' strategies for discovering and modeling agood chain structure constitutes a mayor computational bottleneck. In thispaper, we present the classifier trellis (CT) method for scalable multi-labelclassification. We compare CT with several recently proposed classifier chainmethods to show that it occupies an important niche: it is highly competitiveon standard multi-label problems, yet it can also scale up to thousands or eventens of thousands of labels.
arxiv-1501-04819 | Separation of undersampled composite signals using the Dantzig selector with overcomplete dictionaries |  http://arxiv.org/abs/1501.04819  | author:Ashley Prater, Lixin Shen category:math.NA stat.ML published:2015-01-20 summary:In many applications one may acquire a composition of several signals thatmay be corrupted by noise, and it is a challenging problem to reliably separatethe components from one another without sacrificing significant details. Addingto the challenge, in a compressive sensing framework, one is given only anundersampled set of linear projections of the composite signal. In this paper,we propose using the Dantzig selector model incorporating an overcompletedictionary to separate a noisy undersampled collection of composite signals,and present an algorithm to efficiently solve the model. The Dantzig selector is a statistical approach to finding a solution to anoisy linear regression problem by minimizing the $\ell_1$ norm of candidatecoefficient vectors while constraining the scope of the residuals. If theunderlying coefficient vector is sparse, then the Dantzig selector performswell in the recovery and separation of the unknown composite signal. In thefollowing, we propose a proximity operator based algorithm to recover andseparate unknown noisy undersampled composite signals through the Dantzigselector. We present numerical simulations comparing the proposed algorithmwith the competing Alternating Direction Method, and the proposed algorithm isfound to be faster, while producing similar quality results. Additionally, wedemonstrate the utility of the proposed algorithm in several experiments byapplying it in various domain applications including the recovery ofcomplex-valued coefficient vectors, the removal of impulse noise from smoothsignals, and the separation and classification of a composition of handwrittendigits.
arxiv-1501-04691 | Tracing the boundaries of materials in transparent vessels using computer vision |  http://arxiv.org/abs/1501.04691  | author:Sagi Eppel category:cs.CV published:2015-01-20 summary:Visual recognition of material boundaries in transparent vessels is valuablefor numerous applications. Such recognition is essential for estimation offill-level, volume and phase-boundaries as well as for tracking of suchchemical processes as precipitation, crystallization, condensation, evaporationand phase-separation. The problem of material boundary recognition in images isparticularly complex for materials with non-flat surfaces, i.e., solids,powders and viscous fluids, in which the material interfaces have unpredictableshapes. This work demonstrates a general method for finding the boundaries ofmaterials inside transparent containers in images. The method uses an image ofthe transparent vessel containing the material and the boundary of the vesselin this image. The recognition is based on the assumption that the materialboundary appears in the image in the form of a curve (with various constraints)whose endpoints are both positioned on the vessel contour. The probability thata curve matches the material boundary in the image is evaluated using a costfunction based on some image properties along this curve. Several imageproperties were examined as indicators for the material boundary. The optimalboundary curve was found using Dijkstra's algorithm. The method wassuccessfully examined for recognition of various types of phase-boundaries,including liquid-air, solid-air and solid-liquid interfaces, as well as forvarious types of glassware containers from everyday life and the chemistrylaboratory (i.e., bottles, beakers, flasks, jars, columns, vials andseparation-funnels). In addition, the method can be easily extended tomaterials carried on top of carrier vessels (i.e., plates, spoons, spatulas).
arxiv-1501-04826 | Entailment Among Probabilistic Implications |  http://arxiv.org/abs/1501.04826  | author:Albert Atserias, José L. Balcázar category:cs.LO cs.DB cs.LG published:2015-01-20 summary:We study a natural variant of the implicational fragment of propositionallogic. Its formulas are pairs of conjunctions of positive literals, relatedtogether by an implicational-like connective; the semantics of this sort ofimplication is defined in terms of a threshold on a conditional probability ofthe consequent, given the antecedent: we are dealing with what the dataanalysis community calls confidence of partial implications or associationrules. Existing studies of redundancy among these partial implications havecharacterized so far only entailment from one premise and entailment from twopremises. By exploiting a previously noted alternative view of this entailmentin terms of linear programming duality, we characterize exactly the cases ofentailment from arbitrary numbers of premises. As a result, we obtain decisionalgorithms of better complexity; additionally, for each potential case ofentailment, we identify a critical confidence threshold and show that it is,actually, intrinsic to each set of premises and antecedent of the conclusion.
arxiv-1501-04920 | Regroupement sémantique de définitions en espagnol |  http://arxiv.org/abs/1501.04920  | author:Gerardo Sierra, Juan-Manuel Torres-Moreno, Alejandro Molina category:cs.IR cs.CL published:2015-01-20 summary:This article focuses on the description and evaluation of a new unsupervisedlearning method of clustering of definitions in Spanish according to theirsemantic. Textual Energy was used as a clustering measure, and we study anadaptation of the Precision and Recall to evaluate our method.
arxiv-1501-04782 | Constructing Binary Descriptors with a Stochastic Hill Climbing Search |  http://arxiv.org/abs/1501.04782  | author:Nenad Markuš, Igor S. Pandžić, Jörgen Ahlberg category:cs.CV published:2015-01-20 summary:Binary descriptors of image patches provide processing speed advantages andrequire less storage than methods that encode the patch appearance with avector of real numbers. We provide evidence that, despite its simplicity, astochastic hill climbing bit selection procedure for descriptor constructiondefeats recently proposed alternatives on a standard discriminative powerbenchmark. The method is easy to implement and understand, has no freeparameters that need fine tuning, and runs fast.
arxiv-1501-04754 | Distributed Data Association in Smart Camera Networks via Dual Decomposition |  http://arxiv.org/abs/1501.04754  | author:Jiuqing Wan, Yuting Nie, Li Liu category:cs.CV published:2015-01-20 summary:One of the fundamental requirements for visual surveillance using smartcamera networks is the correct association of each persons observationsgenerated on different cameras. Recently, distributed data association thatinvolves only local information processing on each camera node and mutualinformation exchanging between neighboring cameras has attracted many researchinterests due to its superiority in large scale applications. In this paper, weformulate the problem of data association in smart camera networks as anInteger Programming problem by introducing a set of linking variables, andpropose two distributed algorithms, namely L-DD and Q-DD, to solve the IntegerProgramming problem using dual decomposition technique. In our algorithms, theoriginal IP problem is decomposed into several sub-problems, which can besolved locally and efficiently on each smart camera, and then differentsub-problems reach consensus on their solutions in a rigorous way by adjustingtheir parameters based on projected sub-gradient optimization. The proposedmethods are simple and flexible, in that (i) we can incorporate any featureextraction and matching technique into our framework to measure the similaritybetween two observations, which is used to define the cost of each link, and(ii) we can decompose the original problem in any way as long as the resultingsub-problem can be solved independently on individual camera. We show thecompetitiveness of our methods in both accuracy and speed by theoreticalanalysis and experimental comparison with state of the art algorithms on tworeal data sets collected by camera networks in our campus garden and officebuilding.
arxiv-1501-04413 | Statistical-mechanical analysis of pre-training and fine tuning in deep learning |  http://arxiv.org/abs/1501.04413  | author:Masayuki Ohzeki category:stat.ML cs.AI cs.LG published:2015-01-19 summary:In this paper, we present a statistical-mechanical analysis of deep learning.We elucidate some of the essential components of deep learning---pre-trainingby unsupervised learning and fine tuning by supervised learning. We formulatethe extraction of features from the training data as a margin criterion in ahigh-dimensional feature-vector space. The self-organized classifier is thensupplied with small amounts of labelled data, as in deep learning. Although weemploy a simple single-layer perceptron model, rather than directly analyzing amulti-layer neural network, we find a nontrivial phase transition that isdependent on the number of unlabelled data in the generalization error of theresultant classifier. In this sense, we evaluate the efficacy of theunsupervised learning component of deep learning. The analysis is performed bythe replica method, which is a sophisticated tool in statistical mechanics. Wevalidate our result in the manner of deep learning, using a simple iterativealgorithm to learn the weight vector on the basis of belief propagation.
arxiv-1501-04621 | Sparse Bayesian Learning for EEG Source Localization |  http://arxiv.org/abs/1501.04621  | author:Sajib Saha, Frank de Hoog, Ya. I. Nesterets, Rajib Rana, M. Tahtali, T. E. Gureyev category:q-bio.QM cs.LG q-bio.NC published:2015-01-19 summary:Purpose: Localizing the sources of electrical activity fromelectroencephalographic (EEG) data has gained considerable attention over thelast few years. In this paper, we propose an innovative source localizationmethod for EEG, based on Sparse Bayesian Learning (SBL). Methods: To betterspecify the sparsity profile and to ensure efficient source localization, theproposed approach considers grouping of the electrical current dipoles insidehuman brain. SBL is used to solve the localization problem in addition withimposed constraint that the electric current dipoles associated with the brainactivity are isotropic. Results: Numerical experiments are conducted on arealistic head model that is obtained by segmentation of MRI images of the headand includes four major components, namely the scalp, the skull, thecerebrospinal fluid (CSF) and the brain, with appropriate relative conductivityvalues. The results demonstrate that the isotropy constraint significantlyimproves the performance of SBL. In a noiseless environment, the proposedmethod was 1 found to accurately (with accuracy of >75%) locate up to 6simultaneously active sources, whereas for SBL without the isotropy constraint,the accuracy of finding just 3 simultaneously active sources was <75%.Conclusions: Compared to the state-of-the-art algorithms, the proposed methodis potentially more consistent in specifying the sparsity profile of humanbrain activity and is able to produce better source localization for EEG.
arxiv-1501-04659 | On the impact of topological properties of smart grids in power losses optimization problems |  http://arxiv.org/abs/1501.04659  | author:Francesca Possemato, Maurizio Paschero, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.CE cs.NE published:2015-01-19 summary:Power losses reduction is one of the main targets for any electrical energydistribution company. In this paper, we face the problem of joint optimizationof both topology and network parameters in a real smart grid. We consider aportion of the Italian electric distribution network managed by the ACEADistribuzione S.p.A. located in Rome. We perform both the power factorcorrection (PFC) for tuning the generators and the distributed feederreconfiguration (DFR) to set the state of the breakers. This joint optimizationproblem is faced considering a suitable objective function and by adoptinggenetic algorithms as global optimization strategy. We analyze admissiblenetwork configurations, showing that some of these violate constraints oncurrent and voltage at branches and nodes. Such violations depend only on puretopological properties of the configurations. We perform tests by feeding thesimulation environment with real data concerning hourly samples of dissipatedand generated active and reactive power values of the ACEA smart grid. Resultsshow that removing the configurations violating the electrical constraints fromthe solution space leads to interesting improvements in terms of power lossreduction. To conclude, we provide also an electrical interpretation of thephenomenon using graph-based pattern analysis techniques.
arxiv-1501-04378 | Instance Significance Guided Multiple Instance Boosting for Robust Visual Tracking |  http://arxiv.org/abs/1501.04378  | author:Jinwu Liu, Yao Lu, Tianfei Zhou category:cs.CV published:2015-01-19 summary:Multiple Instance Learning (MIL) recently provides an appealing way toalleviate the drifting problem in visual tracking. Following thetracking-by-detection framework, an online MILBoost approach is developed thatsequentially chooses weak classifiers by maximizing the bag likelihood. In thispaper, we extend this idea towards incorporating the instance significanceestimation into the online MILBoost framework. First, instead of treating allinstances equally, with each instance we associate a significance-coefficientthat represents its contribution to the bag likelihood. The coefficients areestimated by a simple Bayesian formula that jointly considers the predictionsfrom several standard MILBoost classifiers. Next, we follow the online boostingframework, and propose a new criterion for the selection of weak classifiers.Experiments with challenging public datasets show that the proposed methodoutperforms both existing MIL based and boosting based trackers.
arxiv-1501-04656 | Microscopic Advances with Large-Scale Learning: Stochastic Optimization for Cryo-EM |  http://arxiv.org/abs/1501.04656  | author:Ali Punjani, Marcus A. Brubaker category:stat.ML cs.CV cs.LG q-bio.QM published:2015-01-19 summary:Determining the 3D structures of biological molecules is a key problem forboth biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promisingtechnique for structure estimation which relies heavily on computationalmethods to reconstruct 3D structures from 2D images. This paper introduces thechallenging Cryo-EM density estimation problem as a novel application forstochastic optimization techniques. Structure discovery is formulated as MAPestimation in a probabilistic latent-variable model, resulting in anoptimization problem to which an array of seven stochastic optimization methodsare applied. The methods are tested on both real and synthetic data, with somemethods recovering reasonable structures in less than one epoch from a randominitialization. Complex quasi-Newton methods are found to converge more slowlythan simple gradient-based methods, but all stochastic methods are found toconverge to similar optima. This method represents a major improvement overexisting methods as it is significantly faster and is able to converge from arandom initialization.
arxiv-1501-04537 | Coupled Depth Learning |  http://arxiv.org/abs/1501.04537  | author:Mohammad Haris Baig, Lorenzo Torresani category:cs.CV published:2015-01-19 summary:In this paper we propose a method for estimating depth from a single imageusing a coarse to fine approach. We argue that modeling the fine depth detailsis easier after a coarse depth map has been computed. We express a global(coarse) depth map of an image as a linear combination of a depth basis learnedfrom training examples. The depth basis captures spatial and statisticalregularities and reduces the problem of global depth estimation to the task ofpredicting the input-specific coefficients in the linear combination. This isformulated as a regression problem from a holistic representation of the image.Crucially, the depth basis and the regression function are {\bf coupled} andjointly optimized by our learning scheme. We demonstrate that this results in asignificant improvement in accuracy compared to direct regression of depthpixel values or approaches learning the depth basis disjointly from theregression function. The global depth estimate is then used as a guidance by alocal refinement method that introduces depth details that were not captured atthe global level. Experiments on the NYUv2 and KITTI datasets show that ourmethod outperforms the existing state-of-the-art at a considerably lowercomputational cost for both training and testing.
arxiv-1501-04587 | Transferring Rich Feature Hierarchies for Robust Visual Tracking |  http://arxiv.org/abs/1501.04587  | author:Naiyan Wang, Siyi Li, Abhinav Gupta, Dit-Yan Yeung category:cs.CV cs.NE published:2015-01-19 summary:Convolutional neural network (CNN) models have demonstrated great success invarious computer vision tasks including image classification and objectdetection. However, some equally important tasks such as visual tracking remainrelatively unexplored. We believe that a major hurdle that hinders theapplication of CNN to visual tracking is the lack of properly labeled trainingdata. While existing applications that liberate the power of CNN often need anenormous amount of training data in the order of millions, visual trackingapplications typically have only one labeled example in the first frame of eachvideo. We address this research issue here by pre-training a CNN offline andthen transferring the rich feature hierarchies learned to online tracking. TheCNN is also fine-tuned during online tracking to adapt to the appearance of thetracked target specified in the first video frame. To fit the characteristicsof object tracking, we first pre-train the CNN to recognize what is an object,and then propose to generate a probability map instead of producing a simpleclass label. Using two challenging open benchmarks for performance evaluation,our proposed tracker has demonstrated substantial improvement over otherstate-of-the-art trackers.
arxiv-1501-04560 | Transductive Multi-view Zero-Shot Learning |  http://arxiv.org/abs/1501.04560  | author:Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Shaogang Gong category:cs.CV cs.DS cs.MM published:2015-01-19 summary:Most existing zero-shot learning approaches exploit transfer learning via anintermediate-level semantic representation shared between an annotatedauxiliary dataset and a target dataset with different classes and noannotation. A projection from a low-level feature space to the semanticrepresentation space is learned from the auxiliary dataset and is appliedwithout adaptation to the target dataset. In this paper we identify twoinherent limitations with these approaches. First, due to having disjoint andpotentially unrelated classes, the projection functions learned from theauxiliary dataset/domain are biased when applied directly to the targetdataset/domain. We call this problem the projection domain shift problem andpropose a novel framework, transductive multi-view embedding, to solve it. Thesecond limitation is the prototype sparsity problem which refers to the factthat for each target class, only a single prototype is available for zero-shotlearning given a semantic representation. To overcome this problem, a novelheterogeneous multi-view hypergraph label propagation method is formulated forzero-shot learning in the transductive embedding space. It effectively exploitsthe complementary information offered by different semantic representations andtakes advantage of the manifold structures of multiple representation spaces ina coherent manner. We demonstrate through extensive experiments that theproposed approach (1) rectifies the projection shift between the auxiliary andtarget domains, (2) exploits the complementarity of multiple semanticrepresentations, (3) significantly outperforms existing methods for bothzero-shot and N-shot recognition on three image and video benchmark datasets,and (4) enables novel cross-view annotation tasks.
arxiv-1501-04370 | Structure Learning in Bayesian Networks of Moderate Size by Efficient Sampling |  http://arxiv.org/abs/1501.04370  | author:Ru He, Jin Tian, Huaiqing Wu category:cs.AI cs.LG stat.ML published:2015-01-19 summary:We study the Bayesian model averaging approach to learning Bayesian networkstructures (DAGs) from data. We develop new algorithms including the firstalgorithm that is able to efficiently sample DAGs according to the exactstructure posterior. The DAG samples can then be used to construct estimatorsfor the posterior of any feature. We theoretically prove good properties of ourestimators and empirically show that our estimators considerably outperform theestimators from the previous state-of-the-art methods.
arxiv-1501-04467 | Implementable confidence sets in high dimensional regression |  http://arxiv.org/abs/1501.04467  | author:Alexandra Carpentier category:stat.ML published:2015-01-19 summary:We consider the setting of linear regression in high dimension. We focus onthe problem of constructing adaptive and honest confidence sets for the sparseparameter \theta, i.e. we want to construct a confidence set for theta thatcontains theta with high probability, and that is as small as possible. The l_2diameter of a such confidence set should depend on the sparsity S of \theta -the larger S, the wider the confidence set. However, in practice, S is unknown.This paper focuses on constructing a confidence set for \theta which contains\theta with high probability, whose diameter is adaptive to the unknownsparsity S, and which is implementable in practice.
arxiv-1501-04505 | Robust Visual Tracking via Convolutional Networks |  http://arxiv.org/abs/1501.04505  | author:Kaihua Zhang, Qingshan Liu, Yi Wu, Ming-Hsuan Yang category:cs.CV published:2015-01-19 summary:Deep networks have been successfully applied to visual tracking by learning ageneric representation offline from numerous training images. However theoffline training is time-consuming and the learned generic representation maybe less discriminative for tracking specific objects. In this paper we presentthat, even without offline training with a large amount of auxiliary data,simple two-layer convolutional networks can be powerful enough to develop arobust representation for visual tracking. In the first frame, we employ thek-means algorithm to extract a set of normalized patches from the target regionas fixed filters, which integrate a series of adaptive contextual filterssurrounding the target to define a set of feature maps in the subsequentframes. These maps measure similarities between each filter and the usefullocal intensity patterns across the target, thereby encoding its localstructural information. Furthermore, all the maps form together a globalrepresentation, which is built on mid-level features, thereby remaining closeto image-level information, and hence the inner geometric layout of the targetis also well preserved. A simple soft shrinkage method with an adaptivethreshold is employed to de-noise the global representation, resulting in arobust sparse representation. The representation is updated via a simple andeffective online strategy, allowing it to robustly adapt to target appearancevariations. Our convolution networks have surprisingly lightweight structure,yet perform favorably against several state-of-the-art methods on the CVPR2013tracking benchmark dataset with 50 challenging videos.
arxiv-1501-04367 | Reconstruction-free action inference from compressive imagers |  http://arxiv.org/abs/1501.04367  | author:Kuldeep Kulkarni, Pavan Turaga category:cs.CV published:2015-01-18 summary:Persistent surveillance from camera networks, such as at parking lots, UAVs,etc., often results in large amounts of video data, resulting in significantchallenges for inference in terms of storage, communication and computation.Compressive cameras have emerged as a potential solution to deal with the datadeluge issues in such applications. However, inference tasks such as actionrecognition require high quality features which implies reconstructing theoriginal video data. Much work in compressive sensing (CS) theory is gearedtowards solving the reconstruction problem, where state-of-the-art methods arecomputationally intensive and provide low-quality results at high compressionrates. Thus, reconstruction-free methods for inference are much desired. Inthis paper, we propose reconstruction-free methods for action recognition fromcompressive cameras at high compression ratios of 100 and above. Recognizingactions directly from CS measurements requires features which are mostlynonlinear and thus not easily applicable. This leads us to search for suchproperties that are preserved in compressive measurements. To this end, wepropose the use of spatio-temporal smashed filters, which are compressivedomain versions of pixel-domain matched filters. We conduct experiments onpublicly available databases and show that one can obtain recognition ratesthat are comparable to the oracle method in uncompressed setup, even for highcompression ratios.
arxiv-1501-04284 | Pairwise Constraint Propagation on Multi-View Data |  http://arxiv.org/abs/1501.04284  | author:Zhiwu Lu, Liwei Wang category:cs.CV cs.LG published:2015-01-18 summary:This paper presents a graph-based learning approach to pairwise constraintpropagation on multi-view data. Although pairwise constraint propagation hasbeen studied extensively, pairwise constraints are usually defined over pairsof data points from a single view, i.e., only intra-view constraint propagationis considered for multi-view tasks. In fact, very little attention has beenpaid to inter-view constraint propagation, which is more challenging sincepairwise constraints are now defined over pairs of data points from differentviews. In this paper, we propose to decompose the challenging inter-viewconstraint propagation problem into semi-supervised learning subproblems sothat they can be efficiently solved based on graph-based label propagation. Tothe best of our knowledge, this is the first attempt to give an efficientsolution to inter-view constraint propagation from a semi-supervised learningviewpoint. Moreover, since graph-based label propagation has been adopted forbasic optimization, we develop two constrained graph construction methods forinterview constraint propagation, which only differ in how the intra-viewpairwise constraints are exploited. The experimental results in cross-viewretrieval have shown the promising performance of our inter-view constraintpropagation.
arxiv-1501-04292 | Image classification by visual bag-of-words refinement and reduction |  http://arxiv.org/abs/1501.04292  | author:Zhiwu Lu, Liwei Wang, Ji-Rong Wen category:cs.CV published:2015-01-18 summary:This paper presents a new framework for visual bag-of-words (BOW) refinementand reduction to overcome the drawbacks associated with the visual BOW modelwhich has been widely used for image classification. Although very influentialin the literature, the traditional visual BOW model has two distinct drawbacks.Firstly, for efficiency purposes, the visual vocabulary is commonly constructedby directly clustering the low-level visual feature vectors extracted fromlocal keypoints, without considering the high-level semantics of images. Thatis, the visual BOW model still suffers from the semantic gap, and thus may leadto significant performance degradation in more challenging tasks (e.g. socialimage classification). Secondly, typically thousands of visual words aregenerated to obtain better performance on a relatively large image dataset. Dueto such large vocabulary size, the subsequent image classification may takesheer amount of time. To overcome the first drawback, we develop a graph-basedmethod for visual BOW refinement by exploiting the tags (easy to accessalthough noisy) of social images. More notably, for efficient imageclassification, we further reduce the refined visual BOW model to a muchsmaller size through semantic spectral clustering. Extensive experimentalresults show the promising performance of the proposed framework for visual BOWrefinement and reduction.
arxiv-1501-04267 | Comment on "Clustering by fast search and find of density peaks" |  http://arxiv.org/abs/1501.04267  | author:Shuliang Wang, Dakui Wang, Caoyuan Li, Yan Li category:cs.LG published:2015-01-18 summary:In [1], a clustering algorithm was given to find the centers of clustersquickly. However, the accuracy of this algorithm heavily depend on thethreshold value of d-c. Furthermore, [1] has not provided any efficient way toselect the threshold value of d-c, that is, one can have to estimate the valueof d_c depend on one's subjective experience. In this paper, based on the datafield [2], we propose a new way to automatically extract the threshold value ofd_c from the original data set by using the potential entropy of data field.For any data set to be clustered, the most reasonable value of d_c can beobjectively calculated from the data set by using our proposed method. The sameexperiments in [1] are redone with our proposed method on the same experimentaldata set used in [1], the results of which shows that the problem to calculatethe threshold value of d_c in [1] has been solved by using our method.
arxiv-1501-04282 | Regularized maximum correntropy machine |  http://arxiv.org/abs/1501.04282  | author:Jim Jing-Yan Wang, Yunji Wang, Bing-Yi Jing, Xin Gao category:cs.LG published:2015-01-18 summary:In this paper we investigate the usage of regularized correntropy frameworkfor learning of classifiers from noisy labels. The class label predictorslearned by minimizing transitional loss functions are sensitive to the noisyand outlying labels of training samples, because the transitional lossfunctions are equally applied to all the samples. To solve this problem, wepropose to learn the class label predictors by maximizing the correntropybetween the predicted labels and the true labels of the training samples, underthe regularized Maximum Correntropy Criteria (MCC) framework. Moreover, weregularize the predictor parameter to control the complexity of the predictor.The learning problem is formulated by an objective function considering theparameter regularization and MCC simultaneously. By optimizing the objectivefunction alternately, we develop a novel predictor learning algorithm. Theexperiments on two chal- lenging pattern classification tasks show that itsignificantly outperforms the machines with transitional loss functions.
arxiv-1501-04325 | Deep Belief Nets for Topic Modeling |  http://arxiv.org/abs/1501.04325  | author:Lars Maaloe, Morten Arngren, Ole Winther category:cs.CL cs.LG stat.ML published:2015-01-18 summary:Applying traditional collaborative filtering to digital publishing ischallenging because user data is very sparse due to the high volume ofdocuments relative to the number of users. Content based approaches, on theother hand, is attractive because textual content is often very informative. Inthis paper we describe large-scale content based collaborative filtering fordigital publishing. To solve the digital publishing recommender problem wecompare two approaches: latent Dirichlet allocation (LDA) and deep belief nets(DBN) that both find low-dimensional latent representations for documents.Efficient retrieval can be carried out in the latent representation. We workboth on public benchmarks and digital media content provided by Issuu, anonline publishing platform. This article also comes with a newly developed deepbelief nets toolbox for topic modeling tailored towards performance evaluationof the DBN model and comparisons to the LDA model.
arxiv-1501-04309 | Information Theory and its Relation to Machine Learning |  http://arxiv.org/abs/1501.04309  | author:Bao-Gang Hu category:cs.IT cs.LG math.IT published:2015-01-18 summary:In this position paper, I first describe a new perspective on machinelearning (ML) by four basic problems (or levels), namely, "What to learn?","How to learn?", "What to evaluate?", and "What to adjust?". The paper stressesmore on the first level of "What to learn?", or "Learning Target Selection".Towards this primary problem within the four levels, I briefly review theexisting studies about the connection between information theoretical learning(ITL [1]) and machine learning. A theorem is given on the relation between theempirically-defined similarity measure and information measures. Finally, aconjecture is proposed for pursuing a unified mathematical interpretation tolearning target selection.
arxiv-1501-04346 | Mathematical Language Processing: Automatic Grading and Feedback for Open Response Mathematical Questions |  http://arxiv.org/abs/1501.04346  | author:Andrew S. Lan, Divyanshu Vats, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.AI cs.CL cs.LG published:2015-01-18 summary:While computer and communication technologies have provided effective meansto scale up many aspects of education, the submission and grading ofassessments such as homework assignments and tests remains a weak link. In thispaper, we study the problem of automatically grading the kinds of open responsemathematical questions that figure prominently in STEM (science, technology,engineering, and mathematics) courses. Our data-driven framework formathematical language processing (MLP) leverages solution data from a largenumber of learners to evaluate the correctness of their solutions, assignpartial-credit scores, and provide feedback to each learner on the likelylocations of any errors. MLP takes inspiration from the success of naturallanguage processing for text data and comprises three main steps. First, weconvert each solution to an open response mathematical question into a seriesof numerical features. Second, we cluster the features from several solutionsto uncover the structures of correct, partially correct, and incorrectsolutions. We develop two different clustering approaches, one that leveragesgeneric clustering algorithms and one based on Bayesian nonparametrics. Third,we automatically grade the remaining (potentially large number of) solutionsbased on their assigned cluster and one instructor-provided grade per cluster.As a bonus, we can track the cluster assignment of each step of a multistepsolution and determine when it departs from a cluster of correct solutions,which enables us to indicate the likely locations of errors to learners. Wetest and validate MLP on real-world MOOC data to demonstrate how it cansubstantially reduce the human effort required in large-scale educationalplatforms.
arxiv-1501-04308 | Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements |  http://arxiv.org/abs/1501.04308  | author:Enea Bongiorno, Aldo Goia category:math.PR math.ST stat.AP stat.ME stat.ML stat.TH 62G99 published:2015-01-18 summary:Asymptotic factorizations for the small-ball probability (SmBP) of a Hilbertvalued random element $X$ are rigorously established and discussed. Inparticular, given the first $d$ principal components (PCs) and as the radius$\varepsilon$ of the ball tends to zero, the SmBP is asymptoticallyproportional to (a) the joint density of the first $d$ PCs, (b) the volume ofthe $d$-dimensional ball with radius $\varepsilon$, and (c) a correction factorweighting the use of a truncated version of the process expansion. Moreover,under suitable assumptions on the spectrum of the covariance operator of $X$and as $d$ diverges to infinity when $\varepsilon$ vanishes, somesimplifications occur. In particular, the SmBP factorizes asymptotically as theproduct of the joint density of the first $d$ PCs and a pure volume parameter.All the provided factorizations allow to define a surrogate intensity of theSmBP that, in some cases, leads to a genuine intensity. To operationalize thestated results, a non-parametric estimator for the surrogate intensity isintroduced and it is proved that the use of estimated PCs, instead of the trueones, does not affect the rate of convergence. Finally, as an illustration,simulations in controlled frameworks are provided.
arxiv-1501-04276 | Correlation Adaptive Subspace Segmentation by Trace Lasso |  http://arxiv.org/abs/1501.04276  | author:Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan category:cs.CV published:2015-01-18 summary:This paper studies the subspace segmentation problem. Given a set of datapoints drawn from a union of subspaces, the goal is to partition them intotheir underlying subspaces they were drawn from. The spectral clustering methodis used as the framework. It requires to find an affinity matrix which is closeto block diagonal, with nonzero entries corresponding to the data point pairsfrom the same subspace. In this work, we argue that both sparsity and thegrouping effect are important for subspace segmentation. A sparse affinitymatrix tends to be block diagonal, with less connections between data pointsfrom different subspaces. The grouping effect ensures that the highly correcteddata which are usually from the same subspace can be grouped together. SparseSubspace Clustering (SSC), by using $\ell^1$-minimization, encourages sparsityfor data selection, but it lacks of the grouping effect. On the contrary,Low-Rank Representation (LRR), by rank minimization, and Least SquaresRegression (LSR), by $\ell^2$-regularization, exhibit strong grouping effect,but they are short in subset selection. Thus the obtained affinity matrix isusually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation(CASS) method by using trace Lasso. CASS is a data correlation dependent methodwhich simultaneously performs automatic data selection and groups correlateddata together. It can be regarded as a method which adaptively balances SSC andLSR. Both theoretical and experimental results show the effectiveness of CASS.
arxiv-1501-04277 | Correntropy Induced L2 Graph for Robust Subspace Clustering |  http://arxiv.org/abs/1501.04277  | author:Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin category:cs.CV published:2015-01-18 summary:In this paper, we study the robust subspace clustering problem, which aims tocluster the given possibly noisy data points into their underlying subspaces. Alarge pool of previous subspace clustering methods focus on the graphconstruction by different regularization of the representation coefficient. Weinstead focus on the robustness of the model to non-Gaussian noises. We proposea new robust clustering method by using the correntropy induced metric, whichis robust for handling the non-Gaussian and impulsive noises. Also we furtherextend the method for handling the data with outlier rows/features. Themultiplicative form of half-quadratic optimization is used to optimize thenon-convex correntropy objective function of the proposed models. Extensiveexperiments on face datasets well demonstrate that the proposed methods aremore robust to corruptions and occlusions.
arxiv-1501-04324 | Phrase Based Language Model For Statistical Machine Translation |  http://arxiv.org/abs/1501.04324  | author:Jia Xu, Geliang Chen category:cs.CL published:2015-01-18 summary:We consider phrase based Language Models (LM), which generalize the commonlyused word level models. Similar concept on phrase based LMs appears in speechrecognition, which is rather specialized and thus less suitable for machinetranslation (MT). In contrast to the dependency LM, we first introduce theexhaustive phrase-based LMs tailored for MT use. Preliminary experimentalresults show that our approach outperform word based LMs with the respect toperplexity and translation quality.
arxiv-1501-04318 | A Generalized Affinity Propagation Clustering Algorithm for Nonspherical Cluster Discovery |  http://arxiv.org/abs/1501.04318  | author:Teng Qiu, Yongjie Li category:cs.LG cs.CV stat.ML published:2015-01-18 summary:Clustering analysis aims to discover the underlying clusters in the datapoints according to their similarities. It has wide applications ranging frombioinformatics to astronomy. Here, we proposed a Generalized AffinityPropagation (G-AP) clustering algorithm. Data points are first organized in asparsely connected in-tree (IT) structure by a physically inspired strategy.Then, additional edges are added to the IT structure for those reachable nodes.This expanded structure is subsequently trimmed by affinity propagation method.Consequently, the underlying cluster structure, with separate clusters,emerges. In contrast to other IT-based methods, G-AP is fully automatic andtakes as input the pairs of similarities between data points only. Unlikeaffinity propagation, G-AP is capable of discovering nonspherical clusters.
arxiv-1501-04244 | Generalised Random Forest Space Overview |  http://arxiv.org/abs/1501.04244  | author:Miron B. Kursa category:cs.LG published:2015-01-17 summary:Assuming a view of the Random Forest as a special case of a nested ensembleof interchangeable modules, we construct a generalisation space allowing one toeasily develop novel methods based on this algorithm. We discuss the role andrequired properties of modules at each level, especially in context of somealready proposed RF generalisations.
arxiv-1501-04140 | A Fast Fractal Image Compression Algorithm Using Predefined Values for Contrast Scaling |  http://arxiv.org/abs/1501.04140  | author:H. Miar Naimi, M. Salarian category:cs.CV published:2015-01-17 summary:In this paper a new fractal image compression algorithm is proposed in whichthe time of encoding process is considerably reduced. The algorithm exploits adomain pool reduction approach, along with using innovative predefined valuesfor contrast scaling factor, S, instead of scanning the parameter space [0,1].Within this approach only domain blocks with entropies greater than a thresholdare considered. As a novel point, it is assumed that in each step of theencoding process, the domain block with small enough distance shall be foundonly for the range blocks with low activity (equivalently low entropy). Thisnovel point is used to find reasonable estimations of S, and use them in theencoding process as predefined values, mentioned above. The algorithm has beenexamined for some well-known images. This result shows that our proposedalgorithm considerably reduces the encoding time producing images that areapproximately the same in quality.
arxiv-1501-04158 | On the Performance of ConvNet Features for Place Recognition |  http://arxiv.org/abs/1501.04158  | author:Niko Sünderhauf, Feras Dayoub, Sareh Shirazi, Ben Upcroft, Michael Milford category:cs.RO cs.CV published:2015-01-17 summary:After the incredible success of deep learning in the computer vision domain,there has been much interest in applying Convolutional Network (ConvNet)features in robotic fields such as visual navigation and SLAM. Unfortunately,there are fundamental differences and challenges involved. Computer visiondatasets are very different in character to robotic camera data, real-timeperformance is essential, and performance priorities can be different. Thispaper comprehensively evaluates and compares the utility of threestate-of-the-art ConvNets on the problems of particular relevance to navigationfor robots; viewpoint-invariance and condition-invariance, and for the firsttime enables real-time place recognition performance using ConvNets with largemaps by integrating a variety of existing (locality-sensitive hashing) andnovel (semantic search space partitioning) optimization techniques. We presentextensive experiments on four real world datasets cultivated to evaluate eachof the specific challenges in place recognition. The results demonstrate thatspeed-ups of two orders of magnitude can be achieved with minimal accuracydegradation, enabling real-time performance. We confirm that networks trainedfor semantic place categorization also perform better at (specific) placerecognition when faced with severe appearance changes and provide a referencefor which networks and layers are optimal for different aspects of the placerecognition problem.
arxiv-1501-04163 | Meaningful Objects Segmentation from SAR Images via A Multi-Scale Non-Local Active Contour Model |  http://arxiv.org/abs/1501.04163  | author:Gui-Song Xia, Gang Liu, Wen Yang category:cs.CV published:2015-01-17 summary:The segmentation of synthetic aperture radar (SAR) images is a longstandingyet challenging task, not only because of the presence of speckle, but also dueto the variations of surface backscattering properties in the images.Tremendous investigations have been made to eliminate the speckle effects forthe segmentation of SAR images, while few work devotes to dealing with thevariations of backscattering coefficients in the images. In order to overcomeboth the two difficulties, this paper presents a novel SAR image segmentationmethod by exploiting a multi-scale active contour model based on the non-localprocessing principle. More precisely, we first formulize the SAR segmentationproblem with an active contour model by integrating the non-local interactionsbetween pairs of patches inside and outside the segmented regions. Secondly, amulti-scale strategy is proposed to speed up the non-local active contoursegmentation procedure and to avoid falling into local minimum for achievingmore accurate segmentation results. Experimental results on simulated and realSAR images demonstrate the efficiency and feasibility of the proposed method:it can not only achieve precise segmentations for images with heavy specklesand non-local intensity variations, but also can be used for SAR images fromdifferent types of sensors.
arxiv-1501-04053 | Stochastic Local Interaction (SLI) Model: Interfacing Machine Learning and Geostatistics |  http://arxiv.org/abs/1501.04053  | author:Dionissios T. Hristopulos category:cs.LG stat.ML published:2015-01-16 summary:Machine learning and geostatistics are powerful mathematical frameworks formodeling spatial data. Both approaches, however, suffer from poor scaling ofthe required computational resources for large data applications. We presentthe Stochastic Local Interaction (SLI) model, which employs a localrepresentation to improve computational efficiency. SLI combines geostatisticsand machine learning with ideas from statistical physics and computationalgeometry. It is based on a joint probability density function defined by anenergy functional which involves local interactions implemented by means ofkernel functions with adaptive local kernel bandwidths. SLI is expressed interms of an explicit, typically sparse, precision (inverse covariance) matrix.This representation leads to a semi-analytical expression for interpolation(prediction), which is valid in any number of dimensions and avoids thecomputationally costly covariance matrix inversion.
arxiv-1501-03959 | Value Iteration with Options and State Aggregation |  http://arxiv.org/abs/1501.03959  | author:Kamil Ciosek, David Silver category:cs.AI cs.LG stat.ML published:2015-01-16 summary:This paper presents a way of solving Markov Decision Processes that combinesstate abstraction and temporal abstraction. Specifically, we combine stateaggregation with the options framework and demonstrate that they work welltogether and indeed it is only after one combines the two that the full benefitof each is realized. We introduce a hierarchical value iteration algorithmwhere we first coarsely solve subgoals and then use these approximate solutionsto exactly solve the MDP. This algorithm solved several problems faster thanvanilla value iteration.
arxiv-1501-04080 | Differentially Private Bayesian Optimization |  http://arxiv.org/abs/1501.04080  | author:Matt J. Kusner, Jacob R. Gardner, Roman Garnett, Kilian Q. Weinberger category:stat.ML published:2015-01-16 summary:Bayesian optimization is a powerful tool for fine-tuning the hyper-parametersof a wide variety of machine learning models. The success of machine learninghas led practitioners in diverse real-world settings to learn classifiers forpractical problems. As machine learning becomes commonplace, Bayesianoptimization becomes an attractive method for practitioners to automate theprocess of classifier hyper-parameter tuning. A key observation is that thedata used for tuning models in these settings is often sensitive. Certain datasuch as genetic predisposition, personal email statistics, and car accidenthistory, if not properly private, may be at risk of being inferred fromBayesian optimization outputs. To address this, we introduce methods forreleasing the best hyper-parameters and classifier accuracy privately.Leveraging the strong theoretical guarantees of differential privacy and knownBayesian optimization convergence bounds, we prove that under a GP assumptionthese private quantities are also near-optimal. Finally, even if thisassumption is not satisfied, we can use different smoothness guarantees toprotect privacy.
arxiv-1501-03915 | Feature Selection based on Machine Learning in MRIs for Hippocampal Segmentation |  http://arxiv.org/abs/1501.03915  | author:Sabina Tangaro, Nicola Amoroso, Massimo Brescia, Stefano Cavuoti, Andrea Chincarini, Rosangela Errico, Paolo Inglese, Giuseppe Longo, Rosalia Maglietta, Andrea Tateo, Giuseppe Riccio, Roberto Bellotti category:physics.med-ph cs.CV cs.LG published:2015-01-16 summary:Neurodegenerative diseases are frequently associated with structural changesin the brain. Magnetic Resonance Imaging (MRI) scans can show these variationsand therefore be used as a supportive feature for a number of neurodegenerativediseases. The hippocampus has been known to be a biomarker for Alzheimerdisease and other neurological and psychiatric diseases. However, it requiresaccurate, robust and reproducible delineation of hippocampal structures. Fullyautomatic methods are usually the voxel based approach, for each voxel a numberof local features were calculated. In this paper we compared four differenttechniques for feature selection from a set of 315 features extracted for eachvoxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrappermethods, respectively, (ii) Sequential Forward Selection and (iii) SequentialBackward Elimination; and (iv) embedded method based on the Random ForestClassifier on a set of 10 T1-weighted brain MRIs and tested on an independentset of 25 subjects. The resulting segmentations were compared with manualreference labelling. By using only 23 features for each voxel (sequentialbackward elimination) we obtained comparable state of-the-art performances withrespect to the standard tool FreeSurfer.
arxiv-1501-03879 | A new ADMM algorithm for the Euclidean median and its application to robust patch regression |  http://arxiv.org/abs/1501.03879  | author:Kunal N. Chaudhury, K. R. Ramakrishnan category:cs.CV published:2015-01-16 summary:The Euclidean Median (EM) of a set of points $\Omega$ in an Euclidean spaceis the point x minimizing the (weighted) sum of the Euclidean distances of x tothe points in $\Omega$. While there exits no closed-form expression for the EM,it can nevertheless be computed using iterative methods such as the Wieszfeldalgorithm. The EM has classically been used as a robust estimator of centralityfor multivariate data. It was recently demonstrated that the EM can be used toperform robust patch-based denoising of images by generalizing the popularNon-Local Means algorithm. In this paper, we propose a novel algorithm forcomputing the EM (and its box-constrained counterpart) using variable splittingand the method of augmented Lagrangian. The attractive feature of this approachis that the subproblems involved in the ADMM-based optimization of theaugmented Lagrangian can be resolved using simple closed-form projections. Theproposed ADMM solver is used for robust patch-based image denoising and isshown to exhibit faster convergence compared to an existing solver.
arxiv-1501-03969 | Nonlinear Model Predictive Control of A Gasoline HCCI Engine Using Extreme Learning Machines |  http://arxiv.org/abs/1501.03969  | author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Dennis Assanis category:cs.SY cs.NE published:2015-01-16 summary:Homogeneous charge compression ignition (HCCI) is a futuristic combustiontechnology that operates with a high fuel efficiency and reduced emissions.HCCI combustion is characterized by complex nonlinear dynamics whichnecessitates a model based control approach for automotive application. HCCIengine control is a nonlinear, multi-input multi-output problem with state andactuator constraints which makes controller design a challenging task. TypicalHCCI controllers make use of a first principles based model which involves along development time and cost associated with expert labor and calibration. Inthis paper, an alternative approach based on machine learning is presentedusing extreme learning machines (ELM) and nonlinear model predictive control(MPC). A recurrent ELM is used to learn the nonlinear dynamics of HCCI engineusing experimental data and is shown to accurately predict the engine behaviorseveral steps ahead in time, suitable for predictive control. Using the ELMengine models, an MPC based control algorithm with a simplified quadraticprogram update is derived for real time implementation. The working andeffectiveness of the MPC approach has been analyzed on a nonlinear HCCI enginemodel for tracking multiple reference quantities along with constraints definedby HCCI states, actuators and operational limits.
arxiv-1501-03952 | Mind the Gap: Subspace based Hierarchical Domain Adaptation |  http://arxiv.org/abs/1501.03952  | author:Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV published:2015-01-16 summary:Domain adaptation techniques aim at adapting a classifier learnt on a sourcedomain to work on the target domain. Exploiting the subspaces spanned byfeatures of the source and target domains respectively is one approach that hasbeen investigated towards solving this problem. These techniques normallyassume the existence of a single subspace for the entire source / targetdomain. In this work, we consider the hierarchical organization of the data andconsider multiple subspaces for the source and target domain based on thehierarchy. We evaluate different subspace based domain adaptation techniquesunder this setting and observe that using different subspaces based on thehierarchy yields consistent improvement over a non-hierarchical baseline
arxiv-1501-03861 | Bayesian Nonparametrics in Topic Modeling: A Brief Tutorial |  http://arxiv.org/abs/1501.03861  | author:Alexander Spangher category:stat.ML published:2015-01-16 summary:Using nonparametric methods has been increasingly explored in Bayesianhierarchical modeling as a way to increase model flexibility. Although thefield shows a lot of promise, inference in many models, including HierachicalDirichlet Processes (HDP), remain prohibitively slow. One promising pathforward is to exploit the submodularity inherent in Indian Buffet Process (IBP)to derive near-optimal solutions in polynomial time. In this work, I willpresent a brief tutorial on Bayesian nonparametric methods, especially as theyare applied to topic modeling. I will show a comparison between differentnon-parametric models and the current state-of-the-art parametric model, LatentDirichlet Allocation (LDA).
arxiv-1501-03975 | Stochastic Gradient Based Extreme Learning Machines For Online Learning of Advanced Combustion Engines |  http://arxiv.org/abs/1501.03975  | author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Dennis Assanis category:cs.NE cs.LG cs.SY published:2015-01-16 summary:In this article, a stochastic gradient based online learning algorithm forExtreme Learning Machines (ELM) is developed (SG-ELM). A stability criterionbased on Lyapunov approach is used to prove both asymptotic stability ofestimation error and stability in the estimated parameters suitable foridentification of nonlinear dynamic systems. The developed algorithm not onlyguarantees stability, but also reduces the computational demand compared to theOS-ELM approach based on recursive least squares. In order to demonstrate theeffectiveness of the algorithm on a real-world scenario, an advanced combustionengine identification problem is considered. The algorithm is applied to twocase studies: An online regression learning for system identification of aHomogeneous Charge Compression Ignition (HCCI) Engine and an onlineclassification learning (with class imbalance) for identifying the dynamicoperating envelope of the HCCI Engine. The results indicate that the accuracyof the proposed SG-ELM is comparable to that of the state-of-the-art but addsstability and a reduction in computational effort.
arxiv-1501-04010 | Coevolutionary intransitivity in games: A landscape analysis |  http://arxiv.org/abs/1501.04010  | author:Hendrik Richter category:cs.NE q-bio.PE published:2015-01-16 summary:Intransitivity is supposed to be a main reason for deficits in coevolutionaryprogress and inheritable superiority. Besides, coevolutionary dynamics ischaracterized by interactions yielding subjective fitness, but aiming atsolutions that are superior with respect to an objective measurement. Such anapproximation of objective fitness may be, for instance, generalizationperformance. In the paper a link between rating-- and ranking--based measuresof intransitivity and fitness landscapes that can address the dichotomy betweensubjective and objective fitness is explored. The approach is illustrated bynumerical experiments involving a simple random game with continuously tunabledegree of randomness.
arxiv-1501-03854 | Understanding Kernel Ridge Regression: Common behaviors from simple functions to density functionals |  http://arxiv.org/abs/1501.03854  | author:Kevin Vu, John Snyder, Li Li, Matthias Rupp, Brandon F. Chen, Tarek Khelif, Klaus-Robert Müller, Kieron Burke category:cs.LG stat.ML published:2015-01-16 summary:Accurate approximations to density functionals have recently been obtainedvia machine learning (ML). By applying ML to a simple function of one variablewithout any random sampling, we extract the qualitative dependence of errors onhyperparameters. We find universal features of the behavior in extreme limits,including both very small and very large length scales, and the noise-freelimit. We show how such features arise in ML models of density functionals.
arxiv-1501-03796 | The Fast Convergence of Incremental PCA |  http://arxiv.org/abs/1501.03796  | author:Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund category:cs.LG stat.ML published:2015-01-15 summary:We consider a situation in which we see samples in $\mathbb{R}^d$ drawni.i.d. from some distribution with mean zero and unknown covariance A. We wishto compute the top eigenvector of A in an incremental fashion - with analgorithm that maintains an estimate of the top eigenvector in O(d) space, andincrementally adjusts the estimate with each new data point that arrives. Twoclassical such schemes are due to Krasulina (1969) and Oja (1983). We givefinite-sample convergence rates for both.
arxiv-1501-03786 | Multi-view learning for multivariate performance measures optimization |  http://arxiv.org/abs/1501.03786  | author:Jim Jing-Yan Wang category:cs.LG published:2015-01-15 summary:In this paper, we propose the problem of optimizing multivariate performancemeasures from multi-view data, and an effective method to solve it. Thisproblem has two features: the data points are presented by multiple views, andthe target of learning is to optimize complex multivariate performancemeasures. We propose to learn a linear discriminant functions for each view,and combine them to construct a overall multivariate mapping function formult-view data. To learn the parameters of the linear dis- criminant functionsof different views to optimize multivariate performance measures, we formulatea optimization problem. In this problem, we propose to minimize the complexityof the linear discriminant functions of each view, encourage the consistencesof the responses of different views over the same data points, and minimize theupper boundary of a given multivariate performance measure. To optimize thisproblem, we employ the cutting-plane method in an iterative algorithm. In eachiteration, we update a set of constrains, and optimize the mapping functionparameter of each view one by one.
arxiv-1501-03771 | Submodular relaxation for inference in Markov random fields |  http://arxiv.org/abs/1501.03771  | author:Anton Osokin, Dmitry Vetrov category:cs.CV math.OC stat.ML published:2015-01-15 summary:In this paper we address the problem of finding the most probable state of adiscrete Markov random field (MRF), also known as the MRF energy minimizationproblem. The task is known to be NP-hard in general and its practicalimportance motivates numerous approximate algorithms. We propose a submodularrelaxation approach (SMR) based on a Lagrangian relaxation of the initialproblem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMRdoes not decompose the graph structure of the initial problem but constructs asubmodular energy that is minimized within the Lagrangian relaxation. Ourapproach is applicable to both pairwise and high-order MRFs and allows to takeinto account global potentials of certain types. We study theoreticalproperties of the proposed approach and evaluate it experimentally.
arxiv-1501-03779 | Computer-assisted polyp matching between optical colonoscopy and CT colonography: a phantom study |  http://arxiv.org/abs/1501.03779  | author:Holger R. Roth, Thomas E. Hampshire, Emma Helbren, Mingxing Hu, Roser Vega, Steve Halligan, David J. Hawkes category:cs.CV published:2015-01-15 summary:Potentially precancerous polyps detected with CT colonography (CTC) need tobe removed subsequently, using an optical colonoscope (OC). Due to largecolonic deformations induced by the colonoscope, even very experiencedcolonoscopists find it difficult to pinpoint the exact location of thecolonoscope tip in relation to polyps reported on CTC. This can cause undulyprolonged OC examinations that are stressful for the patient, colonoscopist andsupporting staff. We developed a method, based on monocular 3D reconstruction from OC images,that automatically matches polyps observed in OC with polyps reported on priorCTC. A matching cost is computed, using rigid point-based registration betweensurface point clouds extracted from both modalities. A 3D printed and paintedphantom of a 25 cm long transverse colon segment was used to validate themethod on two medium sized polyps. Results indicate that the matching cost issmaller at the correct corresponding polyp between OC and CTC: the value is 3.9times higher at the incorrect polyp, comparing the correct match between polypsto the incorrect match. Furthermore, we evaluate the matching of thereconstructed polyp from OC with other colonic endoluminal surface structuressuch as haustral folds and show that there is a minimum at the correct polypfrom CTC. Automated matching between polyps observed at OC and prior CTC wouldfacilitate the biopsy or removal of true-positive pathology or exclusion offalse-positive CTC findings, and would reduce colonoscopy false-negative(missed) polyps. Ultimately, such a method might reduce healthcare costs,patient inconvenience and discomfort.
arxiv-1501-03844 | Evaluating accuracy of community detection using the relative normalized mutual information |  http://arxiv.org/abs/1501.03844  | author:Pan Zhang category:physics.soc-ph cs.SI stat.ML published:2015-01-15 summary:The Normalized Mutual Information (NMI) has been widely used to evaluate theaccuracy of community detection algorithms. However in this article we showthat the NMI is seriously affected by systematic errors due to finite size ofnetworks, and may give a wrong estimate of performance of algorithms in somecases. We give a simple theory to the finite-size effect of NMI and test ourtheory numerically. Then we propose a new metric for the accuracy of communitydetection, namely the relative Normalized Mutual Information (rNMI), whichconsiders statistical significance of the NMI by comparing it with the expectedNMI of random partitions. Our numerical experiments show that the rNMIovercomes the finite-size effect of the NMI.
arxiv-1501-04009 | Visual Analytics of Image-Centric Cohort Studies in Epidemiology |  http://arxiv.org/abs/1501.04009  | author:Bernhard Preim, Paul Klemm, Helwig Hauser, Katrin Hegenscheid, Steffen Oeltze, Klaus Toennies, Henry Völzke category:cs.CV cs.CY published:2015-01-15 summary:Epidemiology characterizes the influence of causes to disease and healthconditions of defined populations. Cohort studies are population-based studiesinvolving usually large numbers of randomly selected individuals and comprisingnumerous attributes, ranging from self-reported interview data to results fromvarious medical examinations, e.g., blood and urine samples. Since recently,medical imaging has been used as an additional instrument to assess riskfactors and potential prognostic information. In this chapter, we discuss suchstudies and how the evaluation may benefit from visual analytics. Clusteranalysis to define groups, reliable image analysis of organs in medical imagingdata and shape space exploration to characterize anatomical shapes are amongthe visual analytics tools that may enable epidemiologists to fully exploit thepotential of their huge and complex data. To gain acceptance, visual analyticstools need to complement more classical epidemiologic tools, primarilyhypothesis-driven statistical analysis.
arxiv-1501-03659 | Quantifying uncertainties on excursion sets under a Gaussian random field prior |  http://arxiv.org/abs/1501.03659  | author:Dario Azzimonti, Julien Bect, Clément Chevalier, David Ginsbourger category:math.ST stat.CO stat.ML stat.TH published:2015-01-15 summary:We focus on the problem of estimating and quantifying uncertainties on theexcursion set of a function under a limited evaluation budget. We adopt aBayesian approach where the objective function is assumed to be a realizationof a Gaussian random field. In this setting, the posterior distribution on theobjective function gives rise to a posterior distribution on excursion sets.Several approaches exist to summarize the distribution of such sets based onrandom closed set theory. While the recently proposed Vorob'ev approachexploits analytical formulae, further notions of variability require MonteCarlo estimators relying on Gaussian random field conditional simulations. Inthe present work we propose a method to choose Monte Carlo simulation pointsand obtain quasi-realizations of the conditional field at fine designs throughaffine predictors. The points are chosen optimally in the sense that theyminimize the posterior expected distance in measure between the excursion setand its reconstruction. The proposed method reduces the computational costs dueto Monte Carlo simulations and enables the computation of quasi-realizations onfine designs in large dimensions. We apply this reconstruction approach toobtain realizations of an excursion set on a fine grid which allow us to give anew measure of uncertainty based on the distance transform of the excursionset. Finally we present a safety engineering test case where the simulationmethod is employed to compute a Monte Carlo estimate of a contour line.
arxiv-1501-03766 | Correlations between the Hurst exponent and the maximal Lyapunov exponent for some low-dimensional discrete conservative dynamical systems |  http://arxiv.org/abs/1501.03766  | author:Mariusz Tarnopolski category:nlin.CD math.DS stat.ML published:2015-01-15 summary:The Chirikov standard map and the 2D Froeschl\'e map are investigated. A fewthousand values of the Hurst exponent (HE) and the maximal Lyapunov exponent(mLE) are plotted in a mixed space of the nonlinear parameter versus theinitial condition. Both characteristic exponents reveal remarkably similarstructures in this mixed space. Moreover, a tight correlation between the HEsand mLEs for the two maps was found: $\rho=0.83$ and $\rho=0.75$ for theChirikov and Froeschl\'e maps, respectively, where $\rho$ is the Spearman rank.Based on this relation, a machine learning (ML) procedure, using the nearestneighbour algorithm, was performed to reproduce the HE distributions based onthe mLE distributions. A few thousand HE and mLE values from the mixed spaceswere used for training, and then using $2-2.4\times 10^5$ mLEs, the HEs wereretrieved. The ML procedure allowed to reproduce the structure of the mixedspaces in great detail. The HE is proposed as an informative parameter in thearea of chaotic control, as it provides expectations about the general trend ina time series.
arxiv-1501-03669 | A Proximal Approach for Sparse Multiclass SVM |  http://arxiv.org/abs/1501.03669  | author:G. Chierchia, Nelly Pustelnik, Jean-Christophe Pesquet, B. Pesquet-Popescu category:cs.LG published:2015-01-15 summary:Sparsity-inducing penalties are useful tools to design multiclass supportvector machines (SVMs). In this paper, we propose a convex optimizationapproach for efficiently and exactly solving the multiclass SVM learningproblem involving a sparse regularization and the multiclass hinge lossformulated by Crammer and Singer. We provide two algorithms: the first onedealing with the hinge loss as a penalty term, and the other one addressing thecase when the hinge loss is enforced through a constraint. The related convexoptimization problems can be efficiently solved thanks to the flexibilityoffered by recent primal-dual proximal algorithms and epigraphical splittingtechniques. Experiments carried out on several datasets demonstrate theinterest of considering the exact expression of the hinge loss rather than asmooth approximation. The efficiency of the proposed algorithms w.r.t. severalstate-of-the-art methods is also assessed through comparisons of executiontimes.
arxiv-1501-03719 | LATCH: Learned Arrangements of Three Patch Codes |  http://arxiv.org/abs/1501.03719  | author:Gil Levi, Tal Hassner category:cs.CV published:2015-01-15 summary:We present a novel means of describing local image appearances using binarystrings. Binary descriptors have drawn increasing interest in recent years dueto their speed and low memory footprint. A known shortcoming of theserepresentations is their inferior performance compared to larger, histogrambased descriptors such as the SIFT. Our goal is to close this performance gapwhile maintaining the benefits attributed to binary representations. To thisend we propose the Learned Arrangements of Three Patch Codes descriptors, orLATCH. Our key observation is that existing binary descriptors are at anincreased risk from noise and local appearance variations. This, as theycompare the values of pixel pairs; changes to either of the pixels can easilylead to changes in descriptor values, hence damaging its performance. In orderto provide more robustness, we instead propose a novel means of comparing pixelpatches. This ostensibly small change, requires a substantial redesign of thedescriptors themselves and how they are produced. Our resulting LATCHrepresentation is rigorously compared to state-of-the-art binary descriptorsand shown to provide far better performance for similar computation and spacerequirements.
arxiv-1501-03755 | Screen Content Image Segmentation Using Least Absolute Deviation Fitting |  http://arxiv.org/abs/1501.03755  | author:Shervin Minaee, Yao Wang category:cs.CV published:2015-01-15 summary:We propose an algorithm for separating the foreground (mainly text and linegraphics) from the smoothly varying background in screen content images. Theproposed method is designed based on the assumption that the background part ofthe image is smoothly varying and can be represented by a linear combination ofa few smoothly varying basis functions, while the foreground text and graphicscreate sharp discontinuity and cannot be modeled by this smooth representation.The algorithm separates the background and foreground using a least absolutedeviation method to fit the smooth model to the image pixels. This algorithmhas been tested on several images from HEVC standard test sequences for screencontent coding, and is shown to have superior performance over other popularmethods, such as k-means clustering based segmentation in DjVu and shapeprimitive extraction and coding (SPEC) algorithm. Such background/foregroundsegmentation are important pre-processing steps for text extraction andseparate coding of background and foreground for compression of screen contentimages.
arxiv-1501-03838 | PAC-Bayes with Minimax for Confidence-Rated Transduction |  http://arxiv.org/abs/1501.03838  | author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML published:2015-01-15 summary:We consider using an ensemble of binary classifiers for transductiveprediction, when unlabeled test data are known in advance. We derive minimaxoptimal rules for confidence-rated prediction in this setting. By usingPAC-Bayes analysis on these rules, we obtain data-dependent performanceguarantees without distributional assumptions on the data. Our analysistechniques are readily extended to a setting in which the predictor is allowedto abstain.
arxiv-1501-03273 | Classification with Low Rank and Missing Data |  http://arxiv.org/abs/1501.03273  | author:Elad Hazan, Roi Livni, Yishay Mansour category:cs.LG published:2015-01-14 summary:We consider classification and regression tasks where we have missing dataand assume that the (clean) data resides in a low rank subspace. Finding ahidden subspace is known to be computationally hard. Nevertheless, using anon-proper formulation we give an efficient agnostic algorithm that classifiesas good as the best linear classifier coupled with the best low-dimensionalsubspace in which the data resides. A direct implication is that our algorithmcan linearly (and non-linearly through kernels) classify provably as well asthe best classifier that has access to the full data.
arxiv-1501-03326 | Unbiased Bayes for Big Data: Paths of Partial Posteriors |  http://arxiv.org/abs/1501.03326  | author:Heiko Strathmann, Dino Sejdinovic, Mark Girolami category:stat.ML cs.LG stat.ME published:2015-01-14 summary:A key quantity of interest in Bayesian inference are expectations offunctions with respect to a posterior distribution. Markov Chain Monte Carlo isa fundamental tool to consistently compute these expectations via averagingsamples drawn from an approximate posterior. However, its feasibility is beingchallenged in the era of so called Big Data as all data needs to be processedin every iteration. Realising that such simulation is an unnecessarily hardproblem if the goal is estimation, we construct a computationally scalablemethodology that allows unbiased estimation of the required expectations --without explicit simulation from the full posterior. The scheme's variance isfinite by construction and straightforward to control, leading to algorithmsthat are provably unbiased and naturally arrive at a desired error tolerance.This is achieved at an average computational complexity that is sub-linear inthe size of the dataset and its free parameters are easy to tune. Wedemonstrate the utility and generality of the methodology on a range of commonstatistical models applied to large-scale benchmark and real-world datasets.
arxiv-1501-03227 | Using Riemannian geometry for SSVEP-based Brain Computer Interface |  http://arxiv.org/abs/1501.03227  | author:Emmanuel K. Kalunga, Sylvain Chevallier, Quentin Barthelemy category:cs.LG stat.ML published:2015-01-14 summary:Riemannian geometry has been applied to Brain Computer Interface (BCI) forbrain signals classification yielding promising results. Studyingelectroencephalographic (EEG) signals from their associated covariance matricesallows a mitigation of common sources of variability (electronic, electrical,biological) by constructing a representation which is invariant to theseperturbations. While working in Euclidean space with covariance matrices isknown to be error-prone, one might take advantage of algorithmic advances ininformation geometry and matrix manifold to implement methods for SymmetricPositive-Definite (SPD) matrices. This paper proposes a comprehensive review ofthe actual tools of information geometry and how they could be applied oncovariance matrices of EEG. In practice, covariance matrices should beestimated, thus a thorough study of all estimators is conducted on real EEGdataset. As a main contribution, this paper proposes an online implementationof a classifier in the Riemannian space and its subsequent assessment inSteady-State Visually Evoked Potential (SSVEP) experimentations.
arxiv-1501-03347 | Dirichlet Process Parsimonious Mixtures for clustering |  http://arxiv.org/abs/1501.03347  | author:Faicel Chamroukhi, Marius Bartcus, Hervé Glotin category:stat.ML cs.LG stat.ME published:2015-01-14 summary:The parsimonious Gaussian mixture models, which exploit an eigenvaluedecomposition of the group covariance matrices of the Gaussian mixture, haveshown their success in particular in cluster analysis. Their estimation is ingeneral performed by maximum likelihood estimation and has also been consideredfrom a parametric Bayesian prospective. We propose new Dirichlet ProcessParsimonious mixtures (DPPM) which represent a Bayesian nonparametricformulation of these parsimonious Gaussian mixture models. The proposed DPPMmodels are Bayesian nonparametric parsimonious mixture models that allow tosimultaneously infer the model parameters, the optimal number of mixturecomponents and the optimal parsimonious mixture structure from the data. Wedevelop a Gibbs sampling technique for maximum a posteriori (MAP) estimation ofthe developed DPMM models and provide a Bayesian model selection framework byusing Bayes factors. We apply them to cluster simulated data and real datasets, and compare them to the standard parsimonious mixture models. Theobtained results highlight the effectiveness of the proposed nonparametricparsimonious mixture models as a good nonparametric alternative for theparametric parsimonious models.
arxiv-1501-03291 | Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models |  http://arxiv.org/abs/1501.03291  | author:Michael U. Gutmann, Jukka Corander category:stat.ML stat.CO stat.ME published:2015-01-14 summary:Our paper deals with inferring simulator-based statistical models given someobserved data. A simulator-based model is a parametrized mechanism whichspecifies how data are generated. It is thus also referred to as generativemodel. We assume that only a finite number of parameters are of interest andallow the generative process to be very general; it may be a noisy nonlineardynamical system with an unrestricted number of hidden variables. This weakassumption is useful for devising realistic models but it renders statisticalinference very difficult. The main challenge is the intractability of thelikelihood function. Several likelihood-free inference methods have beenproposed which share the basic idea of identifying the parameters by findingvalues for which the discrepancy between simulated and observed data is small.A major obstacle to using these methods is their computational cost. The costis largely due to the need to repeatedly simulate data sets and the lack ofknowledge about how the parameters affect the discrepancy. We propose astrategy which combines probabilistic modeling of the discrepancy withoptimization to facilitate likelihood-free inference. The strategy isimplemented using Bayesian optimization and is shown to accelerate theinference through a reduction in the number of required simulations by severalorders of magnitude.
arxiv-1501-03214 | Quantifying Prosodic Variability in Middle English Alliterative Poetry |  http://arxiv.org/abs/1501.03214  | author:Roger Bilisoly category:stat.AP cs.CL published:2015-01-14 summary:Interest in the mathematical structure of poetry dates back to at least the19th century: after retiring from his mathematics position, J. J. Sylvesterwrote a book on prosody called $\textit{The Laws of Verse}$. Today there isinterest in the computer analysis of poems, and this paper discusses how astatistical approach can be applied to this task. Starting with the definitionof what Middle English alliteration is, $\textit{Sir Gawain and the GreenKnight}$ and William Langland's $\textit{Piers Plowman}$ are used to illustratethe methodology. Theory first developed for analyzing data from a Riemannianmanifold turns out to be applicable to strings allowing one to compute ageneralized mean and variance for textual data, which is applied to the poemsabove. The ratio of these two variances produces the analogue of the F test,and resampling allows p-values to be estimated. Consequently, this methodologyprovides a way to compare prosodic variability between two texts.
arxiv-1501-03271 | Higher dimensional homodyne filtering for suppression of incidental phase artifacts in multichannel MRI |  http://arxiv.org/abs/1501.03271  | author:Joseph Suresh Paul, Uma Krishna Swamy Pillai category:cs.CV physics.med-ph published:2015-01-14 summary:The aim of this paper is to introduce procedural steps for extension of the1D homodyne phase correction for k-space truncation in all gradient encodingdirections. Compared to the existing method applied to 2D partial k-space,signal losses introduced by the phase correction filter is observed to beminimal for the extended approach. In addition, the modified form of phasecorrection mitigates Incidental Phase Artifacts (IPA) due to truncation. Forparallel imaging with undersampling along phase encode direction, the extendedhomodyne filtering is shown to be effective for minimizing these artifacts wheneach of the channel k-spaces are truncated along both phase and frequencyencode directions. This is illustrated with 2D partial k-space for flowcompensated multichannel Susceptibility Weighted Imaging (SWI). Extension ofour method to 3D partial k-space shows improved reconstruction of flowinformation in phase contrast angiography.
arxiv-1501-03320 | Image enhancement in intensity projected multichannel MRI using spatially adaptive directional anisotropic diffusion |  http://arxiv.org/abs/1501.03320  | author:P. K. Akshara, J. S. Paul category:cs.CV published:2015-01-14 summary:Anisotropic Diffusion is widely used for noise reduction with simultaneouspreservation of vascular structures in maximum intensity projected (MIP)angiograms. However, extension to minimum intensity projected (mIP) venogramsin Susceptibility Weighted Imaging (SWI) poses difficulties due to spatiallyvarying baseline. Here, we introduce a modified version of the directionalanisotropic diffusion which allows us to simultaneously reduce the noise andenhance vascular structures reconstructed using both M/mIP angiograms. Thismethod is based on spatial adaptation of the diffusion function, separately inthe directions of the gradient, and along those of the minimum and maximumcurvatures. The existing approach of directional anisotropic diffusion usesbinary switched diffusion function to ensure diffusion along the direction ofmaximum curvature stopped near the vessel borders. Here, the choice of athreshold for detecting the upper limit of diffusion becomes difficult in thepresence of spatially varying baseline. Also, the approach of using vesselnessmeasure to steer the diffusion process results in structural discontinuitiesdue to junction suppression in mIP. The merits of the proposed method includeelimination of the need for an apriori choice of a threshold to detect thevessel, and problems due to junction suppression. The proposed method is alsoextended to multi-channel phase contrast angiogram.
arxiv-1501-03302 | Hard to Cheat: A Turing Test based on Answering Questions about Images |  http://arxiv.org/abs/1501.03302  | author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.LG published:2015-01-14 summary:Progress in language and image understanding by machines has sparkled theinterest of the research community in more open-ended, holistic tasks, andrefueled an old AI dream of building intelligent machines. We discuss a fewprominent challenges that characterize such holistic tasks and argue for"question answering about images" as a particular appealing instance of such aholistic task. In particular, we point out that it is a version of a TuringTest that is likely to be more robust to over-interpretations and contrast itwith tasks like grounding and generation of descriptions. Finally, we discusstools to measure progress in this field.
arxiv-1501-02995 | Improved 8-point Approximate DCT for Image and Video Compression Requiring Only 14 Additions |  http://arxiv.org/abs/1501.02995  | author:U. S. Potluri, A. Madanayake, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Edirisuriya category:cs.MM cs.CV cs.NA stat.ME published:2015-01-13 summary:Video processing systems such as HEVC requiring low energy consumption neededfor the multimedia market has lead to extensive development in fast algorithmsfor the efficient approximation of 2-D DCT transforms. The DCT is employed in amultitude of compression standards due to its remarkable energy compactionproperties. Multiplier-free approximate DCT transforms have been proposed thatoffer superior compression performance at very low circuit complexity. Suchapproximations can be realized in digital VLSI hardware using additions andsubtractions only, leading to significant reductions in chip area and powerconsumption compared to conventional DCTs and integer transforms. In thispaper, we introduce a novel 8-point DCT approximation that requires only 14addition operations and no multiplications. The proposed transform possesseslow computational complexity and is compared to state-of-the-art DCTapproximations in terms of both algorithm complexity and peak signal-to-noiseratio. The proposed DCT approximation is a candidate for reconfigurable videostandards such as HEVC. The proposed transform and several other DCTapproximations are mapped to systolic-array digital architectures andphysically realized as digital prototype circuits using FPGA technology andmapped to 45 nm CMOS technology.
arxiv-1501-02990 | Random Bits Regression: a Strong General Predictor for Big Data |  http://arxiv.org/abs/1501.02990  | author:Yi Wang, Yi Li, Momiao Xiong, Li Jin category:stat.ML cs.LG published:2015-01-13 summary:To improve accuracy and speed of regressions and classifications, we presenta data-based prediction method, Random Bits Regression (RBR). This method firstgenerates a large number of random binary intermediate/derived features basedon the original input matrix, and then performs regularized linear/logisticregression on those intermediate/derived features to predict the outcome.Benchmark analyses on a simulated dataset, UCI machine learning repositorydatasets and a GWAS dataset showed that RBR outperforms other popular methodsin accuracy and robustness. RBR (available onhttps://sourceforge.net/projects/rbr/) is very fast and requires reasonablememories, therefore, provides a strong, robust and fast predictor in the bigdata era.
arxiv-1501-03001 | On Generalizing the C-Bound to the Multiclass and Multi-label Settings |  http://arxiv.org/abs/1501.03001  | author:Francois Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy category:stat.ML cs.LG published:2015-01-13 summary:The C-bound, introduced in Lacasse et al., gives a tight upper bound on therisk of a binary majority vote classifier. In this work, we present a firststep towards extending this work to more complex outputs, by providinggeneralizations of the C-bound to the multiclass and multi-label settings.
arxiv-1501-02894 | A Modified No Search Algorithm for Fractal Image Compression |  http://arxiv.org/abs/1501.02894  | author:Mehdi. Salarian, Babak. Mohamadinia, Jalil Rasekhi category:cs.CV cs.MM published:2015-01-13 summary:Fractal image compression has some desirable properties like high quality athigh compression ratio, fast decoding, and resolution independence. Thereforeit can be used for many applications such as texture mapping and patternrecognition and image watermarking. But it suffers from long encoding time dueto its need to find the best match between sub blocks. This time is related tothe approach that is used. In this paper we present a fast encoding Algorithmbased on no search method. Our goal is that more blocks are covered in initialstep of quad tree algorithm. Experimental result has been compared with othernew fast fractal coding methods, showing it is better in term of bit rate insame condition while the other parameters are fixed.
arxiv-1501-02859 | $\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates and Convergence Guarantees |  http://arxiv.org/abs/1501.02859  | author:Saiprasad Ravishankar, Yoram Bresler category:stat.ML cs.LG published:2015-01-13 summary:Many applications in signal processing benefit from the sparsity of signalsin a certain transform domain or dictionary. Synthesis sparsifying dictionariesthat are directly adapted to data have been popular in applications such asimage denoising, inpainting, and medical image reconstruction. In this work, wefocus instead on the sparsifying transform model, and study the learning ofwell-conditioned square sparsifying transforms. The proposed algorithmsalternate between a $\ell_0$ "norm"-based sparse coding step, and a non-convextransform update step. We derive the exact analytical solution for each ofthese steps. The proposed solution for the transform update step achieves theglobal minimum in that step, and also provides speedups over iterativesolutions involving conjugate gradients. We establish that our alternatingalgorithms are globally convergent to the set of local minimizers of thenon-convex transform learning problems. In practice, the algorithms areinsensitive to initialization. We present results illustrating the promisingperformance and significant speed-ups of transform learning over synthesisK-SVD in image denoising.
arxiv-1501-03209 | Neural Implementation of Probabilistic Models of Cognition |  http://arxiv.org/abs/1501.03209  | author:Milad Kharratzadeh, Thomas R. Shultz category:cs.NE q-bio.NC published:2015-01-13 summary:Bayesian models of cognition hypothesize that human brains make sense of databy representing probability distributions and applying Bayes' rule to find thebest explanation for available data. Understanding the neural mechanismsunderlying probabilistic models remains important because Bayesian modelsprovide a computational framework, rather than specifying mechanisticprocesses. Here, we propose a deterministic neural-network model whichestimates and represents probability distributions from observable events --- aphenomenon related to the concept of probability matching. Our model learns torepresent probabilities without receiving any representation of them from theexternal world, but rather by experiencing the occurrence patterns ofindividual events. Our neural implementation of probability matching is pairedwith a neural module applying Bayes' rule, forming a comprehensive neuralscheme to simulate human Bayesian learning and inference. Our model alsoprovides novel explanations of base-rate neglect, a notable deviation fromBayes.
arxiv-1501-03002 | An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context |  http://arxiv.org/abs/1501.03002  | author:Pascal Germain, Amaury Habrard, Francois Laviolette, Emilie Morvant category:stat.ML cs.LG published:2015-01-13 summary:This paper provides a theoretical analysis of domain adaptation based on thePAC-Bayesian theory. We propose an improvement of the previous domainadaptation bound obtained by Germain et al. in two ways. We first give anothergeneralization bound tighter and easier to interpret. Moreover, we provide anew analysis of the constant term appearing in the bound that can be of highinterest for developing new algorithmic solutions.
arxiv-1501-03084 | Deep Learning with Nonparametric Clustering |  http://arxiv.org/abs/1501.03084  | author:Gang Chen category:cs.LG 68T10 I.2.6 published:2015-01-13 summary:Clustering is an essential problem in machine learning and data mining. Onevital factor that impacts clustering performance is how to learn or design thedata representation (or features). Fortunately, recent advances in deeplearning can learn unsupervised features effectively, and have yielded state ofthe art performance in many classification problems, such as characterrecognition, object recognition and document categorization. However, littleattention has been paid to the potential of deep learning for unsupervisedclustering problems. In this paper, we propose a deep belief network withnonparametric clustering. As an unsupervised method, our model first leveragesthe advantages of deep learning for feature representation and dimensionreduction. Then, it performs nonparametric clustering under a maximum marginframework -- a discriminative clustering model and can be trained onlineefficiently in the code space. Lastly model parameters are refined in the deepbelief network. Thus, this model can learn features for clustering and infermodel complexity in an unified framework. The experimental results show theadvantage of our approach over competitive baselines.
arxiv-1501-03015 | Exploring the efficacy of molecular fragments of different complexity in computational SAR modeling |  http://arxiv.org/abs/1501.03015  | author:Albrecht Zimmermann, Björn Bringmann, Luc De Raedt category:cs.CE cs.LG published:2015-01-13 summary:An important first step in computational SAR modeling is to transform thecompounds into a representation that can be processed by predictive modelingtechniques. This is typically a feature vector where each feature indicates thepresence or absence of a molecular fragment. While the traditional approach toSAR modeling employed size restricted fingerprints derived from path fragments,much research in recent years focussed on mining more complex graph basedfragments. Today, there seems to be a growing consensus in the data miningcommunity that these more expressive fragments should be more useful. Wequestion this consensus and show experimentally that fragments of lowcomplexity, i.e. sequences, perform better than equally large sets of morecomplex ones, an effect we explain by pairwise correlation among fragments andthe ability of a fragment set to encode compounds from different classesdistinctly. The size restriction on these sets is based on ordering thefragments by class-correlation scores. In addition, we also evaluate theeffects of using a significance value instead of a length restriction for pathfragments and find a significant reduction in the number of features withlittle loss in performance.
