arxiv-600-1 | Semi-Supervised Collective Classification via Hybrid Label Regularization | http://arxiv.org/pdf/1206.6467v1.pdf | author:Luke McDowell, David Aha category:cs.LG stat.ML published:2012-06-27 summary:Many classification problems involve data instances that are interlinked witheach other, such as webpages connected by hyperlinks. Techniques for"collective classification" (CC) often increase accuracy for such data graphs,but usually require a fully-labeled training graph. In contrast, we examine howto improve the semi-supervised learning of CC models when given only asparsely-labeled graph, a common situation. We first describe how to use novelcombinations of classifiers to exploit the different characteristics of therelational features vs. the non-relational features. We also extend the ideasof "label regularization" to such hybrid classifiers, enabling them to leveragethe unlabeled data to bias the learning process. We find that these techniques,which are efficient and easy to implement, significantly increase accuracy onthree real datasets. In addition, our results explain conflicting findings fromprior related studies.
arxiv-600-2 | Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation | http://arxiv.org/pdf/1206.6468v1.pdf | author:Gautham Mysore, Maneesh Sahani category:cs.LG cs.SD stat.ML published:2012-06-27 summary:The past decade has seen substantial work on the use of non-negative matrixfactorization and its probabilistic counterparts for audio source separation.Although able to capture audio spectral structure well, these models neglectthe non-stationarity and temporal dynamics that are important properties ofaudio. The recently proposed non-negative factorial hidden Markov model(N-FHMM) introduces a temporal dimension and improves source separationperformance. However, the factorial nature of this model makes the complexityof inference exponential in the number of sound sources. Here, we present aBayesian variant of the N-FHMM suited to an efficient variational inferencealgorithm, whose complexity is linear in the number of sound sources. Ouralgorithm performs comparably to exact inference in the original N-FHMM but issignificantly faster. In typical configurations of the N-FHMM, our methodachieves around a 30x increase in speed.
arxiv-600-3 | Inferring Latent Structure From Mixed Real and Categorical Relational Data | http://arxiv.org/pdf/1206.6469v1.pdf | author:Esther Salazar, Matthew Cain, Elise Darling, Stephen Mitroff, Lawrence Carin category:cs.LG stat.ML published:2012-06-27 summary:We consider analysis of relational data (a matrix), in which the rowscorrespond to subjects (e.g., people) and the columns correspond to attributes.The elements of the matrix may be a mix of real and categorical. Each subjectand attribute is characterized by a latent binary feature vector, and aninferred matrix maps each row-column pair of binary feature vectors to anobserved matrix element. The latent binary features of the rows are modeled viaa multivariate Gaussian distribution with low-rank covariance matrix, and theGaussian random variables are mapped to latent binary features via a probitlink. The same type construction is applied jointly to the columns. The modelinfers latent, low-dimensional binary features associated with each row andeach column, as well correlation structure between all rows and between allcolumns.
arxiv-600-4 | A Combinatorial Algebraic Approach for the Identifiability of Low-Rank Matrix Completion | http://arxiv.org/pdf/1206.6470v1.pdf | author:Franz Kiraly, Ryota Tomioka category:cs.LG cs.DM cs.NA stat.ML published:2012-06-27 summary:In this paper, we review the problem of matrix completion and expose itsintimate relations with algebraic geometry, combinatorics and graph theory. Wepresent the first necessary and sufficient combinatorial conditions formatrices of arbitrary rank to be identifiable from a set of matrix entries,yielding theoretical constraints and new algorithms for the problem of matrixcompletion. We conclude by algorithmically evaluating the tightness of thegiven conditions and algorithms for practically relevant matrix sizes, showingthat the algebraic-combinatoric approach can lead to improvements overstate-of-the-art matrix completion methods.
arxiv-600-5 | On Causal and Anticausal Learning | http://arxiv.org/pdf/1206.6471v1.pdf | author:Bernhard Schoelkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, Joris Mooij category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of function estimation in the case where anunderlying causal model can be inferred. This has implications for popularscenarios such as covariate shift, concept drift, transfer learning andsemi-supervised learning. We argue that causal knowledge may facilitate someapproaches for a given problem, and rule out others. In particular, weformulate a hypothesis for when semi-supervised learning can help, andcorroborate it with empirical results.
arxiv-600-6 | An Efficient Approach to Sparse Linear Discriminant Analysis | http://arxiv.org/pdf/1206.6472v1.pdf | author:Luis Francisco Sanchez Merchante, Yves Grandvalet, Gerrad Govaert category:cs.LG stat.ML published:2012-06-27 summary:We present a novel approach to the formulation and the resolution of sparseLinear Discriminant Analysis (LDA). Our proposal, is based on penalized OptimalScoring. It has an exact equivalence with penalized LDA, contrary to themulti-class approaches based on the regression of class indicator that havebeen proposed so far. Sparsity is obtained thanks to a group-Lasso penalty thatselects the same features in all discriminant directions. Our experimentsdemonstrate that this approach generates extremely parsimonious models withoutcompromising prediction performances. Besides prediction, the resulting sparsediscriminant directions are also amenable to low-dimensional representations ofdata. Our algorithm is highly efficient for medium to large number ofvariables, and is thus particularly well suited to the analysis of geneexpression data.
arxiv-600-7 | Compositional Planning Using Optimal Option Models | http://arxiv.org/pdf/1206.6473v1.pdf | author:David Silver, Kamil Ciosek category:cs.AI cs.LG published:2012-06-27 summary:In this paper we introduce a framework for option model composition. Optionmodels are temporal abstractions that, like macro-operators in classicalplanning, jump directly from a start state to an end state. Prior work hasfocused on constructing option models from primitive actions, by intra-optionmodel learning; or on using option models to construct a value function, byinter-option planning. We present a unified view of intra- and inter-optionmodel learning, based on a major generalisation of the Bellman equation. Ourfundamental operation is the recursive composition of option models into otheroption models. This key idea enables compositional planning over many levels ofabstraction. We illustrate our framework using a dynamic programming algorithmthat simultaneously constructs optimal option models for multiple subgoals, andalso searches over those option models to provide rapid progress towards othersubgoals.
arxiv-600-8 | Estimation of Simultaneously Sparse and Low Rank Matrices | http://arxiv.org/pdf/1206.6474v1.pdf | author:Emile Richard, Pierre-Andre Savalle, Nicolas Vayatis category:cs.DS cs.LG cs.NA stat.ML published:2012-06-27 summary:The paper introduces a penalized matrix estimation procedure aiming atsolutions which are sparse and low-rank at the same time. Such structures arisein the context of social networks or protein interactions where underlyinggraphs have adjacency matrices which are block-diagonal in the appropriatebasis. We introduce a convex mixed penalty which involves $\ell_1$-norm andtrace norm simultaneously. We obtain an oracle inequality which indicates howthe two effects interact according to the nature of the target matrix. We boundgeneralization error in the link prediction problem. We also develop proximaldescent strategies to solve the optimization problem efficiently and evaluateperformance on synthetic and real data sets.
arxiv-600-9 | Similarity Learning for Provably Accurate Sparse Linear Classification | http://arxiv.org/pdf/1206.6476v1.pdf | author:Aurelien Bellet, Amaury Habrard, Marc Sebban category:cs.LG stat.ML published:2012-06-27 summary:In recent years, the crucial importance of metrics in machine learningalgorithms has led to an increasing interest for optimizing distance andsimilarity functions. Most of the state of the art focus on learningMahalanobis distances (requiring to fulfill a constraint of positivesemi-definiteness) for use in a local k-NN algorithm. However, no theoreticallink is established between the learned metrics and their performance inclassification. In this paper, we make use of the formal framework of goodsimilarities introduced by Balcan et al. to design an algorithm for learning anon PSD linear similarity optimized in a nonlinear feature space, which is thenused to build a global linear classifier. We show that our approach has uniformstability and derive a generalization bound on the classification error.Experiments performed on various datasets confirm the effectiveness of ourapproach compared to state-of-the-art methods and provide evidence that (i) itis fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.
arxiv-600-10 | Discovering Support and Affiliated Features from Very High Dimensions | http://arxiv.org/pdf/1206.6477v1.pdf | author:Yiteng Zhai, Mingkui Tan, Ivor Tsang, Yew Soon Ong category:cs.LG stat.ML published:2012-06-27 summary:In this paper, a novel learning paradigm is presented to automaticallyidentify groups of informative and correlated features from very highdimensions. Specifically, we explicitly incorporate correlation measures asconstraints and then propose an efficient embedded feature selection methodusing recently developed cutting plane strategy. The benefits of the proposedalgorithm are two-folds. First, it can identify the optimal discriminative anduncorrelated feature subset to the output labels, denoted here as SupportFeatures, which brings about significant improvements in prediction performanceover other state of the art feature selection methods considered in the paper.Second, during the learning process, the underlying group structures ofcorrelated features associated with each support feature, denoted as AffiliatedFeatures, can also be discovered without any additional cost. These affiliatedfeatures serve to improve the interpretations on the learning tasks. Extensiveempirical studies on both synthetic and very high dimensional real-worlddatasets verify the validity and efficiency of the proposed method.
arxiv-600-11 | Maximum Margin Output Coding | http://arxiv.org/pdf/1206.6478v1.pdf | author:Yi Zhang, Jeff Schneider category:cs.LG stat.ML published:2012-06-27 summary:In this paper we study output coding for multi-label prediction. For amulti-label output coding to be discriminative, it is important that codewordsfor different label vectors are significantly different from each other. In themeantime, unlike in traditional coding theory, codewords in output coding areto be predicted from the input, so it is also critical to have a predictablelabel encoding. To find output codes that are both discriminative and predictable, we firstpropose a max-margin formulation that naturally captures these two properties.We then convert it to a metric learning formulation, but with an exponentiallylarge number of constraints as commonly encountered in structured predictionproblems. Without a label structure for tractable inference, we useovergenerating (i.e., relaxation) techniques combined with the cutting planemethod for optimization. In our empirical study, the proposed output coding scheme outperforms avariety of existing multi-label prediction methods for image, text and musicclassification.
arxiv-600-12 | The Landmark Selection Method for Multiple Output Prediction | http://arxiv.org/pdf/1206.6479v1.pdf | author:Krishnakumar Balasubramanian, Guy Lebanon category:cs.LG stat.ML published:2012-06-27 summary:Conditional modeling x \to y is a central problem in machine learning. Asubstantial research effort is devoted to such modeling when x is highdimensional. We consider, instead, the case of a high dimensional y, where x iseither low dimensional or high dimensional. Our approach is based on selectinga small subset y_L of the dimensions of y, and proceed by modeling (i) x \toy_L and (ii) y_L \to y. Composing these two models, we obtain a conditionalmodel x \to y that possesses convenient statistical properties. Multi-labelclassification and multivariate regression experiments on several datasets showthat this model outperforms the one vs. all approach as well as severalsophisticated multiple output prediction methods.
arxiv-600-13 | A Dantzig Selector Approach to Temporal Difference Learning | http://arxiv.org/pdf/1206.6480v1.pdf | author:Matthieu Geist, Bruno Scherrer, Alessandro Lazaric, Mohammad Ghavamzadeh category:cs.LG stat.ML published:2012-06-27 summary:LSTD is a popular algorithm for value function approximation. Whenever thenumber of features is larger than the number of samples, it must be paired withsome form of regularization. In particular, L1-regularization methods tend toperform feature selection by promoting sparsity, and thus, are well-suited forhigh-dimensional problems. However, since LSTD is not a simple regressionalgorithm, but it solves a fixed--point problem, its integration withL1-regularization is not straightforward and might come with some drawbacks(e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce anovel algorithm obtained by integrating LSTD with the Dantzig Selector. Weinvestigate the performance of the proposed algorithm and its relationship withthe existing regularized approaches, and show how it addresses some of theirdrawbacks.
arxiv-600-14 | Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning | http://arxiv.org/pdf/1206.6481v1.pdf | author:Yuhong Guo, Min Xiao category:cs.CL cs.IR cs.LG published:2012-06-27 summary:In many multilingual text classification problems, the documents in differentlanguages often share the same set of categories. To reduce the labeling costof training a classification model for each individual language, it isimportant to transfer the label knowledge gained from one language to anotherlanguage by conducting cross language classification. In this paper we developa novel subspace co-regularized multi-view learning method for cross languagetext classification. This method is built on parallel corpora produced bymachine translation. It jointly minimizes the training error of each classifierin each language while penalizing the distance between the subspacerepresentations of parallel documents. Our empirical study on a large set ofcross language text classification tasks shows the proposed method consistentlyoutperforms a number of inductive methods, domain adaptation methods, andmulti-view learning methods.
arxiv-600-15 | Modeling Images using Transformed Indian Buffet Processes | http://arxiv.org/pdf/1206.6482v1.pdf | author:Ke Zhai, Yuening Hu, Sinead Williamson, Jordan Boyd-Graber category:cs.CV cs.LG stat.ML published:2012-06-27 summary:Latent feature models are attractive for image modeling, since imagesgenerally contain multiple objects. However, many latent feature models ignorethat objects can appear at different locations or require pre-segmentation ofimages. While the transformed Indian buffet process (tIBP) provides a methodfor modeling transformation-invariant features in unsegmented binary images,its current form is inappropriate for real images because of its computationalcost and modeling assumptions. We combine the tIBP with likelihoods appropriatefor real images and develop an efficient inference, using the cross-correlationbetween images and features, that is theoretically and empirically faster thanexisting inference techniques. Our method discovers reasonable components andachieve effective image reconstruction in natural images.
arxiv-600-16 | Subgraph Matching Kernels for Attributed Graphs | http://arxiv.org/pdf/1206.6483v1.pdf | author:Nils Kriege, Petra Mutzel category:cs.LG stat.ML published:2012-06-27 summary:We propose graph kernels based on subgraph matchings, i.e.structure-preserving bijections between subgraphs. While recently proposedkernels based on common subgraphs (Wale et al., 2008; Shervashidze et al.,2009) in general can not be applied to attributed graphs, our approach allowsto rate mappings of subgraphs by a flexible scoring scheme comparing vertex andedge attributes by kernels. We show that subgraph matching kernels generalizeseveral known kernels. To compute the kernel we propose a graph-theoreticalalgorithm inspired by a classical relation between common subgraphs of twographs and cliques in their product graph observed by Levi (1973). Encouragingexperimental results on a classification task of real-world graphs arepresented.
arxiv-600-17 | Apprenticeship Learning for Model Parameters of Partially Observable Environments | http://arxiv.org/pdf/1206.6484v1.pdf | author:Takaki Makino, Johane Takeuchi category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We consider apprenticeship learning, i.e., having an agent learn a task byobserving an expert demonstrating the task in a partially observableenvironment when the model of the environment is uncertain. This setting isuseful in applications where the explicit modeling of the environment isdifficult, such as a dialogue system. We show that we can extract informationabout the environment model by inferring action selection process behind thedemonstration, under the assumption that the expert is choosing optimal actionsbased on knowledge of the true model of the target environment. Proposedalgorithms can achieve more accurate estimates of POMDP parameters and betterpolicies from a short demonstration, compared to methods that learns only fromthe reaction from the environment.
arxiv-600-18 | Greedy Algorithms for Sparse Reinforcement Learning | http://arxiv.org/pdf/1206.6485v1.pdf | author:Christopher Painter-Wakefield, Ronald Parr category:cs.LG stat.ML published:2012-06-27 summary:Feature selection and regularization are becoming increasingly prominenttools in the efforts of the reinforcement learning (RL) community to expand thereach and applicability of RL. One approach to the problem of feature selectionis to impose a sparsity-inducing form of regularization on the learning method.Recent work on $L_1$ regularization has adapted techniques from the supervisedlearning literature for use with RL. Another approach that has received renewedattention in the supervised learning community is that of using a simplealgorithm that greedily adds new features. Such algorithms have many of thegood properties of the $L_1$ regularization methods, while also being extremelyefficient and, in some cases, allowing theoretical guarantees on recovery ofthe true form of a sparse target function from sampled data. This paperconsiders variants of orthogonal matching pursuit (OMP) applied toreinforcement learning. The resulting algorithms are analyzed and comparedexperimentally with existing $L_1$ regularized approaches. We demonstrate thatperhaps the most natural scenario in which one might hope to achieve sparserecovery fails; however, one variant, OMP-BRM, provides promising theoreticalguarantees under certain assumptions on the feature dictionary. Anothervariant, OMP-TD, empirically outperforms prior methods both in approximationaccuracy and efficiency on several benchmark problems.
arxiv-600-19 | Flexible Modeling of Latent Task Structures in Multitask Learning | http://arxiv.org/pdf/1206.6486v1.pdf | author:Alexandre Passos, Piyush Rai, Jacques Wainer, Hal Daume III category:cs.LG stat.ML published:2012-06-27 summary:Multitask learning algorithms are typically designed assuming some fixed, apriori known latent structure shared by all the tasks. However, it is usuallyunclear what type of latent task structure is the most appropriate for a givenmultitask learning problem. Ideally, the "right" latent task structure shouldbe learned in a data-driven manner. We present a flexible, nonparametricBayesian model that posits a mixture of factor analyzers structure on thetasks. The nonparametric aspect makes the model expressive enough to subsumemany existing models of latent task structures (e.g, mean-regularized tasks,clustered tasks, low-rank or linear/non-linear subspace assumption on tasks,etc.). Moreover, it can also learn more general task structures, addressing theshortcomings of such models. We present a variational inference algorithm forour model. Experimental results on synthetic and real-world datasets, on bothregression and classification problems, demonstrate the effectiveness of theproposed method.
arxiv-600-20 | An Adaptive Algorithm for Finite Stochastic Partial Monitoring | http://arxiv.org/pdf/1206.6487v1.pdf | author:Gabor Bartok, Navid Zolghadr, Csaba Szepesvari category:cs.LG cs.GT stat.ML published:2012-06-27 summary:We present a new anytime algorithm that achieves near-optimal regret for anyinstance of finite stochastic partial monitoring. In particular, the newalgorithm achieves the minimax regret, within logarithmic factors, for both"easy" and "hard" problems. For easy problems, it additionally achieveslogarithmic individual regret. Most importantly, the algorithm is adaptive inthe sense that if the opponent strategy is in an "easy region" of the strategyspace then the regret grows as if the problem was easy. As an implication, weshow that under some reasonable additional assumptions, the algorithm enjoys anO(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.(2011).
arxiv-600-21 | The Nonparanormal SKEPTIC | http://arxiv.org/pdf/1206.6488v1.pdf | author:Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman category:stat.ME cs.LG stat.ML published:2012-06-27 summary:We propose a semiparametric approach, named nonparanormal skeptic, forestimating high dimensional undirected graphical models. In terms of modeling,we consider the nonparanormal family proposed by Liu et al (2009). In terms ofestimation, we exploit nonparametric rank-based correlation coefficientestimators including the Spearman's rho and Kendall's tau. In high dimensionalsettings, we prove that the nonparanormal skeptic achieves the optimalparametric rate of convergence in both graph and parameter estimation. Thisresult suggests that the nonparanormal graphical models are a safe replacementof the Gaussian graphical models, even when the data are Gaussian.
arxiv-600-22 | Learning Markov Network Structure using Brownian Distance Covariance | http://arxiv.org/pdf/1206.6361v1.pdf | author:Ehsan Khoshgnauz category:stat.ML cs.LG published:2012-06-27 summary:In this paper, we present a simple non-parametric method for learning thestructure of undirected graphs from data that drawn from an underlying unknowndistribution. We propose to use Brownian distance covariance to estimate theconditional independences between the random variables and encodes pairwiseMarkov graph. This framework can be applied in high-dimensional setting, wherethe number of parameters much be larger than the sample size.
arxiv-600-23 | Efficient Selection of Disambiguating Actions for Stereo Vision | http://arxiv.org/pdf/1206.6878v1.pdf | author:Monika Schaeffer, Ron Parr category:cs.CV published:2012-06-27 summary:In many domains that involve the use of sensors, such as robotics or sensornetworks, there are opportunities to use some form of active sensing todisambiguate data from noisy or unreliable sensors. These disambiguatingactions typically take time and expend energy. One way to choose the nextdisambiguating action is to select the action with the greatest expectedentropy reduction, or information gain. In this work, we consider activesensing in aid of stereo vision for robotics. Stereo vision is a powerfulsensing technique for mobile robots, but it can fail in scenes that lack strongtexture. In such cases, a structured light source, such as vertical laser linecan be used for disambiguation. By treating the stereo matching problem as aspecially structured HMM-like graphical model, we demonstrate that for a scanline with n columns and maximum stereo disparity d, the entropy minimizing aimpoint for the laser can be selected in O(nd) time - cost no greater than thestereo algorithm itself. In contrast, a typical HMM formulation would suggestat least O(nd^2) time for the entropy calculation alone.
arxiv-600-24 | Variable noise and dimensionality reduction for sparse Gaussian processes | http://arxiv.org/pdf/1206.6873v1.pdf | author:Edward Snelson, Zoubin Ghahramani category:cs.LG stat.ML published:2012-06-27 summary:The sparse pseudo-input Gaussian process (SPGP) is a new approximation methodfor speeding up GP regression in the case of a large number of data points N.The approximation is controlled by the gradient optimization of a small set ofM `pseudo-inputs', thereby reducing complexity from N^3 to NM^2. One limitationof the SPGP is that this optimization space becomes impractically big for highdimensional data sets. This paper addresses this limitation by performingautomatic dimensionality reduction. A projection of the input space to a lowdimensional space is learned in a supervised manner, alongside thepseudo-inputs, which now live in this reduced space. The paper alsoinvestigates the suitability of the SPGP for modeling data with input-dependentnoise. A further extension of the model is made to make it even more powerfulin this regard - we learn an uncertainty parameter for each pseudo-input. Thecombination of sparsity, reduced dimension, and input-dependent noise makes itpossible to apply GPs to much larger and more complex data sets than waspreviously practical. We demonstrate the benefits of these methods on severalsynthetic and real world problems.
arxiv-600-25 | A Self-Supervised Terrain Roughness Estimator for Off-Road Autonomous Driving | http://arxiv.org/pdf/1206.6872v1.pdf | author:David Stavens, Sebastian Thrun category:cs.CV cs.LG cs.RO published:2012-06-27 summary:We present a machine learning approach for estimating the second derivativeof a drivable surface, its roughness. Robot perception generally focuses on thefirst derivative, obstacle detection. However, the second derivative is alsoimportant due to its direct relation (with speed) to the shock the vehicleexperiences. Knowing the second derivative allows a vehicle to slow down inadvance of rough terrain. Estimating the second derivative is challenging dueto uncertainty. For example, at range, laser readings may be so sparse thatsignificant information about the surface is missing. Also, a high degree ofprecision is required in projecting laser readings. This precision may beunavailable due to latency or error in the pose estimation. We model thesesources of error as a multivariate polynomial. Its coefficients are learnedusing the shock data as ground truth -- the accelerometers are used to trainthe lasers. The resulting classifier operates on individual laser readings froma road surface described by a 3D point cloud. The classifier identifiessections of road where the second derivative is likely to be large. Thus, thevehicle can slow down in advance, reducing the shock it experiences. Thealgorithm is an evolution of one we used in the 2005 DARPA Grand Challenge. Weanalyze it using data from that route.
arxiv-600-26 | Ranking by Dependence - A Fair Criteria | http://arxiv.org/pdf/1206.6871v1.pdf | author:Harald Steck category:cs.LG stat.ML published:2012-06-27 summary:Estimating the dependences between random variables, and ranking themaccordingly, is a prevalent problem in machine learning. Pursuing frequentistand information-theoretic approaches, we first show that the p-value and themutual information can fail even in simplistic situations. We then propose twoconditions for regularizing an estimator of dependence, which leads to a simpleyet effective new measure. We discuss its advantages and compare it towell-established model-selection criteria. Apart from that, we derive a simpleconstraint for regularizing parameter estimates in a graphical model. Thisresults in an analytical approximation for the optimal value of the equivalentsample size, which agrees very well with the more involved Bayesian approach inour experiments.
arxiv-600-27 | Incremental Model-based Learners With Formal Learning-Time Guarantees | http://arxiv.org/pdf/1206.6870v1.pdf | author:Alexander L. Strehl, Lihong Li, Michael L. Littman category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Model-based learning algorithms have been shown to use experience efficientlywhen learning to solve Markov Decision Processes (MDPs) with finite state andaction spaces. However, their high computational cost due to repeatedly solvingan internal model inhibits their use in large-scale problems. We propose amethod based on real-time dynamic programming (RTDP) to speed up twomodel-based algorithms, RMAX and MBIE (model-based interval estimation),resulting in computationally much faster algorithms with little loss comparedto existing bounds. Specifically, our two new learning algorithms, RTDP-RMAXand RTDP-IE, have considerably smaller computational demands than RMAX andMBIE. We develop a general theoretical framework that allows us to prove thatboth are efficient learners in a PAC (probably approximately correct) sense. Wealso present an experimental evaluation of these new algorithms that helpsquantify the tradeoff between computational and experience demands.
arxiv-600-28 | Bayesian Random Fields: The Bethe-Laplace Approximation | http://arxiv.org/pdf/1206.6868v1.pdf | author:Max Welling, Sridevi Parise category:cs.LG stat.ML published:2012-06-27 summary:While learning the maximum likelihood value of parameters of an undirectedgraphical model is hard, modelling the posterior distribution over parametersgiven data is harder. Yet, undirected models are ubiquitous in computer visionand text modelling (e.g. conditional random fields). But where Bayesianapproaches for directed models have been very successful, a proper Bayesiantreatment of undirected models in still in its infant stages. We propose a newmethod for approximating the posterior of the parameters given data based onthe Laplace approximation. This approximation requires the computation of thecovariance matrix over features which we compute using the linear responseapproximation based in turn on loopy belief propagation. We develop the theoryfor conditional and 'unconditional' random fields with or without hiddenvariables. In the conditional setting we introduce a new variant of baggingsuitable for structured domains. Here we run the loopy max-product algorithm ona 'super-graph' composed of graphs for individual models sampled from theposterior and connected by constraints. Experiments on real world data validatethe proposed methods.
arxiv-600-29 | A Non-Parametric Bayesian Method for Inferring Hidden Causes | http://arxiv.org/pdf/1206.6865v1.pdf | author:Frank Wood, Thomas Griffiths, Zoubin Ghahramani category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We present a non-parametric Bayesian approach to structure learning withhidden causes. Previous Bayesian treatments of this problem define a prior overthe number of hidden causes and use algorithms such as reversible jump Markovchain Monte Carlo to move between solutions. In contrast, we assume that thenumber of hidden causes is unbounded, but only a finite number influenceobservable variables. This makes it possible to use a Gibbs sampler toapproximate the distribution over causal structures. We evaluate theperformance of both approaches in discovering hidden causes in simulated data,and use our non-parametric approach to discover hidden causes in a real medicaldataset.
arxiv-600-30 | Infinite Hidden Relational Models | http://arxiv.org/pdf/1206.6864v1.pdf | author:Zhao Xu, Volker Tresp, Kai Yu, Hans-Peter Kriegel category:cs.AI cs.DB cs.LG published:2012-06-27 summary:In many cases it makes sense to model a relationship symmetrically, notimplying any particular directionality. Consider the classical example of arecommendation system where the rating of an item by a user shouldsymmetrically be dependent on the attributes of both the user and the item. Theattributes of the (known) relationships are also relevant for predictingattributes of entities and for predicting attributes of new relations. Inrecommendation systems, the exploitation of relational attributes is oftenreferred to as collaborative filtering. Again, in many applications one mightprefer to model the collaborative effect in a symmetrical way. In this paper wepresent a relational model, which is completely symmetrical. The key innovationis that we introduce for each entity (or object) an infinite-dimensional latentvariable as part of a Dirichlet process (DP) model. We discuss inference in themodel, which is based on a DP Gibbs sampler, i.e., the Chinese restaurantprocess. We extend the Chinese restaurant process to be applicable torelational modeling. Our approach is evaluated in three applications. One is arecommendation system based on the MovieLens data set. The second applicationconcerns the prediction of the function of yeast genes/proteins on the data setof KDD Cup 2001 using a multi-relational model. The third application involvesa relational medical domain. The experimental results show that our model givessignificantly improved estimates of attributes describing relationships orentities in complex relational models.
arxiv-600-31 | Bayesian Multicategory Support Vector Machines | http://arxiv.org/pdf/1206.6863v1.pdf | author:Zhihua Zhang, Michael I. Jordan category:cs.LG stat.ML published:2012-06-27 summary:We show that the multi-class support vector machine (MSVM) proposed by Leeet. al. (2004), can be viewed as a MAP estimation procedure under anappropriate probabilistic interpretation of the classifier. We also show thatthis interpretation can be extended to a hierarchical Bayesian architecture andto a fully-Bayesian inference procedure for multi-class classification based ondata augmentation. We present empirical results that show that the advantagesof the Bayesian formalism are obtained without a loss in classificationaccuracy.
arxiv-600-32 | On the Number of Samples Needed to Learn the Correct Structure of a Bayesian Network | http://arxiv.org/pdf/1206.6862v1.pdf | author:Or Zuk, Shiri Margel, Eytan Domany category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Bayesian Networks (BNs) are useful tools giving a natural and compactrepresentation of joint probability distributions. In many applications oneneeds to learn a Bayesian Network (BN) from data. In this context, it isimportant to understand the number of samples needed in order to guarantee asuccessful learning. Previous work have studied BNs sample complexity, yet itmainly focused on the requirement that the learned distribution will be closeto the original distribution which generated the data. In this work, we study adifferent aspect of the learning, namely the number of samples needed in orderto learn the correct structure of the network. We give both asymptotic results,valid in the large sample limit, and experimental results, demonstrating thelearning behavior for feasible sample sizes. We show that structure learning isa more difficult task, compared to approximating the correct distribution, inthe sense that it requires a much larger number of samples, regardless of thecomputational power available for the learner.
arxiv-600-33 | Predicting Conditional Quantiles via Reduction to Classification | http://arxiv.org/pdf/1206.6860v1.pdf | author:John Langford, Roberto Oliveira, Bianca Zadrozny category:cs.LG stat.ML published:2012-06-27 summary:We show how to reduce the process of predicting general order statistics (andthe median in particular) to solving classification. The accompanyingtheoretical statement shows that the regret of the classifier bounds the regretof the quantile regression under a quantile loss. We also test this reductionempirically against existing quantile regression methods on large real-worlddatasets and discover that it provides state-of-the-art performance.
arxiv-600-34 | Sequential Document Representations and Simplicial Curves | http://arxiv.org/pdf/1206.6858v1.pdf | author:Guy Lebanon category:cs.IR cs.LG published:2012-06-27 summary:The popular bag of words assumption represents a document as a histogram ofword occurrences. While computationally efficient, such a representation isunable to maintain any sequential information. We present a continuous anddifferentiable sequential document representation that goes beyond the bag ofwords assumption, and yet is efficient and effective. This representationemploys smooth curves in the multinomial simplex to account for sequentialinformation. We discuss the representation and its geometric properties anddemonstrate its applicability for the task of text classification.
arxiv-600-35 | Faster Gaussian Summation: Theory and Experiment | http://arxiv.org/pdf/1206.6857v1.pdf | author:Dongryeol Lee, Alexander G. Gray category:cs.LG cs.NA stat.ML published:2012-06-27 summary:We provide faster algorithms for the problem of Gaussian summation, whichoccurs in many machine learning methods. We develop two new extensions - anO(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and anew error control scheme integrating any arbitrary approximation method -within the best discretealgorithmic framework using adaptive hierarchical datastructures. We rigorously evaluate these techniques empirically in the contextof optimal bandwidth selection in kernel density estimation, revealing thestrengths and weaknesses of current state-of-the-art approaches for the firsttime. Our results demonstrate that the new error control scheme yields improvedperformance, whereas the series expansion approach is only effective in lowdimensions (five or less).
arxiv-600-36 | Online Structured Prediction via Coactive Learning | http://arxiv.org/pdf/1205.4213v2.pdf | author:Pannaga Shivaswamy, Thorsten Joachims category:cs.LG cs.AI cs.IR published:2012-05-18 summary:We propose Coactive Learning as a model of interaction between a learningsystem and a human user, where both have the common goal of providing resultsof maximum utility to the user. At each step, the system (e.g. search engine)receives a context (e.g. query) and predicts an object (e.g. ranking). The userresponds by correcting the system if necessary, providing a slightly improved-- but not necessarily optimal -- object as feedback. We argue that suchfeedback can often be inferred from observable user behavior, for example, fromclicks in web-search. Evaluating predictions by their cardinal utility to theuser, we propose efficient learning algorithms that have ${\calO}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithmnever observes cardinal utility values as in conventional online learning. Wedemonstrate the applicability of our model and learning algorithms on a movierecommendation task, as well as ranking for web-search.
arxiv-600-37 | Structured Priors for Structure Learning | http://arxiv.org/pdf/1206.6852v1.pdf | author:Vikash Mansinghka, Charles Kemp, Thomas Griffiths, Joshua Tenenbaum category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Traditional approaches to Bayes net structure learning typically assumelittle regularity in graph structure other than sparseness. However, in manycases, we expect more systematicity: variables in real-world systems oftengroup into classes that predict the kinds of probabilistic dependencies theyparticipate in. Here we capture this form of prior knowledge in a hierarchicalBayesian framework, and exploit it to enable structure learning and typediscovery from small datasets. Specifically, we present a nonparametricgenerative model for directed acyclic graphs as a prior for Bayes net structurelearning. Our model assumes that variables come in one or more classes and thatthe prior probability of an edge existing between two variables is a functiononly of their classes. We derive an MCMC algorithm for simultaneous inferenceof the number of classes, the class assignments of variables, and the Bayes netstructure over variables. For several realistic, sparse datasets, we show thatthe bias towards systematicity of connections provided by our model yields moreaccurate learned networks than a traditional, uniform prior approach, and thatthe classes found by our model are appropriate.
arxiv-600-38 | A compact, hierarchical Q-function decomposition | http://arxiv.org/pdf/1206.6851v1.pdf | author:Bhaskara Marthi, Stuart Russell, David Andre category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Previous work in hierarchical reinforcement learning has faced a dilemma:either ignore the values of different possible exit states from a subroutine,thereby risking suboptimal behavior, or represent those values explicitlythereby incurring a possibly large representation cost because exit valuesrefer to nonlocal aspects of the world (i.e., all subsequent rewards). Thispaper shows that, in many cases, one can avoid both of these problems. Thesolution is based on recursively decomposing the exit value function in termsof Q-functions at higher levels of the hierarchy. This leads to an intuitivelyappealing runtime architecture in which a parent subroutine passes to its childa value function on the exit states and the child reasons about how its choicesaffect the exit value. We also identify structural conditions on the valuefunction and transition distributions that allow much more conciserepresentations of exit state distributions, leading to further stateabstraction. In essence, the only variables whose exit values need beconsidered are those that the parent cares about and the child affects. Wedemonstrate the utility of our algorithms on a series of increasingly complexenvironments.
arxiv-600-39 | Identifying the Relevant Nodes Without Learning the Model | http://arxiv.org/pdf/1206.6847v1.pdf | author:Jose M. Pena, Roland Nilsson, Johan Björkegren, Jesper Tegnér category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We propose a method to identify all the nodes that are relevant to computeall the conditional probability distributions for a given set of nodes. Ourmethod is simple, effcient, consistent, and does not require learning aBayesian network first. Therefore, our method can be applied tohigh-dimensional databases, e.g. gene expression databases.
arxiv-600-40 | Approximate Separability for Weak Interaction in Dynamic Systems | http://arxiv.org/pdf/1206.6846v1.pdf | author:Avi Pfeffer category:cs.LG cs.AI stat.ML published:2012-06-27 summary:One approach to monitoring a dynamic system relies on decomposition of thesystem into weakly interacting subsystems. An earlier paper introduced a notionof weak interaction called separability, and showed that it leads to exactpropagation of marginals for prediction. This paper addresses two questionsleft open by the earlier paper: can we define a notion of approximateseparability that occurs naturally in practice, and do separability andapproximate separability lead to accurate monitoring? The answer to bothquestions is afirmative. The paper also analyzes the structure of approximatelyseparable decompositions, and provides some explanation as to why these modelsperform well.
arxiv-600-41 | Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick Breaking Representation | http://arxiv.org/pdf/1206.6845v1.pdf | author:Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling category:stat.ME cs.LG stat.ML published:2012-06-27 summary:Nonparametric Bayesian approaches to clustering, information retrieval,language modeling and object recognition have recently shown great promise as anew paradigm for unsupervised data analysis. Most contributions have focused onthe Dirichlet process mixture models or extensions thereof for which efficientGibbs samplers exist. In this paper we explore Gibbs samplers for infinitecomplexity mixture models in the stick breaking representation. The advantageof this representation is improved modeling flexibility. For instance, one candesign the prior distribution over cluster sizes or couple multiple infinitemixture models (e.g. over time) at the level of their parameters (i.e. thedependent Dirichlet process model). However, Gibbs samplers for infinitemixture models (as recently introduced in the statistics literature) seem tomix poorly over cluster labels. Among others issues, this can have the adverseeffect that labels for the same cluster in coupled mixture models are mixed up.We introduce additional moves in these samplers to improve mixing over clusterlabels and to bring clusters into correspondence. An application to modeling ofstorm trajectories is used to illustrate these ideas.
arxiv-600-42 | Chi-square Tests Driven Method for Learning the Structure of Factored MDPs | http://arxiv.org/pdf/1206.6842v1.pdf | author:Thomas Degris, Olivier Sigaud, Pierre-Henri Wuillemin category:cs.LG cs.AI stat.ML published:2012-06-27 summary:SDYNA is a general framework designed to address large stochasticreinforcement learning problems. Unlike previous model based methods in FMDPs,it incrementally learns the structure and the parameters of a RL problem usingsupervised learning techniques. Then, it integrates decision-theoric planningalgorithms based on FMDPs to compute its policy. SPITI is an instanciation ofSDYNA that exploits ITI, an incremental decision tree algorithm, to learn thereward function and the Dynamic Bayesian Networks with local structuresrepresenting the transition function of the problem. These representations areused by an incremental version of the Structured Value Iteration algorithm. Inorder to learn the structure, SPITI uses Chi-Square tests to detect theindependence between two probability distributions. Thus, we study the relationbetween the threshold used in the Chi-Square test, the size of the model builtand the relative error of the value function of the induced policy with respectto the optimal value. We show that, on stochastic problems, one can tune thethreshold so as to generate both a compact model and an efficient policy. Then,we show that SPITI, while keeping its model compact, uses the generalizationproperty of its learning method to perform better than a stochastic classicaltabular algorithm in large RL problem with an unknown structure. We alsointroduce a new measure based on Chi-Square to qualify the accuracy of themodel learned by SPITI. We qualitatively show that the generalization propertyin SPITI within the FMDP framework may prevent an exponential growth of thetime required to learn the structure of large stochastic RL problems.
arxiv-600-43 | Continuous Time Markov Networks | http://arxiv.org/pdf/1206.6838v1.pdf | author:Tal El-Hay, Nir Friedman, Daphne Koller, Raz Kupferman category:cs.AI cs.LG published:2012-06-27 summary:A central task in many applications is reasoning about processes that changein a continuous time. The mathematical framework of Continuous Time MarkovProcesses provides the basic foundations for modeling such systems. Recently,Nodelman et al introduced continuous time Bayesian networks (CTBNs), whichallow a compact representation of continuous-time processes over a factoredstate space. In this paper, we introduce continuous time Markov networks(CTMNs), an alternative representation language that represents a differenttype of continuous-time dynamics. In many real life processes, such asbiological and chemical systems, the dynamics of the process can be naturallydescribed as an interplay between two forces - the tendency of each entity tochange its state, and the overall fitness or energy function of the entiresystem. In our model, the first force is described by a continuous-timeproposal process that suggests possible local changes to the state of thesystem at different rates. The second force is represented by a Markov networkthat encodes the fitness, or desirability, of different states; a proposedlocal change is then accepted with a probability that is a function of thechange in the fitness distribution. We show that the fitness distribution isalso the stationary distribution of the Markov process, so that thisrepresentation provides a characterization of a temporal process whosestationary distribution has a compact graphical representation. This allows usto naturally capture a different type of structure in complex dynamicalprocesses, such as evolving biological sequences. We describe the semantics ofthe representation, its basic properties, and how it compares to CTBNs. We alsoprovide algorithms for learning such models from data, and discuss itsapplicability to biological sequence evolution.
arxiv-600-44 | Matrix Tile Analysis | http://arxiv.org/pdf/1206.6833v1.pdf | author:Inmar Givoni, Vincent Cheung, Brendan J. Frey category:cs.LG cs.CE cs.NA stat.ML published:2012-06-27 summary:Many tasks require finding groups of elements in a matrix of numbers, symbolsor class likelihoods. One approach is to use efficient bi- or tri-linearfactorization techniques including PCA, ICA, sparse matrix factorization andplaid analysis. These techniques are not appropriate when addition andmultiplication of matrix elements are not sensibly defined. More directly,methods like bi-clustering can be used to classify matrix elements, but thesemethods make the overly-restrictive assumption that the class of each elementis a function of a row class and a column class. We introduce a generalcomputational problem, `matrix tile analysis' (MTA), which consists ofdecomposing a matrix into a set of non-overlapping tiles, each of which isdefined by a subset of usually nonadjacent rows and columns. MTA does notrequire an algebra for combining tiles, but must search over discretecombinations of tile assignments. Exact MTA is a computationally intractableinteger programming problem, but we describe an approximate iterative techniqueand a computationally efficient sum-product relaxation of the integer program.We compare the effectiveness of these methods to PCA and plaid on hundreds ofrandomly generated tasks. Using double-gene-knockout data, we show that MTAfinds groups of interacting yeast genes that have biologically-relatedfunctions.
arxiv-600-45 | Convex Structure Learning for Bayesian Networks: Polynomial Feature Selection and Approximate Ordering | http://arxiv.org/pdf/1206.6832v1.pdf | author:Yuhong Guo, Dale Schuurmans category:cs.LG stat.ML published:2012-06-27 summary:We present a new approach to learning the structure and parameters of aBayesian network based on regularized estimation in an exponential familyrepresentation. Here we show that, given a fixed variable order, the optimalstructure and parameters can be learned efficiently, even without restrictingthe size of the parent sets. We then consider the problem of optimizing thevariable order for a given set of features. This is still a computationallyhard problem, but we present a convex relaxation that yields an optimal 'soft'ordering in polynomial time. One novel aspect of the approach is that we do notperform a discrete search over DAG structures, nor over variable orders, butinstead solve a continuous relaxation that can then be rounded to obtain avalid network structure. We conduct an experimental comparison against standardstructure search procedures over standard objectives, which cope with localminima, and evaluate the advantages of using convex relaxations that reduce theeffects of local minima.
arxiv-600-46 | The AI&M Procedure for Learning from Incomplete Data | http://arxiv.org/pdf/1206.6830v1.pdf | author:Manfred Jaeger category:stat.ME cs.AI cs.LG published:2012-06-27 summary:We investigate methods for parameter learning from incomplete data that isnot missing at random. Likelihood-based methods then require the optimizationof a profile likelihood that takes all possible missingness mechanisms intoaccount. Optimzing this profile likelihood poses two main difficulties:multiple (local) maxima, and its very high-dimensional parameter space. In thispaper a new method is presented for optimizing the profile likelihood thataddresses the second difficulty: in the proposed AI&M (adjusting imputation andmazimization) procedure the optimization is performed by operations in thespace of data completions, rather than directly in the parameter space of theprofile likelihood. We apply the AI&M method to learning parameters forBayesian networks. The method is compared against conservative inference, whichtakes into account each possible data completion, and against EM. The resultsindicate that likelihood-based inference is still feasible in the case ofunknown missingness mechanisms, and that conservative inference isunnecessarily weak. On the other hand, our results also provide evidence thatthe EM algorithm is still quite effective when the data is not missing atrandom.
arxiv-600-47 | Advances in exact Bayesian structure discovery in Bayesian networks | http://arxiv.org/pdf/1206.6828v1.pdf | author:Mikko Koivisto category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We consider a Bayesian method for learning the Bayesian network structurefrom complete data. Recently, Koivisto and Sood (2004) presented an algorithmthat for any single edge computes its marginal posterior probability in O(n2^n) time, where n is the number of attributes; the number of parents perattribute is bounded by a constant. In this paper we show that the posteriorprobabilities for all the n (n - 1) potential edges can be computed in O(n 2^n)total time. This result is achieved by a forward-backward technique and fastMoebius transform algorithms, which are of independent interest. The resultingspeedup by a factor of about n^2 allows us to experimentally study thestatistical power of learning moderate-size networks. We report results from asimulation study that covers data sets with 20 to 10,000 records over 5 to 25discrete attributes
arxiv-600-48 | Gene Expression Time Course Clustering with Countably Infinite Hidden Markov Models | http://arxiv.org/pdf/1206.6824v1.pdf | author:Matthew Beal, Praveen Krishnamurthy category:cs.LG cs.CE stat.ML published:2012-06-27 summary:Most existing approaches to clustering gene expression time course data treatthe different time points as independent dimensions and are invariant topermutations, such as reversal, of the experimental time course. Approachesutilizing HMMs have been shown to be helpful in this regard, but are hamperedby having to choose model architectures with appropriate complexities. Here wepropose for a clustering application an HMM with a countably infinite statespace; inference in this model is possible by recasting it in the hierarchicalDirichlet process (HDP) framework (Teh et al. 2006), and hence we call it theHDP-HMM. We show that the infinite model outperforms model selection methodsover finite models, and traditional time-independent methods, as measured by avariety of external and internal indices for clustering on two large publiclyavailable data sets. Moreover, we show that the infinite models utilize morehidden states and employ richer architectures (e.g. state-to-state transitions)without the damaging effects of overfitting.
arxiv-600-49 | Discriminative Learning via Semidefinite Probabilistic Models | http://arxiv.org/pdf/1206.6815v1.pdf | author:Koby Crammer, Amir Globerson category:cs.LG stat.ML published:2012-06-27 summary:Discriminative linear models are a popular tool in machine learning. Thesecan be generally divided into two types: The first is linear classifiers, suchas support vector machines, which are well studied and provide state-of-the-artresults. One shortcoming of these models is that their output (known as the'margin') is not calibrated, and cannot be translated naturally into adistribution over the labels. Thus, it is difficult to incorporate such modelsas components of larger systems, unlike probabilistic based approaches. Thesecond type of approach constructs class conditional distributions using anonlinearity (e.g. log-linear models), but is occasionally worse in terms ofclassification error. We propose a supervised learning method which combinesthe best of both approaches. Specifically, our method provides a distributionover the labels, which is a linear function of the model parameters. As aconsequence, differences between probabilities are linear functions, a propertywhich most probabilistic models (e.g. log-linear) do not have. Our model assumes that classes correspond to linear subspaces (rather than tohalf spaces). Using a relaxed projection operator, we construct a measure whichevaluates the degree to which a given vector 'belongs' to a subspace, resultingin a distribution over labels. Interestingly, this view is closely related tosimilar concepts in quantum detection theory. The resulting models can betrained either to maximize the margin or to optimize average likelihoodmeasures. The corresponding optimization problems are semidefinite programswhich can be solved efficiently. We illustrate the performance of our algorithmon real world datasets, and show that it outperforms 2nd order kernel methods.
arxiv-600-50 | An Empirical Comparison of Algorithms for Aggregating Expert Predictions | http://arxiv.org/pdf/1206.6814v1.pdf | author:Varsha Dani, Omid Madani, David M Pennock, Sumit Sanghai, Brian Galebach category:cs.AI cs.LG published:2012-06-27 summary:Predicting the outcomes of future events is a challenging problem for which avariety of solution methods have been explored and attempted. We present anempirical comparison of a variety of online and offline adaptive algorithms foraggregating experts' predictions of the outcomes of five years of US NationalFootball League games (1319 games) using expert probability elicitationsobtained from an Internet contest called ProbabilitySports. We find that it isdifficult to improve over simple averaging of the predictions in terms ofprediction accuracy, but that there is room for improvement in quadratic loss.Somewhat surprisingly, a Bayesian estimation algorithm which estimates thevariance of each expert's prediction exhibits the most consistent superiorperformance over simple averaging among our collection of algorithms.
arxiv-600-51 | A concentration theorem for projections | http://arxiv.org/pdf/1206.6813v1.pdf | author:Sanjoy Dasgupta, Daniel Hsu, Nakul Verma category:cs.LG stat.ML published:2012-06-27 summary:X in R^D has mean zero and finite second moments. We show that there is aprecise sense in which almost all linear projections of X into R^d (for d < D)look like a scale-mixture of spherical Gaussians -- specifically, a mixture ofdistributions N(0, sigma^2 I_d) where the weight of the particular sigmacomponent is P ( X ^2 = sigma^2 D). The extent of this effect depends uponthe ratio of d to D, and upon a particular coefficient of eccentricity of X'sdistribution. We explore this result in a variety of experiments.
arxiv-600-52 | Scaling Life-long Off-policy Learning | http://arxiv.org/pdf/1206.6262v1.pdf | author:Adam White, Joseph Modayil, Richard S. Sutton category:cs.AI cs.LG published:2012-06-27 summary:We pursue a life-long learning approach to artificial intelligence that makesextensive use of reinforcement learning algorithms. We build on our prior workwith general value functions (GVFs) and the Horde architecture. GVFs have beenshown able to represent a wide variety of facts about the world's dynamics thatmay be useful to a long-lived agent (Sutton et al. 2011). We have alsopreviously shown scaling - that thousands of on-policy GVFs can be learnedaccurately in real-time on a mobile robot (Modayil, White & Sutton 2011). Thatwork was limited in that it learned about only one policy at a time, whereasthe greatest potential benefits of life-long learning come from learning aboutmany policies in parallel, as we explore in this paper. Many new challengesarise in this off-policy learning setting. To deal with convergence andefficiency challenges, we utilize the recently introduced GTD({\lambda})algorithm. We show that GTD({\lambda}) with tile coding can simultaneouslylearn hundreds of predictions for five simple target policies while following asingle random behavior policy, assessing accuracy with interspersed on-policytests. To escape the need for the tests, which preclude further scaling, weintroduce and empirically vali- date two online estimators of the off-policyobjective (MSPBE). Finally, we use the more efficient of the two estimators todemonstrate off-policy learning at scale - the learning of value functions forone thousand policies in real time on a physical robot. This abilityconstitutes a significant step towards scaling life-long off-policy learning.
arxiv-600-53 | Discrete Elastic Inner Vector Spaces with Application in Time Series and Sequence Mining | http://arxiv.org/pdf/1206.6196v1.pdf | author:Pierre-François Marteau, Nicolas Bonnel, Gilbas Ménier category:cs.LG cs.DB published:2012-06-27 summary:This paper proposes a framework dedicated to the construction of what we calldiscrete elastic inner product allowing one to embed sets of non-uniformlysampled multivariate time series or sequences of varying lengths into innerproduct space structures. This framework is based on a recursive definitionthat covers the case of multiple embedded time elastic dimensions. We provethat such inner products exist in our general framework and show how a simpleinstance of this inner product class operates on some prospective applications,while generalizing the Euclidean inner product. Classification experimentationson time series and symbolic sequences datasets demonstrate the benefits that wecan expect by embedding time series or sequences into elastic inner spacesrather than into classical Euclidean spaces. These experiments show goodaccuracy when compared to the euclidean distance or even dynamic programmingalgorithms while maintaining a linear algorithmic complexity at exploitationstage, although a quadratic indexing phase beforehand is required.
arxiv-600-54 | Directed Time Series Regression for Control | http://arxiv.org/pdf/1206.6141v1.pdf | author:Yi-Hao Kao, Benjamin Van Roy category:cs.LG cs.SY stat.ML published:2012-06-26 summary:We propose directed time series regression, a new approach to estimatingparameters of time-series models for use in certainty equivalent modelpredictive control. The approach combines merits of least squares regressionand empirical optimization. Through a computational study involving astochastic version of a well known inverted pendulum balancing problem, wedemonstrate that directed time series regression can generate significantimprovements in controller performance over either of the aforementionedalternatives.
arxiv-600-55 | A Unifying Analysis of Projected Gradient Descent for $\ell_p$-constrained Least Squares | http://arxiv.org/pdf/1107.4623v5.pdf | author:Sohail Bahmani, Bhiksha Raj category:math.NA cs.IT math.IT math.OC stat.ML published:2011-07-22 summary:In this paper we study the performance of the Projected Gradient Descent(PGD)algorithm for $\ell_{p}$-constrained least squares problems that arise in theframework of Compressed Sensing. Relying on the Restricted Isometry Property,we provide convergence guarantees for this algorithm for the entire range of$0\leq p\leq1$, that include and generalize the existing results for theIterative Hard Thresholding algorithm and provide a new accuracy guarantee forthe Iterative Soft Thresholding algorithm as special cases. Our results suggestthat in this group of algorithms, as $p$ increases from zero to one, conditionsrequired to guarantee accuracy become stricter and robustness to noisedeteriorates.
arxiv-600-56 | Predictive Approaches For Gaussian Process Classifier Model Selection | http://arxiv.org/pdf/1206.6038v1.pdf | author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG stat.ML published:2012-06-26 summary:In this paper we consider the problem of Gaussian process classifier (GPC)model selection with different Leave-One-Out (LOO) Cross Validation (CV) basedoptimization criteria and provide a practical algorithm using LOO predictivedistributions with such criteria to select hyperparameters. Apart from thestandard average negative logarithm of predictive probability (NLP), we alsoconsider smoothed versions of criteria such as F-measure and Weighted ErrorRate (WER), which are useful for handling imbalanced data. Unlike theregression case, LOO predictive distributions for the classifier case areintractable. We use approximate LOO predictive distributions arrived fromExpectation Propagation (EP) approximation. We conduct experiments on severalreal world benchmark datasets. When the NLP criterion is used for optimizingthe hyperparameters, the predictive approaches show better or comparable NLPgeneralization performance with existing GPC approaches. On the other hand,when the F-measure criterion is used, the F-measure generalization performanceimproves significantly on several datasets. Overall, the EP-based predictivealgorithm comes out as an excellent choice for GP classifier model selectionwith different optimization criteria.
arxiv-600-57 | An Additive Model View to Sparse Gaussian Process Classifier Design | http://arxiv.org/pdf/1206.6030v1.pdf | author:Sundararajan Sellamanickam, Shirish Shevade category:cs.LG stat.ML published:2012-06-26 summary:We consider the problem of designing a sparse Gaussian process classifier(SGPC) that generalizes well. Viewing SGPC design as constructing an additivemodel like in boosting, we present an efficient and effective SGPC designmethod to perform a stage-wise optimization of a predictive loss function. Weintroduce new methods for two key components viz., site parameter estimationand basis vector selection in any SGPC design. The proposed adaptive samplingbased basis vector selection method aids in achieving improved generalizationperformance at a reduced computational cost. This method can also be used inconjunction with any other site parameter estimation methods. It has similarcomputational and storage complexities as the well-known information vectormachine and is suitable for large datasets. The hyperparameters can bedetermined by optimizing a predictive loss function. The experimental resultsshow better generalization performance of the proposed basis vector selectionmethod on several benchmark datasets, particularly for relatively smaller basisvector set sizes or on difficult datasets.
arxiv-600-58 | Transductive Classification Methods for Mixed Graphs | http://arxiv.org/pdf/1206.6015v1.pdf | author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG stat.ML published:2012-06-26 summary:In this paper we provide a principled approach to solve a transductiveclassification problem involving a similar graph (edges tend to connect nodeswith same labels) and a dissimilar graph (edges tend to connect nodes withopposing labels). Most of the existing methods, e.g., InformationRegularization (IR), Weighted vote Relational Neighbor classifier (WvRN) etc,assume that the given graph is only a similar graph. We extend the IR and WvRNmethods to deal with mixed graphs. We evaluate the proposed extensions onseveral benchmark datasets as well as two real world datasets and demonstratethe usefulness of our ideas.
arxiv-600-59 | Reading Dependencies from Covariance Graphs | http://arxiv.org/pdf/1010.4504v3.pdf | author:Jose M. Peña category:stat.ML cs.AI math.ST stat.TH published:2010-10-21 summary:The covariance graph (aka bi-directed graph) of a probability distribution$p$ is the undirected graph $G$ where two nodes are adjacent iff theircorresponding random variables are marginally dependent in $p$. In this paper,we present a graphical criterion for reading dependencies from $G$, under theassumption that $p$ satisfies the graphoid properties as well as weaktransitivity and composition. We prove that the graphical criterion is soundand complete in certain sense. We argue that our assumptions are not toorestrictive. For instance, all the regular Gaussian probability distributionssatisfy them.
arxiv-600-60 | Graph Based Classification Methods Using Inaccurate External Classifier Information | http://arxiv.org/pdf/1206.5915v1.pdf | author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG published:2012-06-26 summary:In this paper we consider the problem of collectively classifying entitieswhere relational information is available across the entities. In practiceinaccurate class distribution for each entity is often available from another(external) classifier. For example this distribution could come from aclassifier built using content features or a simple dictionary. Given therelational and inaccurate external classifier information, we consider twograph based settings in which the problem of collective classification can besolved. In the first setting the class distribution is used to fix labels to asubset of nodes and the labels for the remaining nodes are obtained like in atransductive setting. In the other setting the class distributions of all nodesare used to define the fitting function part of a graph regularized objectivefunction. We define a generalized objective function that handles both thesettings. Methods like harmonic Gaussian field and local-global consistency(LGC) reported in the literature can be seen as special cases. We extend theLGC and weighted vote relational neighbor classification (WvRN) methods tosupport usage of external classifier information. We also propose an efficientleast squares regularization (LSR) based method and relate it to informationregularization methods. All the methods are evaluated on several benchmark andreal world datasets. Considering together speed, robustness and accuracy,experimental results indicate that the LSR and WvRN-extension methods performbetter than other methods.
arxiv-600-61 | Exact Recovery of Sparsely-Used Dictionaries | http://arxiv.org/pdf/1206.5882v1.pdf | author:Daniel A. Spielman, Huan Wang, John Wright category:cs.LG cs.IT math.IT published:2012-06-26 summary:We consider the problem of learning sparsely used dictionaries with anarbitrary square dictionary and a random, sparse coefficient matrix. We provethat $O (n \log n)$ samples are sufficient to uniquely determine thecoefficient matrix. Based on this proof, we design a polynomial-time algorithm,called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove thatit probably recovers the dictionary and coefficient matrix when the coefficientmatrix is sufficiently sparse. Simulation results show that ER-SpUD reveals thetrue dictionary as well as the coefficients with probability higher than manystate-of-the-art algorithms.
arxiv-600-62 | A meta-analysis of state-of-the-art electoral prediction from Twitter data | http://arxiv.org/pdf/1206.5851v1.pdf | author:Daniel Gayo-Avello category:cs.SI cs.CL cs.CY physics.soc-ph published:2012-06-25 summary:Electoral prediction from Twitter data is an appealing research topic. Itseems relatively straightforward and the prevailing view is overly optimistic.This is problematic because while simple approaches are assumed to be goodenough, core problems are not addressed. Thus, this paper aims to (1) provide abalanced and critical review of the state of the art; (2) cast light on thepresume predictive power of Twitter data; and (3) depict a roadmap to pushforward the field. Hence, a scheme to characterize Twitter prediction methodsis proposed. It covers every aspect from data collection to performanceevaluation, through data processing and vote inference. Using that scheme,prior research is analyzed and organized to explain the main approaches takenup to date but also their weaknesses. This is the first meta-analysis of thewhole body of research regarding electoral prediction from Twitter data. Itreveals that its presumed predictive power regarding electoral prediction hasbeen rather exaggerated: although social media may provide a glimpse onelectoral outcomes current research does not provide strong evidence to supportit can replace traditional polls. Finally, future lines of research along witha set of requirements they must fulfill are provided.
arxiv-600-63 | Towards a Mathematical Foundation of Immunology and Amino Acid Chains | http://arxiv.org/pdf/1205.6031v2.pdf | author:Wen-Jun Shen, Hau-San Wong, Quan-Wu Xiao, Xin Guo, Stephen Smale category:stat.ML cs.LG q-bio.GN published:2012-05-28 summary:We attempt to set a mathematical foundation of immunology and amino acidchains. To measure the similarities of these chains, a kernel on strings isdefined using only the sequence of the chains and a good amino acidsubstitution matrix (e.g. BLOSUM62). The kernel is used in learning machines topredict binding affinities of peptides to human leukocyte antigens DR (HLA-DR)molecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsenet.al. 2010) benchmark databases, our algorithm achieves the state-of-the-artperformance. The kernel is also used to define a distance on an HLA-DR alleleset based on which a clustering analysis precisely recovers the serotypeclassifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010).These results suggest that our kernel relates well the chain structure of bothpeptides and HLA-DR molecules to their biological functions, and that it offersa simple, powerful and promising methodology to immunology and amino acid chainstudies.
arxiv-600-64 | Speeding up the construction of slow adaptive walks | http://arxiv.org/pdf/1206.5559v1.pdf | author:Susan Khor category:cs.NE published:2012-06-25 summary:An algorithm (bliss) is proposed to speed up the construction of slowadaptive walks. Slow adaptive walks are adaptive walks biased towards closerpoints or smaller move steps. They were previously introduced to explore asearch space, e.g. to detect potential local optima or to assess the ruggednessof a fitness landscape. To avoid the quadratic cost of computing Hammingdistance (HD) for all-pairs of strings in a set in order to find the set ofclosest strings for each string, strings are sorted and clustered by bliss suchthat similar strings are more likely to get paired off for HD computation. Toefficiently arrange the strings by similarity, bliss employs the idea of sharednon-overlapping position specific subsequences between strings which isinspired by an alignment-free protein sequence comparison algorithm. Tests areperformed to evaluate the quality of b-walks, i.e. slow adaptive walksconstructed from the output of bliss, on enumerated search spaces. Finally,b-walks are applied to explore larger search spaces with the help ofWang-Landau sampling.
arxiv-600-65 | Search space analysis with Wang-Landau sampling and slow adaptive walks | http://arxiv.org/pdf/1112.5980v2.pdf | author:Susan Khor category:cs.NE published:2011-12-27 summary:Two complementary techniques for analyzing search spaces are proposed: (i) analgorithm to detect search points with potential to be local optima; and (ii) aslightly adjusted Wang-Landau sampling algorithm to explore larger searchspaces. The detection algorithm assumes that local optima are points which areeasier to reach and harder to leave by a slow adaptive walker. A slow adaptivewalker moves to a nearest fitter point. Thus, points with larger outgoing stepsizes relative to incoming step sizes are marked using the local optima scoreformulae as potential local optima points (PLOPs). Defining local optima inthese more general terms allows their detection within the closure of a subsetof a search space, and the sampling of a search space unshackled by aparticular move set. Tests are done with NK and HIFF problems to confirm thatPLOPs detected in the manner proposed retain characteristics of local optima,and that the adjusted Wang-Landau samples are more representative of the searchspace than samples produced by choosing points uniformly at random. While ourapproach shows promise, more needs to be done to reduce its computation costthat it may pave a way toward analyzing larger search spaces of practicalmeaning.
arxiv-600-66 | Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns | http://arxiv.org/pdf/1112.2988v2.pdf | author:Tsvi Achler category:cs.CV published:2011-12-13 summary:Matching animal-like flexibility in recognition and the ability to quicklyincorporate new information remains difficult. Limits are yet to be adequatelyaddressed in neural models and recognition algorithms. This work proposes aconfiguration for recognition that maintains the same function of conventionalalgorithms but avoids combinatorial problems. Feedforward recognitionalgorithms such as classical artificial neural networks and machine learningalgorithms are known to be subject to catastrophic interference and forgetting.Modifying or learning new information (associations between patterns andlabels) causes loss of previously learned information. I demonstrate usingmathematical analysis how supervised generative models, with feedforward andfeedback connections, can emulate feedforward algorithms yet avoid catastrophicinterference and forgetting. Learned information in generative models is storedin a more intuitive form that represents the fixed points or solutions of thenetwork and moreover displays similar difficulties as cognitive phenomena.Brain-like capabilities and limits associated with generative models suggestthe brain may perform recognition and store information using a similarapproach. Because of the central role of recognition, progress understandingthe underlying principles may reveal significant insight on how to better studyand integrate with the brain.
arxiv-600-67 | Keyphrase Based Arabic Summarizer (KPAS) | http://arxiv.org/pdf/1206.5384v1.pdf | author:Tarek El-Shishtawy, Fatma El-Ghannam category:cs.CL cs.AI published:2012-06-23 summary:This paper describes a computationally inexpensive and efficient genericsummarization algorithm for Arabic texts. The algorithm belongs to extractivesummarization family, which reduces the problem into representative sentencesidentification and extraction sub-problems. Important keyphrases of thedocument to be summarized are identified employing combinations of statisticaland linguistic features. The sentence extraction algorithm exploits keyphrasesas the primary attributes to rank a sentence. The present experimental work,demonstrates different techniques for achieving various summarization goalsincluding: informative richness, coverage of both main and auxiliary topics,and keeping redundancy to a minimum. A scoring scheme is then adopted thatbalances between these summarization goals. To evaluate the resulted Arabicsummaries with well-established systems, aligned English/Arabic texts are usedthrough the experiments.
arxiv-600-68 | Analysis of a Nature Inspired Firefly Algorithm based Back-propagation Neural Network Training | http://arxiv.org/pdf/1206.5360v1.pdf | author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.AI cs.NE published:2012-06-23 summary:Optimization algorithms are normally influenced by meta-heuristic approach.In recent years several hybrid methods for optimization are developed to findout a better solution. The proposed work using meta-heuristic Nature Inspiredalgorithm is applied with back-propagation method to train a feed-forwardneural network. Firefly algorithm is a nature inspired meta-heuristicalgorithm, and it is incorporated into back-propagation algorithm to achievefast and improved convergence rate in training feed-forward neural network. Theproposed technique is tested over some standard data set. It is found thatproposed method produces an improved convergence within very few iteration.This performance is also analyzed and compared to genetic algorithm basedback-propagation. It is observed that proposed method consumes less time toconverge and providing improved convergence rate with minimum feed-forwardneural network design.
arxiv-600-69 | Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior | http://arxiv.org/pdf/1206.1088v2.pdf | author:Yutian Chen, Max Welling category:stat.ML cs.LG published:2012-06-05 summary:In recent years a number of methods have been developed for automaticallylearning the (sparse) connectivity structure of Markov Random Fields. Thesemethods are mostly based on L1-regularized optimization which has a number ofdisadvantages such as the inability to assess model uncertainty and expensivecross-validation to find the optimal regularization parameter. Moreover, themodel's predictive performance may degrade dramatically with a suboptimal valueof the regularization parameter (which is sometimes desirable to inducesparseness). We propose a fully Bayesian approach based on a "spike and slab"prior (similar to L0 regularization) that does not suffer from theseshortcomings. We develop an approximate MCMC method combining Langevin dynamicsand reversible jump MCMC to conduct inference in this model. Experiments showthat the proposed model learns a good combination of the structure andparameter values without the need for separate hyper-parameter tuning.Moreover, the model's predictive performance is much more robust than L1-basedmethods with hyper-parameter settings that induce highly sparse modelstructures.
arxiv-600-70 | Moving Object Detection by Detecting Contiguous Outliers in the Low-Rank Representation | http://arxiv.org/pdf/1109.0882v2.pdf | author:Xiaowei Zhou, Can Yang, Weichuan Yu category:cs.CV published:2011-09-05 summary:Object detection is a fundamental step for automated video analysis in manyvision applications. Object detection in a video is usually performed by objectdetectors or background subtraction techniques. Often, an object detectorrequires manually labeled examples to train a binary classifier, whilebackground subtraction needs a training sequence that contains no objects tobuild a background model. To automate the analysis, object detection without aseparate training phase becomes a critical task. People have tried to tacklethis task by using motion information. But existing motion-based methods areusually limited when coping with complex scenarios such as nonrigid motion anddynamic background. In this paper, we show that above challenges can beaddressed in a unified framework named DEtecting Contiguous Outliers in theLOw-rank Representation (DECOLOR). This formulation integrates object detectionand background learning into a single process of optimization, which can besolved by an alternating algorithm efficiently. We explain the relationsbetween DECOLOR and other sparsity-based methods. Experiments on both simulateddata and real sequences demonstrate that DECOLOR outperforms thestate-of-the-art approaches and it can work effectively on a wide range ofcomplex scenarios.
arxiv-600-71 | Leaf vein segmentation using Odd Gabor filters and morphological operations | http://arxiv.org/pdf/1206.5157v1.pdf | author:Vini Katyal, Aviral category:cs.CV cs.AI published:2012-06-22 summary:Leaf vein forms the basis of leaf characterization and classification.Different species have different leaf vein patterns. It is seen that leaf veinsegmentation will help in maintaining a record of all the leaves according totheir specific pattern of veins thus provide an effective way to retrieve andstore information regarding various plant species in database as well asprovide an effective means to characterize plants on the basis of leaf veinstructure which is unique for every species. The algorithm proposes a new wayof segmentation of leaf veins with the use of Odd Gabor filters and the use ofmorphological operations for producing a better output. The Odd Gabor filtergives an efficient output and is robust and scalable as compared with theexisting techniques as it detects the fine fiber like veins present in leavesmuch more efficiently.
arxiv-600-72 | Hidden Markov Models with mixtures as emission distributions | http://arxiv.org/pdf/1206.5102v1.pdf | author:Stevenn Volant, Caroline Bérard, Marie-Laure Martin-Magniette, Stéphane Robin category:stat.ML cs.LG stat.CO published:2012-06-22 summary:In unsupervised classification, Hidden Markov Models (HMM) are used toaccount for a neighborhood structure between observations. The emissiondistributions are often supposed to belong to some parametric family. In thispaper, a semiparametric modeling where the emission distributions are a mixtureof parametric distributions is proposed to get a higher flexibility. We showthat the classical EM algorithm can be adapted to infer the model parameters.For the initialisation step, starting from a large number of components, ahierarchical method to combine them into the hidden states is proposed. Threelikelihood-based criteria to select the components to be combined arediscussed. To estimate the number of hidden states, BIC-like criteria arederived. A simulation study is carried out both to determine the bestcombination between the merging criteria and the model selection criteria andto evaluate the accuracy of classification. The proposed method is alsoillustrated using a biological dataset from the model plant Arabidopsisthaliana. A R package HMMmix is freely available on the CRAN.
arxiv-600-73 | A generic framework for video understanding applied to group behavior recognition | http://arxiv.org/pdf/1206.5065v1.pdf | author:Sofia Zaidenberg, Bernard Boulay, François Bremond category:cs.CV published:2012-06-22 summary:This paper presents an approach to detect and track groups of people invideo-surveillance applications, and to automatically recognize their behavior.This method keeps track of individuals moving together by maintaining a spacialand temporal group coherence. First, people are individually detected andtracked. Second, their trajectories are analyzed over a temporal window andclustered using the Mean-Shift algorithm. A coherence value describes how wella set of people can be described as a group. Furthermore, we propose a formalevent description language. The group events recognition approach issuccessfully validated on 4 camera views from 3 datasets: an airport, a subway,a shopping center corridor and an entrance hall.
arxiv-600-74 | Convergence of the Continuous Time Trajectories of Isotropic Evolution Strategies on Monotonic C^2-composite Functions | http://arxiv.org/pdf/1206.4968v1.pdf | author:Youhei Akimoto, Anne Auger, Nikolaus Hansen category:cs.NE math.OC published:2012-06-21 summary:The Information-Geometric Optimization (IGO) has been introduced as a unifiedframework for stochastic search algorithms. Given a parametrized family ofprobability distributions on the search space, the IGO turns an arbitraryoptimization problem on the search space into an optimization problem on theparameter space of the probability distribution family and defines a naturalgradient ascent on this space. From the natural gradients defined over theentire parameter space we obtain continuous time trajectories which are thesolutions of an ordinary differential equation (ODE). Via discretization, theIGO naturally defines an iterated gradient ascent algorithm. Depending on thechosen distribution family, the IGO recovers several known algorithms such asthe pure rank-\mu update CMA-ES. Consequently, the continuous timeIGO-trajectory can be viewed as an idealization of the original algorithm. Inthis paper we study the continuous time trajectories of the IGO given thefamily of isotropic Gaussian distributions. These trajectories are adeterministic continuous time model of the underlying evolution strategy in thelimit for population size to infinity and change rates to zero. On functionsthat are the composite of a monotone and a convex-quadratic function, we provethe global convergence of the solution of the ODE towards the global optimum.We extend this result to composites of monotone and twice continuouslydifferentiable functions and prove local convergence towards local optima.
arxiv-600-75 | A Pointillism Approach for Natural Language Processing of Social Media | http://arxiv.org/pdf/1206.4958v1.pdf | author:Peiyou Song, Anhei Shu, Anyu Zhou, Dan Wallach, Jedidiah R. Crandall category:cs.IR cs.CL cs.SI published:2012-06-21 summary:The Chinese language poses challenges for natural language processing basedon the unit of a word even for formal uses of the Chinese language, socialmedia only makes word segmentation in Chinese even more difficult. In thisdocument we propose a pointillism approach to natural language processing.Rather than words that have individual meanings, the basic unit of apointillism approach is trigrams of characters. These grams take on meaning inaggregate when they appear together in a way that is correlated over time. Our results from three kinds of experiments show that when words and topicsdo have a meme-like trend, they can be reconstructed from only trigrams. Forexample, for 4-character idioms that appear at least 99 times in one day in ourdata, the unconstrained precision (that is, precision that allows for deviationfrom a lexicon when the result is just as correct as the lexicon version of theword or phrase) is 0.93. For longer words and phrases collected fromWiktionary, including neologisms, the unconstrained precision is 0.87. Weconsider these results to be very promising, because they suggest that it isfeasible for a machine to reconstruct complex idioms, phrases, and neologismswith good precision without any notion of words. Thus the colorful and baroqueuses of language that typify social media in challenging languages such asChinese may in fact be accessible to machines.
arxiv-600-76 | Multi-Level Error-Resilient Neural Networks with Learning | http://arxiv.org/pdf/1202.2770v4.pdf | author:Amir Hesam Salavati, Amin Karbasi category:cs.NE cs.AI cs.IT math.IT published:2012-02-13 summary:The problem of neural network association is to retrieve a previouslymemorized pattern from its noisy version using a network of neurons. An idealneural network should include three components simultaneously: a learningalgorithm, a large pattern retrieval capacity and resilience against noise.Prior works in this area usually improve one or two aspects at the cost of thethird. Our work takes a step forward in closing this gap. More specifically, we showthat by forcing natural constraints on the set of learning patterns, we candrastically improve the retrieval capacity of our neural network. Moreover, wedevise a learning algorithm whose role is to learn those patterns satisfyingthe above mentioned constraints. Finally we show that our neural network cancope with a fair amount of noise.
arxiv-600-77 | Portraits of Julius Caesar: a proposal for 3D analysis | http://arxiv.org/pdf/1206.4866v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2012-06-21 summary:Here I suggest the use of a 3D scanning and rendering to create some virtualcopies of ancient artifacts to study and compare them. In particular, thisapproach could be interesting for some roman marble busts, two of which areportraits of Julius Caesar, and the third is a realistic portrait of a manrecently found at Arles, France. The comparison of some images indicates that athree-dimensional visualization is necessary.
arxiv-600-78 | Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA | http://arxiv.org/pdf/1203.5443v2.pdf | author:Martin Pelikan, Mark W. Hauschild, Pier Luca Lanzi category:cs.NE cs.AI cs.LG published:2012-03-24 summary:An automated technique has recently been proposed to transfer learning in thehierarchical Bayesian optimization algorithm (hBOA) based on distance-basedstatistics. The technique enables practitioners to improve hBOA efficiency bycollecting statistics from probabilistic models obtained in previous hBOA runsand using the obtained statistics to bias future hBOA runs on similar problems.The purpose of this paper is threefold: (1) test the technique on severalclasses of NP-complete problems, including MAXSAT, spin glasses and minimumvertex cover; (2) demonstrate that the technique is effective even whenprevious runs were done on problems of different size; (3) provide empiricalevidence that combining transfer learning with other efficiency enhancementtechniques can often yield nearly multiplicative speedups.
arxiv-600-79 | The Kernelized Stochastic Batch Perceptron | http://arxiv.org/pdf/1204.0566v2.pdf | author:Andrew Cotter, Shai Shalev-Shwartz, Nathan Srebro category:cs.LG published:2012-04-03 summary:We present a novel approach for training kernel Support Vector Machines,establish learning runtime guarantees for our method that are better then thoseof any other known kernelized SVM optimization approach, and show that ourmethod works well in practice compared to existing alternatives.
arxiv-600-80 | Scaled Sparse Linear Regression | http://arxiv.org/pdf/1104.4595v2.pdf | author:Tingni Sun, Cun-Hui Zhang category:stat.ML math.ST stat.TH published:2011-04-24 summary:Scaled sparse linear regression jointly estimates the regression coefficientsand noise level in a linear model. It chooses an equilibrium with a sparseregression method by iteratively estimating the noise level via the meanresidual square and scaling the penalty in proportion to the estimated noiselevel. The iterative algorithm costs little beyond the computation of a path orgrid of the sparse regression estimator for penalty levels above a properthreshold. For the scaled lasso, the algorithm is a gradient descent in aconvex minimization of a penalized joint loss function for the regressioncoefficients and noise level. Under mild regularity conditions, we prove thatthe scaled lasso simultaneously yields an estimator for the noise level and anestimated coefficient vector satisfying certain oracle inequalities forprediction, the estimation of the noise level and the regression coefficients.These inequalities provide sufficient conditions for the consistency andasymptotic normality of the noise level estimator, including certain caseswhere the number of variables is of greater order than the sample size.Parallel results are provided for the least squares estimation after modelselection by the scaled lasso. Numerical results demonstrate the superiorperformance of the proposed methods over an earlier proposal of joint convexminimization.
arxiv-600-81 | On Sensitivity of the MAP Bayesian Network Structure to the Equivalent Sample Size Parameter | http://arxiv.org/pdf/1206.5293v1.pdf | author:Tomi Silander, Petri Kontkanen, Petri Myllymaki category:cs.LG stat.ML published:2012-06-20 summary:BDeu marginal likelihood score is a popular model selection criterion forselecting a Bayesian network structure based on sample data. Thisnon-informative scoring criterion assigns same score for network structuresthat encode same independence statements. However, before applying the BDeuscore, one must determine a single parameter, the equivalent sample size alpha.Unfortunately no generally accepted rule for determining the alpha parameterhas been suggested. This is disturbing, since in this paper we show through aseries of concrete experiments that the solution of the network structureoptimization problem is highly sensitive to the chosen alpha parameter value.Based on these results, we are able to give explanations for how and why thisphenomenon happens, and discuss ideas for solving this problem.
arxiv-600-82 | Improved Dynamic Schedules for Belief Propagation | http://arxiv.org/pdf/1206.5291v1.pdf | author:Charles Sutton, Andrew McCallum category:cs.LG cs.AI stat.ML published:2012-06-20 summary:Belief propagation and its variants are popular methods for approximateinference, but their running time and even their convergence depend greatly onthe schedule used to send the messages. Recently, dynamic update schedules havebeen shown to converge much faster on hard networks than static schedules,namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithmwastes message updates: many messages are computed solely to determine theirpriority, and are never actually performed. In this paper, we show thatestimating the residual, rather than calculating it directly, leads tosignificant decreases in the number of messages required for convergence, andin the total running time. The residual is estimated using an upper bound basedon recent work on message errors in BP. On both synthetic and real-worldnetworks, this dramatically decreases the running time of BP, in some cases bya factor of five, without affecting the quality of the solution.
arxiv-600-83 | Imitation Learning with a Value-Based Prior | http://arxiv.org/pdf/1206.5290v1.pdf | author:Umar Syed, Robert E. Schapire category:cs.LG cs.AI stat.ML published:2012-06-20 summary:The goal of imitation learning is for an apprentice to learn how to behave ina stochastic environment by observing a mentor demonstrating the correctbehavior. Accurate prior knowledge about the correct behavior can reduce theneed for demonstrations from the mentor. We present a novel approach toencoding prior knowledge about the correct behavior, where we assume that thisprior knowledge takes the form of a Markov Decision Process (MDP) that is usedby the apprentice as a rough and imperfect model of the mentor's behavior.Specifically, taking a Bayesian approach, we treat the value of a policy inthis modeling MDP as the log prior probability of the policy. In other words,we assume a priori that the mentor's behavior is likely to be a high valuepolicy in the modeling MDP, though quite possibly different from the optimalpolicy. We describe an efficient algorithm that, given a modeling MDP and a setof demonstrations by a mentor, provably converges to a stationary point of thelog posterior of the mentor's policy, where the posterior is computed withrespect to the "value based" prior. We also present empirical evidence thatthis prior does in fact speed learning of the mentor's policy, and is animprovement in our experiments over similar previous methods.
arxiv-600-84 | MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies | http://arxiv.org/pdf/1206.5286v1.pdf | author:Yair Weiss, Chen Yanover, Talya Meltzer category:cs.AI cs.LG stat.ML published:2012-06-20 summary:Finding the most probable assignment (MAP) in a general graphical model isknown to be NP hard but good approximations have been attained with max-productbelief propagation (BP) and its variants. In particular, it is known that usingBP on a single-cycle graph or tree reweighted BP on an arbitrary graph willgive the MAP solution if the beliefs have no ties. In this paper we extend thesetting under which BP can be used to provably extract the MAP. We defineConvex BP as BP algorithms based on a convex free energy approximation and showthat this class includes ordinary BP with single-cycle, tree reweighted BP andmany other BP variants. We show that when there are no ties, fixed-points ofconvex max-product BP will provably give the MAP solution. We also show thatconvex sum-product BP at sufficiently small temperatures can be used to solvelinear programs that arise from relaxing the MAP problem. Finally, we derive anovel condition that allows us to derive the MAP solution even if some of theconvex BP beliefs have ties. In experiments, we show that our theorems allow usto find the MAP in many real-world instances of graphical models where exactinference using junction-tree is impossible.
arxiv-600-85 | Bayesian Active Distance Metric Learning | http://arxiv.org/pdf/1206.5283v1.pdf | author:Liu Yang, Rong Jin, Rahul Sukthankar category:cs.LG stat.ML published:2012-06-20 summary:Distance metric learning is an important component for many tasks, such asstatistical classification and content-based image retrieval. Existingapproaches for learning distance metrics from pairwise constraints typicallysuffer from two major problems. First, most algorithms only offer pointestimation of the distance metric and can therefore be unreliable when thenumber of training examples is small. Second, since these algorithms generallyselect their training examples at random, they can be inefficient if labelingeffort is limited. This paper presents a Bayesian framework for distance metriclearning that estimates a posterior distribution for the distance metric fromlabeled pairwise constraints. We describe an efficient algorithm based on thevariational method for the proposed Bayesian approach. Furthermore, we applythe proposed Bayesian framework to active distance metric learning by selectingthose unlabeled example pairs with the greatest uncertainty in relativedistance. Experiments in classification demonstrate that the proposed frameworkachieves higher classification accuracy and identifies more informativetraining examples than the non-Bayesian approach and state-of-the-art distancemetric learning algorithms.
arxiv-600-86 | A Characterization of Markov Equivalence Classes for Directed Acyclic Graphs with Latent Variables | http://arxiv.org/pdf/1206.5282v1.pdf | author:Jiji Zhang category:stat.ME cs.LG stat.ML published:2012-06-20 summary:Different directed acyclic graphs (DAGs) may be Markov equivalent in thesense that they entail the same conditional independence relations among theobserved variables. Meek (1995) characterizes Markov equivalence classes forDAGs (with no latent variables) by presenting a set of orientation rules thatcan correctly identify all arrow orientations shared by all DAGs in a Markovequivalence class, given a member of that class. For DAG models with latentvariables, maximal ancestral graphs (MAGs) provide a neat representation thatfacilitates model search. Earlier work (Ali et al. 2005) has identified a setof orientation rules sufficient to construct all arrowheads common to a Markovequivalence class of MAGs. In this paper, we provide extra rules sufficient toconstruct all common tails as well. We end up with a set of orientation rulessound and complete for identifying commonalities across a Markov equivalenceclass of MAGs, which is particularly useful for causal inference.
arxiv-600-87 | Learning Selectively Conditioned Forest Structures with Applications to DBNs and Classification | http://arxiv.org/pdf/1206.5281v1.pdf | author:Brian D. Ziebart, Anind K. Dey, J Andrew Bagnell category:cs.LG stat.ML published:2012-06-20 summary:Dealing with uncertainty in Bayesian Network structures using maximum aposteriori (MAP) estimation or Bayesian Model Averaging (BMA) is oftenintractable due to the superexponential number of possible directed, acyclicgraphs. When the prior is decomposable, two classes of graphs where efficientlearning can take place are tree structures, and fixed-orderings with limitedin-degree. We show how MAP estimates and BMA for selectively conditionedforests (SCF), a combination of these two classes, can be computed efficientlyfor ordered sets of variables. We apply SCFs to temporal data to learn DynamicBayesian Networks having an intra-timestep forest and inter-timestep limitedin-degree structure, improving model accuracy over DBNs without the combinationof structures. We also apply SCFs to Bayes Net classification to learnselective forest augmented Naive Bayes classifiers. We argue that the built-infeature selection of selective augmented Bayes classifiers makes thempreferable to similar non-selective classifiers based on empirical evidence.
arxiv-600-88 | Fast Nonparametric Conditional Density Estimation | http://arxiv.org/pdf/1206.5278v1.pdf | author:Michael P. Holmes, Alexander G. Gray, Charles Lee Isbell category:stat.ME cs.LG stat.ML published:2012-06-20 summary:Conditional density estimation generalizes regression by modeling a fulldensity f(yjx) rather than only the expected value E(yjx). This is importantfor many tasks, including handling multi-modality and generating predictionintervals. Though fundamental and widely applicable, nonparametric conditionaldensity estimators have received relatively little attention from statisticiansand little or none from the machine learning community. None of that work hasbeen applied to greater than bivariate data, presumably due to thecomputational difficulty of data-driven bandwidth selection. We describe thedouble kernel conditional density estimator and derive fast dual-tree-basedalgorithms for bandwidth selection using a maximum likelihood criterion. Thesetechniques give speedups of up to 3.8 million in our experiments, and enablethe first applications to previously intractable large multivariate datasets,including a redshift prediction problem from the Sloan Digital Sky Survey.
arxiv-600-89 | Accuracy Bounds for Belief Propagation | http://arxiv.org/pdf/1206.5277v1.pdf | author:Alexander T. Ihler category:cs.AI cs.LG stat.ML published:2012-06-20 summary:The belief propagation (BP) algorithm is widely applied to performapproximate inference on arbitrary graphical models, in part due to itsexcellent empirical properties and performance. However, little is knowntheoretically about when this algorithm will perform well. Using recentanalysis of convergence and stability properties in BP and new results onapproximations in binary systems, we derive a bound on the error in BP'sestimates for pairwise Markov random fields over discrete valued randomvariables. Our bound is relatively simple to compute, and compares favorablywith a previous method of bounding the accuracy of BP.
arxiv-600-90 | BADREX: In situ expansion and coreference of biomedical abbreviations using dynamic regular expressions | http://arxiv.org/pdf/1206.4522v1.pdf | author:Phil Gooch category:cs.CL published:2012-06-20 summary:BADREX uses dynamically generated regular expressions to annotate termdefinition-term abbreviation pairs, and corefers unpaired acronyms andabbreviations back to their initial definition in the text. Against theMedstract corpus BADREX achieves precision and recall of 98% and 97%, andagainst a much larger corpus, 90% and 85%, respectively. BADREX yields improvedperformance over previous approaches, requires no training data and allowsruntime customisation of its input parameters. BADREX is freely available fromhttps://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as aplugin for the General Architecture for Text Engineering (GATE) framework andis licensed under the GPLv3.
arxiv-600-91 | On Discarding, Caching, and Recalling Samples in Active Learning | http://arxiv.org/pdf/1206.5274v1.pdf | author:Ashish Kapoor, Eric J. Horvitz category:cs.LG stat.ML published:2012-06-20 summary:We address challenges of active learning under scarce informational resourcesin non-stationary environments. In real-world settings, data labeled andintegrated into a predictive model may become invalid over time. However, thedata can become informative again with switches in context and such changes mayindicate unmodeled cyclic or other temporal dynamics. We explore principles fordiscarding, caching, and recalling labeled data points in active learning basedon computations of value of information. We review key concepts and study thevalue of the methods via investigations of predictive performance and costs ofacquiring data for simulated and real-world data sets.
arxiv-600-92 | Nonparametric Bayes Pachinko Allocation | http://arxiv.org/pdf/1206.5270v1.pdf | author:Wei Li, David Blei, Andrew McCallum category:cs.IR cs.LG stat.ML published:2012-06-20 summary:Recent advances in topic models have explored complicated structureddistributions to represent topic correlation. For example, the pachinkoallocation model (PAM) captures arbitrary, nested, and possibly sparsecorrelations between topics using a directed acyclic graph (DAG). While PAMprovides more flexibility and greater expressive power than previous modelslike latent Dirichlet allocation (LDA), it is also more difficult to determinethe appropriate topic structure for a specific dataset. In this paper, wepropose a nonparametric Bayesian prior for PAM based on a variant of thehierarchical Dirichlet process (HDP). Although the HDP can capture topiccorrelations defined by nested data structure, it does not automaticallydiscover such correlations from unstructured data. By assuming an HDP-basedprior for PAM, we are able to learn both the number of topics and how thetopics are correlated. We evaluate our model on synthetic and real-world textdatasets, and show that nonparametric PAM achieves performance matching thebest of PAM without manually tuning the number of topics.
arxiv-600-93 | Collaborative Filtering and the Missing at Random Assumption | http://arxiv.org/pdf/1206.5267v1.pdf | author:Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney category:cs.LG cs.IR stat.ML published:2012-06-20 summary:Rating prediction is an important application, and a popular research topicin collaborative filtering. However, both the validity of learning algorithms,and the validity of standard testing procedures rest on the assumption thatmissing ratings are missing at random (MAR). In this paper we present theresults of a user study in which we collect a random sample of ratings fromcurrent users of an online radio service. An analysis of the rating datacollected in the study shows that the sample of random ratings has markedlydifferent properties than ratings of user-selected songs. When asked to reporton their own rating behaviour, a large number of users indicate they believetheir opinion of a song does affect whether they choose to rate that song, aviolation of the MAR condition. Finally, we present experimental resultsshowing that incorporating an explicit model of the missing data mechanism canlead to significant improvements in prediction performance on the random sampleof ratings.
arxiv-600-94 | Consensus ranking under the exponential model | http://arxiv.org/pdf/1206.5265v1.pdf | author:Marina Meila, Kapil Phadnis, Arthur Patterson, Jeff A. Bilmes category:cs.LG cs.AI stat.ML published:2012-06-20 summary:We analyze the generalized Mallows model, a popular exponential model overrankings. Estimating the central (or consensus) ranking from data is NP-hard.We obtain the following new results: (1) We show that search methods canestimate both the central ranking pi0 and the model parameters theta exactly.The search is n! in the worst case, but is tractable when the true distributionis concentrated around its mode; (2) We show that the generalized Mallows modelis jointly exponential in (pi0; theta), and introduce the conjugate prior forthis model class; (3) The sufficient statistics are the pairwise marginalprobabilities that item i is preferred to item j. Preliminary experimentsconfirm the theoretical predictions and compare the new algorithm and existingheuristics.
arxiv-600-95 | Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods | http://arxiv.org/pdf/1206.5264v1.pdf | author:Gergely Neu, Csaba Szepesvari category:cs.LG stat.ML published:2012-06-20 summary:In this paper we propose a novel gradient algorithm to learn a policy from anexpert's observed behavior assuming that the expert behaves optimally withrespect to some unknown reward function of a Markovian Decision Problem. Thealgorithm's aim is to find a reward function such that the resulting optimalpolicy matches well the expert's observed behavior. The main difficulty is thatthe mapping from the parameters to policies is both nonsmooth and highlyredundant. Resorting to subdifferentials solves the first difficulty, while thesecond one is over- come by computing natural gradients. We tested the proposedmethod in two artificial domains and found it to be more reliable and efficientthan some previous methods.
arxiv-600-96 | Reading Dependencies from Polytree-Like Bayesian Networks | http://arxiv.org/pdf/1206.5263v1.pdf | author:Jose M. Pena category:cs.AI cs.LG stat.ML published:2012-06-20 summary:We present a graphical criterion for reading dependencies from the minimaldirected independence map G of a graphoid p when G is a polytree and psatisfies composition and weak transitivity. We prove that the criterion issound and complete. We argue that assuming composition and weak transitivity isnot too restrictive.
arxiv-600-97 | Mixture-of-Parents Maximum Entropy Markov Models | http://arxiv.org/pdf/1206.5261v1.pdf | author:David S. Rosenberg, Dan Klein, Ben Taskar category:cs.LG cs.AI stat.ML published:2012-06-20 summary:We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), aclass of directed graphical models extending MEMMs. The MoP-MEMM allowstractable incorporation of long-range dependencies between nodes by restrictingthe conditional distribution of each node to be a mixture of distributionsgiven the parents. We show how to efficiently compute the exact marginalposterior node distributions, regardless of the range of the dependencies. Thisenables us to model non-sequential correlations present within text documents,as well as between interconnected documents, such as hyperlinked web pages. Weapply the MoP-MEMM to a named entity recognition task and a web pageclassification task. In each, our model shows significant improvement over thebasic MEMM, and is competitive with other long-range sequence models that useapproximate inference.
arxiv-600-98 | Discovering Patterns in Biological Sequences by Optimal Segmentation | http://arxiv.org/pdf/1206.5256v1.pdf | author:Joseph Bockhorst, Nebojsa Jojic category:cs.CE cs.LG q-bio.QM stat.AP published:2012-06-20 summary:Computational methods for discovering patterns of local correlations insequences are important in computational biology. Here we show how to determinethe optimal partitioning of aligned sequences into non-overlapping segmentssuch that positions in the same segment are strongly correlated while positionsin different segments are not. Our approach involves discovering the hiddenvariables of a Bayesian network that interact with observed sequences so as toform a set of independent mixture models. We introduce a dynamic program toefficiently discover the optimal segmentation, or equivalently the optimal setof hidden variables. We evaluate our approach on two computational biologytasks. One task is related to the design of vaccines against polymorphicpathogens and the other task involves analysis of single nucleotidepolymorphisms (SNPs) in human DNA. We show how common tasks in these problemsnaturally correspond to inference procedures in the learned models. Error ratesof our learned models for the prediction of missing SNPs are up to 1/3 lessthan the error rates of a state-of-the-art SNP prediction method. Source codeis available at www.uwm.edu/~joebock/segmentation.
arxiv-600-99 | Statistical Translation, Heat Kernels and Expected Distances | http://arxiv.org/pdf/1206.5248v1.pdf | author:Joshua Dillon, Yi Mao, Guy Lebanon, Jian Zhang category:cs.LG cs.CV cs.IR stat.ML published:2012-06-20 summary:High dimensional structured data such as text and images is often poorlyunderstood and misrepresented in statistical modeling. The standard histogramrepresentation suffers from high variance and performs poorly in general. Weexplore novel connections between statistical translation, heat kernels onmanifolds and graphs, and expected distances. These connections provide a newframework for unsupervised metric learning for text documents. Experimentsindicate that the resulting distances are generally superior to their morestandard counterparts.
arxiv-600-100 | Bayesian structure learning using dynamic programming and MCMC | http://arxiv.org/pdf/1206.5247v1.pdf | author:Daniel Eaton, Kevin Murphy category:cs.LG stat.ML published:2012-06-20 summary:MCMC methods for sampling from the space of DAGs can mix poorly due to thelocal nature of the proposals that are commonly used. It has been shown thatsampling from the space of node orders yields better results [FK03, EW06].Recently, Koivisto and Sood showed how one can analytically marginalize overorders using dynamic programming (DP) [KS04, Koi06]. Their method computes theexact marginal posterior edge probabilities, thus avoiding the need for MCMC.Unfortunately, there are four drawbacks to the DP technique: it can only usemodular priors, it can only compute posteriors over modular features, it isdifficult to compute a predictive density, and it takes exponential time andspace. We show how to overcome the first three of these problems by using theDP algorithm as a proposal distribution for MCMC in DAG space. We show thatthis hybrid technique converges to the posterior faster than other methods,resulting in more accurate structure learning and higher predictive likelihoodson test data.
arxiv-600-101 | A new parameter Learning Method for Bayesian Networks with Qualitative Influences | http://arxiv.org/pdf/1206.5245v1.pdf | author:Ad Feelders category:cs.AI cs.LG stat.ME published:2012-06-20 summary:We propose a new method for parameter learning in Bayesian networks withqualitative influences. This method extends our previous work from networks ofbinary variables to networks of discrete variables with ordered values. Thespecified qualitative influences correspond to certain order restrictions onthe parameters in the network. These parameters may therefore be estimatedusing constrained maximum likelihood estimation. We propose an alternativemethod, based on the isotonic regression. The constrained maximum likelihoodestimates are fairly complicated to compute, whereas computation of theisotonic regression estimates only requires the repeated application of thePool Adjacent Violators algorithm for linear orders. Therefore, the isotonicregression estimator is to be preferred from the viewpoint of computationalcomplexity. Through experiments on simulated and real data, we show that thenew learning method is competitive in performance to the constrained maximumlikelihood estimator, and that both estimators improve on the standardestimator.
arxiv-600-102 | Convergent Propagation Algorithms via Oriented Trees | http://arxiv.org/pdf/1206.5243v1.pdf | author:Amir Globerson, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-06-20 summary:Inference problems in graphical models are often approximated by casting themas constrained optimization problems. Message passing algorithms, such asbelief propagation, have previously been suggested as methods for solving theseoptimization problems. However, there are few convergence guarantees for suchalgorithms, and the algorithms are therefore not guaranteed to solve thecorresponding optimization problem. Here we present an oriented treedecomposition algorithm that is guaranteed to converge to the global optimum ofthe Tree-Reweighted (TRW) variational problem. Our algorithm performs localupdates in the convex dual of the TRW problem - an unconstrained generalizedgeometric program. Primal updates, also local, correspond to orientedreparametrization operations that leave the distribution intact.
arxiv-600-103 | Shift-Invariance Sparse Coding for Audio Classification | http://arxiv.org/pdf/1206.5241v1.pdf | author:Roger Grosse, Rajat Raina, Helen Kwong, Andrew Y. Ng category:cs.LG stat.ML published:2012-06-20 summary:Sparse coding is an unsupervised learning algorithm that learns a succincthigh-level representation of the inputs given only unlabeled data; itrepresents each input as a sparse linear combination of a set of basisfunctions. Originally applied to modeling the human visual cortex, sparsecoding has also been shown to be useful for self-taught learning, in which thegoal is to solve a supervised classification task given access to additionalunlabeled data drawn from different classes than that in the supervisedlearning problem. Shift-invariant sparse coding (SISC) is an extension ofsparse coding which reconstructs a (usually time-series) input using all of thebasis functions in all possible shifts. In this paper, we present an efficientalgorithm for learning SISC bases. Our method is based on iteratively solvingtwo large convex optimization problems: The first, which computes the linearcoefficients, is an L1-regularized linear least squares problem withpotentially hundreds of thousands of variables. Existing methods typically usea heuristic to select a small subset of the variables to optimize, but wepresent a way to efficiently compute the exact solution. The second, whichsolves for bases, is a constrained linear least squares problem. By optimizingover complex-valued variables in the Fourier domain, we reduce the couplingbetween the different variables, allowing the problem to be solved efficiently.We show that SISC's learned high-level representations of speech and musicprovide useful features for classification tasks within those domains. Whenapplied to classification, under certain conditions the learned featuresoutperform state of the art spectral and cepstral features.
arxiv-600-104 | Analysis of Semi-Supervised Learning with the Yarowsky Algorithm | http://arxiv.org/pdf/1206.5240v1.pdf | author:Gholam Reza Haffari, Anoop Sarkar category:cs.LG stat.ML published:2012-06-20 summary:The Yarowsky algorithm is a rule-based semi-supervised learning algorithmthat has been successfully applied to some problems in computationallinguistics. The algorithm was not mathematically well understood until (Abney2004) which analyzed some specific variants of the algorithm, and also proposedsome new algorithms for bootstrapping. In this paper, we extend Abney's workand show that some of his proposed algorithms actually optimize (an upper-boundon) an objective function based on a new definition of cross-entropy which isbased on a particular instantiation of the Bregman distance between probabilitydistributions. Moreover, we suggest some new algorithms for rule-basedsemi-supervised learning and show connections with harmonic functions andminimum multi-way cuts in graph-based semi-supervised learning.
arxiv-600-105 | Geometric representations for minimalist grammars | http://arxiv.org/pdf/1101.5076v6.pdf | author:Peter beim Graben, Sabrina Gerth category:cs.CL published:2011-01-26 summary:We reformulate minimalist grammars as partial functions on term algebras forstrings and trees. Using filler/role bindings and tensor productrepresentations, we construct homomorphisms for these data structures intogeometric vector spaces. We prove that the structure-building functions as wellas simple processors for minimalist languages can be realized by piecewiselinear operators in representation space. We also propose harmony, i.e. thedistance of an intermediate processing step from the final well-formed state inrepresentation space, as a measure of processing complexity. Finally, weillustrate our findings by means of two particular arithmetic and fractalrepresentations.
arxiv-600-106 | Active Learning Using Smooth Relative Regret Approximations with Applications | http://arxiv.org/pdf/1110.2136v3.pdf | author:Nir Ailon, Ron Begleiter, Esther Ezra category:cs.LG published:2011-10-10 summary:The disagreement coefficient of Hanneke has become a central data independentinvariant in proving active learning rates. It has been shown in various waysthat a concept class with low complexity together with a bound on thedisagreement coefficient at an optimal solution allows active learning ratesthat are superior to passive learning ones. We present a different tool for pool based active learning which follows fromthe existence of a certain uniform version of low disagreement coefficient, butis not equivalent to it. In fact, we present two fundamental active learningproblems of significant interest for which our approach allows nontrivialactive learning bounds. However, any general purpose method relying on thedisagreement coefficient bounds only fails to guarantee any useful bounds forthese problems. The tool we use is based on the learner's ability to compute an estimator ofthe difference between the loss of any hypotheses and some fixed "pivotal"hypothesis to within an absolute error of at most $\eps$ times the
arxiv-600-107 | Gray Image extraction using Fuzzy Logic | http://arxiv.org/pdf/1206.4391v1.pdf | author:Koushik Mondal, Paramartha Dutta, Siddhartha Bhattacharyya category:cs.CV cs.AI published:2012-06-20 summary:Fuzzy systems concern fundamental methodology to represent and processuncertainty and imprecision in the linguistic information. The fuzzy systemsthat use fuzzy rules to represent the domain knowledge of the problem are knownas Fuzzy Rule Base Systems (FRBS). On the other hand image segmentation andsubsequent extraction from a noise-affected background, with the help ofvarious soft computing methods, are relatively new and quite popular due tovarious reasons. These methods include various Artificial Neural Network (ANN)models (primarily supervised in nature), Genetic Algorithm (GA) basedtechniques, intensity histogram based methods etc. providing an extractionsolution working in unsupervised mode happens to be even more interestingproblem. Literature suggests that effort in this respect appears to be quiterudimentary. In the present article, we propose a fuzzy rule guided noveltechnique that is functional devoid of any external intervention duringexecution. Experimental results suggest that this approach is an efficient onein comparison to different other techniques extensively addressed inliterature. In order to justify the supremacy of performance of our proposedtechnique in respect of its competitors, we take recourse to effective metricslike Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal to NoiseRatio (PSNR).
arxiv-600-108 | Joint Reconstruction of Multi-view Compressed Images | http://arxiv.org/pdf/1206.4326v1.pdf | author:Vijayaraghavan Thirumalai, Pascal Frossard category:cs.MM cs.CV published:2012-06-19 summary:The distributed representation of correlated multi-view images is animportant problem that arise in vision sensor networks. This paper concentrateson the joint reconstruction problem where the distributively compressedcorrelated images are jointly decoded in order to improve the reconstructionquality of all the compressed images. We consider a scenario where the imagescaptured at different viewpoints are encoded independently using common codingsolutions (e.g., JPEG, H.264 intra) with a balanced rate distribution amongdifferent cameras. A central decoder first estimates the underlying correlationmodel from the independently compressed images which will be used for the jointsignal recovery. The joint reconstruction is then cast as a constrained convexoptimization problem that reconstructs total-variation (TV) smooth images thatcomply with the estimated correlation model. At the same time, we addconstraints that force the reconstructed images to be consistent with theircompressed versions. We show by experiments that the proposed jointreconstruction scheme outperforms independent reconstruction in terms of imagequality, for a given target bit rate. In addition, the decoding performance ofour proposed algorithm compares advantageously to state-of-the-art distributedcoding schemes based on disparity learning and on the DISCOVER.
arxiv-600-109 | Distributed Lossy Source Coding Using Real-Number Codes | http://arxiv.org/pdf/1111.0654v2.pdf | author:Mojtaba Vaezi, Fabrice Labeau category:cs.IT cs.CV cs.NI math.IT published:2011-11-02 summary:We show how real-number codes can be used to compress correlated sources, andestablish a new framework for lossy distributed source coding, in which wequantize compressed sources instead of compressing quantized sources. Thischange in the order of binning and quantization blocks makes it possible tomodel correlation between continuous-valued sources more realistically andcorrect quantization error when the sources are completely correlated. Theencoding and decoding procedures are described in detail, for discrete Fouriertransform (DFT) codes. Reconstructed signal, in the mean squared error sense,is seen to be better than that in the conventional approach.
arxiv-600-110 | Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev Inequality | http://arxiv.org/pdf/1101.3755v2.pdf | author:Shriprakash Sinha category:cs.CV cs.AI published:2011-01-19 summary:Approximating adequate number of clusters in multidimensional data is an openarea of research, given a level of compromise made on the quality of acceptableresults. The manuscript addresses the issue by formulating a transductiveinductive learning algorithm which uses multivariate Chebyshev inequality.Considering clustering problem in imaging, theoretical proofs for a particularlevel of compromise are derived to show the convergence of the reconstructionerror to a finite value with increasing (a) number of unseen examples and (b)the number of clusters, respectively. Upper bounds for these error rates arealso proved. Non-parametric estimates of these error from a random sample ofsequences empirically point to a stable number of clusters. Lastly, thegeneralization of algorithm can be applied to multidimensional data sets fromdifferent fields.
arxiv-600-111 | Clustered Bandits | http://arxiv.org/pdf/1206.4169v1.pdf | author:Loc Bui, Ramesh Johari, Shie Mannor category:cs.LG published:2012-06-19 summary:We consider a multi-armed bandit setting that is inspired by real-worldapplications in e-commerce. In our setting, there are a few types of users,each with a specific response to the different arms. When a user enters thesystem, his type is unknown to the decision maker. The decision maker caneither treat each user separately ignoring the previously observed users, orcan attempt to take advantage of knowing that only few types exist and clusterthe users according to their response to the arms. We devise algorithms thatcombine the usual exploration-exploitation tradeoff with clustering of usersand demonstrate the value of clustering. In the process of developingalgorithms for the clustered setting, we propose and analyze simple algorithmsfor the setup where a decision maker knows that a user belongs to one of fewtypes, but does not know which one.
arxiv-600-112 | Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information | http://arxiv.org/pdf/1206.4116v1.pdf | author:Makoto Yamada, Leonid Sigal, Michalis Raptis, Masashi Sugiyama category:stat.ML cs.AI published:2012-06-19 summary:The goal of temporal alignment is to establish time correspondence betweentwo sequences, which has many applications in a variety of areas such as speechprocessing, bioinformatics, computer vision, and computer graphics. In thispaper, we propose a novel temporal alignment method called least-squaresdynamic time warping (LSDTW). LSDTW finds an alignment that maximizesstatistical dependency between sequences, measured by a squared-loss variant ofmutual information. The benefit of this novel information-theoretic formulationis that LSDTW can align sequences with different lengths, differentdimensionality, high non-linearity, and non-Gaussianity in a computationallyefficient manner. In addition, model parameters such as an initial alignmentmatrix can be systematically optimized by cross-validation. We demonstrate theusefulness of LSDTW through experiments on synthetic and real-world Kinectaction recognition datasets.
arxiv-600-113 | ConeRANK: Ranking as Learning Generalized Inequalities | http://arxiv.org/pdf/1206.4110v1.pdf | author:Truyen T. Tran, Duc Son Pham category:cs.LG cs.IR published:2012-06-19 summary:We propose a new data mining approach in ranking documents based on theconcept of cone-based generalized inequalities between vectors. A partialordering between two vectors is made with respect to a proper cone and thuslearning the preferences is formulated as learning proper cones. A pairwiselearning-to-rank algorithm (ConeRank) is proposed to learn a non-negativesubspace, formulated as a polyhedral cone, over document-pair differences. Thealgorithm is regularized by controlling the `volume' of the cone. Theexperimental studies on the latest and largest ranking dataset LETOR 4.0 showsthat ConeRank is competitive against other recent ranking approaches.
arxiv-600-114 | The Ultrasound Visualization Pipeline - A Survey | http://arxiv.org/pdf/1206.3975v1.pdf | author:Åsmund Birkeland, Veronika Solteszova, Dieter Hönigmann, Odd Helge Gilja, Svein Brekke, Timo Ropinski, Ivan Viola category:cs.GR cs.CV published:2012-06-18 summary:Ultrasound is one of the most frequently used imaging modality in medicine.The high spatial resolution, its interactive nature and non-invasiveness makesit the first choice in many examinations. Image interpretation is one ofultrasound's main challenges. Much training is required to obtain a confidentskill level in ultrasound-based diagnostics. State-of-the-art graphicstechniques is needed to provide meaningful visualizations of ultrasound inreal-time. In this paper we present the process-pipeline for ultrasoundvisualization, including an overview of the tasks performed in the specificsteps. To provide an insight into the trends of ultrasound visualizationresearch, we have selected a set of significant publications and divided theminto a technique-based taxonomy covering the topics pre-processing,segmentation, registration, rendering and augmented reality. For the differenttechnique types we discuss the difference between ultrasound-based techniquesand techniques for other modalities.
arxiv-600-115 | Discriminative Probabilistic Prototype Learning | http://arxiv.org/pdf/1206.4686v1.pdf | author:Edwin Bonilla, Antonio Robles-Kelly category:cs.LG stat.ML published:2012-06-18 summary:In this paper we propose a simple yet powerful method for learningrepresentations in supervised learning scenarios where each original inputdatapoint is described by a set of vectors and their associated outputs may begiven by soft labels indicating, for example, class probabilities. We representan input datapoint as a mixture of probabilities over the corresponding set offeature vectors where each probability indicates how likely each vector is tobelong to an unknown prototype pattern. We propose a probabilistic model thatparameterizes these prototype patterns in terms of hidden variables andtherefore it can be trained with conventional approaches based on likelihoodmaximization. More importantly, both the model parameters and the prototypepatterns can be learned from data in a discriminative way. We show that ourmodel can be seen as a probabilistic generalization of learning vectorquantization (LVQ). We apply our method to the problems of shapeclassification, hyperspectral imaging classification and people's work classcategorization, showing the superior performance of our method compared to thestandard prototype-based classification approach and other competitivebenchmark methods.
arxiv-600-116 | Sparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling | http://arxiv.org/pdf/1206.4685v1.pdf | author:Yan Liu, Taha Bahadori, Hongfei Li category:stat.ME cs.LG stat.AP published:2012-06-18 summary:In many applications of time series models, such as climate analysis andsocial media analysis, we are often interested in extreme events, such asheatwave, wind gust, and burst of topics. These time series data usuallyexhibit a heavy-tailed distribution rather than a Gaussian distribution. Thisposes great challenges to existing approaches due to the significantlydifferent assumptions on the data distributions and the lack of sufficient pastdata on extreme events. In this paper, we propose the Sparse-GEV model, alatent state model based on the theory of extreme value modeling toautomatically learn sparse temporal dependence and make predictions. Our modelis theoretically significant because it is among the first models to learnsparse temporal dependencies among multivariate extreme value time series. Wedemonstrate the superior performance of our algorithm to the state-of-artmethods, including Granger causality, copula approach, and transfer entropy, onone synthetic dataset, one climate dataset and two Twitter datasets.
arxiv-600-117 | Marginalized Denoising Autoencoders for Domain Adaptation | http://arxiv.org/pdf/1206.4683v1.pdf | author:Minmin Chen, Zhixiang Xu, Kilian Weinberger, Fei Sha category:cs.LG published:2012-06-18 summary:Stacked denoising autoencoders (SDAs) have been successfully used to learnnew representations for domain adaptation. Recently, they have attained recordaccuracy on standard benchmark tasks of sentiment analysis across differenttext domains. SDAs learn robust data representations by reconstruction,recovering original features from data that are artificially corrupted withnoise. In this paper, we propose marginalized SDA (mSDA) that addresses twocrucial limitations of SDAs: high computational cost and lack of scalability tohigh-dimensional features. In contrast to SDAs, our approach of mSDAmarginalizes noise and thus does not require stochastic gradient descent orother optimization algorithms to learn parameters ? in fact, they are computedin closed-form. Consequently, mSDA, which can be implemented in only 20 linesof MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude.Furthermore, the representations learnt by mSDA are as effective as thetraditional SDAs, attaining almost identical accuracies in benchmark tasks.
arxiv-600-118 | Copula-based Kernel Dependency Measures | http://arxiv.org/pdf/1206.4682v1.pdf | author:Barnabas Poczos, Zoubin Ghahramani, Jeff Schneider category:cs.LG math.ST stat.ML stat.TH published:2012-06-18 summary:The paper presents a new copula based method for measuring dependence betweenrandom variables. Our approach extends the Maximum Mean Discrepancy to thecopula of the joint distribution. We prove that this approach has severaladvantageous properties. Similarly to Shannon mutual information, the proposeddependence measure is invariant to any strictly increasing transformation ofthe marginal variables. This is important in many applications, for example infeature selection. The estimator is consistent, robust to outliers, and usesrank statistics only. We derive upper bounds on the convergence rate andpropose independence tests too. We illustrate the theoretical contributionsthrough a series of experiments in feature selection and low-dimensionalembedding of distributions.
arxiv-600-119 | LPQP for MAP: Putting LP Solvers to Better Use | http://arxiv.org/pdf/1206.4681v1.pdf | author:Patrick Pletscher, Sharon Wulff category:cs.LG stat.ML published:2012-06-18 summary:MAP inference for general energy functions remains a challenging problem.While most efforts are channeled towards improving the linear programming (LP)based relaxation, this work is motivated by the quadratic programming (QP)relaxation. We propose a novel MAP relaxation that penalizes theKullback-Leibler divergence between the LP pairwise auxiliary variables, and QPequivalent terms given by the product of the unaries. We develop two efficientalgorithms based on variants of this relaxation. The algorithms minimize thenon-convex objective using belief propagation and dual decomposition asbuilding blocks. Experiments on synthetic and real-world data show that thesolutions returned by our algorithms substantially improve over the LPrelaxation.
arxiv-600-120 | Fast Prediction of New Feature Utility | http://arxiv.org/pdf/1206.4680v1.pdf | author:Hoyt Koepke, Mikhail Bilenko category:cs.LG math.ST stat.TH published:2012-06-18 summary:We study the new feature utility prediction problem: statistically testingwhether adding a new feature to the data representation can improve predictiveaccuracy on a supervised learning task. In many applications, identifying newinformative features is the primary pathway for improving performance. However,evaluating every potential feature by re-training the predictor with it can becostly. The paper describes an efficient, learner-independent technique forestimating new feature utility without re-training based on the currentpredictor's outputs. The method is obtained by deriving a connection betweenloss reduction potential and the new feature's correlation with the lossgradient of the current predictor. This leads to a simple yet powerfulhypothesis testing procedure, for which we prove consistency. Our theoreticalanalysis is accompanied by empirical evaluation on standard benchmarks and alarge-scale industrial dataset.
arxiv-600-121 | Factorized Asymptotic Bayesian Hidden Markov Models | http://arxiv.org/pdf/1206.4679v1.pdf | author:Ryohei Fujimaki, Kohei Hayashi category:cs.LG stat.ML published:2012-06-18 summary:This paper addresses the issue of model selection for hidden Markov models(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which hasbeen recently developed for model selection on independent hidden variables(i.e., mixture models), for time-dependent hidden variables. As with FAB inmixture models, FAB for HMMs is derived as an iterative lower boundmaximization algorithm of a factorized information criterion (FIC). Itinherits, from FAB for mixture models, several desirable properties forlearning HMMs, such as asymptotic consistency of FIC with marginallog-likelihood, a shrinkage effect for hidden state selection, monotonicincrease of the lower FIC bound through the iterative optimization. Further, itdoes not have a tunable hyper-parameter, and thus its model selection processcan be fully automated. Experimental results shows that FAB outperformsstates-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM interms of model selection accuracy and computational efficiency.
arxiv-600-122 | Linear Regression with Limited Observation | http://arxiv.org/pdf/1206.4678v1.pdf | author:Elad Hazan, Tomer Koren category:cs.LG stat.ML published:2012-06-18 summary:We consider the most common variants of linear regression, including Ridge,Lasso and Support-vector regression, in a setting where the learner is allowedto observe only a fixed number of attributes of each example at training time.We present simple and efficient algorithms for these problems: for Lasso andRidge regression they need the same total number of attributes (up toconstants) as do full-information algorithms, for reaching a certain accuracy.For Support-vector regression, we require exponentially less attributescompared to the state of the art. By that, we resolve an open problem recentlyposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds tobe justified by superior performance compared to the state of the art.
arxiv-600-123 | Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching | http://arxiv.org/pdf/1206.4677v1.pdf | author:Marthinus Du Plessis, Masashi Sugiyama category:cs.LG stat.ML published:2012-06-18 summary:In real-world classification problems, the class balance in the trainingdataset does not necessarily reflect that of the test dataset, which can causesignificant estimation bias. If the class ratio of the test dataset is known,instance re-weighting or resampling allows systematical bias correction.However, learning the class ratio of the test dataset is challenging when nolabeled data is available from the test domain. In this paper, we propose toestimate the class ratio in the test dataset by matching probabilitydistributions of training and test input data. We demonstrate the utility ofthe proposed approach through experiments.
arxiv-600-124 | Clustering by Low-Rank Doubly Stochastic Matrix Decomposition | http://arxiv.org/pdf/1206.4676v1.pdf | author:Zhirong Yang, Erkki Oja category:cs.LG cs.CV cs.NA stat.ML published:2012-06-18 summary:Clustering analysis by nonnegative low-rank approximations has achievedremarkable progress in the past decade. However, most approximation approachesin this direction are still restricted to matrix factorization. We propose anew low-rank learning method to improve the clustering performance, which isbeyond matrix factorization. The approximation is based on a two-step bipartiterandom walk through virtual cluster nodes, where the approximation is formed byonly cluster assigning probabilities. Minimizing the approximation errormeasured by Kullback-Leibler divergence is equivalent to maximizing thelikelihood of a discriminative model, which endows our method with a solidprobabilistic interpretation. The optimization is implemented by a relaxedMajorization-Minimization algorithm that is advantageous in finding good localminima. Furthermore, we point out that the regularized algorithm with Dirichletprior only serves as initialization. Experimental results show that the newmethod has strong performance in clustering purity for various datasets,especially for large-scale manifold data.
arxiv-600-125 | Finding Botnets Using Minimal Graph Clusterings | http://arxiv.org/pdf/1206.4675v1.pdf | author:Peter Haider, Tobias Scheffer category:cs.CR cs.DC cs.LG published:2012-06-18 summary:We study the problem of identifying botnets and the IP addresses which theycomprise, based on the observation of a fraction of the global email spamtraffic. Observed mailing campaigns constitute evidence for joint botnetmembership, they are represented by cliques in the graph of all messages. Noevidence against an association of nodes is ever available. We reduce theproblem of identifying botnets to a problem of finding a minimal clustering ofthe graph of messages. We directly model the distribution of clusterings giventhe input graph; this avoids potential errors caused by distributionalassumptions of a generative model. We report on a case study in which weevaluate the model by its ability to predict the spam campaign that a given IPaddress is going to participate in.
arxiv-600-126 | Comparison-Based Learning with Rank Nets | http://arxiv.org/pdf/1206.4674v1.pdf | author:Amin Karbasi, Stratis Ioannidis, laurent Massoulie category:cs.LG cs.DS stat.ML published:2012-06-18 summary:We consider the problem of search through comparisons, where a user ispresented with two candidate objects and reveals which is closer to herintended target. We study adaptive strategies for finding the target, thatrequire knowledge of rank relationships but not actual distances betweenobjects. We propose a new strategy based on rank nets, and show that for targetdistributions with a bounded doubling constant, it finds the target in a numberof comparisons close to the entropy of the target distribution and, hence, ofthe optimum. We extend these results to the case of noisy oracles, and comparethis strategy to prior art over multiple datasets.
arxiv-600-127 | Group Sparse Additive Models | http://arxiv.org/pdf/1206.4673v1.pdf | author:Junming Yin, Xi Chen, Eric Xing category:cs.LG stat.ML published:2012-06-18 summary:We consider the problem of sparse variable selection in nonparametricadditive models, with the prior knowledge of the structure among the covariatesto encourage those variables within a group to be selected jointly. Previousworks either study the group sparsity in the parametric setting (e.g., grouplasso), or address the problem in the non-parametric setting without exploitingthe structural information (e.g., sparse additive models). In this paper, wepresent a new method, called group sparse additive models (GroupSpAM), whichcan handle group sparsity in additive models. We generalize the l1/l2 norm toHilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, wederive a novel thresholding condition for identifying the functional sparsityat the group level, and propose an efficient block coordinate descent algorithmfor constructing the estimate. We demonstrate by simulation that GroupSpAMsubstantially outperforms the competing methods in terms of support recoveryand prediction accuracy in additive models, and also conduct a comparativeexperiment on a real breast cancer dataset.
arxiv-600-128 | Efficient Active Algorithms for Hierarchical Clustering | http://arxiv.org/pdf/1206.4672v1.pdf | author:Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, Aarti Singh category:cs.LG stat.ML published:2012-06-18 summary:Advances in sensing technologies and the growth of the internet have resultedin an explosion in the size of modern datasets, while storage and processingpower continue to lag behind. This motivates the need for algorithms that areefficient, both in terms of the number of measurements needed and running time.To combat the challenges associated with large datasets, we propose a generalframework for active hierarchical clustering that repeatedly runs anoff-the-shelf clustering algorithm on small subsets of the data and comes withguarantees on performance, measurement complexity and runtime complexity. Weinstantiate this framework with a simple spectral clustering algorithm andprovide concrete results on its performance, showing that, under someassumptions, this algorithm recovers all clusters of size ?(log n) using O(nlog^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.Through extensive experimentation we also demonstrate that this framework ispractically alluring.
arxiv-600-129 | Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling | http://arxiv.org/pdf/1206.4671v1.pdf | author:Changyou Chen, Nan Ding, Wray Buntine category:cs.LG stat.ML published:2012-06-18 summary:We develop dependent hierarchical normalized random measures and apply themto dynamic topic modeling. The dependency arises via superposition, subsamplingand point transition on the underlying Poisson processes of these measures. Themeasures used include normalised generalised Gamma processes that demonstratepower law properties, unlike Dirichlet processes used previously in dynamictopic modeling. Inference for the model includes adapting a recently developedslice sampler to directly manipulate the underlying Poisson process.Experiments performed on news, blogs, academic and Twitter collectionsdemonstrate the technique gives superior perplexity over a number of previousmodels.
arxiv-600-130 | State-Space Inference for Non-Linear Latent Force Models with Application to Satellite Orbit Prediction | http://arxiv.org/pdf/1206.4670v1.pdf | author:Jouni Hartikainen, Mari Seppanen, Simo Sarkka category:cs.IT astro-ph.EP cs.LG math.IT published:2012-06-18 summary:Latent force models (LFMs) are flexible models that combine mechanisticmodelling principles (i.e., physical models) with non-parametric data-drivencomponents. Several key applications of LFMs need non-linearities, whichresults in analytically intractable inference. In this work we show hownon-linear LFMs can be represented as non-linear white noise driven state-spacemodels and present an efficient non-linear Kalman filtering and smoothing basedmethod for approximate state and parameter inference. We illustrate theperformance of the proposed methodology via two simulated examples, and applyit to a real-world problem of long-term prediction of GPS satellite orbits.
arxiv-600-131 | Sparse Additive Functional and Kernel CCA | http://arxiv.org/pdf/1206.4669v1.pdf | author:Sivaraman Balakrishnan, Kriti Puniyani, John Lafferty category:cs.LG stat.ML published:2012-06-18 summary:Canonical Correlation Analysis (CCA) is a classical tool for findingcorrelations among the components of two random vectors. In recent years, CCAhas been widely applied to the analysis of genomic data, where it is common forresearchers to perform multiple assays on a single set of patient samples.Recent work has proposed sparse variants of CCA to address the highdimensionality of such data. However, classical and sparse CCA are based onlinear models, and are thus limited in their ability to find generalcorrelations. In this paper, we present two approaches to high-dimensionalnonparametric CCA, building on recent developments in high-dimensionalnonparametric regression. We present estimation procedures for both approaches,and analyze their theoretical properties in the high-dimensional setting. Wedemonstrate the effectiveness of these procedures in discovering nonlinearcorrelations via extensive simulations, as well as through experiments withgenomic data.
arxiv-600-132 | Approximate Principal Direction Trees | http://arxiv.org/pdf/1206.4668v1.pdf | author:Mark McCartin-Lim, Andrew McGregor, Rui Wang category:cs.LG cs.DS stat.ML published:2012-06-18 summary:We introduce a new spatial data structure for high dimensional data calledthe \emph{approximate principal direction tree} (APD tree) that adapts to theintrinsic dimension of the data. Our algorithm ensures vector-quantizationaccuracy similar to that of computationally-expensive PCA trees with similartime-complexity to that of lower-accuracy RP trees. APD trees use a small number of power-method iterations to find splittingplanes for recursively partitioning the data. As such they provide a naturaltrade-off between the running-time and accuracy achieved by RP and PCA trees.Our theoretical results establish a) strong performance guarantees regardlessof the convergence rate of the power-method and b) that $O(\log d)$ iterationssuffice to establish the guarantee of PCA trees when the intrinsic dimension is$d$. We demonstrate this trade-off and the efficacy of our data structure onboth the CPU and GPU.
arxiv-600-133 | A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices | http://arxiv.org/pdf/1206.4666v1.pdf | author:Mingjun Zhong, Mark Girolami category:stat.CO cs.LG stat.ME published:2012-06-18 summary:We present a Bayesian scheme for the approximate diagonalisation of severalsquare matrices which are not necessarily symmetric. A Gibbs sampler is derivedto simulate samples of the common eigenvectors and the eigenvalues for thesematrices. Several synthetic examples are used to illustrate the performance ofthe proposed Gibbs sampler and we then provide comparisons to several otherjoint diagonalization algorithms, which shows that the Gibbs sampler achievesthe state-of-the-art performance on the examples considered. As a byproduct,the output of the Gibbs sampler could be used to estimate the log marginallikelihood, however we employ the approximation based on the Bayesianinformation criterion (BIC) which in the synthetic examples consideredcorrectly located the number of common eigenvectors. We then succesfullyapplied the sampler to the source separation problem as well as the commonprincipal component analysis and the common spatial pattern analysis problems.
arxiv-600-134 | Nonparametric variational inference | http://arxiv.org/pdf/1206.4665v1.pdf | author:Samuel Gershman, Matt Hoffman, David Blei category:cs.LG stat.ML published:2012-06-18 summary:Variational methods are widely used for approximate posterior inference.However, their use is typically limited to families of distributions that enjoyparticular conjugacy properties. To circumvent this limitation, we propose afamily of variational approximations inspired by nonparametric kernel densityestimation. The locations of these kernels and their bandwidth are treated asvariational parameters and optimized to improve an approximate lower bound onthe marginal likelihood of the data. Using multiple kernels allows theapproximation to capture multiple modes of the posterior, unlike most othervariational approximations. We demonstrate the efficacy of the nonparametricapproximation with a hierarchical logistic regression model and a nonlinearmatrix factorization model. We obtain predictive performance as good as orbetter than more specialized variational methods and sample-basedapproximations. The method is easy to apply to more general graphical modelsfor which standard variational methods are difficult to derive.
arxiv-600-135 | Tighter Variational Representations of f-Divergences via Restriction to Probability Measures | http://arxiv.org/pdf/1206.4664v1.pdf | author:Avraham Ruderman, Mark Reid, Dario Garcia-Garcia, James Petterson category:cs.LG stat.ML published:2012-06-18 summary:We show that the variational representations for f-divergences currently usedin the literature can be tightened. This has implications to a number ofmethods recently proposed based on this representation. As an exampleapplication we use our tighter representation to derive a general f-divergenceestimator based on two i.i.d. samples and derive the dual program for thisestimator that performs well empirically. We also point out a connectionbetween our estimator and MMD.
arxiv-600-136 | The Convexity and Design of Composite Multiclass Losses | http://arxiv.org/pdf/1206.4663v1.pdf | author:Mark Reid, Robert Williamson, Peng Sun category:cs.LG stat.ML published:2012-06-18 summary:We consider composite loss functions for multiclass prediction comprising aproper (i.e., Fisher-consistent) loss over probability distributions and aninverse link function. We establish conditions for their (strong) convexity andexplore the implications. We also show how the separation of concerns affordedby using this composite representation allows for the design of families oflosses with the same Bayes risk.
arxiv-600-137 | Bayesian Watermark Attacks | http://arxiv.org/pdf/1206.4662v1.pdf | author:Ivo Shterev, David Dunson category:cs.CR cs.LG cs.MM published:2012-06-18 summary:This paper presents an application of statistical machine learning to thefield of watermarking. We propose a new attack model on additivespread-spectrum watermarking systems. The proposed attack is based on Bayesianstatistics. We consider the scenario in which a watermark signal is repeatedlyembedded in specific, possibly chosen based on a secret message bitstream,segments (signals) of the host data. The host signal can represent a patch ofpixels from an image or a video frame. We propose a probabilistic model thatinfers the embedded message bitstream and watermark signal, directly from thewatermarked data, without access to the decoder. We develop an efficient Markovchain Monte Carlo sampler for updating the model parameters from theirconjugate full conditional posteriors. We also provide a variational Bayesiansolution, which further increases the convergence speed of the algorithm.Experiments with synthetic and real image signals demonstrate that the attackmodel is able to correctly infer a large part of the message bitstream andobtain a very accurate estimate of the watermark signal.
arxiv-600-138 | Predicting accurate probabilities with a ranking loss | http://arxiv.org/pdf/1206.4661v1.pdf | author:Aditya Menon, Xiaoqian Jiang, Shankar Vembu, Charles Elkan, Lucila Ohno-Machado category:cs.LG stat.ML published:2012-06-18 summary:In many real-world applications of machine learning classifiers, it isessential to predict the probability of an example belonging to a particularclass. This paper proposes a simple technique for predicting probabilitiesbased on optimizing a ranking loss, followed by isotonic regression. Thissemi-parametric technique offers both good ranking and regression performance,and models a richer set of probability distributions than statisticalworkhorses such as logistic regression. We provide experimental results thatshow the effectiveness of this technique on real-world applications ofprobability prediction.
arxiv-600-139 | Learning with Augmented Features for Heterogeneous Domain Adaptation | http://arxiv.org/pdf/1206.4660v1.pdf | author:Lixin Duan, Dong Xu, Ivor Tsang category:cs.LG published:2012-06-18 summary:We propose a new learning method for heterogeneous domain adaptation (HDA),in which the data from the source domain and the target domain are representedby heterogeneous features with different dimensions. Using two differentprojection matrices, we first transform the data from two domains into a commonsubspace in order to measure the similarity between the data from two domains.We then propose two new feature mapping functions to augment the transformeddata with their original features and zeros. The existing learning methods(e.g., SVM and SVR) can be readily incorporated with our newly proposedaugmented feature representations to effectively utilize the data from bothdomains for HDA. Using the hinge loss function in SVM as an example, weintroduce the detailed objective function in our method called HeterogeneousFeature Augmentation (HFA) for a linear case and also describe itskernelization in order to efficiently cope with the data with very highdimensions. Moreover, we also develop an alternating optimization algorithm toeffectively solve the nontrivial optimization problem in our HFA method.Comprehensive experiments on two benchmark datasets clearly demonstrate thatHFA outperforms the existing HDA methods.
arxiv-600-140 | Max-Margin Nonparametric Latent Feature Models for Link Prediction | http://arxiv.org/pdf/1206.4659v1.pdf | author:Jun Zhu category:cs.LG stat.ML published:2012-06-18 summary:We present a max-margin nonparametric latent feature model, which unites theideas of max-margin learning and Bayesian nonparametrics to discoverdiscriminative latent features for link prediction and automatically infer theunknown latent social dimension. By minimizing a hinge-loss using the linearexpectation operator, we can perform posterior inference efficiently withoutdealing with a highly nonlinear link likelihood function; by using afully-Bayesian formulation, we can avoid tuning regularization constants.Experimental results on real datasets appear to demonstrate the benefitsinherited from max-margin learning and fully-Bayesian nonparametric inference.
arxiv-600-141 | Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data | http://arxiv.org/pdf/1206.4658v1.pdf | author:Dongwoo Kim, Suin Kim, Alice Oh category:cs.LG stat.ML published:2012-06-18 summary:We describe a nonparametric topic model for labeled data. The model uses amixture of random measures (MRM) as a base distribution of the Dirichletprocess (DP) of the HDP framework, so we call it the DP-MRM. To model labeleddata, we define a DP distributed random measure for each label, and theresulting model generates an unbounded number of topics for each label. Weapply DP-MRM on single-labeled and multi-labeled corpora of documents andcompare the performance on label prediction with MedLDA, LDA-SVM, andLabeled-LDA. We further enhance the model by incorporating ddCRP and modelingmulti-labeled images for image segmentation and object labeling, comparing theperformance with nCuts and rddCRP.
arxiv-600-142 | Projection-free Online Learning | http://arxiv.org/pdf/1206.4657v1.pdf | author:Elad Hazan, Satyen Kale category:cs.LG cs.DS published:2012-06-18 summary:The computational bottleneck in applying online learning to massive data setsis usually the projection step. We present efficient online learning algorithmsthat eschew projections in favor of much more efficient linear optimizationsteps using the Frank-Wolfe technique. We obtain a range of regret bounds foronline convex optimization, with better bounds for specific cases such asstochastic online smooth convex optimization. Besides the computational advantage, other desirable features of ouralgorithms are that they are parameter-free in the stochastic case and producesparse decisions. We apply our algorithms to computationally intensiveapplications of collaborative filtering, and show the theoretical improvementsto be clearly visible on standard datasets.
arxiv-600-143 | Machine Learning that Matters | http://arxiv.org/pdf/1206.4656v1.pdf | author:Kiri Wagstaff category:cs.LG cs.AI stat.ML published:2012-06-18 summary:Much of current machine learning (ML) research has lost its connection toproblems of import to the larger world of science and society. From thisperspective, there exist glaring limitations in the data sets we investigate,the metrics we employ for evaluation, and the degree to which results arecommunicated back to their originating domains. What changes are needed to howwe conduct research to increase the impact that ML has? We present six ImpactChallenges to explicitly focus the field?s energy and attention, and we discussexisting obstacles that must be addressed. We aim to inspire ongoing discussionand focus on ML that matters.
arxiv-600-144 | Modelling transition dynamics in MDPs with RKHS embeddings | http://arxiv.org/pdf/1206.4655v1.pdf | author:Steffen Grunewalder, Guy Lever, Luca Baldassarre, Massi Pontil, Arthur Gretton category:cs.LG published:2012-06-18 summary:We propose a new, nonparametric approach to learning and representingtransition dynamics in Markov decision processes (MDPs), which can be combinedeasily with dynamic programming methods for policy optimisation and valueestimation. This approach makes use of a recently developed representation ofconditional distributions as \emph{embeddings} in a reproducing kernel Hilbertspace (RKHS). Such representations bypass the need for estimating transitionprobabilities or densities, and apply to any domain on which kernels can bedefined. This avoids the need to calculate intractable integrals, sinceexpectations are represented as RKHS inner products whose computation haslinear complexity in the number of points used to represent the embedding. Weprovide guarantees for the proposed applications in MDPs: in the context of avalue iteration algorithm, we prove convergence to either the optimal policy,or to the closest projection of the optimal policy in our model class (anRKHS), under reasonable assumptions. In experiments, we investigate a learningtask in a typical classical control setting (the under-actuated pendulum), andon a navigation problem where only images from a sensor are observed. Forpolicy optimisation we compare with least-squares policy iteration where aGaussian process is used for value function estimation. For value estimation wealso compare to the NPDP method. Our approach achieves better performance inall experiments.
arxiv-600-145 | A Generalized Loop Correction Method for Approximate Inference in Graphical Models | http://arxiv.org/pdf/1206.4654v1.pdf | author:Siamak Ravanbakhsh, Chun-Nam Yu, Russell Greiner category:cs.AI cs.LG stat.ML published:2012-06-18 summary:Belief Propagation (BP) is one of the most popular methods for inference inprobabilistic graphical models. BP is guaranteed to return the correct answerfor tree structures, but can be incorrect or non-convergent for loopy graphicalmodels. Recently, several new approximate inference algorithms based on cavitydistribution have been proposed. These methods can account for the effect ofloops by incorporating the dependency between BP messages. Alternatively,region-based approximations (that lead to methods such as Generalized BeliefPropagation) improve upon BP by considering interactions within small clustersof variables, thus taking small loops within these clusters into account. Thispaper introduces an approach, Generalized Loop Correction (GLC), that benefitsfrom both of these types of loop correction. We show how GLC relates to thesetwo families of inference methods, then provide empirical evidence that GLCworks effectively in general, and can be significantly more accurate than bothcorrection schemes.
arxiv-600-146 | Dimensionality Reduction by Local Discriminative Gaussians | http://arxiv.org/pdf/1206.4653v1.pdf | author:Nathan Parrish, Maya Gupta category:cs.LG cs.CV stat.ML published:2012-06-18 summary:We present local discriminative Gaussian (LDG) dimensionality reduction, asupervised dimensionality reduction technique for classification. The LDGobjective function is an approximation to the leave-one-out training error of alocal quadratic discriminant analysis classifier, and thus acts locally to eachtraining point in order to find a mapping where similar data can bediscriminated from dissimilar data. While other state-of-the-art lineardimensionality reduction methods require gradient descent or iterative solutionapproaches, LDG is solved with a single eigen-decomposition. Thus, it scalesbetter for datasets with a large number of feature dimensions or trainingexamples. We also adapt LDG to the transfer learning setting, and show that itachieves good performance when the test data distribution differs from that ofthe training data.
arxiv-600-147 | The Most Persistent Soft-Clique in a Set of Sampled Graphs | http://arxiv.org/pdf/1206.4652v1.pdf | author:Novi Quadrianto, Chao Chen, Christoph Lampert category:cs.LG cs.AI published:2012-06-18 summary:When searching for characteristic subpatterns in potentially noisy graphdata, it appears self-evident that having multiple observations would be betterthan having just one. However, it turns out that the inconsistencies introducedwhen different graph instances have different edge sets pose a seriouschallenge. In this work we address this challenge for the problem of findingmaximum weighted cliques. We introduce the concept of most persistent soft-clique. This is subset ofvertices, that 1) is almost fully or at least densely connected, 2) occurs inall or almost all graph instances, and 3) has the maximum weight. We present ameasure of clique-ness, that essentially counts the number of edge missing tomake a subset of vertices into a clique. With this measure, we show that theproblem of finding the most persistent soft-clique problem can be cast eitheras: a) a max-min two person game optimization problem, or b) a min-min softmargin optimization problem. Both formulations lead to the same solution whenusing a partial Lagrangian method to solve the optimization problems. Byexperiments on synthetic data and on real social network data, we show that theproposed method is able to reliably find soft cliques in graph data, even ifthat is distorted by random noise or unreliable observations.
arxiv-600-148 | Is margin preserved after random projection? | http://arxiv.org/pdf/1206.4651v1.pdf | author:Qinfeng Shi, Chunhua Shen, Rhys Hill, Anton van den Hengel category:cs.LG cs.CV stat.ML published:2012-06-18 summary:Random projections have been applied in many machine learning algorithms.However, whether margin is preserved after random projection is non-trivial andnot well studied. In this paper we analyse margin distortion after randomprojection, and give the conditions of margin preservation for binaryclassification problems. We also extend our analysis to margin for multiclassproblems, and provide theoretical bounds on multiclass margin on the projecteddata.
arxiv-600-149 | Analysis of Kernel Mean Matching under Covariate Shift | http://arxiv.org/pdf/1206.4650v1.pdf | author:Yaoliang Yu, Csaba Szepesvari category:cs.LG stat.ML published:2012-06-18 summary:In real supervised learning scenarios, it is not uncommon that the trainingand test sample follow different probability distributions, thus rendering thenecessity to correct the sampling bias. Focusing on a particular covariateshift problem, we derive high probability confidence bounds for the kernel meanmatching (KMM) estimator, whose convergence rate turns out to depend on someregularity measure of the regression function and also on some capacity measureof the kernel. By comparing KMM with the natural plug-in estimator, weestablish the superiority of the former hence provide concreteevidence/understanding to the effectiveness of KMM under covariate shift.
arxiv-600-150 | Learning Efficient Structured Sparse Models | http://arxiv.org/pdf/1206.4649v1.pdf | author:Alex Bronstein, Pablo Sprechmann, Guillermo Sapiro category:cs.LG cs.CV stat.ML published:2012-06-18 summary:We present a comprehensive framework for structured sparse coding andmodeling extending the recent ideas of using learnable fast regressors toapproximate exact sparse codes. For this purpose, we develop a novelblock-coordinate proximal splitting method for the iterative solution ofhierarchical sparse coding problems, and show an efficient feed forwardarchitecture derived from its iteration. This architecture faithfullyapproximates the exact structured sparse codes with a fraction of thecomplexity of the standard optimization methods. We also show that by usingdifferent training objective functions, learnable sparse encoders are no longerrestricted to be mere approximants of the exact sparse code for a pre-givendictionary, as in earlier formulations, but can be rather used as full-featuredsparse encoders or even modelers. A simple implementation shows several ordersof magnitude speedup compared to the state-of-the-art at minimal performancedegradation, making the proposed framework suitable for real time andlarge-scale applications.
arxiv-600-151 | Two-Manifold Problems with Applications to Nonlinear System Identification | http://arxiv.org/pdf/1206.4648v1.pdf | author:Byron Boots, Geoff Gordon category:cs.LG published:2012-06-18 summary:Recently, there has been much interest in spectral approaches to learningmanifolds---so-called kernel eigenmap methods. These methods have had somesuccesses, but their applicability is limited because they are not robust tonoise. To address this limitation, we look at two-manifold problems, in whichwe simultaneously reconstruct two related manifolds, each representing adifferent view of the same data. By solving these interconnected learningproblems together, two-manifold algorithms are able to succeed where anon-integrated approach would fail: each view allows us to suppress noise inthe other, reducing bias. We propose a class of algorithms for two-manifoldproblems, based on spectral decomposition of cross-covariance operators inHilbert space, and discuss when two-manifold problems are useful. Finally, wedemonstrate that solving a two-manifold problem can aid in learning a nonlineardynamical system from limited data.
arxiv-600-152 | Active Learning for Matching Problems | http://arxiv.org/pdf/1206.4647v1.pdf | author:Laurent Charlin, Rich Zemel, Craig Boutilier category:cs.LG cs.AI cs.IR published:2012-06-18 summary:Effective learning of user preferences is critical to easing user burden invarious types of matching problems. Equally important is active query selectionto further reduce the amount of preference information users must provide. Weaddress the problem of active learning of user preferences for matchingproblems, introducing a novel method for determining probabilistic matchings,and developing several new active learning strategies that are sensitive to thespecific matching objective. Experiments with real-world data sets spanningdiverse domains demonstrate that matching-sensitive active learning
arxiv-600-153 | Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings | http://arxiv.org/pdf/1206.4646v1.pdf | author:Max Vladymyrov, Miguel Carreira-Perpinan category:cs.LG stat.ML published:2012-06-18 summary:Stochastic neighbor embedding (SNE) and related nonlinear manifold learningalgorithms achieve high-quality low-dimensional representations of similaritydata, but are notoriously slow to train. We propose a generic formulation ofembedding algorithms that includes SNE and other existing algorithms, and studytheir relation with spectral methods and graph Laplacians. This allows us todefine several partial-Hessian optimization strategies, characterize theirglobal and local convergence, and evaluate them empirically. We achieve up totwo orders of magnitude speedup over existing training methods with a strategy(which we call the spectral direction) that adds nearly no overhead to thegradient and yet is simple, scalable and applicable to several existing andfuture embedding algorithms.
arxiv-600-154 | Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design | http://arxiv.org/pdf/1206.4645v1.pdf | author:Lauren Hannah, David Dunson category:cs.LG cs.NA stat.ME stat.ML published:2012-06-18 summary:Convex regression is a promising area for bridging statistical estimation anddeterministic convex optimization. New piecewise linear convex regressionmethods are fast and scalable, but can have instability when used toapproximate constraints or objective functions for optimization. Ensemblemethods, like bagging, smearing and random partitioning, can alleviate thisproblem and maintain the theoretical properties of the underlying estimator. Weempirically examine the performance of ensemble methods for prediction andoptimization, and then apply them to device modeling and constraintapproximation for geometric programming based circuit design.
arxiv-600-155 | Groupwise Constrained Reconstruction for Subspace Clustering | http://arxiv.org/pdf/1206.4644v1.pdf | author:Ruijiang Li, Bin Li, Ke Zhang, Cheng Jin, Xiangyang Xue category:cs.LG stat.ML published:2012-06-18 summary:Reconstruction based subspace clustering methods compute a selfreconstruction matrix over the samples and use it for spectral clustering toobtain the final clustering result. Their success largely relies on theassumption that the underlying subspaces are independent, which, however, doesnot always hold in the applications with increasing number of subspaces. Inthis paper, we propose a novel reconstruction based subspace clustering modelwithout making the subspace independence assumption. In our model, certainproperties of the reconstruction matrix are explicitly characterized using thelatent cluster indicators, and the affinity matrix used for spectral clusteringcan be directly built from the posterior of the latent cluster indicatorsinstead of the reconstruction matrix. Experimental results on both syntheticand real-world datasets show that the proposed model can outperform thestate-of-the-art methods.
arxiv-600-156 | Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty | http://arxiv.org/pdf/1206.4643v1.pdf | author:Shie Mannor, Ofir Mebel, Huan Xu category:cs.LG cs.GT cs.SY published:2012-06-18 summary:We consider Markov decision processes under parameter uncertainty. Previousstudies all restrict to the case that uncertainties among different states areuncoupled, which leads to conservative solutions. In contrast, we introduce anintuitive concept, termed "Lightning Does not Strike Twice," to model coupleduncertain parameters. Specifically, we require that the system can deviate fromits nominal parameters only a bounded number of times. We give probabilisticguarantees indicating that this model represents real life situations anddevise tractable algorithms for computing optimal control policies using thisconcept.
arxiv-600-157 | Fast Computation of Subpath Kernel for Trees | http://arxiv.org/pdf/1206.4642v1.pdf | author:Daisuke Kimura, Hisashi Kashima category:cs.DS cs.LG stat.ML published:2012-06-18 summary:The kernel method is a potential approach to analyzing structured data suchas sequences, trees, and graphs; however, unordered trees have not beeninvestigated extensively. Kimura et al. (2011) proposed a kernel function forunordered trees on the basis of their subpaths, which are verticalsubstructures of trees responsible for hierarchical information in them. Theirkernel exhibits practically good performance in terms of accuracy and speed;however, linear-time computation is not guaranteed theoretically, unlike thecase of the other unordered tree kernel proposed by Vishwanathan and Smola(2003). In this paper, we propose a theoretically guaranteed linear-time kernelcomputation algorithm that is practically fast, and we present an efficientprediction algorithm whose running time depends only on the size of the inputtree. Experimental results show that the proposed algorithms are quiteefficient in practice.
arxiv-600-158 | Total Variation and Euler's Elastica for Supervised Learning | http://arxiv.org/pdf/1206.4641v1.pdf | author:Tong Lin, Hanlin Xue, Ling Wang, Hongbin Zha category:cs.LG cs.CV stat.ML published:2012-06-18 summary:In recent years, total variation (TV) and Euler's elastica (EE) have beensuccessfully applied to image processing tasks such as denoising andinpainting. This paper investigates how to extend TV and EE to the supervisedlearning settings on high dimensional data. The supervised learning problem canbe formulated as an energy functional minimization under Tikhonovregularization scheme, where the energy is composed of a squared loss and atotal variation smoothing (or Euler's elastica smoothing). Its solution viavariational principles leads to an Euler-Lagrange PDE. However, the PDE isalways high-dimensional and cannot be directly solved by common methods.Instead, radial basis functions are utilized to approximate the targetfunction, reducing the problem to finding the linear coefficients of basisfunctions. We apply the proposed methods to supervised learning tasks(including binary classification, multi-class classification, and regression)on benchmark data sets. Extensive experiments have demonstrated promisingresults of the proposed methods.
arxiv-600-159 | Stability of matrix factorization for collaborative filtering | http://arxiv.org/pdf/1206.4640v1.pdf | author:Yu-Xiang Wang, Huan Xu category:cs.NA cs.LG stat.ML published:2012-06-18 summary:We study the stability vis a vis adversarial noise of matrix factorizationalgorithm for matrix completion. In particular, our results include: (I) webound the gap between the solution matrix of the factorization method and theground truth in terms of root mean square error; (II) we treat the matrixfactorization as a subspace fitting problem and analyze the difference betweenthe solution subspace and the ground truth; (III) we analyze the predictionerror of individual users based on the subspace stability. We apply theseresults to the problem of collaborative filtering under manipulator attack,which leads to useful insights and guidelines for collaborative filteringsystem design.
arxiv-600-160 | Adaptive Regularization for Weight Matrices | http://arxiv.org/pdf/1206.4639v1.pdf | author:Koby Crammer, Gal Chechik category:cs.LG cs.AI published:2012-06-18 summary:Algorithms for learning distributions over weight-vectors, such as AROW wererecently shown empirically to achieve state-of-the-art performance at variousproblems, with strong theoretical guaranties. Extending these algorithms tomatrix models pose challenges since the number of free parameters in thecovariance of the distribution scales as $n^4$ with the dimension $n$ of thematrix, and $n$ tends to be large in real applications. We describe, analyzeand experiment with two new algorithms for learning distribution of matrixmodels. Our first algorithm maintains a diagonal covariance over the parametersand can handle large covariance matrices. The second algorithm factors thecovariance to capture inter-features correlation while keeping the number ofparameters linear in the size of the original matrix. We analyze bothalgorithms in the mistake bound model and show a superior precision performanceof our approach over other algorithms in two tasks: retrieving similar images,and ranking similar documents. The factored algorithm is shown to attain fasterconvergence rate.
arxiv-600-161 | Efficient Euclidean Projections onto the Intersection of Norm Balls | http://arxiv.org/pdf/1206.4638v1.pdf | author:Adams Wei Yu, Hao Su, Li Fei-Fei category:cs.LG stat.ML published:2012-06-18 summary:Using sparse-inducing norms to learn robust models has received increasingattention from many fields for its attractive properties. Projection-basedmethods have been widely applied to learning tasks constrained by such norms.As a key building block of these methods, an efficient operator for Euclideanprojection onto the intersection of $\ell_1$ and $\ell_{1,q}$ norm balls$(q=2\text{or}\infty)$ is proposed in this paper. We prove that the projectioncan be reduced to finding the root of an auxiliary function which is piecewisesmooth and monotonic. Hence, a bisection algorithm is sufficient to solve theproblem. We show that the time complexity of our solution is $O(n+g\log g)$ for$q=2$ and $O(n\log n)$ for $q=\infty$, where $n$ is the dimensionality of thevector to be projected and $g$ is the number of disjoint groups; we confirmthis complexity by experimentation. Empirical study reveals that our methodachieves significantly better performance than classical methods in terms ofrunning time and memory usage. We further show that embedded with our efficientprojection operator, projection-based algorithms can solve regression problemswith composite norm constraints more efficiently than other methods and givesuperior accuracy.
arxiv-600-162 | Learning to Identify Regular Expressions that Describe Email Campaigns | http://arxiv.org/pdf/1206.4637v1.pdf | author:Paul Prasse, Christoph Sawade, Niels Landwehr, Tobias Scheffer category:cs.LG cs.CL stat.ML published:2012-06-18 summary:This paper addresses the problem of inferring a regular expression from agiven set of strings that resembles, as closely as possible, the regularexpression that a human expert would have written to identify the language.This is motivated by our goal of automating the task of postmasters of an emailservice who use regular expressions to describe and blacklist email spamcampaigns. Training data contains batches of messages and corresponding regularexpressions that an expert postmaster feels confident to blacklist. We modelthis task as a learning problem with structured output spaces and anappropriate loss function, derive a decoder and the resulting optimizationproblem, and a report on a case study conducted with an email service.
arxiv-600-163 | Modeling Latent Variable Uncertainty for Loss-based Learning | http://arxiv.org/pdf/1206.4636v1.pdf | author:M. Pawan Kumar, Ben Packer, Daphne Koller category:cs.LG cs.AI cs.CV published:2012-06-18 summary:We consider the problem of parameter estimation using weakly superviseddatasets, where a training sample consists of the input and a partiallyspecified annotation, which we refer to as the output. The missing informationin the annotation is modeled using latent variables. Previous methodsoverburden a single distribution with two separate tasks: (i) modeling theuncertainty in the latent variables during training; and (ii) making accuratepredictions for the output and the latent variables during testing. We proposea novel framework that separates the demands of the two tasks using twodistributions: (i) a conditional distribution to model the uncertainty of thelatent variables for a given input-output pair; and (ii) a delta distributionto predict the output and the latent variables for a given input. Duringlearning, we encourage agreement between the two distributions by minimizing aloss-based dissimilarity coefficient. Our approach generalizes latent SVM intwo important ways: (i) it models the uncertainty over latent variables insteadof relying on a pointwise estimate; and (ii) it allows the use of lossfunctions that depend on latent variables, which greatly increases itsapplicability. We demonstrate the efficacy of our approach on two challengingproblems---object detection and action detection---using publicly availabledatasets.
arxiv-600-164 | Deep Mixtures of Factor Analysers | http://arxiv.org/pdf/1206.4635v1.pdf | author:Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton category:cs.LG stat.ML published:2012-06-18 summary:An efficient way to learn deep density models that have many layers of latentvariables is to learn one layer at a time using a model that has only one layerof latent variables. After learning each layer, samples from the posteriordistributions for that layer are used as training data for learning the nextlayer. This approach is commonly used with Restricted Boltzmann Machines, whichare undirected graphical models with a single hidden layer, but it can also beused with Mixtures of Factor Analysers (MFAs) which are directed graphicalmodels. In this paper, we present a greedy layer-wise learning algorithm forDeep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be convertedto an equivalent shallow MFA by multiplying together the factor loadingmatrices at different levels, learning and inference are much more efficient ina DMFA and the sharing of each lower-level factor loading matrix by manydifferent higher level MFAs prevents overfitting. We demonstrate empiricallythat DMFAs learn better density models than both MFAs and two types ofRestricted Boltzmann Machine on a wide variety of datasets.
arxiv-600-165 | Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting | http://arxiv.org/pdf/1206.4634v1.pdf | author:Ning Xie, Hirotaka Hachiya, Masashi Sugiyama category:cs.LG cs.GR stat.ML published:2012-06-18 summary:Oriental ink painting, called Sumi-e, is one of the most appealing paintingstyles that has attracted artists around the world. Major challenges incomputer-based Sumi-e simulation are to abstract complex scene information anddraw smooth and natural brush strokes. To automatically find such strokes, wepropose to model the brush as a reinforcement learning agent, and learn desiredbrush-trajectories by maximizing the sum of rewards in the policy searchframework. We also provide elaborate design of actions, states, and rewardstailored for a Sumi-e agent. The effectiveness of our proposed approach isdemonstrated through simulated Sumi-e experiments.
arxiv-600-166 | Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel-Based Online Learning | http://arxiv.org/pdf/1206.4633v1.pdf | author:Peilin Zhao, Jialei Wang, Pengcheng Wu, Rong Jin, Steven C. H. Hoi category:cs.LG stat.ML published:2012-06-18 summary:Kernel-based online learning has often shown state-of-the-art performance formany online learning tasks. It, however, suffers from a major shortcoming, thatis, the unbounded number of support vectors, making it non-scalable andunsuitable for applications with large-scale datasets. In this work, we studythe problem of bounded kernel-based online learning that aims to constrain thenumber of support vectors by a predefined budget. Although several algorithmshave been proposed in literature, they are neither computationally efficientdue to their intensive budget maintenance strategy nor effective due to the useof simple Perceptron algorithm. To overcome these limitations, we propose aframework for bounded kernel-based online learning based on an online gradientdescent approach. We propose two efficient algorithms of bounded onlinegradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD bymaintaining support vectors using uniform sampling, and (ii) BOGD++ bymaintaining support vectors using non-uniform sampling. We present theoreticalanalysis of regret bound for both algorithms, and found promising empiricalperformance in terms of both efficacy and efficiency by comparing them toseveral well-known algorithms for bounded kernel-based online learning onlarge-scale datasets.
arxiv-600-167 | A Complete Analysis of the l_1,p Group-Lasso | http://arxiv.org/pdf/1206.4632v1.pdf | author:Julia Vogt, Volker Roth category:cs.LG math.OC stat.ML published:2012-06-18 summary:The Group-Lasso is a well-known tool for joint regularization in machinelearning methods. While the l_{1,2} and the l_{1,\infty} version have beenstudied in detail and efficient algorithms exist, there are still openquestions regarding other l_{1,p} variants. We characterize conditions forsolutions of the l_{1,p} Group-Lasso for all p-norms with 1 <= p <= \infty, andwe present a unified active set algorithm. For all p-norms, a highly efficientprojected gradient algorithm is presented. This new algorithm enables us tocompare the prediction performance of many variants of the Group-Lasso in amulti-task learning setting, where the aim is to solve many learning problemsin parallel which are coupled via the Group-Lasso constraint. We conductlarge-scale experiments on synthetic data and on two real-world data sets. Inaccordance with theoretical characterizations of the different norms we observethat the weak-coupling norms with p between 1.5 and 2 consistently outperformthe strong-coupling norms with p >> 2.
arxiv-600-168 | Residual Component Analysis: Generalising PCA for more flexible inference in linear-Gaussian models | http://arxiv.org/pdf/1206.4560v1.pdf | author:Alfredo Kalaitzis, Neil Lawrence category:cs.LG stat.ML published:2012-06-18 summary:Probabilistic principal component analysis (PPCA) seeks a low dimensionalrepresentation of a data set in the presence of independent spherical Gaussiannoise. The maximum likelihood solution for the model is an eigenvalue problemon the sample covariance matrix. In this paper we consider the situation wherethe data variance is already partially explained by other actors, for examplesparse conditional dependencies between the covariates, or temporalcorrelations leaving some residual variance. We decompose the residual varianceinto its components through a generalised eigenvalue problem, which we callresidual component analysis (RCA). We explore a range of new algorithms thatarise from the framework, including one that factorises the covariance of aGaussian density into a low-rank and a sparse-inverse component. We illustratethe ideas on the recovery of a protein-signaling network, a gene expressiontime-series data set and the recovery of the human skeleton from motion capture3-D cloud data.
arxiv-600-169 | Efficient Decomposed Learning for Structured Prediction | http://arxiv.org/pdf/1206.4630v1.pdf | author:Rajhans Samdani, Dan Roth category:cs.LG published:2012-06-18 summary:Structured prediction is the cornerstone of several machine learningapplications. Unfortunately, in structured prediction settings with expressiveinter-variable interactions, exact inference-based learning algorithms, e.g.Structural SVM, are often intractable. We present a new way, DecomposedLearning (DecL), which performs efficient learning by restricting the inferencestep to a limited part of the structured spaces. We provide characterizationsbased on the structure, target parameters, and gold labels, under which DecL isequivalent to exact learning. We then show that in real world settings, whereour theoretical assumptions may not completely hold, DecL-based algorithms aresignificantly more efficient and as accurate as exact learning.
arxiv-600-170 | Multiple Kernel Learning from Noisy Labels by Stochastic Programming | http://arxiv.org/pdf/1206.4629v1.pdf | author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Lijun Zhang, Yang Zhou category:cs.LG published:2012-06-18 summary:We study the problem of multiple kernel learning from noisy labels. This isin contrast to most of the previous studies on multiple kernel learning thatmainly focus on developing efficient algorithms and assume perfectly labeledtraining examples. Directly applying the existing multiple kernel learningalgorithms to noisily labeled examples often leads to suboptimal performancedue to the incorrect class assignments. We address this challenge by castingmultiple kernel learning from noisy labels into a stochastic programmingproblem, and presenting a minimax formulation. We develop an efficientalgorithm for solving the related convex-concave optimization problem with afast convergence rate of $O(1/T)$ where $T$ is the number of iterations.Empirical studies on UCI data sets verify both the effectiveness of theproposed framework and the efficiency of the proposed optimization algorithm.
arxiv-600-171 | Robust PCA in High-dimension: A Deterministic Approach | http://arxiv.org/pdf/1206.4628v1.pdf | author:Jiashi Feng, Huan Xu, Shuicheng Yan category:cs.LG stat.ML published:2012-06-18 summary:We consider principal component analysis for contaminated data-set in thehigh dimensional regime, where the dimensionality of each observation iscomparable or even more than the number of observations. We propose adeterministic high-dimensional robust PCA algorithm which inherits alltheoretical properties of its randomized counterpart, i.e., it is tractable,robust to contaminated points, easily kernelizable, asymptotic consistent andachieves maximal robustness -- a breakdown point of 50%. More importantly, theproposed method exhibits significantly better computational efficiency, whichmakes it suitable for large-scale real applications.
arxiv-600-172 | Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models | http://arxiv.org/pdf/1206.4627v1.pdf | author:Jean Honorio category:cs.LG stat.ML published:2012-06-18 summary:We study the convergence rate of stochastic optimization of exact (NP-hard)objectives, for which only biased estimates of the gradient are available. Wemotivate this problem in the context of learning the structure and parametersof Ising models. We first provide a convergence-rate analysis of deterministicerrors for forward-backward splitting (FBS). We then extend our analysis tobiased stochastic errors, by first characterizing a family of samplers andproviding a high probability bound that allows understanding not only FBS, butalso proximal gradient (PG) methods. We derive some interesting conclusions:FBS requires only a logarithmically increasing number of random samples inorder to converge (although at a very low rate); the required number of randomsamples is the same for the deterministic and the biased stochastic setting forFBS and basic PG; accelerated PG is not guaranteed to converge in the biasedstochastic setting.
arxiv-600-173 | On-Line Portfolio Selection with Moving Average Reversion | http://arxiv.org/pdf/1206.4626v1.pdf | author:Bin Li, Steven C. H. Hoi category:cs.CE cs.LG q-fin.PM published:2012-06-18 summary:On-line portfolio selection has attracted increasing interests in machinelearning and AI communities recently. Empirical evidences show that stock'shigh and low prices are temporary and stock price relatives are likely tofollow the mean reversion phenomenon. While the existing mean reversionstrategies are shown to achieve good empirical performance on many realdatasets, they often make the single-period mean reversion assumption, which isnot always satisfied in some real datasets, leading to poor performance whenthe assumption does not hold. To overcome the limitation, this article proposesa multiple-period mean reversion, or so-called Moving Average Reversion (MAR),and a new on-line portfolio selection strategy named "On-Line Moving AverageReversion" (OLMAR), which exploits MAR by applying powerful online learningtechniques. From our empirical results, we found that OLMAR can overcome thedrawback of existing mean reversion algorithms and achieve significantly betterresults, especially on the datasets where the existing mean reversionalgorithms failed. In addition to superior trading performance, OLMAR also runsextremely fast, further supporting its practical applicability to a wide rangeof applications.
arxiv-600-174 | Optimizing F-measure: A Tale of Two Approaches | http://arxiv.org/pdf/1206.4625v1.pdf | author:Ye Nan, Kian Ming Chai, Wee Sun Lee, Hai Leong Chieu category:cs.LG published:2012-06-18 summary:F-measures are popular performance metrics, particularly for tasks withimbalanced data sets. Algorithms for learning to maximize F-measures follow twoapproaches: the empirical utility maximization (EUM) approach learns aclassifier having optimal performance on training data, while thedecision-theoretic approach learns a probabilistic model and then predictslabels with maximum expected F-measure. In this paper, we investigate thetheoretical justifications and connections for these two approaches, and westudy the conditions under which one approach is preferable to the other usingsynthetic and real datasets. Given accurate models, our results suggest thatthe two approaches are asymptotically equivalent given large training and testsets. Nevertheless, empirically, the EUM approach appears to be more robustagainst model misspecification, and given a good model, the decision-theoreticapproach appears to be better for handling rare classes and a common domainadaptation scenario.
arxiv-600-175 | Robust Multiple Manifolds Structure Learning | http://arxiv.org/pdf/1206.4624v1.pdf | author:Dian Gong, Xuemei Zhao, Gerard Medioni category:cs.LG stat.ML published:2012-06-18 summary:We present a robust multiple manifolds structure learning (RMMSL) scheme torobustly estimate data structures under the multiple low intrinsic dimensionalmanifolds assumption. In the local learning stage, RMMSL efficiently estimateslocal tangent space by weighted low-rank matrix factorization. In the globallearning stage, we propose a robust manifold clustering method based on localstructure learning results. The proposed clustering method is designed to getthe flattest manifolds clusters by introducing a novel curved-level similarityfunction. Our approach is evaluated and compared to state-of-the-art methods onsynthetic data, handwritten digit images, human motion capture data andmotorbike videos. We demonstrate the effectiveness of the proposed approach,which yields higher clustering accuracy, and produces promising results forchallenging tasks of human motion segmentation and motion flow learning fromvideos.
arxiv-600-176 | On the Size of the Online Kernel Sparsification Dictionary | http://arxiv.org/pdf/1206.4623v1.pdf | author:Yi Sun, Faustino Gomez, Juergen Schmidhuber category:cs.LG stat.ML published:2012-06-18 summary:We analyze the size of the dictionary constructed from online kernelsparsification, using a novel formula that expresses the expected determinantof the kernel Gram matrix in terms of the eigenvalues of the covarianceoperator. Using this formula, we are able to connect the cardinality of thedictionary with the eigen-decay of the covariance operator. In particular, weshow that under certain technical conditions, the size of the dictionary willalways grow sub-linearly in the number of data points, and, as a consequence,the kernel linear regressor constructed from the resulting dictionary isconsistent.
arxiv-600-177 | A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training | http://arxiv.org/pdf/1206.4622v1.pdf | author:Aaron Defazio, Tiberio Caetano category:cs.LG cs.IR stat.ML published:2012-06-18 summary:Item neighbourhood methods for collaborative filtering learn a weighted graphover the set of items, where each item is connected to those it is most similarto. The prediction of a user's rating on an item is then given by that ratingof neighbouring items, weighted by their similarity. This paper presents a newneighbourhood approach which we call item fields, whereby an undirectedgraphical model is formed over the item graph. The resulting prediction rule isa simple generalization of the classical approaches, which takes into accountnon-local information in the graph, allowing its best results to be obtainedwhen using drastically fewer edges than other neighbourhood approaches. A fastapproximate maximum entropy training method based on the Bethe approximation ispresented, which uses a simple gradient ascent procedure. When usingprecomputed sufficient statistics on the Movielens datasets, our method isfaster than maximum likelihood approaches by two orders of magnitude.
arxiv-600-178 | Path Integral Policy Improvement with Covariance Matrix Adaptation | http://arxiv.org/pdf/1206.4621v1.pdf | author:Freek Stulp, Olivier Sigaud category:cs.LG published:2012-06-18 summary:There has been a recent focus in reinforcement learning on addressingcontinuous state and action problems by optimizing parameterized policies. PI2is a recent example of this approach. It combines a derivation from firstprinciples of stochastic optimal control with tools from statistical estimationtheory. In this paper, we consider PI2 as a member of the wider family ofmethods which share the concept of probability-weighted averaging toiteratively update parameters to optimize a cost function. We compare PI2 toother members of the same family - Cross-Entropy Methods and CMAES - at theconceptual level and in terms of performance. The comparison suggests thederivation of a novel algorithm which we call PI2-CMA for "Path Integral PolicyImprovement with Covariance Matrix Adaptation". PI2-CMA's main advantage isthat it determines the magnitude of the exploration noise automatically.
arxiv-600-179 | Improved Information Gain Estimates for Decision Tree Induction | http://arxiv.org/pdf/1206.4620v1.pdf | author:Sebastian Nowozin category:cs.LG stat.ML published:2012-06-18 summary:Ensembles of classification and regression trees remain popular machinelearning methods because they define flexible non-parametric models thatpredict well and are computationally efficient both during training andtesting. During induction of decision trees one aims to find predicates thatare maximally informative about the prediction target. To select goodpredicates most approaches estimate an information-theoretic scoring function,the information gain, both for classification and regression problems. We pointout that the common estimation procedures are biased and show that by replacingthem with improved estimators of the discrete and the differential entropy wecan obtain better decision trees. In effect our modifications yield improvedpredictive performance and are simple to implement in any decision tree code.
arxiv-600-180 | Inductive Kernel Low-rank Decomposition with Priors: A Generalized Nystrom Method | http://arxiv.org/pdf/1206.4619v1.pdf | author:Kai Zhang, Liang Lan, Jun Liu, andreas Rauber, Fabian Moerchen category:cs.LG published:2012-06-18 summary:Low-rank matrix decomposition has gained great popularity recently in scalingup kernel methods to large amounts of data. However, some limitations couldprevent them from working effectively in certain domains. For example, manyexisting approaches are intrinsically unsupervised, which does not incorporateside information (e.g., class labels) to produce task specific decompositions;also, they typically work "transductively", i.e., the factorization does notgeneralize to new samples, so the complete factorization needs to be recomputedwhen new samples become available. To solve these problems, in this paper wepropose an"inductive"-flavored method for low-rank kernel decomposition withpriors. We achieve this by generalizing the Nystr\"om method in a novel way. Onthe one hand, our approach employs a highly flexible, nonparametric structurethat allows us to generalize the low-rank factors to arbitrarily new samples;on the other hand, it has linear time and space complexities, which can beorders of magnitudes faster than existing approaches and renders greatefficiency in learning a low-rank kernel decomposition. Empirical resultsdemonstrate the efficacy and efficiency of the proposed method.
arxiv-600-181 | Compact Hyperplane Hashing with Bilinear Functions | http://arxiv.org/pdf/1206.4618v1.pdf | author:Wei Liu, Jun Wang, Yadong Mu, Sanjiv Kumar, Shih-Fu Chang category:cs.LG stat.ML published:2012-06-18 summary:Hyperplane hashing aims at rapidly searching nearest points to a hyperplane,and has shown practical impact in scaling up active learning with SVMs.Unfortunately, the existing randomized methods need long hash codes to achievereasonable search accuracy and thus suffer from reduced search speed and largememory overhead. To this end, this paper proposes a novel hyperplane hashingtechnique which yields compact hash codes. The key idea is the bilinear form ofthe proposed hash functions, which leads to higher collision probability thanthe existing hyperplane hash functions when using random projections. Tofurther increase the performance, we propose a learning based framework inwhich the bilinear functions are directly learned from the data. This resultsin short yet discriminative codes, and also boosts the search performance overthe random projection based solutions. Large-scale active learning experimentscarried out on two datasets with up to one million samples demonstrate theoverall superiority of the proposed approach.
arxiv-600-182 | Continuous Inverse Optimal Control with Locally Optimal Examples | http://arxiv.org/pdf/1206.4617v1.pdf | author:Sergey Levine, Vladlen Koltun category:cs.LG cs.AI stat.ML published:2012-06-18 summary:Inverse optimal control, also known as inverse reinforcement learning, is theproblem of recovering an unknown reward function in a Markov decision processfrom expert demonstrations of the optimal policy. We introduce a probabilisticinverse optimal control algorithm that scales gracefully with taskdimensionality, and is suitable for large, continuous domains where evencomputing a full policy is impractical. By using a local approximation of thereward function, our method can also drop the assumption that thedemonstrations are globally optimal, requiring only local optimality. Thisallows it to learn from examples that are unsuitable for prior methods.
arxiv-600-183 | A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling | http://arxiv.org/pdf/1206.4616v1.pdf | author:Drausin Wulsin, Shane Jensen, Brian Litt category:stat.AP cs.LG stat.ML published:2012-06-18 summary:Driven by the multi-level structure of human intracranialelectroencephalogram (iEEG) recordings of epileptic seizures, we introduce anew variant of a hierarchical Dirichlet Process---the multi-level clusteringhierarchical Dirichlet Process (MLC-HDP)---that simultaneously clustersdatasets on multiple levels. Our seizure dataset contains brain activityrecorded in typically more than a hundred individual channels for each seizureof each patient. The MLC-HDP model clusters over channels-types, seizure-types,and patient-types simultaneously. We describe this model and its implementationin detail. We also present the results of a simulation study comparing theMLC-HDP to a similar model, the Nested Dirichlet Process and finallydemonstrate the MLC-HDP's use in modeling seizures across multiple patients. Wefind the MLC-HDP's clustering to be comparable to independent human physicianclusterings. To our knowledge, the MLC-HDP model is the first in the epilepsyliterature capable of clustering seizures within and between patients.
arxiv-600-184 | Levy Measure Decompositions for the Beta and Gamma Processes | http://arxiv.org/pdf/1206.4615v1.pdf | author:Yingjian Wang, Lawrence Carin category:stat.ME cs.LG math.ST stat.TH published:2012-06-18 summary:We develop new representations for the Levy measures of the beta and gammaprocesses. These representations are manifested in terms of an infinite sum ofwell-behaved (proper) beta and gamma distributions. Further, we demonstrate howthese infinite sums may be truncated in practice, and explicitly characterizetruncation errors. We also perform an analysis of the characteristics ofposterior distributions, based on the proposed decompositions. Thedecompositions provide new insights into the beta and gamma processes (andtheir generalizations), and we demonstrate how the proposed representationunifies some properties of the two. This paper is meant to provide a rigorousfoundation for and new perspectives on Levy processes, as these are ofincreasing importance in machine learning.
arxiv-600-185 | Information-theoretic Semi-supervised Metric Learning via Entropy Regularization | http://arxiv.org/pdf/1206.4614v1.pdf | author:Gang Niu, Bo Dai, Makoto Yamada, Masashi Sugiyama category:cs.LG stat.ML published:2012-06-18 summary:We propose a general information-theoretic approach called Seraph(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metriclearning that does not rely upon the manifold assumption. Given the probabilityparameterized by a Mahalanobis distance, we maximize the entropy of thatprobability on labeled data and minimize it on unlabeled data following entropyregularization, which allows the supervised and unsupervised parts to beintegrated in a natural and meaningful way. Furthermore, Seraph is regularizedby encouraging a low-rank projection induced from the metric. The optimizationof Seraph is solved efficiently and stably by an EM-like scheme with theanalytical E-Step and convex M-Step. Experiments demonstrate that Seraphcompares favorably with many well-known global and local metric learningmethods.
arxiv-600-186 | Near-Optimal BRL using Optimistic Local Transitions | http://arxiv.org/pdf/1206.4613v1.pdf | author:Mauricio Araya, Olivier Buffet, Vincent Thomas category:cs.AI cs.LG stat.ML published:2012-06-18 summary:Model-based Bayesian Reinforcement Learning (BRL) allows a foundformalization of the problem of acting optimally while facing an unknownenvironment, i.e., avoiding the exploration-exploitation dilemma. However,algorithms explicitly addressing BRL suffer from such a combinatorial explosionthat a large body of work relies on heuristic algorithms. This paper introducesBOLT, a simple and (almost) deterministic heuristic algorithm for BRL which isoptimistic about the transition function. We analyze BOLT's sample complexity,and show that under certain parameters, the algorithm is near-optimal in theBayesian sense with high probability. Then, experimental results highlight thekey differences of this method compared to previous work.
arxiv-600-187 | Exact Soft Confidence-Weighted Learning | http://arxiv.org/pdf/1206.4612v1.pdf | author:Jialei Wang, Peilin Zhao, Steven C. H. Hoi category:cs.LG published:2012-06-18 summary:In this paper, we propose a new Soft Confidence-Weighted (SCW) onlinelearning scheme, which enables the conventional confidence-weighted learningmethod to handle non-separable cases. Unlike the previous confidence-weightedlearning algorithms, the proposed soft confidence-weighted learning methodenjoys all the four salient properties: (i) large margin training, (ii)confidence weighting, (iii) capability to handle non-separable data, and (iv)adaptive margin. Our experimental results show that the proposed SCW algorithmssignificantly outperform the original CW algorithm. When comparing with avariety of state-of-the-art algorithms (including AROW, NAROW and NHERD), wefound that SCW generally achieves better or at least comparable predictiveaccuracy, but enjoys significant advantage of computational efficiency (i.e.,smaller number of updates and lower time cost).
arxiv-600-188 | A Convex Feature Learning Formulation for Latent Task Structure Discovery | http://arxiv.org/pdf/1206.4611v1.pdf | author:Pratik Jawanpuria, J. Saketha Nath category:cs.LG stat.ML published:2012-06-18 summary:This paper considers the multi-task learning problem and in the setting wheresome relevant features could be shared across few related tasks. Most of theexisting methods assume the extent to which the given tasks are related orshare a common feature space to be known apriori. In real-world applicationshowever, it is desirable to automatically discover the groups of related tasksthat share a feature space. In this paper we aim at searching the exponentiallylarge space of all possible groups of tasks that may share a feature space. Themain contribution is a convex formulation that employs a graph-basedregularizer and simultaneously discovers few groups of related tasks, havingclose-by task parameters, as well as the feature space shared within eachgroup. The regularizer encodes an important structure among the groups of tasksleading to an efficient algorithm for solving it: if there is no feature spaceunder which a group of tasks has close-by task parameters, then there does notexist such a feature space for any of its supersets. An efficient active setalgorithm that exploits this simplification and performs a clever search in theexponentially large space is presented. The algorithm is guaranteed to solvethe proposed formulation (within some precision) in a time polynomial in thenumber of groups of related tasks discovered. Empirical results on benchmarkdatasets show that the proposed formulation achieves good generalization andoutperforms state-of-the-art multi-task learning algorithms in some cases.
arxiv-600-189 | Manifold Relevance Determination | http://arxiv.org/pdf/1206.4610v1.pdf | author:Andreas Damianou, Carl Ek, Michalis Titsias, Neil Lawrence category:cs.LG cs.CV stat.ML published:2012-06-18 summary:In this paper we present a fully Bayesian latent variable model whichexploits conditional nonlinear(in)-dependence structures to learn an efficientlatent representation. The latent space is factorized to represent shared andprivate information from multiple views of the data. In contrast to previousapproaches, we introduce a relaxation to the discrete segmentation and allowfor a "softly" shared latent space. Further, Bayesian techniques allow us toautomatically estimate the dimensionality of the latent spaces. The model iscapable of capturing structure underlying extremely high dimensional spaces.This is illustrated by modelling unprocessed images with tenths of thousands ofpixels. This also allows us to directly generate novel images from the trainedmodel by sampling from the discovered latent spaces. We also demonstrate themodel by prediction of human pose in an ambiguous setting. Our Bayesianframework allows us to perform disambiguation in a principled manner byincluding latent space priors which incorporate the dynamic nature of the data.
arxiv-600-190 | On multi-view feature learning | http://arxiv.org/pdf/1206.4609v1.pdf | author:Roland Memisevic category:cs.CV cs.LG stat.ML published:2012-06-18 summary:Sparse coding is a common approach to learning local features for objectrecognition. Recently, there has been an increasing interest in learningfeatures from spatio-temporal, binocular, or other multi-observation data,where the goal is to encode the relationship between images rather than thecontent of a single image. We provide an analysis of multi-view featurelearning, which shows that hidden variables encode transformations by detectingrotation angles in the eigenspaces shared among multiple image warps. Ouranalysis helps explain recent experimental results showing thattransformation-specific features emerge when training complex cell models onvideos. Our analysis also shows that transformation-invariant features canemerge as a by-product of learning representations of transformations.
arxiv-600-191 | A Hybrid Algorithm for Convex Semidefinite Optimization | http://arxiv.org/pdf/1206.4608v1.pdf | author:Soeren Laue category:cs.LG cs.DS cs.NA stat.ML published:2012-06-18 summary:We present a hybrid algorithm for optimizing a convex, smooth function overthe cone of positive semidefinite matrices. Our algorithm converges to theglobal optimal solution and can be used to solve general large-scalesemidefinite programs and hence can be readily applied to a variety of machinelearning problems. We show experimental results on three machine learningproblems (matrix completion, metric learning, and sparse PCA) . Our approachoutperforms state-of-the-art algorithms.
arxiv-600-192 | Distributed Tree Kernels | http://arxiv.org/pdf/1206.4607v1.pdf | author:Fabio Massimo Zanzotto, Lorenzo Dell'Arciprete category:cs.LG stat.ML published:2012-06-18 summary:In this paper, we propose the distributed tree kernels (DTK) as a novelmethod to reduce time and space complexity of tree kernels. Using a linearcomplexity algorithm to compute vectors for trees, we embed feature spaces oftree fragments in low-dimensional spaces where the kernel computation isdirectly done with dot product. We show that DTKs are faster, correlate withtree kernels, and obtain a statistically similar performance in two naturallanguage processing tasks.
arxiv-600-193 | TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings | http://arxiv.org/pdf/1206.4606v1.pdf | author:Chao Liu, Yi-Min Wang category:cs.LG cs.AI stat.ML published:2012-06-18 summary:This paper revisits the problem of analyzing multiple ratings given bydifferent judges. Different from previous work that focuses on distilling thetrue labels from noisy crowdsourcing ratings, we emphasize gaining diagnosticinsights into our in-house well-trained judges. We generalize the well-knownDawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic modelsunder the same "TrueLabel + Confusion" paradigm, and show that our proposedhierarchical Bayesian model, called HybridConfusion, consistently outperformsDawidSkene on both synthetic and real-world data sets.
arxiv-600-194 | Learning the Experts for Online Sequence Prediction | http://arxiv.org/pdf/1206.4604v1.pdf | author:Elad Eban, Aharon Birnbaum, Shai Shalev-Shwartz, Amir Globerson category:cs.LG cs.AI published:2012-06-18 summary:Online sequence prediction is the problem of predicting the next element of asequence given previous elements. This problem has been extensively studied inthe context of individual sequence prediction, where no prior assumptions aremade on the origin of the sequence. Individual sequence prediction algorithmswork quite well for long sequences, where the algorithm has enough time tolearn the temporal structure of the sequence. However, they might give poorpredictions for short sequences. A possible remedy is to rely on the generalmodel of prediction with expert advice, where the learner has access to a setof $r$ experts, each of which makes its own predictions on the sequence. It iswell known that it is possible to predict almost as well as the best expert ifthe sequence length is order of $\log(r)$. But, without firm prior knowledge onthe problem, it is not clear how to choose a small set of {\em good} experts.In this paper we describe and analyze a new algorithm that learns a good set ofexperts using a training set of previously observed sequences. We demonstratethe merits of our approach by applying it on the task of click prediction onthe web.
arxiv-600-195 | Quasi-Newton Methods: A New Direction | http://arxiv.org/pdf/1206.4602v1.pdf | author:Philipp Hennig, Martin Kiefel category:cs.NA cs.LG stat.ML published:2012-06-18 summary:Four decades after their invention, quasi-Newton methods are still state ofthe art in unconstrained numerical optimization. Although not usuallyinterpreted thus, these are learning algorithms that fit a local quadraticapproximation to the objective function. We show that many, including the mostpopular, quasi-Newton methods can be interpreted as approximations of Bayesianlinear regression under varying prior assumptions. This new notion elucidatessome shortcomings of classical algorithms, and lights the way to a novelnonparametric quasi-Newton method, which is able to make more efficient use ofavailable information at computational cost similar to its predecessors.
arxiv-600-196 | Convex Multitask Learning with Flexible Task Clusters | http://arxiv.org/pdf/1206.4601v1.pdf | author:Wenliang Zhong, James Kwok category:cs.LG stat.ML published:2012-06-18 summary:Traditionally, multitask learning (MTL) assumes that all the tasks arerelated. This can lead to negative transfer when tasks are indeed incoherent.Recently, a number of approaches have been proposed that alleviate this problemby discovering the underlying task clusters or relationships. However, they arelimited to modeling these relationships at the task level, which may berestrictive in some applications. In this paper, we propose a novel MTLformulation that captures task relationships at the feature-level. Depending onthe interactions among tasks and features, the proposed method constructdifferent task clusters for different features, without even the need ofpre-specifying the number of clusters. Computationally, the proposedformulation is strongly convex, and can be efficiently solved by acceleratedproximal methods. Experiments are performed on a number of synthetic andreal-world data sets. Under various degrees of task relationships, the accuracyof the proposed method is consistently among the best. Moreover, thefeature-specific task clusters obtained agree with the known/plausible taskstructures of the data.
arxiv-600-197 | Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes | http://arxiv.org/pdf/1206.4600v1.pdf | author:Murat Dundar, Ferit Akova, Alan Qi, Bartek Rajwa category:cs.LG stat.ML published:2012-06-18 summary:We present a framework for online inference in the presence of anonexhaustively defined set of classes that incorporates supervisedclassification with class discovery and modeling. A Dirichlet process prior(DPP) model defined over class distributions ensures that both known andunknown class distributions originate according to a common base distribution.In an attempt to automatically discover potentially interesting classformations, the prior model is coupled with a suitably chosen data model, andsequential Monte Carlo sampling is used to perform online inference. Ourresearch is driven by a biodetection application, where a new class of pathogenmay suddenly appear, and the rapid increase in the number of samplesoriginating from this class indicates the onset of an outbreak.
arxiv-600-198 | A Unified Robust Classification Model | http://arxiv.org/pdf/1206.4599v1.pdf | author:Akiko Takeda, Hiroyuki Mitsugi, Takafumi Kanamori category:cs.LG stat.ML published:2012-06-18 summary:A wide variety of machine learning algorithms such as support vector machine(SVM), minimax probability machine (MPM), and Fisher discriminant analysis(FDA), exist for binary classification. The purpose of this paper is to providea unified classification model that includes the above models through a robustoptimization approach. This unified model has several benefits. One is that theextensions and improvements intended for SVM become applicable to MPM and FDA,and vice versa. Another benefit is to provide theoretical results to abovelearning methods at once by dealing with the unified model. We give astatistical interpretation of the unified classification model and propose anon-convex optimization algorithm that can be applied to non-convex variants ofexisting learning methods.
arxiv-600-199 | DANCo: Dimensionality from Angle and Norm Concentration | http://arxiv.org/pdf/1206.3881v1.pdf | author:Claudio Ceruti, Simone Bassis, Alessandro Rozza, Gabriele Lombardi, Elena Casiraghi, Paola Campadelli category:cs.LG stat.ML published:2012-06-18 summary:In the last decades the estimation of the intrinsic dimensionality of adataset has gained considerable importance. Despite the great deal of researchwork devoted to this task, most of the proposed solutions prove to beunreliable when the intrinsic dimensionality of the input dataset is high andthe manifold where the points lie is nonlinearly embedded in a higherdimensional space. In this paper we propose a novel robust intrinsicdimensionality estimator that exploits the twofold complementary informationconveyed both by the normalized nearest neighbor distances and by the anglescomputed on couples of neighboring points, providing also closed-forms for theKullback-Leibler divergences of the respective distributions. Experimentsperformed on both synthetic and real datasets highlight the robustness and theeffectiveness of the proposed algorithm when compared to state of the artmethodologies.
arxiv-600-200 | An Analysis of the Methods Employed for Breast Cancer Diagnosis | http://arxiv.org/pdf/1206.3777v1.pdf | author:Mahjabeen Mirza Beg, Monika Jain category:cs.NE q-bio.TO published:2012-06-17 summary:Breast cancer research over the last decade has been tremendous. The groundbreaking innovations and novel methods help in the early detection, in settingthe stages of the therapy and in assessing the response of the patient to thetreatment. The prediction of the recurrent cancer is also crucial for thesurvival of the patient. This paper studies various techniques used for thediagnosis of breast cancer. Different methods are explored for their merits andde-merits for the diagnosis of breast lesion. Some of the methods are yetunproven but the studies look very encouraging. It was found that the recentuse of the combination of Artificial Neural Networks in most of the instancesgives accurate results for the diagnosis of breast cancer and their use canalso be extended to other diseases.
arxiv-600-201 | The Stability of Convergence of Curve Evolutions in Vector Fields | http://arxiv.org/pdf/1206.4042v1.pdf | author:Junyan Wang, Kap Luk Chan category:cs.CV math.AP published:2012-06-17 summary:Curve evolution is often used to solve computer vision problems. If the curveevolution fails to converge, we would not be able to solve the targeted problemin a lifetime. This paper studies the theoretical aspect of the convergence ofa type of general curve evolutions. We establish a theory for analyzing andimproving the stability of the convergence of the general curve evolutions.Based on this theory, we ascertain that the convergence of a known curveevolution is marginal stable. We propose a way of modifying the original curveevolution equation to improve the stability of the convergence according to ourtheory. Numerical experiments show that the modification improves theconvergence of the curve evolution, which validates our theory.
arxiv-600-202 | Constraint-free Graphical Model with Fast Learning Algorithm | http://arxiv.org/pdf/1206.3721v1.pdf | author:Kazuya Takabatake, Shotaro Akaho category:cs.LG stat.ML published:2012-06-17 summary:In this paper, we propose a simple, versatile model for learning thestructure and parameters of multivariate distributions from a data set.Learning a Markov network from a given data set is not a simple problem,because Markov networks rigorously represent Markov properties, and this rigorimposes complex constraints on the design of the networks. Our proposed modelremoves these constraints, acquiring important aspects from the informationgeometry. The proposed parameter- and structure-learning algorithms are simpleto execute as they are based solely on local computation at each node.Experiments demonstrate that our algorithms work appropriately.
arxiv-600-203 | How important are Deformable Parts in the Deformable Parts Model? | http://arxiv.org/pdf/1206.3714v1.pdf | author:Santosh K. Divvala, Alexei A. Efros, Martial Hebert category:cs.CV cs.AI cs.LG published:2012-06-16 summary:The main stated contribution of the Deformable Parts Model (DPM) detector ofFelzenszwalb et al. (over the Histogram-of-Oriented-Gradients approach of Dalaland Triggs) is the use of deformable parts. A secondary contribution is thelatent discriminative learning. Tertiary is the use of multiple components. Acommon belief in the vision community (including ours, before this study) isthat their ordering of contributions reflects the performance of detector inpractice. However, what we have experimentally found is that the ordering ofimportance might actually be the reverse. First, we show that by increasing thenumber of components, and switching the initialization step from theiraspect-ratio, left-right flipping heuristics to appearance-based clustering,considerable improvement in performance is obtained. But more intriguingly, weshow that with these new components, the part deformations can now becompletely switched off, yet obtaining results that are almost on par with theoriginal DPM detector. Finally, we also show initial results for using multiplecomponents on a different problem -- scene classification, suggesting that thisidea might have wider applications in addition to object detection.
arxiv-600-204 | Unsupervised adaptation of brain machine interface decoders | http://arxiv.org/pdf/1206.3666v1.pdf | author:Tayfun Gürel, Carsten Mehring category:cs.LG q-bio.NC published:2012-06-16 summary:The performance of neural decoders can degrade over time due tononstationarities in the relationship between neuronal activity and behavior.In this case, brain-machine interfaces (BMI) require adaptation of theirdecoders to maintain high performance across time. One way to achieve this isby use of periodical calibration phases, during which the BMI system (or anexternal human demonstrator) instructs the user to perform certain movements orbehaviors. This approach has two disadvantages: (i) calibration phasesinterrupt the autonomous operation of the BMI and (ii) between two calibrationphases the BMI performance might not be stable but continuously decrease. Abetter alternative would be that the BMI decoder is able to continuously adaptin an unsupervised manner during autonomous BMI operation, i.e. without knowingthe movement intentions of the user. In the present article, we present an efficient method for such unsupervisedtraining of BMI systems for continuous movement control. The proposed methodutilizes a cost function derived from neuronal recordings, which guides alearning algorithm to evaluate the decoding parameters. We verify theperformance of our adaptive method by simulating a BMI user with an optimalfeedback control model and its interaction with our adaptive BMI decoder. Thesimulation results show that the cost function and the algorithm yield fast andprecise trajectories towards targets at random orientations on a 2-dimensionalcomputer screen. For initially unknown and non-stationary tuning parameters,our unsupervised method is still able to generate precise trajectories and tokeep its performance stable in the long term. The algorithm can optionally workalso with neuronal error signals instead or in conjunction with the proposedunsupervised adaptation.
arxiv-600-205 | Feature Based Fuzzy Rule Base Design for Image Extraction | http://arxiv.org/pdf/1206.3633v1.pdf | author:Koushik Mondal, Paramartha Dutta, Siddhartha Bhattacharyya category:cs.CV cs.AI published:2012-06-16 summary:In the recent advancement of multimedia technologies, it becomes a majorconcern of detecting visual attention regions in the field of image processing.The popularity of the terminal devices in a heterogeneous environment of themultimedia technology gives us enough scope for the betterment of imagevisualization. Although there exist numerous methods, feature based imageextraction becomes a popular one in the field of image processing. Theobjective of image segmentation is the domain-independent partition of theimage into a set of regions, which are visually distinct and uniform withrespect to some property, such as grey level, texture or colour. Segmentationand subsequent extraction can be considered the first step and key issue inobject recognition, scene understanding and image analysis. Its applicationarea encompasses mobile devices, industrial quality control, medicalappliances, robot navigation, geophysical exploration, military applications,etc. In all these areas, the quality of the final results depends largely onthe quality of the preprocessing work. Most of the times, acquiringspurious-free preprocessing data requires a lot of application cum mathematicalintensive background works. We propose a feature based fuzzy rule guided noveltechnique that is functionally devoid of any external intervention duringexecution. Experimental results suggest that this approach is an efficient onein comparison to different other techniques extensively addressed inliterature. In order to justify the supremacy of performance of our proposedtechnique in respect of its competitors, we take recourse to effective metricslike Mean Squared Error (MSE), Mean Absolute Error (MAE) and Peak Signal toNoise Ratio (PSNR).
arxiv-600-206 | Blind PSF estimation and methods of deconvolution optimization | http://arxiv.org/pdf/1206.3594v1.pdf | author:Yu. A. Bunyak, O. Yu. Sofina, R. N. Kvetnyy category:cs.CV published:2012-06-15 summary:We have shown that the left side null space of the autoregression (AR) matrixoperator is the lexicographical presentation of the point spread function (PSF)on condition the AR parameters are common for original and blurred images. Themethod of inverse PSF evaluation with regularization functional as the functionof surface area is offered. The inverse PSF was used for primary imageestimation. Two methods of original image estimate optimization were designedbasing on maximum entropy generalization of sought and blurred imagesconditional probability density and regularization. The first method usesbalanced variations of convolution and deconvolution transforms to obtainingiterative schema of image optimization. The variations balance was defined bydynamic regularization basing on condition of iteration process convergence.The regularization has dynamic character because depends on current andprevious image estimate variations. The second method implements theregularization of deconvolution optimization in curved space with metricdefined on image estimate surface. It is basing on target functional invarianceto fluctuations of optimal argument value. The given iterative schemas havefaster convergence in comparison with known ones, so they can be used forreconstruction of high resolution images series in real time.
arxiv-600-207 | Improved Spectral-Norm Bounds for Clustering | http://arxiv.org/pdf/1206.3204v2.pdf | author:Pranjal Awasthi, Or Sheffet category:cs.LG cs.DS published:2012-06-14 summary:Aiming to unify known results about clustering mixtures of distributionsunder separation conditions, Kumar and Kannan[2010] introduced a deterministiccondition for clustering datasets. They showed that this single deterministiccondition encompasses many previously studied clustering assumptions. Morespecifically, their proximity condition requires that in the target$k$-clustering, the projection of a point $x$ onto the line joining its clustercenter $\mu$ and some other center $\mu'$, is a large additive factor closer to$\mu$ than to $\mu'$. This additive factor can be roughly described as $k$times the spectral norm of the matrix representing the differences between thegiven (known) dataset and the means of the (unknown) target clustering.Clearly, the proximity condition implies center separation -- the distancebetween any two centers must be as large as the above mentioned bound. In this paper we improve upon the work of Kumar and Kannan along severalaxes. First, we weaken the center separation bound by a factor of $\sqrt{k}$,and secondly we weaken the proximity condition by a factor of $k$. Using theseweaker bounds we still achieve the same guarantees when all points satisfy theproximity condition. We also achieve better guarantees when only$(1-\epsilon)$-fraction of the points satisfy the weaker proximity condition.The bulk of our analysis relies only on center separation under which one canproduce a clustering which (i) has low error, (ii) has low $k$-means cost, and(iii) has centers very close to the target centers. Our improved separation condition allows us to match the results of thePlanted Partition Model of McSherry[2001], improve upon the results ofOstrovsky et al[2006], and improve separation results for mixture of Gaussianmodels in a particular setting.
arxiv-600-208 | General Upper Bounds on the Running Time of Parallel Evolutionary Algorithms | http://arxiv.org/pdf/1206.3522v1.pdf | author:Jörg Lässig, Dirk Sudholt category:cs.NE F.2.2 published:2012-06-15 summary:We present a new method for analyzing the running time of parallelevolutionary algorithms with spatially structured populations. Based on thefitness-level method, it yields upper bounds on the expected parallel runningtime. This allows to rigorously estimate the speedup gained by parallelization.Tailored results are given for common migration topologies: ring graphs, torusgraphs, hypercubes, and the complete graph. Example applications forpseudo-Boolean optimization show that our method is easy to apply and that itgives powerful results. In our examples the possible speedup increases with thedensity of the topology. Surprisingly, even sparse topologies like ring graphslead to a significant speedup for many functions while not increasing the totalnumber of function evaluations by more than a constant factor. We also identifywhich number of processors yield asymptotically optimal speedups, thus givinghints on how to parametrize parallel evolutionary algorithms.
arxiv-600-209 | A Novel Approach for Protein Structure Prediction | http://arxiv.org/pdf/1206.3509v1.pdf | author:Saurabh Sarkar, Prateek Malhotra, Virender Guman category:cs.LG q-bio.BM published:2012-06-15 summary:The idea of this project is to study the protein structure and sequencerelationship using the hidden markov model and artificial neural network. Inthis context we have assumed two hidden markov models. In first model we havetaken protein secondary structures as hidden and protein sequences as observed.In second model we have taken protein sequences as hidden and proteinstructures as observed. The efficiencies for both the hidden markov models havebeen calculated. The results show that the efficiencies of first model isgreater that the second one .These efficiencies are cross validated usingartificial neural network. This signifies the importance of protein secondarystructures as the main hidden controlling factors due to which we observe aparticular amino acid sequence. This also signifies that protein secondarystructure is more conserved in comparison to amino acid sequence.
arxiv-600-210 | Functional Currents : a new mathematical tool to model and analyse functional shapes | http://arxiv.org/pdf/1206.3564v1.pdf | author:Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG published:2012-06-15 summary:This paper introduces the concept of functional current as a mathematicalframework to represent and treat functional shapes, i.e. sub-manifold supportedsignals. It is motivated by the growing occurrence, in medical imaging andcomputational anatomy, of what can be described as geometrico-functional data,that is a data structure that involves a deformable shape (roughly a finitedimensional sub manifold) together with a function defined on this shape takingvalue in another manifold. Indeed, if mathematical currents have already proved to be very efficienttheoretically and numerically to model and process shapes as curves orsurfaces, they are limited to the manipulation of purely geometrical objects.We show that the introduction of the concept of functional currents offers agenuine solution to the simultaneous processing of the geometric and signalinformation of any functional shape. We explain how functional currents can beequipped with a Hilbertian norm mixing geometrical and functional content offunctional shapes nicely behaving under geometrical and functionalperturbations and paving the way to various processing algorithms. Weillustrate this potential on two problems: the redundancy reduction offunctional shapes representations through matching pursuit schemes onfunctional currents and the simultaneous geometric and functional registrationof functional shapes under diffeomorphic transport.
arxiv-600-211 | On the Cover-Hart Inequality: What's a Sample of Size One Worth? | http://arxiv.org/pdf/1206.3381v1.pdf | author:Tilmann Gneiting category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-06-15 summary:Bob predicts a future observation based on a sample of size one. Alice candraw a sample of any size before issuing her prediction. How much better canshe do than Bob? Perhaps surprisingly, under a large class of loss functions,which we refer to as the Cover-Hart family, the best Alice can do is to halveBob's risk. In this sense, half the information in an infinite sample iscontained in a sample of size one. The Cover-Hart family is a convex cone thatincludes metrics and negative definite functions, subject to slight regularityconditions. These results may help explain the small relative differences inempirical performance measures in applied classification and forecastingproblems, as well as the success of reasoning and learning by analogy ingeneral, and nearest neighbor techniques in particular.
arxiv-600-212 | Multiple Operator-valued Kernel Learning | http://arxiv.org/pdf/1203.1596v2.pdf | author:Hachem Kadri, Alain Rakotomamonjy, Francis Bach, Philippe Preux category:stat.ML cs.LG published:2012-03-07 summary:Positive definite operator-valued kernels generalize the well-known notion ofreproducing kernels, and are naturally adapted to multi-output learningsituations. This paper addresses the problem of learning a finite linearcombination of infinite-dimensional operator-valued kernels which are suitablefor extending functional data analysis methods to nonlinear contexts. We studythis problem in the case of kernel ridge regression for functional responseswith an lr-norm constraint on the combination coefficients. The resultingoptimization problem is more involved than those of multiple scalar-valuedkernel learning since operator-valued kernels pose more technical andtheoretical issues. We propose a multiple operator-valued kernel learningalgorithm based on solving a system of linear operator equations by using ablock coordinatedescent procedure. We experimentally validate our approach on afunctional regression task in the context of finger movement prediction inbrain-computer interfaces.
arxiv-600-213 | Identifiability and Unmixing of Latent Parse Trees | http://arxiv.org/pdf/1206.3137v1.pdf | author:Daniel Hsu, Sham M. Kakade, Percy Liang category:stat.ML cs.LG published:2012-06-14 summary:This paper explores unsupervised learning of parsing models along twodirections. First, which models are identifiable from infinite data? We use ageneral technique for numerically checking identifiability based on the rank ofa Jacobian matrix, and apply it to several standard constituency and dependencyparsing models. Second, for identifiable models, how do we estimate theparameters efficiently? EM suffers from local optima, while recent work usingspectral methods cannot be directly applied since the topology of the parsetree varies across sentences. We develop a strategy, unmixing, which deals withthis additional complexity for restricted classes of parsing models.
arxiv-600-214 | Revisiting k-means: New Algorithms via Bayesian Nonparametrics | http://arxiv.org/pdf/1111.0352v2.pdf | author:Brian Kulis, Michael I. Jordan category:cs.LG stat.ML published:2011-11-02 summary:Bayesian models offer great flexibility for clusteringapplications---Bayesian nonparametrics can be used for modeling infinitemixtures, and hierarchical Bayesian models can be utilized for sharing clustersacross multiple data sets. For the most part, such flexibility is lacking inclassical clustering methods such as k-means. In this paper, we revisit thek-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspiredby the asymptotic connection between k-means and mixtures of Gaussians, we showthat a Gibbs sampling algorithm for the Dirichlet process mixture approaches ahard clustering algorithm in the limit, and further that the resultingalgorithm monotonically minimizes an elegant underlying k-means-like clusteringobjective that includes a penalty for the number of clusters. We generalizethis analysis to the case of clustering multiple data sets through a similarasymptotic argument with the hierarchical Dirichlet process. We also discussfurther extensions that highlight the benefits of our analysis: i) a spectralrelaxation involving thresholded eigenvectors, and ii) a normalized cut graphclustering algorithm that does not fix the number of clusters in the graph.
arxiv-600-215 | Statistical Consistency of Finite-dimensional Unregularized Linear Classification | http://arxiv.org/pdf/1206.3072v1.pdf | author:Matus Telgarsky category:cs.LG stat.ML published:2012-06-14 summary:This manuscript studies statistical properties of linear classifiers obtainedthrough minimization of an unregularized convex risk over a finite sample.Although the results are explicitly finite-dimensional, inputs may be passedthrough feature maps; in this way, in addition to treating the consistency oflogistic regression, this analysis also handles boosting over a finite weaklearning class with, for instance, the exponential, logistic, and hinge losses.In this finite-dimensional setting, it is still possible to fit arbitrarydecision boundaries: scaling the complexity of the weak learning class with thesample size leads to the optimal classification risk almost surely.
arxiv-600-216 | Decentralized Learning for Multi-player Multi-armed Bandits | http://arxiv.org/pdf/1206.3582v1.pdf | author:Dileep Kalathil, Naumaan Nayyar, Rahul Jain category:math.OC cs.LG cs.SY published:2012-06-14 summary:We consider the problem of distributed online learning with multiple playersin multi-armed bandits (MAB) models. Each player can pick among multiple arms.When a player picks an arm, it gets a reward. We consider both i.i.d. rewardmodel and Markovian reward model. In the i.i.d. model each arm is modelled asan i.i.d. process with an unknown distribution with an unknown mean. In theMarkovian model, each arm is modelled as a finite, irreducible, aperiodic andreversible Markov chain with an unknown probability transition matrix andstationary distribution. The arms give different rewards to different players.If two players pick the same arm, there is a "collision", and neither of themget any reward. There is no dedicated control channel for coordination orcommunication among the players. Any other communication between the users iscostly and will add to the regret. We propose an online index-based distributedlearning policy called ${\tt dUCB_4}$ algorithm that trades off\textit{exploration v. exploitation} in the right way, and achieves expectedregret that grows at most as near-$O(\log^2 T)$. The motivation comes fromopportunistic spectrum access by multiple secondary users in cognitive radionetworks wherein they must pick among various wireless channels that lookdifferent to different users. This is the first distributed learning algorithmfor multi-player MABs to the best of our knowledge.
arxiv-600-217 | Regret Bound by Variation for Online Convex Optimization | http://arxiv.org/pdf/1111.6337v4.pdf | author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu category:cs.LG published:2011-11-28 summary:In citep{Hazan-2008-extract}, the authors showed that the regret of onlinelinear optimization can be bounded by the total variation of the cost vectors.In this paper, we extend this result to general online convex optimization. Wefirst analyze the limitations of the algorithm in \citep{Hazan-2008-extract}when applied it to online convex optimization. We then present two algorithmsfor online convex optimization whose regrets are bounded by the variation ofcost functions. We finally consider the bandit setting, and present arandomized algorithm for online bandit convex optimization with avariation-based regret bound. We show that the regret bound for online banditconvex optimization is optimal when the variation of cost functions isindependent of the number of trials.
arxiv-600-218 | Hybrid Variational/Gibbs Collapsed Inference in Topic Models | http://arxiv.org/pdf/1206.3297v1.pdf | author:Max Welling, Yee Whye Teh, Hilbert Kappen category:cs.LG stat.ML published:2012-06-13 summary:Variational Bayesian inference and (collapsed) Gibbs sampling are the twoimportant classes of inference algorithms for Bayesian networks. Both havetheir advantages and disadvantages: collapsed Gibbs sampling is unbiased but isalso inefficient for large count values and requires averaging over manysamples to reduce variance. On the other hand, variational Bayesian inferenceis efficient and accurate for large count values but suffers from bias forsmall counts. We propose a hybrid algorithm that combines the best of bothworlds: it samples very small counts and applies variational updates to largecounts. This hybridization is shown to significantly improve testset perplexityrelative to variational inference at no computational cost.
arxiv-600-219 | Flexible Priors for Exemplar-based Clustering | http://arxiv.org/pdf/1206.3294v1.pdf | author:Daniel Tarlow, Richard S. Zemel, Brendan J. Frey category:cs.LG stat.ML published:2012-06-13 summary:Exemplar-based clustering methods have been shown to produce state-of-the-artresults on a number of synthetic and real-world clustering problems. They areappealing because they offer computational benefits over latent-mean models andcan handle arbitrary pairwise similarity measures between data points. However,when trying to recover underlying structure in clustering problems, tailoredsimilarity measures are often not enough; we also desire control over thedistribution of cluster sizes. Priors such as Dirichlet process priors allowthe number of clusters to be unspecified while expressing priors over datapartitions. To our knowledge, they have not been applied to exemplar-basedmodels. We show how to incorporate priors, including Dirichlet process priors,into the recently introduced affinity propagation algorithm. We develop anefficient maxproduct belief propagation algorithm for our new model anddemonstrate experimentally how the expanded range of clustering priors allowsus to better recover true clusterings in situations where we have someinformation about the generating process.
arxiv-600-220 | Propagation using Chain Event Graphs | http://arxiv.org/pdf/1206.3293v1.pdf | author:Peter Thwaites, Jim Q. Smith, Robert G. Cowell category:cs.AI cs.CL published:2012-06-13 summary:A Chain Event Graph (CEG) is a graphial model which designed to embodyconditional independencies in problems whose state spaces are highly asymmetricand do not admit a natural product structure. In this paer we present aprobability propagation algorithm which uses the topology of the CEG to build atransporter CEG. Intriungly,the transporter CEG is directly analogous to thetriangulated Bayesian Network (BN) in the more conventional junction treepropagation algorithms used with BNs. The propagation method uses factorizationformulae also analogous to (but different from) the ones using potentials oncliques and separators of the BN. It appears that the methods will be typicallymore efficient than the BN algorithms when applied to contexts where there issignificant asymmetry present.
arxiv-600-221 | Modelling local and global phenomena with sparse Gaussian processes | http://arxiv.org/pdf/1206.3290v1.pdf | author:Jarno Vanhatalo, Aki Vehtari category:cs.LG stat.ML published:2012-06-13 summary:Much recent work has concerned sparse approximations to speed up the Gaussianprocess regression from the unfavorable O(n3) scaling in computational time toO(nm2). Thus far, work has concentrated on models with one covariance function.However, in many practical situations additive models with multiple covariancefunctions may perform better, since the data may contain both long and shortlength-scale phenomena. The long length-scales can be captured with globalsparse approximations, such as fully independent conditional (FIC), and theshort length-scales can be modeled naturally by covariance functions withcompact support (CS). CS covariance functions lead to naturally sparsecovariance matrices, which are computationally cheaper to handle than fullcovariance matrices. In this paper, we propose a new sparse Gaussian processmodel with two additive components: FIC for the long length-scales and CScovariance function for the short length-scales. We give theoretical andexperimental results and show that under certain conditions the proposed modelhas the same computational complexity as FIC. We also compare the modelperformance of the proposed model to additive models approximated by fully andpartially independent conditional (PIC). We use real data sets and show thatour model outperforms FIC and PIC approximations for data sets with twoadditive phenomena.
arxiv-600-222 | Learning the Bayesian Network Structure: Dirichlet Prior versus Data | http://arxiv.org/pdf/1206.3287v1.pdf | author:Harald Steck category:cs.LG stat.ME stat.ML published:2012-06-13 summary:In the Bayesian approach to structure learning of graphical models, theequivalent sample size (ESS) in the Dirichlet prior over the model parameterswas recently shown to have an important effect on the maximum-a-posterioriestimate of the Bayesian network structure. In our first contribution, wetheoretically analyze the case of large ESS-values, which complements previouswork: among other results, we find that the presence of an edge in a Bayesiannetwork is favoured over its absence even if both the Dirichlet prior and thedata imply independence, as long as the conditional empirical distribution isnotably different from uniform. In our second contribution, we focus onrealistic ESS-values, and provide an analytical approximation to the "optimal"ESS-value in a predictive sense (its accuracy is also validatedexperimentally): this approximation provides an understanding as to whichproperties of the data have the main effect determining the "optimal"ESS-value.
arxiv-600-223 | Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping | http://arxiv.org/pdf/1206.3285v1.pdf | author:Richard S. Sutton, Csaba Szepesvari, Alborz Geramifard, Michael P. Bowling category:cs.AI cs.LG cs.SY published:2012-06-13 summary:We consider the problem of efficiently learning optimal control policies andvalue functions over large state spaces in an online setting in which estimatesmust be available after each interaction with the world. This paper develops anexplicitly model-based approach extending the Dyna architecture to linearfunction approximation. Dynastyle planning proceeds by generating imaginaryexperience from the world model and then applying model-free reinforcementlearning algorithms to the imagined state transitions. Our main results are toprove that linear Dyna-style planning converges to a unique solutionindependent of the generating distribution, under natural conditions. In thepolicy evaluation setting, we prove that the limit point is the least-squares(LSTD) solution. An implication of our results is that prioritized-sweeping canbe soundly extended to the linear approximation case, backing up to precedingfeatures rather than to preceding states. We introduce two versions ofprioritized sweeping with linear Dyna and briefly illustrate their performanceempirically on the Mountain Car and Boyan Chain problems.
arxiv-600-224 | The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric Prior for Latent Features | http://arxiv.org/pdf/1206.3279v1.pdf | author:Kurt T. Miller, Thomas Griffiths, Michael I. Jordan category:cs.LG stat.ML published:2012-06-13 summary:Nonparametric Bayesian models are often based on the assumption that theobjects being modeled are exchangeable. While appropriate in some applications(e.g., bag-of-words models for documents), exchangeability is sometimes assumedsimply for computational reasons; non-exchangeable models might be a betterchoice for applications based on subject matter. Drawing on ideas fromgraphical models and phylogenetics, we describe a non-exchangeable prior for aclass of nonparametric latent feature models that is nearly as efficientcomputationally as its exchangeable counterpart. Our model is applicable to thegeneral setting in which the dependencies between objects can be expressedusing a tree, where edge lengths indicate the strength of relationships. Wedemonstrate an application to modeling probabilistic choice.
arxiv-600-225 | Learning Hidden Markov Models for Regression using Path Aggregation | http://arxiv.org/pdf/1206.3275v1.pdf | author:Keith Noto, Mark Craven category:cs.LG cs.CE q-bio.QM published:2012-06-13 summary:We consider the task of learning mappings from sequential data to real-valuedresponses. We present and evaluate an approach to learning a type of hiddenMarkov model (HMM) for regression. The learning process involves inferring thestructure and parameters of a conventional HMM, while simultaneously learning aregression model that maps features that characterize paths through the modelto continuous responses. Our results, in both synthetic and biological domains,demonstrate the value of jointly learning the two components of our approach.
arxiv-600-226 | Small Sample Inference for Generalization Error in Classification Using the CUD Bound | http://arxiv.org/pdf/1206.3274v1.pdf | author:Eric B. Laber, Susan A. Murphy category:cs.LG stat.ML published:2012-06-13 summary:Confidence measures for the generalization error are crucial when smalltraining samples are used to construct classifiers. A common approach is toestimate the generalization error by resampling and then assume the resampledestimator follows a known distribution to form a confidence set [Kohavi 1995,Martin 1996,Yang 2006]. Alternatively, one might bootstrap the resampledestimator of the generalization error to form a confidence set. Unfortunately,these methods do not reliably provide sets of the desired confidence. The poorperformance appears to be due to the lack of smoothness of the generalizationerror as a function of the learned classifier. This results in a non-normaldistribution of the estimated generalization error. We construct a confidenceset for the generalization error by use of a smooth upper bound on thedeviation between the resampled estimate and generalization error. Theconfidence set is formed by bootstrapping this upper bound. In cases in whichthe approximation class for the classifier can be represented as a parametricadditive model, we provide a computationally efficient algorithm. This methodexhibits superior performance across a series of test and simulated data sets.
arxiv-600-227 | Estimation and Clustering with Infinite Rankings | http://arxiv.org/pdf/1206.3270v1.pdf | author:Marina Meila, Le Bao category:cs.LG stat.ML published:2012-06-13 summary:This paper presents a natural extension of stagewise ranking to the the caseof infinitely many items. We introduce the infinite generalized Mallows model(IGM), describe its properties and give procedures to estimate it from data.For estimation of multimodal distributions we introduce theExponential-Blurring-Mean-Shift nonparametric clustering algorithm. Theexperiments highlight the properties of the new model and demonstrate thatinfinite models can be simple, elegant and practical.
arxiv-600-228 | Bayesian Out-Trees | http://arxiv.org/pdf/1206.3269v1.pdf | author:Tony S. Jebara category:cs.LG stat.ML published:2012-06-13 summary:A Bayesian treatment of latent directed graph structure for non-iid data isprovided where each child datum is sampled with a directed conditionaldependence on a single unknown parent datum. The latent graph structure isassumed to lie in the family of directed out-tree graphs which leads toefficient Bayesian inference. The latent likelihood of the data and itsgradients are computable in closed form via Tutte's directed matrix treetheorem using determinants and inverses of the out-Laplacian. This novellikelihood subsumes iid likelihood, is exchangeable and yields efficientunsupervised and semi-supervised learning algorithms. In addition to handlingtaxonomy and phylogenetic datasets the out-tree assumption performssurprisingly well as a semi-parametric density estimator on standard iiddatasets. Experiments with unsupervised and semisupervised learning are shownon various UCI and taxonomy datasets.
arxiv-600-229 | Convergent Message-Passing Algorithms for Inference over General Graphs with Convex Free Energies | http://arxiv.org/pdf/1206.3262v1.pdf | author:Tamir Hazan, Amnon Shashua category:cs.LG stat.ML published:2012-06-13 summary:Inference problems in graphical models can be represented as a constrainedoptimization of a free energy function. It is known that when the Bethe freeenergy is used, the fixedpoints of the belief propagation (BP) algorithmcorrespond to the local minima of the free energy. However BP fails to convergein many cases of interest. Moreover, the Bethe free energy is non-convex forgraphical models with cycles thus introducing great difficulty in derivingefficient algorithms for finding local minima of the free energy for generalgraphs. In this paper we introduce two efficient BP-like algorithms, onesequential and the other parallel, that are guaranteed to converge to theglobal minimum, for any graph, over the class of energies known as "convex freeenergies". In addition, we propose an efficient heuristic for setting theparameters of the convex free energy based on the structure of the graph.
arxiv-600-230 | Causal discovery of linear acyclic models with arbitrary distributions | http://arxiv.org/pdf/1206.3260v1.pdf | author:Patrik O. Hoyer, Aapo Hyvarinen, Richard Scheines, Peter L. Spirtes, Joseph Ramsey, Gustavo Lacerda, Shohei Shimizu category:stat.ML cs.AI cs.LG published:2012-06-13 summary:An important task in data analysis is the discovery of causal relationshipsbetween observed variables. For continuous-valued data, linear acyclic causalmodels are commonly used to model the data-generating process, and theinference of such models is a well-studied problem. However, existing methodshave significant limitations. Methods based on conditional independencies(Spirtes et al. 1993; Pearl 2000) cannot distinguish betweenindependence-equivalent models, whereas approaches purely based on IndependentComponent Analysis (Shimizu et al. 2006) are inapplicable to data which ispartially Gaussian. In this paper, we generalize and combine the twoapproaches, to yield a method able to learn the model structure in many casesfor which the previous methods provide answers that are either incorrect or arenot as informative as possible. We give exact graphical conditions for when twodistinct models represent the same family of distributions, and empiricallydemonstrate the power of our method through thorough simulations.
arxiv-600-231 | Cumulative distribution networks and the derivative-sum-product algorithm | http://arxiv.org/pdf/1206.3259v1.pdf | author:Jim Huang, Brendan J. Frey category:cs.LG stat.ML published:2012-06-13 summary:We introduce a new type of graphical model called a "cumulative distributionnetwork" (CDN), which expresses a joint cumulative distribution as a product oflocal functions. Each local function can be viewed as providing evidence aboutpossible orderings, or rankings, of variables. Interestingly, we find that theconditional independence properties of CDNs are quite different from othergraphical models. We also describe a messagepassing algorithm that efficientlycomputes conditional cumulative distributions. Due to the unique independenceproperties of the CDN, these messages do not in general have a one-to-onecorrespondence with messages exchanged in standard algorithms, such as beliefpropagation. We demonstrate the application of CDNs for structured rankinglearning using a previously-studied multi-player gaming dataset.
arxiv-600-232 | Constrained Approximate Maximum Entropy Learning of Markov Random Fields | http://arxiv.org/pdf/1206.3257v1.pdf | author:Varun Ganapathi, David Vickrey, John Duchi, Daphne Koller category:cs.LG stat.ML published:2012-06-13 summary:Parameter estimation in Markov random fields (MRFs) is a difficult task, inwhich inference over the network is run in the inner loop of a gradient descentprocedure. Replacing exact inference with approximate methods such as loopybelief propagation (LBP) can suffer from poor convergence. In this paper, weprovide a different approach for combining MRF learning and Betheapproximation. We consider the dual of maximum likelihood Markov networklearning - maximizing entropy with moment matching constraints - and thenapproximate both the objective and the constraints in the resultingoptimization problem. Unlike previous work along these lines (Teh & Welling,2003), our formulation allows parameter sharing between features in a generallog-linear model, parameter regularization and conditional training. We showthat piecewise training (Sutton & McCallum, 2005) is a very restricted specialcase of this formulation. We study two optimization strategies: one based on asingle convex approximation and one that uses repeated convex approximations.We show results on several real-world networks that demonstrate that thesealgorithms can significantly outperform learning with loopy and piecewise. Ourresults also provide a framework for analyzing the trade-offs of differentrelaxations of the entropy objective and of the constraints.
arxiv-600-233 | Multi-View Learning over Structured and Non-Identical Outputs | http://arxiv.org/pdf/1206.3256v1.pdf | author:Kuzman Ganchev, Joao Graca, John Blitzer, Ben Taskar category:cs.LG stat.ML published:2012-06-13 summary:In many machine learning problems, labeled training data is limited butunlabeled data is ample. Some of these problems have instances that can befactored into multiple views, each of which is nearly sufficent in determiningthe correct labels. In this paper we present a new algorithm for probabilisticmulti-view learning which uses the idea of stochastic agreement between viewsas regularization. Our algorithm works on structured and unstructured problemsand easily generalizes to partial agreement scenarios. For the full agreementcase, our algorithm minimizes the Bhattacharyya distance between the models ofeach view, and performs better than CoBoosting and two-view Perceptron onseveral flat and structured classification problems.
arxiv-600-234 | Latent Topic Models for Hypertext | http://arxiv.org/pdf/1206.3254v1.pdf | author:Amit Gruber, Michal Rosen-Zvi, Yair Weiss category:cs.IR cs.CL cs.LG stat.ML published:2012-06-13 summary:Latent topic models have been successfully applied as an unsupervised topicdiscovery technique in large document collections. With the proliferation ofhypertext document collection such as the Internet, there has also been greatinterest in extending these approaches to hypertext [6, 9]. These approachestypically model links in an analogous fashion to how they model words - thedocument-link co-occurrence matrix is modeled in the same way that thedocument-word co-occurrence matrix is modeled in standard topic models. In thispaper we present a probabilistic generative model for hypertext documentcollections that explicitly models the generation of links. Specifically, linksfrom a word w to a document d depend directly on how frequent the topic of w isin d, in addition to the in-degree of d. We show how to perform EM learning onthis model efficiently. By not modeling links as analogous to words, we end upusing far fewer free parameters and obtain better link prediction results.
arxiv-600-235 | Convex Point Estimation using Undirected Bayesian Transfer Hierarchies | http://arxiv.org/pdf/1206.3252v1.pdf | author:Gal Elidan, Ben Packer, Geremy Heitz, Daphne Koller category:cs.LG stat.ML published:2012-06-13 summary:When related learning tasks are naturally arranged in a hierarchy, anappealing approach for coping with scarcity of instances is that of transferlearning using a hierarchical Bayes framework. As fully Bayesian computationscan be difficult and computationally demanding, it is often desirable to useposterior point estimates that facilitate (relatively) efficient prediction.However, the hierarchical Bayes framework does not always lend itself naturallyto this maximum aposteriori goal. In this work we propose an undirectedreformulation of hierarchical Bayes that relies on priors in the form ofsimilarity measures. We introduce the notion of "degree of transfer" weights oncomponents of these similarity measures, and show how they can be automaticallylearned within a joint probabilistic framework. Importantly, our reformulationresults in a convex objective for many learning problems, thus facilitatingoptimal posterior point estimation using standard optimization techniques. Inaddition, we no longer require proper priors, allowing for flexible andstraightforward specification of joint distributions over transfer hierarchies.We show that our framework is effective for learning models that are part oftransfer hierarchies for two real-life tasks: object shape modeling usingGaussian density estimation and document classification.
arxiv-600-236 | Projected Subgradient Methods for Learning Sparse Gaussians | http://arxiv.org/pdf/1206.3249v1.pdf | author:John Duchi, Stephen Gould, Daphne Koller category:cs.LG stat.ML published:2012-06-13 summary:Gaussian Markov random fields (GMRFs) are useful in a broad range ofapplications. In this paper we tackle the problem of learning a sparse GMRF ina high-dimensional space. Our approach uses the l1-norm as a regularization onthe inverse covariance matrix. We utilize a novel projected gradient method,which is faster than previous methods in practice and equal to the bestperforming of these in asymptotic complexity. We also extend the l1-regularizedobjective to the problem of sparsifying entire blocks within the inversecovariance matrix. Our methods generalize fairly easily to this case, whileother methods do not. We demonstrate that our extensions give bettergeneralization performance on two real domains--biological network analysis anda 2D-shape modeling image task.
arxiv-600-237 | Learning Convex Inference of Marginals | http://arxiv.org/pdf/1206.3247v1.pdf | author:Justin Domke category:cs.LG stat.ML published:2012-06-13 summary:Graphical models trained using maximum likelihood are a common tool forprobabilistic inference of marginal distributions. However, this approachsuffers difficulties when either the inference process or the model isapproximate. In this paper, the inference process is first defined to be theminimization of a convex function, inspired by free energy approximations.Learning is then done directly in terms of the performance of the inferenceprocess at univariate marginal prediction. The main novelty is that this is adirect minimization of emperical risk, where the risk measures the accuracy ofpredicted marginals.
arxiv-600-238 | Bounds on the Bethe Free Energy for Gaussian Networks | http://arxiv.org/pdf/1206.3243v1.pdf | author:Botond Cseke, Tom Heskes category:cs.LG stat.ML published:2012-06-13 summary:We address the problem of computing approximate marginals in Gaussianprobabilistic models by using mean field and fractional Bethe approximations.As an extension of Welling and Teh (2001), we define the Gaussian fractionalBethe free energy in terms of the moment parameters of the approximatemarginals and derive an upper and lower bound for it. We give necessaryconditions for the Gaussian fractional Bethe free energies to be bounded frombelow. It turns out that the bounding condition is the same as the pairwisenormalizability condition derived by Malioutov et al. (2006) as a sufficientcondition for the convergence of the message passing algorithm. By giving acounterexample, we disprove the conjecture in Welling and Teh (2001): even whenthe Bethe free energy is not bounded from below, it can possess a local minimumto which the minimization algorithms can converge.
arxiv-600-239 | Multi-View Learning in the Presence of View Disagreement | http://arxiv.org/pdf/1206.3242v1.pdf | author:C. Christoudias, Raquel Urtasun, Trevor Darrell category:cs.LG stat.ML published:2012-06-13 summary:Traditional multi-view learning approaches suffer in the presence of viewdisagreement,i.e., when samples in each view do not belong to the same classdue to view corruption, occlusion or other noise processes. In this paper wepresent a multi-view learning approach that uses a conditional entropycriterion to detect view disagreement. Once detected, samples with viewdisagreement are filtered and standard multi-view learning methods can besuccessfully applied to the remaining samples. Experimental evaluation onsynthetic and audio-visual databases demonstrates that the detection andfiltering of view disagreement considerably increases the performance oftraditional multi-view learning approaches.
arxiv-600-240 | Approximating the Partition Function by Deleting and then Correcting for Model Edges | http://arxiv.org/pdf/1206.3241v1.pdf | author:Arthur Choi, Adnan Darwiche category:cs.LG stat.ML published:2012-06-13 summary:We propose an approach for approximating the partition function which isbased on two steps: (1) computing the partition function of a simplified modelwhich is obtained by deleting model edges, and (2) rectifying the result byapplying an edge-by-edge correction. The approach leads to an intuitiveframework in which one can trade-off the quality of an approximation with thecomplexity of computing it. It also includes the Bethe free energyapproximation as a degenerate case. We develop the approach theoretically inthis paper and provide a number of empirical results that reveal its practicalutility.
arxiv-600-241 | Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression | http://arxiv.org/pdf/1206.3238v1.pdf | author:Liefeng Bo, Cristian Sminchisescu category:cs.LG stat.ML published:2012-06-13 summary:We propose a variable decomposition algorithm -greedy block coordinatedescent (GBCD)- in order to make dense Gaussian process regression practicalfor large scale problems. GBCD breaks a large scale optimization into a seriesof small sub-problems. The challenge in variable decomposition algorithms isthe identification of a subproblem (the active set of variables) that yieldsthe largest improvement. We analyze the limitations of existing methods andcast the active set selection into a zero-norm constrained optimization problemthat we solve using greedy methods. By directly estimating the decrease in theobjective function, we obtain not only efficient approximate solutions forGBCD, but we are also able to demonstrate that the method is globallyconvergent. Empirical comparisons against competing dense methods likeConjugate Gradient or SMO show that GBCD is an order of magnitude faster.Comparisons against sparse GP methods show that GBCD is both accurate andcapable of handling datasets of 100,000 samples or more.
arxiv-600-242 | Clique Matrices for Statistical Graph Decomposition and Parameterising Restricted Positive Definite Matrices | http://arxiv.org/pdf/1206.3237v1.pdf | author:David Barber category:cs.DM cs.LG stat.ML published:2012-06-13 summary:We introduce Clique Matrices as an alternative representation of undirectedgraphs, being a generalisation of the incidence matrix representation. Here weuse clique matrices to decompose a graph into a set of possibly overlappingclusters, de ned as well-connected subsets of vertices. The decomposition isbased on a statistical description which encourages clusters to be wellconnected and few in number. Inference is carried out using a variationalapproximation. Clique matrices also play a natural role in parameterisingpositive de nite matrices under zero constraints on elements of the matrix. Weshow that clique matrices can parameterise all positive de nite matricesrestricted according to a decomposable graph and form a structured FactorAnalysis approximation in the non-decomposable case.
arxiv-600-243 | Learning Inclusion-Optimal Chordal Graphs | http://arxiv.org/pdf/1206.3236v1.pdf | author:Vincent Auvray, Louis Wehenkel category:cs.LG cs.DS stat.ML published:2012-06-13 summary:Chordal graphs can be used to encode dependency models that are representableby both directed acyclic and undirected graphs. This paper discusses a verysimple and efficient algorithm to learn the chordal structure of aprobabilistic model from data. The algorithm is a greedy hill-climbing searchalgorithm that uses the inclusion boundary neighborhood over chordal graphs. Inthe limit of a large sample size and under appropriate hypotheses on thescoring criterion, we prove that the algorithm will find a structure that isinclusion-optimal when the dependency model of the data-generating distributioncan be represented exactly by an undirected graph. The algorithm is evaluatedon simulated datasets.
arxiv-600-244 | An efficient hierarchical graph based image segmentation | http://arxiv.org/pdf/1206.2807v1.pdf | author:Silvio Jamil F. Guimarães, Jean Cousty, Yukiko Kenmochi, Laurent Najman category:cs.CV published:2012-06-13 summary:Hierarchical image segmentation provides region-oriented scalespace, i.e., aset of image segmentations at different detail levels in which thesegmentations at finer levels are nested with respect to those at coarserlevels. Most image segmentation algorithms, such as region merging algorithms,rely on a criterion for merging that does not lead to a hierarchy, and forwhich the tuning of the parameters can be difficult. In this work, we propose ahierarchical graph based image segmentation relying on a criterion popularizedby Felzenzwalb and Huttenlocher. We illustrate with both real and syntheticimages, showing efficiency, ease of use, and robustness of our method.
arxiv-600-245 | CORL: A Continuous-state Offset-dynamics Reinforcement Learner | http://arxiv.org/pdf/1206.3231v1.pdf | author:Emma Brunskill, Bethany Leffler, Lihong Li, Michael L. Littman, Nicholas Roy category:cs.LG stat.ML published:2012-06-13 summary:Continuous state spaces and stochastic, switching dynamics characterize anumber of rich, realworld domains, such as robot navigation across varyingterrain. We describe a reinforcementlearning algorithm for learning in thesedomains and prove for certain environments the algorithm is probablyapproximately correct with a sample complexity that scales polynomially withthe state-space dimension. Unfortunately, no optimal planning techniques existin general for such problems; instead we use fitted value iteration to solvethe learned MDP, and include the error due to approximate planning in ourbounds. Finally, we report an experiment using a robotic car driving overvarying terrain to demonstrate that these dynamics representations adequatelycapture real-world dynamics and that our algorithm can be used to efficientlysolve such problems.
arxiv-600-246 | IDS: An Incremental Learning Algorithm for Finite Automata | http://arxiv.org/pdf/1206.2691v1.pdf | author:Muddassar A. Sindhu, Karl Meinke category:cs.LG cs.DS cs.FL published:2012-06-13 summary:We present a new algorithm IDS for incremental learning of deterministicfinite automata (DFA). This algorithm is based on the concept of distinguishingsequences introduced in (Angluin81). We give a rigorous proof that two versionsof this learning algorithm correctly learn in the limit. Finally we present anempirical performance analysis that compares these two algorithms, focussing onlearning times and different types of learning queries. We conclude that IDS isan efficient algorithm for software engineering applications of automatalearning, such as testing and model inference.
arxiv-600-247 | Sparse Prediction with the $k$-Support Norm | http://arxiv.org/pdf/1204.5043v2.pdf | author:Andreas Argyriou, Rina Foygel, Nathan Srebro category:stat.ML cs.LG published:2012-04-23 summary:We derive a novel norm that corresponds to the tightest convex relaxation ofsparsity combined with an $\ell_2$ penalty. We show that this new {\em$k$-support norm} provides a tighter relaxation than the elastic net and isthus a good replacement for the Lasso or the elastic net in sparse predictionproblems. Through the study of the $k$-support norm, we also bound thelooseness of the elastic net, thus shedding new light on it and providingjustification for its use.
arxiv-600-248 | A Novel Windowing Technique for Efficient Computation of MFCC for Speaker Recognition | http://arxiv.org/pdf/1206.2437v1.pdf | author:Md. Sahidullah, Goutam Saha category:cs.CV published:2012-06-12 summary:In this paper, we propose a novel family of windowing technique to computeMel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognitionfrom speech. The proposed method is based on fundamental property of discretetime Fourier transform (DTFT) related to differentiation in frequency domain.Classical windowing scheme such as Hamming window is modified to obtainderivatives of discrete time Fourier transform coefficients. It has beenmathematically shown that the slope and phase of power spectrum are inherentlyincorporated in newly computed cepstrum. Speaker recognition systems based onour proposed family of window functions are shown to attain substantial andconsistent performance improvement over baseline single tapered Hamming windowas well as recently proposed multitaper windowing technique.
arxiv-600-249 | Complex Orthogonal Matching Pursuit and Its Exact Recovery Conditions | http://arxiv.org/pdf/1206.2197v1.pdf | author:Rong Fan, Qun Wan, Yipeng Liu, Hui Chen, Xiao Zhang category:cs.IT math.IT math.NA stat.ML published:2012-06-11 summary:In this paper, we present new results on using orthogonal matching pursuit(OMP), to solve the sparse approximation problem over redundant dictionariesfor complex cases (i.e., complex measurement vector, complex dictionary andcomplex additive white Gaussian noise (CAWGN)). A sufficient condition that OMPcan recover the optimal representation of an exactly sparse signal in thecomplex cases is proposed both in noiseless and bound Gaussian noise settings.Similar to exact recovery condition (ERC) results in real cases, we extend themto complex case and derivate the corresponding ERC in the paper. It leveragesthis theory to show that OMP succeed for k-sparse signal from a class ofcomplex dictionary. Besides, an application with geometrical theory ofdiffraction (GTD) model is presented for complex cases. Finally, simulationexperiments illustrate the validity of the theoretical analysis.
arxiv-600-250 | Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation | http://arxiv.org/pdf/1206.2190v1.pdf | author:Jian-feng Yan, Zhi-Qiang Liu, Yang Gao, Jia Zeng category:cs.LG published:2012-06-11 summary:This paper presents a novel communication-efficient parallel beliefpropagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop aparallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency ofparallel topic modeling, we further use Zipf's law to reduce the totalcommunication cost in PBP. Extensive experiments on different data setsdemonstrate that CE-PBP achieves a higher topic modeling accuracy and reducesmore than 80% communication cost than the state-of-the-art parallel Gibbssampling (PGS) algorithm.
arxiv-600-251 | Comments on "On Approximating Euclidean Metrics by Weighted t-Cost Distances in Arbitrary Dimension" | http://arxiv.org/pdf/1206.2061v1.pdf | author:M. Emre Celebi, Hassan A. Kingravi, Fatih Celiker category:cs.NA cs.CV G.1.2; I.5 published:2012-06-10 summary:Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recentlyintroduced a class of distance functions called weighted t-cost distances thatgeneralize m-neighbor, octagonal, and t-cost distances. He proved that weightedt-cost distances form a family of metrics and derived an approximation for theEuclidean norm in $\mathbb{Z}^n$. In this note we compare this approximation totwo previously proposed Euclidean norm approximations and demonstrate that theempirical average errors given by Mukherjee are significantly optimistic in$\mathbb{R}^n$. We also propose a simple normalization scheme that improves theaccuracy of his approximation substantially with respect to both average andmaximum relative errors.
arxiv-600-252 | Dimension Reduction by Mutual Information Discriminant Analysis | http://arxiv.org/pdf/1206.2058v1.pdf | author:Ali Shadvar category:cs.CV cs.IT cs.LG math.IT published:2012-06-10 summary:In the past few decades, researchers have proposed many discriminant analysis(DA) algorithms for the study of high-dimensional data in a variety ofproblems. Most DA algorithms for feature extraction are based ontransformations that simultaneously maximize the between-class scatter andminimize the withinclass scatter matrices. This paper presents a novel DAalgorithm for feature extraction using mutual information (MI). However, it isnot always easy to obtain an accurate estimation for high-dimensional MI. Inthis paper, we propose an efficient method for feature extraction that is basedon one-dimensional MI estimations. We will refer to this algorithm as mutualinformation discriminant analysis (MIDA). The performance of this proposedmethod was evaluated using UCI databases. The results indicate that MIDAprovides robust performance over different data sets with differentcharacteristics and that MIDA always performs better than, or at leastcomparable to, the best performing algorithms.
arxiv-600-253 | Temporal expression normalisation in natural language texts | http://arxiv.org/pdf/1206.2010v1.pdf | author:Michele Filannino category:cs.CL cs.IR 68U02 D.3.2 published:2012-06-10 summary:Automatic annotation of temporal expressions is a research challenge of greatinterest in the field of information extraction. In this report, I describe anovel rule-based architecture, built on top of a pre-existing system, which isable to normalise temporal expressions detected in English texts. Gold standardtemporally-annotated resources are limited in size and this makes researchdifficult. The proposed system outperforms the state-of-the-art systems withrespect to TempEval-2 Shared Task (value attribute) and achieves substantiallybetter results with respect to the pre-existing system on top of which it hasbeen developed. I will also introduce a new free corpus consisting of 2822unique annotated temporal expressions. Both the corpus and the system arefreely available on-line.
arxiv-600-254 | Developing a model for a text database indexed pedagogically for teaching the Arabic language | http://arxiv.org/pdf/1206.2009v1.pdf | author:Asma Boudhief, Mohsen Maraoui, Mounir Zrigui category:cs.CL published:2012-06-10 summary:In this memory we made the design of an indexing model for Arabic languageand adapting standards for describing learning resources used (the LOM andtheir application profiles) with learning conditions such as levels educationof students, their levels of understanding...the pedagogical context withtaking into account the repre-sentative elements of the text, text'slength,...in particular, we highlight the specificity of the Arabic languagewhich is a complex language, characterized by its flexion, its voyellation andits agglutination.
arxiv-600-255 | Improvement of Loadability in Distribution System Using Genetic Algorithm | http://arxiv.org/pdf/1206.1953v1.pdf | author:Mojtaba Nouri, Mahdi Bayat Mokhtari, Sohrab Mirsaeidi, Mohammad Reza Miveh category:cs.SY cs.NE published:2012-06-09 summary:Generally during recent decades due to development of power systems, themethods for delivering electrical energy to consumers, and because of voltagevariations is a very important problem, the power plants follow this criteria.The good solution for improving transfer and distribution of electrical powerthe majority of consumers prefer to use energy near the loads .So small unitsthat are connected to distribution system named "Decentralized Generation" or"Dispersed Generation". Deregulated in power industry and development ofrenewable energies are the most important factors in developing this type ofelectricity generation. Today DG has a key role in electrical distributionsystems. For example we can refer to improving reliability indices, improvementof stability and reduction of losses in power system. One of the key problemsin using DG's, is allocation of these sources in distribution networks. Loadability in distribution systems and its improvement has an effective role inthe operation of power systems. However, placement of distributed generationsources in order to improve the distribution system load ability index was notconsidered, we show DG placement and allocation with genetic algorithmoptimization method maximize load ability of power systems .This methodimplemented on the IEEE Standard bench marks. The results show theeffectiveness of the proposed algorithm .Another benefits of DG in selectedpositions are also studied and compared.
arxiv-600-256 | OCT Segmentation Survey and Summary Reviews and a Novel 3D Segmentation Algorithm and a Proof of Concept Implementation | http://arxiv.org/pdf/1204.6725v2.pdf | author:Serguei A. Mokhov, Yankui Sun category:cs.CV physics.optics published:2012-04-30 summary:We overview the existing OCT work, especially the practical aspects of it. Wecreate a novel algorithm for 3D OCT segmentation with the goals of speed and/oraccuracy while remaining flexible in the design and implementation for futureextensions and improvements. The document at this point is a running draftbeing iteratively "developed" as a progress report as the work and surveyadvance. It contains the review and summarization of select OCT works, thedesign and implementation of the OCTMARF experimentation application and someresults.
arxiv-600-257 | On the Necessity of Irrelevant Variables | http://arxiv.org/pdf/1203.2557v3.pdf | author:David P. Helmbold, Philip M. Long category:cs.LG published:2012-03-12 summary:This work explores the effects of relevant and irrelevant boolean variableson the accuracy of classifiers. The analysis uses the assumption that thevariables are conditionally independent given the class, and focuses on anatural family of learning algorithms for such sources when the relevantvariables have a small advantage over random guessing. The main result is thatalgorithms relying predominately on irrelevant variables have errorprobabilities that quickly go to 0 in situations where algorithms that limitthe use of irrelevant variables have errors bounded below by a positiveconstant. We also show that accurate learning is possible even when there areso few examples that one cannot determine with high confidence whether or notany individual variable is relevant.
arxiv-600-258 | Multi-timescale Nexting in a Reinforcement Learning Robot | http://arxiv.org/pdf/1112.1133v3.pdf | author:Joseph Modayil, Adam White, Richard S. Sutton category:cs.LG cs.RO published:2011-12-06 summary:The term "nexting" has been used by psychologists to refer to the propensityof people and many other animals to continually predict what will happen nextin an immediate, local, and personal sense. The ability to "next" constitutes abasic kind of awareness and knowledge of one's environment. In this paper wepresent results with a robot that learns to next in real time, predictingthousands of features of the world's state, including all sensory inputs, attimescales from 0.1 to 8 seconds. This was achieved by treating each statefeature as a reward-like target and applying temporal-difference methods tolearn a corresponding value function with a discount rate corresponding to thetimescale. We show that two thousand predictions, each dependent on sixthousand state features, can be learned and updated online at better than 10Hzon a laptop computer, using the standard TD(lambda) algorithm with linearfunction approximation. We show that this approach is efficient enough to bepractical, with most of the learning complete within 30 minutes. We also showthat a single tile-coded feature representation suffices to accurately predictmany different signals at a significant range of timescales. Finally, we showthat the accuracy of our learned predictions compares favorably with theoptimal off-line solution.
arxiv-600-259 | Signal Recovery on Incoherent Manifolds | http://arxiv.org/pdf/1202.1595v2.pdf | author:Chinmay Hegde, Richard G. Baraniuk category:cs.IT math.IT stat.ML published:2012-02-08 summary:Suppose that we observe noisy linear measurements of an unknown signal thatcan be modeled as the sum of two component signals, each of which arises from anonlinear sub-manifold of a high dimensional ambient space. We introduce SPIN,a first order projected gradient method to recover the signal components.Despite the nonconvex nature of the recovery problem and the possibility ofunderdetermined measurements, SPIN provably recovers the signal components,provided that the signal manifolds are incoherent and that the measurementoperator satisfies a certain restricted isometry property. SPIN significantlyextends the scope of current recovery models and algorithms for low dimensionallinear inverse problems and matches (or exceeds) the current state of the artin terms of performance.
arxiv-600-260 | Preconditioned Temporal Difference Learning | http://arxiv.org/pdf/0704.1409v3.pdf | author:Yao HengShuai category:cs.LG cs.AI published:2007-04-11 summary:This paper has been withdrawn by the author. This draft is withdrawn for itspoor quality in english, unfortunately produced by the author when he was juststarting his science route. Look at the ICML version instead:http://icml2008.cs.helsinki.fi/papers/111.pdf
arxiv-600-261 | Memory-Efficient Topic Modeling | http://arxiv.org/pdf/1206.1147v2.pdf | author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG cs.IR published:2012-06-06 summary:As one of the simplest probabilistic topic modeling techniques, latentDirichlet allocation (LDA) has found many important applications in textmining, computer vision and computational biology. Recent training algorithmsfor LDA can be interpreted within a unified message passing framework. However,message passing requires storing previous messages with a large amount ofmemory space, increasing linearly with the number of documents or the number oftopics. Therefore, the high memory usage is often a major problem for topicmodeling of massive corpora containing a large number of topics. To reduce thespace complexity, we propose a novel algorithm without storing previousmessages for training LDA: tiny belief propagation (TBP). The basic idea of TBPrelates the message passing algorithms with the non-negative matrixfactorization (NMF) algorithms, which absorb the message updating into themessage passing process, and thus avoid storing previous messages. Experimentalresults on four large data sets confirm that TBP performs comparably well oreven better than current state-of-the-art training algorithms for LDA but witha much less memory consumption. TBP can do topic modeling when massive corporacannot fit in the computer memory, for example, extracting thematic topics from7 GB PUBMED corpora on a common desktop computer with 2GB memory.
arxiv-600-262 | Multiple Kernel Learning: A Unifying Probabilistic Viewpoint | http://arxiv.org/pdf/1103.0897v3.pdf | author:Hannes Nickisch, Matthias Seeger category:stat.ML published:2011-03-04 summary:We present a probabilistic viewpoint to multiple kernel learning unifyingwell-known regularised risk approaches and recent advances in approximateBayesian inference relaxations. The framework proposes a general objectivefunction suitable for regression, robust regression and classification that islower bound of the marginal likelihood and contains many regularised riskapproaches as special cases. Furthermore, we derive an efficient and provablyconvergent optimisation algorithm.
arxiv-600-263 | Soil Data Analysis Using Classification Techniques and Soil Attribute Prediction | http://arxiv.org/pdf/1206.1557v1.pdf | author:Jay Gholap, Anurag Ingole, Jayesh Gohil, Shailesh Gargade, Vahida Attar category:cs.AI stat.AP stat.ML published:2012-06-07 summary:Agricultural research has been profited by technical advances such asautomation, data mining. Today, data mining is used in a vast areas and manyoff-the-shelf data mining system products and domain specific data miningapplication soft wares are available, but data mining in agricultural soildatasets is a relatively a young research field. The large amounts of data thatare nowadays virtually harvested along with the crops have to be analyzed andshould be used to their full extent. This research aims at analysis of soildataset using data mining techniques. It focuses on classification of soilusing various algorithms available. Another important purpose is to predictuntested attributes using regression technique, and implementation of automatedsoil sample classification.
arxiv-600-264 | Performance Analysis of Unsymmetrical trimmed median as detector on image noises and its Fpga implementation | http://arxiv.org/pdf/1206.1552v1.pdf | author:K. Vasanth, V. Jawahar Senthil Kumar category:cs.CV published:2012-06-07 summary:This Paper Analyze the performance of Unsymmetrical trimmed median, which isused as detector for the detection of impulse noise, Gaussian noise and mixednoise is proposed. The proposed algorithm uses a fixed 3x3 window for theincreasing noise densities. The pixels in the current window are arranged insorting order using a improved snake like sorting algorithm with reducedcomparator. The processed pixel is checked for the occurrence of outliers, ifthe absolute difference between processed pixels is greater than fixedthreshold. Under high noise densities the processed pixel is also noisy hencethe median is checked using the above procedure. if found true then the pixelis considered as noisy hence the corrupted pixel is replaced by the median ofthe current processing window. If median is also noisy then replace thecorrupted pixel with unsymmetrical trimmed median else if the pixel is termeduncorrupted and left unaltered. The proposed algorithm (PA) is tested onvarying detail images for various noises. The proposed algorithm effectivelyremoves the high density fixed value impulse noise, low density random valuedimpulse noise, low density Gaussian noise and lower proportion of mixed noise.The proposed algorithm is targeted on Xc3e5000-5fg900 FPGA using Xilinx 7.1compiler version which requires less number of slices, optimum speed and lowpower when compared to the other median finding architectures.
arxiv-600-265 | Off-Line Arabic Handwriting Character Recognition Using Word Segmentation | http://arxiv.org/pdf/1206.1518v1.pdf | author:Manal A. Abdullah, Lulwah M. Al-Harigy, Hanadi H. Al-Fraidi category:cs.CV published:2012-06-07 summary:The ultimate aim of handwriting recognition is to make computers able to readand/or authenticate human written texts, with a performance comparable to oreven better than that of humans. Reading means that the computer is given apiece of handwriting and it provides the electronic transcription of that (e.g.in ASCII format). Two types of handwriting: on-line and offline. The mostimportant purpose of off-line handwriting recognition is in protection systemsand authentication. Arabic Handwriting scripts are much more complicated incomparison to Latin scripts. This paper introduces a simple and novelmethodology to authenticate Arabic handwriting characters. Reaching our aim, webuilt our own character database. The research methodology depends on twostages: The first is character extraction where preprocessing the word and thenapply segmentation process to obtain the character. The second is the characterrecognition by matching the characters comprising the word with the letters inthe database. Our results ensure character recognition with 81%. We eliminateFAR by using similarity percent between 45-55%. Our research is coded usingMATLAB.
arxiv-600-266 | Optimizing Face Recognition Using PCA | http://arxiv.org/pdf/1206.1515v1.pdf | author:Manal Abdullah, Majda Wazzan, Sahar Bo-saeed category:cs.CV published:2012-06-07 summary:Principle Component Analysis PCA is a classical feature extraction and datarepresentation technique widely used in pattern recognition. It is one of themost successful techniques in face recognition. But it has drawback of highcomputational especially for big size database. This paper conducts a study tooptimize the time complexity of PCA (eigenfaces) that does not affects therecognition performance. The authors minimize the participated eigenvectorswhich consequently decreases the computational time. A comparison is done tocompare the differences between the recognition time in the original algorithmand in the enhanced algorithm. The performance of the original and the enhancedproposed algorithm is tested on face94 face database. Experimental results showthat the recognition time is reduced by 35% by applying our proposed enhancedalgorithm. DET Curves are used to illustrate the experimental results.
arxiv-600-267 | On applying Neuro - Computing in E-com Domain | http://arxiv.org/pdf/1206.1443v1.pdf | author:Asif Perwej category:cs.NE published:2012-06-07 summary:Prior studies have generally suggested that Artificial Neural Networks (ANNs)are superior to conventional statistical models in predicting consumer buyingbehavior. There are, however, contradicting findings which raise question overusefulness of ANNs. This paper discusses development of three neural networksfor modeling consumer e-commerce behavior and compares the findings toequivalent logistic regression models. The results showed that ANNs predicte-commerce adoption slightly more accurately than logistic models but this ishardly justifiable given the added complexity. Further, ANNs seem to be highlyadaptive, particularly when a small sample is coupled with a large number ofnodes in hidden layers which, in turn, limits the neural networks'generalisability.
arxiv-600-268 | A New Greedy Algorithm for Multiple Sparse Regression | http://arxiv.org/pdf/1206.1402v1.pdf | author:Ali Jalali, Sujay Sanghavi category:stat.ML cs.LG published:2012-06-07 summary:This paper proposes a new algorithm for multiple sparse regression in highdimensions, where the task is to estimate the support and values of several(typically related) sparse vectors from a few noisy linear measurements. Ouralgorithm is a "forward-backward" greedy procedure that -- uniquely -- operateson two distinct classes of objects. In particular, we organize our targetsparse vectors as a matrix; our algorithm involves iterative addition andremoval of both (a) individual elements, and (b) entire rows (corresponding toshared features), of the matrix. Analytically, we establish that our algorithm manages to recover the supports(exactly) and values (approximately) of the sparse vectors, under assumptionssimilar to existing approaches based on convex optimization. However, ouralgorithm has a much smaller computational complexity. Perhaps mostinterestingly, it is seen empirically to require visibly fewer samples. Oursrepresents the first attempt to extend greedy algorithms to the class of modelsthat can only/best be represented by a combination of component structuralassumptions (sparse and group-sparse, in our case).
arxiv-600-269 | The Generalization Ability of Online Algorithms for Dependent Data | http://arxiv.org/pdf/1110.2529v2.pdf | author:Alekh Agarwal, John C. Duchi category:stat.ML cs.LG math.OC published:2011-10-11 summary:We study the generalization performance of online learning algorithms trainedon samples coming from a dependent source of data. We show that thegeneralization error of any stable online algorithm concentrates around itsregret--an easily computable statistic of the online performance of thealgorithm--when the underlying ergodic process is $\beta$- or $\phi$-mixing. Weshow high probability error bounds assuming the loss function is convex, and wealso establish sharp convergence rates and deviation bounds for strongly convexlosses and several linear prediction problems such as linear and logisticregression, least-squares SVM, and boosting on dependent data. In addition, ourresults have straightforward applications to stochastic optimization withdependent data, and our analysis requires only martingale convergencearguments; we need not rely on more powerful statistical tools such asempirical process theory.
arxiv-600-270 | Evidence-Based Robust Design of Deflection Actions for Near Earth Objects | http://arxiv.org/pdf/1206.1309v1.pdf | author:Federico Zuiani, Massimiliano Vasile, Alison Gibbings category:cs.CE cs.NE math.OC stat.AP published:2012-06-06 summary:This paper presents a novel approach to the robust design of deflectionactions for Near Earth Objects (NEO). In particular, the case of deflection bymeans of Solar-pumped Laser ablation is studied here in detail. The basic ideabehind Laser ablation is that of inducing a sublimation of the NEO surface,which produces a low thrust thereby slowly deviating the asteroid from itsinitial Earth threatening trajectory. This work investigates the integrateddesign of the Space-based Laser system and the deflection action generated bylaser ablation under uncertainty. The integrated design is formulated as amulti-objective optimisation problem in which the deviation is maximised andthe total system mass is minimised. Both the model for the estimation of thethrust produced by surface laser ablation and the spacecraft system model areassumed to be affected by epistemic uncertainties (partial or complete lack ofknowledge). Evidence Theory is used to quantify these uncertainties andintroduce them in the optimisation process. The propagation of the trajectoryof the NEO under the laser-ablation action is performed with a novel approachbased on an approximated analytical solution of Gauss' Variational Equations.An example of design of the deflection of asteroid Apophis with a swarm ofspacecraft is presented.
arxiv-600-271 | MACS: An Agent-Based Memetic Multiobjective Optimization Algorithm Applied to Space Trajectory Design | http://arxiv.org/pdf/1206.1305v1.pdf | author:Massimiliano Vasile, Federico Zuiani category:cs.CE cs.NE math.OC published:2012-06-06 summary:This paper presents an algorithm for multiobjective optimization that blendstogether a number of heuristics. A population of agents combines heuristicsthat aim at exploring the search space both globally and in a neighborhood ofeach agent. These heuristics are complemented with a combination of a local andglobal archive. The novel agent- based algorithm is tested at first on a set ofstandard problems and then on three specific problems in space trajectorydesign. Its performance is compared against a number of state-of-the-artmultiobjective optimisation algorithms that use the Pareto dominance asselection criterion: NSGA-II, PAES, MOPSO, MTS. The results demonstrate thatthe agent-based search can identify parts of the Pareto set that the otheralgorithms were not able to capture. Furthermore, convergence is statisticallybetter although the variance of the results is in some cases higher.
arxiv-600-272 | Get out the vote: Determining support or opposition from Congressional floor-debate transcripts | http://arxiv.org/pdf/cs/0607062v3.pdf | author:Matt Thomas, Bo Pang, Lillian Lee category:cs.CL cs.SI physics.soc-ph I.2.7 published:2006-07-12 summary:We investigate whether one can determine from the transcripts of U.S.Congressional floor debates whether the speeches represent support of oropposition to proposed legislation. To address this problem, we exploit thefact that these speeches occur as part of a discussion; this allows us to usesources of information regarding relationships between discourse segments, suchas whether a given utterance indicates agreement with the opinion expressed byanother. We find that the incorporation of such information yields substantialimprovements over classifying speeches in isolation.
arxiv-600-273 | Gossip Learning with Linear Models on Fully Distributed Data | http://arxiv.org/pdf/1109.1396v3.pdf | author:Róbert Ormándi, István Hegedüs, Márk Jelasity category:cs.LG cs.DC published:2011-09-07 summary:Machine learning over fully distributed data poses an important problem inpeer-to-peer (P2P) applications. In this model we have one data record at eachnetwork node, but without the possibility to move raw data due to privacyconsiderations. For example, user profiles, ratings, history, or sensorreadings can represent this case. This problem is difficult, because there isno possibility to learn local models, the system model offers almost noguarantees for reliability, yet the communication cost needs to be kept low.Here we propose gossip learning, a generic approach that is based on multiplemodels taking random walks over the network in parallel, while applying anonline learning algorithm to improve themselves, and getting combined viaensemble learning methods. We present an instantiation of this approach for thecase of classification with linear models. Our main contribution is an ensemblelearning method which---through the continuous combination of the models in thenetwork---implements a virtual weighted voting mechanism over an exponentialnumber of models at practically no extra cost as compared to independent randomwalks. We prove the convergence of the method theoretically, and performextensive experiments on benchmark datasets. Our experimental analysisdemonstrates the performance and robustness of the proposed approach.
arxiv-600-274 | Inverse-Category-Frequency based supervised term weighting scheme for text categorization | http://arxiv.org/pdf/1012.2609v4.pdf | author:Deqing Wang, Hui Zhang category:cs.LG cs.AI published:2010-12-13 summary:Term weighting schemes often dominate the performance of many classifiers,such as kNN, centroid-based classifier and SVMs. The widely used term weightingscheme in text categorization, i.e., tf.idf, is originated from informationretrieval (IR) field. The intuition behind idf for text categorization seemsless reasonable than IR. In this paper, we introduce inverse category frequency(icf) into term weighting scheme and propose two novel approaches, i.e., tf.icfand icf-based supervised term weighting schemes. The tf.icf adopts icf tosubstitute idf factor and favors terms occurring in fewer categories, ratherthan fewer documents. And the icf-based approach combines icf and relevancefrequency (rf) to weight terms in a supervised way. Our cross-classifier andcross-corpus experiments have shown that our proposed approaches are superioror comparable to six supervised term weighting schemes and three traditionalschemes in terms of macro-F1 and micro-F1.
arxiv-600-275 | Memetic Artificial Bee Colony Algorithm for Large-Scale Global Optimization | http://arxiv.org/pdf/1206.1074v1.pdf | author:Iztok Fister, Iztok Fister Jr., Janez Brest, Viljem Žumer category:cs.NE cs.AI published:2012-06-05 summary:Memetic computation (MC) has emerged recently as a new paradigm of efficientalgorithms for solving the hardest optimization problems. On the other hand,artificial bees colony (ABC) algorithms demonstrate good performances whensolving continuous and combinatorial optimization problems. This study tries touse these technologies under the same roof. As a result, a memetic ABC (MABC)algorithm has been developed that is hybridized with two local searchheuristics: the Nelder-Mead algorithm (NMA) and the random walk with directionexploitation (RWDE). The former is attended more towards exploration, while thelatter more towards exploitation of the search space. The stochastic adaptationrule was employed in order to control the balancing between exploration andexploitation. This MABC algorithm was applied to a Special suite on Large ScaleContinuous Global Optimization at the 2012 IEEE Congress on EvolutionaryComputation. The obtained results the MABC are comparable with the results ofDECC-G, DECC-G*, and MLCC.
arxiv-600-276 | Hedge detection as a lens on framing in the GMO debates: A position paper | http://arxiv.org/pdf/1206.1066v1.pdf | author:Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian Danescu-Niculescu-Mizil, Jennifer Spindel category:cs.CL published:2012-06-05 summary:Understanding the ways in which participants in public discussions frametheir arguments is important in understanding how public opinion is formed. Inthis paper, we adopt the position that it is time for morecomputationally-oriented research on problems involving framing. In theinterests of furthering that goal, we propose the following specific,interesting and, we believe, relatively accessible question: In the controversyregarding the use of genetically-modified organisms (GMOs) in agriculture, dopro- and anti-GMO articles differ in whether they choose to adopt a"scientific" tone? Prior work on the rhetoric and sociology of science suggests that hedging maydistinguish popular-science text from text written by professional scientistsfor their colleagues. We propose a detailed approach to studying whether hedgedetection can be used to understanding scientific framing in the GMO debates,and provide corpora to facilitate this study. Some of our preliminary analysessuggest that hedges occur less frequently in scientific discourse than inpopular text, a finding that contradicts prior assertions in the literature. Wehope that our initial work and data will encourage others to pursue thispromising line of inquiry.
arxiv-600-277 | Nearly optimal solutions for the Chow Parameters Problem and low-weight approximation of halfspaces | http://arxiv.org/pdf/1206.0985v1.pdf | author:Anindya De, Ilias Diakonikolas, Vitaly Feldman, Rocco A. Servedio category:cs.CC cs.DS cs.LG published:2012-06-05 summary:The \emph{Chow parameters} of a Boolean function $f: \{-1,1\}^n \to \{-1,1\}$are its $n+1$ degree-0 and degree-1 Fourier coefficients. It has been knownsince 1961 (Chow, Tannenbaum) that the (exact values of the) Chow parameters ofany linear threshold function $f$ uniquely specify $f$ within the space of allBoolean functions, but until recently (O'Donnell and Servedio) nothing wasknown about efficient algorithms for \emph{reconstructing} $f$ (exactly orapproximately) from exact or approximate values of its Chow parameters. Werefer to this reconstruction problem as the \emph{Chow Parameters Problem.} Our main result is a new algorithm for the Chow Parameters Problem which,given (sufficiently accurate approximations to) the Chow parameters of anylinear threshold function $f$, runs in time $\tilde{O}(n^2)\cdot(1/\eps)^{O(\log^2(1/\eps))}$ and with high probability outputs arepresentation of an LTF $f'$ that is $\eps$-close to $f$. The only previousalgorithm (O'Donnell and Servedio) had running time $\poly(n) \cdot2^{2^{\tilde{O}(1/\eps^2)}}.$ As a byproduct of our approach, we show that for any linear thresholdfunction $f$ over $\{-1,1\}^n$, there is a linear threshold function $f'$ whichis $\eps$-close to $f$ and has all weights that are integers at most $\sqrt{n}\cdot (1/\eps)^{O(\log^2(1/\eps))}$. This significantly improves the bestprevious result of Diakonikolas and Servedio which gave a $\poly(n) \cdot2^{\tilde{O}(1/\eps^{2/3})}$ weight bound, and is close to the known lowerbound of $\max\{\sqrt{n},$ $(1/\eps)^{\Omega(\log \log (1/\eps))}\}$ (Goldberg,Servedio). Our techniques also yield improved algorithms for related problemsin learning theory.
arxiv-600-278 | Manifold estimation and singular deconvolution under Hausdorff loss | http://arxiv.org/pdf/1109.4540v2.pdf | author:Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2011-09-21 summary:We find lower and upper bounds for the risk of estimating a manifold inHausdorff distance under several models. We also show that there are closeconnections between manifold estimation and the problem of deconvolving asingular measure.
arxiv-600-279 | A Mixed Observability Markov Decision Process Model for Musical Pitch | http://arxiv.org/pdf/1206.0855v1.pdf | author:Pouyan Rafiei Fard, Keyvan Yahya category:cs.AI cs.LG published:2012-06-05 summary:Partially observable Markov decision processes have been widely used toprovide models for real-world decision making problems. In this paper, we willprovide a method in which a slightly different version of them called Mixedobservability Markov decision process, MOMDP, is going to join with ourproblem. Basically, we aim at offering a behavioural model for interaction ofintelligent agents with musical pitch environment and we will show that howMOMDP can shed some light on building up a decision making model for musicalpitch conveniently.
arxiv-600-280 | Kullback-Leibler aggregation and misspecified generalized linear models | http://arxiv.org/pdf/0911.2919v5.pdf | author:Philippe Rigollet category:stat.ML math.ST stat.TH published:2009-11-16 summary:In a regression setup with deterministic design, we study the pureaggregation problem and introduce a natural extension from the Gaussiandistribution to distributions in the exponential family. While this extensionbears strong connections with generalized linear models, it does not requireidentifiability of the parameter or even that the model on the systematiccomponent is true. It is shown that this problem can be solved by constrainedand/or penalized likelihood maximization and we derive sharp oracleinequalities that hold both in expectation and with high probability. Finallyall the bounds are proved to be optimal in a minimax sense.
arxiv-600-281 | Changepoint Detection over Graphs with the Spectral Scan Statistic | http://arxiv.org/pdf/1206.0773v1.pdf | author:James Sharpnack, Alessandro Rinaldo, Aarti Singh category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-06-04 summary:We consider the change-point detection problem of deciding, based on noisymeasurements, whether an unknown signal over a given graph is constant or isinstead piecewise constant over two connected induced subgraphs of relativelylow cut size. We analyze the corresponding generalized likelihood ratio (GLR)statistics and relate it to the problem of finding a sparsest cut in a graph.We develop a tractable relaxation of the GLR statistic based on thecombinatorial Laplacian of the graph, which we call the spectral scanstatistic, and analyze its properties. We show how its performance as a testingprocedure depends directly on the spectrum of the graph, and use this result toexplicitly derive its asymptotic properties on few significant graphtopologies. Finally, we demonstrate both theoretically and by simulations thatthe spectral scan statistic can outperform naive testing procedures based onedge thresholding and $\chi^2$ testing.
arxiv-600-282 | Topological graph clustering with thin position | http://arxiv.org/pdf/1206.0771v1.pdf | author:Jesse Johnson category:math.GT cs.LG stat.ML published:2012-06-04 summary:A clustering algorithm partitions a set of data points into smaller sets(clusters) such that each subset is more tightly packed than the whole. Manyapproaches to clustering translate the vector data into a graph with edgesreflecting a distance or similarity metric on the points, then look for highlyconnected subgraphs. We introduce such an algorithm based on ideas borrowedfrom the topological notion of thin position for knots and 3-dimensionalmanifolds.
arxiv-600-283 | Theoretical foundation for CMA-ES from information geometric perspective | http://arxiv.org/pdf/1206.0730v1.pdf | author:Youhei Akimoto, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi category:cs.NE published:2012-06-04 summary:This paper explores the theoretical basis of the covariance matrix adaptationevolution strategy (CMA-ES) from the information geometry viewpoint. To establish a theoretical foundation for the CMA-ES, we focus on a geometricstructure of a Riemannian manifold of probability distributions equipped withthe Fisher metric. We define a function on the manifold which is theexpectation of fitness over the sampling distribution, and regard the goal ofupdate of the parameters of sampling distribution in the CMA-ES as maximizationof the expected fitness. We investigate the steepest ascent learning for theexpected fitness maximization, where the steepest ascent direction is given bythe natural gradient, which is the product of the inverse of the Fisherinformation matrix and the conventional gradient of the function. Our first result is that we can obtain under some types of parameterizationof multivariate normal distribution the natural gradient of the expectedfitness without the need for inversion of the Fisher information matrix. Wefind that the update of the distribution parameters in the CMA-ES is the sameas natural gradient learning for expected fitness maximization. Our secondresult is that we derive the range of learning rates such that a step in thedirection of the exact natural gradient improves the parameters in the expectedfitness. We see from the close relation between the CMA-ES and natural gradientlearning that the default setting of learning rates in the CMA-ES seemssuitable in terms of monotone improvement in expected fitness. Then, we discussthe relation to the expectation-maximization framework and provide aninformation geometric interpretation of the CMA-ES.
arxiv-600-284 | Multi-Sparse Signal Recovery for Compressive Sensing | http://arxiv.org/pdf/1206.0663v1.pdf | author:Yipeng Liu, Ivan Gligorijevic, Vladimir Matic, Maarten De Vos, Sabine Van Huffel category:cs.IT cs.SY math.IT math.OC stat.ML published:2012-06-04 summary:Signal recovery is one of the key techniques of Compressive sensing (CS). Itreconstructs the original signal from the linear sub-Nyquist measurements.Classical methods exploit the sparsity in one domain to formulate the L0 normoptimization. Recent investigation shows that some signals are sparse inmultiple domains. To further improve the signal reconstruction performance, wecan exploit this multi-sparsity to generate a new convex programming model. Thelatter is formulated with multiple sparsity constraints in multiple domains andthe linear measurement fitting constraint. It improves signal recoveryperformance by additional a priori information. Since some EMG signals exhibitsparsity both in time and frequency domains, we take them as example innumerical experiments. Results show that the newly proposed method achievesbetter performance for multi-sparse signals.
arxiv-600-285 | Web search queries can predict stock market volumes | http://arxiv.org/pdf/1110.4784v3.pdf | author:Ilaria Bordino, Stefano Battiston, Guido Caldarelli, Matthieu Cristelli, Antti Ukkonen, Ingmar Weber category:q-fin.ST cs.LG physics.soc-ph published:2011-10-21 summary:We live in a computerized and networked society where many of our actionsleave a digital trace and affect other people's actions. This has lead to theemergence of a new data-driven research field: mathematical methods of computerscience, statistical physics and sociometry provide insights on a wide range ofdisciplines ranging from social science to human mobility. A recent importantdiscovery is that query volumes (i.e., the number of requests submitted byusers to search engines on the www) can be used to track and, in some cases, toanticipate the dynamics of social phenomena. Successful exemples includeunemployment levels, car and home sales, and epidemics spreading. Few recentworks applied this approach to stock prices and market sentiment. However, itremains unclear if trends in financial markets can be anticipated by thecollective wisdom of on-line users on the web. Here we show that tradingvolumes of stocks traded in NASDAQ-100 are correlated with the volumes ofqueries related to the same stocks. In particular, query volumes anticipate inmany cases peaks of trading by one day or more. Our analysis is carried out ona unique dataset of queries, submitted to an important web search engine, whichenable us to investigate also the user behavior. We show that the query volumedynamics emerges from the collective but seemingly uncoordinated activity ofmany users. These findings contribute to the debate on the identification ofearly warnings of financial systemic risk, based on the activity of users ofthe www.
arxiv-600-286 | CELL: Connecting Everyday Life in an archipeLago | http://arxiv.org/pdf/1204.6325v2.pdf | author:Konstantinos Chorianopoulos, Vassiliki Tsaknaki category:cs.HC cs.LG published:2012-04-27 summary:We explore the design of a seamless broadcast communication system thatbrings together the distributed community of remote secondary educationschools. In contrast to higher education, primary and secondary educationestablishments should remain distributed, in order to maintain a balance ofurban and rural life in the developing and the developed world. We plan todeploy an ambient and social interactive TV platform (physical installation,authoring tools, interactive content) that supports social communication in apositive way. In particular, we present the physical design and the conceptualmodel of the system.
arxiv-600-287 | Greedy expansions in convex optimization | http://arxiv.org/pdf/1206.0393v1.pdf | author:V. N. Temlyakov category:stat.ML math.OC 41A65 published:2012-06-02 summary:This paper is a follow up to the previous author's paper on convexoptimization. In that paper we began the process of adjusting greedy-typealgorithms from nonlinear approximation for finding sparse solutions of convexoptimization problems. We modified there three the most popular in nonlinearapproximation in Banach spaces greedy algorithms -- Weak Chebyshev GreedyAlgorithm, Weak Greedy Algorithm with Free Relaxation and Weak Relaxed GreedyAlgorithm -- for solving convex optimization problems. We continue to studysparse approximate solutions to convex optimization problems. It is known thatin many engineering applications researchers are interested in an approximatesolution of an optimization problem as a linear combination of elements from agiven system of elements. There is an increasing interest in building suchsparse approximate solutions using different greedy-type algorithms. In thispaper we concentrate on greedy algorithms that provide expansions, which meansthat the approximant at the $m$th iteration is equal to the sum of theapproximant from the previous iteration ($(m-1)$th iteration) and one elementfrom the dictionary with an appropriate coefficient. The problem of greedyexpansions of elements of a Banach space is well studied in nonlinearapproximation theory. At a first glance the setting of a problem of expansionof a given element and the setting of the problem of expansion in anoptimization problem are very different. However, it turns out that the sametechnique can be used for solving both problems. We show how the techniquedeveloped in nonlinear approximation theory, in particular, the greedyexpansions technique can be adjusted for finding a sparse solution of anoptimization problem given by an expansion with respect to a given dictionary.
arxiv-600-288 | Greedy approximation in convex optimization | http://arxiv.org/pdf/1206.0392v1.pdf | author:V. N. Temlyakov category:stat.ML math.OC 41A65 published:2012-06-02 summary:We study sparse approximate solutions to convex optimization problems. It isknown that in many engineering applications researchers are interested in anapproximate solution of an optimization problem as a linear combination ofelements from a given system of elements. There is an increasing interest inbuilding such sparse approximate solutions using different greedy-typealgorithms. The problem of approximation of a given element of a Banach spaceby linear combinations of elements from a given system (dictionary) is wellstudied in nonlinear approximation theory. At a first glance the settings ofapproximation and optimization problems are very different. In theapproximation problem an element is given and our task is to find a sparseapproximation of it. In optimization theory an energy function is given and weshould find an approximate sparse solution to the minimization problem. Itturns out that the same technique can be used for solving both problems. Weshow how the technique developed in nonlinear approximation theory, inparticular, the greedy approximation technique can be adjusted for finding asparse solution of an optimization problem.
arxiv-600-289 | UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser Approach | http://arxiv.org/pdf/1206.0381v1.pdf | author:Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear category:cs.CL published:2012-06-02 summary:Universal Networking Language (UNL) is a declarative formal language that isused to represent semantic data extracted from natural language texts. Thispaper presents a novel approach to converting Bangla natural language text intoUNL using a method known as Predicate Preserving Parser (PPP) technique. PPPperforms morphological, syntactic and semantic, and lexical analysis of textsynchronously. This analysis produces a semantic-net like structure representedusing UNL. We demonstrate how Bangla texts are analyzed following the PPPtechnique to produce UNL documents which can then be translated into any othersuitable natural language facilitating the opportunity to develop a universallanguage translation method via UNL.
arxiv-600-290 | Automated Word Puzzle Generation via Topic Dictionaries | http://arxiv.org/pdf/1206.0377v1.pdf | author:Balazs Pinter, Gyula Voros, Zoltan Szabo, Andras Lorincz category:cs.CL math.CO 68T50, 15A23 published:2012-06-02 summary:We propose a general method for automated word puzzle generation. Contrary toprevious approaches in this novel field, the presented method does not rely onhighly structured datasets obtained with serious human annotation effort: itonly needs an unstructured and unannotated corpus (i.e., document collection)as input. The method builds upon two additional pillars: (i) a topic model,which induces a topic dictionary from the input corpus (examples include e.g.,latent semantic analysis, group-structured dictionaries or latent Dirichletallocation), and (ii) a semantic similarity measure of word pairs. Our methodcan (i) generate automatically a large number of proper word puzzles ofdifferent types, including the odd one out, choose the related word andseparate the topics puzzle. (ii) It can easily create domain-specific puzzlesby replacing the corpus component. (iii) It is also capable of automaticallygenerating puzzles with parameterizable levels of difficulty suitable for,e.g., beginners or intermediate learners.
arxiv-600-291 | A Route Confidence Evaluation Method for Reliable Hierarchical Text Categorization | http://arxiv.org/pdf/1206.0335v1.pdf | author:Nima Hatami, Camelia Chira, Giuliano Armano category:cs.IR cs.LG published:2012-06-02 summary:Hierarchical Text Categorization (HTC) is becoming increasingly importantwith the rapidly growing amount of text data available in the World Wide Web.Among the different strategies proposed to cope with HTC, the Local Classifierper Node (LCN) approach attains good performance by mirroring the underlyingclass hierarchy while enforcing a top-down strategy in the testing step.However, the problem of embedding hierarchical information (parent-childrelationship) to improve the performance of HTC systems still remains open. Aconfidence evaluation method for a selected route in the hierarchy is proposedto evaluate the reliability of the final candidate labels in an HTC system. Inorder to take into account the information embedded in the hierarchy, weightfactors are used to take into account the importance of each level. Anacceptance/rejection strategy in the top-down decision making process isproposed, which improves the overall categorization accuracy by rejecting a fewpercentage of samples, i.e., those with low reliability score. Experimentalresults on the Reuters benchmark dataset (RCV1- v2) confirm the effectivenessof the proposed method, compared to other state-of-the art HTC methods.
arxiv-600-292 | Sparse Trace Norm Regularization | http://arxiv.org/pdf/1206.0333v1.pdf | author:Jianhui Chen, Jieping Ye category:cs.LG stat.ML published:2012-06-02 summary:We study the problem of estimating multiple predictive functions from adictionary of basis functions in the nonparametric regression setting. Ourestimation scheme assumes that each predictive function can be estimated in theform of a linear combination of the basis functions. By assuming that thecoefficient matrix admits a sparse low-rank structure, we formulate thefunction estimation problem as a convex program regularized by the trace normand the $\ell_1$-norm simultaneously. We propose to solve the convex programusing the accelerated gradient (AG) method and the alternating direction methodof multipliers (ADMM) respectively; we also develop efficient algorithms tosolve the key components in both AG and ADMM. In addition, we conducttheoretical analysis on the proposed function estimation scheme: we derive akey property of the optimal solution to the convex program; based on anassumption on the basis functions, we establish a performance bound of theproposed function estimation scheme (via the composite regularization).Simulation studies demonstrate the effectiveness and efficiency of the proposedalgorithms.
arxiv-600-293 | Rapid Feature Extraction for Optical Character Recognition | http://arxiv.org/pdf/1206.0238v1.pdf | author:M. Zahid Hossain, M. Ashraful Amin, Hong Yan category:cs.CV I.5.2; I.7.5 published:2012-06-01 summary:Feature extraction is one of the fundamental problems of characterrecognition. The performance of character recognition system is depends onproper feature extraction and correct classifier selection. In this article, arapid feature extraction method is proposed and named as Celled Projection (CP)that compute the projection of each section formed through partitioning animage. The recognition performance of the proposed method is compared withother widely used feature extraction methods that are intensively studied formany different scripts in literature. The experiments have been conducted usingBangla handwritten numerals along with three different well known classifierswhich demonstrate comparable results including 94.12% recognition accuracyusing celled projection.
arxiv-600-294 | Multiclass Learning Approaches: A Theoretical Comparison with Implications | http://arxiv.org/pdf/1205.6432v2.pdf | author:Amit Daniely, Sivan Sabato, Shai Shalev Shwartz category:cs.LG published:2012-05-29 summary:We theoretically analyze and compare the following five popular multiclassclassification methods: One vs. All, All Pairs, Tree-based classifiers, ErrorCorrecting Output Codes (ECOC) with randomly generated code matrices, andMulticlass SVM. In the first four methods, the classification is based on areduction to binary classification. We consider the case where the binaryclassifier comes from a class of VC dimension $d$, and in particular from theclass of halfspaces over $\reals^d$. We analyze both the estimation error andthe approximation error of these methods. Our analysis reveals interestingconclusions of practical relevance, regarding the success of the differentapproaches under various conditions. Our proof technique employs tools from VCtheory to analyze the \emph{approximation error} of hypothesis classes. This isin sharp contrast to most, if not all, previous uses of VC theory, which onlydeal with estimation error.
arxiv-600-295 | OpenGM: A C++ Library for Discrete Graphical Models | http://arxiv.org/pdf/1206.0111v1.pdf | author:Bjoern Andres, Thorsten Beier, Joerg H. Kappes category:cs.AI cs.MS stat.ML published:2012-06-01 summary:OpenGM is a C++ template library for defining discrete graphical models andperforming inference on these models, using a wide range of state-of-the-artalgorithms. No restrictions are imposed on the factor graph to allow forhigher-order factors and arbitrary neighborhood structures. Large models withrepetitive structure are handled efficiently because (i) functions that occurrepeatedly need to be stored only once, and (ii) distinct functions can beimplemented differently, using different encodings alongside each other in thesame model. Several parametric functions (e.g. metrics), sparse and dense valuetables are provided and so is an interface for custom C++ code. Algorithms areseparated by design from the representation of graphical models and are easilyexchangeable. OpenGM, its algorithms, HDF5 file format and command line toolsare modular and extendible.
arxiv-600-296 | Language Acquisition in Computers | http://arxiv.org/pdf/1206.0042v1.pdf | author:Megan Belzner, Sean Colin-Ellerin, Jorge H. Roman category:cs.CL I.2.6; I.2.7 published:2012-05-31 summary:This project explores the nature of language acquisition in computers, guidedby techniques similar to those used in children. While existing naturallanguage processing methods are limited in scope and understanding, our systemaims to gain an understanding of language from first principles and henceminimal initial input. The first portion of our system was implemented in Javaand is focused on understanding the morphology of language using bigrams. Weuse frequency distributions and differences between them to define anddistinguish languages. English and French texts were analyzed to determine adifference threshold of 55 before the texts are considered to be in differentlanguages, and this threshold was verified using Spanish texts. The secondportion of our system focuses on gaining an understanding of the syntax of alanguage using a recursive method. The program uses one of two possible methodsto analyze given sentences based on either sentence patterns or surroundingwords. Both methods have been implemented in C++. The program is able tounderstand the structure of simple sentences and learn new words. In addition,we have provided some suggestions regarding future work and potentialextensions of the existing program.
arxiv-600-297 | Oriented and Degree-generated Block Models: Generating and Inferring Communities with Inhomogeneous Degree Distributions | http://arxiv.org/pdf/1205.7009v1.pdf | author:Yaojia Zhu, Xiaoran Yan, Cristopher Moore category:cs.SI physics.soc-ph stat.ML published:2012-05-31 summary:The stochastic block model is a powerful tool for inferring communitystructure from network topology. However, it predicts a Poisson degreedistribution within each community, while most real-world networks have aheavy-tailed degree distribution. The degree-corrected block model canaccommodate arbitrary degree distributions within communities. But since ittakes the vertex degrees as parameters rather than generating them, it cannotuse them to help it classify the vertices, and its natural generalization todirected graphs cannot even use the orientations of the edges. In this paper,we present variants of the block model with the best of both worlds: they canuse vertex degrees and edge orientations in the classification process, whiletolerating heavy-tailed degree distributions within communities. We show thatfor some networks, including synthetic networks and networks of wordadjacencies in English text, these new block models achieve a higher accuracythan either standard or degree-corrected block models.
arxiv-600-298 | Beyond $\ell_1$-norm minimization for sparse signal recovery | http://arxiv.org/pdf/1205.6849v1.pdf | author:Hassan Mansour category:cs.IT cs.LG math.IT published:2012-05-30 summary:Sparse signal recovery has been dominated by the basis pursuit denoise (BPDN)problem formulation for over a decade. In this paper, we propose an algorithmthat outperforms BPDN in finding sparse solutions to underdetermined linearsystems of equations at no additional computational cost. Our algorithm, calledWSPGL1, is a modification of the spectral projected gradient for $\ell_1$minimization (SPGL1) algorithm in which the sequence of LASSO subproblems arereplaced by a sequence of weighted LASSO subproblems with constant weightsapplied to a support estimate. The support estimate is derived from the dataand is updated at every iteration. The algorithm also modifies the Pareto curveat every iteration to reflect the new weighted $\ell_1$ minimization problemthat is being solved. We demonstrate through extensive simulations that thesparse recovery performance of our algorithm is superior to that of $\ell_1$minimization and approaches the recovery performance of iterative re-weighted$\ell_1$ (IRWL1) minimization of Cand{\`e}s, Wakin, and Boyd, although it doesnot match it in general. Moreover, our algorithm has the computational cost ofa single BPDN problem.
arxiv-600-299 | Fingerprint Gender Classification using Wavelet Transform and Singular Value Decomposition | http://arxiv.org/pdf/1205.6745v1.pdf | author:P Gnanasivam, Dr. S Muttan category:cs.CV published:2012-05-30 summary:A novel method of gender Classification from fingerprint is proposed based ondiscrete wavelet transform (DWT) and singular value decomposition (SVD). Theclassification is achieved by extracting the energy computed from all thesub-bands of DWT combined with the spatial features of non-zero singular valuesobtained from the SVD of fingerprint images. K nearest neighbor (KNN) used as aclassifier. This method is experimented with the internal database of 3570fingerprints finger prints in which 1980 were male fingerprints and 1590 werefemale fingerprints. Finger-wise gender classification is achieved which is94.32% for the left hand little fingers of female persons and 95.46% for theleft hand index finger of male persons. Gender classification for any finger ofmale persons tested is attained as 91.67% and 84.69% for female personsrespectively. Overall classification rate is 88.28% has been achieved.
arxiv-600-300 | Template-Cut: A Pattern-Based Segmentation Paradigm | http://arxiv.org/pdf/1205.6605v1.pdf | author:Jan Egger, Bernd Freisleben, Christopher Nimsky, Tina Kapur category:cs.CV published:2012-05-30 summary:We present a scale-invariant, template-based segmentation paradigm that setsup a graph and performs a graph cut to separate an object from the background.Typically graph-based schemes distribute the nodes of the graph uniformly andequidistantly on the image, and use a regularizer to bias the cut towards aparticular shape. The strategy of uniform and equidistant nodes does not allowthe cut to prefer more complex structures, especially when areas of the objectare indistinguishable from the background. We propose a solution by introducingthe concept of a "template shape" of the target object in which the nodes aresampled non-uniformly and non-equidistantly on the image. We evaluate it on2D-images where the object's textures and backgrounds are similar, and largeareas of the object have the same gray level appearance as the background. Wealso evaluate it in 3D on 60 brain tumor datasets for neurosurgical planningpurposes.
