arxiv-4200-1 | Exploring Deep and Recurrent Architectures for Optimal Control | http://arxiv.org/pdf/1311.1761v1.pdf | author:Sergey Levine category:cs.LG cs.AI cs.NE cs.RO cs.SY published:2013-11-07 summary:Sophisticated multilayer neural networks have achieved state of the artresults on multiple supervised tasks. However, successful applications of suchmultilayer networks to control have so far been limited largely to theperception portion of the control pipeline. In this paper, we explore theapplication of deep and recurrent neural networks to a continuous,high-dimensional locomotion task, where the network is used to represent acontrol policy that maps the state of the system (represented by joint angles)directly to the torques at each joint. By using a recent reinforcement learningalgorithm called guided policy search, we can successfully train neural networkcontrollers with thousands of parameters, allowing us to compare a variety ofarchitectures. We discuss the differences between the locomotion control taskand previous supervised perception tasks, present experimental resultscomparing various architectures, and discuss future directions in theapplication of techniques from deep learning to the problem of optimal control.
arxiv-4200-2 | Biometric Signature Processing & Recognition Using Radial Basis Function Network | http://arxiv.org/pdf/1311.1694v1.pdf | author:Ankit Chadha, Neha Satam, Vibha Wali category:cs.CV published:2013-11-07 summary:Automatic recognition of signature is a challenging problem which hasreceived much attention during recent years due to its many applications indifferent fields. Signature has been used for long time for verification andauthentication purpose. Earlier methods were manual but nowadays they aregetting digitized. This paper provides an efficient method to signaturerecognition using Radial Basis Function Network. The network is trained withsample images in database. Feature extraction is performed before using themfor training. For testing purpose, an image is made to undergorotation-translation-scaling correction and then given to network. The networksuccessfully identifies the original image and gives correct output for storeddatabase images also. The method provides recognition rate of approximately 80%for 200 samples.
arxiv-4200-3 | The Maximum Entropy Relaxation Path | http://arxiv.org/pdf/1311.1644v1.pdf | author:Moshe Dubiner, Matan Gavish, Yoram Singer category:cs.LG math.OC stat.ML published:2013-11-07 summary:The relaxed maximum entropy problem is concerned with finding a probabilitydistribution on a finite set that minimizes the relative entropy to a givenprior distribution, while satisfying relaxed max-norm constraints with respectto a third observed multinomial distribution. We study the entire relaxationpath for this problem in detail. We show existence and a geometric descriptionof the relaxation path. Specifically, we show that the maximum entropyrelaxation path admits a planar geometric description as an increasing,piecewise linear function in the inverse relaxation parameter. We derive fastalgorithms for tracking the path. In various realistic settings, our algorithmsrequire $O(n\log(n))$ operations for probability distributions on $n$ points,making it possible to handle large problems. Once the path has been recovered,we show that given a validation set, the family of admissible models is reducedfrom an infinite family to a small, discrete set. We demonstrate the merits ofour approach in experiments with synthetic data and discuss its potential forthe estimation of compact n-gram language models.
arxiv-4200-4 | Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics | http://arxiv.org/pdf/1311.1539v1.pdf | author:Edward Grefenstette category:cs.CL cs.LG math.CT math.LO I.2.7 published:2013-11-06 summary:This thesis is about the problem of compositionality in distributionalsemantics. Distributional semantics presupposes that the meanings of words area function of their occurrences in textual contexts. It models words asdistributions over these contexts and represents them as vectors in highdimensional spaces. The problem of compositionality for such models concernsitself with how to produce representations for larger units of text bycomposing the representations of smaller units of text. This thesis focuses on a particular approach to this compositionalityproblem, namely using the categorical framework developed by Coecke, Sadrzadeh,and Clark, which combines syntactic analysis formalisms with distributionalsemantic representations of meaning to produce syntactically motivatedcomposition operations. This thesis shows how this approach can betheoretically extended and practically implemented to produce concretecompositional distributional models of natural language semantics. Itfurthermore demonstrates that such models can perform on par with, or betterthan, other competing approaches in the field of natural language processing. There are three principal contributions to computational linguistics in thisthesis. The first is to extend the DisCoCat framework on the syntactic frontand semantic front, incorporating a number of syntactic analysis formalisms andproviding learning procedures allowing for the generation of concretecompositional distributional models. The second contribution is to evaluate themodels developed from the procedures presented here, showing that theyoutperform other compositional distributional models present in the literature.The third contribution is to show how using category theory to solve linguisticproblems forms a sound basis for research, illustrated by examples of work onthis topic, that also suggest directions for future research.
arxiv-4200-5 | Exploration in Interactive Personalized Music Recommendation: A Reinforcement Learning Approach | http://arxiv.org/pdf/1311.6355v1.pdf | author:Xinxi Wang, Yi Wang, David Hsu, Ye Wang category:cs.MM cs.IR cs.LG H.3.3; H.5.5 published:2013-11-06 summary:Current music recommender systems typically act in a greedy fashion byrecommending songs with the highest user ratings. Greedy recommendation,however, is suboptimal over the long term: it does not actively gatherinformation on user preferences and fails to recommend novel songs that arepotentially interesting. A successful recommender system must balance the needsto explore user preferences and to exploit this information for recommendation.This paper presents a new approach to music recommendation by formulating thisexploration-exploitation trade-off as a reinforcement learning task called themulti-armed bandit. To learn user preferences, it uses a Bayesian model, whichaccounts for both audio content and the novelty of recommendations. Apiecewise-linear approximation to the model and a variational inferencealgorithm are employed to speed up Bayesian inference. One additional benefitof our approach is a single unified model for both music recommendation andplaylist generation. Both simulation results and a user study indicate strongpotential for the new approach.
arxiv-4200-6 | The Squared-Error of Generalized LASSO: A Precise Analysis | http://arxiv.org/pdf/1311.0830v2.pdf | author:Samet Oymak, Christos Thrampoulidis, Babak Hassibi category:cs.IT math.IT math.OC stat.ML published:2013-11-04 summary:We consider the problem of estimating an unknown signal $x_0$ from noisylinear observations $y = Ax_0 + z\in R^m$. In many practical instances, $x_0$has a certain structure that can be captured by a structure inducing convexfunction $f(\cdot)$. For example, $\ell_1$ norm can be used to encourage asparse solution. To estimate $x_0$ with the aid of $f(\cdot)$, we consider thewell-known LASSO method and provide sharp characterization of its performance.We assume the entries of the measurement matrix $A$ and the noise vector $z$have zero-mean normal distributions with variances $1$ and $\sigma^2$respectively. For the LASSO estimator $x^*$, we attempt to calculate theNormalized Square Error (NSE) defined as $\frac{\x^*-x_0\_2^2}{\sigma^2}$ asa function of the noise level $\sigma$, the number of observations $m$ and thestructure of the signal. We show that, the structure of the signal $x_0$ andchoice of the function $f(\cdot)$ enter the error formulae through the summaryparameters $D(cone)$ and $D(\lambda)$, which are defined as the Gaussiansquared-distances to the subdifferential cone and to the $\lambda$-scaledsubdifferential, respectively. The first LASSO estimator assumes a-prioriknowledge of $f(x_0)$ and is given by $\arg\min_{x}\{{\y-Ax\_2}~\text{subjectto}~f(x)\leq f(x_0)\}$. We prove that its worst case NSE is achieved when$\sigma\rightarrow 0$ and concentrates around $\frac{D(cone)}{m-D(cone)}$.Secondly, we consider $\arg\min_{x}\{\y-Ax\_2+\lambda f(x)\}$, for some$\lambda\geq 0$. This time the NSE formula depends on the choice of $\lambda$and is given by $\frac{D(\lambda)}{m-D(\lambda)}$. We then establish a mappingbetween this and the third estimator $\arg\min_{x}\{\frac{1}{2}\y-Ax\_2^2+\lambda f(x)\}$. Finally, for a number of important structured signal classes,we translate our abstract formulae to closed-form upper bounds on the NSE.
arxiv-4200-7 | Face Recognition via Globality-Locality Preserving Projections | http://arxiv.org/pdf/1311.1279v1.pdf | author:Sheng Huang, Dan Yang, Fei Yang, Yongxin Ge, Xiaohong Zhang, Jiwen Lu category:cs.CV published:2013-11-06 summary:We present an improved Locality Preserving Projections (LPP) method, namedGloablity-Locality Preserving Projections (GLPP), to preserve both the globaland local geometric structures of data. In our approach, an additionalconstraint of the geometry of classes is imposed to the objective function ofconventional LPP for respecting some more global manifold structures. Moreover,we formulate a two-dimensional extension of GLPP (2D-GLPP) as an example toshow how to extend GLPP with some other statistical techniques. We apply ourworks to face recognition on four popular face databases, namely ORL, Yale,FERET and LFW-A databases, and extensive experimental results demonstrate thatthe considered global manifold information can significantly improve theperformance of LPP and the proposed face recognition methods outperform thestate-of-the-arts.
arxiv-4200-8 | Randomized Dimension Reduction on Massive Data | http://arxiv.org/pdf/1211.1642v2.pdf | author:Stoyan Georgiev, Sayan Mukherjee category:stat.ML stat.ME published:2012-11-07 summary:Scalability of statistical estimators is of increasing importance in modernapplications and dimension reduction is often used to extract relevantinformation from data. A variety of popular dimension reduction approaches canbe framed as symmetric generalized eigendecomposition problems. In this paperwe outline how taking into account the low rank structure assumption implicitin these dimension reduction approaches provides both computational andstatistical advantages. We adapt recent randomized low-rank approximationalgorithms to provide efficient solutions to three dimension reduction methods:Principal Component Analysis (PCA), Sliced Inverse Regression (SIR), andLocalized Sliced Inverse Regression (LSIR). A key observation in this paper isthat randomization serves a dual role, improving both computational andstatistical performance. This point is highlighted in our experiments on realand simulated data.
arxiv-4200-9 | Quality Assessment of Pixel-Level ImageFusion Using Fuzzy Logic | http://arxiv.org/pdf/1311.1223v1.pdf | author:Srinivasa Rao Dammavalam, Seetha Maddala, M. H. M. Krishna Prasad category:cs.CV published:2013-11-05 summary:Image fusion is to reduce uncertainty and minimize redundancy in the outputwhile maximizing relevant information from two or more images of a scene into asingle composite image that is more informative and is more suitable for visualperception or processing tasks like medical imaging, remote sensing, concealedweapon detection, weather forecasting, biometrics etc. Image fusion combinesregistered images to produce a high quality fused image with spatial andspectral information. The fused image with more information will improve theperformance of image analysis algorithms used in different applications. Inthis paper, we proposed a fuzzy logic method to fuse images from differentsensors, in order to enhance the quality and compared proposed method with twoother methods i.e. image fusion using wavelet transform and weighted averagediscrete wavelet transform based image fusion using genetic algorithm (hereonwards abbreviated as GA) along with quality evaluation parameters imagequality index (IQI), mutual information measure (MIM), root mean square error(RMSE), peak signal to noise ratio (PSNR), fusion factor (FF), fusion symmetry(FS) and fusion index (FI) and entropy. The results obtained from proposedfuzzy based image fusion approach improves quality of fused image as comparedto earlier reported methods, wavelet transform based image fusion and weightedaverage discrete wavelet transform based image fusion using genetic algorithm.
arxiv-4200-10 | Identifying Purpose Behind Electoral Tweets | http://arxiv.org/pdf/1311.1194v1.pdf | author:Saif M. Mohammad, Svetlana Kiritchenko, Joel Martin category:cs.CL published:2013-11-05 summary:Tweets pertaining to a single event, such as a national election, can numberin the hundreds of millions. Automatically analyzing them is beneficial in manydownstream natural language applications such as question answering andsummarization. In this paper, we propose a new task: identifying the purposebehind electoral tweets--why do people post election-oriented tweets? We showthat identifying purpose is correlated with the related phenomenon of sentimentand emotion detection, but yet significantly different. Detecting purpose has anumber of applications including detecting the mood of the electorate,estimating the popularity of policies, identifying key issues of contention,and predicting the course of events. We create a large dataset of electoraltweets and annotate a few thousand tweets for purpose. We develop a system thatautomatically classifies electoral tweets as per their purpose, obtaining anaccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-classtask (both accuracies well above the most-frequent-class baseline). Finally, weshow that resources developed for emotion detection are also helpful fordetecting purpose.
arxiv-4200-11 | Statistical Inference in Hidden Markov Models using $k$-segment Constraints | http://arxiv.org/pdf/1311.1189v1.pdf | author:Michalis K. Titsias, Christopher Yau, Christopher C. Holmes category:stat.ME cs.LG stat.ML published:2013-11-05 summary:Hidden Markov models (HMMs) are one of the most widely used statisticalmethods for analyzing sequence data. However, the reporting of output from HMMshas largely been restricted to the presentation of the most-probable (MAP)hidden state sequence, found via the Viterbi algorithm, or the sequence of mostprobable marginals using the forward-backward (F-B) algorithm. In this article,we expand the amount of information we could obtain from the posteriordistribution of an HMM by introducing linear-time dynamic programmingalgorithms that, we collectively call $k$-segment algorithms, that allow us toi) find MAP sequences, ii) compute posterior probabilities and iii) simulatesample paths conditional on a user specified number of segments, i.e.contiguous runs in a hidden state, possibly of a particular type. We illustratethe utility of these methods using simulated and real examples and highlightthe application of prospective and retrospective use of these methods forfitting HMMs or exploring existing model fits.
arxiv-4200-12 | Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages | http://arxiv.org/pdf/1311.1169v1.pdf | author:Dániel Kondor, István Csabai, László Dobos, János Szüle, Norbert Barankai, Tamás Hanyecz, Tamás Sebők, Zsófia Kallus, Gábor Vattay category:cs.CL published:2013-11-05 summary:Principal component analysis (PCA) and related techniques have beensuccessfully employed in natural language processing. Text mining applicationsin the age of the online social media (OSM) face new challenges due toproperties specific to these use cases (e.g. spelling issues specific to textsposted by users, the presence of spammers and bots, service announcements,etc.). In this paper, we employ a Robust PCA technique to separate typicaloutliers and highly localized topics from the low-dimensional structure presentin language use in online social networks. Our focus is on identifyinggeospatial features among the messages posted by the users of the Twittermicroblogging service. Using a dataset which consists of over 200 milliongeolocated tweets collected over the course of a year, we investigate whetherthe information present in word usage frequencies can be used to identifyregional features of language use and topics of interest. Using the PCA pursuitmethod, we are able to identify important low-dimensional features, whichconstitute smoothly varying functions of the geographic location.
arxiv-4200-13 | Online Learning with Multiple Operator-valued Kernels | http://arxiv.org/pdf/1311.0222v2.pdf | author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML published:2013-11-01 summary:We consider the problem of learning a vector-valued function f in an onlinelearning setting. The function f is assumed to lie in a reproducing Hilbertspace of operator-valued kernels. We describe two online algorithms forlearning f while taking into account the output structure. A first contributionis an algorithm, ONORMA, that extends the standard kernel-based online learningalgorithm NORMA from scalar-valued to operator-valued setting. We report acumulative error bound that holds both for classification and regression. Wethen define a second algorithm, MONORMA, which addresses the limitation ofpre-defining the output structure in ONORMA by learning sequentially a linearcombination of operator-valued kernels. Our experiments show that the proposedalgorithms achieve good performance results with low computational cost.
arxiv-4200-14 | Motion and audio analysis in mobile devices for remote monitoring of physical activities and user authentication | http://arxiv.org/pdf/1311.1132v1.pdf | author:Hamed Ketabdar, Jalaluddin Qureshi, Pan Hui category:cs.HC cs.CV published:2013-11-05 summary:In this article we propose the use of accelerometer embedded by default insmartphone as a cost-effective, reliable and efficient way to provide remotephysical activity monitoring for the elderly and people requiring healthcareservice. Mobile phones are regularly carried by users during their day-to-daywork routine, physical movement information can be captured by the mobile phoneaccelerometer, processed and sent to a remote server for monitoring. Theacceleration pattern can deliver information related to the pattern of physicalactivities the user is engaged in. We further show how this technique can beextended to provide implicit real-time security by analysing unexpectedmovements captured by the phone accelerometer, and automatically locking thephone in such situation to prevent unauthorised access. This technique is alsoshown to provide implicit continuous user authentication, by capturing regularuser movements such as walking, and requesting for re-authentication wheneverit detects a non-regular movement.
arxiv-4200-15 | Pseudo-likelihood methods for community detection in large sparse networks | http://arxiv.org/pdf/1207.2340v3.pdf | author:Arash A. Amini, Aiyou Chen, Peter J. Bickel, Elizaveta Levina category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH published:2012-07-10 summary:Many algorithms have been proposed for fitting network models withcommunities, but most of them do not scale well to large networks, and oftenfail on sparse networks. Here we propose a new fast pseudo-likelihood methodfor fitting the stochastic block model for networks, as well as a variant thatallows for an arbitrary degree distribution by conditioning on degrees. We showthat the algorithms perform well under a range of settings, including on verysparse networks, and illustrate on the example of a network of political blogs.We also propose spectral clustering with perturbations, a method of independentinterest, which works well on sparse networks where regular spectral clusteringfails, and use it to provide an initial value for pseudo-likelihood. We provethat pseudo-likelihood provides consistent estimates of the communities under amild condition on the starting value, for the case of a block model with twocommunities.
arxiv-4200-16 | Inverting Nonlinear Dimensionality Reduction with Scale-Free Radial Basis Function Interpolation | http://arxiv.org/pdf/1305.0258v2.pdf | author:Nathan D. Monnig, Bengt Fornberg, Francois G. Meyer category:math.NA cs.NA stat.ML published:2013-05-01 summary:Nonlinear dimensionality reduction embeddings computed from datasets do notprovide a mechanism to compute the inverse map. In this paper, we address theproblem of computing a stable inverse map to such a general bi-Lipschitz map.Our approach relies on radial basis functions (RBFs) to interpolate the inversemap everywhere on the low-dimensional image of the forward map. We demonstratethat the scale-free cubic RBF kernel performs better than the Gaussian kernel:it does not suffer from ill-conditioning, and does not require the choice of ascale. The proposed construction is shown to be similar to the Nystr\"omextension of the eigenvectors of the symmetric normalized graph Laplacianmatrix. Based on this observation, we provide a new interpretation of theNystr\"om extension with suggestions for improvement.
arxiv-4200-17 | Polyhedrons and Perceptrons Are Functionally Equivalent | http://arxiv.org/pdf/1311.1090v1.pdf | author:Daniel Crespin category:cs.NE 68T01 C.1.3; I.2.6 published:2013-11-05 summary:Mathematical definitions of polyhedrons and perceptron networks arediscussed. The formalization of polyhedrons is done in a rather traditionalway. For networks, previously proposed systems are developed. Perceptronnetworks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)are introduced. The main theme is that single output perceptron neural networksand characteristic functions of polyhedrons are one and the same class offunctions. A rigorous formulation and proof that three layers suffice isobtained. The various constructions and results are among several stepsrequired for algorithms that replace incremental and statistical learning withmore efficient, direct and exact geometric methods for calculation ofperceptron architecture and weights.
arxiv-4200-18 | Fifth-order canonical polyadic decomposition with partial symmetry via joint diagonalization for combined independent component analysis and canonical / Parallel factor analysis | http://arxiv.org/pdf/1311.1040v1.pdf | author:Xiao-Feng Gong, Cheng-Yuan Wang, Ya-Na Hao, Qiu-Hua Lin category:stat.ML cs.LG published:2013-11-05 summary:Recently, there has been a trend to combine independent component analysisand canonical / parallel factor analysis (ICA-CPA) for an enhanced robustnessfor the computation of CPA, and ICA-CPA could be further converted into theproblem of canonical polyadic decomposition (CPD) of a 5th-order partiallysymmetric tensor, by calculating the 4th-order cumulant of a trilinear mixture.In this study, we propose a new 5th-order CPD algorithm constrained withpartial symmetry using joint diagonalization. As the main steps involved in theproposed algorithm undergo no updating iterations for the loading matrices, itis much faster than the existing algorithm based on alternating least squaresand enhanced line search, and therefore could be used as a nice initializationfor the latter. Simulation results are given to examine the performance of theproposed algorithm.
arxiv-4200-19 | Correlated random features for fast semi-supervised learning | http://arxiv.org/pdf/1306.5554v2.pdf | author:Brian McWilliams, David Balduzzi, Joachim M. Buhmann category:stat.ML cs.LG published:2013-06-24 summary:This paper presents Correlated Nystrom Views (XNV), a fast semi-supervisedalgorithm for regression and classification. The algorithm draws on two mainideas. First, it generates two views consisting of computationally inexpensiverandom features. Second, XNV applies multiview regression using CanonicalCorrelation Analysis (CCA) on unlabeled data to bias the regression towardsuseful features. It has been shown that, if the views contains accurateestimators, CCA regression can substantially reduce variance with a minimalincrease in bias. Random views are justified by recent theoretical andempirical work showing that regression with random features closelyapproximates kernel regression, implying that random views can be expected tocontain accurate estimators. We show that XNV consistently outperforms astate-of-the-art algorithm for semi-supervised learning: substantiallyimproving predictive performance and reducing the variability of performance ona wide variety of real-world datasets, whilst also reducing runtime by ordersof magnitude.
arxiv-4200-20 | A Divide-and-Conquer Solver for Kernel Support Vector Machines | http://arxiv.org/pdf/1311.0914v1.pdf | author:Cho-Jui Hsieh, Si Si, Inderjit S. Dhillon category:cs.LG published:2013-11-04 summary:The kernel support vector machine (SVM) is one of the most widely usedclassification methods; however, the amount of computation required becomes thebottleneck when facing millions of samples. In this paper, we propose andanalyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In thedivision step, we partition the kernel SVM problem into smaller subproblems byclustering the data, so that each subproblem can be solved independently andefficiently. We show theoretically that the support vectors identified by thesubproblem solution are likely to be support vectors of the entire kernel SVMproblem, provided that the problem is partitioned appropriately by kernelclustering. In the conquer step, the local solutions from the subproblems areused to initialize a global coordinate descent solver, which converges quicklyas suggested by our analysis. By extending this idea, we develop a multilevelDivide-and-Conquer SVM algorithm with adaptive clustering and early predictionstrategy, which outperforms state-of-the-art methods in terms of trainingspeed, testing accuracy, and memory usage. As an example, on the covtypedataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM inobtaining the exact SVM solution (to within $10^{-6}$ relative error) whichachieves 96.15% prediction accuracy. Moreover, with our proposed earlyprediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes,which is more than 100 times faster than LIBSVM.
arxiv-4200-21 | A Comparative Study on Linguistic Feature Selection in Sentiment Polarity Classification | http://arxiv.org/pdf/1311.0833v1.pdf | author:Zitao Liu category:cs.CL published:2013-11-04 summary:Sentiment polarity classification is perhaps the most widely studied topic.It classifies an opinionated document as expressing a positive or negativeopinion. In this paper, using movie review dataset, we perform a comparativestudy with different single kind linguistic features and the combinations ofthese features. We find that the classic topic-based classifier(Naive Bayes andSupport Vector Machine) do not perform as well on sentiment polarityclassification. And we find that with some combination of different linguisticfeatures, the classification accuracy can be boosted a lot. We give somereasonable explanations about these boosting outcomes.
arxiv-4200-22 | TOP-SPIN: TOPic discovery via Sparse Principal component INterference | http://arxiv.org/pdf/1311.1406v1.pdf | author:Martin Takáč, Selin Damla Ahipaşaoğlu, Ngai-Man Cheung, Peter Richtárik category:cs.CV cs.IR cs.LG published:2013-11-04 summary:We propose a novel topic discovery algorithm for unlabeled images based onthe bag-of-words (BoW) framework. We first extract a dictionary of visual wordsand subsequently for each image compute a visual word occurrence histogram. Weview these histograms as rows of a large matrix from which we extract sparseprincipal components (PCs). Each PC identifies a sparse combination of visualwords which co-occur frequently in some images but seldom appear in others.Each sparse PC corresponds to a topic, and images whose interference with thePC is high belong to that topic, revealing the common parts possessed by theimages. We propose to solve the associated sparse PCA problems using anAlternating Maximization (AM) method, which we modify for purpose ofefficiently extracting multiple PCs in a deflation scheme. Our approach attacksthe maximization problem in sparse PCA directly and is scalable tohigh-dimensional data. Experiments on automatic topic discovery and categoryprediction demonstrate encouraging performance of our approach.
arxiv-4200-23 | Distributed Exploration in Multi-Armed Bandits | http://arxiv.org/pdf/1311.0800v1.pdf | author:Eshcar Hillel, Zohar Karnin, Tomer Koren, Ronny Lempel, Oren Somekh category:cs.LG published:2013-11-04 summary:We study exploration in Multi-Armed Bandits in a setting where $k$ playerscollaborate in order to identify an $\epsilon$-optimal arm. Our motivationcomes from recent employment of bandit algorithms in computationally intensive,large-scale applications. Our results demonstrate a non-trivial tradeoffbetween the number of arm pulls required by each of the players, and the amountof communication between them. In particular, our main result shows that byallowing the $k$ players to communicate only once, they are able to learn$\sqrt{k}$ times faster than a single player. That is, distributing learning to$k$ players gives rise to a factor $\sqrt{k}$ parallel speed-up. We complementthis result with a lower bound showing this is in general the best possible. Onthe other extreme, we present an algorithm that achieves the ideal factor $k$speed-up in learning performance, with communication only logarithmic in$1/\epsilon$.
arxiv-4200-24 | A Parallel Compressive Imaging Architecture for One-Shot Acquisition | http://arxiv.org/pdf/1311.0646v1.pdf | author:Tomas Björklund, Enrico Magli category:cs.CV astro-ph.IM published:2013-11-04 summary:A limitation of many compressive imaging architectures lies in the sequentialnature of the sensing process, which leads to long sensing times. In this paperwe present a novel architecture that uses fewer detectors than the number ofreconstructed pixels and is able to acquire the image in a single acquisition.This paves the way for the development of video architectures that acquireseveral frames per second. We specifically address the diffraction problem,showing that deconvolution normally used to recover diffraction blur can bereplaced by convolution of the sensing matrix, and how measurements of a 0/1physical sensing matrix can be converted to -1/1 compressive sensing matrixwithout any extra acquisitions. Simulations of our architecture show that theimage quality is comparable to that of a classic Compressive Imaging camera,whereas the proposed architecture avoids long acquisition times due tosequential sensing. This one-shot procedure also allows to employ a fixedsensing matrix instead of a complex device such as a Digital Micro Mirror arrayor Spatial Light Modulator. It also enables imaging at bandwidths where theseare not efficient.
arxiv-4200-25 | A Parallel SGD method with Strong Convergence | http://arxiv.org/pdf/1311.0636v1.pdf | author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan, Leon Bottou category:cs.LG cs.DC published:2013-11-04 summary:This paper proposes a novel parallel stochastic gradient descent (SGD) methodthat is obtained by applying parallel sets of SGD iterations (each setoperating on one node using the data residing in it) for finding the directionin each iteration of a batch descent method. The method has strong convergenceproperties. Experiments on datasets with high dimensional feature spaces showthe value of this method.
arxiv-4200-26 | A Gang of Bandits | http://arxiv.org/pdf/1306.0811v3.pdf | author:Nicolò Cesa-Bianchi, Claudio Gentile, Giovanni Zappella category:cs.LG cs.SI stat.ML published:2013-06-04 summary:Multi-armed bandit problems are receiving a great deal of attention becausethey adequately formalize the exploration-exploitation trade-offs arising inseveral industrially relevant applications, such as online advertisement and,more generally, recommendation systems. In many cases, however, theseapplications have a strong social component, whose integration in the banditalgorithm could lead to a dramatic performance increase. For instance, we maywant to serve content to a group of users by taking advantage of an underlyingnetwork of social relationships among them. In this paper, we introduce novelalgorithmic approaches to the solution of such networked bandit problems. Morespecifically, we design and analyze a global strategy which allocates a banditalgorithm to each network node (user) and allows it to "share" signals(contexts and payoffs) with the neghboring nodes. We then derive two morescalable variants of this strategy based on different ways of clustering thegraph nodes. We experimentally compare the algorithm and its variants tostate-of-the-art methods for contextual bandits that do not use the relationalinformation. Our experiments, carried out on synthetic and real-world datasets,show a marked increase in prediction performance obtained by exploiting thenetwork structure.
arxiv-4200-27 | Stochastic Dual Coordinate Ascent with Alternating Direction Multiplier Method | http://arxiv.org/pdf/1311.0622v1.pdf | author:Taiji Suzuki category:stat.ML published:2013-11-04 summary:We propose a new stochastic dual coordinate ascent technique that can beapplied to a wide range of regularized learning problems. Our method is basedon Alternating Direction Multiplier Method (ADMM) to deal with complexregularization functions such as structured regularizations. Although theoriginal ADMM is a batch method, the proposed method offers a stochastic updaterule where each iteration requires only one or few sample observations.Moreover, our method can naturally afford mini-batch update and it gives speedup of convergence. We show that, under mild assumptions, our method convergesexponentially. The numerical experiments show that our method actually performsefficiently.
arxiv-4200-28 | Q-Gaussian Swarm Quantum Particle Intelligence on Predicting Global Minimum of Potential Energy Function | http://arxiv.org/pdf/1311.0598v1.pdf | author:Hiqmet Kamberaj category:cs.NE published:2013-11-04 summary:We present a newly developed -Gaussian Swarm Quantum-like ParticleOptimization (q-GSQPO) algorithm to determine the global minimum of thepotential energy function. Swarm Quantum-like Particle Optimization (SQPO)algorithms have been derived using different attractive potential fields torepresent swarm particles moving in a quantum environment, where the one whichuses a harmonic oscillator potential as attractive field is considered as animproved version. In this paper, we propose a new SQPO that uses -Gaussianprobability density function for the attractive potential field (q-GSQPO)rather than Gaussian one (GSQPO) which corresponds to harmonic potential. Theperformance of the q-GSQPO is compared against the GSQPO. The new algorithmoutperforms the GSQPO on most of the time in convergence to the global optimumby increasing the efficiency of sampling the phase space and avoiding thepremature convergence to local minima. Moreover, the computational efforts werecomparable for both algorithms. We tested the algorithm to determine the lowestenergy configurations of a particle moving in a 2, 5, 10, and 50 dimensionalspaces.
arxiv-4200-29 | Anatomical Feature-guided Volumeric Registration of Multimodal Prostate MRI | http://arxiv.org/pdf/1307.1739v2.pdf | author:Xin Zhao, Arie Kaufman category:cs.CV cs.GR published:2013-07-06 summary:Radiological imaging of prostate is becoming more popular among researchersand clinicians in searching for diseases, primarily cancer. Scans might beacquired at different times, with patient movement between scans, or withdifferent equipment, resulting in multiple datasets that need to be registered.For this issue, we introduce a registration method using anatomicalfeature-guided mutual information. Prostate scans of the same patient taken inthree different orientations are first aligned for the accurate detection ofanatomical features in 3D. Then, our pipeline allows for multiple modalitiesregistration through the use of anatomical features, such as the interiorurethra of prostate and gland utricle, in a bijective way. The novelty of thisapproach is the application of anatomical features as the pre-specifiedcorresponding landmarks for prostate registration. We evaluate the registrationresults through both artificial and clinical datasets. Registration accuracy isevaluated by performing statistical analysis of local intensity differences orspatial differences of anatomical landmarks between various MR datasets.Evaluation results demonstrate that our method statistics-significantlyimproves the quality of registration. Although this strategy is tested forMRI-guided brachytherapy, the preliminary results from these experimentssuggest that it can be also applied to other settings such as transrectalultrasound-guided or CT-guided therapy, where the integration of preoperativeMRI may have a significant impact upon treatment planning and guidance.
arxiv-4200-30 | Thompson Sampling for Online Learning with Linear Experts | http://arxiv.org/pdf/1311.0468v1.pdf | author:Aditya Gopalan category:stat.ML cs.LG published:2013-11-03 summary:In this note, we present a version of the Thompson sampling algorithm for theproblem of online linear generalization with full information (i.e., theexperts setting), studied by Kalai and Vempala, 2005. The algorithm uses aGaussian prior and time-varying Gaussian likelihoods, and we show that itessentially reduces to Kalai and Vempala's Follow-the-Perturbed-Leaderstrategy, with exponentially distributed noise replaced by Gaussian noise. Thisimplies sqrt(T) regret bounds for Thompson sampling (with time-varyinglikelihood) for online learning with full information.
arxiv-4200-31 | Thompson Sampling for Complex Bandit Problems | http://arxiv.org/pdf/1311.0466v1.pdf | author:Aditya Gopalan, Shie Mannor, Yishay Mansour category:stat.ML cs.LG published:2013-11-03 summary:We consider stochastic multi-armed bandit problems with complex actions overa set of basic arms, where the decision maker plays a complex action ratherthan a basic arm in each round. The reward of the complex action is somefunction of the basic arms' rewards, and the feedback observed may notnecessarily be the reward per-arm. For instance, when the complex actions aresubsets of the arms, we may only observe the maximum reward over the chosensubset. Thus, feedback across complex actions may be coupled due to the natureof the reward function. We prove a frequentist regret bound for Thompsonsampling in a very general setting involving parameter, action and observationspaces and a likelihood function over them. The bound holds fordiscretely-supported priors over the parameter space and without additionalstructural properties such as closed-form posteriors, conjugate prior structureor independence across arms. The regret bound scales logarithmically with timebut, more importantly, with an improved constant that non-trivially capturesthe coupling across complex actions due to the structure of the rewards. Asapplications, we derive improved regret bounds for classes of complex banditproblems involving selecting subsets of arms, including the first nontrivialregret bounds for nonlinear MAX reward feedback from subsets.
arxiv-4200-32 | An Adaptive Amoeba Algorithm for Shortest Path Tree Computation in Dynamic Graphs | http://arxiv.org/pdf/1311.0460v1.pdf | author:Xiaoge Zhang, Qi Liu, Yong Hu, Felix T. S. Chan, Sankaran Mahadevan, Zili Zhang, Yong Deng category:cs.NE published:2013-11-03 summary:This paper presents an adaptive amoeba algorithm to address the shortest pathtree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weightupdates consists of three categories: edge weight increases, edge weightdecreases, the mixture of them. Existing work on this problem solve this issuethrough analyzing the nodes influenced by the edge weight updates and recomputethese affected vertices. However, when the network becomes big, the processwill become complex. The proposed method can overcome the disadvantages of theexisting approaches. The most important feature of this algorithm is itsadaptivity. When the edge weight changes, the proposed algorithm can recognizethe affected vertices and reconstruct them spontaneously. To evaluate theproposed adaptive amoeba algorithm, we compare it with the Label Settingalgorithm and Bellman-Ford algorithm. The comparison results demonstrate theeffectiveness of the proposed method.
arxiv-4200-33 | On multi-class learning through the minimization of the confusion matrix norm | http://arxiv.org/pdf/1303.4015v2.pdf | author:Sokol Koço, Cécile Capponi category:cs.LG published:2013-03-16 summary:In imbalanced multi-class classification problems, the misclassification rateas an error measure may not be a relevant choice. Several methods have beendeveloped where the performance measure retained richer information than themere misclassification rate: misclassification costs, ROC-based information,etc. Following this idea of dealing with alternate measures of performance, wepropose to address imbalanced classification problems by using a new measure tobe optimized: the norm of the confusion matrix. Indeed, recent results showthat using the norm of the confusion matrix as an error measure can be quiteinteresting due to the fine-grain informations contained in the matrix,especially in the case of imbalanced classes. Our first contribution thenconsists in showing that optimizing criterion based on the confusion matrixgives rise to a common background for cost-sensitive methods aimed at dealingwith imbalanced classes learning problems. As our second contribution, wepropose an extension of a recent multi-class boosting method --- namelyAdaBoost.MM --- to the imbalanced class problem, by greedily minimizing theempirical norm of the confusion matrix. A theoretical analysis of theproperties of the proposed method is presented, while experimental resultsillustrate the behavior of the algorithm and show the relevancy of the approachcompared to other methods.
arxiv-4200-34 | Second Croatian Computer Vision Workshop (CCVW 2013) | http://arxiv.org/pdf/1310.0319v3.pdf | author:Sven Lončarić, Siniša Šegvić category:cs.CV published:2013-10-01 summary:Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013,http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb,Croatia. Workshop was organized by the Center of Excellence for Computer Visionof the University of Zagreb.
arxiv-4200-35 | Inferring clonal evolution of tumors from single nucleotide somatic mutations | http://arxiv.org/pdf/1210.3384v4.pdf | author:Wei Jiao, Shankar Vembu, Amit G. Deshwar, Lincoln Stein, Quaid Morris category:cs.LG q-bio.PE q-bio.QM stat.ML published:2012-10-11 summary:High-throughput sequencing allows the detection and quantification offrequencies of somatic single nucleotide variants (SNV) in heterogeneous tumorcell populations. In some cases, the evolutionary history and populationfrequency of the subclonal lineages of tumor cells present in the sample can bereconstructed from these SNV frequency measurements. However, automated methodsto do this reconstruction are not available and the conditions under whichreconstruction is possible have not been described. We describe the conditions under which the evolutionary history can beuniquely reconstructed from SNV frequencies from single or multiple samplesfrom the tumor population and we introduce a new statistical model, PhyloSub,that infers the phylogeny and genotype of the major subclonal lineagesrepresented in the population of cancer cells. It uses a Bayesian nonparametricprior over trees that groups SNVs into major subclonal lineages andautomatically estimates the number of lineages and their ancestry. We samplefrom the joint posterior distribution over trees to identify evolutionaryhistories and cell population frequencies that have the highest probability ofgenerating the observed SNV frequency data. When multiple phylogenies areconsistent with a given set of SNV frequencies, PhyloSub represents theuncertainty in the tumor phylogeny using a partial order plot. Experiments on asimulated dataset and two real datasets comprising tumor samples from acutemyeloid leukemia and chronic lymphocytic leukemia patients demonstrate thatPhyloSub can infer both linear (or chain) and branching lineages and itsinferences are in good agreement with ground truth, where it is available.
arxiv-4200-36 | Data-based approximate policy iteration for nonlinear continuous-time optimal control design | http://arxiv.org/pdf/1311.0396v1.pdf | author:Biao Luo, Huai-Ning Wu, Tingwen Huang, Derong Liu category:cs.SY math.OC stat.ML published:2013-11-02 summary:This paper addresses the model-free nonlinear optimal problem withgeneralized cost functional, and a data-based reinforcement learning techniqueis developed. It is known that the nonlinear optimal control problem relies onthe solution of the Hamilton-Jacobi-Bellman (HJB) equation, which is anonlinear partial differential equation that is generally impossible to besolved analytically. Even worse, most of practical systems are too complicatedto establish their accurate mathematical model. To overcome these difficulties,we propose a data-based approximate policy iteration (API) method by using realsystem data rather than system model. Firstly, a model-free policy iterationalgorithm is derived for constrained optimal control problem and itsconvergence is proved, which can learn the solution of HJB equation and optimalcontrol policy without requiring any knowledge of system mathematical model.The implementation of the algorithm is based on the thought of actor-criticstructure, where actor and critic neural networks (NNs) are employed toapproximate the control policy and cost function, respectively. To update theweights of actor and critic NNs, a least-square approach is developed based onthe method of weighted residuals. The whole data-based API method includes twoparts, where the first part is implemented online to collect real systeminformation, and the second part is conducting offline policy iteration tolearn the solution of HJB equation and the control policy. Then, the data-basedAPI algorithm is simplified for solving unconstrained optimal control problemof nonlinear and linear systems. Finally, we test the efficiency of thedata-based API control design method on a simple nonlinear system, and furtherapply it to a rotational/translational actuator system. The simulation resultsdemonstrate the effectiveness of the proposed method.
arxiv-4200-37 | Coherence and sufficient sampling densities for reconstruction in compressed sensing | http://arxiv.org/pdf/1302.2767v2.pdf | author:Franz J. Király, Louis Theran category:cs.LG cs.IT math.AG math.IT stat.ML published:2013-02-12 summary:We give a new, very general, formulation of the compressed sensing problem interms of coordinate projections of an analytic variety, and derive sufficientsampling rates for signal reconstruction. Our bounds are linear in thecoherence of the signal space, a geometric parameter independent of thespecific signal and measurement, and logarithmic in the ambient dimension wherethe signal is presented. We exemplify our approach by deriving sufficientsampling densities for low-rank matrix completion and distance matrixcompletion which are independent of the true matrix.
arxiv-4200-38 | Multivariate Generalized Gaussian Process Models | http://arxiv.org/pdf/1311.0360v1.pdf | author:Antoni B. Chan category:stat.ML published:2013-11-02 summary:We propose a family of multivariate Gaussian process models for correlatedoutputs, based on assuming that the likelihood function takes the generic formof the multivariate exponential family distribution (EFD). We denote this modelas a multivariate generalized Gaussian process model, and derive Taylor andLaplace algorithms for approximate inference on the generic model. Byinstantiating the EFD with specific parameter functions, we obtain two novel GPmodels (and corresponding inference algorithms) for correlated outputs: 1) aVon-Mises GP for angle regression; and 2) a Dirichlet GP for regressing on themultinomial simplex.
arxiv-4200-39 | Spatio-temporal variation of conversational utterances on Twitter | http://arxiv.org/pdf/1310.2479v3.pdf | author:Christian M. Alis, May T. Lim category:physics.soc-ph cs.CL cs.SI published:2013-10-09 summary:Conversations reflect the existing norms of a language. Previously, we foundthat utterance lengths in English fictional conversations in books and movieshave shortened over a period of 200 years. In this work, we show that thisshortening occurs even for a brief period of 3 years (September 2009-December2012) using 229 million utterances from Twitter. Furthermore, the subset ofgeographically-tagged tweets from the United States show an inverse proportionbetween utterance lengths and the state-level percentage of the Blackpopulation. We argue that shortening of utterances can be explained by theincreasing usage of jargon including coined words.
arxiv-4200-40 | Parsimonious Shifted Asymmetric Laplace Mixtures | http://arxiv.org/pdf/1311.0317v1.pdf | author:Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne, Paula M. Murray category:stat.ME stat.CO stat.ML published:2013-11-01 summary:A family of parsimonious shifted asymmetric Laplace mixture models isintroduced. We extend the mixture of factor analyzers model to the shiftedasymmetric Laplace distribution. Imposing constraints on the constitute partsof the resulting decomposed component scale matrices leads to a family ofparsimonious models. An explicit two-stage parameter estimation procedure isdescribed, and the Bayesian information criterion and the integrated completedlikelihood are compared for model selection. This novel family of models isapplied to real data, where it is compared to its Gaussian analogue withinclustering and classification paradigms.
arxiv-4200-41 | Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional Regression | http://arxiv.org/pdf/1311.0274v1.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST cs.IT cs.LG math.IT stat.ME stat.TH published:2013-11-01 summary:We consider the problem of fitting the parameters of a high-dimensionallinear regression model. In the regime where the number of parameters $p$ iscomparable to or exceeds the sample size $n$, a successful approach uses an$\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately,unlike for linear estimators (e.g., ordinary least squares), nowell-established method exists to compute confidence intervals or p-values onthe basis of the Lasso estimator. Very recently, a line of work\cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed thisproblem by constructing a debiased version of the Lasso estimator. In thispaper, we study this approach for random design model, under the assumptionthat a good estimator exists for the precision matrix of the design. Ouranalysis improves over the state of the art in that it establishes nearlyoptimal \emph{average} testing power if the sample size $n$ asymptoticallydominates $s_0 (\log p)^2$, with $s_0$ being the sparsity level (number ofnon-zero coefficients). Earlier work obtains provable guarantees only for muchlarger sample size, namely it requires $n$ to asymptotically dominate $(s_0\log p)^2$. In particular, for random designs with a sparse precision matrix we show thatan estimator thereof having the required properties can be computedefficiently. Finally, we evaluate this approach on synthetic data and compareit with earlier proposals.
arxiv-4200-42 | Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture | http://arxiv.org/pdf/1305.6659v2.pdf | author:Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin category:cs.LG stat.ML published:2013-05-28 summary:This paper presents a novel algorithm, based upon the dependent Dirichletprocess mixture model (DDPMM), for clustering batch-sequential data containingan unknown number of evolving clusters. The algorithm is derived via alow-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM,and provides a hard clustering with convergence guarantees similar to those ofthe k-means algorithm. Empirical results from a synthetic test with movingGaussian clusters and a test with real ADS-B aircraft trajectory datademonstrate that the algorithm requires orders of magnitude less computationaltime than contemporary probabilistic and hard clustering algorithms, whileproviding higher accuracy on the examined datasets.
arxiv-4200-43 | Dropout Training as Adaptive Regularization | http://arxiv.org/pdf/1307.1493v2.pdf | author:Stefan Wager, Sida Wang, Percy Liang category:stat.ML cs.LG stat.ME published:2013-07-04 summary:Dropout and other feature noising schemes control overfitting by artificiallycorrupting the training data. For generalized linear models, dropout performs aform of adaptive regularization. Using this viewpoint, we show that the dropoutregularizer is first-order equivalent to an L2 regularizer applied afterscaling the features by an estimate of the inverse diagonal Fisher informationmatrix. We also establish a connection to AdaGrad, an online learningalgorithm, and find that a close relative of AdaGrad operates by repeatedlysolving linear dropout-regularized problems. By casting dropout asregularization, we develop a natural semi-supervised algorithm that usesunlabeled data to create a better adaptive regularizer. We apply this idea todocument classification tasks, and show that it consistently boosts theperformance of dropout training, improving on state-of-the-art results on theIMDB reviews dataset.
arxiv-4200-44 | Convergence Properties of Kronecker Graphical Lasso Algorithms | http://arxiv.org/pdf/1204.0585v4.pdf | author:Theodoros Tsiligkaridis, Alfred O. Hero III, Shuheng Zhou category:stat.ME stat.ML published:2012-04-03 summary:This paper studies iteration convergence of Kronecker graphical lasso(KGLasso) algorithms for estimating the covariance of an i.i.d. Gaussian randomsample under a sparse Kronecker-product covariance model and MSE convergencerates. The KGlasso model, originally called the transposable regularizedcovariance model by Allen ["Transposable regularized covariance models with anapplication to missing data imputation," Ann. Appl. Statist., vol. 4, no. 2,pp. 764-790, 2010], implements a pair of $\ell_1$ penalties on each Kroneckerfactor to enforce sparsity in the covariance estimator. The KGlasso algorithmgeneralizes Glasso, introduced by Yuan and Lin ["Model selection and estimationin the Gaussian graphical model," Biometrika, vol. 94, pp. 19-35, 2007] andBanerjee ["Model selection through sparse maximum likelihood estimation formultivariate Gaussian or binary data," J. Mach. Learn. Res., vol. 9, pp.485-516, Mar. 2008], to estimate covariances having Kronecker product form. Italso generalizes the unpenalized ML flip-flop (FF) algorithm of Dutilleul ["TheMLE algorithm for the matrix normal distribution," J. Statist. Comput. Simul.,vol. 64, pp. 105-123, 1999] and Werner ["On estimation of covariance matriceswith Kronecker product structure," IEEE Trans. Signal Process., vol. 56, no. 2,pp. 478-491, Feb. 2008] to estimation of sparse Kronecker factors. We establishthat the KGlasso iterates converge pointwise to a local maximum of thepenalized likelihood function. We derive high dimensional rates of convergenceto the true covariance as both the number of samples and the number ofvariables go to infinity. Our results establish that KGlasso has significantlyfaster asymptotic convergence than Glasso and FF. Simulations are presentedthat validate the results of our analysis.
arxiv-4200-45 | Reinforcement Learning for Matrix Computations: PageRank as an Example | http://arxiv.org/pdf/1311.2889v1.pdf | author:Vivek S. Borkar, Adwaitvedant S. Mathkar category:cs.LG cs.SI stat.ML published:2013-11-01 summary:Reinforcement learning has gained wide popularity as a technique forsimulation-driven approximate dynamic programming. A less known aspect is thatthe very reasons that make it effective in dynamic programming can also beleveraged for using it for distributed schemes for certain matrix computationsinvolving non-negative matrices. In this spirit, we propose a reinforcementlearning algorithm for PageRank computation that is fashioned after analogousschemes for approximate dynamic programming. The algorithm has the advantage ofease of distributed implementation and more importantly, of being model-free,i.e., not dependent on any specific assumptions about the transitionprobabilities in the random web-surfer model. We analyze its convergence andfinite time behavior and present some supporting numerical experiments.
arxiv-4200-46 | Iterative Bilateral Filtering of Polarimetric SAR Data | http://arxiv.org/pdf/1311.0162v1.pdf | author:Olivier D'Hondt, Stéphane Guillaso, Olaf Hellwich category:cs.CV published:2013-11-01 summary:In this paper, we introduce an iterative speckle filtering method forpolarimetric SAR (PolSAR) images based on the bilateral filter. To locallyadapt to the spatial structure of images, this filter relies on pixelsimilarities in both spatial and radiometric domains. To deal with polarimetricdata, we study the use of similarities based on a statistical distance calledKullback-Leibler divergence as well as two geodesic distances on Riemannianmanifolds. To cope with speckle, we propose to progressively refine the resultthanks to an iterative scheme. Experiments are run over synthetic andexperimental data. First, simulations are generated to study the effects offiltering parameters in terms of polarimetric reconstruction error, edgepreservation and smoothing of homogeneous areas. Comparison with other methodsshows that our approach compares well to other state of the art methods in theextraction of polarimetric information and shows superior performance for edgerestoration and noise smoothing. The filter is then applied to experimentaldata sets from ESAR and FSAR sensors (DLR) at L-band and S-band, respectively.These last experiments show the ability of the filter to restore structuressuch as buildings and roads and to preserve boundaries between regions whileachieving a high amount of smoothing in homogeneous areas.
arxiv-4200-47 | Reconstruction of Complex-Valued Fractional Brownian Motion Fields Based on Compressive Sampling and Its Application to PSF Interpolation in Weak Lensing Survey | http://arxiv.org/pdf/1311.0124v1.pdf | author:Andriyan B. Suksmono category:cs.CV astro-ph.CO published:2013-11-01 summary:A new reconstruction method of complex-valued fractional Brownian motion(CV-fBm) field based on Compressive Sampling (CS) is proposed. The decayproperty of Fourier coefficients magnitude of the fBm signals/ fields indicatesthat fBms are compressible. Therefore, a few numbers of samples will besufficient for a CS based method to reconstruct the full field. Theeffectiveness of the proposed method is showed by simulating, random sampling,and reconstructing CV-fBm fields. Performance evaluation shows advantages ofthe proposed method over boxcar filtering and thin plate methods. It is alsofound that the reconstruction performance depends on both of the fBm's Hurstparameter and the number of samples, which in fact is consistent with the CSreconstruction theory. In contrast to other fBm or fractal interpolationmethods, the proposed CS based method does not require the knowledge of fractalparameters in the reconstruction process; the inherent sparsity is justsufficient for the CS to do the reconstruction. Potential applicability of theproposed method in weak gravitational lensing survey, particularly forinterpolating non-smooth PSF (Point Spread Function) distribution representingdistortion by a turbulent field is also discussed.
arxiv-4200-48 | Structure-preserving color transformations using Laplacian commutativity | http://arxiv.org/pdf/1311.0119v1.pdf | author:Davide Eynard, Artiom Kovnatsky, Michael M. Bronstein category:cs.CV cs.GR math.SP published:2013-11-01 summary:Mappings between color spaces are ubiquitous in image processing problemssuch as gamut mapping, decolorization, and image optimization for color-blindpeople. Simple color transformations often result in information loss andambiguities (for example, when mapping from RGB to grayscale), and one wishesto find an image-specific transformation that would preserve as much aspossible the structure of the original image in the target color space. In thispaper, we propose Laplacian colormaps, a generic framework forstructure-preserving color transformations between images. We use the imageLaplacian to capture the structural information, and show that if the colortransformation between two images preserves the structure, the respectiveLaplacians have similar eigenvectors, or in other words, are approximatelyjointly diagonalizable. Employing the relation between joint diagonalizabilityand commutativity of matrices, we use Laplacians commutativity as a criterionof color mapping quality and minimize it w.r.t. the parameters of a colortransformation to achieve optimal structure preservation. We show numerousapplications of our approach, including color-to-gray conversion, gamutmapping, multispectral image fusion, and image optimization for color deficientviewers.
arxiv-4200-49 | Bayesian inference as iterated random functions with applications to sequential inference in graphical models | http://arxiv.org/pdf/1311.0072v1.pdf | author:Arash A. Amini, XuanLong Nguyen category:stat.ML math.ST stat.ME stat.TH published:2013-11-01 summary:We propose a general formalism of iterated random functions with semigroupproperty, under which exact and approximate Bayesian posterior updates can beviewed as specific instances. A convergence theory for iterated randomfunctions is presented. As an application of the general theory we analyzeconvergence behaviors of exact and approximate message-passing algorithms thatarise in a sequential change point detection problem formulated via a latentvariable directed graphical model. The sequential inference algorithm and itssupporting theory are illustrated by simulated examples.
arxiv-4200-50 | Parameterless Optimal Approximate Message Passing | http://arxiv.org/pdf/1311.0035v1.pdf | author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-10-31 summary:Iterative thresholding algorithms are well-suited for high-dimensionalproblems in sparse recovery and compressive sensing. The performance of thisclass of algorithms depends heavily on the tuning of certain thresholdparameters. In particular, both the final reconstruction error and theconvergence rate of the algorithm crucially rely on how the threshold parameteris set at each step of the algorithm. In this paper, we propose aparameter-free approximate message passing (AMP) algorithm that sets thethreshold parameter at each iteration in a fully automatic way without eitherhaving an information about the signal to be reconstructed or needing anytuning from the user. We show that the proposed method attains both the minimumreconstruction error and the highest convergence rate. Our method is based onapplying the Stein unbiased risk estimate (SURE) along with a modified gradientdescent to find the optimal threshold in each iteration. Motivated by theconnections between AMP and LASSO, it could be employed to find the solution ofthe LASSO for the optimal regularization parameter. To the best of ourknowledge, this is the first work concerning parameter tuning that obtains thefastest convergence rate with theoretical guarantees.
arxiv-4200-51 | Convergence analysis of kernel LMS algorithm with pre-tuned dictionary | http://arxiv.org/pdf/1310.8618v1.pdf | author:Jie Chen, Wei Gao, Cédric Richard, Jose-Carlos M. Bermudez category:stat.ML published:2013-10-31 summary:The kernel least-mean-square (KLMS) algorithm is an appealing tool for onlineidentification of nonlinear systems due to its simplicity and robustness. Inaddition to choosing a reproducing kernel and setting filter parameters,designing a KLMS adaptive filter requires to select a so-called dictionary inorder to get a finite-order model. This dictionary has a significant impact onperformance, and requires careful consideration. Theoretical analysis of KLMSas a function of dictionary setting has rarely, if ever, been addressed in theliterature. In an analysis previously published by the authors, the dictionaryelements were assumed to be governed by the same probability density functionof the input data. In this paper, we modify this study by considering thedictionary as part of the filter parameters to be set. This theoreticalanalysis paves the way for future investigations on KLMS dictionary design.
arxiv-4200-52 | Nonlinear unmixing of hyperspectral images using a semiparametric model and spatial regularization | http://arxiv.org/pdf/1310.8612v1.pdf | author:Jie Chen, Cédric Richard, Alfred O. Hero III category:stat.ML published:2013-10-31 summary:Incorporating spatial information into hyperspectral unmixing procedures hasbeen shown to have positive effects, due to the inherent spatial-spectralduality in hyperspectral scenes. Current research works that consider spatialinformation are mainly focused on the linear mixing model. In this paper, weinvestigate a variational approach to incorporating spatial correlation into anonlinear unmixing procedure. A nonlinear algorithm operating in reproducingkernel Hilbert spaces, associated with an $\ell_1$ local variation norm as thespatial regularizer, is derived. Experimental results, with both synthetic andreal data, illustrate the effectiveness of the proposed scheme.
arxiv-4200-53 | Spatial statistics, image analysis and percolation theory | http://arxiv.org/pdf/1310.8574v1.pdf | author:Mikhail Langovoy, Michael Habeck, Bernhard Schölkopf category:stat.AP stat.ML published:2013-10-31 summary:We develop a novel method for detection of signals and reconstruction ofimages in the presence of random noise. The method uses results frompercolation theory. We specifically address the problem of detection ofmultiple objects of unknown shapes in the case of nonparametric noise. Thenoise density is unknown and can be heavy-tailed. The objects of interest haveunknown varying intensities. No boundary shape constraints are imposed on theobjects, only a set of weak bulk conditions is required. We view the objectdetection problem as a multiple hypothesis testing for discrete statisticalinverse problems. We present an algorithm that allows to detect greyscaleobjects of various shapes in noisy images. We prove results on consistency andalgorithmic complexity of our procedures. Applications to cryo-electronmicroscopy are presented.
arxiv-4200-54 | A dependent partition-valued process for multitask clustering and time evolving network modelling | http://arxiv.org/pdf/1303.3265v2.pdf | author:Konstantina Palla, David A. Knowles, Zoubin Ghahramani category:stat.ML published:2013-03-13 summary:The fundamental aim of clustering algorithms is to partition data points. Weconsider tasks where the discovered partition is allowed to vary with somecovariate such as space or time. One approach would be to usefragmentation-coagulation processes, but these, being Markov processes, arerestricted to linear or tree structured covariate spaces. We define apartition-valued process on an arbitrary covariate space using Gaussianprocesses. We use the process to construct a multitask clustering model whichpartitions datapoints in a similar way across multiple data sources, and a timeseries model of network data which allows cluster assignments to vary overtime. We describe sampling algorithms for inference and apply our method todefining cancer subtypes based on different types of cellular characteristics,finding regulatory modules from gene expression data from multiple humanpopulations, and discovering time varying community structure in a socialnetwork.
arxiv-4200-55 | Reinforcement Learning Framework for Opportunistic Routing in WSNs | http://arxiv.org/pdf/1310.8467v1.pdf | author:G. Srinivas Rao, A. V. Ramana category:cs.NI cs.LG published:2013-10-31 summary:Routing packets opportunistically is an essential part of multihop ad hocwireless sensor networks. The existing routing techniques are not adaptiveopportunistic. In this paper we have proposed an adaptive opportunistic routingscheme that routes packets opportunistically in order to ensure that packetloss is avoided. Learning and routing are combined in the framework thatexplores the optimal routing possibilities. In this paper we implemented thisReinforced learning framework using a customer simulator. The experimentalresults revealed that the scheme is able to exploit the opportunistic tooptimize routing of packets even though the network structure is unknown.
arxiv-4200-56 | An Improved K-means Clustering Based Approach to Detect a DNA Structure in H&E Image of Mouse Tissue Reacted with CD4-Green Antigen | http://arxiv.org/pdf/1310.3399v2.pdf | author:B U V Prashanth, P Narahari Sastry, V Rajesh category:cs.CV published:2013-10-12 summary:In this manuscript we present the technique to detect and analyze the DNArich structure in Haemotoxylin & Eosin (H&E) image of a tissue treated withanti CD4 green antigen. The detection of DNA rich structure can be consideredas a detection of blue nuclei present through the biomedical signal/imageprocessing technique performed on the image of the tissue obtained by theScanning Electron Microscope(SEM). Earlier the tissue treated with the anti CD4green antigen, is stained with the H&E staining solution.
arxiv-4200-57 | Novel Factorization Strategies for Higher Order Tensors: Implications for Compression and Recovery of Multi-linear Data | http://arxiv.org/pdf/1307.0805v3.pdf | author:Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer category:cs.IT cs.CV math.IT published:2013-07-02 summary:In this paper we propose novel methods for compression and recovery ofmultilinear data under limited sampling. We exploit the recently proposedtensor- Singular Value Decomposition (t-SVD)[1], which is a group theoreticframework for tensor decomposition. In contrast to popular existing tensordecomposition techniques such as higher-order SVD (HOSVD), t-SVD has optimalityproperties similar to the truncated SVD for matrices. Based on t-SVD, we firstconstruct novel tensor-rank like measures to characterize informational andstructural complexity of multilinear data. Following that we outline acomplexity penalized algorithm for tensor completion from missing entries. Asan application, 3-D and 4-D (color) video data compression and recovery areconsidered. We show that videos with linear camera motion can be representedmore efficiently using t-SVD compared to traditional approaches based onvectorizing or flattening of the tensors. Application of the proposed tensorcompletion algorithm for video recovery from missing entries is shown to yielda superior performance over existing methods. In conclusion we point outseveral research directions and implications to online prediction ofmultilinear data.
arxiv-4200-58 | Safe and Efficient Screening For Sparse Support Vector Machine | http://arxiv.org/pdf/1310.8320v1.pdf | author:Zheng Zhao, Jun Liu category:cs.LG stat.ML published:2013-10-30 summary:Screening is an effective technique for speeding up the training process of asparse learning model by removing the features that are guaranteed to beinactive the process. In this paper, we present a efficient screening techniquefor sparse support vector machine based on variational inequality. Thetechnique is both efficient and safe.
arxiv-4200-59 | Distributed k-Means and k-Median Clustering on General Topologies | http://arxiv.org/pdf/1306.0604v3.pdf | author:Maria Florina Balcan, Steven Ehrlich, Yingyu Liang category:cs.LG cs.DC stat.ML published:2013-06-03 summary:This paper provides new algorithms for distributed clustering for two popularcenter-based objectives, k-median and k-means. These algorithms have provableguarantees and improve communication complexity over existing approaches.Following a classic approach in clustering by \cite{har2004coresets}, we reducethe problem of finding a clustering with low cost to the problem of finding acoreset of small size. We provide a distributed method for constructing aglobal coreset which improves over the previous methods by reducing thecommunication complexity, and which works over general communicationtopologies. Experimental results on large scale data sets show that thisapproach outperforms other coreset-based distributed clustering algorithms.
arxiv-4200-60 | Para-active learning | http://arxiv.org/pdf/1310.8243v1.pdf | author:Alekh Agarwal, Leon Bottou, Miroslav Dudik, John Langford category:cs.LG stat.ML published:2013-10-30 summary:Training examples are not all equally informative. Active learning strategiesleverage this observation in order to massively reduce the number of examplesthat need to be labeled. We leverage the same observation to build a genericstrategy for parallelizing learning algorithms. This strategy is effectivebecause the search for informative examples is highly parallelizable andbecause we show that its performance does not deteriorate when the siftingprocess relies on a slightly outdated model. Parallel active learning isparticularly attractive to train nonlinear models with non-linearrepresentations because there are few practical parallel learning algorithmsfor such models. We report preliminary experiments using both kernel SVMs andSGD-trained neural networks.
arxiv-4200-61 | Prediction of breast cancer recurrence using Classification Restricted Boltzmann Machine with Dropping | http://arxiv.org/pdf/1308.6324v2.pdf | author:Jakub M. Tomczak category:cs.LG published:2013-08-28 summary:In this paper, we apply Classification Restricted Boltzmann Machine(ClassRBM) to the problem of predicting breast cancer recurrence. According tothe Polish National Cancer Registry, in 2010 only, the breast cancer causedalmost 25% of all diagnosed cases of cancer in Poland. We propose how to useClassRBM for predicting breast cancer return and discovering relevant inputs(symptoms) in illness reappearance. Next, we outline a general probabilisticframework for learning Boltzmann machines with masks, which we refer to asDropping. The fashion of generating masks leads to different learning methods,i.e., DropOut, DropConnect. We propose a new method called DropPart which is ageneralization of DropConnect. In DropPart the Beta distribution instead ofBernoulli distribution in DropConnect is used. At the end, we carry out anexperiment using real-life dataset consisting of 949 cases, provided by theInstitute of Oncology Ljubljana.
arxiv-4200-62 | Compressive Optical Deflectometric Tomography: A Constrained Total-Variation Minimization Approach | http://arxiv.org/pdf/1209.0654v2.pdf | author:Adriana Gonzalez, Laurent Jacques, Christophe De Vleeschouwer, Philippe Antoine category:cs.CV math.OC published:2012-09-04 summary:Optical Deflectometric Tomography (ODT) provides an accurate characterizationof transparent materials whose complex surfaces present a real challenge formanufacture and control. In ODT, the refractive index map (RIM) of atransparent object is reconstructed by measuring light deflection undermultiple orientations. We show that this imaging modality can be made"compressive", i.e., a correct RIM reconstruction is achievable with far lessobservations than required by traditional Filtered Back Projection (FBP)methods. Assuming a cartoon-shape RIM model, this reconstruction is driven byminimizing the map Total-Variation under a fidelity constraint with theavailable observations. Moreover, two other realistic assumptions are added toimprove the stability of our approach: the map positivity and a frontiercondition. Numerically, our method relies on an accurate ODT sensing model andon a primal-dual minimization scheme, including easily the sensing operator andthe proposed RIM constraints. We conclude this paper by demonstrating the powerof our method on synthetic and experimental data under various compressivescenarios. In particular, the compressiveness of the stabilized ODT problem isdemonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of360 incident light angles for moderately noisy sensing.
arxiv-4200-63 | Tracking Deformable Parts via Dynamic Conditional Random Fields | http://arxiv.org/pdf/1311.0262v1.pdf | author:Suofei Zhang, Zhixin Sun, Xu Cheng, Zhenyang Wu category:cs.CV cs.MM published:2013-10-30 summary:Despite the success of many advanced tracking methods in this area, trackingtargets with drastic variation of appearance such as deformation, view changeand partial occlusion in video sequences is still a challenge in practicalapplications. In this letter, we take these serious tracking problems intoaccount simultaneously, proposing a dynamic graph based model to track objectand its deformable parts at multiple resolutions. The method introduces welllearned structural object detection models into object tracking applications asprior knowledge to deal with deformation and view change. Meanwhile, itexplicitly formulates partial occlusion by integrating spatial potentials andtemporal potentials with an unparameterized occlusion handling mechanism in thedynamic conditional random field framework. Empirical results demonstrate thatthe method outperforms state-of-the-art trackers on different challenging videosequences.
arxiv-4200-64 | Description and Evaluation of Semantic Similarity Measures Approaches | http://arxiv.org/pdf/1310.8059v1.pdf | author:Thabet Slimani category:cs.CL published:2013-10-30 summary:In recent years, semantic similarity measure has a great interest in SemanticWeb and Natural Language Processing (NLP). Several similarity measures havebeen developed, being given the existence of a structured knowledgerepresentation offered by ontologies and corpus which enable semanticinterpretation of terms. Semantic similarity measures compute the similaritybetween concepts/terms included in knowledge sources in order to performestimations. This paper discusses the existing semantic similarity methodsbased on structure, information content and feature approaches. Additionally,we present a critical evaluation of several categories of semantic similarityapproaches based on two standard benchmarks. The aim of this paper is to givean efficient evaluation of all these measures which help researcher andpractitioners to select the measure that best fit for their requirements.
arxiv-4200-65 | Trading USDCHF filtered by Gold dynamics via HMM coupling | http://arxiv.org/pdf/1308.0900v2.pdf | author:Donny Lee category:stat.ML cs.LG 91G99, 60J22 published:2013-08-05 summary:We devise a USDCHF trading strategy using the dynamics of gold as a filter.Our strategy involves modelling both USDCHF and gold using a coupled hiddenMarkov model (CHMM). The observations will be indicators, RSI and CCI, whichwill be used as triggers for our trading signals. Upon decoding the model ineach iteration, we can get the next most probable state and the next mostprobable observation. Hopefully by taking advantage of intermarket analysis andthe Markov property implicit in the model, trading with these most probablevalues will produce profitable results.
arxiv-4200-66 | Online Ensemble Learning for Imbalanced Data Streams | http://arxiv.org/pdf/1310.8004v1.pdf | author:Boyu Wang, Joelle Pineau category:cs.LG stat.ML published:2013-10-30 summary:While both cost-sensitive learning and online learning have been studiedextensively, the effort in simultaneously dealing with these two issues islimited. Aiming at this challenge task, a novel learning framework is proposedin this paper. The key idea is based on the fusion of online ensemblealgorithms and the state of the art batch mode cost-sensitive bagging/boostingalgorithms. Within this framework, two separately developed research areas arebridged together, and a batch of theoretically sound online cost-sensitivebagging and online cost-sensitive boosting algorithms are first proposed.Unlike other online cost-sensitive learning algorithms lacking theoreticalanalysis of asymptotic properties, the convergence of the proposed algorithmsis guaranteed under certain conditions, and the experimental evidence withbenchmark data sets also validates the effectiveness and efficiency of theproposed methods.
arxiv-4200-67 | Necessary and Sufficient Conditions for Novel Word Detection in Separable Topic Models | http://arxiv.org/pdf/1310.7994v1.pdf | author:Weicong Ding, Prakash Ishwar, Mohammad H. Rohban, Venkatesh Saligrama category:cs.LG cs.IR stat.ML published:2013-10-30 summary:The simplicial condition and other stronger conditions that imply it haverecently played a central role in developing polynomial time algorithms withprovable asymptotic consistency and sample complexity guarantees for topicestimation in separable topic models. Of these algorithms, those that relysolely on the simplicial condition are impractical while the practical onesneed stronger conditions. In this paper, we demonstrate, for the first time,that the simplicial condition is a fundamental, algorithm-independent,information-theoretic necessary condition for consistent separable topicestimation. Furthermore, under solely the simplicial condition, we present apractical quadratic-complexity algorithm based on random projections whichconsistently detects all novel words of all topics using only up tosecond-order empirical word moments. This algorithm is amenable to distributedimplementation making it attractive for 'big-data' scenarios involving anetwork of large distributed databases.
arxiv-4200-68 | Automatic Classification of Variable Stars in Catalogs with missing data | http://arxiv.org/pdf/1310.7868v1.pdf | author:Karim Pichara, Pavlos Protopapas category:astro-ph.IM cs.LG stat.ML published:2013-10-29 summary:We present an automatic classification method for astronomical catalogs withmissing data. We use Bayesian networks, a probabilistic graphical model, thatallows us to perform inference to pre- dict missing values given observed dataand dependency relationships between variables. To learn a Bayesian networkfrom incomplete data, we use an iterative algorithm that utilises samplingmethods and expectation maximization to estimate the distributions andprobabilistic dependencies of variables from data with missing values. To testour model we use three catalogs with missing data (SAGE, 2MASS and UBVI) andone complete catalog (MACHO). We examine how classification accuracy changeswhen information from missing data catalogs is included, how our methodcompares to traditional missing data approaches and at what computational cost.Integrating these catalogs with missing data we find that classification ofvariable objects improves by few percent and by 15% for quasar detection whilekeeping the computational cost the same.
arxiv-4200-69 | A comparison of bandwidth selectors for mean shift clustering | http://arxiv.org/pdf/1310.7855v1.pdf | author:José E. Chacón, Pablo Monfort category:stat.ML published:2013-10-29 summary:We explore the performance of several automatic bandwidth selectors,originally designed for density gradient estimation, as data-based proceduresfor nonparametric, modal clustering. The key tool to obtain a clustering fromdensity gradient estimators is the mean shift algorithm, which allows to obtaina partition not only of the data sample, but also of the whole space. Theresults of our simulation study suggest that most of the methods consideredhere, like cross validation and plug in bandwidth selectors, are useful forcluster analysis via the mean shift algorithm.
arxiv-4200-70 | An Unsupervised Feature Learning Approach to Improve Automatic Incident Detection | http://arxiv.org/pdf/1310.7795v1.pdf | author:Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Liao category:cs.LG published:2013-10-29 summary:Sophisticated automatic incident detection (AID) technology plays a key rolein contemporary transportation systems. Though many papers were devoted tostudy incident classification algorithms, few study investigated how to enhancefeature representation of incidents to improve AID performance. In this paper,we propose to use an unsupervised feature learning algorithm to generate higherlevel features to represent incidents. We used real incident data in theexperiments and found that effective feature mapping function can be learntfrom the data crosses the test sites. With the enhanced features, detectionrate (DR), false alarm rate (FAR) and mean time to detect (MTTD) aresignificantly improved in all of the three representative cases. This approachalso provides an alternative way to reduce the amount of labeled data, which isexpensive to obtain, required in training better incident classifiers since thefeature learning is unsupervised.
arxiv-4200-71 | Structured Optimal Transmission Control in Network-coded Two-way Relay Channels | http://arxiv.org/pdf/1310.7679v1.pdf | author:Ni Ding, Parastoo Sadeghi, Rodney A. Kennedy category:cs.SY stat.ML published:2013-10-29 summary:This paper considers a transmission control problem in network-coded two-wayrelay channels (NC-TWRC), where the relay buffers random symbol arrivals fromtwo users, and the channels are assumed to be fading. The problem is modeled bya discounted infinite horizon Markov decision process (MDP). The objective isto find a transmission control policy that minimizes the symbol delay, bufferoverflow and transmission power consumption and error rate simultaneously andin the long run. By using the concepts of submodularity, multimodularity andL-natural convexity, we study the structure of the optimal policy searched bydynamic programming (DP) algorithm. We show that the optimal transmissionpolicy is nondecreasing in queue occupancies or/and channel states undercertain conditions such as the chosen values of parameters in the MDP model,channel modeling method, modulation scheme and the preservation of stochasticdominance in the transitions of system states. The results derived in thispaper can be used to relieve the high complexity of DP and facilitate real-timecontrol.
arxiv-4200-72 | Optimization of Clustering for Clustering-based Image Denoising | http://arxiv.org/pdf/1306.2967v3.pdf | author:Mohsen Joneidi, Mostafa Sadeghi category:cs.CV published:2013-06-12 summary:In this paper, the problem of de-noising of an image contaminated withadditive white Gaussian noise (AWGN) is studied. This subject has beencontinued to be an open problem in signal processing for more than 50 years. Inthe present paper, we suggest a method based on global clustering of imageconstructing blocks. Noting that the type of clustering plays an important rolein clustering-based de-noising methods, we address two questions about theclustering. First, which parts of data should be considered for clustering?Second, what data clustering method is suitable for de-noising? Clustering isexploited to learn an over complete dictionary. By obtaining sparsedecomposition of the noisy image blocks in terms of the dictionary atoms, thede-noised version is achieved. Experimental results show that our dictionarylearning framework outperforms traditional dictionary learning methods such asK-SVD.
arxiv-4200-73 | Understanding Evolutionary Potential in Virtual CPU Instruction Set Architectures | http://arxiv.org/pdf/1309.0719v2.pdf | author:David M. Bryson, Charles Ofria category:cs.NE published:2013-09-03 summary:We investigate fundamental decisions in the design of instruction setarchitectures for linear genetic programs that are used as both model systemsin evolutionary biology and underlying solution representations in evolutionarycomputation. We subjected digital organisms with each tested architecture toseven different computational environments designed to present a range ofevolutionary challenges. Our goal was to engineer a general purposearchitecture that would be effective under a broad range of evolutionaryconditions. We evaluated six different types of architectural features for thevirtual CPUs: (1) genetic flexibility: we allowed digital organisms to moreprecisely modify the function of genetic instructions, (2) memory: we providedan increased number of registers in the virtual CPUs, (3) decoupled sensors andactuators: we separated input and output operations to enable greater controlover data flow. We also tested a variety of methods to regulate expression: (4)explicit labels that allow programs to dynamically refer to specific genomepositions, (5) position-relative search instructions, and (6) multiple new flowcontrol instructions, including conditionals and jumps. Each of these featuresalso adds complication to the instruction set and risks slowing evolution dueto epistatic interactions. Two features (multiple argument specification andseparated I/O) demonstrated substantial improvements int the majority of testenvironments. Some of the remaining tested modifications were detrimental,thought most exhibit no systematic effects on evolutionary potential,highlighting the robustness of digital evolution. Combined, these observationsenhance our understanding of how instruction architecture impacts evolutionarypotential, enabling the creation of architectures that support more rapidevolution of complex solutions to a broad range of challenges.
arxiv-4200-74 | Distributed Matrix Completion and Robust Factorization | http://arxiv.org/pdf/1107.0789v7.pdf | author:Lester Mackey, Ameet Talwalkar, Michael I. Jordan category:cs.LG cs.DS cs.NA math.NA stat.ML published:2011-07-05 summary:If learning methods are to scale to the massive sizes of modern datasets, itis essential for the field of machine learning to embrace parallel anddistributed computing. Inspired by the recent development of matrixfactorization methods with rich theory but poor computational complexity and bythe relative ease of mapping matrices onto distributed architectures, weintroduce a scalable divide-and-conquer framework for noisy matrixfactorization. We present a thorough theoretical analysis of this framework inwhich we characterize the statistical errors introduced by the "divide" stepand control their magnitude in the "conquer" step, so that the overallalgorithm enjoys high-probability estimation guarantees comparable to those ofits base algorithm. We also present experiments in collaborative filtering andvideo background modeling that demonstrate the near-linear to superlinearspeed-ups attainable with this approach.
arxiv-4200-75 | Compressed Sensing SAR Imaging with Multilook Processing | http://arxiv.org/pdf/1310.7217v1.pdf | author:Jian Fang, Zongben Xu, Bingchen Zhang, Wen Hong, Yirong Wu category:cs.IT cs.CV math.IT published:2013-10-27 summary:Multilook processing is a widely used speckle reduction approach in syntheticaperture radar (SAR) imaging. Conventionally, it is achieved by incoherentlysumming of some independent low-resolution images formulated from overlappingsubbands of the SAR signal. However, in the context of compressive sensing (CS)SAR imaging, where the samples are collected at sub-Nyquist rate, the dataspectrum is highly aliased that hinders the direct application of the existingmultilook techniques. In this letter, we propose a new CS-SAR imaging methodthat can realize multilook processing simultaneously during imagereconstruction. The main idea is to replace the SAR observation matrix by theinverse of multilook procedures, which is then combined with random samplingmatrix to yield a multilook CS-SAR observation model. Then a joint sparseregularization model, considering pixel dependency of subimages, is derived toform multilook images. The suggested SAR imaging method can not onlyreconstruct sparse scene efficiently below Nyquist rate, but is also able toachieve a comparable reduction of speckles during reconstruction. Simulationresults are finally provided to demonstrate the effectiveness of the proposedmethod.
arxiv-4200-76 | Generalized Thompson Sampling for Contextual Bandits | http://arxiv.org/pdf/1310.7163v1.pdf | author:Lihong Li category:cs.LG cs.AI stat.ML stat.OT 62L05 I.2.6 published:2013-10-27 summary:Thompson Sampling, one of the oldest heuristics for solving multi-armedbandits, has recently been shown to demonstrate state-of-the-art performance.The empirical success has led to great interests in theoretical understandingof this heuristic. In this paper, we approach this problem in a way verydifferent from existing efforts. In particular, motivated by the connectionbetween Thompson Sampling and exponentiated updates, we propose a new family ofalgorithms called Generalized Thompson Sampling in the expert-learningframework, which includes Thompson Sampling as a special case. Similar to mostexpert-learning algorithms, Generalized Thompson Sampling uses a loss functionto adjust the experts' weights. General regret bounds are derived, which arealso instantiated to two important loss functions: square loss and logarithmicloss. In contrast to existing bounds, our results apply to quite generalcontextual bandits. More importantly, they quantify the effect of the "prior"distribution on the regret bounds.
arxiv-4200-77 | Studying a Chaotic Spiking Neural Model | http://arxiv.org/pdf/1310.7115v1.pdf | author:Mohammad Alhawarat, Waleed Nazih, Mohammad Eldesouki category:cs.AI cs.NE published:2013-10-26 summary:Dynamics of a chaotic spiking neuron model are being studied mathematicallyand experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed tofurther understand the model and improve it. Chaos has many interestingproperties such as sensitivity to initial conditions, space filling, controland synchronization. As suggested by biologists, these properties may beexploited and play vital role in carrying out computational tasks in humanbrain. The NDS model has some limitations; in thus paper the model isinvestigated to overcome some of these limitations in order to enhance themodel. Therefore, the models parameters are tuned and the resulted dynamics arestudied. Also, the discretization method of the model is considered. Moreover,a mathematical analysis is carried out to reveal the underlying dynamics of themodel after tuning of its parameters. The results of the aforementioned methodsrevealed some facts regarding the NDS attractor and suggest the stabilizationof a large number of unstable periodic orbits (UPOs) which might correspond tomemories in phase space.
arxiv-4200-78 | Efficient Information Theoretic Clustering on Discrete Lattices | http://arxiv.org/pdf/1310.7114v1.pdf | author:Christian Bauckhage, Kristian Kersting category:cs.CV published:2013-10-26 summary:We consider the problem of clustering data that reside on discrete, lowdimensional lattices. Canonical examples for this setting are found in imagesegmentation and key point extraction. Our solution is based on a recentapproach to information theoretic clustering where clusters result from aniterative procedure that minimizes a divergence measure. We replace costlyprocessing steps in the original algorithm by means of convolutions. Theseallow for highly efficient implementations and thus significantly reduceruntime. This paper therefore bridges a gap between machine learning and signalprocessing.
arxiv-4200-79 | Algorithm Runtime Prediction: Methods & Evaluation | http://arxiv.org/pdf/1211.0906v2.pdf | author:Frank Hutter, Lin Xu, Holger H. Hoos, Kevin Leyton-Brown category:cs.AI cs.LG cs.PF stat.ML 68T20 I.2.8; I.2.6 published:2012-11-05 summary:Perhaps surprisingly, it is possible to predict how long an algorithm willtake to run on a previously unseen input, using machine learning techniques tobuild a model of the algorithm's runtime as a function of problem-specificinstance features. Such models have important applications to algorithmanalysis, portfolio-based algorithm selection, and the automatic configurationof parameterized algorithms. Over the past decade, a wide variety of techniqueshave been studied for building such models. Here, we describe extensions andimprovements of existing models, new families of models, and -- perhaps mostimportantly -- a much more thorough treatment of algorithm parameters as modelinputs. We also comprehensively describe new and existing features forpredicting algorithm runtime for propositional satisfiability (SAT), travellingsalesperson (TSP) and mixed integer programming (MIP) problems. We evaluatethese innovations through the largest empirical analysis of its kind, comparingto a wide range of runtime modelling techniques from the literature. Ourexperiments consider 11 algorithms and 35 instance distributions; they alsospan a very wide range of SAT, MIP, and TSP instances, with the leaststructured having been generated uniformly at random and the most structuredhaving emerged from real industrial applications. Overall, we demonstrate thatour new models yield substantially better runtime predictions than previousapproaches in terms of their generalization to new problem instances, to newalgorithms from a parameterized space, and to both simultaneously.
arxiv-4200-80 | Scaling SVM and Least Absolute Deviations via Exact Data Reduction | http://arxiv.org/pdf/1310.7048v1.pdf | author:Jie Wang, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2013-10-25 summary:The support vector machine (SVM) is a widely used method for classification.Although many efforts have been devoted to develop efficient solvers, itremains challenging to apply SVM to large-scale problems. A nice property ofSVM is that the non-support vectors have no effect on the resulting classifier.Motivated by this observation, we present fast and efficient screening rules todiscard non-support vectors by analyzing the dual problem of SVM viavariational inequalities (DVI). As a result, the number of data instances to beentered into the optimization can be substantially reduced. Some appealingfeatures of our screening method are: (1) DVI is safe in the sense that thevectors discarded by DVI are guaranteed to be non-support vectors; (2) the dataset needs to be scanned only once to run the screening, whose computationalcost is negligible compared to that of solving the SVM problem; (3) DVI isindependent of the solvers and can be integrated with any existing efficientsolvers. We also show that the DVI technique can be extended to detectnon-support vectors in the least absolute deviations regression (LAD). To thebest of our knowledge, there are currently no screening methods for LAD. Wehave evaluated DVI on both synthetic and real data sets. Experiments indicatethat DVI significantly outperforms the existing state-of-the-art screeningrules for SVM, and is very effective in discarding non-support vectors for LAD.The speedup gained by DVI rules can be up to two orders of magnitude.
arxiv-4200-81 | MLI: An API for Distributed Machine Learning | http://arxiv.org/pdf/1310.5426v2.pdf | author:Evan R. Sparks, Ameet Talwalkar, Virginia Smith, Jey Kottalam, Xinghao Pan, Joseph Gonzalez, Michael J. Franklin, Michael I. Jordan, Tim Kraska category:cs.LG cs.DC stat.ML published:2013-10-21 summary:MLI is an Application Programming Interface designed to address thechallenges of building Machine Learn- ing algorithms in a distributed settingbased on data-centric computing. Its primary goal is to simplify thedevelopment of high-performance, scalable, distributed algorithms. Our initialresults show that, relative to existing systems, this interface can be used tobuild distributed implementations of a wide variety of common Machine Learningalgorithms with minimal complexity and highly competitive performance andscalability.
arxiv-4200-82 | A feasible roadmap for unsupervised deconvolution of two-source mixed gene expressions | http://arxiv.org/pdf/1310.7033v1.pdf | author:Niya Wang, Eric P. Hoffman, Robert Clarke, Zhen Zhang, David M. Herrington, Ie-Ming Shih, Douglas A. Levine, Guoqiang Yu, Jianhua Xuan, Yue Wang category:stat.ML q-bio.GN q-bio.QM stat.AP published:2013-10-25 summary:Tissue heterogeneity is a major confounding factor in studying individualpopulations that cannot be resolved directly by global profiling. Experimentalsolutions to mitigate tissue heterogeneity are expensive, time consuming,inapplicable to existing data, and may alter the original gene expressionpatterns. Here we ask whether it is possible to deconvolute two-source mixedexpressions (estimating both proportions and cell-specific profiles) from twoor more heterogeneous samples without requiring any prior knowledge. Supportedby a well-grounded mathematical framework, we argue that both constituentproportions and cell-specific expressions can be estimated in a completelyunsupervised mode when cell-specific marker genes exist, which do not have tobe known a priori, for each of constituent cell types. We demonstrate theperformance of unsupervised deconvolution on both simulation and real geneexpression data, together with perspective discussions.
arxiv-4200-83 | Predicting the NFL using Twitter | http://arxiv.org/pdf/1310.6998v1.pdf | author:Shiladitya Sinha, Chris Dyer, Kevin Gimpel, Noah A. Smith category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-10-25 summary:We study the relationship between social media output and National FootballLeague (NFL) games, using a dataset containing messages from Twitter and NFLgame statistics. Specifically, we consider tweets pertaining to specific teamsand games in the NFL season and use them alongside statistical game data tobuild predictive models for future game outcomes (which team will win?) andsports betting outcomes (which team will win with the point spread? will thetotal points be over/under the line?). We experiment with several feature setsand find that simple features using large volumes of tweets can match or exceedthe performance of more traditional features that use game statistics.
arxiv-4200-84 | Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models | http://arxiv.org/pdf/1301.2724v2.pdf | author:Manfred Opper, Ulrich Paquet, Ole Winther category:stat.ML published:2013-01-12 summary:Expectation Propagation (EP) provides a framework for approximate inference.When the model under consideration is over a latent Gaussian field, with theapproximation being Gaussian, we show how these approximations cansystematically be corrected. A perturbative expansion is made of the exact butintractable correction, and can be applied to the model's partition functionand other moments of interest. The correction is expressed over thehigher-order cumulants which are neglected by EP's local matching of moments.Through the expansion, we see that EP is correct to first order. By consideringhigher orders, corrections of increasing polynomial complexity can be appliedto the approximation. The second order provides a correction in quadratic time,which we apply to an array of Gaussian process and Ising models. Thecorrections generalize to arbitrarily complex approximating families, which weillustrate on tree-structured Ising model approximations. Furthermore, theyprovide a polynomial-time assessment of the approximation error. We alsoprovide both theoretical and practical insights on the exactness of the EPsolution.
arxiv-4200-85 | Boosting the concordance index for survival data - a unified framework to derive and evaluate biomarker combinations | http://arxiv.org/pdf/1307.6417v2.pdf | author:Andreas Mayr, Matthias Schmid category:stat.AP stat.ME stat.ML published:2013-07-24 summary:The development of molecular signatures for the prediction of time-to-eventoutcomes is a methodologically challenging task in bioinformatics andbiostatistics. Although there are numerous approaches for the derivation ofmarker combinations and their evaluation, the underlying methodology oftensuffers from the problem that different optimization criteria are mixed duringthe feature selection, estimation and evaluation steps. This might result inmarker combinations that are only suboptimal regarding the evaluation criterionof interest. To address this issue, we propose a unified framework to deriveand evaluate biomarker combinations. Our approach is based on the concordanceindex for time-to-event data, which is a non-parametric measure to quantify thediscrimatory power of a prediction rule. Specifically, we propose acomponent-wise boosting algorithm that results in linear biomarker combinationsthat are optimal with respect to a smoothed version of the concordance index.We investigate the performance of our algorithm in a large-scale simulationstudy and in two molecular data sets for the prediction of survival in breastcancer patients. Our numerical results show that the new approach is not onlymethodologically sound but can also lead to a higher discriminatory power thantraditional approaches for the derivation of gene signatures.
arxiv-4200-86 | Gender Classification Using Gradient Direction Pattern | http://arxiv.org/pdf/1310.6808v1.pdf | author:Mohammad shahidul Islam category:cs.CV I.5.4 published:2013-10-25 summary:A novel methodology for gender classification is presented in this paper. Itextracts feature from local region of a face using gray color intensitydifference. The facial area is divided into sub-regions and GDP histogramextracted from those regions are concatenated into a single vector to representthe face. The classification accuracy obtained by using support vector machinehas outperformed all traditional feature descriptors for gender classification.It is evaluated on the images collected from FERET database and obtained veryhigh accuracy.
arxiv-4200-87 | A Tensor Approach to Learning Mixed Membership Community Models | http://arxiv.org/pdf/1302.2684v4.pdf | author:Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade category:cs.LG cs.SI stat.ML published:2013-02-12 summary:Community detection is the task of detecting hidden communities from observedinteractions. Guaranteed community detection has so far been mostly limited tomodels with non-overlapping communities such as the stochastic block model. Inthis paper, we remove this restriction, and provide guaranteed communitydetection for a family of probabilistic network models with overlappingcommunities, termed as the mixed membership Dirichlet model, first introducedby Airoldi et al. This model allows for nodes to have fractional memberships inmultiple communities and assumes that the community memberships are drawn froma Dirichlet distribution. Moreover, it contains the stochastic block model as aspecial case. We propose a unified approach to learning these models via atensor spectral decomposition method. Our estimator is based on low-ordermoment tensor of the observed network, consisting of 3-star counts. Ourlearning method is fast and is based on simple linear algebraic operations,e.g. singular value decomposition and tensor power iterations. We provideguaranteed recovery of community memberships and model parameters and present acareful finite sample analysis of our learning method. As an important specialcase, our results match the best known scaling requirements for the(homogeneous) stochastic block model.
arxiv-4200-88 | Durkheim Project Data Analysis Report | http://arxiv.org/pdf/1310.6775v1.pdf | author:Linas Vepstas category:cs.AI cs.CL cs.LG published:2013-10-24 summary:This report describes the suicidality prediction models created under theDARPA DCAPS program in association with the Durkheim Project[http://durkheimproject.org/]. The models were built primarily fromunstructured text (free-format clinician notes) for several hundred patientrecords obtained from the Veterans Health Administration (VHA). The models wereconstructed using a genetic programming algorithm applied to bag-of-words andbag-of-phrases datasets. The influence of additional structured data wasexplored but was found to be minor. Given the small dataset size,classification between cohorts was high fidelity (98%). Cross-validationsuggests these models are reasonably predictive, with an accuracy of 50% to 69%on five rotating folds, with ensemble averages of 58% to 67%. One particularlynoteworthy result is that word-pairs can dramatically improve classificationaccuracy; but this is the case only when one of the words in the pair isalready known to have a high predictive value. By contrast, the set of allpossible word-pairs does not improve on a simple bag-of-words model.
arxiv-4200-89 | Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive Writing for Linking Identities | http://arxiv.org/pdf/1310.6772v1.pdf | author:Thamar Solorio, Ragib Hasan, Mainul Mizan category:cs.CL cs.CR cs.CY published:2013-10-24 summary:This paper describes the corpus of sockpuppet cases we gathered fromWikipedia. A sockpuppet is an online user account created with a fake identityfor the purpose of covering abusive behavior and/or subverting the editingregulation process. We used a semi-automated method for crawling and curating adataset of real sockpuppet investigation cases. To the best of our knowledge,this is the first corpus available on real-world deceptive writing. We describethe process for crawling the data and some preliminary results that can be usedas baseline for benchmarking research. The dataset will be released under aCreative Commons license from our project website: http://docsig.cis.uab.edu.
arxiv-4200-90 | Combining Structured and Unstructured Randomness in Large Scale PCA | http://arxiv.org/pdf/1310.6304v2.pdf | author:Nikos Karampatziakis, Paul Mineiro category:cs.LG published:2013-10-23 summary:Principal Component Analysis (PCA) is a ubiquitous tool with manyapplications in machine learning including feature construction, subspaceembedding, and outlier detection. In this paper, we present an algorithm forcomputing the top principal components of a dataset with a large number of rows(examples) and columns (features). Our algorithm leverages both structured andunstructured random projections to retain good accuracy while beingcomputationally efficient. We demonstrate the technique on the winningsubmission the KDD 2010 Cup.
arxiv-4200-91 | Pseudo vs. True Defect Classification in Printed Circuits Boards using Wavelet Features | http://arxiv.org/pdf/1310.6654v1.pdf | author:Sahil Sikka, Karan Sikka, M. K. Bhuyan, Yuji Iwahori category:cs.CV published:2013-10-24 summary:In recent years, Printed Circuit Boards (PCB) have become the backbone of alarge number of consumer electronic devices leading to a surge in theirproduction. This has made it imperative to employ automatic inspection systemsto identify manufacturing defects in PCB before they are installed in therespective systems. An important task in this regard is the classification ofdefects as either true or pseudo defects, which decides if the PCB is to bere-manufactured or not. This work proposes a novel approach to detect mostcommon defects in the PCBs. The problem has been approached by employing highlydiscriminative features based on multi-scale wavelet transform, which arefurther boosted by using a kernalized version of the support vector machines(SVM). A real world printed circuit board dataset has been used forquantitative analysis. Experimental results demonstrated the efficacy of theproposed method.
arxiv-4200-92 | Active Learning of Linear Embeddings for Gaussian Processes | http://arxiv.org/pdf/1310.6740v1.pdf | author:Roman Garnett, Michael A. Osborne, Philipp Hennig category:stat.ML cs.LG 68T05 published:2013-10-24 summary:We propose an active learning method for discovering low-dimensionalstructure in high-dimensional Gaussian process (GP) tasks. Such problems areincreasingly frequent and important, but have hitherto presented severepractical difficulties. We further introduce a novel technique forapproximately marginalizing GP hyperparameters, yielding marginal predictionsrobust to hyperparameter mis-specification. Our method offers an efficientmeans of performing GP regression, quadrature, or Bayesian optimization inhigh-dimensional spaces.
arxiv-4200-93 | Mean Field Bayes Backpropagation: scalable training of multilayer neural networks with binary weights | http://arxiv.org/pdf/1310.1867v4.pdf | author:Daniel Soudry, Ron Meir category:stat.ML published:2013-10-07 summary:Significant success has been reported recently using deep neural networks forclassification. Such large networks can be computationally intensive, evenafter training is over. Implementing these trained networks in hardware chipswith a limited precision of synaptic weights may improve their speed and energyefficiency by several orders of magnitude, thus enabling their integration intosmall and low-power electronic devices. With this motivation, we develop acomputationally efficient learning algorithm for multilayer neural networkswith binary weights, assuming all the hidden neurons have a fan-out of one.This algorithm, derived within a Bayesian probabilistic online setting, isshown to work well for both synthetic and real-world problems, performingcomparably to algorithms with real-valued weights, while retainingcomputational tractability.
arxiv-4200-94 | Sparse Predictive Structure of Deconvolved Functional Brain Networks | http://arxiv.org/pdf/1310.6547v1.pdf | author:Tommaso Furlanello, Marco Cristoforetti, Cesare Furlanello, Giuseppe Jurman category:q-bio.NC q-bio.QM stat.ML published:2013-10-24 summary:The functional and structural representation of the brain as a complexnetwork is marked by the fact that the comparison of noisy and intrinsicallycorrelated high-dimensional structures between experimental conditions orgroups shuns typical mass univariate methods. Furthermore most networkestimation methods cannot distinguish between real and spurious correlationarising from the convolution due to nodes' interaction, which thus introducesadditional noise in the data. We propose a machine learning pipeline aimed atidentifying multivariate differences between brain networks associated todifferent experimental conditions. The pipeline (1) leverages the deconvolvedindividual contribution of each edge and (2) maps the task into a sparseclassification problem in order to construct the associated "sparse deconvolvedpredictive network", i.e., a graph with the same nodes of those compared butwhose edge weights are defined by their relevance for out of sample predictionsin classification. We present an application of the proposed method by decodingthe covert attention direction (left or right) based on the single-trialfunctional connectivity matrix extracted from high-frequencymagnetoencephalography (MEG) data. Our results demonstrate how networkdeconvolution matched with sparse classification methods outperforms typicalapproaches for MEG decoding.
arxiv-4200-95 | Randomized co-training: from cortical neurons to machine learning and back again | http://arxiv.org/pdf/1310.6536v1.pdf | author:David Balduzzi category:cs.LG q-bio.NC stat.ML published:2013-10-24 summary:Despite its size and complexity, the human cortex exhibits strikinganatomical regularities, suggesting there may simple meta-algorithms underlyingcortical learning and computation. We expect such meta-algorithms to be ofinterest since they need to operate quickly, scalably and effectively withlittle-to-no specialized assumptions. This note focuses on a specific question: How can neurons use vast quantitiesof unlabeled data to speed up learning from the comparatively rare labelsprovided by reward systems? As a partial answer, we propose randomizedco-training as a biologically plausible meta-algorithm satisfying the aboverequirements. As evidence, we describe a biologically-inspired algorithm,Correlated Nystrom Views (XNV) that achieves state-of-the-art performance insemi-supervised learning, and sketch work in progress on a neuronalimplementation.
arxiv-4200-96 | Universalities of Reproducing Kernels Revisited | http://arxiv.org/pdf/1310.5543v2.pdf | author:Benxun Wang, Haizhang Zhang category:stat.ML published:2013-10-21 summary:Kernel methods have been widely applied to machine learning and otherquestions of approximating an unknown function from its finite sample data. Toensure arbitrary accuracy of such approximation, various denseness conditionsare imposed on the selected kernel. This note contributes to the study ofuniversal, characteristic, and $C_0$-universal kernels. We first give simpleand direct description of the difference and relation among these three kindsof universalities of kernels. We then focus on translation-invariant andweighted polynomial kernels. A simple and shorter proof of the knowncharacterization of characteristic translation-invariant kernels will bepresented. The main purpose of the note is to give a delicate discussion on theuniversalities of weighted polynomial kernels.
arxiv-4200-97 | Fast 3D Salient Region Detection in Medical Images using GPUs | http://arxiv.org/pdf/1310.6736v1.pdf | author:Rahul Thota, Sharan Vaswani, Amit Kale, Nagavijayalakshmi Vydyanathan category:cs.CV published:2013-10-24 summary:Automated detection of visually salient regions is an active area of researchin computer vision. Salient regions can serve as inputs for object detectors aswell as inputs for region based registration algorithms. In this paper weconsider the problem of speeding up computationally intensive bottom-up salientregion detection in 3D medical volumes.The method uses the Kadir Bradyformulation of saliency. We show that in the vicinity of a salient region,entropy is a monotonically increasing function of the degree of overlap of acandidate window with the salient region. This allows us to initialize a sparseseed-point grid as the set of tentative salient region centers and iterativelyconverge to the local entropy maxima, thereby reducing the computationcomplexity compared to the Kadir Brady approach of performing this computationat every point in the image. We propose two different approaches for achievingthis. The first approach involves evaluating entropy in the four quadrantsaround the seed point and iteratively moving in the direction that increasesentropy. The second approach we propose makes use of mean shift trackingframework to affect entropy maximizing moves. Specifically, we propose the useof uniform pmf as the target distribution to seek high entropy regions. Wedemonstrate the use of our algorithm on medical volumes for left ventricledetection in PET images and tumor localization in brain MR sequences.
arxiv-4200-98 | Can Facial Uniqueness be Inferred from Impostor Scores? | http://arxiv.org/pdf/1310.6376v1.pdf | author:Abhishek Dutta, Raymond Veldhuis, Luuk Spreeuwers category:cs.CV published:2013-10-23 summary:In Biometrics, facial uniqueness is commonly inferred from impostorsimilarity scores. In this paper, we show that such uniqueness measures arehighly unstable in the presence of image quality variations like pose, noiseand blur. We also experimentally demonstrate the instability of a recentlyintroduced impostor-based uniqueness measure of [Klare and Jain 2013] whensubject to poor quality facial images.
arxiv-4200-99 | Provable Bounds for Learning Some Deep Representations | http://arxiv.org/pdf/1310.6343v1.pdf | author:Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma category:cs.LG cs.AI stat.ML published:2013-10-23 summary:We give algorithms with provable guarantees that learn a class of deep netsin the generative model view popularized by Hinton and others. Our generativemodel is an $n$ node multilayer neural net that has degree at most $n^{\gamma}$for some $\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Ouralgorithm learns {\em almost all} networks in this class with polynomialrunning time. The sample complexity is quadratic or cubic depending upon thedetails of the model. The algorithm uses layerwise learning. It is based upon a novel idea ofobserving correlations among features and using these to infer the underlyingedge structure via a global graph recovery procedure. The analysis of thealgorithm reveals interesting structure of neural networks with random edgeweights.
arxiv-4200-100 | Risk aversion as an evolutionary adaptation | http://arxiv.org/pdf/1310.6338v1.pdf | author:Arend Hintze, Randal S. Olson, Christoph Adami, Ralph Hertwig category:q-bio.PE cs.GT cs.NE published:2013-10-23 summary:Risk aversion is a common behavior universal to humans and animals alike.Economists have traditionally defined risk preferences by the curvature of theutility function. Psychologists and behavioral economists also make use ofconcepts such as loss aversion and probability weighting to model riskaversion. Neurophysiological evidence suggests that loss aversion has itsorigins in relatively ancient neural circuitries (e.g., ventral striatum).Could there thus be an evolutionary origin to risk avoidance? We study thisquestion by evolving strategies that adapt to play the equivalent mean payoffgamble. We hypothesize that risk aversion in the equivalent mean payoff gambleis beneficial as an adaptation to living in small groups, and find that apreference for risk averse strategies only evolves in small populations of lessthan 1,000 individuals, while agents exhibit no such strategy preference inlarger populations. Further, we discover that risk aversion can also evolve inlarger populations, but only when the population is segmented into small groupsof around 150 individuals. Finally, we observe that risk aversion only evolveswhen the gamble is a rare event that has a large impact on the individual'sfitness. These findings align with earlier reports that humans lived in smallgroups for a large portion of their evolutionary history. As such, we suggestthat rare, high-risk, high-payoff events such as mating and mate competitioncould have driven the evolution of risk averse behavior in humans living insmall groups.
arxiv-4200-101 | Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery EEG in Rehabilitation Training | http://arxiv.org/pdf/1310.6288v1.pdf | author:Hao Zhang, Liqing Zhang category:stat.ML cs.AI cs.LG published:2013-10-23 summary:Current studies about motor imagery based rehabilitation training systems forstroke subjects lack an appropriate analytic method, which can achieve aconsiderable classification accuracy, at the same time detects gradual changesof imagery patterns during rehabilitation process and disinters potentialmechanisms about motor function recovery. In this study, we propose an adaptiveboosting algorithm based on the cortex plasticity and spectral band shifts.This approach models the usually predetermined spatial-spectral configurationsin EEG study into variable preconditions, and introduces a new heuristic ofstochastic gradient boost for training base learners under these preconditions.We compare our proposed algorithm with commonly used methods on datasetscollected from 2 months' clinical experiments. The simulation resultsdemonstrate the effectiveness of the method in detecting the variations ofstroke patients' EEG patterns. By chronologically reorganizing the weightparameters of the learned additive model, we verify the spatial compensatorymechanism on impaired cortex and detect the changes of accentuation bands inspectral domain, which may contribute important prior knowledge forrehabilitation practice.
arxiv-4200-102 | A Ray-based Approach for Boundary Estimation of Fiber Bundles Derived from Diffusion Tensor Imaging | http://arxiv.org/pdf/1310.6092v1.pdf | author:Miriam H. A. Bauer, Sebastiano Barbieri, Jan Klein, Jan Egger, Daniela Kuhnt, Bernd Freisleben, Horst K. Hahn, Christopher Nimsky category:cs.CV published:2013-10-23 summary:Diffusion Tensor Imaging (DTI) is a non-invasive imaging technique thatallows estimation of the location of white matter tracts in-vivo, based on themeasurement of water diffusion properties. For each voxel, a second-ordertensor can be calculated by using diffusion-weighted sequences (DWI) that aresensitive to the random motion of water molecules. Given at least 6diffusion-weighted images with different gradients and one unweighted image,the coefficients of the symmetric diffusion tensor matrix can be calculated.Deriving the eigensystem of the tensor, the eigenvectors and eigenvalues can becalculated to describe the three main directions of diffusion and itsmagnitude. Using DTI data, fiber bundles can be determined, to gain informationabout eloquent brain structures. Especially in neurosurgery, information aboutlocation and dimension of eloquent structures like the corticospinal tract orthe visual pathways is of major interest. Therefore, the fiber bundle boundaryhas to be determined. In this paper, a novel ray-based approach for boundaryestimation of tubular structures is presented.
arxiv-4200-103 | Inferring Fitness in Finite Populations with Moran-like dynamics | http://arxiv.org/pdf/1303.4566v3.pdf | author:Marc Harper category:math.DS cs.NE q-bio.PE 91A22 published:2013-03-19 summary:Biological fitness is not an observable quantity and must be inferred frompopulation dynamics. Bayesian inference applied to the Moran process andvariants yields a robust inference method that can infer fitness in populationsevolving via a Moran dynamic and generalizations. Information about fitness isderived solely from birth-events in birth-death and death-birth processes inwhich selection acts proportionally to fitness, which allows the method to beapplied to populations on a network where the network itself may be changing intime. Populations may also be allowed to change size while still allowingestimates for fitness to be inferred.
arxiv-4200-104 | Optimally fuzzy temporal memory | http://arxiv.org/pdf/1211.5189v2.pdf | author:Karthik H. Shankar, Marc W. Howard category:cs.AI cs.LG published:2012-11-22 summary:Any learner with the ability to predict the future of a structuredtime-varying signal must maintain a memory of the recent past. If the signalhas a characteristic timescale relevant to future prediction, the memory can bea simple shift register---a moving window extending into the past, requiringstorage resources that linearly grows with the timescale to be represented.However, an independent general purpose learner cannot a priori know thecharacteristic prediction-relevant timescale of the signal. Moreover, manynaturally occurring signals show scale-free long range correlations implyingthat the natural prediction-relevant timescale is essentially unbounded. Hencethe learner should maintain information from the longest possible timescaleallowed by resource availability. Here we construct a fuzzy memory system thatoptimally sacrifices the temporal accuracy of information in a scale-freefashion in order to represent prediction-relevant information fromexponentially long timescales. Using several illustrative examples, wedemonstrate the advantage of the fuzzy memory system over a shift register intime series forecasting of natural signals. When the available storageresources are limited, we suggest that a general purpose learner would bebetter off committing to such a fuzzy memory system.
arxiv-4200-105 | Multiple Kernel Learning for Brain-Computer Interfacing | http://arxiv.org/pdf/1310.6067v1.pdf | author:Wojciech Samek, Alexander Binder, Klaus-Robert Müller category:stat.ML published:2013-10-22 summary:Combining information from different sources is a common way to improveclassification accuracy in Brain-Computer Interfacing (BCI). For instance, insmall sample settings it is useful to integrate data from other subjects orsessions in order to improve the estimation quality of the spatial filters orthe classifier. Since data from different subjects may show large variability,it is crucial to weight the contributions according to importance. Manymulti-subject learning algorithms determine the optimal weighting in a separatestep by using heuristics, however, without ensuring that the selected weightsare optimal with respect to classification. In this work we apply MultipleKernel Learning (MKL) to this problem. MKL has been widely used for featurefusion in computer vision and allows to simultaneously learn the classifier andthe optimal weighting. We compare the MKL method to two baseline approaches andinvestigate the reasons for performance improvement.
arxiv-4200-106 | Skin Segmentation based Elastic Bunch Graph Matching for efficient multiple Face Recognition | http://arxiv.org/pdf/1310.6066v1.pdf | author:Sayantan Sarkar category:cs.CV published:2013-10-22 summary:This paper is aimed at developing and combining different algorithms for facedetection and face recognition to generate an efficient mechanism that candetect and recognize the facial regions of input image. For the detection offace from complex region, skin segmentation isolates the face-like regions in acomplex image and following operations of morphology and template matchingrejects false matches to extract facial region. For the recognition of theface, the image database is now converted into a database of facial segments.Hence, implementing the technique of Elastic Bunch Graph matching (EBGM) afterskin segmentation generates Face Bunch Graphs that acutely represents thefeatures of an individual face enhances the quality of the training set. Thisincreases the matching probability significantly.
arxiv-4200-107 | Word Spotting in Cursive Handwritten Documents using Modified Character Shape Codes | http://arxiv.org/pdf/1310.6063v1.pdf | author:Sayantan Sarkar category:cs.CV published:2013-10-22 summary:There is a large collection of Handwritten English paper documents ofHistorical and Scientific importance. But paper documents are not recognizeddirectly by computer. Hence the closest way of indexing these documents is bystoring their document digital image. Hence a large database of document imagescan replace the paper documents. But the document and data corresponding toeach image cannot be directly recognized by the computer. This paper applies the technique of word spotting using Modified CharacterShape Code to Handwritten English document images for quick and efficient querysearch of words on a database of document images. It is different from otherWord Spotting techniques as it implements two level of selection for wordsegments to match search query. First based on word size and then based oncharacter shape code of query. It makes the process faster and more efficientand reduces the need of multiple pre-processing.
arxiv-4200-108 | Combined l_1 and greedy l_0 penalized least squares for linear model selection | http://arxiv.org/pdf/1310.6062v1.pdf | author:Piotr Pokarowski, Jan Mielniczuk category:stat.ML published:2013-10-22 summary:We introduce a computationally effective algorithm for a linear modelselection consisting of three steps: screening--ordering--selection (SOS).Screening of predictors is based on the thresholded Lasso that is l_1 penalizedleast squares. The screened predictors are then fitted using least squares (LS)and ordered with respect to their t statistics. Finally, a model is selectedusing greedy generalized information criterion (GIC) that is l_0 penalized LSin a nested family induced by the ordering. We give non-asymptotic upper boundson error probability of each step of the SOS algorithm in terms of bothpenalties. Then we obtain selection consistency for different (n, p) scenariosunder conditions which are needed for screening consistency of the Lasso. Forthe traditional setting (n >p) we give Sanov-type bounds on the errorprobabilities of the ordering--selection algorithm. Its surprising consequenceis that the selection error of greedy GIC is asymptotically not larger than ofexhaustive GIC. We also obtain new bounds on prediction and estimation errorsfor the Lasso which are proved in parallel for the algorithm used in practiceand its formal version.
arxiv-4200-109 | Improvement of Automatic Hemorrhages Detection Methods Using Shapes Recognition | http://arxiv.org/pdf/1310.5999v1.pdf | author:Nidhal Khdhair El Abbadi, Enas Hamood Al Saadi category:cs.CV published:2013-10-22 summary:Diabetic Retinopathy is a medical condition where the retina is damagedbecause fluid leaks from blood vessels into the retina. The presence ofhemorrhages in the retina is the earliest symptom of diabetic retinopathy. Thenumber and shape of hemorrhages is used to indicate the severity of thedisease. Early automated hemorrhage detection can help reduce the incidence ofblindness. This paper introduced new method depending on the hemorrhage shapeto detect the dot hemorrhage (DH), its number, and size at early stage, thiscan be achieved by reducing the retinal image details. Detection and recognizethe DH by following three sequential steps, removing the fovea, removing thevasculature and recognize DH by determining the circularity for all the objectsin the image, finally determine the shape factor which is related to DHrecognition, this stage strengthens the recognition process. The proposedmethod recognizes and separates all the DH.
arxiv-4200-110 | Fusion of Hyperspectral and Panchromatic Images using Spectral Uumixing Results | http://arxiv.org/pdf/1310.5965v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2013-10-22 summary:Hyperspectral imaging, due to providing high spectral resolution images, isone of the most important tools in the remote sensing field. Because oftechnological restrictions hyperspectral sensors has a limited spatialresolution. On the other hand panchromatic image has a better spatialresolution. Combining this information together can provide a betterunderstanding of the target scene. Spectral unmixing of mixed pixels inhyperspectral images results in spectral signature and abundance fractions ofendmembers but gives no information about their location in a mixed pixel. Inthis paper we have used spectral unmixing results of hyperspectral images andsegmentation results of panchromatic image for data fusion. The proposed methodhas been applied on simulated data using AVRIS Indian Pines datasets. Resultsshow that this method can effectively combine information in hyperspectral andpanchromatic images.
arxiv-4200-111 | Improving the methods of email classification based on words ontology | http://arxiv.org/pdf/1310.5963v1.pdf | author:Foruzan Kiamarzpour, Rouhollah Dianat, Mohammad bahrani, Mehdi Sadeghzadeh category:cs.IR cs.CL published:2013-10-22 summary:The Internet has dramatically changed the relationship among people and theirrelationships with others people and made the valuable information availablefor the users. Email is the service, which the Internet provides today for itsown users; this service has attracted most of the users' attention due to thelow cost. Along with the numerous benefits of Email, one of the weaknesses ofthis service is that the number of received emails is continually beingenhanced, thus the ways are needed to automatically filter these disturbingletters. Most of these filters utilize a combination of several techniques suchas the Black or white List, using the keywords and so on in order to identifythe spam more accurately In this paper, we introduce a new method to classifythe spam. We are seeking to increase the accuracy of Email classification bycombining the output of several decision trees and the concept of ontology.
arxiv-4200-112 | Generic identification of binary-valued hidden Markov processes | http://arxiv.org/pdf/1101.3712v6.pdf | author:Alexander Schönhuth category:math.ST math.AG stat.ML stat.TH published:2011-01-19 summary:The generic identification problem is to decide whether a stochastic process$(X_t)$ is a hidden Markov process and if yes to infer its parameters for allbut a subset of parametrizations that form a lower-dimensional subvariety inparameter space. Partial answers so far available depend on extra assumptionson the processes, which are usually centered around stationarity. Here wepresent a general solution for binary-valued hidden Markov processes. Ourapproach is rooted in algebraic statistics hence it is geometric in nature. Wefind that the algebraic varieties associated with the probability distributionsof binary-valued hidden Markov processes are zero sets of determinantalequations which draws a connection to well-studied objects from algebra. As aconsequence, our solution allows for algorithmic implementation based onelementary (linear) algebraic routines.
arxiv-4200-113 | PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification | http://arxiv.org/pdf/1202.6228v6.pdf | author:Emilie Morvant, Sokol Koço, Liva Ralaivola category:stat.ML cs.LG published:2012-02-28 summary:In this work, we propose a PAC-Bayes bound for the generalization risk of theGibbs classifier in the multi-class classification framework. The novelty ofour work is the critical use of the confusion matrix of a classifier as anerror measure; this puts our contribution in the line of work aiming at dealingwith performance measure that are richer than mere scalar criterion such as themisclassification rate. Thanks to very recent and beautiful results on matrixconcentration inequalities, we derive two bounds showing that the trueconfusion risk of the Gibbs classifier is upper-bounded by its empirical riskplus a term depending on the number of training examples in each class. To thebest of our knowledge, this is the first PAC-Bayes bounds based on confusionmatrices.
arxiv-4200-114 | Sparse Representation of a Polytope and Recovery of Sparse Signals and Low-rank Matrices | http://arxiv.org/pdf/1306.1154v2.pdf | author:T. Tony Cai, Anru Zhang category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-06-05 summary:This paper considers compressed sensing and affine rank minimization in bothnoiseless and noisy cases and establishes sharp restricted isometry conditionsfor sparse signal and low-rank matrix recovery. The analysis relies on a keytechnical tool which represents points in a polytope by convex combinations ofsparse vectors. The technique is elementary while leads to sharp results. It is shown that for any given constant $t\ge {4/3}$, in compressed sensing$\delta_{tk}^A < \sqrt{(t-1)/t}$ guarantees the exact recovery of all $k$sparse signals in the noiseless case through the constrained $\ell_1$minimization, and similarly in affine rank minimization$\delta_{tr}^\mathcal{M}< \sqrt{(t-1)/t}$ ensures the exact reconstruction ofall matrices with rank at most $r$ in the noiseless case via the constrainednuclear norm minimization. Moreover, for any $\epsilon>0$,$\delta_{tk}^A<\sqrt{\frac{t-1}{t}}+\epsilon$ is not sufficient to guaranteethe exact recovery of all $k$-sparse signals for large $k$. Similar result alsoholds for matrix recovery. In addition, the conditions $\delta_{tk}^A <\sqrt{(t-1)/t}$ and $\delta_{tr}^\mathcal{M}< \sqrt{(t-1)/t}$ are also shown tobe sufficient respectively for stable recovery of approximately sparse signalsand low-rank matrices in the noisy case.
arxiv-4200-115 | RANSAC: Identification of Higher-Order Geometric Features and Applications in Humanoid Robot Soccer | http://arxiv.org/pdf/1310.5781v1.pdf | author:Madison Flannery, Shannon Fenn, David Budden category:cs.RO cs.AI cs.CV published:2013-10-22 summary:The ability for an autonomous agent to self-localise is directly proportionalto the accuracy and precision with which it can perceive salient featureswithin its local environment. The identification of such features byrecognising geometric profile allows robustness against lighting variations,which is necessary in most industrial robotics applications. This paper detailsa framework by which the random sample consensus (RANSAC) algorithm, oftenapplied to parameter fitting in linear models, can be extended to identifyhigher-order geometric features. Goalpost identification within humanoid robotsoccer is investigated as an application, with the developed system yielding anorder-of-magnitude improvement in classification performance relative to atraditional histogramming methodology.
arxiv-4200-116 | Contextual Hypergraph Modelling for Salient Object Detection | http://arxiv.org/pdf/1310.5767v1.pdf | author:Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton van den Hengel category:cs.CV published:2013-10-22 summary:Salient object detection aims to locate objects that capture human attentionwithin images. Previous approaches often pose this as a problem of imagecontrast analysis. In this work, we model an image as a hypergraph thatutilizes a set of hyperedges to capture the contextual properties of imagepixels or regions. As a result, the problem of salient object detection becomesone of finding salient vertices and hyperedges in the hypergraph. The mainadvantage of hypergraph modeling is that it takes into account each pixel's (orregion's) affinity with its neighborhood as well as its separation from imagebackground. Furthermore, we propose an alternative approach based oncenter-versus-surround contextual contrast analysis, which performs salientobject detection by optimizing a cost-sensitive support vector machine (SVM)objective function. Experimental results on four challenging datasetsdemonstrate the effectiveness of the proposed approaches against thestate-of-the-art approaches to salient object detection.
arxiv-4200-117 | Determination, Calculation and Representation of the Upper and Lower Sealing Zones During Virtual Stenting of Aneurysms | http://arxiv.org/pdf/1310.5755v1.pdf | author:Jan Egger, Miriam H. A. Bauer, Stefan Großkopf, Christina Biermann, Bernd Freisleben, Christopher Nimsky category:cs.CV physics.med-ph q-bio.TO published:2013-10-21 summary:In this contribution, a novel method for stent simulation in preoperativecomputed tomography angiography (CTA) acquisitions of patients is presentedwhere the sealing zones are automatically calculated and visualized. The methodis eligible for non-bifurcated and bifurcated stents (Y-stents). Results of theproposed stent simulation with an automatic calculation of the sealing zonesfor specific diseases (abdominal aortic aneurysms (AAA), thoracic aorticaneurysms (TAA), iliac aneurysms) are presented. The contribution is organizedas follows. Section 2 presents the proposed approach. In Section 3,experimental results are discussed. Section 4 concludes the contribution andoutlines areas for future work.
arxiv-4200-118 | A Kernel for Hierarchical Parameter Spaces | http://arxiv.org/pdf/1310.5738v1.pdf | author:Frank Hutter, Michael A. Osborne category:stat.ML cs.LG published:2013-10-21 summary:We define a family of kernels for mixed continuous/discrete hierarchicalparameter spaces and show that they are positive definite.
arxiv-4200-119 | Distributed parameter estimation of discrete hierarchical models via marginal likelihoods | http://arxiv.org/pdf/1310.5666v1.pdf | author:Helene Massam, Nanwei Wang category:stat.ML published:2013-10-21 summary:We consider discrete graphical models Markov with respect to a graph $G$ andpropose two distributed marginal methods to estimate the maximum likelihoodestimate of the canonical parameter of the model. Both methods are based on arelaxation of the marginal likelihood obtained by considering the density ofthe variables represented by a vertex $v$ of $G$ and a neighborhood. The twomethods differ by the size of the neighborhood of $v$. We show that theestimates are consistent and that those obtained with the larger neighborhoodhave smaller asymptotic variance than the ones obtained through the smallerneighborhood.
arxiv-4200-120 | Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization | http://arxiv.org/pdf/1302.4385v2.pdf | author:Nicolas Gillis, Robert Luce category:stat.ML math.OC published:2013-02-18 summary:Nonnegative matrix factorization (NMF) has been shown recently to betractable under the separability assumption, under which all the columns of theinput data matrix belong to the convex cone generated by only a few of thesecolumns. Bittorf, Recht, R\'e and Tropp (`Factoring nonnegative matrices withlinear programs', NIPS 2012) proposed a linear programming (LP) model, referredto as Hottopixx, which is robust under any small perturbation of the inputmatrix. However, Hottopixx has two important drawbacks: (i) the input matrixhas to be normalized, and (ii) the factorization rank has to be known inadvance. In this paper, we generalize Hottopixx in order to resolve these twodrawbacks, that is, we propose a new LP model which does not requirenormalization and detects the factorization rank automatically. Moreover, thenew LP model is more flexible, significantly more tolerant to noise, and caneasily be adapted to handle outliers and other noise models. Finally, we showon several synthetic datasets that it outperforms Hottopixx while competingfavorably with two state-of-the-art methods.
arxiv-4200-121 | Devnagari Handwritten Numeral Recognition using Geometric Features and Statistical Combination Classifier | http://arxiv.org/pdf/1310.5619v1.pdf | author:Vikas J. Dongre, Vijay H. Mankar category:cs.CV published:2013-10-21 summary:This paper presents a Devnagari Numerical recognition method based onstatistical discriminant functions. 17 geometric features based on pixelconnectivity, lines, line directions, holes, image area, perimeter,eccentricity, solidity, orientation etc. are used for representing thenumerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear,Diagquadratic and Mahalanobis distance are used for classification. 1500handwritten numerals are used for training. Another 1500 handwritten numeralsare used for testing. Experimental results show that Linear, Quadratic andMahalanobis discriminant functions provide better results. Results of thesethree Discriminants are fed to a majority voting type Combination classifier.It is found that Combination classifier offers better results over individualclassifiers.
arxiv-4200-122 | Least Squares Revisited: Scalable Approaches for Multi-class Prediction | http://arxiv.org/pdf/1310.1949v2.pdf | author:Alekh Agarwal, Sham M. Kakade, Nikos Karampatziakis, Le Song, Gregory Valiant category:cs.LG stat.ML published:2013-10-07 summary:This work provides simple algorithms for multi-class (and multi-label)prediction in settings where both the number of examples n and the datadimension d are relatively large. These robust and parameter free algorithmsare essentially iterative least-squares updates and very versatile both intheory and in practice. On the theoretical front, we present several variantswith convergence guarantees. Owing to their effective use of second-orderstructure, these algorithms are substantially better than first-order methodsin many practical scenarios. On the empirical side, we present a scalablestagewise variant of our approach, which achieves dramatic computationalspeedups over popular optimization packages such as Liblinear and Vowpal Wabbiton standard datasets (MNIST and CIFAR-10), while attaining state-of-the-artaccuracies.
arxiv-4200-123 | Convex Discriminative Multitask Clustering | http://arxiv.org/pdf/1303.2130v2.pdf | author:Xiao-Lei Zhang category:cs.LG published:2013-03-08 summary:Multitask clustering tries to improve the clustering performance of multipletasks simultaneously by taking their relationship into account. Most existingmultitask clustering algorithms fall into the type of generative clustering,and none are formulated as convex optimization problems. In this paper, wepropose two convex Discriminative Multitask Clustering (DMTC) algorithms toaddress the problems. Specifically, we first propose a Bayesian DMTC framework.Then, we propose two convex DMTC objectives within the framework. The firstone, which can be seen as a technical combination of the convex multitaskfeature learning and the convex Multiclass Maximum Margin Clustering (M3C),aims to learn a shared feature representation. The second one, which can beseen as a combination of the convex multitask relationship learning and M3C,aims to learn the task relationship. The two objectives are solved in a uniformprocedure by the efficient cutting-plane algorithm. Experimental results on atoy problem and two benchmark datasets demonstrate the effectiveness of theproposed algorithms.
arxiv-4200-124 | Towards Application of the RBNK Model | http://arxiv.org/pdf/1310.5568v1.pdf | author:Larry Bull category:cs.CE cs.NE published:2013-10-21 summary:The computational modeling of genetic regulatory networks is now commonplace, either by fitting a system to experimental data or by exploring thebehaviour of abstract systems with the aim of identifying underlyingprinciples. This paper presents an approach to the latter, considering theresponse to environmental changes of a well-known model placed upon tunablefitness landscapes. The effects on genome size and gene connectivity areexplored.
arxiv-4200-125 | Ship Detection and Segmentation using Image Correlation | http://arxiv.org/pdf/1310.5542v1.pdf | author:Alexander Kadyrov, Hui Yu, Honghai Liu category:cs.CV published:2013-10-21 summary:There have been intensive research interests in ship detection andsegmentation due to high demands on a wide range of civil applications in thelast two decades. However, existing approaches, which are mainly based onstatistical properties of images, fail to detect smaller ships and boats.Specifically, known techniques are not robust enough in view of inevitablesmall geometric and photometric changes in images consisting of ships. In thispaper a novel approach for ship detection is proposed based on correlation ofmaritime images. The idea comes from the observation that a fine pattern of thesea surface changes considerably from time to time whereas the ship appearancebasically keeps unchanged. We want to examine whether the images have a commonunaltered part, a ship in this case. To this end, we developed a method -Focused Correlation (FC) to achieve robustness to geometric distortions of theimage content. Various experiments have been conducted to evaluate theeffectiveness of the proposed approach.
arxiv-4200-126 | Stable and robust sampling strategies for compressive imaging | http://arxiv.org/pdf/1210.2380v3.pdf | author:Felix Krahmer, Rachel Ward category:cs.CV cs.IT math.IT math.NA published:2012-10-08 summary:In many signal processing applications, one wishes to acquire images that aresparse in transform domains such as spatial finite differences or waveletsusing frequency domain samples. For such applications, overwhelming empiricalevidence suggests that superior image reconstruction can be obtained throughvariable density sampling strategies that concentrate on lower frequencies. Thewavelet and Fourier transform domains are not incoherent because low-orderwavelets and low-order frequencies are correlated, so compressive sensingtheory does not immediately imply sampling strategies and reconstructionguarantees. In this paper we turn to a more refined notion of coherence -- theso-called local coherence -- measuring for each sensing vector separately howcorrelated it is to the sparsity basis. For Fourier measurements and Haarwavelet sparsity, the local coherence can be controlled and bounded explicitly,so for matrices comprised of frequencies sampled from a suitable inverse squarepower-law density, we can prove the restricted isometry property withnear-optimal embedding dimensions. Consequently, the variable-density samplingstrategy we provide allows for image reconstructions that are stable tosparsity defects and robust to measurement noise. Our results cover bothreconstruction by $\ell_1$-minimization and by total variation minimization.The local coherence framework developed in this paper should be of independentinterest in sparse recovery problems more generally, as it implies that foroptimal sparse recovery results, it suffices to have bounded \emph{average}coherence from sensing basis to sparsity basis -- as opposed to bounded maximalcoherence -- as long as the sampling strategy is adapted accordingly.
arxiv-4200-127 | Multi-Task Regularization with Covariance Dictionary for Linear Classifiers | http://arxiv.org/pdf/1310.5393v1.pdf | author:Fanyi Xiao, Ruikun Luo, Zhiding Yu category:cs.LG published:2013-10-21 summary:In this paper we propose a multi-task linear classifier learning problemcalled D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covarianceshared by all tasks to do multi-task knowledge transfer among different tasks.We formally define the learning problem of D-SVM and show two interpretationsof this problem, from both the probabilistic and kernel perspectives. From theprobabilistic perspective, we show that our learning formulation is actually aMAP estimation on all optimization variables. We also show its equivalence to amultiple kernel learning problem in which one is trying to find a re-weightingkernel for features from a dictionary of basis (despite the fact that onlylinear classifiers are learned). Finally, we describe an alternativeoptimization scheme to minimize the objective function and present empiricalstudies to valid our algorithm.
arxiv-4200-128 | Bayesian Extensions of Kernel Least Mean Squares | http://arxiv.org/pdf/1310.5347v1.pdf | author:Il Memming Park, Sohan Seth, Steven Van Vaerenbergh category:stat.ML cs.LG published:2013-10-20 summary:The kernel least mean squares (KLMS) algorithm is a computationally efficientnonlinear adaptive filtering method that "kernelizes" the celebrated (linear)least mean squares algorithm. We demonstrate that the least mean squaresalgorithm is closely related to the Kalman filtering, and thus, the KLMS can beinterpreted as an approximate Bayesian filtering method. This allows us tosystematically develop extensions of the KLMS by modifying the underlyingstate-space and observation models. The resulting extensions introduce manydesirable properties such as "forgetting", and the ability to learn fromdiscrete data, while retaining the computational simplicity and time complexityof the original algorithm.
arxiv-4200-129 | Graph-Based Approaches to Clustering Network-Constrained Trajectory Data | http://arxiv.org/pdf/1310.5249v1.pdf | author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:cs.LG published:2013-10-19 summary:Clustering trajectory data attracted considerable attention in the last fewyears. Most of prior work assumed that moving objects can move freely in aneuclidean space and did not consider the eventual presence of an underlyingroad network and its influence on evaluating the similarity betweentrajectories. In this paper, we present an approach to clustering suchnetwork-constrained trajectory data. More precisely we aim at discoveringgroups of road segments that are often travelled by the same trajectories. Toachieve this end, we model the interactions between segments w.r.t. theirsimilarity as a weighted graph to which we apply a community detectionalgorithm to discover meaningful clusters. We showcase our proposition throughexperimental results obtained on synthetic datasets.
arxiv-4200-130 | Image Restoration using Total Variation with Overlapping Group Sparsity | http://arxiv.org/pdf/1310.3447v2.pdf | author:Jun Liu, Ting-Zhu Huang, Ivan W. Selesnick, Xiao-Guang Lv, Po-Yu Chen category:cs.CV math.NA published:2013-10-13 summary:Image restoration is one of the most fundamental issues in imaging science.Total variation (TV) regularization is widely used in image restorationproblems for its capability to preserve edges. In the literature, however, itis also well known for producing staircase-like artifacts. Usually, thehigh-order total variation (HTV) regularizer is an good option except itsover-smoothing property. In this work, we study a minimization problem wherethe objective includes an usual $l_2$ data-fidelity term and an overlappinggroup sparsity total variation regularizer which can avoid staircase effect andallow edges preserving in the restored image. We also proposed a fast algorithmfor solving the corresponding minimization problem and compare our method withthe state-of-the-art TV based methods and HTV based method. The numericalexperiments illustrate the efficiency and effectiveness of the proposed methodin terms of PSNR, relative error and computing time.
arxiv-4200-131 | Advances in Hyperspectral Image Classification: Earth monitoring with statistical learning methods | http://arxiv.org/pdf/1310.5107v1.pdf | author:Gustavo Camps-Valls, Devis Tuia, Lorenzo Bruzzone, Jón Atli Benediktsson category:cs.CV published:2013-10-18 summary:Hyperspectral images show similar statistical properties to natural grayscaleor color photographic images. However, the classification of hyperspectralimages is more challenging because of the very high dimensionality of thepixels and the small number of labeled examples typically available forlearning. These peculiarities lead to particular signal processing problems,mainly characterized by indetermination and complex manifolds. The framework ofstatistical learning has gained popularity in the last decade. New methods havebeen presented to account for the spatial homogeneity of images, to includeuser's interaction via active learning, to take advantage of the manifoldstructure with semisupervised learning, to extract and encode invariances, orto adapt classifiers and image representations to unseen yet similar scenes.This tutuorial reviews the main advances for hyperspectral remote sensing imageclassification through illustrative examples.
arxiv-4200-132 | Duality between subgradient and conditional gradient methods | http://arxiv.org/pdf/1211.6302v3.pdf | author:Francis Bach category:cs.LG math.OC stat.ML published:2012-11-27 summary:Given a convex optimization problem and its dual, there are many possiblefirst-order algorithms. In this paper, we show the equivalence between mirrordescent algorithms and algorithms generalizing the conditional gradient method.This is done through convex duality, and implies notably that for certainproblems, such as for supervised machine learning problems with non-smoothlosses or problems regularized by non-smooth regularizers, the primalsubgradient method and the dual conditional gradient method are formallyequivalent. The dual interpretation leads to a form of line search for mirrordescent, as well as guarantees of convergence for primal-dual certificates.
arxiv-4200-133 | Regularization in Relevance Learning Vector Quantization Using l one Norms | http://arxiv.org/pdf/1310.5095v1.pdf | author:Martin Riedel, Marika Kästner, Fabrice Rossi, Thomas Villmann category:stat.ML cs.LG published:2013-10-18 summary:We propose in this contribution a method for l one regularization inprototype based relevance learning vector quantization (LVQ) for sparserelevance profiles. Sparse relevance profiles in hyperspectral data analysisfade down those spectral bands which are not necessary for classification. Inparticular, we consider the sparsity in the relevance profile enforced by LASSOoptimization. The latter one is obtained by a gradient learning scheme using adifferentiable parametrized approximation of the $l_{1}$-norm, which has anupper error bound. We extend this regularization idea also to the matrixlearning variant of LVQ as the natural generalization of relevance learning.
arxiv-4200-134 | Kernel Multivariate Analysis Framework for Supervised Subspace Learning: A Tutorial on Linear and Kernel Multivariate Methods | http://arxiv.org/pdf/1310.5089v1.pdf | author:Jerónimo Arenas-García, Kaare Brandt Petersen, Gustavo Camps-Valls, Lars Kai Hansen category:stat.ML cs.LG published:2013-10-18 summary:Feature extraction and dimensionality reduction are important tasks in manyfields of science dealing with signal processing and analysis. The relevance ofthese techniques is increasing as current sensory devices are developed withever higher resolution, and problems involving multimodal data sources becomemore common. A plethora of feature extraction methods are available in theliterature collectively grouped under the field of Multivariate Analysis (MVA).This paper provides a uniform treatment of several methods: Principal ComponentAnalysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis(CCA) and Orthonormalized PLS (OPLS), as well as their non-linear extensionsderived by means of the theory of reproducing kernel Hilbert spaces. We alsoreview their connections to other methods for classification and statisticaldependence estimation, and introduce some recent developments to deal with theextreme cases of large-scale and low-sized problems. To illustrate the wideapplicability of these methods in both classification and regression problems,we analyze their performance in a benchmark of publicly available data sets,and pay special attention to specific real applications involving audioprocessing for music genre prediction and hyperspectral satellite images forEarth and climate monitoring.
arxiv-4200-135 | On the Suitable Domain for SVM Training in Image Coding | http://arxiv.org/pdf/1310.5082v1.pdf | author:Gustavo Camps-Valls, Juan Gutiérrez, Gabriel Gómez-Pérez, Jesús Malo category:cs.CV cs.LG stat.ML published:2013-10-18 summary:Conventional SVM-based image coding methods are founded on independentlyrestricting the distortion in every image coefficient at some particular imagerepresentation. Geometrically, this implies allowing arbitrary signaldistortions in an $n$-dimensional rectangle defined by the$\varepsilon$-insensitivity zone in each dimension of the selected imagerepresentation domain. Unfortunately, not every image representation domain iswell-suited for such a simple, scalar-wise, approach because statistical and/orperceptual interactions between the coefficients may exist. These interactionsimply that scalar approaches may induce distortions that do not follow theimage statistics and/or are perceptually annoying. Taking into account theserelations would imply using non-rectangular $\varepsilon$-insensitivity regions(allowing coupled distortions in different coefficients), which is beyond theconventional SVM formulation. In this paper, we report a condition on the suitable domain for developingefficient SVM image coding schemes. We analytically demonstrate that no lineardomain fulfills this condition because of the statistical and perceptualinter-coefficient relations that exist in these domains. This theoreticalresult is experimentally confirmed by comparing SVM learning in previouslyreported linear domains and in a recently proposed non-linear perceptual domainthat simultaneously reduces the statistical and perceptual relations (so it iscloser to fulfilling the proposed condition). These results highlight therelevance of an appropriate choice of the image representation before SVMlearning.
arxiv-4200-136 | Robustness of Random Forest-based gene selection methods | http://arxiv.org/pdf/1305.4525v3.pdf | author:Miron B. Kursa category:cs.LG q-bio.QM published:2013-05-20 summary:Gene selection is an important part of microarray data analysis because itprovides information that can lead to a better mechanistic understanding of aninvestigated phenomenon. At the same time, gene selection is very difficultbecause of the noisy nature of microarray data. As a consequence, geneselection is often performed with machine learning methods. The Random Forestmethod is particularly well suited for this purpose. In this work, fourstate-of-the-art Random Forest-based feature selection methods were compared ina gene selection context. The analysis focused on the stability of selectionbecause, although it is necessary for determining the significance of results,it is often ignored in similar studies. The comparison of post-selection accuracy in the validation of Random Forestclassifiers revealed that all investigated methods were equivalent in thiscontext. However, the methods substantially differed with respect to the numberof selected genes and the stability of selection. Of the analysed methods, theBoruta algorithm predicted the most genes as potentially important. The post-selection classifier error rate, which is a frequently used measure,was found to be a potentially deceptive measure of gene selection quality. Whenthe number of consistently selected genes was considered, the Boruta algorithmwas clearly the best. Although it was also the most computationally intensivemethod, the Boruta algorithm's computational demands could be reduced to levelscomparable to those of other algorithms by replacing the Random Forestimportance with a comparable measure from Random Ferns (a similar butsimplified classifier). Despite their design assumptions, the minimal optimalselection methods, were found to select a high fraction of false positives.
arxiv-4200-137 | Distributional semantics beyond words: Supervised learning of analogy and paraphrase | http://arxiv.org/pdf/1310.5042v1.pdf | author:Peter D. Turney category:cs.LG cs.AI cs.CL cs.IR published:2013-10-18 summary:There have been several efforts to extend distributional semantics beyondindividual words, to measure the similarity of word pairs, phrases, andsentences (briefly, tuples; ordered sets of words, contiguous ornoncontiguous). One way to extend beyond words is to compare two tuples using afunction that combines pairwise similarities between the component words in thetuples. A strength of this approach is that it works with both relationalsimilarity (analogy) and compositional similarity (paraphrase). However, pastwork required hand-coding the combination function for different tasks. Themain contribution of this paper is that combination functions are generated bysupervised learning. We achieve state-of-the-art results in measuringrelational similarity between word pairs (SAT analogies and SemEval~2012 Task2) and measuring compositional similarity between noun-modifier phrases andunigrams (multiple-choice paraphrase questions).
arxiv-4200-138 | Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear Spectral Penalties | http://arxiv.org/pdf/1310.4977v1.pdf | author:Marco Signoretto, Lieven De Lathauwer, Johan A. K. Suykens category:cs.LG published:2013-10-18 summary:We present a general framework to learn functions in tensor productreproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on anovel representer theorem suitable for existing as well as new spectralpenalties for tensors. When the functions in the TP-RKHS are defined on theCartesian product of finite discrete sets, in particular, our main problemformulation admits as a special case existing tensor completion problems. Otherspecial cases include transfer learning with multimodal side information andmultilinear multitask learning. For the latter case, our kernel-based view isinstrumental to derive nonlinear extensions of existing model classes. We givea novel algorithm and show in experiments the usefulness of the proposedextensions.
arxiv-4200-139 | A Logic-based Approach for Recognizing Textual Entailment Supported by Ontological Background Knowledge | http://arxiv.org/pdf/1310.4938v1.pdf | author:Andreas Wotzlaw, Ravi Coote category:cs.CL cs.AI cs.LO published:2013-10-18 summary:We present the architecture and the evaluation of a new system forrecognizing textual entailment (RTE). In RTE we want to identify automaticallythe type of a logical relation between two input texts. In particular, we areinterested in proving the existence of an entailment between them. We conceiveour system as a modular environment allowing for a high-coverage syntactic andsemantic text analysis combined with logical inference. For the syntactic andsemantic analysis we combine a deep semantic analysis with a shallow onesupported by statistical models in order to increase the quality and theaccuracy of results. For RTE we use logical inference of first-order employingmodel-theoretic techniques and automated reasoning tools. The inference issupported with problem-relevant background knowledge extracted automaticallyand on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, orother, more experimental sources with, e.g., manually defined presuppositionresolutions, or with axiomatized general and common sense knowledge. Theresults show that fine-grained and consistent knowledge coming from diversesources is a necessary condition determining the correctness and traceabilityof results.
arxiv-4200-140 | Text Classification For Authorship Attribution Analysis | http://arxiv.org/pdf/1310.4909v1.pdf | author:M. Sudheep Elayidom, Chinchu Jose, Anitta Puthussery, Neenu K Sasi category:cs.DL cs.CL cs.LG published:2013-10-18 summary:Authorship attribution mainly deals with undecided authorship of literarytexts. Authorship attribution is useful in resolving issues like uncertainauthorship, recognize authorship of unknown texts, spot plagiarism so on.Statistical methods can be used to set apart the approach of an authornumerically. The basic methodologies that are made use in computationalstylometry are word length, sentence length, vocabulary affluence, frequenciesetc. Each author has an inborn style of writing, which is particular tohimself. Statistical quantitative techniques can be used to differentiate theapproach of an author in a numerical way. The problem can be broken down intothree sub problems as author identification, author characterization andsimilarity detection. The steps involved are pre-processing, extractingfeatures, classification and author identification. For this differentclassifiers can be used. Here fuzzy learning classifier and SVM are used. Afterauthor identification the SVM was found to have more accuracy than Fuzzyclassifier. Later combined the classifiers to obtain a better accuracy whencompared to individual SVM and fuzzy classifier.
arxiv-4200-141 | Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution | http://arxiv.org/pdf/1310.4891v1.pdf | author:Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian C. Lovell category:cs.CV published:2013-10-18 summary:Recent advances in computer vision and machine learning suggest that a widerange of problems can be addressed more appropriately by consideringnon-Euclidean geometry. In this paper we explore sparse dictionary learningover the space of linear subspaces, which form Riemannian structures known asGrassmann manifolds. To this end, we propose to embed Grassmann manifolds intothe space of symmetric matrices by an isometric mapping, which enables us todevise a closed-form solution for updating a Grassmann dictionary, atom byatom. Furthermore, to handle non-linearity in data, we propose a kernelisedversion of the dictionary learning algorithm. Experiments on severalclassification tasks (face recognition, action recognition, dynamic textureclassification) show that the proposed approach achieves considerableimprovements in discrimination accuracy, in comparison to state-of-the-artmethods such as kernelised Affine Hull Method and graph-embedding Grassmanndiscriminant analysis.
arxiv-4200-142 | Analyzing Big Data with Dynamic Quantum Clustering | http://arxiv.org/pdf/1310.2700v2.pdf | author:M. Weinstein, F. Meirer, A. Hume, Ph. Sciau, G. Shaked, R. Hofstetter, E. Persi, A. Mehta, D. Horn category:cs.LG published:2013-10-10 summary:How does one search for a needle in a multi-dimensional haystack withoutknowing what a needle is and without knowing if there is one in the haystack?This kind of problem requires a paradigm shift - away from hypothesis drivensearches of the data - towards a methodology that lets the data speak foritself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is apowerful visual method that works with big, high-dimensional data. It exploitsvariations of the density of the data (in feature space) and unearths subsetsof the data that exhibit correlations among all the measured variables. Theoutcome of a DQC analysis is a movie that shows how and why sets of data-pointsare eventually classified as members of simple clusters or as members of - whatwe call - extended structures. This allows DQC to be successfully used in anon-conventional exploratory mode where one searches data for unexpectedinformation without the need to model the data. We show how this works for big,complex, real-world datasets that come from five distinct fields: i.e., x-raynano-chemistry, condensed matter, biology, seismology and finance. Thesestudies show how DQC excels at uncovering unexpected, small - but meaningful -subsets of the data that contain important information. We also establish animportant new result: namely, that big, complex datasets often containinteresting structures that will be missed by many conventional clusteringtechniques. Experience shows that these structures appear frequently enoughthat it is crucial to know they can exist, and that when they do, they encodeimportant hidden information. In short, we not only demonstrate that DQC can beflexibly applied to datasets that present significantly different challenges,we also show how a simple analysis can be used to look for the needle in thehaystack, determine what it is, and find what this means.
arxiv-4200-143 | Fine-grained Categorization -- Short Summary of our Entry for the ImageNet Challenge 2012 | http://arxiv.org/pdf/1310.4759v1.pdf | author:Christoph Göring, Alexander Freytag, Erik Rodner, Joachim Denzler category:cs.CV published:2013-10-17 summary:In this paper, we tackle the problem of visual categorization of dog breeds,which is a surprisingly challenging task due to simultaneously present lowinterclass distances and high intra-class variances. Our approach combinesseveral techniques well known in our community but often not utilized forfine-grained recognition: (1) automatic segmentation, (2) efficient part detection, and (3) combinationof multiple features. In particular, we demonstrate that a simple head detectorembedded in an off-the-shelf recognition pipeline can improve recognitionaccuracy quite significantly, highlighting the importance of part features forfine-grained recognition tasks. Using our approach, we achieved a 24.59% meanaverage precision performance on the Stanford dog dataset.
arxiv-4200-144 | Calibration of an Articulated Camera System with Scale Factor Estimation | http://arxiv.org/pdf/1310.4713v1.pdf | author:Junzhou Chen, Kin Hong Wong category:cs.CV cs.CG published:2013-10-17 summary:Multiple Camera Systems (MCS) have been widely used in many visionapplications and attracted much attention recently. There are two principletypes of MCS, one is the Rigid Multiple Camera System (RMCS); the other is theArticulated Camera System (ACS). In a RMCS, the relative poses (relative 3-Dposition and orientation) between the cameras are invariant. While, in an ACS,the cameras are articulated through movable joints, the relative pose betweenthem may change. Therefore, through calibration of an ACS we want to find notonly the relative poses between the cameras but also the positions of thejoints in the ACS. In this paper, we developed calibration algorithms for the ACS using a simpleconstraint: the joint is fixed relative to the cameras connected with it duringthe transformations of the ACS. When the transformations of the cameras in anACS can be estimated relative to the same coordinate system, the positions ofthe joints in the ACS can be calculated by solving linear equations. However,in a non-overlapping view ACS, only the ego-transformations of the cameras andcan be estimated. We proposed a two-steps method to deal with this problem. Inboth methods, the ACS is assumed to have performed general transformations in astatic environment. The efficiency and robustness of the proposed methods aretested by simulation and real experiments. In the real experiment, theintrinsic and extrinsic parameters of the ACS are obtained simultaneously byour calibration procedure using the same image sequences, no extra datacapturing step is required. The corresponding trajectory is recovered andillustrated using the calibration results of the ACS. Since the estimatedtranslations of different cameras in an ACS may scaled by different scalefactors, a scale factor estimation algorithm is also proposed. To ourknowledge, we are the first to study the calibration of ACS.
arxiv-4200-145 | Discriminative Link Prediction using Local Links, Node Features and Community Structure | http://arxiv.org/pdf/1310.4579v1.pdf | author:Abir De, Niloy Ganguly, Soumen Chakrabarti category:cs.LG cs.SI physics.soc-ph published:2013-10-17 summary:A link prediction (LP) algorithm is given a graph, and has to rank, for eachnode, other nodes that are candidates for new linkage. LP is strongly motivatedby social search and recommendation applications. LP techniques often focus onglobal properties (graph conductance, hitting or commute times, Katz score) orlocal properties (Adamic-Adar and many variations, or node feature vectors),but rarely combine these signals. Furthermore, neither of these extremesexploit link densities at the intermediate level of communities. In this paperwe describe a discriminative LP algorithm that exploits two new signals. First,a co-clustering algorithm provides community level link density estimates,which are used to qualify observed links with a surprise value. Second, linksin the immediate neighborhood of the link to be predicted are not interpretedat face value, but through a local model of node feature similarities. Thesesignals are combined into a discriminative link predictor. We evaluate the newpredictor using five diverse data sets that are standard in the literature. Wereport on significant accuracy boosts compared to standard LP methods(including Adamic-Adar and random walk). Apart from the new predictor, anothercontribution is a rigorous protocol for benchmarking and reporting LPalgorithms, which reveals the regions of strengths and weaknesses of all thepredictors studied here, and establishes the new proposal as the most robust.
arxiv-4200-146 | Thompson Sampling in Dynamic Systems for Contextual Bandit Problems | http://arxiv.org/pdf/1310.5008v1.pdf | author:Tianbing Xu, Yaming Yu, John Turner, Amelia Regan category:cs.LG published:2013-10-17 summary:We consider the multiarm bandit problems in the timevarying dynamic systemfor rich structural features. For the nonlinear dynamic model, we propose theapproximate inference for the posterior distributions based on LaplaceApproximation. For the context bandit problems, Thompson Sampling is adoptedbased on the underlying posterior distributions of the parameters. Morespecifically, we introduce the discount decays on the previous samples impactand analyze the different decay rates with the underlying sample dynamics.Consequently, the exploration and exploitation is adaptively tradeoff accordingto the dynamics in the system.
arxiv-4200-147 | Online Classification Using a Voted RDA Method | http://arxiv.org/pdf/1310.5007v1.pdf | author:Tianbing Xu, Jianfeng Gao, Lin Xiao, Amelia Regan category:cs.LG stat.ML published:2013-10-17 summary:We propose a voted dual averaging method for online classification problemswith explicit regularization. This method employs the update rule of theregularized dual averaging (RDA) method, but only on the subsequence oftraining examples where a classification error is made. We derive a bound onthe number of mistakes made by this method on the training set, as well as itsgeneralization error rate. We also introduce the concept of relative strengthof regularization, and show how it affects the mistake bound and generalizationperformance. We experimented with the method using $\ell_1$ regularization on alarge-scale natural language processing task, and obtained state-of-the-artclassification performance with fairly sparse models.
arxiv-4200-148 | A systematic comparison of supervised classifiers | http://arxiv.org/pdf/1311.0202v1.pdf | author:D. R. Amancio, C. H. Comin, D. Casanova, G. Travieso, O. M. Bruno, F. A. Rodrigues, L. da F. Costa category:cs.LG published:2013-10-17 summary:Pattern recognition techniques have been employed in a myriad of industrial,medical, commercial and academic applications. To tackle such a diversity ofdata, many techniques have been devised. However, despite the long tradition ofpattern recognition research, there is no technique that yields the bestclassification in all scenarios. Therefore, the consideration of as many aspossible techniques presents itself as an fundamental practice in applicationsaiming at high accuracy. Typical works comparing methods either emphasize theperformance of a given algorithm in validation tests or systematically comparevarious algorithms, assuming that the practical use of these methods is done byexperts. In many occasions, however, researchers have to deal with theirpractical classification tasks without an in-depth knowledge about theunderlying mechanisms behind parameters. Actually, the adequate choice ofclassifiers and parameters alike in such practical circumstances constitutes along-standing problem and is the subject of the current paper. We carried out astudy on the performance of nine well-known classifiers implemented by the Wekaframework and compared the dependence of the accuracy with their configurationparameter configurations. The analysis of performance with default parametersrevealed that the k-nearest neighbors method exceeds by a large margin theother methods when high dimensional datasets are considered. When otherconfiguration of parameters were allowed, we found that it is possible toimprove the quality of SVM in more than 20% even if parameters are setrandomly. Taken together, the investigation conducted in this paper suggeststhat, apart from the SVM implementation, Weka's default configuration ofparameters provides an performance close the one achieved with the optimalconfiguration.
arxiv-4200-149 | A New Monte Carlo Based Algorithm for the Gaussian Process Classification Problem | http://arxiv.org/pdf/1302.7220v2.pdf | author:Amir F. Atiya, Hatem A. Fayed, Ahmed H. Abdel-Gawad category:stat.ML published:2013-02-28 summary:Gaussian process is a very promising novel technology that has been appliedto both the regression problem and the classification problem. While for theregression problem it yields simple exact solutions, this is not the case forthe classification problem, because we encounter intractable integrals. In thispaper we develop a new derivation that transforms the problem into that ofevaluating the ratio of multivariate Gaussian orthant integrals. Moreover, wedevelop a new Monte Carlo procedure that evaluates these integrals. It is basedon some aspects of bootstrap sampling and acceptancerejection. The proposedapproach has beneficial properties compared to the existing Markov Chain MonteCarlo approach, such as simplicity, reliability, and speed.
arxiv-4200-150 | Distributed Representations of Words and Phrases and their Compositionality | http://arxiv.org/pdf/1310.4546v1.pdf | author:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean category:cs.CL cs.LG stat.ML published:2013-10-16 summary:The recently introduced continuous Skip-gram model is an efficient method forlearning high-quality distributed vector representations that capture a largenumber of precise syntactic and semantic word relationships. In this paper wepresent several extensions that improve both the quality of the vectors and thetraining speed. By subsampling of the frequent words we obtain significantspeedup and also learn more regular word representations. We also describe asimple alternative to the hierarchical softmax called negative sampling. Aninherent limitation of word representations is their indifference to word orderand their inability to represent idiomatic phrases. For example, the meaningsof "Canada" and "Air" cannot be easily combined to obtain "Air Canada".Motivated by this example, we present a simple method for finding phrases intext, and show that learning good vector representations for millions ofphrases is possible.
arxiv-4200-151 | Inference, Sampling, and Learning in Copula Cumulative Distribution Networks | http://arxiv.org/pdf/1310.4456v1.pdf | author:Stefan Douglas Webb category:stat.ML cs.LG published:2013-10-16 summary:The cumulative distribution network (CDN) is a recently developed class ofprobabilistic graphical models (PGMs) permitting a copula factorization, inwhich the CDF, rather than the density, is factored. Despite there being muchrecent interest within the machine learning community about copularepresentations, there has been scarce research into the CDN, its amalgamationwith copula theory, and no evaluation of its performance. Algorithms forinference, sampling, and learning in these models are underdeveloped comparedthose of other PGMs, hindering widerspread use. One advantage of the CDN is that it allows the factors to be parameterized ascopulae, combining the benefits of graphical models with those of copulatheory. In brief, the use of a copula parameterization enables greatermodelling flexibility by separating representation of the marginals from thedependence structure, permitting more efficient and robust learning. Anotheradvantage is that the CDN permits the representation of implicit latentvariables, whose parameterization and connectivity are not required to bespecified. Unfortunately, that the model can encode only latent relationshipsbetween variables severely limits its utility. In this thesis, we present inference, learning, and sampling for CDNs, andfurther the state-of-the-art. First, we explain the basics of copula theory andthe representation of copula CDNs. Then, we discuss inference in the models,and develop the first sampling algorithm. We explain standard learning methods,propose an algorithm for learning from data missing completely at random(MCAR), and develop a novel algorithm for learning models of arbitrarytreewidth and size. Properties of the models and algorithms are investigatedthrough Monte Carlo simulations. We conclude with further discussion of theadvantages and limitations of CDNs, and suggest future work.
arxiv-4200-152 | Multiple Attractor Cellular Automata (MACA) for Addressing Major Problems in Bioinformatics | http://arxiv.org/pdf/1310.4495v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi Nedunuri category:cs.CE cs.LG published:2013-10-16 summary:CA has grown as potential classifier for addressing major problems inbioinformatics. Lot of bioinformatics problems like predicting the proteincoding region, finding the promoter region, predicting the structure of proteinand many other problems in bioinformatics can be addressed through CellularAutomata. Even though there are some prediction techniques addressing theseproblems, the approximate accuracy level is very less. An automated procedurewas proposed with MACA (Multiple Attractor Cellular Automata) which can addressall these problems. The genetic algorithm is also used to find rules with goodfitness values. Extensive experiments are conducted for reporting the accuracyof the proposed tool. The average accuracy of MACA when tested with ENCODE,BG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%.
arxiv-4200-153 | An FCA-based Boolean Matrix Factorisation for Collaborative Filtering | http://arxiv.org/pdf/1310.4366v1.pdf | author:Elena Nenova, Dmitry I. Ignatov, Andrey V. Konstantinov category:cs.IR cs.DS stat.ML H.2.8; H.2.3 published:2013-10-16 summary:We propose a new approach for Collaborative Filtering which is based onBoolean Matrix Factorisation (BMF) and Formal Concept Analysis. In a series ofexperiments on real data (Movielens dataset) we compare the approach with theSVD- and NMF-based algorithms in terms of Mean Average Error (MAE). One of theexperimental consequences is that it is enough to have a binary-scaled ratingdata to obtain almost the same quality in terms of MAE by BMF than for theSVD-based algorithm in case of non-scaled data.
arxiv-4200-154 | Bayesian Information Sharing Between Noise And Regression Models Improves Prediction of Weak Effects | http://arxiv.org/pdf/1310.4362v1.pdf | author:Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J Kangas, Pasi Soininen, Marjo-Riitta Järvelin, Mika Ala-Korpela, Samuel Kaski category:stat.ML cs.LG published:2013-10-16 summary:We consider the prediction of weak effects in a multiple-output regressionsetup, when covariates are expected to explain a small amount, less than$\approx 1%$, of the variance of the target variables. To facilitate theprediction of the weak effects, we constrain our model structure by introducinga novel Bayesian approach of sharing information between the regression modeland the noise model. Further reduction of the effective number of parameters isachieved by introducing an infinite shrinkage prior and group sparsity in thecontext of the Bayesian reduced rank regression, and using the Bayesianinfinite factor model as a flexible low-rank noise model. In our experimentsthe model incorporating the novelties outperformed alternatives in genomicprediction of rich phenotype data. In particular, the information sharingbetween the noise and regression models led to significant improvement inprediction accuracy.
arxiv-4200-155 | Supervised Heterogeneous Multiview Learning for Joint Association Study and Disease Diagnosis | http://arxiv.org/pdf/1304.7284v2.pdf | author:Shandian Zhe, Zenglin Xu, Yuan Qi category:cs.LG cs.CE stat.ML published:2013-04-26 summary:Given genetic variations and various phenotypical traits, such as MagneticResonance Imaging (MRI) features, we consider two important and related tasksin biomedical research: i)to select genetic and phenotypical markers fordisease diagnosis and ii) to identify associations between genetic andphenotypical data. These two tasks are tightly coupled because underlyingassociations between genetic variations and phenotypical features contain thebiological basis for a disease. While a variety of sparse models have beenapplied for disease diagnosis and canonical correlation analysis and itsextensions have bee widely used in association studies (e.g., eQTL analysis),these two tasks have been treated separately. To unify these two tasks, wepresent a new sparse Bayesian approach for joint association study and diseasediagnosis. In this approach, common latent features are extracted fromdifferent data sources based on sparse projection matrices and used to predictmultiple disease severity levels based on Gaussian process ordinal regression;in return, the disease status is used to guide the discovery of relationshipsbetween the data sources. The sparse projection matrices not only revealinteractions between data sources but also select groups of biomarkers relatedto the disease. To learn the model from data, we develop an efficientvariational expectation maximization algorithm. Simulation results demonstratethat our approach achieves higher accuracy in both predicting ordinal labelsand discovering associations between data sources than alternative methods. Weapply our approach to an imaging genetics dataset for the study of Alzheimer'sDisease (AD). Our method identifies biologically meaningful relationshipsbetween genetic variations, MRI features, and AD status, and achievessignificantly higher accuracy for predicting ordinal AD stages than thecompeting methods.
arxiv-4200-156 | Multilabel Consensus Classification | http://arxiv.org/pdf/1310.4252v1.pdf | author:Sihong Xie, Xiangnan Kong, Jing Gao, Wei Fan, Philip S. Yu category:stat.ML cs.LG published:2013-10-16 summary:In the era of big data, a large amount of noisy and incomplete data can becollected from multiple sources for prediction tasks. Combining multiple modelsor data sources helps to counteract the effects of low data quality and thebias of any single model or data source, and thus can improve the robustnessand the performance of predictive models. Out of privacy, storage and bandwidthconsiderations, in certain circumstances one has to combine the predictionsfrom multiple models or data sources to obtain the final predictions withoutaccessing the raw data. Consensus-based prediction combination algorithms areeffective for such situations. However, current research on predictioncombination focuses on the single label setting, where an instance can have oneand only one label. Nonetheless, data nowadays are usually multilabeled, suchthat more than one label have to be predicted at the same time. Directapplications of existing prediction combination methods to multilabel settingscan lead to degenerated performance. In this paper, we address the challengesof combining predictions from multiple multilabel classifiers and propose twonovel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) andMLCM-a (MLCM for microAUC). These algorithms can capture label correlationsthat are common in multilabel classifications, and optimize correspondingperformance metrics. Experimental results on popular multilabel classificationtasks verify the theoretical analysis and effectiveness of the proposedmethods.
arxiv-4200-157 | On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy Linear Measurements | http://arxiv.org/pdf/1306.4391v2.pdf | author:Akshay Soni, Jarvis Haupt category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-06-18 summary:Recent breakthrough results in compressive sensing (CS) have established thatmany high dimensional signals can be accurately recovered from a relativelysmall number of non-adaptive linear observations, provided that the signalspossess a sparse representation in some basis. Subsequent efforts have shownthat the performance of CS can be improved by exploiting additional structurein the locations of the nonzero signal coefficients during inference, or byutilizing some form of data-dependent adaptive measurement focusing during thesensing process. To our knowledge, our own previous work was the first toestablish the potential benefits that can be achieved when fusing the notionsof adaptive sensing and structured sparsity -- that work examined the task ofsupport recovery from noisy linear measurements, and established that anadaptive sensing strategy specifically tailored to signals that are tree-sparsecan significantly outperform adaptive and non-adaptive sensing strategies thatare agnostic to the underlying structure. In this work we establish fundamentalperformance limits for the task of support recovery of tree-sparse signals fromnoisy measurements, in settings where measurements may be obtained eithernon-adaptively (using a randomized Gaussian measurement strategy motivated byinitial CS investigations) or by any adaptive sensing strategy. Our mainresults here imply that the adaptive tree sensing procedure analyzed in ourprevious work is nearly optimal, in the sense that no other sensing andestimation strategy can perform fundamentally better for identifying thesupport of tree-sparse signals.
arxiv-4200-158 | Distributed Low-rank Subspace Segmentation | http://arxiv.org/pdf/1304.5583v2.pdf | author:Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I. Jordan category:cs.CV cs.DC cs.LG stat.ML published:2013-04-20 summary:Vision problems ranging from image clustering to motion segmentation tosemi-supervised learning can naturally be framed as subspace segmentationproblems, in which one aims to recover multiple low-dimensional subspaces fromnoisy and corrupted input data. Low-Rank Representation (LRR), a convexformulation of the subspace segmentation problem, is provably and empiricallyaccurate on small problems but does not scale to the massive sizes of modernvision datasets. Moreover, past work aimed at scaling up low-rank matrixfactorization is not applicable to LRR given its non-decomposable constraints.In this work, we propose a novel divide-and-conquer algorithm for large-scalesubspace segmentation that can cope with LRR's non-decomposable constraints andmaintains LRR's strong recovery guarantees. This has immediate implications forthe scalability of subspace segmentation, which we demonstrate on a benchmarkface recognition dataset and in simulations. We then introduce novelapplications of LRR-based subspace segmentation to large-scale semi-supervisedlearning for multimedia event detection, concept detection, and image tagging.In each case, we obtain state-of-the-art results and order-of-magnitude speedups.
arxiv-4200-159 | On Measure Concentration of Random Maximum A-Posteriori Perturbations | http://arxiv.org/pdf/1310.4227v1.pdf | author:Francesco Orabona, Tamir Hazan, Anand D. Sarwate, Tommi Jaakkola category:cs.LG math.PR published:2013-10-15 summary:The maximum a-posteriori (MAP) perturbation framework has emerged as a usefulapproach for inference and learning in high dimensional complex models. Bymaximizing a randomly perturbed potential function, MAP perturbations generateunbiased samples from the Gibbs distribution. Unfortunately, the computationalcost of generating so many high-dimensional random variables can beprohibitive. More efficient algorithms use sequential sampling strategies basedon the expected value of low dimensional MAP perturbations. This paper developsnew measure concentration inequalities that bound the number of samples neededto estimate such expected values. Applying the general result to MAPperturbations can yield a more efficient algorithm to approximate sampling fromthe Gibbs distribution. The measure concentration result is of general interestand may be applicable to other areas involving expected estimations.
arxiv-4200-160 | Adaptive Temporal Compressive Sensing for Video | http://arxiv.org/pdf/1302.3446v3.pdf | author:Xin Yuan, Jianbo Yang, Patrick Llull, Xuejun Liao, Guillermo Sapiro, David J. Brady, Lawrence Carin category:stat.AP cs.CV cs.MM published:2013-02-14 summary:This paper introduces the concept of adaptive temporal compressive sensing(CS) for video. We propose a CS algorithm to adapt the compression ratio basedon the scene's temporal complexity, computed from the compressed data, withoutcompromising the quality of the reconstructed video. The temporal adaptivity ismanifested by manipulating the integration time of the camera, opening thepossibility to real-time implementation. The proposed algorithm is ageneralized temporal CS approach that can be incorporated with a diverse set ofexisting hardware systems.
arxiv-4200-161 | Exact Learning of RNA Energy Parameters From Structure | http://arxiv.org/pdf/1310.4223v1.pdf | author:Hamidreza Chitsaz, Mohammad Aminisharifabad category:q-bio.BM cs.LG published:2013-10-15 summary:We consider the problem of exact learning of parameters of a linear RNAenergy model from secondary structure data. A necessary and sufficientcondition for learnability of parameters is derived, which is based oncomputing the convex hull of union of translated Newton polytopes of inputsequences. The set of learned energy parameters is characterized as the convexcone generated by the normal vectors to those facets of the resulting polytopethat are incident to the origin. In practice, the sufficient condition may notbe satisfied by the entire training data set; hence, computing a maximal subsetof training data for which the sufficient condition is satisfied is oftendesired. We show that problem is NP-hard in general for an arbitrarydimensional feature space. Using a randomized greedy algorithm, we select asubset of RNA STRAND v2.0 database that satisfies the sufficient condition forseparate A-U, C-G, G-U base pair counting model. The set of learned energyparameters includes experimentally measured energies of A-U, C-G, and G-Upairs; hence, our parameter set is in agreement with the Turner parameters.
arxiv-4200-162 | Optimal Sensor Placement and Enhanced Sparsity for Classification | http://arxiv.org/pdf/1310.4217v1.pdf | author:B. W. Brunton, S. L. Brunton, J. L. Proctor, J. N. Kutz category:cs.CV published:2013-10-15 summary:The goal of compressive sensing is efficient reconstruction of data from fewmeasurements, sometimes leading to a categorical decision. If onlyclassification is required, reconstruction can be circumvented and themeasurements needed are orders-of-magnitude sparser still. We define enhancedsparsity as the reduction in number of measurements required for classificationover reconstruction. In this work, we exploit enhanced sparsity and learnspatial sensor locations that optimally inform a categorical decision. Thealgorithm solves an l1-minimization to find the fewest entries of the fullmeasurement vector that exactly reconstruct the discriminant vector in featurespace. Once the sensor locations have been identified from the training data,subsequent test samples are classified with remarkable efficiency, achievingperformance comparable to that obtained by discrimination using the full image.Sensor locations may be learned from full images, or from a random subsample ofpixels. For classification between more than two categories, we introduce acoupling parameter whose value tunes the number of sensors selected, tradingaccuracy for economy. We demonstrate the algorithm on example datasets fromimage recognition using PCA for feature extraction and LDA for discrimination;however, the method can be broadly applied to non-image data and adapted towork with other methods for feature extraction and discrimination.
arxiv-4200-163 | Sharp Inequalities for $f$-divergences | http://arxiv.org/pdf/1302.0336v2.pdf | author:Adityanand Guntuboyina, Sujayam Saha, Geoffrey Schiebinger category:math.ST cs.IT math.IT math.OC math.PR stat.ML stat.TH published:2013-02-02 summary:$f$-divergences are a general class of divergences between probabilitymeasures which include as special cases many commonly used divergences inprobability, mathematical statistics and information theory such asKullback-Leibler divergence, chi-squared divergence, squared Hellingerdistance, total variation distance etc. In this paper, we study the problem ofmaximizing or minimizing an $f$-divergence between two probability measuressubject to a finite number of constraints on other $f$-divergences. We showthat these infinite-dimensional optimization problems can all be reduced tooptimization problems over small finite dimensional spaces which are tractable.Our results lead to a comprehensive and unified treatment of the problem ofobtaining sharp inequalities between $f$-divergences. We demonstrate that manyof the existing results on inequalities between $f$-divergences can be obtainedas special cases of our results and we also improve on some existing non-sharpinequalities.
arxiv-4200-164 | The BeiHang Keystroke Dynamics Authentication System | http://arxiv.org/pdf/1310.4485v1.pdf | author:Juan Liu, Baochang Zhang, Linlin Shen, Jianzhuang Liu, Jason Zhao category:cs.CR cs.LG published:2013-10-15 summary:Keystroke Dynamics is an important biometric solution for personauthentication. Based upon keystroke dynamics, this paper designs an embeddedpassword protection device, develops an online system, collects two publicdatabases for promoting the research on keystroke authentication, exploits theGabor filter bank to characterize keystroke dynamics, and provides benchmarkresults of three popular classification algorithms, one-class support vectormachine, Gaussian classifier, and nearest neighbour classifier.
arxiv-4200-165 | Sparse Matrix Inversion with Scaled Lasso | http://arxiv.org/pdf/1202.2723v2.pdf | author:Tingni Sun, Cun-Hui Zhang category:math.ST stat.ML stat.TH published:2012-02-13 summary:We propose a new method of learning a sparse nonnegative-definite targetmatrix. Our primary example of the target matrix is the inverse of a populationcovariance or correlation matrix. The algorithm first estimates each column ofthe target matrix by the scaled Lasso and then adjusts the matrix estimator tobe symmetric. The penalty level of the scaled Lasso for each column iscompletely determined by data via convex minimization, without usingcross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate ofconvergence in the spectrum norm under conditions of weaker form than those inthe existing analyses of other $\ell_1$ regularized algorithms, and has fasterguaranteed rate of convergence when the ratio of the $\ell_1$ and spectrumnorms of the target inverse matrix diverges to infinity. A simulation studydemonstrates the computational feasibility and superb performance of theproposed method. Our analysis also provides new performance bounds for the Lasso and scaledLasso to guarantee higher concentration of the error at a smaller thresholdlevel than previous analyses, and to allow the use of the union bound incolumn-by-column applications of the scaled Lasso without an adjustment of thepenalty level. In addition, the least squares estimation after the scaled Lassoselection is considered and proven to guarantee performance bounds similar tothat of the scaled Lasso.
arxiv-4200-166 | Green Heron Swarm Optimization Algorithm - State-of-the-Art of a New Nature Inspired Discrete Meta-Heuristics | http://arxiv.org/pdf/1310.3805v1.pdf | author:Chiranjib Sur, Anupam Shukla category:cs.NE published:2013-10-14 summary:Many real world problems are NP-Hard problems are a very large part of themcan be represented as graph based problems. This makes graph theory a veryimportant and prevalent field of study. In this work a new bio-inspiredmeta-heuristics called Green Heron Swarm Optimization (GHOSA) Algorithm isbeing introduced which is inspired by the fishing skills of the bird. Thealgorithm basically suited for graph based problems like combinatorialoptimization etc. However introduction of an adaptive mathematical variationoperator called Location Based Neighbour Influenced Variation (LBNIV) makes itsuitable for high dimensional continuous domain problems. The new algorithm isbeing operated on the traditional benchmark equations and the results arecompared with Genetic Algorithm and Particle Swarm Optimization. The algorithmis also operated on Travelling Salesman Problem, Quadratic Assignment Problem,Knapsack Problem dataset. The procedure to operate the algorithm on theResource Constraint Shortest Path and road network optimization is alsodiscussed. The results clearly demarcates the GHOSA algorithm as an efficientalgorithm specially considering that the number of algorithms for the discreteoptimization is very low and robust and more explorative algorithm is requiredin this age of social networking and mostly graph based problem scenarios.
arxiv-4200-167 | Misfire Detection in IC Engine using Kstar Algorithm | http://arxiv.org/pdf/1310.3717v1.pdf | author:Anish Bahri, V Sugumaran, S Babu Devasenapati category:cs.CV published:2013-10-14 summary:Misfire in an IC Engine continues to be a problem leading to reduced fuelefficiency, increased power loss and emissions containing heavy concentrationof hydrocarbons. Misfiring creates a unique vibration pattern attributed to aparticular cylinder. Useful features can be extracted from these patterns andcan be analyzed to detect misfire. Statistical features from these vibrationsignals were extracted. Out of these, useful features were identified using theJ48 decision tree algorithm and selected features were used for classificationusing the Kstar algorithm. In this paper performance analysis of Kstaralgorithm is presented.
arxiv-4200-168 | Online Ranking: Discrete Choice, Spearman Correlation and Other Feedback | http://arxiv.org/pdf/1308.6797v5.pdf | author:Nir Ailon category:cs.LG cs.GT stat.ML published:2013-08-30 summary:Given a set $V$ of $n$ objects, an online ranking system outputs at each timestep a full ranking of the set, observes a feedback of some form and suffers aloss. We study the setting in which the (adversarial) feedback is an element in$V$, and the loss is the position (0th, 1st, 2nd...) of the item in theoutputted ranking. More generally, we study a setting in which the feedback isa subset $U$ of at most $k$ elements in $V$, and the loss is the sum of thepositions of those elements. We present an algorithm of expected regret $O(n^{3/2}\sqrt{Tk})$ over a timehorizon of $T$ steps with respect to the best single ranking in hindsight. Thisimproves previous algorithms and analyses either by a factor of either$\Omega(\sqrt{k})$, a factor of $\Omega(\sqrt{\log n})$ or by improving runningtime from quadratic to $O(n\log n)$ per round. We also prove a matching lowerbound. Our techniques also imply an improved regret bound for online rankaggregation over the Spearman correlation measure, and to other more complexranking loss functions.
arxiv-4200-169 | Variance Adjusted Actor Critic Algorithms | http://arxiv.org/pdf/1310.3697v1.pdf | author:Aviv Tamar, Shie Mannor category:stat.ML cs.LG cs.SY published:2013-10-14 summary:We present an actor-critic framework for MDPs where the objective is thevariance-adjusted expected return. Our critic uses linear functionapproximation, and we extend the concept of compatible features to thevariance-adjusted setting. We present an episodic actor-critic algorithm andshow that it converges almost surely to a locally optimal point of theobjective function.
arxiv-4200-170 | Predicting college basketball match outcomes using machine learning techniques: some results and lessons learned | http://arxiv.org/pdf/1310.3607v1.pdf | author:Albrecht Zimmermann, Sruthi Moorthy, Zifan Shi category:cs.LG stat.AP published:2013-10-14 summary:Most existing work on predicting NCAAB matches has been developed in astatistical context. Trusting the capabilities of ML techniques, particularlyclassification learners, to uncover the importance of features and learn theirrelationships, we evaluated a number of different paradigms on this task. Inthis paper, we summarize our work, pointing out that attributes seem to be moreimportant than models, and that there seems to be an upper limit to predictivequality.
arxiv-4200-171 | Flow-Based Algorithms for Local Graph Clustering | http://arxiv.org/pdf/1307.2855v2.pdf | author:Lorenzo Orecchia, Zeyuan Allen Zhu category:cs.DS cs.LG stat.ML published:2013-07-10 summary:Given a subset S of vertices of an undirected graph G, the cut-improvementproblem asks us to find a subset S that is similar to A but has smallerconductance. A very elegant algorithm for this problem has been given byAndersen and Lang [AL08] and requires solving a small number ofsingle-commodity maximum flow computations over the whole graph G. In thispaper, we introduce LocalImprove, the first cut-improvement algorithm that islocal, i.e. that runs in time dependent on the size of the input set A ratherthan on the size of the entire graph. Moreover, LocalImprove achieves thislocal behaviour while essentially matching the same theoretical guarantee asthe global algorithm of Andersen and Lang. The main application of LocalImprove is to the design of betterlocal-graph-partitioning algorithms. All previously known local algorithms forgraph partitioning are random-walk based and can only guarantee an outputconductance of O(\sqrt{OPT}) when the target set has conductance OPT \in [0,1].Very recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT /\sqrt{CONN}) where the internal connectivity parameter CONN \in [0,1] isdefined as the reciprocal of the mixing time of the random walk over the graphinduced by the target set. In this work, we show how to use LocalImprove toobtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). Thisyields the first flow-based algorithm. Moreover, its performance strictlyoutperforms the ones based on random walks and surprisingly matches that of thebest known global algorithm, which is SDP-based, in this parameter regime[MMV12]. Finally, our results show that spectral methods are not the only viableapproach to the construction of local graph partitioning algorithm and opendoor to the study of algorithms with even better approximation and localityguarantees.
arxiv-4200-172 | Can Twitter Predict Royal Baby's Name ? | http://arxiv.org/pdf/1310.3500v1.pdf | author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.CY published:2013-10-13 summary:In this paper, we analyze the existence of possible correlation betweenpublic opinion of twitter users and the decision-making of persons who areinfluential in the society. We carry out this analysis on the example of thediscussion of probable name of the British crown baby, born in July, 2013. Inour study, we use the methods of quantitative processing of natural language,the theory of frequent sets, the algorithms of visual displaying of users'communities. We also analyzed the time dynamics of keyword frequencies. Theanalysis showed that the main predictable name was dominating in the spectrumof names before the official announcement. Using the theories of frequent sets,we showed that the full name consisting of three component names was the partof top 5 by the value of support. It was revealed that the structure ofdynamically formed users' communities participating in the discussion isdetermined by only a few leaders who influence significantly the viewpoints ofother users.
arxiv-4200-173 | Forecasting of Events by Tweet Data Mining | http://arxiv.org/pdf/1310.3499v1.pdf | author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.CY published:2013-10-13 summary:This paper describes the analysis of quantitative characteristics of frequentsets and association rules in the posts of Twitter microblogs related todifferent event discussions. For the analysis, we used a theory of frequentsets, association rules and a theory of formal concept analysis. We revealedthe frequent sets and association rules which characterize the semanticrelations between the concepts of analyzed subjects. The support of somefrequent sets reaches its global maximum before the expected event but withsome time delay. Such frequent sets may be considered as predictive markersthat characterize the significance of expected events for blogosphere users. Weshowed that the time dynamics of confidence in some revealed association rulescan also have predictive characteristics. Exceeding a certain threshold may bea signal for corresponding reaction in the society within the time intervalbetween the maximum and the probable coming of an event. In this paper, weconsidered two types of events: the Olympic tennis tournament final in London,2012 and the prediction of Eurovision 2013 winner.
arxiv-4200-174 | Predicting Social Links for New Users across Aligned Heterogeneous Social Networks | http://arxiv.org/pdf/1310.3492v1.pdf | author:Jiawei Zhang, Xiangnan Kong, Philip S. Yu category:cs.SI cs.LG physics.soc-ph published:2013-10-13 summary:Online social networks have gained great success in recent years and many ofthem involve multiple kinds of nodes and complex relationships. Among theserelationships, social links among users are of great importance. Many existinglink prediction methods focus on predicting social links that will appear inthe future among all users based upon a snapshot of the social network. Inreal-world social networks, many new users are joining in the service everyday. Predicting links for new users are more important. Different fromconventional link prediction problems, link prediction for new users are morechallenging due to the following reasons: (1) differences in informationdistributions between new users and the existing active users (i.e., oldusers); (2) lack of information from the new users in the network. We propose alink prediction method called SCAN-PS (Supervised Cross Aligned Networks linkprediction with Personalized Sampling), to solve the link prediction problemfor new users with information transferred from both the existing active usersin the target network and other source networks through aligned accounts. Weproposed a within-target-network personalized sampling method to process theexisting active users' information in order to accommodate the differences ininformation distributions before the intra-network knowledge transfer. SCAN-PScan also exploit information in other source networks, where the user accountsare aligned with the target network. In this way, SCAN-PS could solve the coldstart problem when information of these new users is total absent in the targetnetwork.
arxiv-4200-175 | A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale SVM Training | http://arxiv.org/pdf/1304.1014v2.pdf | author:Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori category:cs.CV cs.AI cs.LG math.OC stat.ML published:2013-04-03 summary:Recently, there has been a renewed interest in the machine learning communityfor variants of a sparse greedy approximation procedure for concaveoptimization known as {the Frank-Wolfe (FW) method}. In particular, thisprocedure has been successfully applied to train large-scale instances ofnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training hasallowed to obtain efficient algorithms but also important theoretical results,including convergence analysis of training algorithms and new characterizationsof model sparsity. In this paper, we present and analyze a novel variant of the FW method basedon a new way to perform away steps, a classic strategy used to accelerate theconvergence of the basic FW procedure. Our formulation and analysis is focusedon a general concave maximization problem on the simplex. However, thespecialization of our algorithm to quadratic forms is strongly related to someclassic methods in computational geometry, namely the Gilbert and MDMalgorithms. On the theoretical side, we demonstrate that the method matches theguarantees in terms of convergence rate and number of iterations obtained byusing classic away steps. In particular, the method enjoys a linear rate ofconvergence, a result that has been recently proved for MDM on quadratic forms. On the practical side, we provide experiments on several classificationdatasets, and evaluate the results using statistical tests. Experiments showthat our method is faster than the FW method with classic away steps, and workswell even in the cases in which classic away steps slow down the algorithm.Furthermore, these improvements are obtained without sacrificing the predictiveaccuracy of the obtained SVM model.
arxiv-4200-176 | Dense Scattering Layer Removal | http://arxiv.org/pdf/1310.3452v1.pdf | author:Qiong Yan, Li Xu, Jiaya Jia category:cs.CV I.4.1 published:2013-10-13 summary:We propose a new model, together with advanced optimization, to separate athick scattering media layer from a single natural image. It is able to handlechallenging underwater scenes and images taken in fog and sandstorm, both ofwhich are with significantly reduced visibility. Our method addresses thecritical issue -- this is, originally unnoticeable impurities will be greatlymagnified after removing the scattering media layer -- with transmission-awareoptimization. We introduce non-local structure-aware regularization to properlyconstrain transmission estimation without introducing the halo artifacts. Aselective-neighbor criterion is presented to convert the unconventionalconstrained optimization problem to an unconstrained one where the latter canbe efficiently solved.
arxiv-4200-177 | Cross-moments computation for stochastic context-free grammars | http://arxiv.org/pdf/1108.0353v2.pdf | author:Velimir M. Ilic, Miroslav D. Ciric, Miomir S. Stankovic category:cs.CL published:2011-08-01 summary:In this paper we consider the problem of efficient computation ofcross-moments of a vector random variable represented by a stochasticcontext-free grammar. Two types of cross-moments are discussed. The samplespace for the first one is the set of all derivations of the context-freegrammar, and the sample space for the second one is the set of all derivationswhich generate a string belonging to the language of the grammar. In the past,this problem was widely studied, but mainly for the cross-moments of scalarvariables and up to the second order. This paper presents new algorithms forcomputing the cross-moments of an arbitrary order, and the previously developedones are derived as special cases.
arxiv-4200-178 | Spectral Classification Using Restricted Boltzmann Machine | http://arxiv.org/pdf/1305.0665v2.pdf | author:Fuqiang Chen, Yan Wu, Yude Bu, Guodong Zhao category:cs.LG published:2013-05-03 summary:In this study, a novel machine learning algorithm, restricted Boltzmannmachine (RBM), is introduced. The algorithm is applied for the spectralclassification in astronomy. RBM is a bipartite generative graphical model withtwo separate layers (one visible layer and one hidden layer), which can extracthigher level features to represent the original data. Despite generative, RBMcan be used for classification when modified with a free energy and a soft-maxfunction. Before spectral classification, the original data is binarizedaccording to some rule. Then we resort to the binary RBM to classifycataclysmic variables (CVs) and non-CVs (one half of all the given data fortraining and the other half for testing). The experiment result showsstate-of-the-art accuracy of 100%, which indicates the efficiency of the binaryRBM algorithm.
arxiv-4200-179 | Negative Binomial Process Count and Mixture Modeling | http://arxiv.org/pdf/1209.3442v3.pdf | author:Mingyuan Zhou, Lawrence Carin category:stat.ME stat.ML published:2012-09-15 summary:The seemingly disjoint problems of count and mixture modeling are unitedunder the negative binomial (NB) process. A gamma process is employed to modelthe rate measure of a Poisson process, whose normalization provides a randomprobability measure for mixture modeling and whose marginalization leads to anNB process for count modeling. A draw from the NB process consists of a Poissondistributed finite number of distinct atoms, each of which is associated with alogarithmic distributed number of data samples. We reveal relationships betweenvarious count- and mixture-modeling distributions and construct aPoisson-logarithmic bivariate distribution that connects the NB and Chineserestaurant table distributions. Fundamental properties of the models aredeveloped, and we derive efficient Bayesian inference. It is shown that withaugmentation and normalization, the NB process and gamma-NB process can bereduced to the Dirichlet process and hierarchical Dirichlet process,respectively. These relationships highlight theoretical, structural andcomputational advantages of the NB process. A variety of NB processes,including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB andzero-inflated-NB processes, with distinct sharing mechanisms, are alsoconstructed. These models are applied to topic modeling, with connections madeto existing algorithms under Poisson factor analysis. Example results show theimportance of inferring both the NB dispersion and probability parameters.
arxiv-4200-180 | On Optimal Probabilities in Stochastic Coordinate Descent Methods | http://arxiv.org/pdf/1310.3438v1.pdf | author:Peter Richtárik, Martin Takáč category:stat.ML cs.DC math.OC published:2013-10-13 summary:We propose and analyze a new parallel coordinate descent method---`NSync---inwhich at each iteration a random subset of coordinates is updated, in parallel,allowing for the subsets to be chosen non-uniformly. We derive convergencerates under a strong convexity assumption, and comment on how to assignprobabilities to the sets to optimize the bound. The complexity and practicalperformance of the method can outperform its uniform variant by an order ofmagnitude. Surprisingly, the strategy of updating a single randomly selectedcoordinate per iteration---with optimal probabilities---may require lessiterations, both in theory and practice, than the strategy of updating allcoordinates at every iteration.
arxiv-4200-181 | Joint Indoor Localization and Radio Map Construction with Limited Deployment Load | http://arxiv.org/pdf/1310.3407v1.pdf | author:Sameh Sorour, Yves Lostanlen, Shahrokh Valaee category:cs.NI cs.LG published:2013-10-12 summary:One major bottleneck in the practical implementation of received signalstrength (RSS) based indoor localization systems is the extensive deploymentefforts required to construct the radio maps through fingerprinting. In thispaper, we aim to design an indoor localization scheme that can be directlyemployed without building a full fingerprinted radio map of the indoorenvironment. By accumulating the information of localized RSSs, this scheme canalso simultaneously construct the radio map with limited calibration. To designthis scheme, we employ a source data set that possesses the same spatialcorrelation of the RSSs in the indoor environment under study. The knowledge ofthis data set is then transferred to a limited number of calibrationfingerprints and one or several RSS observations with unknown locations, inorder to perform direct localization of these observations using manifoldalignment. We test two different source data sets, namely a simulated radiopropagation map and the environments plan coordinates. For moving users, weexploit the correlation of their observations to improve the localizationaccuracy. The online testing in two indoor environments shows that the plancoordinates achieve better results than the simulated radio maps, and anegligible degradation with 70-85% reduction in calibration load.
arxiv-4200-182 | Identification of biologically relevant subtypes via preweighted sparse clustering | http://arxiv.org/pdf/1304.3760v2.pdf | author:Sheila Gaynor, Eric Bair category:stat.ME cs.LG q-bio.QM stat.AP stat.ML published:2013-04-13 summary:Cluster analysis methods are used to identify homogeneous subgroups in a dataset. Frequently one applies cluster analysis in order to identify biologicallyinteresting subgroups. In particular, one may wish to identify subgroups thatare associated with a particular outcome of interest. Conventional clusteringmethods often fail to identify such subgroups, particularly when there are alarge number of high-variance features in the data set. Conventional methodsmay identify clusters associated with these high-variance features when onewishes to obtain secondary clusters that are more interesting biologically ormore strongly associated with a particular outcome of interest. We describe amodification of the sparse clustering method of Witten and Tibshirani (2010)that can be used to identify such secondary clusters or clusters associatedwith an outcome of interest. We show that this method can correctly identifysuch clusters of interest in several simulation scenarios. The method is alsoapplied to a large case-control study of TMD and a leukemia microarray dataset.
arxiv-4200-183 | PCG-Cut: Graph Driven Segmentation of the Prostate Central Gland | http://arxiv.org/pdf/1310.3366v1.pdf | author:Jan Egger category:cs.CV published:2013-10-12 summary:Prostate cancer is the most abundant cancer in men, with over 200,000expected new cases and around 28,000 deaths in 2012 in the US alone. In thisstudy, the segmentation results for the prostate central gland (PCG) in MRscans are presented. The aim of this research study is to apply a graph-basedalgorithm to automated segmentation (i.e. delineation) of organ limits for theprostate central gland. The ultimate goal is to apply automated segmentationapproach to facilitate efficient MR-guided biopsy and radiation treatmentplanning. The automated segmentation algorithm used is graph-driven based on aspherical template. Therefore, rays are sent through the surface points of apolyhedron to sample the graph's nodes. After graph construction - which onlyrequires the center of the polyhedron defined by the user and located insidethe prostate center gland - the minimal cost closed set on the graph iscomputed via a polynomial time s-t-cut, which results in the segmentation ofthe prostate center gland's boundaries and volume. The algorithm has beenrealized as a C++ modul within the medical research platform MeVisLab and theground truth of the central gland boundaries were manually extracted byclinical experts (interventional radiologists) with several years of experiencein prostate treatment. For evaluation the automated segmentations of theproposed scheme have been compared with the manual segmentations, yielding anaverage Dice Similarity Coefficient (DSC) of 78.94 +/- 10.85%.
arxiv-4200-184 | Visualizing Bags of Vectors | http://arxiv.org/pdf/1310.3333v1.pdf | author:Sriramkumar Balasubramanian, Raghuram Reddy Nagireddy category:cs.IR cs.CL cs.LG published:2013-10-12 summary:The motivation of this work is two-fold - a) to compare between two differentmodes of visualizing data that exists in a bag of vectors format b) to proposea theoretical model that supports a new mode of visualizing data. Visualizinghigh dimensional data can be achieved using Minimum Volume Embedding, but thedata has to exist in a format suitable for computing similarities whilepreserving local distances. This paper compares the visualization between twomethods of representing data and also proposes a new method providing samplevisualizations for that method.
arxiv-4200-185 | Comunication-Efficient Algorithms for Statistical Optimization | http://arxiv.org/pdf/1209.4129v3.pdf | author:Yuchen Zhang, John C. Duchi, Martin Wainwright category:stat.ML cs.LG stat.CO published:2012-09-19 summary:We analyze two communication-efficient algorithms for distributed statisticaloptimization on large-scale data sets. The first algorithm is a standardaveraging method that distributes the $N$ data samples evenly to $\nummac$machines, performs separate minimization on each subset, and then averages theestimates. We provide a sharp analysis of this average mixture algorithm,showing that under a reasonable set of conditions, the combined parameterachieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$.Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rateachievable by a centralized algorithm having access to all $\totalnumobs$samples. The second algorithm is a novel method, based on an appropriate formof bootstrap subsampling. Requiring only a single round of communication, ithas mean-squared error that decays as $\order(N^{-1} + (N/m)^{-3})$, and so ismore robust to the amount of parallelization. In addition, we show that astochastic gradient-based method attains mean-squared error decaying as$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties inthe rate of convergence. We also provide experimental evaluation of ourmethods, investigating their performance both on simulated data and on alarge-scale regression problem from the internet search domain. In particular,we show that our methods can be used to efficiently solve an advertisementprediction problem from the Chinese SoSo Search Engine, which involves logisticregression with $N \approx 2.4 \times 10^8$ samples and $d \approx 740,000$covariates.
arxiv-4200-186 | A quantum teleportation inspired algorithm produces sentence meaning from word meaning and grammatical structure | http://arxiv.org/pdf/1305.0556v2.pdf | author:Stephen Clark, Bob Coecke, Edward Grefenstette, Stephen Pulman, Mehrnoosh Sadrzadeh category:cs.CL quant-ph 68T50 I.2.7 published:2013-05-02 summary:We discuss an algorithm which produces the meaning of a sentence givenmeanings of its words, and its resemblance to quantum teleportation. In fact,this protocol was the main source of inspiration for this algorithm which hasmany applications in the area of Natural Language Processing.
arxiv-4200-187 | Guarantees of Total Variation Minimization for Signal Recovery | http://arxiv.org/pdf/1301.6791v6.pdf | author:Jian-Feng Cai, Weiyu Xu category:cs.IT cs.CV cs.LG math.IT published:2013-01-28 summary:In this paper, we consider using total variation minimization to recoversignals whose gradients have a sparse support, from a small number ofmeasurements. We establish the proof for the performance guarantee of totalvariation (TV) minimization in recovering \emph{one-dimensional} signal withsparse gradient support. This partially answers the open problem of proving thefidelity of total variation minimization in such a setting \cite{TVMulti}. Inparticular, we have shown that the recoverable gradient sparsity can growlinearly with the signal dimension when TV minimization is used. Recoverablesparsity thresholds of TV minimization are explicitly computed for1-dimensional signal by using the Grassmann angle framework. We also extend ourresults to TV minimization for multidimensional signals. Stability ofrecovering signal itself using 1-D TV minimization has also been establishedthrough a property called "almost Euclidean property for 1-dimensional TVnorm". We further give a lower bound on the number of random Gaussianmeasurements for recovering 1-dimensional signal vectors with $N$ elements and$K$-sparse gradients. Interestingly, the number of needed measurements is lowerbounded by $\Omega((NK)^{\frac{1}{2}})$, rather than the $O(K\log(N/K))$ boundfrequently appearing in recovering $K$-sparse signal vectors.
arxiv-4200-188 | PACE: Pattern Accurate Computationally Efficient Bootstrapping for Timely Discovery of Cyber-Security Concepts | http://arxiv.org/pdf/1308.4648v3.pdf | author:Nikki McNeil, Robert A. Bridges, Michael D. Iannacone, Bogdan Czejdo, Nicolas Perez, John R. Goodall category:cs.IR cs.CL IEEE published:2013-08-21 summary:Public disclosure of important security information, such as knowledge ofvulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, andother online sources months before proper classification into structureddatabases. In order to facilitate timely discovery of such knowledge, wepropose a novel semi-supervised learning algorithm, PACE, for identifying andclassifying relevant entities in text sources. The main contribution of thispaper is an enhancement of the traditional bootstrapping method for entityextraction by employing a time-memory trade-off that simultaneously circumventsa costly corpus search while strengthening pattern nomination, which shouldincrease accuracy. An implementation in the cyber-security domain is discussedas well as challenges to Natural Language Processing imposed by the securitydomain.
arxiv-4200-189 | Deep Multiple Kernel Learning | http://arxiv.org/pdf/1310.3101v1.pdf | author:Eric Strobl, Shyam Visweswaran category:stat.ML cs.LG published:2013-10-11 summary:Deep learning methods have predominantly been applied to large artificialneural networks. Despite their state-of-the-art performance, these largenetworks typically do not generalize well to datasets with limited samplesizes. In this paper, we take a different approach by learning multiple layersof kernels. We combine kernels at each layer and then optimize over an estimateof the support vector machine leave-one-out error rather than the dualobjective function. Our experiments on a variety of datasets show that eachlayer successively increases performance with only a few base kernels.
arxiv-4200-190 | Two discussions of the paper "Bayesian measures of model complexity and fit" by D. Spiegelhalter et al., Read before The Royal Statistical Society at a meeting organized by the Research Section on Wednesday, March 13th, 2002 | http://arxiv.org/pdf/1310.2905v2.pdf | author:E. Moreno, F. -J. Vazquez-Polo, C. P. Robert category:stat.ME stat.ML published:2013-10-10 summary:These are the written discussions of the paper "Bayesian measures of modelcomplexity and fit" by D. Spiegelhalter et al. (2002), following thediscussions given at the Annual Meeting of the Royal Statistical Society inNewcastle-upon-Tyne on September 3rd, 2013.
arxiv-4200-191 | Flexible High-dimensional Classification Machines and Their Asymptotic Properties | http://arxiv.org/pdf/1310.3004v1.pdf | author:Xingye Qiao, Lingsong Zhang category:stat.ML published:2013-10-11 summary:Classification is an important topic in statistics and machine learning withgreat potential in many real applications. In this paper, we investigate twopopular large margin classification methods, Support Vector Machine (SVM) andDistance Weighted Discrimination (DWD), under two contexts: thehigh-dimensional, low-sample size data and the imbalanced data. A unifiedfamily of classification machines, the FLexible Assortment MachinE (FLAME) isproposed, within which DWD and SVM are special cases. The FLAME family helps toidentify the similarities and differences between SVM and DWD. It is well knownthat many classifiers overfit the data in the high-dimensional setting; andothers are sensitive to the imbalanced data, that is, the class with a largersample size overly influences the classifier and pushes the decision boundarytowards the minority class. SVM is resistant to the imbalanced data issue, butit overfits high-dimensional data sets by showing the undesired data-pilingphenomena. The DWD method was proposed to improve SVM in the high-dimensionalsetting, but its decision boundary is sensitive to the imbalanced ratio ofsample sizes. Our FLAME family helps to understand an intrinsic connectionbetween SVM and DWD, and improves both methods by providing a better trade-offbetween sensitivity to the imbalanced data and overfitting the high-dimensionaldata. Several asymptotic properties of the FLAME classifiers are studied.Simulations and real data applications are investigated to illustrate theusefulness of the FLAME classifiers.
arxiv-4200-192 | Spontaneous Analogy by Piggybacking on a Perceptual System | http://arxiv.org/pdf/1310.2955v1.pdf | author:Marc Pickett, David W. Aha category:cs.AI cs.LG published:2013-10-10 summary:Most computational models of analogy assume they are given a delineatedsource domain and often a specified target domain. These systems do not addresshow analogs can be isolated from large domains and spontaneously retrieved fromlong-term memory, a process we call spontaneous analogy. We present a systemthat represents relational structures as feature bags. Using thisrepresentation, our system leverages perceptual algorithms to automaticallycreate an ontology of relational structures and to efficiently retrieve analogsfor new relational structures from long-term memory. We provide a demonstrationof our approach that takes a set of unsegmented stories, constructs an ontologyof analogical schemas (corresponding to plot devices), and uses this ontologyto efficiently find analogs within new stories, yielding significanttime-savings over linear analog retrieval at a small accuracy cost.
arxiv-4200-193 | Privacy Aware Learning | http://arxiv.org/pdf/1210.2085v2.pdf | author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright category:stat.ML cs.IT cs.LG math.IT published:2012-10-07 summary:We study statistical risk minimization problems under a privacy model inwhich the data is kept confidential even from the learner. In this localprivacy framework, we establish sharp upper and lower bounds on the convergencerates of statistical estimation procedures. As a consequence, we exhibit aprecise tradeoff between the amount of privacy the data preserves and theutility, as measured by convergence rate, of any statistical estimator orlearning procedure.
arxiv-4200-194 | Robust Dequantized Compressive Sensing | http://arxiv.org/pdf/1207.0577v2.pdf | author:Ji Liu, Stephen J. Wright category:stat.ML cs.LG published:2012-07-03 summary:We consider the reconstruction problem in compressed sensing in which theobservations are recorded in a finite number of bits. They may thus containquantization errors (from being rounded to the nearest representable value) andsaturation errors (from being outside the range of representable values). Ourformulation has an objective of weighted $\ell_2$-$\ell_1$ type, along withconstraints that account explicitly for quantization and saturation errors, andis solved with an augmented Lagrangian method. We prove a consistency resultfor the recovered solution, stronger than those that have appeared to date inthe literature, showing in particular that asymptotic consistency can beobtained without oversampling. We present extensive computational comparisonswith formulations proposed previously, and variants thereof.
arxiv-4200-195 | Wavelet methods for shape perception in electro-sensing | http://arxiv.org/pdf/1310.2842v1.pdf | author:Habib Ammari, Stéphane Mallat, Irène Waldspurger, Han Wang category:math.NA cs.CV published:2013-10-10 summary:This paper aims at presenting a new approach to the electro-sensing problemusing wavelets. It provides an efficient algorithm for recognizing the shape ofa target from micro-electrical impedance measurements. Stability and resolutioncapabilities of the proposed algorithm are quantified in numerical simulations.
arxiv-4200-196 | Kronecker Sum Decompositions of Space-Time Data | http://arxiv.org/pdf/1307.7306v2.pdf | author:Kristjan Greenewald, Theodoros Tsiligkaridis, Alfred O Hero III category:stat.ME stat.ML published:2013-07-27 summary:In this paper we consider the use of the space vs. time Kronecker productdecomposition in the estimation of covariance matrices for spatio-temporaldata. This decomposition imposes lower dimensional structure on the estimatedcovariance matrix, thus reducing the number of samples required for estimation.To allow a smooth tradeoff between the reduction in the number of parameters(to reduce estimation variance) and the accuracy of the covarianceapproximation (affecting estimation bias), we introduce a diagonally loadedmodification of the sum of kronecker products representation [1]. We derive aCramer-Rao bound (CRB) on the minimum attainable mean squared predictorcoefficient estimation error for unbiased estimators of Kronecker structuredcovariance matrices. We illustrate the accuracy of the diagonally loadedKronecker sum decomposition by applying it to video data of human activity.
arxiv-4200-197 | Gibbs Max-margin Topic Models with Data Augmentation | http://arxiv.org/pdf/1310.2816v1.pdf | author:Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang category:stat.ML cs.LG stat.CO stat.ME published:2013-10-10 summary:Max-margin learning is a powerful approach to building classifiers andstructured output predictors. Recent work on max-margin supervised topic modelshas successfully integrated it with Bayesian topic models to discoverdiscriminative latent semantic structures and make accurate predictions forunseen testing data. However, the resulting learning problems are usually hardto solve because of the non-smoothness of the margin loss. Existing approachesto building max-margin supervised topic models rely on an iterative procedureto solve multiple latent SVM subproblems with additional mean-field assumptionson the desired posterior distributions. This paper presents an alternativeapproach by defining a new max-margin loss. Namely, we present Gibbs max-marginsupervised topic models, a latent variable Gibbs classifier to discover hiddentopic representations for various tasks, including classification, regressionand multi-task learning. Gibbs max-margin supervised topic models minimize anexpected margin loss, which is an upper bound of the existing margin lossderived from an expected prediction rule. By introducing augmented variablesand integrating out the Dirichlet variables analytically by conjugacy, wedevelop simple Gibbs sampling algorithms with no restricting assumptions and noneed to solve SVM subproblems. Furthermore, each step of the"augment-and-collapse" Gibbs sampling algorithms has an analytical conditionaldistribution, from which samples can be easily drawn. Experimental resultsdemonstrate significant improvements on time efficiency. The classificationperformance is also significantly improved over competitors on binary,multi-class and multi-label classification tasks.
arxiv-4200-198 | MizAR 40 for Mizar 40 | http://arxiv.org/pdf/1310.2805v1.pdf | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO cs.MS published:2013-10-10 summary:As a present to Mizar on its 40th anniversary, we develop an AI/ATP systemthat in 30 seconds of real time on a 14-CPU machine automatically proves 40% ofthe theorems in the latest official version of the Mizar Mathematical Library(MML). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML. To achieve that, a large suiteof AI/ATP methods is employed and further developed. We implement the mostuseful methods efficiently, to scale them to the 150000 formulas in MML. Thisreduces the training times over the corpus to 1-3 seconds, allowing a simplepractical deployment of the methods in the online automated reasoning servicefor the Mizar users (MizAR).
arxiv-4200-199 | Lemma Mining over HOL Light | http://arxiv.org/pdf/1310.2797v1.pdf | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO published:2013-10-10 summary:Large formal mathematical libraries consist of millions of atomic inferencesteps that give rise to a corresponding number of proved statements (lemmas).Analogously to the informal mathematical practice, only a tiny fraction of suchstatements is named and re-used in later proofs by formal mathematicians. Inthis work, we suggest and implement criteria defining the estimated usefulnessof the HOL Light lemmas for proving further theorems. We use these criteria tomine the large inference graph of all lemmas in the core HOL Light library,adding thousands of the best lemmas to the pool of named statements that can bere-used in later proofs. The usefulness of the new lemmas is then evaluated bycomparing the performance of automated proving of the core HOL Light theoremswith and without such added lemmas.
arxiv-4200-200 | Bayesian Estimation of White Matter Atlas from High Angular Resolution Diffusion Imaging | http://arxiv.org/pdf/1310.3233v1.pdf | author:Jia Du, Alvina Goh, Anqi Qiu category:cs.CV published:2013-10-10 summary:We present a Bayesian probabilistic model to estimate the brain white matteratlas from high angular resolution diffusion imaging (HARDI) data. This modelincorporates a shape prior of the white matter anatomy and the likelihood ofindividual observed HARDI datasets. We first assume that the atlas is generatedfrom a known hyperatlas through a flow of diffeomorphisms and its shape priorcan be constructed based on the framework of large deformation diffeomorphicmetric mapping (LDDMM). LDDMM characterizes a nonlinear diffeomorphic shapespace in a linear space of initial momentum uniquely determining diffeomorphicgeodesic flows from the hyperatlas. Therefore, the shape prior of the HARDIatlas can be modeled using a centered Gaussian random field (GRF) model of theinitial momentum. In order to construct the likelihood of observed HARDIdatasets, it is necessary to study the diffeomorphic transformation ofindividual observations relative to the atlas and the probabilisticdistribution of orientation distribution functions (ODFs). To this end, weconstruct the likelihood related to the transformation using the sameconstruction as discussed for the shape prior of the atlas. The probabilisticdistribution of ODFs is then constructed based on the ODF Riemannian manifold.We assume that the observed ODFs are generated by an exponential map of randomtangent vectors at the deformed atlas ODF. Hence, the likelihood of the ODFscan be modeled using a GRF of their tangent vectors in the ODF Riemannianmanifold. We solve for the maximum a posteriori using theExpectation-Maximization algorithm and derive the corresponding updateequations. Finally, we illustrate the HARDI atlas constructed based on aChinese aging cohort of 94 adults and compare it with that generated byaveraging the coefficients of spherical harmonics of the ODF across subjects.
arxiv-4200-201 | Localized Iterative Methods for Interpolation in Graph Structured Data | http://arxiv.org/pdf/1310.2646v1.pdf | author:Sunil K. Narang, Akshay Gadde, Eduard Sanou, Antonio Ortega category:cs.LG published:2013-10-09 summary:In this paper, we present two localized graph filtering based methods forinterpolating graph signals defined on the vertices of arbitrary graphs fromonly a partial set of samples. The first method is an extension of previouswork on reconstructing bandlimited graph signals from partially observedsamples. The iterative graph filtering approach very closely approximates thesolution proposed in the that work, while being computationally more efficient.As an alternative, we propose a regularization based framework in which wedefine the cost of reconstruction to be a combination of smoothness of thegraph signal and the reconstruction error with respect to the known samples,and find solutions that minimize this cost. We provide both a closed formsolution and a computationally efficient iterative solution of the optimizationproblem. The experimental results on the recommendation system datasetsdemonstrate effectiveness of the proposed methods.
arxiv-4200-202 | Duality in Graphical Models | http://arxiv.org/pdf/1310.2641v1.pdf | author:Dhafer Malouche, Bala Rajaratnam, Benjamin T. Rolfs category:math.PR stat.ML published:2013-10-09 summary:Graphical models have proven to be powerful tools for representinghigh-dimensional systems of random variables. One example of such a model isthe undirected graph, in which lack of an edge represents conditionalindependence between two random variables given the rest. Another example isthe bidirected graph, in which absence of edges encodes pairwise marginalindependence. Both of these classes of graphical models have been extensivelystudied, and while they are considered to be dual to one another, except in afew instances this duality has not been thoroughly investigated. In this paper,we demonstrate how duality between undirected and bidirected models can be usedto transport results for one class of graphical models to the dual model in atransparent manner. We proceed to apply this technique to extend previouslyexisting results as well as to prove new ones, in three important domains.First, we discuss the pairwise and global Markov properties for undirected andbidirected models, using the pseudographoid and reverse-pseudographoid ruleswhich are weaker conditions than the typically used intersection andcomposition rules. Second, we investigate these pseudographoid and reversepseudographoid rules in the context of probability distributions, using theconcept of duality in the process. Duality allows us to quickly relate them tothe more familiar intersection and composition properties. Third and finally,we apply the dualization method to understand the implications of faithfulness,which in turn leads to a more general form of an existing result.
arxiv-4200-203 | Confidence-constrained joint sparsity recovery under the Poisson noise model | http://arxiv.org/pdf/1309.1193v2.pdf | author:E. Chunikhina, R. Raich, T. Nguyen category:stat.ML cs.LG published:2013-09-04 summary:Our work is focused on the joint sparsity recovery problem where the commonsparsity pattern is corrupted by Poisson noise. We formulate theconfidence-constrained optimization problem in both least squares (LS) andmaximum likelihood (ML) frameworks and study the conditions for perfectreconstruction of the original row sparsity and row sparsity pattern. However,the confidence-constrained optimization problem is non-convex. Using convexrelaxation, an alternative convex reformulation of the problem is proposed. Weevaluate the performance of the proposed approach using simulation results onsynthetic data and show the effectiveness of proposed row sparsity and rowsparsity pattern recovery framework.
arxiv-4200-204 | Understanding Boltzmann Machine and Deep Learning via A Confident Information First Principle | http://arxiv.org/pdf/1302.3931v7.pdf | author:Xiaozhao Zhao, Yuexian Hou, Qian Yu, Dawei Song, Wenjie Li category:cs.NE cs.LG stat.ML published:2013-02-16 summary:Typical dimensionality reduction methods focus on directly reducing thenumber of random variables while retaining maximal variations in the data. Inthis paper, we consider the dimensionality reduction in parameter spaces ofbinary multivariate distributions. We propose a generalConfident-Information-First (CIF) principle to maximally preserve parameterswith confident estimates and rule out unreliable or noisy parameters. Formally,the confidence of a parameter can be assessed by its Fisher information, whichestablishes a connection with the inverse variance of any unbiased estimate forthe parameter via the Cram\'{e}r-Rao bound. We then revisit Boltzmann machines(BM) and theoretically show that both single-layer BM without hidden units(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.This can not only help us uncover and formalize the essential parts of thetarget density that SBM and RBM capture, but also suggest that the deep neuralnetwork consisting of several layers of RBM can be seen as the layer-wiseapplication of CIF. Guided by the theoretical analysis, we develop asample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM anda CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP arestudied in a series of density estimation experiments.
arxiv-4200-205 | Towards common-sense reasoning via conditional simulation: legacies of Turing in Artificial Intelligence | http://arxiv.org/pdf/1212.4799v2.pdf | author:Cameron E. Freer, Daniel M. Roy, Joshua B. Tenenbaum category:cs.AI math.LO stat.ML published:2012-12-19 summary:The problem of replicating the flexibility of human common-sense reasoninghas captured the imagination of computer scientists since the early days ofAlan Turing's foundational work on computation and the philosophy of artificialintelligence. In the intervening years, the idea of cognition as computationhas emerged as a fundamental tenet of Artificial Intelligence (AI) andcognitive science. But what kind of computation is cognition? We describe a computational formalism centered around a probabilistic Turingmachine called QUERY, which captures the operation of probabilisticconditioning via conditional simulation. Through several examples and analyses,we demonstrate how the QUERY abstraction can be used to cast common-sensereasoning as probabilistic inference in a statistical model of our observationsand the uncertain structure of the world that generated that experience. Thisformulation is a recent synthesis of several research programs in AI andcognitive science, but it also represents a surprising convergence of severalof Turing's pioneering insights in AI, the foundations of computation, andstatistics.
arxiv-4200-206 | M-Power Regularized Least Squares Regression | http://arxiv.org/pdf/1310.2451v1.pdf | author:Julien Audiffren, Hachem Kadri category:stat.ML cs.LG published:2013-10-09 summary:Regularization is used to find a solution that both fits the data and issufficiently smooth, and thereby is very effective for designing and refininglearning algorithms. But the influence of its exponent remains poorlyunderstood. In particular, it is unclear how the exponent of the reproducingkernel Hilbert space (RKHS) regularization term affects the accuracy and theefficiency of kernel-based learning algorithms. Here we consider regularizedleast squares regression (RLSR) with an RKHS regularization raised to the powerof m, where m is a variable real exponent. We design an efficient algorithm forsolving the associated minimization problem, we provide a theoretical analysisof its stability, and we {compare it %/ demonstrate its advantage with respectto computational complexity, speed of convergence and prediction accuracy to%/over} the classical kernel ridge regression algorithm where theregularization exponent m is fixed at 2. Our results show that the m-power RLSRproblem can be solved efficiently, and support the suggestion that one can usea regularization term that grows significantly slower than the standardquadratic growth in the RKHS norm.}
arxiv-4200-207 | Development of Marathi Part of Speech Tagger Using Statistical Approach | http://arxiv.org/pdf/1310.0575v2.pdf | author:Jyoti Singh, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:Part-of-speech (POS) tagging is a process of assigning the words in a textcorresponding to a particular part of speech. A fundamental version of POStagging is the identification of words as nouns, verbs, adjectives etc. Forprocessing natural languages, Part of Speech tagging is a prominent tool. It isone of the simplest as well as most constant and statistical model for many NLPapplications. POS Tagging is an initial stage of linguistics, text analysislike information retrieval, machine translator, text to speech synthesis,information extraction etc. In POS Tagging we assign a Part of Speech tag toeach word in a sentence and literature. Various approaches have been proposedto implement POS taggers. In this paper we present a Marathi part of speechtagger. It is morphologically rich language. Marathi is spoken by the nativepeople of Maharashtra. The general approach used for development of tagger isstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clearidea about all the algorithms with suitable examples. It also introduces a tagset for Marathi which can be used for tagging Marathi text. In this paper wehave shown the development of the tagger as well as compared to check theaccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,Trigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%respectively.
arxiv-4200-208 | Learning-Based Procedural Content Generation | http://arxiv.org/pdf/1308.6415v2.pdf | author:Jonathan Roberts, Ke Chen category:cs.AI cs.HC cs.LG cs.NE published:2013-08-29 summary:Procedural content generation (PCG) has recently become one of the hottesttopics in computational intelligence and AI game researches. Among a variety ofPCG techniques, search-based approaches overwhelmingly dominate PCG developmentat present. While SBPCG leads to promising results and successful applications,it poses a number of challenges ranging from representation to evaluation ofthe content being generated. In this paper, we present an alternative yetgeneric PCG framework, named learning-based procedure content generation(LBPCG), to provide potential solutions to several challenging problems inexisting PCG techniques. By exploring and exploiting information gained in gamedevelopment and public beta test via data-driven learning, our framework cangenerate robust content adaptable to end-user or target players on-line withminimal interruption to their experience. Furthermore, we develop enablingtechniques to implement the various models required in our framework. For aproof of concept, we have developed a prototype based on the classic opensource first-person shooter game, Quake. Simulation results suggest that ourframework is promising in generating quality content.
arxiv-4200-209 | Discriminative Relational Topic Models | http://arxiv.org/pdf/1310.2409v1.pdf | author:Ning Chen, Jun Zhu, Fei Xia, Bo Zhang category:cs.LG cs.IR stat.ML published:2013-10-09 summary:Many scientific and engineering fields involve analyzing network data. Fordocument networks, relational topic models (RTMs) provide a probabilisticgenerative process to describe both the link structure and document contents,and they have shown promise on predicting network structures and discoveringlatent topic representations. However, existing RTMs have limitations in boththe restricted model expressiveness and incapability of dealing with imbalancednetwork data. To expand the scope and improve the inference accuracy of RTMs,this paper presents three extensions: 1) unlike the common link likelihood witha diagonal weight matrix that allows the-same-topic interactions only, wegeneralize it to use a full weight matrix that captures all pairwise topicinteractions and is applicable to asymmetric networks; 2) instead of doingstandard Bayesian inference, we perform regularized Bayesian inference(RegBayes) with a regularization parameter to deal with the imbalanced linkstructure issue in common real networks and improve the discriminative abilityof learned latent representations; and 3) instead of doing variationalapproximation with strict mean-field assumptions, we present collapsed Gibbssampling algorithms for the generalized relational topic models by exploringdata augmentation without making restricting assumptions. Under the genericRegBayes framework, we carefully investigate two popular discriminative lossfunctions, namely, the logistic log-loss and the max-margin hinge loss.Experimental results on several real network datasets demonstrate thesignificance of these extensions on improving the prediction performance, andthe time efficiency can be dramatically improved with a simple fastapproximation method.
arxiv-4200-210 | Improved Bayesian Logistic Supervised Topic Models with Data Augmentation | http://arxiv.org/pdf/1310.2408v1.pdf | author:Jun Zhu, Xun Zheng, Bo Zhang category:cs.LG cs.CL stat.AP stat.ML published:2013-10-09 summary:Supervised topic models with a logistic likelihood have two issues thatpotentially limit their practical use: 1) response variables are usuallyover-weighted by document word counts; and 2) existing variational inferencemethods make strict mean-field assumptions. We address these issues by: 1)introducing a regularization constant to better balance the two parts based onan optimization formulation of Bayesian inference; and 2) developing a simpleGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables andcollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithmhas analytical forms of each conditional distribution without making anyrestricting assumptions and can be easily parallelized. Empirical resultsdemonstrate significant improvements on prediction performance and timeefficiency.
arxiv-4200-211 | Neural perceptual model to global-local vision for recognition of the logical structure of administrative documents | http://arxiv.org/pdf/1310.7440v1.pdf | author:Boulbaba Ben Ammar category:cs.CV published:2013-10-09 summary:This paper gives the definition of Transparent Neural Network "TNN" for thesimulation of the globallocal vision and its application to the segmentation ofadministrative document image. We have developed and have adapted a recognitionmethod which models the contextual effects reported from studies inexperimental psychology. Then, we evaluated and tested the TNN and themulti-layer perceptron "MLP", which showed its effectiveness in the field ofthe recognition, in order to show that the TNN is clearer for the user and morepowerful on the level of the recognition. Indeed, the TNN is the only systemwhich makes it possible to recognize the document and its structure.
arxiv-4200-212 | The Generalized Traveling Salesman Problem solved with Ant Algorithms | http://arxiv.org/pdf/1310.2350v1.pdf | author:Camelia-M. Pintea, Petrica C. Pop, Camelia Chira category:cs.AI cs.NE published:2013-10-09 summary:A well known N P-hard problem called the Generalized Traveling SalesmanProblem (GTSP) is considered. In GTSP the nodes of a complete undirected graphare partitioned into clusters. The objective is to find a minimum cost tourpassing through exactly one node from each cluster. An exact exponential timealgorithm and an effective meta-heuristic algorithm for the problem arepresented. The meta-heuristic proposed is a modified Ant Colony System (ACS)algorithm called Reinforcing Ant Colony System (RACS) which introduces newcorrection rules in the ACS algorithm. Computational results are reported formany standard test problems. The proposed algorithm is competitive with theother already proposed heuristics for the GTSP in both solution quality andcomputational time.
arxiv-4200-213 | Simplifying Energy Optimization using Partial Enumeration | http://arxiv.org/pdf/1303.1749v2.pdf | author:Carl Olsson, Johannes Ulen, Yuri Boykov, Vladimir Kolmogorov category:cs.CV published:2013-03-07 summary:Energies with high-order non-submodular interactions have been shown to bevery useful in vision due to their high modeling power. Optimization of suchenergies, however, is generally NP-hard. A naive approach that works for smallproblem instances is exhaustive search, that is, enumeration of all possiblelabelings of the underlying graph. We propose a general minimization approachfor large graphs based on enumeration of labelings of certain small patches.This partial enumeration technique reduces complex high-order energyformulations to pairwise Constraint Satisfaction Problems with unary costs(uCSP), which can be efficiently solved using standard methods like TRW-S. Ourapproach outperforms a number of existing state-of-the-art algorithms on wellknown difficult problems (e.g. curvature regularization, stereo,deconvolution); it gives near global minimum and better speed. Our main application of interest is curvature regularization. In the contextof segmentation, our partial enumeration technique allows to evaluate curvaturedirectly on small patches using a novel integral geometry approach.
arxiv-4200-214 | Treating clitics with minimalist grammars | http://arxiv.org/pdf/1310.2527v1.pdf | author:Maxime Amblard category:cs.CL cs.LO published:2013-10-08 summary:We propose an extension of Stabler's version of clitics treatment for a widercoverage of the French language. For this, we present the lexical entriesneeded in the lexicon. Then, we show the recognition of complex syntacticphenomena as (left and right) dislo- cation, clitic climbing over modal andextraction from determiner phrase. The aim of this presentation is thesyntax-semantic interface for clitics analyses in which we will stress onclitic climbing over verb and raising verb.
arxiv-4200-215 | Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet | http://arxiv.org/pdf/1401.3230v1.pdf | author:K Paramesha, K C Ravishankar category:cs.CL cs.IR published:2013-10-08 summary:The task of sentiment analysis of reviews is carried out using manually built/ automatically generated lexicon resources of their own with which terms arematched with lexicon to compute the term count for positive and negativepolarity. On the other hand the Sentiwordnet, which is quite different fromother lexicon resources that gives scores (weights) of the positive andnegative polarity for each word. The polarity of a word namely positive,negative and neutral have the score ranging between 0 to 1 indicates thestrength/weight of the word with that sentiment orientation. In this paper, weshow that using the Sentiwordnet, how we could enhance the performance of theclassification at both sentence and document level.
arxiv-4200-216 | Graphical law beneath each written natural language | http://arxiv.org/pdf/1307.6235v4.pdf | author:Anindya Kumar Biswas category:physics.gen-ph cs.CL published:2013-07-18 summary:We study twenty four written natural languages. We draw in the log scale,number of words starting with a letter vs rank of the letter, both normalised.We find that all the graphs are of the similar type. The graphs aretantalisingly closer to the curves of reduced magnetisation vs reducedtemperature for magnetic materials. We make a weak conjecture that a curve ofmagnetisation underlies a written natural language.
arxiv-4200-217 | A Robust Variational Model for Positive Image Deconvolution | http://arxiv.org/pdf/1310.2085v1.pdf | author:Martin Welk category:cs.CV published:2013-10-08 summary:In this paper, an iterative method for robust deconvolution with positivityconstraints is discussed. It is based on the known variational interpretationof the Richardson-Lucy iterative deconvolution as fixed-point iteration for theminimisation of an information divergence functional under a multiplicativeperturbation model. The asymmetric penaliser function involved in thisfunctional is then modified into a robust penaliser, and complemented with aregulariser. The resulting functional gives rise to a fixed point iterationthat we call robust and regularised Richardson-Lucy deconvolution. It achievesan image restoration quality comparable to state-of-the-art robust variationaldeconvolution with a computational efficiency similar to that of the originalRichardson-Lucy method. Experiments on synthetic and real-world image datademonstrate the performance of the proposed method.
arxiv-4200-218 | Predicting Students' Performance Using ID3 And C4.5 Classification Algorithms | http://arxiv.org/pdf/1310.2071v1.pdf | author:Kalpesh Adhatrao, Aditya Gaykar, Amiraj Dhawan, Rohit Jha, Vipul Honrao category:cs.CY cs.LG published:2013-10-08 summary:An educational institution needs to have an approximate prior knowledge ofenrolled students to predict their performance in future academics. This helpsthem to identify promising students and also provides them an opportunity topay attention to and improve those who would probably get lower grades. As asolution, we have developed a system which can predict the performance ofstudents from their previous performances using concepts of data miningtechniques under Classification. We have analyzed the data set containinginformation about students, such as gender, marks scored in the boardexaminations of classes X and XII, marks and rank in entrance examinations andresults in first year of the previous batch of students. By applying the ID3(Iterative Dichotomiser 3) and C4.5 classification algorithms on this data, wehave predicted the general and individual performance of freshly admittedstudents in future examinations.
arxiv-4200-219 | Distributed Coordinate Descent Method for Learning with Big Data | http://arxiv.org/pdf/1310.2059v1.pdf | author:Peter Richtárik, Martin Takáč category:stat.ML cs.DC cs.LG math.OC published:2013-10-08 summary:In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent methodfor solving loss minimization problems with big data. We initially partitionthe coordinates (features) and assign each partition to a different node of acluster. At every iteration, each node picks a random subset of the coordinatesfrom those it owns, independently from the other computers, and in parallelcomputes and applies updates to the selected coordinates based on a simpleclosed-form formula. We give bounds on the number of iterations sufficient toapproximately solve the problem with high probability, and show how it dependson the data and on the partitioning. We perform numerical experiments with aLASSO instance described by a 3TB matrix.
arxiv-4200-220 | The role of RGB-D benchmark datasets: an overview | http://arxiv.org/pdf/1310.2053v1.pdf | author:Kai Berger category:cs.CV published:2013-10-08 summary:The advent of the Microsoft Kinect three years ago stimulated not only thecomputer vision community for new algorithms and setups to tackle well-knownproblems in the community but also sparked the launch of several new benchmarkdatasets to which future algorithms can be compared 019 to. This review of theliterature and industry developments concludes that the current RGB-D benchmarkdatasets can be useful to determine the accuracy of a variety of applicationsof a single or multiple RGB-D sensors.
arxiv-4200-221 | A State Of the Art Report on Research in Multiple RGB-D sensor Setups | http://arxiv.org/pdf/1310.2050v1.pdf | author:Kai Berger category:cs.CV published:2013-10-08 summary:That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and endconsumer sector has been anticipated by the developers. That it also impactedin rigorous computer vision research has probably been a surprise to the wholecommunity. Shortly before the commercial deployment of its successor, KinectOne, the research literature fills with resumees and state-of-the art papers tosummarize the development over the past 3 years. This particular reportdescribes significant research projects which have built on sensoring setupsthat include two or more RGB-D sensors in one scene.
arxiv-4200-222 | Fast Multi-Instance Multi-Label Learning | http://arxiv.org/pdf/1310.2049v1.pdf | author:Sheng-Jun Huang, Zhi-Hua Zhou category:cs.LG published:2013-10-08 summary:In many real-world tasks, particularly those involving data objects withcomplicated semantics such as images and texts, one object can be representedby multiple instances and simultaneously be associated with multiple labels.Such tasks can be formulated as multi-instance multi-label learning (MIML)problems, and have been extensively studied during the past few years. ExistingMIML approaches have been found useful in many applications; however, most ofthem can only handle moderate-sized data. To efficiently handle large datasets, in this paper we propose the MIMLfast approach, which first constructs alow-dimensional subspace shared by all labels, and then trains label specificlinear models to optimize approximated ranking loss via stochastic gradientdescent. Although the MIML problem is complicated, MIMLfast is able to achieveexcellent performance by exploiting label relations with shared space anddiscovering sub-concepts for complicated labels. Experiments show that theperformance of MIMLfast is highly competitive to state-of-the-art techniques,whereas its time cost is much less; particularly, on a data set with 20K bagsand 180K instances, MIMLfast is more than 100 times faster than existing MIMLapproaches. On a larger data set where none of existing approaches can returnresults in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach isable to identify the most representative instance for each label, and thusproviding a chance to understand the relation between input patterns and outputlabel semantics.
arxiv-4200-223 | Smoothness-Constrained Image Recovery from Block-Based Random Projections | http://arxiv.org/pdf/1310.7813v1.pdf | author:Giulio Coluccia, Diego Valsesia, Enrico Magli category:cs.CV cs.IT math.IT published:2013-10-08 summary:In this paper we address the problem of visual quality of imagesreconstructed from block-wise random projections. Independent reconstruction ofthe blocks can severely affect visual quality, by displaying artifacts alongblock borders. We propose a method to enforce smoothness across block bordersby modifying the sensing and reconstruction process so as to employ partiallyoverlapping blocks. The proposed algorithm accomplishes this by computing afast preview from the blocks, whose purpose is twofold. On one hand, it allowsto enforce a set of constraints to drive the reconstruction algorithm towards asmooth solution, imposing the similarity of block borders. On the other hand,the preview is used as a predictor of the entire block, allowing to recover theprediction error, only. The quality improvement over the result of independentreconstruction can be easily assessed both visually and in terms of PSNR andSSIM index.
arxiv-4200-224 | Learning with Submodular Functions: A Convex Optimization Perspective | http://arxiv.org/pdf/1111.6453v2.pdf | author:Francis Bach category:cs.LG math.OC published:2011-11-28 summary:Submodular functions are relevant to machine learning for at least tworeasons: (1) some problems may be expressed directly as the optimization ofsubmodular functions and (2) the lovasz extension of submodular functionsprovides a useful set of regularization functions for supervised andunsupervised learning. In this monograph, we present the theory of submodularfunctions from a convex analysis perspective, presenting tight links betweencertain polyhedra, combinatorial optimization and convex optimization problems.In particular, we show how submodular function minimization is equivalent tosolving a wide variety of convex optimization problems. This allows thederivation of new efficient algorithms for approximate and exact submodularfunction minimization with theoretical guarantees and good practicalperformance. By listing many examples of submodular functions, we reviewvarious applications to machine learning, such as clustering, experimentaldesign, sensor placement, graphical model structure learning or subsetselection, as well as a family of structured sparsity-inducing norms that canbe derived and used from submodular functions.
arxiv-4200-225 | Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization | http://arxiv.org/pdf/1309.2375v2.pdf | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG cs.NA stat.CO published:2013-09-10 summary:We introduce a proximal version of the stochastic dual coordinate ascentmethod and show how to accelerate the method using an inner-outer iterationprocedure. We analyze the runtime of the framework and obtain rates thatimprove state-of-the-art results for various key machine learning optimizationproblems including SVM, logistic regression, ridge regression, Lasso, andmulticlass SVM. Experiments validate our theoretical findings.
arxiv-4200-226 | Feature Selection Strategies for Classifying High Dimensional Astronomical Data Sets | http://arxiv.org/pdf/1310.1976v1.pdf | author:Ciro Donalek, Arun Kumar A., S. G. Djorgovski, Ashish A. Mahabal, Matthew J. Graham, Thomas J. Fuchs, Michael J. Turmon, N. Sajeeth Philip, Michael Ting-Chang Yang, Giuseppe Longo category:astro-ph.IM cs.CV published:2013-10-08 summary:The amount of collected data in many scientific fields is increasing, all ofthem requiring a common task: extract knowledge from massive, multi parametricdata sets, as rapidly and efficiently possible. This is especially true inastronomy where synoptic sky surveys are enabling new research frontiers in thetime domain astronomy and posing several new object classification challengesin multi dimensional spaces; given the high number of parameters available foreach object, feature selection is quickly becoming a crucial task in analyzingastronomical data sets. Using data sets extracted from the ongoing CatalinaReal-Time Transient Surveys (CRTS) and the Kepler Mission we illustrate avariety of feature selection strategies used to identify the subsets that givethe most information and the results achieved applying these techniques tothree major astronomical problems.
arxiv-4200-227 | ARKref: a rule-based coreference resolution system | http://arxiv.org/pdf/1310.1975v1.pdf | author:Brendan O'Connor, Michael Heilman category:cs.CL published:2013-10-08 summary:ARKref is a tool for noun phrase coreference. It is a deterministic,rule-based system that uses syntactic information from a constituent parser,and semantic information from an entity recognition component. Its architectureis based on the work of Haghighi and Klein (2009). ARKref was originallywritten in 2009. At the time of writing, the last released version was in March2011. This document describes that version, which is open-source and publiclyavailable at: http://www.ark.cs.cmu.edu/ARKref
arxiv-4200-228 | Named entity recognition using conditional random fields with non-local relational constraints | http://arxiv.org/pdf/1310.1964v1.pdf | author:Flavio Massimiliano Cecchini, Elisabetta Fersini category:cs.CL published:2013-10-07 summary:We begin by introducing the Computer Science branch of Natural LanguageProcessing, then narrowing the attention on its subbranch of InformationExtraction and particularly on Named Entity Recognition, discussing briefly itsmain methodological approaches. It follows an introduction to state-of-the-artConditional Random Fields under the form of linear chains. Subsequently, theidea of constrained inference as a way to model long-distance relationships ina text is presented, based on an Integer Linear Programming representation ofthe problem. Adding such relationships to the problem as automatically inferredlogical formulas, translatable into linear conditions, we propose to solve theresulting more complex problem with the aid of Lagrangian relaxation, of whichsome technical details are explained. Lastly, we give some experimentalresults.
arxiv-4200-229 | Bayesian Optimization With Censored Response Data | http://arxiv.org/pdf/1310.1947v1.pdf | author:Frank Hutter, Holger Hoos, Kevin Leyton-Brown category:cs.AI cs.LG stat.ML G.3; G.1.6 published:2013-10-07 summary:Bayesian optimization (BO) aims to minimize a given blackbox function using amodel that is updated whenever new evidence about the function becomesavailable. Here, we address the problem of BO under partially right-censoredresponse data, where in some evaluations we only obtain a lower bound on thefunction value. The ability to handle such response data allows us toadaptively censor costly function evaluations in minimization problems wherethe cost of a function evaluation corresponds to the function value. Oneimportant application giving rise to such censored data is theruntime-minimizing variant of the algorithm configuration problem: findingsettings of a given parametric algorithm that minimize the runtime required forsolving problem instances from a given distribution. We demonstrate thatterminating slow algorithm runs prematurely and handling the resultingright-censored observations can substantially improve the state of the art inmodel-based algorithm configuration.
arxiv-4200-230 | Discriminative Features via Generalized Eigenvectors | http://arxiv.org/pdf/1310.1934v1.pdf | author:Nikos Karampatziakis, Paul Mineiro category:cs.LG stat.ML published:2013-10-07 summary:Representing examples in a way that is compatible with the underlyingclassifier can greatly enhance the performance of a learning system. In thispaper we investigate scalable techniques for inducing discriminative featuresby taking advantage of simple second order structure in the data. We focus onmulticlass classification and show that features extracted from the generalizedeigenvectors of the class conditional second moments lead to classifiers withexcellent empirical performance. Moreover, these features have attractivetheoretical properties, such as inducing representations that are invariant tolinear transformations of the input. We evaluate classifiers built from thesefeatures on three different tasks, obtaining state of the art results.
arxiv-4200-231 | Singular Value Decomposition of Images from Scanned Photographic Plates | http://arxiv.org/pdf/1310.1869v1.pdf | author:Vasil Kolev, Katya Tsvetkova, Milcho Tsvetkov category:cs.CV astro-ph.IM cs.CE published:2013-10-07 summary:We want to approximate the mxn image A from scanned astronomical photographicplates (from the Sofia Sky Archive Data Center) by using far fewer entries thanin the original matrix. By using rank of a matrix, k we remove the redundantinformation or noise and use as Wiener filter, when rank k<m or k<n. With thisapproximation more than 98% compression ration of image of astronomical platewithout that image details, is obtained. The SVD of images from scannedphotographic plates (SPP) is considered and its possible image compression.
arxiv-4200-232 | Early Fire Detection Using HEP and Space-time Analysis | http://arxiv.org/pdf/1310.1855v1.pdf | author:Junzhou Chen, Yong You category:cs.CV cs.MM published:2013-10-07 summary:In this article, a video base early fire alarm system is developed bymonitoring the smoke in the scene. There are two major contributions in thiswork. First, to find the best texture feature for smoke detection, a generalframework, named Histograms of Equivalent Patterns (HEP), is adopted to achievean extensive evaluation of various kinds of texture features. Second, the\emph{Block based Inter-Frame Difference} (BIFD) and a improved version ofLBP-TOP are proposed and ensembled to describe the space-time characteristicsof the smoke. In order to reduce the false alarms, the Smoke History Image(SHI) is utilized to register the recent classification results of candidatesmoke blocks. Experimental results using SVM show that the proposed method canachieve better accuracy and less false alarm compared with the state-of-the-arttechnologies.
arxiv-4200-233 | Parallel coordinate descent for the Adaboost problem | http://arxiv.org/pdf/1310.1840v1.pdf | author:Olivier Fercoq category:cs.LG math.OC stat.ML published:2013-10-07 summary:We design a randomised parallel version of Adaboost based on previous studieson parallel coordinate descent. The algorithm uses the fact that the logarithmof the exponential loss is a function with coordinate-wise Lipschitz continuousgradient, in order to define the step lengths. We provide the proof ofconvergence for this randomised Adaboost algorithm and a theoreticalparallelisation speedup factor. We finally provide numerical examples onlearning problems of various sizes that show that the algorithm is competitivewith concurrent approaches, especially for large scale problems.
arxiv-4200-234 | Learning Non-Parametric Basis Independent Models from Point Queries via Low-Rank Methods | http://arxiv.org/pdf/1310.1826v1.pdf | author:Hemant Tyagi, Volkan Cevher category:stat.ML math.NA published:2013-10-07 summary:We consider the problem of learning multi-ridge functions of the form f(x) =g(Ax) from point evaluations of f. We assume that the function f is defined onan l_2-ball in R^d, g is twice continuously differentiable almost everywhere,and A \in R^{k \times d} is a rank k matrix, where k << d. We propose arandomized, polynomial-complexity sampling scheme for estimating suchfunctions. Our theoretical developments leverage recent techniques from lowrank matrix recovery, which enables us to derive a polynomial time estimator ofthe function f along with uniform approximation guarantees. We prove that ourscheme can also be applied for learning functions of the form: f(x) =\sum_{i=1}^{k} g_i(a_i^T x), provided f satisfies certain smoothness conditionsin a neighborhood around the origin. We also characterize the noise robustnessof the scheme. Finally, we present numerical examples to illustrate thetheoretical bounds in action.
arxiv-4200-235 | End-to-End Text Recognition with Hybrid HMM Maxout Models | http://arxiv.org/pdf/1310.1811v1.pdf | author:Ouais Alsharif, Joelle Pineau category:cs.CV published:2013-10-07 summary:The problem of detecting and recognizing text in natural scenes has proved tobe more challenging than its counterpart in documents, with most of theprevious work focusing on a single part of the problem. In this work, wepropose new solutions to the character and word recognition problems and thenshow how to combine these solutions in an end-to-end text-recognition system.We do so by leveraging the recently introduced Maxout networks along withhybrid HMM models that have proven useful for voice recognition. Using theseelements, we build a tunable and highly accurate recognition system that beatsstate-of-the-art results on all the sub-problems for both the ICDAR 2003 andSVT benchmark datasets.
arxiv-4200-236 | Generalized Negative Binomial Processes and the Representation of Cluster Structures | http://arxiv.org/pdf/1310.1800v1.pdf | author:Mingyuan Zhou category:stat.ME math.ST stat.ML stat.TH published:2013-10-07 summary:The paper introduces the concept of a cluster structure to define a jointdistribution of the sample size and its exchangeable random partitions. Thecluster structure allows the probability distribution of the random partitionsof a subset of the sample to be dependent on the sample size, a feature notpresented in a partition structure. A generalized negative binomial processcount-mixture model is proposed to generate a cluster structure, where in theprior the number of clusters is finite and Poisson distributed and the clustersizes follow a truncated negative binomial distribution. The number and sizesof clusters can be controlled to exhibit distinct asymptotic behaviors. Uniquemodel properties are illustrated with example clustering results using ageneralized Polya urn sampling scheme. The paper provides new methods togenerate exchangeable random partitions and to control both the cluster-numberand cluster-size distributions.
arxiv-4200-237 | Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix Factorization | http://arxiv.org/pdf/1208.1237v3.pdf | author:Nicolas Gillis, Stephen A. Vavasis category:stat.ML cs.LG math.OC published:2012-08-06 summary:In this paper, we study the nonnegative matrix factorization problem underthe separability assumption (that is, there exists a cone spanned by a smallsubset of the columns of the input nonnegative data matrix containing allcolumns), which is equivalent to the hyperspectral unmixing problem under thelinear mixing model and the pure-pixel assumption. We present a family of fastrecursive algorithms, and prove they are robust under any small perturbationsof the input data matrix. This family generalizes several existinghyperspectral unmixing algorithms and hence provides for the first time atheoretical justification of their better practical performance.
arxiv-4200-238 | Constructing Low Star Discrepancy Point Sets with Genetic Algorithms | http://arxiv.org/pdf/1304.1978v2.pdf | author:Carola Doerr, Francois-Michel De Rainville category:cs.NE cs.NA F.2.1; I.2.8 published:2013-04-07 summary:Geometric discrepancies are standard measures to quantify the irregularity ofdistributions. They are an important notion in numerical integration. One ofthe most important discrepancy notions is the so-called \emph{stardiscrepancy}. Roughly speaking, a point set of low star discrepancy valueallows for a small approximation error in quasi-Monte Carlo integration. It isthus the most studied discrepancy notion. In this work we present a new algorithm to compute point sets of low stardiscrepancy. The two components of the algorithm (for the optimization and theevaluation, respectively) are based on evolutionary principles. Our algorithmclearly outperforms existing approaches. To the best of our knowledge, it isalso the first algorithm which can be adapted easily to optimize inverse stardiscrepancies.
arxiv-4200-239 | Potts model, parametric maxflow and k-submodular functions | http://arxiv.org/pdf/1310.1771v1.pdf | author:Igor Gridchyn, Vladimir Kolmogorov category:cs.CV published:2013-10-07 summary:The problem of minimizing the Potts energy function frequently occurs incomputer vision applications. One way to tackle this NP-hard problem wasproposed by Kovtun [19,20]. It identifies a part of an optimal solution byrunning $k$ maxflow computations, where $k$ is the number of labels. The numberof "labeled" pixels can be significant in some applications, e.g. 50-93% in ourtests for stereo. We show how to reduce the runtime to $O(\log k)$ maxflowcomputations (or one {\em parametric maxflow} computation). Furthermore, theoutput of our algorithm allows to speed-up the subsequent alpha expansion forthe unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al.[7] for {\em Tree Metrics}. We also show a connection to {\em $k$-submodularfunctions} from combinatorial optimization, and discuss {\em $k$-submodularrelaxations} for general energy functions.
arxiv-4200-240 | Laplace approximation for logistic Gaussian process density estimation and regression | http://arxiv.org/pdf/1211.0174v3.pdf | author:Jaakko Riihimäki, Aki Vehtari category:stat.CO stat.ME stat.ML published:2012-11-01 summary:Logistic Gaussian process (LGP) priors provide a flexible alternative formodelling unknown densities. The smoothness properties of the density estimatescan be controlled through the prior covariance structure of the LGP, but thechallenge is the analytically intractable inference. In this paper, we presentapproximate Bayesian inference for LGP density estimation in a grid usingLaplace's method to integrate over the non-Gaussian posterior distribution oflatent function values and to determine the covariance function parameters withtype-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace'smethod with MAP is sufficiently fast for practical interactive visualisation of1D and 2D densities. Our experiments with simulated and real 1D data sets showthat the estimation accuracy is close to a Markov chain Monte Carloapproximation and state-of-the-art hierarchical infinite Gaussian mixturemodels. We also construct a reduced-rank approximation to speed up thecomputations for dense 2D grids, and demonstrate density regression with theproposed Laplace approach.
arxiv-4200-241 | Online Unsupervised Feature Learning for Visual Tracking | http://arxiv.org/pdf/1310.1690v1.pdf | author:Fayao Liu, Chunhua Shen, Ian Reid, Anton van den Hengel category:cs.CV published:2013-10-07 summary:Feature encoding with respect to an over-complete dictionary learned byunsupervised methods, followed by spatial pyramid pooling, and linearclassification, has exhibited powerful strength in various vision applications.Here we propose to use the feature learning pipeline for visual tracking.Tracking is implemented using tracking-by-detection and the resulted frameworkis very simple yet effective. First, online dictionary learning is used tobuild a dictionary, which captures the appearance changes of the trackingtarget as well as the background changes. Given a test image window, we extractlocal image patches from it and each local patch is encoded with respect to thedictionary. The encoded features are then pooled over a spatial pyramid to forman aggregated feature vector. Finally, a simple linear classifier is trained onthese features. Our experiments show that the proposed powerful---albeit simple---tracker,outperforms all the state-of-the-art tracking methods that we have tested.Moreover, we evaluate the performance of different dictionary learning andfeature encoding methods in the proposed tracking framework, and analyse theimpact of each component in the tracking scenario. We also demonstrate theflexibility of feature learning by plugging it into Hare et al.'s trackingmethod. The outcome is, to our knowledge, the best tracker ever reported, whichfacilitates the advantages of both feature learning and structured outputprediction.
arxiv-4200-242 | MINT: Mutual Information based Transductive Feature Selection for Genetic Trait Prediction | http://arxiv.org/pdf/1310.1659v1.pdf | author:Dan He, Irina Rish, David Haws, Simon Teyssedre, Zivan Karaman, Laxmi Parida category:cs.LG cs.CE published:2013-10-07 summary:Whole genome prediction of complex phenotypic traits using high-densitygenotyping arrays has attracted a great deal of attention, as it is relevant tothe fields of plant and animal breeding and genetic epidemiology. As the numberof genotypes is generally much bigger than the number of samples, predictivemodels suffer from the curse-of-dimensionality. The curse-of-dimensionalityproblem not only affects the computational efficiency of a particular genomicselection method, but can also lead to poor performance, mainly due tocorrelation among markers. In this work we proposed the first transductivefeature selection method based on the MRMR (Max-Relevance and Min-Redundancy)criterion which we call MINT. We applied MINT on genetic trait predictionproblems and showed that in general MINT is a better feature selection methodthan the state-of-the-art inductive method mRMR.
arxiv-4200-243 | Efficient Evolutionary Algorithm for Single-Objective Bilevel Optimization | http://arxiv.org/pdf/1303.3901v2.pdf | author:Ankur Sinha, Pekka Malo, Kalyanmoy Deb category:cs.NE published:2013-03-15 summary:Bilevel optimization problems are a class of challenging optimizationproblems, which contain two levels of optimization tasks. In these problems,the optimal solutions to the lower level problem become possible feasiblecandidates to the upper level problem. Such a requirement makes theoptimization problem difficult to solve, and has kept the researchers busytowards devising methodologies, which can efficiently handle the problem.Despite the efforts, there hardly exists any effective methodology, which iscapable of handling a complex bilevel problem. In this paper, we introducebilevel evolutionary algorithm based on quadratic approximations (BLEAQ) ofoptimal lower level variables with respect to the upper level variables. Theapproach is capable of handling bilevel problems with different kinds ofcomplexities in relatively smaller number of function evaluations. Ideas fromclassical optimization have been hybridized with evolutionary methods togenerate an efficient optimization algorithm for generic bilevel problems. Theefficacy of the algorithm has been shown on two sets of test problems. Thefirst set is a recently proposed SMD test set, which contains problems withcontrollable complexities, and the second set contains standard test problemscollected from the literature. The proposed method has been evaluated againsttwo benchmarks, and the performance gain is observed to be significant.
arxiv-4200-244 | Towards Minimax Online Learning with Unknown Time Horizon | http://arxiv.org/pdf/1307.8187v2.pdf | author:Haipeng Luo, Robert E. Schapire category:cs.LG published:2013-07-31 summary:We consider online learning when the time horizon is unknown. We apply aminimax analysis, beginning with the fixed horizon case, and then moving on totwo unknown-horizon settings, one that assumes the horizon is chosen randomlyaccording to some known distribution, and the other which allows the adversaryfull control over the horizon. For the random horizon setting with restrictedlosses, we derive a fully optimal minimax algorithm. And for the adversarialhorizon setting, we prove a nontrivial lower bound which shows that theadversary obtains strictly more power than when the horizon is fixed and known.Based on the minimax solution of the random horizon setting, we then propose anew adaptive algorithm which "pretends" that the horizon is drawn from adistribution from a special family, but no matter how the actual horizon ischosen, the worst-case regret is of the optimal rate. Furthermore, ouralgorithm can be combined and applied in many ways, for instance, to onlineconvex optimization, follow the perturbed leader, exponential weights algorithmand first order bounds. Experiments show that our algorithm outperforms manyother existing algorithms in an online linear optimization setting.
arxiv-4200-245 | Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning | http://arxiv.org/pdf/1310.1597v1.pdf | author:Mengqiu Wang, Christopher D. Manning category:cs.CL cs.AI published:2013-10-06 summary:We consider a multilingual weakly supervised learning scenario whereknowledge from annotated corpora in a resource-rich language is transferred viabitext to guide the learning in other languages. Past approaches project labelsacross bitext and use them as features or gold labels for training. We proposea new method that projects model expectations rather than labels, whichfacilities transfer of model uncertainty across language boundaries. We encodeexpectations as constraints and train a discriminative CRF model usingGeneralized Expectation Criteria (Mann and McCallum, 2010). Evaluated onstandard Chinese-English and German-English NER datasets, our methoddemonstrates F1 scores of 64% and 60% when no labeled data is used. Attainingthe same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.Furthermore, when combined with labeled examples, our method yields significantimprovements over state-of-the-art supervised methods, achieving best reportednumbers to date on Chinese OntoNotes and German CoNLL-03 datasets.
arxiv-4200-246 | Evolution of the Modern Phase of Written Bangla: A Statistical Study | http://arxiv.org/pdf/1310.1590v1.pdf | author:Paheli Bhattacharya, Arnab Bhattacharya category:cs.CL I.2.7 published:2013-10-06 summary:Active languages such as Bangla (or Bengali) evolve over time due to avariety of social, cultural, economic, and political issues. In this paper, weanalyze the change in the written form of the modern phase of Banglaquantitatively in terms of character-level, syllable-level, morpheme-level andword-level features. We collect three different types of corpora---classical,newspapers and blogs---and test whether the differences in their features arestatistically significant. Results suggest that there are significant changesin the length of a word when measured in terms of characters, but there is notmuch difference in usage of different characters, syllables and morphemes in aword or of different words in a sentence. To the best of our knowledge, this isthe first work on Bangla of this kind.
arxiv-4200-247 | Copula Mixed-Membership Stochastic Blockmodel for Intra-Subgroup Correlations | http://arxiv.org/pdf/1306.2733v2.pdf | author:Xuhui Fan, Longbing Cao, Richard Yi Da Xu category:cs.LG stat.ML published:2013-06-12 summary:The \emph{Mixed-Membership Stochastic Blockmodel (MMSB)} is a popularframework for modeling social network relationships. It can fully exploit eachindividual node's participation (or membership) in a social structure. Despiteits powerful representations, this model makes an assumption that thedistributions of relational membership indicators between two nodes areindependent. Under many social network settings, however, it is possible thatcertain known subgroups of people may have high or low correlations in terms oftheir membership categories towards each other, and such prior informationshould be incorporated into the model. To this end, we introduce a \emph{CopulaMixed-Membership Stochastic Blockmodel (cMMSB)} where an individual Copulafunction is employed to jointly model the membership pairs of those nodeswithin the subgroup of interest. The model enables the use of various Copulafunctions to suit the scenario, while maintaining the membership's marginaldistribution, as needed, for modeling membership indicators with other nodesoutside of the subgroup of interest. We describe the proposed model and itsinference algorithm in detail for both the finite and infinite cases. In theexperiment section, we compare our algorithms with other popular models interms of link prediction, using both synthetic and real world data.
arxiv-4200-248 | Learning Hidden Structures with Relational Models by Adequately Involving Rich Information in A Network | http://arxiv.org/pdf/1310.1545v1.pdf | author:Xuhui Fan, Richard Yi Da Xu, Longbing Cao, Yin Song category:cs.LG cs.SI stat.ML published:2013-10-06 summary:Effectively modelling hidden structures in a network is very practical buttheoretically challenging. Existing relational models only involve very limitedinformation, namely the binary directional link data, embedded in a network tolearn hidden networking structures. There is other rich and meaningfulinformation (e.g., various attributes of entities and more granular informationthan binary elements such as "like" or "dislike") missed, which play a criticalrole in forming and understanding relations in a network. In this work, wepropose an informative relational model (InfRM) framework to adequately involverich information and its granularity in a network, including metadatainformation about each entity and various forms of link data. Firstly, aneffective metadata information incorporation method is employed on the priorinformation from relational models MMSB and LFRM. This is to encourage theentities with similar metadata information to have similar hidden structures.Secondly, we propose various solutions to cater for alternative forms of linkdata. Substantial efforts have been made towards modelling appropriateness andefficiency, for example, using conjugate priors. We evaluate our framework andits inference algorithms in different datasets, which shows the generality andeffectiveness of our models in capturing implicit structures in networks.
arxiv-4200-249 | DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition | http://arxiv.org/pdf/1310.1531v1.pdf | author:Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell category:cs.CV published:2013-10-06 summary:We evaluate whether features extracted from the activation of a deepconvolutional network trained in a fully supervised fashion on a large, fixedset of object recognition tasks can be re-purposed to novel generic tasks. Ourgeneric tasks may differ significantly from the originally trained tasks andthere may be insufficient labeled or unlabeled data to conventionally train oradapt a deep architecture to the new tasks. We investigate and visualize thesemantic clustering of deep convolutional features with respect to a variety ofsuch tasks, including scene recognition, domain adaptation, and fine-grainedrecognition challenges. We compare the efficacy of relying on various networklevels to define a fixed feature, and report novel results that significantlyoutperform the state-of-the-art on several important vision challenges. We arereleasing DeCAF, an open-source implementation of these deep convolutionalactivation features, along with all associated network parameters to enablevision researchers to be able to conduct experimentation with deeprepresentations across a range of visual concept learning paradigms.
arxiv-4200-250 | Contraction Principle based Robust Iterative Algorithms for Machine Learning | http://arxiv.org/pdf/1310.1518v1.pdf | author:Rangeet Mitra, Amit Kumar Mishra category:cs.LG stat.ML published:2013-10-05 summary:Iterative algorithms are ubiquitous in the field of data mining. Widely knownexamples of such algorithms are the least mean square algorithm,backpropagation algorithm of neural networks. Our contribution in this paper isan improvement upon this iterative algorithms in terms of their respectiveperformance metrics and robustness. This improvement is achieved by a newscaling factor which is multiplied to the error term. Our analysis shows thatin essence, we are minimizing the corresponding LASSO cost function, which isthe reason of its increased robustness. We also give closed form expressionsfor the number of iterations for convergence and the MSE floor of the originalcost function for a minimum targeted value of the L1 norm. As a concludingtheme based on the stochastic subgradient algorithm, we give a comparisonbetween the well known Dantzig selector and our algorithm based on contractionprinciple. By these simulations we attempt to show the optimality of ourapproach for any widely used parent iterative optimization problem.
arxiv-4200-251 | High dimensional Sparse Gaussian Graphical Mixture Model | http://arxiv.org/pdf/1308.3381v3.pdf | author:Anani Lotsi, Ernst Wit category:stat.ML cs.LG published:2013-08-15 summary:This paper considers the problem of networks reconstruction fromheterogeneous data using a Gaussian Graphical Mixture Model (GGMM). It is wellknown that parameter estimation in this context is challenging due to largenumbers of variables coupled with the degeneracy of the likelihood. We proposeas a solution a penalized maximum likelihood technique by imposing an $l_{1}$penalty on the precision matrix. Our approach shrinks the parameters therebyresulting in better identifiability and variable selection. We use theExpectation Maximization (EM) algorithm which involves the graphical LASSO toestimate the mixing coefficients and the precision matrices. We show that undercertain regularity conditions the Penalized Maximum Likelihood (PML) estimatesare consistent. We demonstrate the performance of the PML estimator throughsimulations and we show the utility of our method for high dimensional dataanalysis in a genomic application.
arxiv-4200-252 | Local Feature or Mel Frequency Cepstral Coefficients - Which One is Better for MLN-Based Bangla Speech Recognition? | http://arxiv.org/pdf/1310.1426v1.pdf | author:Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman, Mohammad Nasiruddin, Md. Abdul Latif, Mohammad Nurul Huda category:cs.CL 68T50 I.2.7 published:2013-10-05 summary:This paper discusses the dominancy of local features (LFs), as input to themultilayer neural network (MLN), extracted from a Bangla input speech over melfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises threestages: (i) LF extraction from input speech, (ii) phoneme probabilitiesextraction using MLN from LF and (iii) the hidden Markov model (HMM) basedclassifier to obtain more accurate phoneme strings. In the experiments onBangla speech corpus prepared by us, it is observed that the LFbased automaticspeech recognition (ASR) system provides higher phoneme correct rate than theMFCC-based system. Moreover, the proposed system requires fewer mixturecomponents in the HMMs.
arxiv-4200-253 | A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages | http://arxiv.org/pdf/1310.1425v1.pdf | author:Mohammad Nasiruddin category:cs.CL 68T50 I.2.7 published:2013-10-05 summary:Word Sense Disambiguation (WSD), the process of automatically identifying themeaning of a polysemous word in a sentence, is a fundamental task in NaturalLanguage Processing (NLP). Progress in this approach to WSD opens up manypromising developments in the field of NLP and its applications. Indeed,improvement over current performance levels could allow us to take a first steptowards natural language understanding. Due to the lack of lexical resources itis sometimes difficult to perform WSD for under-resourced languages. This paperis an investigation on how to initiate research in WSD for under-resourcedlanguages by applying Word Sense Induction (WSI) and suggests some interestingtopics to focus on.
arxiv-4200-254 | Narrowing the Gap: Random Forests In Theory and In Practice | http://arxiv.org/pdf/1310.1415v1.pdf | author:Misha Denil, David Matheson, Nando de Freitas category:stat.ML cs.LG published:2013-10-04 summary:Despite widespread interest and practical use, the theoretical properties ofrandom forests are still not well understood. In this paper we contribute tothis understanding in two ways. We present a new theoretically tractablevariant of random regression forests and prove that our algorithm isconsistent. We also provide an empirical evaluation, comparing our algorithmand other theoretically tractable random forest models to the random forestalgorithm used in practice. Our experiments provide insight into the relativeimportance of different simplifications that theoreticians have made to obtaintractable models for analysis.
arxiv-4200-255 | Multiple Kernel Sparse Representations for Supervised and Unsupervised Learning | http://arxiv.org/pdf/1303.0582v2.pdf | author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:cs.CV published:2013-03-03 summary:In complex visual recognition tasks it is typical to adopt multipledescriptors, that describe different aspects of the images, for obtaining animproved recognition performance. Descriptors that have diverse forms can befused into a unified feature space in a principled manner using kernel methods.Sparse models that generalize well to the test data can be learned in theunified kernel space, and appropriate constraints can be incorporated forapplication in supervised and unsupervised learning. In this paper, we proposeto perform sparse coding and dictionary learning in the multiple kernel space,where the weights of the ensemble kernel are tuned based on graph-embeddingprinciples such that class discrimination is maximized. In our proposedalgorithm, dictionaries are inferred using multiple levels of 1-D subspaceclustering in the kernel space, and the sparse codes are obtained using asimple levelwise pursuit scheme. Empirical results for object recognition andimage clustering show that our algorithm outperforms existing sparse codingbased approaches, and compares favorably to other state-of-the-art methods.
arxiv-4200-256 | Sequential Monte Carlo Bandits | http://arxiv.org/pdf/1310.1404v1.pdf | author:Michael Cherkassky, Luke Bornn category:stat.ML cs.LG stat.ME published:2013-10-04 summary:In this paper we propose a flexible and efficient framework for handlingmulti-armed bandits, combining sequential Monte Carlo algorithms withhierarchical Bayesian modeling techniques. The framework naturally encompassesrestless bandits, contextual bandits, and other bandit variants under a singleinferential model. Despite the model's generality, we propose efficient MonteCarlo algorithms to make inference scalable, based on recent developments insequential Monte Carlo methods. Through two simulation studies, the frameworkis shown to outperform other empirical methods, while also naturally scaling tomore complex problems for which existing approaches can not cope. Additionally,we successfully apply our framework to online video-based advertisingrecommendation, and show its increased efficacy as compared to current state ofthe art bandit algorithms.
arxiv-4200-257 | A Novel Progressive Image Scanning and Reconstruction Scheme based on Compressed Sensing and Linear Prediction | http://arxiv.org/pdf/1310.1259v1.pdf | author:Giulio Coluccia, Enrico Magli category:cs.IT cs.CV math.IT published:2013-10-04 summary:Compressed sensing (CS) is an innovative technique allowing to representsignals through a small number of their linear projections. In this paper weaddress the application of CS to the scenario of progressive acquisition of 2Dvisual signals in a line-by-line fashion. This is an important setting whichencompasses diverse systems such as flatbed scanners and remote sensingimagers. The use of CS in such setting raises the problem of reconstructing avery high number of samples, as are contained in an image, from their linearprojections. Conventional reconstruction algorithms, whose complexity is cubicin the number of samples, are computationally intractable. In this paper wedevelop an iterative reconstruction algorithm that reconstructs an image byiteratively estimating a row, and correlating adjacent rows by means of linearprediction. We develop suitable predictors and test the proposed algorithm inthe context of flatbed scanners and remote sensing imaging systems. We showthat this approach can significantly improve the results of separatereconstruction of each row, providing very good reconstruction quality withreasonable complexity.
arxiv-4200-258 | Reading Stockholm Riots 2013 in social media by text-mining | http://arxiv.org/pdf/1310.1249v1.pdf | author:Andrzej Jarynowski, Amir Rostami category:cs.SI cs.CL physics.soc-ph stat.AP published:2013-10-04 summary:The riots in Stockholm in May 2013 were an event that reverberated in theworld media for its dimension of violence that had spread through the Swedishcapital. In this study we have investigated the role of social media increating media phenomena via text mining and natural language processing. Wehave focused on two channels of communication for our analysis: Twitter andPoloniainfo.se (Forum of Polish community in Sweden). Our preliminary resultsshow some hot topics driving discussion related mostly to Swedish Police andSwedish Politics by counting word usage. Typical features for mediaintervention are presented. We have built networks of most popular phrases,clustered by categories (geography, media institution, etc.). Sentimentanalysis shows negative connotation with Police. The aim of this preliminaryexploratory quantitative study was to generate questions and hypotheses, whichwe could carefully follow by deeper more qualitative methods.
arxiv-4200-259 | The Novel Approach of Adaptive Twin Probability for Genetic Algorithm | http://arxiv.org/pdf/1310.1227v1.pdf | author:Anagha P. Khedkar, Shaila Subbaraman category:cs.NE published:2013-10-04 summary:The performance of GA is measured and analyzed in terms of its performanceparameters against variations in its genetic operators and associatedparameters. Since last four decades huge numbers of researchers have beenworking on the performance of GA and its enhancement. This earlier researchwork on analyzing the performance of GA enforces the need to furtherinvestigate the exploration and exploitation characteristics and observe itsimpact on the behavior and overall performance of GA. This paper introduces thenovel approach of adaptive twin probability associated with the advanced twinoperator that enhances the performance of GA. The design of the advanced twinoperator is extrapolated from the twin offspring birth due to single ovulationin natural genetic systems as mentioned in the earlier works. The twinprobability of this operator is adaptively varied based on the fitness of bestindividual thereby relieving the GA user from statically defining its value.This novel approach of adaptive twin probability is experimented and tested onthe standard benchmark optimization test functions. The experimental resultsshow the increased accuracy in terms of the best individual and reducedconvergence time.
arxiv-4200-260 | Spatially Scalable Compressed Image Sensing with Hybrid Transform and Inter-layer Prediction Model | http://arxiv.org/pdf/1310.1221v1.pdf | author:Diego Valsesia, Enrico Magli category:cs.IT cs.CV cs.MM math.IT published:2013-10-04 summary:Compressive imaging is an emerging application of compressed sensing, devotedto acquisition, encoding and reconstruction of images using random projectionsas measurements. In this paper we propose a novel method to provide a scalableencoding of an image acquired by means of compressed sensing techniques. Twobit-streams are generated to provide two distinct quality levels: alow-resolution base layer and full-resolution enhancement layer. In theproposed method we exploit a fast preview of the image at the encoder in orderto perform inter-layer prediction and encode the prediction residuals only. Theproposed method successfully provides resolution and quality scalability withmodest complexity and it provides gains in the quality of the reconstructedimages with respect to separate encoding of the quality layers. Remarkably, wealso show that the scheme can also provide significant gains with respect to adirect, non-scalable system, thus accomplishing two features at once:scalability and improved reconstruction performance.
arxiv-4200-261 | Labeled Directed Acyclic Graphs: a generalization of context-specific independence in directed graphical models | http://arxiv.org/pdf/1310.1187v1.pdf | author:Johan Pensar, Henrik Nyman, Timo Koski, Jukka Corander category:stat.ML cs.AI cs.LG published:2013-10-04 summary:We introduce a novel class of labeled directed acyclic graph (LDAG) modelsfor finite sets of discrete variables. LDAGs generalize earlier proposals forallowing local structures in the conditional probability distribution of anode, such that unrestricted label sets determine which edges can be deletedfrom the underlying directed acyclic graph (DAG) for a given context. Severalproperties of these models are derived, including a generalization of theconcept of Markov equivalence classes. Efficient Bayesian learning of LDAGs isenabled by introducing an LDAG-based factorization of the Dirichlet prior forthe model parameters, such that the marginal likelihood can be calculatedanalytically. In addition, we develop a novel prior distribution for the modelstructures that can appropriately penalize a model for its labeling complexity.A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hillclimbing approach is used for illustrating the useful properties of LDAG modelsfor both real and synthetic data sets.
arxiv-4200-262 | Spectral Clustering with Epidemic Diffusion | http://arxiv.org/pdf/1303.2663v2.pdf | author:Laura M. Smith, Kristina Lerman, Cristina Garcia-Cardona, Allon G. Percus, Rumi Ghosh category:cs.SI cs.LG physics.soc-ph stat.ML I.5.3 published:2013-03-11 summary:Spectral clustering is widely used to partition graphs into distinct modulesor communities. Existing methods for spectral clustering use the eigenvaluesand eigenvectors of the graph Laplacian, an operator that is closely associatedwith random walks on graphs. We propose a new spectral partitioning method thatexploits the properties of epidemic diffusion. An epidemic is a dynamic processthat, unlike the random walk, simultaneously transitions to all the neighborsof a given node. We show that the replicator, an operator describing epidemicdiffusion, is equivalent to the symmetric normalized Laplacian of a reweightedgraph with edges reweighted by the eigenvector centralities of their incidentnodes. Thus, more weight is given to edges connecting more central nodes. Wedescribe a method that partitions the nodes based on the componentwise ratio ofthe replicator's second eigenvector to the first, and compare its performanceto traditional spectral clustering techniques on synthetic graphs with knowncommunity structure. We demonstrate that the replicator gives preference todense, clique-like structures, enabling it to more effectively discovercommunities that may be obscured by dense intercommunity linking.
arxiv-4200-263 | Spatio-temporal wavelet regularization for parallel MRI reconstruction: application to functional MRI | http://arxiv.org/pdf/1201.0022v3.pdf | author:Lotfi Chaari, Sébastien Mériaux, Jean-Christophe Pesquet, Philippe Ciuciu category:stat.AP cs.CV physics.med-ph published:2011-12-23 summary:Parallel MRI is a fast imaging technique that enables the acquisition ofhighly resolved images in space or/and in time. The performance of parallelimaging strongly depends on the reconstruction algorithm, which can proceedeither in the original k-space (GRAPPA, SMASH) or in the image domain(SENSE-like methods). To improve the performance of the widely used SENSEalgorithm, 2D- or slice-specific regularization in the wavelet domain has beendeeply investigated. In this paper, we extend this approach using 3D-waveletrepresentations in order to handle all slices together and addressreconstruction artifacts which propagate across adjacent slices. The gaininduced by such extension (3D-Unconstrained Wavelet Regularized -SENSE:3D-UWR-SENSE) is validated on anatomical image reconstruction where no temporalacquisition is considered. Another important extension accounts for temporalcorrelations that exist between successive scans in functional MRI (fMRI). Inaddition to the case of 2D+t acquisition schemes addressed by some othermethods like kt-FOCUSS, our approach allows us to deal with 3D+t acquisitionschemes which are widely used in neuroimaging. The resulting 3D-UWR-SENSE and4D-UWR-SENSE reconstruction schemes are fully unsupervised in the sense thatall regularization parameters are estimated in the maximum likelihood sense ona reference scan. The gain induced by such extensions is illustrated on bothanatomical and functional image reconstruction, and also measured in terms ofstatistical sensitivity for the 4D-UWR-SENSE approach during a fastevent-related fMRI protocol. Our 4D-UWR-SENSE algorithm outperforms the SENSEreconstruction at the subject and group levels (15 subjects) for differentcontrasts of interest (eg, motor or computation tasks) and using differentparallel acceleration factors (R=2 and R=4) on 2x2x3mm3 EPI images.
arxiv-4200-264 | Compressed Counting Meets Compressed Sensing | http://arxiv.org/pdf/1310.1076v1.pdf | author:Ping Li, Cun-Hui Zhang, Tong Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT published:2013-10-03 summary:Compressed sensing (sparse signal recovery) has been a popular and importantresearch topic in recent years. By observing that natural signals are oftennonnegative, we propose a new framework for nonnegative signal recovery usingCompressed Counting (CC). CC is a technique built on maximally-skewed p-stablerandom projections originally developed for data stream computations. Ourrecovery procedure is computationally very efficient in that it requires onlyone linear scan of the coordinates. Our analysis demonstrates that, when0<p<=0.5, it suffices to use M= O(C/eps^p log N) measurements so that allcoordinates will be recovered within eps additive precision, in one scan of thecoordinates. The constant C=1 when p->0 and C=pi/2 when p=0.5. In particular,when p->0 the required number of measurements is essentially M=K\log N, where Kis the number of nonzero coordinates of the signal.
arxiv-4200-265 | Machine Teaching for Bayesian Learners in the Exponential Family | http://arxiv.org/pdf/1306.4947v2.pdf | author:Xiaojin Zhu category:cs.LG published:2013-06-20 summary:What if there is a teacher who knows the learning goal and wants to designgood training data for a machine learner? We propose an optimal teachingframework aimed at learners who employ Bayesian models. Our framework isexpressed as an optimization problem over teaching examples that balance thefuture loss of the learner and the effort of the teacher. This optimizationproblem is in general hard. In the case where the learner employs conjugateexponential family models, we present an approximate algorithm for finding theoptimal teaching set. Our algorithm optimizes the aggregate sufficientstatistics, then unpacks them into actual teaching examples. We give severalexamples to illustrate our framework.
arxiv-4200-266 | Multivariate regression and fit function uncertainty | http://arxiv.org/pdf/1310.1022v1.pdf | author:Peter Kovesarki, Ian C. Brock category:stat.ML stat.CO 62J02 published:2013-10-03 summary:This article describes a multivariate polynomial regression method where theuncertainty of the input parameters are approximated with Gaussiandistributions, derived from the central limit theorem for large weighted sums,directly from the training sample. The estimated uncertainties can bepropagated into the optimal fit function, as an alternative to the statisticalbootstrap method. This uncertainty can be propagated further into a lossfunction like quantity, with which it is possible to calculate the expectedloss function, and allows to select the optimal polynomial degree withstatistical significance. Combined with simple phase space splitting methods,it is possible to model most features of the training data even with low degreepolynomials or constants.
arxiv-4200-267 | Cross-Recurrence Quantification Analysis of Categorical and Continuous Time Series: an R package | http://arxiv.org/pdf/1310.0201v2.pdf | author:Moreno I. Coco, Rick Dale category:cs.CL stat.AP published:2013-10-01 summary:This paper describes the R package crqa to perform cross-recurrencequantification analysis of two time series of either a categorical orcontinuous nature. Streams of behavioral information, from eye movements tolinguistic elements, unfold over time. When two people interact, such as inconversation, they often adapt to each other, leading these behavioral levelsto exhibit recurrent states. In dialogue, for example, interlocutors adapt toeach other by exchanging interactive cues: smiles, nods, gestures, choice ofwords, and so on. In order for us to capture closely the goings-on of dynamicinteraction, and uncover the extent of coupling between two individuals, weneed to quantify how much recurrence is taking place at these levels. Methodsavailable in crqa would allow researchers in cognitive science to pose suchquestions as how much are two people recurrent at some level of analysis, whatis the characteristic lag time for one person to maximally match another, orwhether one person is leading another. First, we set the theoretical ground tounderstand the difference between 'correlation' and 'co-visitation' whencomparing two time series, using an aggregative or cross-recurrence approach.Then, we describe more formally the principles of cross-recurrence, and showwith the current package how to carry out analyses applying them. We end thepaper by comparing computational efficiency, and results' consistency, of crqaR package, with the benchmark MATLAB toolbox crptoolbox. We show perfectcomparability between the two libraries on both levels.
arxiv-4200-268 | Developments in the theory of randomized shortest paths with a comparison of graph node distances | http://arxiv.org/pdf/1212.1666v2.pdf | author:Ilkka Kivimäki, Masashi Shimbo, Marco Saerens category:stat.ML published:2012-12-07 summary:There have lately been several suggestions for parametrized distances on agraph that generalize the shortest path distance and the commute time orresistance distance. The need for developing such distances has risen from theobservation that the above-mentioned common distances in many situations failto take into account the global structure of the graph. In this article, wedevelop the theory of one family of graph node distances, known as therandomized shortest path dissimilarity, which has its foundation in statisticalphysics. We show that the randomized shortest path dissimilarity can be easilycomputed in closed form for all pairs of nodes of a graph. Moreover, we come upwith a new definition of a distance measure that we call the free energydistance. The free energy distance can be seen as an upgrade of the randomizedshortest path dissimilarity as it defines a metric, in addition to which itsatisfies the graph-geodetic property. The derivation and computation of thefree energy distance are also straightforward. We then make a comparisonbetween a set of generalized distances that interpolate between the shortestpath distance and the commute time, or resistance distance. This comparisonfocuses on the applicability of the distances in graph node clustering andclassification. The comparison, in general, shows that the parametrizeddistances perform well in the tasks. In particular, we see that the resultsobtained with the free energy distance are among the best in all theexperiments.
arxiv-4200-269 | Efficient pedestrian detection by directly optimize the partial area under the ROC curve | http://arxiv.org/pdf/1310.0900v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG published:2013-10-03 summary:Many typical applications of object detection operate within a prescribedfalse-positive range. In this situation the performance of a detector should beassessed on the basis of the area under the ROC curve over that range, ratherthan over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC).Effective cascade-based classification, for example, depends on training nodeclassifiers that achieve the maximal detection rate at a moderate falsepositive rate, e.g., around 40% to 50%. We propose a novel ensemble learningmethod which achieves a maximal detection rate at a user-defined range of falsepositive rates by directly optimizing the partial AUC using structuredlearning. By optimizing for different ranges of false positive rates, theproposed method can be used to train either a single strong classifier or anode classifier forming part of a cascade classifier. Experimental results onboth synthetic and real-world data sets demonstrate the effectiveness of ourapproach, and we show that it is possible to train state-of-the-art pedestriandetectors using the proposed structured ensemble learning method.
arxiv-4200-270 | Multiple Kernel Learning in the Primal for Multi-modal Alzheimer's Disease Classification | http://arxiv.org/pdf/1310.0890v1.pdf | author:Fayao Liu, Luping Zhou, Chunhua Shen, Jianping Yin category:cs.LG cs.CE published:2013-10-03 summary:To achieve effective and efficient detection of Alzheimer's disease (AD),many machine learning methods have been introduced into this realm. However,the general case of limited training samples, as well as different featurerepresentations typically makes this problem challenging. In this work, wepropose a novel multiple kernel learning framework to combine multi-modalfeatures for AD classification, which is scalable and easy to implement.Contrary to the usual way of solving the problem in the dual space, we look atthe optimization from a new perspective. By conducting Fourier transform on theGaussian kernel, we explicitly compute the mapping function, which leads to amore straightforward solution of the problem in the primal space. Furthermore,we impose the mixed $L_{21}$ norm constraint on the kernel weights, known asthe group lasso regularization, to enforce group sparsity among differentfeature modalities. This actually acts as a role of feature modality selection,while at the same time exploiting complementary information among differentkernels. Therefore it is able to extract the most discriminative features forclassification. Experiments on the ADNI data set demonstrate the effectivenessof the proposed method.
arxiv-4200-271 | Prior-free and prior-dependent regret bounds for Thompson Sampling | http://arxiv.org/pdf/1304.5758v2.pdf | author:Sébastien Bubeck, Che-Yu Liu category:stat.ML cs.LG published:2013-04-21 summary:We consider the stochastic multi-armed bandit problem with a priordistribution on the reward distributions. We are interested in studyingprior-free and prior-dependent regret bounds, very much in the same spirit asthe usual distribution-free and distribution-dependent bounds for thenon-Bayesian stochastic bandit. Building on the techniques of Audibert andBubeck [2009] and Russo and Roy [2013] we first show that Thompson Samplingattains an optimal prior-free bound in the sense that for any priordistribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. Thisresult is unimprovable in the sense that there exists a prior distribution suchthat any algorithm has a Bayesian regret bounded from below by $\frac{1}{20}\sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al.[2013] (where the optimal mean is known as well as a lower bound on thesmallest gap) and we show that in this case the regret of Thompson Sampling isin fact uniformly bounded over time, thus showing that Thompson Sampling cangreatly take advantage of the nice properties of these priors.
arxiv-4200-272 | Stemmers for Tamil Language: Performance Analysis | http://arxiv.org/pdf/1310.0754v1.pdf | author:M. Thangarasu, R. Manavalan category:cs.CL published:2013-10-02 summary:Stemming is the process of extracting root word from the given inflectionword and also plays significant role in numerous application of NaturalLanguage Processing (NLP). Tamil Language raises several challenges to NLP,since it has rich morphological patterns than other languages. The rule basedapproach light-stemmer is proposed in this paper, to find stem word for giveninflection Tamil word. The performance of proposed approach is compared to arule based suffix removal stemmer based on correctly and incorrectly predicted.The experimental result clearly show that the proposed approach light stemmerfor Tamil language perform better than suffix removal stemmer and also moreeffective in Information Retrieval System (IRS).
arxiv-4200-273 | Using the Random Sprays Retinex Algorithm for Global Illumination Estimation | http://arxiv.org/pdf/1310.0307v2.pdf | author:Nikola Banić, Sven Lončarić category:cs.CV published:2013-10-01 summary:In this paper the use of Random Sprays Retinex (RSR) algorithm for globalillumination estimation is proposed and its feasibility tested. Like otheralgorithms based on the Retinex model, RSR also provides local illuminationestimation and brightness adjustment for each pixel and it is faster than otherpath-wise Retinex algorithms. As the assumption of the uniform illuminationholds in many cases, it should be possible to use the mean of localillumination estimations of RSR as a global illumination estimation for imageswith (assumed) uniform illumination allowing also the accuracy to be easilymeasured. Therefore we propose a method for estimating global illuminationestimation based on local RSR results. To our best knowledge this is the firsttime that RSR algorithm is used to obtain global illumination estimation. Forour tests we use a publicly available color constancy image database fortesting. The results are presented and discussed and it turns out that theproposed method outperforms many existing unsupervised color constancyalgorithms. The source code is available athttp://www.fer.unizg.hr/ipg/resources/color_constancy/.
arxiv-4200-274 | Boundary identification of events in clinical named entity recognition | http://arxiv.org/pdf/1308.1004v3.pdf | author:Azad Dehghan category:cs.CL published:2013-08-05 summary:The problem of named entity recognition in the medical/clinical domain hasgained increasing attention do to its vital role in a wide range of clinicaldecision support applications. The identification of complete and correct termspan is vital for further knowledge synthesis (e.g., coding/mapping conceptsthesauruses and classification standards). This paper investigates boundaryadjustment by sequence labeling representations models and post-processingtechniques in the problem of clinical named entity recognition (recognition ofclinical events). Using current state-of-the-art sequence labeling algorithm(conditional random fields), we show experimentally that sequence labelingrepresentation and post-processing can be significantly helpful in strictboundary identification of clinical events.
arxiv-4200-275 | Rule Based Stemmer in Urdu | http://arxiv.org/pdf/1310.0581v1.pdf | author:Vaishali Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:Urdu is a combination of several languages like Arabic, Hindi, English,Turkish, Sanskrit etc. It has a complex and rich morphology. This is the reasonwhy not much work has been done in Urdu language processing. Stemming is usedto convert a word into its respective root form. In stemming, we separate thesuffix and prefix from the word. It is useful in search engines, naturallanguage processing and word processing, spell checkers, word parsing, wordfrequency and count studies. This paper presents a rule based stemmer for Urdu.The stemmer that we have discussed here is used in information retrieval. Wehave also evaluated our results by verifying it with a human expert.
arxiv-4200-276 | Subjective and Objective Evaluation of English to Urdu Machine Translation | http://arxiv.org/pdf/1310.0578v1.pdf | author:Vaishali Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:Machine translation is research based area where evaluation is very importantphenomenon for checking the quality of MT output. The work is based on theevaluation of English to Urdu Machine translation. In this research work wehave evaluated the translation quality of Urdu language which has beentranslated by using different Machine Translation systems like Google, Babylonand Ijunoon. The evaluation process is done by using two approaches - Humanevaluation and Automatic evaluation. We have worked for both the approacheswhere in human evaluation emphasis is given to scales and parameters while inautomatic evaluation emphasis is given to some automatic metric such as BLEU,GTM, METEOR and ATEC.
arxiv-4200-277 | Learning Lambek grammars from proof frames | http://arxiv.org/pdf/1310.0576v1.pdf | author:Roberto Bonato, Christian Retoré category:cs.LG cs.AI cs.LO math.LO published:2013-10-02 summary:In addition to their limpid interface with semantics, categorial grammarsenjoy another important property: learnability. This was first noticed byBuskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorialgrammars. What about Lambek categorial grammars? In a previous paper we showed thatproduct free Lambek grammars where learnable from structured sentences, thestructures being incomplete natural deductions. These grammars were shown to beunlearnable from strings by Foret and Le Nir. In the present paper we show thatLambek grammars, possibly with product, are learnable from proof frames thatare incomplete proof nets. After a short reminder on grammatical inference \`a la Gold, we provide analgorithm that learns Lambek grammars with product from proof frames and weprove its convergence. We do so for 1-valued also known as rigid Lambekgrammars with product, since standard techniques can extend our result to$k$-valued grammars. Because of the correspondence between cut-free proof netsand normal natural deductions, our initial result on product free Lambekgrammars can be recovered. We are sad to dedicate the present paper to Philippe Darondeau, with whom westarted to study such questions in Rennes at the beginning of the millennium,and who passed away prematurely. We are glad to dedicate the present paper to Jim Lambek for his 90 birthday:he is the living proof that research is an eternal learning process.
arxiv-4200-278 | Improving the Quality of MT Output using Novel Name Entity Translation Scheme | http://arxiv.org/pdf/1310.0573v1.pdf | author:Deepti Bhalla, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:This paper presents a novel approach to machine translation by combining thestate of art name entity translation scheme. Improper translation of nameentities lapse the quality of machine translated output. In this work, nameentities are transliterated by using statistical rule based approach. Thispaper describes the translation and transliteration of name entities fromEnglish to Punjabi. We have experimented on four types of name entities whichare: Proper names, Location names, Organization names and miscellaneous.Various rules for the purpose of syllabification have been constructed.Transliteration of name entities is accomplished with the help of Probabilitycalculation. N-Gram probabilities for the extracted syllables have beencalculated using statistical machine translation toolkit MOSES.
arxiv-4200-279 | A Robust Alternating Direction Method for Constrained Hybrid Variational Deblurring Model | http://arxiv.org/pdf/1309.0123v2.pdf | author:Ryan Wen Liu, Tian Xu category:cs.CV 65K10, 68U10 I.4.4; G.1.6 published:2013-08-31 summary:In this work, a new constrained hybrid variational deblurring model isdeveloped by combining the non-convex first- and second-order total variationregularizers. Moreover, a box constraint is imposed on the proposed model toguarantee high deblurring performance. The developed constrained hybridvariational model could achieve a good balance between preserving image detailsand alleviating ringing artifacts. In what follows, we present thecorresponding numerical solution by employing an iteratively reweightedalgorithm based on alternating direction method of multipliers. Theexperimental results demonstrate the superior performance of the proposedmethod in terms of quantitative and qualitative image quality assessments.
arxiv-4200-280 | Online Learning of Dynamic Parameters in Social Networks | http://arxiv.org/pdf/1310.0432v1.pdf | author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI stat.ML published:2013-10-01 summary:This paper addresses the problem of online learning in a dynamic setting. Weconsider a social network in which each individual observes a private signalabout the underlying state of the world and communicates with her neighbors ateach time period. Unlike many existing approaches, the underlying state isdynamic, and evolves according to a geometric random walk. We view the scenarioas an optimization problem where agents aim to learn the true state whilesuffering the smallest possible loss. Based on the decomposition of the globalloss function, we introduce two update mechanisms, each of which generates anestimate of the true state. We establish a tight bound on the rate of change ofthe underlying state, under which individuals can track the parameter with abounded variance. Then, we characterize explicit expressions for the steadystate mean-square deviation(MSD) of the estimates from the truth, perindividual. We observe that only one of the estimators recovers the optimalMSD, which underscores the impact of the objective function decomposition onthe learning quality. Finally, we provide an upper bound on the regret of theproposed methods, measured as an average of errors in estimating the parameterin a finite time.
arxiv-4200-281 | Joint Bayesian estimation of close subspaces from noisy measurements | http://arxiv.org/pdf/1310.0376v1.pdf | author:Olivier Besson, Nicolas Dobigeon, Jean-Yves Tourneret category:stat.ME stat.ML published:2013-10-01 summary:In this letter, we consider two sets of observations defined as subspacesignals embedded in noise and we wish to analyze the distance between these twosubspaces. The latter entails evaluating the angles between the subspaces, anissue reminiscent of the well-known Procrustes problem. A Bayesian approach isinvestigated where the subspaces of interest are considered as random with ajoint prior distribution (namely a Bingham distribution), which allows thecloseness of the two subspaces to be adjusted. Within this framework, theminimum mean-square distance estimator of both subspaces is formulated andimplemented via a Gibbs sampler. A simpler scheme based on alternative maximuma posteriori estimation is also presented. The new schemes are shown to providemore accurate estimates of the angles between the subspaces, compared tosingular value decomposition based independent estimation of the two subspaces.
arxiv-4200-282 | The complex-valued encoding for dicision-making based on aliasing data | http://arxiv.org/pdf/1310.0365v1.pdf | author:P. A. Golovinski, V. A. Astapenko category:cs.CV published:2013-10-01 summary:It is proposed a complex valued channel encoding for multidimensional data.The basic approach contains overlapping of complex nonlinear mappings. Itsdevelopment leads to sparse representation of multi-channel data, increasingtheir dimensions and the distance between the images.
arxiv-4200-283 | An Overview and Evaluation of Various Face and Eyes Detection Algorithms for Driver Fatigue Monitoring Systems | http://arxiv.org/pdf/1310.0317v1.pdf | author:Markan Lopar, Slobodan Ribarić category:cs.CV published:2013-10-01 summary:In this work various methods and algorithms for face and eyes detection areexamined in order to decide which of them are applicable for use in a driverfatigue monitoring system. In the case of face detection the standardViola-Jones face detector has shown best results, while the method of findingthe eye centers by means of gradients has proven to be most appropriate in thecase of eyes detection. The later method has also a potential for retrievingbehavioral parameters needed for estimation of the level of driver fatigue.This possibility will be examined in future work.
arxiv-4200-284 | Classifying Traffic Scenes Using The GIST Image Descriptor | http://arxiv.org/pdf/1310.0316v1.pdf | author:Ivan Sikirić, Karla Brkić, Siniša Šegvić category:cs.CV published:2013-10-01 summary:This paper investigates classification of traffic scenes in a very lowbandwidth scenario, where an image should be coded by a small number offeatures. We introduce a novel dataset, called the FM1 dataset, consisting of5615 images of eight different traffic scenes: open highway, open road,settlement, tunnel, tunnel exit, toll booth, heavy traffic and the overpass. Weevaluate the suitability of the GIST descriptor as a representation of theseimages, first by exploring the descriptor space using PCA and k-meansclustering, and then by using an SVM classifier and recording its 10-foldcross-validation performance on the introduced FM1 dataset. The obtainedrecognition rates are very encouraging, indicating that the use of the GISTdescriptor alone could be sufficiently descriptive even when very highperformance is required.
arxiv-4200-285 | Computer Vision Systems in Road Vehicles: A Review | http://arxiv.org/pdf/1310.0315v1.pdf | author:Kristian Kovačić, Edouard Ivanjko, Hrvoje Gold category:cs.CV published:2013-10-01 summary:The number of road vehicles significantly increased in recent decades. Thistrend accompanied a build-up of road infrastructure and development of variouscontrol systems to increase road traffic safety, road capacity and travelcomfort. In traffic safety significant development has been made and today'ssystems more and more include cameras and computer vision methods. Cameras areused as part of the road infrastructure or in vehicles. In this paper a reviewon computer vision systems in vehicles from the stand point of trafficengineering is given. Safety problems of road vehicles are presented, currentstate of the art in-vehicle vision systems is described and open problems withfuture research directions are discussed.
arxiv-4200-286 | Global Localization Based on 3D Planar Surface Segments | http://arxiv.org/pdf/1310.0314v1.pdf | author:Robert Cupec, Emmanuel Karlo Nyarko, Damir Filko, Andrej Kitanov, Ivan Petrović category:cs.CV published:2013-10-01 summary:Global localization of a mobile robot using planar surface segments extractedfrom depth images is considered. The robot's environment is represented by atopological map consisting of local models, each representing a particularlocation modeled by a set of planar surface segments. The discussedlocalization approach segments a depth image acquired by a 3D camera intoplanar surface segments which are then matched to model surface segments. Therobot pose is estimated by the Extended Kalman Filter using surface segmentpairs as measurements. The reliability and accuracy of the considered approachare experimentally evaluated using a mobile robot equipped by a MicrosoftKinect sensor.
arxiv-4200-287 | Multiclass Road Sign Detection using Multiplicative Kernel | http://arxiv.org/pdf/1310.0311v1.pdf | author:Valentina Zadrija, Siniša Šegvić category:cs.CV published:2013-10-01 summary:We consider the problem of multiclass road sign detection using aclassification function with multiplicative kernel comprised from two kernels.We show that problems of detection and within-foreground classification can bejointly solved by using one kernel to measure object-background differences andanother one to account for within-class variations. The main idea behind thisapproach is that road signs from different foreground variations can sharefeatures that discriminate them from backgrounds. The classification functiontraining is accomplished using SVM, thus feature sharing is obtained throughsupport vector sharing. Training yields a family of linear detectors, whereeach detector corresponds to a specific foreground training sample. Theredundancy among detectors is alleviated using k-medoids clustering. Finally,we report detection and classification results on a set of road sign imagesobtained from a camera on a moving vehicle.
arxiv-4200-288 | A Novel Georeferenced Dataset for Stereo Visual Odometry | http://arxiv.org/pdf/1310.0310v1.pdf | author:Ivan Krešo, Marko Ševrović, Siniša Šegvić category:cs.CV published:2013-10-01 summary:In this work, we present a novel dataset for assessing the accuracy of stereovisual odometry. The dataset has been acquired by a small-baseline stereo rigmounted on the top of a moving car. The groundtruth is supplied by a consumergrade GPS device without IMU. Synchronization and alignment between GPSreadings and stereo frames are recovered after the acquisition. We show thatthe attained groundtruth accuracy allows to draw useful conclusions inpractice. The presented experiments address influence of camera calibration,baseline distance and zero-disparity features to the achieved reconstructionperformance.
arxiv-4200-289 | Combining Spatio-Temporal Appearance Descriptors and Optical Flow for Human Action Recognition in Video Data | http://arxiv.org/pdf/1310.0308v1.pdf | author:Karla Brkić, Srđan Rašić, Axel Pinz, Siniša Šegvić, Zoran Kalafatić category:cs.CV published:2013-10-01 summary:This paper proposes combining spatio-temporal appearance (STA) descriptorswith optical flow for human action recognition. The STA descriptors are localhistogram-based descriptors of space-time, suitable for building a partialrepresentation of arbitrary spatio-temporal phenomena. Because of thepossibility of iterative refinement, they are interesting in the context ofonline human action recognition. We investigate the use of dense optical flowas the image function of the STA descriptor for human action recognition, usingtwo different algorithms for computing the flow: the Farneb\"ack algorithm andthe TVL1 algorithm. We provide a detailed analysis of the influencing opticalflow algorithm parameters on the produced optical flow fields. An extensiveexperimental validation of optical flow-based STA descriptors in human actionrecognition is performed on the KTH human action dataset. The encouragingexperimental results suggest the potential of our approach in online humanaction recognition.
arxiv-4200-290 | Flexible Visual Quality Inspection in Discrete Manufacturing | http://arxiv.org/pdf/1310.0306v1.pdf | author:Tomislav Petković, Darko Jurić, Sven Lončarić category:cs.CV published:2013-10-01 summary:Most visual quality inspections in discrete manufacturing are composed oflength, surface, angle or intensity measurements. Those are implemented asend-user configurable inspection tools that should not require an imageprocessing expert to set up. Currently available software solutions providingsuch capability use a flowchart based programming environment, but do not fullyaddress an inspection flowchart robustness and can require a redefinition ofthe flowchart if a small variation is introduced. In this paper we propose anacquire-register-analyze image processing pattern designed for discretemanufacturing that aims to increase the robustness of the inspection flowchartby consistently addressing variations in product position, orientation andsize. A proposed pattern is transparent to the end-user and simplifies theflowchart. We describe a developed software solution that is a practicalimplementation of the proposed pattern. We give an example of its real-life usein industrial production of electric components.
arxiv-4200-291 | Filtering for More Accurate Dense Tissue Segmentation in Digitized Mammograms | http://arxiv.org/pdf/1310.0305v1.pdf | author:Mario Muštra, Mislav Grgić category:cs.CV published:2013-10-01 summary:Breast tissue segmentation into dense and fat tissue is important fordetermining the breast density in mammograms. Knowing the breast density isimportant both in diagnostic and computer-aided detection applications. Thereare many different ways to express the density of a breast and good qualitysegmentation should provide the possibility to perform accurate classificationno matter which classification rule is being used. Knowing the right breastdensity and having the knowledge of changes in the breast density could give ahint of a process which started to happen within a patient. Mammogramsgenerally suffer from a problem of different tissue overlapping which resultsin the possibility of inaccurate detection of tissue types. Fibroglandulartissue presents rather high attenuation of X-rays and is visible as brighter inthe resulting image but overlapping fibrous tissue and blood vessels couldeasily be replaced with fibroglandular tissue in automatic segmentationalgorithms. Small blood vessels and microcalcifications are also shown asbright objects with similar intensities as dense tissue but do have someproperties which makes possible to suppress them from the final results. Inthis paper we try to divide dense and fat tissue by suppressing the scatteredstructures which do not represent glandular or dense tissue in order to dividemammograms more accurately in the two major tissue types. For suppressing bloodvessels and microcalcifications we have used Gabor filters of different sizeand orientation and a combination of morphological operations on filtered imagewith enhanced contrast.
arxiv-4200-292 | Surface Registration Using Genetic Algorithm in Reduced Search Space | http://arxiv.org/pdf/1310.0302v1.pdf | author:Vedran Hrgetić, Tomislav Pribanić category:cs.CV published:2013-10-01 summary:Surface registration is a technique that is used in various areas such asobject recognition and 3D model reconstruction. Problem of surface registrationcan be analyzed as an optimization problem of seeking a rigid motion betweentwo different views. Genetic algorithms can be used for solving thisoptimization problem, both for obtaining the robust parameter estimation andfor its fine-tuning. The main drawback of genetic algorithms is that they aretime consuming which makes them unsuitable for online applications. Modernacquisition systems enable the implementation of the solutions that wouldimmediately give the information on the rotational angles between the differentviews, thus reducing the dimension of the optimization problem. The paper givesan analysis of the genetic algorithm implemented in the conditions when therotation matrix is known and a comparison of these results with results whenthis information is not available.
arxiv-4200-293 | On the origin of ambiguity in efficient communication | http://arxiv.org/pdf/1107.0193v3.pdf | author:Jordi Fortuny, Bernat Corominas-Murtra category:cs.CL published:2011-07-01 summary:This article studies the emergence of ambiguity in communication through theconcept of logical irreversibility and within the framework of Shannon'sinformation theory. This leads us to a precise and general expression of theintuition behind Zipf's vocabulary balance in terms of a symmetry equationbetween the complexities of the coding and the decoding processes that imposesan unavoidable amount of logical uncertainty in natural communication.Accordingly, the emergence of irreversible computations is required if thecomplexities of the coding and the decoding processes are balanced in asymmetric scenario, which means that the emergence of ambiguous codes is anecessary condition for natural communication to succeed.
arxiv-4200-294 | Object Detection Using Keygraphs | http://arxiv.org/pdf/1310.0171v1.pdf | author:Marcelo Hashimoto, Roberto Marcondes Cesar Junior category:cs.CV published:2013-10-01 summary:We propose a new framework for object detection based on a generalization ofthe keypoint correspondence framework. This framework is based on replacingkeypoints by keygraphs, i.e. isomorph directed graphs whose vertices arekeypoints, in order to explore relative and structural information. Unlikesimilar works in the literature, we deal directly with graphs in the entirepipeline: we search for graph correspondences instead of searching forindividual point correspondences and then building graph correspondences fromthem afterwards. We also estimate the pose from graph correspondences insteadof falling back to point correspondences through a voting table. Thecontributions of this paper are the proposed framework and an implementationthat properly handles its inherent issues of loss of locality and combinatorialexplosion, showing its viability for real-time applications. In particular, weintroduce the novel concept of keytuples to solve a running time issue. Theaccuracy of the implementation is shown by results of over 800 experiments witha well-known database of images. The speed is illustrated by real-time trackingwith two different cameras in ordinary hardware.
arxiv-4200-295 | Improving CUR Matrix Decomposition and the Nyström Approximation via Adaptive Sampling | http://arxiv.org/pdf/1303.4207v7.pdf | author:Shusen Wang, Zhihua Zhang category:cs.LG cs.NA published:2013-03-18 summary:The CUR matrix decomposition and the Nystr\"{o}m approximation are twoimportant low-rank matrix approximation techniques. The Nystr\"{o}m methodapproximates a symmetric positive semidefinite matrix in terms of a smallnumber of its columns, while CUR approximates an arbitrary data matrix by asmall number of its columns and rows. Thus, CUR decomposition can be regardedas an extension of the Nystr\"{o}m approximation. In this paper we establish a more general error bound for the adaptivecolumn/row sampling algorithm, based on which we propose more accurate CUR andNystr\"{o}m algorithms with expected relative-error bounds. The proposed CURand Nystr\"{o}m algorithms also have low time complexity and can avoidmaintaining the whole data matrix in RAM. In addition, we give theoreticalanalysis for the lower error bounds of the standard Nystr\"{o}m method and theensemble Nystr\"{o}m method. The main theoretical results established in thispaper are novel, and our analysis makes no special assumption on the datamatrices.
arxiv-4200-296 | Structured learning of sum-of-submodular higher order energy functions | http://arxiv.org/pdf/1309.7512v2.pdf | author:Alexander Fix, Thorsten Joachims, Sam Park, Ramin Zabih category:cs.CV cs.LG stat.ML published:2013-09-28 summary:Submodular functions can be exactly minimized in polynomial time, and thespecial case that graph cuts solve with max flow \cite{KZ:PAMI04} has hadsignificant impact in computer vision\cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we addressthe important class of sum-of-submodular (SoS) functions\cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via avariant of max flow called submodular flow \cite{Edmonds:ADM77}. SoS functionscan naturally express higher order priors involving, e.g., local image patches;however, it is difficult to fully exploit their expressive power because theyhave so many parameters. Rather than trying to formulate existing higher orderpriors as an SoS function, we take a discriminative learning approach,effectively searching the space of SoS functions for a higher order prior thatperforms well on our training set. We adopt a structural SVM approach\cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the trainingproblem in terms of quadratic programming; as a result we can efficientlysearch the space of SoS priors via an extended cutting-plane algorithm. We alsoshow how the state-of-the-art max flow method for vision problems\cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flowproblem. Experimental comparisons are made against the OpenCV implementation ofthe GrabCut interactive segmentation technique \cite{Rother:GrabCut04}, whichuses hand-tuned parameters instead of machine learning. On a standard dataset\cite{Gulshan:CVPR10} our method learns higher order priors with hundreds ofparameter values, and produces significantly better segmentations. While ourfocus is on binary labeling problems, we show that our techniques can benaturally generalized to handle more than two labels.
arxiv-4200-297 | An information measure for comparing top $k$ lists | http://arxiv.org/pdf/1310.0110v1.pdf | author:Arun Konagurthu, James Collier category:cs.IT cs.LG math.IT published:2013-10-01 summary:Comparing the top $k$ elements between two or more ranked results is a commontask in many contexts and settings. A few measures have been proposed tocompare top $k$ lists with attractive mathematical properties, but they face anumber of pitfalls and shortcomings in practice. This work introduces a newmeasure to compare any two top k lists based on measuring the information theselists convey. Our method investigates the compressibility of the lists, and thelength of the message to losslessly encode them gives a natural and robustmeasure of their variability. This information-theoretic measure objectivelyreconciles all the main considerations that arise when measuring(dis-)similarity between lists: the extent of their non-overlapping elements ineach of the lists; the amount of disarray among overlapping elements betweenthe lists; the measurement of displacement of actual ranks of their overlappingelements.
arxiv-4200-298 | Personal Identification from Lip-Print Features using a Statistical Model | http://arxiv.org/pdf/1310.0036v1.pdf | author:Saptarshi Bhattacharjee, S Arunkumar, Samir Kumar Bandyopadhyay category:cs.CV published:2013-09-30 summary:This paper presents a novel approach towards identification of human beingsfrom the statistical analysis of their lip prints. Lip features are extractedby studying the spatial orientations of the grooves present in lip prints ofindividuals using standard edge detection techniques. Horizontal, vertical anddiagonal groove features are analysed using connected-component analysis togenerate the region-specific edge datasets. Comparison between test andreference sample datasets against a threshold value to define a match yieldsatisfactory results. FAR, FRR and ROC metrics have been used to gauge theperformance of the algorithm for real-world deployment in unimodal andmultimodal biometric verification systems.
arxiv-4200-299 | An Image-Based Fluid Surface Pattern Model | http://arxiv.org/pdf/1309.7912v1.pdf | author:Mauro de Amorim, Ricardo Fabbri, Lucia Maria dos Santos Pinto, Francisco Duarte Moura Neto category:cs.CV published:2013-09-30 summary:This work aims at generating a model of the ocean surface and its dynamicsfrom one or more video cameras. The idea is to model wave patterns from videoas a first step towards a larger system of photogrammetric monitoring of marineconditions for use in offshore oil drilling platforms. The first part of theproposed approach consists in reducing the dimensionality of sensor data madeup of the many pixels of each frame of the input video streams. This enablesfinding a concise number of most relevant parameters to model the temporaldataset, yielding an efficient data-driven model of the evolution of theobserved surface. The second part proposes stochastic modeling to bettercapture the patterns embedded in the data. One can then draw samples from thefinal model, which are expected to simulate the behavior of previously observedflow, in order to determine conditions that match new observations. In thispaper we focus on proposing and discussing the overall approach and oncomparing two different techniques for dimensionality reduction in the firststage: principal component analysis and diffusion maps. Work is underway on thesecond stage of constructing better stochastic models of fluid surface dynamicsas proposed here.
arxiv-4200-300 | Linear Regression as a Non-Cooperative Game | http://arxiv.org/pdf/1309.7824v1.pdf | author:Stratis Ioannidis, Patrick Loiseau category:cs.GT cs.LG math.ST stat.TH published:2013-09-30 summary:Linear regression amounts to estimating a linear model that maps features(e.g., age or gender) to corresponding data (e.g., the answer to a survey orthe outcome of a medical exam). It is a ubiquitous tool in experimentalsciences. We study a setting in which features are public but the data isprivate information. While the estimation of the linear model may be useful toparticipating individuals, (if, e.g., it leads to the discovery of a treatmentto a disease), individuals may be reluctant to disclose their data due toprivacy concerns. In this paper, we propose a generic game-theoretic model toexpress this trade-off. Users add noise to their data before releasing it. Inparticular, they choose the variance of this noise to minimize a costcomprising two components: (a) a privacy cost, representing the loss of privacyincurred by the release; and (b) an estimation cost, representing theinaccuracy in the linear model estimate. We study the Nash equilibria of thisgame, establishing the existence of a unique non-trivial equilibrium. Wedetermine its efficiency for several classes of privacy and estimation costs,using the concept of the price of stability. Finally, we prove that, for aspecific estimation cost, the generalized least-square estimator is optimalamong all linear unbiased estimators in our non-cooperative setting: thisresult extends the famous Aitken/Gauss-Markov theorem in statistics,establishing that its conclusion persists even in the presence of strategicindividuals.
