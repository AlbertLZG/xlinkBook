arxiv-4200-1 | Learning ambiguous functions by neural networks | http://arxiv.org/abs/1310.1250 | author:Rui Ligeiro, R. Vilela Mendes category:cs.NE cs.LG 68T37, 82C32 published:2013-08-15 summary:It is not, in general, possible to have access to all variables thatdetermine the behavior of a system. Having identified a number of variableswhose values can be accessed, there may still be hidden variables whichinfluence the dynamics of the system. The result is model ambiguity in thesense that, for the same (or very similar) input values, different objectiveoutputs should have been obtained. In addition, the degree of ambiguity mayvary widely across the whole range of input values. Thus, to evaluate theaccuracy of a model it is of utmost importance to create a method to obtain thedegree of reliability of each output result. In this paper we present such ascheme composed of two coupled artificial neural networks: the first one beingresponsible for outputting the predicted value, whereas the other evaluates thereliability of the output, which is learned from the error values of the firstone. As an illustration, the scheme is applied to a model for tracking slopesin a straw chamber and to a credit scoring model.
arxiv-4200-2 | A Secure and Comparable Text Encryption Algorithm | http://arxiv.org/abs/1308.3294 | author:Nicholas Kersting category:cs.CR cs.CL cs.CY cs.SI published:2013-08-15 summary:This paper discloses a simple algorithm for encrypting text messages, basedon the NP-completeness of the subset sum problem, such that the similaritybetween encryptions is roughly proportional to the semantic similarity betweentheir generating messages. This allows parties to compare encrypted messagesfor semantic overlap without trusting an intermediary and might be applied, forexample, as a means of finding scientific collaborators over the Internet.
arxiv-4200-3 | Complete stability analysis of a heuristic ADP control design | http://arxiv.org/abs/1308.3282 | author:Yury Sokolov, Robert Kozma, Ludmilla D. Werbos, Paul J. Werbos category:cs.NE cs.SY published:2013-08-15 summary:This paper provides new stability results for Action-Dependent HeuristicDynamic Programming (ADHDP), using a control algorithm that iterativelyimproves an internal model of the external world in the autonomous system basedon its continuous interaction with the environment. We extend previous resultsby ADHDP control to the case of general multi-layer neural networks with deeplearning across all layers. In particular, we show that the introduced controlapproach is uniformly ultimately bounded (UUB) under specific conditions on thelearning rates, without explicit constraints on the temporal discount factor.We demonstrate the benefit of our results to the control of linear andnonlinear systems, including the cart-pole balancing problem. Our results showsignificantly improved learning and control performance as compared to thestate-of-art.
arxiv-4200-4 | Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations | http://arxiv.org/abs/1308.3513 | author:Finale Doshi-Velez, George Konidaris category:cs.LG cs.AI published:2013-08-15 summary:Control applications often feature tasks with similar, but not identical,dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP),a framework that parametrizes a family of related dynamical systems with alow-dimensional set of latent factors, and introduce a semiparametricregression approach for learning its structure from data. In the controlsetting, we show that a learned HiP-MDP rapidly identifies the dynamics of anew task instance, allowing an agent to flexibly adapt to task variations.
arxiv-4200-5 | The algorithm of noisy k-means | http://arxiv.org/abs/1308.3314 | author:Camille Brunet, SÃ©bastien Loustau category:stat.ML cs.LG published:2013-08-15 summary:In this note, we introduce a new algorithm to deal with finite dimensionalclustering with errors in variables. The design of this algorithm is based onrecent theoretical advances (see Loustau (2013a,b)) in statistical learningwith errors in variables. As the previous mentioned papers, the algorithm mixesdifferent tools from the inverse problem literature and the machine learningcommunity. Coarsely, it is based on a two-step procedure: (1) a deconvolutionstep to deal with noisy inputs and (2) Newton's iterations as the populark-means.
arxiv-4200-6 | System and Methods for Converting Speech to SQL | http://arxiv.org/abs/1308.3106 | author:Sachin Kumar, Ashish Kumar, Pinaki Mitra, Girish Sundaram category:cs.CL cs.DB published:2013-08-14 summary:This paper concerns with the conversion of a Spoken English Language Queryinto SQL for retrieving data from RDBMS. A User submits a query as speechsignal through the user interface and gets the result of the query in the textformat. We have developed the acoustic and language models using which a speechutterance can be converted into English text query and thus natural languageprocessing techniques can be applied on this English text query to generate anequivalent SQL query. For conversion of speech into English text HTK and Juliustools have been used and for conversion of English text query into SQL query wehave implemented a System which uses rule based translation to translateEnglish Language Query into SQL Query. The translation uses lexical analyzer,parser and syntax directed translation techniques like in compilers. JFLex andBYACC tools have been used to build lexical analyzer and parser respectively.System is domain independent i.e. system can run on different database as itgenerates lex files from the underlying database.
arxiv-4200-7 | Compact Relaxations for MAP Inference in Pairwise MRFs with Piecewise Linear Priors | http://arxiv.org/abs/1308.3101 | author:Christopher Zach, Christian Haene category:cs.CV cs.LG stat.ML published:2013-08-14 summary:Label assignment problems with large state spaces are important tasksespecially in computer vision. Often the pairwise interaction (or smoothnessprior) between labels assigned at adjacent nodes (or pixels) can be describedas a function of the label difference. Exact inference in such labeling tasksis still difficult, and therefore approximate inference methods based on alinear programming (LP) relaxation are commonly used in practice. In this workwe study how compact linear programs can be constructed for general piecwiselinear smoothness priors. The number of unknowns is O(LK) per pairwise cliquein terms of the state space size $L$ and the number of linear segments K. Thiscompares to an O(L^2) size complexity of the standard LP relaxation if thepiecewise linear structure is ignored. Our compact construction and thestandard LP relaxation are equivalent and lead to the same (approximate) labelassignment.
arxiv-4200-8 | Normalized Google Distance of Multisets with Applications | http://arxiv.org/abs/1308.3177 | author:Andrew R. Cohen, P. M. B. Vitanyi category:cs.IR cs.LG published:2013-08-14 summary:Normalized Google distance (NGD) is a relative semantic distance based on theWorld Wide Web (or any other large electronic database, for instance Wikipedia)and a search engine that returns aggregate page counts. The earlier NGD betweenpairs of search terms (including phrases) is not sufficient for allapplications. We propose an NGD of finite multisets of search terms that isbetter for many applications. This gives a relative semantics shared by amultiset of search terms. We give applications and compare the results withthose obtained using the pairwise NGD. The derivation of NGD method is based onKolmogorov complexity.
arxiv-4200-9 | Guiding Designs of Self-Organizing Swarms: Interactive and Automated Approaches | http://arxiv.org/abs/1308.3400 | author:Hiroki Sayama category:cs.NE nlin.AO published:2013-08-14 summary:Self-organization of heterogeneous particle swarms is rich in its dynamicsbut hard to design in a traditional top-down manner, especially when many typesof kinetically distinct particles are involved. In this chapter, we discuss howwe have been addressing this problem by (1) utilizing and enhancing interactiveevolutionary design methods and (2) realizing spontaneous evolution of selforganizing swarms within an artificial ecosystem.
arxiv-4200-10 | Confidence Sets Based on Thresholding Estimators in High-Dimensional Gaussian Regression Models | http://arxiv.org/abs/1308.3201 | author:Ulrike Schneider category:math.ST stat.ME stat.ML stat.TH published:2013-08-14 summary:We study confidence intervals based on hard-thresholding, soft-thresholding,and adaptive soft-thresholding in a linear regression model where the number ofregressors $k$ may depend on and diverge with sample size $n$. In addition tothe case of known error variance, we define and study versions of theestimators when the error variance is unknown. In the known variance case, weprovide an exact analysis of the coverage properties of such intervals infinite samples. We show that these intervals are always larger than thestandard interval based on the least-squares estimator. Asymptotically, theintervals based on the thresholding estimators are larger even by an order ofmagnitude when the estimators are tuned to perform consistent variableselection. For the unknown-variance case, we provide non-trivial lower boundsfor the coverage probabilities in finite samples and conduct an asymptoticanalysis where the results from the known-variance case can be shown to carryover asymptotically if the number of degrees of freedom $n-k$ tends to infinityfast enough in relation to the thresholding parameter.
arxiv-4200-11 | An interactive engine for multilingual video browsing using semantic content | http://arxiv.org/abs/1308.3225 | author:M. Ben Halima, M. Hamroun, S. Ben Moussa, A. M. Alimi category:cs.MM cs.CV cs.IR published:2013-08-14 summary:The amount of audio-visual information has increased dramatically with theadvent of High Speed Internet. Furthermore, technological advances in recentyears in the field of information technology, have simplified the use of videodata in various fields by the general public. This made it possible to storelarge collections of video documents into computer systems. To enable efficientuse of these collections, it is necessary to develop tools to facilitate accessto these documents and handling them. In this paper we propose a method forindexing and retrieval of video sequences in a video database of largedimension, based on a weighting technique to calculate the degree of membershipof a concept in a video also a structuring of the data of the audio-visual(context / concept / video) and a relevance feedback mechanism.
arxiv-4200-12 | Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index | http://arxiv.org/abs/1308.3052 | author:Wufeng Xue, Lei Zhang, Xuanqin Mou, Alan C. Bovik category:cs.CV published:2013-08-14 summary:It is an important task to faithfully evaluate the perceptual quality ofoutput images in many applications such as image compression, image restorationand multimedia streaming. A good image quality assessment (IQA) model shouldnot only deliver high quality prediction accuracy but also be computationallyefficient. The efficiency of IQA metrics is becoming particularly important dueto the increasing proliferation of high-volume visual data in high-speednetworks. We present a new effective and efficient IQA model, called gradientmagnitude similarity deviation (GMSD). The image gradients are sensitive toimage distortions, while different local structures in a distorted image sufferdifferent degrees of degradations. This motivates us to explore the use ofglobal variation of gradient based local quality map for overall image qualityprediction. We find that the pixel-wise gradient magnitude similarity (GMS)between the reference and distorted images combined with a novel poolingstrategy the standard deviation of the GMS map can predict accuratelyperceptual image quality. The resulting GMSD algorithm is much faster than moststate-of-the-art IQA methods, and delivers highly competitive predictionaccuracy.
arxiv-4200-13 | Impulse Noise Removal In Speech Using Wavelets | http://arxiv.org/abs/1310.7447 | author:R. C. Nongpiur category:cs.CV published:2013-08-14 summary:A new method for removing impulse noise from speech in the wavelet transformdomain is proposed. The method utilizes the multiresolution property of thewavelet transform, which provides finer time resolution at the higherfrequencies than the short-time Fourier transform (STFT), to effectivelyidentify and remove impulse noise. It uses two features of speech todiscriminate speech from impulse noise: one is the slow time-varying nature ofspeech and the other is the Lipschitz regularity of the speech components. Onthe basis of these features, an algorithm has been developed to identify andsuppress wavelet coefficients that correspond to impulse noise. Experimentresults show that the new method is able to significantly reduce impulse noisewithout degrading the quality of the speech signal or introducing any audibleartifacts.
arxiv-4200-14 | Arabic Text Recognition in Video Sequences | http://arxiv.org/abs/1308.3243 | author:M. Ben Halima, H. Karray, A. M. Alimi category:cs.MM cs.CL cs.CV published:2013-08-14 summary:In this paper, we propose a robust approach for text extraction andrecognition from Arabic news video sequence. The text included in videosequences is an important needful for indexing and searching system. However,this text is difficult to detect and recognize because of the variability ofits size, their low resolution characters and the complexity of thebackgrounds. To solve these problems, we propose a system performing in twomain tasks: extraction and recognition of text. Our system is tested on avaried database composed of different Arabic news programs and the obtainedresults are encouraging and show the merits of our approach.
arxiv-4200-15 | Average Drift Analysis and its Application | http://arxiv.org/abs/1308.3080 | author:Jun He, Tianshi Chen, Xin Yao category:cs.NE published:2013-08-14 summary:Drift analysis is a useful tool for estimating the running time ofevolutionary algorithms. A new representation of drift analysis, called averagedrift analysis, is described in this paper. It takes a weaker requirement thanpoint-wise drift analysis does. Point-wise drift theorems are corollaries ofour average drift theorems. Therefore average drift analysis is more powerfulthan point-wise drift analysis. To demonstrate the application of average driftanalysis, we choose a (1+N) evolutionary algorithms for linear-like functionsas a case study. Linear-like functions are proposed as a natural extension oflinear functions. For the (1+N) evolutionary algorithms to maximise linear-likefunctions, the lower and upper bounds on their running time have been derivedusing the average drift analysis.
arxiv-4200-16 | Community Detection in Sparse Random Networks | http://arxiv.org/abs/1308.2955 | author:Ery Arias-Castro, Nicolas Verzelen category:math.ST stat.ML stat.TH published:2013-08-13 summary:We consider the problem of detecting a tight community in a sparse randomnetwork. This is formalized as testing for the existence of a dense randomsubgraph in a random graph. Under the null hypothesis, the graph is arealization of an Erd\"os-R\'enyi graph on $N$ vertices and with connectionprobability $p_0$; under the alternative, there is an unknown subgraph on $n$vertices where the connection probability is p1 > p0. In Arias-Castro andVerzelen (2012), we focused on the asymptotically dense regime where p0 islarge enough that np0>(n/N)^{o(1)}. We consider here the asymptotically sparseregime where p0 is small enough that np0<(n/N)^{c0} for some c0>0. As before,we derive information theoretic lower bounds, and also establish theperformance of various tests. Compared to our previous work, the arguments forthe lower bounds are based on the same technology, but are substantially moretechnical in the details; also, the methods we study are different: besides avariant of the scan statistic, we study other statistics such as the size ofthe largest connected component, the number of triangles, the eigengap of theadjacency matrix, etc. Our detection bounds are sharp, except in the Poissonregime where we were not able to fully characterize the constant arising in thebound.
arxiv-4200-17 | Composite Self-Concordant Minimization | http://arxiv.org/abs/1308.2867 | author:Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher category:stat.ML cs.LG math.OC published:2013-08-13 summary:We propose a variable metric framework for minimizing the sum of aself-concordant function and a possibly non-smooth convex function, endowedwith an easily computable proximal operator. We theoretically establish theconvergence of our framework without relying on the usual Lipschitz gradientassumption on the smooth part. An important highlight of our work is a new setof analytic step-size selection and correction procedures based on thestructure of the problem. We describe concrete algorithmic instances of ourframework for several interesting applications and demonstrate them numericallyon both synthetic and real data.
arxiv-4200-18 | An efficient ant based qos aware intelligent temporally ordered routing algorithm for manets | http://arxiv.org/abs/1308.2762 | author:Debajit Sensarma, Koushik Majumder category:cs.NI cs.NE published:2013-08-13 summary:A Mobile Ad hoc network (MANET) is a self configurable network connected bywireless links. This type of network is only suitable for temporarycommunication links as it is infrastructure-less and there is no centralisedcontrol. Providing QoS aware routing is a challenging task in this type ofnetwork due to dynamic topology and limited resources. The main purpose of QoSaware routing is to find a feasible path from source to destination which willsatisfy two or more end to end QoS constrains. Therefore, the task of designingan efficient routing algorithm which will satisfy all the quality of servicerequirements and be robust and adaptive is considered as a highly challengingproblem. In this work we have designed a new efficient and energy awaremultipath routing algorithm based on ACO framework, inspired by the behavioursof biological ants. Basically by considering QoS constraints and artificialants we have designed an intelligent version of classical Temporally OrderedRouting Algorithm (TORA) which will increase network lifetime and decreasepacket loss and average end to end delay that makes this algorithm suitable forreal time and multimedia applications.
arxiv-4200-19 | When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity | http://arxiv.org/abs/1308.2853 | author:Animashree Anandkumar, Daniel Hsu, Majid Janzamin, Sham Kakade category:cs.LG cs.IR math.NA math.ST stat.ML stat.TH published:2013-08-13 summary:Overcomplete latent representations have been very popular for unsupervisedfeature learning in recent years. In this paper, we specify which overcompletemodels can be identified given observable moments of a certain order. Weconsider probabilistic admixture or topic models in the overcomplete regime,where the number of latent topics can greatly exceed the size of the observedword vocabulary. While general overcomplete topic models are not identifiable,we establish generic identifiability under a constraint, referred to as topicpersistence. Our sufficient conditions for identifiability involve a novel setof "higher order" expansion conditions on the topic-word matrix or thepopulation structure of the model. This set of higher-order expansionconditions allow for overcomplete models, and require the existence of aperfect matching from latent topics to higher order observed words. Weestablish that random structured topic models are identifiable w.h.p. in theovercomplete regime. Our identifiability results allows for general(non-degenerate) distributions for modeling the topic proportions, and thus, wecan handle arbitrarily correlated topics in our framework. Our identifiabilityresults imply uniqueness of a class of tensor decompositions with structuredsparsity which is contained in the class of Tucker decompositions, but is moregeneral than the Candecomp/Parafac (CP) decomposition.
arxiv-4200-20 | Semistability-Based Convergence Analysis for Paracontracting Multiagent Coordination Optimization | http://arxiv.org/abs/1308.2930 | author:Qing Hui, Haopeng Zhang category:cs.SY cs.NE math.OC 90C59, 93D99 published:2013-08-13 summary:This sequential technical report extends some of the previous results weposted at arXiv:1306.0225.
arxiv-4200-21 | Multiclass learnability and the ERM principle | http://arxiv.org/abs/1308.2893 | author:Amit Daniely, Sivan Sabato, Shai Ben-David, Shai Shalev-Shwartz category:cs.LG published:2013-08-13 summary:We study the sample complexity of multiclass prediction in several learningsettings. For the PAC setting our analysis reveals a surprising phenomenon: Insharp contrast to binary classification, we show that there exist multiclasshypothesis classes for which some Empirical Risk Minimizers (ERM learners) havelower sample complexity than others. Furthermore, there are classes that arelearnable by some ERM learners, while other ERM learners will fail to learnthem. We propose a principle for designing good ERM learners, and use thisprinciple to prove tight bounds on the sample complexity of learning {\emsymmetric} multiclass hypothesis classes---classes that are invariant underpermutations of label names. We further provide a characterization of mistakeand regret bounds for multiclass learning in the online setting and the banditsetting, using new generalizations of Littlestone's dimension.
arxiv-4200-22 | Toward the Coevolution of Novel Vertical-Axis Wind Turbines | http://arxiv.org/abs/1308.3136 | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE published:2013-08-13 summary:The production of renewable and sustainable energy is one of the mostimportant challenges currently facing mankind. Wind has made an increasingcontribution to the world's energy supply mix, but still remains a long wayfrom reaching its full potential. In this paper, we investigate the use ofartificial evolution to design vertical-axis wind turbine prototypes that arephysically instantiated and evaluated under fan generated wind conditions.Initially a conventional evolutionary algorithm is used to explore the designspace of a single wind turbine and later a cooperative coevolutionary algorithmis used to explore the design space of an array of wind turbines. Artificialneural networks are used throughout as surrogate models to assist learning andfound to reduce the number of fabrications required to reach a higheraerodynamic efficiency. Unlike in other approaches, such as computational fluiddynamics simulations, no mathematical formulations are used and no modelassumptions are made.
arxiv-4200-23 | KL-based Control of the Learning Schedule for Surrogate Black-Box Optimization | http://arxiv.org/abs/1308.2655 | author:Ilya Loshchilov, Marc Schoenauer, MichÃ¨le Sebag category:cs.LG cs.AI stat.ML published:2013-08-12 summary:This paper investigates the control of an ML component within the CovarianceMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-boxoptimization. The known CMA-ES weakness is its sample complexity, the number ofevaluations of the objective function needed to approximate the global optimum.This weakness is commonly addressed through surrogate optimization, learning anestimate of the objective function a.k.a. surrogate model, and replacing mostevaluations of the true objective function with the (inexpensive) evaluation ofthe surrogate model. This paper presents a principled control of the learningschedule (when to relearn the surrogate model), based on the Kullback-Leiblerdivergence of the current search distribution and the training distribution ofthe former surrogate model. The experimental validation of the proposedapproach shows significant performance gains on a comprehensive set ofill-conditioned benchmark problems, compared to the best state of the artincluding the quasi-Newton high-precision BFGS method.
arxiv-4200-24 | B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language | http://arxiv.org/abs/1308.2696 | author:A. Paxton, R. Dale category:cs.CL published:2013-08-12 summary:Discourse analysis may seek to characterize not only the overall compositionof a given text but also the dynamic patterns within the data. This technicalreport introduces a data format intended to facilitate multi-levelinvestigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired bythe long-form data format required for mixed-effects modeling, B(eo)W(u)LFstructures linguistic data into an expanded matrix encoding any number ofresearchers-specified markers, making it ideal for recurrence-based analyses.While we do not necessarily claim to be the first to use methods along theselines, we have created a series of tools utilizing Python and MATLAB to enablesuch discourse analyses and demonstrate them using 319 lines of the Old Englishepic poem, Beowulf, translated into modern English.
arxiv-4200-25 | Faster gradient descent and the efficient recovery of images | http://arxiv.org/abs/1308.2464 | author:Hui Huang, Uri Ascher category:cs.CV cs.NA math.NA published:2013-08-12 summary:Much recent attention has been devoted to gradient descent algorithms wherethe steepest descent step size is replaced by a similar one from a previousiteration or gets updated only once every second step, thus forming a {\emfaster gradient descent method}. For unconstrained convex quadraticoptimization these methods can converge much faster than steepest descent. Butthe context of interest here is application to certain ill-posed inverseproblems, where the steepest descent method is known to have a smoothing,regularizing effect, and where a strict optimization solution is not necessary. Specifically, in this paper we examine the effect of replacing steepestdescent by a faster gradient descent algorithm in the practical context ofimage deblurring and denoising tasks. We also propose several highly efficientschemes for carrying out these tasks independently of the step size selection,as well as a scheme for the case where both blur and significant noise arepresent. In the above context there are situations where many steepest descent stepsare required, thus building slowness into the solution procedure. Our generalconclusion regarding gradient descent methods is that in such cases the fastergradient descent methods offer substantial advantages. In other situationswhere no such slowness buildup arises the steepest descent method can still bevery effective.
arxiv-4200-26 | Local image registration a comparison for bilateral registration mammography | http://arxiv.org/abs/1308.2654 | author:JosÃ© M. Celaya-Padilla, Juan Rodriguez-Rojas, Victor Trevino, JosÃ© G. Gerardo Tamez-Pena category:cs.CV published:2013-08-12 summary:Early tumor detection is key in reducing the number of breast cancer deathand screening mammography is one of the most widely available and reliablemethod for early detection. However, it is difficult for the radiologist toprocess with the same attention each case, due the large amount of images to beread. Computer aided detection (CADe) systems improve tumor detection rate; butthe current efficiency of these systems is not yet adequate and the correctinterpretation of CADe outputs requires expert human intervention. Computeraided diagnosis systems (CADx) are being designed to improve cancer diagnosisaccuracy, but they have not been efficiently applied in breast cancer. CADxefficiency can be enhanced by considering the natural mirror symmetry betweenthe right and left breast. The objective of this work is to evaluateco-registration algorithms for the accurate alignment of the left to rightbreast for CADx enhancement. A set of mammograms were artificially altered tocreate a ground truth set to evaluate the registration efficiency of DEMONs,and SPLINE deformable registration algorithms. The registration accuracy wasevaluated using mean square errors, mutual information and correlation. Theresults on the 132 images proved that the SPLINE deformable registrationover-perform the DEMONS on mammography images.
arxiv-4200-27 | A radial basis function neural network based approach for the electrical characteristics estimation of a photovoltaic module | http://arxiv.org/abs/1308.2375 | author:Francesco Bonanno, Giacomo Capizzi, Christian Napoli, Giorgio Graditi, Giuseppe Marco Tina category:cs.NE published:2013-08-11 summary:The design process of photovoltaic (PV) modules can be greatly enhanced byusing advanced and accurate models in order to predict accurately theirelectrical output behavior. The main aim of this paper is to investigate theapplication of an advanced neural network based model of a module to improvethe accuracy of the predicted output I--V and P--V curves and to keep inaccount the change of all the parameters at different operating conditions.Radial basis function neural networks (RBFNN) are here utilized to predict theoutput characteristic of a commercial PV module, by reading only the data ofsolar irradiation and temperature. A lot of available experimental data wereused for the training of the RBFNN, and a backpropagation algorithm wasemployed. Simulation and experimental validation is reported.
arxiv-4200-28 | Exploratory Analysis of Highly Heterogeneous Document Collections | http://arxiv.org/abs/1308.2359 | author:Arun S. Maiya, John P. Thompson, Francisco Loaiza-Lemos, Robert M. Rolfe category:cs.CL cs.HC cs.IR published:2013-08-11 summary:We present an effective multifaceted system for exploratory analysis ofhighly heterogeneous document collections. Our system is based on intelligentlytagging individual documents in a purely automated fashion and exploiting thesetags in a powerful faceted browsing framework. Tagging strategies employedinclude both unsupervised and supervised approaches based on machine learningand natural language processing. As one of our key tagging strategies, weintroduce the KERA algorithm (Keyword Extraction for Reports and Articles).KERA extracts topic-representative terms from individual documents in a purelyunsupervised fashion and is revealed to be significantly more effective thanstate-of-the-art methods. Finally, we evaluate our system in its ability tohelp users locate documents pertaining to military critical technologies burieddeep in a large heterogeneous sea of information.
arxiv-4200-29 | Hidden Structure and Function in the Lexicon | http://arxiv.org/abs/1308.2428 | author:Olivier Picard, MÃ©lanie Lord, Alexandre Blondin-MassÃ©, Odile Marcotte, Marcos Lopes, Stevan Harnad category:cs.CL published:2013-08-11 summary:How many words are needed to define all the words in a dictionary?Graph-theoretic analysis reveals that about 10% of a dictionary is a uniqueKernel of words that define one another and all the rest, but this is not thesmallest such subset. The Kernel consists of one huge strongly connectedcomponent (SCC), about half its size, the Core, surrounded by many small SCCs,the Satellites. Core words can define one another but not the rest of thedictionary. The Kernel also contains many overlapping Minimal Grounding Sets(MGSs), each about the same size as the Core, each part-Core, part-Satellite.MGS words can define all the rest of the dictionary. They are learned earlier,more concrete and more frequent than the rest of the dictionary. Satellitewords, not correlated with age or frequency, are less concrete (more abstract)words that are also needed for full lexical power.
arxiv-4200-30 | CDfdr: A Comparison Density Approach to Local False Discovery Rate Estimation | http://arxiv.org/abs/1308.2403 | author:Subhadeep Mukhopadhyay category:stat.ME math.ST stat.AP stat.ML stat.TH published:2013-08-11 summary:Efron et al. (2001) proposed empirical Bayes formulation of the frequentistBenjamini and Hochbergs False Discovery Rate method (Benjamini andHochberg,1995). This article attempts to unify the `two cultures' usingconcepts of comparison density and distribution function. We have also shownhow almost all of the existing local fdr methods can be viewed as proposingvarious model specification for comparison density - unifies the vastliterature of false discovery methods under one concept and notation.
arxiv-4200-31 | Collective Mind: cleaning up the research and experimentation mess in computer engineering using crowdsourcing, big data and machine learning | http://arxiv.org/abs/1308.2410 | author:Grigori Fursin category:cs.SE cs.HC stat.ML published:2013-08-11 summary:Software and hardware co-design and optimization of HPC systems has becomeintolerably complex, ad-hoc, time consuming and error prone due to enormousnumber of available design and optimization choices, complex interactionsbetween all software and hardware components, and multiple strict requirementsplaced on performance, power consumption, size, reliability and cost. Wepresent our novel long-term holistic and practical solution to this problembased on customizable, plugin-based, schema-free, heterogeneous, open-sourceCollective Mind repository and infrastructure with unified web interfaces andon-line advise system. This collaborative framework distributes analysis andmulti-objective off-line and on-line auto-tuning of computer systems among manyparticipants while utilizing any available smart phone, tablet, laptop, clusteror data center, and continuously observing, classifying and modeling theirrealistic behavior. Any unexpected behavior is analyzed using shared datamining and predictive modeling plugins or exposed to the community atcTuning.org for collaborative explanation, top-down complexity reduction,incremental problem decomposition and detection of correlating program,architecture or run-time properties (features). Gradually increasingoptimization knowledge helps to continuously improve optimization heuristics ofany compiler, predict optimizations for new programs or suggest efficientrun-time (online) tuning and adaptation strategies depending on end-userrequirements. We decided to share all our past research artifacts includinghundreds of codelets, numerical applications, data sets, models, universalexperimental analysis and auto-tuning pipelines, self-tuning machine learningbased meta compiler, and unified statistical analysis and machine learningplugins in a public repository to initiate systematic, reproducible andcollaborative research, development and experimentation with a new publicationmodel where experiments and techniques are validated, ranked and improved bythe community.
arxiv-4200-32 | Fast image segmentation and restoration using parametric curve evolution with junctions and topology changes | http://arxiv.org/abs/1308.2292 | author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA published:2013-08-10 summary:Curve evolution schemes for image segmentation based on a region basedcontour model allowing for junctions, vector-valued images and topology changesare introduced. Together with an a posteriori denoising in the segmentedhomogeneous regions this leads to a fast and efficient method for imagesegmentation and restoration. An uneven spread of mesh points is avoided byusing the tangential degrees of freedom. Several numerical simulations onartificial test problems and on real images illustrate the performance of themethod.
arxiv-4200-33 | Second order scattering descriptors predict fMRI activity due to visual textures | http://arxiv.org/abs/1310.1257 | author:Michael Eickenberg, Fabian Pedregosa, Senoussi Mehdi, Alexandre Gramfort, Bertrand Thirion category:cs.CV published:2013-08-10 summary:Second layer scattering descriptors are known to provide good classificationperformance on natural quasi-stationary processes such as visual textures dueto their sensitivity to higher order moments and continuity with respect tosmall deformations. In a functional Magnetic Resonance Imaging (fMRI)experiment we present visual textures to subjects and evaluate the predictivepower of these descriptors with respect to the predictive power of simplecontour energy - the first scattering layer. We are able to conclude not onlythat invariant second layer scattering coefficients better encode voxelactivity, but also that well predicted voxels need not necessarily lie in knownretinotopic regions.
arxiv-4200-34 | Finite Element Model Updating Using Fish School Search Optimization Method | http://arxiv.org/abs/1308.2307 | author:I. Boulkabeit, L. Mthembu, T. Marwala, F. De Lima Neto category:cs.CE cs.NE published:2013-08-10 summary:A recent nature inspired optimization algorithm, Fish School Search (FSS) isapplied to the finite element model (FEM) updating problem. This method istested on a GARTEUR SM-AG19 aeroplane structure. The results of this algorithmare compared with two other metaheuristic algorithms; Genetic Algorithm (GA)and Particle Swarm Optimization (PSO). It is observed that on average, the FSSand PSO algorithms give more accurate results than the GA. A minor modificationto the FSS is proposed. This modification improves the performance of FSS onthe FEM updating problem which has a constrained search space.
arxiv-4200-35 | High-Dimensional Regression with Gaussian Mixtures and Partially-Latent Response Variables | http://arxiv.org/abs/1308.2302 | author:Antoine Deleforge, Florence Forbes, Radu Horaud category:cs.LG stat.ML published:2013-08-10 summary:In this work we address the problem of approximating high-dimensional datawith a low-dimensional representation. We make the following contributions. Wepropose an inverse regression method which exchanges the roles of input andresponse, such that the low-dimensional variable becomes the regressor, andwhich is tractable. We introduce a mixture of locally-linear probabilisticmapping model that starts with estimating the parameters of inverse regression,and follows with inferring closed-form solutions for the forward parameters ofthe high-dimensional regression problem of interest. Moreover, we introduce apartially-latent paradigm, such that the vector-valued response variable iscomposed of both observed and latent entries, thus being able to deal with datacontaminated by experimental artifacts that cannot be explained with noisemodels. The proposed probabilistic formulation could be viewed as alatent-variable augmentation of regression. We devise expectation-maximization(EM) procedures based on a data augmentation strategy which facilitates themaximum-likelihood search over the model parameters. We propose twoaugmentation schemes and we describe in detail the associated EM inferenceprocedures that may well be viewed as generalizations of a number of EMregression, dimension reduction, and factor analysis algorithms. The proposedframework is validated with both synthetic and real data. We provideexperimental evidence that our method outperforms several existing regressiontechniques.
arxiv-4200-36 | Learning Features and their Transformations by Spatial and Temporal Spherical Clustering | http://arxiv.org/abs/1308.2350 | author:Jayanta K. Dutta, Bonny Banerjee category:cs.NE cs.CV q-bio.NC I.2; I.4; I.5 published:2013-08-10 summary:Learning features invariant to arbitrary transformations in the data is arequirement for any recognition system, biological or artificial. It is nowwidely accepted that simple cells in the primary visual cortex respond tofeatures while the complex cells respond to features invariant to differenttransformations. We present a novel two-layered feedforward neural model thatlearns features in the first layer by spatial spherical clustering andinvariance to transformations in the second layer by temporal sphericalclustering. Learning occurs in an online and unsupervised manner following theHebbian rule. When exposed to natural videos acquired by a camera mounted on acat's head, the first and second layer neurons in our model develop simple andcomplex cell-like receptive field properties. The model can predict by learninglateral connections among the first layer neurons. A topographic map to theirspatial features emerges by exponentially decaying the flow of activation withdistance from one neuron to another in the first layer that fire in closetemporal proximity, thereby minimizing the pooling length in an online mannersimultaneously with feature learning.
arxiv-4200-37 | Coding for Random Projections | http://arxiv.org/abs/1308.2218 | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:cs.LG cs.DS cs.IT math.IT stat.CO published:2013-08-09 summary:The method of random projections has become very popular for large-scaleapplications in statistical learning, information retrieval, bio-informaticsand other applications. Using a well-designed coding scheme for the projecteddata, which determines the number of bits needed for each projected value andhow to allocate these bits, can significantly improve the effectiveness of thealgorithm, in storage cost as well as computational speed. In this paper, westudy a number of simple coding schemes, focusing on the task of similarityestimation and on an application to training linear classifiers. We demonstratethat uniform quantization outperforms the standard existing influential method(Datar et. al. 2004). Indeed, we argue that in many cases coding with just asmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bitcoding scheme that generally performs well in practice, as confirmed by ourexperiments on training linear support vector machines (SVM).
arxiv-4200-38 | Accuracy of Latent-Variable Estimation in Bayesian Semi-Supervised Learning | http://arxiv.org/abs/1308.2029 | author:Keisuke Yamazaki category:stat.ML published:2013-08-09 summary:Hierarchical probabilistic models, such as Gaussian mixture models, arewidely used for unsupervised learning tasks. These models consist of observableand latent variables, which represent the observable data and the underlyingdata-generation process, respectively. Unsupervised learning tasks, such ascluster analysis, are regarded as estimations of latent variables based on theobservable ones. The estimation of latent variables in semi-supervisedlearning, where some labels are observed, will be more precise than that inunsupervised, and one of the concerns is to clarify the effect of the labeleddata. However, there has not been sufficient theoretical analysis of theaccuracy of the estimation of latent variables. In a previous study, adistribution-based error function was formulated, and its asymptotic form wascalculated for unsupervised learning with generative models. It has been shownthat, for the estimation of latent variables, the Bayes method is more accuratethan the maximum-likelihood method. The present paper reveals the asymptoticforms of the error function in Bayesian semi-supervised learning for bothdiscriminative and generative models. The results show that the generativemodel, which uses all of the given data, performs better when the model is wellspecified.
arxiv-4200-39 | Satellite image classification methods and Landsat 5TM bands | http://arxiv.org/abs/1308.1801 | author:Jamshid Tamouk, Nasser Lotfi, Mina Farmanbar category:cs.CV astro-ph.IM published:2013-08-08 summary:This paper attempts to find the most accurate classification method amongparallelepiped, minimum distance and chain methods. Moreover, this study alsochallenges to find the suitable combination of bands, which can lead to betterresults in case combinations of bands occur. After comparing these threemethods, the chain method over perform the other methods with 79% overallaccuracy. Hence, it is more accurate than minimum distance with 67% andparallelepiped with 65%. On the other hand, based on bands features, and alsoby combining several researchers' findings, a table was created which includesthe main objects on the land and the suitable combination of the bands foraccurately detecting of landcover objects. During this process, it was observedthat band 4 (out of 7 bands of Landsat 5TM) is the band, which can be used forincreasing the accuracy of the combined bands in detecting objects on the land.
arxiv-4200-40 | A Framework for the Analysis of Computational Imaging Systems with Practical Applications | http://arxiv.org/abs/1308.1981 | author:Kaushik Mitra, Oliver Cossairt, Ashok Veeraraghavan category:cs.CV I.4 published:2013-08-08 summary:Over the last decade, a number of Computational Imaging (CI) systems havebeen proposed for tasks such as motion deblurring, defocus deblurring andmultispectral imaging. These techniques increase the amount of light reachingthe sensor via multiplexing and then undo the deleterious effects ofmultiplexing by appropriate reconstruction algorithms. Given the widespreadappeal and the considerable enthusiasm generated by these techniques, adetailed performance analysis of the benefits conferred by this approach isimportant. Unfortunately, a detailed analysis of CI has proven to be a challengingproblem because performance depends equally on three components: (1) theoptical multiplexing, (2) the noise characteristics of the sensor, and (3) thereconstruction algorithm. A few recent papers have performed analysis takingmultiplexing and noise characteristics into account. However, analysis of CIsystems under state-of-the-art reconstruction algorithms, most of which exploitsignal prior models, has proven to be unwieldy. In this paper, we present acomprehensive analysis framework incorporating all three components. In order to perform this analysis, we model the signal priors using aGaussian Mixture Model (GMM). A GMM prior confers two unique characteristics.Firstly, GMM satisfies the universal approximation property which says that anyprior density function can be approximated to any fidelity using a GMM withappropriate number of mixtures. Secondly, a GMM prior lends itself toanalytical tractability allowing us to derive simple expressions for the`minimum mean square error' (MMSE), which we use as a metric to characterizethe performance of CI systems. We use our framework to analyze severalpreviously proposed CI techniques, giving conclusive answer to the question:`How much performance gain is due to use of a signal prior and how much is dueto multiplexing?
arxiv-4200-41 | OFF-Set: One-pass Factorization of Feature Sets for Online Recommendation in Persistent Cold Start Settings | http://arxiv.org/abs/1308.1792 | author:Michal Aharon, Natalie Aizenberg, Edward Bortnikov, Ronny Lempel, Roi Adadi, Tomer Benyamini, Liron Levin, Ran Roth, Ohad Serfaty category:cs.LG cs.IR H.4; D.2.8 published:2013-08-08 summary:One of the most challenging recommendation tasks is recommending to a new,previously unseen user. This is known as the 'user cold start' problem.Assuming certain features or attributes of users are known, one approach forhandling new users is to initially model them based on their features. Motivated by an ad targeting application, this paper describes an extremeonline recommendation setting where the cold start problem is perpetual. Everyuser is encountered by the system just once, receives a recommendation, andeither consumes or ignores it, registering a binary reward. We introduce One-pass Factorization of Feature Sets, OFF-Set, a novelrecommendation algorithm based on Latent Factor analysis, which models users bymapping their features to a latent space. Furthermore, OFF-Set is able to modelnon-linear interactions between pairs of features. OFF-Set is designed forpurely online recommendation, performing lightweight updates of its model pereach recommendation-reward observation. We evaluate OFF-Set against severalstate of the art baselines, and demonstrate its superiority on realad-targeting data.
arxiv-4200-42 | Time series modeling with pruned multi-layer perceptron and 2-stage damped least-squares method | http://arxiv.org/abs/1308.1940 | author:Cyril Voyant, Wani W. Tamas, Christophe Paoli, AurÃ©lia Balu, Marc Muselli, Marie Laure Nivet, Gilles Notton category:cs.NE published:2013-08-08 summary:A Multi-Layer Perceptron (MLP) defines a family of artificial neural networksoften used in TS modeling and forecasting. Because of its "black box" aspect,many researchers refuse to use it. Moreover, the optimization (often based onthe exhaustive approach where "all" configurations are tested) and learningphases of this artificial intelligence tool (often based on theLevenberg-Marquardt algorithm; LMA) are weaknesses of this approach(exhaustively and local minima). These two tasks must be repeated depending onthe knowledge of each new problem studied, making the process, long, laboriousand not systematically robust. In this paper a pruning process is proposed.This method allows, during the training phase, to carry out an inputs selectingmethod activating (or not) inter-nodes connections in order to verify ifforecasting is improved. We propose to use iteratively the popular dampedleast-squares method to activate inputs and neurons. A first pass is applied to10% of the learning sample to determine weights significantly different from 0and delete other. Then a classical batch process based on LMA is used with thenew MLP. The validation is done using 25 measured meteorological TS andcross-comparing the prediction results of the classical LMA and the 2-stageLMA.
arxiv-4200-43 | Predicting protein contact map using evolutionary and physical constraints by integer programming (extended version) | http://arxiv.org/abs/1308.1975 | author:Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML published:2013-08-08 summary:Motivation. Protein contact map describes the pairwise spatial and functionalrelationship of residues in a protein and contains key information for protein3D structure prediction. Although studied extensively, it remains verychallenging to predict contact map using only sequence information. Mostexisting methods predict the contact map matrix element-by-element, ignoringcorrelation among contacts and physical feasibility of the whole contact map. Acouple of recent methods predict contact map based upon residue co-evolution,taking into consideration contact correlation and enforcing a sparsityrestraint, but these methods require a very large number of sequence homologsfor the protein under consideration and the resultant contact map may be stillphysically unfavorable. Results. This paper presents a novel method PhyCMAP for contact mapprediction, integrating both evolutionary and physical restraints by machinelearning and integer linear programming (ILP). The evolutionary restraintsinclude sequence profile, residue co-evolution and context-specific statisticalpotential. The physical restraints specify more concrete relationship amongcontacts than the sparsity restraint. As such, our method greatly reduces thesolution space of the contact map matrix and thus, significantly improvesprediction accuracy. Experimental results confirm that PhyCMAP outperformscurrently popular methods no matter how many sequence homologs are availablefor the protein under consideration. PhyCMAP can predict contacts withinminutes after PSIBLAST search for sequence homologs is done, much faster thanthe two recent methods PSICOV and EvFold. See http://raptorx.uchicago.edu for the web server.
arxiv-4200-44 | The Royal Birth of 2013: Analysing and Visualising Public Sentiment in the UK Using Twitter | http://arxiv.org/abs/1308.1847 | author:Vu Dung Nguyen, Blesson Varghese, Adam Barker category:cs.CL cs.IR cs.SI physics.soc-ph published:2013-08-08 summary:Analysis of information retrieved from microblogging services such as Twittercan provide valuable insight into public sentiment in a geographic region. Thisinsight can be enriched by visualising information in its geographic context.Two underlying approaches for sentiment analysis are dictionary-based andmachine learning. The former is popular for public sentiment analysis, and thelatter has found limited use for aggregating public sentiment from Twitterdata. The research presented in this paper aims to extend the machine learningapproach for aggregating public sentiment. To this end, a framework foranalysing and visualising public sentiment from a Twitter corpus is developed.A dictionary-based approach and a machine learning approach are implementedwithin the framework and compared using one UK case study, namely the royalbirth of 2013. The case study validates the feasibility of the framework foranalysis and rapid visualisation. One observation is that there is goodcorrelation between the results produced by the popular dictionary-basedapproach and the machine learning approach when large volumes of tweets areanalysed. However, for rapid analysis to be possible faster methods need to bedeveloped using big data techniques and parallel methods.
arxiv-4200-45 | A Multi-Swarm Cellular PSO based on Clonal Selection Algorithm in Dynamic Environments | http://arxiv.org/abs/1308.1484 | author:Somayeh Nabizadeh, Alireza Rezvanian, Mohammd Reza Meybodi category:cs.NE cs.AI published:2013-08-07 summary:Many real-world problems are dynamic optimization problems. In this case, theoptima in the environment change dynamically. Therefore, traditionaloptimization algorithms disable to track and find optima. In this paper, a newmulti-swarm cellular particle swarm optimization based on clonal selectionalgorithm (CPSOC) is proposed for dynamic environments. In the proposedalgorithm, the search space is partitioned into cells by a cellular automaton.Clustered particles in each cell, which make a sub-swarm, are evolved by theparticle swarm optimization and clonal selection algorithm. Experimentalresults on Moving Peaks Benchmark demonstrate the superiority of the CPSOC itspopular methods.
arxiv-4200-46 | Challenges of Big Data Analysis | http://arxiv.org/abs/1308.1479 | author:Jianqing Fan, Fang Han, Han Liu category:stat.ML published:2013-08-07 summary:Big Data bring new opportunities to modern society and challenges to datascientists. On one hand, Big Data hold great promises for discovering subtlepopulation patterns and heterogeneities that are not possible with small-scaledata. On the other hand, the massive sample size and high dimensionality of BigData introduce unique computational and statistical challenges, includingscalability and storage bottleneck, noise accumulation, spurious correlation,incidental endogeneity, and measurement errors. These challenges aredistinguished and require new computational and statistical paradigm. Thisarticle give overviews on the salient features of Big Data and how thesefeatures impact on paradigm change on statistical and computational methods aswell as computing architectures. We also provide various new perspectives onthe Big Data analysis and computation. In particular, we emphasis on theviability of the sparsest solution in high-confidence set and point out thatexogeneous assumptions in most statistical methods for Big Data can not bevalidated due to incidental endogeneity. They can lead to wrong statisticalinferences and consequently wrong scientific conclusions.
arxiv-4200-47 | Logical analysis of natural language semantics to solve the problem of computer understanding | http://arxiv.org/abs/1308.1507 | author:Yuriy Ostapov category:cs.CL published:2013-08-07 summary:An object--oriented approach to create a natural language understandingsystem is considered. The understanding program is a formal system built on thebase of predicative calculus. Horn's clauses are used as well--formed formulas.An inference is based on the principle of resolution. Sentences of naturallanguage are represented in the view of typical predicate set. These predicatesdescribe physical objects and processes, abstract objects, categories andsemantic relations between objects. Predicates for concrete assertions aresaved in a database. To describe the semantics of classes for physical objects,abstract concepts and processes, a knowledge base is applied. The proposedrepresentation of natural language sentences is a semantic net. Nodes of suchnet are typical predicates. This approach is perspective as, firstly, suchtypification of nodes facilitates essentially forming of processing algorithmsand object descriptions, secondly, the effectiveness of algorithms is increased(particularly for the great number of nodes), thirdly, to describe thesemantics of words, encyclopedic knowledge is used, and this permitsessentially to extend the class of solved problems.
arxiv-4200-48 | A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid | http://arxiv.org/abs/1308.1603 | author:Dietmar Volz category:cs.NE cs.AI nlin.AO stat.ML 92F99 published:2013-08-07 summary:It will be shown that according to theorems of K. Menger, every neuron gridif identified with a curve is able to preserve the adopted qualitativestructure of a data space. Furthermore, if this identification is made, theneuron grid structure can always be mapped to a subset of a universal neurongrid which is constructable in three space dimensions. Conclusions will bedrawn for established neuron grid types as well as neural fields.
arxiv-4200-49 | Empirical Entropy, Minimax Regret and Minimax Risk | http://arxiv.org/abs/1308.1147 | author:Alexander Rakhlin, Karthik Sridharan, Alexandre B. Tsybakov category:math.ST cs.LG stat.TH published:2013-08-06 summary:We consider the random design regression model with square loss. We propose amethod that aggregates empirical minimizers (ERM) over appropriately chosenrandom subsets and reduces to ERM in the extreme case, and we establish sharporacle inequalities for its risk. We show that, under the $\epsilon^{-p}$growth of the empirical $\epsilon$-entropy, the excess risk of the proposedmethod attains the rate $n^{-\frac{2}{2+p}}$ for $p\in(0,2]$ and $n^{-1/p}$ for$p> 2$ where $n$ is the sample size. Furthermore, for $p\in(0,2]$, the excessrisk rate matches the behavior of the minimax risk of function estimation inregression problems under the well-specified model. This yields a conclusionthat the rates of statistical estimation in well-specified models (minimaxrisk) and in misspecified models (minimax regret) are equivalent in the regime$p\in(0,2]$. In other words, for $p\in(0,2]$ the problem of statisticallearning enjoys the same minimax rate as the problem of statistical estimation.On the contrary, for $p>2$ we show that the rates of the minimax regret are, ingeneral, slower than for the minimax risk. Our oracle inequalities also implythe $v\log(n/v)/n$ rates for Vapnik-Chervonenkis type classes of dimension $v$without the usual convexity assumption on the class; we show that these ratesare optimal. Finally, for a slightly modified method, we derive a bound on theexcess risk of $s$-sparse convex aggregation improving that of (Lounici 07) andproviding the optimal rate.
arxiv-4200-50 | Bayesian ensemble learning for image denoising | http://arxiv.org/abs/1308.1374 | author:Hyuntaek Oh category:cs.CV published:2013-08-06 summary:Natural images are often affected by random noise and image denoising haslong been a central topic in Computer Vision. Many algorithms have beenintroduced to remove the noise from the natural images, such as Gaussian,Wiener filtering and wavelet thresholding. However, many of these algorithmsremove the fine edges and make them blur. Recently, many promising denoisingalgorithms have been introduced such as Non-local Means, Fields of Experts, andBM3D. In this paper, we explore Bayesian method of ensemble learning for imagedenoising. Ensemble methods seek to combine multiple different algorithms toretain the strengths of all methods and the weaknesses of none. Bayesianensemble models are Non-local Means and Fields of Experts, the very successfulrecent algorithms. The Non-local Means presumes that the image contains anextensive amount of self-similarity. The approach of the Fields of Expertsmodel extends traditional Markov Random Field model by learning potentialfunctions over extended pixel neighborhoods. The two models are implemented andimage denoising is performed on natural images. The experimental resultsobtained are used to compare with the single algorithm and discuss the ensemblelearning and their approaches. Comparing to the results of Non-local Means andFields of Experts, Ensemble learning showed improvement nearly 1dB.
arxiv-4200-51 | Multimodal Approach for Video Surveillance Indexing and Retrieval | http://arxiv.org/abs/1308.1150 | author:Ali Wali, Adel M. Alimi category:cs.MM cs.CV published:2013-08-06 summary:In this paper, we present an overview of a multimodal system to indexing andsearching video sequence by the content that has been developed within theREGIMVid project. A large part of our system has been developed as part ofTRECVideo evaluation. The MAVSIR platform provides High-level featureextraction from audio-visual content and concept/event-based video retrieval.We illustrate the architecture of the system as well as provide an overview ofthe descriptors supported to date. Then we demonstrate the usefulness of thetoolbox in the context of feature extraction, concepts/events learning andretrieval in large collections of video surveillance dataset. The results areencouraging as we are able to get good results on several event categories,while for all events we have gained valuable insights and experience.
arxiv-4200-52 | Invariances of random fields paths, with applications in Gaussian Process Regression | http://arxiv.org/abs/1308.1359 | author:David Ginsbourger, Olivier Roustant, Nicolas Durrande category:math.ST math.PR stat.ME stat.ML stat.TH published:2013-08-06 summary:We study pathwise invariances of centred random fields that can be controlledthrough the covariance. A result involving composition operators is obtained insecond-order settings, and we show that various path properties includingadditivity boil down to invariances of the covariance kernel. These results areextended to a broader class of operators in the Gaussian case, via the Lo\`eveisometry. Several covariance-driven pathwise invariances are illustrated,including fields with symmetric paths, centred paths, harmonic paths, or sparsepaths. The proposed approach delivers a number of promising results andperspectives in Gaussian process regression.
arxiv-4200-53 | On b-bit min-wise hashing for large-scale regression and classification with sparse data | http://arxiv.org/abs/1308.1269 | author:Rajen D. Shah, Nicolai Meinshausen category:math.ST stat.ML stat.TH published:2013-08-06 summary:Large-scale regression problems where both the number of variables, $p$, andthe number of observations, $n$, may be large and in the order of millions ormore, are becoming increasingly more common. Typically the data are sparse:only a fraction of a percent of the entries in the design matrix are non-zero.Nevertheless, often the only computationally feasible approach is to performdimension reduction to obtain a new design matrix with far fewer columns, andthen work with this compressed data. $b$-bit min-wise hashing (Li and Konig, 2011) is a promising dimensionreduction scheme for sparse matrices. In this work we study the predictionerror of procedures which perform regression in the new lower-dimensional spaceafter applying the method. For both linear and logistic models we show that theaverage prediction error vanishes asymptotically as long as $q \\beta^*\_2^2/n \rightarrow 0$, where $q$ is the average number of non-zero entries in eachrow of the design matrix and $\beta^*$ is the coefficient of the linearpredictor. We also show that ordinary least squares or ridge regression applied to thereduced data in a sense amounts to a non-parametric regression and can in factallow us fit more flexible models. We obtain non-asymptotic prediction errorbounds for interaction models and for models where an unknown row normalisationmust be applied before the signal is linear in the predictors.
arxiv-4200-54 | Science Fiction as a Worldwide Phenomenon: A Study of International Creation, Consumption and Dissemination | http://arxiv.org/abs/1308.1292 | author:Elysia Wells category:cs.DL cs.CL cs.SI physics.soc-ph published:2013-08-06 summary:This paper examines the international nature of science fiction. The focus ofthis research is to determine whether science fiction is primarily Englishspeaking and Western or global; being created and consumed by people innon-Western, non-English speaking countries? Science fiction's internationalpresence was found in three ways, by network analysis, by examining a onlineretailer and with a survey. Condor, a program developed by GalaxyAdvisors wasused to determine if science fiction is being talked about by non-Englishspeakers. An analysis of the international Amazon.com websites was done todiscover if it was being consumed worldwide. A survey was also conducted to seeif people had experience with science fiction. All three research methodsrevealed similar results. Science fiction was found to be international, withscience fiction creators originating in different countries and writing in ahost of different languages. English and non-English science fiction was beingcreated and consumed all over the world, not just in the English speaking West.
arxiv-4200-55 | The Group Lasso for Design of Experiments | http://arxiv.org/abs/1308.1196 | author:Kentaro Tanaka, Masami Miyakawa category:stat.ML 62K05 published:2013-08-06 summary:We introduce an application of the group lasso to design of exper- iments. Weshow that the problem of constructing an optimal design matrix can betransformed into a problem of the group lasso. We also give a numerical examplethat we can obtain several orthogonal arrays as the solutions of the grouplasso problems.
arxiv-4200-56 | Spatial-Aware Dictionary Learning for Hyperspectral Image Classification | http://arxiv.org/abs/1308.1187 | author:Ali Soltani-Farani, Hamid R. Rabiee, Seyyed Abbas Hosseini category:cs.CV cs.LG published:2013-08-06 summary:This paper presents a structured dictionary-based model for hyperspectraldata that incorporates both spectral and contextual characteristics of aspectral sample, with the goal of hyperspectral image classification. The ideais to partition the pixels of a hyperspectral image into a number of spatialneighborhoods called contextual groups and to model each pixel with a linearcombination of a few dictionary elements learned from the data. Since pixelsinside a contextual group are often made up of the same materials, their linearcombinations are constrained to use common elements from the dictionary. Tothis end, dictionary learning is carried out with a joint sparse regularizer toinduce a common sparsity pattern in the sparse coefficients of each contextualgroup. The sparse coefficients are then used for classification using a linearSVM. Experimental results on a number of real hyperspectral images confirm theeffectiveness of the proposed representation for hyperspectral imageclassification. Moreover, experiments with simulated multispectral data showthat the proposed model is capable of finding representations that mayeffectively be used for classification of multispectral-resolution samples.
arxiv-4200-57 | Boundary identification of events in clinical named entity recognition | http://arxiv.org/abs/1308.1004 | author:Azad Dehghan category:cs.CL published:2013-08-05 summary:The problem of named entity recognition in the medical/clinical domain hasgained increasing attention do to its vital role in a wide range of clinicaldecision support applications. The identification of complete and correct termspan is vital for further knowledge synthesis (e.g., coding/mapping conceptsthesauruses and classification standards). This paper investigates boundaryadjustment by sequence labeling representations models and post-processingtechniques in the problem of clinical named entity recognition (recognition ofclinical events). Using current state-of-the-art sequence labeling algorithm(conditional random fields), we show experimentally that sequence labelingrepresentation and post-processing can be significantly helpful in strictboundary identification of clinical events.
arxiv-4200-58 | Fast Semidifferential-based Submodular Function Optimization | http://arxiv.org/abs/1308.1006 | author:Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.DM cs.LG published:2013-08-05 summary:We present a practical and powerful new framework for both unconstrained andconstrained submodular function optimization based on discretesemidifferentials (sub- and super-differentials). The resulting algorithms,which repeatedly compute and then efficiently optimize submodularsemigradients, offer new and generalize many old methods for submodularoptimization. Our approach, moreover, takes steps towards providing a unifyingparadigm applicable to both submodular min- imization and maximization,problems that historically have been treated quite distinctly. The practicalityof our algorithms is important since interest in submodularity, owing to itsnatural and wide applicability, has recently been in ascendance within machinelearning. We analyze theoretical properties of our algorithms for minimizationand maximization, and show that many state-of-the-art maximization algorithmsare special cases. Lastly, we complement our theoretical analyses withsupporting empirical experiments.
arxiv-4200-59 | Image interpolation using Shearlet based iterative refinement | http://arxiv.org/abs/1308.1126 | author:H. Lakshman, W. -Q Lim, H. Schwarz, D. Marpe, G. Kutyniok, T. Wiegand category:cs.CV 94A08 65T60 published:2013-08-05 summary:This paper proposes an image interpolation algorithm exploiting sparserepresentation for natural images. It involves three main steps: (a) obtainingan initial estimate of the high resolution image using linear methods like FIRfiltering, (b) promoting sparsity in a selected dictionary through iterativethresholding, and (c) extracting high frequency information from theapproximation to refine the initial estimate. For the sparse modeling, ashearlet dictionary is chosen to yield a multiscale directional representation.The proposed algorithm is compared to several state-of-the-art methods toassess its objective as well as subjective performance. Compared to the cubicspline interpolation method, an average PSNR gain of around 0.8 dB is observedover a dataset of 200 images.
arxiv-4200-60 | Trading USDCHF filtered by Gold dynamics via HMM coupling | http://arxiv.org/abs/1308.0900 | author:Donny Lee category:stat.ML cs.LG 91G99, 60J22 published:2013-08-05 summary:We devise a USDCHF trading strategy using the dynamics of gold as a filter.Our strategy involves modelling both USDCHF and gold using a coupled hiddenMarkov model (CHMM). The observations will be indicators, RSI and CCI, whichwill be used as triggers for our trading signals. Upon decoding the model ineach iteration, we can get the next most probable state and the next mostprobable observation. Hopefully by taking advantage of intermarket analysis andthe Markov property implicit in the model, trading with these most probablevalues will produce profitable results.
arxiv-4200-61 | Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels | http://arxiv.org/abs/1308.1009 | author:Ping Li, Gennady Samorodnitsky, John Hopcroft category:cs.LG cs.DS cs.IR published:2013-08-05 summary:The method of stable random projections is popular for efficiently computingthe Lp distances in high dimension (where 0<p<=2), using small space. Becauseit adopts nonadaptive linear projections, this method is naturally suitablewhen the data are collected in a dynamic streaming fashion (i.e., turnstiledata streams). In this paper, we propose to use only the signs of the projecteddata and analyze the probability of collision (i.e., when the two signsdiffer). We derive a bound of the collision probability which is exact when p=2and becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,Cauchy random projections), we show that the probability of collision can beaccurately approximated as functions of the chi-square similarity. For example,when the (un-normalized) data are binary, the maximum approximation error ofthe collision probability is smaller than 0.0192. In text and visionapplications, the chi-square similarity is a popular measure for nonnegativedata when the features are generated from histograms. Our experiments confirmthat the proposed method is promising for large-scale learning applications.
arxiv-4200-62 | Coevolutionary networks of reinforcement-learning agents | http://arxiv.org/abs/1308.1049 | author:Ardeshir Kianercy, Aram Galstyan category:cs.MA cs.LG nlin.AO published:2013-08-05 summary:This paper presents a model of network formation in repeated games where theplayers adapt their strategies and network ties simultaneously using a simplereinforcement-learning scheme. It is demonstrated that the coevolutionarydynamics of such systems can be described via coupled replicator equations. Weprovide a comprehensive analysis for three-player two-action games, which isthe minimum system size with nontrivial structural dynamics. In particular, wecharacterize the Nash equilibria (NE) in such games and examine the localstability of the rest points corresponding to those equilibria. We also studygeneral n-player networks via both simulations and analytical methods and findthat in the absence of exploration, the stable equilibria consist of starmotifs as the main building blocks of the network. Furthermore, in all stableequilibria the agents play pure strategies, even when the game allows mixed NE.Finally, we study the impact of exploration on learning outcomes, and observethat there is a critical exploration rate above which the symmetric anduniformly connected network topology becomes stable.
arxiv-4200-63 | Theoretical Issues for Global Cumulative Treatment Analysis (GCTA) | http://arxiv.org/abs/1308.1066 | author:Jeff Shrager category:stat.AP cs.LG published:2013-08-05 summary:Adaptive trials are now mainstream science. Recently, researchers have takenthe adaptive trial concept to its natural conclusion, proposing what we call"Global Cumulative Treatment Analysis" (GCTA). Similar to the adaptive trial,decision making and data collection and analysis in the GCTA are continuous andintegrated, and treatments are ranked in accord with the statistics of thisinformation, combined with what offers the most information gain. Where GCTAdiffers from an adaptive trial, or, for that matter, from any trial design, isthat all patients are implicitly participants in the GCTA process, regardlessof whether they are formally enrolled in a trial. This paper discusses some ofthe theoretical and practical issues that arise in the design of a GCTA, alongwith some preliminary thoughts on how they might be approached.
arxiv-4200-64 | Context Specific Event Model For News Articles | http://arxiv.org/abs/1308.0897 | author:Kowcika A, Uma Maheswari, Geetha T V category:cs.CL cs.IR published:2013-08-05 summary:We present a new context based event indexing and event ranking model forNews Articles. The context event clusters formed from the UNL Graphs uses themodified scoring scheme for segmenting events which is followed by clusteringof events. From the context clusters obtained three models are developed-Identification of Main and Sub events; Event Indexing and Event Ranking. Basedon the properties considered from the UNL Graphs for the modified scoring mainevents and sub events associated with main-events are identified. The temporaldetails obtained from the context cluster are stored using hashmap datastructure. The temporal details are place-where the event took; person-whoinvolved in that event; time-when the event took place. Based on theinformation collected from the context clusters three indices are generated-Time index, Person index, and Place index. This index gives complete detailsabout every event obtained from context clusters. A new scoring scheme isintroduced for ranking the events. The scoring scheme for event ranking givesweight-age based on the priority level of the events. The priority levelincludes the occurrence of the event in the title of the document, eventfrequency, and inverse document frequency of the events.
arxiv-4200-65 | Head Gesture Recognition using Optical Flow based Classification with Reinforcement of GMM based Background Subtraction | http://arxiv.org/abs/1308.0890 | author:Parimita Saikia, Karen Das category:cs.CV published:2013-08-05 summary:This paper describes a technique of real time head gesture recognitionsystem. The method includes Gaussian mixture model (GMM) accompanied by opticalflow algorithm which provided us the required information regarding headmovement. The proposed model can be implemented in various control system. Weare also presenting the result and implementation of both mentioned method.
arxiv-4200-66 | Risk-consistency of cross-validation with lasso-type procedures | http://arxiv.org/abs/1308.0810 | author:Darren Homrighausen, Daniel J. McDonald category:math.ST stat.ML stat.TH published:2013-08-04 summary:The lasso and related sparsity inducing algorithms have been the target ofsubstantial theoretical and applied research. Correspondingly, many results areknown about their behavior for a fixed or optimally chosen tuning parameterspecified up to unknown constants. In practice, however, this oracle tuningparameter is inaccessible, so one must instead use the data to choose a tuningparameter. Common statistical practice is to use one of a few variants ofcross-validation for this task. However, very little is known about thetheoretical properties of the resulting predictions using data-dependentmethods. We consider the high-dimensional setting with random design whereinthe number of predictors $p$ grows with the number of observations $n$. We showthat the lasso remains risk consistent relative to its linear oracle even whenthe tuning parameter is chosen via cross-validation and the true model is notnecessarily linear. We generalize these results to the group lasso and$\sqrt{\mbox{lasso}}$ and compare the performance of cross-validation to othertuning parameter selection methods via simulations.
arxiv-4200-67 | MonoStream: A Minimal-Hardware High Accuracy Device-free WLAN Localization System | http://arxiv.org/abs/1308.0768 | author:Ibrahim Sabek, Moustafa Youssef category:cs.NI cs.LG published:2013-08-04 summary:Device-free (DF) localization is an emerging technology that allows thedetection and tracking of entities that do not carry any devices norparticipate actively in the localization process. Typically, DF systems requirea large number of transmitters and receivers to achieve acceptable accuracy,which is not available in many scenarios such as homes and small businesses. Inthis paper, we introduce MonoStream as an accurate single-stream DFlocalization system that leverages the rich Channel State Information (CSI) aswell as MIMO information from the physical layer to provide accurate DFlocalization with only one stream. To boost its accuracy and attain lowcomputational requirements, MonoStream models the DF localization problem as anobject recognition problem and uses a novel set of CSI-context features andtechniques with proven accuracy and efficiency. Experimental evaluation in twotypical testbeds, with a side-by-side comparison with the state-of-the-art,shows that MonoStream can achieve an accuracy of 0.95m with at least 26%enhancement in median distance error using a single stream only. Thisenhancement in accuracy comes with an efficient execution of less than 23ms perlocation update on a typical laptop. This highlights the potential ofMonoStream usage for real-time DF tracking applications.
arxiv-4200-68 | Generating Sequences With Recurrent Neural Networks | http://arxiv.org/abs/1308.0850 | author:Alex Graves category:cs.NE cs.CL published:2013-08-04 summary:This paper shows how Long Short-term Memory recurrent neural networks can beused to generate complex sequences with long-range structure, simply bypredicting one data point at a time. The approach is demonstrated for text(where the data are discrete) and online handwriting (where the data arereal-valued). It is then extended to handwriting synthesis by allowing thenetwork to condition its predictions on a text sequence. The resulting systemis able to generate highly realistic cursive handwriting in a wide variety ofstyles.
arxiv-4200-69 | Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text | http://arxiv.org/abs/1308.0701 | author:Meisam Booshehri, Abbas Malekpour, Peter Luksch, Kamran Zamanifar, Shahdad Shariatmadari category:cs.IR cs.CL 68Txx I.2.6 published:2013-08-03 summary:In this position paper we present a new approach for discovering some specialclasses of assertional knowledge in the text by using large RDF repositories,resulting in the extraction of new non-taxonomic ontological relations. Also weuse inductive reasoning beside our approach to make it outperform. Then, weprepare a case study by applying our approach on sample data and illustrate thesoundness of our proposed approach. Moreover in our point of view current LODcloud is not a suitable base for our proposal in all informational domains.Therefore we figure out some directions based on prior works to enrich datasetsof Linked Data by using web mining. The result of such enrichment can be reusedfor further relation extraction and ontology enrichment from unstructured freetext documents.
arxiv-4200-70 | Exploring The Contribution of Unlabeled Data in Financial Sentiment Analysis | http://arxiv.org/abs/1308.0658 | author:Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Shaoyi Liao category:cs.CL cs.LG published:2013-08-03 summary:With the proliferation of its applications in various industries, sentimentanalysis by using publicly available web data has become an active researcharea in text classification during these years. It is argued by researchersthat semi-supervised learning is an effective approach to this problem since itis capable to mitigate the manual labeling effort which is usually expensiveand time-consuming. However, there was a long-term debate on the effectivenessof unlabeled data in text classification. This was partially caused by the factthat many assumptions in theoretic analysis often do not hold in practice. Weargue that this problem may be further understood by adding an additionaldimension in the experiment. This allows us to address this problem in theperspective of bias and variance in a broader view. We show that the well-knownperformance degradation issue caused by unlabeled data can be reproduced as asubset of the whole scenario. We argue that if the bias-variance trade-off isto be better balanced by a more effective feature selection method unlabeleddata is very likely to boost the classification performance. We then propose afeature selection framework in which labeled and unlabeled training samples areboth considered. We discuss its potential in achieving such a balance. Besides,the application in financial sentiment analysis is chosen because it not onlyexemplifies an important application, the data possesses better illustrativepower as well. The implications of this study in text classification andfinancial sentiment analysis are both discussed.
arxiv-4200-71 | Nonlinear Time Series Modeling by LPTime,Nonparametric Empirical Learning | http://arxiv.org/abs/1308.0642 | author:Subhadeep Mukhopadhyay, Emanuel Parzen category:math.ST stat.AP stat.ME stat.ML stat.TH published:2013-08-03 summary:We describe a new comprehensive approach to nonlinear time series analysisand modeling based on recently developed theory on unified algorithms of datascience via LP modeling. We introduce novel data-specific mid-distributionbased Legendre Polynomial(LP) like nonlinear transformations of the originaltime series Y(t) that enables us to adapt all the existing stationary linearGaussian time series modeling strategy and made it applicable for non-Gaussianand nonlinear processes in a robust fashion. The emphasis of the present paperis on empirical time series modeling via the algorithm LPTime. We describe eachstage of the model building process, associated theoretical concepts andillustrate with daily S&P 500 return data between Jan/2/1963 - Dec/31/2009. Ourproposed LPTime algorithm systematically discovers all the `stylized facts' ofthe financial time series automatically all at once, which were previouslynoted by many researchers one at a time.
arxiv-4200-72 | A Comparison of Named Entity Recognition Tools Applied to Biographical Texts | http://arxiv.org/abs/1308.0661 | author:Samet AtdaÄ, Vincent Labatut category:cs.IR cs.CL published:2013-08-03 summary:Named entity recognition (NER) is a popular domain of natural languageprocessing. For this reason, many tools exist to perform this task. Amongstother points, they differ in the processing method they rely upon, the entitytypes they can detect, the nature of the text they can handle, and theirinput/output formats. This makes it difficult for a user to select anappropriate NER tool for a specific situation. In this article, we try toanswer this question in the context of biographic texts. For this matter, wefirst constitute a new corpus by annotating Wikipedia articles. We then selectpublicly available, well known and free for research NER tools for comparison:Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We applythem to our corpus, assess their performances and compare them. Whenconsidering overall performances, a clear hierarchy emerges: Stanford has thebest results, followed by LingPipe, Illionois and OpenCalais. However, a moredetailed evaluation performed relatively to entity types and article categorieshighlights the fact their performances are diversely influenced by thosefactors. This complementarity opens an interesting perspective regarding thecombination of these individual tools in order to improve performance.
arxiv-4200-73 | United Statistical Algorithm, Small and Big Data: Future OF Statistician | http://arxiv.org/abs/1308.0641 | author:Emanuel Parzen, Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH published:2013-08-02 summary:This article provides the role of big idea statisticians in future of BigData Science. We describe the `United Statistical Algorithms' framework forcomprehensive uni?cation of traditional and novel statistical methods formodeling Small Data and Big Data, especially mixed data (discrete, continuous).
arxiv-4200-74 | Using Incomplete Information for Complete Weight Annotation of Road Networks -- Extended Version | http://arxiv.org/abs/1308.0484 | author:Bin Yang, Manohar Kaul, Christian S. Jensen category:cs.LG cs.DB published:2013-08-02 summary:We are witnessing increasing interests in the effective use of road networks.For example, to enable effective vehicle routing, weighted-graph models oftransportation networks are used, where the weight of an edge captures somecost associated with traversing the edge, e.g., greenhouse gas (GHG) emissionsor travel time. It is a precondition to using a graph model for routing thatall edges have weights. Weights that capture travel times and GHG emissions canbe extracted from GPS trajectory data collected from the network. However, GPStrajectory data typically lack the coverage needed to assign weights to alledges. This paper formulates and addresses the problem of annotating all edgesin a road network with travel cost based weights from a set of trips in thenetwork that cover only a small fraction of the edges, each with an associatedground-truth travel cost. A general framework is proposed to solve the problem.Specifically, the problem is modeled as a regression problem and solved byminimizing a judiciously designed objective function that takes into accountthe topology of the road network. In particular, the use of weighted PageRankvalues of edges is explored for assigning appropriate weights to all edges, andthe property of directional adjacency of edges is also taken into account toassign weights. Empirical studies with weights capturing travel time and GHGemissions on two road networks (Skagen, Denmark, and North Jutland, Denmark)offer insight into the design properties of the proposed techniques and offerevidence that the techniques are effective.
arxiv-4200-75 | Sparse Dictionary-based Attributes for Action Recognition and Summarization | http://arxiv.org/abs/1308.0290 | author:Qiang Qiu, Zhuolin Jiang, Rama Chellappa category:cs.CV published:2013-08-01 summary:We present an approach for dictionary learning of action attributes viainformation maximization. We unify the class distribution and appearanceinformation into an objective function for learning a sparse dictionary ofaction attributes. The objective function maximizes the mutual informationbetween what has been learned and what remains to be learned in terms ofappearance information and class distribution for each dictionary atom. Wepropose a Gaussian Process (GP) model for sparse representation to optimize thedictionary objective function. The sparse coding property allows a kernel withcompact support in GP to realize a very efficient dictionary learning process.Hence we can describe an action video by a set of compact and discriminativeaction attributes. More importantly, we can recognize modeled action categoriesin a sparse feature space, which can be generalized to unseen and unmodeledaction categories. Experimental results demonstrate the effectiveness of ourapproach in action recognition and summarization.
arxiv-4200-76 | Learning Robust Subspace Clustering | http://arxiv.org/abs/1308.0273 | author:Qiang Qiu, Guillermo Sapiro category:cs.CV published:2013-08-01 summary:We propose a low-rank transformation-learning framework to robustify subspaceclustering. Many high-dimensional data, such as face images and motionsequences, lie in a union of low-dimensional subspaces. The subspace clusteringproblem has been extensively studied in the literature to partition suchhigh-dimensional data into clusters corresponding to their underlyinglow-dimensional subspaces. However, low-dimensional intrinsic structures areoften violated for real-world observations, as they can be corrupted by errorsor deviate from ideal models. We propose to address this by learning a lineartransformation on subspaces using matrix rank, via its convex surrogate nuclearnorm, as the optimization criteria. The learned linear transformation restoresa low-rank structure for data from the same subspace, and, at the same time,forces a high-rank structure for data from different subspaces. In this way, wereduce variations within the subspaces, and increase separations between thesubspaces for more accurate subspace clustering. This proposed learned robustsubspace clustering framework significantly enhances the performance ofexisting subspace clustering methods. To exploit the low-rank structures of thetransformed subspaces, we further introduce a subspace clustering technique,called Robust Sparse Subspace Clustering, which efficiently combines robust PCAwith sparse modeling. We also discuss the online learning of thetransformation, and learning of the transformation while simultaneouslyreducing the data dimensionality. Extensive experiments using public datasetsare presented, showing that the proposed approach significantly outperformsstate-of-the-art subspace clustering methods.
arxiv-4200-77 | Sparse arrays of signatures for online character recognition | http://arxiv.org/abs/1308.0371 | author:Benjamin Graham category:cs.CV cs.NE published:2013-08-01 summary:In mathematics the signature of a path is a collection of iterated integrals,commonly used for solving differential equations. We show that the pathsignature, used as a set of features for consumption by a convolutional neuralnetwork (CNN), improves the accuracy of online character recognition---that isthe task of reading characters represented as a collection of paths. Usingdatasets of letters, numbers, Assamese and Chinese characters, we show that thefirst, second, and even the third iterated integrals contain useful informationfor consumption by a CNN. On the CASIA-OLHWDB1.1 3755 Chinese character dataset, our approach gave atest error of 3.58%, compared with 5.61% for a traditional CNN [Ciresan etal.]. A CNN trained on the CASIA-OLHWDB1.0-1.2 datasets won the ICDAR2013Online Isolated Chinese Character recognition competition. Computationally, we have developed a sparse CNN implementation that make itpractical to train CNNs with many layers of max-pooling. Extending the MNISTdataset by translations, our sparse CNN gets a test error of 0.31%.
arxiv-4200-78 | MAS for video objects segmentation and tracking based on active contours and SURF descriptor | http://arxiv.org/abs/1308.0315 | author:Mohamed Chakroun, Ali Wali, Adel M. Alimi category:cs.MM cs.CV published:2013-08-01 summary:In computer vision, video segmentation and tracking is an importantchallenging issue. In this paper, we describe a new video sequencessegmentation and tracking algorithm based on MAS "multi-agent systems" and SURF"Speeded Up Robust Features". Our approach consists in modelling a multi-agentsystem for segmenting the first image from a video sequence and trackingobjects in the video sequences. The used agents are supervisor and exploratoragents, they are communicating between them and they inspire in their behaviorfrom active contours approaches. The tracking of objects is based on SURFdescriptors "Speed Up Robust Features". We used the DIMA platform and "APIAteji PX" (an extension of the Java language to facilitate parallel programmingon heterogeneous architectures) to implement this algorithm. The experimentalresults indicate that the proposed algorithm is more robust and faster thanprevious approaches.
arxiv-4200-79 | Domain-invariant Face Recognition using Learned Low-rank Transformation | http://arxiv.org/abs/1308.0275 | author:Qiang Qiu, Guillermo Sapiro, Ching-Hui Chen category:cs.CV published:2013-08-01 summary:We present a low-rank transformation approach to compensate for facevariations due to changes in visual domains, such as pose and illumination. Thekey idea is to learn discriminative linear transformations for face imagesusing matrix rank as the optimization criteria. The learned lineartransformations restore a shared low-rank structure for faces from the samesubject, and, at the same time, force a high-rank structure for faces fromdifferent subjects. In this way, among the transformed faces, we reducevariations caused by domain changes within the classes, and increaseseparations between the classes for better face recognition across domains.Extensive experiments using public datasets are presented to demonstrate theeffectiveness of our approach for face recognition across domains. Thepotential of the approach for feature extraction in generic object recognitionand coded aperture design are discussed as well.
arxiv-4200-80 | Design and Development of an Expert System to Help Head of University Departments | http://arxiv.org/abs/1308.0356 | author:Shervan Fekri-Ershad, Hadi Tajalizadeh, Shahram Jafari category:cs.AI cs.LG published:2013-08-01 summary:One of the basic tasks which is responded for head of each universitydepartment, is employing lecturers based on some default factors such asexperience, evidences, qualifies and etc. In this respect, to help the heads,some automatic systems have been proposed until now using machine learningmethods, decision support systems (DSS) and etc. According to advantages anddisadvantages of the previous methods, a full automatic system is designed inthis paper using expert systems. The proposed system is included two mainsteps. In the first one, the human expert's knowledge is designed as decisiontrees. The second step is included an expert system which is evaluated usingextracted rules of these decision trees. Also, to improve the quality of theproposed system, a majority voting algorithm is proposed as post processingstep to choose the best lecturer which satisfied more expert's decision treesfor each course. The results are shown that the designed system averageaccuracy is 78.88. Low computational complexity, simplicity to program and aresome of other advantages of the proposed system.
arxiv-4200-81 | Hybrid Focal Stereo Networks for Pattern Analysis in Homogeneous Scenes | http://arxiv.org/abs/1308.0365 | author:Emanuel Aldea, Khurom H. Kiyani category:cs.CV published:2013-08-01 summary:In this paper we address the problem of multiple camera calibration in thepresence of a homogeneous scene, and without the possibility of employingcalibration object based methods. The proposed solution exploits salientfeatures present in a larger field of view, but instead of employing activevision we replace the cameras with stereo rigs featuring a long focal analysiscamera, as well as a short focal registration camera. Thus, we are able topropose an accurate solution which does not require intrinsic variation modelsas in the case of zooming cameras. Moreover, the availability of the two viewssimultaneously in each rig allows for pose re-estimation between rigs as oftenas necessary. The algorithm has been successfully validated in an indoorsetting, as well as on a difficult scene featuring a highly dense pilgrim crowdin Makkah.
arxiv-4200-82 | An Enhanced Features Extractor for a Portfolio of Constraint Solvers | http://arxiv.org/abs/1308.0227 | author:Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro category:cs.AI cs.LG published:2013-08-01 summary:Recent research has shown that a single arbitrarily efficient solver can besignificantly outperformed by a portfolio of possibly slower on-averagesolvers. The solver selection is usually done by means of (un)supervisedlearning techniques which exploit features extracted from the problemspecification. In this paper we present an useful and flexible framework thatis able to extract an extensive set of features from a Constraint(Satisfaction/Optimization) Problem defined in possibly different modelinglanguages: MiniZinc, FlatZinc or XCSP. We also report some empirical resultsshowing that the performances that can be obtained using these features areeffective and competitive with state of the art CSP portfolio techniques.
arxiv-4200-83 | Compositional Dictionaries for Domain Adaptive Face Recognition | http://arxiv.org/abs/1308.0271 | author:Qiang Qiu, Rama Chellappa category:cs.CV published:2013-08-01 summary:We present a dictionary learning approach to compensate for thetransformation of faces due to changes in view point, illumination, resolution,etc. The key idea of our approach is to force domain-invariant sparse coding,i.e., design a consistent sparse representation of the same face in differentdomains. In this way, classifiers trained on the sparse codes in the sourcedomain consisting of frontal faces for example can be applied to the targetdomain (consisting of faces in different poses, illumination conditions, etc)without much loss in recognition accuracy. The approach is to first learn adomain base dictionary, and then describe each domain shift (identity, pose,illumination) using a sparse representation over the base dictionary. Thedictionary adapted to each domain is expressed as sparse linear combinations ofthe base dictionary. In the context of face recognition, with the proposedcompositional dictionary approach, a face image can be decomposed into sparserepresentations for a given subject, pose and illumination respectively. Thisapproach has three advantages: first, the extracted sparse representation for asubject is consistent across domains and enables pose and illuminationinsensitive face recognition. Second, sparse representations for pose andillumination can subsequently be used to estimate the pose and illuminationcondition of a face image. Finally, by composing sparse representations forsubject and the different domains, we can also perform pose alignment andillumination normalization. Extensive experiments using two public facedatasets are presented to demonstrate the effectiveness of our approach forface recognition.
arxiv-4200-84 | Who and Where: People and Location Co-Clustering | http://arxiv.org/abs/1307.8405 | author:Zixuan Wang, Jinyun Yan category:cs.CV published:2013-07-31 summary:In this paper, we consider the clustering problem on images where each imagecontains patches in people and location domains. We exploit the correlationbetween people and location domains, and proposed a semi-supervisedco-clustering algorithm to cluster images. Our algorithm updates thecorrelation links at the runtime, and produces clustering in both domainssimultaneously. We conduct experiments in a manually collected dataset and aFlickr dataset. The result shows that the such correlation improves theclustering performance.
arxiv-4200-85 | A Time and Space Efficient Junction Tree Architecture | http://arxiv.org/abs/1308.0187 | author:Stephen Pasteris category:cs.AI cs.LG published:2013-07-31 summary:The junction tree algorithm is a way of computing marginals of booleanmultivariate probability distributions that factorise over sets of randomvariables. The junction tree algorithm first constructs a tree called ajunction tree who's vertices are sets of random variables. The algorithm thenperforms a generalised version of belief propagation on the junction tree. TheShafer-Shenoy and Hugin architectures are two ways to perform this beliefpropagation that tradeoff time and space complexities in different ways: Huginpropagation is at least as fast as Shafer-Shenoy propagation and in the casesthat we have large vertices of high degree is significantly faster. However,this speed increase comes at the cost of an increased space complexity. Thispaper first introduces a simple novel architecture, ARCH-1, which has the bestof both worlds: the speed of Hugin propagation and the low space requirementsof Shafer-Shenoy propagation. A more complicated novel architecture, ARCH-2, isthen introduced which has, up to a factor only linear in the maximumcardinality of any vertex, time and space complexities at least as good asARCH-1 and in the cases that we have large vertices of high degree issignificantly faster than ARCH-1.
arxiv-4200-86 | A Prototyping Environment for Integrated Artificial Attention Systems | http://arxiv.org/abs/1307.8233 | author:Jan TÃ¼nnermann, Markus Hennig, Michael Silbernagel, BÃ¤rbel Mertsching category:cs.CV published:2013-07-31 summary:Artificial visual attention systems aim to support technical systems invisual tasks by applying the concepts of selective attention observed in humansand other animals. Such systems are typically evaluated against ground truthobtained from human gaze-data or manually annotated test images. When appliedto robotics, the systems are required to be adaptable to the target system.Here, we describe a flexible environment based on a robotic middleware layerallowing the development and testing of attention-guided vision systems. Insuch a framework, the systems can be tested with input from various sources,different attention algorithms at the core, and diverse subsequent tasks.
arxiv-4200-87 | Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) | http://arxiv.org/abs/1307.8430 | author:Bryan R. Conroy, Jennifer M. Walz, Brian Cheung, Paul Sajda category:cs.LG stat.ML published:2013-07-31 summary:We present an efficient algorithm for simultaneously training sparsegeneralized linear models across many related problems, which may arise frombootstrapping, cross-validation and nonparametric permutation testing. Ourapproach leverages the redundancies across problems to obtain significantcomputational improvements relative to solving the problems sequentially by aconventional algorithm. We demonstrate our fast simultaneous training ofgeneralized linear models (FaSTGLZ) algorithm on a number of real-worlddatasets, and we run otherwise computationally intensive bootstrapping andpermutation test analyses that are typically necessary for obtainingstatistically rigorous classification results and meaningful interpretation.Code is freely available at http://liinc.bme.columbia.edu/fastglz.
arxiv-4200-88 | A Novel Architecture for Relevant Blog Page Identifcation | http://arxiv.org/abs/1307.8225 | author:Deepti Kapri, Rosy Madaan, A. K Sharma, Ashutosh Dixit category:cs.IR cs.CL published:2013-07-31 summary:Blogs are undoubtedly the richest source of information available incyberspace. Blogs can be of various natures i.e. personal blogs which containposts on mixed issues or blogs can be domain specific which contains posts onparticular topics, this is the reason, they offer wide variety of relevantinformation which is often focused. A general search engine gives back a hugecollection of web pages which may or may not give correct answers, as web isthe repository of information of all kinds and a user has to go through variousdocuments before he gets what he was originally looking for, which is a verytime consuming process. So, the search can be made more focused and accurate ifit is limited to blogosphere instead of web pages. The reason being that theblogs are more focused in terms of information. So, User will only get relatedblogs in response to his query. These results will be then ranked according toour proposed method and are finally presented in front of user in descendingorder
arxiv-4200-89 | The Power of Localization for Efficiently Learning Linear Separators with Noise | http://arxiv.org/abs/1307.8371 | author:Pranjal Awasthi, Maria Florina Balcan, Philip M. Long category:cs.LG cs.CC cs.DS stat.ML published:2013-07-31 summary:We introduce a new approach for designing computationally efficient learningalgorithms that are tolerant to noise. We demonstrate the effectiveness of ourapproach by designing algorithms with improved noise tolerance guarantees forlearning linear separators. We consider the malicious noise model of Valiant and the adversarial labelnoise model of Kearns, Schapire, and Sellie. For malicious noise, where theadversary can corrupt an $\eta$ of fraction both the label part and the featurepart, we provide a polynomial-time algorithm for learning linear separators in$\Re^d$ under the uniform distribution with near information-theoretic optimalnoise tolerance of $\eta = \Omega(\epsilon)$. We also get similar improvementsfor the adversarial label noise model. We obtain similar results for moregeneral classes of distributions including isotropic log-concave distributions. In addition, our algorithms achieve a label complexity whose dependence onthe error parameter $\epsilon$ is {\em exponentially better} than that of anypassive algorithm. This provides the first polynomial-time active learningalgorithm for learning linear separators in the presence of adversarial labelnoise, as well as the first analysis of active learning under the maliciousnoise model.
arxiv-4200-90 | Tracking Extrema in Dynamic Environment using Multi-Swarm Cellular PSO with Local Search | http://arxiv.org/abs/1307.8279 | author:Somayeh Nabizadeh, Alireza Rezvanian, Mohammad Reza Meybodi category:cs.AI cs.NE published:2013-07-31 summary:Many real-world phenomena can be modelled as dynamic optimization problems.In such cases, the environment problem changes dynamically and therefore,conventional methods are not capable of dealing with such problems. In thispaper, a novel multi-swarm cellular particle swarm optimization algorithm isproposed by clustering and local search. In the proposed algorithm, the searchspace is partitioned into cells, while the particles identify changes in thesearch space and form clusters to create sub-swarms. Then a local search isapplied to improve the solutions in the each cell. Simulation results forstatic standard benchmarks and dynamic environments show superiority of theproposed method over other alternative approaches.
arxiv-4200-91 | Towards Minimax Online Learning with Unknown Time Horizon | http://arxiv.org/abs/1307.8187 | author:Haipeng Luo, Robert E. Schapire category:cs.LG published:2013-07-31 summary:We consider online learning when the time horizon is unknown. We apply aminimax analysis, beginning with the fixed horizon case, and then moving on totwo unknown-horizon settings, one that assumes the horizon is chosen randomlyaccording to some known distribution, and the other which allows the adversaryfull control over the horizon. For the random horizon setting with restrictedlosses, we derive a fully optimal minimax algorithm. And for the adversarialhorizon setting, we prove a nontrivial lower bound which shows that theadversary obtains strictly more power than when the horizon is fixed and known.Based on the minimax solution of the random horizon setting, we then propose anew adaptive algorithm which "pretends" that the horizon is drawn from adistribution from a special family, but no matter how the actual horizon ischosen, the worst-case regret is of the optimal rate. Furthermore, ouralgorithm can be combined and applied in many ways, for instance, to onlineconvex optimization, follow the perturbed leader, exponential weights algorithmand first order bounds. Experiments show that our algorithm outperforms manyother existing algorithms in an online linear optimization setting.
arxiv-4200-92 | The Planning-ahead SMO Algorithm | http://arxiv.org/abs/1307.8305 | author:Tobias Glasmachers category:cs.LG published:2013-07-31 summary:The sequential minimal optimization (SMO) algorithm and variants thereof arethe de facto standard method for solving large quadratic programs for supportvector machine (SVM) training. In this paper we propose a simple yet powerfulmodification. The main emphasis is on an algorithm improving the SMO step sizeby planning-ahead. The theoretical analysis ensures its convergence to theoptimum. Experiments involving a large number of datasets were carried out todemonstrate the superiority of the new algorithm.
arxiv-4200-93 | Posterior Contraction Rates of the Phylogenetic Indian Buffet Processes | http://arxiv.org/abs/1307.8229 | author:Mengjie Chen, Chao Gao, Hongyu Zhao category:stat.ML math.ST q-bio.QM stat.AP stat.TH published:2013-07-31 summary:By expressing prior distributions as general stochastic processes,nonparametric Bayesian methods provide a flexible way to incorporate priorknowledge and constrain the latent structure in statistical inference. TheIndian buffet process (IBP) is such an example that can be used to define aprior distribution on infinite binary features, where the exchangeability amongsubjects is assumed. The phylogenetic Indian buffet process (pIBP), aderivative of IBP, enables the modeling of non-exchangeability among subjectsthrough a stochastic process on a rooted tree, which is similar to that used inphylogenetics, to describe relationships among the subjects. In this paper, westudy the theoretical properties of IBP and pIBP under a binary factor model.We establish the posterior contraction rates for both IBP and pIBP andsubstantiate the theoretical results through simulation studies. This is thefirst work addressing the frequentist property of the posterior behaviors ofIBP and pIBP. We also demonstrated its practical usefulness by applying pIBPprior to a real data example arising in the field of cancer genomics where theexchangeability among subjects is violated.
arxiv-4200-94 | Optimistic Concurrency Control for Distributed Unsupervised Learning | http://arxiv.org/abs/1307.8049 | author:Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael I. Jordan category:cs.LG cs.AI cs.DC published:2013-07-30 summary:Research on distributed machine learning algorithms has focused primarily onone of two extremes - algorithms that obey strict concurrency constraints oralgorithms that obey few or no such constraints. We consider an intermediatealternative in which algorithms optimistically assume that conflicts areunlikely and if conflicts do arise a conflict-resolution protocol is invoked.We view this "optimistic concurrency control" paradigm as particularlyappropriate for large-scale machine learning algorithms, particularly in theunsupervised setting. We demonstrate our approach in three problem areas:clustering, feature learning and online facility location. We evaluate ourmethods via large-scale experiments in a cluster computing environment.
arxiv-4200-95 | Extracting Information-rich Part of Texts using Text Denoising | http://arxiv.org/abs/1307.8060 | author:Rushdi Shams category:cs.IR cs.CL published:2013-07-30 summary:The aim of this paper is to report on a novel text reduction technique,called Text Denoising, that highlights information-rich content when processinga large volume of text data, especially from the biomedical domain. The corefeature of the technique, the text readability index, embodies the hypothesisthat complex text is more information-rich than the rest. When applied on taskslike biomedical relation bearing text extraction, keyphrase indexing andextracting sentences describing protein interactions, it is evident that thereduced set of text produced by text denoising is more information-rich thanthe rest.
arxiv-4200-96 | Hybrid Affinity Propagation | http://arxiv.org/abs/1307.7851 | author:Jingdong Wang, Hao Xu, Xian-Sheng Hua, Shipeng Li category:cs.CV published:2013-07-30 summary:In this paper, we address a problem of managing tagged images with hybridsummarization. We formulate this problem as finding a few image exemplars torepresent the image set semantically and visually, and solve it in a hybrid wayby exploiting both visual and textual information associated with images. Wepropose a novel approach, called homogeneous and heterogeneous messagepropagation ($\text{H}^\text{2}\text{MP}$). Similar to the affinity propagation(AP) approach, $\text{H}^\text{2}\text{MP}$ reduce the conventional\emph{vector} message propagation to \emph{scalar} message propagation to makethe algorithm more efficient. Beyond AP that can only handle homogeneous data,$\text{H}^\text{2}\text{MP}$ generalizes it to exploit extra heterogeneousrelations and the generalization is non-trivial as the reduction to scalarmessages from vector messages is more challenging. The main advantages of ourapproach lie in 1) that $\text{H}^\text{2}\text{MP}$ exploits visual similarityand in addition the useful information from the associated tags, including theassociations relation between images and tags and the relations within tags,and 2) that the summary is both visually and semantically satisfactory. Inaddition, our approach can also present a textual summary to a tagged imagecollection, which can be used to automatically generate a textual description.The experimental results demonstrate the effectiveness and efficiency of theroposed approach.
arxiv-4200-97 | An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human Attention | http://arxiv.org/abs/1307.7848 | author:Lucas Paletta, Katrin Santner, Gerald Fritz category:cs.CV published:2013-07-30 summary:This work describes a computer vision system that enables pervasive mappingand monitoring of human attention. The key contribution is that our methodologyenables full 3D recovery of the gaze pointer, human view frustum and associatedhuman centered measurements directly into an automatically computed 3D model inreal-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3Dmodeling, localization and fully automated annotation of ROIs (regions ofinterest) within the acquired 3D model. This innovative methodology will opennew avenues for attention studies in real world environments, bringing newpotential into automated processing for human factors technologies.
arxiv-4200-98 | Efficient Energy Minimization for Enforcing Statistics | http://arxiv.org/abs/1307.7800 | author:Yongsub Lim, Kyomin Jung, Pushmeet Kohli category:cs.CV published:2013-07-30 summary:Energy minimization algorithms, such as graph cuts, enable the computation ofthe MAP solution under certain probabilistic models such as Markov randomfields. However, for many computer vision problems, the MAP solution under themodel is not the ground truth solution. In many problem scenarios, the systemhas access to certain statistics of the ground truth. For instance, in imagesegmentation, the area and boundary length of the object may be known. In thesecases, we want to estimate the most probable solution that is consistent withsuch statistics, i.e., satisfies certain equality or inequality constraints. The above constrained energy minimization problem is NP-hard in general, andis usually solved using Linear Programming formulations, which relax theintegrality constraints. This paper proposes a novel method that finds thediscrete optimal solution of such problems by maximizing the correspondingLagrangian dual. This method can be applied to any constrained energyminimization problem whose unconstrained version is polynomial time solvable,and can handle multiple, equality or inequality, and linear or non-linearconstraints. We demonstrate the efficacy of our method on theforeground/background image segmentation problem, and show that it producesimpressive segmentation results with less error, and runs more than 20 timesfaster than the state-of-the-art LP relaxation based approaches.
arxiv-4200-99 | Protein (Multi-)Location Prediction: Using Location Inter-Dependencies in a Probabilistic Framework | http://arxiv.org/abs/1307.7795 | author:Ramanuja Simha, Hagit Shatkay category:q-bio.QM cs.CE cs.LG q-bio.GN published:2013-07-30 summary:Knowing the location of a protein within the cell is important forunderstanding its function, role in biological processes, and potential use asa drug target. Much progress has been made in developing computational methodsthat predict single locations for proteins, assuming that proteins localize toa single location. However, it has been shown that proteins localize tomultiple locations. While a few recent systems have attempted to predictmultiple locations of proteins, they typically treat locations as independentor capture inter-dependencies by treating each locations-combination present inthe training set as an individual location-class. We present a new method and apreliminary system we have developed that directly incorporatesinter-dependencies among locations into the multiple-location-predictionprocess, using a collection of Bayesian network classifiers. We evaluate oursystem on a dataset of single- and multi-localized proteins. Our results,obtained by incorporating inter-dependencies are significantly higher thanthose obtained by classifiers that do not use inter-dependencies. Theperformance of our system on multi-localized proteins is comparable to a topperforming system (YLoc+), without restricting predictions to be based only onlocation-combinations present in the training set.
arxiv-4200-100 | Neural Network Capacity for Multilevel Inputs | http://arxiv.org/abs/1307.8104 | author:Matt Stowe, Subhash Kak category:cs.NE published:2013-07-30 summary:This paper examines the memory capacity of generalized neural networks.Hopfield networks trained with a variety of learning techniques areinvestigated for their capacity both for binary and non-binary alphabets. It isshown that the capacity can be much increased when multilevel inputs are used.New learning strategies are proposed to increase Hopfield network capacity, andthe scalability of these methods is also examined in respect to size of thenetwork. The ability to recall entire patterns from stimulation of a singleneuron is examined for the increased capacity networks.
arxiv-4200-101 | DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering | http://arxiv.org/abs/1307.8136 | author:Brian P. Kent, Alessandro Rinaldo, Timothy Verstynen category:stat.ME cs.LG stat.ML published:2013-07-30 summary:The level set tree approach of Hartigan (1975) provides a probabilisticallybased and highly interpretable encoding of the clustering behavior of adataset. By representing the hierarchy of data modes as a dendrogram of thelevel sets of a density estimator, this approach offers many advantages forexploratory analysis and clustering, especially for complex andhigh-dimensional data. Several R packages exist for level set tree estimation,but their practical usefulness is limited by computational inefficiency,absence of interactive graphical capabilities and, from a theoreticalperspective, reliance on asymptotic approximations. To make it easier forpractitioners to capture the advantages of level set trees, we have written thePython package DeBaCl for DEnsity-BAsed CLustering. In this article weillustrate how DeBaCl's level set tree estimates can be used for difficultclustering tasks and interactive graphical data analysis. The package isintended to promote the practical use of level set trees through improvementsin computational efficiency and a high degree of user customization. Inaddition, the flexible algorithms implemented in DeBaCl enjoy finite sampleaccuracy, as demonstrated in recent literature on density clustering. Finally,we show the level set tree framework can be easily extended to deal withfunctional data.
arxiv-4200-102 | Multi-dimensional Parametric Mincuts for Constrained MAP Inference | http://arxiv.org/abs/1307.7793 | author:Yongsub Lim, Kyomin Jung, Pushmeet Kohli category:cs.LG cs.AI published:2013-07-30 summary:In this paper, we propose novel algorithms for inferring the Maximum aPosteriori (MAP) solution of discrete pairwise random field models undermultiple constraints. We show how this constrained discrete optimizationproblem can be formulated as a multi-dimensional parametric mincut problem viaits Lagrangian dual, and prove that our algorithm isolates all constraintinstances for which the problem can be solved exactly. These multiple solutionsenable us to even deal with `soft constraints' (higher order penaltyfunctions). Moreover, we propose two practical variants of our algorithm tosolve problems with hard constraints. We also show how our method can beapplied to solve various constrained discrete optimization problems such assubmodular minimization and shortest path computation. Experimental evaluationusing the foreground-background image segmentation problem with statisticconstraints reveals that our method is faster and its results are closer to theground truth labellings compared with the popular continuous relaxation basedmethods.
arxiv-4200-103 | Scalable $k$-NN graph construction | http://arxiv.org/abs/1307.7852 | author:Jingdong Wang, Jing Wang, Gang Zeng, Zhuowen Tu, Rui Gan, Shipeng Li category:cs.CV cs.LG stat.ML published:2013-07-30 summary:The $k$-NN graph has played a central role in increasingly populardata-driven techniques for various learning and vision tasks; yet, finding anefficient and effective way to construct $k$-NN graphs remains a challenge,especially for large-scale high-dimensional data. In this paper, we propose anew approach to construct approximate $k$-NN graphs with emphasis in:efficiency and accuracy. We hierarchically and randomly divide the data pointsinto subsets and build an exact neighborhood graph over each subset, achievinga base approximate neighborhood graph; we then repeat this process for severaltimes to generate multiple neighborhood graphs, which are combined to yield amore accurate approximate neighborhood graph. Furthermore, we propose aneighborhood propagation scheme to further enhance the accuracy. We show boththeoretical and empirical accuracy and efficiency of our approach to $k$-NNgraph construction and demonstrate significant speed-up in dealing with largescale visual data.
arxiv-4200-104 | Energy Distribution of EEG Signals: EEG Signal Wavelet-Neural Network Classifier | http://arxiv.org/abs/1307.7897 | author:Ibrahim Omerhodzic, Samir Avdakovic, Amir Nuhanovic, Kemal Dizdarevic category:cs.NE q-bio.NC published:2013-07-30 summary:In this paper, a wavelet-based neural network (WNN) classifier forrecognizing EEG signals is implemented and tested under three sets EEG signals(healthy subjects, patients with epilepsy and patients with epileptic syndromeduring the seizure). First, the Discrete Wavelet Transform (DWT) with theMulti-Resolution Analysis (MRA) is applied to decompose EEG signal atresolution levels of the components of the EEG signal (delta, theta, alpha,beta and gamma) and the Parsevals theorem are employed to extract thepercentage distribution of energy features of the EEG signal at differentresolution levels. Second, the neural network (NN) classifies these extractedfeatures to identify the EEGs type according to the percentage distribution ofenergy features. The performance of the proposed algorithm has been evaluatedusing in total 300 EEG signals. The results showed that the proposed classifierhas the ability of recognizing and classifying EEG signals efficiently.
arxiv-4200-105 | On the accuracy of the Viterbi alignment | http://arxiv.org/abs/1307.7948 | author:Kristi Kuljus, JÃ¼ri Lember category:stat.ME cs.LG stat.CO published:2013-07-30 summary:In a hidden Markov model, the underlying Markov chain is usually hidden.Often, the maximum likelihood alignment (Viterbi alignment) is used as itsestimate. Although having the biggest likelihood, the Viterbi alignment canbehave very untypically by passing states that are at most unexpected. To avoidsuch situations, the Viterbi alignment can be modified by forcing it not topass these states. In this article, an iterative procedure for improving theViterbi alignment is proposed and studied. The iterative approach is comparedwith a simple bunch approach where a number of states with low probability areall replaced at the same time. It can be seen that the iterative way ofadjusting the Viterbi alignment is more efficient and it has several advantagesover the bunch approach. The same iterative algorithm for improving the Viterbialignment can be used in the case of peeping, that is when it is possible toreveal hidden states. In addition, lower bounds for classificationprobabilities of the Viterbi alignment under different conditions on the modelparameters are studied.
arxiv-4200-106 | Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction | http://arxiv.org/abs/1307.7973 | author:Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier category:cs.CL cs.IR cs.LG published:2013-07-30 summary:This paper proposes a novel approach for relation extraction from free textwhich is trained to jointly use information from the text and from existingknowledge. Our model is based on two scoring functions that operate by learninglow-dimensional embeddings of words and of entities and relationships from aknowledge base. We empirically show on New York Times articles aligned withFreebase relations that our approach is able to efficiently use the extrainformation provided by a large subset of Freebase data (4M entities, 23krelationships) to improve over existing methods that rely on text featuresalone.
arxiv-4200-107 | Likelihood-ratio calibration using prior-weighted proper scoring rules | http://arxiv.org/abs/1307.7981 | author:Niko BrÃ¼mmer, George Doddington category:stat.ML cs.LG published:2013-07-30 summary:Prior-weighted logistic regression has become a standard tool for calibrationin speaker recognition. Logistic regression is the optimization of the expectedvalue of the logarithmic scoring rule. We generalize this via a parametricfamily of proper scoring rules. Our theoretical analysis shows how differentmembers of this family induce different relative weightings over a spectrum ofapplications of which the decision thresholds range from low to high. Specialattention is given to the interaction between prior weighting and properscoring rule parameters. Experiments on NIST SRE'12 suggest that forapplications with low false-alarm rate requirements, scoring rules tailored toemphasize higher score thresholds may give better accuracy than logisticregression.
arxiv-4200-108 | Sharp Threshold for Multivariate Multi-Response Linear Regression via Block Regularized Lasso | http://arxiv.org/abs/1307.7993 | author:Weiguang Wang, Yingbin Liang, Eric P. Xing category:cs.LG stat.ML published:2013-07-30 summary:In this paper, we investigate a multivariate multi-response (MVMR) linearregression problem, which contains multiple linear regression models withdifferently distributed design matrices, and different regression and outputvectors. The goal is to recover the support union of all regression vectorsusing $l_1/l_2$-regularized Lasso. We characterize sufficient and necessaryconditions on sample complexity \emph{as a sharp threshold} to guaranteesuccessful recovery of the support union. Namely, if the sample size is abovethe threshold, then $l_1/l_2$-regularized Lasso correctly recovers the supportunion; and if the sample size is below the threshold, $l_1/l_2$-regularizedLasso fails to recover the support union. In particular, the thresholdprecisely captures the impact of the sparsity of regression vectors and thestatistical properties of the design matrices on sample complexity. Therefore,the threshold function also captures the advantages of joint support unionrecovery using multi-task Lasso over individual support recovery usingsingle-task Lasso.
arxiv-4200-109 | A Study on Classification in Imbalanced and Partially-Labelled Data Streams | http://arxiv.org/abs/1307.8012 | author:R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers category:astro-ph.IM cs.LG published:2013-07-30 summary:The domain of radio astronomy is currently facing significant computationalchallenges, foremost amongst which are those posed by the development of theworld's largest radio telescope, the Square Kilometre Array (SKA). Preliminaryspecifications for this instrument suggest that the final design willincorporate between 2000 and 3000 individual 15 metre receiving dishes, whichtogether can be expected to produce a data rate of many TB/s. Given such a highdata rate, it becomes crucial to consider how this information will beprocessed and stored to maximise its scientific utility. In this paper, weconsider one possible data processing scenario for the SKA, for the purposes ofan all-sky pulsar survey. In particular we treat the selection of promisingsignals from the SKA processing pipeline as a data stream classificationproblem. We consider the feasibility of classifying signals that arrive via anunlabelled and heavily class imbalanced data stream, using currently availablealgorithms and frameworks. Our results indicate that existing stream learnersexhibit unacceptably low recall on real astronomical data when used in standardconfiguration; however, good false positive performance and comparable accuracyto static learners, suggests they have definite potential as an on-linesolution to this particular big data challenge.
arxiv-4200-110 | Extracting Connected Concepts from Biomedical Texts using Fog Index | http://arxiv.org/abs/1307.8057 | author:Rushdi Shams, Robert E. Mercer category:cs.CL cs.IR published:2013-07-30 summary:In this paper, we establish Fog Index (FI) as a text filter to locate thesentences in texts that contain connected biomedical concepts of interest. Todo so, we have used 24 random papers each containing four pairs of connectedconcepts. For each pair, we categorize sentences based on whether they containboth, any or none of the concepts. We then use FI to measure difficulty of thesentences of each category and find that sentences containing both of theconcepts have low readability. We rank sentences of a text according to theirFI and select 30 percent of the most difficult sentences. We use an associationmatrix to track the most frequent pairs of concepts in them. This matrixreports that the first filter produces some pairs that hold almost noconnections. To remove these unwanted pairs, we use the Equally WeightedHarmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as asecond filter. Experimental results demonstrate the effectiveness of ourmethod.
arxiv-4200-111 | Integration of 3D Object Recognition and Planning for Robotic Manipulation: A Preliminary Report | http://arxiv.org/abs/1307.7466 | author:Damien Jade Duff, Esra Erdem, Volkan Patoglu category:cs.AI cs.CV cs.RO published:2013-07-29 summary:We investigate different approaches to integrating object recognition andplanning in a tabletop manipulation domain with the set of objects used in the2012 RoboCup@Work competition. Results of our preliminary experiments showthat, with some approaches, close integration of perception and planningimproves the quality of plans, as well as the computation times of feasibleplans.
arxiv-4200-112 | Participation anticipating in elections using data mining methods | http://arxiv.org/abs/1307.7429 | author:Amin Babazadeh Sangar, Seyyed Reza Khaze, Laya Ebrahimi category:cs.CY cs.LG published:2013-07-29 summary:Anticipating the political behavior of people will be considerable help forelection candidates to assess the possibility of their success and to beacknowledged about the public motivations to select them. In this paper, weprovide a general schematic of the architecture of participation anticipatingsystem in presidential election by using KNN, Classification Tree and Na\"iveBayes and tools orange based on crisp which had hopeful output. To test andassess the proposed model, we begin to use the case study by selecting 100qualified persons who attend in 11th presidential election of Islamic republicof Iran and anticipate their participation in Kohkiloye & Boyerahmad. Weindicate that KNN can perform anticipation and classification processes withhigh accuracy in compared with two other algorithms to anticipateparticipation.
arxiv-4200-113 | Data mining application for cyber space users tendency in blog writing: a case study | http://arxiv.org/abs/1307.7432 | author:Farhad Soleimanian Gharehchopogh, Seyyed Reza Khaze category:cs.CY cs.LG published:2013-07-29 summary:Blogs are the recent emerging media which relies on information technologyand technological advance. Since the mass media in some less-developed anddeveloping countries are in government service and their policies are developedbased on governmental interests, so blogs are provided for ideas and exchangingopinions. In this paper, we highlighted performed simulations from obtainedinformation from 100 users and bloggers in Kohkiloye and Boyer Ahmad Provinceand using Weka 3.6 tool and c4.5 algorithm by applying decision tree with morethan %82 precision for getting future tendency anticipation of users toblogging and using in strategically areas.
arxiv-4200-114 | Automatic Mammogram image Breast Region Extraction and Removal of Pectoral Muscle | http://arxiv.org/abs/1307.7474 | author:R. Subash Chandra Boss, K. Thangavel, D. Arul Pon Daniel category:cs.CV published:2013-07-29 summary:Currently Mammography is a most effective imaging modality used byradiologists for the screening of breast cancer. Finding an accurate, robustand efficient breast region segmentation technique still remains a challengingproblem in digital mammography. Extraction of the breast profile region and theremoval of pectoral muscle are essential pre-processing steps in Computer AidedDiagnosis (CAD) system for the diagnosis of breast cancer. Primarily it allowsthe search for abnormalities to be limited to the region of the breast tissuewithout undue influence from the background of the mammogram. The presence ofpectoral muscle in mammograms biases detection procedures, which recommendsremoving the pectoral muscle during mammogram image pre-processing. Thepresence of pectoral muscle in mammograms may disturb or influence thedetection of breast cancer as the pectoral muscle and mammographic parenchymasappear similar. The goal of breast region extraction is reducing the image sizewithout losing anatomic information, it improve the accuracy of the overall CADsystem. The main objective of this study is to propose an automated method toidentify the pectoral muscle in Medio-Lateral Oblique (MLO) view mammograms. Inthis paper, we proposed histogram based 8-neighborhood connected componentlabelling method for breast region extraction and removal of pectoral muscle.The proposed method is evaluated by using the mean values of accuracy anderror. The comparative analysis shows that the proposed method identifies thebreast region more accurately.
arxiv-4200-115 | Tight Lower Bounds for Homology Inference | http://arxiv.org/abs/1307.7666 | author:Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.CG math.ST stat.TH published:2013-07-29 summary:The homology groups of a manifold are important topological invariants thatprovide an algebraic summary of the manifold. These groups contain richtopological information, for instance, about the connected components, holes,tunnels and sometimes the dimension of the manifold. In earlier work, we haveconsidered the statistical problem of estimating the homology of a manifoldfrom noiseless samples and from noisy samples under several different noisemodels. We derived upper and lower bounds on the minimax risk for this problem.In this note we revisit the noiseless case. In previous work we used Le Cam'slemma to establish a lower bound that differed from the upper bound of Niyogi,Smale and Weinberger by a polynomial factor in the condition number. In this note we use a different construction based on the direct analysis ofthe likelihood ratio test to show that the upper bound of Niyogi, Smale andWeinberger is in fact tight, thus establishing rate optimal asymptotic minimaxbounds for the problem. The techniques we use here extend in a straightforwardway to the noisy settings considered in our earlier work.
arxiv-4200-116 | Safe Screening With Variational Inequalities and Its Application to LASSO | http://arxiv.org/abs/1307.7577 | author:Jun Liu, Zheng Zhao, Jie Wang, Jieping Ye category:cs.LG stat.ML published:2013-07-29 summary:Sparse learning techniques have been routinely used for feature selection asthe resulting model usually has a small number of non-zero entries. Safescreening, which eliminates the features that are guaranteed to have zerocoefficients for a certain value of the regularization parameter, is atechnique for improving the computational efficiency. Safe screening is gainingincreasing attention since 1) solving sparse learning formulations usually hasa high computational cost especially when the number of features is large and2) one needs to try several regularization parameters to select a suitablemodel. In this paper, we propose an approach called "Sasvi" (Safe screeningwith variational inequalities). Sasvi makes use of the variational inequalitythat provides the sufficient and necessary optimality condition for the dualproblem. Several existing approaches for Lasso screening can be casted asrelaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safescreening rule. We further study the monotone properties of Sasvi for Lasso,based on which a sure removal regularization parameter can be identified foreach feature. Experimental results on both synthetic and real data sets arereported to demonstrate the effectiveness of the proposed Sasvi for Lassoscreening.
arxiv-4200-117 | Union of Low-Rank Subspaces Detector | http://arxiv.org/abs/1307.7521 | author:Mohsen Joneidi, Parvin Ahmadi, Mostafa Sadeghi, Nazanin Rahnavard category:cs.IT cs.CV math.IT published:2013-07-29 summary:The problem of signal detection using a flexible and general model isconsidered. Due to applicability and flexibility of sparse signalrepresentation and approximation, it has attracted a lot of attention in manysignal processing areas. In this paper, we propose a new detection method basedon sparse decomposition in a union of subspaces (UoS) model. Our proposeddetector uses a dictionary that can be interpreted as a bank of matchedsubspaces. This improves the performance of signal detection, as it is ageneralization for detectors. Low-rank assumption for the desired signalsimplies that the representations of these signals in terms of some proper baseswould be sparse. Our proposed detector exploits sparsity in its decision rule.We demonstrate the high efficiency of our method in the cases of voice activitydetection in speech processing.
arxiv-4200-118 | A new approach in dynamic traveling salesman problem: a hybrid of ant colony optimization and descending gradient | http://arxiv.org/abs/1307.7435 | author:Farhad Soleimanian Gharehchopogh, Isa Maleki, Seyyed Reza Khaze category:cs.NE published:2013-07-29 summary:Nowadays swarm intelligence-based algorithms are being used widely tooptimize the dynamic traveling salesman problem (DTSP). In this paper, we haveused mixed method of Ant Colony Optimization (AOC)and gradient descent tooptimize DTSP which differs with ACO algorithm in evaporation rate andinnovative data. This approach prevents premature convergence and scape fromlocal optimum spots and also makes it possible to find better solutions foralgorithm. In this paper, we are going to offer gradient descent and ACOalgorithm which in comparison to some former methods it shows that algorithmhas significantly improved routes optimization.
arxiv-4200-119 | Borel Isomorphic Dimensionality Reduction of Data and Supervised Learning | http://arxiv.org/abs/1307.8333 | author:Stan Hatko category:stat.ML published:2013-07-29 summary:In this project we further investigate the idea of reducing thedimensionality of datasets using a Borel isomorphism with the purpose ofsubsequently applying supervised learning algorithms, as originally suggestedby my supervisor V. Pestov (in 2011 Dagstuhl preprint). Any consistent learningalgorithm, for example kNN, retains universal consistency after a Borelisomorphism is applied. A series of concrete examples of Borel isomorphismsthat reduce the number of dimensions in a dataset is provided, based onmultiplying the data by orthogonal matrices before the dimensionality reducingBorel isomorphism is applied. We test the accuracy of the resulting classifierin a lower dimensional space with various data sets. Working with a phonemevoice recognition dataset, of dimension 256 with 5 classes (phonemes), we showthat a Borel isomorphic reduction to dimension 16 leads to a minimal drop inaccuracy. In conclusion, we discuss further prospects of the method.
arxiv-4200-120 | Learning Frames from Text with an Unsupervised Latent Variable Model | http://arxiv.org/abs/1307.7382 | author:Brendan O'Connor category:cs.CL published:2013-07-28 summary:We develop a probabilistic latent-variable model to discover semanticframes---types of events and their participants---from corpora. We present aDirichlet-multinomial model in which frames are latent categories that explainthe linking of verb-subject-object triples, given document-level sparsity. Weanalyze what the model learns, and compare it to FrameNet, noting it learnssome novel and interesting frames. This document also contains a discussion ofinference issues, including concentration parameter learning; and a small-scaleerror analysis of syntactic parsing accuracy.
arxiv-4200-121 | Kronecker Sum Decompositions of Space-Time Data | http://arxiv.org/abs/1307.7306 | author:Kristjan Greenewald, Theodoros Tsiligkaridis, Alfred O Hero III category:stat.ME stat.ML published:2013-07-27 summary:In this paper we consider the use of the space vs. time Kronecker productdecomposition in the estimation of covariance matrices for spatio-temporaldata. This decomposition imposes lower dimensional structure on the estimatedcovariance matrix, thus reducing the number of samples required for estimation.To allow a smooth tradeoff between the reduction in the number of parameters(to reduce estimation variance) and the accuracy of the covarianceapproximation (affecting estimation bias), we introduce a diagonally loadedmodification of the sum of kronecker products representation [1]. We derive aCramer-Rao bound (CRB) on the minimum attainable mean squared predictorcoefficient estimation error for unbiased estimators of Kronecker structuredcovariance matrices. We illustrate the accuracy of the diagonally loadedKronecker sum decomposition by applying it to video data of human activity.
arxiv-4200-122 | Self-Learning for Player Localization in Sports Video | http://arxiv.org/abs/1307.7198 | author:Kenji Okuma, David G. Lowe, James J. Little category:cs.CV cs.AI published:2013-07-27 summary:This paper introduces a novel self-learning framework that automates thelabel acquisition process for improving models for detecting players inbroadcast footage of sports games. Unlike most previous self-learningapproaches for improving appearance-based object detectors from videos, weallow an unknown, unconstrained number of target objects in a more generalizedvideo sequence with non-static camera views. Our self-learning approach uses alatent SVM learning algorithm and deformable part models to represent the shapeand colour information of players, constraining their motions, and learns thecolour of the playing field by a gentle Adaboost algorithm. We combine thoseimage cues and discover additional labels automatically from unlabelled data.In our experiments, our approach exploits both labelled and unlabelled data insparsely labelled videos of sports games, providing a mean performanceimprovement of over 20% in the average precision for detecting sports playersand improved tracking, when videos contain very few labelled images.
arxiv-4200-123 | A Review of Machine Learning based Anomaly Detection Techniques | http://arxiv.org/abs/1307.7286 | author:Harjinder Kaur, Gurpreet Singh, Jaspreet Minhas category:cs.LG cs.CR published:2013-07-27 summary:Intrusion detection is so much popular since the last two decades whereintrusion is attempted to break into or misuse the system. It is mainly of twotypes based on the intrusions, first is Misuse or signature based detection andthe other is Anomaly detection. In this paper Machine learning based methodswhich are one of the types of Anomaly detection techniques is discussed.
arxiv-4200-124 | Learning to Understand by Evolving Theories | http://arxiv.org/abs/1307.7303 | author:Martin E. Mueller, Madhura D. Thosar category:cs.LG cs.AI published:2013-07-27 summary:In this paper, we describe an approach that enables an autonomous system toinfer the semantics of a command (i.e. a symbol sequence representing anaction) in terms of the relations between changes in the observations and theaction instances. We present a method of how to induce a theory (i.e. asemantic description) of the meaning of a command in terms of a minimal set ofbackground knowledge. The only thing we have is a sequence of observations fromwhich we extract what kinds of effects were caused by performing the command.This way, we yield a description of the semantics of the action and, hence, adefinition.
arxiv-4200-125 | Multi-view Laplacian Support Vector Machines | http://arxiv.org/abs/1307.7024 | author:Shiliang Sun category:cs.LG stat.ML published:2013-07-26 summary:We propose a new approach, multi-view Laplacian support vector machines(SVMs), for semi-supervised learning under the multi-view scenario. Itintegrates manifold regularization and multi-view regularization into the usualformulation of SVMs and is a natural extension of SVMs from supervised learningto multi-view semi-supervised learning. The function optimization problem in areproducing kernel Hilbert space is converted to an optimization in afinite-dimensional Euclidean space. After providing a theoretical bound for thegeneralization performance of the proposed method, we further give aformulation of the empirical Rademacher complexity which affects the boundsignificantly. From this bound and the empirical Rademacher complexity, we cangain insights into the roles played by different regularization terms to thegeneralization performance. Experimental results on synthetic and real-worlddata sets are presented, which validate the effectiveness of the proposedmulti-view Laplacian SVMs approach.
arxiv-4200-126 | Infinite Mixtures of Multivariate Gaussian Processes | http://arxiv.org/abs/1307.7028 | author:Shiliang Sun category:cs.LG stat.ML published:2013-07-26 summary:This paper presents a new model called infinite mixtures of multivariateGaussian processes, which can be used to learn vector-valued functions andapplied to multitask learning. As an extension of the single multivariateGaussian process, the mixture model has the advantages of modeling multimodaldata and alleviating the computationally cubic complexity of the multivariateGaussian process. A Dirichlet process prior is adopted to allow the (possiblyinfinite) number of mixture components to be automatically inferred fromtraining data, and Markov chain Monte Carlo sampling techniques are used forparameter and latent variable inference. Preliminary experimental results onmultivariate regression show the feasibility of the proposed model.
arxiv-4200-127 | A Comprehensive Evaluation of Machine Learning Techniques for Cancer Class Prediction Based on Microarray Data | http://arxiv.org/abs/1307.7050 | author:Khalid Raza, Atif N Hasan category:cs.LG cs.CE published:2013-07-26 summary:Prostate cancer is among the most common cancer in males and itsheterogeneity is well known. Its early detection helps making therapeuticdecision. There is no standard technique or procedure yet which is full-proofin predicting cancer class. The genomic level changes can be detected in geneexpression data and those changes may serve as standard model for any randomcancer data for class prediction. Various techniques were implied on prostatecancer data set in order to accurately predict cancer class including machinelearning techniques. Huge number of attributes and few number of sample inmicroarray data leads to poor machine learning, therefore the most challengingpart is attribute reduction or non significant gene reduction. In this work wehave compared several machine learning techniques for their accuracy inpredicting the cancer class. Machine learning is effective when number ofattributes (genes) are larger than the number of samples which is rarelypossible with gene expression data. Attribute reduction or gene filtering isabsolutely required in order to make the data more meaningful as most of thegenes do not participate in tumor development and are irrelevant for cancerprediction. Here we have applied combination of statistical techniques such asinter-quartile range and t-test, which has been effective in filteringsignificant genes and minimizing noise from data. Further we have done acomprehensive evaluation of ten state-of-the-art machine learning techniquesfor their accuracy in class prediction of prostate cancer. Out of thesetechniques, Bayes Network out performed with an accuracy of 94.11% followed byNavie Bayes with an accuracy of 91.17%. To cross validate our results, wemodified our training dataset in six different way and found that averagesensitivity, specificity, precision and accuracy of Bayes Network is highestamong all other techniques used.
arxiv-4200-128 | Memcapacitive neural networks | http://arxiv.org/abs/1307.6921 | author:Y. V. Pershin, M. Di Ventra category:cs.ET cs.NE q-bio.NC published:2013-07-26 summary:We show that memcapacitive (memory capacitive) systems can be used assynapses in artificial neural networks. As an example of our approach, wediscuss the architecture of an integrate-and-fire neural network based onmemcapacitive synapses. Moreover, we demonstrate that thespike-timing-dependent plasticity can be simply realized with some of thesedevices. Memcapacitive synapses are a low-energy alternative to memristivesynapses for neuromorphic computation.
arxiv-4200-129 | Reduced egomotion estimation drift using omnidirectional views | http://arxiv.org/abs/1307.6962 | author:Yalin Bastanlar category:cs.CV cs.RO published:2013-07-26 summary:Estimation of camera motion from a given image sequence becomes degraded asthe length of the sequence increases. In this letter, this phenomenon isdemonstrated and an approach to increase the estimation accuracy is proposed.The proposed method uses an omnidirectional camera in addition to theperspective one and takes advantage of its enlarged view by exploiting thecorrespondences between the omnidirectional and perspective images. Simulatedand real image experiments show that the proposed approach improves theestimation accuracy.
arxiv-4200-130 | Finite State Machine Synthesis for Evolutionary Hardware | http://arxiv.org/abs/1307.6995 | author:Andrey Bereza, Maksim Lyashov, Luis Blanco category:cs.NE cs.FL published:2013-07-26 summary:This article considers application of genetic algorithms for finite machinesynthesis. The resulting genetic finite state machines synthesis algorithmallows for creation of machines with less number of states and within shortertime. This makes it possible to use hardware-oriented genetic finite machinessynthesis algorithm in autonomous systems on reconfigurable platforms.
arxiv-4200-131 | MixedGrad: An O(1/T) Convergence Rate Algorithm for Stochastic Smooth Optimization | http://arxiv.org/abs/1307.7192 | author:Mehrdad Mahdavi, Rong Jin category:cs.LG math.OC published:2013-07-26 summary:It is well known that the optimal convergence rate for stochasticoptimization of smooth functions is $O(1/\sqrt{T})$, which is same asstochastic optimization of Lipschitz continuous convex functions. This is incontrast to optimizing smooth functions using full gradients, which yields aconvergence rate of $O(1/T^2)$. In this work, we consider a new setup foroptimizing smooth functions, termed as {\bf Mixed Optimization}, which allowsto access both a stochastic oracle and a full gradient oracle. Our goal is tosignificantly improve the convergence rate of stochastic optimization of smoothfunctions by having an additional small number of accesses to the full gradientoracle. We show that, with an $O(\ln T)$ calls to the full gradient oracle andan $O(T)$ calls to the stochastic oracle, the proposed mixed optimizationalgorithm is able to achieve an optimization error of $O(1/T)$.
arxiv-4200-132 | A Novel Architecture For Question Classification Based Indexing Scheme For Efficient Question Answering | http://arxiv.org/abs/1307.6937 | author:Renu Mudgal, Rosy Madaan, A. K. Sharma, Ashutosh Dixit category:cs.IR cs.CL published:2013-07-26 summary:Question answering system can be seen as the next step in informationretrieval, allowing users to pose question in natural language and receivecompact answers. For the Question answering system to be successful, researchhas shown that the correct classification of question with respect to theexpected answer type is requisite. We propose a novel architecture for questionclassification and searching in the index, maintained on the basis of expectedanswer types, for efficient question answering. The system uses the criteriafor Answer Relevance Score for finding the relevance of each answer returned bythe system. On analysis of the proposed system, it has been found that thesystem has shown promising results than the existing systems based on questionclassification.
arxiv-4200-133 | Sequential Transfer in Multi-armed Bandit with Finite Set of Models | http://arxiv.org/abs/1307.6887 | author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG published:2013-07-25 summary:Learning from prior tasks and transferring that experience to improve futureperformance is critical for building lifelong learning agents. Although resultsin supervised and reinforcement learning show that transfer may significantlyimprove the learning performance, most of the literature on transfer is focusedon batch learning tasks. In this paper we study the problem of\textit{sequential transfer in online learning}, notably in the multi-armedbandit framework, where the objective is to minimize the cumulative regret overa sequence of tasks by incrementally transferring knowledge from prior tasks.We introduce a novel bandit algorithm based on a method-of-moments approach forthe estimation of the possible tasks and derive regret bounds for it.
arxiv-4200-134 | Streaming Variational Bayes | http://arxiv.org/abs/1307.6769 | author:Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, Michael I. Jordan category:stat.ML cs.LG published:2013-07-25 summary:We present SDA-Bayes, a framework for (S)treaming, (D)istributed,(A)synchronous computation of a Bayesian posterior. The framework makesstreaming updates to the estimated posterior according to a user-specifiedapproximation batch primitive. We demonstrate the usefulness of our framework,with variational Bayes (VB) as the primitive, by fitting the latent Dirichletallocation model to two large-scale document collections. We demonstrate theadvantages of our algorithm over stochastic variational inference (SVI) bycomparing the two after a single pass through a known amount of data---a casewhere SVI may be applied---and in the streaming setting, where SVI does notapply.
arxiv-4200-135 | A Propound Method for the Improvement of Cluster Quality | http://arxiv.org/abs/1307.6814 | author:Shveta Kundra Bhatia, V. S. Dixit category:cs.LG published:2013-07-25 summary:In this paper Knockout Refinement Algorithm (KRA) is proposed to refineoriginal clusters obtained by applying SOM and K-Means clustering algorithms.KRA Algorithm is based on Contingency Table concepts. Metrics are computed forthe Original and Refined Clusters. Quality of Original and Refined Clusters arecompared in terms of metrics. The proposed algorithm (KRA) is tested in theeducational domain and results show that it generates better quality clustersin terms of improved metric values.
arxiv-4200-136 | Information content versus word length in natural language: A reply to Ferrer-i-Cancho and Moscoso del Prado Martin [arXiv:1209.1751] | http://arxiv.org/abs/1307.6726 | author:Steven T. Piantadosi, Harry Tily, Edward Gibson category:cs.CL math.PR published:2013-07-25 summary:Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751]argued that an observed linear relationship between word length and averagesurprisal (Piantadosi, Tily, & Gibson, 2011) is not evidence for communicativeefficiency in human language. We discuss several shortcomings of their approachand critique: their model critically rests on inaccurate assumptions, isincapable of explaining key surprisal patterns in language, and is incompatiblewith recent behavioral results. More generally, we argue that statisticalmodels must not critically rely on assumptions that are incompatible with thereal system under study.
arxiv-4200-137 | Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example | http://arxiv.org/abs/1307.6616 | author:Shaobo Lin, Chen Xu, Jingshan Zeng, Jian Fang category:cs.LG stat.ML published:2013-07-25 summary:$l^q$-regularization has been demonstrated to be an attractive technique inmachine learning and statistical modeling. It attempts to improve thegeneralization (prediction) capability of a machine (model) throughappropriately shrinking its coefficients. The shape of a $l^q$ estimatordiffers in varying choices of the regularization order $q$. In particular,$l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smoothridge regression. This makes the order $q$ a potential tuning parameter inapplications. To facilitate the use of $l^{q}$-regularization, we intend toseek for a modeling strategy where an elaborative selection on $q$ isavoidable. In this spirit, we place our investigation within a generalframework of $l^{q}$-regularized kernel learning under a sample dependenthypothesis space (SDHS). For a designated class of kernel functions, we showthat all $l^{q}$ estimators for $0< q < \infty$ attain similar generalizationerror bounds. These estimated bounds are almost optimal in the sense that up toa logarithmic factor, the upper and lower bounds are asymptotically identical.This finding tentatively reveals that, in some modeling contexts, the choice of$q$ might not have a strong impact in terms of the generalization capability.From this perspective, $q$ can be arbitrarily specified, or specified merely byother no generalization criteria like smoothness, computational complexity,sparsity, etc..
arxiv-4200-138 | Matching-Constrained Active Contours | http://arxiv.org/abs/1307.6303 | author:Junyan Wang, Kap Luk Chan category:cs.CV published:2013-07-24 summary:In object segmentation by active contours, the initial contour is oftenrequired. Conventionally, the initial contour is provided by the user. Thispaper extends the conventional active contour model by incorporating featurematching in the formulation, which gives rise to a novel matching-constrainedactive contour. The numerical solution to the new optimization model providesan automated framework of object segmentation without user intervention. Themain idea is to incorporate feature point matching as a constraint in activecontour models. To this effect, we obtain a mathematical model of interiorpoints to boundary contour such that matching of interior feature points givescontour alignment, and we formulate the matching score as a constraint toactive contour model such that the feature matching of maximum score that givesthe contour alignment provides the initial feasible solution to the constrainedoptimization model of segmentation. The constraint also ensures that theoptimal contour does not deviate too much from the initial contour.Projected-gradient descent equations are derived to solve the constrainedoptimization. In the experiments, we show that our method is capable ofachieving the automatic object segmentation, and it outperforms the relatedmethods.
arxiv-4200-139 | Storing non-uniformly distributed messages in networks of neural cliques | http://arxiv.org/abs/1307.6410 | author:Bartosz Boguslawski, Vincent Gripon, Fabrice Seguin, FrÃ©dÃ©ric Heitzmann category:cs.NE cs.SY published:2013-07-24 summary:Associative memories are data structures that allow retrieval of storedmessages from part of their content. They thus behave similarly to human brainthat is capable for instance of retrieving the end of a song given itsbeginning. Among different families of associative memories, sparse ones areknown to provide the best efficiency (ratio of the number of bits stored tothat of bits used). Nevertheless, it is well known that non-uniformity of thestored messages can lead to dramatic decrease in performance. We introduceseveral strategies to allow efficient storage of non-uniform messages inrecently introduced sparse associative memories. We analyse and discuss themethods introduced. We also present a practical application example.
arxiv-4200-140 | When is the majority-vote classifier beneficial? | http://arxiv.org/abs/1307.6522 | author:Mu Zhu category:math.ST stat.ML stat.TH published:2013-07-24 summary:In his seminal work, Schapire (1990) proved that weak classifiers could beimproved to achieve arbitrarily high accuracy, but he never implied that asimple majority-vote mechanism could always do the trick. By comparing theasymptotic misclassification error of the majority-vote classifier with theaverage individual error, we discover an interesting phase-transitionphenomenon. For binary classification with equal prior probabilities, ourresult implies that, for the majority-vote mechanism to work, the collection ofweak classifiers must meet the minimum requirement of having an average truepositive rate of at least 50% and an average false positive rate of at most50%.
arxiv-4200-141 | Boosting the concordance index for survival data - a unified framework to derive and evaluate biomarker combinations | http://arxiv.org/abs/1307.6417 | author:Andreas Mayr, Matthias Schmid category:stat.AP stat.ME stat.ML published:2013-07-24 summary:The development of molecular signatures for the prediction of time-to-eventoutcomes is a methodologically challenging task in bioinformatics andbiostatistics. Although there are numerous approaches for the derivation ofmarker combinations and their evaluation, the underlying methodology oftensuffers from the problem that different optimization criteria are mixed duringthe feature selection, estimation and evaluation steps. This might result inmarker combinations that are only suboptimal regarding the evaluation criterionof interest. To address this issue, we propose a unified framework to deriveand evaluate biomarker combinations. Our approach is based on the concordanceindex for time-to-event data, which is a non-parametric measure to quantify thediscrimatory power of a prediction rule. Specifically, we propose acomponent-wise boosting algorithm that results in linear biomarker combinationsthat are optimal with respect to a smoothed version of the concordance index.We investigate the performance of our algorithm in a large-scale simulationstudy and in two molecular data sets for the prediction of survival in breastcancer patients. Our numerical results show that the new approach is not onlymethodologically sound but can also lead to a higher discriminatory power thantraditional approaches for the derivation of gene signatures.
arxiv-4200-142 | Cluster Trees on Manifolds | http://arxiv.org/abs/1307.6515 | author:Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.LG published:2013-07-24 summary:In this paper we investigate the problem of estimating the cluster tree for adensity $f$ supported on or near a smooth $d$-dimensional manifold $M$isometrically embedded in $\mathbb{R}^D$. We analyze a modified version of a$k$-nearest neighbor based algorithm recently proposed by Chaudhuri andDasgupta. The main results of this paper show that under mild assumptions on$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not onthe ambient dimension $D$. We also show that similar (albeit non-algorithmic)results can be obtained for kernel density estimators. We sketch a constructionof a sample complexity lower bound instance for a natural class of manifoldoblivious clustering algorithms. We further briefly consider the known manifoldcase and show that in this case a spatially adaptive algorithm achieves betterrates.
arxiv-4200-143 | Time-Series Classification Through Histograms of Symbolic Polynomials | http://arxiv.org/abs/1307.6365 | author:Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme category:cs.AI cs.DB cs.LG published:2013-07-24 summary:Time-series classification has attracted considerable research attention dueto the various domains where time-series data are observed, ranging frommedicine to econometrics. Traditionally, the focus of time-seriesclassification has been on short time-series data composed of a unique patternwith intraclass pattern distortions and variations, while recently there havebeen attempts to focus on longer series composed of various local patterns.This study presents a novel method which can detect local patterns in longtime-series via fitting local polynomial functions of arbitrary degrees. Thecoefficients of the polynomial functions are converted to symbolic words viaequivolume discretizations of the coefficients' distributions. The symbolicpolynomial words enable the detection of similar local patterns by assigningthe same words to similar polynomials. Moreover, a histogram of the frequenciesof the words is constructed from each time-series' bag of words. Each row ofthe histogram enables a new representation for the series and symbolize theexistence of local patterns and their frequencies. Experimental evidencedemonstrates outstanding results of our method compared to the state-of-artbaselines, by exhibiting the best classification accuracies in all the datasetsand having statistically significant improvements in the absolute majority ofexperiments.
arxiv-4200-144 | Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier | http://arxiv.org/abs/1307.6143 | author:Niko Brummer category:stat.ML cs.LG published:2013-07-23 summary:This report works out the details of a closed-form, fully Bayesian,multiclass, openset, generative pattern classifier using multivariate Gaussianlikelihoods, with conjugate priors. The generative model has a commonwithin-class covariance, which is proportional to the between-class covariancein the conjugate prior. The scalar proportionality constant is the only pluginparameter. All other model parameters are intergated out in closed form. Anexpression is given for the model evidence, which can be used to make pluginestimates for the proportionality constant. Pattern recognition is done via thepredictive likeihoods of classes for which training data is available, as wellas a predicitve likelihood for any as yet unseen class.
arxiv-4200-145 | Human and Automatic Evaluation of English-Hindi Machine Translation | http://arxiv.org/abs/1307.6163 | author:Nisheeth Joshi, Hemant Darbari, Iti Mathur category:cs.CL published:2013-07-23 summary:For the past 60 years, Research in machine translation is going on. For thedevelopment in this field, a lot of new techniques are being developed eachday. As a result, we have witnessed development of many automatic machinetranslators. A manager of machine translation development project needs to knowthe performance increase/decrease, after changes have been done in his system.Due to this reason, a need for evaluation of machine translation systems wasfelt. In this article, we shall present the evaluation of some machinetranslators. This evaluation will be done by a human evaluator and by someautomatic evaluation metrics, which will be done at sentence, document andsystem level. In the end we shall also discuss the comparison between theevaluations.
arxiv-4200-146 | A Near-Optimal Dynamic Learning Algorithm for Online Matching Problems with Concave Returns | http://arxiv.org/abs/1307.5934 | author:Xiao Alison Chen, Zizhuo Wang category:cs.DS cs.LG math.OC published:2013-07-23 summary:We consider an online matching problem with concave returns. This problem isa significant generalization of the Adwords allocation problem and has vastapplications in online advertising. In this problem, a sequence of items arrivesequentially and each has to be allocated to one of the bidders, who bid acertain value for each item. At each time, the decision maker has to allocatethe current item to one of the bidders without knowing the future bids and theobjective is to maximize the sum of some concave functions of each bidder'saggregate value. In this work, we propose an algorithm that achievesnear-optimal performance for this problem when the bids arrive in a randomorder and the input data satisfies certain conditions. The key idea of ouralgorithm is to learn the input data pattern dynamically: we solve a sequenceof carefully chosen partial allocation problems and use their optimal solutionsto assist with the future decision. Our analysis belongs to the primal-dualparadigm, however, the absence of linearity of the objective function and thedynamic feature of the algorithm makes our analysis quite unique.
arxiv-4200-147 | Numerical Methods for Coupled Reconstruction and Registration in Digital Breast Tomosynthesis | http://arxiv.org/abs/1307.6008 | author:Guang Yang, John H. Hipwell, David J. Hawkes, Simon R. Arridge category:cs.CV physics.med-ph published:2013-07-23 summary:Digital Breast Tomosynthesis (DBT) provides an insight into the fine detailsof normal fibroglandular tissues and abnormal lesions by reconstructing apseudo-3D image of the breast. In this respect, DBT overcomes a majorlimitation of conventional X-ray mammography by reducing the confoundingeffects caused by the superposition of breast tissue. In a breast cancerscreening or diagnostic context, a radiologist is interested in detectingchange, which might be indicative of malignant disease. To help automate thistask image registration is required to establish spatial correspondence betweentime points. Typically, images, such as MRI or CT, are first reconstructed andthen registered. This approach can be effective if reconstructing using acomplete set of data. However, for ill-posed, limited-angle problems such asDBT, estimating the deformation is complicated by the significant artefactsassociated with the reconstruction, leading to severe inaccuracies in theregistration. This paper presents a mathematical framework, which couples thetwo tasks and jointly estimates both image intensities and the parameters of atransformation. We evaluate our methods using various computational digital phantoms,uncompressed breast MR images, and in-vivo DBT simulations. Firstly, we compareboth iterative and simultaneous methods to the conventional, sequential methodusing an affine transformation model. We show that jointly estimating imageintensities and parametric transformations gives superior results with respectto reconstruction fidelity and registration accuracy. Also, we incorporate anon-rigid B-spline transformation model into our simultaneous method. Theresults demonstrate a visually plausible recovery of the deformation withpreservation of the reconstruction fidelity.
arxiv-4200-148 | Modeling Human Decision-making in Generalized Gaussian Multi-armed Bandits | http://arxiv.org/abs/1307.6134 | author:Paul Reverdy, Vaibhav Srivastava, Naomi E. Leonard category:cs.LG math.OC stat.ML published:2013-07-23 summary:We present a formal model of human decision-making in explore-exploit tasksusing the context of multi-armed bandit problems, where the decision-maker mustchoose among multiple options with uncertain rewards. We address the standardmulti-armed bandit problem, the multi-armed bandit problem with transitioncosts, and the multi-armed bandit problem on graphs. We focus on the case ofGaussian rewards in a setting where the decision-maker uses Bayesian inferenceto estimate the reward values. We model the decision-maker's prior knowledgewith the Bayesian prior on the mean reward. We develop the upper credible limit(UCL) algorithm for the standard multi-armed bandit problem and show that thisdeterministic algorithm achieves logarithmic cumulative expected regret, whichis optimal performance for uninformative priors. We show how good priors andgood assumptions on the correlation structure among arms can greatly enhancedecision-making performance, even over short time horizons. We extend to thestochastic UCL algorithm and draw several connections to human decision-makingbehavior. We present empirical data from human experiments and show that humanperformance is efficiently captured by the stochastic UCL algorithm withappropriate parameters. For the multi-armed bandit problem with transitioncosts and the multi-armed bandit problem on graphs, we generalize the UCLalgorithm to the block UCL algorithm and the graphical block UCL algorithm,respectively. We show that these algorithms also achieve logarithmic cumulativeexpected regret and require a sub-logarithmic expected number of transitionsamong arms. We further illustrate the performance of these algorithms withnumerical examples.
arxiv-4200-149 | Bayesian Fusion of Multi-Band Images | http://arxiv.org/abs/1307.5996 | author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV stat.ME published:2013-07-23 summary:In this paper, a Bayesian fusion technique for remotely sensed multi-bandimages is presented. The observed images are related to the high spectral andhigh spatial resolution image to be recovered through physical degradations,e.g., spatial and spectral blurring and/or subsampling defined by the sensorcharacteristics. The fusion problem is formulated within a Bayesian estimationframework. An appropriate prior distribution exploiting geometricalconsideration is introduced. To compute the Bayesian estimator of the scene ofinterest from its posterior distribution, a Markov chain Monte Carlo algorithmis designed to generate samples asymptotically distributed according to thetarget distribution. To efficiently sample from this high-dimensiondistribution, a Hamiltonian Monte Carlo step is introduced in the Gibbssampling strategy. The efficiency of the proposed fusion method is evaluatedwith respect to several state-of-the-art fusion techniques. In particular, lowspatial resolution hyperspectral and multispectral images are fused to producea high spatial resolution hyperspectral image.
arxiv-4200-150 | Online Optimization in Dynamic Environments | http://arxiv.org/abs/1307.5944 | author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.LG math.OC published:2013-07-23 summary:High-velocity streams of high-dimensional data pose significant "big data"analysis challenges across a range of applications and settings. Onlinelearning and online convex programming play a significant role in the rapidrecovery of important or anomalous information from these large datastreams.While recent advances in online learning have led to novel and rapidlyconverging algorithms, these methods are unable to adapt to nonstationaryenvironments arising in real-world problems. This paper describes a dynamicmirror descent framework which addresses this challenge, yielding lowtheoretical regret bounds and accurate, adaptive, and computationally efficientalgorithms which are applicable to broad classes of problems. The methods arecapable of learning and adapting to an underlying and possibly time-varyingdynamical model. Empirical results in the context of dynamic texture analysis,solar flare detection, sequential compressed sensing of a dynamic scene,traffic surveillance,tracking self-exciting point processes and networkbehavior in the Enron email corpus support the core theoretical findings.
arxiv-4200-151 | 6th International Symposium on Attention in Cognitive Systems 2013 | http://arxiv.org/abs/1307.6170 | author:Lucas Paletta, Laurent Itti, BjÃ¶rn Schuller, Fang Fang category:cs.CV published:2013-07-22 summary:This volume contains the papers accepted at the 6th International Symposiumon Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5,2013. The aim of this symposium is to highlight the central role of attentionon various kinds of performance in cognitive systems processing. It bringstogether researchers and developers from both academia and industry, fromcomputer vision, robotics, perception psychology, psychophysics andneuroscience, in order to provide an interdisciplinary forum to present andcommunicate on computational models of attention, with the focus oninterdependencies with visual cognition. Furthermore, it intends to investigaterelevant objectives for performance comparison, to document and to investigatepromising application domains, and to discuss visual attention with referenceto other aspects of AI enabled systems.
arxiv-4200-152 | Dimension Reduction via Colour Refinement | http://arxiv.org/abs/1307.5697 | author:Martin Grohe, Kristian Kersting, Martin Mladenov, Erkal Selman category:cs.DS cs.DM cs.LG math.OC published:2013-07-22 summary:Colour refinement is a basic algorithmic routine for graph isomorphismtesting, appearing as a subroutine in almost all practical isomorphism solvers.It partitions the vertices of a graph into "colour classes" in such a way thatall vertices in the same colour class have the same number of neighbours inevery colour class. Tinhofer (Disc. App. Math., 1991), Ramana, Scheinerman, andUllman (Disc. Math., 1994) and Godsil (Lin. Alg. and its App., 1997)established a tight correspondence between colour refinement and fractionalisomorphisms of graphs, which are solutions to the LP relaxation of a naturalILP formulation of graph isomorphism. We introduce a version of colour refinement for matrices and extend existingquasilinear algorithms for computing the colour classes. Then we generalise thecorrespondence between colour refinement and fractional automorphisms anddevelop a theory of fractional automorphisms and isomorphisms of matrices. We apply our results to reduce the dimensions of systems of linear equationsand linear programs. Specifically, we show that any given LP L can efficientlybe transformed into a (potentially) smaller LP L' whose number of variables andconstraints is the number of colour classes of the colour refinement algorithm,applied to a matrix associated with the LP. The transformation is such that wecan easily (by a linear mapping) map both feasible and optimal solutions backand forth between the two LPs. We demonstrate empirically that colourrefinement can indeed greatly reduce the cost of solving linear programs.
arxiv-4200-153 | A New Strategy of Cost-Free Learning in the Class Imbalance Problem | http://arxiv.org/abs/1307.5730 | author:Xiaowan Zhang, Bao-Gang Hu category:cs.LG published:2013-07-22 summary:In this work, we define cost-free learning (CFL) formally in comparison withcost-sensitive learning (CSL). The main difference between them is that a CFLapproach seeks optimal classification results without requiring any costinformation, even in the class imbalance problem. In fact, several CFLapproaches exist in the related studies, such as sampling and somecriteria-based pproaches. However, to our best knowledge, none of the existingCFL and CSL approaches are able to process the abstaining classificationsproperly when no information is given about errors and rejects. Based oninformation theory, we propose a novel CFL which seeks to maximize normalizedmutual information of the targets and the decision outputs of classifiers.Using the strategy, we can deal with binary/multi-class classificationswith/without abstaining. Significant features are observed from the newstrategy. While the degree of class imbalance is changing, the proposedstrategy is able to balance the errors and rejects accordingly andautomatically. Another advantage of the strategy is its ability of derivingoptimal rejection thresholds for abstaining classifications and the"equivalent" costs in binary classifications. The connection between rejectionthresholds and ROC curve is explored. Empirical investigation is made onseveral benchmark data sets in comparison with other existing approaches. Theclassification results demonstrate a promising perspective of the strategy inmachine learning.
arxiv-4200-154 | Understanding Humans' Strategies in Maze Solving | http://arxiv.org/abs/1307.5713 | author:Min Zhao, Andre G. Marquez category:cs.CV cs.AI q-bio.NC published:2013-07-22 summary:Navigating through a visual maze relies on the strategic use of eye movementsto select and identify the route. When navigating the maze, there aretrade-offs between exploring to the environment and relying on memory. Thisstudy examined strategies used to navigating through novel and familiar mazesthat were viewed from above and traversed by a mouse cursor. Eye and mousemovements revealed two modes that almost never occurred concurrently:exploration and guidance. Analyses showed that people learned mazes and wereable to devise and carry out complex, multi-faceted strategies that traded-offvisual exploration against active motor performance. These strategies took intoaccount available visual information, memory, confidence, the estimated cost intime for exploration, and idiosyncratic tolerance for error. Understanding thestrategies humans used for maze solving is valuable for applications incognitive neuroscience as well as in AI, robotics and human-robot interactions.
arxiv-4200-155 | Kinetic Energy Plus Penalty Functions for Sparse Estimation | http://arxiv.org/abs/1307.5601 | author:Zhihua Zhang, Shibo Zhao, Zebang Shen, Shuchang Zhou category:stat.ML published:2013-07-22 summary:In this paper we propose and study a family of sparsity-inducing penaltyfunctions. Since the penalty functions are related to the kinetic energy inspecial relativity, we call them \emph{kinetic energy plus} (KEP) functions. Weconstruct the KEP function by using the concave conjugate of a$\chi^2$-distance function and present several novel insights into the KEPfunction with $q=1$. In particular, we derive a thresholding operator based onthe KEP function, and prove its mathematical properties and asymptoticproperties in sparsity modeling. Moreover, we show that a coordinate descentalgorithm is especially appropriate for the KEP function. Additionally, wediscuss the relationship of KEP with the penalty functions $\ell_{1/2}$ andMCP. The theoretical and empirical analysis validates that the KEP function iseffective and efficient in high-dimensional data modeling.
arxiv-4200-156 | Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery | http://arxiv.org/abs/1307.5870 | author:Cun Mu, Bo Huang, John Wright, Donald Goldfarb category:stat.ML cs.LG published:2013-07-22 summary:Recovering a low-rank tensor from incomplete information is a recurringproblem in signal processing and machine learning. The most popular convexrelaxation of this problem minimizes the sum of the nuclear norms of theunfoldings of the tensor. We show that this approach can be substantiallysuboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank$r$ from Gaussian measurements requires $\Omega(r n^{K-1})$ observations. Incontrast, a certain (intractable) nonconvex formulation needs only $O(r^K +nrK)$ observations. We introduce a very simple, new convex relaxation, whichpartially bridges this gap. Our new formulation succeeds with $O(r^{\lfloor K/2\rfloor}n^{\lceil K/2 \rceil})$ observations. While these results pertain toGaussian measurements, simulations strongly suggest that the new norm alsooutperforms the sum of nuclear norms for tensor completion from a random subsetof entries. Our lower bound for the sum-of-nuclear-norms model follows from a new resulton recovering signals with multiple sparse structures (e.g. sparse, low rank),which perhaps surprisingly demonstrates the significant suboptimality of thecommonly used recovery approach via minimizing the sum of individual sparsityinducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-ranktensor recovery however opens the possibility in reducing the sample complexityby exploiting several structures jointly.
arxiv-4200-157 | Saliency-Guided Perceptual Grouping Using Motion Cues in Region-Based Artificial Visual Attention | http://arxiv.org/abs/1307.5710 | author:Jan TÃ¼nnermann, Dieter Enns, BÃ¤rbel Mertsching category:cs.CV published:2013-07-22 summary:Region-based artificial attention constitutes a framework for bio-inspiredattentional processes on an intermediate abstraction level for the use incomputer vision and mobile robotics. Segmentation algorithms produce regions ofcoherently colored pixels. These serve as proto-objects on which theattentional processes determine image portions of relevance. A singleregion---which not necessarily represents a full object---constitutes the focusof attention. For many post-attentional tasks, however, such as identifying ortracking objects, single segments are not sufficient. Here, we present asaliency-guided approach that groups regions that potentially belong to thesame object based on proximity and similarity of motion. We compare our resultsto object selection by thresholding saliency maps and a furtherattention-guided strategy.
arxiv-4200-158 | Is Bottom-Up Attention Useful for Scene Recognition? | http://arxiv.org/abs/1307.5702 | author:Samuel F. Dodge, Lina J. Karam category:cs.CV published:2013-07-22 summary:The human visual system employs a selective attention mechanism to understandthe visual world in an eficient manner. In this paper, we show howcomputational models of this mechanism can be exploited for the computer visionapplication of scene recognition. First, we consider saliency weighting andsaliency pruning, and provide a comparison of the performance of differentattention models in these approaches in terms of classification accuracy.Pruning can achieve a high degree of computational savings withoutsignificantly sacrificing classification accuracy. In saliency weighting,however, we found that classification performance does not improve. Inaddition, we present a new method to incorporate salient and non-salientregions for improved classification accuracy. We treat the salient andnon-salient regions separately and combine them using Multiple Kernel Learning.We evaluate our approach using the UIUC sports dataset and find that with asmall training size, our method improves upon the classification accuracy ofthe baseline bag of features approach.
arxiv-4200-159 | Top-down and Bottom-up Feature Combination for Multi-sensor Attentive Robots | http://arxiv.org/abs/1307.5720 | author:Esther L. Colombini, Alexandre S. SimÃµes, Carlos H. C. Ribeiro category:cs.RO cs.CV published:2013-07-22 summary:The information available to robots in real tasks is widely distributed bothin time and space, requiring the agent to search for relevant data. In humans,that face the same problem when sounds, images and smells are presented totheir sensors in a daily scene, a natural system is applied: Attention. Asvision plays an important role in our routine, most research regardingattention has involved this sensorial system and the same has been replicatedto the robotics field. However,most of the robotics tasks nowadays do not relyonly in visual data, that are still costly. To allow the use of attentiveconcepts with other robotics sensors that are usually used in tasks such asnavigation, self-localization, searching and mapping, a generic attentionalmodel has been previously proposed. In this work, feature mapping functionswere designed to build feature maps to this attentive model from data fromrange scanner and sonar sensors. Experiments were performed in a high fidelitysimulated robotics environment and results have demonstrated the capability ofthe model on dealing with both salient stimuli and goal-driven attention overmultiple features extracted from multiple sensors.
arxiv-4200-160 | Visual saliency estimation by integrating features using multiple kernel learning | http://arxiv.org/abs/1307.5693 | author:Yasin Kavak, Erkut Erdem, Aykut Erdem category:cs.CV published:2013-07-22 summary:In the last few decades, significant achievements have been attained inpredicting where humans look at images through different computational models.However, how to determine contributions of different visual features to overallsaliency still remains an open problem. To overcome this issue, a recent classof models formulates saliency estimation as a supervised learning problem andaccordingly apply machine learning techniques. In this paper, we also addressthis challenging problem and propose to use multiple kernel learning (MKL) tocombine information coming from different feature dimensions and to performintegration at an intermediate level. Besides, we suggest to use responses of arecently proposed filterbank of object detectors, known as Object-Bank, asadditional semantic high-level features. Here we show that our MKL-basedframework together with the proposed object-specific features providestate-of-the-art performance as compared to SVM or AdaBoost-based saliencymodels.
arxiv-4200-161 | A Novel Equation based Classifier for Detecting Human in Images | http://arxiv.org/abs/1307.5591 | author:Subra Mukherjee, Karen Das category:cs.CV published:2013-07-22 summary:Shape based classification is one of the most challenging tasks in the fieldof computer vision. Shapes play a vital role in object recognition. The basicshapes in an image can occur in varying scale, position and orientation. Andspecially when detecting human, the task becomes more challenging owing to thelargely varying size, shape, posture and clothing of human. So, in our work wedetect human, based on the head-shoulder shape as it is the most unvarying partof human body. Here, firstly a new and a novel equation named as the OmegaEquation that describes the shape of human head-shoulder is developed and basedon this equation, a classifier is designed particularly for detecting humanpresence in a scene. The classifier detects human by analyzing some of thediscriminative features of the values of the parameters obtained from the Omegaequation. The proposed method has been tested on a variety of shape datasettaking into consideration the complexities of human head-shoulder shape. In allthe experiments the proposed method demonstrated satisfactory results.
arxiv-4200-162 | Performance comparison of State-of-the-art Missing Value Imputation Algorithms on Some Bench mark Datasets | http://arxiv.org/abs/1307.5599 | author:M. Naresh Kumar category:cs.LG stat.ML published:2013-07-22 summary:Decision making from data involves identifying a set of attributes thatcontribute to effective decision making through computational intelligence. Thepresence of missing values greatly influences the selection of right set ofattributes and this renders degradation in classification accuracies of theclassifiers. As missing values are quite common in data collection phase duringfield experiments or clinical trails appropriate handling would improve theclassifier performance. In this paper we present a review of recently developedmissing value imputation algorithms and compare their performance on some benchmark datasets.
arxiv-4200-163 | A study of parameters affecting visual saliency assessment | http://arxiv.org/abs/1307.5691 | author:Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit category:cs.CV published:2013-07-22 summary:Since the early 2000s, computational visual saliency has been a very activeresearch area. Each year, more and more new models are published in the maincomputer vision conferences. Nowadays, one of the big challenges is to find away to fairly evaluate all of these models. In this paper, a new framework isproposed to assess models of visual saliency. This evaluation is divided intothree experiments leading to the proposition of a new evaluation framework.Each experiment is based on a basic question: 1) there are two ground truthsfor saliency evaluation: what are the differences between eye fixations andmanually segmented salient regions?, 2) the properties of the salient regions:for example, do large, medium and small salient regions present differentdifficulties for saliency models? and 3) the metrics used to assess saliencymodels: what advantages would there be to mix them with PCA? Statisticalanalysis is used here to answer each of these three questions.
arxiv-4200-164 | Multi-horizon solar radiation forecasting for Mediterranean locations using time series models | http://arxiv.org/abs/1307.6179 | author:Cyril Voyant, Christophe Paoli, Marc Muselli, Marie Laure Nivet category:physics.ao-ph cs.NE published:2013-07-22 summary:Considering the grid manager's point of view, needs in terms of prediction ofintermittent energy like the photovoltaic resource can be distinguishedaccording to the considered horizon: following days (d+1, d+2 and d+3), nextday by hourly step (h+24), next hour (h+1) and next few minutes (m+5 e.g.).Through this work, we have identified methodologies using time series modelsfor the prediction horizon of global radiation and photovoltaic power. What wepresent here is a comparison of different predictors developed and tested topropose a hierarchy. For horizons d+1 and h+1, without advanced ad hoc timeseries pre-processing (stationarity) we find it is not easy to differentiatebetween autoregressive moving average (ARMA) and multilayer perceptron (MLP).However we observed that using exogenous variables improves significantly theresults for MLP . We have shown that the MLP were more adapted for horizonsh+24 and m+5. In summary, our results are complementary and improve theexisting prediction techniques with innovative tools: stationarity, numericalweather prediction combination, MLP and ARMA hybridization, multivariateanalysis, time index, etc.
arxiv-4200-165 | An Adaptive GMM Approach to Background Subtraction for Application in Real Time Surveillance | http://arxiv.org/abs/1307.5800 | author:Subra Mukherjee, Karen Das category:cs.CV published:2013-07-22 summary:Efficient security management has become an important parameter in todaysworld. As the problem is growing, there is an urgent need for the introductionof advanced technology and equipment to improve the state-of art ofsurveillance. In this paper we propose a model for real time backgroundsubtraction using AGMM. The proposed model is robust and adaptable to dynamicbackground, fast illumination changes, repetitive motion. Also we haveincorporated a method for detecting shadows using the Horpresert color model.The proposed model can be employed for monitoring areas where movement or entryis highly restricted. So on detection of any unexpected events in the scene analarm can be triggered and hence we can achieve real time surveillance even inthe absence of constant human monitoring.
arxiv-4200-166 | Appearance Descriptors for Person Re-identification: a Comprehensive Review | http://arxiv.org/abs/1307.5748 | author:Riccardo Satta category:cs.CV published:2013-07-22 summary:In video-surveillance, person re-identification is the task of recognisingwhether an individual has already been observed over a network of cameras.Typically, this is achieved by exploiting the clothing appearance, as classicalbiometric traits like the face are impractical in real-world video surveillancescenarios. Clothing appearance is represented by means of low-level\textit{local} and/or \textit{global} features of the image, usually extractedaccording to some part-based body model to treat different body parts (e.g.torso and legs) independently. This paper provides a comprehensive review ofcurrent approaches to build appearance descriptors for personre-identification. The most relevant techniques are described in detail, andcategorised according to the body models and features used. The aim of thiswork is to provide a structured body of knowledge and a starting point forresearchers willing to conduct novel investigations on this challenging topic.
arxiv-4200-167 | Online Tracking Parameter Adaptation based on Evaluation | http://arxiv.org/abs/1307.5653 | author:Duc Phu Chau, Julien Badie, FranÃ§ois Bremond, Monique Thonnat category:cs.CV published:2013-07-22 summary:Parameter tuning is a common issue for many tracking algorithms. In order tosolve this problem, this paper proposes an online parameter tuning to adapt atracking algorithm to various scene contexts. In an offline training phase,this approach learns how to tune the tracker parameters to cope with differentcontexts. In the online control phase, once the tracking quality is evaluatedas not good enough, the proposed approach computes the current context andtunes the tracking parameters using the learned values. The experimentalresults show that the proposed approach improves the performance of thetracking algorithm and outperforms recent state of the art trackers. This paperbrings two contributions: (1) an online tracking evaluation, and (2) a methodto adapt online tracking parameters to scene contexts.
arxiv-4200-168 | New Optimization Approach Using Clustering-Based Parallel Genetic Algorithm | http://arxiv.org/abs/1307.5667 | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:In many global Optimization Problems, it is required to evaluate a globalpoint (min or max) in large space that calculation effort is very high. In thispaper is presented new approach for optimization problem with subdivisionlabeling method (SLM) but in this method for higher dimensional has highcalculation effort. Clustering-Based Parallel Genetic Algorithm (CBPGA) inoptimization problems is one of the solutions of this problem. That the initialpopulation is crossing points and subdividing in each step is according tomutation. After labeling all of crossing points, selecting is according topolytope that has complete label. In this method we propose an algorithm, basedon parallelization scheme using master-slave. SLM algorithm is implemented byCBPGA and compared the experimental results. The numerical examples andnumerical results show that SLMCBPGA is improved speed up and efficiency.
arxiv-4200-169 | Rotational Mutation Genetic Algorithm on optimization Problems | http://arxiv.org/abs/1307.5838 | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:Optimization problem, nowadays, have more application in all major but theyhave problem in computation. Calculation of the optimum point in the spaceswith the above dimensions is very time consuming. In this paper, there ispresented a new approach for the optimization of continuous functions withrotational mutation that is called RM. The proposed algorithm starts from thepoint which has best fitness value by elitism mechanism. Then, method ofrotational mutation is used to reach optimal point. In this paper, RM algorithmis implemented by GA(Briefly RMGA) and is compared with other well- knownalgorithms: DE, PGA, Grefensstette and Eshelman [15, 16] and numerical andsimulation results show that RMGA achieve global optimal point with moredecision by smaller generations.
arxiv-4200-170 | Solving Traveling Salesman Problem by Marker Method | http://arxiv.org/abs/1307.5674 | author:Masoumeh Vali category:cs.NE cs.DS math.OC published:2013-07-22 summary:In this paper we use marker method and propose a new mutation operator thatselects the nearest neighbor among all near neighbors solving TravelingSalesman Problem.
arxiv-4200-171 | A New Approach for Finding the Global Optimal Point Using Subdividing Labeling Method (SLM) | http://arxiv.org/abs/1307.5839 | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:In most global optimization problems, finding global optimal point inthemultidimensional and great search space needs high computations. In this paper,we present a new approach to find global optimal point with the low computationand few steps using subdividing labeling method (SLM) which can also be used inthe multi-dimensional and great search space. In this approach, in each step,crossing points will be labeled and complete label polytope search space ofselected polytope will be subdivided after being selected. SLM algorithm findsthe global point until h (subdivision function) turns into zero. SLM will beimplemented on five applications and compared with the latest techniques suchas random search, random search-walk and simulated annealing method. Theresults of the proposed method demonstrate that our new approach is faster andmore reliable and presents an optimal time complexity O (logn).
arxiv-4200-172 | Sub-Dividing Genetic Method for Optimization Problems | http://arxiv.org/abs/1307.5679 | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:Nowadays, optimization problem have more application in all major but theyhave problem in computation. Computation global point in continuous functionshave high calculation and this became clearer in large space .In this paper, weproposed Sub- Dividing Genetic Method(SGM) that have less computation thanother method for achieving global points . This method userotation mutation andcrossover based sub-division method that sub diving method is used for minimizesearch space and rotation mutation with crossover is used for finding globaloptimal points. In experimental, SGM algorithm is implemented on De Jongfunction. The numerical examples show that SGM is performed more optimal thanother methods such as Grefensstette, Random Value, and PNG.
arxiv-4200-173 | Sub- Diving Labeling Method for Optimization Problem by Genetic Algorithm | http://arxiv.org/abs/1307.5840 | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:In many global Optimization Problems, it is required to evaluate a globalpoint (min or max) in large space that calculation effort is very high. In thispaper is presented new approach for optimization problem with subdivisionlabeling method (SLM) but in this method for higher dimensional has highcomputational. SLM Genetic Algorithm (SLMGA) in optimization problems is one ofthe solutions of this problem. In proposed algorithm the initial population iscrossing points and subdividing in each step is according to mutation. RSLMGAis compared with other well known algorithms: DE, PGA, Grefensstette andEshelman and numerical results show that RSLMGA achieve global optimal pointwith more decision by smaller generations.
arxiv-4200-174 | Using a Dynamic Neural Field Model to Explore a Direct Collicular Inhibition Account of Inhibition of Return | http://arxiv.org/abs/1307.5684 | author:Jason Satel, Ross Story, Matthew D. Hilchey, Zhiguo Wang, Raymond M. Klein category:q-bio.NC cs.CV published:2013-07-22 summary:When the interval between a transient ash of light (a "cue") and a secondvisual response signal (a "target") exceeds at least 200ms, responding isslowest in the direction indicated by the first signal. This phenomenon iscommonly referred to as inhibition of return (IOR). The dynamic neural fieldmodel (DNF) has proven to have broad explanatory power for IOR, effectivelycapturing many empirical results. Previous work has used a short-termdepression (STD) implementation of IOR, but this approach fails to explain manybehavioral phenomena observed in the literature. Here, we explore a variantmodel of IOR involving a combination of STD and delayed direct collicularinhibition. We demonstrate that this hybrid model can better reproduceestablished behavioural results. We use the results of this model to proposeseveral experiments that would yield particularly valuable insight into thenature of the neurophysiological mechanisms underlying IOR.
arxiv-4200-175 | Mixtures of Common Skew-t Factor Analyzers | http://arxiv.org/abs/1307.5558 | author:Paula M. Murray, Paul D. McNicholas, Ryan P. Browne category:stat.ME stat.AP stat.CO stat.ML published:2013-07-21 summary:A mixture of common skew-t factor analyzers model is introduced formodel-based clustering of high-dimensional data. By assuming common componentfactor loadings, this model allows clustering to be performed in the presenceof a large number of mixture components or when the number of dimensions is toolarge to be well-modelled by the mixtures of factor analyzers model or avariant thereof. Furthermore, assuming that the component densities follow askew-t distribution allows robust clustering of skewed data. The alternatingexpectation-conditional maximization algorithm is employed for parameterestimation. We demonstrate excellent clustering performance when our model isapplied to real and simulated data.This paper marks the first time that skewedcommon factors have been used.
arxiv-4200-176 | Regularized Discrete Optimal Transport | http://arxiv.org/abs/1307.5551 | author:Sira Ferradans, Nicolas Papadakis, Gabriel PeyrÃ©, Jean-FranÃ§ois Aujol category:cs.CV cs.DM math.OC published:2013-07-21 summary:This article introduces a generalization of the discrete optimal transport,with applications to color image manipulations. This new formulation includes arelaxation of the mass conservation constraint and a regularization term. Thesetwo features are crucial for image processing tasks, which necessitate to takeinto account families of multimodal histograms, with large mass variationacross modes. The corresponding relaxed and regularized transportation problem is thesolution of a convex optimization problem. Depending on the regularizationused, this minimization can be solved using standard linear programming methodsor first order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, whichis robust to mass variation across images color palettes. Furthermore, theregularization of the transport plan helps to remove colorization artifacts dueto noise amplification. We also extend this framework to the computation of barycenters ofdistributions. The barycenter is the solution of an optimization problem, whichis separately convex with respect to the barycenter and the transportationplans, but not jointly convex. A block coordinate descent scheme converges to astationary point of the energy. We show that the resulting algorithm can beused for color normalization across several images. The relaxed and regularizedbarycenter defines a common color palette for those images. Applying colortransfer toward this average palette performs a color normalization of theinput images.
arxiv-4200-177 | A New Optimization Approach Based on Rotational Mutation and Crossover Operator | http://arxiv.org/abs/1307.5534 | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-21 summary:Evaluating a global optimal point in many global optimization problems inlarge space is required to more calculations. In this paper, there is presenteda new approach for the continuous functions optimization with rotationalmutation and crossover operator. This proposed method (RMC) starts from thepoint which has best fitness value by elitism mechanism and after thatrotational mutation and crossover operator are used to reach optimal point. RMCmethod is implemented by GA (Briefly RMCGA) and is compared with otherwellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] andnumerical and simulating results show that RMCGA achieve global optimal pointwith more decision by smaller generations.
arxiv-4200-178 | On GROUSE and Incremental SVD | http://arxiv.org/abs/1307.5494 | author:Laura Balzano, Stephen J. Wright category:cs.NA cs.LG stat.ML published:2013-07-21 summary:GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incrementalalgorithm for identifying a subspace of Rn from a sequence of vectors in thissubspace, where only a subset of components of each vector is revealed at eachiteration. Recent analysis has shown that GROUSE converges locally at anexpected linear rate, under certain assumptions. GROUSE has a similar flavor tothe incremental singular value decomposition algorithm, which updates the SVDof a matrix following addition of a single column. In this paper, we modify theincremental SVD approach to handle missing data, and demonstrate that thismodified approach is equivalent to GROUSE, for a certain choice of analgorithmic parameter.
arxiv-4200-179 | Optimal Recombination in Genetic Algorithms | http://arxiv.org/abs/1307.5519 | author:Anton V. Eremeev, Julia V. Kovalenko category:cs.NE cs.DS published:2013-07-21 summary:This paper surveys results on complexity of the optimal recombination problem(ORP), which consists in finding the best possible offspring as a result of arecombination operator in a genetic algorithm, given two parent solutions. Weconsider efficient reductions of the ORPs, allowing to establish polynomialsolvability or NP-hardness of the ORPs, as well as direct proofs of hardnessresults.
arxiv-4200-180 | A scalable stage-wise approach to large-margin multi-class loss based boosting | http://arxiv.org/abs/1307.5497 | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.LG published:2013-07-21 summary:We present a scalable and effective classification model to train multi-classboosting for multi-class classification problems. Shen and Hao introduced adirect formulation of multi- class boosting in the sense that it directlymaximizes the multi- class margin [C. Shen and Z. Hao, "A direct formulationfor totally-corrective multi- class boosting", in Proc. IEEE Conf. Comp. Vis.Patt. Recogn., 2011]. The major problem of their approach is its highcomputational complexity for training, which hampers its application onreal-world problems. In this work, we propose a scalable and simple stage-wisemulti-class boosting method, which also directly maximizes the multi-classmargin. Our approach of- fers a few advantages: 1) it is simple andcomputationally efficient to train. The approach can speed up the training timeby more than two orders of magnitude without sacrificing the classificationaccuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice ofparameters and empirically demonstrates excellent generalization performance.Experimental results on challenging multi-class machine learning and visiontasks demonstrate that the proposed approach substantially improves theconvergence rate and accuracy of the final visual detector at no additionalcomputational cost compared to existing multi-class boosting.
arxiv-4200-181 | Clustering Algorithm for Gujarati Language | http://arxiv.org/abs/1307.5393 | author:Miral Patel, Prem Balani category:cs.CL published:2013-07-20 summary:Natural language processing area is still under research. But now a day it ison platform for worldwide researchers. Natural language processing includesanalyzing the language based on its structure and then tagging of each wordappropriately with its grammar base. Here we have 50,000 tagged words set andwe try to cluster those Gujarati words based on proposed algorithm, we havedefined our own algorithm for processing. Many clustering techniques areavailable Ex. Single linkage, complete, linkage,average linkage, Hear no ofclusters to be formed are not known, so it is all depends on the type of dataset provided . Clustering is preprocess for stemming . Stemming is the processwhere root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun andplural form.
arxiv-4200-182 | Towards Distribution-Free Multi-Armed Bandits with Combinatorial Strategies | http://arxiv.org/abs/1307.5438 | author:Xiang-yang Li, Shaojie Tang, Yaqin Zhou category:cs.LG published:2013-07-20 summary:In this paper we study a generalized version of classical multi-armed bandits(MABs) problem by allowing for arbitrary constraints on constituent bandits ateach decision point. The motivation of this study comes from many situationsthat involve repeatedly making choices subject to arbitrary constraints in anuncertain environment: for instance, regularly deciding which advertisements todisplay online in order to gain high click-through-rate without knowing userpreferences, or what route to drive home each day under uncertain weather andtraffic conditions. Assume that there are $K$ unknown random variables (RVs),i.e., arms, each evolving as an \emph{i.i.d} stochastic process over time. Ateach decision epoch, we select a strategy, i.e., a subset of RVs, subject toarbitrary constraints on constituent RVs. We then gain a reward that is a linear combination of observations onselected RVs. The performance of prior results for this problem heavily depends on thedistribution of strategies generated by corresponding learning policy. Forexample, if the reward-difference between the best and second best strategyapproaches zero, prior result may lead to arbitrarily large regret. Meanwhile, when there are exponential number of possible strategies at eachdecision point, naive extension of a prior distribution-free policy would causepoor performance in terms of regret, computation and space complexity. To this end, we propose an efficient Distribution-Free Learning (DFL) policythat achieves zero regret, regardless of the probability distribution of theresultant strategies. Our learning policy has both $O(K)$ time complexity and $O(K)$ spacecomplexity. In successive generations, we show that even if finding the optimalstrategy at each decision point is NP-hard, our policy still allows forapproximated solutions while retaining near zero-regret.
arxiv-4200-183 | Non-stationary Stochastic Optimization | http://arxiv.org/abs/1307.5449 | author:O. Besbes, Y. Gur, A. Zeevi category:math.PR cs.LG stat.ML published:2013-07-20 summary:We consider a non-stationary variant of a sequential stochastic optimizationproblem, in which the underlying cost functions may change along the horizon.We propose a measure, termed variation budget, that controls the extent of saidchange, and study how restrictions on this budget impact achievableperformance. We identify sharp conditions under which it is possible to achievelong-run-average optimality and more refined performance measures such as rateoptimality that fully characterize the complexity of such problems. In doingso, we also establish a strong connection between two rather disparate strandsof literature: adversarial online convex optimization; and the more traditionalstochastic approximation paradigm (couched in a non-stationary setting). Thisconnection is the key to deriving well performing policies in the latter, byleveraging structure of optimal policies in the former. Finally, tight boundson the minimax regret allow us to quantify the "price of non-stationarity,"which mathematically captures the added complexity embedded in a temporallychanging environment versus a stationary one.
arxiv-4200-184 | A convex pseudo-likelihood framework for high dimensional partial correlation estimation with convergence guarantees | http://arxiv.org/abs/1307.5381 | author:Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam category:stat.ME stat.CO stat.ML published:2013-07-20 summary:Sparse high dimensional graphical model selection is a topic of much interestin modern day statistics. A popular approach is to apply l1-penalties to either(1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods,with the latter having the distinct advantage that they do not explicitlyassume Gaussianity. As none of the popular methods proposed for solvingpseudo-likelihood based objective functions have provable convergenceguarantees, it is not clear if corresponding estimators exist or are evencomputable, or if they actually yield correct partial correlation graphs. Thispaper proposes a new pseudo-likelihood based graphical model selection methodthat aims to overcome some of the shortcomings of current methods, but at thesame time retain all their respective strengths. In particular, we introduce anovel framework that leads to a convex formulation of the partial covarianceregression graph problem, resulting in an objective function comprised ofquadratic forms. The objective is then optimized via a coordinate-wiseapproach. The specific functional form of the objective function facilitatesrigorous convergence analysis leading to convergence guarantees; an importantproperty that cannot be established using standard results, when the dimensionis larger than the sample size, as is often the case in high dimensionalapplications. These convergence guarantees ensure that estimators arewell-defined under very general conditions, and are always computable. Inaddition, the approach yields estimators that have good large sample propertiesand also respect symmetry. Furthermore, application to simulated/real data,timing comparisons and numerical convergence is demonstrated. We also present anovel unifying framework that places all graphical pseudo-likelihood methods asspecial cases of a more general formulation, leading to important insights.
arxiv-4200-185 | The Cluster Graphical Lasso for improved estimation of Gaussian graphical models | http://arxiv.org/abs/1307.5339 | author:Kean Ming Tan, Daniela Witten, Ali Shojaie category:stat.ML stat.ME published:2013-07-19 summary:We consider the task of estimating a Gaussian graphical model in thehigh-dimensional setting. The graphical lasso, which involves maximizing theGaussian log likelihood subject to an l1 penalty, is a well-studied approachfor this task. We begin by introducing a surprising connection between thegraphical lasso and hierarchical clustering: the graphical lasso in effectperforms a two-step procedure, in which (1) single linkage hierarchicalclustering is performed on the variables in order to identify connectedcomponents, and then (2) an l1-penalized log likelihood is maximized on thesubset of variables within each connected component. In other words, thegraphical lasso determines the connected components of the estimated networkvia single linkage clustering. Unfortunately, single linkage clustering isknown to perform poorly in certain settings. Therefore, we propose the clustergraphical lasso, which involves clustering the features using an alternative tosingle linkage clustering, and then performing the graphical lasso on thesubset of variables within each cluster. We establish model selectionconsistency for this technique, and demonstrate its improved performancerelative to the graphical lasso in a simulation study, as well as inapplications to an equities data set, a university webpage data set, and a geneexpression data set.
arxiv-4200-186 | Random Binary Mappings for Kernel Learning and Efficient SVM | http://arxiv.org/abs/1307.5161 | author:Gemma Roig, Xavier Boix, Luc Van Gool category:cs.CV cs.LG stat.ML published:2013-07-19 summary:Support Vector Machines (SVMs) are powerful learners that have led tostate-of-the-art results in various computer vision problems. SVMs suffer fromvarious drawbacks in terms of selecting the right kernel, which depends on theimage descriptors, as well as computational and memory efficiency. This paperintroduces a novel kernel, which serves such issues well. The kernel is learnedby exploiting a large amount of low-complex, randomized binary mappings of theinput feature. This leads to an efficient SVM, while also alleviating the taskof kernel selection. We demonstrate the capabilities of our kernel on 6standard vision benchmarks, in which we combine several common imagedescriptors, namely histograms (Flowers17 and Daimler), attribute-likedescriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).Results show that our kernel learning adapts well to the different descriptorstypes, achieving the performance of the kernels specifically tuned for eachimage descriptor, and with similar evaluation cost as efficient SVM methods.
arxiv-4200-187 | Automated Defect Localization via Low Rank Plus Outlier Modeling of Propagating Wavefield Data | http://arxiv.org/abs/1307.5102 | author:Stefano Gonella, Jarvis D. Haupt category:cs.CV published:2013-07-19 summary:This work proposes an agnostic inference strategy for material diagnostics,conceived within the context of laser-based non-destructive evaluation methods,which extract information about structural anomalies from the analysis ofacoustic wavefields measured on the structure's surface by means of a scanninglaser interferometer. The proposed approach couples spatiotemporal windowingwith low rank plus outlier modeling, to identify a priori unknown deviations inthe propagating wavefields caused by material inhomogeneities or defects, usingvirtually no knowledge of the structural and material properties of the medium.This characteristic makes the approach particularly suitable for diagnosticsscenarios where the mechanical and material models are complex, unknown, orunreliable. We demonstrate our approach in a simulated environment usingbenchmark point and line defect localization problems based on propagatingflexural waves in a thin plate.
arxiv-4200-188 | Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts | http://arxiv.org/abs/1307.5336 | author:Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius category:cs.CL cs.IR q-fin.CP I.2.7 published:2013-07-19 summary:The use of robo-readers to analyze news texts is an emerging technology trendin computational finance. In recent research, a substantial effort has beeninvested to develop sophisticated financial polarity-lexicons that can be usedto investigate how financial sentiments relate to future company performance.However, based on experience from other fields, where sentiment analysis iscommonly applied, it is well-known that the overall semantic orientation of asentence may differ from the prior polarity of individual words. The objectiveof this article is to investigate how semantic orientations can be betterdetected in financial and economic news by accommodating the overallphrase-structure information and domain-specific use of language. Our threemain contributions are: (1) establishment of a human-annotated financephrase-bank, which can be used as benchmark for training and evaluatingalternative models; (2) presentation of a technique to enhance financiallexicons with attributes that help to identify expected direction of eventsthat affect overall sentiment; (3) development of a linearized phrase-structuremodel for detecting contextual semantic orientations in financial and economicnews texts. The relevance of the newly added lexicon features and the benefitof using the proposed learning-algorithm are demonstrated in a comparativestudy against previously used general sentiment models as well as the popularword frequency models used in recent financial studies. The proposed frameworkis parsimonious and avoids the explosion in feature-space caused by the use ofconventional n-gram features.
arxiv-4200-189 | Model-Based Policy Gradients with Parameter-Based Exploration by Least-Squares Conditional Density Estimation | http://arxiv.org/abs/1307.5118 | author:Syogo Mori, Voot Tangkaratt, Tingting Zhao, Jun Morimoto, Masashi Sugiyama category:stat.ML cs.LG published:2013-07-19 summary:The goal of reinforcement learning (RL) is to let an agent learn an optimalcontrol policy in an unknown environment so that future expected rewards aremaximized. The model-free RL approach directly learns the policy based on datasamples. Although using many samples tends to improve the accuracy of policylearning, collecting a large number of samples is often expensive in practice.On the other hand, the model-based RL approach first estimates the transitionmodel of the environment and then learns the policy based on the estimatedtransition model. Thus, if the transition model is accurately learned from asmall amount of data, the model-based approach can perform better than themodel-free approach. In this paper, we propose a novel model-based RL method bycombining a recently proposed model-free policy search method called policygradients with parameter-based exploration and the state-of-the-art transitionmodel estimator called least-squares conditional density estimation. Throughexperiments, we demonstrate the practical usefulness of the proposed method.
arxiv-4200-190 | Speaker Independent Continuous Speech to Text Converter for Mobile Application | http://arxiv.org/abs/1307.5736 | author:R. Sandanalakshmi, P. Abinaya Viji, M. Kiruthiga, M. Manjari, M. Sharina category:cs.CL cs.NE cs.SD published:2013-07-19 summary:An efficient speech to text converter for mobile application is presented inthis work. The prime motive is to formulate a system which would give optimumperformance in terms of complexity, accuracy, delay and memory requirements formobile environment. The speech to text converter consists of two stages namelyfront-end analysis and pattern recognition. The front end analysis involvespreprocessing and feature extraction. The traditional voice activity detectionalgorithms which track only energy cannot successfully identify potentialspeech from input because the unwanted part of the speech also has some energyand appears to be speech. In the proposed system, VAD that calculates energy ofhigh frequency part separately as zero crossing rate to differentiate noisefrom speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used asfeature extraction method and Generalized Regression Neural Network is used asrecognizer. MFCC provides low word error rate and better feature extraction.Neural Network improves the accuracy. Thus a small database containing allpossible syllable pronunciation of the user is sufficient to give recognitionaccuracy closer to 100%. Thus the proposed technique entertains realization ofreal time speaker independent applications like mobile phones, PDAs etc.
arxiv-4200-191 | Kernel Adaptive Metropolis-Hastings | http://arxiv.org/abs/1307.5302 | author:Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, Arthur Gretton category:stat.ML cs.LG published:2013-07-19 summary:A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for thepurpose of sampling from a target distribution with strongly nonlinear support.The algorithm embeds the trajectory of the Markov chain into a reproducingkernel Hilbert space (RKHS), such that the feature space covariance of thesamples informs the choice of proposal. The procedure is computationallyefficient and straightforward to implement, since the RKHS moves can beintegrated out analytically: our proposal distribution in the original space isa normal distribution whose mean and covariance depend on where the currentsample lies in the support of the target distribution, and adapts to its localcovariance structure. Furthermore, the procedure requires neither gradients norany other higher order information about the target, making it particularlyattractive for contexts such as Pseudo-Marginal MCMC. Kernel AdaptiveMetropolis-Hastings outperforms competing fixed and adaptive samplers onmultivariate, highly nonlinear target distributions, arising in both real-worldand synthetic examples. Code may be downloaded athttps://github.com/karlnapf/kameleon-mcmc.
arxiv-4200-192 | Tensor-based formulation and nuclear norm regularization for multi-energy computed tomography | http://arxiv.org/abs/1307.5348 | author:Oguz Semerci, Ning Hao, Misha E. Kilmer, Eric L. Miller category:cs.CV physics.med-ph published:2013-07-19 summary:The development of energy selective, photon counting X-ray detectors allowsfor a wide range of new possibilities in the area of computed tomographic imageformation. Under the assumption of perfect energy resolution, here we propose atensor-based iterative algorithm that simultaneously reconstructs the X-rayattenuation distribution for each energy. We use a multi-linear image modelrather than a more standard "stacked vector" representation in order to developnovel tensor-based regularizers. Specifically, we model the multi-spectralunknown as a 3-way tensor where the first two dimensions are space and thethird dimension is energy. This approach allows for the design of tensornuclear norm regularizers, which like its two dimensional counterpart, is aconvex function of the multi-spectral unknown. The solution to the resultingconvex optimization problem is obtained using an alternating direction methodof multipliers (ADMM) approach. Simulation results shows that the generalizedtensor nuclear norm can be used as a stand alone regularization technique forthe energy selective (spectral) computed tomography (CT) problem and whencombined with total variation regularization it enhances the regularizationcapabilities especially at low energy images where the effects of noise aremost prominent.
arxiv-4200-193 | Making Laplacians commute | http://arxiv.org/abs/1307.6549 | author:Michael M. Bronstein, Klaus Glashoff, Terry A. Loring category:cs.CV cs.GR math.SP published:2013-07-19 summary:In this paper, we construct multimodal spectral geometry by finding a pair ofclosest commuting operators (CCO) to a given pair of Laplacians. The CCOs arejointly diagonalizable and hence have the same eigenbasis. Our constructionnaturally extends classical data analysis tools based on spectral geometry,such as diffusion maps and spectral clustering. We provide several syntheticand real examples of applications in dimensionality reduction, shape analysis,and clustering, demonstrating that our method better captures the inherentstructure of multi-modal data.
arxiv-4200-194 | Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization | http://arxiv.org/abs/1307.4847 | author:Zheng Wen, Benjamin Van Roy category:cs.LG cs.AI cs.SY stat.ML published:2013-07-18 summary:We consider the problem of reinforcement learning over episodes of afinite-horizon deterministic system and as a solution propose optimisticconstraint propagation (OCP), an algorithm designed to synthesize efficientexploration and value function generalization. We establish that when the truevalue function lies within a given hypothesis class, OCP selects optimalactions over all but at most K episodes, where K is the eluder dimension of thegiven hypothesis class. We establish further efficiency and asymptoticperformance guarantees that apply even if the true value function does not liein the given hypothesis class, for the special case where the hypothesis classis the span of pre-specified indicator functions over disjoint sets. We alsodiscuss the computational complexity of OCP and present computational resultsinvolving two illustrative examples.
arxiv-4200-195 | Says who? Automatic Text-Based Content Analysis of Television News | http://arxiv.org/abs/1307.4879 | author:Carlos Castillo, Gianmarco De Francisci Morales, Marcelo Mendoza, Nasir Khan category:cs.CL cs.IR published:2013-07-18 summary:We perform an automatic analysis of television news programs, based on theclosed captions that accompany them. Specifically, we collect all the newsbroadcasted in over 140 television channels in the US during a period of sixmonths. We start by segmenting, processing, and annotating the closed captionsautomatically. Next, we focus on the analysis of their linguistic style and onmentions of people using NLP methods. We present a series of key insights aboutnews providers, people in the news, and we discuss the biases that can beuncovered by automatic means. These insights are contrasted by looking at thedata from multiple points of view, including qualitative assessment.
arxiv-4200-196 | Large-scale Multi-label Learning with Missing Labels | http://arxiv.org/abs/1307.5101 | author:Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, Inderjit S. Dhillon category:cs.LG published:2013-07-18 summary:The multi-label classification problem has generated significant interest inrecent years. However, existing approaches do not adequately address two keychallenges: (a) the ability to tackle problems with a large number (saymillions) of labels, and (b) the ability to handle data with missing labels. Inthis paper, we directly address both these problems by studying the multi-labelproblem in a generic empirical risk minimization (ERM) framework. Ourframework, despite being simple, is surprisingly able to encompass severalrecent label-compression based methods which can be derived as special cases ofour method. To optimize the ERM problem, we develop techniques that exploit thestructure of specific loss functions - such as the squared loss function - tooffer efficient algorithms. We further show that our learning framework admitsformal excess risk bounds even in the presence of missing labels. Our riskbounds are tight and demonstrate better generalization performance for low-rankpromoting trace-norm regularization when compared to (rank insensitive)Frobenius norm regularization. Finally, we present extensive empirical resultson a variety of benchmark datasets and show that our methods performsignificantly better than existing label compression based methods and canscale up to very large datasets such as the Wikipedia dataset.
arxiv-4200-197 | Robust Subspace Clustering via Thresholding | http://arxiv.org/abs/1307.4891 | author:Reinhard Heckel, Helmut BÃ¶lcskei category:stat.ML cs.IT cs.LG math.IT published:2013-07-18 summary:The problem of clustering noisy and incompletely observed high-dimensionaldata points into a union of low-dimensional subspaces and a set of outliers isconsidered. The number of subspaces, their dimensions, and their orientationsare assumed unknown. We propose a simple low-complexity subspace clusteringalgorithm, which applies spectral clustering to an adjacency matrix obtained bythresholding the correlations between data points. In other words, theadjacency matrix is constructed from the nearest neighbors of each data pointin spherical distance. A statistical performance analysis shows that thealgorithm exhibits robustness to additive noise and succeeds even when thesubspaces intersect. Specifically, our results reveal an explicit tradeoffbetween the affinity of the subspaces and the tolerable noise level. Wefurthermore prove that the algorithm succeeds even when the data points areincompletely observed with the number of missing entries allowed to be (up to alog-factor) linear in the ambient dimension. We also propose a simple schemethat provably detects outliers, and we present numerical results on real andsynthetic data.
arxiv-4200-198 | Video Text Localization using Wavelet and Shearlet Transforms | http://arxiv.org/abs/1307.4990 | author:Purnendu Banerjee, B. B. Chaudhuri category:cs.CV published:2013-07-18 summary:Text in video is useful and important in indexing and retrieving the videodocuments efficiently and accurately. In this paper, we present a new method oftext detection using a combined dictionary consisting of wavelets and arecently introduced transform called shearlets. Wavelets provide optimallysparse expansion for point-like structures and shearlets provide optimallysparse expansions for curve-like structures. By combining these two features wehave computed a high frequency sub-band to brighten the text part. Then K-meansclustering is used for obtaining text pixels from the Standard Deviation (SD)of combined coefficient of wavelets and shearlets as well as the union ofwavelets and shearlets features. Text parts are obtained by groupingneighboring regions based on geometric properties of the classified outputframe of unsupervised K-means classification. The proposed method tested on astandard as well as newly collected database shows to be superior to someexisting methods.
arxiv-4200-199 | On the Necessity of Mixed Models: Dynamical Frustrations in the Mind | http://arxiv.org/abs/1307.4986 | author:Diego Gabriel Krivochen category:nlin.CD cs.CL math.DS published:2013-07-18 summary:In the present work we will present and analyze some basic processes at thelocal and global level in linguistic derivations that seem to go beyond thelimits of Markovian or Turing-like computation, and require, in our opinion, aquantum processor. We will first present briefly the working hypothesis andthen focus on the empirical domain. At the same time, we will argue that amodel appealing to only one kind of computation (be it quantum or not) isnecessarily insufficient, and thus both linear and non-linear formal models areto be invoked in order to pursue a fuller understanding of mental computationswithin a unified framework.
arxiv-4200-200 | Graphical law beneath each written natural language | http://arxiv.org/abs/1307.6235 | author:Anindya Kumar Biswas category:physics.gen-ph cs.CL published:2013-07-18 summary:We study twenty four written natural languages. We draw in the log scale,number of words starting with a letter vs rank of the letter, both normalised.We find that all the graphs are of the similar type. The graphs aretantalisingly closer to the curves of reduced magnetisation vs reducedtemperature for magnetic materials. We make a weak conjecture that a curve ofmagnetisation underlies a written natural language.
arxiv-4200-201 | Supervised Metric Learning with Generalization Guarantees | http://arxiv.org/abs/1307.4514 | author:AurÃ©lien Bellet category:cs.LG stat.ML published:2013-07-17 summary:The crucial importance of metrics in machine learning algorithms has led toan increasing interest in optimizing distance and similarity functions, an areaof research known as metric learning. When data consist of feature vectors, alarge body of work has focused on learning a Mahalanobis distance. Less workhas been devoted to metric learning from structured objects (such as strings ortrees), most of it focusing on optimizing a notion of edit distance. Weidentify two important limitations of current metric learning approaches.First, they allow to improve the performance of local algorithms such ask-nearest neighbors, but metric learning for global algorithms (such as linearclassifiers) has not been studied so far. Second, the question of thegeneralization ability of metric learning methods has been largely ignored. Inthis thesis, we propose theoretical and algorithmic contributions that addressthese limitations. Our first contribution is the derivation of a new kernelfunction built from learned edit probabilities. Our second contribution is anovel framework for learning string and tree edit similarities inspired by therecent theory of (e,g,t)-good similarity functions. Using uniform stabilityarguments, we establish theoretical guarantees for the learned similarity thatgive a bound on the generalization error of a linear classifier built from thatsimilarity. In our third contribution, we extend these ideas to metric learningfrom feature vectors by proposing a bilinear similarity learning method thatefficiently optimizes the (e,g,t)-goodness. Generalization guarantees arederived for our approach, highlighting that our method minimizes a tighterbound on the generalization error of the classifier. Our last contribution is aframework for establishing generalization bounds for a large class of existingmetric learning algorithms based on a notion of algorithmic robustness.
arxiv-4200-202 | Universally Elevating the Phase Transition Performance of Compressed Sensing: Non-Isometric Matrices are Not Necessarily Bad Matrices | http://arxiv.org/abs/1307.4502 | author:Weiyu Xu, Myung Cho category:cs.IT math.IT math.OC stat.ML published:2013-07-17 summary:In compressed sensing problems, $\ell_1$ minimization or Basis Pursuit wasknown to have the best provable phase transition performance of recoverablesparsity among polynomial-time algorithms. It is of great theoretical andpractical interest to find alternative polynomial-time algorithms which performbetter than $\ell_1$ minimization. \cite{Icassp reweighted l_1}, \cite{Isitreweighted l_1}, \cite{XuScaingLaw} and \cite{iterativereweightedjournal} haveshown that a two-stage re-weighted $\ell_1$ minimization algorithm can boostthe phase transition performance for signals whose nonzero elements follow anamplitude probability density function (pdf) $f(\cdot)$ whose $t$-th derivative$f^{t}(0) \neq 0$ for some integer $t \geq 0$. However, for signals whosenonzero elements are strictly suspended from zero in distribution (for example,constant-modulus, only taking values `$+d$' or `$-d$' for some nonzero realnumber $d$), no polynomial-time signal recovery algorithms were known toprovide better phase transition performance than plain $\ell_1$ minimization,especially for dense sensing matrices. In this paper, we show that apolynomial-time algorithm can universally elevate the phase-transitionperformance of compressed sensing, compared with $\ell_1$ minimization, evenfor signals with constant-modulus nonzero elements. Contrary to conventionalwisdoms that compressed sensing matrices are desired to be isometric, we showthat non-isometric matrices are not necessarily bad sensing matrices. In thispaper, we also provide a framework for recovering sparse signals when sensingmatrices are not isometric.
arxiv-4200-203 | Processing stationary noise: model and parameter selection in variational methods | http://arxiv.org/abs/1307.4592 | author:JÃ©rÃ´me Fehrenbach, Pierre Weiss category:cs.CV math.OC stat.AP published:2013-07-17 summary:Additive or multiplicative stationary noise recently became an importantissue in applied fields such as microscopy or satellite imaging. Relatively fewworks address the design of dedicated denoising methods compared to the usualwhite noise setting. We recently proposed a variational algorithm to tacklethis issue. In this paper, we analyze this problem from a statistical point ofview and provide deterministic properties of the solutions of the associatedvariational problems. In the first part of this work, we demonstrate that inmany practical problems, the noise can be assimilated to a colored Gaussiannoise. We provide a quantitative measure of the distance between a stationaryprocess and the corresponding Gaussian process. In the second part, we focus onthe Gaussian setting and analyze denoising methods which consist of minimizingthe sum of a total variation term and an $l^2$ data fidelity term. While theconstrained formulation of this problem allows to easily tune the parameters,the Lagrangian formulation can be solved more efficiently since the problem isstrongly convex. Our second contribution consists in providing analyticalvalues of the regularization parameter in order to approximately satisfyMorozov's discrepancy principle.
arxiv-4200-204 | Content Based Image Retrieval System using Feature Classification with Modified KNN Algorithm | http://arxiv.org/abs/1307.4717 | author:T. Dharani, I. Laurence Aroquiaraj category:cs.CV published:2013-07-17 summary:Feature means countenance, remote sensing scene objects with similarcharacteristics, associated to interesting scene elements in the imageformation process. They are classified into three types in image processing,that is low, middle and high. Low level features are color, texture and middlelevel feature is shape and high level feature is semantic gap of objects. Animage retrieval system is a computer system for browsing, searching andretrieving images from a large image database. Content Based Image Retrieval isa technique which uses visual features of image such as color, shape, textureto search user required image from large image database according to userrequests in the form of a query. MKNN is an enhancing method of KNN. Theproposed KNN classification is called MKNN. MKNN contains two parts forprocessing, they are validity of the train samples and applying weighted KNN.The validity of each point is computed according to its neighbors. In ourproposal, Modified K-Nearest Neighbor can be considered a kind of weighted KNNso that the query label is approximated by weighting the neighbors of thequery.
arxiv-4200-205 | Veni Vidi Vici, A Three-Phase Scenario For Parameter Space Analysis in Image Analysis and Visualization | http://arxiv.org/abs/1307.6544 | author:M. A. El-Dosuky category:cs.CV published:2013-07-17 summary:Automatic analysis of the enormous sets of images is a critical task in lifesciences. This faces many challenges such as: algorithms are highlyparameterized, significant human input is intertwined, and lacking a standardmeta-visualization approach. This paper proposes an alternative iterativeapproach for optimizing input parameters, saving time by minimizing the userinvolvement, and allowing for understanding the workflow of algorithms anddiscovering new ones. The main focus is on developing an interactivevisualization technique that enables users to analyze the relationships betweensampled input parameters and corresponding output. This technique isimplemented as a prototype called Veni Vidi Vici, or "I came, I saw, Iconquered." This strategy is inspired by the mathematical formulas of numberingcomputable functions and is developed atop ImageJ, a scientific imageprocessing program. A case study is presented to investigate the proposedframework. Finally, the paper explores some potential future issues in theapplication of the proposed approach in parameter space analysis invisualization.
arxiv-4200-206 | From Bandits to Experts: A Tale of Domination and Independence | http://arxiv.org/abs/1307.4564 | author:Noga Alon, NicolÃ² Cesa-Bianchi, Claudio Gentile, Yishay Mansour category:cs.LG stat.ML published:2013-07-17 summary:We consider the partial observability model for multi-armed bandits,introduced by Mannor and Shamir. Our main result is a characterization ofregret in the directed observability model in terms of the dominating andindependence numbers of the observability graph. We also show that in theundirected case, the learner can achieve optimal regret without even accessingthe observability graph before selecting an action. Both results are shownusing variants of the Exp3 algorithm operating on the observability graph in atime-efficient manner.
arxiv-4200-207 | A New Convex Relaxation for Tensor Completion | http://arxiv.org/abs/1307.4653 | author:Bernardino Romera-Paredes, Massimiliano Pontil category:cs.LG math.OC stat.ML published:2013-07-17 summary:We study the problem of learning a tensor from a set of linear measurements.A prominent methodology for this problem is based on a generalization of tracenorm regularization, which has been used extensively for learning low rankmatrices, to the tensor setting. In this paper, we highlight some limitationsof this approach and propose an alternative convex relaxation on the Euclideanball. We then describe a technique to solve the associated regularizationproblem, which builds upon the alternating direction method of multipliers.Experiments on one synthetic dataset and two real datasets indicate that theproposed method improves significantly over tensor trace norm regularization interms of estimation error, while remaining computationally tractable.
arxiv-4200-208 | Mammogram Edge Detection Using Hybrid Soft Computing Methods | http://arxiv.org/abs/1307.4516 | author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV published:2013-07-17 summary:Image segmentation is a crucial step in a wide range of method imageprocessing systems. It is useful in visualization of the different objectspresent in the image. In spite of the several methods available in theliterature, image segmentation still a challenging problem in most of imageprocessing applications. The challenge comes from the fuzziness of imageobjects and the overlapping of the different regions. Detection of edges in animage is a very important step towards understanding image features. There arelarge numbers of edge detection operators available, each designed to besensitive to certain types of edges. The Quality of edge detection can bemeasured from several criteria objectively. Some criteria are proposed in termsof mathematical measurement, some of them are based on application andimplementation requirements. Since edges often occur at image locationsrepresenting object boundaries, edge detection is extensively used in imagesegmentation when images are divided into areas corresponding to differentobjects. This can be used specifically for enhancing the tumor area inmammographic images. Different methods are available for edge detection likeRoberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novelalgorithms for edge detection has been proposed for mammographic images. Breastboundary, pectoral region and tumor location can be seen clearly by using thismethod. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edgeoperators are used and their results are displayed. Experimental resultsdemonstrate the effectiveness of the proposed approach.
arxiv-4200-209 | A Safe Screening Rule for Sparse Logistic Regression | http://arxiv.org/abs/1307.4145 | author:Jie Wang, Jiayu Zhou, Jun Liu, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2013-07-16 summary:The l1-regularized logistic regression (or sparse logistic regression) is awidely used method for simultaneous classification and feature selection.Although many recent efforts have been devoted to its efficient implementation,its application to high dimensional data still poses significant challenges. Inthis paper, we present a fast and effective sparse logistic regressionscreening rule (Slores) to identify the 0 components in the solution vector,which may lead to a substantial reduction in the number of features to beentered to the optimization. An appealing feature of Slores is that the dataset needs to be scanned only once to run the screening and its computationalcost is negligible compared to that of solving the sparse logistic regressionproblem. Moreover, Slores is independent of solvers for sparse logisticregression, thus Slores can be integrated with any existing solver to improvethe efficiency. We have evaluated Slores using high-dimensional data sets fromdifferent applications. Extensive experimental results demonstrate that Sloresoutperforms the existing state-of-the-art screening rules and the efficiency ofsolving sparse logistic regression is improved by one magnitude in general.
arxiv-4200-210 | Efficient Mixed-Norm Regularization: Algorithms and Safe Screening Methods | http://arxiv.org/abs/1307.4156 | author:Jie Wang, Jun Liu, Jieping Ye category:cs.LG stat.ML published:2013-07-16 summary:Sparse learning has recently received increasing attention in many areasincluding machine learning, statistics, and applied mathematics. The mixed-normregularization based on the l1q norm with q>1 is attractive in manyapplications of regression and classification in that it facilitates groupsparsity in the model. The resulting optimization problem is, however,challenging to solve due to the inherent structure of the mixed-normregularization. Existing work deals with special cases with q=1, 2, infinity,and they cannot be easily extended to the general case. In this paper, wepropose an efficient algorithm based on the accelerated gradient method forsolving the general l1q-regularized problem. One key building block of theproposed algorithm is the l1q-regularized Euclidean projection (EP_1q). Ourtheoretical analysis reveals the key properties of EP_1q and illustrates whyEP_1q for the general q is significantly more challenging to solve than thespecial cases. Based on our theoretical analysis, we develop an efficientalgorithm for EP_1q by solving two zero finding problems. To further improvethe efficiency of solving large dimensional mixed-norm regularized problems, wepropose a screening method which is able to quickly identify the inactivegroups, i.e., groups that have 0 components in the solution. This may lead tosubstantial reduction in the number of groups to be entered to theoptimization. An appealing feature of our screening method is that the data setneeds to be scanned only once to run the screening. Compared to that of solvingthe mixed-norm regularized problems, the computational cost of our screeningtest is negligible. The key of the proposed screening method is an accuratesensitivity analysis of the dual optimal solution when the regularizationparameter varies. Experimental results demonstrate the efficiency of theproposed algorithm.
arxiv-4200-211 | The Fitness Level Method with Tail Bounds | http://arxiv.org/abs/1307.4274 | author:Carsten Witt category:cs.NE published:2013-07-16 summary:The fitness-level method, also called the method of f-based partitions, is anintuitive and widely used technique for the running time analysis of randomizedsearch heuristics. It was originally defined to prove upper and lower bounds onthe expected running time. Recently, upper tail bounds were added to thetechnique; however, these tail bounds only apply to running times that are atleast twice as large as the expectation. We remove this restriction and supplement the fitness-level method with sharptail bounds, including lower tails. As an exemplary application, we prove thatthe running time of randomized local search on OneMax is sharply concentratedaround n ln n - 0.1159 n.
arxiv-4200-212 | A Brief Review of Nature-Inspired Algorithms for Optimization | http://arxiv.org/abs/1307.4186 | author:Iztok Fister Jr., Xin-She Yang, Iztok Fister, Janez Brest, DuÅ¡an Fister category:cs.NE published:2013-07-16 summary:Swarm intelligence and bio-inspired algorithms form a hot topic in thedevelopments of new algorithms inspired by nature. These nature-inspiredmetaheuristic algorithms can be based on swarm intelligence, biologicalsystems, physical and chemical systems. Therefore, these algorithms can becalled swarm-intelligence-based, bio-inspired, physics-based andchemistry-based, depending on the sources of inspiration. Though not all ofthem are efficient, a few algorithms have proved to be very efficient and thushave become popular tools for solving real-world problems. Some algorithms areinsufficiently studied. The purpose of this review is to present a relativelycomprehensive list of all the algorithms in the literature, so as to inspirefurther research.
arxiv-4200-213 | Rule Based Transliteration Scheme for English to Punjabi | http://arxiv.org/abs/1307.4300 | author:Deepti Bhalla, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-07-15 summary:Machine Transliteration has come out to be an emerging and a very importantresearch area in the field of machine translation. Transliteration basicallyaims to preserve the phonological structure of words. Proper transliteration ofname entities plays a very significant role in improving the quality of machinetranslation. In this paper we are doing machine transliteration forEnglish-Punjabi language pair using rule based approach. We have constructedsome rules for syllabification. Syllabification is the process to extract orseparate the syllable from the words. In this we are calculating theprobabilities for name entities (Proper names and location). For those wordswhich do not come under the category of name entities, separate probabilitiesare being calculated by using relative frequency through a statistical machinetranslation toolkit known as MOSES. Using these probabilities we aretransliterating our input text from English to Punjabi.
arxiv-4200-214 | Part of Speech Tagging of Marathi Text Using Trigram Method | http://arxiv.org/abs/1307.4299 | author:Jyoti Singh, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-07-15 summary:In this paper we present a Marathi part of speech tagger. It is amorphologically rich language. It is spoken by the native people ofMaharashtra. The general approach used for development of tagger is statisticalusing trigram Method. The main concept of trigram is to explore the most likelyPOS for a token based on given information of previous two tags by calculatingprobabilities to determine which is the best sequence of a tag. In this paperwe show the development of the tagger. Moreover we have also shown theevaluation done.
arxiv-4200-215 | On Soft Power Diagrams | http://arxiv.org/abs/1307.3949 | author:Steffen Borgwardt category:cs.LG math.OC stat.ML published:2013-07-15 summary:Many applications in data analysis begin with a set of points in a Euclideanspace that is partitioned into clusters. Common tasks then are to devise aclassifier deciding which of the clusters a new point is associated to, findingoutliers with respect to the clusters, or identifying the type of clusteringused for the partition. One of the common kinds of clusterings are (balanced) least-squaresassignments with respect to a given set of sites. For these, there is a'separating power diagram' for which each cluster lies in its own cell. In the present paper, we aim for efficient algorithms for outlier detectionand the computation of thresholds that measure how similar a clustering is to aleast-squares assignment for fixed sites. For this purpose, we devise a newmodel for the computation of a 'soft power diagram', which allows a softseparation of the clusters with 'point counting properties'; e.g. we are ableto prescribe how many points we want to classify as outliers. As our results hold for a more general non-convex model of free sites, wedescribe it and our proofs in this more general way. Its locally optimalsolutions satisfy the aforementioned point counting properties. For our targetapplications that use fixed sites, our algorithms are efficiently solvable toglobal optimality by linear programming.
arxiv-4200-216 | The Fundamental Learning Problem that Genetic Algorithms with Uniform Crossover Solve Efficiently and Repeatedly As Evolution Proceeds | http://arxiv.org/abs/1307.3824 | author:Keki M. Burjorjee category:cs.NE cs.AI cs.CC cs.DM cs.LG published:2013-07-15 summary:This paper establishes theoretical bonafides for implicit concurrentmultivariate effect evaluation--implicit concurrency for short---a broad andversatile computational learning efficiency thought to underliegeneral-purpose, non-local, noise-tolerant optimization in genetic algorithmswith uniform crossover (UGAs). We demonstrate that implicit concurrency isindeed a form of efficient learning by showing that it can be used to obtainclose-to-optimal bounds on the time and queries required to approximatelycorrectly solve a constrained version (k=7, \eta=1/5) of a recognizablecomputational learning problem: learning parities with noisy membershipqueries. We argue that a UGA that treats the noisy membership query oracle as afitness function can be straightforwardly used to approximately correctly learnthe essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time,where n is the total number of attributes. Our proof relies on an accessiblesymmetry argument and the use of statistical hypothesis testing to reject aglobal null hypothesis at the 10^-100 level of significance. It is, to the bestof our knowledge, the first relatively rigorous identification of efficientcomputational learning in an evolutionary algorithm on a non-trivial learningproblem.
arxiv-4200-217 | An alternative Gospel of structure: order, composition, processes | http://arxiv.org/abs/1307.4038 | author:Bob Coecke category:math.CT cs.CL quant-ph published:2013-07-15 summary:We survey some basic mathematical structures, which arguably are moreprimitive than the structures taught at school. These structures are orders,with or without composition, and (symmetric) monoidal categories. We listseveral `real life' incarnations of each of these. This paper also serves as anintroduction to these structures and their current and potentially future usesin linguistics, physics and knowledge representation.
arxiv-4200-218 | Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust Speech Recognition | http://arxiv.org/abs/1307.4048 | author:D. S. Pavan Kumar, N. Vishnu Prasad, Vikas Joshi, S. Umesh category:cs.LG cs.CV stat.ML published:2013-07-15 summary:In this paper, a modification to the training process of the popular SPLICEalgorithm has been proposed for noise robust speech recognition. Themodification is based on feature correlations, and enables this stereo-basedalgorithm to improve the performance in all noise conditions, especially inunseen cases. Further, the modified framework is extended to work fornon-stereo datasets where clean and noisy training utterances, but not stereocounterparts, are required. Finally, an MLLR-based computationally efficientrun-time noise adaptation method in SPLICE framework has been proposed. Themodified SPLICE shows 8.6% absolute improvement over SPLICE in Test C ofAurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93%absolute improvements over Aurora-2 and Aurora-4 baseline models respectively.Run-time adaptation shows 9.89% absolute improvement in modified framework ascompared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLRadaptation on HMMs.
arxiv-4200-219 | Multiview Hessian Discriminative Sparse Coding for Image Annotation | http://arxiv.org/abs/1307.3811 | author:Weifeng Liu, Dacheng Tao, Jun Cheng, Yuanyan Tang category:cs.MM cs.CV cs.IT math.IT published:2013-07-15 summary:Sparse coding represents a signal sparsely by using an overcompletedictionary, and obtains promising performance in practical computer visionapplications, especially for signal restoration tasks such as image denoisingand image inpainting. In recent years, many discriminative sparse codingalgorithms have been developed for classification problems, but they cannotnaturally handle visual data represented by multiview features. In addition,existing sparse coding algorithms use graph Laplacian to model the localgeometry of the data distribution. It has been identified that Laplacianregularization biases the solution towards a constant function which possiblyleads to poor extrapolating power. In this paper, we present multiview Hessiandiscriminative sparse coding (mHDSC) which seamlessly integrates Hessianregularization with discriminative sparse coding for multiview learningproblems. In particular, mHDSC exploits Hessian regularization to steer thesolution which varies smoothly along geodesics in the manifold, and treats thelabel information as an additional view of feature for incorporating thediscriminative power for image annotation. We conduct extensive experiments onPASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for imageannotation.
arxiv-4200-220 | Learning Markov networks with context-specific independences | http://arxiv.org/abs/1307.3964 | author:Alejandro Edera, Federico SchlÃ¼ter, Facundo Bromberg category:cs.AI cs.LG stat.ML published:2013-07-15 summary:Learning the Markov network structure from data is a problem that hasreceived considerable attention in machine learning, and in many otherapplication fields. This work focuses on a particular approach for this purposecalled independence-based learning. Such approach guarantees the learning ofthe correct structure efficiently, whenever data is sufficient for representingthe underlying distribution. However, an important issue of such approach isthat the learned structures are encoded in an undirected graph. The problemwith graphs is that they cannot encode some types of independence relations,such as the context-specific independences. They are a particular case ofconditional independences that is true only for a certain assignment of itsconditioning set, in contrast to conditional independences that must hold forall its assignments. In this work we present CSPC, an independence-basedalgorithm for learning structures that encode context-specific independences,and encoding them in a log-linear model, instead of a graph. The central ideaof CSPC is combining the theoretical guarantees provided by theindependence-based approach with the benefits of representing complexstructures by using features in a log-linear model. We present experiments in asynthetic case, showing that CSPC is more accurate than the state-of-the-art IBalgorithms when the underlying distribution contains CSIs.
arxiv-4200-221 | Bayesian Structured Prediction Using Gaussian Processes | http://arxiv.org/abs/1307.3846 | author:Sebastien Bratieres, Novi Quadrianto, Zoubin Ghahramani category:stat.ML cs.LG published:2013-07-15 summary:We introduce a conceptually novel structured prediction model, GPstruct,which is kernelized, non-parametric and Bayesian, by design. We motivate themodel with respect to existing approaches, among others, conditional randomfields (CRFs), maximum margin Markov networks (M3N), and structured supportvector machines (SVMstruct), which embody only a subset of its properties. Wepresent an inference procedure based on Markov Chain Monte Carlo. The frameworkcan be instantiated for a wide range of structured objects such as linearchains, trees, grids, and other general graphs. As a proof of concept, themodel is benchmarked on several natural language processing tasks and a videogesture segmentation task involving a linear chain structure. We showprediction accuracies for GPstruct which are comparable to or exceeding thoseof CRFs and SVMstruct.
arxiv-4200-222 | Probabilistic inverse reinforcement learning in unknown environments | http://arxiv.org/abs/1307.3785 | author:Aristide C. Y. Tossou, Christos Dimitrakakis category:stat.ML cs.LG published:2013-07-14 summary:We consider the problem of learning by demonstration from agents acting inunknown stochastic Markov environments or games. Our aim is to estimate agentpreferences in order to construct improved policies for the same task that theagents are trying to solve. To do so, we extend previous probabilisticapproaches for inverse reinforcement learning in known MDPs to the case ofunknown dynamics or opponents. We do this by deriving two simplifiedprobabilistic models of the demonstrator's policy and utility. Fortractability, we use maximum a posteriori estimation rather than full Bayesianinference. Under a flat prior, this results in a convex optimisation problem.We find that the resulting algorithms are highly competitive against a varietyof other methods for inverse reinforcement learning that do have knowledge ofthe dynamics.
arxiv-4200-223 | Handwritten Digits Recognition using Deep Convolutional Neural Network: An Experimental Study using EBlearn | http://arxiv.org/abs/1307.3782 | author:Karim M. Mahmoud category:cs.NE cs.CV published:2013-07-14 summary:In this paper, results of an experimental study of a deep convolution neuralnetwork architecture which can classify different handwritten digits usingEBLearn library are reported. The purpose of this neural network is to classifyinput images into 10 different classes or digits (0-9) and to explore newfindings. The input dataset used consists of digits images of size 32X32 ingrayscale (MNIST dataset).
arxiv-4200-224 | A Minimal Six-Point Auto-Calibration Algorithm | http://arxiv.org/abs/1307.3759 | author:Evgeniy Martyushev category:cs.CV published:2013-07-14 summary:A non-iterative auto-calibration algorithm is presented. It deals with aminimal set of six scene points in three views taken by a camera with fixed butunknown intrinsic parameters. Calibration is based on the image correspondencesonly. The algorithm is implemented and validated on synthetic image data.
arxiv-4200-225 | On Analyzing Estimation Errors due to Constrained Connections in Online Review Systems | http://arxiv.org/abs/1307.3687 | author:Junzhou Zhao category:cs.SI cs.LG published:2013-07-14 summary:Constrained connection is the phenomenon that a reviewer can only review asubset of products/services due to narrow range of interests or limitedattention capacity. In this work, we study how constrained connections canaffect estimation performance in online review systems (ORS). We find thatreviewers' constrained connections will cause poor estimation performance, bothfrom the measurements of estimation accuracy and Bayesian Cramer Rao lowerbound.
arxiv-4200-226 | Map of Life: Measuring and Visualizing Species' Relatedness with "Molecular Distance Maps" | http://arxiv.org/abs/1307.3755 | author:Lila Kari, Kathleen A. Hill, Abu Sadat Sayem, Nathaniel Bryans, Katelyn Davis, Nikesh S. Dattani category:q-bio.GN cs.CV q-bio.PE q-bio.QM 92, 68 published:2013-07-14 summary:We propose a novel combination of methods that (i) portrays quantitativecharacteristics of a DNA sequence as an image, (ii) computes distances betweenthese images, and (iii) uses these distances to output a map wherein eachsequence is a point in a common Euclidean space. In the resulting "MolecularDistance Map" each point signifies a DNA sequence, and the geometric distancebetween any two points reflects the degree of relatedness between thecorresponding sequences and species. Molecular Distance Maps present compelling visual representations ofrelationships between species and could be used for taxonomic clarifications,for species identification, and for studies of evolutionary history. One of theadvantages of this method is its general applicability since, as sequencealignment is not required, the DNA sequences chosen for comparison can becompletely different regions in different genomes. In fact, this method can beused to compare any two DNA sequences. For example, in our dataset of 3,176mitochondrial DNA sequences, it correctly finds the mtDNA sequences mostclosely related to that of the anatomically modern human (the Neanderthal, theDenisovan, and the chimp), and it finds that the sequence most different fromit belongs to a cucumber. Furthermore, our method can be used to compare realsequences to artificial, computer-generated, DNA sequences. For example, it isused to determine that the distances between a Homo sapiens sapiens mtDNA andartificial sequences of the same length and same trinucleotide frequencies canbe larger than the distance between the same human mtDNA and the mtDNA of afruit-fly. We demonstrate this method's promising potential for taxonomicalclarifications by applying it to a diverse variety of cases that have beenhistorically controversial, such as the genus Polypterus, the family Tarsiidae,and the vast (super)kingdom Protista.
arxiv-4200-227 | MCMC Learning | http://arxiv.org/abs/1307.3617 | author:Varun Kanade, Elchanan Mossel category:cs.LG stat.ML published:2013-07-13 summary:The theory of learning under the uniform distribution is rich and deep, withconnections to cryptography, computational complexity, and the analysis ofboolean functions to name a few areas. This theory however is very limited dueto the fact that the uniform distribution and the corresponding Fourier basisare rarely encountered as a statistical model. A family of distributions that vastly generalizes the uniform distribution onthe Boolean cube is that of distributions represented by Markov Random Fields(MRF). Markov Random Fields are one of the main tools for modeling highdimensional data in many areas of statistics and machine learning. In this paper we initiate the investigation of extending central ideas,methods and algorithms from the theory of learning under the uniformdistribution to the setup of learning concepts given examples from MRFdistributions. In particular, our results establish a novel connection betweenproperties of MCMC sampling of MRFs and learning under the MRF distribution.
arxiv-4200-228 | Minimum Error Rate Training and the Convex Hull Semiring | http://arxiv.org/abs/1307.3675 | author:Chris Dyer category:cs.LG I.2.6; I.2.7 published:2013-07-13 summary:We describe the line search used in the minimum error rate training algorithmMERT as the "inside score" of a weighted proof forest under a semiring definedin terms of well-understood operations from computational geometry. Thisconception leads to a straightforward complexity analysis of the dynamicprogramming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009)and practical approaches to implementation.
arxiv-4200-229 | A Data Management Approach for Dataset Selection Using Human Computation | http://arxiv.org/abs/1307.3673 | author:Alexandros Ntoulas, Omar Alonso, Vasilis Kandylas category:cs.LG cs.IR published:2013-07-13 summary:As the number of applications that use machine learning algorithms increases,the need for labeled data useful for training such algorithms intensifies. Getting labels typically involves employing humans to do the annotation,which directly translates to training and working costs. Crowdsourcingplatforms have made labeling cheaper and faster, but they still involvesignificant costs, especially for the cases where the potential set ofcandidate data to be labeled is large. In this paper we describe a methodologyand a prototype system aiming at addressing this challenge for Web-scaleproblems in an industrial setting. We discuss ideas on how to efficientlyselect the data to use for training of machine learning algorithms in anattempt to reduce cost. We show results achieving good performance with reducedcost by carefully selecting which instances to label. Our proposed algorithm ispresented as part of a framework for managing and generating training datasets,which includes, among other components, a human computation element.
arxiv-4200-230 | Fractionally-Supervised Classification | http://arxiv.org/abs/1307.3598 | author:Irene Vrbik, Paul D. McNicholas category:stat.ME stat.AP stat.CO stat.ML published:2013-07-13 summary:Traditionally, there are three species of classification: unsupervised,supervised, and semi-supervised. Supervised and semi-supervised classificationdiffer by whether or not weight is given to unlabelled observations in theclassification procedure. In unsupervised classification, or clustering, allobservations are unlabeled and hence full weight is given to unlabelledobservations. When some observations are unlabelled, it can be very difficultto \textit{a~priori} choose the optimal level of supervision, and theconsequences of a sub-optimal choice can be non-trivial. A flexiblefractionally-supervised approach to classification is introduced, where anylevel of supervision --- ranging from unsupervised to supervised --- can beattained. Our approach uses a weighted likelihood, wherein weights control therelative role that labelled and unlabelled data have in building a classifier.A comparison between our approach and the traditional species is presentedusing simulated and real data. Gaussian mixture models are used as a vehicle toillustrate our fractionally-supervised classification approach; however, it isbroadly applicable and variations on the postulated model can be easily made.
arxiv-4200-231 | Optimal Bounds on Approximation of Submodular and XOS Functions by Juntas | http://arxiv.org/abs/1307.3301 | author:Vitaly Feldman, Jan Vondrak category:cs.DS cs.CC cs.LG published:2013-07-12 summary:We investigate the approximability of several classes of real-valuedfunctions by functions of a small number of variables ({\em juntas}). Our mainresults are tight bounds on the number of variables required to approximate afunction $f:\{0,1\}^n \rightarrow [0,1]$ within $\ell_2$-error $\epsilon$ overthe uniform distribution: 1. If $f$ is submodular, then it is $\epsilon$-closeto a function of $O(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon})$ variables.This is an exponential improvement over previously known results. We note that$\Omega(\frac{1}{\epsilon^2})$ variables are necessary even for linearfunctions. 2. If $f$ is fractionally subadditive (XOS) it is $\epsilon$-closeto a function of $2^{O(1/\epsilon^2)}$ variables. This result holds for allfunctions with low total $\ell_1$-influence and is a real-valued analogue ofFriedgut's theorem for boolean functions. We show that $2^{\Omega(1/\epsilon)}$variables are necessary even for XOS functions. As applications of these results, we provide learning algorithms over theuniform distribution. For XOS functions, we give a PAC learning algorithm thatruns in time $2^{poly(1/\epsilon)} poly(n)$. For submodular functions we givean algorithm in the more demanding PMAC learning model (Balcan and Harvey,2011) which requires a multiplicative $1+\gamma$ factor approximation withprobability at least $1-\epsilon$ over the target distribution. Our uniformdistribution algorithm runs in time $2^{poly(1/(\gamma\epsilon))} poly(n)$.This is the first algorithm in the PMAC model that over the uniformdistribution can achieve a constant approximation factor arbitrarily close to 1for all submodular functions. As follows from the lower bounds in (Feldman etal., 2013) both of these algorithms are close to optimal. We also giveapplications for proper learning, testing and agnostic learning with valuequeries of these classes.
arxiv-4200-232 | Improving the quality of Gujarati-Hindi Machine Translation through part-of-speech tagging and stemmer-assisted transliteration | http://arxiv.org/abs/1307.3310 | author:Juhi Ameta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-07-12 summary:Machine Translation for Indian languages is an emerging research area.Transliteration is one such module that we design while designing a translationsystem. Transliteration means mapping of source language text into the targetlanguage. Simple mapping decreases the efficiency of overall translationsystem. We propose the use of stemming and part-of-speech tagging fortransliteration. The effectiveness of translation can be improved if we usepart-of-speech tagging and stemming assisted transliteration.We have shown thatmuch of the content in Gujarati gets transliterated while being processed fortranslation to Hindi language.
arxiv-4200-233 | Opinion Mining and Analysis: A survey | http://arxiv.org/abs/1307.3336 | author:Arti Buche, Dr. M. B. Chandak, Akshay Zadgaonkar category:cs.CL cs.IR published:2013-07-12 summary:The current research is focusing on the area of Opinion Mining also called assentiment analysis due to sheer volume of opinion rich web resources such asdiscussion forums, review sites and blogs are available in digital form. Oneimportant problem in sentiment analysis of product reviews is to producesummary of opinions based on product features. We have surveyed and analyzed inthis paper, various techniques that have been developed for the key tasks ofopinion mining. We have provided an overall picture of what is involved indeveloping a software system for opinion mining on the basis of our survey andanalysis.
arxiv-4200-234 | On-line Bayesian parameter estimation in general non-linear state-space models: A tutorial and new results | http://arxiv.org/abs/1307.3490 | author:Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni, J. Fraser Forbes category:stat.CO stat.AP stat.ME stat.ML published:2013-07-12 summary:On-line estimation plays an important role in process control and monitoring.Obtaining a theoretical solution to the simultaneous state-parameter estimationproblem for non-linear stochastic systems involves solving complexmulti-dimensional integrals that are not amenable to analytical solution. Whilebasic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms forsimultaneous estimation exist, it is well recognized that there is a need formaking these on-line algorithms non-degenerate, fast and applicable toprocesses with missing measurements. To overcome the deficiencies intraditional algorithms, this work proposes a Bayesian approach to on-line stateand parameter estimation. Its extension to handle missing data in real-time isalso provided. The simultaneous estimation is performed by filtering anextended vector of states and parameters using an adaptivesequential-importance-resampling (SIR) filter with a kernel density estimationmethod. The approach uses an on-line optimization algorithm based onKullback-Leibler (KL) divergence to allow adaptation of the SIR filter forcombined state-parameter estimation. An optimal tuning rule to control thewidth of the kernel and the variance of the artificial noise added to theparameters is also proposed. The approach is illustrated through numericalexamples.
arxiv-4200-235 | Unsupervised Gene Expression Data using Enhanced Clustering Method | http://arxiv.org/abs/1307.3337 | author:T. Chandrasekhar, K. Thangavel, E. Elayaraja, E. N. Sathishkumar category:cs.CE cs.LG published:2013-07-12 summary:Microarrays are made it possible to simultaneously monitor the expressionprofiles of thousands of genes under various experimental conditions.Identification of co-expressed genes and coherent patterns is the central goalin microarray or gene expression data analysis and is an important task inbioinformatics research. Feature selection is a process to select featureswhich are more informative. It is one of the important steps in knowledgediscovery. The problem is that not all features are important. Some of thefeatures may be redundant, and others may be irrelevant and noisy. In this workthe unsupervised Gene selection method and Enhanced Center InitializationAlgorithm (ECIA) with K-Means algorithms have been applied for clustering ofGene Expression Data. This proposed clustering algorithm overcomes thedrawbacks in terms of specifying the optimal number of clusters andinitialization of good cluster centroids. Gene Expression Data show that couldidentify compact clusters with performs well in terms of the SilhouetteCoefficients cluster measure.
arxiv-4200-236 | Energy-aware adaptive bi-Lipschitz embeddings | http://arxiv.org/abs/1307.3457 | author:Bubacarr Bah, Ali Sadeghian, Volkan Cevher category:cs.LG cs.IT math.IT 68Q99 published:2013-07-12 summary:We propose a dimensionality reducing matrix design based on training datawith constraints on its Frobenius norm and number of rows. Our design criteriais aimed at preserving the distances between the data points in thedimensionality reduced space as much as possible relative to their distances inoriginal data space. This approach can be considered as a deterministicBi-Lipschitz embedding of the data points. We introduce a scalable learningalgorithm, dubbed AMUSE, and provide a rigorous estimation guarantee byleveraging game theoretic tools. We also provide a generalizationcharacterization of our matrix based on our sample data. We use compressivesensing problems as an example application of our problem, where the Frobeniusnorm design constraint translates into the sensing energy.
arxiv-4200-237 | Speedy Object Detection based on Shape | http://arxiv.org/abs/1307.3439 | author:Y. Jayanta Singh, Shalu Gupta category:cs.CV published:2013-07-12 summary:This study is a part of design of an audio system for in-house objectdetection system for visually impaired, low vision personnel by birth or by anaccident or due to old age. The input of the system will be scene and output asaudio. Alert facility is provided based on severity levels of the objects(snake, broke glass etc) and also during difficulties. The study proposedtechniques to provide speedy detection of objects based on shapes and itsscale. Features are extraction to have minimum spaces using dynamic scaling.From a scene, clusters of objects are formed based on the scale and shape.Searching is performed among the clusters initially based on the shape, scale,mean cluster value and index of object(s). The minimum operation to detect thepossible shape of the object is performed. In case the object does not have alikely matching shape, scale etc, then the several operations required for anobject detection will not perform; instead, it will declared as a new object.In such way, this study finds a speedy way of detecting objects.
arxiv-4200-238 | Thompson Sampling for 1-Dimensional Exponential Family Bandits | http://arxiv.org/abs/1307.3400 | author:Nathaniel Korda, Emilie Kaufmann, Remi Munos category:stat.ML published:2013-07-12 summary:Thompson Sampling has been demonstrated in many complex bandit models,however the theoretical guarantees available for the parametric multi-armedbandit are still limited to the Bernoulli case. Here we extend them by provingasymptotic optimality of the algorithm using the Jeffreys prior for1-dimensional exponential family bandits. Our proof builds on previous work,but also makes extensive use of closed forms for Kullback-Leibler divergenceand Fisher information (and thus Jeffreys prior) available in an exponentialfamily. This allow us to give a finite time exponential concentrationinequality for posterior distributions on exponential families that may be ofinterest in its own right. Moreover our analysis covers some distributions forwhich no optimistic algorithm has yet been proposed, including heavy-tailedexponential families.
arxiv-4200-239 | Image color transfer to evoke different emotions based on color combinations | http://arxiv.org/abs/1307.3581 | author:Li He, Hairong Qi, Russell Zaretzki category:cs.CV cs.GR published:2013-07-12 summary:In this paper, a color transfer framework to evoke different emotions forimages based on color combinations is proposed. The purpose of this colortransfer is to change the "look and feel" of images, i.e., evoking differentemotions. Colors are confirmed as the most attractive factor in images. Inaddition, various studies in both art and science areas have concluded thatother than single color, color combinations are necessary to evoke specificemotions. Therefore, we propose a novel framework to transfer color of imagesbased on color combinations, using a predefined color emotion model. Thecontribution of this new framework is three-fold. First, users do not need toprovide reference images as used in traditional color transfer algorithms. Inmost situations, users may not have enough aesthetic knowledge or path tochoose desired reference images. Second, because of the usage of colorcombinations instead of single color for emotions, a new color transferalgorithm that does not require an image library is proposed. Third, againbecause of the usage of color combinations, artifacts that are normally seen intraditional frameworks using single color are avoided. We present encouragingresults generated from this new framework and its potential in several possibleapplications including color transfer of photos and paintings.
arxiv-4200-240 | Performance Analysis of Clustering Algorithms for Gene Expression Data | http://arxiv.org/abs/1307.3549 | author:T. Chandrasekhar, K. Thangavel, E. Elayaraja category:cs.CE cs.LG published:2013-07-12 summary:Microarray technology is a process that allows thousands of genessimultaneously monitor to various experimental conditions. It is used toidentify the co-expressed genes in specific cells or tissues that are activelyused to make proteins, This method is used to analysis the gene expression, animportant task in bioinformatics research. Cluster analysis of gene expressiondata has proved to be a useful tool for identifying co-expressed genes,biologically relevant groupings of genes and samples. In this paper we analysedK-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to groupthe microarray data sets on the basic of ISODATA. AGMFI is to generate initialvalues for merge and Spilt factor, maximum merge times instead of selectingefficient values as in ISODATA. The initial seeds for each cluster werenormally chosen either sequentially or randomly. The quality of the finalclusters was found to be influenced by these initial seeds. For the real lifeproblems, the suitable number of clusters cannot be predicted. To overcome theabove drawback the current research focused on developing the clusteringalgorithms without giving the initial number of clusters.
arxiv-4200-241 | Non-Elitist Genetic Algorithm as a Local Search Method | http://arxiv.org/abs/1307.3463 | author:Anton Eremeev category:cs.NE published:2013-07-12 summary:Sufficient conditions are found under which the iterated non-elitist geneticalgorithm with tournament selection first visits a local optimum inpolynomially bounded time on average. It is shown that these conditions aresatisfied on a class of problems with guaranteed local optima (GLO) ifappropriate parameters of the algorithm are chosen.
arxiv-4200-242 | A two-layer Conditional Random Field for the classification of partially occluded objects | http://arxiv.org/abs/1307.3043 | author:Sergey Kosov, Pushmeet Kohli, Franz Rottensteiner, Christian Heipke category:cs.CV published:2013-07-11 summary:Conditional Random Fields (CRF) are among the most popular techniques forimage labelling because of their flexibility in modelling dependencies betweenthe labels and the image features. This paper proposes a novel CRF-frameworkfor image labeling problems which is capable to classify partially occludedobjects. Our approach is evaluated on aerial near-vertical images as well as onurban street-view images and compared with another methods.
arxiv-4200-243 | Fast Exact Search in Hamming Space with Multi-Index Hashing | http://arxiv.org/abs/1307.2982 | author:Mohammad Norouzi, Ali Punjani, David J. Fleet category:cs.CV cs.AI cs.DS cs.IR published:2013-07-11 summary:There is growing interest in representing image data and feature descriptorsusing compact binary codes for fast near neighbor search. Although binary codesare motivated by their use as direct indices (addresses) into a hash table,codes longer than 32 bits are not being used as such, as it was thought to beineffective. We introduce a rigorous way to build multiple hash tables onbinary code substrings that enables exact k-nearest neighbor search in Hammingspace. The approach is storage efficient and straightforward to implement.Theoretical analysis shows that the algorithm exhibits sub-linear run-timebehavior for uniformly distributed codes. Empirical results show dramaticspeedups over a linear scan baseline for datasets of up to one billion codes of64, 128, or 256 bits.
arxiv-4200-244 | Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images | http://arxiv.org/abs/1307.2965 | author:Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer, Shaohua Kevin Zhou category:cs.CV published:2013-07-11 summary:The automatic segmentation of human knee cartilage from 3D MR images is auseful yet challenging task due to the thin sheet structure of the cartilagewith diffuse boundaries and inhomogeneous intensities. In this paper, wepresent an iterative multi-class learning method to segment the femoral, tibialand patellar cartilage simultaneously, which effectively exploits the spatialcontextual constraints between bone and cartilage, and also between differentcartilages. First, based on the fact that the cartilage grows in only certainarea of the corresponding bone surface, we extract the distance features of notonly to the surface of the bone, but more informatively, to the denselyregistered anatomical landmarks on the bone surface. Second, we introduce a setof iterative discriminative classifiers that at each iteration, probabilitycomparison features are constructed from the class confidence maps derived bypreviously learned classifiers. These features automatically embed the semanticcontext information between different cartilages of interest. Validated on atotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, theproposed approach demonstrates high robustness and accuracy of segmentation incomparison with existing state-of-the-art MR cartilage segmentation methods.
arxiv-4200-245 | Accuracy of MAP segmentation with hidden Potts and Markov mesh prior models via Path Constrained Viterbi Training, Iterated Conditional Modes and Graph Cut based algorithms | http://arxiv.org/abs/1307.2971 | author:Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez category:cs.LG cs.CV stat.ML published:2013-07-11 summary:In this paper, we study statistical classification accuracy of two differentMarkov field environments for pixelwise image segmentation, considering thelabels of the image as hidden states and solving the estimation of such labelsas a solution of the MAP equation. The emission distribution is assumed thesame in all models, and the difference lays in the Markovian prior hypothesismade over the labeling random field. The a priori labeling knowledge will bemodeled with a) a second order anisotropic Markov Mesh and b) a classicalisotropic Potts model. Under such models, we will consider three differentsegmentation procedures, 2D Path Constrained Viterbi training for the HiddenMarkov Mesh, a Graph Cut based segmentation for the first order isotropic Pottsmodel, and ICM (Iterated Conditional Modes) for the second order isotropicPotts model. We provide a unified view of all three methods, and investigate goodness offit for classification, studying the influence of parameter estimation,computational gain, and extent of automation in the statistical measuresOverall Accuracy, Relative Improvement and Kappa coefficient, allowing robustand accurate statistical analysis on synthetic and real-life experimental datacoming from the field of Dental Diagnostic Radiography. All algorithms, usingthe learned parameters, generate good segmentations with little interactionwhen the images have a clear multimodal histogram. Suboptimal learning provesto be frail in the case of non-distinctive modes, which limits the complexityof usable models, and hence the achievable error rate as well. All Matlab code written is provided in a toolbox available for download fromour website, following the Reproducible Research Paradigm.
arxiv-4200-246 | Conversion of Braille to Text in English, Hindi and Tamil Languages | http://arxiv.org/abs/1307.2997 | author:S. Padmavathi, Manojna K. S. S, S. Sphoorthy Reddy, D. Meenakshy category:cs.CV published:2013-07-11 summary:The Braille system has been used by the visually impaired for reading andwriting. Due to limited availability of the Braille text books an efficientusage of the books becomes a necessity. This paper proposes a method to converta scanned Braille document to text which can be read out to many through thecomputer. The Braille documents are pre processed to enhance the dots andreduce the noise. The Braille cells are segmented and the dots from each cellis extracted and converted in to a number sequence. These are mapped to theappropriate alphabets of the language. The converted text is spoken out througha speech synthesizer. The paper also provides a mechanism to type the Braillecharacters through the number pad of the keyboard. The typed Braille characteris mapped to the alphabet and spoken out. The Braille cell has a standardrepresentation but the mapping differs for each language. In this paper mappingof English, Hindi and Tamil are considered.
arxiv-4200-247 | A New Approach to the Solution of Economic Dispatch Using Particle Swarm Optimization with Simulated Annealing | http://arxiv.org/abs/1307.3014 | author:V. Karthikeyan, S. Senthilkumar, V. J. Vijayalakshmi category:cs.CE cs.NE published:2013-07-11 summary:A new approach to the solution of Economic Dispatch using Particle SwarmOptimization is presented. It is the progression of allocating productionamongst the dedicated units such that the restriction forced are fulfilled andthe power needs are reduced. More just, the soft computing method has receivedsupplementary concentration and was used in a quantity of successful andsensible applications. Here, an attempt has been made to find out the minimumcost by using Particle Swarm Optimization Algorithm using the data of threegenerating units. In this work, data has been taken such as the losscoefficients with the max-min power limit and cost function. PSO and SimulatedAnnealing are functional to put out the least amount for dissimilar energyrequirements. When the outputs are compared with the conventional method, PSOseems to give an improved result with enhanced convergence feature. All themethods are executed in MATLAB environment. The effectiveness and feasibilityof the proposed method were demonstrated by three generating units case study.Output gives hopeful results, signifying that the projected method ofcalculation is competent of economically formative advanced eminence solutionsaddressing economic dispatch problems.
arxiv-4200-248 | Statistical Active Learning Algorithms for Noise Tolerance and Differential Privacy | http://arxiv.org/abs/1307.3102 | author:Maria Florina Balcan, Vitaly Feldman category:cs.LG cs.DS stat.ML published:2013-07-11 summary:We describe a framework for designing efficient active learning algorithmsthat are tolerant to random classification noise and aredifferentially-private. The framework is based on active learning algorithmsthat are statistical in the sense that they rely on estimates of expectationsof functions of filtered random examples. It builds on the powerful statisticalquery framework of Kearns (1993). We show that any efficient active statistical learning algorithm can beautomatically converted to an efficient active learning algorithm which istolerant to random classification noise as well as other forms of"uncorrelated" noise. The complexity of the resulting algorithms hasinformation-theoretically optimal quadratic dependence on $1/(1-2\eta)$, where$\eta$ is the noise rate. We show that commonly studied concept classes including thresholds,rectangles, and linear separators can be efficiently actively learned in ourframework. These results combined with our generic conversion lead to the firstcomputationally-efficient algorithms for actively learning some of theseconcept classes in the presence of random classification noise that provideexponential improvement in the dependence on the error $\epsilon$ over theirpassive counterparts. In addition, we show that our algorithms can beautomatically converted to efficient active differentially-private algorithms.This leads to the first differentially-private active learning algorithms withexponential label savings over the passive case.
arxiv-4200-249 | Genetic approach for arabic part of speech tagging | http://arxiv.org/abs/1307.3489 | author:Bilel Ben Ali, Fethi Jarray category:cs.CL cs.NE 68T50 published:2013-07-11 summary:With the growing number of textual resources available, the ability tounderstand them becomes critical. An essential first step in understandingthese sources is the ability to identify the part of speech in each sentence.Arabic is a morphologically rich language, wich presents a challenge for partof speech tagging. In this paper, our goal is to propose, improve and implementa part of speech tagger based on a genetic alorithm. The accuracy obtained withthis method is comparable to that of other probabilistic approaches.
arxiv-4200-250 | Contrast Enhancement And Brightness Preservation Using Multi- Decomposition Histogram Equalization | http://arxiv.org/abs/1307.3054 | author:Sayali Nimkar, Sanal Varghese, Sucheta Shrivastava category:cs.CV published:2013-07-11 summary:Histogram Equalization (HE) has been an essential addition to the ImageEnhancement world. Enhancement techniques like Classical Histogram Equalization(CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE)and Recursive Mean Separate Histogram Equalization (RMSHE) methods enhancecontrast, however, brightness is not well preserved with these methods, whichgives an unpleasant look to the final image obtained. Thus, we introduce anovel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminatethe drawbacks of the earlier methods. In MDHE, we have decomposed the inputsixty-four parts, applied CHE in each of the sub-images and then finallyinterpolated them in correct order. The final image after MDHE results incontrast enhanced and brightness preserved image compared to all othertechniques mentioned above. We have calculated the various parameters likePSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supportedby bar graphs, histograms and the parameter calculations at the end.
arxiv-4200-251 | Minimum Distance Estimation for Robust High-Dimensional Regression | http://arxiv.org/abs/1307.3227 | author:AurÃ©lie C. Lozano, Nicolai Meinshausen category:stat.ME stat.ML published:2013-07-11 summary:We propose a minimum distance estimation method for robust regression insparse high-dimensional settings. The traditional likelihood-based estimatorslack resilience against outliers, a critical issue when dealing withhigh-dimensional noisy data. Our method, Minimum Distance Lasso (MD-Lasso),combines minimum distance functionals, customarily used in nonparametricestimation for their robustness, with l1-regularization for high-dimensionalregression. The geometry of MD-Lasso is key to its consistency and robustness.The estimator is governed by a scaling parameter that caps the influence ofoutliers: the loss per observation is locally convex and close to quadratic forsmall squared residuals, and flattens for squared residuals larger than thescaling parameter. As the parameter approaches infinity, the estimator becomesequivalent to least-squares Lasso. MD-Lasso enjoys fast convergence rates undermild conditions on the model error distribution, which hold for any of thesolutions in a convexity region around the true parameter and in certain casesfor every solution. Remarkably, a first-order optimization method is able toproduce iterates very close to the consistent solutions, with geometricconvergence and regardless of the initialization. A connection is establishedwith re-weighted least-squares that intuitively explains MD-Lasso robustness.The merits of our method are demonstrated through simulation and eQTL dataanalysis.
arxiv-4200-252 | Fuzzy Fibers: Uncertainty in dMRI Tractography | http://arxiv.org/abs/1307.3271 | author:Thomas Schultz, Anna Vilanova, Ralph Brecheisen, Gordon Kindlmann category:cs.CV published:2013-07-11 summary:Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)allows for noninvasive reconstruction of fiber bundles in the human brain. Inthis chapter, we discuss sources of error and uncertainty in this technique,and review strategies that afford a more reliable interpretation of theresults. This includes methods for computing and rendering probabilistictractograms, which estimate precision in the face of measurement noise andartifacts. However, we also address aspects that have received less attentionso far, such as model selection, partial voluming, and the impact ofparameters, both in preprocessing and in fiber tracking itself. We conclude bygiving impulses for future research.
arxiv-4200-253 | Fast gradient descent for drifting least squares regression, with application to bandits | http://arxiv.org/abs/1307.3176 | author:Nathaniel Korda, Prashanth L. A., RÃ©mi Munos category:cs.LG stat.ML published:2013-07-11 summary:Online learning algorithms require to often recompute least squaresregression estimates of parameters. We study improving the computationalcomplexity of such algorithms by using stochastic gradient descent (SGD) typeschemes in place of classic regression solvers. We show that SGD schemesefficiently track the true solutions of the regression problems, even in thepresence of a drift. This finding coupled with an $O(d)$ improvement incomplexity, where $d$ is the dimension of the data, make them attractive forimplementation in the big data settings. In the case when strong convexity inthe regression problem is guaranteed, we provide bounds on the error both inexpectation and high probability (the latter is often needed to providetheoretical guarantees for higher level algorithms), despite the drifting leastsquares solution. As an example of this case we prove that the regretperformance of an SGD version of the PEGE linear bandit algorithm[Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself onlyby a factor of $O(\log^4 n)$. When strong convexity of the regression problemcannot be guaranteed, we investigate using an adaptive regularisation. We makean empirical study of an adaptively regularised, SGD version of LinUCB [Li etal. 2010] in a news article recommendation application, which uses the largescale news recommendation dataset from Yahoo! front page. These experimentsshow a large gain in computational complexity, with a consistently low trackingerror and click-through-rate (CTR) performance that is $75\%$ close.
arxiv-4200-254 | Between Sense and Sensibility: Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems | http://arxiv.org/abs/1307.3040 | author:Mehul Bhatt category:cs.AI cs.CL cs.CV cs.HC cs.RO published:2013-07-11 summary:What lies between `\emph{sensing}' and `\emph{sensibility}'? In other words,what kind of cognitive processes mediate sensing capability, and the formationof sensible impressions ---e.g., abstractions, analogies, hypotheses and theoryformation, beliefs and their revision, argument formation--- in domain-specificproblem solving, or in regular activities of everyday living, working andsimply going around in the environment? How can knowledge and reasoning aboutsuch capabilities, as exhibited by humans in particular problem contexts, beused as a model and benchmark for the development of collaborative cognitive(interaction) systems concerned with human assistance, assurance, andempowerment? We pose these questions in the context of a range of assistive technologiesconcerned with \emph{visuo-spatial perception and cognition} tasks encompassingaspects such as commonsense, creativity, and the application of specialistdomain knowledge and problem-solving thought processes. Assistive technologiesbeing considered include: (a) human activity interpretation; (b) high-levelcognitive rovotics; (c) people-centred creative design in domains such asarchitecture & digital media creation, and (d) qualitative analyses geographicinformation systems. Computational narratives not only provide a rich cognitivebasis, but they also serve as a benchmark of functional performance in ourdevelopment of computational cognitive assistance systems. We posit thatcomputational narrativisation pertaining to space, actions, and change providesa useful model of \emph{visual} and \emph{spatio-temporal thinking} within awide-range of problem-solving tasks and application areas where collaborativecognitive systems could serve an assistive and empowering function.
arxiv-4200-255 | Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image Fusion | http://arxiv.org/abs/1307.2818 | author:Harbinder Singh, Vinay Kumar, Sunil Bhooshan category:cs.MM cs.CV published:2013-07-10 summary:We develop a multiexposure image fusion method based on texture features,which exploits the edge preserving and intraregion smoothing property ofnonlinear diffusion filters based on partial differential equations (PDE). Withthe captured multiexposure image series, we first decompose images into baselayers and detail layers to extract sharp details and fine details,respectively. The magnitude of the gradient of the image intensity is utilizedto encourage smoothness at homogeneous regions in preference to inhomogeneousregions. Then, we have considered texture features of the base layer togenerate a mask (i.e., decision mask) that guides the fusion of base layers inmultiresolution fashion. Finally, well-exposed fused image is obtained thatcombines fused base layer and the detail layers at each scale across all theinput exposures. Proposed algorithm skipping complex High Dynamic Range Image(HDRI) generation and tone mapping steps to produce detail preserving image fordisplay on standard dynamic range display devices. Moreover, our technique iseffective for blending flash/no-flash image pair and multifocus images, thatis, images focused on different targets.
arxiv-4200-256 | Flow-Based Algorithms for Local Graph Clustering | http://arxiv.org/abs/1307.2855 | author:Lorenzo Orecchia, Zeyuan Allen Zhu category:cs.DS cs.LG stat.ML published:2013-07-10 summary:Given a subset S of vertices of an undirected graph G, the cut-improvementproblem asks us to find a subset S that is similar to A but has smallerconductance. A very elegant algorithm for this problem has been given byAndersen and Lang [AL08] and requires solving a small number ofsingle-commodity maximum flow computations over the whole graph G. In thispaper, we introduce LocalImprove, the first cut-improvement algorithm that islocal, i.e. that runs in time dependent on the size of the input set A ratherthan on the size of the entire graph. Moreover, LocalImprove achieves thislocal behaviour while essentially matching the same theoretical guarantee asthe global algorithm of Andersen and Lang. The main application of LocalImprove is to the design of betterlocal-graph-partitioning algorithms. All previously known local algorithms forgraph partitioning are random-walk based and can only guarantee an outputconductance of O(\sqrt{OPT}) when the target set has conductance OPT \in [0,1].Very recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT /\sqrt{CONN}) where the internal connectivity parameter CONN \in [0,1] isdefined as the reciprocal of the mixing time of the random walk over the graphinduced by the target set. In this work, we show how to use LocalImprove toobtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). Thisyields the first flow-based algorithm. Moreover, its performance strictlyoutperforms the ones based on random walks and surprisingly matches that of thebest known global algorithm, which is SDP-based, in this parameter regime[MMV12]. Finally, our results show that spectral methods are not the only viableapproach to the construction of local graph partitioning algorithm and opendoor to the study of algorithms with even better approximation and localityguarantees.
arxiv-4200-257 | Error Rate Bounds in Crowdsourcing Models | http://arxiv.org/abs/1307.2674 | author:Hongwei Li, Bin Yu, Dengyong Zhou category:stat.ML cs.LG stat.AP published:2013-07-10 summary:Crowdsourcing is an effective tool for human-powered computation on manytasks challenging for computers. In this paper, we provide finite-sampleexponential bounds on the error rate (in probability and in expectation) ofhyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. Thebounds can be applied to analyze many common prediction methods, including themajority voting and weighted majority voting. These bound results could beuseful for controlling the error rate and designing better algorithms. We showthat the oracle Maximum A Posterior (MAP) rule approximately optimizes ourupper bound on the mean error rate for any hyperplane binary labeling rule, andpropose a simple data-driven weighted majority voting (WMV) rule (calledone-step WMV) that attempts to approximate the oracle MAP and has a provabletheoretical guarantee on the error rate. Moreover, we use simulated and realdata to demonstrate that the data-driven EM-MAP rule is a good approximation tothe oracle MAP rule, and to demonstrate that the mean error rate of thedata-driven EM-MAP rule is also bounded by the mean error rate bound of theoracle MAP rule with estimated parameters plugging into the bound.
arxiv-4200-258 | Optimisation dans la dÃ©tection de communautÃ©s recouvrantes et Ã©quilibre de Nash | http://arxiv.org/abs/1307.2715 | author:Michel Crampes, Michel PlantiÃ©, Marie Lopez category:stat.ML published:2013-07-10 summary:Community detection in graphs has been the subject of many algorithms. Recentmethods want to optimize a modularity function which shows a maximum ofrelationships within communities and found a minimum of inter-communityrelations. these algorithms are applied to unipartite, multipartite anddirected graphs. However, given the NP-completeness of the problem, thesealgorithms are heuristics that do not guarantee an optimum. In this paper weintroduce an algorithm which, based on an approximate solution obtained througha efficient detection algorithm, modifie it to achieve a local optimum based ona function. this reassignment function is a potential function and thereforethe computed optimum is a Nash equilibrium. We supplement our method with anoverlap function that allows to have simultaneously the two detection modes.Several experiments show the interest of our approach.
arxiv-4200-259 | Selection Mammogram Texture Descriptors Based on Statistics Properties Backpropagation Structure | http://arxiv.org/abs/1307.6542 | author:Shofwatul 'Uyun, Sri Hartati, Agus Harjoko, Subanar category:cs.CV published:2013-07-10 summary:Computer Aided Diagnosis (CAD) system has been developed for the earlydetection of breast cancer, one of the most deadly cancer for women. The benignof mammogram has different texture from malignant. There are fifty mammogramimages used in this work which are divided for training and testing. Therefore,the selection of the right texture to determine the level of accuracy of CADsystem is important. The first and second order statistics are the texturefeature extraction methods which can be used on a mammogram. This workclassifies texture descriptor into nine groups where the extraction of featuresis classified using backpropagation learning with two types of multi-layerperceptron (MLP). The best texture descriptor as selected when the value ofregression 1 appears in both the MLP-1 and the MLP-2 with the number of epochesless than 1000. The results of testing show that the best selected texturedescriptor is the second order (combination) using all direction (0, 45, 90 and135) that have twenty four descriptors.
arxiv-4200-260 | Controlling the Precision-Recall Tradeoff in Differential Dependency Network Analysis | http://arxiv.org/abs/1307.2611 | author:Diane Oyen, Alexandru Niculescu-Mizil, Rachel Ostroff, Alex Stewart, Vincent P. Clark category:stat.ML cs.LG published:2013-07-09 summary:Graphical models have gained a lot of attention recently as a tool forlearning and representing dependencies among variables in multivariate data.Often, domain scientists are looking specifically for differences among thedependency networks of different conditions or populations (e.g. differencesbetween regulatory networks of different species, or differences betweendependency networks of diseased versus healthy populations). The standardmethod for finding these differences is to learn the dependency networks foreach condition independently and compare them. We show that this approach isprone to high false discovery rates (low precision) that can render theanalysis useless. We then show that by imposing a bias towards learning similardependency networks for each condition the false discovery rates can be reducedto acceptable levels, at the cost of finding a reduced number of differences.Algorithms developed in the transfer learning literature can be used to varythe strength of the imposed similarity bias and provide a natural mechanism tosmoothly adjust this differential precision-recall tradeoff to cater to therequirements of the analysis conducted. We present real case studies(oncological and neurological) where domain experts use the proposed techniqueto extract useful differential networks that shed light on the biologicalprocesses involved in cancer and brain function.
arxiv-4200-261 | Tuned Models of Peer Assessment in MOOCs | http://arxiv.org/abs/1307.2579 | author:Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, Daphne Koller category:cs.LG cs.AI cs.HC stat.AP stat.ML published:2013-07-09 summary:In massive open online courses (MOOCs), peer grading serves as a criticaltool for scaling the grading of complex, open-ended assignments to courses withtens or hundreds of thousands of students. But despite promising initialtrials, it does not always deliver accurate results compared to human experts.In this paper, we develop algorithms for estimating and correcting for graderbiases and reliabilities, showing significant improvement in peer gradingaccuracy on real data with 63,199 peer grades from Coursera's HCI courseofferings --- the largest peer grading networks analysed to date. We relategrader biases and reliabilities to other student factors such as studentengagement, performance as well as commenting style. We also show that ourmodel can lead to more intelligent assignment of graders to gradees.
arxiv-4200-262 | General Drift Analysis with Tail Bounds | http://arxiv.org/abs/1307.2559 | author:Per Kristian Lehre, Carsten Witt category:cs.NE published:2013-07-09 summary:Drift analysis is one of the state-of-the-art techniques for the runtimeanalysis of randomized search heuristics. In recent years, many different drifttheorems, including additive, multiplicative and variable drift, have beendeveloped, applied and partly generalized or adapted to particular processes. Acomprehensive overview article was missing. We provide not only such an overview but also present a universal drifttheorem that generalizes virtually all existing drift theorems found in theliterature. On the one hand, the new theorem bounds the expected first hittingtime of optimal states in the underlying stochastic process. On the other hand,it also allows for general upper and lower tail bounds on the hitting time,which were not known before except for the special case of upper bounds inmultiplicative drift scenarios. As a proof of concept, the new tail bounds areapplied to prove very precise sharp-concentration results on the running timeof the (1+1) EA on OneMax, general linear functions and LeadingOnes. Moreover,user-friendly specializations of the general drift theorem are given.
arxiv-4200-263 | Image Fusion Technologies In Commercial Remote Sensing Packages | http://arxiv.org/abs/1307.2440 | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV published:2013-07-09 summary:Several remote sensing software packages are used to the explicit purpose ofanalyzing and visualizing remotely sensed data, with the developing of remotesensing sensor technologies from last ten years. Accord-ing to literature, theremote sensing is still the lack of software tools for effective informationextraction from remote sensing data. So, this paper provides a state-of-art ofmulti-sensor image fusion technologies as well as review on the qualityevaluation of the single image or fused images in the commercial remote sensingpack-ages. It also introduces program (ALwassaiProcess) developed for imagefusion and classification.
arxiv-4200-264 | Major Limitations of Satellite images | http://arxiv.org/abs/1307.2434 | author:Firouz A. Al-Wassai, N. V. Kalyankar category:cs.CV published:2013-07-09 summary:Remote sensing has proven to be a powerful tool for the monitoring of theEarth surface to improve our perception of our surroundings has led tounprecedented developments in sensor and information technologies. However,technologies for effective use of the data and for extracting usefulinformation from the data of Remote sensing are still very limited since nosingle sensor combines the optimal spectral, spatial and temporal resolution.This paper briefly reviews the limitations of satellite remote sensing. Also,reviews on the problems of image fusion techniques. The conclusion of this,According to literature, the remote sensing is still the lack of software toolsfor effective information extraction from remote sensing data. The trade-off inspectral and spatial resolution will remain and new advanced data fusionapproaches are needed to make optimal use of remote sensors for extract themost useful information.
arxiv-4200-265 | Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning | http://arxiv.org/abs/1307.2312 | author:Diane Oyen, Terran Lane category:stat.ML cs.LG published:2013-07-09 summary:Bayesian network structure learning algorithms with limited data are beingused in domains such as systems biology and neuroscience to gain insight intothe underlying processes that produce observed data. Learning reliable networksfrom limited data is difficult, therefore transfer learning can improve therobustness of learned networks by leveraging data from related tasks. Existingtransfer learning algorithms for Bayesian network structure learning give asingle maximum a posteriori estimate of network models. Yet, many other modelsmay be equally likely, and so a more informative result is provided by Bayesianstructure discovery. Bayesian structure discovery algorithms estimate posteriorprobabilities of structural features, such as edges. We present transferlearning for Bayesian structure discovery which allows us to explore the sharedand unique structural features among related tasks. Efficient computationrequires that our transfer learning objective factors into local calculations,which we prove is given by a broad class of transfer biases. Theoretically, weshow the efficiency of our approach. Empirically, we show that compared tosingle task learning, transfer learning is better able to positively identifytrue edges. We apply the method to whole-brain neuroimaging data.
arxiv-4200-266 | Transmodal Analysis of Neural Signals | http://arxiv.org/abs/1307.2150 | author:Yaroslav O. Halchenko, Michael Hanke, James V. Haxby, Stephen Jose Hanson, Christoph S. Herrmann category:q-bio.NC cs.LG q-bio.QM published:2013-07-08 summary:Localizing neuronal activity in the brain, both in time and in space, is acentral challenge to advance the understanding of brain function. Because ofthe inability of any single neuroimaging techniques to cover all aspects atonce, there is a growing interest to combine signals from multiple modalitiesin order to benefit from the advantages of each acquisition method. Due to thecomplexity and unknown parameterization of any suggested complete model of BOLDresponse in functional magnetic resonance imaging (fMRI), the development of areliable ultimate fusion approach remains difficult. But besides the primarygoal of superior temporal and spatial resolution, conjoint analysis of datafrom multiple imaging modalities can alternatively be used to segregate neuralinformation from physiological and acquisition noise. In this paper we suggesta novel methodology which relies on constructing a quantifiable mapping of datafrom one modality (electroencephalography; EEG) into another (fMRI), calledtransmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts tomap neural data embedded within the EEG signal into its reflection in fMRIdata. Assessing the mapping performance on unseen data allows to localize brainareas where a significant portion of the signal could be reliablyreconstructed, hence the areas neural activity of which is reflected in bothEEG and fMRI data. Consecutive analysis of the learnt model allows to localizeareas associated with specific frequency bands of EEG, or areas functionallyrelated (connected or coherent) to any given EEG sensor. We demonstrate theperformance of TRANSfusion on artificial and real data from an auditoryexperiment. We further speculate on possible alternative uses: cross-modal datafiltering and EEG-driven interpolation of fMRI signals to obtain arbitrarilyhigh temporal sampling of BOLD.
arxiv-4200-267 | A PAC-Bayesian Tutorial with A Dropout Bound | http://arxiv.org/abs/1307.2118 | author:David McAllester category:cs.LG published:2013-07-08 summary:This tutorial gives a concise overview of existing PAC-Bayesian theoryfocusing on three generalization bounds. The first is an Occam bound whichhandles rules with finite precision parameters and which states thatgeneralization loss is near training loss when the number of bits needed towrite the rule is small compared to the sample size. The second is aPAC-Bayesian bound providing a generalization guarantee for posteriordistributions rather than for individual rules. The PAC-Bayesian boundnaturally handles infinite precision rule parameters, $L_2$ regularization,{\em provides a bound for dropout training}, and defines a natural notion of asingle distinguished PAC-Bayesian posterior distribution. The third bound is atraining-variance bound --- a kind of bias-variance analysis but with biasreplaced by expected training loss. The training-variance bound dominates theother bounds but is more difficult to interpret. It seems to suggest variancereduction methods such as bagging and may ultimately provide a more meaningfulanalysis of dropouts.
arxiv-4200-268 | B-tests: Low Variance Kernel Two-Sample Tests | http://arxiv.org/abs/1307.1954 | author:Wojciech Zaremba, Arthur Gretton, Matthew Blaschko category:cs.LG stat.ML published:2013-07-08 summary:A family of maximum mean discrepancy (MMD) kernel two-sample tests isintroduced. Members of the test family are called Block-tests or B-tests, sincethe test statistic is an average over MMDs computed on subsets of the samples.The choice of block size allows control over the tradeoff between test powerand computation time. In this respect, the $B$-test family combines favorableproperties of previously proposed MMD two-sample tests: B-tests are morepowerful than a linear time test where blocks are just pairs of samples, yetthey are more computationally efficient than a quadratic time test where asingle large block incorporating all the samples is used to compute aU-statistic. A further important advantage of the B-tests is theirasymptotically Normal null distribution: this is by contrast with theU-statistic, which is degenerate under the null hypothesis, and for whichestimates of the null distribution are computationally demanding. Recentresults on kernel selection for hypothesis testing transfer seamlessly to theB-tests, yielding a means to optimize test power via kernel choice.
arxiv-4200-269 | Using Clustering to extract Personality Information from socio economic data | http://arxiv.org/abs/1307.1998 | author:Alexandros Ladas, Uwe Aickelin, Jon Garibaldi, Eamonn Ferguson category:cs.LG cs.CE published:2013-07-08 summary:It has become apparent that models that have been applied widely ineconomics, including Machine Learning techniques and Data Mining methods,should take into consideration principles that derive from the theories ofPersonality Psychology in order to discover more comprehensive knowledgeregarding complicated economic behaviours. In this work, we present a method toextract Behavioural Groups by using simple clustering techniques that canpotentially reveal aspects of the Personalities for their members. We believethat this is very important because the psychological information regarding thePersonalities of individuals is limited in real world applications and becauseit can become a useful tool in improving the traditional models of KnowledgeEconomy.
arxiv-4200-270 | The blessing of transitivity in sparse and stochastic networks | http://arxiv.org/abs/1307.2302 | author:Karl Rohe, Tai Qin category:stat.ML published:2013-07-08 summary:The interaction between transitivity and sparsity, two common features inempirical networks, implies that there are local regions of large sparsenetworks that are dense. We call this the blessing of transitivity and it hasconsequences for both modeling and inference. Extant research suggests thatstatistical inference for the Stochastic Blockmodel is more difficult when theedges are sparse. However, this conclusion is confounded by the fact that theasymptotic limit in all of the previous studies is not merely sparse, but alsonon-transitive. To retain transitivity, the blocks cannot grow faster than theexpected degree. Thus, in sparse models, the blocks must remain asymptoticallysmall. \n Previous algorithmic research demonstrates that small "local"clusters are more amenable to computation, visualization, and interpretationwhen compared to "global" graph partitions. This paper provides the firststatistical results that demonstrate how these small transitive clusters arealso more amenable to statistical estimation. Theorem 2 shows that a "local"clustering algorithm can, with high probability, detect a transitive stochasticblock of a fixed size (e.g. 30 nodes) embedded in a large graph. The onlyconstraint on the ambient graph is that it is large and sparse--it could begenerated at random or by an adversary--suggesting a theoretical explanationfor the robust empirical performance of local clustering algorithms.
arxiv-4200-271 | Finding the creatures of habit; Clustering households based on their flexibility in using electricity | http://arxiv.org/abs/1307.2111 | author:Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden category:cs.LG cs.CE published:2013-07-08 summary:Changes in the UK electricity market, particularly with the roll out of smartmeters, will provide greatly increased opportunities for initiatives intendedto change households' electricity usage patterns for the benefit of the overallsystem. Users show differences in their regular behaviours and clusteringhouseholds into similar groupings based on this variability provides forefficient targeting of initiatives. Those people who are stuck into a regularpattern of activity may be the least receptive to an initiative to changebehaviour. A sample of 180 households from the UK are clustered into fourgroups as an initial test of the concept and useful, actionable groupings arefound.
arxiv-4200-272 | Bridging Information Criteria and Parameter Shrinkage for Model Selection | http://arxiv.org/abs/1307.2307 | author:Kun Zhang, Heng Peng, Laiwan Chan, Aapo Hyvarinen category:stat.ML cs.LG published:2013-07-08 summary:Model selection based on classical information criteria, such as BIC, isgenerally computationally demanding, but its properties are well studied. Onthe other hand, model selection based on parameter shrinkage by $\ell_1$-typepenalties is computationally efficient. In this paper we make an attempt tocombine their strengths, and propose a simple approach that penalizes thelikelihood with data-dependent $\ell_1$ penalties as in adaptive Lasso andexploits a fixed penalization parameter. Even for finite samples, its modelselection results approximately coincide with those based on informationcriteria; in particular, we show that in some special cases, this approach andthe corresponding information criterion produce exactly the same model. One canalso consider this approach as a way to directly determine the penalizationparameter in adaptive Lasso to achieve information criteria-like modelselection. As extensions, we apply this idea to complex models includingGaussian mixture model and mixture of factor analyzers, whose model selectionis traditionally difficult to do; by adopting suitable penalties, we providecontinuous approximators to the corresponding information criteria, which areeasy to optimize and enable efficient model selection.
arxiv-4200-273 | Intelligent Hybrid Man-Machine Translation Quality Estimation | http://arxiv.org/abs/1307.1872 | author:Ibrahim Sabek, Noha A. Yousri, Nagwa Elmakky, Mona Habib category:cs.CL published:2013-07-07 summary:Inferring evaluation scores based on human judgments is invaluable comparedto using current evaluation metrics which are not suitable for real-timeapplications e.g. post-editing. However, these judgments are much moreexpensive to collect especially from expert translators, compared to evaluationbased on indicators contrasting source and translation texts. This workintroduces a novel approach for quality estimation by combining learntconfidence scores from a probabilistic inference model based on humanjudgments, with selective linguistic features-based scores, where the proposedinference model infers the credibility of given human ranks to solve thescarcity and inconsistency issues of human judgments. Experimental results,using challenging language-pairs, demonstrate improvement in correlation withhuman judgments over traditional evaluation metrics.
arxiv-4200-274 | Loss minimization and parameter estimation with heavy tails | http://arxiv.org/abs/1307.1827 | author:Daniel Hsu, Sivan Sabato category:cs.LG stat.ML published:2013-07-07 summary:This work studies applications and generalizations of a simple estimationtechnique that provides exponential concentration under heavy-taileddistributions, assuming only bounded low-order moments. We show that thetechnique can be used for approximate minimization of smooth and stronglyconvex losses, and specifically for least squares linear regression. Forinstance, our $d$-dimensional estimator requires just$\tilde{O}(d\log(1/\delta))$ random samples to obtain a constant factorapproximation to the optimal least squares loss with probability $1-\delta$,without requiring the covariates or noise to be bounded or subgaussian. Weprovide further applications to sparse linear regression and low-rankcovariance matrix estimation with similar allowances on the noise and covariatedistributions. The core technique is a generalization of the median-of-meansestimator to arbitrary metric spaces.
arxiv-4200-275 | Ensemble Methods for Multi-label Classification | http://arxiv.org/abs/1307.1769 | author:Lior Rokach, Alon Schclar, Ehud Itach category:stat.ML cs.LG 68T05, 68Q32 published:2013-07-06 summary:Ensemble methods have been shown to be an effective tool for solvingmulti-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,each member of the ensemble is associated with a small randomly-selected subsetof k labels. Then, a single label classifier is trained according to eachcombination of elements in the subset. In this paper we adopt a similarapproach, however, instead of randomly choosing subsets, we select the minimumrequired subsets of k labels that cover all labels and meet additionalconstraints such as coverage of inter-label correlations. Construction of thecover is achieved by formulating the subset selection as a minimum set coveringproblem (SCP) and solving it by using approximation algorithms. Every coverneeds only to be prepared once by offline algorithms. Once prepared, a covermay be applied to the classification of any given multi-label dataset whoseproperties conform with those of the cover. The contribution of this paper istwo-fold. First, we introduce SCP as a general framework for constructing labelcovers while allowing the user to incorporate cover construction constraints.We demonstrate the effectiveness of this framework by proposing twoconstruction constraints whose enforcement produces covers that improve theprediction performance of random selection. Second, we provide theoreticalbounds that quantify the probabilities of random selection to produce coversthat meet the proposed construction criteria. The experimental results indicatethat the proposed methods improve multi-label classification accuracy andstability compared with the RAKEL algorithm and to other state-of-the-artalgorithms.
arxiv-4200-276 | Anatomical Feature-guided Volumeric Registration of Multimodal Prostate MRI | http://arxiv.org/abs/1307.1739 | author:Xin Zhao, Arie Kaufman category:cs.CV cs.GR published:2013-07-06 summary:Radiological imaging of prostate is becoming more popular among researchersand clinicians in searching for diseases, primarily cancer. Scans might beacquired at different times, with patient movement between scans, or withdifferent equipment, resulting in multiple datasets that need to be registered.For this issue, we introduce a registration method using anatomicalfeature-guided mutual information. Prostate scans of the same patient taken inthree different orientations are first aligned for the accurate detection ofanatomical features in 3D. Then, our pipeline allows for multiple modalitiesregistration through the use of anatomical features, such as the interiorurethra of prostate and gland utricle, in a bijective way. The novelty of thisapproach is the application of anatomical features as the pre-specifiedcorresponding landmarks for prostate registration. We evaluate the registrationresults through both artificial and clinical datasets. Registration accuracy isevaluated by performing statistical analysis of local intensity differences orspatial differences of anatomical landmarks between various MR datasets.Evaluation results demonstrate that our method statistics-significantlyimproves the quality of registration. Although this strategy is tested forMRI-guided brachytherapy, the preliminary results from these experimentssuggest that it can be also applied to other settings such as transrectalultrasound-guided or CT-guided therapy, where the integration of preoperativeMRI may have a significant impact upon treatment planning and guidance.
arxiv-4200-277 | Approximate dynamic programming using fluid and diffusion approximations with applications to power management | http://arxiv.org/abs/1307.1759 | author:Wei Chen, Dayu Huang, Ankur A. Kulkarni, Jayakrishnan Unnikrishnan, Quanyan Zhu, Prashant Mehta, Sean Meyn, Adam Wierman category:cs.LG math.OC published:2013-07-06 summary:Neuro-dynamic programming is a class of powerful techniques for approximatingthe solution to dynamic programming equations. In their most computationallyattractive formulations, these techniques provide the approximate solution onlywithin a prescribed finite-dimensional function class. Thus, the question thatalways arises is how should the function class be chosen? The goal of thispaper is to propose an approach using the solutions to associated fluid anddiffusion approximations. In order to illustrate this approach, the paperfocuses on an application to dynamic speed scaling for power management incomputer processors.
arxiv-4200-278 | A Sub-block Based Image Retrieval Using Modified Integrated Region Matching | http://arxiv.org/abs/1307.1561 | author:E. R. Vimina, K. Poulose Jacob category:cs.IR cs.CV published:2013-07-05 summary:This paper proposes a content based image retrieval (CBIR) system using thelocal colour and texture features of selected image sub-blocks and globalcolour and shape features of the image. The image sub-blocks are roughlyidentified by segmenting the image into partitions of different configuration,finding the edge density in each partition using edge thresholding followed bymorphological dilation. The colour and texture features of the identifiedregions are computed from the histograms of the quantized HSV colour space andGray Level Co- occurrence Matrix (GLCM) respectively. The colour and texturefeature vectors is computed for each region. The shape features are computedfrom the Edge Histogram Descriptor (EHD). A modified Integrated Region Matching(IRM) algorithm is used for finding the minimum distance between the sub-blocksof the query and target image. Experimental results show that the proposedmethod provides better retrieving result than retrieval using some of theexisting methods.
arxiv-4200-279 | Comparing Data-mining Algorithms Developed for Longitudinal Observational Databases | http://arxiv.org/abs/1307.1584 | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE cs.DB published:2013-07-05 summary:Longitudinal observational databases have become a recent interest in thepost marketing drug surveillance community due to their ability of presenting anew perspective for detecting negative side effects. Algorithms mininglongitudinal observation databases are not restricted by many of thelimitations associated with the more conventional methods that have beendeveloped for spontaneous reporting system databases. In this paper weinvestigate the robustness of four recently developed algorithms that minelongitudinal observational databases by applying them to The Health ImprovementNetwork (THIN) for six drugs with well document known negative side effects.Our results show that none of the existing algorithms was able to consistentlyidentify known adverse drug reactions above events related to the cause of thedrug and no algorithm was superior.
arxiv-4200-280 | Supervised Learning and Anti-learning of Colorectal Cancer Classes and Survival Rates from Cellular Biology Parameters | http://arxiv.org/abs/1307.1599 | author:Chris Roadknight, Uwe Aickelin, Guoping Qiu, John Scholefield, Lindy Durrant category:cs.LG cs.CE stat.ML published:2013-07-05 summary:In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. Attempts are madeto learn relationships between attributes (physical and immunological) and theresulting tumour stage and survival. Results for conventional machine learningapproaches can be considered poor, especially for predicting tumour stages forthe most important types of cancer. This poor performance is furtherinvestigated and compared with a synthetic, dataset based on the logicalexclusive-OR function and it is shown that there is a significant level of'anti-learning' present in all supervised methods used and this can beexplained by the highly dimensional, complex and sparsely representativedataset. For predicting the stage of cancer from the immunological attributes,anti-learning approaches outperform a range of popular algorithms.
arxiv-4200-281 | Polyglot: Distributed Word Representations for Multilingual NLP | http://arxiv.org/abs/1307.1662 | author:Rami Al-Rfou, Bryan Perozzi, Steven Skiena category:cs.CL cs.LG published:2013-07-05 summary:Distributed word representations (word embeddings) have recently contributedto competitive performance in language modeling and several NLP tasks. In thiswork, we train word embeddings for more than 100 languages using theircorresponding Wikipedias. We quantitatively demonstrate the utility of our wordembeddings by using them as the sole features for training a part of speechtagger for a subset of these languages. We find their performance to becompetitive with near state-of-art methods in English, Danish and Swedish.Moreover, we investigate the semantic features captured by these embeddingsthrough the proximity of word groupings. We will release these embeddingspublicly to help researchers in the development and enhancement of multilingualapplications.
arxiv-4200-282 | Stochastic Optimization of PCA with Capped MSG | http://arxiv.org/abs/1307.1674 | author:Raman Arora, Andrew Cotter, Nathan Srebro category:stat.ML cs.LG published:2013-07-05 summary:We study PCA as a stochastic optimization problem and propose a novelstochastic approximation algorithm which we refer to as "Matrix StochasticGradient" (MSG), as well as a practical variant, Capped MSG. We study themethod both theoretically and empirically.
arxiv-4200-283 | Biomarker Clustering of Colorectal Cancer Data to Complement Clinical Classification | http://arxiv.org/abs/1307.1601 | author:Chris Roadknight, Uwe Aickelin, Alex Ladas, Daniele Soria, John Scholefield, Lindy Durrant category:cs.LG cs.CE published:2013-07-05 summary:In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. Attempts are madeto cluster this dataset and important subsets of it in an effort tocharacterize the data and validate existing standards for tumourclassification. It is apparent from optimal clustering that existing tumourclassification is largely unrelated to immunological factors within a patientand that there may be scope for re-evaluating treatment options and survivalestimates based on a combination of tumour physiology and patienthistochemistry.
arxiv-4200-284 | The Application of a Data Mining Framework to Energy Usage Profiling in Domestic Residences using UK data | http://arxiv.org/abs/1307.1380 | author:Ian Dent, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG stat.AP published:2013-07-04 summary:This paper describes a method for defining representative load profiles fordomestic electricity users in the UK. It considers bottom up and clusteringmethods and then details the research plans for implementing and improvingexisting framework approaches based on the overall usage profile. The workfocuses on adapting and applying analysis framework approaches to UK energydata in order to determine the effectiveness of creating a few (single figures)archetypical users with the intention of improving on the current methods ofdetermining usage profiles. The work is currently in progress and the paperdetails initial results using data collected in Milton Keynes around 1990.Various possible enhancements to the work are considered including a splitbased on temperature to reflect the varying UK weather conditions.
arxiv-4200-285 | AdaBoost and Forward Stagewise Regression are First-Order Convex Optimization Methods | http://arxiv.org/abs/1307.1192 | author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:stat.ML cs.LG math.OC published:2013-07-04 summary:Boosting methods are highly popular and effective supervised learning methodswhich combine weak learners into a single accurate model with good statisticalperformance. In this paper, we analyze two well-known boosting methods,AdaBoost and Incremental Forward Stagewise Regression (FS$_\varepsilon$), byestablishing their precise connections to the Mirror Descent algorithm, whichis a first-order method in convex optimization. As a consequence of theseconnections we obtain novel computational guarantees for these boostingmethods. In particular, we characterize convergence bounds of AdaBoost, relatedto both the margin and log-exponential loss function, for any step-sizesequence. Furthermore, this paper presents, for the first time, precisecomputational complexity results for FS$_\varepsilon$.
arxiv-4200-286 | Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice | http://arxiv.org/abs/1307.1275 | author:Fangxiang Feng, Ruifan Li, Xiaojie Wang category:cs.LG cs.NE published:2013-07-04 summary:This paper describes our solution to the multi-modal learning challenge ofICML. This solution comprises constructing three-level representations in threeconsecutive stages and choosing correct tag words with a data-specificstrategy. Firstly, we use typical methods to obtain level-1 representations.Each image is represented using MPEG-7 and gist descriptors with additionalfeatures released by the contest organizers. And the corresponding word tagsare represented by bag-of-words model with a dictionary of 4000 words.Secondly, we learn the level-2 representations using two stacked RBMs for eachmodality. Thirdly, we propose a bimodal auto-encoder to learn thesimilarities/dissimilarities between the pairwise image-tags as level-3representations. Finally, during the test phase, based on one observation ofthe dataset, we come up with a data-specific strategy to choose the correct tagwords leading to a leap of an improved overall performance. Our final averageaccuracy on the private test set is 100%, which ranks the first place in thischallenge.
arxiv-4200-287 | Further results on dissimilarity spaces for hyperspectral images RF-CBIR | http://arxiv.org/abs/1307.1289 | author:Miguel Angel Veganzones, Mihai Datcu, Manuel GraÃ±a category:cs.IR cs.CV published:2013-07-04 summary:Content-Based Image Retrieval (CBIR) systems are powerful search tools inimage databases that have been little applied to hyperspectral images.Relevance feedback (RF) is an iterative process that uses machine learningtechniques and user's feedback to improve the CBIR systems performance. Wepursued to expand previous research in hyperspectral CBIR systems built ondissimilarity functions defined either on spectral and spatial featuresextracted by spectral unmixing techniques, or on dictionaries extracted bydictionary-based compressors. These dissimilarity functions were not suitablefor direct application in common machine learning techniques. We propose to usea RF general approach based on dissimilarity spaces which is more appropriatefor the application of machine learning algorithms to the hyperspectralRF-CBIR. We validate the proposed RF method for hyperspectral CBIR systems overa real hyperspectral dataset.
arxiv-4200-288 | Creating Personalised Energy Plans. From Groups to Individuals using Fuzzy C Means Clustering | http://arxiv.org/abs/1307.1385 | author:Ian Dent, Christian Wagner, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG published:2013-07-04 summary:Changes in the UK electricity market mean that domestic users will berequired to modify their usage behaviour in order that supplies can bemaintained. Clustering allows usage profiles collected at the household levelto be clustered into groups and assigned a stereotypical profile which can beused to target marketing campaigns. Fuzzy C Means clustering extends this byallowing each household to be a member of many groups and hence provides theopportunity to make personalised offers to the household dependent on theirdegree of membership of each group. In addition, feedback can be provided onhow user's changing behaviour is moving them towards more "green" or costeffective stereotypical usage.
arxiv-4200-289 | Clustering of Complex Networks and Community Detection Using Group Search Optimization | http://arxiv.org/abs/1307.1372 | author:G. Kishore Kumar, V. K. Jayaraman category:cs.NE cs.DS published:2013-07-04 summary:Group Search Optimizer(GSO) is one of the best algorithms, is very new in thefield of Evolutionary Computing. It is very robust and efficient algorithm,which is inspired by animal searching behaviour. The paper describes anapplication of GSO to clustering of networks. We have tested GSO against fivestandard benchmark datasets, GSO algorithm is proved very competitive in termsof accuracy and convergence speed.
arxiv-4200-290 | Examining the Classification Accuracy of TSVMs with ?Feature Selection in Comparison with the GLAD Algorithm | http://arxiv.org/abs/1307.1387 | author:Hala Helmi, Jon M. Garibaldi, Uwe Aickelin category:cs.LG cs.CE published:2013-07-04 summary:Gene expression data sets are used to classify and predict patient diagnosticcategories. As we know, it is extremely difficult and expensive to obtain geneexpression labelled examples. Moreover, conventional supervised approachescannot function properly when labelled data (training examples) areinsufficient using Support Vector Machines (SVM) algorithms. Therefore, in thispaper, we suggest Transductive Support Vector Machines (TSVMs) assemi-supervised learning algorithms, learning with both labelled samples dataand unlabelled samples to perform the classification of microarray data. Toprune the superfluous genes and samples we used a feature selection methodcalled Recursive Feature Elimination (RFE), which is supposed to enhance theoutput of classification and avoid the local optimization problem. We examinedthe classification prediction accuracy of the TSVM-RFE algorithm in comparisonwith the Genetic Learning Across Datasets (GLAD) algorithm, as both aresemi-supervised learning methods. Comparing these two methods, we found thatthe TSVM-RFE surpassed both a SVM using RFE and GLAD.
arxiv-4200-291 | Quiet in Class: Classification, Noise and the Dendritic Cell Algorithm | http://arxiv.org/abs/1307.1391 | author:Feng Gu, Jan Feyereisl, Robert Oates, Jenna Reps, Julie Greensmith, Uwe Aickelin category:cs.LG cs.CR published:2013-07-04 summary:Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yieldedseveral criticisms about its underlying structure and operation. As a result,several alterations and fixes have been suggested in the literature to correctfor these findings. A contribution of this work is to investigate the effectsof replacing the classification stage of the DCA (which is known to be flawed)with a traditional machine learning technique. This work goes on to questionthe merits of those unique properties of the DCA that are yet to be thoroughlyanalysed. If none of these properties can be found to have a benefit overtraditional approaches, then "fixing" the DCA is arguably less efficient thansimply creating a new algorithm. This work examines the dynamic filteringproperty of the DCA and questions the utility of this unique feature for theanomaly detection problem. It is found that this feature, while advantageousfor noisy, time-ordered classification, is not as useful as a traditionalstatic filter for processing a synthetic dataset. It is concluded that thereare still unique features of the DCA left to investigate. Areas that may be ofbenefit to the Artificial Immune Systems community are suggested.
arxiv-4200-292 | Dropout Training as Adaptive Regularization | http://arxiv.org/abs/1307.1493 | author:Stefan Wager, Sida Wang, Percy Liang category:stat.ML cs.LG stat.ME published:2013-07-04 summary:Dropout and other feature noising schemes control overfitting by artificiallycorrupting the training data. For generalized linear models, dropout performs aform of adaptive regularization. Using this viewpoint, we show that the dropoutregularizer is first-order equivalent to an L2 regularizer applied afterscaling the features by an estimate of the inverse diagonal Fisher informationmatrix. We also establish a connection to AdaGrad, an online learningalgorithm, and find that a close relative of AdaGrad operates by repeatedlysolving linear dropout-regularized problems. By casting dropout asregularization, we develop a natural semi-supervised algorithm that usesunlabeled data to create a better adaptive regularizer. We apply this idea todocument classification tasks, and show that it consistently boosts theperformance of dropout training, improving on state-of-the-art results on theIMDB reviews dataset.
arxiv-4200-293 | Detect adverse drug reactions for drug Alendronate | http://arxiv.org/abs/1307.1394 | author:Yihui Liu, Uwe Aickelin category:cs.CE cs.LG published:2013-07-04 summary:Adverse drug reaction (ADR) is widely concerned for public health issue. Inthis study we propose an original approach to detect the ADRs using featurematrix and feature selection. The experiments are carried out on the drugSimvastatin. Major side effects for the drug are detected and betterperformance is achieved compared to other computerized methods. The detectedADRs are based on the computerized method, further investigation is needed.
arxiv-4200-294 | Toward Guaranteed Illumination Models for Non-Convex Objects | http://arxiv.org/abs/1307.1437 | author:Yuqian Zhang, Cun Mu, Han-wen Kuo, John Wright category:cs.CV published:2013-07-04 summary:Illumination variation remains a central challenge in object detection andrecognition. Existing analyses of illumination variation typically pertain toconvex, Lambertian objects, and guarantee quality of approximation in anaverage case sense. We show that it is possible to build V(vertex)-descriptionconvex cone models with worst-case performance guarantees, for non-convexLambertian objects. Namely, a natural verification test based on the angle tothe constructed cone guarantees to accept any image which is sufficientlywell-approximated by an image of the object under some admissible lightingcondition, and guarantees to reject any image that does not have a sufficientlygood approximation. The cone models are generated by sampling pointilluminations with sufficient density, which follows from a new perturbationbound for point images in the Lambertian model. As the number of point imagesrequired for guaranteed verification may be large, we introduce a newformulation for cone preserving dimensionality reduction, which leverages toolsfrom sparse and low-rank decomposition to reduce the complexity, whilecontrolling the approximation error with respect to the original cone.
arxiv-4200-295 | Discovering Sequential Patterns in a UK General Practice Database | http://arxiv.org/abs/1307.1411 | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE stat.AP published:2013-07-04 summary:The wealth of computerised medical information becoming readily availablepresents the opportunity to examine patterns of illnesses, therapies andresponses. These patterns may be able to predict illnesses that a patient islikely to develop, allowing the implementation of preventative actions. In thispaper sequential rule mining is applied to a General Practice database to findrules involving a patients age, gender and medical history. By incorporatingthese rules into current health-care a patient can be highlighted assusceptible to a future illness based on past or current illnesses, gender andyear of birth. This knowledge has the ability to greatly improve health-careand reduce health-care costs.
arxiv-4200-296 | Separation of cardiac and respiratory components from the electrical bio-impedance signal using PCA and fast ICA | http://arxiv.org/abs/1307.0915 | author:Yar M. Mughal, A. Krivoshei, P. Annus category:stat.AP stat.ML published:2013-07-03 summary:This paper is an attempt to separate cardiac and respiratory signals from anelectrical bio-impedance (EBI) dataset. For this two well-known algorithms,namely Principal Component Analysis (PCA) and Independent Component Analysis(ICA), were used to accomplish the task. The ability of the PCA and the ICAmethods first reduces the dimension and attempt to separate the usefulcomponents of the EBI, the cardiac and respiratory ones accordingly. It wasinvestigated with an assumption, that no motion artefacts are present. To carryout this procedure the two channel complex EBI measurements were provided usingclassical Kelvin type four electrode configurations for the each complexchannel. Thus four real signals were used as inputs for the PCA and fast ICA.The results showed, that neither PCA nor ICA nor combination of them can notaccurately separate the components at least are used only two complex (fourreal valued) input components.
arxiv-4200-297 | A Novel Robust Method to Add Watermarks to Bitmap Images by Fading Technique | http://arxiv.org/abs/1307.1166 | author:Firas A. Jassim category:cs.CV cs.MM published:2013-07-03 summary:Digital water marking is one of the essential fields in image security andcopyright protection. The proposed technique in this paper was based on theprinciple of protecting images by hide an invisible watermark in the image. Thetechnique starts with merging the cover image and the watermark image withsuitable ratios, i.e., 99% from the cover image will be merged with 1% from thewatermark image. Technically, the fading process is irreversible but with theproposed technique, the probability to reconstruct the original watermark imageis great. There is no perceptible difference between the original andwatermarked image by human eye. The experimental results show that the proposedtechnique proven its ability to hide images that have the same size of thecover image. Three performance measures were implemented to support theproposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the threemeasures have excellent values.
arxiv-4200-298 | On the minimal teaching sets of two-dimensional threshold functions | http://arxiv.org/abs/1307.1058 | author:Max A. Alekseyev, Marina G. Basova, Nikolai Yu. Zolotykh category:math.CO cs.LG math.NT published:2013-07-03 summary:It is known that a minimal teaching set of any threshold function on thetwodimensional rectangular grid consists of 3 or 4 points. We derive exactformulae for the numbers of functions corresponding to these values and furtherrefine them in the case of a minimal teaching set of size 3. We also prove thatthe average cardinality of the minimal teaching sets of threshold functions isasymptotically 7/2. We further present corollaries of these results concerning some specialarrangements of lines in the plane.
arxiv-4200-299 | Extending UML for Conceptual Modeling of Annotation of Medical Images | http://arxiv.org/abs/1307.0937 | author:Mouhamed Gaith Ayadi, Riadh Bouslimi, Jalel Akaichi category:cs.CV published:2013-07-03 summary:Imaging has occupied a huge role in the management of patients, whetherhospitalized or not. Depending on the patients clinical problem, a variety ofimaging modalities were available for use. This gave birth of the annotation ofmedical image process. The annotation is intended to image analysis and solvethe problem of semantic gap. The reason for image annotation is due to increasein acquisition of images. Physicians and radiologists feel better while usingannotation techniques for faster remedy in surgery and medicine due to thefollowing reasons: giving details to the patients, searching the present andpast records from the larger databases, and giving solutions to them in afaster and more accurate way. However, classical conceptual modeling does notincorporate the specificity of medical domain specially the annotation ofmedical image. The design phase is the most important activity in thesuccessful building of annotation process. For this reason, we focus in thispaper on presenting the conceptual modeling of the annotation of medical imageby defining a new profile using the StarUML extensibility mechanism.
arxiv-4200-300 | A Unified Framework of Elementary Geometric Transformation Representation | http://arxiv.org/abs/1307.0998 | author:F. Lu, Z. Chen category:cs.CV published:2013-07-03 summary:As an extension of projective homology, stereohomology is proposed via anextension of Desargues theorem and the extended Desargues configuration.Geometric transformations such as reflection, translation, central symmetry,central projection, parallel projection, shearing, central dilation, scaling,and so on are all included in stereohomology and represented asHouseholder-Chen elementary matrices. Hence all these geometric transformationsare called elementary. This makes it possible to represent these elementarygeometric transformations in homogeneous square matrices independent of aparticular choice of coordinate system.
