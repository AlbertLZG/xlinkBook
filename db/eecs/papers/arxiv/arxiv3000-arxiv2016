arxiv-3000-1 | On Considering Uncertainty and Alternatives in Low-Level Vision | http://arxiv.org/pdf/1303.1460v1.pdf | author:Steven M. LaValle, Seth A. Hutchinson category:cs.AI cs.CV published:2013-03-06 summary:In this paper we address the uncertainty issues involved in the low-levelvision task of image segmentation. Researchers in computer vision have workedextensively on this problem, in which the goal is to partition (or segment) animage into regions that are homogeneous or uniform in some sense. Thissegmentation is often utilized by some higher level process, such as an objectrecognition system. We show that by considering uncertainty in a Bayesianformalism, we can use statistical image models to build an approximaterepresentation of a probability distribution over a space of alternativesegmentations. We give detailed descriptions of the various levels ofuncertainty associated with this problem, discuss the interaction of prior andposterior distributions, and provide the operations for constructing thisrepresentation.
arxiv-3000-2 | A Hybrid Approach to Extract Keyphrases from Medical Documents | http://arxiv.org/pdf/1303.1441v2.pdf | author:Kamal Sarkar category:cs.IR cs.CL published:2013-03-06 summary:Keyphrases are the phrases, consisting of one or more words, representing theimportant concepts in the articles. Keyphrases are useful for a variety oftasks such as text summarization, automatic indexing,clustering/classification, text mining etc. This paper presents a hybridapproach to keyphrase extraction from medical documents. The keyphraseextraction approach presented in this paper is an amalgamation of two methods:the first one assigns weights to candidate keyphrases based on an effectivecombination of features such as position, term frequency, inverse documentfrequency and the second one assign weights to candidate keyphrases using someknowledge about their similarities to the structure and characteristics ofkeyphrases available in the memory (stored list of keyphrases). An efficientcandidate keyphrase identification method as the first component of theproposed keyphrase extraction system has also been introduced in this paper.The experimental results show that the proposed hybrid approach performs betterthan some state-of-the art keyphrase extraction approaches.
arxiv-3000-3 | Convex and Scalable Weakly Labeled SVMs | http://arxiv.org/pdf/1303.1271v5.pdf | author:Yu-Feng Li, Ivor W. Tsang, James T. Kwok, Zhi-Hua Zhou category:cs.LG published:2013-03-06 summary:In this paper, we study the problem of learning from weakly labeled data,where labels of the training examples are incomplete. This includes, forexample, (i) semi-supervised learning where labels are partially known; (ii)multi-instance learning where labels are implicitly known; and (iii) clusteringwhere labels are completely unknown. Unlike supervised learning, learning withweak labels involves a difficult Mixed-Integer Programming (MIP) problem.Therefore, it can suffer from poor scalability and may also get stuck in localminimum. In this paper, we focus on SVMs and propose the WellSVM via a novellabel generation strategy. This leads to a convex relaxation of the originalMIP, which is at least as tight as existing convex Semi-Definite Programming(SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVMsubproblems that are much more scalable than previous convex SDP relaxations.Experiments on three weakly labeled learning tasks, namely, (i) semi-supervisedlearning; (ii) multi-instance learning for locating regions of interest incontent-based information retrieval; and (iii) clustering, clearly demonstrateimproved performance, and WellSVM is also readily applicable on large datasets.
arxiv-3000-4 | Japanese-Spanish Thesaurus Construction Using English as a Pivot | http://arxiv.org/pdf/1303.1232v1.pdf | author:Jessica Ramírez, Masayuki Asahara, Yuji Matsumoto category:cs.CL cs.AI published:2013-03-06 summary:We present the results of research with the goal of automatically creating amultilingual thesaurus based on the freely available resources of Wikipedia andWordNet. Our goal is to increase resources for natural language processingtasks such as machine translation targeting the Japanese-Spanish language pair.Given the scarcity of resources, we use existing English resources as a pivotfor creating a trilingual Japanese-Spanish-English thesaurus. Our approachconsists of extracting the translation tuples from Wikipedia, disambiguatingthem by mapping them to WordNet word senses. We present results comparing twomethods of disambiguation, the first using VSM on Wikipedia article texts andWordNet definitions, and the second using categorical information extractedfrom Wikipedia, We find that mixing the two methods produces favorable results.Using the proposed method, we have constructed a multilingualSpanish-Japanese-English thesaurus consisting of 25,375 entries. The samemethod can be applied to any pair of languages that are linked to English inWikipedia.
arxiv-3000-5 | GURLS: a Least Squares Library for Supervised Learning | http://arxiv.org/pdf/1303.0934v1.pdf | author:Andrea Tacchetti, Pavan K Mallapragada, Matteo Santoro, Lorenzo Rosasco category:cs.LG cs.AI cs.MS published:2013-03-05 summary:We present GURLS, a least squares, modular, easy-to-extend software libraryfor efficient supervised learning. GURLS is targeted to machine learningpractitioners, as well as non-specialists. It offers a number state-of-the-arttraining strategies for medium and large-scale learning, and routines forefficient model selection. The library is particularly well suited formulti-output problems (multi-category/multi-label). GURLS is currentlyavailable in two independent implementations: Matlab and C++. It takesadvantage of the favorable properties of regularized least squares algorithm toexploit advanced tools in linear algebra. Routines to handle computations withvery large matrices by means of memory-mapped storage and distributed taskexecution are available. The package is distributed under the BSD licence andis available for download at https://github.com/CBCL/GURLS.
arxiv-3000-6 | GBM Volumetry using the 3D Slicer Medical Image Computing Platform | http://arxiv.org/pdf/1303.0964v1.pdf | author:Jan Egger, Tina Kapur, Andriy Fedorov, Steve Pieper, James V. Miller, Harini Veeraraghavan, Bernd Freisleben, Alexandra Golby, Christopher Nimsky, Ron Kikinis category:cs.CV published:2013-03-05 summary:Volumetric change in glioblastoma multiforme (GBM) over time is a criticalfactor in treatment decisions. Typically, the tumor volume is computed on aslice-by-slice basis using MRI scans obtained at regular intervals. (3D)Slicer- a free platform for biomedical research - provides an alternative to thismanual slice-by-slice segmentation process, which is significantly faster andrequires less user interaction. In this study, 4 physicians segmented GBMs in10 patients, once using the competitive region-growing based GrowCutsegmentation module of Slicer, and once purely by drawing boundaries completelymanually on a slice-by-slice basis. Furthermore, we provide a variabilityanalysis for three physicians for 12 GBMs. The time required for GrowCutsegmentation was on an average 61% of the time required for a pure manualsegmentation. A comparison of Slicer-based segmentation with manualslice-by-slice segmentation resulted in a Dice Similarity Coefficient of 88.43+/- 5.23% and a Hausdorff Distance of 2.32 +/- 5.23 mm.
arxiv-3000-7 | A Genetic algorithm to solve the container storage space allocation problem | http://arxiv.org/pdf/1303.1051v1.pdf | author:I. Ayachi, R. Kammarti, M. Ksouri, P. Borne, :, LAGIS, Ecole Centrale de Lille, :, LACS, Ecole Nationale des category:cs.NE published:2013-03-05 summary:This paper presented a genetic algorithm (GA) to solve the container storageproblem in the port. This problem is studied with different container typessuch as regular, open side, open top, tank, empty and refrigerated containers.The objective of this problem is to determine an optimal containersarrangement, which respects customers delivery deadlines, reduces the rehandleoperations of containers and minimizes the stop time of the container ship. Inthis paper, an adaptation of the genetic algorithm to the container storageproblem is detailed and some experimental results are presented and discussed.The proposed approach was compared to a Last In First Out (LIFO) algorithmapplied to the same problem and has recorded good results
arxiv-3000-8 | Verifying a platform for digital imaging: a multi-tool strategy | http://arxiv.org/pdf/1303.1420v2.pdf | author:Jónathan Heras, Gadea Mata, Ana Romero, Julio Rubio, Rubén Sáenz category:cs.SE cs.CV published:2013-03-05 summary:Fiji is a Java platform widely used by biologists and other experimentalscientists to process digital images. In particular, in our research - madetogether with a biologists team; we use Fiji in some pre-processing stepsbefore undertaking a homological digital processing of images. In a previouswork, we have formalised the correctness of the programs which use homologicaltechniques to analyse digital images. However, the verification of Fiji'spre-processing step was missed. In this paper, we present a multi-tool approachfilling this gap, based on the combination of Why/Krakatoa, Coq and ACL2.
arxiv-3000-9 | Classification with Asymmetric Label Noise: Consistency and Maximal Denoising | http://arxiv.org/pdf/1303.1208v2.pdf | author:Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, Clayton Scott category:stat.ML cs.LG published:2013-03-05 summary:In many real-world classification problems, the labels of training examplesare randomly corrupted. Most previous theoretical work on classification withlabel noise assumes that the two classes are separable, that the label noise isindependent of the true class label, or that the noise proportions for eachclass are known. In this work, we give conditions that are necessary andsufficient for the true class-conditional distributions to be identifiable.These conditions are weaker than those analyzed previously, and allow for theclasses to be nonseparable and the noise levels to be asymmetric and unknown.The conditions essentially state that a majority of the observed labels arecorrect and that the true class-conditional distributions are "mutuallyirreducible," a concept we introduce that limits the similarity of the twodistributions. For any label noise problem, there is a unique pair of trueclass-conditional distributions satisfying the proposed conditions, and weargue that this pair corresponds in a certain sense to maximal denoising of theobserved distributions. Our results are facilitated by a connection to "mixture proportionestimation," which is the problem of estimating the maximal proportion of onedistribution that is present in another. We establish a novel rate ofconvergence result for mixture proportion estimation, and apply this to obtainconsistency of a discrimination rule based on surrogate loss minimization.Experimental results on benchmark data and a nuclear particle classificationproblem demonstrate the efficacy of our approach.
arxiv-3000-10 | An Equivalence between the Lasso and Support Vector Machines | http://arxiv.org/pdf/1303.1152v2.pdf | author:Martin Jaggi category:cs.LG stat.ML F.2.2; I.5.1 published:2013-03-05 summary:We investigate the relation of two fundamental tools in machine learning andsignal processing, that is the support vector machine (SVM) for classification,and the Lasso technique used in regression. We show that the resultingoptimization problems are equivalent, in the following sense. Given anyinstance of an $\ell_2$-loss soft-margin (or hard-margin) SVM, we construct aLasso instance having the same optimal solutions, and vice versa. As a consequence, many existing optimization algorithms for both SVMs andLasso can also be applied to the respective other problem instances. Also, theequivalence allows for many known theoretical insights for SVM and Lasso to betranslated between the two settings. One such implication gives a simplekernelized version of the Lasso, analogous to the kernels used in the SVMsetting. Another consequence is that the sparsity of a Lasso solution is equalto the number of support vectors for the corresponding SVM instance, and thatone can use screening rules to prune the set of support vectors. Furthermore,we can relate sublinear time algorithms for the two problems, and give a newsuch algorithm variant for the Lasso. We also study the regularization pathsfor both methods.
arxiv-3000-11 | Impulsive Noise Mitigation in Powerline Communications Using Sparse Bayesian Learning | http://arxiv.org/pdf/1303.1217v1.pdf | author:Jing Lin, Marcel Nassar, Brian L. Evans category:stat.ML cs.IT math.IT published:2013-03-05 summary:Additive asynchronous and cyclostationary impulsive noise limitscommunication performance in OFDM powerline communication (PLC) systems.Conventional OFDM receivers assume additive white Gaussian noise and henceexperience degradation in communication performance in impulsive noise.Alternate designs assume a parametric statistical model of impulsive noise anduse the model parameters in mitigating impulsive noise. These receivers requireoverhead in training and parameter estimation, and degrade due to model andparameter mismatch, especially in highly dynamic environments. In this paper,we model impulsive noise as a sparse vector in the time domain without anyother assumptions, and apply sparse Bayesian learning methods for estimationand mitigation without training. We propose three iterative algorithms withdifferent complexity vs. performance trade-offs: (1) we utilize the noiseprojection onto null and pilot tones to estimate and subtract the noiseimpulses; (2) we add the information in the data tones to perform joint noiseestimation and OFDM detection; (3) we embed our algorithm into a decisionfeedback structure to further enhance the performance of coded systems. Whencompared to conventional OFDM PLC receivers, the proposed receivers achieve SNRgains of up to 9 dB in coded and 10 dB in uncoded systems in the presence ofimpulsive noise.
arxiv-3000-12 | Hybrid Maximum Likelihood Modulation Classification Using Multiple Radios | http://arxiv.org/pdf/1303.0775v2.pdf | author:Onur Ozdemir, Ruoyu Li, Pramod K. Varshney category:cs.IT math.IT stat.ML published:2013-03-04 summary:The performance of a modulation classifier is highly sensitive to channelsignal-to-noise ratio (SNR). In this paper, we focus on amplitude-phasemodulations and propose a modulation classification framework based oncentralized data fusion using multiple radios and the hybrid maximum likelihood(ML) approach. In order to alleviate the computational complexity associatedwith ML estimation, we adopt the Expectation Maximization (EM) algorithm. Dueto SNR diversity, the proposed multi-radio framework provides robustness tochannel SNR. Numerical results show the superiority of the proposed approachwith respect to single radio approaches as well as to modulation classifiersusing moments based estimators.
arxiv-3000-13 | Bayesian Compressed Regression | http://arxiv.org/pdf/1303.0642v2.pdf | author:Rajarshi Guhaniyogi, David B. Dunson category:stat.ML cs.LG published:2013-03-04 summary:As an alternative to variable selection or shrinkage in high dimensionalregression, we propose to randomly compress the predictors prior to analysis.This dramatically reduces storage and computational bottlenecks, performingwell when the predictors can be projected to a low dimensional linear subspacewith minimal loss of information about the response. As opposed to existingBayesian dimensionality reduction approaches, the exact posterior distributionconditional on the compressed data is available analytically, speeding upcomputation by many orders of magnitude while also bypassing robustness issuesdue to convergence and mixing problems with MCMC. Model averaging is used toreduce sensitivity to the random projection matrix, while accommodatinguncertainty in the subspace dimension. Strong theoretical support is providedfor the approach by showing near parametric convergence rates for thepredictive density in the large p small n asymptotic paradigm. Practicalperformance relative to competitors is illustrated in simulations and real dataapplications.
arxiv-3000-14 | Spatial Fuzzy C Means PET Image Segmentation of Neurodegenerative Disorder | http://arxiv.org/pdf/1303.0647v1.pdf | author:A. Meena, R. Raja category:cs.CV published:2013-03-04 summary:Nuclear image has emerged as a promising research work in medical field.Images from different modality meet its own challenge. Positron EmissionTomography (PET) image may help to precisely localize disease to assist inplanning the right treatment for each case and saving valuable time. In thispaper, a novel approach of Spatial Fuzzy C Means (PET SFCM) clusteringalgorithm is introduced on PET scan image datasets. The proposed algorithm isincorporated the spatial neighborhood information with traditional FCM andupdating the objective function of each cluster. This algorithm is implementedand tested on huge data collection of patients with brain neuro degenerativedisorder such as Alzheimers disease. It has demonstrated its effectiveness bytesting it for real world patient data sets. Experimental results are comparedwith conventional FCM and K Means clustering algorithm. The performance of thePET SFCM provides satisfactory results compared with other two algorithms
arxiv-3000-15 | Symmetry Based Cluster Approach for Automatic Recognition of the Epileptic Focus in Brain Using PET Scan Image : An Analysis | http://arxiv.org/pdf/1303.0645v1.pdf | author:A. Meena, R. Raja category:cs.CV published:2013-03-04 summary:Recognition of epileptic focal point is the important diagnosis whenscreening the epilepsy patients for latent surgical cures. The accuratelocalization is challenging one because of the low spatial resolution imageswith more noisy data. Positron Emission Tomography (PET) has now replaced theissues and caring a high resolution. This paper focuses the research ofautomated localization of epileptic seizures in brain functional images usingsymmetry based cluster approach. This approach presents a fully automatedsymmetry based brain abnormality detection method for PET sequences. PET imagesare spatially normalized to Digital Imaging and Communications in Medicine(DICOM) standard and then it has been trained using symmetry based clusterapproach using Medical Image Processing, Analysis & Visualization (MIPAV) tool.The performance evolution is considered by the metric like accuracy ofdiagnosis. The obtained result is surely assists the surgeon for the automatedidentification of seizures focus.
arxiv-3000-16 | Denoising Deep Neural Networks Based Voice Activity Detection | http://arxiv.org/pdf/1303.0663v1.pdf | author:Xiao-Lei Zhang, Ji Wu category:cs.LG cs.SD stat.ML published:2013-03-04 summary:Recently, the deep-belief-networks (DBN) based voice activity detection (VAD)has been proposed. It is powerful in fusing the advantages of multiplefeatures, and achieves the state-of-the-art performance. However, the deeplayers of the DBN-based VAD do not show an apparent superiority to theshallower layers. In this paper, we propose a denoising-deep-neural-network(DDNN) based VAD to address the aforementioned problem. Specifically, wepre-train a deep neural network in a special unsupervised denoising greedylayer-wise mode, and then fine-tune the whole network in a supervised way bythe common back-propagation algorithm. In the pre-training phase, we take thenoisy speech signals as the visible layer and try to extract a new feature thatminimizes the reconstruction cross-entropy loss between the noisy speechsignals and its corresponding clean speech signals. Experimental results showthat the proposed DDNN-based VAD not only outperforms the DBN-based VAD butalso shows an apparent performance improvement of the deep layers overshallower layers.
arxiv-3000-17 | Learning AMP Chain Graphs and some Marginal Models Thereof under Faithfulness: Extended Version | http://arxiv.org/pdf/1303.0691v3.pdf | author:Jose M. Peña category:stat.ML cs.AI cs.LG published:2013-03-04 summary:This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)interpretation. In particular, we present a constraint based algorithm forlearning an AMP chain graph a given probability distribution is faithful to.Moreover, we show that the extension of Meek's conjecture to AMP chain graphsdoes not hold, which compromises the development of efficient and correctscore+search learning algorithms under assumptions weaker than faithfulness. We also introduce a new family of graphical models that consists ofundirected and bidirected edges. We name this new family maximalcovariance-concentration graphs (MCCGs) because it includes both covariance andconcentration graphs as subfamilies. However, every MCCG can be seen as theresult of marginalizing out some nodes in an AMP CG. We describe global, localand pairwise Markov properties for MCCGs and prove their equivalence. Wecharacterize when two MCCGs are Markov equivalent, and show that every Markovequivalence class of MCCGs has a distinguished member. We present a constraintbased algorithm for learning a MCCG a given probability distribution isfaithful to. Finally, we present a graphical criterion for reading dependencies from aMCCG of a probability distribution that satisfies the graphoid properties, weaktransitivity and composition. We prove that the criterion is sound and completein certain sense.
arxiv-3000-18 | Omega Model for Human Detection and Counting for application in Smart Surveillance System | http://arxiv.org/pdf/1303.0633v1.pdf | author:Subra Mukherjee, Karen Das category:cs.CV published:2013-03-04 summary:Driven by the significant advancements in technology and social issues suchas security management, there is a strong need for Smart Surveillance System inour society today. One of the key features of a Smart Surveillance System isefficient human detection and counting such that the system can decide andlabel events on its own. In this paper we propose a new, novel and robustmodel, The Omega Model, for detecting and counting human beings present in thescene. The proposed model employs a set of four distinct descriptors foridentifying the unique features of the head, neck and shoulder regions of aperson. This unique head neck shoulder signature given by the Omega Modelexploits the challenges such as inter person variations in size and shape ofpeoples head, neck and shoulder regions to achieve robust detection of humanbeings even under partial occlusion, dynamically changing background andvarying illumination conditions. After experimentation we observe and analyzethe influences of each of the four descriptors on the system performance andcomputation speed and conclude that a weight based decision making systemproduces the best results. Evaluation results on a number of images indicatethe validation of our method in actual situation.
arxiv-3000-19 | Indian Sign Language Recognition Using Eigen Value Weighted Euclidean Distance Based Classification Technique | http://arxiv.org/pdf/1303.0634v1.pdf | author:Joyeeta Singha, Karen Das category:cs.CV published:2013-03-04 summary:Sign Language Recognition is one of the most growing fields of researchtoday. Many new techniques have been developed recently in these fields. Herein this paper, we have proposed a system using Eigen value weighted Euclideandistance as a classification technique for recognition of various SignLanguages of India. The system comprises of four parts: Skin Filtering, HandCropping, Feature Extraction and Classification. Twenty four signs wereconsidered in this paper, each having ten samples, thus a total of two hundredforty images was considered for which recognition rate obtained was 97 percent.
arxiv-3000-20 | Personalized News Recommendation with Context Trees | http://arxiv.org/pdf/1303.0665v2.pdf | author:Florent Garcin, Christos Dimitrakakis, Boi Faltings category:cs.IR cs.LG stat.ML published:2013-03-04 summary:The profusion of online news articles makes it difficult to find interestingarticles, a problem that can be assuaged by using a recommender system to bringthe most relevant news stories to readers. However, news recommendation ischallenging because the most relevant articles are often new content seen byfew users. In addition, they are subject to trends and preference changes overtime, and in many cases we do not have sufficient information to profile thereader. In this paper, we introduce a class of news recommendation systems based oncontext trees. They can provide high-quality news recommendation to anonymousvisitors based on present browsing behaviour. We show that context-treerecommender systems provide good prediction accuracy and recommendationnovelty, and they are sufficiently flexible to capture the unique properties ofnews articles.
arxiv-3000-21 | Riemannian metrics for neural networks I: feedforward networks | http://arxiv.org/pdf/1303.0818v5.pdf | author:Yann Ollivier category:cs.NE cs.IT cs.LG math.DG math.IT 68T05 published:2013-03-04 summary:We describe four algorithms for neural network training, each adapted todifferent scalability constraints. These algorithms are mathematicallyprincipled and invariant under a number of transformations in data and networkrepresentation, from which performance is thus independent. These algorithmsare obtained from the setting of differential geometry, and are based on eitherthe natural gradient using the Fisher information matrix, or on Hessianmethods, scaled down in a specific way to allow for scalability while keepingsome of their key mathematical properties.
arxiv-3000-22 | Supplement to "Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs" | http://arxiv.org/pdf/1303.0632v2.pdf | author:Yangbo He, Jinzhu Jia, Bin Yu category:stat.ML math.CO published:2013-03-04 summary:This supplementary material includes three parts: some preliminary results,four examples, an experiment, three new algorithms, and all proofs of theresults in the paper "Reversible MCMC on Markov equivalence classes of sparsedirected acyclic graphs".
arxiv-3000-23 | The Convergence Rate of Majority Vote under Exchangeability | http://arxiv.org/pdf/1303.0727v1.pdf | author:Miles E. Lopes category:math.PR cs.SI math.ST stat.ML stat.TH published:2013-03-04 summary:Majority vote plays a fundamental role in many applications of statistics,such as ensemble classifiers, crowdsourcing, and elections. When using majorityvote as a prediction rule, it is of basic interest to ask "How many votes areneeded to obtain a reliable prediction?" In the context of binaryclassification with Random Forests or Bagging, we give a precise answer: Iferr_t denotes the test error achieved by the majority vote of t \geq 1classifiers, and err* denotes its nominal limiting value, then under basicregularity conditions, err_t = err* + c/t + o(1/t), where c is a constant givenby a simple formula. More generally, we show that if V_1,V_2,... is anexchangeable Bernoulli sequence with mixture distribution F, and the majorityvote is written as M_t=median(V_1,...,V_t), then 1-\E[M_t] = F(1/2)+(F"(1/2)/8)(1/t)+o(1/t) when F is sufficiently smooth.
arxiv-3000-24 | Multivariate Temporal Dictionary Learning for EEG | http://arxiv.org/pdf/1303.0742v1.pdf | author:Quentin Barthélemy, Cédric Gouy-Pailler, Yoann Isaac, Antoine Souloumiac, Anthony Larue, Jérôme I. Mars category:cs.LG q-bio.NC stat.ML published:2013-03-04 summary:This article addresses the issue of representing electroencephalographic(EEG) signals in an efficient way. While classical approaches use a fixed Gabordictionary to analyze EEG signals, this article proposes a data-driven methodto obtain an adapted dictionary. To reach an efficient dictionary learning,appropriate spatial and temporal modeling is required. Inter-channels links aretaken into account in the spatial multivariate model, and shift-invariance isused for the temporal model. Multivariate learned kernels are informative (afew atoms code plentiful energy) and interpretable (the atoms can have aphysiological meaning). Using real EEG data, the proposed method is shown tooutperform the classical multichannel matching pursuit used with a Gabordictionary, as measured by the representative power of the learned dictionaryand its spatial flexibility. Moreover, dictionary learning can captureinterpretable patterns: this ability is illustrated on real data, learning aP300 evoked potential.
arxiv-3000-25 | Automatic symmetry based cluster approach for anomalous brain identification in PET scan image : An Analysis | http://arxiv.org/pdf/1303.0644v1.pdf | author:A. Meena, K. Raja category:cs.CV published:2013-03-04 summary:Medical image segmentation is referred to the segmentation of known anatomicstructures from different medical images. Normally, the medical data researchesare more complicated and an exclusive structures. This computer aided diagnosisis used for assisting doctors in evaluating medical imagery or in recognizingabnormal findings in a medical image. To integrate the specialized knowledgefor medical data processing is helpful to form a real useful healthcaredecision making system. This paper studies the different symmetry baseddistances applied in clustering algorithms and analyzes symmetry approach forPositron Emission Tomography (PET) scan image segmentation. Unlike CT and MRI,the PET scan identifies the structure of blood flow to and from organs. PETscan also helps in early diagnosis of cancer and heart, brain and gastrointestinal ailments and to detect the progress of treatment. In this paper, thescope diagnostic task expands for PET image in various brain functions.
arxiv-3000-26 | Recognition of Facial Expression Using Eigenvector Based Distributed Features and Euclidean Distance Based Decision Making Technique | http://arxiv.org/pdf/1303.0635v1.pdf | author:Jeemoni Kalita, Karen Das category:cs.CV published:2013-03-04 summary:In this paper, an Eigenvector based system has been presented to recognizefacial expressions from digital facial images. In the approach, firstly theimages were acquired and cropping of five significant portions from the imagewas performed to extract and store the Eigenvectors specific to theexpressions. The Eigenvectors for the test images were also computed, andfinally the input facial image was recognized when similarity was obtained bycalculating the minimum Euclidean distance between the test image and thedifferent expressions.
arxiv-3000-27 | Top-down particle filtering for Bayesian decision trees | http://arxiv.org/pdf/1303.0561v2.pdf | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG published:2013-03-03 summary:Decision tree learning is a popular approach for classification andregression in machine learning and statistics, and Bayesianformulations---which introduce a prior distribution over decision trees, andformulate learning as posterior inference given data---have been shown toproduce competitive performance. Unlike classic decision tree learningalgorithms like ID3, C4.5 and CART, which work in a top-down manner, existingBayesian algorithms produce an approximation to the posterior distribution byevolving a complete tree (or collection thereof) iteratively via local MonteCarlo modifications to the structure of the tree, e.g., using Markov chainMonte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm thatinstead works in a top-down manner, mimicking the behavior and speed of classicalgorithms. We demonstrate empirically that our approach delivers accuracycomparable to the most popular MCMC method, but operates more than an order ofmagnitude faster, and thus represents a better computation-accuracy tradeoff.
arxiv-3000-28 | Genetic Programming for Document Segmentation and Region Classification Using Discipulus | http://arxiv.org/pdf/1303.0460v1.pdf | author:N. Priyadharshini, M. S. Vijaya category:cs.CV cs.NE published:2013-03-03 summary:Document segmentation is a method of rending the document into distinctregions. A document is an assortment of information and a standard mode ofconveying information to others. Pursuance of data from documents involves tonof human effort, time intense and might severely prohibit the usage of datasystems. So, automatic information pursuance from the document has become a bigissue. It is been shown that document segmentation will facilitate to beat suchproblems. This paper proposes a new approach to segment and classify thedocument regions as text, image, drawings and table. Document image is dividedinto blocks using Run length smearing rule and features are extracted fromevery blocks. Discipulus tool has been used to construct the Geneticprogramming based classifier model and located 97.5% classification accuracy.
arxiv-3000-29 | Statistical sentiment analysis performance in Opinum | http://arxiv.org/pdf/1303.0446v1.pdf | author:Boyan Bonev, Gema Ramírez-Sánchez, Sergio Ortiz Rojas category:cs.CL published:2013-03-03 summary:The classification of opinion texts in positive and negative is becoming asubject of great interest in sentiment analysis. The existence of many labeledopinions motivates the use of statistical and machine-learning methods.First-order statistics have proven to be very limited in this field. The Opinumapproach is based on the order of the words without using any syntactic andsemantic information. It consists of building one probabilistic model for thepositive and another one for the negative opinions. Then the test opinions arecompared to both models and a decision and confidence measure are calculated.In order to reduce the complexity of the training corpus we first lemmatize thetexts and we replace most named-entities with wildcards. Opinum presents anaccuracy above 81% for Spanish opinions in the financial products domain. Inthis work we discuss which are the most important factors that have impact onthe classification performance.
arxiv-3000-30 | Detecting and resolving spatial ambiguity in text using named entity extraction and self learning fuzzy logic techniques | http://arxiv.org/pdf/1303.0445v1.pdf | author:Kanagavalli V R, Raja. K category:cs.IR cs.CL published:2013-03-03 summary:Information extraction identifies useful and relevant text in a document andconverts unstructured text into a form that can be loaded into a databasetable. Named entity extraction is a main task in the process of informationextraction and is a classification problem in which words are assigned to oneor more semantic classes or to a default non-entity class. A word which canbelong to one or more classes and which has a level of uncertainty in it can bebest handled by a self learning Fuzzy Logic Technique. This paper proposes amethod for detecting the presence of spatial uncertainty in the text anddealing with spatial ambiguity using named entity extraction techniques coupledwith self learning fuzzy logic techniques
arxiv-3000-31 | Distributed Evolutionary Computation: A New Technique for Solving Large Number of Equations | http://arxiv.org/pdf/1303.0462v1.pdf | author:Moslema Jahan, M. M. A. Hashem, Gazi Abdullah Shahriar category:cs.NE published:2013-03-03 summary:Evolutionary computation techniques have mostly been used to solve variousoptimization and learning problems successfully. Evolutionary algorithm is moreeffective to gain optimal solution(s) to solve complex problems thantraditional methods. In case of problems with large set of parameters,evolutionary computation technique incurs a huge computational burden for asingle processing unit. Taking this limitation into account, this paperpresents a new distributed evolutionary computation technique, which decomposesdecision vectors into smaller components and achieves optimal solution in ashort time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA)Hybrid Evolutionary Algorithm is distributed incorporating cluster computation.Moreover, two new selection methods named Best All Selection (BAS) and TwinSelection (TS) are introduced for selecting best fit solution vector.Experimental results show that optimal solution is achieved for different kindsof problems having huge parameters and a considerable speedup is obtained inproposed distributed system.
arxiv-3000-32 | A Cumulative Multi-Niching Genetic Algorithm for Multimodal Function Optimization | http://arxiv.org/pdf/1304.0751v1.pdf | author:Matthew Hall category:cs.NE published:2013-03-03 summary:This paper presents a cumulative multi-niching genetic algorithm (CMN GA),designed to expedite optimization problems that have computationally-expensivemultimodal objective functions. By never discarding individuals from thepopulation, the CMN GA makes use of the information from every objectivefunction evaluation as it explores the design space. A fitness-relatedpopulation density control over the design space reduces unnecessary objectivefunction evaluations. The algorithm's novel arrangement of genetic operationsprovides fast and robust convergence to multiple local optima. Benchmark testsalongside three other multi-niching algorithms show that the CMN GA has agreater convergence ability and provides an order-of-magnitude reduction in thenumber of objective function evaluations required to achieve a given level ofconvergence.
arxiv-3000-33 | Learning Stable Multilevel Dictionaries for Sparse Representations | http://arxiv.org/pdf/1303.0448v2.pdf | author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:cs.CV stat.ML published:2013-03-03 summary:Sparse representations using learned dictionaries are being increasingly usedwith success in several data processing and machine learning applications. Theavailability of abundant training data necessitates the development ofefficient, robust and provably good dictionary learning algorithms. Algorithmicstability and generalization are desirable characteristics for dictionarylearning algorithms that aim to build global dictionaries which can efficientlymodel any test data similar to the training samples. In this paper, we proposean algorithm to learn dictionaries for sparse representations from large scaledata, and prove that the proposed learning algorithm is stable andgeneralizable asymptotically. The algorithm employs a 1-D subspace clusteringprocedure, the K-hyperline clustering, in order to learn a hierarchicaldictionary with multiple levels. We also propose an information-theoreticscheme to estimate the number of atoms needed in each level of learning anddevelop an ensemble approach to learn robust dictionaries. Using the proposeddictionaries, the sparse code for novel test data can be computed using alow-complexity pursuit procedure. We demonstrate the stability andgeneralization characteristics of the proposed algorithm using simulations. Wealso evaluate the utility of the multilevel dictionaries in compressed recoveryand subspace learning applications.
arxiv-3000-34 | A Semantic approach for effective document clustering using WordNet | http://arxiv.org/pdf/1303.0489v1.pdf | author:Leena H. Patil, Mohammed Atique category:cs.CL cs.IR published:2013-03-03 summary:Now a days, the text document is spontaneously increasing over the internet,e-mail and web pages and they are stored in the electronic database format. Toarrange and browse the document it becomes difficult. To overcome such problemthe document preprocessing, term selection, attribute reduction and maintainingthe relationship between the important terms using background knowledge,WordNet, becomes an important parameters in data mining. In these paper thedifferent stages are formed, firstly the document preprocessing is done byremoving stop words, stemming is performed using porter stemmer algorithm, wordnet thesaurus is applied for maintaining relationship between the importantterms, global unique words, and frequent word sets get generated, Secondly,data matrix is formed, and thirdly terms are extracted from the documents byusing term selection approaches tf-idf, tf-df, and tf2 based on their minimumthreshold value. Further each and every document terms gets preprocessed, wherethe frequency of each term within the document is counted for representation.The purpose of this approach is to reduce the attributes and find the effectiveterm selection method using WordNet for better clustering accuracy. Experimentsare evaluated on Reuters Transcription Subsets, wheat, trade, money grain, andship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group(Hardware), 20 News group (Computer Graphics) etc.
arxiv-3000-35 | Multiple Kernel Sparse Representations for Supervised and Unsupervised Learning | http://arxiv.org/pdf/1303.0582v2.pdf | author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:cs.CV published:2013-03-03 summary:In complex visual recognition tasks it is typical to adopt multipledescriptors, that describe different aspects of the images, for obtaining animproved recognition performance. Descriptors that have diverse forms can befused into a unified feature space in a principled manner using kernel methods.Sparse models that generalize well to the test data can be learned in theunified kernel space, and appropriate constraints can be incorporated forapplication in supervised and unsupervised learning. In this paper, we proposeto perform sparse coding and dictionary learning in the multiple kernel space,where the weights of the ensemble kernel are tuned based on graph-embeddingprinciples such that class discrimination is maximized. In our proposedalgorithm, dictionaries are inferred using multiple levels of 1-D subspaceclustering in the kernel space, and the sparse codes are obtained using asimple levelwise pursuit scheme. Empirical results for object recognition andimage clustering show that our algorithm outperforms existing sparse codingbased approaches, and compares favorably to other state-of-the-art methods.
arxiv-3000-36 | Scale Selection of Adaptive Kernel Regression by Joint Saliency Map for Nonrigid Image Registration | http://arxiv.org/pdf/1303.0479v2.pdf | author:Zhuangming Shen, Jiuai Sun, Hui Zhang, Binjie Qin category:cs.CV published:2013-03-03 summary:Joint saliency map (JSM) [1] was developed to assign high joint saliencyvalues to the corresponding saliency structures (called Joint SaliencyStructures, JSSs) but zero or low joint saliency values to the outliers (ormismatches) that are introduced by missing correspondence or local largedeformations between the reference and moving images to be registered. JSMguides the local structure matching in nonrigid registration by emphasizingthese JSSs' sparse deformation vectors in adaptive kernel regression ofhierarchical sparse deformation vectors for iterative dense deformationreconstruction. By designing an effective superpixel-based local structurescale estimator to compute the reference structure's structure scale, wefurther propose to determine the scale (the width) of kernels in the adaptivekernel regression through combining the structure scales to JSM-based scales ofmismatch between the local saliency structures. Therefore, we can adaptivelyselect the sample size of sparse deformation vectors to reconstruct the densedeformation vectors for accurately matching the every local structures in thetwo images. The experimental results demonstrate better accuracy of our methodin aligning two images with missing correspondence and local large deformationthan the state-of-the-art methods.
arxiv-3000-37 | Sparse PCA through Low-rank Approximations | http://arxiv.org/pdf/1303.0551v2.pdf | author:Dimitris S. Papailiopoulos, Alexandros G. Dimakis, Stavros Korokythakis category:stat.ML cs.IT cs.LG math.IT published:2013-03-03 summary:We introduce a novel algorithm that computes the $k$-sparse principalcomponent of a positive semidefinite matrix $A$. Our algorithm is combinatorialand operates by examining a discrete set of special vectors lying in alow-dimensional eigen-subspace of $A$. We obtain provable approximationguarantees that depend on the spectral decay profile of the matrix: the fasterthe eigenvalue decay, the better the quality of our approximation. For example,if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-timeapproximation algorithm for any desired accuracy. A key algorithmic component of our scheme is a combinatorial featureelimination step that is provably safe and in practice significantly reducesthe running complexity of our algorithm. We implement our algorithm and test iton multiple artificial and real data sets. Due to the feature elimination step,it is possible to perform sparse PCA on data sets consisting of millions ofentries in a few minutes. Our experimental evaluation shows that our scheme isnearly optimal while finding very sparse vectors. We compare to the prior stateof the art and show that our scheme matches or outperforms previous algorithmsin all tested data sets.
arxiv-3000-38 | Clubs-based Particle Swarm Optimization | http://arxiv.org/pdf/1303.0323v1.pdf | author:Wesam Elshamy, Hassan M Emara, Ahmed Bahgat category:cs.NE 68T20 published:2013-03-02 summary:This paper introduces a new dynamic neighborhood network for particle swarmoptimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO)algorithm, each particle initially joins a default number of what we call'clubs'. Each particle is affected by its own experience and the experience ofthe best performing member of the clubs it is a member of. Clubs membership isdynamic, where the worst performing particles socialize more by joining moreclubs to learn from other particles and the best performing particles are madeto socialize less by leaving clubs to reduce their strong influence on othermembers. Particles return gradually to default membership level when they stopshowing extreme performance. Inertia weights of swarm members are made randomwithin a predefined range. This proposed dynamic neighborhood algorithm iscompared with other two algorithms having static neighborhood topologies on aset of classic benchmark problems. The results showed superior performance forC-PSO regarding escaping local optima and convergence speed.
arxiv-3000-39 | Learning Hash Functions Using Column Generation | http://arxiv.org/pdf/1303.0339v1.pdf | author:Xi Li, Guosheng Lin, Chunhua Shen, Anton van den Hengel, Anthony Dick category:cs.LG published:2013-03-02 summary:Fast nearest neighbor searching is becoming an increasingly important tool insolving many large-scale problems. Recently a number of approaches to learningdata-dependent hash functions have been developed. In this work, we propose acolumn generation based method for learning data-dependent hash functions onthe basis of proximity comparison information. Given a set of triplets thatencode the pairwise proximity comparison information, our method learns hashfunctions that preserve the relative comparison relationships in the data aswell as possible within the large-margin learning framework. The learningprocedure is implemented using column generation and hence is named CGHash. Ateach iteration of the column generation procedure, the best hash function isselected. Unlike most other hashing methods, our method generalizes to new datapoints naturally; and has a training objective which is convex, thus ensuringthat the global optimum can be identified. Experiments demonstrate that theproposed method learns compact binary codes and that its retrieval performancecompares favorably with state-of-the-art methods when tested on a few benchmarkdatasets.
arxiv-3000-40 | Probing the statistical properties of unknown texts: application to the Voynich Manuscript | http://arxiv.org/pdf/1303.0347v1.pdf | author:Diego R. Amancio, Eduardo G. Altmann, Diego Rybski, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL published:2013-03-02 summary:While the use of statistical physics methods to analyze large corpora hasbeen useful to unveil many patterns in texts, no comprehensive investigationhas been performed investigating the properties of statistical measurementsacross different languages and texts. In this study we propose a framework thataims at determining if a text is compatible with a natural language and whichlanguages are closest to it, without any knowledge of the meaning of the words.The approach is based on three types of statistical measurements, i.e. obtainedfrom first-order statistics of word properties in a text, from the topology ofcomplex networks representing text, and from intermittency concepts where textis treated as a time series. Comparative experiments were performed with theNew Testament in 15 different languages and with distinct books in English andPortuguese in order to quantify the dependency of the different measurements onthe language and on the story being told in the book. The metrics found to beinformative in distinguishing real texts from their shuffled versions includeassortativity, degree and selectivity of words. As an illustration, we analyzean undeciphered medieval manuscript known as the Voynich Manuscript. We showthat it is mostly compatible with natural languages and incompatible withrandom texts. We also obtain candidates for key-words of the Voynich Manuscriptwhich could be helpful in the effort of deciphering it. Because we were able toidentify statistical measurements that are more dependent on the syntax than onthe semantics, the framework may also serve for text analysis inlanguage-dependent applications.
arxiv-3000-41 | Structure-semantics interplay in complex networks and its effects on the predictability of similarity in texts | http://arxiv.org/pdf/1303.0350v1.pdf | author:Diego R. Amancio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:cs.CL physics.soc-ph published:2013-03-02 summary:There are different ways to define similarity for grouping similar texts intoclusters, as the concept of similarity may depend on the purpose of the task.For instance, in topic extraction similar texts mean those within the samesemantic field, whereas in author recognition stylistic features should beconsidered. In this study, we introduce ways to classify texts employingconcepts of complex networks, which may be able to capture syntactic, semanticand even pragmatic features. The interplay between the various metrics of thecomplex networks is analyzed with three applications, namely identification ofmachine translation (MT) systems, evaluation of quality of machine translatedtexts and authorship recognition. We shall show that topological features ofthe networks representing texts can enhance the ability to identify MT systemsin particular cases. For evaluating the quality of MT texts, on the other hand,high correlation was obtained with methods capable of capturing the semantics.This was expected because the golden standards used are themselves based onword co-occurrence. Notwithstanding, the Katz similarity, which involvessemantic and structure in the comparison of texts, achieved the highestcorrelation with the NIST measurement, indicating that in some cases thecombination of both approaches can improve the ability to quantify quality inMT. In authorship recognition, again the topological features were relevant insome contexts, though for the books and authors analyzed good results wereobtained with semantic features as well. Because hybrid approaches encompassingsemantic and topological features have not been extensively used, we believethat the methodology proposed here may be useful to enhance text classificationconsiderably, as it combines well-established strategies.
arxiv-3000-42 | Inductive Sparse Subspace Clustering | http://arxiv.org/pdf/1303.0362v1.pdf | author:Xi Peng, Lei Zhang, Zhang Yi category:cs.LG published:2013-03-02 summary:Sparse Subspace Clustering (SSC) has achieved state-of-the-art clusteringquality by performing spectral clustering over a $\ell^{1}$-norm basedsimilarity graph. However, SSC is a transductive method which does not handlewith the data not used to construct the graph (out-of-sample data). For eachnew datum, SSC requires solving $n$ optimization problems in O(n) variables forperforming the algorithm over the whole data set, where $n$ is the number ofdata points. Therefore, it is inefficient to apply SSC in fast onlineclustering and scalable graphing. In this letter, we propose an inductivespectral clustering algorithm, called inductive Sparse Subspace Clustering(iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts theassumption that high-dimensional data actually lie on the low-dimensionalmanifold such that out-of-sample data could be grouped in the embedding spacelearned from in-sample data. Experimental results show that iSSC is promisingin clustering out-of-sample data.
arxiv-3000-43 | On the convergence of the IRLS algorithm in Non-Local Patch Regression | http://arxiv.org/pdf/1303.0417v2.pdf | author:Kunal N. Chaudhury category:cs.CV stat.ML published:2013-03-02 summary:Recently, it was demonstrated in [CS2012,CS2013] that the robustness of theclassical Non-Local Means (NLM) algorithm [BCM2005] can be improved byincorporating $\ell^p (0 < p \leq 2)$ regression into the NLM framework. Thisgeneral optimization framework, called Non-Local Patch Regression (NLPR),contains NLM as a special case. Denoising results on synthetic and naturalimages show that NLPR consistently performs better than NLM beyond a moderatenoise level, and significantly so when $p$ is close to zero. An iterativelyreweighted least-squares (IRLS) algorithm was proposed for solving theregression problem in NLPR, where the NLM output was used to initialize theiterations. Based on exhaustive numerical experiments, we observe that the IRLSalgorithm is globally convergent (for arbitrary initialization) in the convexregime $1 \leq p \leq 2$, and locally convergent (fails very rarely using NLMinitialization) in the non-convex regime $0 < p < 1$. In this letter, we adaptthe "majorize-minimize" framework introduced in [Voss1980] to explain theseobservations. [CS2012] Chaudhury et al. (2012), "Non-local Euclidean medians," IEEE SignalProcessing Letters. [CS2013] Chaudhury et al. (2013), "Non-local patch regression: Robust imagedenoising in patch space," IEEE ICASSP. [BCM2005] Buades et al. (2005), "A review of image denoising algorithms, witha new one," Multiscale Modeling and Simulation. [Voss1980] Voss et al. (1980), "Linear convergence of generalized Weiszfeld'smethod," Computing.
arxiv-3000-44 | Matrix Completion via Max-Norm Constrained Optimization | http://arxiv.org/pdf/1303.0341v2.pdf | author:T. Tony Cai, Wen-Xin Zhou category:cs.LG cs.IT math.IT stat.ML 62H12, 15A83 published:2013-03-02 summary:Matrix completion has been well studied under the uniform sampling model andthe trace-norm regularized methods perform well both theoretically andnumerically in such a setting. However, the uniform sampling model isunrealistic for a range of applications and the standard trace-norm relaxationcan behave very poorly when the sampling distribution is non-uniform. In this paper we propose and analyze a max-norm constrained empirical riskminimization method for noisy matrix completion under a general sampling model.The optimal rate of convergence is established under the Frobenius norm loss inthe context of approximately low-rank matrix reconstruction. It is shown thatthe max-norm constrained method is minimax rate-optimal and it yields a uni?edand robust approximate recovery guarantee, with respect to the samplingdistributions. The computational effectiveness of this method is also studied,based on a first-order algorithm for solving convex programs involving amax-norm constraint.
arxiv-3000-45 | Second-Order Non-Stationary Online Learning for Regression | http://arxiv.org/pdf/1303.0140v1.pdf | author:Nina Vaits, Edward Moroshko, Koby Crammer category:cs.LG stat.ML published:2013-03-01 summary:The goal of a learner, in standard online learning, is to have the cumulativeloss not much larger compared with the best-performing function from some fixedclass. Numerous algorithms were shown to have this gap arbitrarily close tozero, compared with the best function that is chosen off-line. Nevertheless,many real-world applications, such as adaptive filtering, are non-stationary innature, and the best prediction function may drift over time. We introduce twonovel algorithms for online regression, designed to work well in non-stationaryenvironment. Our first algorithm performs adaptive resets to forget thehistory, while the second is last-step min-max optimal in context of a drift.We analyze both algorithms in the worst-case regret framework and show thatthey maintain an average loss close to that of the best slowly changingsequence of linear functions, as long as the cumulative drift is sublinear. Inaddition, in the stationary case, when no drift occurs, our algorithms sufferlogarithmic regret, as for previous algorithms. Our bounds improve over theexisting ones, and simulations demonstrate the usefulness of these algorithmscompared with other state-of-the-art approaches.
arxiv-3000-46 | Exploiting the Accumulated Evidence for Gene Selection in Microarray Gene Expression Data | http://arxiv.org/pdf/1303.0156v1.pdf | author:G. Prat, Ll. Belanche category:cs.CE cs.LG q-bio.QM I.5.2 published:2013-03-01 summary:Machine Learning methods have of late made significant efforts to solvingmultidisciplinary problems in the field of cancer classification usingmicroarray gene expression data. Feature subset selection methods can play animportant role in the modeling process, since these tasks are characterized bya large number of features and a few observations, making the modeling anon-trivial undertaking. In this particular scenario, it is extremely importantto select genes by taking into account the possible interactions with othergene subsets. This paper shows that, by accumulating the evidence in favour (oragainst) each gene along the search process, the obtained gene subsets mayconstitute better solutions, either in terms of predictive accuracy or genesize, or in both. The proposed technique is extremely simple and applicable ata negligible overhead in cost.
arxiv-3000-47 | Bio-Signals-based Situation Comparison Approach to Predict Pain | http://arxiv.org/pdf/1303.0076v2.pdf | author:Uri Kartoun category:stat.AP cs.LG stat.ML published:2013-03-01 summary:This paper describes a time-series-based classification approach to identifysimilarities between bio-medical-based situations. The proposed approach allowsclassifying collections of time-series representing bio-medical measurements,i.e., situations, regardless of the type, the length and the quantity of thetime-series a situation comprised of.
arxiv-3000-48 | On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution beyond the diffraction limit | http://arxiv.org/pdf/1303.0166v1.pdf | author:Stefan Harmeling, Michael Hirsch, Bernhard Schölkopf category:physics.optics stat.ML published:2013-03-01 summary:We establish a link between Fourier optics and a recent construction from themachine learning community termed the kernel mean map. Using the Fraunhoferapproximation, it identifies the kernel with the squared Fourier transform ofthe aperture. This allows us to use results about the invertibility of thekernel mean map to provide a statement about the invertibility of Fraunhoferdiffraction, showing that imaging processes with arbitrarily small aperturescan in principle be invertible, i.e., do not lose information, provided theobjects to be imaged satisfy a generic condition. A real world experiment showsthat we can super-resolve beyond the Rayleigh limit.
arxiv-3000-49 | Maximal Information Divergence from Statistical Models defined by Neural Networks | http://arxiv.org/pdf/1303.0268v1.pdf | author:Guido Montufar, Johannes Rauh, Nihat Ay category:math.ST stat.ML stat.TH published:2013-03-01 summary:We review recent results about the maximal values of the Kullback-Leiblerinformation divergence from statistical models defined by neural networks,including naive Bayes models, restricted Boltzmann machines, deep beliefnetworks, and various classes of exponential families. We illustrate approachesto compute the maximal divergence from a given model starting from simple sub-or super-models. We give a new result for deep and narrow belief networks withfinite-valued units.
arxiv-3000-50 | Inverse Signal Classification for Financial Instruments | http://arxiv.org/pdf/1303.0283v2.pdf | author:Uri Kartoun category:cs.LG cs.IR q-fin.ST stat.ML published:2013-03-01 summary:The paper presents new machine learning methods: signal composition, whichclassifies time-series regardless of length, type, and quantity; andself-labeling, a supervised-learning enhancement. The paper describes furtherthe implementation of the methods on a financial search engine system using acollection of 7,881 financial instruments traded during 2011 to identifyinverse behavior among the time-series.
arxiv-3000-51 | A Method for Comparing Hedge Funds | http://arxiv.org/pdf/1303.0073v2.pdf | author:Uri Kartoun category:q-fin.ST cs.IR cs.LG stat.ML published:2013-03-01 summary:The paper presents new machine learning methods: signal composition, whichclassifies time-series regardless of length, type, and quantity; andself-labeling, a supervised-learning enhancement. The paper describes furtherthe implementation of the methods on a financial search engine system toidentify behavioral similarities among time-series representing monthly returnsof 11,312 hedge funds operated during approximately one decade (2000 - 2010).The presented approach of cross-category and cross-location classificationassists the investor to identify alternative investments.
arxiv-3000-52 | Label-dependent Feature Extraction in Social Networks for Node Classification | http://arxiv.org/pdf/1303.0095v1.pdf | author:Tomasz Kajdanowicz, Przemyslaw Kazienko, Piotr Doskocz category:cs.SI cs.LG I.2.8; I.2.11 published:2013-03-01 summary:A new method of feature extraction in the social network for within-networkclassification is proposed in the paper. The method provides new featurescalculated by combination of both: network structure information and classlabels assigned to nodes. The influence of various features on classificationperformance has also been studied. The experiments on real-world data haveshown that features created owing to the proposed method can lead tosignificant improvement of classification accuracy.
arxiv-3000-53 | One-Class Support Measure Machines for Group Anomaly Detection | http://arxiv.org/pdf/1303.0309v2.pdf | author:Krikamol Muandet, Bernhard Schölkopf category:stat.ML cs.LG published:2013-03-01 summary:We propose one-class support measure machines (OCSMMs) for group anomalydetection which aims at recognizing anomalous aggregate behaviors of datapoints. The OCSMMs generalize well-known one-class support vector machines(OCSVMs) to a space of probability measures. By formulating the problem asquantile estimation on distributions, we can establish an interestingconnection to the OCSVMs and variable kernel density estimators (VKDEs) overthe input space on which the distributions are defined, bridging the gapbetween large-margin methods and kernel density estimators. In particular, weshow that various types of VKDEs can be considered as solutions to a class ofregularization problems studied in this paper. Experiments on Sloan Digital SkySurvey dataset and High Energy Particle Physics dataset demonstrate thebenefits of the proposed framework in real-world applications.
arxiv-3000-54 | Online Similarity Prediction of Networked Data from Known and Unknown Graphs | http://arxiv.org/pdf/1302.7263v3.pdf | author:Claudio Gentile, Mark Herbster, Stephen Pasteris category:cs.LG published:2013-02-28 summary:We consider online similarity prediction problems over networked data. Webegin by relating this task to the more standard class prediction problem,showing that, given an arbitrary algorithm for class prediction, we canconstruct an algorithm for similarity prediction with "nearly" the same mistakebound, and vice versa. After noticing that this general construction iscomputationally infeasible, we target our study to {\em feasible} similarityprediction algorithms on networked data. We initially assume that the networkstructure is {\em known} to the learner. Here we observe that Matrix Winnow\cite{w07} has a near-optimal mistake guarantee, at the price of cubicprediction time per round. This motivates our effort for an efficientimplementation of a Perceptron algorithm with a weaker mistake guarantee butwith only poly-logarithmic prediction time. Our focus then turns to thechallenging case of networks whose structure is initially {\em unknown} to thelearner. In this novel setting, where the network structure is onlyincrementally revealed, we obtain a mistake-bounded algorithm with a quadraticprediction time per round.
arxiv-3000-55 | Parameter Identification of Induction Motor Using Modified Particle Swarm Optimization Algorithm | http://arxiv.org/pdf/1302.7080v1.pdf | author:Hassan M Emara, Wesam Elshamy, Ahmed Bahgat category:cs.NE 68T05 published:2013-02-28 summary:This paper presents a new technique for induction motor parameteridentification. The proposed technique is based on a simple startup test usinga standard V/F inverter. The recorded startup currents are compared to thatobtained by simulation of an induction motor model. A Modified PSO optimizationis used to find out the best model parameter that minimizes the sum squareerror between the measured and the simulated currents. The performance of themodified PSO is compared with other optimization methods including line search,conventional PSO and Genetic Algorithms. Simulation results demonstrate theability of the proposed technique to capture the true values of the machineparameters and the superiority of the results obtained using the modified PSOover other optimization techniques.
arxiv-3000-56 | Learning Theory in the Arithmetic Hierarchy | http://arxiv.org/pdf/1302.7069v1.pdf | author:Achilles Beros category:math.LO cs.LG cs.LO 03D80, 68Q32 published:2013-02-28 summary:We consider the arithmetic complexity of index sets of uniformly computablyenumerable families learnable under different learning criteria. We determinethe exact complexity of these sets for the standard notions of finite learning,learning in the limit, behaviorally correct learning and anomalous learning inthe limit. In proving the $\Sigma_5^0$-completeness result for behaviorallycorrect learning we prove a result of independent interest; if a uniformlycomputably enumerable family is not learnable, then for any computable learnerthere is a $\Delta_2^0$ enumeration witnessing failure.
arxiv-3000-57 | Polyploidy and Discontinuous Heredity Effect on Evolutionary Multi-Objective Optimization | http://arxiv.org/pdf/1302.7051v2.pdf | author:Wesam Elshamy, Hassan M Emara, Ahmed Bahgat category:cs.NE 60G15 published:2013-02-28 summary:This paper examines the effect of mimicking discontinuous heredity caused bycarrying more than one chromosome in some living organisms cells inEvolutionary Multi-Objective Optimization algorithms. In this representation,the phenotype may not fully reflect the genotype. By doing so we are mimickingliving organisms inheritance mechanism, where traits may be silently carriedfor many generations to reappear later. Representations with different numberof chromosomes in each solution vector are tested on different benchmarkproblems with high number of decision variables and objectives. A comparisonwith Non-Dominated Sorting Genetic Algorithm-II is done on all problems.
arxiv-3000-58 | Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization | http://arxiv.org/pdf/1302.7043v1.pdf | author:Evangelos E. Papalexakis, Tom M. Mitchell, Nicholas D. Sidiropoulos, Christos Faloutsos, Partha Pratim Talukdar, Brian Murphy category:stat.ML cs.LG published:2013-02-28 summary:How can we correlate neural activity in the human brain as it responds towords, with behavioral data expressed as answers to questions about these samewords? In short, we want to find latent variables, that explain both the brainactivity, as well as the behavioral responses. We show that this is an instanceof the Coupled Matrix-Tensor Factorization (CMTF) problem. We proposeScoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problemand produces a sparse latent low-rank subspace of the data. In our experiments,we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithmfor CMTF, along with a 5 fold increase in sparsity. Moreover, we extendScoup-SMT to handle missing data without degradation of performance. We applyScoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, humansubjects) tensor and a (nouns, properties) matrix, with coupling along thenouns dimension. Scoup-SMT is able to find meaningful latent variables, as wellas to predict brain activity with competitive accuracy. Finally, we demonstratethe generality of Scoup-SMT, by applying it on a Facebook dataset (users,friends, wall-postings); there, Scoup-SMT spots spammer-like anomalies.
arxiv-3000-59 | K Means Segmentation of Alzheimers Disease in PET scan datasets: An implementation | http://arxiv.org/pdf/1302.7082v1.pdf | author:A. Meena, K. Raja category:cs.CV cs.NE published:2013-02-28 summary:The Positron Emission Tomography (PET) scan image requires expertise in thesegmentation where clustering algorithm plays an important role in theautomation process. The algorithm optimization is concluded based on theperformance, quality and number of clusters extracted. This paper is proposedto study the commonly used K Means clustering algorithm and to discuss a brieflist of toolboxes for reproducing and extending works presented in medicalimage analysis. This work is compiled using AForge .NET framework in windowsenvironment and MATrix LABoratory (MATLAB 7.0.1)
arxiv-3000-60 | Continuous-time Infinite Dynamic Topic Models | http://arxiv.org/pdf/1302.7088v1.pdf | author:Wesam Elshamy category:cs.IR stat.AP stat.ML 68T10 published:2013-02-28 summary:Topic models are probabilistic models for discovering topical themes incollections of documents. In real world applications, these models provide uswith the means of organizing what would otherwise be unstructured collections.They can help us cluster a huge collection into different topics or find asubset of the collection that resembles the topical theme found in an articleat hand. The first wave of topic models developed were able to discover the prevailingtopics in a big collection of documents spanning a period of time. It was laterrealized that these time-invariant models were not capable of modeling 1) thetime varying number of topics they discover and 2) the time changing structureof these topics. Few models were developed to address this two deficiencies.The online-hierarchical Dirichlet process models the documents with a timevarying number of topics. It varies the structure of the topics over time aswell. However, it relies on document order, not timestamps to evolve the modelover time. The continuous-time dynamic topic model evolves topic structure incontinuous-time. However, it uses a fixed number of topics over time. In this dissertation, I present a model, the continuous-time infinite dynamictopic model, that combines the advantages of these two models 1) theonline-hierarchical Dirichlet process, and 2) the continuous-time dynamic topicmodel. More specifically, the model I present is a probabilistic topic modelthat does the following: 1) it changes the number of topics over continuoustime, and 2) it changes the topic structure over continuous-time. I compared the model I developed with the two other models with differentsetting values. The results obtained were favorable to my model and showed theneed for having a model that has a continuous-time varying number of topics andtopic structure.
arxiv-3000-61 | Content Based Image Retrieval System Using NOHIS-tree | http://arxiv.org/pdf/1302.7039v1.pdf | author:Mounira Taileb category:cs.IR cs.CV cs.DB H.3.3 published:2013-02-28 summary:Content-based image retrieval (CBIR) has been one of the most importantresearch areas in computer vision. It is a widely used method for searchingimages in huge databases. In this paper we present a CBIR system calledNOHIS-Search. The system is based on the indexing technique NOHIS-tree. The twophases of the system are described and the performance of the system isillustrated with the image database ImagEval. NOHIS-Search system was comparedto other two CBIR systems; the first that using PDDP indexing algorithm and thesecond system is that using the sequential search. Results show thatNOHIS-Search system outperforms the two other systems.
arxiv-3000-62 | A New Monte Carlo Based Algorithm for the Gaussian Process Classification Problem | http://arxiv.org/pdf/1302.7220v2.pdf | author:Amir F. Atiya, Hatem A. Fayed, Ahmed H. Abdel-Gawad category:stat.ML published:2013-02-28 summary:Gaussian process is a very promising novel technology that has been appliedto both the regression problem and the classification problem. While for theregression problem it yields simple exact solutions, this is not the case forthe classification problem, because we encounter intractable integrals. In thispaper we develop a new derivation that transforms the problem into that ofevaluating the ratio of multivariate Gaussian orthant integrals. Moreover, wedevelop a new Monte Carlo procedure that evaluates these integrals. It is basedon some aspects of bootstrap sampling and acceptancerejection. The proposedapproach has beneficial properties compared to the existing Markov Chain MonteCarlo approach, such as simplicity, reliability, and speed.
arxiv-3000-63 | Using Artificial Intelligence Models in System Identification | http://arxiv.org/pdf/1302.7096v1.pdf | author:Wesam Elshamy category:cs.NE cs.SY 68T05 published:2013-02-28 summary:Artificial Intelligence (AI) techniques are known for its ability in tacklingproblems found to be unyielding to traditional mathematical methods. A recentaddition to these techniques are the Computational Intelligence (CI) techniqueswhich, in most cases, are nature or biologically inspired techniques. DifferentCI techniques found their way to many control engineering applications,including system identification, and the results obtained by many researcherswere encouraging. However, most control engineers and researchers used thebasic CI models as is or slightly modified them to match their needs.Henceforth, the merits of one model over the other was not clear, and fullpotential of these models was not exploited. In this research, Genetic Algorithm (GA) and Particle Swarm Optimization(PSO) methods, which are different CI techniques, are modified to best suit themultimodal problem of system identification. In the first case of GA, anextension to the basic algorithm, which is inspired from nature as well, wasdeployed by introducing redundant genetic material. This extension, which comein handy in living organisms, did not result in significant performanceimprovement to the basic algorithm. In the second case, the Clubs-based PSO(C-PSO) dynamic neighborhood structure was introduced to replace the basicstatic structure used in canonical PSO algorithms. This modification of theneighborhood structure resulted in significant performance of the algorithmregarding convergence speed, and equipped it with a tool to handle multimodalproblems. To understand the suitability of different GA and PSO techniques in theproblem of system identification, they were used in an induction motor'sparameter identification problem. The results enforced previous conclusions andshowed the superiority of PSO in general over the GA in such a multimodalproblem.
arxiv-3000-64 | Sparse Shape Reconstruction | http://arxiv.org/pdf/1303.0018v1.pdf | author:Alireza Aghasi, Justin Romberg category:math.FA cs.CV math-ph math.DG math.MP published:2013-02-28 summary:This paper introduces a new shape-based image reconstruction techniqueapplicable to a large class of imaging problems formulated in a variationalsense. Given a collection of shape priors (a shape dictionary), we define ourproblem as choosing the right elements and geometrically composing them throughbasic set operations to characterize desired regions in the image. Thiscombinatorial problem can be relaxed and then solved using classical descentmethods. The main component of this relaxation is forming certain compactlysupported functions which we call "knolls", and reformulating the shaperepresentation as a basis expansion in terms of such functions. To selectsuitable elements of the dictionary, our problem ultimately reduces to solvinga nonlinear program with sparsity constraints. We provide a new sparsenonlinear reconstruction technique to approach this problem. The performance ofproposed technique is demonstrated with some standard imaging problemsincluding image segmentation, X-ray tomography and diffusive tomography.
arxiv-3000-65 | Source Separation using Regularized NMF with MMSE Estimates under GMM Priors with Online Learning for The Uncertainties | http://arxiv.org/pdf/1302.7283v1.pdf | author:Emad M. Grais, Hakan Erdogan category:cs.LG cs.NA published:2013-02-28 summary:We propose a new method to enforce priors on the solution of the nonnegativematrix factorization (NMF). The proposed algorithm can be used for denoising orsingle-channel source separation (SCSS) applications. The NMF solution isguided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussianmixture prior models (GMM) for the source signal. In SCSS applications, thespectra of the observed mixed signal are decomposed as a weighted linearcombination of trained basis vectors for each source using NMF. In this work,the NMF decomposition weight matrices are treated as a distorted image by adistortion operator, which is learned directly from the observed signals. TheMMSE estimate of the weights matrix under GMM prior and log-normal distributionfor the distortion is then found to improve the NMF decomposition results. TheMMSE estimate is embedded within the optimization objective to form a novelregularized NMF cost function. The corresponding update rules for the newobjectives are derived in this paper. Experimental results show that, theproposed regularized NMF algorithm improves the source separation performancecompared with using NMF without prior or with other prior models.
arxiv-3000-66 | Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average | http://arxiv.org/pdf/1302.7175v2.pdf | author:Hado van Hasselt category:stat.ML cs.AI cs.LG stat.ME published:2013-02-28 summary:We investigate the accuracy of the two most common estimators for the maximumexpected value of a general set of random variables: a generalization of themaximum sample average, and cross validation. No unbiased estimator exists andwe show that it is non-trivial to select a good estimator without knowledgeabout the distributions of the random variables. We investigate and bound thebias and variance of the aforementioned estimators and prove consistency. Thevariance of cross validation can be significantly reduced, but not withoutrisking a large bias. The bias and variance of different variants of crossvalidation are shown to be very problem-dependent, and a wrong choice can leadto very inaccurate estimates.
arxiv-3000-67 | Bayesian Consensus Clustering | http://arxiv.org/pdf/1302.7280v1.pdf | author:Eric F. Lock, David B. Dunson category:stat.ML cs.LG published:2013-02-28 summary:The task of clustering a set of objects based on multiple sources of dataarises in several modern applications. We propose an integrative statisticalmodel that permits a separate clustering of the objects for each data source.These separate clusterings adhere loosely to an overall consensus clustering,and hence they are not independent. We describe a computationally scalableBayesian framework for simultaneous estimation of both the consensus clusteringand the source-specific clusterings. We demonstrate that this flexible approachis more robust than joint clustering of all data sources, and is more powerfulthan clustering each data source separately. This work is motivated by theintegrated analysis of heterogeneous biomedical data, and we present anapplication to subtype identification of breast cancer tumor samples usingpublicly available data from The Cancer Genome Atlas. Software is available athttp://people.duke.edu/~el113/software.html.
arxiv-3000-68 | Fast Matching by 2 Lines of Code for Large Scale Face Recognition Systems | http://arxiv.org/pdf/1302.7180v1.pdf | author:Dong Yi, Zhen Lei, Yang Hu, Stan Z. Li category:cs.CV published:2013-02-28 summary:In this paper, we propose a method to apply the popular cascade classifierinto face recognition to improve the computational efficiency while keepinghigh recognition rate. In large scale face recognition systems, because theprobability of feature templates coming from different subjects is very high,most of the matching pairs will be rejected by the early stages of the cascade.Therefore, the cascade can improve the matching speed significantly. On theother hand, using the nested structure of the cascade, we could drop somestages at the end of feature to reduce the memory and bandwidth usage in someresources intensive system while not sacrificing the performance too much. Thecascade is learned by two steps. Firstly, some kind of prepared features aregrouped into several nested stages. And then, the threshold of each stage islearned to achieve user defined verification rate (VR). In the paper, we take alandmark based Gabor+LDA face recognition system as baseline to illustrate theprocess and advantages of the proposed method. However, the use of this methodis very generic and not limited in face recognition, which can be easilygeneralized to other biometrics as a post-processing module. Experiments on theFERET database show the good performance of our baseline and an experiment on aself-collected large scale database illustrates that the cascade can improvethe matching speed significantly.
arxiv-3000-69 | KSU KDD: Word Sense Induction by Clustering in Topic Space | http://arxiv.org/pdf/1302.7056v1.pdf | author:Wesam Elshamy, Doina Caragea, William Hsu category:cs.CL cs.AI stat.AP stat.ML 68T05 published:2013-02-28 summary:We describe our language-independent unsupervised word sense inductionsystem. This system only uses topic features to cluster different word sensesin their global context topic space. Using unlabeled data, this system trains alatent Dirichlet allocation (LDA) topic model then uses it to infer the topicsdistribution of the test instances. By clustering these topics distributions intheir topic space we cluster them into different senses. Our hypothesis is thatcloseness in topic space reflects similarity between different word senses.This system participated in SemEval-2 word sense induction and disambiguationtask and achieved the second highest V-measure score among all other systems.
arxiv-3000-70 | Community Detection in Random Networks | http://arxiv.org/pdf/1302.7099v1.pdf | author:Ery Arias-Castro, Nicolas Verzelen category:math.ST stat.ML stat.TH published:2013-02-28 summary:We formalize the problem of detecting a community in a network into testingwhether in a given (random) graph there is a subgraph that is unusually dense.We observe an undirected and unweighted graph on N nodes. Under the nullhypothesis, the graph is a realization of an Erd\"os-R\'enyi graph withprobability p0. Under the (composite) alternative, there is a subgraph of nnodes where the probability of connection is p1 > p0. We derive a detectionlower bound for detecting such a subgraph in terms of N, n, p0, p1 and exhibita test that achieves that lower bound. We do this both when p0 is known andunknown. We also consider the problem of testing in polynomial-time. As anaside, we consider the problem of detecting a clique, which is intimatelyrelated to the planted clique problem. Our focus in this paper is in thequasi-normal regime where n p0 is either bounded away from zero, or tends tozero slowly.
arxiv-3000-71 | Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization | http://arxiv.org/pdf/1302.6677v1.pdf | author:Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman category:cs.LG cs.AI stat.ML published:2013-02-27 summary:Integration is affected by the curse of dimensionality and quickly becomesintractable as the dimensionality of the problem grows. We propose a randomizedalgorithm that, with high probability, gives a constant-factor approximation ofa general discrete integral defined over an exponentially large set. Thisalgorithm relies on solving only a small number of instances of a discretecombinatorial optimization problem subject to randomly generated parityconstraints used as a hash function. As an application, we demonstrate thatwith a small number of MAP queries we can efficiently approximate the partitionfunction of discrete graphical models, which can in turn be used, for instance,for marginal computation or model selection.
arxiv-3000-72 | A bag-of-paths framework for network data analysis | http://arxiv.org/pdf/1302.6766v1.pdf | author:Kevin Françoisse, Ilkka Kivimäki, Amin Mantrach, Fabrice Rossi, Marco Saerens category:stat.ML published:2013-02-27 summary:This work introduces a generic framework, called the bag-of-paths (BoP), thatcan be used for link and network data analysis. The primary application of thisframework, investigated in this paper, is the definition of distance measuresbetween nodes enjoying some nice properties. More precisely, let us assume aweighted directed graph G where a cost is associated to each arc. Within thiscontext, consider a bag containing all the possible paths between pairs ofnodes in G. Then, following, a probability distribution on this countable setof paths through the graph is defined by minimizing the total expected costbetween all pairs of nodes while fixing the total relative entropy spread inthe graph. This results in a Boltzmann distribution on the set of paths suchthat long (high-cost) paths have a low probability of being sampled from thebag, while short (low-cost) paths have a high probability of being sampled.Within this probabilistic framework, the BoP probabilities, P(s=i,e=j), ofdrawing a path starting from node i (s=i) and ending in node j (e=j) can easilybe computed in closed form by a simple matrix inversion. Various applicationsof this framework are currently investigated, e.g., the definition of distancemeasures between the nodes of G, betweenness indexes, network criticalitymeasures, edit distances, etc. As a first step, this paper describes thegeneral BoP framework and introduces two families of distance measures betweennodes. In addition to being a distance measure, one of these two quantities hasthe interesting property of interpolating between the shortest path and thecommute cost distances. Experimental results on semi-supervised tasks show thatthese distance families are competitive with other state-of-the-art approaches.
arxiv-3000-73 | Ending-based Strategies for Part-of-speech Tagging | http://arxiv.org/pdf/1302.6777v1.pdf | author:Greg Adams, Beth Millar, Eric Neufeld, Tim Philip category:cs.CL published:2013-02-27 summary:Probabilistic approaches to part-of-speech tagging rely primarily onwhole-word statistics about word/tag combinations as well as contextualinformation. But experience shows about 4 per cent of tokens encountered intest sets are unknown even when the training set is as large as a millionwords. Unseen words are tagged using secondary strategies that exploit wordfeatures such as endings, capitalizations and punctuation marks. In this work,word-ending statistics are primary and whole-word statistics are secondary.First, a tagger was trained and tested on word endings only. Subsequentexperiments added back whole-word statistics for the words occurring mostfrequently in the training set. As grew larger, performance was expected toimprove, in the limit performing the same as word-based taggers. Surprisingly,the ending-based tagger initially performed nearly as well as the word-basedtagger; in the best case, its performance significantly exceeded that of theword-based tagger. Lastly, and unexpectedly, an effect of negative returns wasobserved - as grew larger, performance generally improved and then declined. Byvarying factors such as ending length and tag-list strategy, we achieved asuccess rate of 97.5 percent.
arxiv-3000-74 | Learning Gaussian Networks | http://arxiv.org/pdf/1302.6808v1.pdf | author:Dan Geiger, David Heckerman category:cs.AI cs.LG stat.ML published:2013-02-27 summary:We describe algorithms for learning Bayesian networks from a combination ofuser knowledge and statistical data. The algorithms have two components: ascoring metric and a search procedure. The scoring metric takes a networkstructure, statistical data, and a user's prior knowledge, and returns a scoreproportional to the posterior probability of the network structure given thedata. The search procedure generates networks for evaluation by the scoringmetric. Previous work has concentrated on metrics for domains containing onlydiscrete variables, under the assumption that data represents a multinomialsample. In this paper, we extend this work, developing scoring metrics fordomains containing all continuous variables or a mixture of discrete andcontinuous variables, under the assumption that continuous data is sampled froma multivariate normal distribution. Our work extends traditional statisticalapproaches for identifying vanishing regression coefficients in that weidentify two important assumptions, called event equivalence and parametermodularity, that when combined allow the construction of prior distributionsfor multivariate normal parameters from a single prior Bayesian networkspecified by a user.
arxiv-3000-75 | Induction of Selective Bayesian Classifiers | http://arxiv.org/pdf/1302.6828v1.pdf | author:Pat Langley, Stephanie Sage category:cs.LG stat.ML published:2013-02-27 summary:In this paper, we examine previous work on the naive Bayesian classifier andreview its limitations, which include a sensitivity to correlated features. Werespond to this problem by embedding the naive Bayesian induction scheme withinan algorithm that c arries out a greedy search through the space of features.We hypothesize that this approach will improve asymptotic accuracy in domainsthat involve correlated features without reducing the rate of learning in onesthat do not. We report experimental results on six natural domains, includingcomparisons with decision-tree induction, that support these hypotheses. Inclosing, we discuss other approaches to extending naive Bayesian classifiersand outline some directions for future research.
arxiv-3000-76 | Missing Entries Matrix Approximation and Completion | http://arxiv.org/pdf/1302.6768v2.pdf | author:Gil Shabat, Yaniv Shmueli, Amir Averbuch category:math.NA cs.LG stat.ML published:2013-02-27 summary:We describe several algorithms for matrix completion and matrix approximationwhen only some of its entries are known. The approximation constraint can beany whose approximated solution is known for the full matrix. For low rankapproximations, similar algorithms appears recently in the literature underdifferent names. In this work, we introduce new theorems for matrixapproximation and show that these algorithms can be extended to handledifferent constraints such as nuclear norm, spectral norm, orthogonalityconstraints and more that are different than low rank approximations. As thealgorithms can be viewed from an optimization point of view, we discuss theirconvergence to global solution for the convex case. We also discuss the optimalstep size and show that it is fixed in each iteration. In addition, the derivedmatrix completion flow is robust and does not require any parameters. Thismatrix completion flow is applicable to different spectral minimizations andcan be applied to physics, mathematics and electrical engineering problems suchas data reconstruction of images and data coming from PDEs such as Helmholtzequation used for electromagnetic waves.
arxiv-3000-77 | Online Learning for Time Series Prediction | http://arxiv.org/pdf/1302.6927v1.pdf | author:Oren Anava, Elad Hazan, Shie Mannor, Ohad Shamir category:cs.LG published:2013-02-27 summary:In this paper we address the problem of predicting a time series using theARMA (autoregressive moving average) model, under minimal assumptions on thenoise terms. Using regret minimization techniques, we develop effective onlinelearning algorithms for the prediction problem, without assuming that the noiseterms are Gaussian, identically distributed or even independent. Furthermore,we show that our algorithm's performances asymptotically approaches theperformance of the best ARMA model in hindsight.
arxiv-3000-78 | Ensemble Sparse Models for Image Analysis | http://arxiv.org/pdf/1302.6957v1.pdf | author:Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Andreas Spanias category:cs.CV published:2013-02-27 summary:Sparse representations with learned dictionaries have been successful inseveral image analysis applications. In this paper, we propose and analyze theframework of ensemble sparse models, and demonstrate their utility in imagerestoration and unsupervised clustering. The proposed ensemble modelapproximates the data as a linear combination of approximations from multiple\textit{weak} sparse models. Theoretical analysis of the ensemble model revealsthat even in the worst-case, the ensemble can perform better than any of itsconstituent individual models. The dictionaries corresponding to the individualsparse models are obtained using either random example selection or boostedapproaches. Boosted approaches learn one dictionary per round such that thedictionary learned in a particular round is optimized for the training exampleshaving high reconstruction error in the previous round. Results with compressedrecovery show that the ensemble representations lead to a better performancecompared to using a single dictionary obtained with the conventionalalternating minimization approach. The proposed ensemble models are also usedfor single image superresolution, and we show that they perform comparably tothe recent approaches. In unsupervised clustering, experiments show that theproposed model performs better than baseline approaches in several standarddatasets.
arxiv-3000-79 | Categorizing Bugs with Social Networks: A Case Study on Four Open Source Software Communities | http://arxiv.org/pdf/1302.6764v2.pdf | author:Marcelo Serrano Zanetti, Ingo Scholtes, Claudio Juan Tessone, Frank Schweitzer category:cs.SE cs.LG cs.SI nlin.AO physics.soc-ph published:2013-02-27 summary:Efficient bug triaging procedures are an important precondition forsuccessful collaborative software engineering projects. Triaging bugs canbecome a laborious task particularly in open source software (OSS) projectswith a large base of comparably inexperienced part-time contributors. In thispaper, we propose an efficient and practical method to identify valid bugreports which a) refer to an actual software bug, b) are not duplicates and c)contain enough information to be processed right away. Our classification isbased on nine measures to quantify the social embeddedness of bug reporters inthe collaboration network. We demonstrate its applicability in a case study,using a comprehensive data set of more than 700,000 bug reports obtained fromthe Bugzilla installation of four major OSS communities, for a period of morethan ten years. For those projects that exhibit the lowest fraction of validbug reports, we find that the bug reporters' position in the collaborationnetwork is a strong indicator for the quality of bug reports. Based on thisfinding, we develop an automated classification scheme that can easily beintegrated into bug tracking platforms and analyze its performance in theconsidered OSS communities. A support vector machine (SVM) to identify validbug reports based on the nine measures yields a precision of up to 90.3% withan associated recall of 38.9%. With this, we significantly improve the resultsobtained in previous case studies for an automated early identification of bugsthat are eventually fixed. Furthermore, our study highlights the potential ofusing quantitative measures of social organization in collaborative softwareengineering. It also opens a broad perspective for the integration of socialawareness in the design of support infrastructures.
arxiv-3000-80 | Online Convex Optimization Against Adversaries with Memory and Application to Statistical Arbitrage | http://arxiv.org/pdf/1302.6937v2.pdf | author:Oren Anava, Elad Hazan, Shie Mannor category:cs.LG published:2013-02-27 summary:The framework of online learning with memory naturally captures learningproblems with temporal constraints, and was previously studied for the expertssetting. In this work we extend the notion of learning with memory to thegeneral Online Convex Optimization (OCO) framework, and present two algorithmsthat attain low regret. The first algorithm applies to Lipschitz continuousloss functions, obtaining optimal regret bounds for both convex and stronglyconvex losses. The second algorithm attains the optimal regret bounds andapplies more broadly to convex losses without requiring Lipschitz continuity,yet is more complicated to implement. We complement our theoretic results withan application to statistical arbitrage in finance: we devise algorithms forconstructing mean-reverting portfolios.
arxiv-3000-81 | Spectrum Bandit Optimization | http://arxiv.org/pdf/1302.6974v4.pdf | author:Marc Lelarge, Alexandre Proutiere, M. Sadegh Talebi category:cs.LG cs.NI math.OC published:2013-02-27 summary:We consider the problem of allocating radio channels to links in a wirelessnetwork. Links interact through interference, modelled as a conflict graph(i.e., two interfering links cannot be simultaneously active on the samechannel). We aim at identifying the channel allocation maximizing the totalnetwork throughput over a finite time horizon. Should we know the average radioconditions on each channel and on each link, an optimal allocation would beobtained by solving an Integer Linear Program (ILP). When radio conditions areunknown a priori, we look for a sequential channel allocation policy thatconverges to the optimal allocation while minimizing on the way the throughputloss or {\it regret} due to the need for exploring sub-optimal allocations. Weformulate this problem as a generic linear bandit problem, and analyze it firstin a stochastic setting where radio conditions are driven by a stationarystochastic process, and then in an adversarial setting where radio conditionscan evolve arbitrarily. We provide new algorithms in both settings and deriveupper bounds on their regrets.
arxiv-3000-82 | Variational Algorithms for Marginal MAP | http://arxiv.org/pdf/1302.6584v3.pdf | author:Qiang Liu, Alexander Ihler category:stat.ML cs.AI cs.IT cs.LG math.IT published:2013-02-26 summary:The marginal maximum a posteriori probability (MAP) estimation problem, whichcalculates the mode of the marginal posterior distribution of a subset ofvariables with the remaining variables marginalized, is an important inferenceproblem in many models, such as those with hidden variables or uncertainparameters. Unfortunately, marginal MAP can be NP-hard even on trees, and hasattracted less attention in the literature compared to the joint MAP(maximization) and marginalization problems. We derive a general dualrepresentation for marginal MAP that naturally integrates the marginalizationand maximization operations into a joint variational optimization problem,making it possible to easily extend most or all variational-based algorithms tomarginal MAP. In particular, we derive a set of "mixed-product" message passingalgorithms for marginal MAP, whose form is a hybrid of max-product, sum-productand a novel "argmax-product" message updates. We also derive a class ofconvergent algorithms based on proximal point methods, including one thattransforms the marginal MAP problem into a sequence of standard marginalizationproblems. Theoretically, we provide guarantees under which our algorithms giveglobally or locally optimal solutions, and provide novel upper bounds on theoptimal objectives. Empirically, we demonstrate that our algorithmssignificantly outperform the existing approaches, including a state-of-the-artalgorithm based on local search methods.
arxiv-3000-83 | Convex vs nonconvex approaches for sparse estimation: GLasso, Multiple Kernel Learning and Hyperparameter GLasso | http://arxiv.org/pdf/1302.6434v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Alessandro Chiuso, Gianluigi Pillonetto category:stat.ML math.OC published:2013-02-26 summary:The popular Lasso approach for sparse estimation can be derived viamarginalization of a joint density associated with a particular stochasticmodel. A different marginalization of the same probabilistic model leads to adifferent non-convex estimator where hyperparameters are optimized. Extendingthese arguments to problems where groups of variables have to be estimated, westudy a computational scheme for sparse estimation that differs from the GroupLasso. Although the underlying optimization problem defining this estimator isnon-convex, an initialization strategy based on a univariate Bayesian forwardselection scheme is presented. This also allows us to define an effectivenon-convex estimator where only one scalar variable is involved in theoptimization process. Theoretical arguments, independent of the correctness ofthe priors entering the sparse model, are included to clarify the advantages ofthis non-convex technique in comparison with other convex estimators. Numericalexperiments are also used to compare the performance of these approaches.
arxiv-3000-84 | Geodesic-based Salient Object Detection | http://arxiv.org/pdf/1302.6557v2.pdf | author:Richard M Jiang category:cs.CV cs.AI published:2013-02-26 summary:Saliency detection has been an intuitive way to provide useful cues forobject detection and segmentation, as desired for many vision and graphicsapplications. In this paper, we provided a robust method for salient objectdetection and segmentation. Other than using various pixel-level contrastdefinitions, we exploited global image structures and proposed a new geodesicmethod dedicated for salient object detection. In the proposed approach, a newgeodesic scheme, namely geodesic tunneling is proposed to tackle with texturesand local chaotic structures. With our new geodesic approach, a geodesicsaliency map is estimated in correspondence to spatial structures in an image.Experimental evaluation on a salient object benchmark dataset validated thatour algorithm consistently outperformed a number of the state-of-art saliencymethods, yielding higher precision and better recall rates. With the robustsaliency estimation, we also present an unsupervised hierarchical salientobject cut scheme simply using adaptive saliency thresholding, which attainedthe highest score in our F-measure test. We also applied our geodesic cutscheme to a number of image editing tasks as demonstrated in additionalexperiments.
arxiv-3000-85 | PSO based Neural Networks vs. Traditional Statistical Models for Seasonal Time Series Forecasting | http://arxiv.org/pdf/1302.6615v1.pdf | author:Ratnadip Adhikari, R. K. Agrawal, Laxmi Kant category:cs.NE 68T05 published:2013-02-26 summary:Seasonality is a distinctive characteristic which is often observed in manypractical time series. Artificial Neural Networks (ANNs) are a class ofpromising models for efficiently recognizing and forecasting seasonal patterns.In this paper, the Particle Swarm Optimization (PSO) approach is used toenhance the forecasting strengths of feedforward ANN (FANN) as well as ElmanANN (EANN) models for seasonal data. Three widely popular versions of the basicPSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.The empirical analysis is conducted on three real-world seasonal time series.Results clearly show that each version of the PSO algorithm achieves notablybetter forecasting accuracies than the standard Backpropagation (BP) trainingmethod for both FANN and EANN models. The neural network forecasting resultsare also compared with those from the three traditional statistical models,viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters(HW) and Support Vector Machine (SVM). The comparison demonstrates that bothPSO and BP based neural networks outperform SARIMA, HW and SVM models for allthree time series datasets. The forecasting performances of ANNs are furtherimproved through combining the outputs from the three PSO based models.
arxiv-3000-86 | An Introductory Study on Time Series Modeling and Forecasting | http://arxiv.org/pdf/1302.6613v1.pdf | author:Ratnadip Adhikari, R. K. Agrawal category:cs.LG stat.ML 68T01 published:2013-02-26 summary:Time series modeling and forecasting has fundamental importance to variouspractical domains. Thus a lot of active research works is going on in thissubject during several years. Many important models have been proposed inliterature for improving the accuracy and effectiveness of time seriesforecasting. The aim of this dissertation work is to present a concisedescription of some popular time series forecasting models used in practice,with their salient features. In this thesis, we have described three importantclasses of time series models, viz. the stochastic, neural networks and SVMbased models, together with their inherent forecasting strengths andweaknesses. We have also discussed about the basic issues related to timeseries modeling, such as stationarity, parsimony, overfitting, etc. Ourdiscussion about different time series models is supported by giving theexperimental forecast results, performed on six real time series datasets.While fitting a model to a dataset, special care is taken to select the mostparsimonious one. To evaluate forecast accuracy as well as to compare amongdifferent models fitted to a time series, we have used the five performancemeasures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of thesix datasets, we have shown the obtained forecast diagram which graphicallydepicts the closeness between the original and forecasted observations. To haveauthenticity as well as clarity in our discussion about time series modelingand forecasting, we have taken the help of various published research worksfrom reputed journals and some standard books.
arxiv-3000-87 | Sparse Frequency Analysis with Sparse-Derivative Instantaneous Amplitude and Phase Functions | http://arxiv.org/pdf/1302.6523v1.pdf | author:Yin Ding, Ivan W. Selesnick category:cs.LG published:2013-02-26 summary:This paper addresses the problem of expressing a signal as a sum of frequencycomponents (sinusoids) wherein each sinusoid may exhibit abrupt changes in itsamplitude and/or phase. The Fourier transform of a narrow-band signal, with adiscontinuous amplitude and/or phase function, exhibits spectral and temporalspreading. The proposed method aims to avoid such spreading by explicitlymodeling the signal of interest as a sum of sinusoids with time-varyingamplitudes. So as to accommodate abrupt changes, it is further assumed that theamplitude/phase functions are approximately piecewise constant (i.e., theirtime-derivatives are sparse). The proposed method is based on a convexvariational (optimization) approach wherein the total variation (TV) of theamplitude functions are regularized subject to a perfect (or approximate)reconstruction constraint. A computationally efficient algorithm is derivedbased on convex optimization techniques. The proposed technique can be used toperform band-pass filtering that is relatively insensitive to narrow-bandamplitude/phase jumps present in data, which normally pose a challenge (due totransients, leakage, etc.). The method is illustrated using both syntheticsignals and human EEG data for the purpose of band-pass filtering and theestimation of phase synchrony indexes.
arxiv-3000-88 | A Conformal Prediction Approach to Explore Functional Data | http://arxiv.org/pdf/1302.6452v1.pdf | author:Jing Lei, Alessandro Rinaldo, Larry Wasserman category:stat.ML cs.LG published:2013-02-26 summary:This paper applies conformal prediction techniques to compute simultaneousprediction bands and clustering trees for functional data. These tools can beused to detect outliers and clusters. Both our prediction bands and clusteringtrees provide prediction sets for the underlying stochastic process with aguaranteed finite sample behavior, under no distributional assumptions. Theprediction sets are also informative in that they correspond to the highdensity region of the underlying process. While ordinary conformal predictionhas high computational cost for functional data, we use the inductive conformalpredictor, together with several novel choices of conformity scores, tosimplify the computation. Our methods are illustrated on some real dataexamples.
arxiv-3000-89 | Segmentation of Alzheimers Disease in PET scan datasets using MATLAB | http://arxiv.org/pdf/1302.6426v1.pdf | author:A. Meena, K. Raja category:cs.NE published:2013-02-26 summary:Positron Emission Tomography (PET) scan images are one of the bio medicalimaging techniques similar to that of MRI scan images but PET scan images arehelpful in finding the development of tumors.The PET scan images requiresexpertise in the segmentation where clustering plays an important role in theautomation process.The segmentation of such images is manual to automate theprocess clustering is used.Clustering is commonly known as unsupervisedlearning process of n dimensional data sets are clustered into k groups so asto maximize the inter cluster similarity and to minimize the intra clustersimilarity.This paper is proposed to implement the commonly used K Means andFuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrixLABoratory (MATLAB) and tested with sample PET scan image. The sample data iscollected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical ImageProcessing and Visualization Tool (MIPAV) are used to compare the resultantimages.
arxiv-3000-90 | The adaptive Gril estimator with a diverging number of parameters | http://arxiv.org/pdf/1302.6390v1.pdf | author:Mohammed El Anbari, Abdallah Mkhadri category:stat.ME cs.LG published:2013-02-26 summary:We consider the problem of variables selection and estimation in linearregression model in situations where the number of parameters diverges with thesample size. We propose the adaptive Generalized Ridge-Lasso (\mbox{AdaGril})which is an extension of the the adaptive Elastic Net. AdaGril incorporatesinformation redundancy among correlated variables for model selection andestimation. It combines the strengths of the quadratic regularization and theadaptively weighted Lasso shrinkage. In this paper, we highlight the groupedselection property for AdaCnet method (one type of AdaGril) in the equalcorrelation case. Under weak conditions, we establish the oracle property ofAdaGril which ensures the optimal large performance when the dimension is high.Consequently, it achieves both goals of handling the problem of collinearity inhigh dimension and enjoys the oracle property. Moreover, we show that AdaGrilestimator achieves a Sparsity Inequality, i. e., a bound in terms of the numberof non-zero components of the 'true' regression coefficient. This bound isobtained under a similar weak Restricted Eigenvalue (RE) condition used forLasso. Simulations studies show that some particular cases of AdaGriloutperform its competitors.
arxiv-3000-91 | Image-based Face Detection and Recognition: "State of the Art" | http://arxiv.org/pdf/1302.6379v1.pdf | author:Faizan Ahmad, Aaima Najam, Zeeshan Ahmed category:cs.CV published:2013-02-26 summary:Face recognition from image or video is a popular topic in biometricsresearch. Many public places usually have surveillance cameras for videocapture and these cameras have their significant value for security purpose. Itis widely acknowledged that the face recognition have played an important rolein surveillance system as it doesn't need the object's cooperation. The actualadvantages of face based identification over other biometrics are uniquenessand acceptance. As human face is a dynamic object having high degree ofvariability in its appearance, that makes face detection a difficult problem incomputer vision. In this field, accuracy and speed of identification is a mainissue. The goal of this paper is to evaluate various face detection and recognitionmethods, provide complete solution for image based face detection andrecognition with higher accuracy, better response rate as an initial step forvideo surveillance. Solution is proposed based on performed tests on variousface rich databases in terms of subjects, pose, emotions, race and light.
arxiv-3000-92 | ML4PG in Computer Algebra verification | http://arxiv.org/pdf/1302.6421v3.pdf | author:Jónathan Heras, Ekaterina Komendantskaya category:cs.LO cs.LG published:2013-02-26 summary:ML4PG is a machine-learning extension that provides statistical proof hintsduring the process of Coq/SSReflect proof development. In this paper, we useML4PG to find proof patterns in the CoqEAL library -- a library that wasdevised to verify the correctness of Computer Algebra algorithms. Inparticular, we use ML4PG to help us in the formalisation of an efficientalgorithm to compute the inverse of triangular matrices.
arxiv-3000-93 | Arriving on time: estimating travel time distributions on large-scale road networks | http://arxiv.org/pdf/1302.6617v1.pdf | author:Timothy Hunter, Aude Hofleitner, Jack Reilly, Walid Krichene, Jerome Thai, Anastasios Kouvelas, Pieter Abbeel, Alexandre Bayen category:cs.LG cs.AI published:2013-02-26 summary:Most optimal routing problems focus on minimizing travel time or distancetraveled. Oftentimes, a more useful objective is to maximize the probability ofon-time arrival, which requires statistical distributions of travel times,rather than just mean values. We propose a method to estimate travel timedistributions on large-scale road networks, using probe vehicle data collectedfrom GPS. We present a framework that works with large input of data, andscales linearly with the size of the network. Leveraging the planar topology ofthe graph, the method computes efficiently the time correlations betweenneighboring streets. First, raw probe vehicle traces are compressed into pairsof travel times and number of stops for each traversed road segment using a`stop-and-go' algorithm developed for this work. The compressed data is thenused as input for training a path travel time model, which couples a Markovmodel along with a Gaussian Markov random field. Finally, scalable inferencealgorithms are developed for obtaining path travel time distributions from thecomposite MM-GMRF model. We illustrate the accuracy and scalability of ourmodel on a 505,000 road link network spanning the San Francisco Bay Area.
arxiv-3000-94 | Non-simplifying Graph Rewriting Termination | http://arxiv.org/pdf/1302.6334v1.pdf | author:Guillaume Bonfante, Bruno Guillaume category:cs.CL cs.CC cs.LO published:2013-02-26 summary:So far, a very large amount of work in Natural Language Processing (NLP) relyon trees as the core mathematical structure to represent linguisticinformations (e.g. in Chomsky's work). However, some linguistic phenomena donot cope properly with trees. In a former paper, we showed the benefit ofencoding linguistic structures by graphs and of using graph rewriting rules tocompute on those structures. Justified by some linguistic considerations, graphrewriting is characterized by two features: first, there is no node creationalong computations and second, there are non-local edge modifications. Underthese hypotheses, we show that uniform termination is undecidable and thatnon-uniform termination is decidable. We describe two termination techniquesbased on weights and we give complexity bound on the derivation length forthese rewriting system.
arxiv-3000-95 | Rate-Distortion Bounds for an Epsilon-Insensitive Distortion Measure | http://arxiv.org/pdf/1302.6315v1.pdf | author:Kazuho Watanabe category:cs.IT cs.LG math.IT published:2013-02-26 summary:Direct evaluation of the rate-distortion function has rarely been achievedwhen it is strictly greater than its Shannon lower bound. In this paper, weconsider the rate-distortion function for the distortion measure defined by anepsilon-insensitive loss function. We first present the Shannon lower boundapplicable to any source distribution with finite differential entropy. Then,focusing on the Laplacian and Gaussian sources, we prove that therate-distortion functions of these sources are strictly greater than theirShannon lower bounds and obtain analytically evaluable upper bounds for therate-distortion functions. Small distortion limit and numerical evaluation ofthe bounds suggest that the Shannon lower bound provides a good approximationto the rate-distortion function for the epsilon-insensitive distortion measure.
arxiv-3000-96 | Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining Techniques | http://arxiv.org/pdf/1302.6310v1.pdf | author:Adesesan . B Adeyemo, Adebola A. Oketola, Emmanuel O. Adetula, O. Osibanjo category:cs.NE published:2013-02-26 summary:Industrial pollution is often considered to be one of the prime factorscontributing to air, water and soil pollution. Sectoral pollution loads(ton/yr) into different media (i.e. air, water and land) in Lagos wereestimated using Industrial Pollution Projected System (IPPS). These werefurther studied using Artificial neural Networks (ANNs), a data miningtechnique that has the ability of detecting and describing patterns in largedata sets with variables that are non- linearly related. Time Lagged RecurrentNetwork (TLRN) appeared as the best Neural Network model among all the neuralnetworks considered which includes Multilayer Perceptron (MLP) Network,Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)Network and Recurrent Network (RN). TLRN modelled the data-sets better than theothers in terms of the mean average error (MAE) (0.14), time (39 s) and linearcorrelation coefficient (0.84). The results showed that Artificial NeuralNetworks (ANNs) technique (i.e., Time Lagged Recurrent Network) is alsoapplicable and effective in environmental assessment study. Keywords:Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial PollutionProjection System (IPPS), Pollution load, Pollution Intensity.
arxiv-3000-97 | A Meta-Theory of Boundary Detection Benchmarks | http://arxiv.org/pdf/1302.5985v1.pdf | author:Xiaodi Hou, Alan Yuille, Christof Koch category:cs.CV published:2013-02-25 summary:Human labeled datasets, along with their corresponding evaluation algorithms,play an important role in boundary detection. We here present a psychophysicalexperiment that addresses the reliability of such benchmarks. To find betterremedies to evaluate the performance of any boundary detection algorithm, wepropose a computational framework to remove inappropriate human labels andestimate the intrinsic properties of boundaries.
arxiv-3000-98 | A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting | http://arxiv.org/pdf/1302.6210v1.pdf | author:Ratnadip Adhikari, R. K. Agrawal category:cs.NE cs.LG 68T05 published:2013-02-25 summary:Enhancing the robustness and accuracy of time series forecasting models is anactive area of research. Recently, Artificial Neural Networks (ANNs) have foundextensive applications in many practical forecasting problems. However, thestandard backpropagation ANN training algorithm has some critical issues, e.g.it has a slow convergence rate and often converges to a local minimum, thecomplex pattern of error surfaces, lack of proper training parameters selectionmethods, etc. To overcome these drawbacks, various improved training methodshave been developed in literature; but, still none of them can be guaranteed asthe best for all problems. In this paper, we propose a novel weighted ensemblescheme which intelligently combines multiple training algorithms to increasethe ANN forecast accuracies. The weight for each training algorithm isdetermined from the performance of the corresponding ANN model on thevalidation dataset. Experimental results on four important time series depictsthat our proposed technique reduces the mentioned shortcomings of individualANN training algorithms to a great extent. Also it achieves significantlybetter forecast accuracies than two other popular statistical models.
arxiv-3000-99 | Phoneme discrimination using $KS$-algebra II | http://arxiv.org/pdf/1302.6194v1.pdf | author:Ondrej Such, Lenka Mackovicova category:cs.SD cs.LG stat.ML I.2.7; I.5.4 published:2013-02-25 summary:$KS$-algebra consists of expressions constructed with four kinds operations,the minimum, maximum, difference and additively homogeneous generalized means.Five families of $Z$-classifiers are investigated on binary classificationtasks between English phonemes. It is shown that the classifiers are able toreflect well known formant characteristics of vowels, while having very smallKolmogoroff's complexity.
arxiv-3000-100 | Phoneme discrimination using KS algebra I | http://arxiv.org/pdf/1302.6031v1.pdf | author:Ondrej Such category:cs.SD cs.AI cs.NE published:2013-02-25 summary:In our work we define a new algebra of operators as a substitute for fuzzylogic. Its primary purpose is for construction of binary discriminators forphonemes based on spectral content. It is optimized for design ofnon-parametric computational circuits, and makes uses of 4 operations: $\min$,$\max$, the difference and generalized additively homogenuous means.
arxiv-3000-101 | On learning parametric-output HMMs | http://arxiv.org/pdf/1302.6009v1.pdf | author:Aryeh Kontorovich, Boaz Nadler, Roi Weiss category:cs.LG math.ST stat.ML stat.TH published:2013-02-25 summary:We present a novel approach for learning an HMM whose outputs are distributedaccording to a parametric family. This is done by {\em decoupling} the learningtask into two steps: first estimating the output parameters, and thenestimating the hidden states transition probabilities. The first step isaccomplished by fitting a mixture model to the output stationary distribution.Given the parameters of this mixture model, the second step is formulated asthe solution of an easily solvable convex quadratic program. We provide anerror analysis for the estimated transition probabilities and show they arerobust to small perturbations in the estimates of the mixture parameters.Finally, we support our analysis with some encouraging empirical results.
arxiv-3000-102 | Image restoration using sparse approximations of spatially varying blur operators in the wavelet domain | http://arxiv.org/pdf/1302.6105v2.pdf | author:Paul Escande, Pierre Weiss, Francois Malgouyres category:math.OC cs.CV math.NA published:2013-02-25 summary:Restoration of images degraded by spatially varying blurs is an issue ofincreasing importance in the context of photography, satellite or microscopyimaging. One of the main difficulty to solve this problem comes from the hugedimensions of the blur matrix. It prevents the use of naive approaches forperforming matrix-vector multiplications. In this paper, we propose toapproximate the blur operator by a matrix sparse in the wavelet domain. Wejustify this approach from a mathematical point of view and investigate theapproximation quality numerically. We finish by showing that the sparsitypattern of the matrix can be pre-defined, which is central in tasks such asblind deconvolution.
arxiv-3000-103 | Four Side Distance: A New Fourier Shape Signature | http://arxiv.org/pdf/1302.5894v1.pdf | author:Sonya Eini, Abdolah Chalechale category:cs.CV published:2013-02-24 summary:Shape is one of the main features in content based image retrieval (CBIR).This paper proposes a new shape signature. In this technique, features of eachshape are extracted based on four sides of the rectangle that covers the shape.The proposed technique is Fourier based and it is invariant to translation,scaling and rotation. The retrieval performance between some commonly usedFourier based signatures and the proposed four sides distance (FSD) signaturehas been tested using MPEG-7 database. Experimental results are shown that theFSD signature has better performance compared with those signatures.
arxiv-3000-104 | Shape Characterization via Boundary Distortion | http://arxiv.org/pdf/1302.5957v1.pdf | author:Xavier Descombes, Serguei Komech category:cs.CV published:2013-02-24 summary:In this paper, we derive new shape descriptors based on a directionalcharacterization. The main idea is to study the behavior of the shapeneighborhood under family of transformations. We obtain a description invariantwith respect to rotation, reflection, translation and scaling. A well-definedmetric is then proposed on the associated feature space. We show the continuityof this metric. Some results on shape retrieval are provided on two databasesto show the accuracy of the proposed shape metric.
arxiv-3000-105 | Probabilistic Non-Local Means | http://arxiv.org/pdf/1302.5762v1.pdf | author:Yue Wu, Brian Tracey, Premkumar Natarajan, Joseph P. Noonan category:cs.CV stat.AP stat.CO published:2013-02-23 summary:In this paper, we propose a so-called probabilistic non-local means (PNLM)method for image denoising. Our main contributions are: 1) we point out defectsof the weight function used in the classic NLM; 2) we successfully derive alltheoretical statistics of patch-wise differences for Gaussian noise; and 3) weemploy this prior information and formulate the probabilistic weights trulyreflecting the similarity between two noisy patches. The probabilistic natureof the new weight function also provides a theoretical basis to choosethresholds rejecting dissimilar patches for fast computations. Our simulationresults indicate the PNLM outperforms the classic NLM and many NLM recentvariants in terms of peak signal noise ratio (PSNR) and structural similarity(SSIM) index. Encouraging improvements are also found when we replace the NLMweights with the probabilistic weights in tested NLM variants.
arxiv-3000-106 | Prediction by Random-Walk Perturbation | http://arxiv.org/pdf/1302.5797v1.pdf | author:Luc Devroye, Gábor Lugosi, Gergely Neu category:cs.LG published:2013-02-23 summary:We propose a version of the follow-the-perturbed-leader online predictionalgorithm in which the cumulative losses are perturbed by independent symmetricrandom walks. The forecaster is shown to achieve an expected regret of theoptimal order O(sqrt(n log N)) where n is the time horizon and N is the numberof experts. More importantly, it is shown that the forecaster changes itsprediction at most O(sqrt(n log N)) times, in expectation. We also extend theanalysis to online combinatorial optimization and show that even in this moregeneral setting, the forecaster rarely switches between experts while having aregret of near-optimal order.
arxiv-3000-107 | Sparse Signal Estimation by Maximally Sparse Convex Optimization | http://arxiv.org/pdf/1302.5729v3.pdf | author:Ivan W. Selesnick, Ilker Bayram category:cs.LG stat.ML published:2013-02-22 summary:This paper addresses the problem of sparsity penalized least squares forapplications in sparse signal processing, e.g. sparse deconvolution. This paperaims to induce sparsity more strongly than L1 norm regularization, whileavoiding non-convex optimization. For this purpose, this paper describes thedesign and use of non-convex penalty functions (regularizers) constrained so asto ensure the convexity of the total cost function, F, to be minimized. Themethod is based on parametric penalty functions, the parameters of which areconstrained to ensure convexity of F. It is shown that optimal parameters canbe obtained by semidefinite programming (SDP). This maximally sparse convex(MSC) approach yields maximally non-convex sparsity-inducing penalty functionsconstrained such that the total cost function, F, is convex. It is demonstratedthat iterative MSC (IMSC) can yield solutions substantially more sparse thanthe standard convex sparsity-inducing approach, i.e., L1 norm minimization.
arxiv-3000-108 | Development of Yes/No Arabic Question Answering System | http://arxiv.org/pdf/1302.5675v1.pdf | author:Wafa N. Bdour, Natheer K. Gharaibeh category:cs.CL cs.IR 14J26 published:2013-02-22 summary:Developing Question Answering systems has been one of the important researchissues because it requires insights from a variety ofdisciplines,including,Artificial Intelligence,Information Retrieval,Information Extraction,Natural Language Processing, and Psychology.In thispaper we realize a formal model for a lightweight semantic based open domainyes/no Arabic question answering system based on paragraph retrieval withvariable length. We propose a constrained semantic representation. Using anexplicit unification framework based on semantic similarities and queryexpansion synonyms and antonyms.This frequently improves the precision of thesystem. Employing the passage retrieval system achieves a better precision byretrieving more paragraphs that contain relevant answers to the question; Itsignificantly reduces the amount of text to be processed by the system.
arxiv-3000-109 | Accelerated Linear SVM Training with Adaptive Variable Selection Frequencies | http://arxiv.org/pdf/1302.5608v1.pdf | author:Tobias Glasmachers, Ürün Dogan category:stat.ML cs.LG published:2013-02-22 summary:Support vector machine (SVM) training is an active research area since thedawn of the method. In recent years there has been increasing interest inspecialized solvers for the important case of linear models. The algorithmpresented by Hsieh et al., probably best known under the name of the"liblinear" implementation, marks a major breakthrough. The method is analog toestablished dual decomposition algorithms for training of non-linear SVMs, butwith greatly reduced computational complexity per update step. This comes atthe cost of not keeping track of the gradient of the objective any more, whichexcludes the application of highly developed working set selection algorithms.We present an algorithmic improvement to this method. We replace uniformworking set selection with an online adaptation of selection frequencies. Theadaptation criterion is inspired by modern second order working set selectionmethods. The same mechanism replaces the shrinking heuristic. This noveltechnique speeds up training in some cases by more than an order of magnitude.
arxiv-3000-110 | Self-similar prior and wavelet bases for hidden incompressible turbulent motion | http://arxiv.org/pdf/1302.5554v2.pdf | author:Patrick Héas, Frédéric Lavancier, Souleymane Kadri-Harouna category:stat.AP cs.CV cs.NA published:2013-02-22 summary:This work is concerned with the ill-posed inverse problem of estimatingturbulent flows from the observation of an image sequence. From a Bayesianperspective, a divergence-free isotropic fractional Brownian motion (fBm) ischosen as a prior model for instantaneous turbulent velocity fields. Thisself-similar prior characterizes accurately second-order statistics of velocityfields in incompressible isotropic turbulence. Nevertheless, the associatedmaximum a posteriori involves a fractional Laplacian operator which is delicateto implement in practice. To deal with this issue, we propose to decompose thedivergent-free fBm on well-chosen wavelet bases. As a first alternative, wepropose to design wavelets as whitening filters. We show that these filters arefractional Laplacian wavelets composed with the Leray projector. As a secondalternative, we use a divergence-free wavelet basis, which takes implicitlyinto account the incompressibility constraint arising from physics. Althoughthe latter decomposition involves correlated wavelet coefficients, we are ableto handle this dependence in practice. Based on these two waveletdecompositions, we finally provide effective and efficient algorithms toapproach the maximum a posteriori. An intensive numerical evaluation proves therelevance of the proposed wavelet-based self-similar priors.
arxiv-3000-111 | The Importance of Clipping in Neurocontrol by Direct Gradient Descent on the Cost-to-Go Function and in Adaptive Dynamic Programming | http://arxiv.org/pdf/1302.5565v1.pdf | author:Michael Fairbank category:cs.LG published:2013-02-22 summary:In adaptive dynamic programming, neurocontrol and reinforcement learning, theobjective is for an agent to learn to choose actions so as to minimise a totalcost function. In this paper we show that when discretized time is used tomodel the motion of the agent, it can be very important to do "clipping" on themotion of the agent in the final time step of the trajectory. By clipping wemean that the final time step of the trajectory is to be truncated such thatthe agent stops exactly at the first terminal state reached, and no distancefurther. We demonstrate that when clipping is omitted, learning performance canfail to reach the optimum; and when clipping is done properly, learningperformance can improve significantly. The clipping problem we describe affects algorithms which use explicitderivatives of the model functions of the environment to calculate a learninggradient. These include Backpropagation Through Time for Control, and methodsbased on Dual Heuristic Dynamic Programming. However the clipping problem doesnot significantly affect methods based on Heuristic Dynamic Programming,Temporal Differences or Policy Gradient Learning algorithms. Similarly, theclipping problem does not affect fixed-length finite-horizon problems.
arxiv-3000-112 | On the performance of a hybrid genetic algorithm in dynamic environments | http://arxiv.org/pdf/1302.5474v2.pdf | author:Quan Yuan, Zhixin Yang category:cs.NE math.OC 68T20 published:2013-02-22 summary:The ability to track the optimum of dynamic environments is important in manypractical applications. In this paper, the capability of a hybrid geneticalgorithm (HGA) to track the optimum in some dynamic environments isinvestigated for different functional dimensions, update frequencies, anddisplacement strengths in different types of dynamic environments. Experimentalresults are reported by using the HGA and some other existing evolutionaryalgorithms in the literature. The results show that the HGA has bettercapability to track the dynamic optimum than some other existing algorithms.
arxiv-3000-113 | Stochastic dynamics of lexicon learning in an uncertain and nonuniform world | http://arxiv.org/pdf/1302.5526v2.pdf | author:Rainer Reisenauer, Kenny Smith, Richard A. Blythe category:physics.soc-ph cs.CL q-bio.NC published:2013-02-22 summary:We study the time taken by a language learner to correctly identify themeaning of all words in a lexicon under conditions where many plausiblemeanings can be inferred whenever a word is uttered. We show that the mostbasic form of cross-situational learning - whereby information from multipleepisodes is combined to eliminate incorrect meanings - can perform badly whenwords are learned independently and meanings are drawn from a nonuniformdistribution. If learners further assume that no two words share a commonmeaning, we find a phase transition between a maximally-efficient learningregime, where the learning time is reduced to the shortest it can possibly be,and a partially-efficient regime where incorrect candidate meanings for wordspersist at late times. We obtain exact results for the word-learning processthrough an equivalence to a statistical mechanical problem of enumerating loopsin the space of word-meaning mappings.
arxiv-3000-114 | Unsupervised edge map scoring: a statistical complexity approach | http://arxiv.org/pdf/1302.5186v2.pdf | author:Javier Gimenez, Jorge Martinez, Ana Georgina Flesia category:cs.CV stat.AP published:2013-02-21 summary:We propose a new Statistical Complexity Measure (SCM) to qualify edge mapswithout Ground Truth (GT) knowledge. The measure is the product of two indices,an \emph{Equilibrium} index $\mathcal{E}$ obtained by projecting the edge mapinto a family of edge patterns, and an \emph{Entropy} index $\mathcal{H}$,defined as a function of the Kolmogorov Smirnov (KS) statistic. This new measure can be used for performance characterization which includes:(i)~the specific evaluation of an algorithm (intra-technique process) in orderto identify its best parameters, and (ii)~the comparison of differentalgorithms (inter-technique process) in order to classify them according totheir quality. Results made over images of the South Florida and Berkeley databases showthat our approach significantly improves over Pratt's Figure of Merit (PFoM)which is the objective reference-based edge map evaluation standard, as ittakes into account more features in its evaluation.
arxiv-3000-115 | A Weight-coded Evolutionary Algorithm for the Multidimensional Knapsack Problem | http://arxiv.org/pdf/1302.5374v4.pdf | author:Quan Yuan, Zhixin Yang category:cs.NE math.OC 90B50 published:2013-02-21 summary:A revised weight-coded evolutionary algorithm (RWCEA) is proposed for solvingmultidimensional knapsack problems. This RWCEA uses a new decoding method andincorporates a heuristic method in initialization. Computational results showthat the RWCEA performs better than a weight-coded evolutionary algorithmproposed by Raidl (1999) and to some existing benchmarks, it can yield betterresults than the ones reported in the OR-library.
arxiv-3000-116 | Basic Classes of Grammars with Prohibition | http://arxiv.org/pdf/1302.5181v1.pdf | author:Mark Burgin category:cs.FL cs.CL published:2013-02-21 summary:A practical tool for natural language modeling and development ofhuman-machine interaction is developed in the context of formal grammars andlanguages. A new type of formal grammars, called grammars with prohibition, isintroduced. Grammars with prohibition provide more powerful tools for naturallanguage generation and better describe processes of language learning than theconventional formal grammars. Here we study relations between languagesgenerated by different grammars with prohibition based on conventional types offormal grammars such as context-free or context sensitive grammars. Besides, wecompare languages generated by different grammars with prohibition andlanguages generated by conventional formal grammars. In particular, it isdemonstrated that they have essentially higher computational power andexpressive possibilities in comparison with the conventional formal grammars.Thus, while conventional formal grammars are recursive and subrecursivealgorithms, many classes of grammars with prohibition are superrecursivealgorithms. Results presented in this work are aimed at the development ofhuman-machine interaction, modeling natural languages, empowerment ofprogramming languages, computer simulation, better software systems, and theoryof recursion.
arxiv-3000-117 | Object Detection in Real Images | http://arxiv.org/pdf/1302.5189v1.pdf | author:Dilip K. Prasad category:cs.CV published:2013-02-21 summary:Object detection and recognition are important problems in computer vision.Since these problems are meta-heuristic, despite a lot of research, practicallyusable, intelligent, real-time, and dynamic object detection/recognitionmethods are still unavailable. We propose a new object detection/recognitionmethod, which improves over the existing methods in every stage of the objectdetection/recognition process. In addition to the usual features, we propose touse geometric shapes, like linear cues, ellipses and quadrangles, as additionalfeatures. The full potential of geometric cues is exploited by using them toextract other features in a robust, computationally efficient, and lessmeta-heuristic manner. We also propose a new hierarchical codebook, whichprovides good generalization and discriminative properties. The codebookenables fast multi-path inference mechanisms based on propagation ofconditional likelihoods, that make it robust to occlusion and noise. It has thecapability of dynamic learning. We also propose a new learning method that hasgenerative and discriminative learning capabilities, does not need large andfully supervised training dataset, and is capable of online learning. Thepreliminary work of detecting geometric shapes in real images has beencompleted. This preliminary work is the focus of this report. Future path forrealizing the proposed object detection/recognition method is also discussed inbrief.
arxiv-3000-118 | Graph-based Generalization Bounds for Learning Binary Relations | http://arxiv.org/pdf/1302.5348v3.pdf | author:Ben London, Bert Huang, Lise Getoor category:cs.LG published:2013-02-21 summary:We investigate the generalizability of learned binary relations: functionsthat map pairs of instances to a logical indicator. This problem hasapplication in numerous areas of machine learning, such as ranking, entityresolution and link prediction. Our learning framework incorporates an examplelabeler that, given a sequence $X$ of $n$ instances and a desired training size$m$, subsamples $m$ pairs from $X \times X$ without replacement. The challengein analyzing this learning scenario is that pairwise combinations of randomvariables are inherently dependent, which prevents us from using traditionallearning-theoretic arguments. We present a unified, graph-based analysis, whichallows us to analyze this dependence using well-known graph identities. We arethen able to bound the generalization error of learned binary relations usingRademacher complexity and algorithmic stability. The rate of uniformconvergence is partially determined by the labeler's subsampling process. Wethus examine how various assumptions about subsampling affect generalization;under a natural random subsampling process, our bounds guarantee$\tilde{O}(1/\sqrt{n})$ uniform convergence.
arxiv-3000-119 | Nonparametric Basis Pursuit via Sparse Kernel-based Learning | http://arxiv.org/pdf/1302.5449v1.pdf | author:Juan Andres Bazerque, Georgios B. Giannakis category:cs.LG cs.CV cs.IT math.IT stat.ML published:2013-02-21 summary:Signal processing tasks as fundamental as sampling, reconstruction, minimummean-square error interpolation and prediction can be viewed under the prism ofreproducing kernel Hilbert spaces. Endowing this vantage point withcontemporary advances in sparsity-aware modeling and processing, promotes thenonparametric basis pursuit advocated in this paper as the overarchingframework for the confluence of kernel-based learning (KBL) approachesleveraging sparse linear regression, nuclear-norm regularization, anddictionary learning. The novel sparse KBL toolbox goes beyond translatingsparse parametric approaches to their nonparametric counterparts, toincorporate new possibilities such as multi-kernel selection and matrixsmoothing. The impact of sparse KBL to signal processing applications isillustrated through test cases from cognitive radio sensing, microarray dataimputation, and network traffic prediction.
arxiv-3000-120 | Obtaining error-minimizing estimates and universal entry-wise error bounds for low-rank matrix completion | http://arxiv.org/pdf/1302.5337v2.pdf | author:Franz J. Király, Louis Theran category:stat.ML math.AG math.CO published:2013-02-21 summary:We propose a general framework for reconstructing and denoising singleentries of incomplete and noisy entries. We describe: effective algorithms fordeciding if and entry can be reconstructed and, if so, for reconstructing anddenoising it; and a priori bounds on the error of each entry, individually. Inthe noiseless case our algorithm is exact. For rank-one matrices, the newalgorithm is fast, admits a highly-parallel implementation, and produces anerror minimizing estimate that is qualitatively close to our theoretical andthe state-of-the-are Nuclear Norm and OptSpace methods.
arxiv-3000-121 | High-Dimensional Probability Estimation with Deep Density Models | http://arxiv.org/pdf/1302.5125v1.pdf | author:Oren Rippel, Ryan Prescott Adams category:stat.ML cs.LG published:2013-02-20 summary:One of the fundamental problems in machine learning is the estimation of aprobability distribution from data. Many techniques have been proposed to studythe structure of data, most often building around the assumption thatobservations lie on a lower-dimensional manifold of high probability. It hasbeen more difficult, however, to exploit this insight to build explicit,tractable density models for high-dimensional data. In this paper, we introducethe deep density model (DDM), a new approach to density estimation. We exploitinsights from deep learning to construct a bijective map to a representationspace, under which the transformation of the distribution of the data isapproximately factorized and has identical and known marginal densities. Thesimplicity of the latent distribution under the model allows us to feasiblyexplore it, and the invertibility of the map to characterize contraction ofmeasure across it. This enables us to compute normalized densities forout-of-sample data. This combination of tractability and flexibility allows usto tackle a variety of probabilistic tasks on high-dimensional datasets,including: rapid computation of normalized densities at test-time withoutevaluating a partition function; generation of samples without MCMC; andcharacterization of the joint entropy of the data.
arxiv-3000-122 | Estimating Continuous Distributions in Bayesian Classifiers | http://arxiv.org/pdf/1302.4964v1.pdf | author:George H. John, Pat Langley category:cs.LG cs.AI stat.ML published:2013-02-20 summary:When modeling a probability distribution with a Bayesian network, we arefaced with the problem of how to handle continuous variables. Most previouswork has either solved the problem by discretizing, or assumed that the dataare generated by a single Gaussian. In this paper we abandon the normalityassumption and instead use statistical methods for nonparametric densityestimation. For a naive Bayesian classifier, we present experimental results ona variety of natural and artificial domains, comparing two methods of densityestimation: assuming normality and modeling each conditional distribution witha single Gaussian; and using nonparametric kernel density estimation. Weobserve large reductions in error on several natural and artificial data sets,which suggests that kernel estimation is a useful tool for learning Bayesianmodels.
arxiv-3000-123 | Consistency of Online Random Forests | http://arxiv.org/pdf/1302.4853v2.pdf | author:Misha Denil, David Matheson, Nando de Freitas category:stat.ML published:2013-02-20 summary:As a testament to their success, the theory of random forests has long beenoutpaced by their application in practice. In this paper, we take a steptowards narrowing this gap by providing a consistency result for online randomforests.
arxiv-3000-124 | A Characterization of the Dirichlet Distribution with Application to Learning Bayesian Networks | http://arxiv.org/pdf/1302.4949v1.pdf | author:Dan Geiger, David Heckerman category:cs.AI cs.LG published:2013-02-20 summary:We provide a new characterization of the Dirichlet distribution. Thischaracterization implies that under assumptions made by several previousauthors for learning belief networks, a Dirichlet prior on the parameters isinevitable.
arxiv-3000-125 | Spectral Clustering with Unbalanced Data | http://arxiv.org/pdf/1302.5134v1.pdf | author:Jing Qian, Venkatesh Saligrama category:stat.ML published:2013-02-20 summary:Spectral clustering (SC) and graph-based semi-supervised learning (SSL)algorithms are sensitive to how graphs are constructed from data. In particularif the data has proximal and unbalanced clusters these algorithms can lead topoor performance on well-known graphs such as $k$-NN, full-RBF,$\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) ornormalized cut (NCut) attempt to tradeoff cut values with cluster sizes, whichare not tailored to unbalanced data. We propose a novel graph partitioningframework, which parameterizes a family of graphs by adaptively modulating nodedegrees in a $k$-NN graph. We then propose a model selection scheme to choosesizable clusters which are separated by smallest cut values. Our framework isable to adapt to varying levels of unbalancedness of data and can be naturallyused for small cluster detection. We theoretically justify our ideas throughlimit cut analysis. Unsupervised and semi-supervised experiments on syntheticand real data sets demonstrate the superiority of our method.
arxiv-3000-126 | A Labeled Graph Kernel for Relationship Extraction | http://arxiv.org/pdf/1302.4874v1.pdf | author:Gonçalo Simões, Helena Galhardas, David Matos category:cs.CL cs.LG published:2013-02-20 summary:In this paper, we propose an approach for Relationship Extraction (RE) basedon labeled graph kernels. The kernel we propose is a particularization of arandom walk kernel that exploits two properties previously studied in the REliterature: (i) the words between the candidate entities or connecting them ina syntactic representation are particularly likely to carry informationregarding the relationship; and (ii) combining information from distinctsources in a kernel may help the RE system make better decisions. We performedexperiments on a dataset of protein-protein interactions and the results showthat our approach obtains effectiveness values that are comparable with thestate-of-the art kernel methods. Moreover, our approach is able to outperformthe state-of-the-art kernels when combined with other kernel methods.
arxiv-3000-127 | Structure Discovery in Nonparametric Regression through Compositional Kernel Search | http://arxiv.org/pdf/1302.4922v4.pdf | author:David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani category:stat.ML cs.LG stat.ME G.3; I.2.6 published:2013-02-20 summary:Despite its importance, choosing the structural form of the kernel innonparametric regression remains a black art. We define a space of kernelstructures which are built compositionally by adding and multiplying a smallnumber of base kernels. We present a method for searching over this space ofstructures which mirrors the scientific discovery process. The learnedstructures can often decompose functions into interpretable components andenable long-range extrapolation on time-series datasets. Our structure searchmethod outperforms many widely used kernels and kernel combination methods on avariety of prediction tasks.
arxiv-3000-128 | An Optical Watermarking Solution for Color Personal Identification Pictures | http://arxiv.org/pdf/1302.4784v1.pdf | author:Tan Yi-zhou, Liu Hai-bo, Huang Shui-hua, Sheng Ben-jian, Pan Zhong-ming category:cs.MM cs.CV physics.optics published:2013-02-20 summary:This paper presents a new approach for embedding authentication informationinto image on printed materials based on optical projection technique. Ourexperimental setup consists of two parts, one is a common camera, and the otheris a LCD projector, which project a pattern on personnel's body (especially onthe face). The pattern, generated by a computer, act as the illumination lightsource with sinusoidal distribution and it is also the watermark signal. For acolor image, the watermark is embedded into the blue channel. While we takepictures (256 *256 and 512*512, 567*390 pixels, respectively), an invisiblemark is embedded directly into magnitude oefficients of Discrete Fouriertransform (DFT) at exposure moment. Both optical an d digital correlation issuitable for detection of this type of watermark. The decoded watermark is aset of concentric circles or sectors in the DFT domain (middle frequenciesregion) which is robust to photographing, printing and scanning. The unlawfulpeople modify or replace the original photograph, and make fake passport(drivers' license and so on). Experiments show, it is difficult to forgecertificates in which a watermark was embedded by our projector-cameracombination based on analogue watermark method rather than classical digitalmethod.
arxiv-3000-129 | NLP and CALL: integration is working | http://arxiv.org/pdf/1302.4814v1.pdf | author:Georges Antoniadis, Sylviane Granger, Olivier Kraif, Claude Ponton, Virginie Zampa category:cs.CL published:2013-02-20 summary:In the first part of this article, we explore the background ofcomputer-assisted learning from its beginnings in the early XIXth century andthe first teaching machines, founded on theories of learning, at the start ofthe XXth century. With the arrival of the computer, it became possible to offerlanguage learners different types of language activities such as comprehensiontasks, simulations, etc. However, these have limits that cannot be overcomewithout some contribution from the field of natural language processing (NLP).In what follows, we examine the challenges faced and the issues raised byintegrating NLP into CALL. We hope to demonstrate that the key to success inintegrating NLP into CALL is to be found in multidisciplinary work betweencomputer experts, linguists, language teachers, didacticians and NLPspecialists.
arxiv-3000-130 | Probabilistic Frame Induction | http://arxiv.org/pdf/1302.4813v1.pdf | author:Jackie Chi Kit Cheung, Hoifung Poon, Lucy Vanderwende category:cs.CL published:2013-02-20 summary:In natural-language discourse, related events tend to appear near each otherto describe a larger scenario. Such structures can be formalized by the notionof a frame (a.k.a. template), which comprises a set of related events andprototypical participants and event transitions. Identifying frames is aprerequisite for information extraction and natural language generation, and isusually done manually. Methods for inducing frames have been proposed recently,but they typically use ad hoc procedures and are difficult to diagnose orextend. In this paper, we propose the first probabilistic approach to frameinduction, which incorporates frames, events, participants as latent topics andlearns those frame and event transitions that best explain the text. The numberof frames is inferred by a novel application of a split-merge method fromsyntactic parsing. In end-to-end evaluations from text to induced frames andextracted facts, our method produced state-of-the-art results whilesubstantially reducing engineering effort.
arxiv-3000-131 | Prediction and Clustering in Signed Networks: A Local to Global Perspective | http://arxiv.org/pdf/1302.5145v2.pdf | author:Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan Natarajan, Ambuj Tewari, Inderjit S. Dhillon category:cs.SI cs.LG published:2013-02-20 summary:The study of social networks is a burgeoning research area. However, mostexisting work deals with networks that simply encode whether relationshipsexist or not. In contrast, relationships in signed networks can be positive("like", "trust") or negative ("dislike", "distrust"). The theory of socialbalance shows that signed networks tend to conform to some local patterns that,in turn, induce certain global characteristics. In this paper, we exploit bothlocal as well as global aspects of social balance theory for two fundamentalproblems in the analysis of signed networks: sign prediction and clustering.Motivated by local patterns of social balance, we first propose two families ofsign prediction methods: measures of social imbalance (MOIs), and supervisedlearning using high order cycles (HOCs). These methods predict signs of edgesbased on triangles and \ell-cycles for relatively small values of \ell.Interestingly, by examining measures of social imbalance, we show that theclassic Katz measure, which is used widely in unsigned link prediction,actually has a balance theoretic interpretation when applied to signednetworks. Furthermore, motivated by the global structure of balanced networks,we propose an effective low rank modeling approach for both sign prediction andclustering. For the low rank modeling approach, we provide theoreticalperformance guarantees via convex relaxations, scale it up to large problemsizes using a matrix factorization based algorithm, and provide extensiveexperimental validation including comparisons with local approaches. Ourexperimental results indicate that, by adopting a more global viewpoint ofbalance structure, we get significant performance and computational gains inprediction and clustering tasks on signed networks. Our work thereforehighlights the usefulness of the global aspect of balance theory for theanalysis of signed networks.
arxiv-3000-132 | Towards a Semantic-based Approach for Modeling Regulatory Documents in Building Industry | http://arxiv.org/pdf/1302.4811v1.pdf | author:Khalil Riad Bouzidi, Catherine Faron-Zucker, Bruno Fies, Olivier Corby, Le-Thanh Nhan category:cs.CL published:2013-02-20 summary:Regulations in the Building Industry are becoming increasingly complex andinvolve more than one technical area. They cover products, components andproject implementation. They also play an important role to ensure the qualityof a building, and to minimize its environmental impact. In this paper, we areparticularly interested in the modeling of the regulatory constraints derivedfrom the Technical Guides issued by CSTB and used to validate TechnicalAssessments. We first describe our approach for modeling regulatory constraintsin the SBVR language, and formalizing them in the SPARQL language. Second, wedescribe how we model the processes of compliance checking described in theCSTB Technical Guides. Third, we show how we implement these processes toassist industrials in drafting Technical Documents in order to acquire aTechnical Assessment; a compliance report is automatically generated to explainthe compliance or noncompliance of this Technical Documents.
arxiv-3000-133 | Fast methods for denoising matrix completion formulations, with applications to robust seismic data interpolation | http://arxiv.org/pdf/1302.4886v3.pdf | author:Aleksandr Y. Aravkin, Rajiv Kumar, Hassan Mansour, Ben Recht, Felix J. Herrmann category:stat.ML cs.LG 62F35, 65K10 published:2013-02-20 summary:Recent SVD-free matrix factorization formulations have enabled rankminimization for systems with millions of rows and columns, paving the way formatrix completion in extremely large-scale applications, such as seismic datainterpolation. In this paper, we consider matrix completion formulations designed to hit atarget data-fitting error level provided by the user, and propose an algorithmcalled LR-BPDN that is able to exploit factorized formulations to solve thecorresponding optimization problem. Since practitioners typically have strongprior knowledge about target error level, this innovation makes it easy toapply the algorithm in practice, leaving only the factor rank to be determined. Within the established framework, we propose two extensions that are highlyrelevant to solving practical challenges of data interpolation. First, wepropose a weighted extension that allows known subspace information to improvethe results of matrix completion formulations. We show how this weighting canbe used in the context of frequency continuation, an essential aspect toseismic data interpolation. Second, we propose matrix completion formulationsthat are robust to large measurement errors in the available data. We illustrate the advantages of LR-BPDN on the collaborative filteringproblem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we usethe new method, along with its robust and subspace re-weighted extensions, toobtain high-quality reconstructions for large scale seismic interpolationproblems with real data, even in the presence of data contamination.
arxiv-3000-134 | Matching Pursuit LASSO Part II: Applications and Sparse Recovery over Batch Signals | http://arxiv.org/pdf/1302.5010v2.pdf | author:Mingkui Tan, Ivor W. Tsang, Li Wang category:cs.CV cs.LG stat.ML published:2013-02-20 summary:Matching Pursuit LASSIn Part I \cite{TanPMLPart1}, a Matching Pursuit LASSO({MPL}) algorithm has been presented for solving large-scale sparse recovery(SR) problems. In this paper, we present a subspace search to further improvethe performance of MPL, and then continue to address another major challenge ofSR -- batch SR with many signals, a consideration which is absent from most ofprevious $\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed tovastly speed up sparse recovery of many signals simultaneously. Comprehensivenumerical experiments on compressive sensing and face recognition tasksdemonstrate the superior performance of MPL and BMPL over other methodsconsidered in this paper, in terms of sparse recovery ability and efficiency.In particular, BMPL is up to 400 times faster than existing $\ell_1$-normmethods considered to be state-of-the-art.O Part II: Applications and SparseRecovery over Batch Signals
arxiv-3000-135 | Breaking the Small Cluster Barrier of Graph Clustering | http://arxiv.org/pdf/1302.4549v2.pdf | author:Nir Ailon, Yudong Chen, Xu Huan category:cs.LG stat.ML published:2013-02-19 summary:This paper investigates graph clustering in the planted cluster model in thepresence of {\em small clusters}. Traditional results dictate that for analgorithm to provably correctly recover the clusters, {\em all} clusters mustbe sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ isthe number of nodes of the graph). We show that this is not really arestriction: by a more refined analysis of the trace-norm based recoveryapproach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove thatsmall clusters, under certain mild assumptions, do not hinder recovery of largeones. Based on this result, we further devise an iterative algorithm to recover{\em almost all clusters} via a "peeling strategy", i.e., recover largeclusters first, leading to a reduced problem, and repeat this procedure. Theseresults are extended to the {\em partial observation} setting, in which only a(chosen) part of the graph is observed.The peeling strategy gives rise to anactive learning algorithm, in which edges adjacent to smaller clusters arequeried more often as large clusters are learned (and removed). From a high level, this paper sheds novel insights on high-dimensionalstatistics and learning structured data, by presenting a structured matrixlearning problem for which a one shot convex relaxation approach necessarilyfails, but a carefully constructed sequence of convex relaxationsdoes the job.
arxiv-3000-136 | Complex networks analysis of language complexity | http://arxiv.org/pdf/1302.4490v1.pdf | author:Diego R. Amancio, Sandra M. Aluisio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL cs.SI published:2013-02-19 summary:Methods from statistical physics, such as those involving complex networks,have been increasingly used in quantitative analysis of linguistic phenomena.In this paper, we represented pieces of text with different levels ofsimplification in co-occurrence networks and found that topological regularitycorrelated negatively with textual complexity. Furthermore, in less complextexts the distance between concepts, represented as nodes, tended to decrease.The complex networks metrics were treated with multivariate pattern recognitiontechniques, which allowed us to distinguish between original texts and theirsimplified versions. For each original text, two simplified versions weregenerated manually with increasing number of simplification operations. Asexpected, distinction was easier for the strongly simplified versions, wherethe most relevant metrics were node strength, shortest paths and diversity.Also, the discrimination of complex texts was improved with higher hierarchicalnetwork metrics, thus pointing to the usefulness of considering wider contextsaround the concepts. Though the accuracy rate in the distinction was not ashigh as in methods using deep linguistic knowledge, the complex networkapproach is still useful for a rapid screening of texts whenever assessingcomplexity is essential to guarantee accessibility to readers with limitedreading ability
arxiv-3000-137 | Optimal Discriminant Functions Based On Sampled Distribution Distance for Modulation Classification | http://arxiv.org/pdf/1302.4773v1.pdf | author:Paulo Urriza, Eric Rebeiz, Danijela Cabric category:stat.ML cs.LG cs.PF published:2013-02-19 summary:In this letter, we derive the optimal discriminant functions for modulationclassification based on the sampled distribution distance. The proposed methodclassifies various candidate constellations using a low complexity approachbased on the distribution distance at specific testpoints along the cumulativedistribution function. This method, based on the Bayesian decision criteria,asymptotically provides the minimum classification error possible given a setof testpoints. Testpoint locations are also optimized to improve classificationperformance. The method provides significant gains over existing approachesthat also use the distribution of the signal features.
arxiv-3000-138 | Bilingual Terminology Extraction Using Multi-level Termhood | http://arxiv.org/pdf/1302.4492v1.pdf | author:Chengzhi Zhang, Dan Wu category:cs.CL published:2013-02-19 summary:Purpose: Terminology is the set of technical words or expressions used inspecific contexts, which denotes the core concept in a formal discipline and isusually applied in the fields of machine translation, information retrieval,information extraction and text categorization, etc. Bilingual terminologyextraction plays an important role in the application of bilingual dictionarycompilation, bilingual Ontology construction, machine translation andcross-language information retrieval etc. This paper addresses the issues ofmonolingual terminology extraction and bilingual term alignment based onmulti-level termhood. Design/methodology/approach: A method based on multi-level termhood isproposed. The new method computes the termhood of the terminology candidate aswell as the sentence that includes the terminology by the comparison of thecorpus. Since terminologies and general words usually have differentlydistribution in the corpus, termhood can also be used to constrain and enhancethe performance of term alignment when aligning bilingual terms on the parallelcorpus. In this paper, bilingual term alignment based on termhood constraintsis presented. Findings: Experiment results show multi-level termhood can get betterperformance than existing method for terminology extraction. If termhood isused as constrain factor, the performance of bilingual term alignment can beimproved.
arxiv-3000-139 | An Ontology for Modelling and Supporting the Process of Authoring Technical Assessments | http://arxiv.org/pdf/1302.4726v1.pdf | author:Khalil Riad Bouzidi, Bruno Fies, Marc Bourdeau, Catherine Faron-Zucker, Nhan Le-Thanh category:cs.IR cs.CL cs.DL published:2013-02-19 summary:In this paper, we present a semantic web approach for modelling the processof creating new technical and regulatory documents related to the Buildingsector. This industry, among other industries, is currently experiencing aphenomenal growth in its technical and regulatory texts. Therefore, it isurgent and crucial to improve the process of creating regulations by automatingit as much as possible. We focus on the creation of particular technicaldocuments issued by the French Scientific and Technical Centre for Building(CSTB), called Technical Assessments, and we propose services based on SemanticWeb models and techniques for modelling the process of their creation.
arxiv-3000-140 | Termhood-based Comparability Metrics of Comparable Corpus in Special Domain | http://arxiv.org/pdf/1302.4489v1.pdf | author:Sa Liu, Chengzhi Zhang category:cs.CL published:2013-02-19 summary:Cross-Language Information Retrieval (CLIR) and machine translation (MT)resources, such as dictionaries and parallel corpora, are scarce and hard tocome by for special domains. Besides, these resources are just limited to a fewlanguages, such as English, French, and Spanish and so on. So, obtainingcomparable corpora automatically for such domains could be an answer to thisproblem effectively. Comparable corpora, that the subcorpora are nottranslations of each other, can be easily obtained from web. Therefore,building and using comparable corpora is often a more feasible option inmultilingual information processing. Comparability metrics is one of key issuesin the field of building and using comparable corpus. Currently, there is nowidely accepted definition or metrics method of corpus comparability. In fact,Different definitions or metrics methods of comparability might be given tosuit various tasks about natural language processing. A new comparability,namely, termhood-based metrics, oriented to the task of bilingual terminologyextraction, is proposed in this paper. In this method, words are ranked bytermhood not frequency, and then the cosine similarities, calculated based onthe ranking lists of word termhood, is used as comparability. Experimentsresults show that termhood-based metrics performs better than traditionalfrequency-based metrics.
arxiv-3000-141 | Good Recognition is Non-Metric | http://arxiv.org/pdf/1302.4673v1.pdf | author:Walter J. Scheirer, Michael J. Wilber, Michael Eckmann, Terrance E. Boult category:cs.CV published:2013-02-19 summary:Recognition is the fundamental task of visual cognition, yet how to formalizethe general recognition problem for computer vision remains an open issue. Theproblem is sometimes reduced to the simplest case of recognizing matchingpairs, often structured to allow for metric constraints. However, visualrecognition is broader than just pair matching -- especially when we considermulti-class training data and large sets of features in a learning context.What we learn and how we learn it has important implications for effectivealgorithms. In this paper, we reconsider the assumption of recognition as apair matching test, and introduce a new formal definition that captures thebroader context of the problem. Through a meta-analysis and an experimentalassessment of the top algorithms on popular data sets, we gain a sense of howoften metric properties are violated by good recognition algorithms. Bystudying these violations, useful insights come to light: we make the case thatlocally metric algorithms should leverage outside information to solve thegeneral recognition problem.
arxiv-3000-142 | A Genetic Algorithm for Power-Aware Virtual Machine Allocation in Private Cloud | http://arxiv.org/pdf/1302.4519v1.pdf | author:Nguyen Quang-Hung, Pham Dac Nien, Nguyen Hoai Nam, Nguyen Huynh Tuong, Nam Thoai category:cs.NE cs.DC published:2013-02-19 summary:Energy efficiency has become an important measurement of scheduling algorithmfor private cloud. The challenge is trade-off between minimizing of energyconsumption and satisfying Quality of Service (QoS) (e.g. performance orresource availability on time for reservation request). We consider resourceneeds in context of a private cloud system to provide resources forapplications in teaching and researching. In which users request computingresources for laboratory classes at start times and non-interrupted duration insome hours in prior. Many previous works are based on migrating techniques tomove online virtual machines (VMs) from low utilization hosts and turn thesehosts off to reduce energy consumption. However, the techniques for migrationof VMs could not use in our case. In this paper, a genetic algorithm forpower-aware in scheduling of resource allocation (GAPA) has been proposed tosolve the static virtual machine allocation problem (SVMAP). Due to limitedresources (i.e. memory) for executing simulation, we created a workload thatcontains a sample of one-day timetable of lab hours in our university. Weevaluate the GAPA and a baseline scheduling algorithm (BFD), which sorts listof virtual machines in start time (i.e. earliest start time first) and usingbest-fit decreasing (i.e. least increased power consumption) algorithm, forsolving the same SVMAP. As a result, the GAPA algorithm obtains total energyconsumption is lower than the baseline algorithm on simulated experimentation.
arxiv-3000-143 | Compactified Horizontal Visibility Graph for the Language Network | http://arxiv.org/pdf/1302.4619v1.pdf | author:D. V. Lande, A. A. Snarskii category:cs.CL cs.DS published:2013-02-19 summary:A compactified horizontal visibility graph for the language network isproposed. It was found that the networks constructed in such way are scalefree, and have a property that among the nodes with largest degrees there arewords that determine not only a text structure communication, but also itsinformational structure.
arxiv-3000-144 | Word sense disambiguation via high order of learning in complex networks | http://arxiv.org/pdf/1302.4471v1.pdf | author:Thiago C. Silva, Diego R. Amancio category:physics.soc-ph cs.CL cs.SI published:2013-02-18 summary:Complex networks have been employed to model many real systems and as amodeling tool in a myriad of applications. In this paper, we use the frameworkof complex networks to the problem of supervised classification in the worddisambiguation task, which consists in deriving a function from the supervised(or labeled) training data of ambiguous words. Traditional supervised dataclassification takes into account only topological or physical features of theinput data. On the other hand, the human (animal) brain performs both low- andhigh-level orders of learning and it has facility to identify patternsaccording to the semantic meaning of the input data. In this paper, we apply ahybrid technique which encompasses both types of learning in the field of wordsense disambiguation and show that the high-level order of learning can reallyimprove the accuracy rate of the model. This evidence serves to demonstratethat the internal structures formed by the words do present patterns that,generally, cannot be correctly unveiled by only traditional techniques.Finally, we exhibit the behavior of the model for different weights of the low-and high-level classifiers by plotting decision boundaries. This study helpsone to better understand the effectiveness of the model.
arxiv-3000-145 | Role of temporal inference in the recognition of textual inference | http://arxiv.org/pdf/1302.5645v1.pdf | author:Djallel Bouneffouf category:cs.CL I.2.7 published:2013-02-18 summary:This project is a part of nature language processing and its aims to developa system of recognition inference text-appointed TIMINF. This type of systemcan detect, given two portions of text, if a text is semantically deducted fromthe other. We focused on making the inference time in this type of system. Forthat we have built and analyzed a body built from questions collected throughthe web. This study has enabled us to classify different types of timesinferences and for designing the architecture of TIMINF which seeks tointegrate a module inference time in a detection system inference text. We alsoassess the performance of sorties TIMINF system on a test corpus with the samestrategy adopted in the challenge RTE.
arxiv-3000-146 | Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization | http://arxiv.org/pdf/1302.4385v2.pdf | author:Nicolas Gillis, Robert Luce category:stat.ML math.OC published:2013-02-18 summary:Nonnegative matrix factorization (NMF) has been shown recently to betractable under the separability assumption, under which all the columns of theinput data matrix belong to the convex cone generated by only a few of thesecolumns. Bittorf, Recht, R\'e and Tropp (`Factoring nonnegative matrices withlinear programs', NIPS 2012) proposed a linear programming (LP) model, referredto as Hottopixx, which is robust under any small perturbation of the inputmatrix. However, Hottopixx has two important drawbacks: (i) the input matrixhas to be normalized, and (ii) the factorization rank has to be known inadvance. In this paper, we generalize Hottopixx in order to resolve these twodrawbacks, that is, we propose a new LP model which does not requirenormalization and detects the factorization rank automatically. Moreover, thenew LP model is more flexible, significantly more tolerant to noise, and caneasily be adapted to handle outliers and other noise models. Finally, we showon several synthetic datasets that it outperforms Hottopixx while competingfavorably with two state-of-the-art methods.
arxiv-3000-147 | Unveiling the relationship between complex networks metrics and word senses | http://arxiv.org/pdf/1302.4465v1.pdf | author:Diego R. Amancio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL cs.SI published:2013-02-18 summary:The automatic disambiguation of word senses (i.e., the identification ofwhich of the meanings is used in a given context for a word that has multiplemeanings) is essential for such applications as machine translation andinformation retrieval, and represents a key step for developing the so-calledSemantic Web. Humans disambiguate words in a straightforward fashion, but thisdoes not apply to computers. In this paper we address the problem of Word SenseDisambiguation (WSD) by treating texts as complex networks, and show that wordsenses can be distinguished upon characterizing the local structure aroundambiguous words. Our goal was not to obtain the best possible disambiguationsystem, but we nevertheless found that in half of the cases our approachoutperforms traditional shallow methods. We show that the hierarchicalconnectivity and clustering of words are usually the most relevant features forWSD. The results reported here shine light on the relationship between semanticand structural parameters of complex networks. They also indicate that whencombined with traditional techniques the complex network approach may be usefulto enhance the discrimination of senses in large texts
arxiv-3000-148 | Metrics for Multivariate Dictionaries | http://arxiv.org/pdf/1302.4242v2.pdf | author:Sylvain Chevallier, Quentin Barthélemy, Jamal Atif category:cs.LG stat.ML K.3.2 published:2013-02-18 summary:Overcomplete representations and dictionary learning algorithms keptattracting a growing interest in the machine learning community. This paperaddresses the emerging problem of comparing multivariate overcompleterepresentations. Despite a recurrent need to rely on a distance for learning orassessing multivariate overcomplete representations, no metrics in theirunderlying spaces have yet been proposed. Henceforth we propose to studyovercomplete representations from the perspective of frame theory and matrixmanifolds. We consider distances between multivariate dictionaries as distancesbetween their spans which reveal to be elements of a Grassmannian manifold. Weintroduce Wasserstein-like set-metrics defined on Grassmannian spaces and studytheir properties both theoretically and numerically. Indeed a deep experimentalstudy based on tailored synthetic datasetsand real EEG signals forBrain-Computer Interfaces (BCI) have been conducted. In particular, theintroduced metrics have been embedded in clustering algorithm and applied toBCI Competition IV-2a for dataset quality assessment. Besides, a principledconnection is made between three close but still disjoint research fields,namely, Grassmannian packing, dictionary learning and compressed sensing.
arxiv-3000-149 | Canonical dual solutions to nonconvex radial basis neural network optimization problem | http://arxiv.org/pdf/1302.4141v1.pdf | author:Vittorio Latorre, David Yang Gao category:cs.NE cs.LG stat.ML published:2013-02-18 summary:Radial Basis Functions Neural Networks (RBFNNs) are tools widely used inregression problems. One of their principal drawbacks is that the formulationcorresponding to the training with the supervision of both the centers and theweights is a highly non-convex optimization problem, which leads to somefundamentally difficulties for traditional optimization theory and methods. This paper presents a generalized canonical duality theory for solving thischallenging problem. We demonstrate that by sequential canonical dualtransformations, the nonconvex optimization problem of the RBFNN can bereformulated as a canonical dual problem (without duality gap). Both globaloptimal solution and local extrema can be classified. Several applications toone of the most used Radial Basis Functions, the Gaussian function, areillustrated. Our results show that even for one-dimensional case, the globalminimizer of the nonconvex problem may not be the best solution to the RBFNNs,and the canonical dual theory is a promising tool for solving general neuralnetworks training problems.
arxiv-3000-150 | Gaussian Process Kernels for Pattern Discovery and Extrapolation | http://arxiv.org/pdf/1302.4245v3.pdf | author:Andrew Gordon Wilson, Ryan Prescott Adams category:stat.ML cs.AI stat.ME published:2013-02-18 summary:Gaussian processes are rich distributions over functions, which provide aBayesian nonparametric approach to smoothing and interpolation. We introducesimple closed form kernels that can be used with Gaussian processes to discoverpatterns and enable extrapolation. These kernels are derived by modelling aspectral density -- the Fourier transform of a kernel -- with a Gaussianmixture. The proposed kernels support a broad class of stationary covariances,but Gaussian process inference remains simple and analytic. We demonstrate theproposed kernels by discovering patterns and performing long rangeextrapolation on synthetic examples, as well as atmospheric CO2 trends andairline passenger data. We also show that we can reconstruct standardcovariances within our framework.
arxiv-3000-151 | Online Learning with Switching Costs and Other Adaptive Adversaries | http://arxiv.org/pdf/1302.4387v2.pdf | author:Nicolo Cesa-Bianchi, Ofer Dekel, Ohad Shamir category:cs.LG stat.ML published:2013-02-18 summary:We study the power of different types of adaptive (nonoblivious) adversariesin the setting of prediction with expert advice, under both full-informationand bandit feedback. We measure the player's performance using a new notion ofregret, also known as policy regret, which better captures the adversary'sadaptiveness to the player's behavior. In a setting where losses are allowed todrift, we characterize ---in a nearly complete manner--- the power of adaptiveadversaries with bounded memories and switching costs. In particular, we showthat with switching costs, the attainable rate with bandit feedback is$\widetilde{\Theta}(T^{2/3})$. Interestingly, this rate is significantly worsethan the $\Theta(\sqrt{T})$ rate attainable with switching costs in thefull-information case. Via a novel reduction from experts to bandits, we alsoshow that a bounded memory adversary can force $\widetilde{\Theta}(T^{2/3})$regret even in the full information case, proving that switching costs areeasier to control than bounded memory adversaries. Our lower bounds rely on anew stochastic adversary strategy that generates loss processes with strongdependencies.
arxiv-3000-152 | Maxout Networks | http://arxiv.org/pdf/1302.4389v4.pdf | author:Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2013-02-18 summary:We consider the problem of designing models to leverage a recently introducedapproximate model averaging technique called dropout. We define a simple newmodel called maxout (so named because its output is the max of a set of inputs,and because it is a natural companion to dropout) designed to both facilitateoptimization by dropout and improve the accuracy of dropout's fast approximatemodel averaging technique. We empirically verify that the model successfullyaccomplishes both of these tasks. We use maxout and dropout to demonstratestate of the art classification performance on four benchmark datasets: MNIST,CIFAR-10, CIFAR-100, and SVHN.
arxiv-3000-153 | On Translation Invariant Kernels and Screw Functions | http://arxiv.org/pdf/1302.4343v1.pdf | author:Purushottam Kar, Harish Karnick category:math.FA cs.LG stat.ML published:2013-02-18 summary:We explore the connection between Hilbertian metrics and positive definitekernels on the real line. In particular, we look at a well-knowncharacterization of translation invariant Hilbertian metrics on the real lineby von Neumann and Schoenberg (1941). Using this result we are able to give analternate proof of Bochner's theorem for translation invariant positivedefinite kernels on the real line (Rudin, 1962).
arxiv-3000-154 | Explaining Zipf's Law via Mental Lexicon | http://arxiv.org/pdf/1302.4383v1.pdf | author:Armen E. Allahverdyan, Weibing Deng, Q. A. Wang category:cs.CL published:2013-02-18 summary:The Zipf's law is the major regularity of statistical linguistics that servedas a prototype for rank-frequency relations and scaling laws in naturalsciences. Here we show that the Zipf's law -- together with its applicabilityfor a single text and its generalizations to high and low frequencies includinghapax legomena -- can be derived from assuming that the words are drawn intothe text with random probabilities. Their apriori density relates, via theBayesian statistics, to general features of the mental lexicon of the authorwho produced the text.
arxiv-3000-155 | Feature Multi-Selection among Subjective Features | http://arxiv.org/pdf/1302.4297v3.pdf | author:Sivan Sabato, Adam Kalai category:cs.LG stat.ML published:2013-02-18 summary:When dealing with subjective, noisy, or otherwise nebulous features, the"wisdom of crowds" suggests that one may benefit from multiple judgments of thesame feature on the same object. We give theoretically-motivated `featuremulti-selection' algorithms that choose, among a large set of candidatefeatures, not only which features to judge but how many times to judge eachone. We demonstrate the effectiveness of this approach for linear regression ona crowdsourced learning task of predicting people's height and weight fromphotos, using features such as 'gender' and 'estimated weight' as well asculturally fraught ones such as 'attractive'.
arxiv-3000-156 | A new scheme of signature extraction for iris authentication | http://arxiv.org/pdf/1302.4043v1.pdf | author:Belhassen Akrout, Imen Khanfir Kallel, Chokri Ben Amar category:cs.CV published:2013-02-17 summary:Iris recognition, a relatively new biometric technology, has greatadvantages, such as variability, stability and security, thus is the mostpromising for high security environment. Iris recognition is proposed in thisreport. We describe some methods, the first one is based on grey levelhistogram to extract the pupil, the second is based on elliptic and parabolicHOUGH transformation to determinate the edge of iris, upper and lower eyelids,the third we used 2D Gabor Wavelets to encode the iris and finally we used theHamming distance for authentication.
arxiv-3000-157 | Understanding Boltzmann Machine and Deep Learning via A Confident Information First Principle | http://arxiv.org/pdf/1302.3931v7.pdf | author:Xiaozhao Zhao, Yuexian Hou, Qian Yu, Dawei Song, Wenjie Li category:cs.NE cs.LG stat.ML published:2013-02-16 summary:Typical dimensionality reduction methods focus on directly reducing thenumber of random variables while retaining maximal variations in the data. Inthis paper, we consider the dimensionality reduction in parameter spaces ofbinary multivariate distributions. We propose a generalConfident-Information-First (CIF) principle to maximally preserve parameterswith confident estimates and rule out unreliable or noisy parameters. Formally,the confidence of a parameter can be assessed by its Fisher information, whichestablishes a connection with the inverse variance of any unbiased estimate forthe parameter via the Cram\'{e}r-Rao bound. We then revisit Boltzmann machines(BM) and theoretically show that both single-layer BM without hidden units(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.This can not only help us uncover and formalize the essential parts of thetarget density that SBM and RBM capture, but also suggest that the deep neuralnetwork consisting of several layers of RBM can be seen as the layer-wiseapplication of CIF. Guided by the theoretical analysis, we develop asample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM anda CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP arestudied in a series of density estimation experiments.
arxiv-3000-158 | Clustering validity based on the most similarity | http://arxiv.org/pdf/1302.3956v1.pdf | author:Raheleh Namayandeh, Farzad Didehvar, Zahra Shojaei category:cs.LG stat.ML 68T05 published:2013-02-16 summary:One basic requirement of many studies is the necessity of classifying data.Clustering is a proposed method for summarizing networks. Clustering methodscan be divided into two categories named model-based approaches and algorithmicapproaches. Since the most of clustering methods depend on their inputparameters, it is important to evaluate the result of a clustering algorithmwith its different input parameters, to choose the most appropriate one. Thereare several clustering validity techniques based on inner density and outerdensity of clusters that represent different metrics to choose the mostappropriate clustering independent of the input parameters. According todependency of previous methods on the input parameters, one challenge in facingwith large systems, is to complete data incrementally that effects on the finalchoice of the most appropriate clustering. Those methods define the existenceof high intensity in a cluster, and low intensity among different clusters asthe measure of choosing the optimal clustering. This measure has a tremendousproblem, not availing all data at the first stage. In this paper, we introducean efficient measure in which maximum number of repetitions for various initialvalues occurs.
arxiv-3000-159 | Gaussian Process Vine Copulas for Multivariate Dependence | http://arxiv.org/pdf/1302.3979v1.pdf | author:David Lopez-Paz, José Miguel Hernández-Lobato, Zoubin Ghahramani category:stat.ME stat.ML published:2013-02-16 summary:Copulas allow to learn marginal distributions separately from themultivariate dependence structure (copula) that links them together into adensity function. Vine factorizations ease the learning of high-dimensionalcopulas by constructing a hierarchy of conditional bivariate copulas. However,to simplify inference, it is common to assume that each of these conditionalbivariate copulas is independent from its conditioning variables. In thispaper, we relax this assumption by discovering the latent functions thatspecify the shape of a conditional copula given its conditioning variables Welearn these functions by following a Bayesian approach based on sparse Gaussianprocesses with expectation propagation for scalable, approximate inference.Experiments on real-world datasets show that, when modeling all conditionaldependencies, we obtain better estimates of the underlying copula of the data.
arxiv-3000-160 | Analysis of Descent-Based Image Registration | http://arxiv.org/pdf/1302.3785v2.pdf | author:Elif Vural, Pascal Frossard category:cs.CV published:2013-02-15 summary:We present a performance analysis for image registration with gradientdescent methods. We consider a typical multiscale registration setting wherethe global 2-D translation between a pair of images is estimated by smoothingthe images and minimizing the distance between them with gradient descent. Ourstudy particularly concentrates on the effect of noise and low-pass filteringon the alignment accuracy. We adopt an analytic representation for images andanalyze the well-behavedness of the image distance function by estimating theneighborhood of translations for which it is free of undesired local minima.This corresponds to the neighborhood of translation vectors that are correctlycomputable with a simple gradient descent minimization. We show that the areaof this neighborhood increases at least quadratically with the smoothing filtersize, which justifies the use of a smoothing step in image registration withlocal optimizers such as gradient descent. We then examine the effect of noiseon the alignment accuracy and derive an upper bound for the alignment error interms of the noise properties and filter size. Our main finding is that theerror increases at a rate that is at least linear with respect to the filtersize. Therefore, smoothing improves the well-behavedness of the distancefunction; however, this comes at the cost of amplifying the alignment error innoisy settings. Our results provide a mathematical insight about whyhierarchical techniques are effective in image registration, suggesting thatthe multiscale coarse-to-fine alignment strategy of these techniques is verysuitable from the perspective of the trade-off between the well-behavedness ofthe objective function and the registration accuracy. To the best of ourknowledge, this is the first such study for descent-based image registration.
arxiv-3000-161 | Robust Image Segmentation in Low Depth Of Field Images | http://arxiv.org/pdf/1302.3900v1.pdf | author:Franz Graf, Hans-Peter Kriegel, Michael Weiler category:cs.CV published:2013-02-15 summary:In photography, low depth of field (DOF) is an important technique toemphasize the object of interest (OOI) within an image. Thus, low DOF imagesare widely used in the application area of macro, portrait or sportsphotography. When viewing a low DOF image, the viewer implicitly concentrateson the regions that are sharper regions of the image and thus segments theimage into regions of interest and non regions of interest which has a majorimpact on the perception of the image. Thus, a robust algorithm for the fullyautomatic detection of the OOI in low DOF images provides valuable informationfor subsequent image processing and image retrieval. In this paper we propose arobust and parameterless algorithm for the fully automatic segmentation of lowDOF images. We compare our method with three similar methods and show thesuperior robustness even though our algorithm does not require any parametersto be set by hand. The experiments are conducted on a real world data set withhigh and low DOF images.
arxiv-3000-162 | Identifying trends in word frequency dynamics | http://arxiv.org/pdf/1302.3892v1.pdf | author:Eduardo G. Altmann, Zakary L. Whichard, Adilson E. Motter category:physics.soc-ph cs.CL q-bio.PE published:2013-02-15 summary:The word-stock of a language is a complex dynamical system in which words canbe created, evolve, and become extinct. Even more dynamic are the short-termfluctuations in word usage by individuals in a population. Building on therecent demonstration that word niche is a strong determinant of future rise orfall in word frequency, here we introduce a model that allows us to distinguishpersistent from temporary increases in frequency. Our model is illustratedusing a 10^8-word database from an online discussion group and a 10^11-wordcollection of digitized books. The model reveals a strong relation betweenchanges in word dissemination and changes in frequency. Aside from theirimplications for short-term word frequency dynamics, these observations arepotentially important for language evolution as new words must survive in theshort term in order to survive in the long term.
arxiv-3000-163 | Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection | http://arxiv.org/pdf/1302.3721v1.pdf | author:Joseph Mellor, Jonathan Shapiro category:cs.LG published:2013-02-15 summary:Thompson Sampling has recently been shown to be optimal in the BernoulliMulti-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumesstationary distributions for the rewards. It is often unrealistic to model thereal world as a stationary distribution. In this paper we derive and evaluatealgorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem.We propose a Thompson Sampling strategy equipped with a Bayesian change pointmechanism to tackle this problem. We develop algorithms for a variety of caseswith constant switching rate: when switching occurs all arms change (GlobalSwitching), switching occurs independently for each arm (Per-Arm Switching),when the switching rate is known and when it must be inferred from data. Thisleads to a family of algorithms we collectively term Change-Point ThompsonSampling (CTS). We show empirical results of the algorithm in 4 artificialenvironments, and 2 derived from real world data; news click-through[Yahoo!,2011] and foreign exchange data[Dukascopy, 2012], comparing them to some otherbandit algorithms. In real world data CTS is the most effective.
arxiv-3000-164 | Quantum Entanglement in Concept Combinations | http://arxiv.org/pdf/1302.3831v2.pdf | author:Diederik Aerts, Sandro Sozzo category:cs.AI cs.CL quant-ph published:2013-02-15 summary:Research in the application of quantum structures to cognitive scienceconfirms that these structures quite systematically appear in the dynamics ofconcepts and their combinations and quantum-based models faithfully representexperimental data of situations where classical approaches are problematical.In this paper, we analyze the data we collected in an experiment on a specificconceptual combination, showing that Bell's inequalities are violated in theexperiment. We present a new refined entanglement scheme to model these datawithin standard quantum theory rules, where 'entangled measurements andentangled evolutions' occur, in addition to the expected 'entangled states',and present a full quantum representation in complex Hilbert space of the data.This stronger form of entanglement in measurements and evolutions might haverelevant applications in the foundations of quantum theory, as well as in theinterpretation of nonlocality tests. It could indeed explain somenon-negligible 'anomalies' identified in EPR-Bell experiments.
arxiv-3000-165 | A Fresnelet-Based Encryption of Medical Images using Arnold Transform | http://arxiv.org/pdf/1302.3702v1.pdf | author:Muhammad Nazeer, Bibi Nargis, Yasir Mehmood Malik, Dai-Gyoung Kim category:cs.CR cs.CV 68U10, 68M11 published:2013-02-15 summary:Medical images are commonly stored in digital media and transmitted viaInternet for certain uses. If a medical information image alters, this can leadto a wrong diagnosis which may create a serious health problem. Moreover,medical images in digital form can easily be modified by wiping off or addingsmall pieces of information intentionally for certain illegal purposes. Hence,the reliability of medical images is an important criterion in a hospitalinformation system. In this paper, Fresnelet transform is employed along withappropriate handling of the Arnold transform and the discrete cosine transformto provide secure distribution of medical images. This method presents a newdata hiding system in which steganography and cryptography are used to preventunauthorized data access. The experimental results exhibit highimperceptibility for embedded images and significant encryption of informationimages.
arxiv-3000-166 | Density Ratio Hidden Markov Models | http://arxiv.org/pdf/1302.3700v1.pdf | author:John A. Quinn, Masashi Sugiyama category:stat.ML cs.LG published:2013-02-15 summary:Hidden Markov models and their variants are the predominant sequentialclassification method in such domains as speech recognition, bioinformatics andnatural language processing. Being generative rather than discriminativemodels, however, their classification performance is a drawback. In this paperwe apply ideas from the field of density ratio estimation to bypass thedifficult step of learning likelihood functions in HMMs. By reformulatinginference and model fitting in terms of density ratios and applying a fastkernel-based estimation method, we show that it is possible to obtain astriking increase in discriminative performance while retaining theprobabilistic qualities of the HMM. We demonstrate experimentally that thisformulation makes more efficient use of training data than alternativeapproaches.
arxiv-3000-167 | Multiclass Data Segmentation using Diffuse Interface Methods on Graphs | http://arxiv.org/pdf/1302.3913v2.pdf | author:Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi, Arjuna Flenner, Allon Percus category:stat.ML 62-XX published:2013-02-15 summary:We present two graph-based algorithms for multiclass segmentation ofhigh-dimensional data. The algorithms use a diffuse interface model based onthe Ginzburg-Landau functional, related to total variation compressed sensingand image processing. A multiclass extension is introduced using the Gibbssimplex, with the functional's double-well potential modified to handle themulticlass case. The first algorithm minimizes the functional using a convexsplitting numerical scheme. The second algorithm is a uses a graph adaptationof the classical numerical Merriman-Bence-Osher (MBO) scheme, which alternatesbetween diffusion and thresholding. We demonstrate the performance of bothalgorithms experimentally on synthetic data, grayscale and color images, andseveral benchmark data sets such as MNIST, COIL and WebKB. We also make use offast numerical solvers for finding the eigenvectors and eigenvalues of thegraph Laplacian, and take advantage of the sparsity of the matrix. Experimentsindicate that the results are competitive with or better than the currentstate-of-the-art multiclass segmentation algorithms.
arxiv-3000-168 | Bio-inspired data mining: Treating malware signatures as biosequences | http://arxiv.org/pdf/1302.3668v1.pdf | author:Ajit Narayanan, Yi Chen category:cs.LG q-bio.QM stat.ML I.2.6 published:2013-02-15 summary:The application of machine learning to bioinformatics problems is wellestablished. Less well understood is the application of bioinformaticstechniques to machine learning and, in particular, the representation ofnon-biological data as biosequences. The aim of this paper is to explore theeffects of giving amino acid representation to problematic machine learningdata and to evaluate the benefits of supplementing traditional machine learningwith bioinformatics tools and techniques. The signatures of 60 computer virusesand 60 computer worms were converted into amino acid representations and firstmultiply aligned separately to identify conserved regions across differentfamilies within each class (virus and worm). This was followed by a secondalignment of all 120 aligned signatures together so that non-conserved regionswere identified prior to input to a number of machine learning techniques.Differences in length between virus and worm signatures after the firstalignment were resolved by the second alignment. Our first set of experimentsindicates that representing computer malware signatures as amino acid sequencesfollowed by alignment leads to greater classification and prediction accuracy.Our second set of experiments indicates that checking the results of datamining from artificial virus and worm data against known proteins can lead togeneralizations being made from the domain of naturally occurring proteins tomalware signatures. However, further work is needed to determine the advantagesand disadvantages of different representations and sequence alignment methodsfor handling problematic machine learning data.
arxiv-3000-169 | Adaptive Temporal Compressive Sensing for Video | http://arxiv.org/pdf/1302.3446v3.pdf | author:Xin Yuan, Jianbo Yang, Patrick Llull, Xuejun Liao, Guillermo Sapiro, David J. Brady, Lawrence Carin category:stat.AP cs.CV cs.MM published:2013-02-14 summary:This paper introduces the concept of adaptive temporal compressive sensing(CS) for video. We propose a CS algorithm to adapt the compression ratio basedon the scene's temporal complexity, computed from the compressed data, withoutcompromising the quality of the reconstructed video. The temporal adaptivity ismanifested by manipulating the integration time of the camera, opening thepossibility to real-time implementation. The proposed algorithm is ageneralized temporal CS approach that can be incorporated with a diverse set ofexisting hardware systems.
arxiv-3000-170 | A Latent Source Model for Nonparametric Time Series Classification | http://arxiv.org/pdf/1302.3639v5.pdf | author:George H. Chen, Stanislav Nikolov, Devavrat Shah category:stat.ML cs.LG cs.SI published:2013-02-14 summary:For classifying time series, a nearest-neighbor approach is widely used inpractice with performance often competitive with or better than more elaboratemethods such as neural networks, decision trees, and support vector machines.We develop theoretical justification for the effectiveness ofnearest-neighbor-like classification of time series. Our guiding hypothesis isthat in many applications, such as forecasting which topics will become trendson Twitter, there aren't actually that many prototypical time series to beginwith, relative to the number of time series we have access to, e.g., topicsbecome trends on Twitter only in a few distinct manners whereas we can collectmassive amounts of Twitter data. To operationalize this hypothesis, we proposea latent source model for time series, which naturally leads to a "weightedmajority voting" classification rule that can be approximated by anearest-neighbor classifier. We establish nonasymptotic performance guaranteesof both weighted majority voting and nearest-neighbor classification under ourmodel accounting for how much of the time series we observe and the modelcomplexity. Experimental results on synthetic data show weighted majorityvoting achieving the same misclassification rate as nearest-neighborclassification while observing less of the time series. We then use weightedmajority to forecast which news topics on Twitter become trends, where we areable to detect such "trending topics" in advance of Twitter 79% of the time,with a mean early advantage of 1 hour and 26 minutes, a true positive rate of95%, and a false positive rate of 4%.
arxiv-3000-171 | An analysis of NK and generalized NK landscapes | http://arxiv.org/pdf/1302.3541v1.pdf | author:Jeffrey S. Buzas, Jeffrey Dinitz category:cs.NE published:2013-02-14 summary:Simulated landscapes have been used for decades to evaluate search strategieswhose goal is to find the landscape location with maximum fitness. Applicationsinclude modeling the capacity of enzymes to catalyze reactions and the clinicaleffectiveness of medical treatments. Understanding properties of landscapes isimportant for understanding search difficulty. This paper presents a novel andtransparent characterization of NK landscapes. We prove that NK landscapes can be represented by parametric linearinteraction models where model coefficients have meaningful interpretations. Wederive the statistical properties of the model coefficients, providing insightinto how the NK algorithm parses importance to main effects and interactions.An important insight derived from the linear model representation is that therank of the linear model defined by the NK algorithm is correlated with thenumber of local optima, a strong determinant of landscape complexity and searchdifficulty. We show that the maximal rank for an NK landscape is achievedthrough epistatic interactions that form partially balanced incomplete blockdesigns. Finally, an analytic expression representing the expected number oflocal optima on the landscape is derived, providing a way to quickly computethe expected number of local optima for very large landscapes.
arxiv-3000-172 | A consistent clustering-based approach to estimating the number of change-points in highly dependent time-series | http://arxiv.org/pdf/1302.3407v1.pdf | author:Azaden Khaleghi, Daniil Ryabko category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2013-02-14 summary:The problem of change-point estimation is considered under a generalframework where the data are generated by unknown stationary ergodic processdistributions. In this context, the consistent estimation of the number ofchange-points is provably impossible. However, it is shown that a consistentclustering method may be used to estimate the number of change points, underthe additional constraint that the correct number of process distributions thatgenerate the data is provided. This additional parameter has a naturalinterpretation in many real-world applications. An algorithm is proposed thatestimates the number of change-points and locates the changes. The proposedalgorithm is shown to be asymptotically consistent; its empirical evaluationsare provided.
arxiv-3000-173 | StructBoost: Boosting Methods for Predicting Structured Output Variables | http://arxiv.org/pdf/1302.3283v3.pdf | author:Chunhua Shen, Guosheng Lin, Anton van den Hengel category:cs.LG published:2013-02-14 summary:Boosting is a method for learning a single accurate predictor by linearlycombining a set of less accurate weak learners. Recently, structured learninghas found many applications in computer vision. Inspired by structured supportvector machines (SSVM), here we propose a new boosting algorithm for structuredoutput prediction, which we refer to as StructBoost. StructBoost supportsnonlinear structured learning by combining a set of weak structured learners.As SSVM generalizes SVM, our StructBoost generalizes standard boostingapproaches such as AdaBoost, or LPBoost to structured learning. The resultingoptimization problem of StructBoost is more challenging than SSVM in the sensethat it may involve exponentially many variables and constraints. In contrast,for SSVM one usually has an exponential number of constraints and acutting-plane method is used. In order to efficiently solve StructBoost, weformulate an equivalent $ 1 $-slack formulation and solve it using acombination of cutting planes and column generation. We show the versatilityand usefulness of StructBoost on a range of problems such as optimizing thetree loss for hierarchical multi-class classification, optimizing the Pascaloverlap criterion for robust visual tracking and learning conditional randomfield parameters for image segmentation.
arxiv-3000-174 | Locally epistatic genomic relationship matrices for genomic association, prediction and selection | http://arxiv.org/pdf/1302.3463v6.pdf | author:Deniz Akdemir category:stat.AP stat.ML published:2013-02-14 summary:As the amount and complexity of genetic information increases it is necessarythat we explore some efficient ways of handling these data. This study takesthe "divide and conquer" approach for analyzing high dimensional genomic data.Our aims include reducing the dimensionality of the problem that has to bedealt one at a time, improving the performance and interpretability of themodels. We propose using the inherent structures in the genome; to divide thebigger problem into manageable parts. In plant and animal breeding studies adistinction is made between the commercial value (additive + epistatic geneticeffects) and the breeding value (additive genetic effects) of an individualsince it is expected that some of the epistatic genetic effects will be lostdue to recombination. In this paper, we argue that the breeder can takeadvantage of some of the epistatic marker effects in regions of lowrecombination. The models introduced here aim to estimate local epistatic lineheritability by using the genetic map information and combine the localadditive and epistatic effects. To this end, we have used semi-parametric mixedmodels with multiple local genomic relationship matrices with hierarchicaltesting designs and lasso post-processing for sparsity in the final model andspeed. Our models produce good predictive performance along with geneticassociation information.
arxiv-3000-175 | Asymptotic Model Selection for Directed Networks with Hidden Variables | http://arxiv.org/pdf/1302.3580v2.pdf | author:Dan Geiger, David Heckerman, Christopher Meek category:cs.LG cs.AI stat.ML published:2013-02-13 summary:We extend the Bayesian Information Criterion (BIC), an asymptoticapproximation for the marginal likelihood, to Bayesian networks with hiddenvariables. This approximation can be used to select models given large samplesof data. The standard BIC as well as our extension punishes the complexity of amodel according to the dimension of its parameters. We argue that the dimensionof a Bayesian network with hidden variables is the rank of the Jacobian matrixof the transformation between the parameters of the network and the parametersof the observable variables. We compute the dimensions of several networksincluding the naive Bayes model with a hidden root node.
arxiv-3000-176 | Building a reordering system using tree-to-string hierarchical model | http://arxiv.org/pdf/1302.3057v1.pdf | author:Jacob Dlougach, Irina Galinskaya category:cs.CL published:2013-02-13 summary:This paper describes our submission to the First Workshop on Reordering forStatistical Machine Translation. We have decided to build a reordering systembased on tree-to-string model, using only publicly available tools toaccomplish this task. With the provided training data we have built atranslation model using Moses toolkit, and then we applied a chart decoder,implemented in Moses, to reorder the sentences. Even though our submission onlycovered English-Farsi language pair, we believe that the approach itself shouldwork regardless of the choice of the languages, so we have also carried out theexperiments for English-Italian and English-Urdu. For these language pairs wehave noticed a significant improvement over the baseline in BLEU, Kendall-Tauand Hamming metrics. A detailed description is given, so that everyone canreproduce our results. Also, some possible directions for further improvementsare discussed.
arxiv-3000-177 | An Efficient Dual Approach to Distance Metric Learning | http://arxiv.org/pdf/1302.3219v1.pdf | author:Chunhua Shen, Junae Kim, Fayao Liu, Lei Wang, Anton van den Hengel category:cs.LG published:2013-02-13 summary:Distance metric learning is of fundamental interest in machine learningbecause the distance metric employed can significantly affect the performanceof many learning methods. Quadratic Mahalanobis metric learning is a popularapproach to the problem, but typically requires solving a semidefiniteprogramming (SDP) problem, which is computationally expensive. Standardinterior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with$D$ the dimension of input data), and can thus only practically solve problemsexhibiting less than a few thousand variables. Since the number of variables is$D (D+1) / 2 $, this implies a limit upon the size of problem that canpractically be solved of around a few hundred dimensions. The complexity of thepopular quadratic Mahalanobis metric learning approach thus limits the size ofproblem to which metric learning can be applied. Here we propose asignificantly more efficient approach to the metric learning problem based onthe Lagrange dual formulation of the problem. The proposed formulation is muchsimpler to implement, and therefore allows much larger Mahalanobis metriclearning problems to be solved. The time complexity of the proposed method is$O (D ^ 3) $, which is significantly lower than that of the SDP approach.Experiments on a variety of datasets demonstrate that the proposed methodachieves an accuracy comparable to the state-of-the-art, but is applicable tosignificantly larger problems. We also show that the proposed method can beapplied to solve more general Frobenius-norm regularized SDP problemsapproximately.
arxiv-3000-178 | Pavlov's dog associative learning demonstrated on synaptic-like organic transistors | http://arxiv.org/pdf/1302.3261v1.pdf | author:O. Bichler, W. Zhao, F. Alibart, S. Pleutin, S. Lenfant, D. Vuillaume, C. Gamrat category:q-bio.NC cs.ET cs.NE published:2013-02-13 summary:In this letter, we present an original demonstration of an associativelearning neural network inspired by the famous Pavlov's dogs experiment. Asingle nanoparticle organic memory field effect transistor (NOMFET) is used toimplement each synapse. We show how the physical properties of this dynamicmemristive device can be used to perform low power write operations for thelearning and implement short-term association using temporal coding and spiketiming dependent plasticity based learning. An electronic circuit was built tovalidate the proposed learning scheme with packaged devices, with goodreproducibility despite the complex synaptic-like dynamic of the NOMFET inpulse regime.
arxiv-3000-179 | Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem | http://arxiv.org/pdf/1302.3268v2.pdf | author:Ittai Abraham, Omar Alonso, Vasilis Kandylas, Aleksandrs Slivkins category:cs.LG published:2013-02-13 summary:Very recently crowdsourcing has become the de facto platform for distributingand collecting human computation for a wide range of tasks and applicationssuch as information retrieval, natural language processing and machinelearning. Current crowdsourcing platforms have some limitations in the area ofquality control. Most of the effort to ensure good quality has to be done bythe experimenter who has to manage the number of workers needed to reach goodresults. We propose a simple model for adaptive quality control in crowdsourcedmultiple-choice tasks which we call the \emph{bandit survey problem}. Thismodel is related to, but technically different from the well-known multi-armedbandit problem. We present several algorithms for this problem, and supportthem with analysis and simulations. Our approach is based in our experienceconducting relevance evaluation for a large commercial search engine.
arxiv-3000-180 | Object Recognition with Imperfect Perception and Redundant Description | http://arxiv.org/pdf/1302.3556v1.pdf | author:Claude Barrouil, Jerome Lemaire category:cs.CV cs.AI published:2013-02-13 summary:This paper deals with a scene recognition system in a robotics contex. Thegeneral problem is to match images with <I>a priori</I> descriptions. A typicalmission would consist in identifying an object in an installation with a visionsystem situated at the end of a manipulator and with a human operator provideddescription, formulated in a pseudo-natural language, and possibly redundant.The originality of this work comes from the nature of the description, from thespecial attention given to the management of imprecision and uncertainty in theinterpretation process and from the way to assess the description redundancy soas to reinforce the overall matching likelihood.
arxiv-3000-181 | Learning Equivalence Classes of Bayesian Networks Structures | http://arxiv.org/pdf/1302.3566v1.pdf | author:David Maxwell Chickering category:cs.AI cs.LG stat.ML published:2013-02-13 summary:Approaches to learning Bayesian networks from data typically combine ascoring function with a heuristic search procedure. Given a Bayesian networkstructure, many of the scoring functions derived in the literature return ascore for the entire equivalence class to which the structure belongs. Whenusing such a scoring function, it is appropriate for the heuristic searchalgorithm to search over equivalence classes of Bayesian networks as opposed toindividual structures. We present the general formulation of a search space forwhich the states of the search correspond to equivalence classes of structures.Using this space, any one of a number of heuristic search algorithms can easilybe applied. We compare greedy search performance in the proposed search spaceto greedy search performance in a search space for which the states correspondto individual Bayesian network structures.
arxiv-3000-182 | Learning Bayesian Networks with Local Structure | http://arxiv.org/pdf/1302.3577v1.pdf | author:Nir Friedman, Moises Goldszmidt category:cs.AI cs.LG stat.ML published:2013-02-13 summary:In this paper we examine a novel addition to the known methods for learningBayesian networks from data that improves the quality of the learned networks.Our approach explicitly represents and learns the local structure in theconditional probability tables (CPTs), that quantify these networks. Thisincreases the space of possible models, enabling the representation of CPTswith a variable number of parameters that depends on the learned localstructures. The resulting learning procedure is capable of inducing models thatbetter emulate the real complexity of the interactions present in the data. Wedescribe the theoretical foundations and practical aspects of learning localstructures, as well as an empirical evaluation of the proposed method. Thisevaluation indicates that learning curves characterizing the procedure thatexploits the local structure converge faster than these of the standardprocedure. Our results also show that networks learned with local structuretend to be more complex (in terms of arcs), yet require less parameters.
arxiv-3000-183 | Exact Methods for Multistage Estimation of a Binomial Proportion | http://arxiv.org/pdf/1302.3447v1.pdf | author:Zhengjia Chen, Xinjia Chen category:math.ST cs.LG cs.NA math.PR stat.TH published:2013-02-13 summary:We first review existing sequential methods for estimating a binomialproportion. Afterward, we propose a new family of group sequential samplingschemes for estimating a binomial proportion with prescribed margin of errorand confidence level. In particular, we establish the uniform controllabilityof coverage probability and the asymptotic optimality for such a family ofsampling schemes. Our theoretical results establish the possibility that theparameters of this family of sampling schemes can be determined so that theprescribed level of confidence is guaranteed with little waste of samples.Analytic bounds for the cumulative distribution functions and expectations ofsample numbers are derived. Moreover, we discuss the inherent connection ofvarious sampling schemes. Numerical issues are addressed for improving theaccuracy and efficiency of computation. Computational experiments are conductedfor comparing sampling schemes. Illustrative examples are given forapplications in clinical trials.
arxiv-3000-184 | Bayesian Learning of Loglinear Models for Neural Connectivity | http://arxiv.org/pdf/1302.3590v1.pdf | author:Kathryn Blackmond Laskey, Laura Martignon category:cs.LG q-bio.NC stat.AP stat.ML published:2013-02-13 summary:This paper presents a Bayesian approach to learning the connectivitystructure of a group of neurons from data on configuration frequencies. A majorobjective of the research is to provide statistical tools for detecting changesin firing patterns with changing stimuli. Our framework is not restricted tothe well-understood case of pair interactions, but generalizes the Boltzmannmachine model to allow for higher order interactions. The paper applies aMarkov Chain Monte Carlo Model Composition (MC3) algorithm to search overconnectivity structures and uses Laplace's method to approximate posteriorprobabilities of structures. Performance of the methods was tested on syntheticdata. The models were also applied to data obtained by Vaadia on multi-unitrecordings of several neurons in the visual cortex of a rhesus monkey in twodifferent attentional states. Results confirmed the experimenters' conjecturethat different attentional states were associated with different interactionstructures.
arxiv-3000-185 | Towards Identification of Relevant Variables in the observed Aerosol Optical Depth Bias between MODIS and AERONET observations | http://arxiv.org/pdf/1302.2969v1.pdf | author:N. K. Malakar, D. J. Lary, D. Gencaga, A. Albayrak, J. Wei category:stat.ML published:2013-02-13 summary:Measurements made by satellite remote sensing, Moderate Resolution ImagingSpectroradiometer (MODIS), and globally distributed Aerosol Robotic Network(AERONET) are compared. Comparison of the two datasets measurements for aerosoloptical depth values show that there are biases between the two data products.In this paper, we present a general framework towards identifying relevant setof variables responsible for the observed bias. We present a general frameworkto identify the possible factors influencing the bias, which might beassociated with the measurement conditions such as the solar and sensor zenithangles, the solar and sensor azimuth, scattering angles, and surfacereflectivity at the various measured wavelengths, etc. Specifically, weperformed analysis for remote sensing Aqua-Land data set, and used machinelearning technique, neural network in this case, to perform multivariateregression between the ground-truth and the training data sets. Finally, weused mutual information between the observed and the predicted values as themeasure of similarity to identify the most relevant set of variables. Thesearch is brute force method as we have to consider all possible combinations.The computations involves a huge number crunching exercise, and we implementedit by writing a job-parallel program.
arxiv-3000-186 | Efficient Approximations for the Marginal Likelihood of Incomplete Data Given a Bayesian Network | http://arxiv.org/pdf/1302.3567v2.pdf | author:David Maxwell Chickering, David Heckerman category:cs.LG cs.AI stat.ML published:2013-02-13 summary:We discuss Bayesian methods for learning Bayesian networks when data sets areincomplete. In particular, we examine asymptotic approximations for themarginal likelihood of incomplete data given a Bayesian network. We considerthe Laplace approximation and the less accurate but more efficient BIC/MDLapproximation. We also consider approximations proposed by Draper (1993) andCheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL,but their accuracy has not been studied in any depth. We compare the accuracyof these approximations under the assumption that the Laplace approximation isthe most accurate. In experiments using synthetic data generated from discretenaive-Bayes models having a hidden root node, we find that the CS measure isthe most accurate.
arxiv-3000-187 | Morphological Analusis Of The Left Ventricular Eendocardial Surface Using A Bag-Of-Features Descriptor | http://arxiv.org/pdf/1302.3155v1.pdf | author:Anirban Mukhopadhyay, Zhen Qian, Suchendra M. Bhandarkar, Tianming Liu, Sarah Rinehart, Szilard Voros category:cs.CV published:2013-02-13 summary:The limitations of conventional imaging techniques have hitherto precluded athorough and formal investigation of the complex morphology of the leftventricular (LV) endocardial surface and its relation to the severity ofCoronary Artery Disease (CAD). Recent developments in high-resolutionMultirow-Detector Computed Tomography (MDCT) scanner technology have enabledthe imaging of LV endocardial surface morphology in a single heart beat.Analysis of high-resolution Computed Tomography (CT) images from a 320-MDCTscanner allows the study of the relationship between percent Diameter Stenosis(DS) of the major coronary arteries and localization of the cardiac segmentsaffected by coronary arterial stenosis. In this paper a novel approach for theanalysis using a combination of rigid transformation-invariant shapedescriptors and a more generalized isometry-invariant Bag-of-Features (BoF)descriptor, is proposed and implemented. The proposed approach is shown to besuccessful in identifying, localizing and quantifying the incidence and extentof CAD and thus, is seen to have a potentially significant clinical impact.Specifically, the association between the incidence and extent of CAD,determined via the percent DS measurements of the major coronary arteries, andthe alterations in the endocardial surface morphology is formally quantified. Amultivariate regression test performed on a strict leave-one-out basis areshown to exhibit a distinct pattern in terms of the correlation coefficientwithin the cardiac segments where the incidence of coronary arterial stenosisis localized.
arxiv-3000-188 | On the Sample Complexity of Learning Bayesian Networks | http://arxiv.org/pdf/1302.3579v1.pdf | author:Nir Friedman, Zohar Yakhini category:cs.LG stat.ML published:2013-02-13 summary:In recent years there has been an increasing interest in learning Bayesiannetworks from data. One of the most effective methods for learning suchnetworks is based on the minimum description length (MDL) principle. Previouswork has shown that this learning procedure is asymptotically successful: withprobability one, it will converge to the target distribution, given asufficient number of samples. However, the rate of this convergence has beenhitherto unknown. In this work we examine the sample complexity of MDL basedlearning procedures for Bayesian networks. We show that the number of samplesneeded to learn an epsilon-close approximation (in terms of entropy distance)with confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog(1/delta)). This means that the sample complexity is a low-order polynomial inthe error threshold and sub-linear in the confidence bound. We also discuss howthe constants in this term depend on the complexity of the target distribution.Finally, we address questions of asymptotic minimality and propose a method forusing the sample complexity results to speed up the learning process.
arxiv-3000-189 | Competing With Strategies | http://arxiv.org/pdf/1302.2672v1.pdf | author:Wei Han, Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.GT cs.LG published:2013-02-12 summary:We study the problem of online learning with a notion of regret defined withrespect to a set of strategies. We develop tools for analyzing the minimaxrates and for deriving regret-minimization algorithms in this scenario. Whilethe standard methods for minimizing the usual notion of regret fail, throughour analysis we demonstrate existence of regret-minimization methods thatcompete with such sets of strategies as: autoregressive algorithms, strategiesbased on statistical models, regularized least squares, and follow theregularized leader strategies. In several cases we also derive efficientlearning algorithms.
arxiv-3000-190 | Latent Self-Exciting Point Process Model for Spatial-Temporal Networks | http://arxiv.org/pdf/1302.2671v3.pdf | author:Yoon-Sik Cho, Aram Galstyan, P. Jeffrey Brantingham, George Tita category:cs.SI cs.LG stat.ML published:2013-02-12 summary:We propose a latent self-exciting point process model that describesgeographically distributed interactions between pairs of entities. In contrastto most existing approaches that assume fully observable interactions, here weconsider a scenario where certain interaction events lack information aboutparticipants. Instead, this information needs to be inferred from the availableobservations. We develop an efficient approximate algorithm based onvariational expectation-maximization to infer unknown participants in an eventgiven the location and the time of the event. We validate the model onsynthetic as well as real-world data, and obtain very promising results on theidentity-inference task. We also use our model to predict the timing andparticipants of future events, and demonstrate that it compares favorably withbaseline approaches.
arxiv-3000-191 | Covariance Estimation in High Dimensions via Kronecker Product Expansions | http://arxiv.org/pdf/1302.2686v10.pdf | author:Theodoros Tsiligkaridis, Alfred O. Hero III category:stat.ME stat.ML published:2013-02-12 summary:This paper presents a new method for estimating high dimensional covariancematrices. The method, permuted rank-penalized least-squares (PRLS), is based ona Kronecker product series expansion of the true covariance matrix. Assuming ani.i.d. Gaussian random sample, we establish high dimensional rates ofconvergence to the true covariance as both the number of samples and the numberof variables go to infinity. For covariance matrices of low separation rank,our results establish that PRLS has significantly faster convergence than thestandard sample covariance matrix (SCM) estimator. The convergence ratecaptures a fundamental tradeoff between estimation error and approximationerror, thus providing a scalable covariance estimation framework in terms ofseparation rank, similar to low rank approximation of covariance matrices. TheMSE convergence rates generalize the high dimensional rates recently obtainedfor the ML Flip-flop algorithm for Kronecker product covariance estimation. Weshow that a class of block Toeplitz covariance matrices is approximatable bylow separation rank and give bounds on the minimal separation rank $r$ thatensures a given level of bias. Simulations are presented to validate thetheoretical bounds. As a real world application, we illustrate the utility ofthe proposed Kronecker covariance estimator for spatio-temporal linear leastsquares prediction of multivariate wind speed measurements.
arxiv-3000-192 | A Tensor Approach to Learning Mixed Membership Community Models | http://arxiv.org/pdf/1302.2684v4.pdf | author:Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade category:cs.LG cs.SI stat.ML published:2013-02-12 summary:Community detection is the task of detecting hidden communities from observedinteractions. Guaranteed community detection has so far been mostly limited tomodels with non-overlapping communities such as the stochastic block model. Inthis paper, we remove this restriction, and provide guaranteed communitydetection for a family of probabilistic network models with overlappingcommunities, termed as the mixed membership Dirichlet model, first introducedby Airoldi et al. This model allows for nodes to have fractional memberships inmultiple communities and assumes that the community memberships are drawn froma Dirichlet distribution. Moreover, it contains the stochastic block model as aspecial case. We propose a unified approach to learning these models via atensor spectral decomposition method. Our estimator is based on low-ordermoment tensor of the observed network, consisting of 3-star counts. Ourlearning method is fast and is based on simple linear algebraic operations,e.g. singular value decomposition and tensor power iterations. We provideguaranteed recovery of community memberships and model parameters and present acareful finite sample analysis of our learning method. As an important specialcase, our results match the best known scaling requirements for the(homogeneous) stochastic block model.
arxiv-3000-193 | Adaptive Metric Dimensionality Reduction | http://arxiv.org/pdf/1302.2752v3.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG cs.DS stat.ML published:2013-02-12 summary:We study adaptive data-dependent dimensionality reduction in the context ofsupervised learning in general metric spaces. Our main statistical contributionis a generalization bound for Lipschitz functions in metric spaces that aredoubling, or nearly doubling. On the algorithmic front, we describe an analogueof PCA for metric spaces: namely an efficient procedure that approximates thedata's intrinsic dimension, which is often much lower than the ambientdimension. Our approach thus leverages the dual benefits of low dimensionality:(1) more efficient algorithms, e.g., for proximity search, and (2) moreoptimistic generalization bounds.
arxiv-3000-194 | Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI | http://arxiv.org/pdf/1302.2712v3.pdf | author:Yue Huang, John Paisley, Qin Lin, Xinghao Ding, Xueyang Fu, Xiao-ping Zhang category:cs.CV physics.med-ph stat.AP published:2013-02-12 summary:We develop a Bayesian nonparametric model for reconstructing magneticresonance images (MRI) from highly undersampled k-space data. We performdictionary learning as part of the image reconstruction process. To this end,we use the beta process as a nonparametric dictionary learning prior forrepresenting an image patch as a sparse combination of dictionary elements. Thesize of the dictionary and the patch-specific sparsity pattern are inferredfrom the data, in addition to other dictionary learning variables. Dictionarylearning is performed directly on the compressed image, and so is tailored tothe MRI being considered. In addition, we investigate a total variation penaltyterm in combination with the dictionary learning model, and show how thedenoising property of dictionary learning removes dependence on regularizationparameters in the noisy setting. We derive a stochastic optimization algorithmbased on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use thealternating direction method of multipliers (ADMM) for efficiently performingtotal variation minimization. We present empirical results on several MRI,which show that the proposed regularization framework can improvereconstruction accuracy over other methods.
arxiv-3000-195 | Coherence and sufficient sampling densities for reconstruction in compressed sensing | http://arxiv.org/pdf/1302.2767v2.pdf | author:Franz J. Király, Louis Theran category:cs.LG cs.IT math.AG math.IT stat.ML published:2013-02-12 summary:We give a new, very general, formulation of the compressed sensing problem interms of coordinate projections of an analytic variety, and derive sufficientsampling rates for signal reconstruction. Our bounds are linear in thecoherence of the signal space, a geometric parameter independent of thespecific signal and measurement, and logarithmic in the ambient dimension wherethe signal is presented. We exemplify our approach by deriving sufficientsampling densities for low-rank matrix completion and distance matrixcompletion which are independent of the true matrix.
arxiv-3000-196 | A new bio-inspired method for remote sensing imagery classification | http://arxiv.org/pdf/1302.2606v2.pdf | author:Amghar Yasmina Teldja, Fizazi Hadria category:cs.NE cs.CV published:2013-02-11 summary:The problem of supervised classification of the satellite image is consideredto be the task of grouping pixels into a number of homogeneous regions in spaceintensity. This paper proposes a novel approach that combines a radial basicfunction clustering network with a growing neural gas include utility factorclassifier to yield improved solutions, obtained with previous networks. Thedouble objective technique is first used to the development of a method toperform the satellite images classification, and finally, the implementation toaddress the issue of the number of nodes in the hidden layer of the classicRadial Basis functions network. Results demonstrating the effectiveness of theproposed technique are provided for numeric remote sensing imagery. Moreover,the remotely sensed image of Oran city in Algeria has been classified using theproposed technique to establish its utility.
arxiv-3000-197 | The trace norm constrained matrix-variate Gaussian process for multitask bipartite ranking | http://arxiv.org/pdf/1302.2576v1.pdf | author:Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh category:cs.LG stat.ML published:2013-02-11 summary:We propose a novel hierarchical model for multitask bipartite ranking. Theproposed approach combines a matrix-variate Gaussian process with a generativemodel for task-wise bipartite ranking. In addition, we employ a novel traceconstrained variational inference approach to impose low rank structure on theposterior matrix-variate Gaussian process. The resulting posterior covariancefunction is derived in closed form, and the posterior mean function is thesolution to a matrix-variate regression with a novel spectral elastic netregularizer. Further, we show that variational inference for the traceconstrained matrix-variate Gaussian process combined with maximum likelihoodparameter estimation for the bipartite ranking model is jointly convex. Ourmotivating application is the prioritization of candidate disease genes. Thegoal of this task is to aid the identification of unobserved associationsbetween human genes and diseases using a small set of observed associations aswell as kernels induced by gene-gene interaction networks and diseaseontologies. Our experimental results illustrate the performance of the proposedmodel on real world datasets. Moreover, we find that the resulting low ranksolution improves the computational scalability of training and testing ascompared to baseline models.
arxiv-3000-198 | Online Regret Bounds for Undiscounted Continuous Reinforcement Learning | http://arxiv.org/pdf/1302.2550v1.pdf | author:Ronald Ortner, Daniil Ryabko category:cs.LG published:2013-02-11 summary:We derive sublinear regret bounds for undiscounted reinforcement learning incontinuous state space. The proposed algorithm combines state aggregation withthe use of upper confidence bounds for implementing optimism in the face ofuncertainty. Beside the existence of an optimal policy which satisfies thePoisson equation, the only assumptions made are Holder continuity of rewardsand transition probabilities.
arxiv-3000-199 | Selecting the State-Representation in Reinforcement Learning | http://arxiv.org/pdf/1302.2552v1.pdf | author:Odalric-Ambrym Maillard, Rémi Munos, Daniil Ryabko category:cs.LG published:2013-02-11 summary:The problem of selecting the right state-representation in a reinforcementlearning problem is considered. Several models (functions mapping pastobservations to a finite set) of the observations are given, and it is knownthat for at least one of these models the resulting state dynamics are indeedMarkovian. Without knowing neither which of the models is the correct one, norwhat are the probabilistic characteristics of the resulting MDP, it is requiredto obtain as much reward as the optimal policy for the correct model (or forthe best of the correct models, if there are several). We propose an algorithmthat achieves that, with a regret of order T^{2/3} where T is the horizon time.
arxiv-3000-200 | Extracting useful rules through improved decision tree induction using information entropy | http://arxiv.org/pdf/1302.2436v1.pdf | author:Mohd Mahmood Ali, Mohd S Qaseem, Lakshmi Rajamani, A Govardhan category:cs.LG published:2013-02-11 summary:Classification is widely used technique in the data mining domain, wherescalability and efficiency are the immediate problems in classificationalgorithms for large databases. We suggest improvements to the existing C4.5decision tree algorithm. In this paper attribute oriented induction (AOI) andrelevance analysis are incorporated with concept hierarchys knowledge andHeightBalancePriority algorithm for construction of decision tree along withMulti level mining. The assignment of priorities to attributes is done byevaluating information entropy, at different levels of abstraction for buildingdecision tree using HeightBalancePriority algorithm. Modified DMQL queries areused to understand and explore the shortcomings of the decision trees generatedby C4.5 classifier for education dataset and the results are compared with theproposed approach.
arxiv-3000-201 | Toric grammars: a new statistical approach to natural language modeling | http://arxiv.org/pdf/1302.2569v1.pdf | author:Olivier Catoni, Thomas Mainguy category:stat.ML cs.CL math.PR published:2013-02-11 summary:We propose a new statistical model for computational linguistics. Rather thantrying to estimate directly the probability distribution of a random sentenceof the language, we define a Markov chain on finite sets of sentences with manyfinite recurrent communicating classes and define our language model as theinvariant probability measures of the chain on each recurrent communicatingclass. This Markov chain, that we call a communication model, recombines ateach step randomly the set of sentences forming its current state, using somegrammar rules. When the grammar rules are fixed and known in advance instead ofbeing estimated on the fly, we can prove supplementary mathematical properties.In particular, we can prove in this case that all states are recurrent states,so that the chain defines a partition of its state space into finite recurrentcommunicating classes. We show that our approach is a decisive departure fromMarkov models at the sentence level and discuss its relationships with ContextFree Grammars. Although the toric grammars we use are closely related toContext Free Grammars, the way we generate the language from the grammar isqualitatively different. Our communication model has two purposes. On the onehand, it is used to define indirectly the probability distribution of a randomsentence of the language. On the other hand it can serve as a (crude) model oflanguage transmission from one speaker to another speaker through thecommunication of a (large) set of sentences.
arxiv-3000-202 | Adaptive-treed bandits | http://arxiv.org/pdf/1302.2489v4.pdf | author:Adam D. Bull category:math.ST stat.ML stat.TH published:2013-02-11 summary:We describe a novel algorithm for noisy global optimisation andcontinuum-armed bandits, with good convergence properties over any continuousreward function having finitely many polynomial maxima. Over such functions,our algorithm achieves square-root regret in bandits, and inverse-square-rooterror in optimisation, without prior information. Our algorithm works byreducing these problems to tree-armed bandits, and we also provide new resultsin this setting. We show it is possible to adaptively combine multiple trees soas to minimise the regret, and also give near-matching lower bounds on theregret in terms of the zooming dimension.
arxiv-3000-203 | Geometrical complexity of data approximators | http://arxiv.org/pdf/1302.2645v2.pdf | author:E. M. Mirkes, A. Zinovyev, A. N. Gorban category:stat.ML cs.LG published:2013-02-11 summary:There are many methods developed to approximate a cloud of vectors embeddedin high-dimensional space by simpler objects: starting from principal pointsand linear manifolds to self-organizing maps, neural gas, elastic maps, varioustypes of principal curves and principal trees, and so on. For each type ofapproximators the measure of the approximator complexity was developed too.These measures are necessary to find the balance between accuracy andcomplexity and to define the optimal approximations of a given type. We proposea measure of complexity (geometrical complexity) which is applicable toapproximators of several types and which allows comparing data approximationsof different types.
arxiv-3000-204 | Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning | http://arxiv.org/pdf/1302.2553v2.pdf | author:Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, Daniil Ryabko category:cs.LG published:2013-02-11 summary:We consider an agent interacting with an environment in a single stream ofactions, observations, and rewards, with no reset. This process is not assumedto be a Markov Decision Process (MDP). Rather, the agent has severalrepresentations (mapping histories of past interactions to a discrete statespace) of the environment with unknown dynamics, only some of which result inan MDP. The goal is to minimize the average regret criterion against an agentwho knows an MDP representation giving the highest optimal reward, and actsoptimally in it. Recent regret bounds for this setting are of order$O(T^{2/3})$ with an additive term constant yet exponential in somecharacteristics of the optimal MDP. We propose an algorithm whose regret after$T$ time steps is $O(\sqrt{T})$, with all constants reasonably small. This isoptimal in $T$ since $O(\sqrt{T})$ is the optimal regret in the setting oflearning in a (single discrete) MDP.
arxiv-3000-205 | Conditional Gradient Algorithms for Norm-Regularized Smooth Convex Optimization | http://arxiv.org/pdf/1302.2325v4.pdf | author:Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski category:math.OC stat.CO stat.ML published:2013-02-10 summary:Motivated by some applications in signal processing and machine learning, weconsider two convex optimization problems where, given a cone $K$, a norm$\\cdot\$ and a smooth convex function $f$, we want either 1) to minimize thenorm over the intersection of the cone and a level set of $f$, or 2) tominimize over the cone the sum of $f$ and a multiple of the norm. We focus onthe case where (a) the dimension of the problem is too large to allow forinterior point algorithms, (b) $\\cdot\$ is "too complicated" to allow forcomputationally cheap Bregman projections required in the first-order proximalgradient algorithms. On the other hand, we assume that {it is relatively easyto minimize linear forms over the intersection of $K$ and the unit$\\cdot\$-ball}. Motivating examples are given by the nuclear norm with $K$being the entire space of matrices, or the positive semidefinite cone in thespace of symmetric matrices, and the Total Variation norm on the space of 2Dimages. We discuss versions of the Conditional Gradient algorithm capable tohandle our problems of interest, provide the related theoretical efficiencyestimates and outline some applications.
arxiv-3000-206 | A Time Series Forest for Classification and Feature Extraction | http://arxiv.org/pdf/1302.2277v2.pdf | author:Houtao Deng, George Runger, Eugene Tuv, Martyanov Vladimir category:cs.LG published:2013-02-09 summary:We propose a tree ensemble method, referred to as time series forest (TSF),for time series classification. TSF employs a combination of the entropy gainand a distance measure, referred to as the Entrance (entropy and distance)gain, for evaluating the splits. Experimental studies show that the Entrancegain criterion improves the accuracy of TSF. TSF randomly samples features ateach tree node and has a computational complexity linear in the length of atime series and can be built using parallel computing techniques such asmulti-core computing used here. The temporal importance curve is also proposedto capture the important temporal characteristics useful for classification.Experimental studies show that TSF using simple features such as mean,deviation and slope outperforms strong competitors such as one-nearest-neighborclassifiers with dynamic time warping, is computationally efficient, and canprovide insights into the temporal characteristics.
arxiv-3000-207 | Learning Universally Quantified Invariants of Linear Data Structures | http://arxiv.org/pdf/1302.2273v1.pdf | author:Pranav Garg, Christof Loding, P. Madhusudan, Daniel Neider category:cs.PL cs.FL cs.LG published:2013-02-09 summary:We propose a new automaton model, called quantified data automata over words,that can model quantified invariants over linear data structures, and buildpoly-time active learning algorithms for them, where the learner is allowed toquery the teacher with membership and equivalence queries. In order to expressinvariants in decidable logics, we invent a decidable subclass of QDAs, calledelastic QDAs, and prove that every QDA has a uniqueminimally-over-approximating elastic QDA. We then give an application of thesetheoretically sound and efficient active learning algorithms in a passivelearning framework and show that we can efficiently learn quantified lineardata structure invariants from samples obtained from dynamic runs for a largeclass of programs.
arxiv-3000-208 | pROST : A Smoothed Lp-norm Robust Online Subspace Tracking Method for Realtime Background Subtraction in Video | http://arxiv.org/pdf/1302.2073v2.pdf | author:Florian Seidel, Clemens Hage, Martin Kleinsteuber category:cs.CV published:2013-02-08 summary:An increasing number of methods for background subtraction use Robust PCA toidentify sparse foreground objects. While many algorithms use the L1-norm as aconvex relaxation of the ideal sparsifying function, we approach the problemwith a smoothed Lp-norm and present pROST, a method for robust online subspacetracking. The algorithm is based on alternating minimization on manifolds.Implemented on a graphics processing unit it achieves realtime performance.Experimental results on a state-of-the-art benchmark for background subtractionon real-world video data indicate that the method succeeds at a broad varietyof background subtraction scenarios, and it outperforms competing approacheswhen video quality is deteriorated by camera jitter.
arxiv-3000-209 | A new compressive video sensing framework for mobile broadcast | http://arxiv.org/pdf/1302.1947v1.pdf | author:Chengbo Li, Hong Jiang, Paul Wilford, Yin Zhang, Mike Scheutzow category:cs.MM cs.CV cs.IT math.IT published:2013-02-08 summary:A new video coding method based on compressive sampling is proposed. In thismethod, a video is coded using compressive measurements on video cubes. Videoreconstruction is performed by minimization of total variation (TV) of thepixelwise DCT coefficients along the temporal direction. A new reconstructionalgorithm is developed from TVAL3, an efficient TV minimization algorithm basedon the alternating minimization and augmented Lagrangian methods. Video codingwith this method is inherently scalable, and has applications in mobilebroadcast.
arxiv-3000-210 | Passive Learning with Target Risk | http://arxiv.org/pdf/1302.2157v2.pdf | author:Mehrdad Mahdavi, Rong Jin category:cs.LG published:2013-02-08 summary:In this paper we consider learning in passive setting but with a slightmodification. We assume that the target expected loss, also referred to astarget risk, is provided in advance for learner as prior knowledge. Unlike moststudies in the learning theory that only incorporate the prior knowledge intothe generalization bounds, we are able to explicitly utilize the target risk inthe learning process. Our analysis reveals a surprising result on the samplecomplexity of learning: by exploiting the target risk in the learningalgorithm, we show that when the loss function is both strongly convex andsmooth, the sample complexity reduces to $\O(\log (\frac{1}{\epsilon}))$, anexponential improvement compared to the sample complexity$\O(\frac{1}{\epsilon})$ for learning with strongly convex loss functions.Furthermore, our proof is constructive and is based on a computationallyefficient stochastic optimization algorithm for such settings which demonstratethat the proposed algorithm is practically useful.
arxiv-3000-211 | Minimax Optimal Algorithms for Unconstrained Linear Optimization | http://arxiv.org/pdf/1302.2176v1.pdf | author:H. Brendan McMahan category:cs.LG published:2013-02-08 summary:We design and analyze minimax-optimal algorithms for online linearoptimization games where the player's choice is unconstrained. The playerstrives to minimize regret, the difference between his loss and the loss of apost-hoc benchmark strategy. The standard benchmark is the loss of the beststrategy chosen from a bounded comparator set. When the the comparison set andthe adversary's gradients satisfy L_infinity bounds, we give the value of thegame in closed form and prove it approaches sqrt(2T/pi) as T -> infinity. Interesting algorithms result when we consider soft constraints on thecomparator, rather than restricting it to a bounded set. As a warmup, weanalyze the game with a quadratic penalty. The value of this game is exactlyT/2, and this value is achieved by perhaps the simplest online algorithm ofall: unprojected gradient descent with a constant learning rate. We then derive a minimax-optimal algorithm for a much softer penaltyfunction. This algorithm achieves good bounds under the standard notion ofregret for any comparator point, without needing to specify the comparator setin advance. The value of this game converges to sqrt{e} as T ->infinity; wegive a closed-form for the exact value as a function of T. The resultingalgorithm is natural in unconstrained investment or betting scenarios, since itguarantees at worst constant loss, while allowing for exponential rewardagainst an "easy" adversary.
arxiv-3000-212 | Data Mining of the Concept "End of the World" in Twitter Microblogs | http://arxiv.org/pdf/1302.2131v1.pdf | author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.IR physics.soc-ph published:2013-02-08 summary:This paper describes the analysis of quantitative characteristics of frequentsets and association rules in the posts of Twitter microblogs, related to thediscussion of "end of the world", which was allegedly predicted on December 21,2012 due to the Mayan calendar. Discovered frequent sets and association rulescharacterize semantic relations between the concepts of analyzed subjects.Thesupport for some fequent sets reaches the global maximum before the expectedevent with some time delay. Such frequent sets may be considered as predictivemarkers that characterize the significance of expected events for blogosphereusers. It was shown that time dynamics of confidence of some revealedassociation rules can also have predictive characteristics. Exceeding a certainthreshold, it may be a signal for the corresponding reaction in the societyduring the time interval between the maximum and probable coming of an event.
arxiv-3000-213 | Surveillance Video Processing Using Compressive Sensing | http://arxiv.org/pdf/1302.1942v1.pdf | author:Hong Jiang, Wei Deng, Zuowei Shen category:cs.CV cs.IT math.IT published:2013-02-08 summary:A compressive sensing method combined with decomposition of a matrix formedwith image frames of a surveillance video into low rank and sparse matrices isproposed to segment the background and extract moving objects in a surveillancevideo. The video is acquired by compressive measurements, and the measurementsare used to reconstruct the video by a low rank and sparse decomposition ofmatrix. The low rank component represents the background, and the sparsecomponent is used to identify moving objects in the surveillance video. Thedecomposition is performed by an augmented Lagrangian alternating directionmethod. Experiments are carried out to demonstrate that moving objects can bereliably extracted with a small amount of measurements.
arxiv-3000-214 | Efficiency for Regularization Parameter Selection in Penalized Likelihood Estimation of Misspecified Models | http://arxiv.org/pdf/1302.2068v1.pdf | author:Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff category:stat.ML published:2013-02-08 summary:It has been shown that AIC-type criteria are asymptotically efficientselectors of the tuning parameter in non-concave penalized regression methodsunder the assumption that the population variance is known or that a consistentestimator is available. We relax this assumption to prove that AIC itself isasymptotically efficient and we study its performance in finite samples. Inclassical regression, it is known that AIC tends to select overly complexmodels when the dimension of the maximum candidate model is large relative tothe sample size. Simulation studies suggest that AIC suffers from the sameshortcomings when used in penalized regression. We therefore propose the use ofthe classical corrected AIC (AICc) as an alternative and prove that itmaintains the desired asymptotic properties. To broaden our results, we furtherprove the efficiency of AIC for penalized likelihood methods in the context ofgeneralized linear models with no dispersion parameter. Similar results existin the literature but only for a restricted set of candidate models. Byemploying results from the classical literature on maximum-likelihoodestimation in misspecified models, we are able to establish this result for ageneral set of candidate models. We use simulations to assess the performanceof AIC and AICc, as well as that of other selectors, in finite samples for bothSCAD-penalized and Lasso regressions and a real data example is considered.
arxiv-3000-215 | Eye-GUIDE (Eye-Gaze User Interface Design) Messaging for Physically-Impaired People | http://arxiv.org/pdf/1302.1649v1.pdf | author:Rommel Anacan, James Greggory Alcayde, Retchel Antegra, Leah Luna category:cs.HC cs.CV published:2013-02-07 summary:Eye-GUIDE is an assistive communication tool designed for the paralyzed orphysically impaired people who were unable to move parts of their bodiesespecially people whose communications are limited only to eye movements. Theprototype consists of a camera and a computer. Camera captures images then itwill be send to the computer, where the computer will be the one to interpretthe data. Thus, Eye-GUIDE focuses on camera-based gaze tracking. The proponentdesigned the prototype to perform simple tasks and provides graphical userinterface in order the paralyzed or physically impaired person can easily useit.
arxiv-3000-216 | A Fast Learning Algorithm for Image Segmentation with Max-Pooling Convolutional Networks | http://arxiv.org/pdf/1302.1690v1.pdf | author:Jonathan Masci, Alessandro Giusti, Dan Cireşan, Gabriel Fricout, Jürgen Schmidhuber category:cs.CV published:2013-02-07 summary:We present a fast algorithm for training MaxPooling Convolutional Networks tosegment images. This type of network yields record-breaking performance in avariety of tasks, but is normally trained on a computationally expensivepatch-by-patch basis. Our new method processes each training image in a singlepass, which is vastly more efficient. We validate the approach in different scenarios and report a 1500-foldspeed-up. In an application to automated steel defect detection andsegmentation, we obtain excellent performance with short training times.
arxiv-3000-217 | Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks | http://arxiv.org/pdf/1302.1700v1.pdf | author:Alessandro Giusti, Dan C. Cireşan, Jonathan Masci, Luca M. Gambardella, Jürgen Schmidhuber category:cs.CV cs.AI published:2013-02-07 summary:Deep Neural Networks now excel at image classification, detection andsegmentation. When used to scan images by means of a sliding window, however,their high computational complexity can bring even the most powerful hardwareto its knees. We show how dynamic programming can speedup the process by ordersof magnitude, even when max-pooling layers are present.
arxiv-3000-218 | Feature Selection for Microarray Gene Expression Data using Simulated Annealing guided by the Multivariate Joint Entropy | http://arxiv.org/pdf/1302.1733v1.pdf | author:Fernando González, Lluís A. Belanche category:q-bio.QM cs.CE cs.LG stat.ML published:2013-02-07 summary:In this work a new way to calculate the multivariate joint entropy ispresented. This measure is the basis for a fast information-theoretic basedevaluation of gene relevance in a Microarray Gene Expression data context. Itslow complexity is based on the reuse of previous computations to calculatecurrent feature relevance. The mu-TAFS algorithm --named as such todifferentiate it from previous TAFS algorithms-- implements a simulatedannealing technique specially designed for feature subset selection. Thealgorithm is applied to the maximization of gene subset relevance in severalpublic-domain microarray data sets. The experimental results show a notoriouslyhigh classification performance and low size subsets formed by biologicallymeaningful genes.
arxiv-3000-219 | An ANN-based Method for Detecting Vocal Fold Pathology | http://arxiv.org/pdf/1302.1772v1.pdf | author:Vahid Majidnezhad, Igor Kheidorov category:cs.LG cs.CV cs.SD published:2013-02-07 summary:There are different algorithms for vocal fold pathology diagnosis. Thesealgorithms usually have three stages which are Feature Extraction, FeatureReduction and Classification. While the third stage implies a choice of avariety of machine learning methods, the first and second stages play acritical role in performance and accuracy of the classification system. In thispaper we present initial study of feature extraction and feature reduction inthe task of vocal fold pathology diagnosis. A new type of feature vector, basedon wavelet packet decomposition and Mel-Frequency-Cepstral-Coefficients(MFCCs), is proposed. Also Principal Component Analysis (PCA) is used forfeature reduction. An Artificial Neural Network is used as a classifier forevaluating the performance of our proposed method.
arxiv-3000-220 | Lensless Compressive Sensing Imaging | http://arxiv.org/pdf/1302.1789v1.pdf | author:Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford category:cs.CV cs.IT math.IT published:2013-02-07 summary:In this paper, we propose a lensless compressive sensing imagingarchitecture. The architecture consists of two components, an aperture assemblyand a sensor. No lens is used. The aperture assembly consists of a twodimensional array of aperture elements. The transmittance of each apertureelement is independently controllable. The sensor is a single detectionelement, such as a single photo-conductive cell. Each aperture element togetherwith the sensor defines a cone of a bundle of rays, and the cones of theaperture assembly define the pixels of an image. Each pixel value of an imageis the integration of the bundle of rays in a cone. The sensor is used fortaking compressive measurements. Each measurement is the integration of rays inthe cones modulated by the transmittance of the aperture elements. Acompressive sensing matrix is implemented by adjusting the transmittance of theindividual aperture elements according to the values of the sensing matrix. Theproposed architecture is simple and reliable because no lens is used.Furthermore, the sharpness of an image from our device is only limited by theresolution of the aperture assembly, but not affected by blurring due todefocus. The architecture can be used for capturing images of visible lights,and other spectra such as infrared, or millimeter waves. Such devices may beused in surveillance applications for detecting anomalies or extractingfeatures such as speed of moving objects. Multiple sensors may be used with asingle aperture assembly to capture multi-view images simultaneously. Aprototype was built by using a LCD panel and a photoelectric sensor forcapturing images of visible spectrum.
arxiv-3000-221 | Lexical Access for Speech Understanding using Minimum Message Length Encoding | http://arxiv.org/pdf/1302.1572v1.pdf | author:Ian Thomas, Ingrid Zukerman, Jonathan Oliver, David Albrecht, Bhavani Raskutti category:cs.CL published:2013-02-06 summary:The Lexical Access Problem consists of determining the intended sequence ofwords corresponding to an input sequence of phonemes (basic speech sounds) thatcome from a low-level phoneme recognizer. In this paper we present aninformation-theoretic approach based on the Minimum Message Length Criterionfor solving the Lexical Access Problem. We model sentences using phonemerealizations seen in training, and word and part-of-speech information obtainedfrom text corpora. We show results on multiple-speaker, continuous, read speechand discuss a heuristic using equivalence classes of similar sounding wordswhich speeds up the recognition process without significant deterioration inrecognition accuracy.
arxiv-3000-222 | Adaptive low rank and sparse decomposition of video using compressive sensing | http://arxiv.org/pdf/1302.1610v2.pdf | author:Fei Yang, Hong Jiang, Zuowei Shen, Wei Deng, Dimitris Metaxas category:cs.IT cs.CV math.IT published:2013-02-06 summary:We address the problem of reconstructing and analyzing surveillance videosusing compressive sensing. We develop a new method that performs videoreconstruction by low rank and sparse decomposition adaptively. Backgroundsubtraction becomes part of the reconstruction. In our method, a backgroundmodel is used in which the background is learned adaptively as the compressivemeasurements are processed. The adaptive method has low latency, and is morerobust than previous methods. We will present experimental results todemonstrate the advantages of the proposed method.
arxiv-3000-223 | Models and Selection Criteria for Regression and Classification | http://arxiv.org/pdf/1302.1545v1.pdf | author:David Heckerman, Christopher Meek category:cs.LG stat.ML published:2013-02-06 summary:When performing regression or classification, we are interested in theconditional probability distribution for an outcome or class variable Y given aset of explanatoryor input variables X. We consider Bayesian models for thistask. In particular, we examine a special class of models, which we callBayesian regression/classification (BRC) models, that can be factored intoindependent conditional (yx) and input (x) models. These models areconvenient, because the conditional model (the portion of the full model thatwe care about) can be analyzed by itself. We examine the practice oftransforming arbitrary Bayesian models to BRC models, and argue that thispractice is often inappropriate because it ignores prior knowledge that may beimportant for learning. In addition, we examine Bayesian methods for learningmodels from data. We discuss two criteria for Bayesian model selection that areappropriate for repression/classification: one described by Spiegelhalter etal. (1993), and another by Buntine (1993). We contrast these two criteria usingthe prequential framework of Dawid (1984), and give sufficient conditions underwhich the criteria agree.
arxiv-3000-224 | Learning Bayesian Nets that Perform Well | http://arxiv.org/pdf/1302.1542v1.pdf | author:Russell Greiner, Adam J. Grove, Dale Schuurmans category:cs.AI cs.LG published:2013-02-06 summary:A Bayesian net (BN) is more than a succinct way to encode a probabilisticdistribution; it also corresponds to a function used to answer queries. A BNcan therefore be evaluated by the accuracy of the answers it returns. Manyalgorithms for learning BNs, however, attempt to optimize another criterion(usually likelihood, possibly augmented with a regularizing term), which isindependent of the distribution of queries that are posed. This paper takes the"performance criteria" seriously, and considers the challenge of computing theBN whose performance - read "accuracy over the distribution of queries" - isoptimal. We show that many aspects of this learning task are more difficultthan the corresponding subtasks in the standard model.
arxiv-3000-225 | Learning Bayesian Networks from Incomplete Databases | http://arxiv.org/pdf/1302.1565v1.pdf | author:Marco Ramoni, Paola Sebastiani category:cs.AI cs.LG published:2013-02-06 summary:Bayesian approaches to learn the graphical structure of Bayesian BeliefNetworks (BBNs) from databases share the assumption that the database iscomplete, that is, no entry is reported as unknown. Attempts to relax thisassumption involve the use of expensive iterative methods to discriminate amongdifferent structures. This paper introduces a deterministic method to learn thegraphical structure of a BBN from a possibly incomplete database. Experimentalevaluations show a significant robustness of this method and a remarkableindependence of its execution time from the number of missing data.
arxiv-3000-226 | Structure and Parameter Learning for Causal Independence and Causal Interaction Models | http://arxiv.org/pdf/1302.1561v2.pdf | author:Christopher Meek, David Heckerman category:cs.AI cs.LG published:2013-02-06 summary:This paper discusses causal independence models and a generalization of thesemodels called causal interaction models. Causal interaction models are modelsthat have independent mechanisms where a mechanism can have several causes. Inaddition to introducing several particular types of causal interaction models,we show how we can apply the Bayesian approach to learning causal interactionmodels obtaining approximate posterior distributions for the models and obtainMAP and ML estimates for the parameters. We illustrate the approach with asimulation study of learning model posteriors.
arxiv-3000-227 | A Bayesian Approach to Learning Bayesian Networks with Local Structure | http://arxiv.org/pdf/1302.1528v2.pdf | author:David Maxwell Chickering, David Heckerman, Christopher Meek category:cs.LG cs.AI stat.ML published:2013-02-06 summary:Recently several researchers have investigated techniques for using data tolearn Bayesian networks containing compact representations for the conditionalprobability distributions (CPDs) stored at each node. The majority of this workhas concentrated on using decision-tree representations for the CPDs. Inaddition, researchers typically apply non-Bayesian (or asymptotically Bayesian)scoring functions such as MDL to evaluate the goodness-of-fit of networks tothe data. In this paper we investigate a Bayesian approach to learning Bayesiannetworks that contain the more general decision-graph representations of theCPDs. First, we describe how to evaluate the posterior probability that is, theBayesian score of such a network, given a database of observed cases. Second,we describe various search spaces that can be used, in conjunction with ascoring function and a search procedure, to identify one or more high-scoringnetworks. Finally, we present an experimental evaluation of the search spaces,using a greedy algorithm and a Bayesian scoring function.
arxiv-3000-228 | Image Segmentation in Video Sequences: A Probabilistic Approach | http://arxiv.org/pdf/1302.1539v1.pdf | author:Nir Friedman, Stuart Russell category:cs.CV cs.AI published:2013-02-06 summary:"Background subtraction" is an old technique for finding moving objects in avideo sequence for example, cars driving on a freeway. The idea is thatsubtracting the current image from a timeaveraged background image will leaveonly nonstationary objects. It is, however, a crude approximation to the taskof classifying each pixel of the current image; it fails with slow-movingobjects and does not distinguish shadows from moving objects. The basic idea ofthis paper is that we can classify each pixel using a model of how that pixellooks when it is part of different classes. We learn a mixture-of-Gaussiansclassification model for each pixel using an unsupervised technique- anefficient, incremental version of EM. Unlike the standard image-averagingapproach, this automatically updates the mixture component for each classaccording to likelihood of membership; hence slow-moving objects are handledperfectly. Our approach also identifies and eliminates shadows much moreeffectively than other techniques such as thresholding. Application of thismethod as part of the Roadwatch traffic surveillance project is expected toresult in significant improvements in vehicle identification and tracking.
arxiv-3000-229 | Learning Belief Networks in Domains with Recursively Embedded Pseudo Independent Submodels | http://arxiv.org/pdf/1302.1549v1.pdf | author:Jun Hu, Yang Xiang category:cs.AI cs.LG published:2013-02-06 summary:A pseudo independent (PI) model is a probabilistic domain model (PDM) whereproper subsets of a set of collectively dependent variables display marginalindependence. PI models cannot be learned correctly by many algorithms thatrely on a single link search. Earlier work on learning PI models has suggesteda straightforward multi-link search algorithm. However, when a domain containsrecursively embedded PI submodels, it may escape the detection of such analgorithm. In this paper, we propose an improved algorithm that ensures thelearning of all embedded PI submodels whose sizes are upper bounded by apredetermined parameter. We show that this improved learning capability onlyincreases the complexity slightly beyond that of the previous algorithm. Theperformance of the new algorithm is demonstrated through experiment.
arxiv-3000-230 | Bounded regret in stochastic multi-armed bandits | http://arxiv.org/pdf/1302.1611v2.pdf | author:Sébastien Bubeck, Vianney Perchet, Philippe Rigollet category:math.ST cs.LG stat.ML stat.TH 62L05 published:2013-02-06 summary:We study the stochastic multi-armed bandit problem when one knows the value$\mu^{(\star)}$ of an optimal arm, as a well as a positive lower bound on thesmallest positive gap $\Delta$. We propose a new randomized policy that attainsa regret {\em uniformly bounded over time} in this setting. We also proveseveral lower bounds, which show in particular that bounded regret is notpossible if one only knows $\Delta$, and bounded regret of order $1/\Delta$ isnot possible if one only knows $\mu^{(\star)}$
arxiv-3000-231 | Sequential Update of Bayesian Network Structure | http://arxiv.org/pdf/1302.1538v1.pdf | author:Nir Friedman, Moises Goldszmidt category:cs.AI cs.LG published:2013-02-06 summary:There is an obvious need for improving the performance and accuracy of aBayesian network as new data is observed. Because of errors in modelconstruction and changes in the dynamics of the domains, we cannot afford toignore the information in new data. While sequential update of parameters for afixed structure can be accomplished using standard techniques, sequentialupdate of network structure is still an open problem. In this paper, weinvestigate sequential update of Bayesian networks were both parameters andstructure are expected to change. We introduce a new approach that allows forthe flexible manipulation of the tradeoff between the quality of the learnednetworks and the amount of information that is maintained about pastobservations. We formally describe our approach including the necessarymodifications to the scoring functions for learning Bayesian networks, evaluateits effectiveness through an empirical study, and extend it to the case ofmissing data.
arxiv-3000-232 | An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering | http://arxiv.org/pdf/1302.1552v1.pdf | author:Michael Kearns, Yishay Mansour, Andrew Y. Ng category:cs.LG stat.ML published:2013-02-06 summary:Assignment methods are at the heart of many algorithms for unsupervisedlearning and clustering - in particular, the well-known K-means andExpectation-Maximization (EM) algorithms. In this work, we study severaldifferent methods of assignment, including the "hard" assignments used byK-means and the ?soft' assignments used by EM. While it is known that K-meansminimizes the distortion on the data and EM maximizes the likelihood, little isknown about the systematic differences of behavior between the two algorithms.Here we shed light on these differences via an information-theoretic analysis.The cornerstone of our results is a simple decomposition of the expecteddistortion, showing that K-means (and its extension for inferring generalparametric densities from unlabeled sample data) must implicitly manage atrade-off between how similar the data assigned to each cluster are, and howthe data are balanced among the clusters. How well the data are balanced ismeasured by the entropy of the partition defined by the hard assignments. Inaddition to letting us predict and verify systematic differences betweenK-means and EM on specific examples, the decomposition allows us to give arather general argument showing that K ?means will consistently find densitieswith less "overlap" than EM. We also study a third natural assignment methodthat we call posterior assignment, that is close in spirit to the softassignments of EM, but leads to a surprisingly different algorithm.
arxiv-3000-233 | Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering | http://arxiv.org/pdf/1302.1612v1.pdf | author:Hanane Froud, Abdelmonaime Lachkar, Said Alaoui Ouatik category:cs.IR cs.CL published:2013-02-06 summary:Arabic Documents Clustering is an important task for obtaining good resultswith the traditional Information Retrieval (IR) systems especially with therapid growth of the number of online documents present in Arabic language.Documents clustering aim to automatically group similar documents in onecluster using different similarity/distance measures. This task is oftenaffected by the documents length, useful information on the documents is oftenaccompanied by a large amount of noise, and therefore it is necessary toeliminate this noise while keeping useful information to boost the performanceof Documents clustering. In this paper, we propose to evaluate the impact oftext summarization using the Latent Semantic Analysis Model on Arabic DocumentsClustering in order to solve problems cited above, using fivesimilarity/distance measures: Euclidean Distance, Cosine Similarity, JaccardCoefficient, Pearson Correlation Coefficient and Averaged Kullback-LeiblerDivergence, for two times: without and with stemming. Our experimental resultsindicate that our proposed approach effectively solves the problems of noisyinformation and documents length, and thus significantly improve the clusteringperformance.
arxiv-3000-234 | Sémantique des déterminants dans un cadre richement typé | http://arxiv.org/pdf/1302.1422v2.pdf | author:Christian Retoré category:cs.CL published:2013-02-06 summary:The variation of word meaning according to the context leads us to enrich thetype system of our syntactical and semantic analyser of French based oncategorial grammars and Montague semantics (or lambda-DRT). The main advantageof a deep semantic analyse is too represent meaning by logical formulae thatcan be easily used e.g. for inferences. Determiners and quantifiers play afundamental role in the construction of those formulae. But in our rich typesystem the usual semantic terms do not work. We propose a solution ins- piredby the tau and epsilon operators of Hilbert, kinds of generic elements andchoice functions. This approach unifies the treatment of the different determi-ners and quantifiers as well as the dynamic binding of pronouns. Above all,this fully computational view fits in well within the wide coverage parserGrail, both from a theoretical and a practical viewpoint.
arxiv-3000-235 | A Polynomial Time Algorithm for Lossy Population Recovery | http://arxiv.org/pdf/1302.1515v2.pdf | author:Ankur Moitra, Michael Saks category:cs.DS cs.LG published:2013-02-06 summary:We give a polynomial time algorithm for the lossy population recoveryproblem. In this problem, the goal is to approximately learn an unknowndistribution on binary strings of length $n$ from lossy samples: for someparameter $\mu$ each coordinate of the sample is preserved with probability$\mu$ and otherwise is replaced by a `?'. The running time and number ofsamples needed for our algorithm is polynomial in $n$ and $1/\varepsilon$ foreach fixed $\mu>0$. This improves on algorithm of Wigderson and Yehudayoff thatruns in quasi-polynomial time for any $\mu > 0$ and the polynomial timealgorithm of Dvir et al which was shown to work for $\mu \gtrapprox 0.30$ byBatman et al. In fact, our algorithm also works in the more general frameworkof Batman et al. in which there is no a priori bound on the size of the supportof the distribution. The algorithm we analyze is implicit in previous work; ourmain contribution is to analyze the algorithm by showing (via linearprogramming duality and connections to complex analysis) that a certain matrixassociated with the problem has a robust local inverse even though itscondition number is exponentially small. A corollary of our result is the firstpolynomial time algorithm for learning DNFs in the restriction access model ofDvir et al.
arxiv-3000-236 | Image Interpolation Using Kriging Technique for Spatial Data | http://arxiv.org/pdf/1302.1294v1.pdf | author:Firas Ajil Jassim, Fawzi Hasan Altaany category:cs.CV published:2013-02-06 summary:Image interpolation has been used spaciously by customary interpolationtechniques. Recently, Kriging technique has been widely implemented insimulation area and geostatistics for prediction. In this article, Krigingtechnique was used instead of the classical interpolation methods to predictthe unknown points in the digital image array. The efficiency of the proposedtechnique was proven using the PSNR and compared with the traditionalinterpolation techniques. The results showed that Kriging technique is almostaccurate as cubic interpolation and in some images Kriging has higher accuracy.A miscellaneous test images have been used to consolidate the proposedtechnique.
arxiv-3000-237 | Hybrid Image Segmentation using Discerner Cluster in FCM and Histogram Thresholding | http://arxiv.org/pdf/1302.1296v1.pdf | author:Firas Ajil Jassim category:cs.CV published:2013-02-06 summary:Image thresholding has played an important role in image segmentation. Thispaper presents a hybrid approach for image segmentation based on thethresholding by fuzzy c-means (THFCM) algorithm for image segmentation. Thegoal of the proposed approach is to find a discerner cluster able to find anautomatic threshold. The algorithm is formulated by applying the standard FCMclustering algorithm to the frequencies (y-values) on the smoothed histogram.Hence, the frequencies of an image can be used instead of the conventionalwhole data of image. The cluster that has the highest peak which represents themaximum frequency in the image histogram will play as an excellent role indetermining a discerner cluster to the grey level image. Then, the pixelsbelong to the discerner cluster represent an object in the gray level histogramwhile the other clusters represent a background. Experimental results withstandard test images have been obtained through the proposed approach (THFCM).
arxiv-3000-238 | Kriging Interpolation Filter to Reduce High Density Salt and Pepper Noise | http://arxiv.org/pdf/1302.1300v1.pdf | author:Firas Ajil Jassim category:cs.CV published:2013-02-06 summary:Image denoising is a critical issue in the field of digital image processing.This paper proposes a novel Salt & Pepper noise suppression by developing aKriging Interpolation Filter (KIF) for image denoising. Gray-level imagesdegraded with Salt & Pepper noise have been considered. A sequential search fornoise detection was made using kXk window size to determine non-noisy pixelsonly. The non-noisy pixels are passed into Kriging interpolation method topredict their absent neighbor pixels that were noisy pixels at the first phase.The utilization of Kriging interpolation filter proves that it is veryimpressive to suppress high noise density. It has been found that KrigingInterpolation filter achieves noise reduction without loss of edges anddetailed information. Comparisons with existing algorithms are done usingquality metrics like PSNR and MSE to assess the proposed filter.
arxiv-3000-239 | Cloud Computing framework for Computer Vision Research:An Introduction | http://arxiv.org/pdf/1302.1326v1.pdf | author:Yu Zhou category:cs.CV cs.DC published:2013-02-06 summary:Cloud computing offers the potential to help scientists to process massivenumber of computing resources often required in machine learning applicationsuch as computer vision problems. This proposal would like to show that whichbenefits can be obtained from cloud in order to help medical image analysisusers (including scientists, clinicians, and research institutes). As securityand privacy of algorithms are important for most of algorithms inventors, thesealgorithms can be hidden in a cloud to allow the users to use the algorithms asa package without any access to see/change their inside. In another word, inthe user part, users send their images to the cloud and configure the algorithmvia an interface. In the cloud part, the algorithms are applied to this imageand the results are returned back to the user. My proposal has two parts: (1)investigate the potential of cloud computing for computer vision problems and(2) study the components of a proposed cloud-based framework for medical imageanalysis application and develop them (depending on the length of theinternship). The investigation part will involve a study on several aspects ofthe problem including security, usability (for medical end users of theservice), appropriate programming abstractions for vision problems, scalabilityand resource requirements. In the second part of this proposal I am going tothoroughly study of the proposed framework components and their relations anddevelop them. The proposed cloud-based framework includes an integratedenvironment to enable scientists and clinicians to access to the previous andcurrent medical image analysis algorithms using a handful user interfacewithout any access to the algorithm codes and procedures.
arxiv-3000-240 | Towards the Rapid Development of a Natural Language Understanding Module | http://arxiv.org/pdf/1302.1380v1.pdf | author:Catarina Moreira, Ana Cristina Mendes, Luísa Coheur, Bruno Martins category:cs.CL published:2013-02-06 summary:When developing a conversational agent, there is often an urgent need to havea prototype available in order to test the application with real users. AWizard of Oz is a possibility, but sometimes the agent should be simplydeployed in the environment where it will be used. Here, the agent should beable to capture as many interactions as possible and to understand how peoplereact to failure. In this paper, we focus on the rapid development of a naturallanguage understanding module by non experts. Our approach follows the learningparadigm and sees the process of understanding natural language as aclassification problem. We test our module with a conversational agent thatanswers questions in the art domain. Moreover, we show how our approach can beused by a natural language interface to a cinema database.
arxiv-3000-241 | Update Rules for Parameter Estimation in Bayesian Networks | http://arxiv.org/pdf/1302.1519v1.pdf | author:Eric Bauer, Daphne Koller, Yoram Singer category:cs.LG stat.ML published:2013-02-06 summary:This paper re-examines the problem of parameter estimation in Bayesiannetworks with missing values and hidden variables from the perspective ofrecent work in on-line learning [Kivinen & Warmuth, 1994]. We provide a unifiedframework for parameter estimation that encompasses both on-line learning,where the model is continuously adapted to new data cases as they arrive, andthe more traditional batch learning, where a pre-accumulated set of samples isused in a one-time model selection process. In the batch case, our frameworkencompasses both the gradient projection algorithm and the EM algorithm forBayesian networks. The framework also leads to new on-line and batch parameterupdate schemes, including a parameterized version of EM. We provide bothempirical and theoretical results indicating that parameterized EM allowsfaster convergence to the maximum likelihood parameters than does standard EM.
arxiv-3000-242 | Exploring Parallelism in Learning Belief Networks | http://arxiv.org/pdf/1302.1529v1.pdf | author:TongSheng Chu, Yang Xiang category:cs.AI cs.LG published:2013-02-06 summary:It has been shown that a class of probabilistic domain models cannot belearned correctly by several existing algorithms which employ a single-linklook ahead search. When a multi-link look ahead search is used, thecomputational complexity of the learning algorithm increases. We study how touse parallelism to tackle the increased complexity in learning such models andto speed up learning in large domains. An algorithm is proposed to decomposethe learning task for parallel processing. A further task decomposition is usedto balance load among processors and to increase the speed-up and efficiency.For learning from very large datasets, we present a regrouping of the availableprocessors such that slow data access through file can be replaced by fastmemory access. Our implementation in a parallel computer demonstrates theeffectiveness of the algorithm.
arxiv-3000-243 | When are the most informative components for inference also the principal components? | http://arxiv.org/pdf/1302.1232v1.pdf | author:Raj Rao Nadakuditi category:math.ST cs.DS cs.IT cs.LG math.IT math.PR stat.TH published:2013-02-05 summary:Which components of the singular value decomposition of a signal-plus-noisedata matrix are most informative for the inferential task of detecting orestimating an embedded low-rank signal matrix? Principal component analysisascribes greater importance to the components that capture the greatestvariation, i.e., the singular vectors associated with the largest singularvalues. This choice is often justified by invoking the Eckart-Young theoremeven though that work addresses the problem of how to best represent asignal-plus-noise matrix using a low-rank approximation and not how tobest_infer_ the underlying low-rank signal component. Here we take a first-principles approach in which we start with asignal-plus-noise data matrix and show how the spectrum of the noise-onlycomponent governs whether the principal or the middle components of thesingular value decomposition of the data matrix will be the informativecomponents for inference. Simply put, if the noise spectrum is supported on aconnected interval, in a sense we make precise, then the use of the principalcomponents is justified. When the noise spectrum is supported on multipleintervals, then the middle components might be more informative than theprincipal components. The end result is a proper justification of the use of principal componentsin the setting where the noise matrix is i.i.d. Gaussian and the identificationof scenarios, generically involving heterogeneous noise models such as mixturesof Gaussians, where the middle components might be more informative than theprincipal components so that they may be exploited to extract additionalprocessing gain. Our results show how the blind use of principal components canlead to suboptimal or even faulty inference because of phase transitions thatseparate a regime where the principal components are informative from a regimewhere they are uninformative.
arxiv-3000-244 | Evolvability Is Inevitable: Increasing Evolvability Without the Pressure to Adapt | http://arxiv.org/pdf/1302.1143v1.pdf | author:Joel Lehman, Kenneth O. Stanley category:cs.NE q-bio.PE published:2013-02-05 summary:Why evolvability appears to have increased over evolutionary time is animportant unresolved biological question. Unlike most candidate explanations,this paper proposes that increasing evolvability can result without anypressure to adapt. The insight is that if evolvability is heritable, then anunbiased drifting process across genotypes can still create a distribution ofphenotypes biased towards evolvability, because evolvable organisms diffusemore quickly through the space of possible phenotypes. Furthermore, becausephenotypic divergence often correlates with founding niches, niche founders mayon average be more evolvable, which through population growth provides agenotypic bias towards evolvability. Interestingly, the combination of thesetwo mechanisms can lead to increasing evolvability without any pressure toout-compete other organisms, as demonstrated through experiments with a seriesof simulated models. Thus rather than from pressure to adapt, evolvability mayinevitably result from any drift through genotypic space combined withevolution's passive tendency to accumulate niches.
arxiv-3000-245 | Large Scale Distributed Acoustic Modeling With Back-off N-grams | http://arxiv.org/pdf/1302.1123v1.pdf | author:Ciprian Chelba, Peng Xu, Fernando Pereira, Thomas Richardson category:cs.CL 68T10 I.2.7 published:2013-02-05 summary:The paper revives an older approach to acoustic modeling that borrows fromn-gram language modeling in an attempt to scale up both the amount of trainingdata and model size (as measured by the number of parameters in the model), toapproximately 100 times larger than current sizes used in automatic speechrecognition. In such a data-rich setting, we can expand the phonetic contextsignificantly beyond triphones, as well as increase the number of Gaussianmixture components for the context-dependent states that allow it. We haveexperimented with contexts that span seven or more context-independent phones,and up to 620 mixture components per state. Dealing with unseen phoneticcontexts is accomplished using the familiar back-off technique used in languagemodeling due to implementation simplicity. The back-off acoustic model isestimated, stored and served using MapReduce distributed computinginfrastructure. Speech recognition experiments are carried out in an N-best list rescoringframework for Google Voice Search. Training big models on large amounts of dataproves to be an effective way to increase the accuracy of a state-of-the-artautomatic speech recognition system. We use 87,000 hours of training data(speech along with transcription) obtained by filtering utterances in VoiceSearch logs on automatic speech recognition confidence. Models ranging in sizebetween 20--40 million Gaussians are estimated using maximum likelihoodtraining. They achieve relative reductions in word-error-rate of 11% and 6%when combined with first-pass models trained using maximum likelihood, andboosted maximum mutual information, respectively. Increasing the context sizebeyond five phones (quinphones) does not help.
arxiv-3000-246 | Image Denoising Using Interquartile Range Filter with Local Averaging | http://arxiv.org/pdf/1302.1007v1.pdf | author:Firas Ajil Jassim category:cs.CV published:2013-02-05 summary:Image denoising is one of the fundamental problems in image processing. Inthis paper, a novel approach to suppress noise from the image is conducted byapplying the interquartile range (IQR) which is one of the statistical methodsused to detect outlier effect from a dataset. A window of size kXk wasimplemented to support IQR filter. Each pixel outside the IQR range of the kXkwindow is treated as noisy pixel. The estimation of the noisy pixels wasobtained by local averaging. The essential advantage of applying IQR filter isto preserve edge sharpness better of the original image. A variety of testimages have been used to support the proposed filter and PSNR was calculatedand compared with median filter. The experimental results on standard testimages demonstrate this filter is simpler and better performing than medianfilter.
arxiv-3000-247 | A Comparison of Relaxations of Multiset Cannonical Correlation Analysis and Applications | http://arxiv.org/pdf/1302.0974v1.pdf | author:Jan Rupnik, Primoz Skraba, John Shawe-Taylor, Sabrina Guettes category:cs.LG published:2013-02-05 summary:Canonical correlation analysis is a statistical technique that is used tofind relations between two sets of variables. An important extension in patternanalysis is to consider more than two sets of variables. This problem can beexpressed as a quadratically constrained quadratic program (QCQP), commonlyreferred to Multi-set Canonical Correlation Analysis (MCCA). This is anon-convex problem and so greedy algorithms converge to local optima withoutany guarantees on global optimality. In this paper, we show that despite beinghighly structured, finding the optimal solution is NP-Hard. This motivates ourrelaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, canbe solved reasonably efficiently and comes with both absolute andoutput-sensitive approximation quality. In addition to theoretical guarantees,we do an extensive comparison of the QCQP method and the SDP relaxation on avariety of synthetic and real world data. Finally, we present two usefulextensions: we incorporate kernel methods and computing multiple sets ofcanonical vectors.
arxiv-3000-248 | The price of bandit information in multiclass online classification | http://arxiv.org/pdf/1302.1043v2.pdf | author:Amit Daniely, Tom Helbertal category:cs.LG published:2013-02-05 summary:We consider two scenarios of multiclass online learning of a hypothesis class$H\subseteq Y^X$. In the {\em full information} scenario, the learner isexposed to instances together with their labels. In the {\em bandit} scenario,the true label is not exposed, but rather an indication whether the learner'sprediction is correct or not. We show that the ratio between the error rates inthe two scenarios is at most $8\cdotY\cdot \log(Y)$ in the realizable case,and $\tilde{O}(\sqrt{Y})$ in the agnostic case. The results are tight up to alogarithmic factor and essentially answer an open question from (Daniely et.al. - Multiclass learnability and the erm principle). We apply these results to the class of $\gamma$-margin multiclass linearclassifiers in $\reals^d$. We show that the bandit error rate of this class is$\tilde{\Theta}(\frac{Y}{\gamma^2})$ in the realizable case and$\tilde{\Theta}(\frac{1}{\gamma}\sqrt{YT})$ in the agnostic case. Thisresolves an open question from (Kakade et. al. - Efficient bandit algorithmsfor online multiclass prediction).
arxiv-3000-249 | RandomBoost: Simplified Multi-class Boosting through Randomization | http://arxiv.org/pdf/1302.0963v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Qinfeng Shi, Anton van den Hengel category:cs.LG published:2013-02-05 summary:We propose a novel boosting approach to multi-class classification problems,in which multiple classes are distinguished by a set of random projectionmatrices in essence. The approach uses random projections to alleviate theproliferation of binary classifiers typically required to perform multi-classclassification. The result is a multi-class classifier with a singlevector-valued parameter, irrespective of the number of classes involved. Twovariants of this approach are proposed. The first method randomly projects theoriginal data into new spaces, while the second method randomly projects theoutputs of learned weak classifiers. These methods are not only conceptuallysimple but also effective and easy to implement. A series of experiments onsynthetic, machine learning and visual recognition data sets demonstrate thatour proposed methods compare favorably to existing multi-class boostingalgorithms in terms of both the convergence rate and classification accuracy.
arxiv-3000-250 | Improved Accuracy of PSO and DE using Normalization: an Application to Stock Price Prediction | http://arxiv.org/pdf/1302.0962v1.pdf | author:Savinderjit Kaur, Veenu Mangat category:cs.NE cs.LG published:2013-02-05 summary:Data Mining is being actively applied to stock market since 1980s. It hasbeen used to predict stock prices, stock indexes, for portfolio management,trend detection and for developing recommender systems. The various algorithmswhich have been used for the same include ANN, SVM, ARIMA, GARCH etc. Differenthybrid models have been developed by combining these algorithms with otheralgorithms like roughest, fuzzy logic, GA, PSO, DE, ACO etc. to improve theefficiency. This paper proposes DE-SVM model (Differential EvolutionSupportvector Machine) for stock price prediction. DE has been used to select bestfree parameters combination for SVM to improve results. The paper also comparesthe results of prediction with the outputs of SVM alone and PSO-SVM model(Particle Swarm Optimization). The effect of normalization of data on theaccuracy of prediction has also been studied.
arxiv-3000-251 | A Non-Binary Associative Memory with Exponential Pattern Retrieval Capacity and Iterative Learning: Extended Results | http://arxiv.org/pdf/1302.1156v2.pdf | author:Amir Hesam Salavati, K. Raj Kumar, Amin Shokrollahi category:cs.NE published:2013-02-05 summary:We consider the problem of neural association for a network of non-binaryneurons. Here, the task is to first memorize a set of patterns using a networkof neurons whose states assume values from a finite number of integer levels.Later, the same network should be able to recall previously memorized patternsfrom their noisy versions. Prior work in this area consider storing a finitenumber of purely random patterns, and have shown that the pattern retrievalcapacities (maximum number of patterns that can be memorized) scale onlylinearly with the number of neurons in the network. In our formulation of the problem, we concentrate on exploiting redundancyand internal structure of the patterns in order to improve the patternretrieval capacity. Our first result shows that if the given patterns have asuitable linear-algebraic structure, i.e. comprise a sub-space of the set ofall possible patterns, then the pattern retrieval capacity is in factexponential in terms of the number of neurons. The second result extends theprevious finding to cases where the patterns have weak minor components, i.e.the smallest eigenvalues of the correlation matrix tend toward zero. We willuse these minor components (or the basis vectors of the pattern null space) toboth increase the pattern retrieval capacity and error correction capabilities. An iterative algorithm is proposed for the learning phase, and two simpleneural update algorithms are presented for the recall phase. Using analyticalresults and simulations, we show that the proposed methods can tolerate a fairamount of errors in the input while being able to memorize an exponentiallylarge number of patterns.
arxiv-3000-252 | Beyond Markov Chains, Towards Adaptive Memristor Network-based Music Generation | http://arxiv.org/pdf/1302.0785v1.pdf | author:Ella Gale, Oliver Matthews, Ben de Lacy Costello, Andrew Adamatzky category:cs.ET cs.AI cs.NE cs.SD 68Txx published:2013-02-04 summary:We undertook a study of the use of a memristor network for music generation,making use of the memristor's memory to go beyond the Markov hypothesis. Seedtransition matrices are created and populated using memristor equations, andwhich are shown to generate musical melodies and change in style over time as aresult of feedback into the transition matrix. The spiking properties of simplememristor networks are demonstrated and discussed with reference toapplications of music making. The limitations of simulating composing memristornetworks in von Neumann hardware is discussed and a hardware solution based onphysical memristor properties is presented.
arxiv-3000-253 | Comparison of Ant-Inspired Gatherer Allocation Approaches using Memristor-Based Environmental Models | http://arxiv.org/pdf/1302.0797v1.pdf | author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.NE published:2013-02-04 summary:Memristors are used to compare three gathering techniques in analready-mapped environment where resource locations are known. The All Sitemodel, which apportions gatherers based on the modeled memristance of thatpath, proves to be good at increasing overall efficiency and decreasing time tofully deplete an environment, however it only works well when the resources areof similar quality. The Leaf Cutter method, based on Leaf Cutter Ant behaviour,assigns all gatherers first to the best resource, and once depleted, uses theAll Site model to spread them out amongst the rest. The Leaf Cutter model isbetter at increasing resource influx in the short-term and vastly out-performsthe All Site model in a more varied environments. It is demonstrated thatmemristor based abstractions of gatherer models provide potential methods forboth the comparison and implementation of agent controls.
arxiv-3000-254 | Centrality-constrained graph embedding | http://arxiv.org/pdf/1302.0870v1.pdf | author:Brian Baingana, Georgios B. Giannakis category:stat.ML cs.CV math.OC published:2013-02-04 summary:Visual rendering of graphs is a key task in the mapping of complex networkdata. Although most graph drawing algorithms emphasize aesthetic appeal,certain applications such as travel-time maps place more importance onvisualization of structural network properties. The present paper advocates agraph embedding approach with centrality considerations to comply with nodehierarchy. The problem is formulated as one of constrained multi-dimensionalscaling (MDS), and it is solved via block coordinate descent iterations withsuccessive approximations and guaranteed convergence to a KKT point. Inaddition, a regularization term enforcing graph smoothness is incorporated withthe goal of reducing edge crossings. Experimental results demonstrate that thealgorithm converges, and can be used to efficiently embed large graphs on theorder of thousands of nodes.
arxiv-3000-255 | Exact Sparse Recovery with L0 Projections | http://arxiv.org/pdf/1302.0895v1.pdf | author:Ping Li, Cun-Hui Zhang category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2013-02-04 summary:Many applications concern sparse signals, for example, detecting anomaliesfrom the differences between consecutive images taken by surveillance cameras.This paper focuses on the problem of recovering a K-sparse signal x in Ndimensions. In the mainstream framework of compressed sensing (CS), the vectorx is recovered from M non-adaptive linear measurements y = xS, where S (of sizeN x M) is typically a Gaussian (or Gaussian-like) design matrix, through someoptimization procedure such as linear programming (LP). In our proposed method, the design matrix S is generated from an$\alpha$-stable distribution with $\alpha\approx 0$. Our decoding algorithmmainly requires one linear scan of the coordinates, followed by a fewiterations on a small number of coordinates which are "undetermined" in theprevious iteration. Comparisons with two strong baselines, linear programming(LP) and orthogonal matching pursuit (OMP), demonstrate that our algorithm canbe significantly faster in decoding speed and more accurate in recoveryquality, for the task of exact spare recovery. Our procedure is robust againstmeasurement noise. Even when there are no sufficient measurements, ouralgorithm can still reliably recover a significant portion of the nonzerocoordinates. To provide the intuition for understanding our method, we also analyze theprocedure by assuming an idealistic setting. Interestingly, when K=2, the"idealized" algorithm achieves exact recovery with merely 3 measurements,regardless of N. For general K, the required sample size of the "idealized"algorithm is about 5K.
arxiv-3000-256 | SMML estimators for exponential families with continuous sufficient statistics | http://arxiv.org/pdf/1302.0581v2.pdf | author:James G. Dowty category:cs.IT math.IT math.ST stat.ML stat.TH published:2013-02-04 summary:The minimum message length principle is an information theoretic criterionthat links data compression with statistical inference. This paper studies thestrict minimum message length (SMML) estimator for $d$-dimensional exponentialfamilies with continuous sufficient statistics, for all $d \ge 1$. Thepartition of an SMML estimator is shown to consist of convex polytopes (i.e.convex polygons when $d=2$) which can be described explicitly in terms of theassertions and coding probabilities. While this result is known, we give a newproof based on the calculus of variations, and this approach gives someinteresting new inequalities for SMML estimators. We also use this result toconstruct an SMML estimator for a $2$-dimensional normal random variable withknown variance and a normal prior on its mean.
arxiv-3000-257 | Coded aperture compressive temporal imaging | http://arxiv.org/pdf/1302.2575v1.pdf | author:Patrick Llull, Xuejun Liao, Xin Yuan, Jianbo Yang, David Kittle, Lawrence Carin, Guillermo Sapiro, David J. Brady category:cs.CV cs.IT math.IT published:2013-02-04 summary:We use mechanical translation of a coded aperture for code division multipleaccess compression of video. We present experimental results for reconstructionat 148 frames per coded snapshot.
arxiv-3000-258 | Multi-scale Visual Attention & Saliency Modelling with Decision Theory | http://arxiv.org/pdf/1302.0689v1.pdf | author:Anh Cat Le Ngo, Li-Minn Ang, Guoping Qiu, Kah-Phooi Seng category:cs.CV published:2013-02-04 summary:Bottom-up saliency, an early human visual processing, behaves like binaryclassification of interest and null hypothesis. Its discriminant power, mutualinformation of image features and class distribution, is closely related tosaliency value by the well-known centre-surround theory. As classificationaccuracy very much depends on window sizes, the discriminant saliency (power)varies according to sampling scales. Discriminating power estimation inmulti-scales framework needs integrating with wavelet transformation and thenestimating statistical discrepancy of two consecutive scales (centre-surroundwindows) by Hidden Markov Tree (HMT) model. Finally, multi-scale discriminantsaliency (MDIS) maps are combined by the maximum information rule to synthesizea final saliency map. All MDIS maps are evaluated with standard quantitativetools (NSS,LCC,AUC) on N.Bruce's database with ground truth data aseye-tracking locations ; as well assessed qualitatively by visual examinationof individual cases. For evaluating MDIS against well-known AIM saliencymethod, simulations are needed and described in details with severalinteresting conclusions, drawn for further research directions.
arxiv-3000-259 | Multi-Robot Informative Path Planning for Active Sensing of Environmental Phenomena: A Tale of Two Algorithms | http://arxiv.org/pdf/1302.0723v2.pdf | author:Nannan Cao, Kian Hsiang Low, John M. Dolan category:cs.LG cs.AI cs.MA cs.RO published:2013-02-04 summary:A key problem of robotic environmental sensing and monitoring is that ofactive sensing: How can a team of robots plan the most informative observationpaths to minimize the uncertainty in modeling and predicting an environmentalphenomenon? This paper presents two principled approaches to efficientinformation-theoretic path planning based on entropy and mutual informationcriteria for in situ active sensing of an important broad class ofwidely-occurring environmental phenomena called anisotropic fields. Ourproposed algorithms are novel in addressing a trade-off between active sensingperformance and time efficiency. An important practical consequence is that ouralgorithms can exploit the spatial correlation structure of Gaussianprocess-based anisotropic fields to improve time efficiency while preservingnear-optimal active sensing performance. We analyze the time complexity of ouralgorithms and prove analytically that they scale better than state-of-the-artalgorithms with increasing planning horizon length. We provide theoreticalguarantees on the active sensing performance of our algorithms for a class ofexploration tasks called transect sampling, which, in particular, can beimproved with longer planning time and/or lower spatial correlation along thetransect. Empirical evaluation on real-world anisotropic field data shows thatour algorithms can perform better or at least as well as the state-of-the-artalgorithms while often incurring a few orders of magnitude less computationaltime, even when the field conditions are less favorable.
arxiv-3000-260 | A game-theoretic framework for classifier ensembles using weighted majority voting with local accuracy estimates | http://arxiv.org/pdf/1302.0540v1.pdf | author:Harris V. Georgiou, Michael E. Mavroforakis category:cs.LG published:2013-02-03 summary:In this paper, a novel approach for the optimal combination of binaryclassifiers is proposed. The classifier combination problem is approached froma Game Theory perspective. The proposed framework of adapted weighted majorityrules (WMR) is tested against common rank-based, Bayesian and simple majoritymodels, as well as two soft-output averaging rules. Experiments with ensemblesof Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) andweighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate thatthis new adaptive WMR model, employing local accuracy estimators and theanalytically computed optimal weights outperform all the other simplecombination rules.
arxiv-3000-261 | Sparse Camera Network for Visual Surveillance -- A Comprehensive Survey | http://arxiv.org/pdf/1302.0446v1.pdf | author:Mingli Song, Dachent Tao, Stephen J. Maybank category:cs.CV published:2013-02-03 summary:Technological advances in sensor manufacture, communication, and computingare stimulating the development of new applications that are transformingtraditional vision systems into pervasive intelligent camera networks. Theanalysis of visual cues in multi-camera networks enables a wide range ofapplications, from smart home and office automation to large area surveillanceand traffic surveillance. While dense camera networks - in which most camerashave large overlapping fields of view - are well studied, we are mainlyconcerned with sparse camera networks. A sparse camera network undertakes largearea surveillance using as few cameras as possible, and most cameras havenon-overlapping fields of view with one another. The task is challenging due tothe lack of knowledge about the topological structure of the network,variations in the appearance and motion of specific tracking targets indifferent views, and the difficulties of understanding composite events in thenetwork. In this review paper, we present a comprehensive survey of recentresearch results to address the problems of intra-camera tracking, topologicalstructure learning, target appearance modeling, and global activityunderstanding in sparse camera networks. A number of current open researchissues are discussed.
arxiv-3000-262 | Local Structure Matching Driven by Joint-Saliency-Structure Adaptive Kernel Regression | http://arxiv.org/pdf/1302.0494v4.pdf | author:Binjie Qin, Zhuangming Shen, Zien Zhou, Jiawei Zhou, Jiuai Sun, Hui Zhang, Mingxing Hu, Yisong Lv category:cs.CV published:2013-02-03 summary:For nonrigid image registration, matching the particular structures (or theoutliers) that have missing correspondence and/or local large deformations, canbe more difficult than matching the common structures with small deformationsin the two images. Most existing works depend heavily on the outliersegmentation to remove the outlier effect in the registration. Moreover, theseworks do not handle simultaneously the missing correspondences and local largedeformations. In this paper, we defined the nonrigid image registration as alocal adaptive kernel regression which locally reconstruct the moving image'sdense deformation vectors from the sparse deformation vectors in themulti-resolution block matching. The kernel function of the kernel regressionadapts its shape and orientation to the reference image's structure to gathermore deformation vector samples of the same structure for the iterativeregression computation, whereby the moving image's local deformations could becompliant with the reference image's local structures. To estimate the localdeformations around the outliers, we use joint saliency map that highlights thecorresponding saliency structures (called Joint Saliency Structures, JSSs) inthe two images to guide the dense deformation reconstruction by emphasizingthose JSSs' sparse deformation vectors in the kernel regression. Theexperimental results demonstrate that by using local JSS adaptive kernelregression, the proposed method achieves almost the best performance inalignment of all challenging image pairs with outlier structures compared withother five state-of-the-art nonrigid registration algorithms.
arxiv-3000-263 | Correcting Camera Shake by Incremental Sparse Approximation | http://arxiv.org/pdf/1302.0439v2.pdf | author:Paul Shearer, Anna C. Gilbert, Alfred O. Hero III category:cs.CV cs.GR published:2013-02-03 summary:The problem of deblurring an image when the blur kernel is unknown remainschallenging after decades of work. Recently there has been rapid progress oncorrecting irregular blur patterns caused by camera shake, but there is stillmuch room for improvement. We propose a new blind deconvolution method usingincremental sparse edge approximation to recover images blurred by camerashake. We estimate the blur kernel first from only the strongest edges in theimage, then gradually refine this estimate by allowing for weaker and weakeredges. Our method competes with the benchmark deblurring performance of thestate-of-the-art while being significantly faster and easier to generalize.
arxiv-3000-264 | Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams for Lambek Calculus | http://arxiv.org/pdf/1302.0393v1.pdf | author:Bob Coecke, Edward Grefenstette, Mehrnoosh Sadrzadeh category:math.LO cs.CL math.CT published:2013-02-02 summary:The Distributional Compositional Categorical (DisCoCat) model is amathematical framework that provides compositional semantics for meanings ofnatural language sentences. It consists of a computational procedure forconstructing meanings of sentences, given their grammatical structure in termsof compositional type-logic, and given the empirically derived meanings oftheir words. For the particular case that the meaning of words is modelledwithin a distributional vector space model, its experimental predictions,derived from real large scale data, have outperformed other empiricallyvalidated methods that could build vectors for a full sentence. This successcan be attributed to a conceptually motivated mathematical underpinning, byintegrating qualitative compositional type-logic and quantitative modelling ofmeaning within a category-theoretic mathematical framework. The type-logic used in the DisCoCat model is Lambek's pregroup grammar.Pregroup types form a posetal compact closed category, which can be passed, ina functorial manner, on to the compact closed structure of vector spaces,linear maps and tensor product. The diagrammatic versions of the equationalreasoning in compact closed categories can be interpreted as the flow of wordmeanings within sentences. Pregroups simplify Lambek's previous type-logic, theLambek calculus, which has been extensively used to formalise and reason aboutvarious linguistic phenomena. The apparent reliance of the DisCoCat onpregroups has been seen as a shortcoming. This paper addresses this concern, bypointing out that one may as well realise a functorial passage from theoriginal type-logic of Lambek, a monoidal bi-closed category, to vector spaces,or to any other model of meaning organised within a monoidal bi-closedcategory. The corresponding string diagram calculus, due to Baez and Stay, nowdepicts the flow of word meanings.
arxiv-3000-265 | Fast Damage Recovery in Robotics with the T-Resilience Algorithm | http://arxiv.org/pdf/1302.0386v1.pdf | author:Sylvain Koos, Antoine Cully, Jean-Baptiste Mouret category:cs.RO cs.AI cs.LG published:2013-02-02 summary:Damage recovery is critical for autonomous robots that need to operate for along time without assistance. Most current methods are complex and costlybecause they require anticipating each potential damage in order to have acontingency plan ready. As an alternative, we introduce the T-resiliencealgorithm, a new algorithm that allows robots to quickly and autonomouslydiscover compensatory behaviors in unanticipated situations. This algorithmequips the robot with a self-model and discovers new behaviors by learning toavoid those that perform differently in the self-model and in reality. Ouralgorithm thus does not identify the damaged parts but it implicitly searchesfor efficient behaviors that do not use them. We evaluate the T-Resiliencealgorithm on a hexapod robot that needs to adapt to leg removal, broken legsand motor failures; we compare it to stochastic local search, policy gradientand the self-modeling algorithm proposed by Bongard et al. The behavior of therobot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Usingonly 25 tests on the robot and an overall running time of 20 minutes,T-Resilience consistently leads to substantially better results than the otherapproaches.
arxiv-3000-266 | Generalization Guarantees for a Binary Classification Framework for Two-Stage Multiple Kernel Learning | http://arxiv.org/pdf/1302.0406v1.pdf | author:Purushottam Kar category:cs.LG stat.ML published:2013-02-02 summary:We present generalization bounds for the TS-MKL framework for two stagemultiple kernel learning. We also present bounds for sparse kernel learningformulations within the TS-MKL framework.
arxiv-3000-267 | Parallel D2-Clustering: Large-Scale Clustering of Discrete Distributions | http://arxiv.org/pdf/1302.0435v2.pdf | author:Yu Zhang, James Z. Wang, Jia Li category:cs.LG cs.CV I.5.3; D.1.3 published:2013-02-02 summary:The discrete distribution clustering algorithm, namely D2-clustering, hasdemonstrated its usefulness in image classification and annotation where eachobject is represented by a bag of weighed vectors. The high computationalcomplexity of the algorithm, however, limits its applications to large-scaleproblems. We present a parallel D2-clustering algorithm with substantiallyimproved scalability. A hierarchical structure for parallel computing isdevised to achieve a balance between the individual-node computation and theintegration process of the algorithm. Additionally, it is shown that even witha single CPU, the hierarchical structure results in significant speed-up.Experiments on real-world large-scale image data, Youtube video data, andprotein sequence data demonstrate the efficiency and wide applicability of theparallel D2-clustering algorithm. The loss in clustering accuracy is minor incomparison with the original sequential algorithm.
arxiv-3000-268 | Sharp Inequalities for $f$-divergences | http://arxiv.org/pdf/1302.0336v2.pdf | author:Adityanand Guntuboyina, Sujayam Saha, Geoffrey Schiebinger category:math.ST cs.IT math.IT math.OC math.PR stat.ML stat.TH published:2013-02-02 summary:$f$-divergences are a general class of divergences between probabilitymeasures which include as special cases many commonly used divergences inprobability, mathematical statistics and information theory such asKullback-Leibler divergence, chi-squared divergence, squared Hellingerdistance, total variation distance etc. In this paper, we study the problem ofmaximizing or minimizing an $f$-divergence between two probability measuressubject to a finite number of constraints on other $f$-divergences. We showthat these infinite-dimensional optimization problems can all be reduced tooptimization problems over small finite dimensional spaces which are tractable.Our results lead to a comprehensive and unified treatment of the problem ofobtaining sharp inequalities between $f$-divergences. We demonstrate that manyof the existing results on inequalities between $f$-divergences can be obtainedas special cases of our results and we also improve on some existing non-sharpinequalities.
arxiv-3000-269 | A New Constructive Method to Optimize Neural Network Architecture and Generalization | http://arxiv.org/pdf/1302.0324v1.pdf | author:Hou Muzhou, Moon Ho Lee category:cs.NE 41A99, 65D15 published:2013-02-02 summary:In this paper, after analyzing the reasons of poor generalization andoverfitting in neural networks, we consider some noise data as a singular valueof a continuous function - jump discontinuity point. The continuous part can beapproximated with the simplest neural networks, which have good generalizationperformance and optimal network architecture, by traditional algorithms such asconstructive algorithm for feed-forward neural networks with incrementaltraining, BP algorithm, ELM algorithm, various constructive algorithm, RBFapproximation and SVM. At the same time, we will construct RBF neural networksto fit the singular value with every error in, and we prove that a functionwith jumping discontinuity points can be approximated by the simplest neuralnetworks with a decay RBF neural networks in by each error, and a function withjumping discontinuity point can be constructively approximated by a decay RBFneural networks in by each error and the constructive part have nogeneralization influence to the whole machine learning system which willoptimize neural network architecture and generalization performance, reduce theoverfitting phenomenon by avoid fitting the noisy data.
arxiv-3000-270 | Sparse MRI for motion correction | http://arxiv.org/pdf/1302.0077v1.pdf | author:Zai Yang, Cishen Zhang, Lihua Xie category:cs.CV physics.bio-ph physics.med-ph published:2013-02-01 summary:MR image sparsity/compressibility has been widely exploited for imagingacceleration with the development of compressed sensing. A sparsity-basedapproach to rigid-body motion correction is presented for the first time inthis paper. A motion is sought after such that the compensated MR image ismaximally sparse/compressible among the infinite candidates. Iterativealgorithms are proposed that jointly estimate the motion and the image content.The proposed method has a lot of merits, such as no need of additional data andloose requirement for the sampling sequence. Promising results are presented todemonstrate its performance.
arxiv-3000-271 | Regression shrinkage and grouping of highly correlated predictors with HORSES | http://arxiv.org/pdf/1302.0256v1.pdf | author:Woncheol Jang, Johan Lim, Nicole A. Lazar, Ji Meng Loh, Donghyeon Yu category:stat.ML 62J07, 62P10 published:2013-02-01 summary:Identifying homogeneous subgroups of variables can be challenging in highdimensional data analysis with highly correlated predictors. We propose a newmethod called Hexagonal Operator for Regression with Shrinkage and EqualitySelection, HORSES for short, that simultaneously selects positively correlatedvariables and identifies them as predictive clusters. This is achieved via aconstrained least-squares problem with regularization that consists of a linearcombination of an L_1 penalty for the coefficients and another L_1 penalty forpairwise differences of the coefficients. This specification of the penaltyfunction encourages grouping of positively correlated predictors combined witha sparsity solution. We construct an efficient algorithm to implement theHORSES procedure. We show via simulation that the proposed method outperformsother variable selection methods in terms of prediction error and parsimony.The technique is demonstrated on two data sets, a small data set from analysisof soil in Appalachia, and a high dimensional data set from a near infrared(NIR) spectroscopy study, showing the flexibility of the methodology.
arxiv-3000-272 | Distribution-Free Distribution Regression | http://arxiv.org/pdf/1302.0082v1.pdf | author:Barnabas Poczos, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.LG math.ST stat.TH published:2013-02-01 summary:`Distribution regression' refers to the situation where a response Y dependson a covariate P where P is a probability distribution. The model is Y=f(P) +mu where f is an unknown regression function and mu is a random error.Typically, we do not observe P directly, but rather, we observe a sample fromP. In this paper we develop theory and methods for distribution-free versionsof distribution regression. This means that we do not make distributionalassumptions about the error term mu and covariate P. We prove that when theeffective dimension is small enough (as measured by the doubling dimension),then the excess prediction risk converges to zero with a polynomial rate.
arxiv-3000-273 | Sparse Multiple Kernel Learning with Geometric Convergence Rate | http://arxiv.org/pdf/1302.0315v1.pdf | author:Rong Jin, Tianbao Yang, Mehrdad Mahdavi category:cs.LG stat.ML published:2013-02-01 summary:In this paper, we study the problem of sparse multiple kernel learning (MKL),where the goal is to efficiently learn a combination of a fixed small number ofkernels from a large pool that could lead to a kernel classifier with a smallprediction error. We develop an efficient algorithm based on the greedycoordinate descent algorithm, that is able to achieve a geometric convergencerate under appropriate conditions. The convergence rate is achieved bymeasuring the size of functional gradients by an empirical $\ell_2$ norm thatdepends on the empirical data distribution. This is in contrast to previousalgorithms that use a functional norm to measure the size of gradients, whichis independent from the data samples. We also establish a generalization errorbound of the learned sparse kernel classifier using the technique of localRademacher complexity.
arxiv-3000-274 | Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks | http://arxiv.org/pdf/1301.7724v2.pdf | author:Gunnar Carlsson, Facundo Mémoli, Alejandro Ribeiro, Santiago Segarra category:cs.LG cs.SI stat.ML published:2013-01-31 summary:This paper considers networks where relationships between nodes arerepresented by directed dissimilarities. The goal is to study methods for thedetermination of hierarchical clusters, i.e., a family of nested partitionsindexed by a connectivity parameter, induced by the given dissimilaritystructures. Our construction of hierarchical clustering methods is based ondefining admissible methods to be those methods that abide by the axioms ofvalue - nodes in a network with two nodes are clustered together at the maximumof the two dissimilarities between them - and transformation - whendissimilarities are reduced, the network may become more clustered but notless. Several admissible methods are constructed and two particular methods,termed reciprocal and nonreciprocal clustering, are shown to provide upper andlower bounds in the space of admissible methods. Alternative clusteringmethodologies and axioms are further considered. Allowing the outcome ofhierarchical clustering to be asymmetric, so that it matches the asymmetry ofthe original data, leads to the inception of quasi-clustering methods. Theexistence of a unique quasi-clustering method is shown. Allowing clustering ina two-node network to proceed at the minimum of the two dissimilaritiesgenerates an alternative axiomatic construction. There is a unique clusteringmethod in this case too. The paper also develops algorithms for the computationof hierarchical clusters using matrix powers on a min-max dioid algebra andstudies the stability of the methods proposed. We proved that most of themethods introduced in this paper are such that similar networks yield similarhierarchical clustering results. Algorithms are exemplified through theirapplication to networks describing internal migration within states of theUnited States (U.S.) and the interrelation between sectors of the U.S. economy.
arxiv-3000-275 | Multi-scale Discriminant Saliency with Wavelet-based Hidden Markov Tree Modelling | http://arxiv.org/pdf/1301.7641v2.pdf | author:Anh Cat Le Ngo, Kenneth Li-Minn Ang, Guoping Qiu, Jasmine Kah-Phooi Seng category:cs.CV published:2013-01-31 summary:The bottom-up saliency, an early stage of humans' visual attention, can beconsidered as a binary classification problem between centre and surroundclasses. Discriminant power of features for the classification is measured asmutual information between distributions of image features and correspondingclasses . As the estimated discrepancy very much depends on considered scalelevel, multi-scale structure and discriminant power are integrated by employingdiscrete wavelet features and Hidden Markov Tree (HMT). With waveletcoefficients and Hidden Markov Tree parameters, quad-tree like label structuresare constructed and utilized in maximum a posterior probability (MAP) of hiddenclass variables at corresponding dyadic sub-squares. Then, a saliency value foreach square block at each scale level is computed with discriminant powerprinciple. Finally, across multiple scales is integrated the final saliency mapby an information maximization rule. Both standard quantitative tools such asNSS, LCC, AUC and qualitative assessments are used for evaluating the proposedmulti-scale discriminant saliency (MDIS) method against the well-knowinformation based approach AIM on its released image collection witheye-tracking data. Simulation results are presented and analysed to verify thevalidity of MDIS as well as point out its limitation for further researchdirection.
arxiv-3000-276 | PyPLN: a Distributed Platform for Natural Language Processing | http://arxiv.org/pdf/1301.7738v2.pdf | author:Flávio Codeço Coelho, Renato Rocha Souza, Álvaro Justen, Flávio Amieiro, Heliana Mello category:cs.CL cs.IR published:2013-01-31 summary:This paper presents a distributed platform for Natural Language Processingcalled PyPLN. PyPLN leverages a vast array of NLP and text processing opensource tools, managing the distribution of the workload on a variety ofconfigurations: from a single server to a cluster of linux servers. PyPLN isdeveloped using Python 2.7.3 but makes it very easy to incorporate othersoftwares for specific tasks as long as a linux version is available. PyPLNfacilitates analyses both at document and corpus level, simplifying managementand publication of corpora and analytical results through an easy to use webinterface. In the current (beta) release, it supports English and Portugueselanguages with support to other languages planned for future releases. Tosupport the Portuguese language PyPLN uses the PALAVRAS parser\citep{Bick2000}.Currently PyPLN offers the following features: Text extraction with encodingnormalization (to UTF-8), part-of-speech tagging, token frequency, semanticannotation, n-gram extraction, word and sentence repertoire, and full-textsearch across corpora. The platform is licensed as GPL-v3.
arxiv-3000-277 | Equitability, mutual information, and the maximal information coefficient | http://arxiv.org/pdf/1301.7745v1.pdf | author:Justin B. Kinney, Gurinder S. Atwal category:q-bio.QM math.ST stat.ME stat.ML stat.TH published:2013-01-31 summary:Reshef et al. recently proposed a new statistical measure, the "maximalinformation coefficient" (MIC), for quantifying arbitrary dependencies betweenpairs of stochastic quantities. MIC is based on mutual information, afundamental quantity in information theory that is widely understood to servethis need. MIC, however, is not an estimate of mutual information. Indeed, itwas claimed that MIC possesses a desirable mathematical property called"equitability" that mutual information lacks. This was not proven; instead itwas argued solely through the analysis of simulated data. Here we show thatthis claim, in fact, is incorrect. First we offer mathematical proof that no(non-trivial) dependence measure satisfies the definition of equitabilityproposed by Reshef et al.. We then propose a self-consistent and more generaldefinition of equitability that follows naturally from the Data ProcessingInequality. Mutual information satisfies this new definition of equitabilitywhile MIC does not. Finally, we show that the simulation evidence offered byReshef et al. was artifactual. We conclude that estimating mutual informationis not only practical for many real-world applications, but also provides anatural solution to the problem of quantifying associations in large data sets.
arxiv-3000-278 | Fast non parametric entropy estimation for spatial-temporal saliency method | http://arxiv.org/pdf/1301.7661v1.pdf | author:Anh Cat Le Ngo, Guoping Qiu, Geoff Underwood, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng category:cs.CV published:2013-01-31 summary:This paper formulates bottom-up visual saliency as center surroundconditional entropy and presents a fast and efficient technique for thecomputation of such a saliency map. It is shown that the new saliencyformulation is consistent with self-information based saliency,decision-theoretic saliency and Bayesian definition of surprises but also facesthe same significant computational challenge of estimating probability densityin very high dimensional spaces with limited samples. We have developed a fastand efficient nonparametric method to make the practical implementation ofthese types of saliency maps possible. By aligning pixels from the center andsurround regions and treating their location coordinates as random variables,we use a k-d partitioning method to efficiently estimating the center surroundconditional entropy. We present experimental results on two publicly availableeye tracking still image databases and show that the new technique iscompetitive with state of the art bottom-up saliency computational methods. Wehave also extended the technique to compute spatiotemporal visual saliency ofvideo and evaluate the bottom-up spatiotemporal saliency against eye trackingdata on a video taken onboard a moving vehicle with the driver's eye beingtracked by a head mounted eye-tracker.
arxiv-3000-279 | Rank regularization and Bayesian inference for tensor completion and extrapolation | http://arxiv.org/pdf/1301.7619v1.pdf | author:Juan Andres Bazerque, Gonzalo Mateos, Georgios B. Giannakis category:cs.IT cs.LG math.IT stat.ML published:2013-01-31 summary:A novel regularizer of the PARAFAC decomposition factors capturing thetensor's rank is proposed in this paper, as the key enabler for completion ofthree-way data arrays with missing entries. Set in a Bayesian framework, thetensor completion method incorporates prior information to enhance itssmoothing and prediction capabilities. This probabilistic approach cannaturally accommodate general models for the data distribution, lending itselfto various fitting criteria that yield optimum estimates in themaximum-a-posteriori sense. In particular, two algorithms are devised forGaussian- and Poisson-distributed data, that minimize the rank-regularizedleast-squares error and Kullback-Leibler divergence, respectively. The proposedtechnique is able to recover the "ground-truth'' tensor rank when tested onsynthetic data, and to complete brain imaging and yeast gene expressiondatasets with 50% and 15% of missing entries respectively, resulting inrecovery errors at -10dB and -15dB.
arxiv-3000-280 | Empirical Analysis of Predictive Algorithms for Collaborative Filtering | http://arxiv.org/pdf/1301.7363v1.pdf | author:John S. Breese, David Heckerman, Carl Kadie category:cs.IR cs.LG published:2013-01-30 summary:Collaborative filtering or recommender systems use a database about userpreferences to predict additional topics or products a new user might like. Inthis paper we describe several algorithms designed for this task, includingtechniques based on correlation coefficients, vector-based similaritycalculations, and statistical Bayesian methods. We compare the predictiveaccuracy of the various methods in a set of representative problem domains. Weuse two basic classes of evaluation metrics. The first characterizes accuracyover a set of individual predictions in terms of average absolute deviation.The second estimates the utility of a ranked list of suggested items. Thismetric uses an estimate of the probability that a user will see arecommendation in an ordered list. Experiments were run for datasets associatedwith 3 application areas, 4 experimental protocols, and the 2 evaluationmetrics for the various algorithms. Results indicate that for a wide range ofconditions, Bayesian networks with decision trees at each node and correlationmethods outperform Bayesian-clustering and vector-similarity methods. Betweencorrelation and Bayesian networks, the preferred method depends on the natureof the dataset, nature of the application (ranked versus one-by-onepresentation), and the availability of votes with which to make predictions.Other considerations include the size of database, speed of predictions, andlearning time.
arxiv-3000-281 | Graphical Models and Exponential Families | http://arxiv.org/pdf/1301.7376v1.pdf | author:Dan Geiger, Christopher Meek category:cs.LG stat.ML published:2013-01-30 summary:We provide a classification of graphical models according to theirrepresentation as subfamilies of exponential families. Undirected graphicalmodels with no hidden variables are linear exponential families (LEFs),directed acyclic graphical models and chain graphs with no hidden variables,including Bayesian networks with several families of local distributions, arecurved exponential families (CEFs) and graphical models with hidden variablesare stratified exponential families (SEFs). An SEF is a finite union of CEFssatisfying a frontier condition. In addition, we illustrate how one canautomatically generate independence and non-independence constraints on thedistributions over the observable variables implied by a Bayesian network withhidden variables. The relevance of these results for model selection isexamined.
arxiv-3000-282 | Minimum Encoding Approaches for Predictive Modeling | http://arxiv.org/pdf/1301.7378v1.pdf | author:Peter D Grunwald, Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri category:cs.LG stat.ML published:2013-01-30 summary:We analyze differences between two information-theoretically motivatedapproaches to statistical inference and model selection: the MinimumDescription Length (MDL) principle, and the Minimum Message Length (MML)principle. Based on this analysis, we present two revised versions of MML: apointwise estimator which gives the MML-optimal single parameter model, and avolumewise estimator which gives the MML-optimal region in the parameter space.Our empirical results suggest that with small data sets, the MDL approachyields more accurate predictions than the MML estimators. The empirical resultsalso demonstrate that the revised MML estimators introduced here perform betterthan the original MML estimator suggested by Wallace and Freeman.
arxiv-3000-283 | Hierarchical Mixtures-of-Experts for Exponential Family Regression Models with Generalized Linear Mean Functions: A Survey of Approximation and Consistency Results | http://arxiv.org/pdf/1301.7390v1.pdf | author:Wenxin Jiang, Martin A. Tanner category:cs.LG stat.ML published:2013-01-30 summary:We investigate a class of hierarchical mixtures-of-experts (HME) models whereexponential family regression models with generalized linear mean functions ofthe form psi(ga+fx^Tfgb) are mixed. Here psi(...) is the inverse link function.Suppose the true response y follows an exponential family regression model withmean function belonging to a class of smooth functions of the form psi(h(fx))where h(...)in W_2^infty (a Sobolev class over [0,1]^{s}). It is shown that theHME probability density functions can approximate the true density, at a rateof O(m^{-2/s}) in L_p norm, and at a rate of O(m^{-4/s}) in Kullback-Leiblerdivergence. These rates can be achieved within the family of HME structureswith no more than s-layers, where s is the dimension of the predictor fx. It isalso shown that likelihood-based inference based on HME is consistent inrecovering the truth, in the sense that as the sample size n and the number ofexperts m both increase, the mean square error of the predicted mean responsegoes to zero. Conditions for such results to hold are stated and discussed.
arxiv-3000-284 | Inferring Informational Goals from Free-Text Queries: A Bayesian Approach | http://arxiv.org/pdf/1301.7382v2.pdf | author:David Heckerman, Eric J. Horvitz category:cs.IR cs.AI cs.CL published:2013-01-30 summary:People using consumer software applications typically do not use technicaljargon when querying an online database of help topics. Rather, they attempt tocommunicate their goals with common words and phrases that describe softwarefunctionality in terms of structure and objects they understand. We describe aBayesian approach to modeling the relationship between words in a user's queryfor assistance and the informational goals of the user. After reviewing thegeneral method, we describe several extensions that center on integratingadditional distinctions and structure about language usage and user goals intothe Bayesian models.
arxiv-3000-285 | The Bayesian Structural EM Algorithm | http://arxiv.org/pdf/1301.7373v1.pdf | author:Nir Friedman category:cs.LG cs.AI stat.ML published:2013-01-30 summary:In recent years there has been a flurry of works on learning Bayesiannetworks from data. One of the hard problems in this area is how to effectivelylearn the structure of a belief network from incomplete data- that is, in thepresence of missing values or hidden variables. In a recent paper, I introducedan algorithm called Structural EM that combines the standard ExpectationMaximization (EM) algorithm, which optimizes parameters, with structure searchfor model selection. That algorithm learns networks based on penalizedlikelihood scores, which include the BIC/MDL score and various approximationsto the Bayesian score. In this paper, I extend Structural EM to deal directlywith Bayesian model selection. I prove the convergence of the resultingalgorithm and show how to apply it for learning a large class of probabilisticmodels, including Bayesian networks and some variants thereof.
arxiv-3000-286 | An Experimental Comparison of Several Clustering and Initialization Methods | http://arxiv.org/pdf/1301.7401v2.pdf | author:Marina Meila, David Heckerman category:cs.LG stat.ML published:2013-01-30 summary:We examine methods for clustering in high dimensions. In the first part ofthe paper, we perform an experimental comparison between three batch clusteringalgorithms: the Expectation-Maximization (EM) algorithm, a winner take allversion of the EM algorithm reminiscent of the K-means algorithm, andmodel-based hierarchical agglomerative clustering. We learn naive-Bayes modelswith a hidden root node, using high-dimensional discrete-variable data sets(both real and synthetic). We find that the EM algorithm significantlyoutperforms the other methods, and proceed to investigate the effect of variousinitialization schemes on the final solution produced by the EM algorithm. Theinitializations that we consider are (1) parameters sampled from anuninformative prior, (2) random perturbations of the marginal distribution ofthe data, and (3) the output of hierarchical agglomerative clustering. Althoughthe methods are substantially different, they lead to learned models that arestrikingly similar in quality.
arxiv-3000-287 | Large Deviation Methods for Approximate Probabilistic Inference | http://arxiv.org/pdf/1301.7392v1.pdf | author:Michael Kearns, Lawrence Saul category:cs.LG stat.ML published:2013-01-30 summary:We study two-layer belief networks of binary random variables in which theconditional probabilities Pr[childlparents] depend monotonically on weightedsums of the parents. In large networks where exact probabilistic inference isintractable, we show how to compute upper and lower bounds on manyprobabilities of interest. In particular, using methods from large deviationtheory, we derive rigorous bounds on marginal probabilities such asPr[children] and prove rates of convergence for the accuracy of our bounds as afunction of network size. Our results apply to networks with generic transferfunction parameterizations of the conditional probability tables, such assigmoid and noisy-OR. They also explicitly illustrate the types of averagingbehavior that can simplify the problem of inference in large networks.
arxiv-3000-288 | Learning Mixtures of DAG Models | http://arxiv.org/pdf/1301.7415v2.pdf | author:Bo Thiesson, Christopher Meek, David Maxwell Chickering, David Heckerman category:cs.LG cs.AI stat.ML published:2013-01-30 summary:We describe computationally efficient methods for learning mixtures in whicheach component is a directed acyclic graphical model (mixtures of DAGs orMDAGs). We argue that simple search-and-score algorithms are infeasible for avariety of problems, and introduce a feasible approach in which parameter andstructure search is interleaved and expected data is treated as real data. Ourapproach can be viewed as a combination of (1) the Cheeseman--Stutz asymptoticapproximation for model posterior probability and (2) theExpectation--Maximization algorithm. We evaluate our procedure for selectingamong MDAGs on synthetic and real examples.
arxiv-3000-289 | Statistical mechanics of complex neural systems and high dimensional data | http://arxiv.org/pdf/1301.7115v1.pdf | author:Madhu Advani, Subhaneil Lahiri, Surya Ganguli category:q-bio.NC stat.ML published:2013-01-30 summary:Recent experimental advances in neuroscience have opened new vistas into theimmense complexity of neuronal networks. This proliferation of data challengesus on two parallel fronts. First, how can we form adequate theoreticalframeworks for understanding how dynamical network processes cooperate acrosswidely disparate spatiotemporal scales to solve important computationalproblems? And second, how can we extract meaningful models of neuronal systemsfrom high dimensional datasets? To aid in these challenges, we give apedagogical review of a collection of ideas and theoretical methods arising atthe intersection of statistical physics, computer science and neurobiology. Weintroduce the interrelated replica and cavity methods, which originated instatistical physics as powerful ways to quantitatively analyze large highlyheterogeneous systems of many interacting degrees of freedom. We also introducethe closely related notion of message passing in graphical models, whichoriginated in computer science as a distributed algorithm capable of solvinglarge inference and optimization problems involving many coupled variables. Wethen show how both the statistical physics and computer science perspectivescan be applied in a wide diversity of contexts to problems arising intheoretical neuroscience and data analysis. Along the way we discuss spinglasses, learning theory, illusions of structure in noise, random matrices,dimensionality reduction, and compressed sensing, all within the unifiedformalism of the replica method. Moreover, we review recent conceptualconnections between message passing in graphical models, and neural computationand learning. Overall, these ideas illustrate how statistical physics andcomputer science might provide a lens through which we can uncover emergentcomputational functions buried deep within the dynamical complexities ofneuronal networks.
arxiv-3000-290 | Mixture Representations for Inference and Learning in Boltzmann Machines | http://arxiv.org/pdf/1301.7393v1.pdf | author:Neil D. Lawrence, Christopher M. Bishop, Michael I. Jordan category:cs.LG stat.ML published:2013-01-30 summary:Boltzmann machines are undirected graphical models with two-state stochasticvariables, in which the logarithms of the clique potentials are quadraticfunctions of the node states. They have been widely studied in the neuralcomputing literature, although their practical applicability has been limitedby the difficulty of finding an effective learning algorithm. Onewell-established approach, known as mean field theory, represents thestochastic distribution using a factorized approximation. However, thecorresponding learning algorithm often fails to find a good solution. Weconjecture that this is due to the implicit uni-modality of the mean fieldapproximation which is therefore unable to capture multi-modality in the truedistribution. In this paper we use variational methods to approximate thestochastic distribution using multi-modal mixtures of factorized distributions.We present results for both inference and learning to demonstrate theeffectiveness of this approach.
arxiv-3000-291 | Information driven self-organization of complex robotic behaviors | http://arxiv.org/pdf/1301.7473v2.pdf | author:Georg Martius, Ralf Der, Nihat Ay category:cs.RO cs.IT cs.LG math.IT published:2013-01-30 summary:Information theory is a powerful tool to express principles to driveautonomous systems because it is domain invariant and allows for an intuitiveinterpretation. This paper studies the use of the predictive information (PI),also called excess entropy or effective measure complexity, of the sensorimotorprocess as a driving force to generate behavior. We study nonlinear andnonstationary systems and introduce the time-local predicting information(TiPI) which allows us to derive exact results together with explicit updaterules for the parameters of the controller in the dynamical systems framework.In this way the information principle, formulated at the level of behavior, istranslated to the dynamics of the synapses. We underpin our results with anumber of case studies with high-dimensional robotic systems. We show thespontaneous cooperativity in a complex physical system with decentralizedcontrol. Moreover, a jointly controlled humanoid robot develops a highbehavioral variety depending on its physics and the environment it isdynamically embedded into. The behavior can be decomposed into a succession oflow-dimensional modes that increasingly explore the behavior space. This is apromising way to avoid the curse of dimensionality which hinders learningsystems to scale well.
arxiv-3000-292 | A note on selection stability: combining stability and prediction | http://arxiv.org/pdf/1301.7118v1.pdf | author:Yixin Fang, Junhui Wang, Wei Sun category:stat.ME stat.ML published:2013-01-30 summary:Recently, many regularized procedures have been proposed for variableselection in linear regression, but their performance depends on the tuningparameter selection. Here a criterion for the tuning parameter selection isproposed, which combines the strength of both stability selection andcross-validation and therefore is referred as the prediction and stabilityselection (PASS). The selection consistency is established assuming the datagenerating model is a subset of the full model, and the small sampleperformance is demonstrated through some simulation studies where theassumption is either held or violated.
arxiv-3000-293 | Approximate Counting of Graphical Models Via MCMC Revisited | http://arxiv.org/pdf/1301.7189v2.pdf | author:Jose M. Peña category:stat.ML cs.AI published:2013-01-30 summary:In Pe\~na (2007), MCMC sampling is applied to approximately calculate theratio of essential graphs (EGs) to directed acyclic graphs (DAGs) for up to 20nodes. In the present paper, we extend that work from 20 to 31 nodes. We alsoextend that work by computing the approximate ratio of connected EGs toconnected DAGs, of connected EGs to EGs, and of connected DAGs to DAGs.Furthermore, we prove that the latter ratio is asymptotically 1. We alsodiscuss the implications of these results for learning DAGs from data.
arxiv-3000-294 | A Multivariate Discretization Method for Learning Bayesian Networks from Mixed Data | http://arxiv.org/pdf/1301.7403v1.pdf | author:Stefano Monti, Gregory F. Cooper category:cs.AI cs.LG published:2013-01-30 summary:In this paper we address the problem of discretization in the context oflearning Bayesian networks (BNs) from data containing both continuous anddiscrete variables. We describe a new technique for <EM>multivariate</EM>discretization, whereby each continuous variable is discretized while takinginto account its interaction with the other variables. The technique is basedon the use of a Bayesian scoring metric that scores the discretization policyfor a continuous variable given a BN structure and the observed data. Since themetric is relative to the BN structure currently being evaluated, thediscretization of a variable needs to be dynamically adjusted as the BNstructure changes.
arxiv-3000-295 | Learning the Structure of Dynamic Probabilistic Networks | http://arxiv.org/pdf/1301.7374v1.pdf | author:Nir Friedman, Kevin Murphy, Stuart Russell category:cs.AI cs.LG published:2013-01-30 summary:Dynamic probabilistic networks are a compact representation of complexstochastic processes. In this paper we examine how to learn the structure of aDPN from data. We extend structure scoring rules for standard probabilisticnetworks to the dynamic case, and show how to search for structure when some ofthe variables are hidden. Finally, we examine two applications where such atechnology might be useful: predicting and classifying dynamic behaviors, andlearning causal orderings in biological processes. We provide empirical resultsthat demonstrate the applicability of our methods in both domains.
arxiv-3000-296 | Learning by Transduction | http://arxiv.org/pdf/1301.7375v1.pdf | author:Alex Gammerman, Volodya Vovk, Vladimir Vapnik category:cs.LG stat.ML published:2013-01-30 summary:We describe a method for predicting a classification of an object givenclassifications of the objects in the training set, assuming that the pairsobject/classification are generated by an i.i.d. process from a continuousprobability distribution. Our method is a modification of Vapnik'ssupport-vector machine; its main novelty is that it gives not only theprediction itself but also a practicable measure of the evidence found insupport of that prediction. We also describe a procedure for assigning degreesof confidence to predictions made by the support vector machine. Someexperimental results are presented, and possible extensions of the algorithmsare discussed.
arxiv-3000-297 | On the Geometry of Bayesian Graphical Models with Hidden Variables | http://arxiv.org/pdf/1301.7411v1.pdf | author:Raffaella Settimi, Jim Q. Smith category:cs.LG stat.ML published:2013-01-30 summary:In this paper we investigate the geometry of the likelihood of the unknownparameters in a simple class of Bayesian directed graphs with hidden variables.This enables us, before any numerical algorithms are employed, to obtaincertain insights in the nature of the unidentifiability inherent in suchmodels, the way posterior densities will be sensitive to prior densities andthe typical geometrical form these posterior densities might take. Many ofthese insights carry over into more complicated Bayesian networks withsystematic missing data.
arxiv-3000-298 | Multi-Step Regression Learning for Compositional Distributional Semantics | http://arxiv.org/pdf/1301.6939v2.pdf | author:Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, Marco Baroni category:cs.CL cs.LG 68T50 published:2013-01-29 summary:We present a model for compositional distributional semantics related to theframework of Coecke et al. (2010), and emulating formal semantics byrepresenting functions as tensors and arguments as vectors. We introduce a newlearning method for tensors, generalising the approach of Baroni and Zamparelli(2010). We evaluate it on two benchmark data sets, and find it to outperformexisting leading methods. We argue in our analysis that the nature of thislearning method also renders it suitable for solving more subtle problemscompositional distributional models might face.
arxiv-3000-299 | Robust Face Recognition via Block Sparse Bayesian Learning | http://arxiv.org/pdf/1301.6847v2.pdf | author:Taiyong Li, Zhilin Zhang category:cs.CV published:2013-01-29 summary:Face recognition (FR) is an important task in pattern recognition andcomputer vision. Sparse representation (SR) has been demonstrated to be apowerful framework for FR. In general, an SR algorithm treats each face in atraining dataset as a basis function, and tries to find a sparse representationof a test face under these basis functions. The sparse representationcoefficients then provide a recognition hint. Early SR algorithms are based ona basic sparse model. Recently, it has been found that algorithms based on ablock sparse model can achieve better recognition rates. Based on this model,in this study we use block sparse Bayesian learning (BSBL) to find a sparserepresentation of a test face for recognition. BSBL is a recently proposedframework, which has many advantages over existing block-sparse-model basedalgorithms. Experimental results on the Extended Yale B, the AR and the CMU PIEface databases show that using BSBL can achieve better recognition rates andhigher robustness than state-of-the-art algorithms in most cases.
arxiv-3000-300 | Link prediction for partially observed networks | http://arxiv.org/pdf/1301.7047v1.pdf | author:Yunpeng Zhao, Elizaveta Levina, Ji Zhu category:stat.ML cs.LG cs.SI published:2013-01-29 summary:Link prediction is one of the fundamental problems in network analysis. Inmany applications, notably in genetics, a partially observed network may notcontain any negative examples of absent edges, which creates a difficulty formany existing supervised learning approaches. We develop a new method whichtreats the observed network as a sample of the true network with differentsampling rates for positive and negative examples. We obtain a relative rankingof potential links by their probabilities, utilizing information on nodecovariates as well as on network topology. Empirically, the method performswell under many settings, including when the observed network is sparse. Weapply the method to a protein-protein interaction network and a schoolfriendship network.
