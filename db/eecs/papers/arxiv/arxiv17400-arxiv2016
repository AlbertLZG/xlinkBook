arxiv-17400-1 | Algebraic Clustering of Affine Subspaces | http://arxiv.org/pdf/1509.06729v2.pdf | author:Manolis C. Tsakiris, Rene Vidal category:cs.CV published:2015-09-22 summary:Subspace clustering is an important problem in machine learning with manyapplications in computer vision and pattern recognition. Prior work has studiedthis problem using algebraic, iterative, statistical, low-rank and sparserepresentation techniques. While these methods have been applied to both linearand affine subspaces, theoretical results have only been established in thecase of linear subspaces. For example, algebraic subspace clustering (ASC) isguaranteed to provide the correct clustering when the data points are ingeneral position and the union of subspaces is transversal. In this paper westudy in a rigorous fashion the properties of ASC in the case of affinesubspaces. Using notions from algebraic geometry, we prove that thehomogenization trick, which embeds points in a union of affine subspaces intopoints in a union of linear subspaces, preserves the general position of thepoints and the transversality of the union of subspaces in the embedded space,thus establishing the correctness of ASC for affine subpaces.
arxiv-17400-2 | Bayesian Inference of Recursive Sequences of Group Activities from Tracks | http://arxiv.org/pdf/1604.06970v1.pdf | author:Ernesto Brau, Colin Dawson, Alfredo Carrillo, David Sidi, Clayton T. Morrison category:cs.AI cs.CV published:2016-04-24 summary:We present a probabilistic generative model for inferring a description ofcoordinated, recursively structured group activities at multiple levels oftemporal granularity based on observations of individuals' trajectories. Themodel accommodates: (1) hierarchically structured groups, (2) activities thatare temporally and compositionally recursive, (3) component roles assigningdifferent subactivity dynamics to subgroups of participants, and (4) anonparametric Gaussian Process model of trajectories. We present an MCMCsampling framework for performing joint inference over recursive activitydescriptions and assignment of trajectories to groups, integrating outcontinuous parameters. We demonstrate the model's expressive power in severalsimulated and complex real-world scenarios from the VIRAT and UCLA Aerial Eventvideo data sets.
arxiv-17400-3 | Agnostic Estimation of Mean and Covariance | http://arxiv.org/pdf/1604.06968v1.pdf | author:Kevin A. Lai, Anup B. Rao, Santosh Vempala category:cs.DS cs.LG stat.ML published:2016-04-24 summary:We consider the problem of estimating the mean and covariance of adistribution from iid samples in $\mathbb{R}^n$, in the presence of an $\eta$fraction of malicious noise; this is in contrast to much recent work where thenoise itself is assumed to be from a distribution of known type. The agnosticproblem includes many interesting special cases, e.g., learning the parametersof a single Gaussian (or finding the best-fit Gaussian) when $\eta$ fraction ofdata is adversarially corrupted, agnostically learning a mixture of Gaussians,agnostic ICA, etc. We present polynomial-time algorithms to estimate the meanand covariance with error guarantees in terms of information-theoretic lowerbounds. As a corollary, we also obtain an agnostic algorithm for Singular ValueDecomposition.
arxiv-17400-4 | Efficient AUC Optimization for Information Ranking Applications | http://arxiv.org/pdf/1511.05202v3.pdf | author:Sean J. Welleck category:cs.IR stat.ML published:2015-11-16 summary:Adequate evaluation of an information retrieval system to estimate futureperformance is a crucial task. Area under the ROC curve (AUC) is widely used toevaluate the generalization of a retrieval system. However, the objectivefunction optimized in many retrieval systems is the error rate and not the AUCvalue. This paper provides an efficient and effective non-linear approach tooptimize AUC using additive regression trees, with a special emphasis on theuse of multi-class AUC (MAUC) because multiple relevance levels are widely usedin many ranking applications. Compared to a conventional linear approach, theperformance of the non-linear approach is comparable on binary-relevancebenchmark datasets and is better on multi-relevance benchmark datasets.
arxiv-17400-5 | Net2Net: Accelerating Learning via Knowledge Transfer | http://arxiv.org/pdf/1511.05641v4.pdf | author:Tianqi Chen, Ian Goodfellow, Jonathon Shlens category:cs.LG published:2015-11-18 summary:We introduce techniques for rapidly transferring the information stored inone neural net into another neural net. The main purpose is to accelerate thetraining of a significantly larger neural net. During real-world workflows, oneoften trains very many different neural networks during the experimentation anddesign process. This is a wasteful process in which each new model is trainedfrom scratch. Our Net2Net technique accelerates the experimentation process byinstantaneously transferring the knowledge from a previous network to each newdeeper or wider network. Our techniques are based on the concept offunction-preserving transformations between neural network specifications. Thisdiffers from previous approaches to pre-training that altered the functionrepresented by a neural net when adding layers to it. Using our knowledgetransfer mechanism to add depth to Inception modules, we demonstrate a newstate of the art accuracy rating on the ImageNet dataset.
arxiv-17400-6 | Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach | http://arxiv.org/pdf/1601.06403v3.pdf | author:Ali Moharrer, Shuangqing Wei, George T. Amariucai, Jing Deng category:cs.IT cs.CV math.IT stat.ML published:2016-01-24 summary:In latent Gaussian trees the pairwise correlation signs between the variablesare intrinsically unrecoverable. Such information is vital since it completelydetermines the direction in which two variables are associated. %suchsingularity has been ignored in many studies. In this work, we resort toinformation theoretical approaches to achieve two fundamental goals: First, wequantify the amount of information loss due to unrecoverable sign information.Second, we show the importance of such information in determining the maximumachievable rate region, in which the observed output vector can be synthesized,given its probability density function. In particular, we model the graphicalmodel as a communication channel and propose a new layered encoding frameworkto synthesize observed data using upper layer Gaussian inputs and independentBernoulli correlation sign inputs from each layer. We find the achievable rateregion for the rate tuples of multi-layer latent Gaussian messages tosynthesize the desired observables.
arxiv-17400-7 | Jacques Lacan's Registers of the Psychoanalytic Field, Applied using Geometric Data Analysis to Edgar Allan Poe's "The Purloined Letter" | http://arxiv.org/pdf/1604.06952v1.pdf | author:Fionn Murtagh, Giuseppe Iurato category:cs.CL stat.ML 62H25, 62H30 published:2016-04-23 summary:In a first investigation, a Lacan-motivated template of the Poe story isfitted to the data. A segmentation of the storyline is used in order to map outthe diachrony. Based on this, it will be shown how synchronous aspects,potentially related to Lacanian registers, can be sought. This demonstrates theeffectiveness of an approach based on a model template of the storylinenarrative. In a second and more comprehensive investigation, we develop anapproach for revealing, that is, uncovering, Lacanian register relationships.Objectives of this work include the wide and general application of ourmethodology. This methodology is strongly based on the "letting the data speak"Correspondence Analysis analytics platform of Jean-Paul Benz\'ecri, that isalso the geometric data analysis, both qualitative and quantitative analytics,developed by Pierre Bourdieu.
arxiv-17400-8 | Probably certifiably correct k-means clustering | http://arxiv.org/pdf/1509.07983v2.pdf | author:Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar category:cs.IT cs.DS cs.LG math.IT math.ST stat.TH published:2015-09-26 summary:Recently, Bandeira [arXiv:1509.00824] introduced a new type of algorithm (theso-called probably certifiably correct algorithm) that combines fast solverswith the optimality certificates provided by convex relaxations. In this paper,we devise such an algorithm for the problem of k-means clustering. First, weprove that Peng and Wei's semidefinite relaxation of k-means is tight with highprobability under a distribution of planted clusters called the stochastic ballmodel. Our proof follows from a new dual certificate for integral solutions ofthis semidefinite program. Next, we show how to test the optimality of aproposed k-means solution using this dual certificate in quasilinear time.Finally, we analyze a version of spectral clustering from Peng and Wei that isdesigned to solve k-means in the case of two clusters. In particular, we showthat this quasilinear-time method typically recovers planted clusters under thestochastic ball model.
arxiv-17400-9 | An information theoretic formulation of the Dictionary Learning and Sparse Coding Problems on Statistical Manifolds | http://arxiv.org/pdf/1604.06939v1.pdf | author:Rudrasis Chakraborty, Monami Banerjee, Victoria Crawford, Baba C. Vemuri category:cs.CV published:2016-04-23 summary:In this work, we propose a novel information theoretic framework fordictionary learning (DL) and sparse coding (SC) on a statistical manifold (themanifold of probability distributions). Unlike the traditional DL and SCframework, our new formulation {\it does not explicitly incorporate anysparsity inducing norm in the cost function but yet yields SCs}. Moreover, weextend this framework to the manifold of symmetric positive definite matrices,$\mathcal{P}_n$. Our algorithm approximates the data points, which areprobability distributions, by the weighted Kullback-Leibeler center (KL-center)of the dictionary atoms. The KL-center is the minimizer of the maximumKL-divergence between the unknown center and members of the set whose center isbeing sought. Further, {\it we proved that this KL-center is a sparsecombination of the dictionary atoms}. Since, the data reside on a statisticalmanifold, the data fidelity term can not be as simple as in the case of thevector-space data. We therefore employ the geodesic distance between the dataand a sparse approximation of the data element. This cost function is minimizedusing an acceleterated gradient descent algorithm. An extensive set ofexperimental results show the effectiveness of our proposed framework. Wepresent several experiments involving a variety of classification problems inComputer Vision applications. Further, we demonstrate the performance of ouralgorithm by comparing it to several state-of-the-art methods both in terms ofclassification accuracy and sparsity.
arxiv-17400-10 | Memory and Information Processing in Recurrent Neural Networks | http://arxiv.org/pdf/1604.06929v1.pdf | author:Alireza Goudarzi, Sarah Marzen, Peter Banda, Guy Feldman, Christof Teuscher, Darko Stefanovic category:cs.NE published:2016-04-23 summary:Recurrent neural networks (RNN) are simple dynamical systems whosecomputational power has been attributed to their short-term memory. Short-termmemory of RNNs has been previously studied analytically only for the case oforthogonal networks, and only under annealed approximation, and uncorrelatedinput. Here for the first time, we present an exact solution to the memorycapacity and the task-solving performance as a function of the structure of agiven network instance, enabling direct determination of thefunction--structure relation in RNNs. We calculate the memory capacity forarbitrary networks with exponentially correlated input and further related itto the performance of the system on signal processing tasks in a supervisedlearning setup. We compute the expected error and the worst-case error bound asa function of the spectra of the network and the correlation structure of itsinputs and outputs. Our results give an explanation for learning andgeneralization of task solving using short-term memory, which is crucial forbuilding alternative computer architectures using physical phenomena based onthe short-term memory principle.
arxiv-17400-11 | Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews | http://arxiv.org/pdf/1512.08183v5.pdf | author:Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao category:cs.CL published:2015-12-27 summary:Despite the loss of semantic information, bag-of-ngram based methods stillachieve state-of-the-art results for tasks such as sentiment classification oflong movie reviews. Many document embeddings methods have been proposed tocapture semantics, but they still can't outperform bag-of-ngram based methodson this task. In this paper, we modify the architecture of the recentlyproposed Paragraph Vector, allowing it to learn document vectors by predictingnot only words, but n-gram features as well. Our model is able to capture bothsemantics and word order in documents while keeping the expressive power oflearned vectors. Experimental results on IMDB movie review dataset shows thatour model outperforms previous deep learning models and bag-of-ngram basedmodels due to the above advantages. More robust results are also obtained whenour model is combined with other models. The source code of our model will bealso published together with this paper.
arxiv-17400-12 | On the Sample Complexity of End-to-end Training vs. Semantic Abstraction Training | http://arxiv.org/pdf/1604.06915v1.pdf | author:Shai Shalev-Shwartz, Amnon Shashua category:cs.LG published:2016-04-23 summary:We compare the end-to-end training approach to a modular approach in which asystem is decomposed into semantically meaningful components. We focus on thesample complexity aspect, in the regime where an extremely high accuracy isnecessary, as is the case in autonomous driving applications. We demonstratecases in which the number of training examples required by the end-to-endapproach is exponentially larger than the number of examples required by thesemantic abstraction approach.
arxiv-17400-13 | TagBook: A Semantic Video Representation without Supervision for Event Detection | http://arxiv.org/pdf/1510.02899v2.pdf | author:Masoud Mazloom, Xirong Li, Cees G. M. Snoek category:cs.CV cs.MM published:2015-10-10 summary:We consider the problem of event detection in video for scenarios where onlyfew, or even zero examples are available for training. For this challengingsetting, the prevailing solutions in the literature rely on a semantic videorepresentation obtained from thousands of pre-trained concept detectors.Different from existing work, we propose a new semantic video representationthat is based on freely available social tagged videos only, without the needfor training any intermediate concept detectors. We introduce a simplealgorithm that propagates tags from a video's nearest neighbors, similar inspirit to the ones used for image retrieval, but redesign it for video eventdetection by including video source set refinement and varying the video tagassignment. We call our approach TagBook and study its construction,descriptiveness and detection performance on the TRECVID 2013 and 2014multimedia event detection datasets and the Columbia Consumer Video dataset.Despite its simple nature, the proposed TagBook video representation isremarkably effective for few-example and zero-example event detection, evenoutperforming very recent state-of-the-art alternatives building on supervisedrepresentations.
arxiv-17400-14 | Why and How to Pay Different Attention to Phrase Alignments of Different Intensities | http://arxiv.org/pdf/1604.06896v1.pdf | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2016-04-23 summary:This work studies comparatively two typical sentence pair classificationtasks: textual entailment (TE) and answer selection (AS), observing that phrasealignments of different intensities contribute differently in these tasks. Weaddress the problems of identifying phrase alignments of flexible granularityand pooling alignments of different intensities for these tasks. Examples forflexible granularity are alignments between two single words, between a singleword and a phrase and between a short phrase and a long phrase. By intensity weroughly mean the degree of match, it ranges from identity over surface-formco-occurrence, rephrasing and other semantic relatedness to unrelated words asin lots of parenthesis text. Prior work (i) has limitations in phrasegeneration and representation, or (ii) conducts alignment at word and phraselevels by handcrafted features or (iii) utilizes a single attention mechanismover alignment intensities without considering the characteristics of specifictasks, which limits the system's effectiveness across tasks. We propose anarchitecture based on Gated Recurrent Unit that supports (i) representationlearning of phrases of arbitrary granularity and (ii) task-specific focusing ofphrase alignments between two sentences by attention pooling. Experimentalresults on TE and AS match our observation and are state-of-the-art.
arxiv-17400-15 | Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging | http://arxiv.org/pdf/1604.05225v2.pdf | author:Jiren Jin, Hideki Nakayama category:cs.CV published:2016-04-18 summary:Automatic image annotation has been an important research topic infacilitating large scale image management and retrieval. Existing methods focuson learning image-tag correlation or correlation between tags to improveannotation accuracy. However, most of these methods evaluate their performanceusing top-k retrieval performance, where k is fixed. Although such settinggives convenience for comparing different methods, it is not the natural waythat humans annotate images. The number of annotated tags should depend onimage contents. Inspired by the recent progress in machine translation andimage captioning, we propose a novel Recurrent Image Annotator (RIA) model thatforms image annotation task as a sequence generation problem so that RIA cannatively predict the proper length of tags according to image contents. Weevaluate the proposed model on various image annotation datasets. In additionto comparing our model with existing methods using the conventional top-kevaluation measures, we also provide our model as a high quality baseline forthe arbitrary length image tagging task. Moreover, the results of ourexperiments show that the order of tags in training phase has a great impact onthe final annotation performance.
arxiv-17400-16 | Dimensionality-Dependent Generalization Bounds for $k$-Dimensional Coding Schemes | http://arxiv.org/pdf/1601.00238v2.pdf | author:Tongliang Liu, Dacheng Tao, Dong Xu category:stat.ML cs.LG published:2016-01-03 summary:The $k$-dimensional coding schemes refer to a collection of methods thatattempt to represent data using a set of representative $k$-dimensionalvectors, and include non-negative matrix factorization, dictionary learning,sparse coding, $k$-means clustering and vector quantization as special cases.Previous generalization bounds for the reconstruction error of the$k$-dimensional coding schemes are mainly dimensionality independent. A majoradvantage of these bounds is that they can be used to analyze thegeneralization error when data is mapped into an infinite- or high-dimensionalfeature space. However, many applications use finite-dimensional data features.Can we obtain dimensionality-dependent generalization bounds for$k$-dimensional coding schemes that are tighter than dimensionality-independentbounds when data is in a finite-dimensional feature space? The answer ispositive. In this paper, we address this problem and derive adimensionality-dependent generalization bound for $k$-dimensional codingschemes by bounding the covering number of the loss function class induced bythe reconstruction error. The bound is of order$\mathcal{O}\left(\left(mk\ln(mkn)/n\right)^{\lambda_n}\right)$, where $m$ isthe dimension of features, $k$ is the number of the columns in the linearimplementation of coding schemes, $n$ is the size of sample, $\lambda_n>0.5$when $n$ is finite and $\lambda_n=0.5$ when $n$ is infinite. We show that ourbound can be tighter than previous results, because it avoids inducing theworst-case upper bound on $k$ of the loss function and converges faster. Theproposed generalization bound is also applied to some specific coding schemesto demonstrate that the dimensionality-dependent bound is an indispensablecomplement to these dimensionality-independent generalization bounds.
arxiv-17400-17 | Text Flow: A Unified Text Detection System in Natural Scene Images | http://arxiv.org/pdf/1604.06877v1.pdf | author:Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan category:cs.CV published:2016-04-23 summary:The prevalent scene text detection approach follows four sequential stepscomprising character candidate detection, false character candidate removal,text line extraction, and text line verification. However, errors occur andaccumulate throughout each of these sequential steps which often lead to lowdetection performance. To address these issues, we propose a unified scene textdetection system, namely Text Flow, by utilizing the minimum cost (min-cost)flow network model. With character candidates detected by cascade boosting, themin-cost flow network model integrates the last three sequential steps into asingle process which solves the error accumulation problem at both characterlevel and text line level effectively. The proposed technique has been testedon three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and amultilingual dataset and it outperforms the state-of-the-art methods on allthree datasets with much higher recall and F-score. The good performance on themultilingual dataset shows that the proposed technique can be used for thedetection of texts in different languages.
arxiv-17400-18 | Contextual object categorization with energy-based model | http://arxiv.org/pdf/1604.06852v1.pdf | author:Changyong Ri, Duho Pak, Cholryong Choe, Suhyang Kim, Yonghak Sin category:cs.CV published:2016-04-23 summary:Object categorization is a hot issue of an image mining. Contextualinformation between objects is one of the important semantic knowledge of animage. However, the previous researches for an object categorization have notmade full use of the contextual information, especially the spatial relationsbetween objects. In addition, the object categorization methods, whichgenerally use the probabilistic graphical models to implement the incorporationof contextual information with appearance of objects, are almost inevitable toevaluate the intractable partition function for normalization. In this work, weintroduced fully-connected fuzzy spatial relations including directional,distance and topological relations between object regions, so the spatialrelational information could be fully utilized. Then, the spatial relationswere considered as well as co-occurrence and appearance of objects by usingenergy-based model, where the energy function was defined as the region-objectassociation potential and the configuration potential of objects. Minimizingthe energy function of whole image arrangement, we obtained the optimal labelset about the image regions and addressed the evaluation of intractablepartition function in conditional random fields. Experimental results show thevalidity and reliability of this proposed method.
arxiv-17400-19 | Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond | http://arxiv.org/pdf/1602.06023v3.pdf | author:Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang category:cs.CL published:2016-02-19 summary:In this work, we cast abstractive text summarization as asequence-to-sequence problem and employ the framework of AttentionalEncoder-Decoder Recurrent Neural Networks to this problem, outperformingstate-of-the art model of Rush et. al. (2015) on two different corpora. We alsomove beyond the basic architecture, and propose several novel models to addressimportant problems in summarization including modeling key-words, capturing thehierarchy of sentence-to-word structure and addressing the problem of wordsthat are key to a document, but rare elsewhere. Our work shows that many of ourproposed solutions contribute to further improvement in performance. Inaddition, we propose a new dataset consisting of multi-sentence summaries, andestablish performance benchmarks for further research.
arxiv-17400-20 | A Computational Model for Situated Task Learning with Interactive Instruction | http://arxiv.org/pdf/1604.06849v1.pdf | author:Shiwali Mohan, James Kirk, John Laird category:cs.AI cs.LG published:2016-04-23 summary:Learning novel tasks is a complex cognitive activity requiring the learner toacquire diverse declarative and procedural knowledge. Prior ACT-R models ofacquiring task knowledge from instruction focused on learning proceduralknowledge from declarative instructions encoded in semantic memory. In thispaper, we identify the requirements for designing compu- tational models thatlearn task knowledge from situated task- oriented interactions with an expertand then describe and evaluate a model of learning from situated interactiveinstruc- tion that is implemented in the Soar cognitive architecture.
arxiv-17400-21 | Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction | http://arxiv.org/pdf/1604.06838v1.pdf | author:Jianfeng Dong, Xirong Li, Cees G. M. Snoek category:cs.CV published:2016-04-23 summary:This paper attacks the challenging problem of cross-media retrieval. That is,given an image find the text best describing its content, or the other wayaround. Different from existing works, which either rely on a joint space, or atext space, we propose to perform cross-media retrieval in a visual space only.We contribute \textit{Word2VisualVec}, a deep neural network architecture thatlearns to predict a deep visual encoding of textual input. We discuss itsarchitecture for prediction of CaffeNet and GoogleNet features, as well as itsloss functions for learning from text/image pairs in large-scale click-throughlogs and image sentences. Experiments on the Clickture-Lite and Flickr8Kcorpora demonstrate the robustness for both Text-to-Image and Image-to-Textretrieval, outperforming the state-of-the-art on both accounts. Interestingly,an embedding in predicted visual feature space is also highly effective whensearching in text only.
arxiv-17400-22 | When Are Nonconvex Problems Not Scary? | http://arxiv.org/pdf/1510.06096v2.pdf | author:Ju Sun, Qing Qu, John Wright category:math.OC cs.IT math.IT stat.ML published:2015-10-21 summary:In this note, we focus on smooth nonconvex optimization problems that obey:(1) all local minimizers are also global; and (2) around any saddle point orlocal maximizer, the objective has a negative directional curvature. Concreteapplications such as dictionary learning, generalized phase retrieval, andorthogonal tensor decomposition are known to induce such structures. Wedescribe a second-order trust-region algorithm that provably converges to aglobal minimizer efficiently, without special initializations. Finally wehighlight alternatives, and open problems in this direction.
arxiv-17400-23 | Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains | http://arxiv.org/pdf/1604.03053v3.pdf | author:Yuan Zhao, Il Memming Park category:stat.ML q-bio.NC published:2016-04-11 summary:A small number of common factors often explain most of the interdependenceamong simultaneously recorded neurons, a signature of underlyinglow-dimensional dynamics. We posit that simple neural coding and computationmanifest as low-dimensional nonlinear dynamics implemented redundantly within alarge population of neurons. Recovering the latent dynamics from observationscan offer a deeper understanding of neural computation. We improve uponpreviously-proposed methods for recovering latent dynamics, which assume eitheran inappropriate observation model or linear dynamics. We propose a practicaland efficient inference method for a generative model with explicit pointprocess observations and an assumption of smooth nonlinear dynamics. Wevalidate our method on both simulated data and population recording fromprimary visual cortex.
arxiv-17400-24 | Finding Optimal Combination of Kernels using Genetic Programming | http://arxiv.org/pdf/1604.02376v2.pdf | author:Jyothi Korra category:cs.CV cs.LG cs.NE published:2016-04-08 summary:In Computer Vision, problem of identifying or classifying the objects presentin an image is called Object Categorization. It is a challenging problem,especially when the images have clutter background, occlusions or differentlighting conditions. Many vision features have been proposed which aid objectcategorization even in such adverse conditions. Past research has shown that,employing multiple features rather than any single features leads to betterrecognition. Multiple Kernel Learning (MKL) framework has been developed forlearning an optimal combination of features for object categorization. ExistingMKL methods use linear combination of base kernels which may not be optimal forobject categorization. Real-world object categorization may need to considercomplex combination of kernels(non-linear) and not only linear combination.Evolving non-linear functions of base kernels using Genetic Programming isproposed in this report. Experiment results show that non-kernel generatedusing genetic programming gives good accuracy as compared to linear combinationof kernels.
arxiv-17400-25 | Can Boosting with SVM as Week Learners Help? | http://arxiv.org/pdf/1604.05242v2.pdf | author:Dinesh Govindaraj category:cs.CV cs.LG published:2016-04-18 summary:Object recognition in images involves identifying objects with partialocclusions, viewpoint changes, varying illumination, cluttered backgrounds.Recent work in object recognition uses machine learning techniques SVM-KNN,Local Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, wewant to utilize SVM as week learners in AdaBoost. Experiments are done withclassifiers like near- est neighbor, k-nearest neighbor, Support vectormachines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariantdescriptors and Pyramid his- togram of gradient descriptors. AdaBoost istrained with set of week classifier as SVMs, each with kernel distance functionon different descriptors. Results shows AdaBoost with SVM outperform othermethods for Object Categorization dataset.
arxiv-17400-26 | Refining Architectures of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.06832v1.pdf | author:Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla category:cs.CV published:2016-04-22 summary:Deep Convolutional Neural Networks (CNNs) have recently evinced immensesuccess for various image recognition tasks. However, a question of paramountimportance is somewhat unanswered in deep learning research - is the selectedCNN optimal for the dataset in terms of accuracy and model size? In this paper,we intend to answer this question and introduce a novel strategy that altersthe architecture of a given CNN for a specified dataset, to potentially enhancethe original accuracy while possibly reducing the model size. We use twooperations for architecture refinement, viz. stretching and symmetricalsplitting. Our procedure starts with a pre-trained CNN for a given dataset, andoptimally decides the stretch and split factors across the network to refinethe architecture. We empirically demonstrate the necessity of the twooperations. We evaluate our approach on two natural scenes attributes datasets,SUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, thatare quite contrasting in their construction. We justify our choice of datasets,and show that they are interestingly distinct from each other, and togetherpose a challenge to our architectural refinement algorithm. Our resultssubstantiate the usefulness of the proposed method.
arxiv-17400-27 | A Probabilistic $\ell_1$ Method for Clustering High Dimensional Data | http://arxiv.org/pdf/1504.01294v2.pdf | author:Tsvetan Asamov, Adi Ben-Israel category:math.ST cs.LG math.OC stat.ML stat.TH published:2015-04-06 summary:In general, the clustering problem is NP-hard, and global optimality cannotbe established for non-trivial instances. For high-dimensional data,distance-based methods for clustering or classification face an additionaldifficulty, the unreliability of distances in very high-dimensional spaces. Wepropose a distance-based iterative method for clustering data in veryhigh-dimensional space, using the $\ell_1$-metric that is less sensitive tohigh dimensionality than the Euclidean distance. For $K$ clusters in$\mathbb{R}^n$, the problem decomposes to $K$ problems coupled byprobabilities, and an iteration reduces to finding $Kn$ weighted medians ofpoints on a line. The complexity of the algorithm is linear in the dimension ofthe data space, and its performance was observed to improve significantly asthe dimension increases.
arxiv-17400-28 | Practical Algorithms for Learning Near-Isometric Linear Embeddings | http://arxiv.org/pdf/1601.00062v2.pdf | author:Jerry Luo, Kayla Shapiro, Hao-Jun Michael Shi, Qi Yang, Kan Zhu category:stat.ML cs.LG math.OC 90C90 published:2016-01-01 summary:We propose two practical non-convex approaches for learning near-isometric,linear embeddings of finite sets of data points. Given a set of training points$\mathcal{X}$, we consider the secant set $S(\mathcal{X})$ that consists of allpairwise difference vectors of $\mathcal{X}$, normalized to lie on the unitsphere. The problem can be formulated as finding a symmetric and positivesemi-definite matrix $\boldsymbol{\Psi}$ that preserves the norms of all thevectors in $S(\mathcal{X})$ up to a distortion parameter $\delta$. Motivated bynon-negative matrix factorization, we reformulate our problem into a Frobeniusnorm minimization problem, which is solved by the Alternating Direction Methodof Multipliers (ADMM) and develop an algorithm, FroMax. Another method solvesfor a projection matrix $\boldsymbol{\Psi}$ by minimizing the restrictedisometry property (RIP) directly over the set of symmetric, postivesemi-definite matrices. Applying ADMM and a Moreau decomposition on a proximalmapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.FroMax is shown to converge faster for smaller $\delta$ while NILE-Proconverges faster for larger $\delta$. Both non-convex approaches are thenempirically demonstrated to be more computationally efficient than prior convexapproaches for a number of applications in machine learning and signalprocessing.
arxiv-17400-29 | Non-convex Global Minimization and False Discovery Rate Control for the TREX | http://arxiv.org/pdf/1604.06815v1.pdf | author:Jacob Bien, Irina Gaynanova, Johannes Lederer, Christian Müller category:stat.ML cs.OH stat.CO stat.ME published:2016-04-22 summary:The TREX is a recently introduced method for performing sparsehigh-dimensional regression. Despite its statistical promise as an alternativeto the lasso, square-root lasso, and scaled lasso, the TREX is computationallychallenging in that it requires solving a non-convex optimization problem. Thispaper shows a remarkable result: despite the non-convexity of the TREX problem,there exists a polynomial-time algorithm that is guaranteed to find the globalminimum. This result adds the TREX to a very short list of non-convexoptimization problems that can be globally optimized (principal componentsanalysis being a famous example). After deriving and developing this newapproach, we demonstrate that (i) the ability of the TREX heuristic to reachthe global minimum is strongly dependent on the difficulty of the underlyingstatistical problem, (ii) the polynomial-time algorithm for TREX permits anovel variable ranking and selection scheme, (iii) this scheme can beincorporated into a rule that controls the false discovery rate (FDR) ofincluded features in the model. To achieve this last aim, we provide anextension of the results of Barber & Candes (2015) to establish that theknockoff filter framework can be applied to the TREX. This investigation thusprovides both a rare case study of a heuristic for non-convex optimization anda novel way of exploiting non-convexity for statistical inference.
arxiv-17400-30 | Training Deep Nets with Sublinear Memory Cost | http://arxiv.org/pdf/1604.06174v2.pdf | author:Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin category:cs.LG published:2016-04-21 summary:We propose a systematic approach to reduce the memory consumption of deepneural network training. Specifically, we design an algorithm that costsO(sqrt(n)) memory to train a n layer network, with only the computational costof an extra forward pass per mini-batch. As many of the state-of-the-art modelshit the upper bound of the GPU memory, our algorithm allows deeper and morecomplex models to be explored, and helps advance the innovations in deeplearning research. We focus on reducing the memory cost to store theintermediate feature maps and gradients during training. Computation graphanalysis is used for automatic in-place operation and memory sharingoptimizations. We show that it is possible to trade computation for memory -giving a more memory efficient training algorithm with a little extracomputation cost. In the extreme case, our analysis also shows that the memoryconsumption can be reduced to O(log n) with as little as O(n log n) extra costfor forward computation. Our experiments show that we can reduce the memorycost of a 1,000-layer deep residual network from 48G to 7G with only 30 percentadditional running time cost on ImageNet problems. Similarly, significantmemory cost reduction is observed in training complex recurrent neural networkson very long sequences.
arxiv-17400-31 | Handwritten Digits Recognition using Deep Convolutional Neural Network: An Experimental Study using EBlearn | http://arxiv.org/pdf/1307.3782v3.pdf | author:Karim M. Mahmoud category:cs.NE cs.CV published:2013-07-14 summary:In this paper, results of an experimental study of a deep convolution neuralnetwork architecture which can classify different handwritten digits usingEBLearn library are reported. The purpose of this neural network is to classifyinput images into 10 different classes or digits (0-9) and to explore newfindings. The input dataset used consists of digits images of size 32X32 ingrayscale (MNIST dataset).
arxiv-17400-32 | evt_MNIST: A spike based version of traditional MNIST | http://arxiv.org/pdf/1604.06751v1.pdf | author:Mazdak Fatahi, Mahmood Ahmadi, Mahyar Shahsavari, Arash Ahmadi, Philippe Devienne category:cs.NE published:2016-04-22 summary:Benchmarks and datasets have important role in evaluation of machine learningalgorithms and neural network implementations. Traditional dataset for imagessuch as MNIST is applied to evaluate efficiency of different trainingalgorithms in neural networks. This demand is different in Spiking NeuralNetworks (SNN) as they require spiking inputs. It is widely believed, in thebiological cortex the timing of spikes is irregular. Poisson distributionsprovide adequate descriptions of the irregularity in generating appropriatespikes. Here, we introduce a spike-based version of MNSIT (handwritten digitsdataset),using Poisson distribution and show the Poissonian property of thegenerated streams. We introduce a new version of evt_MNIST which can be usedfor neural network evaluation.
arxiv-17400-33 | Learning a Tree-Structured Ising Model in Order to Make Predictions | http://arxiv.org/pdf/1604.06749v1.pdf | author:Guy Bresler, Mina Karzand category:math.ST cs.IT math.IT math.PR stat.ML stat.TH published:2016-04-22 summary:We study the problem of learning a tree graphical model from samples suchthat low-order marginals are accurate. We define a distance ("small set TV" orssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets$\mathcal{S}$ of a given size, of the total variation between the marginals ofP and Q on $\mathcal{S}$. Approximating a distribution to within small ssTVallows making predictions based on partial observations. Focusing on pairwisemarginals and tree-structured Ising models on $p$ nodes with maximum edgestrength $\beta$, we prove that $\max\{e^{2\beta}, \eta^{-2}\} \log p$ i.i.d.samples suffices to get a distribution (from the same class) with ssTV at most$\eta$ from the one generating the samples.
arxiv-17400-34 | Latent Contextual Bandits and their Application to Personalized Recommendations for New Users | http://arxiv.org/pdf/1604.06743v1.pdf | author:Li Zhou, Emma Brunskill category:cs.LG cs.AI published:2016-04-22 summary:Personalized recommendations for new users, also known as the cold-startproblem, can be formulated as a contextual bandit problem. Existing contextualbandit algorithms generally rely on features alone to capture user variability.Such methods are inefficient in learning new users' interests. In this paper wepropose Latent Contextual Bandits. We consider both the benefit of leveraging aset of learned latent user classes for new users, and how we can learn suchlatent classes from prior users. We show that our approach achieves a betterregret bound than existing algorithms. We also demonstrate the benefit of ourapproach using a large real world dataset and a preliminary user study.
arxiv-17400-35 | Entity Embeddings of Categorical Variables | http://arxiv.org/pdf/1604.06737v1.pdf | author:Cheng Guo, Felix Berkhahn category:cs.LG published:2016-04-22 summary:We map categorical variables in a function approximation problem intoEuclidean spaces, which are the entity embeddings of the categorical variables.The mapping is learned by a neural network during the standard supervisedtraining process. Entity embedding not only reduces memory usage and speeds upneural networks compared with one-hot encoding, but more importantly by mappingsimilar values close to each other in the embedding space it reveals theintrinsic properties of the categorical variables. We applied it successfullyin a recent Kaggle competition and were able to reach the third position withrelative simple features. We further demonstrate in this paper that entityembedding helps the neural network to generalize better when the data is sparseand statistics is unknown. Thus it is especially useful for datasets with lotsof high cardinality features, where other methods tend to overfit. We alsodemonstrate that the embeddings obtained from the trained neural network boostthe performance of all tested machine learning methods considerably when usedas the input features instead. As entity embedding defines a distance measurefor categorical variables it can be used for visualizing categorical data andfor data clustering.
arxiv-17400-36 | Developing an ICU scoring system with interaction terms using a genetic algorithm | http://arxiv.org/pdf/1604.06730v1.pdf | author:Chee Chun Gan, Gerard Learmonth category:cs.NE cs.LG stat.ML published:2016-04-22 summary:ICU mortality scoring systems attempt to predict patient mortality usingpredictive models with various clinical predictors. Examples of such systemsare APACHE, SAPS and MPM. However, most such scoring systems do not activelylook for and include interaction terms, despite physicians intuitively takingsuch interactions into account when making a diagnosis. One barrier toincluding such terms in predictive models is the difficulty of using mostvariable selection methods in high-dimensional datasets. A genetic algorithmframework for variable selection with logistic regression models is used tosearch for two-way interaction terms in a clinical dataset of adult ICUpatients, with separate models being built for each category of diagnosis uponadmittance to the ICU. The models had good discrimination across allcategories, with a weighted average AUC of 0.84 (>0.90 for several categories)and the genetic algorithm was able to find several significant interactionterms, which may be able to provide greater insight into mortality predictionfor health practitioners. The GA selected models had improved performanceagainst stepwise selection and random forest models, and provides greaterflexibility in terms of variable selection by being able to optimize over anymodeler-defined model performance metric instead of a specific variableimportance metric.
arxiv-17400-37 | An improved chromosome formulation for genetic algorithms applied to variable selection with the inclusion of interaction terms | http://arxiv.org/pdf/1604.06727v1.pdf | author:Chee Chun Gan, Gerard Learmonth category:stat.ML cs.NE published:2016-04-22 summary:Genetic algorithms are a well-known method for tackling the problem ofvariable selection. As they are non-parametric and can use a large variety offitness functions, they are well-suited as a variable selection wrapper thatcan be applied to many different models. In almost all cases, the chromosomeformulation used in these genetic algorithms consists of a binary vector oflength n for n potential variables indicating the presence or absence of thecorresponding variables. While the aforementioned chromosome formulation hasexhibited good performance for relatively small n, there are potential problemswhen the size of n grows very large, especially when interaction terms areconsidered. We introduce a modification to the standard chromosome formulationthat allows for better scalability and model sparsity when interaction termsare included in the predictor search space. Experimental results show that theindexed chromosome formulation demonstrates improved computational efficiencyand sparsity on high-dimensional datasets with interaction terms compared tothe standard chromosome formulation.
arxiv-17400-38 | Exploiting Deep Semantics and Compositionality of Natural Language for Human-Robot-Interaction | http://arxiv.org/pdf/1604.06721v1.pdf | author:Manfred Eppe, Sean Trott, Jerome Feldman category:cs.AI cs.CL cs.RO published:2016-04-22 summary:We develop a natural language interface for human robot interaction thatimplements reasoning about deep semantics in natural language. To realize therequired deep analysis, we employ methods from cognitive linguistics, namelythe modular and compositional framework of Embodied Construction Grammar (ECG)[Feldman, 2009]. Using ECG, robots are able to solve fine-grained referenceresolution problems and other issues related to deep semantics andcompositionality of natural language. This also includes verbal interactionwith humans to clarify commands and queries that are too ambiguous to beexecuted safely. We implement our NLU framework as a ROS package and presentproof-of-concept scenarios with different robots, as well as a survey on thestate of the art.
arxiv-17400-39 | Learning rotation invariant convolutional filters for texture classification | http://arxiv.org/pdf/1604.06720v1.pdf | author:Diego Marcos, Michele Volpi, Devis Tuia category:cs.CV published:2016-04-22 summary:We present a method for learning discriminative steerable filters using ashallow Convolutional Neural Network (CNN). We encode rotation invariancedirectly in the model by tying the weights of groups of filters to severalrotated versions of the canonical filter in the group. These filters can beused to extract rotation invariant features well-suited for imageclassification. We test this learning procedure on a texture classificationbenchmark, where the orientations of the training images differ from those ofthe test images. We obtain results comparable to or better than thestate-of-the-art. Besides numerical advantages, our proposed rotation invariantCNN decreases the number of parameters to be learned, thus showing morerobustness in small training set scenarios than a standard CNN.
arxiv-17400-40 | Multiscale Segmentation via Bregman Distances and Nonlinear Spectral Analysis | http://arxiv.org/pdf/1604.06665v1.pdf | author:Leonie Zeune, Guus van Dalum, Leon W. M. M. Terstappen, S. A. van Gils, Christoph Brune category:math.NA cs.CV math.SP published:2016-04-22 summary:In biomedical imaging reliable segmentation of objects (e.g. from small cellsup to large organs) is of fundamental importance for automated medicaldiagnosis. New approaches for multi-scale segmentation can considerably improveperformance in case of natural variations in intensity, size and shape. Thispaper aims at segmenting objects of interest based on shape contours andautomatically finding multiple objects with different scales. The overallstrategy of this work is to combine nonlinear segmentation with scales spacesand spectral decompositions recently introduced in literature. For this wegeneralize a variational segmentation model based on total variation usingBregman distances to construct an inverse scale space. This offers the newmodel to be accomplished by a scale analysis approach based on a spectraldecomposition of the total variation. As a result we obtain a very efficient,(nearly) parameter-free multiscale segmentation method that comes with anadaptive regularization parameter choice. The added benefit of our method isdemonstrated by systematic synthetic tests and its usage in a new biomedicaltoolbox for identifying and classifying circulating tumor cells. Due to thenature of nonlinear diffusion underlying, the mathematical concepts in thiswork offer promising extensions to nonlocal classification problems.
arxiv-17400-41 | Detecting state of aggression in sentences using CNN | http://arxiv.org/pdf/1604.06650v1.pdf | author:Rodmonga Potapova, Denis Gordeev category:cs.CL published:2016-04-22 summary:In this article we study verbal expression of aggression and its detectionusing machine learning and neural networks methods. We test our results usingour corpora of messages from anonymous imageboards. We also compare Randomforest classifier with convolutional neural network for "Movie reviews with onesentence per review" corpus.
arxiv-17400-42 | Automatic verbal aggression detection for Russian and American imageboards | http://arxiv.org/pdf/1604.06648v1.pdf | author:Denis Gordeev category:cs.CL published:2016-04-22 summary:The problem of aggression for Internet communities is rampant. Anonymousforums usually called imageboards are notorious for their aggressive anddeviant behaviour even in comparison with other Internet communities. Thisstudy is aimed at studying ways of automatic detection of verbal expression ofaggression for the most popular American (4chan.org) and Russian (2ch.hk)imageboards. A set of 1,802,789 messages was used for this study. The machinelearning algorithm word2vec was applied to detect the state of aggression. Adecent result is obtained for English (88%), the results for Russian are yet tobe improved.
arxiv-17400-43 | Synthetic Data for Text Localisation in Natural Images | http://arxiv.org/pdf/1604.06646v1.pdf | author:Ankush Gupta, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2016-04-22 summary:In this paper we introduce a new method for text detection in natural images.The method comprises two contributions: First, a fast and scalable engine togenerate synthetic images of text in clutter. This engine overlays synthetictext to existing background images in a natural way, accounting for the local3D scene geometry. Second, we use the synthetic images to train aFully-Convolutional Regression Network (FCRN) which efficiently performs textdetection and bounding-box regression at all locations and multiple scales inan image. We discuss the relation of FCRN to the recently-introduced YOLOdetector, as well as other end-to-end object detection systems based on deeplearning. The resulting detection network significantly out performs currentmethods for text detection in natural images, achieving an F-measure of 84.2%on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images persecond on a GPU.
arxiv-17400-44 | $γ$-regression: Robust and Sparse Regression based on $γ$-divergence | http://arxiv.org/pdf/1604.06637v1.pdf | author:Takayuki Kawashima, Hironori Fujisawa category:stat.ME stat.ML published:2016-04-22 summary:In high-dimensional data, many sparse regression methods have been proposed.However, they may not be robust against outliers. Recently, the use of densitypower weight has been studied for robust parameter estimation and thecorresponding divergences have been discussed. One of such divergences is the$\gamma$-divergence and the robust estimator using the $\gamma$-divergence isknown for having a strong robustness. In this paper, we consider the robust andsparse regression based on $\gamma$-divergence. We extend the$\gamma$-divergence to the regression problem and show that it has a strongrobustness under heavy contamination even when outliers are heterogeneous. Theloss function is constructed by an empirical estimate of the$\gamma$-divergence with sparse regularization and the parameter estimate isdefined as the minimizer of the loss function. To obtain the robust and sparseestimate, we propose an efficient update algorithm which has a monotonedecreasing property of the loss function. Particularly, we discuss a linearregression problem with $L_1$ regularization in detail. In numericalexperiments, we see that the proposed method outperforms past sparse and robustmethods.
arxiv-17400-45 | Bridging LSTM Architecture and the Neural Dynamics during Reading | http://arxiv.org/pdf/1604.06635v1.pdf | author:Peng Qian, Xipeng Qiu, Xuanjing Huang category:cs.CL cs.AI cs.LG cs.NE published:2016-04-22 summary:Recently, the long short-term memory neural network (LSTM) has attracted wideinterest due to its success in many tasks. LSTM architecture consists of amemory cell and three gates, which looks similar to the neuronal networks inthe brain. However, there still lacks the evidence of the cognitiveplausibility of LSTM architecture as well as its working mechanism. In thispaper, we study the cognitive plausibility of LSTM by aligning its internalarchitecture with the brain activity observed via fMRI when the subjects read astory. Experiment results show that the artificial memory vector in LSTM canaccurately predict the observed sequential brain activities, indicating thecorrelation between LSTM architecture and the cognitive process of storyreading.
arxiv-17400-46 | Optimizing top precision performance measure of content-based image retrieval by learning similarity function | http://arxiv.org/pdf/1604.06620v1.pdf | author:Jingbin Wang, Lihui Shi, Haoxiang Wang, Jiandong Meng, Jim Jing-Yan Wang, Qingquan Sun, Yi Gu category:cs.CV published:2016-04-22 summary:In this paper we study the problem of content-based image retrieval. In thisproblem, the most popular performance measure is the top precision measure, andthe most important component of a retrieval system is the similarity functionused to compare a query image against a database image. However, up to now,there is no existing similarity learning method proposed to optimize the topprecision measure. To fill this gap, in this paper, we propose a novelsimilarity learning method to maximize the top precision measure. We model thisproblem as a minimization problem with an objective function as the combinationof the losses of the relevant images ranked behind the top-ranked irrelevantimage, and the squared $\ell_2$ norm of the similarity function parameter. Thisminimization problem is solved as a quadratic programming problem. Theexperiments over two benchmark data sets show the advantages of the proposedmethod over other similarity learning methods when the top precision is used asthe performance measure.
arxiv-17400-47 | Recurrent Memory Networks for Language Modeling | http://arxiv.org/pdf/1601.01272v2.pdf | author:Ke Tran, Arianna Bisazza, Christof Monz category:cs.CL published:2016-01-06 summary:Recurrent Neural Networks (RNN) have obtained excellent result in manynatural language processing (NLP) tasks. However, understanding andinterpreting the source of this success remains a challenge. In this paper, wepropose Recurrent Memory Network (RMN), a novel RNN architecture, that not onlyamplifies the power of RNN but also facilitates our understanding of itsinternal functioning and allows us to discover underlying patterns in data. Wedemonstrate the power of RMN on language modeling and sentence completiontasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)network on three large German, Italian, and English dataset. Additionally weperform in-depth analysis of various linguistic dimensions that RMN captures.On Sentence Completion Challenge, for which it is essential to capture sentencecoherence, our RMN obtains 69.2% accuracy, surpassing the previousstate-of-the-art by a large margin.
arxiv-17400-48 | K-Bit-Swap: A New Operator For Real-Coded Evolutionary Algorithms | http://arxiv.org/pdf/1604.06607v1.pdf | author:Aram Ter-Sarkisov, Stephen Marsland category:cs.NE I.2 published:2016-04-22 summary:There has been a variety of crossover operators proposed for Real-CodedGenetic Algorithms (RCGAs), which recombine values from the same location inpairs of strings. In this article we present a recombination operator for RC-GAs that selects the locations randomly in both parents, and compare it tomainstream crossover operators in a set of experiments on a range of standardmultidimensional optimization problems and a clustering problem. We present twovariants of the operator, either selecting both bits uniformly at random in thestrings, or sampling the second bit from a normal distribution centered at theselected location in the first string. While the operator is biased towardsexploitation of fitness space, the random selection of the second bit for swap-ping makes it slightly less exploitation-biased. Extensive statistical analysisusing a non-parametric test shows the advantage of the new recombinationoperators on a range of test functions.
arxiv-17400-49 | Sentiment Analysis of Twitter Data: A Survey of Techniques | http://arxiv.org/pdf/1601.06971v3.pdf | author:Vishal. A. Kharde, Prof. Sheetal. Sonawane category:cs.CL I.2.7 published:2016-01-26 summary:With the advancement of web technology and its growth, there is a huge volumeof data present in the web for internet users and a lot of data is generatedtoo. Internet has become a platform for online learning, exchanging ideas andsharing opinions. Social networking sites like Twitter, Facebook, Google+ arerapidly gaining popularity as they allow people to share and express theirviews about topics,have discussion with different communities, or post messagesacross the world. There has been lot of work in the field of sentiment analysisof twitter data. This survey focuses mainly on sentiment analysis of twitterdata which is helpful to analyze the information in the tweets where opinionsare highly unstructured, heterogeneous and are either positive or negative, orneutral in some cases. In this paper, we provide a survey and a comparativeanalyses of existing techniques for opinion mining like machine learning andlexicon-based approaches, together with evaluation metrics. Using variousmachine learning algorithms like Naive Bayes, Max Entropy, and Support VectorMachine, we provide a research on twitter data streams.General challenges andapplications of Sentiment Analysis on Twitter are also discussed in this paper.
arxiv-17400-50 | SweLL on the rise: Swedish Learner Language corpus for European Reference Level studies | http://arxiv.org/pdf/1604.06583v1.pdf | author:Elena Volodina, Ildikó Pilán, Ingegerd Enström, Lorena Llozhi, Peter Lundkvist, Gunlög Sundberg, Monica Sandell category:cs.CL published:2016-04-22 summary:We present a new resource for Swedish, SweLL, a corpus of Swedish Learneressays linked to learners' performance according to the Common EuropeanFramework of Reference (CEFR). SweLL consists of three subcorpora - SpIn,SW1203 and Tisus, collected from three different educational establishments.The common metadata for all subcorpora includes age, gender, native languages,time of residence in Sweden, type of written task. Depending on the subcorpus,learner texts may contain additional information, such as text genres, topics,grades. Five of the six CEFR levels are represented in the corpus: A1, A2, B1,B2 and C1 comprising in total 339 essays. C2 level is not included sincecourses at C2 level are not offered. The work flow consists of collection ofessays and permits, essay digitization and registration, meta-data annotation,automatic linguistic annotation. Inter-rater agreement is presented on thebasis of SW1203 subcorpus. The work on SweLL is still ongoing with more than100 essays waiting in the pipeline. This article both describes the resourceand the "how-to" behind the compilation of SweLL.
arxiv-17400-51 | Kernelized Covariance for Action Recognition | http://arxiv.org/pdf/1604.06582v1.pdf | author:Jacopo Cavazza, Andrea Zunino, Marco San Biagio, Vittorio Murino category:cs.CV published:2016-04-22 summary:In this paper we aim at increasing the descriptive power of the covariancematrix, limited in capturing linear mutual dependencies between variables only.We present a rigorous and principled mathematical pipeline to recover thekernel trick for computing the covariance matrix, enhancing it to model morecomplex, non-linear relationships conveyed by the raw data. To this end, wepropose Kernelized-COV, which generalizes the original covariancerepresentation without compromising the efficiency of the computation. In theexperiments, we validate the proposed framework against many previousapproaches in the literature, scoring on par or superior with respect to thestate of the art on benchmark datasets for 3D action recognition.
arxiv-17400-52 | CT-Mapper: Mapping Sparse Multimodal Cellular Trajectories using a Multilayer Transportation Network | http://arxiv.org/pdf/1604.06577v1.pdf | author:Fereshteh Asgari, Alexis Sultan, Haoyi Xiong, Vincent Gauthier, Mounim El-Yacoubi category:cs.SI cs.LG published:2016-04-22 summary:Mobile phone data have recently become an attractive source of informationabout mobility behavior. Since cell phone data can be captured in a passive wayfor a large user population, they can be harnessed to collect well-sampledmobility information. In this paper, we propose CT-Mapper, an unsupervisedalgorithm that enables the mapping of mobile phone traces over a multimodaltransport network. One of the main strengths of CT-Mapper is its capability tomap noisy sparse cellular multimodal trajectories over a multilayertransportation network where the layers have different physical properties andnot only to map trajectories associated with a single layer. Such a network ismodeled by a large multilayer graph in which the nodes correspond tometro/train stations or road intersections and edges correspond to connectionsbetween them. The mapping problem is modeled by an unsupervised HMM where theobservations correspond to sparse user mobile trajectories and the hiddenstates to the multilayer graph nodes. The HMM is unsupervised as the transitionand emission probabilities are inferred using respectively the physicaltransportation properties and the information on the spatial coverage ofantenna base stations. To evaluate CT-Mapper we collected cellular traces withtheir corresponding GPS trajectories for a group of volunteer users in Parisand vicinity (France). We show that CT-Mapper is able to accurately retrievethe real cell phone user paths despite the sparsity of the observed tracetrajectories. Furthermore our transition probability model is up to 20% moreaccurate than other naive models.
arxiv-17400-53 | Convolutional Two-Stream Network Fusion for Video Action Recognition | http://arxiv.org/pdf/1604.06573v1.pdf | author:Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman category:cs.CV published:2016-04-22 summary:Recent applications of Convolutional Neural Networks (ConvNets) for humanaction recognition in videos have proposed different solutions forincorporating the appearance and motion information. We study a number of waysof fusing ConvNet towers both spatially and temporally in order to best takeadvantage of this spatio-temporal information. We make the following findings:(i) that rather than fusing at the softmax layer, a spatial and temporalnetwork can be fused at a convolution layer without loss of performance, butwith a substantial saving in parameters; (ii) that it is better to fuse suchnetworks spatially at the last convolutional layer than earlier, and thatadditionally fusing at the class prediction layer can boost accuracy; finally(iii) that pooling of abstract convolutional features over spatiotemporalneighbourhoods further boosts performance. Based on these studies we propose anew ConvNet architecture for spatiotemporal fusion of video snippets, andevaluate its performance on standard benchmarks where this architectureachieves state-of-the-art results.
arxiv-17400-54 | A Classifier-guided Approach for Top-down Salient Object Detection | http://arxiv.org/pdf/1604.06570v1.pdf | author:Hisham Cholakkal, Jubin Johnson, Deepu Rajan category:cs.CV published:2016-04-22 summary:We propose a framework for top-down salient object detection thatincorporates a tightly coupled image classification module. The classifier istrained on novel category-aware sparse codes computed on object dictionariesused for saliency modeling. A misclassification indicates that thecorresponding saliency model is inaccurate. Hence, the classifier selectsimages for which the saliency models need to be updated. The category-awaresparse coding produces better image classification accuracy as compared toconventional sparse coding with a reduced computational complexity. Asaliency-weighted max-pooling is proposed to improve image classification,which is further used to refine the saliency maps. Experimental results onGraz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salientobject detection. Although the role of the classifier is to support salientobject detection, we evaluate its performance in image classification and alsoillustrate the utility of thresholded saliency maps for image segmentation.
arxiv-17400-55 | Car Type Recognition with Deep Neural Networks | http://arxiv.org/pdf/1602.07125v2.pdf | author:Heikki Huttunen, Fatemeh Shokrollahi Yancheshmeh, Ke Chen category:cs.CV published:2016-02-23 summary:In this paper we study automatic recognition of cars of four types: Bus,Truck, Van and Small car. For this problem we consider two data drivenframeworks: a deep neural network and a support vector machine using SIFTfeatures. The accuracy of the methods is validated with a database of over 6500images, and the resulting prediction accuracy is over 97 %. This clearlyexceeds the accuracies of earlier studies that use manually engineered featureextraction pipelines.
arxiv-17400-56 | Neural Generative Question Answering | http://arxiv.org/pdf/1512.01337v4.pdf | author:Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, Xiaoming Li category:cs.CL published:2015-12-04 summary:This paper presents an end-to-end neural network model, named NeuralGenerative Question Answering (GENQA), that can generate answers to simplefactoid questions, based on the facts in a knowledge-base. More specifically,the model is built on the encoder-decoder framework for sequence-to-sequencelearning, while equipped with the ability to enquire the knowledge-base, and istrained on a corpus of question-answer pairs, with their associated triples inthe knowledge-base. Empirical study shows the proposed model can effectivelydeal with the variations of questions and answers, and generate right andnatural answers by referring to the facts in the knowledge-base. The experimenton question answering demonstrates that the proposed model can outperform anembedding-based QA model as well as a neural dialogue model trained on the samedata.
arxiv-17400-57 | Tracklet Association by Online Target-Specific Metric Learning and Coherent Dynamics Estimation | http://arxiv.org/pdf/1511.06654v2.pdf | author:Bing Wang, Gang Wang, Kap Luk Chan, Li Wang category:cs.CV published:2015-11-20 summary:In this paper, we present a novel method based on online target-specificmetric learning and coherent dynamics estimation for tracklet (track fragment)association by network flow optimization in long-term multi-person tracking.Our proposed framework aims to exploit appearance and motion cues to preventidentity switches during tracking and to recover missed detections.Furthermore, target-specific metrics (appearance cue) and motion dynamics(motion cue) are proposed to be learned and estimated online, i.e. during thetracking process. Our approach is effective even when such cues fail toidentify or follow the target due to occlusions or object-to-objectinteractions. We also propose to learn the weights of these two tracking cuesto handle the difficult situations, such as severe occlusions andobject-to-object interactions effectively. Our method has been validated onseveral public datasets and the experimental results show that it outperformsseveral state-of-the-art tracking methods.
arxiv-17400-58 | Creation of a Deep Convolutional Auto-Encoder in Caffe | http://arxiv.org/pdf/1512.01596v3.pdf | author:Volodymyr Turchenko, Artur Luczak category:cs.NE cs.CV cs.LG 68Txx published:2015-12-04 summary:The development of a deep (stacked) convolutional auto-encoder in the Caffedeep learning framework is presented in this paper. We describe simpleprinciples which we used to create this model in Caffe. The proposed model ofconvolutional auto-encoder does not have pooling/unpooling layers yet. Theresults of our experimental research show comparable accuracy of dimensionalityreduction in comparison with a classic auto-encoder on the example of MNISTdataset.
arxiv-17400-59 | Dependency Parsing with LSTMs: An Empirical Evaluation | http://arxiv.org/pdf/1604.06529v1.pdf | author:Adhiguna Kuncoro, Yuichiro Sawai, Kevin Duh, Yuji Matsumoto category:cs.CL cs.LG cs.NE published:2016-04-22 summary:We propose a transition-based dependency parser using Recurrent NeuralNetworks with Long Short-Term Memory (LSTM) units. This extends the feedforwardneural network parser of Chen and Manning (2014) and enables modelling ofentire sequences of shift/reduce transition decisions. On the Google WebTreebank, our LSTM parser is competitive with the best feedforward parser onoverall accuracy and notably achieves more than 3% improvement for long-rangedependencies, which has proved difficult for previous transition-based parsersdue to error propagation and limited context information. Our findingsadditionally suggest that dropout regularisation on the embedding layer iscrucial to improve the LSTM's generalisation.
arxiv-17400-60 | Variational inference for rare variant detection in deep, heterogeneous next-generation sequencing data | http://arxiv.org/pdf/1604.04280v2.pdf | author:Fan Zhang, Patrick Flaherty category:q-bio.GN stat.ML published:2016-04-14 summary:The detection of rare variants is important for understanding the geneticheterogeneity in mixed samples. Recently, next-generation sequencing (NGS)technologies have enabled the identification of single nucleotide variants(SNVs) in mixed samples with high resolution. Yet, the noise inherent in thebiological processes involved in next-generation sequencing necessitates theuse of statistical methods to identify true rare variants. We propose a novelBayesian statistical model and a variational expectation-maximization (EM)algorithm to estimate non-reference allele frequency (NRAF) and identify SNVsin heterogeneous cell populations. We demonstrate that our variational EMalgorithm has comparable sensitivity and specificity compared with a MarkovChain Monte Carlo (MCMC) sampling inference algorithm, and is morecomputationally efficient on tests of low coverage ($27\times$ and $298\times$)data. Furthermore, we show that our model with a variational EM inferencealgorithm has higher specificity than many state-of-the-art algorithms. In ananalysis of a directed evolution longitudinal yeast data set, we are able toidentify a time-series trend in non-reference allele frequency and detect novelvariants that have not yet been reported. Our model also detects the emergenceof a beneficial variant earlier than was previously shown, and a pair ofconcomitant variants.
arxiv-17400-61 | Opt: A Domain Specific Language for Non-linear Least Squares Optimization in Graphics and Imaging | http://arxiv.org/pdf/1604.06525v1.pdf | author:Zachary DeVito, Michael Mara, Michael Zollhöfer, Gilbert Bernstein, Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher, Matthias Nießner category:cs.GR cs.CV cs.PL published:2016-04-22 summary:Many graphics and vision problems are naturally expressed as optimizationswith either linear or non-linear least squares objective functions over visualdata, such as images and meshes. The mathematical descriptions of thesefunctions are extremely concise, but their implementation in real code istedious, especially when optimized for real-time performance in interactiveapplications. We propose a new language, Opt (available underhttp://optlang.org), in which a user simply writes energy functions over image-or graph-structured unknowns, and a compiler automatically generatesstate-of-the-art GPU optimization kernels. The end result is a system in whichreal-world energy functions in graphics and vision applications are expressiblein tens of lines of code. They compile directly into highly-optimized GPUsolver implementations with performance competitive with the best publishedhand-tuned, application-specific GPU solvers, and 1-2 orders of magnitudebeyond a general-purpose auto-generated solver.
arxiv-17400-62 | Improving Human Action Recognition by Non-action Classification | http://arxiv.org/pdf/1604.06397v2.pdf | author:Yang Wang, Minh Hoai category:cs.CV published:2016-04-21 summary:In this paper we consider the task of recognizing human actions in realisticvideo where human actions are dominated by irrelevant factors. We first studythe benefits of removing non-action video segments, which are the ones that donot portray any human action. We then learn a non-action classifier and use itto down-weight irrelevant video segments. The non-action classifier is trainedusing ActionThread, a dataset with shot-level annotation for the occurrence orabsence of a human action. The non-action classifier can be used to identifynon-action shots with high precision and subsequently used to improve theperformance of action recognition systems.
arxiv-17400-63 | Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation | http://arxiv.org/pdf/1512.04650v2.pdf | author:Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu category:cs.CL published:2015-12-15 summary:The attentional mechanism has proven to be effective in improving end-to-endneural machine translation. However, due to the intricate structural divergencebetween natural languages, unidirectional attention-based models might onlycapture partial aspects of attentional regularities. We propose agreement-basedjoint training for bidirectional attention-based end-to-end neural machinetranslation. Instead of training source-to-target and target-to-sourcetranslation models independently,our approach encourages the two complementarymodels to agree on word alignment matrices on the same training data.Experiments on Chinese-English and English-French translation tasks show thatagreement-based joint training significantly improves both alignment andtranslation quality over independent training.
arxiv-17400-64 | Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs | http://arxiv.org/pdf/1601.02129v2.pdf | author:Zheng Shou, Dongang Wang, Shih-Fu Chang category:cs.CV published:2016-01-09 summary:We address temporal action localization in untrimmed long videos. This isimportant because videos in real applications are usually unconstrained andcontain multiple action instances plus video content of background scenes orother activities. To address this challenging issue, we exploit theeffectiveness of deep networks in temporal action localization via threesegment-based 3D ConvNets: (1) a proposal network identifies candidate segmentsin a long video that may contain actions; (2) a classification network learnsone-vs-all action classification model to serve as initialization for thelocalization network; and (3) a localization network fine-tunes on the learnedclassification network to localize each action instance. We propose a novelloss function for the localization network to explicitly consider temporaloverlap and therefore achieve high temporal localization accuracy. Only theproposal network and the localization network are used during prediction. Ontwo large-scale benchmarks, our approach achieves significantly superiorperformances compared with other state-of-the-art systems: mAP increases from1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014,when the overlap threshold for evaluation is set to 0.5.
arxiv-17400-65 | Zero-Shot Learning via Joint Latent Similarity Embedding | http://arxiv.org/pdf/1511.04512v2.pdf | author:Ziming Zhang, Venkatesh Saligrama category:cs.CV published:2015-11-14 summary:Zero-shot recognition (ZSR) deals with the problem of predicting class labelsfor target domain instances based on source domain side information (e.g.attributes) of unseen classes. We formulate ZSR as a binary prediction problem.Our resulting classifier is class-independent. It takes an arbitrary pair ofsource and target domain instances as input and predicts whether or not theycome from the same class, i.e. whether there is a match. We model the posteriorprobability of a match since it is a sufficient statistic and propose a latentprobabilistic model in this context. We develop a joint discriminative learningframework based on dictionary learning to jointly learn the parameters of ourmodel for both domains, which ultimately leads to our class-independentclassifier. Many of the existing embedding methods can be viewed as specialcases of our probabilistic model. On ZSR our method shows 4.90\% improvementover the state-of-the-art in accuracy averaged across four benchmark datasets.We also adapt ZSR method for zero-shot retrieval and show 22.45\% improvementaccordingly in mean average precision (mAP).
arxiv-17400-66 | Online Action Detection | http://arxiv.org/pdf/1604.06506v1.pdf | author:Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees Snoek, Tinne Tuytelaars category:cs.CV published:2016-04-21 summary:In online action detection, the goal is to detect the start of an action in avideo stream as soon as it happens. For instance, if a child is chasing a ball,an autonomous car should recognize what is going on and respond immediately.This is a very challenging problem for four reasons. First, only partialactions are observed. Second, there is a large variability in negative data.Third, the start of the action is unknown, so it is unclear over what timewindow the information should be integrated. Finally, in real world data, largewithin-class variability exists. This problem has been addressed before, butonly to some extent. Our contributions to online action detection are threefold. First, weintroduce a realistic dataset composed of 27 episodes from 6 popular TV series.The dataset spans over 16 hours of footage annotated with 30 action classes,totaling about 5,000 action instances. Second, we analyze and compare variousbaseline methods, showing this is a challenging problem for which none of themethods provides a good solution. Third, we analyze the change in performancewhen there is a variation in viewpoint, occlusion, truncation, etc. Weintroduce an evaluation protocol for fair comparison. The dataset, thebaselines and the models will all be made publicly available to encouragefurther research on online action detection on realistic data.
arxiv-17400-67 | State of the Art Control of Atari Games Using Shallow Reinforcement Learning | http://arxiv.org/pdf/1512.01563v2.pdf | author:Yitao Liang, Marlos C. Machado, Erik Talvitie, Michael Bowling category:cs.LG published:2015-12-04 summary:The recently introduced Deep Q-Networks (DQN) algorithm has gained attentionas one of the first successful combinations of deep neural networks andreinforcement learning. Its promise was demonstrated in the Arcade LearningEnvironment (ALE), a challenging framework composed of dozens of Atari 2600games used to evaluate general competency in AI. It achieved dramaticallybetter results than earlier approaches, showing that its ability to learn goodrepresentations is quite robust and general. This paper attempts to understandthe principles that underlie DQN's impressive performance and to bettercontextualize its success. We systematically evaluate the importance of keyrepresentational biases encoded by DQN's network by proposing simple linearrepresentations that make use of these concepts. Incorporating thesecharacteristics, we obtain a computationally practical feature set thatachieves competitive performance to DQN in the ALE. Besides offering insightinto the strengths and weaknesses of DQN, we provide a generic representationfor the ALE, significantly reducing the burden of learning a representation foreach game. Moreover, we also provide a simple, reproducible benchmark for thesake of comparison to future work in the ALE.
arxiv-17400-68 | Towards O(1) Seeding of K-Means | http://arxiv.org/pdf/1511.05933v5.pdf | author:Sayantan Dasgupta category:cs.LG published:2015-11-18 summary:K-means is one of the most widely used algorithms for clustering in DataMining applications, which attempts to minimize the sum of Euclidean distanceof the points in the clusters from the respective means of the clusters. Thesimplicity and scalability of K-means makes it very appealing. However, K-meanssuffers from local minima problem, and comes with no guarantee to converge tothe optimal cost. K-means++ tries to address the problem by seeding the meansusing a distance based sampling scheme. However, seeding the means in K-means++needs O(K) passes through the entire dataset, which could be very costly inlarge amount of dataset. Here we propose a method of seeding initial meansbased on higher order moments of the data, which takes O(1) passes through theentire dataset to extract the initial set of means. Our method yieldscompetitive performance with respect to all the existing K-means algorithms,whilst avoiding the expensive mean selection steps of K-means++ and otherheuristics. We demonstrate the performance of our algorithm in comparison withthe existing algorithms on various benchmark datasets.
arxiv-17400-69 | Efficient Training of Very Deep Neural Networks for Supervised Hashing | http://arxiv.org/pdf/1511.04524v2.pdf | author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV cs.LG cs.NE published:2015-11-14 summary:In this paper, we propose training very deep neural networks (DNNs) forsupervised learning of hash codes. Existing methods in this context trainrelatively "shallow" networks limited by the issues arising in back propagation(e.e. vanishing gradients) as well as computational efficiency. We propose anovel and efficient training algorithm inspired by alternating direction methodof multipliers (ADMM) that overcomes some of these limitations. Our methoddecomposes the training process into independent layer-wise local updatesthrough auxiliary variables. Empirically we observe that our training algorithmalways converges and its computational complexity is linearly proportional tothe number of edges in the networks. Empirically we manage to train DNNs with64 hidden layers and 1024 nodes per layer for supervised hashing in about 3hours using a single GPU. Our proposed very deep supervised hashing (VDSH)method significantly outperforms the state-of-the-art on several benchmarkdatasets.
arxiv-17400-70 | Stabilized Sparse Online Learning for Sparse Data | http://arxiv.org/pdf/1604.06498v1.pdf | author:Yuting Ma, Tian Zheng category:stat.ML cs.LG published:2016-04-21 summary:Stochastic gradient descent (SGD) is commonly used for optimization inlarge-scale machine learning problems. Langford et al. (2009) introduce asparse online learning method to induce sparsity via truncated gradient. Withhigh-dimensional sparse data, however, the method suffers from slow convergenceand high variance due to the heterogeneity in feature sparsity. To mitigatethis issue, we introduce a stabilized truncated stochastic gradient descentalgorithm. We employ a soft-thresholding scheme on the weight vector where theimposed shrinkage is adaptive to the amount of information available in eachfeature. The variability in the resulted sparse weight vector is furthercontrolled by stability selection integrated with the informative truncation.To facilitate better convergence, we adopt an annealing strategy on thetruncation rate, which leads to a balanced trade-off between exploration andexploitation in learning a sparse weight vector. Numerical experiments showthat our algorithm compares favorably with the original algorithm in terms ofprediction accuracy, achieved sparsity and stability.
arxiv-17400-71 | Off-the-Grid Recovery of Piecewise Constant Images from Few Fourier Samples | http://arxiv.org/pdf/1510.00384v2.pdf | author:Greg Ongie, Mathews Jacob category:cs.CV published:2015-10-01 summary:We introduce a method to recover a continuous domain representation of apiecewise constant two-dimensional image from few low-pass Fourier samples.Assuming the edge set of the image is localized to the zero set of atrigonometric polynomial, we show the Fourier coefficients of the partialderivatives of the image satisfy a linear annihilation relation. We presentnecessary and sufficient conditions for unique recovery of the image fromfinite low-pass Fourier samples using the annihilation relation. We alsopropose a practical two-stage recovery algorithm which is robust tomodel-mismatch and noise. In the first stage we estimate a continuous domainrepresentation of the edge set of the image. In the second stage we perform anextrapolation in Fourier domain by a least squares two-dimensional linearprediction, which recovers the exact Fourier coefficients of the underlyingimage. We demonstrate our algorithm on the super-resolution recovery of MRIphantoms and real MRI data from low-pass Fourier samples, which shows benefitsover standard approaches for single-image super-resolution MRI.
arxiv-17400-72 | Humans and deep networks largely agree on which kinds of variation make object recognition harder | http://arxiv.org/pdf/1604.06486v1.pdf | author:Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, Timothée Masquelier category:cs.CV q-bio.NC published:2016-04-21 summary:View-invariant object recognition is a challenging problem, which hasattracted much attention among the psychology, neuroscience, and computervision communities. Humans are notoriously good at it, even if some variationsare presumably more difficult to handle than others (e.g. 3D rotations). Humansare thought to solve the problem through hierarchical processing along theventral stream, which progressively extracts more and more invariant visualfeatures. This feed-forward architecture has inspired a new generation ofbio-inspired computer vision systems called deep convolutional neural networks(DCNN), which are currently the best algorithms for object recognition innatural images. Here, for the first time, we systematically compared humanfeed-forward vision and DCNNs at view-invariant object recognition using thesame images and controlling for both the kinds of transformation as well astheir magnitude. We used four object categories and images were rendered from3D computer models. In total, 89 human subjects participated in 10 experimentsin which they had to discriminate between two or four categories after rapidpresentation with backward masking. We also tested two recent DCNNs on the sametasks. We found that humans and DCNNs largely agreed on the relativedifficulties of each kind of variation: rotation in depth is by far the hardesttransformation to handle, followed by scale, then rotation in plane, andfinally position. This suggests that humans recognize objects mainly through 2Dtemplate matching, rather than by constructing 3D object models, and that DCNNsare not too unreasonable models of human feed-forward vision. Also, our resultsshow that the variation levels in rotation in depth and scale strongly modulateboth humans' and DCNNs' recognition performances. We thus argue that thesevariations should be controlled in the image datasets used in vision research.
arxiv-17400-73 | Understanding How Image Quality Affects Deep Neural Networks | http://arxiv.org/pdf/1604.04004v2.pdf | author:Samuel Dodge, Lina Karam category:cs.CV published:2016-04-14 summary:Image quality is an important practical challenge that is often overlooked inthe design of machine vision systems. Commonly, machine vision systems aretrained and tested on high quality image datasets, yet in practicalapplications the input images can not be assumed to be of high quality.Recently, deep neural networks have obtained state-of-the-art performance onmany machine vision tasks. In this paper we provide an evaluation of 4state-of-the-art deep neural network models for image classification underquality distortions. We consider five types of quality distortions: blur,noise, contrast, JPEG, and JPEG2000 compression. We show that the existingnetworks are susceptible to these quality distortions, particularly to blur andnoise. These results enable future work in developing deep neural networks thatare more invariant to quality distortions.
arxiv-17400-74 | Visual Congruent Ads for Image Search | http://arxiv.org/pdf/1604.06481v1.pdf | author:Yannis Kalantidis, Ayman Farahat, Lyndon Kennedy, Ricardo Baeza-Yates, David A. Shamma category:cs.CV cs.HC published:2016-04-21 summary:The quality of user experience online is affected by the relevance andplacement of advertisements. We propose a new system for selecting anddisplaying visual advertisements in image search result sets. Our methodcompares the visual similarity of candidate ads to the image search results andselects the most visually similar ad to be displayed. The method furtherselects an appropriate location in the displayed image grid to minimize theperceptual visual differences between the ad and its neighbors. We conduct anexperiment with about 900 users and find that our proposed method providessignificant improvement in the users' overall satisfaction with the imagesearch experience, without diminishing the users' ability to see the ad orrecall the advertised brand.
arxiv-17400-75 | LOH and behold: Web-scale visual search, recommendation and clustering using Locally Optimized Hashing | http://arxiv.org/pdf/1604.06480v1.pdf | author:Yannis Kalantidis, Lyndon Kennedy, Huy Nguyen, Clayton Mellina, David A. Shamma category:cs.CV cs.IR cs.MM published:2016-04-21 summary:We present a multimedia system based on a novel matching signature able toperform de-duplucation, search, clustering and visual recommendations in a waythat is easily implemented in generic distributed computing environments.Starting from a state-of-the-art algorithm, we propose a novel hashing-basedmatching system that allow for fast search and is easily implemented indistributed system languages like PIG, as it only requires set intersectionsand summations to compute. We make the following contributions: a) we propose anovel hashing method for visual search using locally optimized codes thatperforms on-par with other state-of-the-art hashing approaches but offers moreflexibility in terms of ranking, b) we extend our matching framework tomultiple image queries and provide a simple and scalable solution that canefficiently produce visual recommendations for query sets of thousands ofimages and cluster collections of hundreds of millions of images, c) we showthat this same representation can be used for efficient de-duplication of imagesearch results, performing better than traditional hashing approaches, whilestill requiring only a few milliseconds to run. In this paper we displayresults on datasets of up to 100 Million images, but in practice our system canfind and rank similar images for millions of users from a search set ofhundreds of millions of images in a runtime on the order of one hour on a largeHadoop cluster.
arxiv-17400-76 | On Detection and Structural Reconstruction of Small-World Random Networks | http://arxiv.org/pdf/1604.06474v1.pdf | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST cs.LG stat.TH published:2016-04-21 summary:In this paper, we study detection and fast reconstruction of the celebratedWatts-Strogatz (WS) small-world random graph model \citep{watts1998collective}which aims to describe real-world complex networks that exhibit both highclustering and short average length properties. The WS model with neighborhoodsize $k$ and rewiring probability probability $\beta$ can be viewed as acontinuous interpolation between a deterministic ring lattice graph and theErd\H{o}s-R\'{e}nyi random graph. We study both the computational andstatistical aspects of detecting the deterministic ring lattice structure (orlocal geographical links, strong ties) in the presence of random connections(or long range links, weak ties), and for its recovery. The phase diagram interms of $(k,\beta)$ is partitioned into several regions according to thedifficulty of the problem. We propose distinct methods for the various regions.
arxiv-17400-77 | Robust Estimators in High Dimensions without the Computational Intractability | http://arxiv.org/pdf/1604.06443v1.pdf | author:Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, Alistair Stewart category:cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2016-04-21 summary:We study high-dimensional distribution learning in an agnostic setting wherean adversary is allowed to arbitrarily corrupt an $\varepsilon$-fraction of thesamples. Such questions have a rich history spanning statistics, machinelearning and theoretical computer science. Even in the most basic settings, theonly known approaches are either computationally inefficient or losedimension-dependent factors in their error guarantees. This raises thefollowing question:Is high-dimensional agnostic distribution learning evenpossible, algorithmically? In this work, we obtain the first computationally efficient algorithms withdimension-independent error guarantees for agnostically learning severalfundamental classes of high-dimensional distributions: (1) a single Gaussian,(2) a product distribution on the hypercube, (3) mixtures of two productdistributions (under a natural balancedness condition), and (4) mixtures ofspherical Gaussians. Our algorithms achieve error that is independent of thedimension, and in many cases scales nearly-linearly with the fraction ofadversarially corrupted samples. Moreover, we develop a general recipe fordetecting and correcting corruptions in high-dimensions, that may be applicableto many other problems.
arxiv-17400-78 | Analysis of the Entropy-guided Switching Trimmed Mean Deviation-based Anisotropic Diffusion filter | http://arxiv.org/pdf/1604.06427v1.pdf | author:Uche A. Nnolim category:cs.CV published:2016-04-21 summary:This report describes the experimental analysis of a proposed switchingfilter-anisotropic diffusion hybrid for the filtering of the fixed value (saltand pepper) impulse noise (FVIN). The filter works well at both low and highnoise densities though it was specifically designed for high noise densitylevels. The filter combines the switching mechanism of decision-based filtersand the partial differential equation-based formulation to yield a powerfulsystem capable of recovering the image signals at very high noise levels.Experimental results indicate that the filter surpasses other filters,especially at very high noise levels. Additionally, its adaptive nature ensuresthat the performance is guided by the metrics obtained from the noisy inputimage. The filter algorithm is of both global and local nature, where theformer is chosen to reduce computation time and complexity, while the latter isused for best results.
arxiv-17400-79 | Stationary signal processing on graphs | http://arxiv.org/pdf/1601.02522v3.pdf | author:Nathanaël Perraudin, Pierre Vandergheynst category:cs.DS stat.AP stat.ML published:2016-01-11 summary:Graphs are a central tool in machine learning and information processing asthey allow to conveniently capture the structure of complex datasets. In thiscontext, it is of high importance to develop flexible models of signals definedover graphs or networks. In this paper, we generalize the traditional conceptof wide sense stationarity to signals defined over the vertices of arbitraryweighted undirected graphs. We show that stationarity is intimately linked tostatistical invariance under a localization operator reminiscent oftranslation. We prove that stationary graph signals are characterized by awell-defined Power Spectral Density that can be efficiently estimated even forlarge graphs. We leverage this new concept to derive Wiener-type estimationprocedures of noisy and partially observed signals and illustrate theperformance of this new model for denoising and regression.
arxiv-17400-80 | Learning Human Identity from Motion Patterns | http://arxiv.org/pdf/1511.03908v4.pdf | author:Natalia Neverova, Christian Wolf, Griffin Lacey, Lex Fridman, Deepak Chandra, Brandon Barbello, Graham Taylor category:cs.LG cs.CV cs.NE published:2015-11-12 summary:We present a large-scale study exploring the capability of temporal deepneural networks to interpret natural human kinematics and introduce the firstmethod for active biometric authentication with mobile inertial sensors. AtGoogle, we have created a first-of-its-kind dataset of human movements,passively collected by 1500 volunteers using their smartphones daily overseveral months. We (1) compare several neural architectures for efficientlearning of temporal multi-modal data representations, (2) propose an optimizedshift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate thediscriminatively-trained dynamic features in a probabilistic generativeframework taking into account temporal characteristics. Our results demonstratethat human kinematics convey important information about user identity and canserve as a valuable component of multi-modal authentication systems.
arxiv-17400-81 | Row-less Universal Schema | http://arxiv.org/pdf/1604.06361v1.pdf | author:Patrick Verga, Andrew McCallum category:cs.CL published:2016-04-21 summary:Universal schema jointly embeds knowledge bases and textual patterns toreason about entities and relations for automatic knowledge base constructionand information extraction. In the past, entity pairs and relations wererepresented as learned vectors with compatibility determined by a scoringfunction, limiting generalization to unseen text patterns and entities.Recently, 'column-less' versions of Universal Schema have used compositionalpattern encoders to generalize to all text patterns. In this work we take thenext step and propose a 'row-less' model of universal schema, removing explicitentity pair representations. Instead of learning vector representations foreach entity pair in our training set, we treat an entity pair as a function ofits relation types. In experimental results on the FB15k-237 benchmark wedemonstrate that we can match the performance of a comparable model withexplicit entity pair representations using a model of attention over relationtypes. We further demonstrate that the model per- forms with nearly the sameaccuracy on entity pairs never seen during training.
arxiv-17400-82 | Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural Networks | http://arxiv.org/pdf/1604.06338v1.pdf | author:Huy Phan, Lars Hertel, Marco Maass, Alfred Mertins category:cs.NE cs.LG cs.SD published:2016-04-21 summary:We present in this paper a simple, yet efficient convolutional neural network(CNN) architecture for robust audio event recognition. Opposing to deep CNNarchitectures with multiple convolutional and pooling layers topped up withmultiple fully connected layers, the proposed network consists of only threelayers: convolutional, pooling, and softmax layer. It has two features to bedistinguishable from the deep architectures that have been proposed for thetask: varying-size convolutional filters at the convolutional layer and 1-maxpooling scheme at the pooling layer. In intuition, the network tends to selectthe most discriminative features from the whole audio signals for recognition.Our proposed CNN not only shows state-of-the-art performance on the standardtask of robust audio event recognition but also outperforms other deeparchitectures up to 4.5% in terms of recognition accuracy, which is equivalentto 76.3% relative error reduction.
arxiv-17400-83 | Markov models for ocular fixation locations in the presence and absence of colour | http://arxiv.org/pdf/1604.06335v1.pdf | author:Adam B. Kashlak, Eoin Devane, Helge Dietert, Henry Jackson category:stat.AP stat.ML published:2016-04-21 summary:We propose to model the fixation locations of the human eye when observing astill image by a Markovian point process in R 2 . Our approach is data drivenusing k-means clustering of the fixation locations to identify distinct salientregions of the image, which in turn correspond to the states of our Markovchain. Bayes factors are computed as model selection criterion to determine thenumber of clusters. Furthermore, we demonstrate that the behaviour of the humaneye differs from this model when colour information is removed from the givenimage.
arxiv-17400-84 | Non-contact hemodynamic imaging reveals the jugular venous pulse waveform | http://arxiv.org/pdf/1604.05213v2.pdf | author:Robert Amelard, Richard L Hughson, Danielle K Greaves, Kaylen J Pfisterer, Jason Leung, David A Clausi, Alexander Wong category:physics.med-ph cs.CV physics.optics published:2016-04-15 summary:Cardiovascular monitoring is important to prevent diseases from progressing.The jugular venous pulse (JVP) waveform offers important clinical informationabout cardiac health, but is not routinely examined due to its invasivecatheterisation procedure. Here, we demonstrate for the first time that the JVPcan be consistently observed in a non-contact manner using a novel light-basedphotoplethysmographic imaging system, coded hemodynamic imaging (CHI). Whiletraditional monitoring methods measure the JVP at a single location, CHI'swide-field imaging capabilities were able to observe the jugular venous pulse'sspatial flow profile for the first time. The important inflection points in theJVP were observed, meaning that cardiac abnormalities can be assessed throughJVP distortions. CHI provides a new way to assess cardiac health throughnon-contact light-based JVP monitoring, and can be used in non-surgicalenvironments for cardiac assessment.
arxiv-17400-85 | Bayesian Approximate Kernel Regression with Variable Selection | http://arxiv.org/pdf/1508.01217v2.pdf | author:Lorin Crawford, Kris C. Wood, Xiang Zhou, Sayan Mukherjee category:stat.ME q-bio.QM stat.AP stat.ML published:2015-08-05 summary:Nonlinear kernel regression models are often used in statistics and machinelearning due to greater accuracy than linear models. Variable selection forkernel regression models is a challenge partly because, unlike the linearregression setting, there is no clear concept of an effect size for regressioncoefficients. In this paper, we propose a novel framework that provides ananalog of the effect size of each explanatory variable for Bayesian kernelregression models when the kernel is shift-invariant---for example the Gaussiankernel. We use function analytic properties of shift-invariant reproducingkernel Hilbert spaces (RKHS) to define a linear vector space that (1) capturesnonlinear structure and (2) can be projected onto the original explanatoryvariables. The projection onto the original explanatory variables serves as theanalog of effect sizes. The specific function analytic property we use is thatshift-invariant kernel functions can be approximated via random Fourier bases.Based on the random Fourier expansion we propose a computationally efficientclass of Bayesian approximate kernel regression (BAKR) models for bothnonlinear regression and binary classification for which one can compute ananalog of effect sizes. By adapting some classical results in compressivesensing we state conditions under which BAKR can recover a sparse set of effectsizes, simultaneous variable selection and regression. We illustrate theutility of BAKR by examining, in some detail, two important problems instatistical genetics: genomic selection (predicting phenotype from genotype)and association mapping (inference of significant variables or loci).State-of-the-art methods for genomic selection and association mapping arebased on kernel regression and linear models, respectively. BAKR is the firstmethod that is competitive in both settings.
arxiv-17400-86 | TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks | http://arxiv.org/pdf/1604.06318v1.pdf | author:Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, Marc Pollefeys category:cs.CV published:2016-04-21 summary:In this paper we present a deep neural network topology that incorporates asimple to implement transformation invariant pooling operator (TI-POOLING).This operator is able to efficiently handle prior knowledge on nuisancevariations in the data, such as rotation or scale changes. Most current methodsusually make use of dataset augmentation to address this issue, but thisrequires larger number of model parameters and more training data, and resultsin significantly increased training time and larger chance of under- oroverfitting. The main reason for these drawbacks is that the learned modelneeds to capture adequate features for all the possible transformations of theinput. On the other hand, we formulate features in convolutional neuralnetworks to be transformation-invariant. We achieve that using parallel siamesearchitectures for the considered transformation set and applying the TI-POOLINGoperator on their outputs before the fully-connected layers. We show that thistopology internally finds the most optimal "canonical" instance of the inputimage for training and therefore limits the redundancy in learned features.This more efficient use of training data results in better performance onpopular benchmark datasets with smaller number of parameters when comparing tostandard convolutional neural networks with dataset augmentation and to otherbaselines.
arxiv-17400-87 | A Novel Approach to Dropped Pronoun Translation | http://arxiv.org/pdf/1604.06285v1.pdf | author:Longyue Wang, Zhaopeng Tu, Xiaojun Zhang, Hang Li, Andy Way, Qun Liu category:cs.CL published:2016-04-21 summary:Dropped Pronouns (DP) in which pronouns are frequently dropped in the sourcelanguage but should be retained in the target language are challenge in machinetranslation. In response to this problem, we propose a semi-supervised approachto recall possibly missing pronouns in the translation. Firstly, we buildtraining data for DP generation in which the DPs are automatically labelledaccording to the alignment information from a parallel corpus. Secondly, webuild a deep learning-based DP generator for input sentences in decoding whenno corresponding references exist. More specifically, the generation istwo-phase: (1) DP position detection, which is modeled as a sequentiallabelling task with recurrent neural networks; and (2) DP prediction, whichemploys a multilayer perceptron with rich features. Finally, we integrate theabove outputs into our translation system to recall missing pronouns by bothextracting rules from the DP-labelled training data and translating theDP-generated input sentences. Experimental results show that our approachachieves a significant improvement of 1.58 BLEU points in translationperformance with 66% F-score for DP generation accuracy.
arxiv-17400-88 | Chinese Song Iambics Generation with Neural Attention-based Model | http://arxiv.org/pdf/1604.06274v1.pdf | author:Qixin Wang, Tianyi Luo, Dong Wang, Chao Xing category:cs.CL published:2016-04-21 summary:Learning and generating Chinese poems is a charming yet challenging task.Traditional approaches involve various language modeling and machinetranslation techniques, however, they perform not as well when generating poemswith complex pattern constraints, for example Song iambics, a famous type ofpoems that involve variable-length sentences and strict rhythmic patterns. Thispaper applies the attention-based sequence-to-sequence model to generateChinese Song iambics. Specifically, we encode the cue sentences by abi-directional Long-Short Term Memory (LSTM) model and then predict the entireiambic with the information provided by the encoder, in the form of anattention-based LSTM that can regularize the generation process by the finestructure of the input cues. Several techniques are investigated to improve themodel, including global context integration, hybrid style training, charactervector initialization and adaptation. Both the automatic and subjectiveevaluation results show that our model indeed can learn the complex structuraland rhythmic patterns of Song iambics, and the generation is rather successful.
arxiv-17400-89 | Automatic 3D Reconstruction of Manifold Meshes via Delaunay Triangulation and Mesh Sweeping | http://arxiv.org/pdf/1604.06258v1.pdf | author:Andrea Romanoni, Amaël Delaunoy, Marc Pollefeys, Matteo Matteucci category:cs.CV published:2016-04-21 summary:In this paper we propose a new approach to incrementally initialize amanifold surface for automatic 3D reconstruction from images. More precisely wefocus on the automatic initialization of a 3D mesh as close as possible to thefinal solution; indeed many approaches require a good initial solution forfurther refinement via multi-view stereo techniques. Our novel algorithmautomatically estimates an initial manifold mesh for surface evolvingmulti-view stereo algorithms, where the manifold property needs to be enforced.It bootstraps from 3D points extracted via Structure from Motion, then iteratesbetween a state-of-the-art manifold reconstruction step and a novel meshsweeping algorithm that looks for new 3D points in the neighborhood of thereconstructed manifold to be added in the manifold reconstruction. Theexperimental results show quantitatively that the mesh sweeping improves theresolution and the accuracy of the manifold reconstruction, allowing a betterconvergence of state-of-the-art surface evolution multi-view stereo algorithms.
arxiv-17400-90 | Sparse group factor analysis for biclustering of multiple data sources | http://arxiv.org/pdf/1512.08808v2.pdf | author:Kerstin Bunte, Eemeli Leppäaho, Inka Saarinen, Samuel Kaski category:cs.LG cs.IR stat.ML published:2015-12-29 summary:Motivation: Modelling methods that find structure in data are necessary withthe current large volumes of genomic data, and there have been various effortsto find subsets of genes exhibiting consistent patterns over subsets oftreatments. These biclustering techniques have focused on one data source,often gene expression data. We present a Bayesian approach for jointbiclustering of multiple data sources, extending a recent method Group FactorAnalysis (GFA) to have a biclustering interpretation with additional sparsityassumptions. The resulting method enables data-driven detection of linearstructure present in parts of the data sources. Results: Our simulation studiesshow that the proposed method reliably infers bi-clusters from heterogeneousdata sources. We tested the method on data from the NCI-DREAM drug sensitivityprediction challenge, resulting in an excellent prediction accuracy. Moreover,the predictions are based on several biclusters which provide insight into thedata sources, in this case on gene expression, DNA methylation, proteinabundance, exome sequence, functional connectivity fingerprints and drugsensitivity.
arxiv-17400-91 | Evaluation of the Effect of Improper Segmentation on Word Spotting | http://arxiv.org/pdf/1604.06243v1.pdf | author:Sounak Dey, Anguelos Nicolaou, Josep Llados, Umapada Pal category:cs.CV published:2016-04-21 summary:Word spotting is an important recognition task in historical documentanalysis. In most cases methods are developed and evaluated assuming perfectword segmentations. In this paper we propose an experimental framework toquantify the effect of goodness of word segmentation has on the performanceachieved by word spotting methods in identical unbiased conditions. Theframework consists of generating systematic distortions on segmentation andretrieving the original queries from the distorted dataset. We apply theframework on the George Washington and Barcelona Marriage Dataset and onseveral established and state-of-the-art methods. The experiments allow for anestimate of the end-to-end performance of word spotting methods.
arxiv-17400-92 | Integrating Deep Features for Material Recognition | http://arxiv.org/pdf/1511.06522v6.pdf | author:Yan Zhang, Mete Ozay, Xing Liu, Takayuki Okatani category:cs.CV cs.LG published:2015-11-20 summary:We propose a method for integration of features extracted using deeprepresentations of Convolutional Neural Networks (CNNs) each of which islearned using a different image dataset of objects and materials for materialrecognition. Given a set of representations of multiple pre-trained CNNs, wefirst compute activations of features using the representations on the imagesto select a set of samples which are best represented by the features. Then, wemeasure the uncertainty of the features by computing the entropy of classdistributions for each sample set. Finally, we compute the contribution of eachfeature to representation of classes for feature selection and integration. Weexamine the proposed method on three benchmark datasets for materialrecognition. Experimental results show that the proposed method achievesstate-of-the-art performance by integrating deep features. Additionally, weintroduce a new material dataset called EFMD by extending Flickr MaterialDatabase (FMD). By the employment of the EFMD with transfer learning forupdating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMDdataset which is close to human performance that is 84.9%.
arxiv-17400-93 | Local Binary Pattern for Word Spotting in Handwritten Historical Document | http://arxiv.org/pdf/1604.05907v2.pdf | author:Sounak Dey, Anguelos Nicolaou, Josep Llados, Umapada Pal category:cs.CV published:2016-04-20 summary:Digital libraries store images which can be highly degraded and to index thiskind of images we resort to word spot- ting as our information retrievalsystem. Information retrieval for handwritten document images is morechallenging due to the difficulties in complex layout analysis, largevariations of writing styles, and degradation or low quality of historicalmanuscripts. This paper presents a simple innovative learning-free method forword spotting from large scale historical documents combining Local BinaryPattern (LBP) and spatial sampling. This method offers three advantages:firstly, it operates in completely learning free paradigm which is verydifferent from unsupervised learning methods, secondly, the computational timeis significantly low because of the LBP features which are very fast tocompute, and thirdly, the method can be used in scenarios where annotations arenot available. Finally we compare the results of our proposed retrieval methodwith the other methods in the literature.
arxiv-17400-94 | Feature Learning based Deep Supervised Hashing with Pairwise Labels | http://arxiv.org/pdf/1511.03855v2.pdf | author:Wu-Jun Li, Sheng Wang, Wang-Cheng Kang category:cs.LG cs.CV H.3.1 published:2015-11-12 summary:Recent years have witnessed wide application of hashing for large-scale imageretrieval. However, most existing hashing methods are based on hand-craftedfeatures which might not be optimally compatible with the hashing procedure.Recently, deep hashing methods have been proposed to perform simultaneousfeature learning and hash-code learning with deep neural networks, which haveshown better performance than traditional hashing methods with hand-craftedfeatures. Most of these deep hashing methods are supervised whose supervisedinformation is given with triplet labels. For another common applicationscenario with pairwise labels, there have not existed methods for simultaneousfeature learning and hash-code learning. In this paper, we propose a novel deephashing method, called deep pairwise-supervised hashing(DPSH), to performsimultaneous feature learning and hash-code learning for applications withpairwise labels. Experiments on real datasets show that our DPSH method canoutperform other methods to achieve the state-of-the-art performance in imageretrieval applications.
arxiv-17400-95 | OCR Error Correction Using Character Correction and Feature-Based Word Classification | http://arxiv.org/pdf/1604.06225v1.pdf | author:Ido Kissos, Nachum Dershowitz category:cs.IR cs.CL published:2016-04-21 summary:This paper explores the use of a learned classifier for post-OCR textcorrection. Experiments with the Arabic language show that this approach, whichintegrates a weighted confusion matrix and a shallow language model, improvesthe vast majority of segmentation and recognition errors, the most frequenttypes of error on our dataset.
arxiv-17400-96 | Articulated Hand Pose Estimation Review | http://arxiv.org/pdf/1604.06195v1.pdf | author:Emad Barsoum category:cs.CV published:2016-04-21 summary:With the increase number of companies focusing on commercializing AugmentedReality (AR), Virtual Reality (VR) and wearable devices, the need for a handbased input mechanism is becoming essential in order to make the experiencenatural, seamless and immersive. Hand pose estimation has progresseddrastically in recent years due to the introduction of commodity depth cameras. Hand pose estimation based on vision is still a challenging problem due toits complexity from self-occlusion (between fingers), close similarity betweenfingers, dexterity of the hands, speed of the pose and the high dimension ofthe hand kinematic parameters. Articulated hand pose estimation is still anopen problem and under intensive research from both academia and industry. The 2 approaches used for hand pose estimation are: discriminative andgenerative. Generative approach is a model based that tries to fit a hand modelto the observed data. Discriminative approach is appearance based, usuallyimplemented with machine learning (ML) and require a large amount of trainingdata. Recent hand pose estimation uses hybrid approach by combining bothdiscriminative and generative methods into a single hand pipeline. In this paper, we focus on reviewing recent progress of hand pose estimationfrom depth sensor. We will survey discriminative methods, generative methodsand hybrid methods. This paper is not a comprehensive review of all hand poseestimation techniques, it is a subset of some of the recent state-of-the-arttechniques.
arxiv-17400-97 | Dynamic matrix factorization with social influence | http://arxiv.org/pdf/1604.06194v1.pdf | author:Aleksandr Y. Aravkin, Kush R. Varshney, Liu Yang category:stat.ML cs.IR cs.SI math.OC published:2016-04-21 summary:Matrix factorization is a key component of collaborative filtering-basedrecommendation systems because it allows us to complete sparse user-by-itemratings matrices under a low-rank assumption that encodes the belief thatsimilar users give similar ratings and that similar items garner similarratings. This paradigm has had immeasurable practical success, but it is notthe complete story for understanding and inferring the preferences of people.First, peoples' preferences and their observable manifestations as ratingsevolve over time along general patterns of trajectories. Second, an individualperson's preferences evolve over time through influence of their socialconnections. In this paper, we develop a unified process model for both typesof dynamics within a state space approach, together with an efficientoptimization scheme for estimation within that model. The model combineselements from recent developments in dynamic matrix factorization, opiniondynamics and social learning, and trust-based recommendation. The estimationbuilds upon recent advances in numerical nonlinear optimization. Empiricalresults on a large-scale data set from the Epinions website demonstrateconsistent reduction in root mean squared error by consideration of the twotypes of dynamics.
arxiv-17400-98 | What we write about when we write about causality: Features of causal statements across large-scale social discourse | http://arxiv.org/pdf/1604.05781v2.pdf | author:Thomas C. McAndrew, Joshua C. Bongard, Christopher M. Danforth, Peter S. Dodds, Paul D. H. Hines, James P. Bagrow category:cs.CY cs.CL cs.SI published:2016-04-20 summary:Identifying and communicating relationships between causes and effects isimportant for understanding our world, but is affected by language structure,cognitive and emotional biases, and the properties of the communication medium.Despite the increasing importance of social media, much remains unknown aboutcausal statements made online. To study real-world causal attribution, weextract a large-scale corpus of causal statements made on the Twitter socialnetwork platform as well as a comparable random control corpus. We comparecausal and control statements using statistical language and sentiment analysistools. We find that causal statements have a number of significant lexical andgrammatical differences compared with controls and tend to be more negative insentiment than controls. Causal statements made online tend to focus on newsand current events, medicine and health, or interpersonal relationships, asshown by topic models. By quantifying the features and potential biases ofcausality communication, this study improves our understanding of the accuracyof information and opinions found online.
arxiv-17400-99 | Evolutionary Image Transition Based on Theoretical Insights of Random Processes | http://arxiv.org/pdf/1604.06187v1.pdf | author:Aneta Neumann, Bradley Alexander, Frank Neumann category:cs.NE published:2016-04-21 summary:Evolutionary algorithms have been widely studied from a theoreticalperspective. In particular, the area of runtime analysis has contributedsignificantly to a theoretical understanding and provided insights into theworking behaviour of these algorithms. We study how these insights intoevolutionary processes can be used for evolutionary art. We introduce thenotion of evolutionary image transition which transfers a given starting imageinto a target image through an evolutionary process. Combining standardmutation effects known from the optimization of the classical benchmarkfunction OneMax and different variants of random walks, we present ways ofperforming evolutionary image transition with different artistic effects.
arxiv-17400-100 | The THUMOS Challenge on Action Recognition for Videos "in the Wild" | http://arxiv.org/pdf/1604.06182v1.pdf | author:Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, Mubarak Shah category:cs.CV published:2016-04-21 summary:Automatically recognizing and localizing wide ranges of human actions hascrucial importance for video understanding. Towards this goal, the THUMOSchallenge was introduced in 2013 to serve as a benchmark for actionrecognition. Until then, video action recognition, including THUMOS challenge,had focused primarily on the classification of pre-segmented (i.e., trimmed)videos, which is an artificial task. In THUMOS 2014, we elevated actionrecognition to a more practical level by introducing temporally untrimmedvideos. These also include `background videos' which share similar scenes andbackgrounds as action videos, but are devoid of the specific actions. The threeeditions of the challenge organized in 2013--2015 have made THUMOS a commonbenchmark for action classification and detection and the annual challenge iswidely attended by teams from around the world. In this paper we describe the THUMOS benchmark in detail and give an overviewof data collection and annotation procedures. We present the evaluationprotocols used to quantify results in the two THUMOS tasks of actionclassification and temporal detection. We also present results of submissionsto the THUMOS 2015 challenge and review the participating approaches.Additionally, we include a comprehensive empirical study evaluating thedifferences in action recognition between trimmed and untrimmed videos, and howwell methods trained on trimmed videos generalize to untrimmed videos. Weconclude by proposing several directions and improvements for future THUMOSchallenges.
arxiv-17400-101 | The Extended Littlestone's Dimension for Learning with Mistakes and Abstentions | http://arxiv.org/pdf/1604.06162v1.pdf | author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG published:2016-04-21 summary:This paper studies classification with an abstention option in the onlinesetting. In this setting, examples arrive sequentially, the learner is given ahypothesis class $\mathcal H$, and the goal of the learner is to either predicta label on each example or abstain, while ensuring that it does not make morethan a pre-specified number of mistakes when it does predict a label. Previous work on this problem has left open two main challenges. First, notmuch is known about the optimality of algorithms, and in particular, about whatan optimal algorithmic strategy is for any individual hypothesis class. Second,while the realizable case has been studied, the more realistic non-realizablescenario is not well-understood. In this paper, we address both challenges.First, we provide a novel measure, called the Extended Littlestone's Dimension,which captures the number of abstentions needed to ensure a certain number ofmistakes. Second, we explore the non-realizable case, and provide upper andlower bounds on the number of abstentions required by an algorithm to guaranteea specified number of mistakes.
arxiv-17400-102 | Deep Adaptive Network: An Efficient Deep Neural Network with Sparse Binary Connections | http://arxiv.org/pdf/1604.06154v1.pdf | author:Xichuan Zhou, Shengli Li, Kai Qin, Kunping Li, Fang Tang, Shengdong Hu, Shujun Liu, Zhi Lin category:cs.LG cs.CV cs.NE published:2016-04-21 summary:Deep neural networks are state-of-the-art models for understanding thecontent of images, video and raw input data. However, implementing a deepneural network in embedded systems is a challenging task, because a typicaldeep neural network, such as a Deep Belief Network using 128x128 images asinput, could exhaust Giga bytes of memory and result in bandwidth and computingbottleneck. To address this challenge, this paper presents a hardware-orienteddeep learning algorithm, named as the Deep Adaptive Network, which attempts toexploit the sparsity in the neural connections. The proposed method adaptivelyreduces the weights associated with negligible features to zero, leading tosparse feedforward network architecture. Furthermore, since the smallproportion of important weights are significantly larger than zero, they can berobustly thresholded and represented using single-bit integers (-1 and +1),leading to implementations of deep neural networks with sparse and binaryconnections. Our experiments showed that, for the application of recognizingMNIST handwritten digits, the features extracted by a two-layer Deep AdaptiveNetwork with about 25% reserved important connections achieved 97.2%classification accuracy, which was almost the same with the standard DeepBelief Network (97.3%). Furthermore, for efficient hardware implementations,the sparse-and-binary-weighted deep neural network could save about 99.3%memory and 99.9% computation units without significant loss of classificationaccuracy for pattern recognition applications.
arxiv-17400-103 | Nonextensive information theoretical machine | http://arxiv.org/pdf/1604.06153v1.pdf | author:Chaobing Song, Shu-Tao Xia category:cs.LG published:2016-04-21 summary:In this paper, we propose a new discriminative model named \emph{nonextensiveinformation theoretical machine (NITM)} based on nonextensive generalization ofShannon information theory. In NITM, weight parameters are treated as randomvariables. Tsallis divergence is used to regularize the distribution of weightparameters and maximum unnormalized Tsallis entropy distribution is used toevaluate fitting effect. On the one hand, it is showed that some well-knownmargin-based loss functions such as $\ell_{0/1}$ loss, hinge loss, squaredhinge loss and exponential loss can be unified by unnormalized Tsallis entropy.On the other hand, Gaussian prior regularization is generalized to Student-tprior regularization with similar computational complexity. The model can besolved efficiently by gradient-based convex optimization and its performance isillustrated on standard datasets.
arxiv-17400-104 | Persian Heritage Image Binarization Competition (PHIBC 2012) | http://arxiv.org/pdf/1306.6263v2.pdf | author:Seyed Morteza Ayatollahi, Hossein Ziaei Nafchi category:cs.CV published:2013-06-26 summary:The first competition on the binarization of historical Persian documents andmanuscripts (PHIBC 2012) has been organized in conjunction with the firstIranian conference on pattern recognition and image analysis (PRIA 2013). Themain objective of PHIBC 2012 is to evaluate performance of the binarizationmethodologies, when applied on the Persian heritage images. This paper providesa report on the methodology and performance of the three submitted algorithmsbased on evaluation measures has been used.
arxiv-17400-105 | Embedded all relevant feature selection with Random Ferns | http://arxiv.org/pdf/1604.06133v1.pdf | author:Miron Bartosz Kursa category:cs.LG published:2016-04-20 summary:Many machine learning methods can produce variable importance scoresexpressing the usability of each feature in context of the produced model;those scores on their own are yet not sufficient to generate feature selection,especially when an all relevant selection is required. Although there arewrapper methods aiming to solve this problem, they introduce a substantialincrease in the required computational effort. In this paper I investigate an idea of incorporating all relevant selectionwithin the training process by producing importance for implicitly generatedshadows, attributes irrelevant by design. I propose and evaluate such a methodin context of random ferns classifier. Experiment results confirm theeffectiveness of such approach, although show that fully stochastic nature ofrandom ferns limits its applicability either to small dimensions or as a partof a broader feature selection procedure.
arxiv-17400-106 | Learning Social Affordance for Human-Robot Interaction | http://arxiv.org/pdf/1604.03692v2.pdf | author:Tianmin Shu, M. S. Ryoo, Song-Chun Zhu category:cs.RO cs.AI cs.CV cs.LG published:2016-04-13 summary:In this paper, we present an approach for robot learning of social affordancefrom human activity videos. We consider the problem in the context ofhuman-robot interaction: Our approach learns structural representations ofhuman-human (and human-object-human) interactions, describing how body-parts ofeach agent move with respect to each other and what spatial relations theyshould maintain to complete each sub-event (i.e., sub-goal). This enables therobot to infer its own movement in reaction to the human body motion, allowingit to naturally replicate such interactions. We introduce the representation of social affordance and propose a generativemodel for its weakly supervised learning from human demonstration videos. Ourapproach discovers critical steps (i.e., latent sub-events) in an interactionand the typical motion associated with them, learning what body-parts should beinvolved and how. The experimental results demonstrate that our Markov ChainMonte Carlo (MCMC) based learning algorithm automatically discoverssemantically meaningful interactive affordance from RGB-D videos, which allowsus to generate appropriate full body motion for an agent.
arxiv-17400-107 | Network of Experts for Large-Scale Image Categorization | http://arxiv.org/pdf/1604.06119v1.pdf | author:Karim Ahmed, Mohammad Haris Baig, Lorenzo Torresani category:cs.CV published:2016-04-20 summary:We present a tree-structured network architecture for large-scale imageclassification. The trunk of the network contains convolutional layersoptimized over all classes. At a given depth, the trunk splits into separatebranches, each dedicated to discriminate a different subset of classes. Eachbranch acts as an expert classifying a set of categories that are difficult totell apart, while the trunk provides common knowledge to all experts in theform of shared features. The training of our "network of experts" is completelyend-to-end: the partition of categories into disjoint subsets is learnedsimultaneously with the parameters of the network trunk and the experts aretrained jointly by minimizing a single learning objective over all classes. Theproposed structure can be built from any existing convolutional neural network(CNN). We demonstrate its generality by adapting 3 popular CNNs for imagecategorization into the form of networks of experts. Our experiments onCIFAR100 and ImageNet show that in each case our method yields a substantialimprovement in accuracy over the base CNN, and gives the best reported resulton CIFAR100. Finally, the improvement in accuracy comes at little additionalcost: compared to the base network, the training time of our model is about1.5X and the number of parameters is comparable or in some cases even lower.
arxiv-17400-108 | Speaker Cluster-Based Speaker Adaptive Training for Deep Neural Network Acoustic Modeling | http://arxiv.org/pdf/1604.06113v1.pdf | author:Wei Chu, Ruxin Chen category:cs.CL published:2016-04-20 summary:A speaker cluster-based speaker adaptive training (SAT) method under deepneural network-hidden Markov model (DNN-HMM) framework is presented in thispaper. During training, speakers that are acoustically adjacent to each otherare hierarchically clustered using an i-vector based distance metric. DNNs withspeaker dependent layers are then adaptively trained for each cluster ofspeakers. Before decoding starts, an unseen speaker in test set is matched tothe closest speaker cluster through comparing i-vector based distances. Thepreviously trained DNN of the matched speaker cluster is used for decodingutterances of the test speaker. The performance of the proposed method on alarge vocabulary spontaneous speech recognition task is evaluated on a trainingset of with 1500 hours of speech, and a test set of 24 speakers with 1774utterances. Comparing to a speaker independent DNN with a baseline word errorrate of 11.6%, a relative 6.8% reduction in word error rate is observed fromthe proposed method.
arxiv-17400-109 | Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs | http://arxiv.org/pdf/1508.00715v2.pdf | author:Zhilin Yang, Jie Tang, William Cohen category:cs.CL cs.SI published:2015-08-04 summary:We study the extent to which online social networks can be connected to openknowledge bases. The problem is referred to as learning social knowledgegraphs. We propose a multi-modal Bayesian embedding model, GenVector, to learnlatent topics that generate word and network embeddings. GenVector leverageslarge-scale unlabeled data with embeddings and represents data of twomodalities---i.e., social network users and knowledge concepts---in a sharedlatent topic space. Experiments on three datasets show that the proposed methodclearly outperforms state-of-the-art methods. We then deploy the method onAMiner, a large-scale online academic search system with a network of38,049,189 researchers with a knowledge base with 35,415,011 concepts. Ourmethod significantly decreases the error rate in an online A/B test with liveusers.
arxiv-17400-110 | Automatic Graphic Logo Detection via Fast Region-based Convolutional Networks | http://arxiv.org/pdf/1604.06083v1.pdf | author:Gonçalo Oliveira, Xavier Frazão, André Pimentel, Bernardete Ribeiro category:cs.CV published:2016-04-20 summary:Brand recognition is a very challenging topic with many useful applicationsin localization recognition, advertisement and marketing. In this paper wepresent an automatic graphic logo detection system that robustly handlesunconstrained imaging conditions. Our approach is based on Fast Region-basedConvolutional Networks (FRCN) proposed by Ross Girshick, which have shownstate-of-the-art performance in several generic object recognition tasks(PASCAL Visual Object Classes challenges). In particular, we use two CNN modelspre-trained with the ILSVRC ImageNet dataset and we look at the selectivesearch of windows `proposals' in the pre-processing stage and data augmentationto enhance the logo recognition rate. The novelty lies in the use of transferlearning to leverage powerful Convolutional Neural Network models trained withlarge-scale datasets and repurpose them in the context of graphic logodetection. Another benefit of this framework is that it allows for multipledetections of graphic logos using regions that are likely to have an object.Experimental results with the FlickrLogos-32 dataset show not only thepromising performance of our developed models with respect to noise and othertransformations a graphic logo can be subject to, but also its superiority overstate-of-the-art systems with hand-crafted models and features.
arxiv-17400-111 | DeepSymmetry: Joint Symmetry and Depth Estimation using Deep Neural Networks | http://arxiv.org/pdf/1604.06079v1.pdf | author:Guilin Liu, Chao Yang, Zimo Li, Duygu Ceylan, Qixing Huang category:cs.CV published:2016-04-20 summary:Due to the abundance of 2D product images from the Internet, developingefficient and scalable algorithms to recover the missing depth information iscentral to many applications. Recent works have addressed the single-view depthestimation problem by utilizing convolutional neural networks. In this paper,we show that exploring symmetry information, which is ubiquitous in man madeobjects, can significantly boost the quality of such depth predictions.Specifically, we propose a new convolutional neural network architecture tofirst estimate dense symmetric correspondences in a product image and thenpropose an optimization which utilizes this information explicitly tosignificantly improve the quality of single-view depth estimations. We haveevaluated our approach extensively, and experimental results show that thisapproach outperforms state-of-the-art depth estimation techniques.
arxiv-17400-112 | Question Answering via Integer Programming over Semi-Structured Knowledge | http://arxiv.org/pdf/1604.06076v1.pdf | author:Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, Dan Roth category:cs.AI cs.CL published:2016-04-20 summary:Answering science questions posed in natural language is an important AIchallenge. Answering such questions often requires non-trivial inference andknowledge that goes beyond factoid retrieval. Yet, most systems for this taskare based on relatively shallow Information Retrieval (IR) and statisticalcorrelation techniques operating on large unstructured corpora. We propose astructured inference system for this task, formulated as an Integer LinearProgram (ILP), that answers natural language questions using a semi-structuredknowledge base derived from text, including questions requiring multi-stepinference and a combination of multiple facts. On a dataset of real, unseenscience questions, our system significantly outperforms (+14%) the bestprevious attempt at structured reasoning for this task, which used Markov LogicNetworks (MLNs). It also improves upon a previous ILP formulation by 17.7%.When combined with unstructured inference methods, the ILP system significantlyboosts overall performance (+10%). Finally, we show our approach issubstantially more robust to a simple answer perturbation compared tostatistical correlation methods.
arxiv-17400-113 | ARSENAL: Automatic Requirements Specification Extraction from Natural Language | http://arxiv.org/pdf/1403.3142v3.pdf | author:Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick Lincoln, Natarajan Shankar, Wilfried Steiner category:cs.CL cs.SE published:2014-03-13 summary:Requirements are informal and semi-formal descriptions of the expectedbehavior of a complex system from the viewpoints of its stakeholders(customers, users, operators, designers, and engineers). However, for thepurpose of design, testing, and verification for critical systems, we cantransform requirements into formal models that can be analyzed automatically.ARSENAL is a framework and methodology for systematically transforming naturallanguage (NL) requirements into analyzable formal models and logicspecifications. These models can be analyzed for consistency andimplementability. The ARSENAL methodology is specialized to individual domains,but the approach is general enough to be adapted to new domains.
arxiv-17400-114 | Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation | http://arxiv.org/pdf/1604.06057v1.pdf | author:Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum category:cs.LG cs.AI cs.CV cs.NE stat.ML published:2016-04-20 summary:Learning goal-directed behavior in environments with sparse feedback is amajor challenge for reinforcement learning algorithms. The primary difficultyarises due to insufficient exploration, resulting in an agent being unable tolearn robust value functions. Intrinsically motivated agents can explore newbehavior for its own sake rather than to directly solve problems. Suchintrinsic behaviors could eventually help the agent solve tasks posed by theenvironment. We present hierarchical-DQN (h-DQN), a framework to integratehierarchical value functions, operating at different temporal scales, withintrinsically motivated deep reinforcement learning. A top-level value functionlearns a policy over intrinsic goals, and a lower-level function learns apolicy over atomic actions to satisfy the given goals. h-DQN allows forflexible goal specifications, such as functions over entities and relations.This provides an efficient space for exploration in complicated environments.We demonstrate the strength of our approach on two problems with very sparse,delayed feedback: (1) a complex discrete MDP with stochastic transitions, and(2) the classic ATARI game `Montezuma's Revenge'.
arxiv-17400-115 | Clustering of Sparse and Approximately Sparse Graphs by Semidefinite Programming | http://arxiv.org/pdf/1603.05296v2.pdf | author:Aleksis Pirinen, Brendan Ames category:math.OC stat.ML published:2016-03-16 summary:As a model problem for clustering, we consider the densest k-disjoint-cliqueproblem of partitioning a weighted complete graph into k disjoint subgraphssuch that the sum of the densities of these subgraphs is maximized. Weestablish that such subgraphs can be recovered from the solution of aparticular semidefinite relaxation with high probability if the input graph issampled from a distribution of clusterable graphs. Specifically, thesemidefinite relaxation is exact if the graph consists of k large disjointsubgraphs, corresponding to clusters, with weight concentrated within thesesubgraphs, plus a moderate number of outliers. Further, we establish that ifnoise is weakly obscuring these clusters, i.e, the between-cluster edges areassigned very small weights, then we can recover significantly smallerclusters. For example, we show that in approximately sparse graphs, where thebetween-cluster weights tend to zero as the size n of the graph tends toinfinity, we can recover clusters of size polylogarithmic in n. Empiricalevidence from numerical simulations is also provided to support thesetheoretical phase transitions to perfect recovery of the cluster structure.
arxiv-17400-116 | Random Projection Estimation of Discrete-Choice Models with Large Choice Sets | http://arxiv.org/pdf/1604.06036v1.pdf | author:Khai X. Chiong, Matthew Shum category:stat.ML published:2016-04-20 summary:We introduce sparse random projection, an important dimension-reduction toolfrom machine learning, for the estimation of discrete-choice models withhigh-dimensional choice sets. Initially, high-dimensional data are compressedinto a lower-dimensional Euclidean space using random projections.Subsequently, estimation proceeds using cyclic monotonicity moment inequalitiesimplied by the multinomial choice model; the estimation procedure issemi-parametric and does not require explicit distributional assumptions to bemade regarding the random utility errors. The random projection procedure isjustified via the Johnson-Lindenstrauss Lemma -- the pairwise distances betweendata points are preserved during data compression, which we exploit to showconvergence of our estimator. The estimator works well in simulations and in anapplication to a supermarket scanner dataset.
arxiv-17400-117 | Constructive Preference Elicitation by Setwise Max-margin Learning | http://arxiv.org/pdf/1604.06020v1.pdf | author:Stefano Teso, Andrea Passerini, Paolo Viappiani category:stat.ML cs.AI cs.LG 68T05 published:2016-04-20 summary:In this paper we propose an approach to preference elicitation that issuitable to large configuration spaces beyond the reach of existingstate-of-the-art approaches. Our setwise max-margin method can be viewed as ageneralization of max-margin learning to sets, and can produce a set of"diverse" items that can be used to ask informative queries to the user.Moreover, the approach can encourage sparsity in the parameter space, in orderto favor the assessment of utility towards combinations of weights thatconcentrate on just few features. We present a mixed integer linear programmingformulation and show how our approach compares favourably with Bayesianpreference elicitation alternatives and easily scales to realistic datasets.
arxiv-17400-118 | Greedy Criterion in Orthogonal Greedy Learning | http://arxiv.org/pdf/1604.05993v1.pdf | author:Lin Xu, Shaobo Lin, Jinshan Zeng, Xia Liu, Zongben Xu category:cs.LG published:2016-04-20 summary:Orthogonal greedy learning (OGL) is a stepwise learning scheme that startswith selecting a new atom from a specified dictionary via the steepest gradientdescent (SGD) and then builds the estimator through orthogonal projection. Inthis paper, we find that SGD is not the unique greedy criterion and introduce anew greedy criterion, called "$\delta$-greedy threshold" for learning. Based onthe new greedy criterion, we derive an adaptive termination rule for OGL. Ourtheoretical study shows that the new learning scheme can achieve the existing(almost) optimal learning rate of OGL. Plenty of numerical experiments areprovided to support that the new scheme can achieve almost optimalgeneralization performance, while requiring less computation than OGL.
arxiv-17400-119 | A topological insight into restricted Boltzmann machines | http://arxiv.org/pdf/1604.05978v1.pdf | author:Decebal Constantin Mocanu, Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta category:cs.NE cs.AI cs.SI published:2016-04-20 summary:Restricted Boltzmann Machines (RBMs) and models derived from them have beensuccessfully used as basic building blocks in deep artificial neural networksfor automatic features extraction, unsupervised weights initialization, butalso as density estimators. Thus, their generative and discriminativecapabilities, but also their computational time are instrumental to a widerange of applications. Our main contribution is to look at RBMs from atopological perspective, bringing insights from network science. Firstly, herewe show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs whichnaturally have a small-world topology. Secondly, we demonstrate both onsynthetic and real-world databases that by constraining RBMs and GRBMs to ascale-free topology (while still considering local neighborhoods and datadistribution), we reduce the number of weights that need to be computed by afew orders of magnitude, at virtually no loss in generative performance.Thirdly, we show that, given the same number of weights, our proposed sparsemodels (which by design have a higher number of hidden neurons) achieve bettergenerative capabilities than standard fully connected RBMs and GRBMs (which bydesign have a smaller number of hidden neurons) at no additional computationalcosts.
arxiv-17400-120 | Computational Drug Repositioning Using Continuous Self-controlled Case Series | http://arxiv.org/pdf/1604.05976v1.pdf | author:Zhaobin Kuang, James Thomson, Michael Caldwell, Peggy Peissig, Ron Stewart, David Page category:stat.AP stat.ML published:2016-04-20 summary:Computational Drug Repositioning (CDR) is the task of discovering potentialnew indications for existing drugs by mining large-scale heterogeneousdrug-related data sources. Leveraging the patient-level temporal orderinginformation between numeric physiological measurements and various drugprescriptions provided in Electronic Health Records (EHRs), we propose aContinuous Self-controlled Case Series (CSCCS) model for CDR. As an initialevaluation, we look for drugs that can control Fasting Blood Glucose (FBG)level in our experiments. Applying CSCCS to the Marshfield Clinic EHR,well-known drugs that are indicated for controlling blood glucose level arerediscovered. Furthermore, some drugs with recent literature support for thepotential effect of blood glucose level control are also identified.
arxiv-17400-121 | Infrared Colorization Using Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.02245v2.pdf | author:Matthias Limmer, Hendrik P. A. Lensch category:cs.CV cs.GR published:2016-04-08 summary:This paper proposes a method for transferring the RGB color spectrum tonear-infrared (NIR) images using deep multi-scale convolutional neuralnetworks. A direct and integrated transfer between NIR and RGB pixels istrained. The trained model does not require any user guidance or a referenceimage database in the recall phase to produce images with a natural appearance.To preserve the rich details of the NIR image, its high frequency features aretransferred to the estimated RGB image. The presented approach is trained andevaluated on a real-world dataset containing a large amount of road sceneimages in summer. The dataset was captured by a multi-CCD NIR/RGB camera, whichensures a perfect pixel to pixel registration.
arxiv-17400-122 | Parametric Object Motion from Blur | http://arxiv.org/pdf/1604.05933v1.pdf | author:Jochen Gast, Anita Sellent, Stefan Roth category:cs.CV published:2016-04-20 summary:Motion blur can adversely affect a number of vision tasks, hence it isgenerally considered a nuisance. We instead treat motion blur as a usefulsignal that allows to compute the motion of objects from a single image.Drawing on the success of joint segmentation and parametric motion models inthe context of optical flow estimation, we propose a parametric object motionmodel combined with a segmentation mask to exploit localized, non-uniformmotion blur. Our parametric image formation model is differentiable w.r.t. themotion parameters, which enables us to generalize marginal-likelihoodtechniques from uniform blind deblurring to localized, non-uniform blur. Atwo-stage pipeline, first in derivative space and then in image space, allowsto estimate both parametric object motion as well as a motion segmentation froma single image alone. Our experiments demonstrate its ability to cope with verychallenging cases of object motion blur.
arxiv-17400-123 | Jansen-MIDAS: a multi-level photomicrograph segmentation software based on isotropic undecimated wavelets | http://arxiv.org/pdf/1604.05921v1.pdf | author:Alexandre Fioravante de Siqueira, Flávio Camargo Cabrera, Wagner Massayuki Nakasuga, Aylton Pagamisse, Aldo Eloizo Job category:cs.CV 68T10 published:2016-04-20 summary:Image segmentation, the process of separating the elements within an image,is frequently used for obtaining information from photomicrographs. However,segmentation methods should be used with reservations: incorrect segmentationcan mislead when interpreting regions of interest (ROI), thus decreasing thesuccess rate of additional procedures. Multi-Level Starlet Segmentation (MLSS)and Multi-Level Starlet Optimal Segmentation (MLSOS) were developed to addressthe photomicrograph segmentation deficiency on general tools. These methodsgave rise to Jansen-MIDAS, an open-source software which a scientist can use toobtain a multi-level threshold segmentation of his/hers photomicrographs. Thissoftware is presented in two versions: a text-based version, for GNU Octave,and a graphical user interface (GUI) version, for MathWorks MATLAB. It can beused to process several types of images, becoming a reliable alternative to thescientist.
arxiv-17400-124 | Mondrian Forests for Large-Scale Regression when Uncertainty Matters | http://arxiv.org/pdf/1506.03805v3.pdf | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG published:2015-06-11 summary:Many real-world regression problems demand a measure of the uncertaintyassociated with each prediction. Standard decision forests deliver efficientstate-of-the-art predictive performance, but high-quality uncertainty estimatesare lacking. Gaussian processes (GPs) deliver uncertainty estimates, butscaling GPs to large-scale data sets comes at the cost of approximating theuncertainty estimates. We extend Mondrian forests, first proposed byLakshminarayanan et al. (2014) for classification problems, to the large-scalenon-parametric regression setting. Using a novel hierarchical Gaussian priorthat dovetails with the Mondrian forest framework, we obtain principleduncertainty estimates, while still retaining the computational advantages ofdecision forests. Through a combination of illustrative examples, real-worldlarge-scale datasets, and Bayesian optimization benchmarks, we demonstrate thatMondrian forests outperform approximate GPs on large-scale regression tasks anddeliver better-calibrated uncertainty assessments than decision-forest-basedmethods.
arxiv-17400-125 | Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention | http://arxiv.org/pdf/1604.03286v2.pdf | author:Théodore Bluche, Jérôme Louradour, Ronaldo Messina category:cs.CV published:2016-04-12 summary:We present an attention-based model for end-to-end handwriting recognition.Our system does not require any segmentation of the input paragraph. The modelis inspired by the differentiable attention models presented recently forspeech recognition, image captioning or translation. The main difference is thecovert and overt attention, implemented as a multi-dimensional LSTM network.Our principal contribution towards handwriting recognition lies in theautomatic transcription without a prior segmentation into lines, which wascrucial in previous approaches. To the best of our knowledge this is the firstsuccessful attempt of end-to-end multi-line handwriting recognition. We carriedout experiments on the well-known IAM Database. The results are encouraging andbring hope to perform full paragraph transcription in the near future.
arxiv-17400-126 | A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion | http://arxiv.org/pdf/1604.05878v1.pdf | author:Johannes Welbl, Guillaume Bouchard, Sebastian Riedel category:cs.CL cs.AI cs.NE stat.ML published:2016-04-20 summary:Embedding-based Knowledge Base Completion models have so far mostly combineddistributed representations of individual entities or relations to computetruth scores of missing links. Facts can however also be represented usingpairwise embeddings, i.e. embeddings for pairs of entities and relations. Inthis paper we explore such bigram embeddings with a flexible FactorizationMachine model and several ablations from it. We investigate the relevance ofvarious bigram types on the fb15k237 dataset and find relative improvementscompared to a compositional model.
arxiv-17400-127 | Distributed Entity Disambiguation with Per-Mention Learning | http://arxiv.org/pdf/1604.05875v1.pdf | author:Tiep Mai, Bichen Shi, Patrick K. Nicholson, Deepak Ajwani, Alessandra Sala category:cs.CL cs.IR published:2016-04-20 summary:Entity disambiguation, or mapping a phrase to its canonical representation ina knowledge base, is a fundamental step in many natural language processingapplications. Existing techniques based on global ranking models fail tocapture the individual peculiarities of the words and hence, either struggle tomeet the accuracy requirements of many real-world applications or they are toocomplex to satisfy real-time constraints of applications. In this paper, we propose a new disambiguation system that learns specializedfeatures and models for disambiguating each ambiguous phrase in the Englishlanguage. To train and validate the hundreds of thousands of learning modelsfor this purpose, we use a Wikipedia hyperlink dataset with more than 170million labelled annotations. We provide an extensive experimental evaluationto show that the accuracy of our approach compares favourably with respect tomany state-of-the-art disambiguation systems. The training required for ourapproach can be easily distributed over a cluster. Furthermore, updating oursystem for new entities or calibrating it for special ones is a computationallyfast process, that does not affect the disambiguation of the other entities.
arxiv-17400-128 | Estimating 3D Trajectories from 2D Projections via Disjunctive Factored Four-Way Conditional Restricted Boltzmann Machines | http://arxiv.org/pdf/1604.05865v1.pdf | author:Decebal Constantin Mocanu, Haitham Bou Ammar, Luis Puig, Eric Eaton, Antonio Liotta category:cs.CV cs.AI published:2016-04-20 summary:Estimation, recognition, and near-future prediction of 3D trajectories basedon their two dimensional projections available from one camera source is anexceptionally difficult problem due to uncertainty in the trajectories andenvironment, high dimensionality of the specific trajectory states, lack ofenough labeled data and so on. In this article, we propose a solution to solvethis problem based on a novel deep learning model dubbed Disjunctive FactoredFour-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our methodimproves state-of-the-art deep learning techniques for high dimensionaltime-series modeling by introducing a novel tensor factorization capable ofdriving forth order Boltzmann machines to considerably lower energy levels, atno computational costs. DFFW-CRBMs are capable of accurately estimating,recognizing, and performing near-future prediction of three-dimensionaltrajectories from their 2D projections while requiring limited amount oflabeled data. We evaluate our method on both simulated and real-world data,showing its effectiveness in predicting and classifying complex balltrajectories and human activities.
arxiv-17400-129 | Scene Parsing with Integration of Parametric and Non-parametric Models | http://arxiv.org/pdf/1604.05848v1.pdf | author:Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang category:cs.CV published:2016-04-20 summary:We adopt Convolutional Neural Networks (CNNs) to be our parametric model tolearn discriminative features and classifiers for local patch classification.Based on the occurrence frequency distribution of classes, an ensemble of CNNs(CNN-Ensemble) are learned, in which each CNN component focuses on learningdifferent and complementary visual patterns. The local beliefs of pixels areoutput by CNN-Ensemble. Considering that visually similar pixels areindistinguishable under local context, we leverage the global scene semanticsto alleviate the local ambiguity. The global scene constraint is mathematicallyachieved by adding a global energy term to the labeling energy function, and itis practically estimated in a non-parametric framework. A large margin basedCNN metric learning method is also proposed for better global beliefestimation. In the end, the integration of local and global beliefs gives riseto the class likelihood of pixels, based on which maximum marginal inference isperformed to generate the label prediction maps. Even without anypost-processing, we achieve state-of-the-art results on the challengingSiftFlow and Barcelona benchmarks.
arxiv-17400-130 | Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive Models | http://arxiv.org/pdf/1604.05819v1.pdf | author:Daniel P. Robinson, Suchi Saria category:stat.ML cs.LG published:2016-04-20 summary:Predictive models are finding an increasing number of applications in manyindustries. As a result, a practical means for trading-off the cost ofdeploying a model versus its effectiveness is needed. Our work is motivated byrisk prediction problems in healthcare. Cost-structures in domains such ashealthcare are quite complex, posing a significant challenge to existingapproaches. We propose a novel framework for designing cost-sensitivestructured regularizers that is suitable for problems with complex costdependencies. We draw upon a surprising connection to boolean circuits. Inparticular, we represent the problem costs as a multi-layer boolean circuit,and then use properties of boolean circuits to define an extended featurevector and a group regularizer that exactly captures the underlying coststructure. The resulting regularizer may then be combined with a fidelityfunction to perform model prediction, for example. For the challengingreal-world application of risk prediction for sepsis in intensive care units,the use of our regularizer leads to models that are in harmony with theunderlying cost structure and thus provide an excellent prediction accuracyversus cost tradeoff.
arxiv-17400-131 | Depth Image Inpainting: Improving Low Rank Matrix Completion with Low Gradient Regularization | http://arxiv.org/pdf/1604.05817v1.pdf | author:Hongyang Xue, Shengming Zhang, Deng Cai category:cs.CV published:2016-04-20 summary:We consider the case of inpainting single depth images. Without correspondingcolor images, previous or next frames, depth image inpainting is quitechallenging. One natural solution is to regard the image as a matrix and adoptthe low rank regularization just as inpainting color images. However, the lowrank assumption does not make full use of the properties of depth images. A shallow observation may inspire us to penalize the non-zero gradients bysparse gradient regularization. However, statistics show that though mostpixels have zero gradients, there is still a non-ignorable part of pixels whosegradients are equal to 1. Based on this specific property of depth images , wepropose a low gradient regularization method in which we reduce the penalty forgradient 1 while penalizing the non-zero gradients to allow for gradual depthchanges. The proposed low gradient regularization is integrated with the lowrank regularization into the low rank low gradient approach for depth imageinpainting. We compare our proposed low gradient regularization with sparsegradient regularization. The experimental results show the effectiveness of ourproposed approach.
arxiv-17400-132 | Deep CNNs for HEp-2 Cells Classification : A Cross-specimen Analysis | http://arxiv.org/pdf/1604.05816v1.pdf | author:Hongwei Li, Jianguo Zhang, Wei-Shi Zheng category:cs.CV published:2016-04-20 summary:Automatic classification of Human Epithelial Type-2 (HEp-2) cells stainingpatterns is an important and yet a challenging problem. Although both shallowand deep methods have been applied, the study of deep convolutional networks(CNNs) on this topic is shallow to date, thus failed to claim its top positionfor this problem. In this paper, we propose a novel study of using CNNs forHEp-2 cells classification focusing on cross-specimen analysis, a keyevaluation for generalization. For the first time, our study reveals severalkey factors of using CNNs for HEp-2 cells classification. Our proposed systemachieves state-of-the-art classification accuracy on public benchmark dataset.Comparative experiments on different training data reveals that addingdifferent specimens,rather than increasing in numbers by affinetransformations, helps to train a good deep model. This opens a new avenue foradopting deep CNNs to HEp-2 cells classification.
arxiv-17400-133 | Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class Collaborative Filtering | http://arxiv.org/pdf/1604.05813v1.pdf | author:Ruining He, Chunbin Lin, Jianguo Wang, Julian McAuley category:cs.IR cs.CV published:2016-04-20 summary:Building successful recommender systems requires uncovering the underlyingdimensions that describe the properties of items as well as users' preferencestoward them. In domains like clothing recommendation, explaining users'preferences requires modeling the visual appearance of the items in question.This makes recommendation especially challenging, due to both the complexityand subtlety of people's 'visual preferences,' as well as the scale anddimensionality of the data and features involved. Ultimately, a successfulmodel should be capable of capturing considerable variance across differentcategories and styles, while still modeling the commonalities explained by`global' structures in order to combat the sparsity (e.g. cold-start),variability, and scale of real-world datasets. Here, we address thesechallenges by building such structures to model the visual dimensions acrossdifferent product categories. With a novel hierarchical embedding architecture,our method accounts for both high-level (colorfulness, darkness, etc.) andsubtle (e.g. casualness) visual characteristics simultaneously.
arxiv-17400-134 | VQA: Visual Question Answering | http://arxiv.org/pdf/1505.00468v6.pdf | author:Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh category:cs.CL cs.CV published:2015-05-03 summary:We propose the task of free-form and open-ended Visual Question Answering(VQA). Given an image and a natural language question about the image, the taskis to provide an accurate natural language answer. Mirroring real-worldscenarios, such as helping the visually impaired, both the questions andanswers are open-ended. Visual questions selectively target different areas ofan image, including background details and underlying context. As a result, asystem that succeeds at VQA typically needs a more detailed understanding ofthe image and complex reasoning than a system producing generic image captions.Moreover, VQA is amenable to automatic evaluation, since many open-endedanswers contain only a few words or a closed set of answers that can beprovided in a multiple-choice format. We provide a dataset containing ~0.25Mimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss theinformation it provides. Numerous baselines and methods for VQA are providedand compared with human performance.
arxiv-17400-135 | A Deep Neural Network for Chinese Zero Pronoun Resolution | http://arxiv.org/pdf/1604.05800v1.pdf | author:Yin Qingyu, Zhang Weinan, Zhang Yu, Liu Ting category:cs.CL published:2016-04-20 summary:This paper investigates the problem of Chinese zero pronoun resolution. Mostexisting approaches are based on machine learning algorithms, usinghand-crafted features, which is labor-intensive. More- over, semanticinformation that is essential in the resolution of noun phrases has not beenaddressed enough by previous approaches on zero pronoun resolution. This isbecause that zero pronouns have no descriptive information, which makes italmost impossible to calculate semantic similarity between the zero pronoun andits candidate antecedents. To deal with these problems, we aim at exploringlearn- ing algorithms that are capable of generating semantic representationsfor zero pronouns, capturing the intricate related- ness between zero pronounsand candidate antecedents, and meanwhile less dependent on extensive featureengineering. To achieve this goal, in this paper, we propose a zeropronoun-specific neural network for Chinese zero pronoun resolution task.Experimental results show that our approach significantly outperforms thestate-of-the- art method.
arxiv-17400-136 | Multi-agent evolutionary systems for the generation of complex virtual worlds | http://arxiv.org/pdf/1604.05792v1.pdf | author:Jan Kruse, Andy M. Connor category:cs.NE published:2016-04-20 summary:Modern films, games and virtual reality applications are dependent onconvincing computer graphics. Highly complex models are a requirement for thesuccessful delivery of many scenes and environments. While workflows such asrendering, compositing and animation have been streamlined to accommodateincreasing demands, modelling complex models is still a laborious task. Thispaper introduces the computational benefits of an Interactive Genetic Algorithm(IGA) to computer graphics modelling while compensating the effects of userfatigue, a common issue with Interactive Evolutionary Computation. Anintelligent agent is used in conjunction with an IGA that offers the potentialto reduce the effects of user fatigue by learning from the choices made by thehuman designer and directing the search accordingly. This workflow acceleratesthe layout and distribution of basic elements to form complex models. Itcaptures the designer's intent through interaction, and encourages playfuldiscovery.
arxiv-17400-137 | Procedural urban environments for FPS games | http://arxiv.org/pdf/1604.05791v1.pdf | author:Jan Kruse, Ricardo Sosa, Andy M. Connor category:cs.AI cs.HC cs.NE published:2016-04-20 summary:This paper presents a novel approach to procedural generation of urban mapsfor First Person Shooter (FPS) games. A multi-agent evolutionary system isemployed to place streets, buildings and other items inside the Unity3D gameengine, resulting in playable video game levels. A computational agent istrained using machine learning techniques to capture the intent of the gamedesigner as part of the multi-agent system, and to enable a semi-automatedaesthetic selection for the underlying genetic algorithm.
arxiv-17400-138 | Evaluation of Deep Learning based Pose Estimation for Sign Language Recognition | http://arxiv.org/pdf/1602.09065v3.pdf | author:Srujana Gattupalli, Amir Ghaderi, Vassilis Athitsos category:cs.CV published:2016-02-29 summary:Human body pose estimation and hand detection are two important tasks forsystems that perform computer vision-based sign language recognition(SLR).However, both tasks are challenging, especially when the input is color videos,with no depth information. Many algorithms have been proposed in the literaturefor these tasks, and some of the most successful recent algorithms are based ondeep learning. In this paper, we introduce a dataset for human pose estimationfor SLR domain. We evaluate the performance of two deep learning based poseestimation methods, by performing user-independent experiments on our dataset.We also perform transfer learning, and we obtain results that demonstrate thattransfer learning can improve pose estimation accuracy. The dataset and resultsfrom these methods can create a useful baseline for future works.
arxiv-17400-139 | Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection | http://arxiv.org/pdf/1604.05766v1.pdf | author:Krishna Kumar Singh, Fanyi Xiao, Yong Jae Lee category:cs.CV published:2016-04-19 summary:The status quo approach to training object detectors requires expensivebounding box annotations. Our framework takes a markedly different direction:we transfer tracked object boxes from weakly-labeled videos to weakly-labeledimages to automatically generate pseudo ground-truth boxes, which replacemanually annotated bounding boxes. We first mine discriminative regions in theweakly-labeled image collection that frequently/rarely appear in thepositive/negative images. We then match those regions to videos and retrievethe corresponding tracked object boxes. Finally, we design a hough transformalgorithm to vote for the best box to serve as the pseudo GT for each image,and use them to train an object detector. Together, these lead tostate-of-the-art weakly-supervised detection results on the PASCAL 2007 and2010 datasets.
arxiv-17400-140 | Sketching and Neural Networks | http://arxiv.org/pdf/1604.05753v1.pdf | author:Amit Daniely, Nevena Lazic, Yoram Singer, Kunal Talwar category:cs.LG cs.AI published:2016-04-19 summary:High-dimensional sparse data present computational and statistical challengesfor supervised learning. We propose compact linear sketches for reducing thedimensionality of the input, followed by a single layer neural network. We showthat any sparse polynomial function can be computed, on nearly all sparsebinary vectors, by a single layer neural network that takes a compact sketch ofthe vector as input. Consequently, when a set of sparse binary vectors isapproximately separable using a sparse polynomial, there exists a single-layerneural network that takes a short sketch as input and correctly classifiesnearly all the points. Previous work has proposed using sketches to reducedimensionality while preserving the hypothesis class. However, the sketch sizehas an exponential dependence on the degree in the case of polynomialclassifiers. In stark contrast, our approach of using improper learning, usinga larger hypothesis class allows the sketch size to have a logarithmicdependence on the degree. Even in the linear case, our approach allows us toimprove on the pesky $O({1}/{{\gamma}^2})$ dependence of random projections, onthe margin $\gamma$. We empirically show that our approach leads to morecompact neural networks than related methods such as feature hashing at equalor better performance.
arxiv-17400-141 | Syntactic and semantic classification of verb arguments using dependency-based and rich semantic features | http://arxiv.org/pdf/1604.05747v1.pdf | author:Francesco Elia category:cs.CL published:2016-04-19 summary:Corpus Pattern Analysis (CPA) has been the topic of Semeval 2015 Task 15,aimed at producing a system that can aid lexicographers in their efforts tobuild a dictionary of meanings for English verbs using the CPA annotationprocess. CPA parsing is one of the subtasks which this annotation process ismade of and it is the focus of this report. A supervised machine-learningapproach has been implemented, in which syntactic features derived from parsetrees and semantic features derived from WordNet and word embeddings are used.It is shown that this approach performs well, even with the data sparsityissues that characterize the dataset, and can obtain better results than othersystem by a margin of about 4% f-score.
arxiv-17400-142 | Yin and Yang: Balancing and Answering Binary Visual Questions | http://arxiv.org/pdf/1511.05099v5.pdf | author:Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh category:cs.CL cs.CV cs.LG published:2015-11-16 summary:The complex compositional structure of language makes problems at theintersection of vision and language challenging. But language also provides astrong prior that can result in good superficial performance, without theunderlying models truly understanding the visual content. This can hinderprogress in pushing state of art in the computer vision aspects of multi-modalAI. In this paper, we address binary Visual Question Answering (VQA) onabstract scenes. We formulate this problem as visual verification of conceptsinquired in the questions. Specifically, we convert the question to a tuplethat concisely summarizes the visual concept to be detected in the image. Ifthe concept can be found in the image, the answer to the question is "yes", andotherwise "no". Abstract scenes play two roles (1) They allow us to focus onthe high-level semantics of the VQA task as opposed to the low-levelrecognition problems, and perhaps more importantly, (2) They provide us themodality to balance the dataset such that language priors are controlled, andthe role of vision is essential. In particular, we collect fine-grained pairsof scenes for every question, such that the answer to the question is "yes" forone scene, and "no" for the other for the exact same question. Indeed, languagepriors alone do not perform better than chance on our balanced dataset.Moreover, our proposed approach matches the performance of a state-of-the-artVQA approach on the unbalanced dataset, and outperforms it on the balanceddataset.
arxiv-17400-143 | Convolutional Models for Joint Object Categorization and Pose Estimation | http://arxiv.org/pdf/1511.05175v6.pdf | author:Mohamed Elhoseiny, Tarek El-Gaaly, Amr Bakry, Ahmed Elgammal category:cs.CV cs.AI cs.LG published:2015-11-16 summary:In the task of Object Recognition, there exists a dichotomy between thecategorization of objects and estimating object pose, where the formernecessitates a view-invariant representation, while the latter requires arepresentation capable of capturing pose information over different categoriesof objects. With the rise of deep architectures, the prime focus has been onobject category recognition. Deep learning methods have achieved wide successin this task. In contrast, object pose regression using these approaches hasreceived relatively much less attention. In this paper we show how deeparchitectures, specifically Convolutional Neural Networks (CNN), can be adaptedto the task of simultaneous categorization and pose estimation of objects. Weinvestigate and analyze the layers of various CNN models and extensivelycompare between them with the goal of discovering how the layers of distributedrepresentations of CNNs represent object pose information and how thiscontradicts with object category representations. We extensively experiment ontwo recent large and challenging multi-view datasets. Our models achieve betterthan state-of-the-art performance on both datasets.
arxiv-17400-144 | Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd | http://arxiv.org/pdf/1512.07506v2.pdf | author:Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, Tae-Kyun Kim category:cs.CV published:2015-12-23 summary:Object detection and 6D pose estimation in the crowd (scenes with multipleobject instances, severe foreground occlusions and background distractors), hasbecome an important problem in many rapidly evolving technological areas suchas robotics and augmented reality. Single shot-based 6D pose estimators withmanually designed features are still unable to tackle the above challenges,motivating the research towards unsupervised feature learning andnext-best-view estimation. In this work, we present a complete framework forboth single shot-based 6D object pose estimation and next-best-view predictionbased on Hough Forests, the state of the art object pose estimator thatperforms classification and regression jointly. Rather than using manuallydesigned features we a) propose an unsupervised feature learnt fromdepth-invariant patches using a Sparse Autoencoder and b) offer an extensiveevaluation of various state of the art features. Furthermore, taking advantageof the clustering performed in the leaf nodes of Hough Forests, we learn toestimate the reduction of uncertainty in other views, formulating the problemof selecting the next-best-view. To further improve pose estimation, we proposean improved joint registration and hypotheses verification module as a finalrefinement step to reject false detections. We provide two additionalchallenging datasets inspired from realistic scenarios to extensively evaluatethe state of the art and our framework. One is related to domestic environmentsand the other depicts a bin-picking scenario mostly found in industrialsettings. We show that our framework significantly outperforms state of the artboth on public and on our datasets.
arxiv-17400-145 | MAGMA: Multi-level accelerated gradient mirror descent algorithm for large-scale convex composite minimization | http://arxiv.org/pdf/1509.05715v2.pdf | author:Vahan Hovhannisyan, Panos Parpas, Stefanos Zafeiriou category:math.OC cs.CV published:2015-09-18 summary:Composite convex optimization models arise in several applications, and areespecially prevalent in inverse problems with a sparsity inducing norm and ingeneral convex optimization with simple constraints. The most widely usedalgorithms for convex composite models are accelerated first order methods,however they can take a large number of iterations to compute an acceptablesolution for large-scale problems. In this paper we propose to speed up firstorder methods by taking advantage of the structure present in many applicationsand in image processing in particular. Our method is based on multi-leveloptimization methods and exploits the fact that many applications that giverise to large scale models can be modelled using varying degrees of fidelity.We use Nesterov's acceleration techniques together with the multi-levelapproach to achieve $\mathcal{O}(1/\sqrt{\epsilon})$ convergence rate, where$\epsilon$ denotes the desired accuracy. The proposed method has a betterconvergence rate than any other existing multi-level method for convexproblems, and in addition has the same rate as accelerated methods, which isknown to be optimal for first-order methods. Moreover, as our numericalexperiments show, on large-scale face recognition problems our algorithm isseveral times faster than the state of the art.
arxiv-17400-146 | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention | http://arxiv.org/pdf/1502.03044v3.pdf | author:Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio category:cs.LG cs.CV published:2015-02-10 summary:Inspired by recent work in machine translation and object detection, weintroduce an attention based model that automatically learns to describe thecontent of images. We describe how we can train this model in a deterministicmanner using standard backpropagation techniques and stochastically bymaximizing a variational lower bound. We also show through visualization howthe model is able to automatically learn to fix its gaze on salient objectswhile generating the corresponding words in the output sequence. We validatethe use of attention with state-of-the-art performance on three benchmarkdatasets: Flickr8k, Flickr30k and MS COCO.
arxiv-17400-147 | An Enhanced Method For Evaluating Automatic Video Summaries | http://arxiv.org/pdf/1401.3590v3.pdf | author:Karim M. Mahmoud category:cs.CV cs.IR published:2014-01-14 summary:Evaluation of automatic video summaries is a challenging problem. In the pastyears, some evaluation methods are presented that utilize only a single featurelike color feature to detect similarity between automatic video summaries andground-truth user summaries. One of the drawbacks of using a single feature isthat sometimes it gives a false similarity detection which makes the assessmentof the quality of the generated video summary less perceptual and not accurate.In this paper, a novel method for evaluating automatic video summaries ispresented. This method is based on comparing automatic video summariesgenerated by video summarization techniques with ground-truth user summaries.The objective of this evaluation method is to quantify the quality of videosummaries, and allow comparing different video summarization techniquesutilizing both color and texture features of the video frames and using theBhattacharya distance as a dissimilarity measure due to its advantages. OurExperiments show that the proposed evaluation method overcomes the drawbacks ofother methods and gives a more perceptual evaluation of the quality of theautomatic video summaries.
arxiv-17400-148 | Online Human Action Detection using Joint Classification-Regression Recurrent Neural Networks | http://arxiv.org/pdf/1604.05633v1.pdf | author:Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan, Jiaying Liu category:cs.CV published:2016-04-19 summary:Human action recognition from well-segmented 3D skeleton data has beenintensively studied and attracting an increasing attention. Online actiondetection goes one step further and is more challenging, which identifies theaction type and localizes the action positions on the fly from the untrimmedstream. In this paper, we study the problem of online action detection from thestreaming skeleton data. We propose a multi-task end-to-end JointClassification-Regression Recurrent Neural Network to better explore the actiontype and temporal localization information. By employing a joint classificationand regression optimization objective, this network is capable of automaticallylocalizing the start and end points of actions more accurately. Specifically,by leveraging the merits of the deep Long Short-Term Memory (LSTM) subnetwork,the proposed model automatically captures the complex long-range temporaldynamics, which naturally avoids the typical sliding window design and thusensures high computational efficiency. Furthermore, the subtask of regressionoptimization provides the ability to forecast the action prior to itsoccurrence. To evaluate our proposed model, we build a large streaming videodataset with annotations. Experimental results on our dataset and the publicG3D dataset both demonstrate very promising performance of our scheme.
arxiv-17400-149 | Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems | http://arxiv.org/pdf/1511.06931v6.pdf | author:Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra, Alexander Miller, Arthur Szlam, Jason Weston category:cs.CL cs.LG published:2015-11-21 summary:A long-term goal of machine learning is to build intelligent conversationalagents. One recent popular approach is to train end-to-end models on a largeamount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals& Le, 2015; Shang et al., 2015). However, this approach leaves many questionsunanswered as an understanding of the precise successes and shortcomings ofeach model is hard to assess. A contrasting recent proposal are the bAbI tasks(Weston et al., 2015b) which are synthetic data that measure the ability oflearning machines at various reasoning tasks over toy language. Unfortunately,those tests are very small and hence may encourage methods that do not scale.In this work, we propose a suite of new tasks of a much larger scale thatattempt to bridge the gap between the two regimes. Choosing the domain ofmovies, we provide tasks that test the ability of models to answer factualquestions (utilizing OMDB), provide personalization (utilizing MovieLens),carry short conversations about the two, and finally to perform on naturaldialogs from Reddit. We provide a dataset covering 75k movie entities and with3.5M training examples. We present results of various models on these tasks,and evaluate their performance.
arxiv-17400-150 | Right whale recognition using convolutional neural networks | http://arxiv.org/pdf/1604.05605v1.pdf | author:Andrei Polzounov, Ilmira Terpugova, Deividas Skiparis, Andrei Mihai category:cs.CV 68 published:2016-04-19 summary:We studied the feasibility of recognizing individual right whales (Eubalaenaglacialis) using convolutional neural networks. Prior studies have shown thatCNNs can be used in wide range of classification and categorization tasks suchas automated human face recognition. To test applicability of deep learning towhale recognition we have developed several models based on best practices fromliterature. Here, we describe the performance of the models. We conclude thatmachine recognition of whales is feasible and comment on the difficulty of theproblem
arxiv-17400-151 | Robust Scene Text Recognition with Automatic Rectification | http://arxiv.org/pdf/1603.03915v2.pdf | author:Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai category:cs.CV published:2016-03-12 summary:Recognizing text in natural images is a challenging task with many unsolvedproblems. Different from those in documents, words in natural images oftenpossess irregular shapes, which are caused by perspective distortion, curvedcharacter placement, etc. We propose RARE (Robust text recognizer withAutomatic REctification), a recognition model that is robust to irregular text.RARE is a specially-designed deep neural network, which consists of a SpatialTransformer Network (STN) and a Sequence Recognition Network (SRN). In testing,an image is firstly rectified via a predicted Thin-Plate-Spline (TPS)transformation, into a more "readable" image for the following SRN, whichrecognizes text through a sequence recognition approach. We show that the modelis able to recognize several types of irregular text, including perspectivetext and curved text. RARE is end-to-end trainable, requiring only images andassociated text labels, making it convenient to train and deploy the model inpractical systems. State-of-the-art or highly-competitive performance achievedon several benchmarks well demonstrates the effectiveness of the proposedmodel.
arxiv-17400-152 | WarpNet: Weakly Supervised Matching for Single-view Reconstruction | http://arxiv.org/pdf/1604.05592v1.pdf | author:Angjoo Kanazawa, David W. Jacobs, Manmohan Chandraker category:cs.CV published:2016-04-19 summary:We present an approach to matching images of objects in fine-grained datasetswithout using part annotations, with an application to the challenging problemof weakly supervised single-view reconstruction. This is in contrast to priorworks that require part annotations, since matching objects across class andpose variations is challenging with appearance features alone. We overcome thischallenge through a novel deep learning architecture, WarpNet, that aligns anobject in one image with a different object in another. We exploit thestructure of the fine-grained dataset to create artificial data for trainingthis network in an unsupervised-discriminative learning approach. The output ofthe network acts as a spatial prior that allows generalization at test time tomatch real images across variations in appearance, viewpoint and articulation.On the CUB-200-2011 dataset of bird categories, we improve the AP over anappearance-only network by 13.6%. We further demonstrate that our WarpNetmatches, together with the structure of fine-grained datasets, allowsingle-view reconstructions with quality comparable to using annotated pointcorrespondences.
arxiv-17400-153 | Locating a Small Cluster Privately | http://arxiv.org/pdf/1604.05590v1.pdf | author:Kobbi Nissim, Uri Stemmer, Salil Vadhan category:cs.DS cs.CR cs.LG published:2016-04-19 summary:We present a new algorithm for locating a small cluster of points withdifferential privacy [Dwork, McSherry, Nissim, and Smith, 2006]. Our algorithmhas implications to private data exploration, clustering, and removal ofoutliers. Furthermore, we use it to significantly relax the requirements of thesample and aggregate technique [Nissim, Raskhodnikova, and Smith, 2007], whichallows compiling of "off the shelf" (non-private) analyses into analyses thatpreserve differential privacy.
arxiv-17400-154 | End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks | http://arxiv.org/pdf/1604.05091v2.pdf | author:Peter Ondruska, Julie Dequaire, Dominic Zeng Wang, Ingmar Posner category:cs.LG cs.AI cs.CV cs.NE cs.RO published:2016-04-18 summary:In this work we present a novel end-to-end framework for tracking andclassifying a robot's surroundings in complex, dynamic and only partiallyobservable real-world environments. The approach deploys a recurrent neuralnetwork to filter an input stream of raw laser measurements in order todirectly infer object locations, along with their identity in both visible andoccluded areas. To achieve this we first train the network using unsupervisedDeep Tracking, a recently proposed theoretical framework for end-to-end spaceoccupancy prediction. We show that by learning to track on a large amount ofunsupervised data, the network creates a rich internal representation of itsenvironment which we in turn exploit through the principle of inductivetransfer of knowledge to perform the task of it's semantic classification. As aresult, we show that only a small amount of labelled data suffices to steer thenetwork towards mastering this additional task. Furthermore we propose a novelrecurrent neural network architecture specifically tailored to tracking andsemantic classification in real-world robotics applications. We demonstrate thetracking and classification performance of the method on real-world datacollected at a busy road junction. Our evaluation shows that the proposedend-to-end framework compares favourably to a state-of-the-art, model-freetracking solution and that it outperforms a conventional one-shot trainingscheme for semantic classification.
arxiv-17400-155 | Using Apache Lucene to Search Vector of Locally Aggregated Descriptors | http://arxiv.org/pdf/1604.05576v1.pdf | author:Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Claudio Gennaro, Lucia Vadicamo category:cs.CV cs.IR published:2016-04-19 summary:Surrogate Text Representation (STR) is a profitable solution to efficientsimilarity search on metric space using conventional text search engines, suchas Apache Lucene. This technique is based on comparing the permutations of somereference objects in place of the original metric distance. However, theAchilles heel of STR approach is the need to reorder the result set of thesearch according to the metric distance. This forces to use a support databaseto store the original objects, which requires efficient random I/O on a fastsecondary memory (such as flash-based storages). In this paper, we propose toextend the Surrogate Text Representation to specifically address a class ofvisual metric objects known as Vector of Locally Aggregated Descriptors (VLAD).This approach is based on representing the individual sub-vectors forming theVLAD vector with the STR, providing a finer representation of the vector andenabling us to get rid of the reordering phase. The experiments on a publiclyavailable dataset show that the extended STR outperforms the baseline STRachieving satisfactory performance near to the one obtained with the originalVLAD vectors.
arxiv-17400-156 | Automatic Content-aware Non-Photorealistic Rendering of Images | http://arxiv.org/pdf/1604.01962v4.pdf | author:Akshay Gadi Patil, Shanmuganathan Raman category:cs.CV published:2016-04-07 summary:Non-photorealistic rendering techniques work on image features and oftenmanipulate a set of characteristics such as edges and texture to achieve adesired depiction of the scene. Most computational photography methodsdecompose an image using edge preserving filters and work on the resulting baseand detail layers independently to achieve desired visual effects. We propose anew approach for content-aware non-photorealistic rendering of images where wemanipulate the visually salient and the non-salient regions separately. Wepropose a novel content-aware framework in order to render an image forapplications such as detail exaggeration, artificial blurring and imageabstraction. The processed regions of the image are blended seamlessly for allthese applications. We demonstrate that content awareness of the proposedmethod leads to automatic generation of non-photorealistic rendering of thesame image for the different applications mentioned above.
arxiv-17400-157 | An Attentive Neural Architecture for Fine-grained Entity Type Classification | http://arxiv.org/pdf/1604.05525v1.pdf | author:Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel category:cs.CL published:2016-04-19 summary:In this work we propose a novel attention-based neural network model for thetask of fine-grained entity type classification that unlike previously proposedmodels recursively composes representations of entity mention contexts. Ourmodel achieves state-of-the-art performance with 74.94% loose micro F1-score onthe well-established FIGER dataset, a relative improvement of 2.59%. We alsoinvestigate the behavior of the attention mechanism of our model and observethat it can learn contextual linguistic expressions that indicate thefine-grained category memberships of an entity.
arxiv-17400-158 | Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR Control with the Ordered l1-Norm | http://arxiv.org/pdf/1511.05864v2.pdf | author:Sangkyun Lee, Damian Brzyski, Malgorzata Bogdan category:stat.ML math.OC published:2015-11-18 summary:In this paper we propose a primal-dual proximal extragradient algorithm tosolve the generalized Dantzig selector (GDS) estimation problem, based on a newconvex-concave saddle-point (SP) reformulation. Our new formulation makes itpossible to adopt recent developments in saddle-point optimization, to achievethe optimal $O(1/k)$ rate of convergence. Compared to the optimal non-SPalgorithms, ours do not require specification of sensitive parameters thataffect algorithm performance or solution quality. We also provide a newanalysis showing a possibility of local acceleration to achieve the rate of$O(1/k^2)$ in special cases even without strong convexity or strong smoothness.As an application, we propose a GDS equipped with the ordered $\ell_1$-norm,showing its false discovery rate control properties in variable selection.Algorithm performance is compared between ours and other alternatives,including the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox,and the accelerated hybrid proximal extragradient techniques.
arxiv-17400-159 | Exploring Segment Representations for Neural Segmentation Models | http://arxiv.org/pdf/1604.05499v1.pdf | author:Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, Ting Liu category:cs.CL published:2016-04-19 summary:Many natural language processing (NLP) tasks can be generalized intosegmentation problem. In this paper, we combine semi-CRF with neural network tosolve NLP segmentation tasks. Our model represents a segment both by composingthe input units and embedding the entire segment. We thoroughly study differentcomposition functions and different segment embeddings. We conduct extensiveexperiments on two typical segmentation tasks: named entity recognition (NER)and Chinese word segmentation (CWS). Experimental results show that our neuralsemi-CRF model benefits from representing the entire segment and achieves thestate-of-the-art performance on CWS benchmark dataset and competitive resultson the CoNLL03 dataset.
arxiv-17400-160 | Deep Saliency with Encoded Low level Distance Map and High Level Features | http://arxiv.org/pdf/1604.05495v1.pdf | author:Gayoung Lee, Yu-Wing Tai, Junmo Kim category:cs.CV published:2016-04-19 summary:Recent advances in saliency detection have utilized deep learning to obtainhigh level features to detect salient regions in a scene. These advances havedemonstrated superior results over previous works that utilize hand-crafted lowlevel features for saliency detection. In this paper, we demonstrate thathand-crafted features can provide complementary information to enhanceperformance of saliency detection that utilizes only high level features. Ourmethod utilizes both high level and low level features for saliency detectionunder a unified deep learning framework. The high level features are extractedusing the VGG-net, and the low level features are compared with other parts ofan image to form a low level distance map. The low level distance map is thenencoded using a convolutional neural network(CNN) with multiple 1X1convolutional and ReLU layers. We concatenate the encoded low level distancemap and the high level features, and connect them to a fully connected neuralnetwork classifier to evaluate the saliency of a query region. Our experimentsshow that our method can further improve the performance of state-of-the-artdeep learning-based saliency detection methods.
arxiv-17400-161 | Understanding Rating Behaviour and Predicting Ratings by Identifying Representative Users | http://arxiv.org/pdf/1604.05468v1.pdf | author:Rahul Kamath, Masanao Ochi, Yutaka Matsuo category:cs.IR cs.AI cs.CL cs.LG published:2016-04-19 summary:Online user reviews describing various products and services are now abundanton the web. While the information conveyed through review texts and ratings iseasily comprehensible, there is a wealth of hidden information in them that isnot immediately obvious. In this study, we unlock this hidden value behind userreviews to understand the various dimensions along which users rate products.We learn a set of users that represent each of these dimensions and use theirratings to predict product ratings. Specifically, we work with restaurantreviews to identify users whose ratings are influenced by dimensions like'Service', 'Atmosphere' etc. in order to predict restaurant ratings andunderstand the variation in rating behaviour across different cuisines. Whileprevious approaches to obtaining product ratings require either a large numberof user ratings or a few review texts, we show that it is possible to predictratings with few user ratings and no review text. Our experiments show that ourapproach outperforms other conventional methods by 16-27% in terms of RMSE.
arxiv-17400-162 | An Online Structural Plasticity Rule for Generating Better Reservoirs | http://arxiv.org/pdf/1604.05459v1.pdf | author:Subhrajit Roy, Arindam Basu category:cs.NE published:2016-04-19 summary:In this article, a novel neuro-inspired low-resolution online unsupervisedlearning rule is proposed to train the reservoir or liquid of Liquid StateMachine. The liquid is a sparsely interconnected huge recurrent network ofspiking neurons. The proposed learning rule is inspired from structuralplasticity and trains the liquid through formation and elimination of synapticconnections. Hence, the learning involves rewiring of the reservoir connectionssimilar to structural plasticity observed in biological neural networks. Thenetwork connections can be stored as a connection matrix and updated in memoryby using Address Event Representation (AER) protocols which are generallyemployed in neuromorphic systems. On investigating the 'pairwise separationproperty' we find that trained liquids provide 1.36 $\pm$ 0.18 times moreinter-class separation while retaining similar intra-class separation ascompared to random liquids. Moreover, analysis of the 'linear separationproperty' reveals that trained liquids are 2.05 $\pm$ 0.27 times better thanrandom liquids. Furthermore, we show that our liquids are able to retain the'generalization' ability and 'generality' of random liquids. A memory analysisshows that trained liquids have 83.67 $\pm$ 5.79 ms longer fading memory thanrandom liquids which have shown 92.8 $\pm$ 5.03 ms fading memory for aparticular type of spike train inputs. We also throw some light on the dynamicsof the evolution of recurrent connections within the liquid. Moreover, comparedto 'Separation Driven Synaptic Modification' - a recently proposed algorithmfor iteratively refining reservoirs, our learning rule provides 9.30%, 15.21%and 12.52% more liquid separations and 2.8%, 9.1% and 7.9% betterclassification accuracies for four, eight and twelve class pattern recognitiontasks respectively.
arxiv-17400-163 | Parts for the Whole: The DCT Norm for Extreme Visual Recovery | http://arxiv.org/pdf/1604.05451v1.pdf | author:Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, Chao Xu category:cs.CV published:2016-04-19 summary:Here we study the extreme visual recovery problem, in which over 90\% ofpixel values in a given image are missing. Existing low rank-based algorithmsare only effective for recovering data with at most 90\% missing values. Thus,we exploit visual data's smoothness property to help solve this challengingextreme visual recovery problem. Based on the Discrete Cosine Transformation(DCT), we propose a novel DCT norm that involves all pixels and produces smoothestimations in any view. Our theoretical analysis shows that the totalvariation (TV) norm, which only achieves local smoothness, is a special case ofthe proposed DCT norm. We also develop a new visual recovery algorithm byminimizing the DCT and nuclear norms to achieve a more visually pleasingestimation. Experimental results on a benchmark image dataset demonstrate thatthe proposed approach is superior to state-of-the-art methods in terms of peaksignal-to-noise ratio and structural similarity.
arxiv-17400-164 | Streaming Label Learning for Modeling Labels on the Fly | http://arxiv.org/pdf/1604.05449v1.pdf | author:Shan You, Chang Xu, Yunhe Wang, Chao Xu, Dacheng Tao category:stat.ML cs.LG published:2016-04-19 summary:It is challenging to handle a large volume of labels in multi-label learning.However, existing approaches explicitly or implicitly assume that all thelabels in the learning process are given, which could be easily violated inchanging environments. In this paper, we define and study streaming labellearning (SLL), i.e., labels are arrived on the fly, to model newly arrivedlabels with the help of the knowledge learned from past labels. The core of SLLis to explore and exploit the relationships between new labels and past labelsand then inherit the relationship into hypotheses of labels to boost theperformance of new classifiers. In specific, we use the labelself-representation to model the label relationship, and SLL will be dividedinto two steps: a regression problem and a empirical risk minimization (ERM)problem. Both problems are simple and can be efficiently solved. We furthershow that SLL can generate a tighter generalization error bound for new labelsthan the general ERM framework with trace norm or Frobenius normregularization. Finally, we implement extensive experiments on variousbenchmark datasets to validate the new setting. And results show that SLL caneffectively handle the constantly emerging new labels and provides excellentclassification performance.
arxiv-17400-165 | Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics | http://arxiv.org/pdf/1501.02629v4.pdf | author:Stéphan Clémençon, Aurélien Bellet, Igor Colin category:stat.ML cs.LG published:2015-01-12 summary:In a wide range of statistical learning problems such as ranking, clusteringor metric learning among others, the risk is accurately estimated by$U$-statistics of degree $d\geq 1$, i.e. functionals of the training data withlow variance that take the form of averages over $k$-tuples. From acomputational perspective, the calculation of such statistics is highlyexpensive even for a moderate sample size $n$, as it requires averaging$O(n^d)$ terms. This makes learning procedures relying on the optimization ofsuch data functionals hardly feasible in practice. It is the major goal of thispaper to show that, strikingly, such empirical risks can be replaced bydrastically computationally simpler Monte-Carlo estimates based on $O(n)$ termsonly, usually referred to as incomplete $U$-statistics, without damaging the$O_{\mathbb{P}}(1/\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)procedures. For this purpose, we establish uniform deviation results describingthe error made when approximating a $U$-process by its incomplete version underappropriate complexity assumptions. Extensions to model selection, fast ratesituations and various sampling techniques are also considered, as well as anapplication to stochastic gradient descent for ERM. Finally, numerical examplesare displayed in order to provide strong empirical evidence that the approachwe promote largely surpasses more naive subsampling techniques.
arxiv-17400-166 | Improving Raw Image Storage Efficiency by Exploiting Similarity | http://arxiv.org/pdf/1604.05442v1.pdf | author:Binqi Zhang, Chen Wang, Bing Bing Zhou, Albert Y. Zomaya category:cs.DC cs.CV published:2016-04-19 summary:To improve the temporal and spatial storage efficiency, researchers haveintensively studied various techniques, including compression anddeduplication. Through our evaluation, we find that methods such as photo tagsor local features help to identify the content-based similar- ity between rawimages. The images can then be com- pressed more efficiently to get betterstorage space sav- ings. Furthermore, storing similar raw images togetherenables rapid data sorting, searching and retrieval if the images are stored ina distributed and large-scale envi- ronment by reducing fragmentation. In thispaper, we evaluated the compressibility by designing experiments and observingthe results. We found that on a statistical basis the higher similarity photoshave, the better com- pression results are. This research helps provide a cluefor future large-scale storage system design.
arxiv-17400-167 | Comparative Study of Instance Based Learning and Back Propagation for Classification Problems | http://arxiv.org/pdf/1604.05429v1.pdf | author:Nadia Kanwal, Erkan Bostanci category:cs.LG published:2016-04-19 summary:The paper presents a comparative study of the performance of Back Propagationand Instance Based Learning Algorithm for classification tasks. The study iscarried out by a series of experiments will all possible combinations ofparameter values for the algorithms under evaluation. The algorithm'sclassification accuracy is compared over a range of datasets and measurementslike Cross Validation, Kappa Statistics, Root Mean Squared Value and TruePositive vs False Positive rate have been used to evaluate their performance.Along with performance comparison, techniques of handling missing values havealso been compared that include Mean or Mode replacement and MultipleImputation. The results showed that parameter adjustment plays vital role inimproving an algorithm's accuracy and therefore, Back Propagation has shownbetter results as compared to Instance Based Learning. Furthermore, the problemof missing values was better handled by Multiple imputation method, however,not suitable for less amount of data.
arxiv-17400-168 | Multi-Source Multi-View Clustering via Discrepancy Penalty | http://arxiv.org/pdf/1604.04029v2.pdf | author:Weixiang Shao, Jiawei Zhang, Lifang He, Philip S. Yu category:cs.LG published:2016-04-14 summary:With the advance of technology, entities can be observed in multiple views.Multiple views containing different types of features can be used forclustering. Although multi-view clustering has been successfully applied inmany applications, the previous methods usually assume the complete instancemapping between different views. In many real-world applications, informationcan be gathered from multiple sources, while each source can contain multipleviews, which are more cohesive for learning. The views under the same sourceare usually fully mapped, but they can be very heterogeneous. Moreover, themappings between different sources are usually incomplete and partiallyobserved, which makes it more difficult to integrate all the views acrossdifferent sources. In this paper, we propose MMC (Multi-source Multi-viewClustering), which is a framework based on collective spectral clustering witha discrepancy penalty across sources, to tackle these challenges. MMC hasseveral advantages compared with other existing methods. First, MMC can dealwith incomplete mapping between sources. Second, it considers the disagreementsbetween sources while treating views in the same source as a cohesive set.Third, MMC also tries to infer the instance similarities across sources toenhance the clustering performance. Extensive experiments conducted onreal-world data demonstrate the effectiveness of the proposed approach.
arxiv-17400-169 | Cognitive state classification using transformed fMRI data | http://arxiv.org/pdf/1604.05413v1.pdf | author:Hariharan Ramasangu, Neelam Sinha category:cs.CV published:2016-04-19 summary:One approach, for understanding human brain functioning, is to analyze thechanges in the brain while performing cognitive tasks. Towards this, FunctionalMagnetic Resonance (fMR) images of subjects performing well-defined tasks arewidely utilized for task-specific analyses. In this work, we propose aprocedure to enable classification between two chosen cognitive tasks, usingtheir respective fMR image sequences. The time series of expert-markedanatomically-mapped relevant voxels are processed and fed as input to theclassical Naive Bayesian and SVM classifiers. The processing involves use ofrandom sieve function, phase information in the data transformed using Fourierand Hilbert transformations. This processing results in improvedclassification, as against using the voxel intensities directly, asillustrated. The novelty of the proposed method lies in utilizing the phaseinformation in the transformed domain, for classifying between the cognitivetasks along with random sieve function chosen with a particular probabilitydistribution. The proposed classification procedure is applied on a publiclyavailable dataset, StarPlus data, with 6 subjects performing the two distinctcognitive tasks of watching either a picture or a sentence. The classificationaccuracy stands at an average of 65.6%(using Naive Bayes classifier) and76.4%(using SVM classifier) for raw data. The corresponding classificationaccuracy stands at 96.8% and 97.5% for Fourier transformed data. For Hilberttransformed data, it is 93.7% and 99%, for 6 subjects, on 2 cognitive tasks.
arxiv-17400-170 | Neural Network Support Vector Detection via a Soft-Label, Hybrid K-Means Classifier | http://arxiv.org/pdf/1602.03822v5.pdf | author:Robert A. Murphy category:cs.LG 60D05, 62C99 published:2016-02-11 summary:We use random geometric graphs to describe clusters of higher dimensionaldata points which are bijectively mapped to a (possibly) lower dimensionalspace where an equivalent random cluster model is used to calculate theexpected number of modes to be found when separating the data of a multi-modaldata set into distinct clusters. Furthermore, as a function of the expectednumber of modes and the number of data points in the sample, an upper bound ona given distance measure is found such that data points have the greatestcorrelation if their mutual distances from a common center is less than orequal to the calculated bound. Anomalies are exposed, which lie outside of theunion of all regularized clusters of data points. Similar to finding a hyperplane which can be shifted along its normal toexpose the maximal distance between binary classes, it is shown that the unionof regularized clusters can be used to define a hyperplane which can be shiftedby a certain amount to separate the data into binary classes and that theshifted hyperplane defines the activation function for a two-classdiscriminating neural network. Lastly, this neural network is used to detectthe set of support vectors which determines the maximally-separating regionbetween the binary classes.
arxiv-17400-171 | End-to-End Training of Deep Visuomotor Policies | http://arxiv.org/pdf/1504.00702v5.pdf | author:Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel category:cs.LG cs.CV cs.RO published:2015-04-02 summary:Policy search methods can allow robots to learn control policies for a widerange of tasks, but practical applications of policy search often requirehand-engineered components for perception, state estimation, and low-levelcontrol. In this paper, we aim to answer the following question: does trainingthe perception and control systems jointly end-to-end provide betterperformance than training each component separately? To this end, we develop amethod that can be used to learn policies that map raw image observationsdirectly to torques at the robot's motors. The policies are represented by deepconvolutional neural networks (CNNs) with 92,000 parameters, and are trainedusing a partially observed guided policy search method, which transforms policysearch into supervised learning, with supervision provided by a simpletrajectory-centric reinforcement learning method. We evaluate our method on arange of real-world manipulation tasks that require close coordination betweenvision and control, such as screwing a cap onto a bottle, and present simulatedcomparisons to a range of prior policy search methods.
arxiv-17400-172 | An Adaptive Learning Mechanism for Selection of Increasingly More Complex Systems | http://arxiv.org/pdf/1604.05393v1.pdf | author:Fouad Khan category:cs.IT cs.LG math.IT published:2016-04-19 summary:Recently it has been demonstrated that causal entropic forces can lead to theemergence of complex phenomena associated with human cognitive niche such astool use and social cooperation. Here I show that even more fundamental traitsassociated with human cognition such as 'self-awareness' can easily bedemonstrated to be arising out of merely a selection for 'better regulators';i.e. systems which respond comparatively better to threats to their existencewhich are internal to themselves. A simple model demonstrates how indeed theaverage self-awareness for a universe of systems continues to rise as lessself-aware systems are eliminated. The model also demonstrates however that themaximum attainable self-awareness for any system is limited by the plasticityand energy availability for that typology of systems. I argue that this rise inself-awareness may be the reason why systems tend towards greater complexity.
arxiv-17400-173 | Learning Dense Correspondence via 3D-guided Cycle Consistency | http://arxiv.org/pdf/1604.05383v1.pdf | author:Tinghui Zhou, Philipp Krähenbühl, Mathieu Aubry, Qixing Huang, Alexei A. Efros category:cs.CV published:2016-04-18 summary:Discriminative deep learning approaches have shown impressive results forproblems where human-labeled ground truth is plentiful, but what about taskswhere labels are difficult or impossible to obtain? This paper tackles one suchproblem: establishing dense visual correspondence across different objectinstances. For this task, although we do not know what the ground-truth is, weknow it should be consistent across instances of that category. We exploit thisconsistency as a supervisory signal to train a convolutional neural network topredict cross-instance correspondences between pairs of images depictingobjects of the same category. For each pair of training images we find anappropriate 3D CAD model and render two synthetic views to link in with thepair, establishing a correspondence flow 4-cycle. We use ground-truthsynthetic-to-synthetic correspondences, provided by the rendering engine, totrain a ConvNet to predict synthetic-to-real, real-to-real andreal-to-synthetic correspondences that are cycle-consistent with theground-truth. At test time, no CAD models are required. We demonstrate that ourend-to-end trained ConvNet supervised by cycle-consistency outperformsstate-of-the-art pairwise matching methods in correspondence-related tasks.
arxiv-17400-174 | Churn analysis using deep convolutional neural networks and autoencoders | http://arxiv.org/pdf/1604.05377v1.pdf | author:Artit Wangperawong, Cyrille Brun, Olav Laudy, Rujikorn Pavasuthipaisit category:stat.ML cs.LG cs.NE published:2016-04-18 summary:Customer temporal behavioral data was represented as images in order toperform churn prediction by leveraging deep learning architectures prominent inimage classification. Supervised learning was performed on labeled data of over6 million customers using deep convolutional neural networks, which achieved anAUC of 0.743 on the test dataset using no more than 12 temporal features foreach customer. Unsupervised learning was conducted using autoencoders to betterunderstand the reasons for customer churn. Images that maximally activate thehidden units of an autoencoder trained with churned customers reveal ampleopportunities for action to be taken to prevent churn among strong data, novoice users.
arxiv-17400-175 | Clustering Comparable Corpora of Russian and Ukrainian Academic Texts: Word Embeddings and Semantic Fingerprints | http://arxiv.org/pdf/1604.05372v1.pdf | author:Andrey Kutuzov, Mikhail Kopotev, Tatyana Sviridenko, Lyubov Ivanova category:cs.CL published:2016-04-18 summary:We present our experience in applying distributional semantics (neural wordembeddings) to the problem of representing and clustering documents in abilingual comparable corpus. Our data is a collection of Russian and Ukrainianacademic texts, for which topics are their academic fields. In order to buildlanguage-independent semantic representations of these documents, we trainneural distributional models on monolingual corpora and learn the optimallinear transformation of vectors from one language to another. The resultingvectors are then used to produce `semantic fingerprints' of documents, servingas input to a clustering algorithm. The presented method is compared to several baselines including `orthographictranslation' with Levenshtein edit distance and outperforms them by a largemargin. We also show that language-independent `semantic fingerprints' aresuperior to multi-lingual clustering algorithms proposed in the previous work,at the same time requiring less linguistic resources.
arxiv-17400-176 | Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems | http://arxiv.org/pdf/1510.06335v2.pdf | author:Matteo Venanzi, John Guiver, Pushmeet Kohli, Nick Jennings category:cs.AI cs.LG published:2015-10-21 summary:Crowdsourcing systems commonly face the problem of aggregating multiplejudgments provided by potentially unreliable workers. In addition, severalaspects of the design of efficient crowdsourcing processes, such as definingworker's bonuses, fair prices and time limits of the tasks, involve knowledgeof the likely duration of the task at hand. Bringing this together, in thiswork we introduce a new time--sensitive Bayesian aggregation method thatsimultaneously estimates a task's duration and obtains reliable aggregations ofcrowdsourced judgments. Our method, called BCCTime, builds on the key insightthat the time taken by a worker to perform a task is an important indicator ofthe likely quality of the produced judgment. To capture this, BCCTime useslatent variables to represent the uncertainty about the workers' completiontime, the tasks' duration and the workers' accuracy. To relate the quality of ajudgment to the time a worker spends on a task, our model assumes that eachtask is completed within a latent time window within which all workers with apropensity to genuinely attempt the labelling task (i.e., no spammers) areexpected to submit their judgments. In contrast, workers with a lowerpropensity to valid labeling, such as spammers, bots or lazy labelers, areassumed to perform tasks considerably faster or slower than the time requiredby normal workers. Specifically, we use efficient message-passing Bayesianinference to learn approximate posterior probabilities of (i) the confusionmatrix of each worker, (ii) the propensity to valid labeling of each worker,(iii) the unbiased duration of each task and (iv) the true label of each task.Using two real-world public datasets for entity linking tasks, we show thatBCCTime produces up to 11% more accurate classifications and up to 100% moreinformative estimates of a task's duration compared to state-of-the-artmethods.
arxiv-17400-177 | Uniform Coherence | http://arxiv.org/pdf/1604.05288v1.pdf | author:Scott Garrabrant, Benya Fallenstein, Abram Demski, Nate Soares category:cs.AI cs.LG math.PR published:2016-04-18 summary:While probability theory is normally applied to external environments, therehas been some recent interest in probabilistic modeling of the outputs ofcomputations that are too expensive to run. Since mathematical logic is apowerful tool for reasoning about computer programs, we consider this problemfrom the perspective of integrating probability and logic. Recent work onassigning probabilities to mathematical statements has used the concept ofcoherent distributions, which satisfy logical constraints such as theprobability of a sentence and its negation summing to one. Although there arealgorithms which converge to a coherent probability distribution in the limit,this yields only weak guarantees about finite approximations of thesedistributions. In our setting, this is a significant limitation: Coherentdistributions assign probability one to all statements provable in a specificlogical theory, such as Peano Arithmetic, which can prove what the output ofany terminating computation is; thus, a coherent distribution must assignprobability one to the output of any terminating computation. To modeluncertainty about computations, we propose to work with approximations tocoherent distributions. We introduce uniform coherence, a strengthening ofcoherence that provides appropriate constraints on finite approximations, andpropose an algorithm which satisfies this criterion.
arxiv-17400-178 | Adaptive Computation Time for Recurrent Neural Networks | http://arxiv.org/pdf/1603.08983v4.pdf | author:Alex Graves category:cs.NE published:2016-03-29 summary:This paper introduces Adaptive Computation Time (ACT), an algorithm thatallows recurrent neural networks to learn how many computational steps to takebetween receiving an input and emitting an output. ACT requires minimal changesto the network architecture, is deterministic and differentiable, and does notadd any noise to the parameter gradients. Experimental results are provided forfour synthetic problems: determining the parity of binary vectors, applyingbinary logic operations, adding integers, and sorting real numbers. Overall,performance is dramatically improved by the use of ACT, which successfullyadapts the number of computational steps to the requirements of the problem. Wealso present character-level language modelling results on the Hutter prizeWikipedia dataset. In this case ACT does not yield large gains in performance;however it does provide intriguing insight into the structure of the data, withmore computation allocated to harder-to-predict transitions, such as spacesbetween words and ends of sentences. This suggests that ACT or other adaptivecomputation methods could provide a generic method for inferring segmentboundaries in sequence data.
arxiv-17400-179 | Asymptotic Convergence in Online Learning with Unbounded Delays | http://arxiv.org/pdf/1604.05280v1.pdf | author:Scott Garrabrant, Nate Soares, Jessica Taylor category:cs.LG cs.AI math.PR published:2016-04-18 summary:We study the problem of predicting the results of computations that are tooexpensive to run, via the observation of the results of smaller computations.We model this as an online learning problem with delayed feedback, where thelength of the delay is unbounded, which we study mainly in a stochasticsetting. We show that in this setting, consistency is not possible in general,and that optimal forecasters might not have average regret going to zero.However, it is still possible to give algorithms that converge asymptoticallyto Bayes-optimal predictions, by evaluating forecasters on specific sparseindependent subsequences of their predictions. We give an algorithm that doesthis, which converges asymptotically on good behavior, and give very weakbounds on how long it takes to converge. We then relate our results back to theproblem of predicting large computations in a deterministic setting.
arxiv-17400-180 | Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts | http://arxiv.org/pdf/1604.05559v1.pdf | author:Melvyn Drag, Gauthaman Vasudevan category:cs.CL published:2016-04-18 summary:We show that an efficient and popular method for calculating bigramfrequencies is unsuitable for bodies of short texts and offer a simplealternative. Our method has the same computational complexity as the old methodand offers an exact count instead of an approximation.
arxiv-17400-181 | Chained Gaussian Processes | http://arxiv.org/pdf/1604.05263v1.pdf | author:Alan D. Saul, James Hensman, Aki Vehtari, Neil D. Lawrence category:stat.ML cs.LG published:2016-04-18 summary:Gaussian process models are flexible, Bayesian non-parametric approaches toregression. Properties of multivariate Gaussians mean that they can be combinedlinearly in the manner of additive models and via a link function (like ingeneralized linear models) to handle non-Gaussian data. However, the linkfunction formalism is restrictive, link functions are always invertible andmust convert a parameter of interest to a linear combination of the underlyingprocesses. There are many likelihoods and models where a non-linear combinationis more appropriate. We term these more general models Chained GaussianProcesses: the transformation of the GPs to the likelihood parameters will notgenerally be invertible, and that implies that linearisation would only bepossible with multiple (localized) links, i.e. a chain. We develop anapproximate inference procedure for Chained GPs that is scalable and applicableto any factorized likelihood. We demonstrate the approximation on a range oflikelihood functions.
arxiv-17400-182 | Risk-Averse Multi-Armed Bandit Problems under Mean-Variance Measure | http://arxiv.org/pdf/1604.05257v1.pdf | author:Sattar Vakili, Qing Zhao category:cs.LG published:2016-04-18 summary:The multi-armed bandit problems have been studied mainly under the measure ofexpected total reward accrued over a horizon of length $T$. In this paper, weaddress the issue of risk in multi-armed bandit problems and develop parallelresults under the measure of mean-variance, a commonly adopted risk measure ineconomics and mathematical finance. We show that the model-specific regret andthe model-independent regret in terms of the mean-variance of the rewardprocess are lower bounded by $\Omega(\log T)$ and $\Omega(T^{2/3})$,respectively. We then show that variations of the UCB policy and the DSEEpolicy developed for the classic risk-neutral MAB achieve these lower bounds.
arxiv-17400-183 | Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions | http://arxiv.org/pdf/1604.05251v1.pdf | author:Carl-Johann Simon-Gabriel, Bernhard Schölkopf category:stat.ML math.FA math.PR G.3 published:2016-04-18 summary:Kernel mean embeddings have recently attracted the attention of the machinelearning community. They map measures $\mu$ from some set $M$ to functions in areproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance oftwo mapped measures is a semi-metric $d_k$ over $M$. We study three questions.(I) For a given kernel, what sets $M$ can be embedded? (II) When is theembedding injective over $M$ (in which case $d_k$ is a metric)? (III) How doesthe $d_k$-induced topology compare to other topologies on $M$? The existingmachine learning literature has addressed these questions in cases where $M$ is(a subset of) the finite regular Borel measures. We unify, improve andgeneralise those results. Our approach naturally leads to continuous andpossibly even injective embeddings of (Schwartz-) distributions, i.e.,generalised measures, but the reader is free to focus on measures only. Inparticular, we systemise and extend various (partly known) equivalences betweendifferent notions of universal, characteristic and strictly positive definitekernels, and show that on an underlying locally compact Hausdorff space, $d_k$metrises the weak convergence of probability measures if and only if $k$ iscontinuous and characteristic.
arxiv-17400-184 | Learning Sparse Additive Models with Interactions in High Dimensions | http://arxiv.org/pdf/1604.05307v1.pdf | author:Hemant Tyagi, Anastasios Kyrillidis, Bernd Gärtner, Andreas Krause category:cs.LG cs.IT math.IT stat.ML published:2016-04-18 summary:A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as aSparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in\mathcal{S}}\phi_{l}(x_l)$, where $\mathcal{S} \subset [d]$, $\mathcal{S} \lld$. Assuming $\phi_l$'s and $\mathcal{S}$ to be unknown, the problem ofestimating $f$ from its samples has been studied extensively. In this work, weconsider a generalized SPAM, allowing for second order interaction terms. Forsome $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, thefunction $f$ is assumed to be of the form: $$f(\mathbf{x}) = \sum_{p \in\mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_{l},x_{l^{\prime}}).$$ Assuming$\phi_{p},\phi_{(l,l^{\prime})}$, $\mathcal{S}_1$ and, $\mathcal{S}_2$ to beunknown, we provide a randomized algorithm that queries $f$ and exactlyrecovers $\mathcal{S}_1,\mathcal{S}_2$. Consequently, this also enables us toestimate the underlying $\phi_p, \phi_{(l,l^{\prime})}$. We derive samplecomplexity bounds for our scheme and also extend our analysis to include thesituation where the queries are corrupted with noise -- either stochastic, orarbitrary but bounded. Lastly, we provide simulation results on synthetic data,that validate our theoretical findings.
arxiv-17400-185 | Pieces-of-parts for supervoxel segmentation with global context: Application to DCE-MRI tumour delineation | http://arxiv.org/pdf/1604.05210v1.pdf | author:Benjamin Irving, James M Franklin, Bartlomiej W Papiez, Ewan M Anderson, Ricky A Sharma, Fergus V Gleeson, Sir Michael Brady, Julia A Schnabel category:cs.CV published:2016-04-18 summary:Rectal tumour segmentation in dynamic contrast-enhanced MRI (DCE-MRI) is achallenging task, and an automated and consistent method would be highlydesirable to improve the modelling and prediction of patient outcomes fromtissue contrast enhancement characteristics - particularly in routine clinicalpractice. A framework is developed to automate DCE-MRI tumour segmentation, byintroducing: perfusion-supervoxels to over-segment and classify DCE-MRI volumesusing the dynamic contrast enhancement characteristics; and the pieces-of-partsgraphical model, which adds global (anatomic) constraints that further refinethe supervoxel components that comprise the tumour. The framework was evaluatedon 23 DCE-MRI scans of patients with rectal adenocarcinomas, and achieved avoxelwise area-under the receiver operating characteristic curve (AUC) of 0.97compared to expert delineations. Creating a binary tumour segmentation, 21 ofthe 23 cases were segmented correctly with a median Dice similarity coefficient(DSC) of 0.63, which is close to the inter-rater variability of thischallenging task. A sec- ond study is also included to demonstrate the method'sgeneralisability and achieved a DSC of 0.71. The framework achieves promisingresults for the underexplored area of rectal tumour segmentation in DCE-MRI,and the methods have potential to be applied to other DCE-MRI and supervoxelsegmentation problems
arxiv-17400-186 | Locally Imposing Function for Generalized Constraint Neural Networks - A Study on Equality Constraints | http://arxiv.org/pdf/1604.05198v1.pdf | author:Linlin Cao, Ran He, Bao-Gang Hu category:cs.NE cs.LG stat.ML published:2016-04-18 summary:This work is a further study on the Generalized Constraint Neural Network(GCNN) model [1], [2]. Two challenges are encountered in the study, that is, toembed any type of prior information and to select its imposing schemes. Thework focuses on the second challenge and studies a new constraint imposingscheme for equality constraints. A new method called locally imposing function(LIF) is proposed to provide a local correction to the GCNN predictionfunction, which therefore falls within Locally Imposing Scheme (LIS). Incomparison, the conventional Lagrange multiplier method is considered asGlobally Imposing Scheme (GIS) because its added constraint term exhibits aglobal impact to its objective function. Two advantages are gained from LISover GIS. First, LIS enables constraints to fire locally and explicitly in thedomain only where they need on the prediction function. Second, constraints canbe implemented within a network setting directly. We attempt to interpretseveral constraint methods graphically from a viewpoint of the localityprinciple. Numerical examples confirm the advantages of the proposed method. Insolving boundary value problems with Dirichlet and Neumann constraints, theGCNN model with LIF is possible to achieve an exact satisfaction of theconstraints.
arxiv-17400-187 | ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation | http://arxiv.org/pdf/1604.05144v1.pdf | author:Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun category:cs.CV published:2016-04-18 summary:Large-scale data is of crucial importance for learning semantic segmentationmodels, but annotating per-pixel masks is a tedious and inefficient procedure.We note that for the topic of interactive image segmentation, scribbles arevery widely used in academic research and commercial software, and arerecognized as one of the most user-friendly ways of interacting. In this paper,we propose to use scribbles to annotate images, and develop an algorithm totrain convolutional networks for semantic segmentation supervised by scribbles.Our algorithm is based on a graphical model that jointly propagates informationfrom scribbles to unmarked pixels and learns network parameters. We presentcompetitive object semantic segmentation results on the PASCAL VOC dataset byusing scribbles as annotations. Scribbles are also favored for annotating stuff(e.g., water, sky, grass) that has no well-defined shape, and our method showsexcellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensivescribble annotations. Our scribble annotations on PASCAL VOC are available athttp://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup
arxiv-17400-188 | Using Self-Contradiction to Learn Confidence Measures in Stereo Vision | http://arxiv.org/pdf/1604.05132v1.pdf | author:Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof category:cs.CV published:2016-04-18 summary:Learned confidence measures gain increasing importance for outlier removaland quality improvement in stereo vision. However, acquiring the necessarytraining data is typically a tedious and time consuming task that involvesmanual interaction, active sensing devices and/or synthetic scenes. To overcomethis problem, we propose a new, flexible, and scalable way for generatingtraining data that only requires a set of stereo images as input. The key ideaof our approach is to use different view points for reasoning aboutcontradictions and consistencies between multiple depth maps generated with thesame stereo algorithm. This enables us to generate a huge amount of trainingdata in a fully automated manner. Among other experiments, we demonstrate thepotential of our approach by boosting the performance of three learnedconfidence measures on the KITTI2012 dataset by simply training them on a vastamount of automatically generated training data rather than a limited amount oflaser ground truth data.
arxiv-17400-189 | Memory controls time perception and intertemporal choices | http://arxiv.org/pdf/1604.05129v1.pdf | author:Pedro A. Ortega, Naftali Tishby category:q-bio.NC cs.AI stat.ML published:2016-04-18 summary:There is a remarkable consensus that human and non-human subjects experiencetemporal distortions in many stages of their perceptual and decision-makingsystems. Similarly, intertemporal choice research has shown thatdecision-makers undervalue future outcomes relative to immediate ones. Here wecombine techniques from information theory and artificial intelligence to showhow both temporal distortions and intertemporal choice preferences can beexplained as a consequence of the coding efficiency of sensorimotorrepresentation. In particular, the model implies that interactions thatconstrain future behavior are perceived as being both longer in duration andmore valuable. Furthermore, using simulations of artificial agents, weinvestigate how memory constraints enforce a renormalization of the perceivedtimescales. Our results show that qualitatively different discount functions,such as exponential and hyperbolic discounting, arise as a consequence of anagent's probabilistic model of the world.
arxiv-17400-190 | Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views | http://arxiv.org/pdf/1512.02497v2.pdf | author:Francisco Massa, Bryan Russell, Mathieu Aubry category:cs.CV cs.LG cs.NE published:2015-12-08 summary:This paper presents an end-to-end convolutional neural network (CNN) for2D-3D exemplar detection. We demonstrate that the ability to adapt the featuresof natural images to better align with those of CAD rendered views is criticalto the success of our technique. We show that the adaptation can be learned bycompositing rendered views of textured object models on natural images. Ourapproach can be naturally incorporated into a CNN detection pipeline andextends the accuracy and speed benefits from recent advances in deep learningto 2D-3D exemplar detection. We applied our method to two tasks: instancedetection, where we evaluated on the IKEA dataset, and object categorydetection, where we out-perform Aubry et al. for "chair" detection on a subsetof the Pascal VOC dataset.
arxiv-17400-191 | BinaryConnect: Training Deep Neural Networks with binary weights during propagations | http://arxiv.org/pdf/1511.00363v3.pdf | author:Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David category:cs.LG cs.CV cs.NE published:2015-11-02 summary:Deep Neural Networks (DNN) have achieved state-of-the-art results in a widerange of tasks, with the best results obtained with large training sets andlarge models. In the past, GPUs enabled these breakthroughs because of theirgreater computational speed. In the future, faster computation at both trainingand test time is likely to be crucial for further progress and for consumerapplications on low-power devices. As a result, there is much interest inresearch and development of dedicated hardware for Deep Learning (DL). Binaryweights, i.e., weights which are constrained to only two possible values (e.g.-1 or 1), would bring great benefits to specialized DL hardware by replacingmany multiply-accumulate operations by simple accumulations, as multipliers arethe most space and power-hungry components of the digital implementation ofneural networks. We introduce BinaryConnect, a method which consists intraining a DNN with binary weights during the forward and backwardpropagations, while retaining precision of the stored weights in whichgradients are accumulated. Like other dropout schemes, we show thatBinaryConnect acts as regularizer and we obtain near state-of-the-art resultswith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.
arxiv-17400-192 | Expected Similarity Estimation for Large-Scale Batch and Streaming Anomaly Detection | http://arxiv.org/pdf/1601.06602v2.pdf | author:Markus Schneider, Wolfgang Ertel, Fabio Ramos category:cs.LG cs.AI published:2016-01-25 summary:We present a novel algorithm for anomaly detection on very large datasets anddata streams. The method, named EXPected Similarity Estimation (EXPoSE), iskernel-based and able to efficiently compute the similarity between new datapoints and the distribution of regular data. The estimator is formulated as aninner product with a reproducing kernel Hilbert space embedding and makes noassumption about the type or shape of the underlying data distribution. We showthat offline (batch) learning with EXPoSE can be done in linear time and online(incremental) learning takes constant time per instance and model update.Furthermore, EXPoSE can make predictions in constant time, while it requiresonly constant memory. In addition we propose different methodologies forconcept drift adaptation on evolving data streams. On several real datasets wedemonstrate that our approach can compete with state of the art algorithms foranomaly detection while being significant faster than techniques with the samediscriminant power.
arxiv-17400-193 | Speech vocoding for laboratory phonology | http://arxiv.org/pdf/1601.05991v2.pdf | author:Milos Cernak, Stefan Benus, Alexandros Lazaridis category:cs.CL cs.SD published:2016-01-22 summary:Using phonological speech vocoding, we propose a platform for exploringrelations between phonology and speech processing, and in broader terms, forexploring relations between the abstract and physical structures of a speechsignal. Our goal is to make a step towards bridging phonology and speechprocessing and to contribute to the program of Laboratory Phonology. We showthree application examples for laboratory phonology: compositional phonologicalspeech modelling, a comparison of phonological systems and an experimentalphonological parametric text-to-speech (TTS) system. The featuralrepresentations of the following three phonological systems are considered inthis work: (i) Government Phonology (GP), (ii) the Sound Pattern of English(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocodedspeech, we conclude that the latter achieves slightly better results than theformer. However, GP - the most compact phonological speech representation -performs comparably to the systems with a higher number of phonologicalfeatures. The parametric TTS based on phonological speech representation, andtrained from an unlabelled audiobook in an unsupervised manner, achievesintelligibility of 85% of the state-of-the-art parametric speech synthesis. Weenvision that the presented approach paves the way for researchers in bothfields to form meaningful hypotheses that are explicitly testable using theconcepts developed and exemplified in this paper. On the one hand, laboratoryphonologists might test the applied concepts of their theoretical models, andon the other hand, the speech processing community may utilize the conceptsdeveloped for the theoretical phonological models for improvements of thecurrent state-of-the-art applications.
arxiv-17400-194 | Pixel-level Encoding and Depth Layering for Instance-level Semantic Labeling | http://arxiv.org/pdf/1604.05096v1.pdf | author:Jonas Uhrig, Marius Cordts, Uwe Franke, Thomas Brox category:cs.CV published:2016-04-18 summary:Recent approaches for instance-aware semantic labeling have augmentedconvolutional neural networks (CNNs) with complex multi-task architectures orcomputationally expensive graphical models. We present a method that leveragesa fully convolutional network (FCN) to predict semantic labels, depth and aninstance-based encoding using each pixel's direction towards its correspondinginstance center. Subsequently, we apply low-level computer vision techniques togenerate state-of-the-art instance segmentation on the street scene datasetsKITTI and Cityscapes. Our approach outperforms existing works by a large marginand can additionally predict absolute distances of individual instances from amonocular image as well as a pixel-level semantic labeling.
arxiv-17400-195 | Mastering $2048$ with Delayed Temporal Coherence Learning, Multi-State Weight Promotion, Redundant Encoding and Carousel Shaping | http://arxiv.org/pdf/1604.05085v1.pdf | author:Wojciech Jaśkowski category:cs.AI cs.LG I.2.6; I.2.8 published:2016-04-18 summary:$2048$ is an engaging single-player, nondeterministic video puzzle game,which, thanks to the simple rules and hard-to-master gameplay, has gainedmassive popularity in recent years. As $2048$ can be conveniently embedded intothe discrete-state Markov decision processes framework, we treat it as atestbed for evaluating existing and new methods in reinforcement learning. Withthe aim to develop a strong $2048$ playing program, we employ temporaldifference learning with systematic n-tuple networks. We show that this basicmethod can be significantly improved with temporal coherence learning,multi-stage function approximator with weight promotion, carousel shaping, andredundant encoding. In addition, we demonstrate how to take advantage of thecharacteristics of the n-tuple network, to improve the algorithmiceffectiveness of the learning process by i) delaying the (decayed) update andapplying lock-free optimistic parallelism to effortlessly make advantage ofmultiple CPU cores. This way, we were able to develop the best known $2048$playing program to date, which confirms the effectiveness of the introducedmethods for discrete-state Markov decision problems.
arxiv-17400-196 | Demonstrating Hybrid Learning in a Flexible Neuromorphic Hardware System | http://arxiv.org/pdf/1604.05080v1.pdf | author:Simon Friedmann, Johannes Schemmel, Andreas Gruebl, Andreas Hartel, Matthias Hock, Karlheinz Meier category:q-bio.NC cs.NE published:2016-04-18 summary:We present results from a new approach to learning and plasticity inneuromorphic hardware systems: to enable flexibility in implementable learningmechanisms while keeping high efficiency associated with neuromorphicimplementations, we combine a general-purpose processor with full-custom analogelements. This processor is operating in parallel with a fully parallel neuromorphicsystem consisting of an array of synapses connected to analog, continuous timeneuron circuits. Novel analog correlation sensor circuits process spike eventsfor each synapse in parallel and in real-time. The processor uses this pre-processing to compute new weights possibly usingadditional information following its program. Therefore, learning rules can be defined in software giving a large degree offlexibility. Synapses realize correlation detection geared towards Spike-Timing DependentPlasticity (STDP) as central computational primitive in the analog domain. Operating at a speed-up factor of 1000 compared to biological time-scale, wemeasure time-constants from tens to hundreds of micro-seconds. We analyze variability across multiple chips and demonstrate learning using amultiplicative STDP rule. We conclude, that the presented approach will enable flexible and efficientlearning as a platform for neuroscientific research and technologicalapplications.
arxiv-17400-197 | Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization | http://arxiv.org/pdf/1604.05073v1.pdf | author:Daniel Beck, Adrià de Gispert, Gonzalo Iglesias, Aurelien Waite, Bill Byrne category:cs.CL published:2016-04-18 summary:We address the problem of automatically finding the parameters of astatistical machine translation system that maximize BLEU scores while ensuringthat decoding speed exceeds a minimum value. We propose the use of BayesianOptimization to efficiently tune the speed-related decoding parameters byeasily incorporating speed as a noisy constraint function. The obtainedparameter values are guaranteed to satisfy the speed constraint with anassociated confidence margin. Across three language pairs and two speedconstraint values, we report overall optimization time reduction compared togrid and random search. We also show that Bayesian Optimization can decouplespeed and BLEU measurements, resulting in a further reduction of overalloptimization time as speed is measured over a small subset of sentences.
arxiv-17400-198 | Learning Sparse Low-Threshold Linear Classifiers | http://arxiv.org/pdf/1212.3276v3.pdf | author:Sivan Sabato, Shai Shalev-Shwartz, Nathan Srebro, Daniel Hsu, Tong Zhang category:stat.ML cs.LG published:2012-12-13 summary:We consider the problem of learning a non-negative linear classifier with a$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. Thisproblem generalizes the problem of learning a $k$-monotone disjunction. Weprove that we can learn efficiently in this setting, at a rate which is linearin both $k$ and the size of the threshold, and that this is the best possiblerate. We provide an efficient online learning algorithm that achieves theoptimal rate, and show that in the batch case, empirical risk minimizationachieves this rate as well. The rates we show are tighter than the uniformconvergence rate, which grows with $k^2$.
arxiv-17400-199 | Loss minimization and parameter estimation with heavy tails | http://arxiv.org/pdf/1307.1827v7.pdf | author:Daniel Hsu, Sivan Sabato category:cs.LG stat.ML published:2013-07-07 summary:This work studies applications and generalizations of a simple estimationtechnique that provides exponential concentration under heavy-taileddistributions, assuming only bounded low-order moments. We show that thetechnique can be used for approximate minimization of smooth and stronglyconvex losses, and specifically for least squares linear regression. Forinstance, our $d$-dimensional estimator requires just$\tilde{O}(d\log(1/\delta))$ random samples to obtain a constant factorapproximation to the optimal least squares loss with probability $1-\delta$,without requiring the covariates or noise to be bounded or subgaussian. Weprovide further applications to sparse linear regression and low-rankcovariance matrix estimation with similar allowances on the noise and covariatedistributions. The core technique is a generalization of the median-of-meansestimator to arbitrary metric spaces.
arxiv-17400-200 | Most Likely Separation of Intensity and Warping Effects in Image Registration | http://arxiv.org/pdf/1604.05027v1.pdf | author:Line Kühnel, Stefan Sommer, Akshay Pai, Lars Lau Raket category:cs.CV published:2016-04-18 summary:This paper introduces a class of mixed-effects models for joint modeling ofspatially correlated intensity variation and warping variation in 2D images.Spatially correlated intensity variation and warp variation are modeled asrandom effects, resulting in a nonlinear mixed-effects model that enablessimultaneous estimation of template and model parameters by optimization of thelikelihood function. We propose an algorithm for fitting the model whichalternates estimation of variance parameters and image registration, thusavoiding potential estimation bias resulting from treating registration as apreprocessing step. We apply the model to datasets of facial images and 2Dbrain magnetic resonance images to illustrate the simultaneous estimation andprediction of intensity and warp effects.
arxiv-17400-201 | Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large Scale Sparse Models | http://arxiv.org/pdf/1604.05024v1.pdf | author:Ziqiang Shi, Rujie Liu category:cs.LG cs.AI published:2016-04-18 summary:PROXTONE is a novel and fast method for optimization of large scalenon-smooth convex problem \cite{shi2015large}. In this work, we try to usePROXTONE method in solving large scale \emph{non-smooth non-convex} problems,for example training of sparse deep neural network (sparse DNN) or sparseconvolutional neural network (sparse CNN) for embedded or mobile device.PROXTONE converges much faster than first order methods, while first ordermethod is easy in deriving and controlling the sparseness of the solutions.Thus in some applications, in order to train sparse models fast, we propose tocombine the merits of both methods, that is we use PROXTONE in the firstseveral epochs to reach the neighborhood of an optimal solution, and then usethe first order method to explore the possibility of sparsity in the followingtraining. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE andPROXTONE$^+$ are tested in our experiments, and which demonstrate both methodsimproved convergence speed twice as fast at least on diverse sparse modellearning problems, and at the same time reduce the size to 0.5\% for DNNmodels. The source of all the algorithms is available upon request.
arxiv-17400-202 | Forecasting Volatility in Indian Stock Market using Artificial Neural Network with Multiple Inputs and Outputs | http://arxiv.org/pdf/1604.05008v1.pdf | author:Tamal Datta Chaudhuri, Indranil Ghosh category:cs.NE published:2016-04-18 summary:Volatility in stock markets has been extensively studied in the appliedfinance literature. In this paper, Artificial Neural Network models based onvarious back propagation algorithms have been constructed to predict volatilityin the Indian stock market through volatility of NIFTY returns and volatilityof gold returns. This model considers India VIX, CBOE VIX, volatility of crudeoil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAXreturns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility ofNikkei returns (NIKKEISDR) as predictor variables. Three sets of experimentshave been performed over three time periods to judge the effectiveness of theapproach.
arxiv-17400-203 | Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval | http://arxiv.org/pdf/1604.04994v1.pdf | author:Xiu-Shen Wei, Jian-Hao Luo, Jianxin Wu category:cs.CV published:2016-04-18 summary:Deep convolutional models pre-trained for the ImageNet classification taskhave been successfully adopted to tasks in other domains, such as texturedescription and object proposal generation, but these tasks require annotationsfor images in the new domain. In this paper, we focus on a novel andchallenging task in the pure unsupervised setting: fine-grained imageretrieval. Even with image labels, fine-grained images are difficult toclassify, let alone the unsupervised retrieval task. We propose the SelectiveConvolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes themain object(s) in fine-grained images, a step that discards noisy backgroundand keeps useful deep descriptors. The selected descriptors are then aggregatedand dimensionality reduced into a short feature vector using the best practiceswe found. SCDA is unsupervised, using no image label or bounding boxannotation. Experiments on four fine-grained datasets confirm the effectivenessof SCDA. Visualization of the SCDA features shows that they correspond tovisual attributes (even subtle ones), which might explain SCDA's high accuracyin fine-grained retrieval.
arxiv-17400-204 | Phase Transitions and a Model Order Selection Criterion for Spectral Graph Clustering | http://arxiv.org/pdf/1604.03159v2.pdf | author:Pin-Yu Chen, Alfred O. Hero category:cs.SI stat.ML published:2016-04-11 summary:One of the longstanding open problems in spectral graph clustering (SGC) isthe so-called model order selection problem: automated selection of the correctnumber of clusters. This is equivalent to the problem of finding the number ofconnected components or communities in an undirected graph. We propose asolution to the SGC model selection problem under a random interconnectionmodel (RIM) using a novel selection criterion that is based on an asymptoticphase transition analysis. Our solution can more generally be applied todiscovering hidden block diagonal structure in symmetric non-negative matrices.Numerical experiments on simulated graphs validate the phase transitionanalysis, and real-world network data is used to validate the performance ofthe proposed model selection procedure.
arxiv-17400-205 | Visual Aesthetic Quality Assessment with Multi-task Deep Learning | http://arxiv.org/pdf/1604.04970v1.pdf | author:Yueying Kao, Ran He, Kaiqi Huang category:cs.CV cs.LG cs.NE published:2016-04-18 summary:This paper considers the problem of assessing visual aesthetic quality withsemantic information. We cast the assessment problem as the main task among amulti-task deep model, and argue that semantic recognition offers the key toaddressing this problem. Based on convolutional neural networks, we propose ageneral multi-task framework with four different structures. In each structure,aesthetic quality assessment task and semantic recognition task are leveraged,and different features are explored to improve the quality assessment.Moreover, an effective strategy of keeping a balanced effect between thesemantic task and aesthetic task is developed to optimize the parameters of ourframework. The correlation analysis among the tasks validates the importance ofthe semantic recognition in aesthetic quality assessment. Extensive experimentsverify the effectiveness of the proposed multi-task framework, and furthercorroborate the above proposition.
arxiv-17400-206 | Gaussian Copula Variational Autoencoders for Mixed Data | http://arxiv.org/pdf/1604.04960v1.pdf | author:Suwon Suh, Seungjin Choi category:stat.ML cs.LG published:2016-04-18 summary:The variational autoencoder (VAE) is a generative model with continuouslatent variables where a pair of probabilistic encoder (bottom-up) and decoder(top-down) is jointly learned by stochastic gradient variational Bayes. Wefirst elaborate Gaussian VAE, approximating the local covariance matrix of thedecoder as an outer product of the principal direction at a position determinedby a sample drawn from Gaussian distribution. We show that this model, referredto as VAE-ROC, better captures the data manifold, compared to the standardGaussian VAE where independent multivariate Gaussian was used to model thedecoder. Then we extend the VAE-ROC to handle mixed categorical and continuousdata. To this end, we employ Gaussian copula to model the local dependency inmixed categorical and continuous data, leading to {\em Gaussian copulavariational autoencoder} (GCVAE). As in VAE-ROC, we use the rank-oneapproximation for the covariance in the Gaussian copula, to capture the localdependency structure in the mixed data. Experiments on various datasetsdemonstrate the useful behaviour of VAE-ROC and GCVAE, compared to the standardVAE.
arxiv-17400-207 | Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition | http://arxiv.org/pdf/1604.04953v1.pdf | author:Zecheng Xie, Zenghui Sun, Lianwen Jin, Ziyong Feng, Shuye Zhang category:cs.CV published:2016-04-18 summary:This paper proposes an end-to-end framework, namely fully convolutionalrecurrent network (FCRN) for handwritten Chinese text recognition (HCTR).Unlike traditional methods that rely heavily on segmentation, our FCRN istrained with online text data directly and learns to associate the pen-tiptrajectory with a sequence of characters. FCRN consists of four parts: apath-signature layer to extract signature features from the input pen-tiptrajectory, a fully convolutional network to learn informative representation,a sequence modeling layer to make per-frame predictions on the input sequenceand a transcription layer to translate the predictions into a label sequence.The FCRN is end-to-end trainable in contrast to conventional methods whosecomponents are separately trained and tuned. We also present a refined beamsearch method that efficiently integrates the language model to decode the FCRNand significantly improve the recognition results. We evaluate the performance of the proposed method on the test sets from thedatabases CASIA-OLHWDB and ICDAR 2013 Chinese handwriting recognitioncompetition, and both achieve state-of-the-art performance with correct ratesof 96.40% and 95.00%, respectively.
arxiv-17400-208 | Optimal Estimation of Low Rank Density Matrices | http://arxiv.org/pdf/1507.05131v4.pdf | author:Vladimir Koltchinskii, Dong Xia category:stat.ML published:2015-07-17 summary:The density matrices are positively semi-definite Hermitian matrices of unittrace that describe the state of a quantum system. The goal of the paper is todevelop minimax lower bounds on error rates of estimation of low rank densitymatrices in trace regression models used in quantum state tomography (inparticular, in the case of Pauli measurements) with explicit dependence of thebounds on the rank and other complexity parameters. Such bounds are establishedfor several statistically relevant distances, including quantum versions ofKullback-Leibler divergence (relative entropy distance) and of Hellingerdistance (so called Bures distance), and Schatten $p$-norm distances. Sharpupper bounds and oracle inequalities for least squares estimator with vonNeumann entropy penalization are obtained showing that minimax lower bounds areattained (up to logarithmic factors) for these distances.
arxiv-17400-209 | Global optimization of factor models using alternating minimization | http://arxiv.org/pdf/1604.04942v1.pdf | author:Lei Le, Martha White category:stat.ML cs.LG published:2016-04-17 summary:Learning new representations in machine learning is often tackled using afactorization of the data. For many such problems, including sparse coding andmatrix completion, learning these factorizations can be difficult, in terms ofefficiency and to guarantee that the solution is a global minimum. Recently, ageneral class of objectives have been introduced, called induced regularizedfactor models (RFMs), which have an induced convex form that enables globaloptimization. Though attractive theoretically, this induced form isimpractical, particularly for large or growing datasets. In this work, weinvestigate the use of a practical alternating minimization algorithms forinduced RFMs, that ensure convergence to global optima. We characterize thestationary points of these models, and, using these insights, highlightpractical choices for the objectives. We then provide theoretical and empiricalevidence that alternating minimization, from a random initialization, convergesto global minima for a large subclass of induced RFMs. In particular, we provethat induced RFMs do not have degenerate saddlepoints and that local minima areactually global minima. Finally, we provide an extensive investigation intopractical optimization choices for using alternating minimization for inducedRFMs, for both batch and stochastic gradient descent.
arxiv-17400-210 | Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation | http://arxiv.org/pdf/1511.07356v2.pdf | author:Sina Honari, Jason Yosinski, Pascal Vincent, Christopher Pal category:cs.CV published:2015-11-23 summary:Deep neural networks with alternating convolutional, max-pooling anddecimation layers are widely used in state of the art architectures forcomputer vision. Max-pooling purposefully discards precise spatial informationin order to create features that are more robust, and typically organized aslower resolution spatial feature maps. On some tasks, such as whole-imageclassification, max-pooling derived features are well suited; however, fortasks requiring precise localization, such as pixel level prediction andsegmentation, max-pooling destroys exactly the information required to performwell. Precise localization may be preserved by shallow convnets without poolingbut at the expense of robustness. Can we have our max-pooled multi-layered cakeand eat it too? Several papers have proposed summation and concatenation basedmethods for combining upsampled coarse, abstract features with finer featuresto produce robust pixel level predictions. Here we introduce another model ---dubbed Recombinator Networks --- where coarse features inform finer featuresearly in their formation such that finer features can make use of severallayers of computation in deciding how to use coarse features. The model istrained once, end-to-end and performs better than summation-basedarchitectures, reducing the error from the previous state of the art on twofacial keypoint datasets, AFW and AFLW, by 30\% and beating the currentstate-of-the-art on 300W without using extra data. We improve performance evenfurther by adding a denoising prediction model based on a novel convnetformulation.
arxiv-17400-211 | Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis | http://arxiv.org/pdf/1604.04939v1.pdf | author:Andreas Damianou, Neil D. Lawrence, Carl Henrik Ek category:stat.ML cs.LG math.PR published:2016-04-17 summary:Factor analysis aims to determine latent factors, or traits, which summarizea given data set. Inter-battery factor analysis extends this notion to multipleviews of the data. In this paper we show how a nonlinear, nonparametric versionof these models can be recovered through the Gaussian process latent variablemodel. This gives us a flexible formalism for multi-view learning where thelatent variables can be used both for exploratory purposes and for learningrepresentations that enable efficient inference for ambiguous estimation tasks.Learning is performed in a Bayesian manner through the formulation of avariational compression scheme which gives a rigorous lower bound on the loglikelihood. Our Bayesian framework provides strong regularization duringtraining, allowing the structure of the latent space to be determinedefficiently and automatically. We demonstrate this by producing the first (toour knowledge) published results of learning from dozens of views, even whendata is scarce. We further show experimental results on several different typesof multi-view data sets and for different kinds of tasks, including exploratorydata analysis, generation, ambiguity modelling through latent priors andclassification.
arxiv-17400-212 | The Variational Gaussian Process | http://arxiv.org/pdf/1511.06499v4.pdf | author:Dustin Tran, Rajesh Ranganath, David M. Blei category:stat.ML cs.LG cs.NE stat.CO published:2015-11-20 summary:Variational inference is a powerful tool for approximate inference, and ithas been recently applied for representation learning with deep generativemodels. We develop the variational Gaussian process (VGP), a Bayesiannonparametric variational family, which adapts its shape to match complexposterior distributions. The VGP generates approximate posterior samples bygenerating latent inputs and warping them through random non-linear mappings;the distribution over random mappings is learned during inference, enabling thetransformed outputs to adapt to varying complexity. We prove a universalapproximation theorem for the VGP, demonstrating its representative power forlearning any model. For inference we present a variational objective inspiredby auto-encoders and perform black box inference over a wide class of models.The VGP achieves new state-of-the-art results for unsupervised learning,inferring models such as the deep latent Gaussian model and the recentlyproposed DRAW.
arxiv-17400-213 | Regularizing Solutions to the MEG Inverse Problem Using Space-Time Separable Covariance Functions | http://arxiv.org/pdf/1604.04931v1.pdf | author:Arno Solin, Pasi Jylänki, Jaakko Kauramäki, Tom Heskes, Marcel A. J. van Gerven, Simo Särkkä category:stat.AP stat.ML published:2016-04-17 summary:In magnetoencephalography (MEG) the conventional approach to sourcereconstruction is to solve the underdetermined inverse problem independentlyover time and space. Here we present how the conventional approach can beextended by regularizing the solution in space and time by a Gaussian process(Gaussian random field) model. Assuming a separable covariance function inspace and time, the computational complexity of the proposed model becomes(without any further assumptions or restrictions) $\mathcal{O}(t^3 + n^3 +m^2n)$, where $t$ is the number of time steps, $m$ is the number of sources,and $n$ is the number of sensors. We apply the method to both simulated andempirical data, and demonstrate the efficiency and generality of our Bayesiansource reconstruction approach which subsumes various classical approaches inthe literature.
arxiv-17400-214 | Some medical applications of example-based super-resolution | http://arxiv.org/pdf/1604.04926v1.pdf | author:Ramin Zabih category:cs.CV published:2016-04-17 summary:Example-based super-resolution (EBSR) reconstructs a high-resolution imagefrom a low-resolution image, given a training set of high-resolution images. Inthis note I propose some applications of EBSR to medical imaging. A particularinteresting application, which I call "x-ray voxelization", approximates theresult of a CT scan from an x-ray image.
arxiv-17400-215 | Generating Semi-Synthetic Validation Benchmarks for Embryomics | http://arxiv.org/pdf/1604.04906v1.pdf | author:Johannes Stegmaier, Julian Arz, Benjamin Schott, Jens C. Otte, Andrei Kobitski, G. Ulrich Nienhaus, Uwe Strähle, Peter Sanders, Ralf Mikut category:cs.CV q-bio.CB q-bio.QM published:2016-04-17 summary:Systematic validation is an essential part of algorithm development. Theenormous dataset sizes and the complexity observed in many recent time-resolved3D fluorescence microscopy imaging experiments, however, prohibit acomprehensive manual ground truth generation. Moreover, existing simulatedbenchmarks in this field are often too simple or too specialized tosufficiently validate the observed image analysis problems. We present a newsemi-synthetic approach to generate realistic 3D+t benchmarks that combineschallenging cellular movement dynamics of real embryos with simulatedfluorescent nuclei and artificial image distortions including variousparametrizable options like cell numbers, acquisition deficiencies or multiviewsimulations. We successfully applied the approach to simulate the developmentof a zebrafish embryo with thousands of cells over 14 hours of its earlyexistence.
arxiv-17400-216 | From Denoising to Compressed Sensing | http://arxiv.org/pdf/1406.4175v5.pdf | author:Christopher A. Metzler, Arian Maleki, Richard G. Baraniuk category:cs.IT math.IT math.ST stat.ML stat.TH published:2014-06-16 summary:A denoising algorithm seeks to remove noise, errors, or perturbations from asignal. Extensive research has been devoted to this arena over the last severaldecades, and as a result, today's denoisers can effectively remove largeamounts of additive white Gaussian noise. A compressed sensing (CS)reconstruction algorithm seeks to recover a structured signal acquired using asmall number of randomized measurements. Typical CS reconstruction algorithmscan be cast as iteratively estimating a signal from a perturbed observation.This paper answers a natural question: How can one effectively employ a genericdenoiser in a CS reconstruction algorithm? In response, we develop an extensionof the approximate message passing (AMP) framework, called Denoising-based AMP(D-AMP), that can integrate a wide class of denoisers within its iterations. Wedemonstrate that, when used with a high performance denoiser for naturalimages, D-AMP offers state-of-the-art CS recovery performance while operatingtens of times faster than competing methods. We explain the exceptionalperformance of D-AMP by analyzing some of its theoretical features. A keyelement in D-AMP is the use of an appropriate Onsager correction term in itsiterations, which coerces the signal perturbation at each iteration to be veryclose to the white Gaussian noise that denoisers are typically designed toremove.
arxiv-17400-217 | An Initial Seed Selection Algorithm for K-means Clustering of Georeferenced Data to Improve Replicability of Cluster Assignments for Mapping Application | http://arxiv.org/pdf/1604.04893v1.pdf | author:Fouad Khan category:cs.LG cs.DS published:2016-04-17 summary:K-means is one of the most widely used clustering algorithms in variousdisciplines, especially for large datasets. However the method is known to behighly sensitive to initial seed selection of cluster centers. K-means++ hasbeen proposed to overcome this problem and has been shown to have betteraccuracy and computational efficiency than k-means. In many clustering problemsthough -such as when classifying georeferenced data for mapping applications-standardization of clustering methodology, specifically, the ability to arriveat the same cluster assignment for every run of the method i.e. replicabilityof the methodology, may be of greater significance than any perceived measureof accuracy, especially when the solution is known to be non-unique, as in thecase of k-means clustering. Here we propose a simple initial seed selectionalgorithm for k-means clustering along one attribute that draws initial clusterboundaries along the 'deepest valleys' or greatest gaps in dataset. Thus, itincorporates a measure to maximize distance between consecutive cluster centerswhich augments the conventional k-means optimization for minimum distancebetween cluster center and cluster members. Unlike existing initializationmethods, no additional parameters or degrees of freedom are introduced to theclustering algorithm. This improves the replicability of cluster assignments byas much as 100% over k-means and k-means++, virtually reducing the varianceover different runs to zero, without introducing any additional parameters tothe clustering process. Further, the proposed method is more computationallyefficient than k-means++ and in some cases, more accurate.
arxiv-17400-218 | Mahalanobis Distance Metric Learning Algorithm for Instance-based Data Stream Classification | http://arxiv.org/pdf/1604.04879v1.pdf | author:Jorge Luis Rivero Perez, Bernardete Ribeiro, Carlos Morell Perez category:cs.LG published:2016-04-17 summary:With the massive data challenges nowadays and the rapid growing oftechnology, stream mining has recently received considerable attention. Toaddress the large number of scenarios in which this phenomenon manifests itselfsuitable tools are required in various research fields. Instance-based datastream algorithms generally employ the Euclidean distance for theclassification task underlying this problem. A novel way to look into thisissue is to take advantage of a more flexible metric due to the increasedrequirements imposed by the data stream scenario. In this paper we present anew algorithm that learns a Mahalanobis metric using similarity anddissimilarity constraints in an online manner. This approach hybridizes aMahalanobis distance metric learning algorithm and a k-NN data streamclassification algorithm with concept drift detection. First, some basicaspects of Mahalanobis distance metric learning are described taking intoaccount key properties as well as online distance metric learning algorithms.Second, we implement specific evaluation methodologies and comparative metricssuch as Q statistic for data stream classification algorithms. Finally, ouralgorithm is evaluated on different datasets by comparing its results with oneof the best instance-based data stream classification algorithm of the state ofthe art. The results demonstrate that our proposal is better
arxiv-17400-219 | From Incremental Meaning to Semantic Unit (phrase by phrase) | http://arxiv.org/pdf/1604.04873v1.pdf | author:Andreas Scherbakov, Ekaterina Vylomova, Fei Liu, Timothy Baldwin category:cs.CL 68T50 I.2.7 published:2016-04-17 summary:This paper describes an experimental approach to Detection of MinimalSemantic Units and their Meaning (DiMSUM), explored within the framework ofSemEval 2016 Task 10. The approach is primarily based on a combination of wordembeddings and parserbased features, and employs unidirectional incrementalcomputation of compositional embeddings for multiword expressions.
arxiv-17400-220 | Two Points Fundamental Matrix | http://arxiv.org/pdf/1604.04848v1.pdf | author:Gil Ben-Artzi, Tavi Halperin, Michael Werman, Shmuel Peleg category:cs.CV published:2016-04-17 summary:It is well known that computing the fundamental matrix of two uncalibratedcameras requires at least seven corresponding points. We present a method tocompute the fundamental matrix between two cameras with only two pairs ofcorresponding points. Given these two points, we show how to find three pairsof corresponding epipolar lines, from which the fundamental matrix can becomputed. Two pairs of epipolar lines, incident to the two pairs of correspondingpoints, are found by maximizing stereo consistency between lines; correspondingepipolar lines yield a good stereo correspondence. These two epipolar linesintersect at the epipoles, giving a third pair of corresponding points. A thirdpair of matching epipolar lines, needed to compute the fundamental matrix, isfound from lines incident to the epipoles. We validate our method using real-world images and compare it tostate-of-the-art methods. Our approach is more accurate by a factor of fivecompared to the standard method using seven corresponding points, and itsaccuracy is comparable to the 8-points algorithm.
arxiv-17400-221 | Subjects and Their Objects: Localizing Interactees for a Person-Centric View of Importance | http://arxiv.org/pdf/1604.04842v1.pdf | author:Chao-Yeh Chen, Kristen Grauman category:cs.CV published:2016-04-17 summary:Understanding images with people often entails understanding their\emph{interactions} with other objects or people. As such, given a novel image,a vision system ought to infer which other objects/people play an importantrole in a given person's activity. However, existing methods are limited tolearning action-specific interactions (e.g., how the pose of a tennis playerrelates to the position of his racquet when serving the ball) for improvedrecognition, making them unequipped to reason about novel interactions withactions or objects unobserved in the training data. We propose to predict the "interactee" in novel images---that is, to localizethe \emph{object} of a person's action. Given an arbitrary image with adetected person, the goal is to produce a saliency map indicating the mostlikely positions and scales where that person's interactee would be found. Tothat end, we explore ways to learn the generic, action-independent connectionsbetween (a) representations of a person's pose, gaze, and scene cues and (b)the interactee object's position and scale. We provide results on a newlycollected UT Interactee dataset spanning more than 10,000 images from SUN,PASCAL, and COCO. We show that the proposed interaction-informed saliencymetric has practical utility for four tasks: contextual object detection, imageretargeting, predicting object importance, and data-driven natural languagescene description. All four scenarios reveal the value in linking the subjectto its object in order to understand the story of an image.
arxiv-17400-222 | SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions | http://arxiv.org/pdf/1604.04835v1.pdf | author:Han Xiao, Minlie Huang, Xiaoyan Zhu category:cs.CL cs.LG published:2016-04-17 summary:Knowledge graph embedding represents the entities and relations as numericalvectors, and then knowledge analysis could be promoted as a numerical method.So far, most methods merely concentrate on the fact triples that are composedby the symbolic entities and relations, while the textual information which issupposed to be most critical in NLP could hardly play a reasonable role. Forthis end, this paper proposes the method SSP which jointly learns from thesymbolic triples and textual descriptions. Our model could interact both twoinformation sources by characterizing the correlations, by which means, thetextual descriptions could make effects to discover semantic relevance andoffer precise semantic embedding. Extensive experiments show our methodachieves the substantial improvements against the state-of-the-art baselines onthe tasks of knowledge graph completion and entity classification.
arxiv-17400-223 | Probabilistic Receiver Architecture Combining BP, MF, and EP for Multi-Signal Detection | http://arxiv.org/pdf/1604.04834v1.pdf | author:Daniel J. Jakubisin, R. Michael Buehrer, Claudio R. C. M. da Silva category:cs.IT math.IT stat.ML published:2016-04-17 summary:Receiver algorithms which combine belief propagation (BP) with the mean field(MF) approximation are well-suited for inference of both continuous anddiscrete random variables. In wireless scenarios involving detection ofmultiple signals, the standard construction of the combined BP-MF frameworkincludes the equalization or multi-user detection functions within the MFsubgraph. In this paper, we show that the MF approximation is not particularlyeffective for multi-signal detection. We develop a new factor graphconstruction for application of the BP-MF framework to problems involving thedetection of multiple signals. We then develop a low-complexity variant to theproposed construction in which Gaussian BP is applied to the equalizationfactors. In this case, the factor graph of the joint probability distributionis divided into three subgraphs: (i) a MF subgraph comprised of the observationfactors and channel estimation, (ii) a Gaussian BP subgraph which is applied tomulti-signal detection, and (iii) a discrete BP subgraph which is applied todemodulation and decoding. Expectation propagation is used to approximatediscrete distributions with a Gaussian distribution and links the discrete BPand Gaussian BP subgraphs. The result is a probabilistic receiver architecturewith strong theoretical justification which can be applied to multi-signaldetection.
arxiv-17400-224 | Visual saliency detection: a Kalman filter based approach | http://arxiv.org/pdf/1604.04825v1.pdf | author:Sourya Roy, Pabitra Mitra category:cs.CV published:2016-04-17 summary:In this paper we propose a Kalman filter aided saliency detection model whichis based on the conjecture that salient regions are considerably different fromour "visual expectation" or they are "visually surprising" in nature. In thiswork, we have structured our model with an immediate objective to predictsaliency in static images. However, the proposed model can be easily extendedfor space-time saliency prediction. Our approach was evaluated using twopublicly available benchmark data sets and results have been compared withother existing saliency models. The results clearly illustrate the superiorperformance of the proposed model over other approaches.
arxiv-17400-225 | Structured Sparse Convolutional Autoencoder | http://arxiv.org/pdf/1604.04812v1.pdf | author:Ehsan Hosseini-Asl, Jacek M. Zurada category:cs.LG cs.NE published:2016-04-17 summary:This paper aims to improve the feature learning in Convolutional Networks(Convnet) by capturing the structure of objects. A new sparsity function isimposed on the extracted featuremap to capture the structure and shape of thelearned object, extracting interpretable features to improve the predictionperformance. The proposed algorithm is based on organizing the activationwithin and across featuremap by constraining the node activities through$\ell_{2}$ and $\ell_{1}$ normalization in a structured form.
arxiv-17400-226 | Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering | http://arxiv.org/pdf/1604.04808v1.pdf | author:Arun Mallya, Svetlana Lazebnik category:cs.CV published:2016-04-16 summary:In this paper, we propose a convolutional deep network model which utilizeslocal and global context through feature fusion to make human activity labelpredictions and achieve state-of-the-art performance on two different activityrecognition datasets, the HICO and MPII Human Pose Dataset. We use MultipleInstance Learning to handle the lack of full person instance-label supervisionand weighted loss to handle the unbalanced training data. Further, we show howexpert knowledge from these specialized datasets can be transferred to improveaccuracy on the Visual Question Answering (VQA) task, in the form of multiplechoice fill-in-the-blank questions (Visual Madlibs). Specifically, we tackletwo types of questions on person's activity and person-object relationship andshow improvements over generic features trained on the ImageNet classificationtask.
arxiv-17400-227 | Supervised and Unsupervised Ensembling for Knowledge Base Population | http://arxiv.org/pdf/1604.04802v1.pdf | author:Nazneen Fatema Rajani, Raymond J. Mooney category:cs.CL cs.LG published:2016-04-16 summary:We present results on combining supervised and unsupervised methods toensemble multiple systems for two popular Knowledge Base Population (KBP)tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery andLinking (TEDL). We demonstrate that our combined system along with auxiliaryfeatures outperforms the best performing system for both tasks in the 2015competition, several ensembling baselines, as well as the state-of-the-artstacking approach to ensembling KBP systems. The success of our technique ontwo different and challenging problems demonstrates the power and generality ofour combined approach to ensembling.
arxiv-17400-228 | Variance Reduction in SGD by Distributed Importance Sampling | http://arxiv.org/pdf/1511.06481v7.pdf | author:Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2015-11-20 summary:Humans are able to accelerate their learning by selecting training materialsthat are the most informative and at the appropriate level of difficulty. Wepropose a framework for distributing deep learning in which one set of workerssearch for the most informative examples in parallel while a single workerupdates the model on examples selected by importance sampling. This leads themodel to update using an unbiased estimate of the gradient which also hasminimum variance when the sampling proposal is proportional to the L2-norm ofthe gradient. We show experimentally that this method reduces gradient varianceeven in a context where the cost of synchronization across machines cannot beignored, and where the factors for importance sampling are not updatedinstantly across the training set.
arxiv-17400-229 | A Hierarchical Genetic Optimization of a Fuzzy Logic System for Flow Control in Micro Grids | http://arxiv.org/pdf/1604.04789v1.pdf | author:Enrico De Santis, Alireza Sadeghian, Antonello Rizzi category:cs.AI cs.NE published:2016-04-16 summary:Computational Intelligence techniques are today widely used to solve complexengineering problems. Bio-inspired algorithms like Genetic Algorithms and FuzzyInference Systems are nowadays adopted as hybrids techniques in the commercialand industrial environment. In this paper, we present an interestingapplication of the FUZZY-GA paradigm to Smart Grids. In particular, this studyfocuses on the possibility of tuning a Fuzzy Rule Base trying to discover, bymeans of a GA, a minimal fuzzy rules set in a Fuzzy Logic Controller (FLC)adopted to perform decision making for the power flow management task in amicrogrid. The RB optimization is obtained through Hierarchical GeneticAlgorithm, based on an encoding scheme inspired by Nature, applied to theoptimization of the FIS parameters. Tests show how the proposed controllerscheme is effective in maximizing the economic return when dealing with theproblem of power flows management in a microgrid, equipped with an energystorage system.
arxiv-17400-230 | ACD: Action Concept Discovery from Image-Sentence Corpora | http://arxiv.org/pdf/1604.04784v1.pdf | author:Jiyang Gao, Chen Sun, Ram Nevatia category:cs.CV published:2016-04-16 summary:Action classification in still images is an important task in computervision. It is challenging as the appearances of ac- tions may vary depending ontheir context (e.g. associated objects). Manually labeling of contextinformation would be time consuming and difficult to scale up. To address thischallenge, we propose a method to automatically discover and cluster actionconcepts, and learn their classifiers from weakly supervised image-sentencecorpora. It obtains candidate action concepts by extracting verb-object pairsfrom sentences and verifies their visualness with the associated images.Candidate action concepts are then clustered by using a multi-modalrepresentation with image embeddings from deep convolutional networks and textembeddings from word2vec. More than one hundred human action conceptclassifiers are learned from the Flickr 30k dataset with no additional humaneffort and promising classification results are obtained. We further apply theAdaBoost algorithm to automatically select and combine relevant action conceptsgiven an action query. Promising results have been shown on the PASCAL VOC 2012action classification benchmark, which has zero overlap with Flickr30k.
arxiv-17400-231 | Efficient Dictionary Learning with Sparseness-Enforcing Projections | http://arxiv.org/pdf/1604.04767v1.pdf | author:Markus Thom, Matthias Rapp, Günther Palm category:cs.LG cs.CV cs.NE published:2016-04-16 summary:Learning dictionaries suitable for sparse coding instead of using engineeredbases has proven effective in a variety of image processing tasks. This paperstudies the optimization of dictionaries on image data where the representationis enforced to be explicitly sparse with respect to a smooth, normalizedsparseness measure. This involves the computation of Euclidean projections ontolevel sets of the sparseness measure. While previous algorithms for thisoptimization problem had at least quasi-linear time complexity, here the firstalgorithm with linear time complexity and constant space complexity isproposed. The key for this is the mathematically rigorous derivation of acharacterization of the projection's result based on a soft-shrinkage function.This theory is applied in an original algorithm called Easy Dictionary Learning(EZDL), which learns dictionaries with a simple and fast-to-computeHebbian-like learning rule. The new algorithm is efficient, expressive andparticularly simple to implement. It is demonstrated that despite itssimplicity, the proposed learning algorithm is able to generate a rich varietyof dictionaries, in particular a topographic organization of atoms or separableatoms. Further, the dictionaries are as expressive as those of benchmarklearning algorithms in terms of the reproduction quality on entire images, andresult in an equivalent denoising performance. EZDL learns approximately 30 %faster than the already very efficient Online Dictionary Learning algorithm,and is therefore eligible for rapid data set analysis and problems with vastquantities of learning samples.
arxiv-17400-232 | Closed loop interactions between spiking neural network and robotic simulators based on MUSIC and ROS | http://arxiv.org/pdf/1604.04764v1.pdf | author:Philipp Weidel, Mikael Djurfeldt, Renato Duarte, Abigail Morrison category:cs.NE cs.RO published:2016-04-16 summary:In order to properly assess the function and computational properties ofsimulated neural systems, it is necessary to account for the nature of thestimuli that drive the system. However, providing stimuli that are rich and yetboth reproducible and amenable to experimental manipulations is technicallychallenging, and even more so if a closed-loop scenario is required. In thiswork, we present a novel approach to solve this problem, connecting roboticsand neural network simulators. We implement a middleware solution that bridgesthe Robotic Operating System (ROS) to the Multi-Simulator Coordinator (MUSIC).This enables any robotic and neural simulators that implement the correspondinginterfaces to be efficiently coupled, allowing real-time performance for a widerange of configurations. This work extends the toolset available forresearchers in both neurorobotics and computational neuroscience, and createsthe opportunity to perform closed-loop experiments of arbitrary complexity toaddress questions in multiple areas, including embodiment, agency, andreinforcement learning.
arxiv-17400-233 | Multi-Oriented Text Detection with Fully Convolutional Networks | http://arxiv.org/pdf/1604.04018v2.pdf | author:Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, Xiang Bai category:cs.CV published:2016-04-14 summary:In this paper, we propose a novel approach for text detec- tion in naturalimages. Both local and global cues are taken into account for localizing textlines in a coarse-to-fine pro- cedure. First, a Fully Convolutional Network(FCN) model is trained to predict the salient map of text regions in a holisticmanner. Then, text line hypotheses are estimated by combining the salient mapand character components. Fi- nally, another FCN classifier is used to predictthe centroid of each character, in order to remove the false hypotheses. Theframework is general for handling text in multiple ori- entations, languagesand fonts. The proposed method con- sistently achieves the state-of-the-artperformance on three text detection benchmarks: MSRA-TD500, ICDAR2015 andICDAR2013.
arxiv-17400-234 | Smoothed Hierarchical Dirichlet Process: A Non-Parametric Approach to Constraint Measures | http://arxiv.org/pdf/1604.04741v1.pdf | author:Cheng Luo, Yang Xiang, Richard Yi Da Xu category:stat.ML published:2016-04-16 summary:Time-varying mixture densities occur in many scenarios, for example, thedistributions of keywords that appear in publications may evolve from year toyear, video frame features associated with multiple targets may evolve in asequence. Any models that realistically cater to this phenomenon must exhibittwo important properties: the underlying mixture densities must have an unknownnumber of mixtures, and there must be some "smoothness" constraints in placefor the adjacent mixture densities. The traditional Hierarchical DirichletProcess (HDP) may be suited to the first property, but certainly not thesecond. This is due to how each random measure in the lower hierarchies issampled independent of each other and hence does not facilitate any temporalcorrelations. To overcome such shortcomings, we proposed a new SmoothedHierarchical Dirichlet Process (sHDP). The key novelty of this model is that weplace a temporal constraint amongst the nearby discrete measures $\{G_j\}$ inthe form of symmetric Kullback-Leibler (KL) Divergence with a fixed bound $B$.Although the constraint we place only involves a single scalar value, itnonetheless allows for flexibility in the corresponding successive measures.Remarkably, it also led us to infer the model within the stick-breaking processwhere the traditional Beta distribution used in stick-breaking is now replacedby a new constraint calculated from $B$. We present the inference algorithm andelaborate on its solutions. Our experiment using NIPS keywords has shown thedesirable effect of the model.
arxiv-17400-235 | Automatic Segmentation of Dynamic Objects from an Image Pair | http://arxiv.org/pdf/1604.04724v1.pdf | author:Sri Raghu Malireddi, Shanmuganathan Raman category:cs.CV published:2016-04-16 summary:Automatic segmentation of objects from a single image is a challengingproblem which generally requires training on large number of images. Weconsider the problem of automatically segmenting only the dynamic objects froma given pair of images of a scene captured from different positions. We exploitdense correspondences along with saliency measures in order to first localizethe interest points on the dynamic objects from the two images. We propose anovel approach based on techniques from computational geometry in order toautomatically segment the dynamic objects from both the images using a top-downsegmentation strategy. We discuss how the proposed approach is unique innovelty compared to other state-of-the-art segmentation algorithms. We showthat the proposed approach for segmentation is efficient in handling largemotions and is able to achieve very good segmentation of the objects fordifferent scenes. We analyse the results with respect to the manually markedground truth segmentation masks created using our own dataset and provide keyobservations in order to improve the work in future.
arxiv-17400-236 | DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression | http://arxiv.org/pdf/1604.04706v1.pdf | author:Parameswaran Raman, Shin Matsushima, Xinhua Zhang, Hyokun Yun, S. V. N. Vishwanathan category:cs.LG stat.ML published:2016-04-16 summary:Multinomial logistic regression is a popular tool in the arsenal of machinelearning algorithms, yet scaling it to datasets with very large number of datapoints and classes has not been trivial. This is primarily because one needs tocompute the log-partition function on every data point. This makes distributingthe computation hard. In this paper, we present a distributed stochasticgradient descent based optimization method (DS-MLR) for scaling up multinomiallogistic regression problems to very large data. Our algorithm exploitsdouble-separability, an attractive property we observe in the objectivefunctions of several models in machine learning, that allows us to distributeboth data and model parameters simultaneously across multiple machines. Inaddition to being easily parallelizable, our algorithm achieves good testaccuracy within a short period of time, with a low overall time and memoryfootprint as demonstrated by empirical results on both single and multi-machinesettings. For instance, on a dataset with 93,805 training instances and 12,294classes, we achieve close to optimal f-score in 10,000 seconds using 2 machineseach having 12 cores.
arxiv-17400-237 | Phone-based Metric as a Predictor for Basic Personality Traits | http://arxiv.org/pdf/1604.04696v1.pdf | author:Bjarke Mønsted, Anders Mollgaard, Joachim Mathiesen category:cs.SI cs.LG physics.soc-ph published:2016-04-16 summary:Basic personality traits are typically assessed through questionnaires. Herewe consider phone-based metrics as a way to asses personality traits. We usedata from smartphones with custom data-collection software distributed to 730individuals. The data includes information about location, physical motion,face-to-face contacts, online social network friends, text messages and calls.The data is further complemented by questionnaire-based data on basicpersonality traits. From the phone-based metrics, we define a set of behavioralvariables, which we use in a prediction of basic personality traits. We findthat predominantly, the Big Five personality traits extraversion and, to somedegree, neuroticism are strongly expressed in our data. As an alternative tothe Big Five, we investigate whether other linear combinations of the 44questions underlying the Big Five Inventory are more predictable. In a tertileclassification problem, basic dimensionality reduction techniques, such asindependent component analysis, increase the predictability relative to thebaseline from $11\%$ to $23\%$. Finally, from a supervised linear classifier,we were able to further improve this predictability to $33\%$. In all cases,the most predictable projections had an overweight of the questions related toextraversion and neuroticism. In addition, our findings indicate that the scoresystem underlying the Big Five Inventory disregards a part of the informationavailable in the 44 questions.
arxiv-17400-238 | Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection | http://arxiv.org/pdf/1604.04693v1.pdf | author:Yu Xiang, Wongun Choi, Yuanqing Lin, Silvio Savarese category:cs.CV published:2016-04-16 summary:In CNN-based object detection methods, region proposal becomes a bottleneckwhen objects exhibit significant scale variation, occlusion or truncation. Inaddition, these methods mainly focus on 2D object detection and cannot estimatedetailed properties of objects. In this paper, we propose subcategory-awareCNNs for object detection. We introduce a novel region proposal network thatuses subcategory information to guide the proposal generating process, and anew detection network for joint detection and subcategory classification. Byusing subcategories related to object pose, we achieve state-of-the-artperformance on both detection and pose estimation on commonly used benchmarks.
arxiv-17400-239 | Dropping Convexity for Faster Semi-definite Optimization | http://arxiv.org/pdf/1509.03917v3.pdf | author:Srinadh Bhojanapalli, Anastasios Kyrillidis, Sujay Sanghavi category:stat.ML cs.DS cs.IT cs.LG cs.NA math.IT math.OC published:2015-09-14 summary:We study the minimization of a convex function $f(X)$ over the set of$n\times n$ positive semi-definite matrices, but when the problem is recast as$\min_U g(U) := f(UU^\top)$, with $U \in \mathbb{R}^{n \times r}$ and $r \leqn$. We study the performance of gradient descent on $g$---which we refer to asFactored Gradient Descent (FGD)---under standard assumptions on the originalfunction $f$. We provide a rule for selecting the step size and, with this choice, showthat the local convergence rate of FGD mirrors that of standard gradientdescent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ forsmooth $f$, and exponentially small in $k$ when $f$ is (restricted) stronglyconvex. In addition, we provide a procedure to initialize FGD for (restricted)strongly convex objectives and when one only has access to $f$ via afirst-order oracle; for several problem instances, such proper initializationleads to global convergence guarantees. FGD and similar procedures are widely used in practice for problems that canbe posed as matrix factorization. To the best of our knowledge, this is thefirst paper to provide precise convergence rate guarantees for general convexfunctions under standard convex assumptions.
arxiv-17400-240 | Anatomy-Aware Measurement of Segmentation Accuracy | http://arxiv.org/pdf/1604.04678v1.pdf | author:Hamid R. Tizhoosh, Ahmed A. Othman category:cs.CV published:2016-04-16 summary:Quantifying the accuracy of segmentation and manual delineation of organs,tissue types and tumors in medical images is a necessary measurement thatsuffers from multiple problems. One major shortcoming of all accuracy measuresis that they neglect the anatomical significance or relevance of differentzones within a given segment. Hence, existing accuracy metrics measure theoverlap of a given segment with a ground-truth without any anatomicaldiscrimination inside the segment. For instance, if we understand the rectalwall or urethral sphincter as anatomical zones, then current accuracy measuresignore their significance when they are applied to assess the quality of theprostate gland segments. In this paper, we propose an anatomy-aware measurementscheme for segmentation accuracy of medical images. The idea is to create a``master gold'' based on a consensus shape containing not just the outline ofthe segment but also the outlines of the internal zones if existent orrelevant. To apply this new approach to accuracy measurement, we introduce theanatomy-aware extensions of both Dice coefficient and Jaccard index andinvestigate their effect using 500 synthetic prostate ultrasound images with 20different segments for each image. We show that through anatomy-sensitivecalculation of segmentation accuracy, namely by considering relevant anatomicalzones, not only the measurement of individual users can change but also theranking of users' segmentation skills may require reordering.
arxiv-17400-241 | Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction | http://arxiv.org/pdf/1604.04677v1.pdf | author:Allen Schmaltz, Yoon Kim, Alexander M. Rush, Stuart M. Shieber category:cs.CL published:2016-04-16 summary:We demonstrate that an attention-based encoder-decoder model can be used forsentence-level grammatical error identification for the Automated Evaluation ofScientific Writing (AESW) Shared Task 2016. The attention-based encoder-decodermodels can be used for the generation of corrections, in addition to erroridentification, which is of interest for certain end-user applications. We showthat a character-based encoder-decoder model is particularly effective,outperforming other results on the AESW Shared Task on its own, and showinggains over a word-based counterpart. Our final model--a combination of threecharacter-based encoder-decoder models, one word-based encoder-decoder model,and a sentence-level CNN--is the highest performing system on the AESW 2016binary prediction Shared Task.
arxiv-17400-242 | Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform | http://arxiv.org/pdf/1604.04676v1.pdf | author:Xinran Liu, Hamid R. Tizhoosh, Jonathan Kofman category:cs.CV published:2016-04-16 summary:Content-based image retrieval (CBIR) in large medical image archives is achallenging and necessary task. Generally, different feature extraction methodsare used to assign expressive and invariant features to each image such thatthe search for similar images comes down to feature classification and/ormatching. The present work introduces a new image retrieval method for medicalapplications that employs a convolutional neural network (CNN) with recentlyintroduced Radon barcodes. We combine neural codes for global classificationwith Radon barcodes for the final retrieval. We also examine image search basedon regions of interest (ROI) matching after image retrieval. The IRMA datasetwith more than 14,000 x-rays images is used to evaluate the performance of ourmethod. Experimental results show that our approach is superior to manypublished works.
arxiv-17400-243 | Radon Features and Barcodes for Medical Image Retrieval via SVM | http://arxiv.org/pdf/1604.04675v1.pdf | author:Shujin Zhu, H. R. Tizhoosh category:cs.CV published:2016-04-16 summary:For more than two decades, research has been performed on content-based imageretrieval (CBIR). By combining Radon projections and the support vectormachines (SVM), a content-based medical image retrieval method is presented inthis work. The proposed approach employs the normalized Radon projections withcorresponding image category labels to build an SVM classifier, and the Radonbarcode database which encodes every image in a binary format is also generatedsimultaneously to tag all images. To retrieve similar images when a query imageis given, Radon projections and the barcode of the query image are generated.Subsequently, the k-nearest neighbor search method is applied to find theimages with minimum Hamming distance of the Radon barcode within the same classpredicted by the trained SVM classifier that uses Radon features. Theperformance of the proposed method is validated by using the IRMA 2009 datasetwith 14,410 x-ray images in 57 categories. The results demonstrate that ourmethod has the capacity to retrieve similar responses for the correctlyidentified query image and even for those mistakenly classified by SVM. Theapproach further is very fast and has low memory requirement.
arxiv-17400-244 | Evolutionary Projection Selection for Radon Barcodes | http://arxiv.org/pdf/1604.04673v1.pdf | author:Hamid R. Tizhoosh, Shahryar Rahnamayan category:cs.CV published:2016-04-16 summary:Recently, Radon transformation has been used to generate barcodes for taggingmedical images. The under-sampled image is projected in certain directions, andeach projection is binarized using a local threshold. The concatenation of thethresholded projections creates a barcode that can be used for tagging orannotating medical images. A small number of equidistant projections, e.g., 4or 8, is generally used to generate short barcodes. However, due to the diversenature of digital images, and since we are only working with a small number ofprojections (to keep the barcode short), taking equidistant projections may notbe the best course of action. In this paper, we proposed to find $n$ optimalprojections, whereas $n\!<\!180$, in order to increase the expressiveness ofRadon barcodes. We show examples for the exhaustive search for the simple casewhen we attempt to find 4 best projections out of 16 equidistant projectionsand compare it with the evolutionary approach in order to establish the benefitof the latter when operating on a small population size as in the case ofmicro-DE. We randomly selected 10 different classes from IRMA dataset (14,400x-ray images in 58 classes) and further randomly selected 5 images per classfor our tests.
arxiv-17400-245 | Parallelizing Word2Vec in Shared and Distributed Memory | http://arxiv.org/pdf/1604.04661v1.pdf | author:Shihao Ji, Nadathur Satish, Sheng Li, Pradeep Dubey category:cs.DC cs.CL stat.ML published:2016-04-15 summary:Word2Vec is a widely used algorithm for extracting low-dimensional vectorrepresentations of words. It generated considerable excitement in the machinelearning and natural language processing (NLP) communities recently due to itsexceptional performance in many NLP applications such as named entityrecognition, sentiment analysis, machine translation and question answering.State-of-the-art algorithms including those by Mikolov et al. have beenparallelized for multi-core CPU architectures but are based on vector-vectoroperations that are memory-bandwidth intensive and do not efficiently usecomputational resources. In this work, we improve reuse of various datastructures in the algorithm through the use of minibatching, hence allowing usto express the problem using matrix multiply operations. We also exploredifferent techniques to parallelize word2vec computation across nodes in acompute cluster, and demonstrate good strong scalability up to 32 nodes. Incombination, these techniques allow us to scale up the computation nearlinearly across cores and nodes, and process hundreds of millions of words persecond, which is the fastest word2vec implementation to the best of ourknowledge.
arxiv-17400-246 | Bags of Local Convolutional Features for Scalable Instance Search | http://arxiv.org/pdf/1604.04653v1.pdf | author:Eva Mohedano, Amaia Salvador, Kevin McGuinness, Ferran Marques, Noel E. O'Connor, Xavier Giro-i-Nieto category:cs.CV cs.MM published:2016-04-15 summary:This work proposes a simple instance retrieval pipeline based on encoding theconvolutional features of CNN using the bag of words aggregation scheme (BoW).Assigning each local array of activations in a convolutional layer to a visualword produces an \textit{assignment map}, a compact representation that relatesregions of an image with a visual word. We use the assignment map for fastspatial reranking, obtaining object localizations that are used for queryexpansion. We demonstrate the suitability of the BoW representation based onlocal CNN features for instance retrieval, achieving competitive performance onthe Oxford and Paris buildings benchmarks. We show that our proposed system forCNN feature aggregation with BoW outperforms state-of-the-art techniques usingsum pooling at a subset of the challenging TRECVid INS benchmark.
arxiv-17400-247 | ModelWizard: Toward Interactive Model Construction | http://arxiv.org/pdf/1604.04639v1.pdf | author:Dylan Hutchison category:cs.PL cs.LG published:2016-04-15 summary:Data scientists engage in model construction to discover machine learningmodels that well explain a dataset, in terms of predictiveness,understandability and generalization across domains. Questions such as "what ifwe model common cause Z" and "what if Y's dependence on X reverses" inspiremany candidate models to consider and compare, yet current tools emphasizeconstructing a final model all at once. To more naturally reflect exploration when debating numerous models, wepropose an interactive model construction framework grounded in composableoperations. Primitive operations capture core steps refining data and modelthat, when verified, form an inductive basis to prove model validity. Derived,composite operations enable advanced model families, both generic andspecialized, abstracted away from low-level details. We prototype our envisioned framework in ModelWizard, a domain-specificlanguage embedded in F# to construct Tabular models. We enumerate languagedesign and demonstrate its use through several applications, emphasizing howlanguage may facilitate creation of complex models. To future engineersdesigning data science languages and tools, we offer ModelWizard's design as anew model construction paradigm, speeding discovery of our universe'sstructure.
arxiv-17400-248 | Convex Biclustering | http://arxiv.org/pdf/1408.0856v4.pdf | author:Eric C. Chi, Genevera I. Allen, Richard G. Baraniuk category:stat.ME stat.ML published:2014-08-05 summary:In the biclustering problem, we seek to simultaneously group observations andfeatures. While biclustering has applications in a wide array of domains,ranging from text mining to collaborative filtering, the problem of identifyingstructure in high dimensional genomic data motivates this work. In thiscontext, biclustering enables us to identify subsets of genes that areco-expressed only within a subset of experimental conditions. We present aconvex formulation of the biclustering problem that possesses a unique globalminimizer and an iterative algorithm, COBRA, that is guaranteed to identify it.Our approach generates an entire solution path of possible biclusters as asingle tuning parameter is varied. We also show how to reduce the problem ofselecting this tuning parameter to solving a trivial modification of the convexbiclustering problem. The key contributions of our work are its simplicity,interpretability, and algorithmic guarantees - features that arguably arelacking in the current alternative algorithms. We demonstrate the advantages ofour approach, which includes stably and reproducibly identifying biclusterings,on simulated and real microarray data.
arxiv-17400-249 | Make Up Your Mind: The Price of Online Queries in Differential Privacy | http://arxiv.org/pdf/1604.04618v1.pdf | author:Mark Bun, Thomas Steinke, Jonathan Ullman category:cs.CR cs.DS cs.LG published:2016-04-15 summary:We consider the problem of answering queries about a sensitive datasetsubject to differential privacy. The queries may be chosen adversarially from alarger set Q of allowable queries in one of three ways, which we list in orderfrom easiest to hardest to answer: Offline: The queries are chosen all at once and the differentially privatemechanism answers the queries in a single batch. Online: The queries are chosen all at once, but the mechanism only receivesthe queries in a streaming fashion and must answer each query before seeing thenext query. Adaptive: The queries are chosen one at a time and the mechanism must answereach query before the next query is chosen. In particular, each query maydepend on the answers given to previous queries. Many differentially private mechanisms are just as efficient in the adaptivemodel as they are in the offline model. Meanwhile, most lower bounds fordifferential privacy hold in the offline setting. This suggests that the threemodels may be equivalent. We prove that these models are all, in fact, distinct. Specifically, we showthat there is a family of statistical queries such that exponentially morequeries from this family can be answered in the offline model than in theonline model. We also exhibit a family of search queries such thatexponentially more queries from this family can be answered in the online modelthan in the adaptive model. We also investigate whether such separations mighthold for simple queries like threshold queries over the real line.
arxiv-17400-250 | On deterministic conditions for subspace clustering under missing data | http://arxiv.org/pdf/1604.04615v1.pdf | author:Wenqi Wang, Shuchin Aeron, Vaneet Aggarwal category:cs.IT math.IT stat.ML published:2016-04-15 summary:In this paper we present deterministic analysis of sufficient conditions forsparse subspace clustering under missing data, when data is assumed to comefrom a Union of Subspaces (UoS) model. In this context we consider two cases,namely Case I when all the points are sampled at the same co-ordinates, andCase II when points are sampled at different locations. We show that resultsfor Case I directly follow from several existing results in the literature,while results for Case II are not as straightforward and we provide a set ofdual conditions under which, perfect clustering holds true. We provideextensive set of simulation results for clustering as well as completion ofdata under missing entries, under the UoS model. Our experimental resultsindicate that in contrast to the full data case, accurate clustering does notimply accurate subspace identification and completion, indicating the naturalorder of relative hardness of these problems.
arxiv-17400-251 | Similarity of symbol frequency distributions with heavy tails | http://arxiv.org/pdf/1510.00277v2.pdf | author:Martin Gerlach, Francesc Font-Clos, Eduardo G. Altmann category:physics.soc-ph cs.CL published:2015-10-01 summary:Quantifying the similarity between symbolic sequences is a traditionalproblem in Information Theory which requires comparing the frequencies ofsymbols in different sequences. In numerous modern applications, ranging fromDNA over music to texts, the distribution of symbol frequencies ischaracterized by heavy-tailed distributions (e.g., Zipf's law). The largenumber of low-frequency symbols in these distributions poses major difficultiesto the estimation of the similarity between sequences, e.g., they hinder anaccurate finite-size estimation of entropies. Here we show analytically how thesystematic (bias) and statistical (fluctuations) errors in these estimationsdepend on the sample size~$N$ and on the exponent~$\gamma$ of the heavy-taileddistribution. Our results are valid for the Shannon entropy $(\alpha=1)$, itscorresponding similarity measures (e.g., the Jensen-Shanon divergence), andalso for measures based on the generalized entropy of order $\alpha$. For small$\alpha$'s, including $\alpha=1$, the errors decay slower than the $1/N$-decayobserved in short-tailed distributions. For $\alpha$ larger than a criticalvalue $\alpha^* = 1+1/\gamma \leq 2$, the $1/N$-decay is recovered. We show thepractical significance of our results by quantifying the evolution of theEnglish language over the last two centuries using a complete $\alpha$-spectrumof measures. We find that frequent words change more slowly than less frequentwords and that $\alpha=2$ provides the most robust measure to quantify languagechange.
arxiv-17400-252 | Estimation of low rank density matrices: bounds in Schatten norms and other distances | http://arxiv.org/pdf/1604.04600v1.pdf | author:Dong Xia, Vladimir Koltchinskii category:stat.ML math.ST stat.TH published:2016-04-15 summary:Let ${\mathcal S}_m$ be the set of all $m\times m$ density matrices(Hermitian positively semi-definite matrices of unit trace). Consider a problemof estimation of an unknown density matrix $\rho\in {\mathcal S}_m$ based onoutcomes of $n$ measurements of observables $X_1,\dots, X_n\in {\mathbb H}_m$(${\mathbb H}_m$ being the space of $m\times m$ Hermitian matrices) for aquantum system identically prepared $n$ times in state $\rho.$ Outcomes$Y_1,\dots, Y_n$ of such measurements could be described by a trace regressionmodel in which ${\mathbb E}_{\rho}(Y_jX_j)={\rm tr}(\rho X_j), j=1,\dots, n.$The design variables $X_1,\dots, X_n$ are often sampled at random from theuniform distribution in an orthonormal basis $\{E_1,\dots, E_{m^2}\}$ of${\mathbb H}_m$ (such as Pauli basis). The goal is to estimate the unknowndensity matrix $\rho$ based on the data $(X_1,Y_1), \dots, (X_n,Y_n).$ Let $$\hat Z:=\frac{m^2}{n}\sum_{j=1}^n Y_j X_j $$ and let $\check \rho$ be theprojection of $\hat Z$ onto the convex set ${\mathcal S}_m$ of densitymatrices. It is shown that for estimator $\check \rho$ the minimax lower boundsin classes of low rank density matrices (established earlier) are attained uplogarithmic factors for all Schatten $p$-norm distances, $p\in [1,\infty]$ andfor Bures version of quantum Hellinger distance. Moreover, for a slightlymodified version of estimator $\check \rho$ the same property holds also forquantum relative entropy (Kullback-Leibler) distance between density matrices.
arxiv-17400-253 | Revisiting Distributed Synchronous SGD | http://arxiv.org/pdf/1604.00981v2.pdf | author:Jianmin Chen, Rajat Monga, Samy Bengio, Rafal Jozefowicz category:cs.LG cs.DC cs.NE published:2016-04-04 summary:The recent success of deep learning approaches for domains like speechrecognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015)stems from many algorithmic improvements but also from the fact that the sizeof available training data has grown significantly over the years, togetherwith the computing power, in terms of both CPUs and GPUs. While a single GPUoften provides algorithmic simplicity and speed up to a given scale of data andmodel, there exist an operating point where a distributed implementation oftraining algorithms for deep architectures becomes necessary. Previous works have been focusing on asynchronous SGD training, which workswell up to a few dozens of workers for some models. In this work, we show thatsynchronous SGD training, with the help of backup workers, can not only achievebetter accuracy, but also reach convergence faster with respect to wall time,i.e. use more workers more efficiently.
arxiv-17400-254 | Learning Temporal Regularity in Video Sequences | http://arxiv.org/pdf/1604.04574v1.pdf | author:Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, Larry S. Davis category:cs.CV published:2016-04-15 summary:Perceiving meaningful activities in a long video sequence is a challengingproblem due to ambiguous definition of 'meaningfulness' as well as clutters inthe scene. We approach this problem by learning a generative model for regularmotion patterns, termed as regularity, using multiple sources with very limitedsupervision. Specifically, we propose two methods that are built upon theautoencoders for their ability to work with little to no supervision. We firstleverage the conventional handcrafted spatio-temporal local features and learna fully connected autoencoder on them. Second, we build a fully convolutionalfeed-forward autoencoder to learn both the local features and the classifiersas an end-to-end learning framework. Our model can capture the regularitiesfrom multiple datasets. We evaluate our methods in both qualitative andquantitative ways - showing the learned regularity of videos in various aspectsand demonstrating competitive performance on anomaly detection datasets as anapplication.
arxiv-17400-255 | CNN-RNN: A Unified Framework for Multi-label Image Classification | http://arxiv.org/pdf/1604.04573v1.pdf | author:Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu category:cs.CV cs.LG cs.NE published:2016-04-15 summary:While deep convolutional neural networks (CNNs) have shown a great success insingle-label image classification, it is important to note that real worldimages generally contain multiple labels, which could correspond to differentobjects, scenes, actions and attributes in an image. Traditional approaches tomulti-label image classification learn independent classifiers for eachcategory and employ ranking or thresholding on the classification results.These techniques, although working well, fail to explicitly exploit the labeldependencies in an image. In this paper, we utilize recurrent neural networks(RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNNframework learns a joint image-label embedding to characterize the semanticlabel dependency as well as the image-label relevance, and it can be trainedend-to-end from scratch to integrate both information in a unified framework.Experimental results on public benchmark datasets demonstrate that the proposedarchitecture achieves better performance than the state-of-the-art multi-labelclassification model
arxiv-17400-256 | Accessing accurate documents by mining auxiliary document information | http://arxiv.org/pdf/1604.04558v1.pdf | author:Jinju Joby, Jyothi Korra category:cs.IR cs.AI cs.LG published:2016-04-15 summary:Earlier techniques of text mining included algorithms like k-means, NaiveBayes, SVM which classify and cluster the text document for mining relevantinformation about the documents. The need for improving the mining techniqueshas us searching for techniques using the available algorithms. This paperproposes one technique which uses the auxiliary information that is presentinside the text documents to improve the mining. This auxiliary information canbe a description to the content. This information can be either useful orcompletely useless for mining. The user should assess the worth of theauxiliary information before considering this technique for text mining. Inthis paper, a combination of classical clustering algorithms is used to minethe datasets. The algorithm runs in two stages which carry out mining atdifferent levels of abstraction. The clustered documents would then beclassified based on the necessary groups. The proposed technique is aimed atimproved results of document clustering.
arxiv-17400-257 | Unsupervised single-particle deep classification via statistical manifold learning | http://arxiv.org/pdf/1604.04539v1.pdf | author:Jiayi Wu, Yongbei Ma, Charles Condgon, Bevin Brett, Shuobing Chen, Qi Ouyang, Youdong Mao category:cs.CV q-bio.QM published:2016-04-15 summary:Structural heterogeneity in single-particle images presents a major challengefor high-resolution cryo-electron microscopy (cryo-EM) structure determination.Here we introduce a statistical manifold learning approach for unsupervisedsingle-particle deep classification. When optimized for Intel high-performancecomputing (HPC) processors, our approach can generate thousands ofreference-free class averages within several hours from hundreds of thousandsof single-particle cryo-EM images. Deep classification thus assists incomputational purification of single-particle datasets for high-resolutionreconstruction.
arxiv-17400-258 | Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models | http://arxiv.org/pdf/1505.04870v3.pdf | author:Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, Svetlana Lazebnik category:cs.CV cs.CL published:2015-05-19 summary:The Flickr30k dataset has become a standard benchmark for sentence-basedimage description. This paper presents Flickr30k Entities, which augments the158k captions from Flickr30k with 244k coreference chains, linking mentions ofthe same entities across different captions for the same image, and associatingthem with 276k manually annotated bounding boxes. Such annotations areessential for continued progress in automatic image description and groundedlanguage understanding. They enable us to define a new benchmark forlocalization of textual entity mentions in an image. We present a strongbaseline for this task that combines an image-text embedding, detectors forcommon objects, a color classifier, and a bias towards selecting largerobjects. While our baseline rivals in accuracy more complex state-of-the-artmodels, we show that its gains cannot be easily parlayed into improvements onsuch tasks as image-sentence retrieval, thus underlining the limitations ofcurrent methods and the need for further research.
arxiv-17400-259 | Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks | http://arxiv.org/pdf/1604.04528v1.pdf | author:Youngbin Park, Sungphill Moon, Il Hong Suh category:cs.CV cs.LG cs.NE cs.RO published:2016-04-15 summary:Kinect skeleton tracker is able to achieve considerable human body trackingperformance in convenient and a low-cost manner. However, The tracker oftencaptures unnatural human poses such as discontinuous and vibrated motions whenself-occlusions occur. A majority of approaches tackle this problem by usingmultiple Kinect sensors in a workspace. Combination of the measurements fromdifferent sensors is then conducted in Kalman filter framework or optimizationproblem is formulated for sensor fusion. However, these methods usually requireheuristics to measure reliability of measurements observed from each Kinectsensor. In this paper, we developed a method to improve Kinect skeleton usingsingle Kinect sensor, in which supervised learning technique was employed tocorrect unnatural tracking motions. Specifically, deep recurrent neuralnetworks were used for improving joint positions and velocities of Kinectskeleton, and three methods were proposed to integrate the refined positionsand velocities for further enhancement. Moreover, we suggested a novel measureto evaluate naturalness of captured motions. We evaluated the proposed approachby comparison with the ground truth obtained using a commercial opticalmaker-based motion capture system.
arxiv-17400-260 | Using Deep Learning for Image-Based Plant Disease Detection | http://arxiv.org/pdf/1604.03169v2.pdf | author:Sharada Prasanna Mohanty, David Hughes, Marcel Salathe category:cs.CV published:2016-04-11 summary:Crop diseases are a major threat to food security, but their rapididentification remains difficult in many parts of the world due to the lack ofthe necessary infrastructure. The combination of increasing global smartphonepenetration and recent advances in computer vision made possible by deeplearning has paved the way for smartphone-assisted disease diagnosis. Using apublic dataset of 54,306 images of diseased and healthy plant leaves collectedunder controlled conditions, we train a deep convolutional neural network toidentify 14 crop species and 26 diseases (or absence thereof). The trainedmodel achieves an accuracy of 99.35% on a held-out test set, demonstrating thefeasibility of this approach. When testing the model on a set of imagescollected from trusted online sources - i.e. taken under conditions differentfrom the images used for training - the model still achieves an accuracy of31.4%. While this accuracy is much higher than the one based on randomselection (2.6%), a more diverse set of training data is needed to improve thegeneral accuracy. Overall, the approach of training deep learning models onincreasingly large and publicly available image datasets presents a clear pathtowards smartphone-assisted crop disease diagnosis on a massive global scale.
arxiv-17400-261 | A short note on extension theorems and their connection to universal consistency in machine learning | http://arxiv.org/pdf/1604.04505v1.pdf | author:Andreas Christmann, Florian Dumpert, Dao-Hong Xiang category:stat.ML cs.LG published:2016-04-15 summary:Statistical machine learning plays an important role in modern statistics andcomputer science. One main goal of statistical machine learning is to provideuniversally consistent algorithms, i.e., the estimator converges in probabilityor in some stronger sense to the Bayes risk or to the Bayes decision function.Kernel methods based on minimizing the regularized risk over a reproducingkernel Hilbert space (RKHS) belong to these statistical machine learningmethods. It is in general unknown which kernel yields optimal results for aparticular data set or for the unknown probability measure. Hence variouskernel learning methods were proposed to choose the kernel and therefore alsoits RKHS in a data adaptive manner. Nevertheless, many practitioners often usethe classical Gaussian RBF kernel or certain Sobolev kernels with good success.The goal of this short note is to offer one possible theoretical explanationfor this empirical fact.
arxiv-17400-262 | Long-term Temporal Convolutions for Action Recognition | http://arxiv.org/pdf/1604.04494v1.pdf | author:Gül Varol, Ivan Laptev, Cordelia Schmid category:cs.CV published:2016-04-15 summary:Typical human actions last several seconds and exhibit characteristicspatio-temporal structure. Recent methods attempt to capture this structure andlearn action representations with convolutional neural networks. Suchrepresentations, however, are typically learned at the level of a few videoframes failing to model actions at their full temporal extent. In this work welearn video representations using neural networks with long-term temporalconvolutions (LTC). We demonstrate that LTC-CNN models with increased temporalextents improve the accuracy of action recognition. We also study the impact ofdifferent low-level representations, such as raw values of video pixels andoptical flow vector fields and demonstrate the importance of high-qualityoptical flow estimation for learning accurate action models. We reportstate-of-the-art results on two challenging benchmarks for human actionrecognition UCF101 (92.7%) and HMDB51 (67.2%).
arxiv-17400-263 | Probing the Intra-Component Correlations within Fisher Vector for Material Classification | http://arxiv.org/pdf/1604.04473v1.pdf | author:Xiaopeng Hong, Xianbiao Qi, Guoying Zhao, Matti Pietikäinen category:cs.CV published:2016-04-15 summary:Fisher vector (FV) has become a popular image representation. One notableunderlying assumption of the FV framework is that local descriptors are welldecorrelated within each cluster so that the covariance matrix for eachGaussian can be simplified to be diagonal. Though the FV usually relies on thePrincipal Component Analysis (PCA) to decorrelate local features, the PCA isapplied to the entire training data and hence it only diagonalizes the\textit{universal} covariance matrix, rather than those w.r.t. the localcomponents. As a result, the local decorrelation assumption is usually notsupported in practice. To relax this assumption, this paper proposes a completed model of the Fishervector, which is termed as the Completed Fisher vector (CFV). The CFV is a moregeneral framework of the FV, since it encodes not only the variances but alsothe correlations of the whitened local descriptors. The CFV thus leads toimproved discriminative power. We take the task of material categorization asan example and experimentally show that: 1) the CFV outperforms the FV underall parameter settings; 2) the CFV is robust to the changes in the number ofcomponents in the mixture; 3) even with a relatively small visual vocabularythe CFV still works well on two challenging datasets.
arxiv-17400-264 | Delta divergence: A novel decision cognizant measure of classifier incongruence | http://arxiv.org/pdf/1604.04451v1.pdf | author:Josef Kittler, Cemre Zor category:cs.LG cs.IT math.IT stat.ML published:2016-04-15 summary:Disagreement between two classifiers regarding the class membership of anobservation in pattern recognition can be indicative of an anomaly and itsnuance. As in general classifiers base their decision on class aposterioriprobabilities, the most natural approach to detecting classifier incongruenceis to use divergence. However, existing divergences are not particularlysuitable to gauge classifier incongruence. In this paper, we postulate theproperties that a divergence measure should satisfy and propose a noveldivergence measure, referred to as Delta divergence. In contrast to existingmeasures, it is decision cognizant. The focus in Delta divergence on thedominant hypotheses has a clutter reducing property, the significance of whichgrows with increasing number of classes. The proposed measure satisfies otherimportant properties such as symmetry, and independence of classifierconfidence. The relationship of the proposed divergence to some baselinemeasures is demonstrated experimentally, showing its superiority.
arxiv-17400-265 | Bayesian linear regression with Student-t assumptions | http://arxiv.org/pdf/1604.04434v1.pdf | author:Chaobing Song, Shu-Tao Xia category:cs.LG stat.ML published:2016-04-15 summary:As an automatic method of determining model complexity using the trainingdata alone, Bayesian linear regression provides us a principled way to selecthyperparameters. But one often needs approximation inference if distributionassumption is beyond Gaussian distribution. In this paper, we propose aBayesian linear regression model with Student-t assumptions (BLRS), which canbe inferred exactly. In this framework, both conjugate prior and expectationmaximization (EM) algorithm are generalized. Meanwhile, we prove that themaximum likelihood solution is equivalent to the standard Bayesian linearregression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS isnearly identical to the EM algorithm for BLRG. It is showed that $q$-EM forBLRS can converge faster than EM for BLRG for the task of predicting onlinenews popularity.
arxiv-17400-266 | The Artificial Mind's Eye: Resisting Adversarials for Convolutional Neural Networks using Internal Projection | http://arxiv.org/pdf/1604.04428v1.pdf | author:Harm Berntsen, Wouter Kuijper, Tom Heskes category:cs.LG cs.NE published:2016-04-15 summary:We introduce a novel type of artificial neural network structure and trainingprocedure that results in networks that are provably, quantitatively morerobust to adversarial samples than classical, end-to-end trained classifiers.The main idea of our approach is to force the network to make predictions onwhat the given instance of the class under consideration would look like andsubsequently test those predictions. By forcing the network to redraw therelevant parts of the image and subsequently comparing this new image to theoriginal, we are having the network give a 'proof' of the presence of theobject.
arxiv-17400-267 | Co-Localization of Audio Sources in Images Using Binaural Features and Locally-Linear Regression | http://arxiv.org/pdf/1408.2700v4.pdf | author:Antoine Deleforge, Radu Horaud, Yoav Schechner, Laurent Girin category:cs.SD cs.MM stat.AP stat.ML published:2014-08-12 summary:This paper addresses the problem of localizing audio sources using binauralmeasurements. We propose a supervised formulation that simultaneously localizesmultiple sources at different locations. The approach is intrinsicallyefficient because, contrary to prior work, it relies neither on sourceseparation, nor on monaural segregation. The method starts with a trainingstage that establishes a locally-linear Gaussian regression model between thedirectional coordinates of all the sources and the auditory features extractedfrom binaural measurements. While fixed-length wide-spectrum sounds (whitenoise) are used for training to reliably estimate the model parameters, we showthat the testing (localization) can be extended to variable-lengthsparse-spectrum sounds (such as speech), thus enabling a wide range ofrealistic applications. Indeed, we demonstrate that the method can be used foraudio-visual fusion, namely to map speech signals onto images and hence tospatially align the audio and visual modalities, thus enabling to discriminatebetween speaking and non-speaking faces. We release a novel corpus of real-roomrecordings that allow quantitative evaluation of the co-localization method inthe presence of one or two sound sources. Experiments demonstrate increasedaccuracy and speed relative to several state-of-the-art methods.
arxiv-17400-268 | Robust Head-Pose Estimation Based on Partially-Latent Mixture of Linear Regression | http://arxiv.org/pdf/1603.09732v2.pdf | author:Vincent Drouard, Radu Horaud, Antoine Deleforge, Silèye Ba, Georgios Evangelidis category:cs.CV published:2016-03-31 summary:Head-pose estimation has many applications, such as social-event analysis,human-robot and human-computer interaction, driving assistance, and so forth.Head-pose estimation is challenging because it must cope with changingillumination conditions, face orientation and appearance variabilities, partialocclusions of facial landmarks, as well as bounding-box-to-face alignmentproblems. We propose a mixture of linear regression method that learns how tomap high-dimensional feature vectors (extracted from bounding-boxes of faces)onto both head-pose parameters and bounding-box shifts, such that at runtimethey are simultaneously predicted. We describe in detail the mapping methodthat combines the merits of manifold learning and of mixture of linearregression. We validate our method with three publicly available datasets andwe thoroughly benchmark four variants of the proposed algorithm with severalstate-of-the-art head-pose estimation methods.
arxiv-17400-269 | Estimating parameters of nonlinear systems using the elitist particle filter based on evolutionary strategies | http://arxiv.org/pdf/1604.04198v2.pdf | author:Christian Huemmer, Christian Hofmann, Roland Maas, Walter Kellermann category:stat.ML published:2016-04-14 summary:In this article, we present the elitist particle filter based on evolutionarystrategies (EPFES) as an efficient approach for nonlinear systemidentification. The EPFES is derived from the frequently-employed state-spacemodel, where the relevant information of the nonlinear system is captured by anunknown state vector. Similar to classical particle filtering, the EPFESconsists of a set of particles and respective weights which represent differentrealizations of the latent state vector and their likelihood of being thesolution of the optimization problem. As main innovation, the EPFES includes anevolutionary elitist-particle selection which combines long-term information(based on recursively-calculated particle weights) with instantaneous samplingfrom an approximated continuous posterior distribution. In this article, theEPFES is shown to be a generalization of the widely-used Gaussian particlefilter and thus evaluated with respect to the latter for two completelydifferent scenarios: First, we consider the so-called univariate nonstationarygrowth model with time-variant latent state variable, where the evolutionaryselection of elitist particles is evaluated for non-recursively calculatedparticle weights. Second, the problem of nonlinear acoustic echo cancellationis addressed in a simulated scenario with speech as input signal: By usinglong-term fitness measures, we highlight the efficacy of the well-generalizingEPFES in estimating the nonlinear system even for large search spaces. Finally,we illustrate similarities between the EPFES and evolutionary algorithms tooutline future improvements by fusing the achievements of both fields ofresearch.
arxiv-17400-270 | Automatically Proving Mathematical Theorems with Evolutionary Algorithms and Proof Assistants | http://arxiv.org/pdf/1602.07455v2.pdf | author:Li-An Yang, Jui-Pin Liu, Chao-Hong Chen, Ying-ping Chen category:cs.NE cs.LO published:2016-02-24 summary:Mathematical theorems are human knowledge able to be accumulated in the formof symbolic representation, and proving theorems has been consideredintelligent behavior. Based on the BHK interpretation and the Curry-Howardisomorphism, proof assistants, software capable of interacting with human forconstructing formal proofs, have been developed in the past several decades.Since proofs can be considered and expressed as programs, proof assistantssimplify and verify a proof by computationally evaluating the programcorresponding to the proof. Thanks to the transformation from logic tocomputation, it is now possible to generate or search for formal proofsdirectly in the realm of computation. Evolutionary algorithms, known to beflexible and versatile, have been successfully applied to handle a variety ofscientific and engineering problems in numerous disciplines for also severaldecades. Examining the feasibility of establishing the link betweenevolutionary algorithms, as the program generator, and proof assistants, as theproof verifier, in order to automatically find formal proofs to a given logicsentence is the primary goal of this study. In the article, we describe indetail our first, ad-hoc attempt to fully automatically prove theorems as wellas the preliminary results. Ten simple theorems from various branches ofmathematics were proven, and most of these theorems cannot be proven by usingthe tactic auto alone in Coq, the adopted proof assistant. The implication andpotential influence of this study are discussed, and the developed source codewith the obtained experimental results are released as open source.
arxiv-17400-271 | Low-Rank Matrix Recovery using Gabidulin Codes in Characteristic Zero | http://arxiv.org/pdf/1604.04397v1.pdf | author:Sven Müelich, Sven Puchinger, Martin Bossert category:cs.IT cs.CV math.IT published:2016-04-15 summary:We present a new approach on low-rank matrix recovery (LRMR) based onGabidulin Codes. Since most applications of LRMR deal with matrices overinfinite fields, we use the recently introduced generalization of Gabidulincodes to fields of characterstic zero. We show that LRMR can be reduced todecoding of Gabidulin codes and discuss which field extensions can be used inthe code construction.
arxiv-17400-272 | Computationally Efficient Bayesian Learning of Gaussian Process State Space Models | http://arxiv.org/pdf/1506.02267v2.pdf | author:Andreas Svensson, Arno Solin, Simo Särkkä, Thomas B. Schön category:stat.CO stat.ML published:2015-06-07 summary:Gaussian processes allow for flexible specification of prior assumptions ofunknown dynamics in state space models. We present a procedure for efficientBayesian learning in Gaussian process state space models, where therepresentation is formed by projecting the problem onto a set of approximateeigenfunctions derived from the prior covariance structure. Learning under thisfamily of models can be conducted using a carefully crafted particle MCMCalgorithm. This scheme is computationally efficient and yet allows for a fullyBayesian treatment of the problem. Compared to conventional systemidentification tools or existing learning methods, we show competitiveperformance and reliable quantification of uncertainties in the model.
arxiv-17400-273 | Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding | http://arxiv.org/pdf/1604.04383v1.pdf | author:Milos Cernak, Alexandros Lazaridis, Afsaneh Asaei, Philip N. Garner category:cs.SD cs.CL published:2016-04-15 summary:Most current very low bit rate (VLBR) speech coding systems use hidden Markovmodel (HMM) based speech recognition/synthesis techniques. This allowstransmission of information (such as phonemes) segment by segment thatdecreases the bit rate. However, the encoder based on a phoneme speechrecognition may create bursts of segmental errors. Segmental errors are furtherpropagated to optional suprasegmental (such as syllable) information coding.Together with the errors of voicing detection in pitch parametrization,HMM-based speech coding creates speech discontinuities and unnatural speechsound artefacts. In this paper, we propose a novel VLBR speech coding framework based onneural networks (NNs) for end-to-end speech analysis and synthesis withoutHMMs. The speech coding framework relies on phonological (sub-phonetic)representation of speech, and it is designed as a composition of deep andspiking NNs: a bank of phonological analysers at the transmitter, and aphonological synthesizer at the receiver, both realised as deep NNs, and aspiking NN as an incremental and robust encoder of syllable boundaries forcoding of continuous fundamental frequency (F0). A combination of phonologicalfeatures defines much more sound patterns than phonetic features defined byHMM-based speech coders, and the finer analysis/synthesis code contributes intosmoother encoded speech. Listeners significantly prefer the NN-based approachdue to fewer discontinuities and speech artefacts of the encoded speech. Asingle forward pass is required during the speech encoding and decoding. Theproposed VLBR speech coding operates at bit rate about 360 bits/sec.
arxiv-17400-274 | Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks | http://arxiv.org/pdf/1604.04382v1.pdf | author:Chuan Li, Michael Wand category:cs.CV published:2016-04-15 summary:This paper proposes Markovian Generative Adversarial Networks (MGANs), amethod for training generative neural networks for efficient texture synthesis.While deep neural network approaches have recently demonstrated remarkableresults in terms of synthesis quality, they still come at considerablecomputational costs (minutes of run-time for low-res images). Our paperaddresses this efficiency issue. Instead of a numerical deconvolution inprevious work, we precompute a feed-forward, strided convolutional network thatcaptures the feature statistics of Markovian patches and is able to directlygenerate outputs of arbitrary dimensions. Such network can directly decodebrown noise to realistic texture, or photos to artistic paintings. Withadversarial training, we obtain quality comparable to recent neural texturesynthesis methods. As no optimization is required any longer at generationtime, our run-time performance (0.25M pixel images at 25Hz) surpasses previousneural texture synthesizers by a significant margin (at least 500 timesfaster). We apply this idea to texture synthesis, style transfer, and videostylization.
arxiv-17400-275 | Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN | http://arxiv.org/pdf/1604.04378v1.pdf | author:Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, Xueqi Cheng category:cs.CL cs.AI cs.LG cs.NE published:2016-04-15 summary:Semantic matching, which aims to determine the matching degree between twotexts, is a fundamental problem for many NLP applications. Recently, deeplearning approach has been applied to this problem and significant improvementshave been achieved. In this paper, we propose to view the generation of theglobal interaction between two texts as a recursive process: i.e. theinteraction of two texts at each position is a composition of the interactionsbetween their prefixes as well as the word level interaction at the currentposition. Based on this idea, we propose a novel deep architecture, namelyMatch-SRNN, to model the recursive matching structure. Firstly, a tensor isconstructed to capture the word level interactions. Then a spatial RNN isapplied to integrate the local interactions recursively, with importancedetermined by four types of gates. Finally, the matching score is calculatedbased on the global interaction. We show that, after degenerated to the exactmatching scenario, Match-SRNN can approximate the dynamic programming processof longest common subsequence. Thus, there exists a clear interpretation forMatch-SRNN. Our experiments on two semantic matching tasks showed theeffectiveness of Match-SRNN, and its ability of visualizing the learnedmatching structure.
arxiv-17400-276 | DARI: Distance metric And Representation Integration for Person Verification | http://arxiv.org/pdf/1604.04377v1.pdf | author:Guangrun Wang, Liang Lin, Shengyong Ding, Ya Li, Qing Wang category:cs.CV published:2016-04-15 summary:The past decade has witnessed the rapid development of feature representationlearning and distance metric learning, whereas the two steps are oftendiscussed separately. To explore their interaction, this work proposes anend-to-end learning framework called DARI, i.e. Distance metric AndRepresentation Integration, and validates the effectiveness of DARI in thechallenging task of person verification. Given the training images annotatedwith the labels, we first produce a large number of triplet units, and each onecontains three images, i.e. one person and the matched/mismatch references. Foreach triplet unit, the distance disparity between the matched pair and themismatched pair tends to be maximized. We solve this objective by building adeep architecture of convolutional neural networks. In particular, theMahalanobis distance matrix is naturally factorized as one top fully-connectedlayer that is seamlessly integrated with other bottom layers representing theimage feature. The image feature and the distance metric can be thussimultaneously optimized via the one-shot backward propagation. On severalpublic datasets, DARI shows very promising performance on re-identifyingindividuals cross cameras against various challenges, and outperforms otherstate-of-the-art approaches.
arxiv-17400-277 | The Chow Form of the Essential Variety in Computer Vision | http://arxiv.org/pdf/1604.04372v1.pdf | author:Gunnar Fløystad, Joe Kileel, Giorgio Ottaviani category:math.AG cs.CV math.AC published:2016-04-15 summary:The Chow form of the essential variety in computer vision is calculated. Ourderivation uses secant varieties, Ulrich sheaves and representation theory.Numerical experiments show that our formula can detect noisy pointcorrespondences between two images.
arxiv-17400-278 | StalemateBreaker: A Proactive Content-Introducing Approach to Automatic Human-Computer Conversation | http://arxiv.org/pdf/1604.04358v1.pdf | author:Xiang Li, Lili Mou, Rui Yan, Ming Zhang category:cs.CL cs.AI cs.IR published:2016-04-15 summary:Existing open-domain human-computer conversation systems are typicallypassive: they either synthesize or retrieve a reply provided a human-issuedutterance. It is generally presumed that humans should take the role to leadthe conversation and introduce new content when a stalemate occurs, and thatthe computer only needs to "respond." In this paper, we proposeStalemateBreaker, a conversation system that can proactively introduce newcontent when appropriate. We design a pipeline to determine when, what, and howto introduce new content during human-computer conversation. We further proposea novel reranking algorithm Bi-PageRank-HITS to enable rich interaction betweenconversation context and candidate replies. Experiments show that both thecontent-introducing approach and the reranking algorithm are effective. Ourfull StalemateBreaker model outperforms a state-of-the-practice conversationsystem by +14.4% p@1 when a stalemate occurs.
arxiv-17400-279 | Recovering Structured Probability Matrices | http://arxiv.org/pdf/1602.06586v4.pdf | author:Qingqing Huang, Sham M. Kakade, Weihao Kong, Gregory Valiant category:cs.LG published:2016-02-21 summary:We consider the problem of accurately recovering a matrix B of size M by M ,which represents a probability distribution over M2 outcomes, given access toan observed matrix of "counts" generated by taking independent samples from thedistribution B. How can structural properties of the underlying matrix B beleveraged to yield computationally efficient and information theoreticallyoptimal reconstruction algorithms? When can accurate reconstruction beaccomplished in the sparse data regime? This basic problem lies at the core ofa number of questions that are currently being considered by differentcommunities, including building recommendation systems and collaborativefiltering in the sparse data regime, community detection in sparse randomgraphs, learning structured models such as topic models or hidden Markovmodels, and the efforts from the natural language processing community tocompute "word embeddings". Our results apply to the setting where B has a low rank structure. For thissetting, we propose an efficient algorithm that accurately recovers theunderlying M by M matrix using Theta(M) samples. This result easily translatesto Theta(M) sample algorithms for learning topic models and learning hiddenMarkov Models. These linear sample complexities are optimal, up to constantfactors, in an extremely strong sense: even testing basic properties of theunderlying matrix (such as whether it has rank 1 or 2) requires Omega(M)samples. We provide an even stronger lower bound where distinguishing whether asequence of observations were drawn from the uniform distribution over Mobservations versus being generated by an HMM with two hidden states requiresOmega(M) observations. This precludes sublinear-sample hypothesis tests forbasic properties, such as identity or uniformity, as well as sublinear sampleestimators for quantities such as the entropy rate of HMMs.
arxiv-17400-280 | Positive Definite Estimation of Large Covariance Matrix Using Generalized Nonconvex Penalties | http://arxiv.org/pdf/1604.04348v1.pdf | author:Fei Wen, Yuan Yang, Peilin Liu, Robert C. Qiu category:cs.IT cs.LG math.IT stat.ML published:2016-04-15 summary:This work addresses the issue of large covariance matrix estimation inhigh-dimensional statistical analysis. Recently, improved iterative algorithmswith positive-definite guarantee have been developed. However, these algorithmscannot be directly extended to use a nonconvex penalty for sparsity inducing.Generally, a nonconvex penalty has the capability of ameliorating the biasproblem of the popular convex lasso penalty, and thus is more advantageous. Inthis work, we propose a class of positive-definite covariance estimators usinggeneralized nonconvex penalties. We develop a first-order algorithm based onthe alternating direction method (ADM) framework to solve the nonconvexoptimization problem efficiently. The convergence of the proposed algorithm hasbeen proved. Further, the statistical properties of the new estimators havebeen analyzed for generalized nonconvex penalties. Moreover, extension of theproposed algorithm to covariance estimation from sketched measurements has beenconsidered. The performances of the proposed estimators have been demonstratedby both a simulation study and a gene clustering example for tumor tissues.
arxiv-17400-281 | High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks | http://arxiv.org/pdf/1604.04339v1.pdf | author:Zifeng Wu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2016-04-15 summary:We propose a method for high-performance semantic image segmentation (orsemantic pixel labelling) based on very deep residual networks, which achievesthe state-of-the-art performance. A few design factors are carefully consideredto this end. We make the following contributions. (i) First, we evaluate differentvariations of a fully convolutional residual network so as to find the bestconfiguration, including the number of layers, the resolution of feature maps,and the size of field-of-view. Our experiments show that further enlarging thefield-of-view and increasing the resolution of feature maps are typicallybeneficial, which however inevitably leads to a higher demand for GPU memories.To walk around the limitation, we propose a new method to simulate a highresolution network with a low resolution network, which can be applied duringtraining and/or testing. (ii) Second, we propose an online bootstrapping methodfor training. We demonstrate that online bootstrapping is critically importantfor achieving good accuracy. (iii) Third we apply the traditional dropout tosome of the residual blocks, which further improves the performance. (iv)Finally, our method achieves the currently best mean intersection-over-union78.3\% on the PASCAL VOC 2012 dataset, as well as on the recent datasetCityscapes.
arxiv-17400-282 | Facial expression recognition based on local region specific features and support vector machines | http://arxiv.org/pdf/1604.04337v1.pdf | author:Deepak Ghimire, Sunghwan Jeong, Joonwhoan Lee, Sang Hyun Park category:cs.CV published:2016-04-15 summary:Facial expressions are one of the most powerful, natural and immediate meansfor human being to communicate their emotions and intensions. Recognition offacial expression has many applications including human-computer interaction,cognitive science, human emotion analysis, personality development etc. In thispaper, we propose a new method for the recognition of facial expressions fromsingle image frame that uses combination of appearance and geometric featureswith support vector machines classification. In general, appearance featuresfor the recognition of facial expressions are computed by dividing face regioninto regular grid (holistic representation). But, in this paper we extractedregion specific appearance features by dividing the whole face region intodomain specific local regions. Geometric features are also extracted fromcorresponding domain specific regions. In addition, important local regions aredetermined by using incremental search approach which results in the reductionof feature dimension and improvement in recognition accuracy. The results offacial expressions recognition using features from domain specific regions arealso compared with the results obtained using holistic representation. Theperformance of the proposed facial expression recognition system has beenvalidated on publicly available extended Cohn-Kanade (CK+) facial expressiondata sets.
arxiv-17400-283 | Recognition of facial expressions based on salient geometric features and support vector machines | http://arxiv.org/pdf/1604.04334v1.pdf | author:Deepak Ghimire, Joonwhoan Lee, Ze-Nian Li, Sunghwan Jeong category:cs.CV published:2016-04-15 summary:Facial expressions convey nonverbal cues which play an important role ininterpersonal relations, and are widely used in behavior interpretation ofemotions, cognitive science, and social interactions. In this paper we analyzedifferent ways of representing geometric feature and present a fully automaticfacial expression recognition (FER) system using salient geometric features. Ingeometric feature-based FER approach, the first important step is to initializeand track dense set of facial points as the expression evolves over time inconsecutive frames. In the proposed system, facial points are initialized usingelastic bunch graph matching (EBGM) algorithm and tracking is performed usingKanade-Lucas-Tomaci (KLT) tracker. We extract geometric features from point,line and triangle composed of tracking results of facial points. The mostdiscriminative line and triangle features are extracted using feature selectivemulti-class AdaBoost with the help of extreme learning machine (ELM)classification. Finally the geometric features for FER are extracted from theboosted line, and triangles composed of facial points. The recognition accuracyusing features from point, line and triangle are analyzed independently. Theperformance of the proposed FER system is evaluated on three different datasets: namely CK+, MMI and MUG facial expression data sets.
arxiv-17400-284 | Latent Model Ensemble with Auto-localization | http://arxiv.org/pdf/1604.04333v1.pdf | author:Miao Sun, Tony X. Han, Xun Xu category:cs.CV published:2016-04-15 summary:Deep Convolutional Neural Networks (CNN) have exhibited superior performancein many visual recognition tasks including image classification, objectdetection, and scene label- ing, due to their large learning capacity andresistance to overfit. For the image classification task, most of the currentdeep CNN- based approaches take the whole size-normalized image as input andhave achieved quite promising results. Compared with the previously dominatingapproaches based on feature extraction, pooling, and classification, the deepCNN-based approaches mainly rely on the learning capability of deep CNN toachieve superior results: the burden of minimizing intra-class variation whilemaximizing inter-class difference is entirely dependent on the implicit featurelearning component of deep CNN; we rely upon the implicitly learned filters andpooling component to select the discriminative regions, which correspond to theactivated neurons. However, if the irrelevant regions constitute a largeportion of the image of interest, the classification performance of the deepCNN, which takes the whole image as input, can be heavily affected. To solvethis issue, we propose a novel latent CNN framework, which treats the mostdiscriminate region as a latent variable. We can jointly learn the global CNNwith the latent CNN to avoid the aforementioned big irrelevant region issue,and our experimental results show the evident advantage of the proposed latentCNN over traditional deep CNN: latent CNN outperforms the state-of-the-artperformance of deep CNN on standard benchmark datasets including the CIFAR-10,CIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.
arxiv-17400-285 | Improving the Robustness of Deep Neural Networks via Stability Training | http://arxiv.org/pdf/1604.04326v1.pdf | author:Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow category:cs.CV cs.LG published:2016-04-15 summary:In this paper we address the issue of output instability of deep neuralnetworks: small perturbations in the visual input can significantly distort thefeature embeddings and output of a neural network. Such instability affectsmany deep architectures with state-of-the-art performance on a wide range ofcomputer vision tasks. We present a general stability training method tostabilize deep networks against small input distortions that result fromvarious types of common image processing, such as compression, rescaling, andcropping. We validate our method by stabilizing the state-of-the-art Inceptionarchitecture against these types of distortions. In addition, we demonstratethat our stabilized model gives robust state-of-the-art performance onlarge-scale near-duplicate detection, similar-image ranking, and classificationon noisy datasets.
arxiv-17400-286 | LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement | http://arxiv.org/pdf/1511.03995v3.pdf | author:Kin Gwn Lore, Adedotun Akintayo, Soumik Sarkar category:cs.CV published:2015-11-12 summary:In surveillance, monitoring and tactical reconnaissance, gathering the rightvisual information from a dynamic environment and accurately processing suchdata are essential ingredients to making informed decisions which determinesthe success of an operation. Camera sensors are often cost-limited in abilityto clearly capture objects without defects from images or videos taken in apoorly-lit environment. The goal in many applications is to enhance thebrightness, contrast and reduce noise content of such images in an on-boardreal-time manner. We propose a deep autoencoder-based approach to identifysignal features from low-light images handcrafting and adaptively brightenimages without over-amplifying the lighter parts in images (i.e., withoutsaturation of image pixels) in high dynamic range. We show that a variant ofthe recently proposed stacked-sparse denoising autoencoder can learn toadaptively enhance and denoise from synthetically darkened and noisy trainingexamples. The network can then be successfully applied to naturally low-lightenvironment and/or hardware degraded images. Results show significantcredibility of deep learning based approaches both visually and by quantitativecomparison with various popular enhancing, state-of-the-art denoising andhybrid enhancing-denoising techniques.
arxiv-17400-287 | Signal Processing on Graphs: Causal Modeling of Big Data | http://arxiv.org/pdf/1503.00173v2.pdf | author:Jonathan Mei, José M. F. Moura category:cs.IT math.IT stat.ML published:2015-02-28 summary:Often, Big Data applications collect a large number of time series, forexample, the financial data of companies quoted in a stock exchange, the healthcare data of all patients that visit the emergency room of a hospital, or thetemperature sequences continuously measured by weather stations across the US.A first task in the analytics of these data is to derive a low dimensionalrepresentation, a graph or discrete manifold, that describes well theinterrelations among the time series and their intrarelations across time. Thispaper presents a computationally tractable algorithm for estimating this graphstructure from the available data. This graph is directed and weighted,possibly representing causal relations, not just reciprocal correlations as inmany existing approaches in the literature. A detailed convergence analysis iscarried out. The algorithm is demonstrated on random graph and real networktime series datasets, and its performance is compared to that of relatedmethods. The adjacency matrices estimated with the new method are close to thetrue graph in the simulated data and consistent with prior physical knowledgein the real dataset tested.
arxiv-17400-288 | Unsupervised Nonlinear Spectral Unmixing based on a Multilinear Mixing Model | http://arxiv.org/pdf/1604.04293v1.pdf | author:Qi Wei, Marcus Chen, Jean-Yves Tourneret, Simon Godsill category:cs.CV published:2016-04-14 summary:In the community of remote sensing, nonlinear mixing models have recentlyreceived particular attention in hyperspectral image processing. In this paper,we present a novel nonlinear spectral unmixing method following the recentmultilinear mixing model of [1], which includes an infinite number of termsrelated to interactions between different endmembers. The proposed unmixingmethod is unsupervised in the sense that the endmembers are estimated jointlywith the abundances and other parameters of interest, i.e., the transitionprobability of undergoing further interactions. Non-negativity and sum-to oneconstraints are imposed on abundances while only nonnegativity is consideredfor endmembers. The resulting unmixing problem is formulated as a constrainednonlinear optimization problem, which is solved by a block coordinate descentstrategy, consisting of updating the endmembers, abundances and transitionprobability iteratively. The proposed method is evaluated and compared withlinear unmixing methods for synthetic and real hyperspectral datasets acquiredby the AVIRIS sensor. The advantage of using non-linear unmixing as opposed tolinear unmixing is clearly shown in these examples.
arxiv-17400-289 | Clustering Financial Time Series: How Long is Enough? | http://arxiv.org/pdf/1603.04017v2.pdf | author:Gautier Marti, Sébastien Andler, Frank Nielsen, Philippe Donnat category:stat.ML q-fin.ST published:2016-03-13 summary:Researchers have used from 30 days to several years of daily returns assource data for clustering financial time series based on their correlations.This paper sets up a statistical framework to study the validity of suchpractices. We first show that clustering correlated random variables from theirobserved values is statistically consistent. Then, we also give a firstempirical answer to the much debated question: How long should the time seriesbe? If too short, the clusters found can be spurious; if too long, dynamics canbe smoothed out.
arxiv-17400-290 | Learning Visual Storylines with Skipping Recurrent Neural Networks | http://arxiv.org/pdf/1604.04279v1.pdf | author:Gunnar A. Sigurdsson, Xinlei Chen, Abhinav Gupta category:cs.CV published:2016-04-14 summary:What does a typical visit to Paris look like? Do people first take photos ofthe Louvre and then the Eiffel Tower? Can we visually model a temporal eventlike "Paris Vacation" using current frameworks? In this paper, we explore howwe can automatically learn the temporal aspects, or storylines of visualconcepts from web data. Previous attempts focus on consecutive image-to-imagetransitions and are unsuccessful at recovering the long-term underlying story.Our novel Skipping Recurrent Neural Network (S-RNN) model, does not attempt topredict each and every data point in the sequence, like classic RNNs. Rather,S-RNN uses a framework that skips through the images in the photo stream toexplore the space of all ordered subsets of the albums via an efficientsampling procedure. This approach reduces the negative impact of strongshort-term correlations, and recovers the latent story more accurately. We showhow our learned storylines can be used to analyze, predict, and summarize photoalbums from Flickr. Our experimental results provide strong qualitative andquantitative evidence that S-RNN is significantly better than other candidatemethods such as LSTMs on learning long-term correlations and recovering latentstorylines. Moreover, we show how storylines can help machines betterunderstand and summarize photo streams by inferring a brief personalized storyof each individual album.
arxiv-17400-291 | Slow and steady feature analysis: higher order temporal coherence in video | http://arxiv.org/pdf/1506.04714v2.pdf | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV published:2015-06-15 summary:How can unlabeled video augment visual learning? Existing methods perform"slow" feature analysis, encouraging the representations of temporally closeframes to exhibit only small differences. While this standard approach capturesthe fact that high-level visual signals change slowly over time, it fails tocapture *how* the visual content changes. We propose to generalize slow featureanalysis to "steady" feature analysis. The key idea is to impose a prior thathigher order derivatives in the learned feature space must be small. To thisend, we train a convolutional neural network with a regularizer on tuples ofsequential frames from unlabeled video. It encourages feature changes over timeto be smooth, i.e., similar to the most recent changes. Using five diversedatasets, including unlabeled YouTube and KITTI videos, we demonstrate ourmethod's impact on object, scene, and action recognition tasks. We further showthat our features learned from unlabeled video can even surpass a standardheavily supervised pretraining approach.
arxiv-17400-292 | Measuring and Predicting Tag Importance for Image Retrieval | http://arxiv.org/pdf/1602.08680v2.pdf | author:Shangwen Li, Sanjay Purushotham, Chen Chen, Yuzhuo Ren, C. -C. Jay Kuo category:cs.CV published:2016-02-28 summary:Textual data such as tags, sentence descriptions are combined with visualcues to reduce the semantic gap for image retrieval applications in today'sMultimodal Image Retrieval (MIR) systems. However, all tags are treated asequally important in these systems, which may result in misalignment betweenvisual and textual modalities during MIR training. This will further lead todegenerated retrieval performance at query time. To address this issue, weinvestigate the problem of tag importance prediction, where the goal is toautomatically predict the tag importance and use it in image retrieval. Toachieve this, we first propose a method to measure the relative importance ofobject and scene tags from image sentence descriptions. Using this as theground truth, we present a tag importance prediction model by exploiting jointvisual, semantic and context cues. The Structural Support Vector Machine (SSVM)formulation is adopted to ensure efficient training of the prediction model.Then, the Canonical Correlation Analysis (CCA) is employed to learn therelation between the image visual feature and tag importance to obtain robustretrieval performance. Experimental results on three real-world datasets show asignificant performance improvement of the proposed MIR with Tag ImportancePrediction (MIR/TIP) system over other MIR systems.
arxiv-17400-293 | Modeling Electrical Daily Demand in Presence of PHEVs in Smart Grids with Supervised Learning | http://arxiv.org/pdf/1604.04213v1.pdf | author:Marco Pellegrini, Farshad Rassaei category:cs.LG published:2016-04-14 summary:Replacing a portion of current light duty vehicles (LDV) with plug-in hybridelectric vehicles (PHEVs) offers the possibility to reduce the dependence onpetroleum fuels together with environmental and economic benefits. The chargingactivity of PHEVs will certainly introduce new load to the power grid. In theframework of the development of a smarter grid, the primary focus of thepresent study is to propose a model for the electrical daily demand in presenceof PHEVs charging. Expected PHEV demand is modeled by the PHEV charging timeand the starting time of charge according to real world data. A normaldistribution for starting time of charge is assumed. Several distributions forcharging time are considered: uniform distribution, Gaussian with positivesupport, Rician distribution and a non-uniform distribution coming from drivingpatterns in real-world data. We generate daily demand profiles by usingreal-world residential profiles throughout 2014 in the presence of differentexpected PHEV demand models. Support vector machines (SVMs), a set ofsupervised machine learning models, are employed in order to find the bestmodel to fit the data. SVMs with radial basis function (RBF) and polynomialkernels were tested. Model performances are evaluated by means of mean squarederror (MSE) and mean absolute percentage error (MAPE). Best results areobtained with RBF kernel: maximum (worst) values for MSE and MAPE were about2.89 10-8 and 0.023, respectively.
arxiv-17400-294 | Neural Implementation of Probabilistic Models of Cognition | http://arxiv.org/pdf/1501.03209v2.pdf | author:Milad Kharratzadeh, Thomas R. Shultz category:cs.NE q-bio.NC published:2015-01-13 summary:Bayesian models of cognition hypothesize that human brains make sense of databy representing probability distributions and applying Bayes' rule to find thebest explanation for available data. Understanding the neural mechanismsunderlying probabilistic models remains important because Bayesian modelsprovide a computational framework, rather than specifying mechanisticprocesses. Here, we propose a deterministic neural-network model whichestimates and represents probability distributions from observable events --- aphenomenon related to the concept of probability matching. Our model learns torepresent probabilities without receiving any representation of them from theexternal world, but rather by experiencing the occurrence patterns ofindividual events. Our neural implementation of probability matching is pairedwith a neural module applying Bayes' rule, forming a comprehensive neuralscheme to simulate human Bayesian learning and inference. Our model alsoprovides novel explanations of base-rate neglect, a notable deviation fromBayes.
arxiv-17400-295 | 1-bit Matrix Completion: PAC-Bayesian Analysis of a Variational Approximation | http://arxiv.org/pdf/1604.04191v1.pdf | author:Vincent Cottet, Pierre Alquier category:stat.ML published:2016-04-14 summary:Due to challenging applications such as collaborative filtering, the matrixcompletion problem has been widely studied in the past few years. Differentapproaches rely on different structure assumptions on the matrix in hand. Here,we focus on the completion of a (possibly) low-rank matrix with binary entries,the so-called 1-bit matrix completion problem. Our approach relies on toolsfrom machine learning theory: empirical risk minimization and its convexrelaxations. We propose an algorithm to compute a variational approximation ofthe pseudo-posterior. Thanks to the convex relaxation, the correspondingminimization problem is bi-convex, and thus the method behaves well inpractice. We also study the performance of this variational approximationthrough PAC-Bayesian learning bounds. On the contrary to previous works thatfocused on upper bounds on the estimation error of M with various matrix norms,we are able to derive from this analysis a PAC bound on the prediction error ofour algorithm. We focus essentially on convex relaxation through the hinge loss, for whichwe present the complete analysis, a complete simulation study and a test on theMovieLens data set. However, we also discuss a variational approximation todeal with the logistic loss.
arxiv-17400-296 | Stochastic Neural Networks with Monotonic Activation Functions | http://arxiv.org/pdf/1601.00034v2.pdf | author:Siamak Ravanbakhsh, Barnabas Poczos, Jeff Schneider, Dale Schuurmans, Russell Greiner category:stat.ML cs.LG cs.NE published:2016-01-01 summary:We propose a Laplace approximation that creates a stochastic unit from anysmooth monotonic activation function, using only Gaussian noise. This paperinvestigates the application of this stochastic approximation in training afamily of Restricted Boltzmann Machines (RBM) that are closely linked toBregman divergences. This family, that we call exponential family RBM(Exp-RBM), is a subset of the exponential family Harmoniums that expressesfamily members through a choice of smooth monotonic non-linearity for eachneuron. Using contrastive divergence along with our Gaussian approximation, weshow that Exp-RBM can learn useful representations using novel stochasticunits.
arxiv-17400-297 | Consistently Estimating Markov Chains with Noisy Aggregate Data | http://arxiv.org/pdf/1604.04182v1.pdf | author:Garrett Bernstein, Daniel Sheldon category:cs.LG stat.ML published:2016-04-14 summary:We address the problem of estimating the parameters of a time-homogeneousMarkov chain given only noisy, aggregate data. This arises when a population ofindividuals behave independently according to a Markov chain, but individualsample paths cannot be observed due to limitations of the observation processor the need to protect privacy. Instead, only population-level counts of thenumber of individuals in each state at each time step are available. When thesecounts are exact, a conditional least squares (CLS) estimator is known to beconsistent and asymptotically normal. We initiate the study of method ofmoments estimators for this problem to handle the more realistic case whenobservations are additionally corrupted by noise. We show that CLS can beinterpreted as a simple "plug-in" method of moments estimator. However, whenobservations are noisy, it is not consistent because it fails to account foradditional variance introduced by the noise. We develop a new, simpler methodof moments estimator that bypasses this problem and is consistent under noisyobservations.
arxiv-17400-298 | Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images | http://arxiv.org/pdf/1512.02017v3.pdf | author:Aravindh Mahendran, Andrea Vedaldi category:cs.CV 68T45 published:2015-12-07 summary:Image representations, from SIFT and bag of visual words to ConvolutionalNeural Networks (CNNs) are a crucial component of almost all computer visionsystems. However, our understanding of them remains limited. In this paper westudy several landmark representations, both shallow and deep, by a number ofcomplementary visualization techniques. These visualizations are based on theconcept of "natural pre-image", namely a natural-looking image whoserepresentation has some notable property. We study in particular three suchvisualizations: inversion, in which the aim is to reconstruct an image from itsrepresentation, activation maximization, in which we search for patterns thatmaximally stimulate a representation component, and caricaturization, in whichthe visual patterns that a representation detects in an image are exaggerated.We pose these as a regularized energy-minimization framework and demonstrateits generality and effectiveness. In particular, we show that this method caninvert representations such as HOG more accurately than recent alternativeswhile being applicable to CNNs too. Among our findings, we show that severallayers in CNNs retain photographically accurate information about the image,with different degrees of geometric and photometric invariance.
arxiv-17400-299 | Distribution-Free Predictive Inference For Regression | http://arxiv.org/pdf/1604.04173v1.pdf | author:Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani, Larry Wasserman category:stat.ME math.ST stat.ML stat.TH published:2016-04-14 summary:We develop a general framework for distribution-free predictive inference inregression, using conformal inference. The proposed methodology allowsconstruction of prediction bands for the response variable using any estimatorof the regression function. The resulting prediction band preserves theconsistency properties of the original estimator under standard assumptions,while guaranteeing finite sample marginal coverage even when the assumptions donot hold. We analyze and compare, both empirically and theoretically, two majorvariants of our conformal procedure: the full conformal inference and splitconformal inference, along with a related jackknife method. These methods offerdifferent tradeoffs between statistical accuracy (length of resultingprediction intervals) and computational efficiency. As extensions, we develop amethod for constructing valid in-sample prediction intervals calledrank-one-out conformal inference, which has essentially the same computationalefficiency as split conformal inference. We also describe an extension of ourprocedures for producing prediction bands with varying local width, in order toadapt to heteroskedascity in the data distribution. Lastly, we propose amodel-free notion of variable importance, called leave-one-covariate-out orLOCO inference. Accompanying our paper is an R package conformalInference thatimplements all of the proposals we have introduced. In the spirit ofreproducibility, all empirical results in this paper can be easily(re)generated using this package.
arxiv-17400-300 | Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval | http://arxiv.org/pdf/1603.04595v2.pdf | author:Olivier Morère, Jie Lin, Antoine Veillard, Vijay Chandrasekhar, Tomaso Poggio category:cs.CV cs.IR published:2016-03-15 summary:The goal of this work is the computation of very compact binary hashes forimage instance retrieval. Our approach has two novel contributions. The firstone is Nested Invariance Pooling (NIP), a method inspired from i-theory, amathematical theory for computing group invariant transformations withfeed-forward neural networks. NIP is able to produce compact andwell-performing descriptors with visual representations extracted fromconvolutional neural networks. We specifically incorporate scale, translationand rotation invariances but the scheme can be extended to any arbitrary setsof transformations. We also show that using moments of increasing orderthroughout nesting is important. The NIP descriptors are then hashed to thetarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novelbatch-level regularization scheme specifically designed for the purpose ofhashing (RBMH). A thorough empirical evaluation with state-of-the-art showsthat the results obtained both with the NIP descriptors and the NIP+RBMH hashesare consistently outstanding across a wide range of datasets.
