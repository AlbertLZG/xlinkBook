arxiv-13200-1 | Sensor Selection by Linear Programming | http://arxiv.org/abs/1509.02954 | author:Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama category:stat.ML cs.LG published:2015-09-09 summary:We learn sensor trees from training data to minimize sensor acquisition costsduring test time. Our system adaptively selects sensors at each stage ifnecessary to make a confident classification. We pose the problem as empiricalrisk minimization over the choice of trees and node decision rules. Wedecompose the problem, which is known to be intractable, into combinatorial(tree structures) and continuous parts (node decision rules) and propose tosolve them separately. Using training data we greedily solve for thecombinatorial tree structures and for the continuous part, which is anon-convex multilinear objective function, we derive convex surrogate lossfunctions that are piecewise linear. The resulting problem can be cast as alinear program and has the advantage of guaranteed convergence, globaloptimality, repeatability and computational efficiency. We show that ourproposed approach outperforms the state-of-art on a number of benchmarkdatasets.
arxiv-13200-2 | Continuous control with deep reinforcement learning | http://arxiv.org/abs/1509.02971 | author:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra category:cs.LG stat.ML published:2015-09-09 summary:We adapt the ideas underlying the success of Deep Q-Learning to thecontinuous action domain. We present an actor-critic, model-free algorithmbased on the deterministic policy gradient that can operate over continuousaction spaces. Using the same learning algorithm, network architecture andhyper-parameters, our algorithm robustly solves more than 20 simulated physicstasks, including classic problems such as cartpole swing-up, dexterousmanipulation, legged locomotion and car driving. Our algorithm is able to findpolicies whose performance is competitive with those found by a planningalgorithm with full access to the dynamics of the domain and its derivatives.We further demonstrate that for many of the tasks the algorithm can learnpolicies end-to-end: directly from raw pixel inputs.
arxiv-13200-3 | Transfer learning approach for financial applications | http://arxiv.org/abs/1509.02807 | author:Cosmin Stamate, George D. Magoulas, Michael S. C. Thomas category:cs.NE published:2015-09-09 summary:Artificial neural networks learn how to solve new problems through acomputationally intense and time consuming process. One way to reduce theamount of time required is to inject preexisting knowledge into the network. Tomake use of past knowledge, we can take advantage of techniques that transferthe knowledge learned from one task, and reuse it on another (sometimesunrelated) task. In this paper we propose a novel selective breeding techniquethat extends the transfer learning with behavioural genetics approach proposedby Kohli, Magoulas and Thomas (2013), and evaluate its performance on financialdata. Numerical evidence demonstrates the credibility of the new approach. Weprovide insights on the operation of transfer learning and highlight thebenefits of using behavioural principles and selective breeding when tackling aset of diverse financial applications problems.
arxiv-13200-4 | Fast Second-Order Stochastic Backpropagation for Variational Inference | http://arxiv.org/abs/1509.02866 | author:Kai Fan, Ziteng Wang, Jeff Beck, James Kwok, Katherine Heller category:stat.ML published:2015-09-09 summary:We propose a second-order (Hessian or Hessian-free) based optimization methodfor variational inference inspired by Gaussian backpropagation, and argue thatquasi-Newton optimization can be developed as well. This is accomplished bygeneralizing the gradient computation in stochastic backpropagation via areparametrization trick with lower complexity. As an illustrative example, weapply this approach to the problems of Bayesian logistic regression andvariational auto-encoder (VAE). Additionally, we compute bounds on theestimator variance of intractable expectations for the family of Lipschitzcontinuous function. Our method is practical, scalable and model free. Wedemonstrate our method on several real-world datasets and provide comparisonswith other stochastic gradient methods to show substantial enhancement inconvergence rates.
arxiv-13200-5 | Semantic Image Segmentation via Deep Parsing Network | http://arxiv.org/abs/1509.02634 | author:Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV published:2015-09-09 summary:This paper addresses semantic image segmentation by incorporating richinformation into Markov Random Field (MRF), including high-order relations andmixture of label contexts. Unlike previous works that optimized MRFs usingiterative algorithm, we solve MRF by proposing a Convolutional Neural Network(CNN), namely Deep Parsing Network (DPN), which enables deterministicend-to-end computation in a single forward pass. Specifically, DPN extends acontemporary CNN architecture to model unary terms and additional layers arecarefully devised to approximate the mean field algorithm (MF) for pairwiseterms. It has several appealing properties. First, different from the recentworks that combined CNN and MRF, where many iterations of MF were required foreach training image during back-propagation, DPN is able to achieve highperformance by approximating one iteration of MF. Second, DPN representsvarious types of pairwise terms, making many existing works as its specialcases. Third, DPN makes MF easier to be parallelized and speeded up inGraphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC2012 dataset, where a single DPN model yields a new state-of-the-artsegmentation accuracy.
arxiv-13200-6 | Sélection de variables par le GLM-Lasso pour la prédiction du risque palustre | http://arxiv.org/abs/1509.02873 | author:Bienvenue Kouwayè, Noël Fonton, Fabrice Rossi category:stat.ML published:2015-09-09 summary:In this study, we propose an automatic learning method for variablesselection based on Lasso in epidemiology context. One of the aim of thisapproach is to overcome the pretreatment of experts in medicine andepidemiology on collected data. These pretreatment consist in recoding somevariables and to choose some interactions based on expertise. The approachproposed uses all available explanatory variables without treatment andgenerate automatically all interactions between them. This lead to highdimension. We use Lasso, one of the robust methods of variable selection inhigh dimension. To avoid over fitting a two levels cross-validation is used.Because the target variable is account variable and the lasso estimators arebiased, variables selected by lasso are debiased by a GLM and used to predictthe distribution of the main vector of malaria which is Anopheles. Results showthat only few climatic and environmental variables are the mains factorsassociated to the malaria risk exposure.
arxiv-13200-7 | Edge-enhancing Filters with Negative Weights | http://arxiv.org/abs/1509.02491 | author:Andrew Knyazev category:cs.CV cs.IT math.CO math.IT 68U10, 05C85 published:2015-09-08 summary:In [DOI:10.1109/ICMEW.2014.6890711], a graph-based denoising is performed byprojecting the noisy image to a lower dimensional Krylov subspace of the graphLaplacian, constructed using nonnegative weights determined by distancesbetween image data corresponding to image pixels. We~extend the construction ofthe graph Laplacian to the case, where some graph weights can be negative.Removing the positivity constraint provides a more accurate inference of agraph model behind the data, and thus can improve quality of filters forgraph-based signal processing, e.g., denoising, compared to the standardconstruction, without affecting the costs.
arxiv-13200-8 | Empirical risk minimization is consistent with the mean absolute percentage error | http://arxiv.org/abs/1509.02357 | author:Arnaud De Myttenaere, Bénédicte Le Grand, Fabrice Rossi category:stat.ML published:2015-09-08 summary:We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We show thatfinding the best model under the MAPE is equivalent to doing weighted MeanAbsolute Error (MAE) regression. We also show that, under some asumptions,universal consistency of Empirical Risk Minimization remains possible using theMAPE.
arxiv-13200-9 | DeepCough: A Deep Convolutional Neural Network in A Wearable Cough Detection System | http://arxiv.org/abs/1509.02512 | author:Justice Amoh, Kofi Odame category:cs.NE cs.LG published:2015-09-08 summary:In this paper, we present a system that employs a wearable acoustic sensorand a deep convolutional neural network for detecting coughs. We evaluate theperformance of our system on 14 healthy volunteers and compare it to that ofother cough detection systems that have been reported in the literature.Experimental results show that our system achieves a classification sensitivityof 95.1% and a specificity of 99.5%.
arxiv-13200-10 | Object Proposals for Text Extraction in the Wild | http://arxiv.org/abs/1509.02317 | author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV published:2015-09-08 summary:Object Proposals is a recent computer vision technique receiving increasinginterest from the research community. Its main objective is to generate arelatively small set of bounding box proposals that are most likely to containobjects of interest. The use of Object Proposals techniques in the scene textunderstanding field is innovative. Motivated by the success of powerful whileexpensive techniques to recognize words in a holistic way, Object Proposalstechniques emerge as an alternative to the traditional text detectors. In this paper we study to what extent the existing generic Object Proposalsmethods may be useful for scene text understanding. Also, we propose a newObject Proposals algorithm that is specifically designed for text and compareit with other generic methods in the state of the art. Experiments show thatour proposal is superior in its ability of producing good quality wordproposals in an efficient way. The source code of our method is made publiclyavailable.
arxiv-13200-11 | Deep Attributes from Context-Aware Regional Neural Codes | http://arxiv.org/abs/1509.02470 | author:Jianwei Luo, Jianguo Li, Jun Wang, Zhiguo Jiang, Yurong Chen category:cs.CV cs.LG cs.NE published:2015-09-08 summary:Recently, many researches employ middle-layer output of convolutional neuralnetwork models (CNN) as features for different visual recognition tasks.Although promising results have been achieved in some empirical studies, suchtype of representations still suffer from the well-known issue of semantic gap.This paper proposes so-called deep attribute framework to alleviate this issuefrom three aspects. First, we introduce object region proposals as intermediato represent target images, and extract features from region proposals. Second,we study aggregating features from different CNN layers for all regionproposals. The aggregation yields a holistic yet compact representation ofinput images. Results show that cross-region max-pooling of soft-max layeroutput outperform all other layers. As soft-max layer directly corresponds tosemantic concepts, this representation is named "deep attributes". Third, weobserve that only a small portion of generated regions by object proposalsalgorithm are correlated to classification target. Therefore, we introducecontext-aware region refining algorithm to pick out contextual regions andbuild context-aware classifiers. We apply the proposed deep attributes framework for various vision tasks.Extensive experiments are conducted on standard benchmarks for three visualrecognition tasks, i.e., image classification, fine-grained recognition andvisual instance retrieval. Results show that deep attribute approaches achievestate-of-the-art results, and outperforms existing peer methods with asignificant margin, even though some benchmarks have little overlap of conceptswith the pre-trained CNN models.
arxiv-13200-12 | Probabilistic Bag-Of-Hyperlinks Model for Entity Linking | http://arxiv.org/abs/1509.02301 | author:Octavian-Eugen Ganea, Marina Ganea, Aurelien Lucchi, Carsten Eickhoff, Thomas Hofmann category:cs.CL published:2015-09-08 summary:Many fundamental problems in natural language processing rely on determiningwhat entities appear in a given text. Commonly referenced as entity linking,this step is a fundamental component of many NLP tasks such as textunderstanding, automatic summarization, semantic search or machine translation.Name ambiguity, word polysemy, context dependencies and a heavy-taileddistribution of entities contribute to the complexity of this problem. We here propose a probabilistic approach that makes use of an effectivegraphical model to perform collective entity disambiguation. Input mentions(i.e.,~linkable token spans) are disambiguated jointly across an entiredocument by combining a document-level prior of entity co-occurrences withlocal information captured from mentions and their surrounding context. Themodel is based on simple sufficient statistics extracted from data, thusrelying on few parameters to be learned. Our method does not require extensive feature engineering, nor an expensivetraining procedure. We use loopy belief propagation to perform approximateinference. The low complexity of our model makes this step sufficiently fastfor real-time usage. We demonstrate the accuracy of our approach on a widerange of benchmark datasets, showing that it matches, and in many casesoutperforms, existing state-of-the-art methods.
arxiv-13200-13 | Improved Twitter Sentiment Prediction through Cluster-then-Predict Model | http://arxiv.org/abs/1509.02437 | author:Rishabh Soni, K. James Mathai category:cs.IR cs.CL cs.LG cs.SI published:2015-09-08 summary:Over the past decade humans have experienced exponential growth in the use ofonline resources, in particular social media and microblogging websites such asFacebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,etc. Many companies have identified these resources as a rich mine of marketingknowledge. This knowledge provides valuable feedback which allows them tofurther develop the next generation of their product. In this paper, sentimentanalysis of a product is performed by extracting tweets about that product andclassifying the tweets showing it as positive and negative sentiment. Theauthors propose a hybrid approach which combines unsupervised learning in theform of K-means clustering to cluster the tweets and then performing supervisedlearning methods such as Decision Trees and Support Vector Machines forclassification.
arxiv-13200-14 | Accelerated graph-based spectral polynomial filters | http://arxiv.org/abs/1509.02468 | author:Andrew Knyazev, Alexander Malyshev category:cs.CV published:2015-09-08 summary:Graph-based spectral denoising is a low-pass filtering using theeigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomialfiltering avoids costly computation of the eigendecomposition by projectionsonto suitable Krylov subspaces. Polynomial filters can be based, e.g., on thebilateral and guided filters. We propose constructing accelerated polynomialfilters by running flexible Krylov subspace based linear and eigenvalue solverssuch as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG)method.
arxiv-13200-15 | Evolving TSP heuristics using Multi Expression Programming | http://arxiv.org/abs/1509.02459 | author:Mihai Oltean, D. Dumitrescu category:cs.AI cs.NE published:2015-09-08 summary:Multi Expression Programming (MEP) is an evolutionary technique that may beused for solving computationally difficult problems. MEP uses a linear solutionrepresentation. Each MEP individual is a string encoding complex expressions(computer programs). A MEP individual may encode multiple solutions of thecurrent problem. In this paper MEP is used for evolving a Traveling SalesmanProblem (TSP) heuristic for graphs satisfying triangle inequality. Evolved MEPheuristic is compared with Nearest Neighbor Heuristic (NN) and Minimum SpanningTree Heuristic (MST) on some difficult problems in TSPLIB. For most of theconsidered problems the evolved MEP heuristic outperforms NN and MST. Theobtained algorithm was tested against some problems in TSPLIB. The resultsemphasizes that evolved MEP heuristic is a powerful tool for solving difficultTSP instances.
arxiv-13200-16 | Nonlinear functional mapping of the human brain | http://arxiv.org/abs/1510.03765 | author:Nicholas Allgaier, Tobias Banaschewski, Gareth Barker, Arun L. W. Bokde, Josh C. Bongard, Uli Bromberg, Christian Büchel, Anna Cattrell, Patricia J. Conrod, Christopher M. Danforth, Sylvane Desrivières, Peter S. Dodds, Herta Flor, Vincent Frouin, Jürgen Gallinat, Penny Gowland, Andreas Heinz, Bernd Ittermann, Scott Mackey, Jean-Luc Martinot, Kevin Murphy, Frauke Nees, Dimitri Papadopoulos-Orfanos, Luise Poustka, Michael N. Smolka, Henrik Walter, Robert Whelan, Gunter Schumann, Hugh Garavan, IMAGEN Consortium category:q-bio.NC cs.NE published:2015-09-08 summary:The field of neuroimaging has truly become data rich, and novel analyticalmethods capable of gleaning meaningful information from large stores of imagingdata are in high demand. Those methods that might also be applicable on thelevel of individual subjects, and thus potentially useful clinically, are ofspecial interest. In the present study, we introduce just such a method, callednonlinear functional mapping (NFM), and demonstrate its application in theanalysis of resting state fMRI from a 242-subject subset of the IMAGEN project,a European study of adolescents that includes longitudinal phenotypic,behavioral, genetic, and neuroimaging data. NFM employs a computationaltechnique inspired by biological evolution to discover and mathematicallycharacterize interactions among ROI (regions of interest), without makinglinear or univariate assumptions. We show that statistics of the resultinginteraction relationships comport with recent independent work, constituting apreliminary cross-validation. Furthermore, nonlinear terms are ubiquitous inthe models generated by NFM, suggesting that some of the interactionscharacterized here are not discoverable by standard linear methods of analysis.We discuss one such nonlinear interaction in the context of a direct comparisonwith a procedure involving pairwise correlation, designed to be an analogouslinear version of functional mapping. We find another such interaction thatsuggests a novel distinction in brain function between drinking andnon-drinking adolescents: a tighter coupling of ROI associated with emotion,reward, and interoceptive processes such as thirst, among drinkers. Finally, weoutline many improvements and extensions of the methodology to reducecomputational expense, complement other analytical tools like graph-theoreticanalysis, and allow for voxel level NFM to eliminate the necessity of ROIselection.
arxiv-13200-17 | Optimizing Static and Adaptive Probing Schedules for Rapid Event Detection | http://arxiv.org/abs/1509.02487 | author:Ahmad Mahmoody, Evgenios M. Kornaropoulos, Eli Upfal category:cs.DS cs.LG published:2015-09-08 summary:We formulate and study a fundamental search and detection problem, ScheduleOptimization, motivated by a variety of real-world applications, ranging frommonitoring content changes on the web, social networks, and user activities todetecting failure on large systems with many individual machines. We consider a large system consists of many nodes, where each node has itsown rate of generating new events, or items. A monitoring application can probea small number of nodes at each step, and our goal is to compute a probingschedule that minimizes the expected number of undiscovered items at thesystem, or equivalently, minimizes the expected time to discover a new item inthe system. We study the Schedule Optimization problem both for deterministic andrandomized memoryless algorithms. We provide lower bounds on the cost of anoptimal schedule and construct close to optimal schedules with rigorousmathematical guarantees. Finally, we present an adaptive algorithm that startswith no prior information on the system and converges to the optimal memorylessalgorithms by adapting to observed data.
arxiv-13200-18 | On Wasserstein Two Sample Testing and Related Families of Nonparametric Tests | http://arxiv.org/abs/1509.02237 | author:Aaditya Ramdas, Nicolas Garcia, Marco Cuturi category:math.ST stat.ML stat.TH published:2015-09-08 summary:Nonparametric two sample or homogeneity testing is a decision theoreticproblem that involves identifying differences between two random variableswithout making parametric assumptions about their underlying distributions. Theliterature is old and rich, with a wide variety of statistics having beingintelligently designed and analyzed, both for the unidimensional and themultivariate setting. Our contribution is to tie together many of these tests,drawing connections between seemingly very different statistics. In this work,our central object is the Wasserstein distance, as we form a chain ofconnections from univariate methods like the Kolmogorov-Smirnov test, PP/QQplots and ROC/ODC curves, to multivariate tests involving energy statistics andkernel based maximum mean discrepancy. Some connections proceed through theconstruction of a \textit{smoothed} Wasserstein distance, and others throughthe pursuit of a "distribution-free" Wasserstein test. Some observations inthis chain are implicit in the literature, while others seem to have not beennoticed thus far. Given nonparametric two sample testing's classical andcontinued importance, we aim to provide useful connections for theorists andpractitioners familiar with one subset of methods but not others.
arxiv-13200-19 | Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition | http://arxiv.org/abs/1509.02412 | author:Mortaza Doulaty, Oscar Saz, Thomas Hain category:cs.CL published:2015-09-08 summary:Speech recognition systems are often highly domain dependent, a fact widelyreported in the literature. However the concept of domain is complex and notbound to clear criteria. Hence it is often not evident if data should beconsidered to be out-of-domain. While both acoustic and language models can bedomain specific, work in this paper concentrates on acoustic modelling. Wepresent a novel method to perform unsupervised discovery of domains usingLatent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains isassumed to exist in the data, whereby each audio segment can be considered tobe a weighted mixture of domain properties. The classification of audiosegments into domains allows the creation of domain specific acoustic modelsfor automatic speech recognition. Experiments are conducted on a dataset ofdiverse speech data covering speech from radio and TV broadcasts, telephoneconversations, meetings, lectures and read speech, with a joint training set of60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation toLDA based domains was shown to yield relative Word Error Rate (WER)improvements of up to 16% relative, compared to pooled training, and up to 10%,compared with models adapted with human-labelled prior domain knowledge.
arxiv-13200-20 | On the complexity of piecewise affine system identification | http://arxiv.org/abs/1509.02348 | author:Fabien Lauer category:stat.ML cs.CC published:2015-09-08 summary:The paper provides results regarding the computational complexity of hybridsystem identification. More precisely, we focus on the estimation of piecewiseaffine (PWA) maps from input-output data and analyze the complexity ofcomputing a global minimizer of the error. Previous work showed that a globalsolution could be obtained for continuous PWA maps with a worst-case complexityexponential in the number of data. In this paper, we show how global optimalitycan be reached for a slightly more general class of possibly discontinuous PWAmaps with a complexity only polynomial in the number of data, however with anexponential complexity with respect to the data dimension. This result isobtained via an analysis of the intrinsic classification subproblem ofassociating the data points to the different modes. In addition, we prove thatthe problem is NP-hard, and thus that the exponential complexity in thedimension is a natural expectation for any exact algorithm.
arxiv-13200-21 | Modelling time evolving interactions in networks through a non stationary extension of stochastic block models | http://arxiv.org/abs/1509.02347 | author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML published:2015-09-08 summary:In this paper, we focus on the stochastic block model (SBM),a probabilistictool describing interactions between nodes of a network using latent clusters.The SBM assumes that the networkhas a stationary structure, in whichconnections of time varying intensity are not taken into account. In otherwords, interactions between two groups are forced to have the same featuresduring the whole observation time. To overcome this limitation,we propose apartition of the whole time horizon, in which interactions are observed, anddevelop a non stationary extension of the SBM,allowing to simultaneouslycluster the nodes in a network along with fixed time intervals in which theinteractions take place. The number of clusters (K for nodes, D for timeintervals) as well as the class memberships are finallyobtained throughmaximizing the complete-data integrated likelihood by means of a greedy searchapproach. After showing that the model works properly with simulated data, wefocus on a real data set. We thus consider the three days ACM Hypertextconference held in Turin,June 29th - July 1st 2009. Proximity interactionsbetween attendees during the first day are modelled and aninterestingclustering of the daily hours is finally obtained, with times ofsocial gathering (e.g. coffee breaks) recovered by the approach. Applicationsto large networks are limited due to the computational complexity of the greedysearch which is dominated bythe number $K\_{max}$ and $D\_{max}$ of clustersused in the initialization. Therefore,advanced clustering tools are consideredto reduce the number of clusters expected in the data, making the greedy searchapplicable to large networks.
arxiv-13200-22 | A Behavior Analysis-Based Game Bot Detection Approach Considering Various Play Styles | http://arxiv.org/abs/1509.02458 | author:Yeounoh Chung, Chang-yong Park, Noo-ri Kim, Hana Cho, Taebok Yoon, Hunjoo Lee, Jee-Hyong Lee category:cs.LG cs.AI published:2015-09-08 summary:An approach for game bot detection in MMORPGs is proposed based on theanalysis of game playing behavior. Since MMORPGs are large scale games, userscan play in various ways. This variety in playing behavior makes it hard todetect game bots based on play behaviors. In order to cope with this problem,the proposed approach observes game playing behaviors of users and groups themby their behavioral similarities. Then, it develops a local bot detection modelfor each player group. Since the locally optimized models can more accuratelydetect game bots within each player group, the combination of those modelsbrings about overall improvement. For a practical purpose of reducing theworkloads of the game servers in service, the game data is collected at a lowresolution in time. Behavioral features are selected and developed toaccurately detect game bots with the low resolution data, considering commonaspects of MMORPG playing. Through the experiment with the real data from agame currently in service, it is shown that the proposed local model approachyields more accurate results.
arxiv-13200-23 | Data-selective Transfer Learning for Multi-Domain Speech Recognition | http://arxiv.org/abs/1509.02409 | author:Mortaza Doulaty, Oscar Saz, Thomas Hain category:cs.LG cs.CL cs.SD published:2015-09-08 summary:Negative transfer in training of acoustic models for automatic speechrecognition has been reported in several contexts such as domain change orspeaker characteristics. This paper proposes a novel technique to overcomenegative transfer by efficient selection of speech data for acoustic modeltraining. Here data is chosen on relevance for a specific target. A submodularfunction based on likelihood ratios is used to determine how acousticallysimilar each training utterance is to a target test set. The approach isevaluated on a wide-domain data set, covering speech from radio and TVbroadcasts, telephone conversations, meetings, lectures and read speech.Experiments demonstrate that the proposed technique both finds relevant dataand limits negative transfer. Results on a 6--hour test set show a relativeimprovement of 4% with data selection over using all data in PLP based models,and 2% with DNN features.
arxiv-13200-24 | Central Pattern Generators for the control of robotic systems | http://arxiv.org/abs/1509.02417 | author:Carlos Garcia-Saura category:cs.RO cs.NE published:2015-09-08 summary:Bio-inspired control of motion is an active field of research with manyapplications in real world tasks. In the case of robotic systems that need toexhibit oscillatory behaviour (i.e. locomotion of snake-type or legged robots),Central Pattern Generators (CPGs) are among the most versatile solutions. Thesecontrollers are often based on loosely-coupled oscillators similar to thosefound in the neural circuits of many animal species, and can be more robust touncertainty (i.e. external perturbations) than traditional control approaches.This project provides an overview of the state-of-the-art in the field of CPGs,and in particular their applications within robotic systems. The project alsotackles the implementation of a CPG-based controller in a small 3D-printedhexapod.
arxiv-13200-25 | A Scalable and Extensible Framework for Superposition-Structured Models | http://arxiv.org/abs/1509.02314 | author:Shenjian Zhao, Cong Xie, Zhihua Zhang category:cs.NA math.OC stat.ML published:2015-09-08 summary:In many learning tasks, structural models usually lead to betterinterpretability and higher generalization performance. In recent years,however, the simple structural models such as lasso are frequently proved to beinsufficient. Accordingly, there has been a lot of work on"superposition-structured" models where multiple structural constraints areimposed. To efficiently solve these "superposition-structured" statisticalmodels, we develop a framework based on a proximal Newton-type method.Employing the smoothed conic dual approach with the LBFGS updating formula, wepropose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.Empirical analysis on various datasets shows that our framework is potentiallypowerful, and achieves super-linear convergence rate for optimizing somepopular "superposition-structured" statistical models such as the fused sparsegroup lasso.
arxiv-13200-26 | A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression | http://arxiv.org/abs/1509.02438 | author:Arnold Salas, Stephen J. Roberts, Michael A. Osborne category:stat.ML published:2015-09-08 summary:Online Passive-Aggressive (PA) learning is a class of online margin-basedalgorithms suitable for a wide range of real-time prediction tasks, includingclassification and regression. PA algorithms are formulated in terms ofdeterministic point-estimation problems governed by a set of user-definedhyperparameters: the approach fails to capture model/prediction uncertainty andmakes their performance highly sensitive to hyperparameter configurations. Inthis paper, we introduce a novel PA learning framework for regression thatovercomes the above limitations. We contribute a Bayesian state-spaceinterpretation of PA regression, along with a novel online variationalinference scheme, that not only produces probabilistic predictions, but alsooffers the benefit of automatic hyperparameter tuning. Experiments with variousreal-world data sets show that our approach performs significantly better thana more standard, linear Gaussian state-space model.
arxiv-13200-27 | HEp-2 Cell Classification: The Role of Gaussian Scale Space Theory as A Pre-processing Approach | http://arxiv.org/abs/1509.02320 | author:Xianbiao Qi, Guoying Zhao, Jie Chen, Matti Pietikäinen category:cs.CV published:2015-09-08 summary:\textit{Indirect Immunofluorescence Imaging of Human Epithelial Type 2}(HEp-2) cells is an effective way to identify the presence of Anti-NuclearAntibody (ANA). Most existing works on HEp-2 cell classification mainly focuson feature extraction, feature encoding and classifier design. Very few effortshave been devoted to study the importance of the pre-processing techniques. Inthis paper, we analyze the importance of the pre-processing, and investigatethe role of Gaussian Scale Space (GSS) theory as a pre-processing approach forthe HEp-2 cell classification task. We validate the GSS pre-processing underthe Local Binary Pattern (LBP) and the Bag-of-Words (BoW) frameworks. Under theBoW framework, the introduced pre-processing approach, using only one LocalOrientation Adaptive Descriptor (LOAD), achieved superior performance on theExecutable Thematic on Pattern Recognition Techniques for IndirectImmunofluorescence (ET-PRT-IIF) image analysis. Our system, using only onefeature, outperformed the winner of the ICPR 2014 contest that combined fourtypes of features. Meanwhile, the proposed pre-processing method is notrestricted to this work; it can be generalized to many existing works.
arxiv-13200-28 | A New Low-Rank Tensor Model for Video Completion | http://arxiv.org/abs/1509.02027 | author:Wenrui Hu, Dacheng Tao, Wensheng Zhang, Yuan Xie, Yehui Yang category:cs.CV published:2015-09-07 summary:In this paper, we propose a new low-rank tensor model based on the circulantalgebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensordenotes a 3-way tensor representation to laterally store 2D data slices inorder. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twisttensor in the Fourier domain, which allows an efficient computation using FFT.On the other, t-TNN is equal to the nuclear norm of block circulantmatricization of the twist tensor in the original domain, which extends thetraditional matrix nuclear norm in a block circulant way. We test the t-TNNmodel on a video completion application that aims to fill missing values andthe experiment results validate its effectiveness, especially when dealing withvideo recorded by a non-stationary panning camera. The block circulantmatricization of the twist tensor can be transformed into a circulant blockrepresentation with nuclear norm invariance. This representation, aftertransformation, exploits the horizontal translation relationship between theframes in a video, and endows the t-TNN model with a more powerful ability toreconstruct panning videos than the existing state-of-the-art low-rank models.
arxiv-13200-29 | Unsupervised Discovery of Linguistic Structure Including Two-level Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization | http://arxiv.org/abs/1509.02208 | author:Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee category:cs.CL published:2015-09-07 summary:Techniques for unsupervised discovery of acoustic patterns are gettingincreasingly attractive, because huge quantities of speech data are becomingavailable but manual annotations remain hard to acquire. In this paper, wepropose an approach for unsupervised discovery of linguistic structure for thetarget spoken language given raw speech data. This linguistic structureincludes two-level (subword-like and word-like) acoustic patterns, the lexiconof word-like patterns in terms of subword-like patterns and the N-gram languagemodel based on word-like patterns. All patterns, models, and parameters can beautomatically learned from the unlabelled speech corpus. This is achieved by aninitialization step followed by three cascaded stages for acoustic, linguistic,and lexical iterative optimization. The lexicon of word-like patterns definesallowed consecutive sequence of HMMs for subword-like patterns. In eachiteration, model training and decoding produces updated labels from which thelexicon and HMMs can be further updated. In this way, model parameters anddecoded labels are respectively optimized in each iteration, and the knowledgeabout the linguistic structure is learned gradually layer after layer. Theproposed approach was tested in preliminary experiments on a corpus of Mandarinbroadcast news, including a task of spoken term detection with performancecompared to a parallel test using models trained in a supervised way. Resultsshow that the proposed system not only yields reasonable performance on itsown, but is also complimentary to existing large vocabulary ASR systems.
arxiv-13200-30 | Matrix Factorisation with Linear Filters | http://arxiv.org/abs/1509.02088 | author:Ömer Deniz Akyıldız category:stat.ML published:2015-09-07 summary:This text investigates relations between two well-known family of algorithms,matrix factorisations and recursive linear filters, by describing aprobabilistic model in which approximate inference corresponds to a matrixfactorisation algorithm. Using the probabilistic model, we derive a matrixfactorisation algorithm as a recursive linear filter. More precisely, we derivea matrix-variate recursive linear filter in order to perform efficientinference in high dimensions. We also show that it is possible to interpret ouralgorithm as a nontrivial stochastic gradient algorithm. Demonstrations andcomparisons on an image restoration task are given.
arxiv-13200-31 | Diffusion tensor imaging with deterministic error bounds | http://arxiv.org/abs/1509.02223 | author:Artur Gorokh, Yury Korolev, Tuomo Valkonen category:cs.CV cs.NA published:2015-09-07 summary:Errors in the data and the forward operator of an inverse problem can behandily modelled using partial order in Banach lattices. We present someexisting results of the theory of regularisation in this novel framework, whereerrors are represented as bounds by means of the appropriate partial order. We apply the theory to Diffusion Tensor Imaging, where correct noisemodelling is challenging: it involves the Rician distribution and the nonlinearStejskal-Tanner equation. Linearisation of the latter in the statisticalframework would complicate the noise model even further. We avoid this usingthe error bounds approach, which preserves simple error structure undermonotone transformations.
arxiv-13200-32 | Unsupervised Spoken Term Detection with Spoken Queries by Multi-level Acoustic Patterns with Varying Model Granularity | http://arxiv.org/abs/1509.02213 | author:Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee category:cs.CL published:2015-09-07 summary:This paper presents a new approach for unsupervised Spoken Term Detectionwith spoken queries using multiple sets of acoustic patterns automaticallydiscovered from the target corpus. The different pattern HMMconfigurations(number of states per model, number of distinct models, number ofGaussians per state)form a three-dimensional model granularity space. Differentsets of acoustic patterns automatically discovered on different points properlydistributed over this three-dimensional space are complementary to one another,thus can jointly capture the characteristics of the spoken terms. Byrepresenting the spoken content and spoken query as sequences of acousticpatterns, a series of approaches for matching the pattern index sequences whileconsidering the signal variations are developed. In this way, not only theon-line computation load can be reduced, but the signal distributions caused bydifferent speakers and acoustic conditions can be reasonably taken care of. Theresults indicate that this approach significantly outperformed the unsupervisedfeature-based DTW baseline by 16.16\% in mean average precision on the TIMITcorpus.
arxiv-13200-33 | Future Localization from an Egocentric Depth Image | http://arxiv.org/abs/1509.02094 | author:Hyun Soo Park, Yedong Niu, Jianbo Shi category:cs.CV published:2015-09-07 summary:This paper presents a method for future localization: to predict a set ofplausible trajectories of ego-motion given a depth image. We predict pathsavoiding obstacles, between objects, even paths turning around a corner intospace behind objects. As a byproduct of the predicted trajectories ofego-motion, we discover in the image the empty space occluded by foregroundobjects. We use no image based features such as semantic labeling/segmentationor object detection/recognition for this algorithm. Inspired by proxemics, werepresent the space around a person using an EgoSpace map, akin to anillustrated tourist map, that measures a likelihood of occlusion at theegocentric coordinate system. A future trajectory of ego-motion is modeled by alinear combination of compact trajectory bases allowing us to constrain thepredicted trajectory. We learn the relationship between the EgoSpace map andtrajectory from the EgoMotion dataset providing in-situ measurements of thefuture trajectory. A cost function that takes into account partial occlusiondue to foreground objects is minimized to predict a trajectory. This costfunction generates a trajectory that passes through the occluded space, whichallows us to discover the empty space behind the foreground objects. Wequantitatively evaluate our method to show predictive validity and apply tovarious real world scenes including walking, shopping, and social interactions.
arxiv-13200-34 | Convexity Shape Constraints for Image Segmentation | http://arxiv.org/abs/1509.02122 | author:Loic A. Royer, David L. Richmond, Carsten Rother, Bjoern Andres, Dagmar Kainmueller category:cs.CV published:2015-09-07 summary:Segmenting an image into multiple components is a central task in computervision. In many practical scenarios, prior knowledge about plausible componentsis available. Incorporating such prior knowledge into models and algorithms forimage segmentation is highly desirable, yet can be non-trivial. In this work,we introduce a new approach that allows, for the first time, to constrain someor all components of a segmentation to have convex shapes. Specifically, weextend the Minimum Cost Multicut Problem by a class of constraints that enforceconvexity. To solve instances of this APX-hard integer linear program tooptimality, we separate the proposed constraints in the branch-and-cut loop ofa state-of-the-art ILP solver. Results on natural and biological imagesdemonstrate the effectiveness of the approach as well as its advantage over thestate-of-the-art heuristic.
arxiv-13200-35 | Fuzzy Jets | http://arxiv.org/abs/1509.02216 | author:Lester Mackey, Benjamin Nachman, Ariel Schwartzman, Conrad Stansbury category:hep-ph stat.ML published:2015-09-07 summary:Collimated streams of particles produced in high energy physics experimentsare organized using clustering algorithms to form jets. To construct jets, theexperimental collaborations based at the Large Hadron Collider (LHC) primarilyuse agglomerative hierarchical clustering schemes known as sequentialrecombination. We propose a new class of algorithms for clustering jets thatuse infrared and collinear safe mixture models. These new algorithms, known asfuzzy jets, are clustered using maximum likelihood techniques and candynamically determine various properties of jets like their size. We show thatthe fuzzy jet size adds additional information to conventional jet taggingvariables. Furthermore, we study the impact of pileup and show that with someslight modifications to the algorithm, fuzzy jets can be stable up to highpileup interaction multiplicities.
arxiv-13200-36 | Enhancing Automatically Discovered Multi-level Acoustic Patterns Considering Context Consistency With Applications in Spoken Term Detection | http://arxiv.org/abs/1509.02217 | author:Cheng-Tao Chung, Wei-Ning Hsu, Cheng-Yi Lee, Lin-Shan Lee category:cs.CL published:2015-09-07 summary:This paper presents a novel approach for enhancing the multiple sets ofacoustic patterns automatically discovered from a given corpus. In a previouswork it was proposed that different HMM configurations (number of states permodel, number of distinct models) for the acoustic patterns form atwo-dimensional space. Multiple sets of acoustic patterns automaticallydiscovered with the HMM configurations properly located on different pointsover this two-dimensional space were shown to be complementary to one another,jointly capturing the characteristics of the given corpus. By representing thegiven corpus as sequences of acoustic patterns on different HMM sets, thepattern indices in these sequences can be relabeled considering the contextconsistency across the different sequences. Good improvements were observed inpreliminary experiments of pattern spoken term detection (STD) performed onboth TIMIT and Mandarin Broadcast News with such enhanced patterns.
arxiv-13200-37 | Structured Prediction with Output Embeddings for Semantic Image Annotation | http://arxiv.org/abs/1509.02130 | author:Ariadna Quattoni, Arnau Ramisa, Pranava Swaroop Madhyastha, Edgar Simo-Serra, Francesc Moreno-Noguer category:cs.CV published:2015-09-07 summary:We address the task of annotating images with semantic tuples. Solving thisproblem requires an algorithm which is able to deal with hundreds of classesfor each argument of the tuple. In such contexts, data sparsity becomes a keychallenge, as there will be a large number of classes for which only a fewexamples are available. We propose handling this by incorporating featurerepresentations of both the inputs (images) and outputs (argument classes) intoa factorized log-linear model, and exploiting the flexibility of scoringfunctions based on bilinear forms. Experiments show that integrating featurerepresentations of the outputs in the structured prediction model leads tobetter overall predictions. We also conclude that the best outputrepresentation is specific for each type of argument.
arxiv-13200-38 | Poisson Subsampling Algorithms for Large Sample Linear Regression in Massive Data | http://arxiv.org/abs/1509.02116 | author:Rong Zhu category:stat.ML published:2015-09-07 summary:Large sample size brings the computation bottleneck for modern data analysis.Subsampling is one of efficient strategies to handle this problem. In previousstudies, researchers make more fo- cus on subsampling with replacement (SSR)than on subsampling without replacement (SSWR). In this paper we investigate akind of SSWR, poisson subsampling (PSS), for fast algorithm in ordinaryleast-square problem. We establish non-asymptotic property, i.e, the errorbound of the correspond- ing subsample estimator, which provide a tradeoffbetween computation cost and approximation efficiency. Besides thenon-asymptotic result, we provide asymptotic consistency and normality of thesubsample estimator. Methodologically, we propose a two-step subsamplingalgorithm, which is efficient with respect to a statistical objective andindependent on the linear model assumption.. Synthetic and real data are usedto empirically study our proposed subsampling strategies. We argue by theseempirical studies that, (1) our proposed two-step algorithm has obviousadvantage when the assumed linear model does not accurate, and (2) the PSSstrategy performs obviously better than SSR when the subsampling ratioincreases.
arxiv-13200-39 | An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture | http://arxiv.org/abs/1509.01978 | author:Darko Brodic, Alessia Amelio, Zoran N. Milivojevic category:cs.CV cs.AI cs.CL I.4; I.2.7 published:2015-09-07 summary:The paper presents a new script classification method for the discriminationof the South Slavic medieval labels. It consists in the textural analysis ofthe script types. In the first step, each letter is coded by the equivalentscript type, which is defined by its typographical features. Obtained codedtext is subjected to the run-length statistical analysis and to the adjacentlocal binary pattern analysis in order to extract the features. The resultshows a diversity between the extracted features of the scripts, which makesthe feature classification more effective. It is the basis for theclassification process of the script identification by using an extension of astate-of-the-art approach for document clustering. The proposed method isevaluated on an example of hand-engraved in stone and hand-printed in paperlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstratevery positive results, which prove the effectiveness of the proposed method.
arxiv-13200-40 | Automated Analysis of Behavioural Variability and Filial Imprinting of Chicks (G. gallus), using Autonomous Robots | http://arxiv.org/abs/1509.01957 | author:A. Gribovskiy, F. Mondada, J. L. Deneubourg, L. Cazenille, N. Bredeche, J. Halloy category:q-bio.QM cs.LG cs.RO physics.bio-ph published:2015-09-07 summary:Inter-individual variability has various impacts in animal social behaviour.This implies that not only collective behaviours have to be studied but alsothe behavioural variability of each member composing the groups. To understandthose effects on group behaviour, we develop a quantitative methodology basedon automated ethograms and autonomous robots to study the inter-individualvariability among social animals. We choose chicks of \textit{Gallus gallusdomesticus} as a classic social animal model system for their suitability inlaboratory and controlled experimentation. Moreover, even domesticated chickenpresent social structures implying forms or leadership and filial imprinting.We develop an imprinting methodology on autonomous robots to study individualand social behaviour of free moving animals. This allows to quantify thebehaviours of large number of animals. We develop an automated experimentalmethodology that allows to make relatively fast controlled experiments andefficient data analysis. Our analysis are based on high-throughput dataallowing a fine quantification of individual behavioural traits. We quantifythe efficiency of various state-of-the-art algorithms to automate data analysisand produce automated ethograms. We show that the use of robots allows toprovide controlled and quantified stimuli to the animals in absence of humanintervention. We quantify the individual behaviour of 205 chicks obtained fromhatching after synchronized fecundation. Our results show a high variability ofindividual behaviours and of imprinting quality and success. Three classes ofchicks are observed with various level of imprinting. Our study shows that theconcomitant use of autonomous robots and automated ethograms allows detailedand quantitative analysis of behavioural patterns of animals in controlledlaboratory experiments.
arxiv-13200-41 | Integrate Document Ranking Information into Confidence Measure Calculation for Spoken Term Detection | http://arxiv.org/abs/1509.01899 | author:Quan Liu, Wu Guo, Zhen-Hua Ling category:cs.CL published:2015-09-07 summary:This paper proposes an algorithm to improve the calculation of confidencemeasure for spoken term detection (STD). Given an input query term, thealgorithm first calculates a measurement named document ranking weight for eachdocument in the speech database to reflect its relevance with the query term bysumming all the confidence measures of the hypothesized term occurrences inthis document. The confidence measure of each term occurrence is thenre-estimated through linear interpolation with the calculated document rankingweight to improve its reliability by integrating document-level information.Experiments are conducted on three standard STD tasks for Tamil, Vietnamese andEnglish respectively. The experimental results all demonstrate that theproposed algorithm achieves consistent improvements over the state-of-the-artmethod for confidence measure calculation. Furthermore, this algorithm is stilleffective even if a high accuracy speech recognizer is not available, whichmakes it applicable for the languages with limited speech resources.
arxiv-13200-42 | Hierarchical Deep Learning Architecture For 10K Objects Classification | http://arxiv.org/abs/1509.01951 | author:Atul Laxman Katole, Krishna Prasad Yellapragada, Amish Kumar Bedi, Sehaj Singh Kalra, Mynepalli Siva Chaitanya category:cs.CV cs.LG cs.NE published:2015-09-07 summary:Evolution of visual object recognition architectures based on ConvolutionalNeural Networks & Convolutional Deep Belief Networks paradigms hasrevolutionized artificial Vision Science. These architectures extract & learnthe real world hierarchical visual features utilizing supervised & unsupervisedlearning approaches respectively. Both the approaches yet cannot scale uprealistically to provide recognition for a very large number of objects as highas 10K. We propose a two level hierarchical deep learning architecture inspiredby divide & conquer principle that decomposes the large scale recognitionarchitecture into root & leaf level model architectures. Each of the root &leaf level models is trained exclusively to provide superior results thanpossible by any 1-level deep learning architecture prevalent today. Theproposed architecture classifies objects in two steps. In the first step theroot level model classifies the object in a high level category. In the secondstep, the leaf level recognition model for the recognized high level categoryis selected among all the leaf models. This leaf level model is presented withthe same input object image which classifies it in a specific category. Also wepropose a blend of leaf level models trained with either supervised orunsupervised learning approaches. Unsupervised learning is suitable wheneverlabelled data is scarce for the specific leaf level models. Currently thetraining of leaf level models is in progress; where we have trained 25 out ofthe total 47 leaf level models as of now. We have trained the leaf models withthe best case top-5 error rate of 3.2% on the validation data set for theparticular leaf models. Also we demonstrate that the validation error of theleaf level models saturates towards the above mentioned accuracy as the numberof epochs are increased to more than sixty.
arxiv-13200-43 | Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical Machine Translation | http://arxiv.org/abs/1509.01938 | author:Katrin Kirchhoff, Bing Zhao, Wen Wang category:cs.CL published:2015-09-07 summary:Statistical machine translation for dialectal Arabic is characterized by alack of data since data acquisition involves the transcription and translationof spoken language. In this study we develop techniques for extracting paralleldata for one particular dialect of Arabic (Iraqi Arabic) from out-of-domaincorpora in different dialects of Arabic or in Modern Standard Arabic. Wecompare two different data selection strategies (cross-entropy based andsubmodular selection) and demonstrate that a very small but highly targetedamount of found data can improve the performance of a baseline machinetranslation system. We furthermore report on preliminary experiments on usingautomatically translated speech data as additional training data.
arxiv-13200-44 | An end-to-end generative framework for video segmentation and recognition | http://arxiv.org/abs/1509.01947 | author:Hilde Kuehne, Juergen Gall, Thomas Serre category:cs.CV published:2015-09-07 summary:We describe an end-to-end generative approach for the segmentation andrecognition of human activities. In this approach, a visual representationbased on reduced Fisher Vectors is combined with a structured temporal modelfor recognition. We show that the statistical properties of Fisher Vectors makethem an especially suitable front-end for generative models such as Gaussianmixtures. The system is evaluated for both the recognition of complexactivities as well as their parsing into action units. Using a variety of videodatasets ranging from human cooking activities to animal behaviors, ourexperiments demonstrate that the resulting architecture outperformsstate-of-the-art approaches for larger datasets, i.e. when sufficient amount ofdata is available for training structured generative models.
arxiv-13200-45 | A Hybrid Approach to Domain-Specific Entity Linking | http://arxiv.org/abs/1509.01865 | author:Alex Olieman, Jaap Kamps, Maarten Marx, Arjan Nusselder category:cs.IR cs.CL H.3.1 published:2015-09-06 summary:The current state-of-the-art Entity Linking (EL) systems are geared towardscorpora that are as heterogeneous as the Web, and therefore performsub-optimally on domain-specific corpora. A key open problem is how toconstruct effective EL systems for specific domains, as knowledge of the localcontext should in principle increase, rather than decrease, effectiveness. Inthis paper we propose the hybrid use of simple specialist linkers incombination with an existing generalist system to address this problem. Ourmain findings are the following. First, we construct a new reusable benchmarkfor EL on a corpus of domain-specific conversations. Second, we test theperformance of a range of approaches under the same conditions, and show thatspecialist linkers obtain high precision in isolation, and high recall whencombined with generalist linkers. Hence, we can effectively exploit localcontext and get the best of both worlds.
arxiv-13200-46 | Deep Online Convex Optimization by Putting Forecaster to Sleep | http://arxiv.org/abs/1509.01851 | author:David Balduzzi category:cs.LG cs.GT cs.NE published:2015-09-06 summary:Methods from convex optimization such as accelerated gradient descent arewidely used as building blocks for deep learning algorithms. However, thereasons for their empirical success are unclear, since neural networks are notconvex and standard guarantees do not apply. This paper develops the firstrigorous link between online convex optimization and error backpropagation onconvolutional networks. The first step is to introduce circadian games, a mildgeneralization of convex games with similar convergence properties. The mainresult is that error backpropagation on a convolutional network is equivalentto playing out a circadian game. It follows immediately that the waking-regretof players in the game (the units in the neural network) controls the overallrate of convergence of the network. Finally, we explore some implications ofthe results: (i) we describe the representations learned by a neural networkgame-theoretically, (ii) propose a learning setting at the level of individualunits that can be plugged into deep architectures, and (iii) propose a newapproach to adaptive model selection by applying bandit algorithms to choosewhich players to wake on each round.
arxiv-13200-47 | Theoretical and Experimental Analyses of Tensor-Based Regression and Classification | http://arxiv.org/abs/1509.01770 | author:Kishan Wimalawarne, Ryota Tomioka, Masashi Sugiyama category:cs.LG stat.ML published:2015-09-06 summary:We theoretically and experimentally investigate tensor-based regression andclassification. Our focus is regularization with various tensor norms,including the overlapped trace norm, the latent trace norm, and the scaledlatent trace norm. We first give dual optimization methods using thealternating direction method of multipliers, which is computationally efficientwhen the number of training samples is moderate. We then theoretically derivean excess risk bound for each tensor norm and clarify their behavior. Finally,we perform extensive experiments using simulated and real data and demonstratethe superiority of tensor-based learning methods over vector- and matrix-basedlearning methods.
arxiv-13200-48 | Sampled Weighted Min-Hashing for Large-Scale Topic Mining | http://arxiv.org/abs/1509.01771 | author:Gibran Fuentes-Pineda, Ivan Vladimir Meza-Ruiz category:cs.LG cs.CL cs.IR published:2015-09-06 summary:We present Sampled Weighted Min-Hashing (SWMH), a randomized approach toautomatically mine topics from large-scale corpora. SWMH generates multiplerandom partitions of the corpus vocabulary based on term co-occurrence andagglomerates highly overlapping inter-partition cells to produce the minedtopics. While other approaches define a topic as a probabilistic distributionover a vocabulary, SWMH topics are ordered subsets of such vocabulary.Interestingly, the topics mined by SWMH underlie themes from the corpus atdifferent levels of granularity. We extensively evaluate the meaningfulness ofthe mined topics both qualitatively and quantitatively on the NIPS (1.7 Kdocuments), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.Additionally, we compare the quality of SWMH with Online LDA topics fordocument representation in classification.
arxiv-13200-49 | Research: Analysis of Transport Model that Approximates Decision Taker's Preferences | http://arxiv.org/abs/1509.01815 | author:Valery Vilisov category:cs.LG cs.AI math.OC stat.AP published:2015-09-06 summary:Paper provides a method for solving the reverse Monge-Kantorovich transportproblem (TP). It allows to accumulate positive decision-taking experience madeby decision-taker in situations that can be presented in the form of TP. Theinitial data for the solution of the inverse TP is the information on orders,inventories and effective decisions take by decision-taker. The result ofsolving the inverse TP contains evaluations of the TPs payoff matrix elements.It can be used in new situations to select the solution corresponding to thepreferences of the decision-taker. The method allows to gain decision-takerexperience, so it can be used by others. The method allows to build the modelof decision-taker preferences in a specific application area. The model can beupdated regularly to ensure its relevance and adequacy to the decision-takersystem of preferences. This model is adaptive to the current preferences of thedecision taker.
arxiv-13200-50 | Hierarchical Completely Random Measures for Mixed Membership Modelling | http://arxiv.org/abs/1509.01817 | author:Gaurav Pandey, Ambedkar Dukkipati category:math.ST cs.LG stat.TH published:2015-09-06 summary:The main aim of this paper is to establish the applicability of a broad classof random measures, that includes the gamma process, for mixed membershipmodelling. We use completely random measures~(CRM) and hierarchical CRM todefine a prior for Poisson processes. We derive the marginal distribution ofthe resultant point process, when the underlying CRM is marginalized out. Usingwell known properties unique to Poisson processes, we were able to derive anexact approach for instantiating a Poisson process with a hierarchical CRMprior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRMmodels based on Chinese restaurant franchise sampling scheme. As an example, wepresent the sum of generalized gamma process (SGGP), and show its applicationin topic-modelling. We show that one can determine the power-law behaviour ofthe topics and words in a Bayesian fashion, by defining a prior on theparameters of SGGP.
arxiv-13200-51 | Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM) for unsupervised RGB-D image segmentation | http://arxiv.org/abs/1509.01788 | author:Md. Abul Hasnat, Olivier Alata, Alain Trémeau category:cs.CV published:2015-09-06 summary:Recent advances in depth imaging sensors provide easy access to thesynchronized depth with color, called RGB-D image. In this paper, we propose anunsupervised method for indoor RGB-D image segmentation and analysis. Weconsider a statistical image generation model based on the color and geometryof the scene. Our method consists of a joint color-spatial-directionalclustering method followed by a statistical planar region merging method. Weevaluate our method on the NYU depth database and compare it with existingunsupervised RGB-D segmentation methods. Results show that, it is comparablewith the state of the art methods and it needs less computation time. Moreover,it opens interesting perspectives to fuse color and geometry in an unsupervisedmanner.
arxiv-13200-52 | A Total Fractional-Order Variation Model for Image Restoration with Non-homogeneous Boundary Conditions and its Numerical Solution | http://arxiv.org/abs/1509.04237 | author:Jianping Zhang, Ke Chen category:cs.CV math.NA published:2015-09-06 summary:To overcome the weakness of a total variation based model for imagerestoration, various high order (typically second order) regularization modelshave been proposed and studied recently. In this paper we analyze and test afractional-order derivative based total $\alpha$-order variation model, whichcan outperform the currently popular high order regularization models. Thereexist several previous works using total $\alpha$-order variations for imagerestoration; however first no analysis is done yet and second all testedformulations, differing from each other, utilize the zero Dirichlet boundaryconditions which are not realistic (while non-zero boundary conditions violatedefinitions of fractional-order derivatives). This paper first reviews someresults of fractional-order derivatives and then analyzes the theoreticalproperties of the proposed total $\alpha$-order variational model rigorously.It then develops four algorithms for solving the variational problem, one basedon the variational Split-Bregman idea and three based on direct solution of thediscretise-optimization problem. Numerical experiments show that, in terms ofrestoration quality and solution efficiency, the proposed model can producehighly competitive results, for smooth images, to two established high ordermodels: the mean curvature and the total generalized variation.
arxiv-13200-53 | Reinforcement Learning with Parameterized Actions | http://arxiv.org/abs/1509.01644 | author:Warwick Masson, Pravesh Ranchod, George Konidaris category:cs.AI cs.LG published:2015-09-05 summary:We introduce a model-free algorithm for learning in Markov decision processeswith parameterized actions-discrete actions with continuous parameters. At eachstep the agent must select both which action to use and which parameters to usewith that action. We introduce the Q-PAMDP algorithm for learning in thesedomains, show that it converges to a local optimum, and compare it to directpolicy search in the goal-scoring and Platform domains.
arxiv-13200-54 | HAMSI: Distributed Incremental Optimization Algorithm Using Quadratic Approximations for Partially Separable Problems | http://arxiv.org/abs/1509.01698 | author:Umut Şimşekli, Hazal Koptagel, Figen Öztoprak, Ş. İlker Birbil, Ali Taylan Cemgil category:stat.ML cs.LG published:2015-09-05 summary:We propose HAMSI, a provably convergent incremental algorithm for solvinglarge-scale partially separable optimization problems that frequently emerge inmachine learning and inferential statistics. The algorithm is based on a localquadratic approximation and hence allows incorporating a second order curvatureinformation to speed-up the convergence. Furthermore, HAMSI needs almost notuning, and it is scalable as well as easily parallelizable. In large-scalesimulation studies with the MovieLens datasets, we illustrate that the methodis superior to a state-of-the-art distributed stochastic gradient descentmethod in terms of convergence behavior. This performance gain comes at theexpense of using memory that scales only linearly with the total size of theoptimization variables. We conclude that HAMSI may be considered as a viablealternative in many scenarios, where first order methods based on variants ofstochastic gradient descent are applicable.
arxiv-13200-55 | Co-interest Person Detection from Multiple Wearable Camera Videos | http://arxiv.org/abs/1509.01654 | author:Yuewei Lin, Kareem Ezzeldeen, Youjie Zhou, Xiaochuan Fan, Hongkai Yu, Hui Qian, Song Wang category:cs.CV published:2015-09-05 summary:Wearable cameras, such as Google Glass and Go Pro, enable video datacollection over larger areas and from different views. In this paper, we tacklea new problem of locating the co-interest person (CIP), i.e., the one who drawsattention from most camera wearers, from temporally synchronized videos takenby multiple wearable cameras. Our basic idea is to exploit the motion patternsof people and use them to correlate the persons across different videos,instead of performing appearance-based matching as in traditional videoco-segmentation/localization. This way, we can identify CIP even if a group ofpeople with similar appearance are present in the view. More specifically, wedetect a set of persons on each frame as the candidates of the CIP and thenbuild a Conditional Random Field (CRF) model to select the one with consistentmotion patterns in different videos and high spacial-temporal consistency ineach video. We collect three sets of wearable-camera videos for testing theproposed algorithm. All the involved people have similar appearances in thecollected videos and the experiments demonstrate the effectiveness of theproposed algorithm.
arxiv-13200-56 | Gravitational Clustering | http://arxiv.org/abs/1509.01659 | author:Armen Aghajanyan category:cs.LG published:2015-09-05 summary:The downfall of many supervised learning algorithms, such as neural networks,is the inherent need for a large amount of training data. Although there is alot of buzz about big data, there is still the problem of doing classificationfrom a small dataset. Other methods such as support vector machines, althoughcapable of dealing with few samples, are inherently binary classifiers, and arein need of learning strategies such as One vs All in the case ofmulti-classification. In the presence of a large number of classes this canbecome problematic. In this paper we present, a novel approach to supervisedlearning through the method of clustering. Unlike traditional methods such asK-Means, Gravitational Clustering does not require the initial number ofclusters, and automatically builds the clusters, individual samples can bearbitrarily weighted and it requires only few samples while staying resilientto over-fitting.
arxiv-13200-57 | Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning | http://arxiv.org/abs/1509.01692 | author:Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin category:cs.CL published:2015-09-05 summary:Recent work on word embeddings has shown that simple vector subtraction overpre-trained embeddings is surprisingly effective at capturing different lexicalrelations, despite lacking explicit supervision. Prior work has evaluated thisintriguing result using a word analogy prediction formulation and hand-selectedrelations, but the generality of the finding over a broader range of lexicalrelation types and different learning settings has not been evaluated. In thispaper, we carry out such an evaluation in two learning settings: (1) spectralclustering to induce word relations, and (2) supervised learning to classifyvector differences into relation types. We find that word embeddings capture asurprising amount of information, and that, under suitable supervised training,vector subtraction generalises well to a broad range of relations, includingover unseen lexical items.
arxiv-13200-58 | Algorithm and Theoretical Analysis for Domain Adaptation Feature Learning with Linear Classifiers | http://arxiv.org/abs/1509.01710 | author:Wenhao Jiang, Feiping Nie, Fu-lai Korris Chung, Heng Huang category:cs.LG published:2015-09-05 summary:Domain adaptation problem arises in a variety of applications where thetraining set (\textit{source} domain) and testing set (\textit{target} domain)follow different distributions. The difficulty of such learning problem lies inhow to bridge the gap between the source distribution and target distribution.In this paper, we give an formal analysis of feature learning algorithms fordomain adaptation with linear classifiers. Our analysis shows that in order toachieve good adaptation performance, the second moments of source domaindistribution and target domain distribution should be similar. Based on such aresult, a new linear feature learning algorithm for domain adaptation isdesigned and proposed. Furthermore, the new algorithm is extended to havemultiple layers, resulting in becoming another linear feature learningalgorithm. The newly introduced method is effective for the domain adaptationtasks on Amazon review dataset and spam dataset from ECML/PKDD 2006 discoverychallenge.
arxiv-13200-59 | Unsupervised Cross-Domain Recognition by Identifying Compact Joint Subspaces | http://arxiv.org/abs/1509.01719 | author:Yuewei Lin, Jing Chen, Yu Cao, Youjie Zhou, Lingfeng Zhang, Yuan Yan Tang, Song Wang category:cs.CV published:2015-09-05 summary:This paper introduces a new method to solve the cross-domain recognitionproblem. Different from the traditional domain adaption methods which rely on aglobal domain shift for all classes between source and target domain, theproposed method is more flexible to capture individual class variations acrossdomains. By adopting a natural and widely used assumption -- "the data samplesfrom the same class should lay on a low-dimensional subspace, even if they comefrom different domains", the proposed method circumvents the limitation of theglobal domain shift, and solves the cross-domain recognition by finding thecompact joint subspaces of source and target domain. Specifically, givenlabeled samples in source domain, we construct subspaces for each of theclasses. Then we construct subspaces in the target domain, called anchorsubspaces, by collecting unlabeled samples that are close to each other andhighly likely all fall into the same class. The corresponding class label isthen assigned by minimizing a cost function which reflects the overlap andtopological structure consistency between subspaces across source and targetdomains, and within anchor subspaces, respectively.We further combine theanchor subspaces to corresponding source subspaces to construct the compactjoint subspaces. Subsequently, one-vs-rest SVM classifiers are trained in thecompact joint subspaces and applied to unlabeled data in the target domain. Weevaluate the proposed method on two widely used datasets: object recognitiondataset for computer vision tasks, and sentiment classification dataset fornatural language processing tasks. Comparison results demonstrate that theproposed method outperforms the comparison methods on both datasets.
arxiv-13200-60 | A commentary on "The now-or-never bottleneck: a fundamental constraint on language", by Christiansen and Chater (2015) | http://arxiv.org/abs/1509.01722 | author:Ramon Ferrer-i-Cancho category:cs.CL published:2015-09-05 summary:In a recent article, Christiansen and Chater (2015) present a fundamentalconstraint on language, i.e. a now-or-never bottleneck that arises from ourfleeting memory, and explore its implications, e.g., chunk-and-pass processing,outlining a framework that promises to unify different areas of research. Herewe explore additional support for this constraint and suggest furtherconnections from quantitative linguistics and information theory.
arxiv-13200-61 | A statistical shape space model of the palate surface trained on 3D MRI scans of the vocal tract | http://arxiv.org/abs/1602.07679 | author:Alexander Hewer, Ingmar Steiner, Timo Bolkart, Stefanie Wuhrer, Korin Richmond category:cs.CV published:2015-09-04 summary:We describe a minimally-supervised method for computing a statistical shapespace model of the palate surface. The model is created from a corpus ofvolumetric magnetic resonance imaging (MRI) scans collected from 12 speakers.We extract a 3D mesh of the palate from each speaker, then train the modelusing principal component analysis (PCA). The palate model is then tested using3D MRI from another corpus and evaluated using a high-resolution optical scan.We find that the error is low even when only a handful of measured coordinatesare available. In both cases, our approach yields promising results. It can beapplied to extract the palate shape from MRI data, and could be useful to otheranalysis modalities, such as electromagnetic articulography (EMA) andultrasound tongue imaging (UTI).
arxiv-13200-62 | CNN Based Hashing for Image Retrieval | http://arxiv.org/abs/1509.01354 | author:Jinma Guo, Jianmin Li category:cs.CV cs.LG I.2.6; H.3.1 published:2015-09-04 summary:Along with data on the web increasing dramatically, hashing is becoming moreand more popular as a method of approximate nearest neighbor search. Previoussupervised hashing methods utilized similarity/dissimilarity matrix to getsemantic information. But the matrix is not easy to construct for a newdataset. Rather than to reconstruct the matrix, we proposed a straightforwardCNN-based hashing method, i.e. binarilizing the activations of a fullyconnected layer with threshold 0 and taking the binary result as hash codes.This method achieved the best performance on CIFAR-10 and was comparable withthe state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested thatthe signs of activations may carry more information than the relative values ofactivations between samples, and that the co-adaption between feature extractorand hash functions is important for hashing.
arxiv-13200-63 | Diffusion-KLMS Algorithm and its Performance Analysis for Non-Linear Distributed Networks | http://arxiv.org/abs/1509.01352 | author:Rangeet Mitra, Vimal Bhatia category:cs.LG cs.DC cs.IT cs.SY math.IT published:2015-09-04 summary:In a distributed network environment, the diffusion-least mean squares (LMS)algorithm gives faster convergence than the original LMS algorithm. It has alsobeen observed that, the diffusion-LMS generally outperforms other distributedLMS algorithms like spatial LMS and incremental LMS. However, both the originalLMS and diffusion-LMS are not applicable in non-linear environments where datamay not be linearly separable. A variant of LMS called kernel-LMS (KLMS) hasbeen proposed in the literature for such non-linearities. In this paper, wepropose kernelised version of diffusion-LMS for non-linear distributedenvironments. Simulations show that the proposed approach has superiorconvergence as compared to algorithms of the same genre. We also introduce atechnique to predict the transient and steady-state behaviour of the proposedalgorithm. The techniques proposed in this work (or algorithms of same genre)can be easily extended to distributed parameter estimation applications likecooperative spectrum sensing and massive multiple input multiple output (MIMO)receiver design which are potential components for 5G communication systems.
arxiv-13200-64 | Giraffe: Using Deep Reinforcement Learning to Play Chess | http://arxiv.org/abs/1509.01549 | author:Matthew Lai category:cs.AI cs.LG cs.NE published:2015-09-04 summary:This report presents Giraffe, a chess engine that uses self-play to discoverall its domain-specific knowledge, with minimal hand-crafted knowledge given bythe programmer. Unlike previous attempts using machine learning only to performparameter-tuning on hand-crafted evaluation functions, Giraffe's learningsystem also performs automatic feature extraction and pattern recognition. Thetrained evaluation function performs comparably to the evaluation functions ofstate-of-the-art chess engines - all of which containing thousands of lines ofcarefully hand-crafted pattern recognizers, tuned over many years by bothcomputer chess experts and human chess masters. Giraffe is the most successfulattempt thus far at using end-to-end machine learning to play chess.
arxiv-13200-65 | Conjugate Gradient Acceleration of Non-Linear Smoothing Filters | http://arxiv.org/abs/1509.01514 | author:Andrew Knyazev, Alexander Malyshev category:cs.CV published:2015-09-04 summary:The most efficient signal edge-preserving smoothing filters, e.g., fordenoising, are non-linear. Thus, their acceleration is challenging and is oftenperformed in practice by tuning filter parameters, such as by increasing thewidth of the local smoothing neighborhood, resulting in more aggressivesmoothing of a single sweep at the cost of increased edge blurring. We proposean alternative technology, accelerating the original filters without tuning, byrunning them through a special conjugate gradient method, not affecting theirquality. The filter non-linearity is dealt with by careful freezing andrestarting. Our initial numerical experiments on toy one-dimensional signalsdemonstrate 20x acceleration of the classical bilateral filter and 3-5xacceleration of the recently developed guided filter.
arxiv-13200-66 | Parallel and Distributed Approaches for Graph Based Semi-supervised Learning | http://arxiv.org/abs/1509.01349 | author:Konstantin Avrachenkov, Vivek Borkar, Krishnakant Saboo category:cs.LG published:2015-09-04 summary:Two approaches for graph based semi-supervised learning are proposed. Thefirstapproach is based on iteration of an affine map. A key element of theaffine map iteration is sparsematrix-vector multiplication, which has severalvery efficient parallel implementations. The secondapproach belongs to theclass of Markov Chain Monte Carlo (MCMC) algorithms. It is based onsampling ofnodes by performing a random walk on the graph. The latter approach isdistributedby its nature and can be easily implemented on several processors orover the network. Boththeoretical and practical evaluations are provided. It isfound that the nodes are classified intotheir class with very small error. Thesampling algorithm's ability to track new incoming nodesand to classify them isalso demonstrated.
arxiv-13200-67 | l1-norm Penalized Orthogonal Forward Regression | http://arxiv.org/abs/1509.01323 | author:Xia Hong, Sheng Chen, Yi Guo, Junbin Gao category:cs.LG stat.ML published:2015-09-04 summary:A l1-norm penalized orthogonal forward regression (l1-POFR) algorithm isproposed based on the concept of leaveone- out mean square error (LOOMSE).Firstly, a new l1-norm penalized cost function is defined in the constructedorthogonal space, and each orthogonal basis is associated with an individuallytunable regularization parameter. Secondly, due to orthogonal computation, theLOOMSE can be analytically computed without actually splitting the data set,and moreover a closed form of the optimal regularization parameter in terms ofminimal LOOMSE is derived. Thirdly, a lower bound for regularization parametersis proposed, which can be used for robust LOOMSE estimation by adaptivelydetecting and removing regressors to an inactive set so that the computationalcost of the algorithm is significantly reduced. Illustrative examples areincluded to demonstrate the effectiveness of this new l1-POFR approach.
arxiv-13200-68 | Object Recognition from Short Videos for Robotic Perception | http://arxiv.org/abs/1509.01602 | author:Ivan Bogun, Anelia Angelova, Navdeep Jaitly category:cs.CV I.5.4 published:2015-09-04 summary:Deep neural networks have become the primary learning technique for objectrecognition. Videos, unlike still images, are temporally coherent which makesthe application of deep networks non-trivial. Here, we investigate how motioncan aid object recognition in short videos. Our approach is based on LongShort-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs,we implement each gate as a convolution. We show that convolutional-based LSTMmodels are capable of learning motion dependencies and are able to improve therecognition accuracy when more frames in a sequence are available. We evaluateour approach on the Washington RGBD Object dataset and on the Washington RGBDScenes dataset. Our approach outperforms deep nets applied to still images andsets a new state-of-the-art in this domain.
arxiv-13200-69 | NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining | http://arxiv.org/abs/1509.03503 | author:Marco Winkler category:cs.SI cs.CV cs.DS physics.soc-ph published:2015-09-04 summary:The detection of triadic subgraph motifs is a common methodology incomplex-networks research. The procedure usually applied in order to detectmotifs evaluates whether a certain subgraph pattern is overrepresented in anetwork as a whole. However, motifs do not necessarily appear frequently inevery region of a graph. For this reason, we recently introduced the frameworkof Node-Specific Pattern Mining (NoSPaM). This work is a manual for animplementation of NoSPaM which can be downloaded from www.mwinkler.eu.
arxiv-13200-70 | Semantic Amodal Segmentation | http://arxiv.org/abs/1509.01329 | author:Yan Zhu, Yuandong Tian, Dimitris Mexatas, Piotr Dollár category:cs.CV published:2015-09-04 summary:Common visual recognition tasks such as classification, object detection, andsemantic segmentation are rapidly reaching maturity, and given the recent rateof progress, it is not unreasonable to conjecture that techniques for many ofthese problems will approach human levels of performance in the next few years.In this paper we look to the future: what is the next frontier in visualrecognition? We offer one possible answer to this question. We propose a detailed imageannotation that captures information beyond the visible pixels and requirescomplex reasoning about full scene structure. Specifically, we create an amodalsegmentation of each image: the full extent of each region is marked, not justthe visible pixels. Annotators outline and name all salient regions in theimage and specify a partial depth order. The result is a rich scene structure,including visible and occluded portions of each region, figure-ground edgeinformation, semantic labels, and object overlap. To date, we have labeled 500 images in the BSDS dataset with at least fiveannotators per image. Critically, the resulting full scene annotation issurprisingly consistent between annotators. For example, for edge detection ourannotations have substantially higher human consistency than the original BSDSedges while providing a greater challenge for existing algorithms. We arecurrently annotating ~5000 images from the MS COCO dataset.
arxiv-13200-71 | Learning Temporal Alignment Uncertainty for Efficient Event Detection | http://arxiv.org/abs/1509.01343 | author:Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon Lucey category:cs.CV published:2015-09-04 summary:In this paper we tackle the problem of efficient video event detection. Weargue that linear detection functions should be preferred in this regard due totheir scalability and efficiency during estimation and evaluation. A popularapproach in this regard is to represent a sequence using a bag of words (BOW)representation due to its: (i) fixed dimensionality irrespective of thesequence length, and (ii) its ability to compactly model the statistics in thesequence. A drawback to the BOW representation, however, is the intrinsicdestruction of the temporal ordering information. In this paper we propose anew representation that leverages the uncertainty in relative temporalalignments between pairs of sequences while not destroying temporal ordering.Our representation, like BOW, is of a fixed dimensionality making it easilyintegrated with a linear detection function. Extensive experiments on CK+,6DMG, and UvA-NEMO databases show significant performance improvements acrossboth isolated and continuous event detection tasks.
arxiv-13200-72 | A nonlinear aggregation type classifier | http://arxiv.org/abs/1509.01604 | author:Alejandro Cholaquidis, Ricardo Fraiman, Juan Kalemkerian, Pamela Llop category:math.ST stat.ML stat.TH published:2015-09-04 summary:We introduce a nonlinear aggregation type classifier for functional datadefined on a separable and complete metric space. The new rule is built up froma collection of $M$ arbitrary training classifiers. If the classifiers areconsistent, then so is the aggregation rule. Moreover, asymptotically theaggregation rule behaves as well as the best of the $M$ classifiers. Theresults of a small simulation are reported both, for high dimensional andfunctional data, and a real data example is analyzed.
arxiv-13200-73 | An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes | http://arxiv.org/abs/1509.01520 | author:Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero, Radu Horaud category:cs.CV stat.ML published:2015-09-04 summary:Object tracking is an ubiquitous problem that appears in many applicationssuch as remote sensing, audio processing, computer vision, human-machineinterfaces, human-robot interaction, etc. Although thoroughly investigated incomputer vision, tracking a time-varying number of persons remains achallenging open problem. In this paper, we propose an on-line variationalBayesian model for multi-person tracking from cluttered visual observationsprovided by person detectors. The contributions of this paper are thefollowings. First, we propose a variational Bayesian framework for tracking anunknown and varying number of persons. Second, our model results in avariational expectation-maximization (VEM) algorithm with closed-formexpressions for the posterior distributions of the latent variables and for theestimation of the model parameters. Third, the proposed model exploitsobservations from multiple detectors, and it is therefore multimodal by nature.Finally, we propose to embed both object-birth and object-visibility processesin an effort to robustly handle person appearances and disappearances overtime. Evaluated on classical multiple person tracking datasets, our methodshows competitive results with respect to state-of-the-art multiple-objecttracking models, such as the probability hypothesis density (PHD) filter amongothers.
arxiv-13200-74 | Predicting SLA Violations in Real Time using Online Machine Learning | http://arxiv.org/abs/1509.01386 | author:Jawwad Ahmed, Andreas Johnsson, Rerngvit Yanggratoke, John Ardelius, Christofer Flinta, Rolf Stadler category:cs.NI cs.LG cs.SE stat.ML published:2015-09-04 summary:Detecting faults and SLA violations in a timely manner is critical fortelecom providers, in order to avoid loss in business, revenue and reputation.At the same time predicting SLA violations for user services in telecomenvironments is difficult, due to time-varying user demands and infrastructureload conditions. In this paper, we propose a service-agnostic online learning approach,whereby the behavior of the system is learned on the fly, in order to predictclient-side SLA violations. The approach uses device-level metrics, which arecollected in a streaming fashion on the server side. Our results show that the approach can produce highly accurate predictions(>90% classification accuracy and < 10% false alarm rate) in scenarios whereSLA violations are predicted for a video-on-demand service under changing loadpatterns. The paper also highlight the limitations of traditional offlinelearning methods, which perform significantly worse in many of the consideredscenarios.
arxiv-13200-75 | Quantization based Fast Inner Product Search | http://arxiv.org/abs/1509.01469 | author:Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, David Simcha category:cs.AI cs.LG stat.ML published:2015-09-04 summary:We propose a quantization based approach for fast approximate Maximum InnerProduct Search (MIPS). Each database vector is quantized in multiple subspacesvia a set of codebooks, learned directly by minimizing the inner productquantization error. Then, the inner product of a query to a database vector isapproximated as the sum of inner products with the subspace quantizers.Different from recently proposed LSH approaches to MIPS, the database vectorsand queries do not need to be augmented in a higher dimensional feature space.We also provide a theoretical analysis of the proposed approach, consisting ofthe concentration results under mild assumptions. Furthermore, if a smallsample of example queries is given at the training time, we propose a modifiedcodebook learning procedure which further improves the accuracy. Experimentalresults on a variety of datasets including those arising from deep neuralnetworks show that the proposed approach significantly outperforms the existingstate-of-the-art.
arxiv-13200-76 | Minimum Spectral Connectivity Projection Pursuit for Unsupervised Classification | http://arxiv.org/abs/1509.01546 | author:David P. Hofmeyr, Nicos G. Pavlidis, Idris A. Eckley category:stat.ML cs.LG 62H30 I.5.3 published:2015-09-04 summary:We study the problem of determining the optimal low dimensional projectionfor maximising the separability of a binary partition of an unlabelled dataset,as measured by spectral graph theory. This is achieved by finding projectionswhich minimise the second eigenvalue of the Laplacian matrices of the projecteddata, which corresponds to a non-convex, non-smooth optimisation problem. Weshow that the optimal univariate projection based on spectral connectivityconverges to the vector normal to the maximum margin hyperplane through thedata, as the scaling parameter is reduced to zero. This establishes aconnection between connectivity as measured by spectral graph theory andmaximal Euclidean separation. It also allows us to apply our methodology to theproblem of finding large margin linear separators. The computational costassociated with each eigen-problem is quadratic in the number of data. Tomitigate this problem, we propose an approximation method using microclusterswith provable approximation error bounds. We evaluate the performance of theproposed method on a large collection of benchmark datasets and find that itcompares favourably with existing methods for projection pursuit and dimensionreduction for unsupervised data partitioning.
arxiv-13200-77 | Better Document-level Sentiment Analysis from RST Discourse Parsing | http://arxiv.org/abs/1509.01599 | author:Parminder Bhatia, Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.AI published:2015-09-04 summary:Discourse structure is the hidden link between surface features anddocument-level properties, such as sentiment polarity. We show that thediscourse analyses produced by Rhetorical Structure Theory (RST) parsers canimprove document-level sentiment analysis, via composition of local informationup the discourse tree. First, we show that reweighting discourse unitsaccording to their position in a dependency representation of the rhetoricalstructure can yield substantial improvements on lexicon-based sentimentanalysis. Next, we present a recursive neural network over the RST structure,which offers significant improvements over classification-based methods.
arxiv-13200-78 | Stochastic gradient variational Bayes for gamma approximating distributions | http://arxiv.org/abs/1509.01631 | author:David A. Knowles category:stat.ML published:2015-09-04 summary:While stochastic variational inference is relatively well known for scalinginference in Bayesian probabilistic models, related methods also offer ways tocircumnavigate the approximation of analytically intractable expectations. Thekey challenge in either setting is controlling the variance of gradientestimates: recent work has shown that for continuous latent variables,particularly multivariate Gaussians, this can be achieved by using the gradientof the log posterior. In this paper we apply the same idea to gamma distributedlatent variables given gamma variational distributions, enablingstraightforward "black box" variational inference in models where sparsity andnon-negativity are appropriate. We demonstrate the method on a recentlyproposed gamma process model for network data, as well as a novel sparse factoranalysis. We outperform generic sampling algorithms and the approach of usingGaussian variational distributions on transformed variables.
arxiv-13200-79 | Chebyshev and Conjugate Gradient Filters for Graph Image Denoising | http://arxiv.org/abs/1509.01624 | author:Dong Tian, Hassan Mansour, Andrew Knyazev, Anthony Vetro category:cs.CV published:2015-09-04 summary:In 3D image/video acquisition, different views are often captured withvarying noise levels across the views. In this paper, we propose a graph-basedimage enhancement technique that uses a higher quality view to enhance adegraded view. A depth map is utilized as auxiliary information to match theperspectives of the two views. Our method performs graph-based filtering of thenoisy image by directly computing a projection of the image to be filtered ontoa lower dimensional Krylov subspace of the graph Laplacian. We discuss twograph spectral denoising methods: first using Chebyshev polynomials, and secondusing iterations of the conjugate gradient algorithm. Our framework generalizespreviously known polynomial graph filters, and we demonstrate through numericalsimulations that our proposed technique produces subjectively cleaner imageswith about 1-3 dB improvement in PSNR over existing polynomial graph filters.
arxiv-13200-80 | Semantic Video Segmentation : Exploring Inference Efficiency | http://arxiv.org/abs/1509.02441 | author:Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen category:cs.CV published:2015-09-04 summary:We explore the efficiency of the CRF inference beyond image level semanticsegmentation and perform joint inference in video frames. The key idea is tocombine best of two worlds: semantic co-labeling and more expressive models.Our formulation enables us to perform inference over ten thousand images withinseconds and makes the system amenable to perform video semantic segmentationmost effectively. On CamVid dataset, with TextonBoost unaries, our proposedmethod achieves up to 8% improvement in accuracy over individual semantic imagesegmentation without additional time overhead. The source code is available athttps://github.com/subtri/video_inference
arxiv-13200-81 | EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis | http://arxiv.org/abs/1509.01509 | author:Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes, Radu Horaud category:cs.CV cs.LG stat.ML published:2015-09-04 summary:Data clustering has received a lot of attention and numerous methods,algorithms and software packages are available. Among these techniques,parametric finite-mixture models play a central role due to their interestingmathematical properties and to the existence of maximum-likelihood estimatorsbased on expectation-maximization (EM). In this paper we propose a new mixturemodel that associates a weight with each observed point. We introduce theweighted-data Gaussian mixture and we derive two EM algorithms. The first oneconsiders a fixed weight for each observation. The second one treats eachweight as a random variable following a gamma distribution. We propose a modelselection method based on a minimum message length criterion, provide a weightinitialization strategy, and validate the proposed algorithms by comparing themwith several state of the art parametric and non-parametric clusteringtechniques. We also demonstrate the effectiveness and robustness of theproposed clustering technique in the presence of heterogeneous data, namelyaudio-visual scene analysis.
arxiv-13200-82 | Coordinate Descent Methods for Symmetric Nonnegative Matrix Factorization | http://arxiv.org/abs/1509.01404 | author:Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon category:cs.NA cs.CV cs.LG math.OC stat.ML published:2015-09-04 summary:Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrixfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMFcan be used for data analysis and in particular for various clustering tasks.In this paper, we propose simple and very efficient coordinate descent schemesto solve this problem, and that can handle large and sparse input matrices. Theeffectiveness of our methods is illustrated on synthetic and real-world datasets, and we show that they perform favorably compared to recentstate-of-the-art methods.
arxiv-13200-83 | Character-level Convolutional Networks for Text Classification | http://arxiv.org/abs/1509.01626 | author:Xiang Zhang, Junbo Zhao, Yann LeCun category:cs.LG cs.CL published:2015-09-04 summary:This article offers an empirical exploration on the use of character-levelconvolutional networks (ConvNets) for text classification. We constructedseveral large-scale datasets to show that character-level convolutionalnetworks could achieve state-of-the-art or competitive results. Comparisons areoffered against traditional models such as bag of words, n-grams and theirTFIDF variants, and deep learning models such as word-based ConvNets andrecurrent neural networks.
arxiv-13200-84 | Deep Broad Learning - Big Models for Big Data | http://arxiv.org/abs/1509.01346 | author:Nayyar A. Zaidi, Geoffrey I. Webb, Mark J. Carman, Francois Petitjean category:cs.LG published:2015-09-04 summary:Deep learning has demonstrated the power of detailed modeling of complexhigh-order (multivariate) interactions in data. For some learning tasks thereis power in learning models that are not only Deep but also Broad. By Broad, wemean models that incorporate evidence from large numbers of features. This isof especial value in applications where many different features andcombinations of features all carry small amounts of information about theclass. The most accurate models will integrate all that information. In thispaper, we propose an algorithm for Deep Broad Learning called DBL. The proposedalgorithm has a tunable parameter $n$, that specifies the depth of the model.It provides straightforward paths towards out-of-core learning for large data.We demonstrate that DBL learns models from large quantities of data withaccuracy that is highly competitive with the state-of-the-art.
arxiv-13200-85 | Efficient Sampling for k-Determinantal Point Processes | http://arxiv.org/abs/1509.01618 | author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:cs.LG published:2015-09-04 summary:Determinantal Point Processes (DPPs) provide probabilistic models overdiscrete sets of items that help model repulsion and diversity. Applicabilityof DPPs to large sets of data is, however, hindered by the expensive matrixoperations involved, especially when sampling. We therefore propose a newefficient approximate two-stage sampling algorithm for discrete k-DPPs. Asopposed to previous approximations, our algorithm aims at minimizing thevariational distance to the original distribution. Experiments indicate thatthe resulting sampling algorithm works well on large data and yields moreaccurate samples than previous approaches.
arxiv-13200-86 | A Dataset for Improved RGBD-based Object Detection and Pose Estimation for Warehouse Pick-and-Place | http://arxiv.org/abs/1509.01277 | author:Colin Rennie, Rahul Shome, Kostas E. Bekris, Alberto F. De Souza category:cs.CV cs.RO published:2015-09-03 summary:An important logistics application of robotics involves manipulators thatpick-and-place objects placed in warehouse shelves. A critical aspect of thistask corre- sponds to detecting the pose of a known object in the shelf usingvisual data. Solving this problem can be assisted by the use of an RGB-Dsensor, which also provides depth information beyond visual data. Nevertheless,it remains a challenging problem since multiple issues need to be addressed,such as low illumination inside shelves, clutter, texture-less and reflectiveobjects as well as the limitations of depth sensors. This paper provides a newrich data set for advancing the state-of-the-art in RGBD- based 3D object poseestimation, which is focused on the challenges that arise when solvingwarehouse pick- and-place tasks. The publicly available data set includesthousands of images and corresponding ground truth data for the objects usedduring the first Amazon Picking Challenge at different poses and clutterconditions. Each image is accompanied with ground truth information to assistin the evaluation of algorithms for object detection. To show the utility ofthe data set, a recent algorithm for RGBD-based pose estimation is evaluated inthis paper. Based on the measured performance of the algorithm on the data set,various modifications and improvements are applied to increase the accuracy ofdetection. These steps can be easily applied to a variety of differentmethodologies for object pose detection and improve performance in the domainof warehouse pick-and-place.
arxiv-13200-87 | Machine Learning Model of the Swift/BAT Trigger Algorithm for Long GRB Population Studies | http://arxiv.org/abs/1509.01228 | author:Philip B Graff, Amy Y Lien, John G Baker, Takanori Sakamoto category:astro-ph.HE stat.ML published:2015-09-03 summary:To draw inferences about gamma-ray burst (GRB) source populations based onSwift observations, it is essential to understand the detection efficiency ofthe Swift burst alert telescope (BAT). This study considers the problem ofmodeling the Swift/BAT triggering algorithm for long GRBs, a computationallyexpensive procedure, and models it using machine learning algorithms. A largesample of simulated GRBs from Lien 2014 is used to train various models: randomforests, boosted decision trees (with AdaBoost), support vector machines, andartificial neural networks. The best models have accuracies of $\gtrsim97\%$($\lesssim 3\%$ error), which is a significant improvement on a cut in GRB fluxwhich has an accuracy of $89.6\%$ ($10.4\%$ error). These models are then usedto measure the detection efficiency of Swift as a function of redshift $z$,which is used to perform Bayesian parameter estimation on the GRB ratedistribution. We find a local GRB rate density of $n_0 \sim0.48^{+0.41}_{-0.23} \ {\rm Gpc}^{-3} {\rm yr}^{-1}$ with power-law indices of$n_1 \sim 1.7^{+0.6}_{-0.5}$ and $n_2 \sim -5.9^{+5.7}_{-0.1}$ for GRBs aboveand below a break point of $z_1 \sim 6.8^{+2.8}_{-3.2}$. This methodology isable to improve upon earlier studies by more accurately modeling Swiftdetection and using this for fully Bayesian model fitting. The code used inthis is analysis is publicly available online(https://github.com/PBGraff/SwiftGRB_PEanalysis).
arxiv-13200-88 | Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias | http://arxiv.org/abs/1509.01004 | author:Yohei Kondo, Kohei Hayashi, Shin-ichi Maeda category:stat.ML cs.LG published:2015-09-03 summary:A common strategy for sparse linear regression is to introduceregularization, which eliminates irrelevant features by letting thecorresponding weights be zeros. However, regularization often shrinks theestimator for relevant features, which leads to incorrect feature selection.Motivated by the above-mentioned issue, we propose Bayesian masking (BM), asparse estimation method which imposes no regularization on the weights. Thekey concept of BM is to introduce binary latent variables that randomly maskfeatures. Estimating the masking rates determines the relevance of the featuresautomatically. We derive a variational Bayesian inference algorithm thatmaximizes the lower bound of the factorized information criterion (FIC), whichis a recently developed asymptotic criterion for evaluating the marginallog-likelihood. In addition, we propose reparametrization to accelerate theconvergence of the derived algorithm. Finally, we show that BM outperformsLasso and automatic relevance determination (ARD) in terms of thesparsity-shrinkage trade-off.
arxiv-13200-89 | Train faster, generalize better: Stability of stochastic gradient descent | http://arxiv.org/abs/1509.01240 | author:Moritz Hardt, Benjamin Recht, Yoram Singer category:cs.LG math.OC stat.ML published:2015-09-03 summary:We show that parametric models trained by a stochastic gradient method (SGM)with few iterations have vanishing generalization error. We prove our resultsby arguing that SGM is algorithmically stable in the sense of Bousquet andElisseeff. Our analysis only employs elementary tools from convex andcontinuous optimization. We derive stability bounds for both convex andnon-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for whymultiple epochs of stochastic gradient methods generalize well in practice. Inthe non-convex case, we give a new interpretation of common practices in neuralnetworks, and formally show that popular techniques for training large deepmodels are indeed stability-promoting. Our findings conceptually underscore theimportance of reducing training time beyond its obvious benefit.
arxiv-13200-90 | Community Detection in Networks with Node Features | http://arxiv.org/abs/1509.01173 | author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML cs.SI physics.soc-ph published:2015-09-03 summary:Many methods have been proposed for community detection in networks, but mostof them do not take into account additional information on the nodes that isoften available in practice. In this paper, we propose a new joint communitydetection criterion that uses both the network edge information and the nodefeatures to detect community structures. One advantage our method has overexisting joint detection approaches is the flexibility of learning the impactof different features which may differ across communities. Another advantage isthe flexibility of choosing the amount of influence the feature information hason communities. The method is asymptotically consistent under the block modelwith additional assumptions on the feature distributions, and performs well onsimulated and real networks.
arxiv-13200-91 | Semi-described and semi-supervised learning with Gaussian processes | http://arxiv.org/abs/1509.01168 | author:Andreas Damianou, Neil D. Lawrence category:stat.ML cs.AI cs.LG math.PR 60G15, 58E30 G.3; I.2.6 published:2015-09-03 summary:Propagating input uncertainty through non-linear Gaussian process (GP)mappings is intractable. This hinders the task of training GPs using uncertainand partially observed inputs. In this paper we refer to this task as"semi-described learning". We then introduce a GP framework that solves both,the semi-described and the semi-supervised learning problems (where missingvalues occur in the outputs). Auto-regressive state space simulation is alsorecognised as a special case of semi-described learning. To achieve our goal wedevelop variational methods for handling semi-described inputs in GPs, andcouple them with algorithms that allow for imputing the missing values whiletreating the uncertainty in a principled, Bayesian manner. Extensiveexperiments on simulated and real-world data study the problems of iterativeforecasting and regression/classification with missing values. The resultssuggest that the principled propagation of uncertainty stemming from ourframework can significantly improve performance in these tasks.
arxiv-13200-92 | On-the-Fly Learning in a Perpetual Learning Machine | http://arxiv.org/abs/1509.00913 | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-09-03 summary:Despite the promise of brain-inspired machine learning, deep neural networks(DNN) have frustratingly failed to bridge the deceptively large gap betweenlearning and memory. Here, we introduce a Perpetual Learning Machine; a newtype of DNN that is capable of brain-like dynamic 'on the fly' learning becauseit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.Thus, we provide the means to unify learning and memory within a machinelearning framework. We also explore the elegant duality of abstraction andsynthesis: the Yin and Yang of deep learning.
arxiv-13200-93 | The influence of Chunking on Dependency Crossing and Distance | http://arxiv.org/abs/1509.01310 | author:Qian Lu, Chunshan Xu, Haitao Liu category:cs.CL published:2015-09-03 summary:This paper hypothesizes that chunking plays important role in reducingdependency distance and dependency crossings. Computer simulations, whencompared with natural languages,show that chunking reduces mean dependencydistance (MDD) of a linear sequence of nodes (constrained by continuity orprojectivity) to that of natural languages. More interestingly, chunking alonebrings about less dependency crossings as well, though having failed to reducethem, to such rarity as found in human languages. These results suggest thatchunking may play a vital role in the minimization of dependency distance, anda somewhat contributing role in the rarity of dependency crossing. In addition,the results point to a possibility that the rarity of dependency crossings isnot a mere side-effect of minimization of dependency distance, but a linguisticphenomenon with its own motivations.
arxiv-13200-94 | Incremental Active Opinion Learning Over a Stream of Opinionated Documents | http://arxiv.org/abs/1509.01288 | author:Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou category:cs.IR cs.CL cs.LG published:2015-09-03 summary:Applications that learn from opinionated documents, like tweets or productreviews, face two challenges. First, the opinionated documents constitute anevolving stream, where both the author's attitude and the vocabulary itself maychange. Second, labels of documents are scarce and labels of words areunreliable, because the sentiment of a word depends on the (unknown) context inthe author's mind. Most of the research on mining over opinionated streamsfocuses on the first aspect of the problem, whereas for the second a continuoussupply of labels from the stream is assumed. Such an assumption though isutopian as the stream is infinite and the labeling cost is prohibitive. To thisend, we investigate the potential of active stream learning algorithms that askfor labels on demand. Our proposed ACOSTREAM 1 approach works with limitedlabels: it uses an initial seed of labeled documents, occasionally requestsadditional labels for documents from the human expert and incrementally adaptsto the underlying stream while exploiting the available labeled documents. Inits core, ACOSTREAM consists of a MNB classifier coupled with "sampling"strategies for requesting class labels for new unlabeled documents. In theexperiments, we evaluate the classifier performance over time by varying: (a)the class distribution of the opinionated stream, while assuming that the setof the words in the vocabulary is fixed but their polarities may change withthe class distribution; and (b) the number of unknown words arriving at eachmoment, while the class polarity may also change. Our results show that activelearning on a stream of opinionated documents, delivers good performance whilerequiring a small selection of labels
arxiv-13200-95 | Image Classification with Rejection using Contextual Information | http://arxiv.org/abs/1509.01287 | author:Filipe Condessa, José Bioucas-Dias, Carlos Castro, John Ozolek, Jelena Kovačević category:cs.CV 68T10 published:2015-09-03 summary:We introduce a new supervised algorithm for image classification withrejection using multiscale contextual information. Rejection is desired inimage-classification applications that require a robust classifier but not theclassification of the entire image. The proposed algorithm combines local andmultiscale contextual information with rejection, improving the classificationperformance. As a probabilistic model for classification, we adopt amultinomial logistic regression. The concept of rejection with contextualinformation is implemented by modeling the classification problem as an energyminimization problem over a graph representing local and multiscalesimilarities of the image. The rejection is introduced through an energy dataterm associated with the classification risk and the contextual informationthrough an energy smoothness term associated with the local and multiscalesimilarities within the image. We illustrate the proposed method on theclassification of images of H&E-stained teratoma tissues.
arxiv-13200-96 | Encoding Prior Knowledge with Eigenword Embeddings | http://arxiv.org/abs/1509.01007 | author:Dominique Osborne, Shashi Narayan, Shay B. Cohen category:cs.CL published:2015-09-03 summary:Canonical correlation analysis (CCA) is a method for reducing the dimensionof data represented using two views. It has been previously used to derive wordembeddings, where one view indicates a word, and the other view indicates itscontext. We describe a way to incorporate prior knowledge into CCA, give atheoretical justification for it, and test it by deriving word embeddings andevaluating them on a myriad of datasets.
arxiv-13200-97 | Probabilistic Neural Network Training for Semi-Supervised Classifiers | http://arxiv.org/abs/1509.01271 | author:Hamidreza Farhidzadeh category:cs.LG published:2015-09-03 summary:In this paper, we propose another version of help-training approach byemploying a Probabilistic Neural Network (PNN) that improves the performance ofthe main discriminative classifier in the semi-supervised strategy. Weintroduce the PNN-training algorithm and use it for training the support vectormachine (SVM) with a few numbers of labeled data and a large number ofunlabeled data. We try to find the best labels for unlabeled data and then useSVM to enhance the classification rate. We test our method on two famousbenchmarks and show the efficiency of our method in comparison with perviousmethods.
arxiv-13200-98 | Fast Clustering and Topic Modeling Based on Rank-2 Nonnegative Matrix Factorization | http://arxiv.org/abs/1509.01208 | author:Da Kuang, Barry Drake, Haesun Park category:cs.LG cs.IR cs.NA F.2.1; H.3.3 published:2015-09-03 summary:The importance of unsupervised clustering and topic modeling is wellrecognized with ever-increasing volumes of text data. In this paper, we proposea fast method for hierarchical clustering and topic modeling called HierNMF2.Our method is based on fast Rank-2 nonnegative matrix factorization (NMF) thatperforms binary clustering and an efficient node splitting rule. Furtherutilizing the final leaf nodes generated in HierNMF2 and the idea ofnonnegative least squares fitting, we propose a new clustering/topic modelingmethod called FlatNMF2 that recovers a flat clustering/topic modeling result ina very simple yet significantly more effective way than any other existingmethods. We implement highly optimized open source software in C++ for bothHierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topicmodeling of document data sets. Substantial experimental tests are presented that illustrate significantimprovements both in computational time as well as quality of solutions. Wecompare our methods to other clustering methods including K-means, standardNMF, and CLUTO, and also topic modeling methods including latent Dirichletallocation (LDA) and recently proposed algorithms for NMF with separabilityconstraints. Overall, we present efficient tools for analyzing large-scale datasets, and techniques that can be generalized to many other data analyticsproblem domains.
arxiv-13200-99 | Training of CC4 Neural Network with Spread Unary Coding | http://arxiv.org/abs/1509.01126 | author:Pushpa Sree Potluri category:cs.NE published:2015-09-03 summary:This paper adapts the corner classification algorithm (CC4) to train theneural networks using spread unary inputs. This is an important problem asspread unary appears to be at the basis of data representation in biologicallearning. The modified CC4 algorithm is tested using the pattern classificationexperiment and the results are found to be good. Specifically, we show that thenumber of misclassified points is not particularly sensitive to the chosenradius of generalization.
arxiv-13200-100 | Machine Learning Methods to Analyze Arabidopsis Thaliana Plant Root Growth | http://arxiv.org/abs/1509.01270 | author:Hamidreza Farhidzadeh category:cs.LG published:2015-09-03 summary:One of the challenging problems in biology is to classify plants based ontheir reaction on genetic mutation. Arabidopsis Thaliana is a plant that is sointeresting, because its genetic structure has some similarities with that ofhuman beings. Biologists classify the type of this plant to mutated and notmutated (wild) types. Phenotypic analysis of these types is a time-consumingand costly effort by individuals. In this paper, we propose a modified featureextraction step by using velocity and acceleration of root growth. In thesecond step, for plant classification, we employed different Support VectorMachine (SVM) kernels and two hybrid systems of neural networks. Gated NegativeCorrelation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE)are two ensemble methods based on complementary feature of classicalclassifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL).The hybrid systems conserve of advantages and decrease the effects ofdisadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improvethe efficiency of classical classifiers, however, some SVM kernels function hasbetter performance than classifiers based on neural network ensemble method.Moreover, kernels consume less time to obtain a classification rate.
arxiv-13200-101 | Vision-Based Road Detection using Contextual Blocks | http://arxiv.org/abs/1509.01122 | author:Caio César Teodoro Mendes, Vincent Frémont, Denis Fernando Wolf category:cs.CV published:2015-09-03 summary:Road detection is a fundamental task in autonomous navigation systems. Inthis paper, we consider the case of monocular road detection, where images aresegmented into road and non-road regions. Our starting point is the well-knownmachine learning approach, in which a classifier is trained to distinguish roadand non-road regions based on hand-labeled images. We proceed by introducingthe use of "contextual blocks" as an efficient way of providing contextualinformation to the classifier. Overall, the proposed methodology, including itsimage feature selection and classifier, was conceived with computational costin mind, leaving room for optimized implementations. Regarding experiments, weperform a sensible evaluation of each phase and feature subset that composesour system. The results show a great benefit from using contextual blocks anddemonstrate their computational efficiency. Finally, we submit our results tothe KITTI road detection benchmark achieving scores comparable with state ofthe art methods.
arxiv-13200-102 | A tree-based kernel for graphs with continuous attributes | http://arxiv.org/abs/1509.01116 | author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG published:2015-09-03 summary:The availability of graph data with node attributes that can be eitherdiscrete or real-valued is constantly increasing. While existing kernel methodsare effective techniques for dealing with graphs having discrete node labels,their adaptation to non-discrete or continuous node attributes has beenlimited, mainly for computational issues. Recently, a few kernels especiallytailored for this domain, have been proposed. In order to alleviate thecomputational problems, the size of the feature space of such kernels tend tobe smaller than the ones of the kernels for discrete node attributes. However,such choice might have a negative impact on the predictive performance. In thispaper, we propose a graph kernel for complex and continuous nodes' attributes,whose features are tree structures extracted from specific graph visits.Experimental results obtained on real-world datasets show that the(approximated version of the) proposed kernel is comparable with currentstate-of-the-art kernels in terms of classification accuracy while requiringshorter running times.
arxiv-13200-103 | A Novice Guide towards Human Motion Analysis and Understanding | http://arxiv.org/abs/1509.01074 | author:Ahmed Nabil Mohamed category:cs.CV published:2015-09-03 summary:Human motion analysis and understanding has been, and is still, the focus ofattention of many disciplines which is considered an obvious indicator of thewide and massive importance of the subject. The purpose of this article is toshed some light on this very important subject, so it can be a good insight fora novice computer vision researcher in this field by providing him/her with awealth of knowledge about the subject covering many directions. There are twomain contributions of this article. The first one investigates various aspectsof some disciplines (e.g., arts, philosophy, psychology, and neuroscience) thatare interested in the subject and review some of their contributions stressingon those that can be useful for computer vision researchers. Moreover, manyexamples are illustrated to indicate the benefits of integrating concepts andresults among different disciplines. The second contribution is concerned withthe subject from the computer vision aspect where we discuss the followingissues. First, we explore many demanding and promising applications to revealthe wide and massive importance of the field. Second, we list various types ofsensors that may be used for acquiring various data. Third, we review differenttaxonomies used for classifying motions. Fourth, we review various processesinvolved in motion analysis. Fifth, we exhibit how different surveys arestructured. Sixth, we examine many of the most cited and recent reviews in thefield that have been published during the past two decades to reveal variousapproaches used for implementing different stages of the problem and refer tovarious algorithms and their suitability for different situations. Moreover, weprovide a long list of public datasets and discuss briefly some examples ofthese datasets. Finally, we provide a general discussion of the subject fromthe aspect of computer vision.
arxiv-13200-104 | Training a Restricted Boltzmann Machine for Classification by Labeling Model Samples | http://arxiv.org/abs/1509.01053 | author:Malte Probst, Franz Rothlauf category:cs.LG published:2015-09-03 summary:We propose an alternative method for training a classification model. Usingthe MNIST set of handwritten digits and Restricted Boltzmann Machines, it ispossible to reach a classification performance competitive to semi-supervisedlearning if we first train a model in an unsupervised fashion on unlabeled dataonly, and then manually add labels to model samples instead of training datasamples with the help of a GUI. This approach can benefit from the fact thatmodel samples can be presented to the human labeler in a video-like fashion,resulting in a higher number of labeled examples. Also, after some initialtraining, hard-to-classify examples can be distinguished from easy onesautomatically, saving manual work.
arxiv-13200-105 | Generating Weather Forecast Texts with Case Based Reasoning | http://arxiv.org/abs/1509.01023 | author:Ibrahim Adeyanju category:cs.AI cs.CL published:2015-09-03 summary:Several techniques have been used to generate weather forecast texts. In thispaper, case based reasoning (CBR) is proposed for weather forecast textgeneration because similar weather conditions occur over time and should havesimilar forecast texts. CBR-METEO, a system for generating weather forecasttexts was developed using a generic framework (jCOLIBRI) which provides modulesfor the standard components of the CBR architecture. The advantage in a CBRapproach is that systems can be built in minimal time with far less humaneffort after initial consultation with experts. The approach depends heavily onthe goodness of the retrieval and revision components of the CBR process. Weevaluated CBRMETEO with NIST, an automated metric which has been shown tocorrelate well with human judgements for this domain. The system showscomparable performance with other NLG systems that perform the same task.
arxiv-13200-106 | Sampling-based Causal Inference in Cue Combination and its Neural Implementation | http://arxiv.org/abs/1509.00998 | author:Zhaofei Yu, Feng Chen, Jianwu Dong, Qionghai Dai category:cs.NE q-bio.NC published:2015-09-03 summary:Causal inference in cue combination is to decide whether the cues have asingle cause or multiple causes. Although the Bayesian causal inference modelexplains the problem of causal inference in cue combination successfully, howcausal inference in cue combination could be implemented by neural circuits, isunclear. The existing method based on calculating log posterior ratio withvariable elimination has the problem of being unrealistic and task-specific. Inthis paper, we take advantages of the special structure of the Bayesian causalinference model and propose a hierarchical inference algorithm based onimportance sampling. A simple neural circuit is designed to implement theproposed inference algorithm. Theoretical analyses and experimental resultsdemonstrate that our algorithm converges to the accurate value as the samplesize goes to infinite. Moreover, the neural circuit we design can be easilygeneralized to implement inference for other problems, such as themulti-stimuli cause inference and the same-different judgment.
arxiv-13200-107 | Sequential Design for Ranking Response Surfaces | http://arxiv.org/abs/1509.00980 | author:Ruimeng Hu, Mike Ludkovski category:stat.ML q-fin.CP stat.CO published:2015-09-03 summary:We propose and analyze sequential design methods for the problem of rankingseveral response surfaces. Namely, given $L \ge 2$ response surfaces over acontinuous input space $\cal X$, the aim is to efficiently find the index ofthe minimal response across the entire $\cal X$. The response surfaces are notknown and have to be noisily sampled one-at-a-time. This setting is motivatedby stochastic control applications and requires joint experimental design bothin space and response-index dimensions. To generate sequential designheuristics we investigate stepwise uncertainty reduction approaches, as well assampling based on posterior classification complexity. We also make connectionsbetween our continuous-input formulation and the discrete framework of pureregret in multi-armed bandits. To model the response surfaces we utilizekriging surrogates. Several numerical examples using both synthetic data and anepidemics control problem are provided to illustrate our approach and theefficacy of respective adaptive designs.
arxiv-13200-108 | A Reconfigurable Mixed-signal Implementation of a Neuromorphic ADC | http://arxiv.org/abs/1509.00967 | author:Ying Xu, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Runchun Wang, Andre van Schaik category:cs.NE published:2015-09-03 summary:We present a neuromorphic Analogue-to-Digital Converter (ADC), which usesintegrate-and-fire (I&F) neurons as the encoders of the analogue signal, withmodulated inhibitions to decohere the neuronal spikes trains. The architectureconsists of an analogue chip and a control module. The analogue chip comprisestwo scan chains and a twodimensional integrate-and-fire neuronal array.Individual neurons are accessed via the chains one by one without any encoderdecoder or arbiter. The control module is implemented on an FPGA (FieldProgrammable Gate Array), which sends scan enable signals to the scan chainsand controls the inhibition for individual neurons. Since the control module isimplemented on an FPGA, it can be easily reconfigured. Additionally, we proposea pulse width modulation methodology for the lateral inhibition, which makesuse of different pulse widths indicating different strengths of inhibition foreach individual neuron to decohere neuronal spikes. Software simulations inthis paper tested the robustness of the proposed ADC architecture to fixedrandom noise. A circuit simulation using ten neurons shows the performance andthe feasibility of the architecture.
arxiv-13200-109 | On TimeML-Compliant Temporal Expression Extraction in Turkish | http://arxiv.org/abs/1509.00963 | author:Dilek Küçük, Doğan Küçük category:cs.CL published:2015-09-03 summary:It is commonly acknowledged that temporal expression extractors are importantcomponents of larger natural language processing systems like informationretrieval and question answering systems. Extraction and normalization oftemporal expressions in Turkish has not been given attention so far except theextraction of some date and time expressions within the course of named entityrecognition. As TimeML is the current standard of temporal expression and eventannotation in natural language texts, in this paper, we present an analysis oftemporal expressions in Turkish based on the related TimeML classification(i.e., date, time, duration, and set expressions). We have created a lexiconfor Turkish temporal expressions and devised considerably wide-coveragepatterns using the lexical classes as the building blocks. We believe that theproposed patterns, together with convenient normalization rules, can be readilyused by prospective temporal expression extraction tools for Turkish.
arxiv-13200-110 | A compact aVLSI conductance-based silicon neuron | http://arxiv.org/abs/1509.00962 | author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE published:2015-09-03 summary:We present an analogue Very Large Scale Integration (aVLSI) implementationthat uses first-order lowpass filters to implement a conductance-based siliconneuron for high-speed neuromorphic systems. The aVLSI neuron consists of a soma(cell body) and a single synapse, which is capable of linearly summing both theexcitatory and inhibitory postsynaptic potentials (EPSP and IPSP) generated bythe spikes arriving from different sources. Rather than biasing the siliconneuron with different parameters for different spiking patterns, as istypically done, we provide digital control signals, generated by an FPGA, tothe silicon neuron to obtain different spiking behaviours. The proposed neuronis only ~26.5 um2 in the IBM 130nm process and thus can be integrated at veryhigh density. Circuit simulations show that this neuron can emulate differentspiking behaviours observed in biological neurons.
arxiv-13200-111 | Manipulated Object Proposal: A Discriminative Object Extraction and Feature Fusion Framework for First-Person Daily Activity Recognition | http://arxiv.org/abs/1509.00651 | author:Changzhi Luo, Bingbing Ni, Jun Yuan, Jianfeng Wang, Shuicheng Yan, Meng Wang category:cs.CV published:2015-09-02 summary:Detecting and recognizing objects interacting with humans lie in the centerof first-person (egocentric) daily activity recognition. However, due to noisycamera motion and frequent changes in viewpoint and scale, most of the previousegocentric action recognition methods fail to capture and model highlydiscriminative object features. In this work, we propose a novel pipeline forfirst-person daily activity recognition, aiming at more discriminative objectfeature representation and object-motion feature fusion. Our object featureextraction and representation pipeline is inspired by the recent success ofobject hypotheses and deep convolutional neural network based detectionframeworks. Our key contribution is a simple yet effective manipulated objectproposal generation scheme. This scheme leverages motion cues such as motionboundary and motion magnitude (in contrast, camera motion is usually consideredas "noise" for most previous methods) to generate a more compact anddiscriminative set of object proposals, which are more closely related to theobjects which are being manipulated. Then, we learn more discriminative objectdetectors from these manipulated object proposals based on region-basedconvolutional neural network (R-CNN). Meanwhile, we develop a network basedfeature fusion scheme which better combines object and motion features. We showin experiments that the proposed framework significantly outperforms thestate-of-the-art recognition performance on a challenging first-person dailyactivity benchmark.
arxiv-13200-112 | Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging | http://arxiv.org/abs/1509.00816 | author:Suren Jayasuriya, Adithya Pediredla, Sriram Sivaramakrishnan, Alyosha Molnar, Ashok Veeraraghavan category:cs.CV published:2015-09-02 summary:A variety of techniques such as light field, structured illumination, andtime-of-flight (TOF) are commonly used for depth acquisition in consumerimaging, robotics and many other applications. Unfortunately, each techniquesuffers from its individual limitations preventing robust depth sensing. Inthis paper, we explore the strengths and weaknesses of combining light fieldand time-of-flight imaging, particularly the feasibility of an on-chipimplementation as a single hybrid depth sensor. We refer to this combination asdepth field imaging. Depth fields combine light field advantages such assynthetic aperture refocusing with TOF imaging advantages such as high depthresolution and coded signal processing to resolve multipath interference. Weshow applications including synthesizing virtual apertures for TOF imaging,improved depth mapping through partial and scattering occluders, and singlefrequency TOF phase unwrapping. Utilizing space, angle, and temporal coding,depth fields can improve depth sensing in the wild and generate new insightsinto the dimensions of light's plenoptic function.
arxiv-13200-113 | Finding Near-Optimal Independent Sets at Scale | http://arxiv.org/abs/1509.00764 | author:Sebastian Lamm, Peter Sanders, Christian Schulz, Darren Strash, Renato F. Werneck category:cs.DS cs.NE cs.SI F.2.2; G.2.2 published:2015-09-02 summary:The independent set problem is NP-hard and particularly difficult to solve inlarge sparse graphs. In this work, we develop an advanced evolutionaryalgorithm, which incorporates kernelization techniques to compute largeindependent sets in huge sparse networks. A recent exact algorithm has shownthat large networks can be solved exactly by employing a branch-and-reducetechnique that recursively kernelizes the graph and performs branching.However, one major drawback of their algorithm is that, for huge graphs,branching still can take exponential time. To avoid this problem, werecursively choose vertices that are likely to be in a large independent set(using an evolutionary approach), then further kernelize the graph. We showthat identifying and removing vertices likely to be in large independent setsopens up the reduction space---which not only speeds up the computation oflarge independent sets drastically, but also enables us to compute high-qualityindependent sets on much larger instances than previously reported in theliterature.
arxiv-13200-114 | DAG-Recurrent Neural Networks For Scene Labeling | http://arxiv.org/abs/1509.00552 | author:Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang category:cs.CV published:2015-09-02 summary:In image labeling, local representations for image units are usuallygenerated from their surrounding image patches, thus long-range contextualinformation is not effectively encoded. In this paper, we introduce recurrentneural networks (RNNs) to address this issue. Specifically, directed acyclicgraph RNNs (DAG-RNNs) are proposed to process DAG-structured images, whichenables the network to model long-range semantic dependencies among imageunits. Our DAG-RNNs are capable of tremendously enhancing the discriminativepower of local representations, which significantly benefits the localclassification. Meanwhile, we propose a novel class weighting function thatattends to rare classes, which phenomenally boosts the recognition accuracy fornon-frequent classes. Integrating with convolution and deconvolution layers,our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow,CamVid and Barcelona benchmarks.
arxiv-13200-115 | On Transitive Consistency for Linear Invertible Transformations between Euclidean Coordinate Systems | http://arxiv.org/abs/1509.00728 | author:Johan Thunberg, Florian Bernard, Jorge Goncalves category:math.OC cs.CV cs.MA cs.NA stat.ML published:2015-09-02 summary:Transitive consistency is an intrinsic property for collections of linearinvertible transformations between Euclidean coordinate frames. In practice,when the transformations are estimated from data, this property is lacking.This work addresses the problem of synchronizing transformations that are nottransitively consistent. Once the transformations have been synchronized, theysatisfy the transitive consistency condition - a transformation from frame $A$to frame $C$ is equal to the composite transformation of first transforming Ato B and then transforming B to C. The coordinate frames correspond to nodes ina graph and the transformations correspond to edges in the same graph. Twodirect or centralized synchronization methods are presented for different graphtopologies; the first one for quasi-strongly connected graphs, and the secondone for connected graphs. As an extension of the second method, an iterativeGauss-Newton method is presented, which is later adapted to the case of affineand Euclidean transformations. Two distributed synchronization methods are alsopresented for orthogonal matrices, which can be seen as distributed versions ofthe two direct or centralized methods; they are similar in nature to standardconsensus protocols used for distributed averaging. When the transformationsare orthogonal matrices, a bound on the optimality gap can be computed.Simulations show that the gap is almost right, even for noise large inmagnitude. This work also contributes on a theoretical level by providinglinear algebraic relationships for transitively consistent transformations. Oneof the benefits of the proposed methods is their simplicity - basic linearalgebraic methods are used, e.g., the Singular Value Decomposition (SVD). For awide range of parameter settings, the methods are numerically validated.
arxiv-13200-116 | Heavy-tailed Independent Component Analysis | http://arxiv.org/abs/1509.00727 | author:Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher category:cs.LG math.ST stat.CO stat.ML stat.TH published:2015-09-02 summary:Independent component analysis (ICA) is the problem of efficiently recoveringa matrix $A \in \mathbb{R}^{n\times n}$ from i.i.d. observations of $X=AS$where $S \in \mathbb{R}^n$ is a random vector with mutually independentcoordinates. This problem has been intensively studied, but all existingefficient algorithms with provable guarantees require that the coordinates$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problemwhere we do not make this assumption, about the second moment. This problemalso has received considerable attention in the applied literature. In thepresent work, we first give a provably efficient algorithm that works under theassumption that for constant $\gamma > 0$, each $S_i$ has finite$(1+\gamma)$-moment, thus substantially weakening the moment requirementcondition for the ICA problem to be solvable. We then give an algorithm thatworks under the assumption that matrix $A$ has orthogonal columns but requiresno moment assumptions. Our techniques draw ideas from convex geometry andexploit standard properties of the multivariate spherical Gaussian distributionin a novel way.
arxiv-13200-117 | Dictionary based Approach to Edge Detection | http://arxiv.org/abs/1509.00714 | author:Nitish Chandra, Kedar Khare category:cs.CV published:2015-09-02 summary:Edge detection is a very essential part of image processing, as quality andaccuracy of detection determines the success of further processing. We havedeveloped a new self learning technique for edge detection using dictionarycomprised of eigenfilters constructed using features of the input image. Thedictionary based method eliminates the need of pre or post processing of theimage and accounts for noise, blurriness, class of image and variation ofillumination during the detection process itself. Since, this method depends onthe characteristics of the image, the new technique can detect edges moreaccurately and capture greater detail than existing algorithms such as Sobel,Prewitt Laplacian of Gaussian, Canny method etc which use generic filters andoperators. We have demonstrated its application on various classes of imagessuch as text, face, barcodes, traffic and cell images. An application of thistechnique to cell counting in a microscopic image is also presented.
arxiv-13200-118 | Analysis of Communication Pattern with Scammers in Enron Corpus | http://arxiv.org/abs/1509.00705 | author:Dinesh Balaji Sashikanth category:cs.CL published:2015-09-02 summary:This paper is an exploratory analysis into fraud detection taking Enron emailcorpus as the case study. The paper posits conclusions like strict servitudeand unquestionable faith among employees as breeding grounds for sham amonghigher executives. We also try to infer on the nature of communication betweenfraudulent employees and between non- fraudulent-fraudulent employees
arxiv-13200-119 | A hybrid COA-DEA method for solving multi-objective problems | http://arxiv.org/abs/1509.00595 | author:Mahdi Gorjestani, Elham Shadkam, Mehdi Parvizi, Sajedeh Aminzadegan category:math.OC cs.NE published:2015-09-02 summary:The Cuckoo optimization algorithm (COA) is developed for solvingsingle-objective problems and it cannot be used for solving multi-objectiveproblems. So the multi-objective cuckoo optimization algorithm based on dataenvelopment analysis (DEA) is developed in this paper and it can gain theefficient Pareto frontiers. This algorithm is presented by the CCR model of DEAand the output-oriented approach of it. The selection criterion is higherefficiency for next iteration of the proposed hybrid method. So the profitfunction of the COA is replaced by the efficiency value that is obtained fromDEA. This algorithm is compared with other methods using some test problems.The results shows using COA and DEA approach for solving multi-objectiveproblems increases the speed and the accuracy of the generated solutions.
arxiv-13200-120 | A DEEP analysis of the META-DES framework for dynamic selection of ensemble of classifiers | http://arxiv.org/abs/1509.00825 | author:Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti category:cs.LG stat.ML published:2015-09-02 summary:Dynamic ensemble selection (DES) techniques work by estimating the level ofcompetence of each classifier from a pool of classifiers. Only the mostcompetent ones are selected to classify a given test sample. Hence, the keyissue in DES is the criterion used to estimate the level of competence of theclassifiers in predicting the label of a given test sample. In order to performa more robust ensemble selection, we proposed the META-DES framework usingmeta-learning, where multiple criteria are encoded as meta-features and arepassed down to a meta-classifier that is trained to estimate the competencelevel of a given classifier. In this technical report, we present astep-by-step analysis of each phase of the framework during training and test.We show how each set of meta-features is extracted as well as their impact onthe estimation of the competence level of the base classifier. Moreover, ananalysis of the impact of several factors in the system performance, such asthe number of classifiers in the pool, the use of different linear baseclassifiers, as well as the size of the validation data. We show that using thedynamic selection of linear classifiers through the META-DES framework, we cansolve complex non-linear classification problems where other combinationtechniques such as AdaBoost cannot.
arxiv-13200-121 | Exploring Online Ad Images Using a Deep Convolutional Neural Network Approach | http://arxiv.org/abs/1509.00568 | author:Michael Fire, Jonathan Schler category:cs.CV published:2015-09-02 summary:Online advertising is a huge, rapidly growing advertising market in today'sworld. One common form of online advertising is using image ads. A decision ismade (often in real time) every time a user sees an ad, and the advertiser iseager to determine the best ad to display. Consequently, many algorithms havebeen developed that calculate the optimal ad to show to the current user at thepresent time. Typically, these algorithms focus on variations of the ad,optimizing among different properties such as background color, image size, orset of images. However, there is a more fundamental layer. Our study looks atnew qualities of ads that can be determined before an ad is shown (rather thanonline optimization) and defines which ads are most likely to be successful. We present a set of novel algorithms that utilize deep-learning imageprocessing, machine learning, and graph theory to investigate onlineadvertising and to construct prediction models which can foresee an image ad'ssuccess. We evaluated our algorithms on a dataset with over 260,000 ad images,as well as a smaller dataset specifically related to the automotive industry,and we succeeded in constructing regression models for ad image click rateprediction. The obtained results emphasize the great potential of usingdeep-learning algorithms to effectively and efficiently analyze image ads andto create better and more innovative online ads. Moreover, the algorithmspresented in this paper can help predict ad success and can be applied toanalyze other large-scale image corpora.
arxiv-13200-122 | Enhancement and Recognition of Reverberant and Noisy Speech by Extending Its Coherence | http://arxiv.org/abs/1509.00533 | author:Scott Wisdom, Thomas Powers, Les Atlas, James Pitton category:cs.SD cs.CL stat.AP published:2015-09-02 summary:Most speech enhancement algorithms make use of the short-time Fouriertransform (STFT), which is a simple and flexible time-frequency decompositionthat estimates the short-time spectrum of a signal. However, the duration ofshort STFT frames are inherently limited by the nonstationarity of speechsignals. The main contribution of this paper is a demonstration of speechenhancement and automatic speech recognition in the presence of reverberationand noise by extending the length of analysis windows. We accomplish thisextension by performing enhancement in the short-time fan-chirp transform(STFChT) domain, an overcomplete time-frequency representation that is coherentwith speech signals over longer analysis window durations than the STFT. Thisextended coherence is gained by using a linear model of fundamental frequencyvariation of voiced speech signals. Our approach centers around using asingle-channel minimum mean-square error log-spectral amplitude (MMSE-LSA)estimator proposed by Habets, which scales coefficients in a time-frequencydomain to suppress noise and reverberation. In the case of multiplemicrophones, we preprocess the data with either a minimum variancedistortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB).We evaluate our algorithm on both speech enhancement and recognition tasks forthe REVERB challenge dataset. Compared to the same processing done in the STFTdomain, our approach achieves significant improvement in terms of objectiveenhancement metrics (including PESQ---the ITU-T standard measurement for speechquality). In terms of automatic speech recognition (ASR) performance asmeasured by word error rate (WER), our experiments indicate that the STFT witha long window is more effective for ASR.
arxiv-13200-123 | A Neural Attention Model for Abstractive Sentence Summarization | http://arxiv.org/abs/1509.00685 | author:Alexander M. Rush, Sumit Chopra, Jason Weston category:cs.CL cs.AI published:2015-09-02 summary:Summarization based on text extraction is inherently limited, butgeneration-style abstractive methods have proven challenging to build. In thiswork, we propose a fully data-driven approach to abstractive sentencesummarization. Our method utilizes a local attention-based model that generateseach word of the summary conditioned on the input sentence. While the model isstructurally simple, it can easily be trained end-to-end and scales to a largeamount of training data. The model shows significant performance gains on theDUC-2004 shared task compared with several strong baselines.
arxiv-13200-124 | What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment | http://arxiv.org/abs/1509.00838 | author:Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.CL cs.AI cs.LG cs.NE published:2015-09-02 summary:We propose an end-to-end, domain-independent neural encoder-aligner-decodermodel for selective generation, i.e., the joint task of content selection andsurface realization. Our model first encodes a full set of over-determineddatabase event records via an LSTM-based recurrent neural network, thenutilizes a novel coarse-to-fine aligner to identify the small subset of salientrecords to talk about, and finally employs a decoder to generate free-formdescriptions of the aligned, selected records. Our model achieves the bestselection and generation results reported to-date (with 59% relativeimprovement in generation) on the benchmark WeatherGov dataset, despite usingno specialized features or linguistic resources. Using an improved k-nearestneighbor beam filter helps further. We also perform a series of ablations andvisualizations to elucidate the contributions of our key model components.Lastly, we evaluate the generalizability of our model on the RoboCup dataset,and get results that are competitive with or better than the state-of-the-art,despite being severely data-starved.
arxiv-13200-125 | Sequential Information Guided Sensing | http://arxiv.org/abs/1509.00130 | author:Ruiyang Song, Yao Xie, Sebastian Pokutta category:cs.IT math.IT math.ST stat.ML stat.TH published:2015-09-01 summary:We study the value of information in sequential compressed sensing bycharacterizing the performance of sequential information guided sensing inpractical scenarios when information is inaccurate. In particular, we assumethe signal distribution is parameterized through Gaussian or Gaussian mixtureswith estimated mean and covariance matrices, and we can measure compressivelythrough a noisy linear projection or using one-sparse vectors, i.e., observingone entry of the signal each time. We establish a set of performance bounds forthe bias and variance of the signal estimator via posterior mean, by capturingthe conditional entropy (which is also related to the size of the uncertainty),and the additional power required due to inaccurate information to reach adesired precision. Based on this, we further study how to estimate covariancebased on direct samples or covariance sketching. Numerical examples alsodemonstrate the superior performance of Info-Greedy Sensing algorithms comparedwith their random and non-adaptive counterparts.
arxiv-13200-126 | Online Supervised Subspace Tracking | http://arxiv.org/abs/1509.00137 | author:Yao Xie, Ruiyang Song, Hanjun Dai, Qingbin Li, Le Song category:cs.LG math.ST stat.ML stat.TH published:2015-09-01 summary:We present a framework for supervised subspace tracking, when there are twotime series $x_t$ and $y_t$, one being the high-dimensional predictors and theother being the response variables and the subspace tracking needs to take intoconsideration of both sequences. It extends the classic online subspacetracking work which can be viewed as tracking of $x_t$ only. Our onlinesufficient dimensionality reduction (OSDR) is a meta-algorithm that can beapplied to various cases including linear regression, logistic regression,multiple linear regression, multinomial logistic regression, support vectormachine, the random dot product model and the multi-scale union-of-subspacemodel. OSDR reduces data-dimensionality on-the-fly with low-computationalcomplexity and it can also handle missing data and dynamic data. OSDR uses analternating minimization scheme and updates the subspace via gradient descenton the Grassmannian manifold. The subspace update can be performed efficientlyutilizing the fact that the Grassmannian gradient with respect to the subspacein many settings is rank-one (or low-rank in certain cases). The optimizationproblem for OSDR is non-convex and hard to analyze in general; we provideconvergence analysis of OSDR in a simple linear regression setting. The goodperformance of OSDR compared with the conventional unsupervised subspacetracking are demonstrated via numerical examples on simulated and real data.
arxiv-13200-127 | Discovery Radiomics for Computed Tomography Cancer Detection | http://arxiv.org/abs/1509.00117 | author:Devinder Kumar, Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:cs.CV published:2015-09-01 summary:Objective: Lung cancer is the leading cause for cancer related deaths. Assuch, there is an urgent need for a streamlined process that can allowradiologists to provide diagnosis with greater efficiency and accuracy. Apowerful tool to do this is radiomics. Method: In this study, we take the ideaof radiomics one step further by introducing the concept of discovery radiomicsfor lung cancer detection using CT imaging data. Rather than using pre-defined,hand-engineered feature models as with current radiomics-driven methods, wediscover custom radiomic sequencers that can generate radiomic sequencesconsisting of abstract imaging-based features tailored for characterizing lungtumour phenotype. In this study, we realize these custom radiomic sequencers asdeep convolutional sequencers using a deep convolutional neural networklearning architecture based on a wealth of CT imaging data. Results: Toillustrate the prognostic power and effectiveness of the radiomic sequencesproduced by the discovered sequencer, we perform a classification betweenmalignant and benign lesions from 93 patients with diagnostic data from theLIDC-IDRI dataset. Using the clinically provided diagnostic data as groundtruth, proposed framework provided an average accuracy of 77.52% via 10-foldcross-validation with a sensitivity of 79.06% and specificity of 76.11%. Wealso perform quantitative analysis to establish the effectiveness of theradiomics sequences. Conclusion: The proposed framework outperforms thestate-of-the art approach for lung lesion classification. Significance: Theseresults illustrate the potential for the proposed discovery radiomics approachin aiding radiologists in improving screening efficiency and accuracy.
arxiv-13200-128 | A Telescopic Binary Learning Machine for Training Neural Networks | http://arxiv.org/abs/1509.00174 | author:Mauro Brunato, Roberto Battiti category:cs.NE I.2.6 published:2015-09-01 summary:This paper proposes a new algorithm based on multi-scale stochastic localsearch with binary representation for training neural networks. In particular, we study the effects of neighborhood evaluation strategies,the effect of the number of bits per weight and that of the maximum weightrange used for mapping binary strings to real values. Following thispreliminary investigation, we propose a telescopic multi-scale version of localsearch where the number of bits is increased in an adaptive manner, leading toa faster search and to local minima of better quality. An analysis related toadapting the number of bits in a dynamic way is also presented. The control onthe number of bits, which happens in a natural manner in the proposed method,is effective to increase the generalization performance. Benchmark tasksinclude a highly non-linear artificial problem, a control problem requiringeither feed-forward or recurrent architectures for feedback control, andchallenging real-world tasks in different application domains. The results demonstrate the effectiveness of the proposed method.
arxiv-13200-129 | Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection | http://arxiv.org/abs/1509.00111 | author:Audrey G. Chung, Mohammad Javad Shafiee, Devinder Kumar, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:cs.CV physics.med-ph q-bio.QM published:2015-09-01 summary:Prostate cancer is the most diagnosed form of cancer in Canadian men, and isthe third leading cause of cancer death. Despite these statistics, prognosis isrelatively good with a sufficiently early diagnosis, making fast and reliableprostate cancer detection crucial. As imaging-based prostate cancer screening,such as magnetic resonance imaging (MRI), requires an experienced medicalprofessional to extensively review the data and perform a diagnosis,radiomics-driven methods help streamline the process and has the potential tosignificantly improve diagnostic accuracy and efficiency, and thus improvingpatient survival rates. These radiomics-driven methods currently rely onhand-crafted sets of quantitative imaging-based features, which are selectedmanually and can limit their ability to fully characterize unique prostatecancer tumour phenotype. In this study, we propose a novel \textit{discoveryradiomics} framework for generating custom radiomic sequences tailored forprostate cancer detection. Discovery radiomics aims to uncover abstractimaging-based features that capture highly unique tumour traits andcharacteristics beyond what can be captured using predefined feature models. Inthis paper, we discover new custom radiomic sequencers for generating newprostate radiomic sequences using multi-parametric MRI data. We evaluated theperformance of the discovered radiomic sequencer against a state-of-the-arthand-crafted radiomic sequencer for computer-aided prostate cancer detectionwith a feedforward neural network using real clinical prostate multi-parametricMRI data. Results for the discovered radiomic sequencer demonstrate goodperformance in prostate cancer detection and clinical decision support relativeto the hand-crafted radiomic sequencer. The use of discovery radiomics showspotential for more efficient and reliable automatic prostate cancer detection.
arxiv-13200-130 | Multi-Sensor Slope Change Detection | http://arxiv.org/abs/1509.00114 | author:Yang Cao, Yao Xie, Nagi Gebraeel category:stat.ML cs.LG math.ST stat.TH published:2015-09-01 summary:We develop a mixture procedure for multi-sensor systems to monitor datastreams for a change-point that causes a gradual degradation to a subset of thestreams. Observations are assumed to be initially normal random variables withknown constant means and variances. After the change-point, observations in thesubset will have increasing or decreasing means. The subset and therate-of-changes are unknown. Our procedure uses a mixture statistics, whichassumes that each sensor is affected by the change-point with probability$p_0$. Analytic expressions are obtained for the average run length (ARL) andthe expected detection delay (EDD) of the mixture procedure, which aredemonstrated to be quite accurate numerically. We establish the asymptoticoptimality of the mixture procedure. Numerical examples demonstrate the goodperformance of the proposed procedure. We also discuss an adaptive mixtureprocedure using empirical Bayes. This paper extends our earlier work ondetecting an abrupt change-point that causes a mean-shift, by tackling thechallenges posed by the non-stationarity of the slope-change problem.
arxiv-13200-131 | Discovery of Web Usage Profiles Using Various Clustering Techniques | http://arxiv.org/abs/1509.00692 | author:Zahid Ansari, Waseem Ahmed, M. F. Azeem, A. Vinaya Babu category:cs.DB cs.IR cs.LG published:2015-09-01 summary:The explosive growth of World Wide Web (WWW) has necessitated the developmentof Web personalization systems in order to understand the user preferences todynamically serve customized content to individual users. To reveal informationabout user preferences from Web usage data, Web Usage Mining (WUM) techniquesare extensively being applied to the Web log data. Clustering techniques arewidely used in WUM to capture similar interests and trends among usersaccessing a Web site. Clustering aims to divide a data set into groups orclusters where inter-cluster similarities are minimized while the intra clustersimilarities are maximized. This paper reviews four of the popularly usedclustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniquesare implemented and tested against the Web user navigational data. Performanceand validity results of each technique are presented and compared.
arxiv-13200-132 | Fingerprinting-Based Positioning in Distributed Massive MIMO Systems | http://arxiv.org/abs/1509.00202 | author:Vladimir Savic, Erik G. Larsson category:cs.IT cs.LG math.IT published:2015-09-01 summary:Location awareness in wireless networks may enable many applications such asemergency services, autonomous driving and geographic routing. Although thereare many available positioning techniques, none of them is adapted to work withmassive multiple-in-multiple-out (MIMO) systems, which represent a leading 5Gtechnology candidate. In this paper, we discuss possible solutions forpositioning of mobile stations using a vector of signals at the base station,equipped with many antennas distributed over deployment area. Our main proposalis to use fingerprinting techniques based on a vector of received signalstrengths. This kind of methods are able to work in highly-cluttered multipathenvironments, and require just one base station, in contrast to standardrange-based and angle-based techniques. We also provide a solution forfingerprinting-based positioning based on Gaussian process regression, anddiscuss main applications and challenges.
arxiv-13200-133 | Robust Face Recognition via Multimodal Deep Face Representation | http://arxiv.org/abs/1509.00244 | author:Changxing Ding, Dacheng Tao category:cs.CV published:2015-09-01 summary:Face images appeared in multimedia applications, e.g., social networks anddigital entertainment, usually exhibit dramatic pose, illumination, andexpression variations, resulting in considerable performance degradation fortraditional face recognition algorithms. This paper proposes a comprehensivedeep learning framework to jointly learn face representation using multimodalinformation. The proposed deep learning structure is composed of a set ofelaborately designed convolutional neural networks (CNNs) and a three-layerstacked auto-encoder (SAE). The set of CNNs extracts complementary facialfeatures from multimodal data. Then, the extracted features are concatenated toform a high-dimensional feature vector, whose dimension is compressed by SAE.All the CNNs are trained using a subset of 9,000 subjects from the publiclyavailable CASIA-WebFace database, which ensures the reproducibility of thiswork. Using the proposed single CNN architecture and limited training data,98.43% verification rate is achieved on the LFW database. Benefited from thecomplementary information contained in multimodal data, our small ensemblesystem achieves higher than 99.0% recognition rate on LFW using publiclyavailable training set.
arxiv-13200-134 | Fast Randomized Singular Value Thresholding for Nuclear Norm Minimization | http://arxiv.org/abs/1509.00296 | author:Tae-Hyun Oh, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon category:cs.CV published:2015-09-01 summary:Rank minimization can be boiled down to tractable surrogate problems, such asNuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems relatedto NNM (or WNNM) can be solved iteratively by applying a closed-form proximaloperator, called Singular Value Thresholding (SVT) (or Weighted SVT), but theysuffer from high computational cost of computing Singular Value Decomposition(SVD) at each iteration. We propose a fast and accurate approximation methodfor SVT, that we call fast randomized SVT (FRSVT), where we avoid directcomputation of SVD. The key idea is to extract an approximate basis for therange of a matrix from its compressed matrix. Given the basis, we compute thepartial singular values of the original matrix from a small factored matrix. Inaddition, by adopting a range propagation technique, our method further speedsup the extraction of approximate basis at each iteration. Our theoreticalanalysis shows the relationship between the approximation bound of SVD and itseffect to NNM via SVT. Along with the analysis, our empirical resultsquantitatively and qualitatively show that our approximation rarely harms theconvergence of the host algorithms. We assess the efficiency and accuracy ofour method on various vision problems, e.g., subspace clustering, weatherartifact removal, and simultaneous multi-image alignment and rectification.
arxiv-13200-135 | Evolving Unipolar Memristor Spiking Neural Networks | http://arxiv.org/abs/1509.00105 | author:David Howard, Larry Bull, Ben De Lacy Costello category:cs.NE published:2015-09-01 summary:Neuromorphic computing --- brainlike computing in hardware --- typicallyrequires myriad CMOS spiking neurons interconnected by a dense mesh ofnanoscale plastic synapses. Memristors are frequently citepd as strong synapsecandidates due to their statefulness and potential for low-powerimplementations. To date, plentiful research has focused on the bipolarmemristor synapse, which is capable of incremental weight alterations and canprovide adaptive self-organisation under a Hebbian learning scheme. In thispaper we consider the Unipolar memristor synapse --- a device capable ofnon-Hebbian switching between only two states (conductive and resistive)through application of a suitable input voltage --- and discuss its suitabilityfor neuromorphic systems. A self-adaptive evolutionary process is used toautonomously find highly fit network configurations. Experimentation on a tworobotics tasks shows that unipolar memristor networks evolve task-solvingcontrollers faster than both bipolar memristor networks and networks containingconstant nonplastic connections whilst performing at least comparably.
arxiv-13200-136 | FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation | http://arxiv.org/abs/1509.00116 | author:M. Salman Asif, Ali Ayremlou, Aswin Sankaranarayanan, Ashok Veeraraghavan, Richard Baraniuk category:cs.CV published:2015-09-01 summary:FlatCam is a thin form-factor lensless camera that consists of a coded maskplaced on top of a bare, conventional sensor array. Unlike a traditional,lens-based camera where an image of the scene is directly recorded on thesensor pixels, each pixel in FlatCam records a linear combination of light frommultiple scene elements. A computational algorithm is then used to demultiplexthe recorded measurements and reconstruct an image of the scene. FlatCam is aninstance of a coded aperture imaging system; however, unlike the vast majorityof related work, we place the coded mask extremely close to the image sensorthat can enable a thin system. We employ a separable mask to ensure that bothcalibration and image reconstruction are scalable in terms of memoryrequirements and computational complexity. We demonstrate the potential of theFlatCam design using two prototypes: one at visible wavelengths and one atinfrared wavelengths.
arxiv-13200-137 | Iterative hypothesis testing for multi-object tracking in presence of features with variable reliability | http://arxiv.org/abs/1509.00313 | author:Amit Kumar K. C., Damien Delannay, Christophe De Vleeschouwer category:cs.CV published:2015-09-01 summary:This paper assumes prior detections of multiple targets at each time instant,and uses a graph-based approach to connect those detections across time, basedon their position and appearance estimates. In contrast to most earlier worksin the field, our framework has been designed to exploit the appearancefeatures, even when they are only sporadically available, or affected by anon-stationary noise, along the sequence of detections. This is done byimplementing an iterative hypothesis testing strategy to progressivelyaggregate the detections into short trajectories, named tracklets.Specifically, each iteration considers a node, named key-node, and investigateshow to link this key-node with other nodes in its neighborhood, under theassumption that the target appearance is defined by the key-node appearanceestimate. This is done through shortest path computation in a temporalneighborhood of the key-node. The approach is conservative in that it onlyaggregates the shortest paths that are sufficiently better compared toalternative paths. It is also multi-scale in that the size of the investigatedneighborhood is increased proportionally to the number of detections alreadyaggregated into the key-node. The multi-scale nature of the process and theprogressive relaxation of its conservativeness makes it both computationallyefficient and effective. Experimental validations are performed extensively on a toy example, a 15minutes long multi-view basketball dataset, and other monocular pedestriandatasets.
arxiv-13200-138 | Sensor-Type Classification in Buildings | http://arxiv.org/abs/1509.00498 | author:Dezhi Hong, Jorge Ortiz, Arka Bhattacharya, Kamin Whitehouse category:cs.LG C.3 published:2015-09-01 summary:Many sensors/meters are deployed in commercial buildings to monitor andoptimize their performance. However, because sensor metadata is inconsistentacross buildings, software-based solutions are tightly coupled to the sensormetadata conventions (i.e. schemas and naming) for each building. Running thesame software across buildings requires significant integration effort. Metadata normalization is critical for scaling the deployment process andallows us to decouple building-specific conventions from the code written forbuilding applications. It also allows us to deal with missing metadata. Oneimportant aspect of normalization is to differentiate sensors by the typeofphenomena being observed. In this paper, we propose a general, simple, yeteffective classification scheme to differentiate sensors in buildings by type.We perform ensemble learning on data collected from over 2000 sensor streams intwo buildings. Our approach is able to achieve more than 92% accuracy forclassification within buildings and more than 82% accuracy for acrossbuildings. We also introduce a method for identifying potential misclassifiedstreams. This is important because it allows us to identify opportunities toattain more input from experts -- input that could help improve classificationaccuracy when ground truth is unavailable. We show that by adjusting athreshold value we are able to identify at least 30% of the misclassifiedinstances.
arxiv-13200-139 | Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and Active Contour | http://arxiv.org/abs/1509.00154 | author:Jalil Rasekhi category:cs.CV stat.ML published:2015-09-01 summary:In this paper we present a new method for motion tracking of tumors in liverultrasound image sequences. Our algorithm has two main steps. In the firststep, we apply mean shift algorithm with multiple features to estimate thecenter of the target in each frame. Target in the first frame is defined usingan ellipse. Edge, texture, and intensity features are extracted from the firstframe, and then mean shift algorithm is applied to each feature separately tofind the center of ellipse related to that feature in the next frame. Thecenter of ellipse will be the weighted average of these centers. By using meanshift actually we estimate the target movement between two consecutive frames.Once the correct ellipsoid in each frame is known, in the second step we applythe Dynamic Directional Gradient Vector Flow (DDGVF) version of active contourmodels, in order to find the correct boundary of tumors. We sample a few pointson the boundary of active contour then translate those points based on thetranslation of the center of ellipsoid in two consecutive frames to determinethe target movement. We use these translated sample points as an initial guessfor active contour in the next frame. Our experimental results show that, thesuggested method provides a reliable performance for liver tumor tracking inultrasound image sequences.
arxiv-13200-140 | Differentially Private Online Learning for Cloud-Based Video Recommendation with Multimedia Big Data in Social Networks | http://arxiv.org/abs/1509.00181 | author:Pan Zhou, Yingxue Zhou, Dapeng Wu, Hai Jin category:cs.LG published:2015-09-01 summary:With the rapid growth in multimedia services and the enormous offers of videocontents in online social networks, users have difficulty in obtaining theirinterests. Therefore, various personalized recommendation systems have beenproposed. However, they ignore that the accelerated proliferation of socialmedia data has led to the big data era, which has greatly impeded the processof video recommendation. In addition, none of them has considered both theprivacy of users' contexts (e,g., social status, ages and hobbies) and videoservice vendors' repositories, which are extremely sensitive and of significantcommercial value. To handle the problems, we propose a cloud-assisteddifferentially private video recommendation system based on distributed onlinelearning. In our framework, service vendors are modeled as distributedcooperative learners, recommending videos according to user's context, whilesimultaneously adapting the video-selection strategy based on user-clickfeedback to maximize total user clicks (reward). Considering the sparsity andheterogeneity of big social media data, we also propose a novel geometricdifferentially private model, which can greatly reduce the performance(recommendation accuracy) loss. Our simulation shows the proposed algorithmsoutperform other existing methods and keep a delicate balance between computingaccuracy and privacy preserving level.
arxiv-13200-141 | Importance Weighted Autoencoders | http://arxiv.org/abs/1509.00519 | author:Yuri Burda, Roger Grosse, Ruslan Salakhutdinov category:cs.LG stat.ML published:2015-09-01 summary:The variational autoencoder (VAE; Kingma, Welling (2014)) is a recentlyproposed generative model pairing a top-down generative network with abottom-up recognition network which approximates posterior inference. Ittypically makes strong assumptions about posterior inference, for instance thatthe posterior distribution is approximately factorial, and that its parameterscan be approximated with nonlinear regression from the observations. As we showempirically, the VAE objective can lead to overly simplified representationswhich fail to use the network's entire modeling capacity. We present theimportance weighted autoencoder (IWAE), a generative model with the samearchitecture as the VAE, but which uses a strictly tighter log-likelihood lowerbound derived from importance weighting. In the IWAE, the recognition networkuses multiple samples to approximate the posterior, giving it increasedflexibility to model complex posteriors which do not fit the VAE modelingassumptions. We show empirically that IWAEs learn richer latent spacerepresentations than VAEs, leading to improved test log-likelihood on densityestimation benchmarks.
arxiv-13200-142 | Learning Deep $\ell_0$ Encoders | http://arxiv.org/abs/1509.00153 | author:Zhangyang Wang, Qing Ling, Thomas S. Huang category:cs.LG stat.ML published:2015-09-01 summary:Despite its nonconvex nature, $\ell_0$ sparse approximation is desirable inmany theoretical and application cases. We study the $\ell_0$ sparseapproximation problem with the tool of deep learning, by proposing Deep$\ell_0$ Encoders. Two typical forms, the $\ell_0$ regularized problem and the$M$-sparse problem, are investigated. Based on solid iterative algorithms, wemodel them as feed-forward neural networks, through introducing novel neuronsand pooling functions. Enforcing such structural priors acts as an effectivenetwork regularization. The deep encoders also enjoy faster inference, largerlearning capacity, and better scalability compared to conventional sparsecoding solutions. Furthermore, under task-driven losses, the models can beconveniently optimized from end to end. Numerical results demonstrate theimpressive performances of the proposed encoders.
arxiv-13200-143 | Adaptive Smoothing Algorithms for Nonsmooth Composite Convex Minimization | http://arxiv.org/abs/1509.00106 | author:Quoc Tran-Dinh category:math.OC stat.ML published:2015-09-01 summary:We propose novel adaptive smoothing algorithms based on Nesterov's smoothingtechnique in [26] for solving nonsmooth composite convex optimization problems.Our methods combine both Nesterov's accelerated proximal gradient scheme and anew homotopy strategy for smoothness parameter. By an appropriate choice ofsmoothing functions, we develop new algorithms that have upto the$\mathcal{O}\left(\frac{1}{\varepsilon}\right)$-optimal worst-case iterationcomplexity while allow one to automatically update the smoothness parameter ateach iteration. We then further exploit the structure of problems to selectsmoothing functions and develop suitable algorithmic variants that reduce thecomplexity-per-iteration. We also specify our algorithms to solve constrainedconvex optimization problems and show their convergence guarantee on the primalsequence of iterates. We demonstrate our algorithms through three numericalexamples and compare them with the nonadaptive algorithm in [26].
arxiv-13200-144 | Learning A Task-Specific Deep Architecture For Clustering | http://arxiv.org/abs/1509.00151 | author:Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang category:cs.LG cs.CV stat.ML published:2015-09-01 summary:While sparse coding-based clustering methods have shown to be successful,their bottlenecks in both efficiency and scalability limit the practical usage.In recent years, deep learning has been proved to be a highly effective,efficient and scalable feature learning tool. In this paper, we propose toemulate the sparse coding-based clustering pipeline in the context of deeplearning, leading to a carefully crafted deep model benefiting from both. Afeed-forward network structure, named TAGnet, is constructed based on agraph-regularized sparse coding algorithm. It is then trained withtask-specific loss functions from end to end. We discover that connecting deeplearning to sparse coding benefits not only the model performance, but also itsinitialization and interpretation. Moreover, by introducing auxiliaryclustering tasks to the intermediate feature hierarchy, we formulate DTAGnetand obtain a further performance boost. Extensive experiments demonstrate thatthe proposed model gains remarkable margins over several state-of-the-artmethods.
arxiv-13200-145 | Metastatic liver tumour segmentation from discriminant Grassmannian manifolds | http://arxiv.org/abs/1509.00083 | author:Samuel Kadoury, Eugene Vorontsov, An Tang category:cs.LG cs.CV published:2015-08-31 summary:The early detection, diagnosis and monitoring of liver cancer progression canbe achieved with the precise delineation of metastatic tumours. However,accurate automated segmentation remains challenging due to the presence ofnoise, inhomogeneity and the high appearance variability of malignant tissue.In this paper, we propose an unsupervised metastatic liver tumour segmentationframework using a machine learning approach based on discriminant Grassmannianmanifolds which learns the appearance of tumours with respect to normal tissue.First, the framework learns within-class and between-class similaritydistributions from a training set of images to discover the optimal manifolddiscrimination between normal and pathological tissue in the liver. Second, aconditional optimisation scheme computes nonlocal pairwise as well aspattern-based clique potentials from the manifold subspace to recognise regionswith similar labelings and to incorporate global consistency in thesegmentation process. The proposed framework was validated on a clinicaldatabase of 43 CT images from patients with metastatic liver cancer. Comparedto state-of-the-art methods, our method achieves a better performance on twoseparate datasets of metastatic liver tumours from different clinical sites,yielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50tumours with an average volume of 27.3 mm3.
arxiv-13200-146 | Action Recognition by Hierarchical Mid-level Action Elements | http://arxiv.org/abs/1508.07654 | author:Tian Lan, Yuke Zhu, Amir Roshan Zamir, Silvio Savarese category:cs.CV published:2015-08-31 summary:Realistic videos of human actions exhibit rich spatiotemporal structures atmultiple levels of granularity: an action can always be decomposed intomultiple finer-grained elements in both space and time. To capture thisintuition, we propose to represent videos by a hierarchy of mid-level actionelements (MAEs), where each MAE corresponds to an action-related spatiotemporalsegment in the video. We introduce an unsupervised method to generate thisrepresentation from videos. Our method is capable of distinguishingaction-related segments from background segments and representing actions atmultiple spatiotemporal resolutions. Given a set of spatiotemporal segmentsgenerated from the training data, we introduce a discriminative clusteringalgorithm that automatically discovers MAEs at multiple levels of granularity.We develop structured models that capture a rich set of spatial, temporal andhierarchical relations among the segments, where the action label and multiplelevels of MAE labels are jointly inferred. The proposed model achievesstate-of-the-art performance in multiple action recognition benchmarks.Moreover, we demonstrate the effectiveness of our model in real-worldapplications such as action recognition in large-scale untrimmed videos andaction parsing.
arxiv-13200-147 | Ethnicity sensitive author disambiguation using semi-supervised learning | http://arxiv.org/abs/1508.07744 | author:Gilles Louppe, Hussein Al-Natsheh, Mateusz Susik, Eamonn Maguire category:cs.DL cs.IR stat.ML published:2015-08-31 summary:Author name disambiguation in bibliographic databases is the problem ofgrouping together scientific publications written by the same person,accounting for potential homonyms and/or synonyms. Among solutions to thisproblem, digital libraries are increasingly offering tools for authors tomanually curate their publications and claim those that are theirs. Indirectly,these tools allow for the inexpensive collection of large annotated trainingdata, which can be further leveraged to build a complementary automateddisambiguation system capable of inferring patterns for identifyingpublications written by the same person. Building on more than 1 millionpublicly released crowdsourced annotations, we propose an automated authordisambiguation solution exploiting this data (i) to learn an accurateclassifier for identifying coreferring authors and (ii) to guide the clusteringof scientific publications by distinct authors in a semi-supervised way. To thebest of our knowledge, our analysis is the first to be carried out on data ofthis size and coverage. With respect to the state of the art, we validate thegeneral pipeline used in most existing solutions, and improve by: (i) proposingphonetic-based blocking strategies, thereby increasing recall; and (ii) addingstrong ethnicity-sensitive features for learning a linkage function, therebytailoring disambiguation to non-Western author names whenever necessary.
arxiv-13200-148 | Value function approximation via low-rank models | http://arxiv.org/abs/1509.00061 | author:Hao Yi Ong category:cs.LG cs.AI published:2015-08-31 summary:We propose a novel value function approximation technique for Markov decisionprocesses. We consider the problem of compactly representing the state-actionvalue function using a low-rank and sparse matrix model. The problem is todecompose a matrix that encodes the true value function into low-rank andsparse components, and we achieve this using Robust Principal ComponentAnalysis (PCA). Under minimal assumptions, this Robust PCA problem can besolved exactly via the Principal Component Pursuit convex optimization problem.We experiment the procedure on several examples and demonstrate that our methodyields approximations essentially identical to the true function.
arxiv-13200-149 | Maximum Persistency via Iterative Relaxed Inference with Graphical Models | http://arxiv.org/abs/1508.07902 | author:Alexander Shekhovtsov, Paul Swoboda, Bogdan Savchynskyy category:cs.CV cs.DS published:2015-08-31 summary:We consider the NP-hard problem of MAP-inference for graphical models. Wepropose a polynomial time practically efficient algorithm for finding a part ofits optimal solution. Specifically, our algorithm marks each label in each nodeof the considered graphical model either as (i) optimal, meaning that itbelongs to all optimal solutions of the inference problem; (ii) non-optimal ifit provably does not belong to any solution; or (iii) undefined, which meansour algorithm can not make a decision regarding the label. Moreover, we proveoptimality of our approach: it delivers in a certain sense the largest totalnumber of labels marked as optimal or non-optimal. We demonstrate superiorityof our approach on problems from machine learning and computer visionbenchmarks.
arxiv-13200-150 | Pure and Hybrid Evolutionary Computing in Global Optimization of Chemical Structures: from Atoms and Molecules to Clusters and Crystals | http://arxiv.org/abs/1509.00028 | author:Kanchan Sarkar, S. P. Bhattacharyya category:cs.NE published:2015-08-31 summary:The growth of evolutionary computing (EC) methods in the exploration ofcomplex potential energy landscapes of atomic and molecular clusters, as wellas crystals over the last decade or so is reviewed. The trend of growthindicates that pure as well as hybrid evolutionary computing techniques inconjunction of DFT has been emerging as a powerful tool, although work onmolecular clusters has been rather limited so far. Some attempts to solve theatomic/molecular Schrodinger Equation (SE) directly by genetic algorithms (GA)are available in literature. At the Born-Oppenheimer level of approximationGA-density methods appear to be a viable tool which could be more extensivelyexplored in the coming years, specially in the context of designing moleculesand materials with targeted properties.
arxiv-13200-151 | Domain Generalization for Object Recognition with Multi-task Autoencoders | http://arxiv.org/abs/1508.07680 | author:Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi category:cs.CV published:2015-08-31 summary:The problem of domain generalization is to take knowledge acquired from anumber of related domains where training data is available, and to thensuccessfully apply it to previously unseen domains. We propose a new featurelearning algorithm, Multi-Task Autoencoder (MTAE), that provides goodgeneralization performance for cross-domain object recognition. Our algorithm extends the standard denoising autoencoder framework bysubstituting artificially induced corruption with naturally occurringinter-domain variability in the appearance of objects. Instead ofreconstructing images from noisy versions, MTAE learns to transform theoriginal image into analogs in multiple related domains. It thereby learnsfeatures that are robust to variations across domains. The learnt features arethen used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognitiondatasets, where the task is to learn features from multiple datasets and tothen predict the image label from unseen datasets. We found that (denoising)MTAE outperforms alternative autoencoder-based models as well as the currentstate-of-the-art algorithms for domain generalization.
arxiv-13200-152 | Approximate Nearest Neighbor Fields in Video | http://arxiv.org/abs/1508.07953 | author:Nir Ben-Zrihem, Lihi Zelnik-Manor category:cs.CV published:2015-08-31 summary:We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search),an algorithm for matching patches of a video to a set of reference patches inreal-time. For each query, RIANN finds potential matches by intersecting ringsaround key points in appearance space. Its search complexity is reverselycorrelated to the amount of temporal change, making it a good fit for videos,where typically most patches change slowly with time. Experiments show thatRIANN is up to two orders of magnitude faster than previous ANN methods, and isthe only solution that operates in real-time. We further demonstrate how RIANNcan be used for real-time video processing and provide examples for a range ofreal-time video applications, including colorization, denoising, and severalartistic effects.
arxiv-13200-153 | Word Representations, Tree Models and Syntactic Functions | http://arxiv.org/abs/1508.07709 | author:Simon Šuster, Gertjan van Noord, Ivan Titov category:cs.CL cs.LG stat.ML published:2015-08-31 summary:Word representations induced from models with discrete latent variables(e.g.\ HMMs) have been shown to be beneficial in many NLP applications. In thiswork, we exploit labeled syntactic dependency trees and formalize the inductionproblem as unsupervised learning of tree-structured hidden Markov models.Syntactic functions are used as additional observed variables in the model,influencing both transition and emission components. Such syntactic informationcan potentially lead to capturing more fine-grain and functional distinctionsbetween words, which, in turn, may be desirable in many NLP applications. Weevaluate the word representations on two tasks -- named entity recognition andsemantic frame identification. We observe improvements from exploitingsyntactic function information in both cases, and the results rivaling those ofstate-of-the-art representation learning methods. Additionally, we revisit therelationship between sequential and unlabeled-tree models and find that theadvantage of the latter is not self-evident.
arxiv-13200-154 | Learning to Aggregate Information for Sequential Inferences | http://arxiv.org/abs/1508.07964 | author:Diyan Teng, Emre Ertin category:stat.ML cs.LG published:2015-08-31 summary:We consider the problem of training a binary sequential classifier under anerror rate constraint. It is well known that for known densities, accumulatingthe likelihood ratio statistics is time optimal under a fixed error rateconstraint. For the case of unknown densities, we formulate the learning forsequential detection problem as a constrained density ratio estimation problem.Specifically, we show that the problem can be posed as a convex optimizationproblem using a Reproducing Kernel Hilbert Space representation for thelog-density ratio function. The proposed binary sequential classifier is testedon synthetic data set and UC Irvine human activity recognition data set,together with previous approaches for density ratio estimation. Our empiricalresults show that the classifier trained through the proposed techniqueachieves smaller average sampling cost than previous classifiers proposed inthe literature for the same error rate.
arxiv-13200-155 | Decentralized Online Optimization with Global Objectives and Local Communication | http://arxiv.org/abs/1508.07933 | author:Soomin Lee, Angelia Nedić, Maxim Raginsky category:math.OC cs.LG cs.SY published:2015-08-31 summary:We consider a decentralized online convex optimization problem in a networkof agents, where each agent controls only a coordinate (or a part) of theglobal decision vector. For such a problem, we propose two decentralizedvariants (ODA-LM and OPDA-TV) of Nesterov's primal-dual algorithm with dualaveraging. In ODA-LM, to mitigate the disagreements on the primal-vectorupdates, the agents implement a generalization of the localinformation-exchange dynamics recently proposed by Li and Marden over a staticundirected graph. In OPDA-TV, the agents implement the broadcast-based push-sumdynamics over a time-varying sequence of uniformly connected digraphs. We showthat the regret bounds in both cases have sublinear growth of $O(\sqrt{T})$,with the time horizon $T$, when the stepsize is of the form $1/\sqrt{t}$ andthe objective functions are Lipschitz-continuous convex functions withLipschitz gradients. We also implement the proposed algorithms on a sensornetwork to complement our theoretical analysis.
arxiv-13200-156 | Neural Machine Translation of Rare Words with Subword Units | http://arxiv.org/abs/1508.07909 | author:Rico Sennrich, Barry Haddow, Alexandra Birch category:cs.CL published:2015-08-31 summary:Neural machine translation (NMT) models typically operate with a fixedvocabulary, but translation is an open-vocabulary problem. Previous workaddresses the translation of out-of-vocabulary words by backing off to adictionary. In this paper, we introduce a simpler and more effective approach,making the NMT model capable of open-vocabulary translation by encoding rareand unknown words as sequences of subword units. This is based on the intuitionthat various word classes are translatable via smaller units than words, forinstance names (via character copying or transliteration), compounds (viacompositional translation), and cognates and loanwords (via phonological andmorphological transformations). We discuss the suitability of different wordsegmentation techniques, including simple character n-gram models and asegmentation based on the byte pair encoding compression algorithm, andempirically show that subword models improve over a back-off dictionarybaseline for the WMT 15 translation tasks English-German and English-Russian by1.1 and 1.3 BLEU, respectively.
arxiv-13200-157 | Online Model Evaluation in a Large-Scale Computational Advertising Platform | http://arxiv.org/abs/1508.07678 | author:Shahriar Shariat, Burkay Orten, Ali Dasdan category:cs.AI stat.ME stat.ML published:2015-08-31 summary:Online media provides opportunities for marketers through which they candeliver effective brand messages to a wide range of audiences. Advertisingtechnology platforms enable advertisers to reach their target audience bydelivering ad impressions to online users in real time. In order to identifythe best marketing message for a user and to purchase impressions at the rightprice, we rely heavily on bid prediction and optimization models. Even thoughthe bid prediction models are well studied in the literature, the equallyimportant subject of model evaluation is usually overlooked. Effective andreliable evaluation of an online bidding model is crucial for making fastermodel improvements as well as for utilizing the marketing budgets moreefficiently. In this paper, we present an experimentation framework for bidprediction models where our focus is on the practical aspects of modelevaluation. Specifically, we outline the unique challenges we encounter in ourplatform due to a variety of factors such as heterogeneous goal definitions,varying budget requirements across different campaigns, high seasonality andthe auction-based environment for inventory purchasing. Then, we introducereturn on investment (ROI) as a unified model performance (i.e., success)metric and explain its merits over more traditional metrics such asclick-through rate (CTR) or conversion rate (CVR). Most importantly, we discusscommonly used evaluation and metric summarization approaches in detail andpropose a more accurate method for online evaluation of new experimental modelsagainst the baseline. Our meta-analysis-based approach addresses variousshortcomings of other methods and yields statistically robust conclusions thatallow us to conclude experiments more quickly in a reliable manner. Wedemonstrate the effectiveness of our evaluation strategy on real campaign datathrough some experiments.
arxiv-13200-158 | Multi-Projector Color Structured-Light Vision | http://arxiv.org/abs/1508.07859 | author:Changsoo Je, Kwang Hee Lee, Sang Wook Lee category:cs.CV cs.GR physics.optics I.2.10; I.4.8 published:2015-08-31 summary:Research interest in rapid structured-light imaging has grown increasinglyfor the modeling of moving objects, and a number of methods have been suggestedfor the range capture in a single video frame. The imaging area of a 3D objectusing a single projector is restricted since the structured light is projectedonly onto a limited area of the object surface. Employing additional projectorsto broaden the imaging area is a challenging problem since simultaneousprojection of multiple patterns results in their superposition in thelight-intersected areas and the recognition of original patterns is by no meanstrivial. This paper presents a novel method of multi-projector colorstructured-light vision based on projector-camera triangulation. By analyzingthe behavior of superposed-light colors in a chromaticity domain, we show thatthe original light colors cannot be properly extracted by the conventionaldirect estimation. We disambiguate multiple projectors by multiplexing theorientations of projector patterns so that the superposed patterns can beseparated by explicit derivative computations. Experimental studies are carriedout to demonstrate the validity of the presented method. The proposed methodincreases the efficiency of range acquisition compared to conventional activestereo using multiple projectors.
arxiv-13200-159 | Bayesian Networks for Variable Groups | http://arxiv.org/abs/1508.07753 | author:Pekka Parviainen, Samuel Kaski category:stat.ML cs.AI published:2015-08-31 summary:Bayesian networks, and especially their structures, are powerful tools forrepresenting conditional independencies and dependencies between randomvariables. In applications where related variables form a priori known groups,chosen to represent different "views" to or aspects of the same entities, onemay be more interested in modeling dependencies between groups of variablesrather than between individual variables. Motivated by this, we study prospectsof representing relationships between variable groups using Bayesian networkstructures. We show that for dependency structures between groups to belearnable, the data have to satisfy the so-called groupwise faithfulnessassumption. We also show that one cannot learn causal relations between groupsusing only groupwise conditional independencies, but also variable-wiserelations are needed. Additionally, we present algorithms for finding thegroupwise dependency structures.
arxiv-13200-160 | A Cognitive Architecture Based on a Learning Classifier System with Spiking Classifiers | http://arxiv.org/abs/1508.07700 | author:David Howard, Larry Bull, Pier-Luca Lanzi category:cs.NE published:2015-08-31 summary:Learning Classifier Systems (LCS) are population-based reinforcement learnersthat were originally designed to model various cognitive phenomena. This paperpresents an explicitly cognitive LCS by using spiking neural networks asclassifiers, providing each classifier with a measure of temporal dynamism. Weemploy a constructivist model of growth of both neurons and synapticconnections, which permits a Genetic Algorithm (GA) to automatically evolvesufficiently-complex neural structures. The spiking classifiers are coupledwith a temporally-sensitive reinforcement learning algorithm, which allows thesystem to perform temporal state decomposition by appropriately rewarding"macro-actions," created by chaining together multiple atomic actions. Thecombination of temporal reinforcement learning and neural informationprocessing is shown to outperform benchmark neural classifier systems, andsuccessfully solve a robotic navigation task.
arxiv-13200-161 | Model Guided Sampling Optimization for Low-dimensional Problems | http://arxiv.org/abs/1508.07741 | author:Lukas Bajer, Martin Holena category:cs.NE stat.ML published:2015-08-31 summary:Optimization of very expensive black-box functions requires utilization ofmaximum information gathered by the process of optimization. Model GuidedSampling Optimization (MGSO) forms a more robust alternative to Jones'Gaussian-process-based EGO algorithm. Instead of EGO's maximizing expectedimprovement, the MGSO uses sampling the probability of improvement which isshown to be helpful against trapping in local minima. Further, the MGSO canreach close-to-optimum solutions faster than standard optimization algorithmson low dimensional or smooth problems.
arxiv-13200-162 | Love Thy Neighbors: Image Annotation by Exploiting Image Metadata | http://arxiv.org/abs/1508.07647 | author:Justin Johnson, Lamberto Ballan, Fei-Fei Li category:cs.CV published:2015-08-30 summary:Some images that are difficult to recognize on their own may become moreclear in the context of a neighborhood of related images with similarsocial-network metadata. We build on this intuition to improve multilabel imageannotation. Our model uses image metadata nonparametrically to generateneighborhoods of related images using Jaccard similarities, then uses a deepneural network to blend visual information from the image and its neighbors.Prior work typically models image metadata parametrically, in contrast, ournonparametric treatment allows our model to perform well even when thevocabulary of metadata changes between training and testing. We performcomprehensive experiments on the NUS-WIDE dataset, where we show that our modeloutperforms state-of-the-art methods for multilabel image annotation even whenour model is forced to generalize to new types of metadata.
arxiv-13200-163 | Computational Sociolinguistics: A Survey | http://arxiv.org/abs/1508.07544 | author:Dong Nguyen, A. Seza Doğruöz, Carolyn P. Rosé, Franciska de Jong category:cs.CL published:2015-08-30 summary:Language is a social phenomenon and variation is inherent to its socialnature. Recently, there has been a surge of interest within the computationallinguistics (CL) community in the social dimension of language. In this articlewe present a survey of the emerging field of "Computational Sociolinguistics"that reflects this increased interest. We aim to provide a comprehensiveoverview of CL research on sociolinguistic themes, featuring topics such as therelation between language and social identity, language use in socialinteraction and multilingual communication. Moreover, we demonstrate thepotential for synergy between the research communities involved, by showing howthe large-scale data-driven methods that are widely used in CL can complementexisting sociolinguistic studies, and how sociolinguistics can inform andchallenge the methods and assumptions employed in CL studies. We hope to conveythe possible benefits of a closer collaboration between the two communities andconclude with a discussion of open challenges.
arxiv-13200-164 | Directional Decision Lists | http://arxiv.org/abs/1508.07643 | author:Marc Goessling, Shan Kang category:stat.ML cs.LG stat.CO published:2015-08-30 summary:In this paper we introduce a novel family of decision lists consisting ofhighly interpretable models which can be learned efficiently in a greedymanner. The defining property is that all rules are oriented in the samedirection. Particular examples of this family are decision lists withmonotonically decreasing (or increasing) probabilities. On simulated data weempirically confirm that the proposed model family is easier to train thangeneral decision lists. We exemplify the practical usability of our approach byidentifying problem symptoms in a manufacturing process.
arxiv-13200-165 | Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing | http://arxiv.org/abs/1508.07569 | author:Gary Pui-Tung Choi, Kin Tat Ho, Lok Ming Lui category:cs.CG cs.CV cs.GR math.DG published:2015-08-30 summary:Point cloud is the most fundamental representation of 3D geometric objects.Analyzing and processing point cloud surfaces is important in computer graphicsand computer vision. However, most of the existing algorithms for surfaceanalysis require connectivity information. Therefore, it is desirable todevelop a mesh structure on point clouds. This task can be simplified with theaid of a parameterization. In particular, conformal parameterizations areadvantageous in preserving the geometric information of the point cloud data.In this paper, we extend a state-of-the-art spherical conformalparameterization algorithm for genus-0 closed meshes to the case of pointclouds, using an improved approximation of the Laplace-Beltrami operator ondata points. Then, we propose an iterative scheme called the North-Southreiteration for achieving a spherical conformal parameterization. A balancingscheme is introduced to enhance the distribution of the sphericalparameterization. High quality triangulations and quadrangulations can then bebuilt on the point clouds with the aid of the parameterizations. Also, themeshes generated are guaranteed to be genus-0 closed meshes. Moreover, usingour proposed spherical conformal parameterization, multilevel representationsof point clouds can be easily constructed. Experimental results demonstrate theeffectiveness of our proposed framework.
arxiv-13200-166 | Feature Selection via Binary Simultaneous Perturbation Stochastic Approximation | http://arxiv.org/abs/1508.07630 | author:Vural Aksakalli, Milad Malekipirbazari category:stat.ML cs.LG published:2015-08-30 summary:Feature selection (FS) has become an indispensable task in dealing withtoday's highly complex pattern recognition problems with massive number offeatures. In this study, we propose a new wrapper approach for FS based onbinary simultaneous perturbation stochastic approximation (BSPSA). Thispseudo-gradient descent stochastic algorithm starts with an initial featurevector and moves toward the optimal feature vector via successive iterations.In each iteration, the current feature vector's individual components areperturbed simultaneously by random offsets from a qualified probabilitydistribution. We present computational experiments on datasets with numbers offeatures ranging from a few dozens to thousands using three widely-usedclassifiers as wrappers: nearest neighbor, decision tree, and linear supportvector machine. We compare our methodology against the full set of features aswell as a binary genetic algorithm and sequential FS methods usingcross-validated classification error rate and AUC as the performance criteria.Our results indicate that features selected by BSPSA compare favorably toalternative methods in general and BSPSA can yield superior feature sets fordatasets with tens of thousands of features by examining an extremely smallfraction of the solution space. We are not aware of any other wrapper FSmethods that are computationally feasible with good convergence properties forsuch large datasets.
arxiv-13200-167 | Dictionary Learning for Blind One Bit Compressed Sensing | http://arxiv.org/abs/1508.07648 | author:Hadi Zayyani, Mehdi Korki, Farrokh Marvasti category:stat.ML cs.IT math.IT published:2015-08-30 summary:This letter proposes a dictionary learning algorithm for blind one bitcompressed sensing. In the blind one bit compressed sensing framework, theoriginal signal to be reconstructed from one bit linear random measurements issparse in an unknown domain. In this context, the multiplication of measurementmatrix $\Ab$ and sparse domain matrix $\Phi$, \ie $\Db=\Ab\Phi$, should belearned. Hence, we use dictionary learning to train this matrix. Towards thatend, an appropriate continuous convex cost function is suggested for one bitcompressed sensing and a simple steepest-descent method is exploited to learnthe rows of the matrix $\Db$. Experimental results show the effectiveness ofthe proposed algorithm against the case of no dictionary learning, speciallywith increasing the number of training signals and the number of signmeasurements.
arxiv-13200-168 | An Event Network for Exploring Open Information | http://arxiv.org/abs/1508.07555 | author:Yanping Chen category:cs.CL published:2015-08-30 summary:In this paper, an event network is presented for exploring open information,where linguistic units about an event are organized for analysing. The processis divided into three steps: document event detection, event networkconstruction and event network analysis. First, by implementing event detectionor tracking, documents are retrospectively (or on-line) organized into documentevents. Secondly, for each of the document event, linguistic units areextracted and combined into event networks. Thirdly, various analytic methodsare proposed for event network analysis. In our application methodologies arepresented for exploring open information.
arxiv-13200-169 | X-TREPAN: a multi class regression and adapted extraction of comprehensible decision tree in artificial neural networks | http://arxiv.org/abs/1508.07551 | author:Awudu Karim, Shangbo Zhou category:cs.LG cs.NE published:2015-08-30 summary:In this work, the TREPAN algorithm is enhanced and extended for extractingdecision trees from neural networks. We empirically evaluated the performanceof the algorithm on a set of databases from real world events. This benchmarkenhancement was achieved by adapting Single-test TREPAN and C4.5 decision treeinduction algorithms to analyze the datasets. The models are then compared withX-TREPAN for comprehensibility and classification accuracy. Furthermore, wevalidate the experimentations by applying statistical methods. Finally, themodified algorithm is extended to work with multi-class regression problems andthe ability to comprehend generalized feed forward networks is achieved.
arxiv-13200-170 | Calibration of One-Class SVM for MV set estimation | http://arxiv.org/abs/1508.07535 | author:Albert Thomas, Vincent Feuillard, Alexandre Gramfort category:stat.ML published:2015-08-30 summary:A general approach for anomaly detection or novelty detection consists inestimating high density regions or Minimum Volume (MV) sets. The One-ClassSupport Vector Machine (OCSVM) is a state-of-the-art algorithm for estimatingsuch regions from high dimensional data. Yet it suffers from practicallimitations. When applied to a limited number of samples it can lead to poorperformance even when picking the best hyperparameters. Moreover the solutionof OCSVM is very sensitive to the selection of hyperparameters which makes ithard to optimize in an unsupervised setting. We present a new approach toestimate MV sets using the OCSVM with a different choice of the parametercontrolling the proportion of outliers. The solution function of the OCSVM islearnt on a training set and the desired probability mass is obtained byadjusting the offset on a test set to prevent overfitting. Models learnt ondifferent train/test splits are then aggregated to reduce the variance inducedby such random splits. Our approach makes it possible to tune thehyperparameters automatically and obtain nested set estimates. Experimentalresults show that our approach outperforms the standard OCSVM formulation whilesuffering less from the curse of dimensionality than kernel density estimates.Results on actual data sets are also presented.
arxiv-13200-171 | Mixed Gaussian-Impulse Noise Removal from Highly Corrupted Images via Adaptive Local and Nonlocal Statistical Priors | http://arxiv.org/abs/1508.07415 | author:Nasser Eslahi, Hami Mahdavinataj, Ali Aghagolzadeh category:cs.CV 62F15 published:2015-08-29 summary:The motivation of this paper is to introduce a novel framework for therestoration of images corrupted by mixed Gaussian-impulse noise. To this aim,first, an adaptive curvelet thresholding criterion is proposed which tries toadaptively remove the perturbations appeared during denoising process. Then, anew statistical regularization term, called joint adaptive statistical prior(JASP), is established which enforces both the local and nonlocal statisticalconsistencies, simultaneously, in a unified manner. Furthermore, a noveltechnique for mixed Gaussian plus impulse noise removal using JASP in avariational scheme is developed--we refer to it as De-JASP. To efficientlysolve the above variational scheme, an efficient alternating minimizationalgorithm based on split Bregman iterative framework is developed. Extensiveexperimental results manifest the effectiveness of the proposed methodcomparing with the current state-of-the-art methods in mixed Gaussian-impulsenoise removal.
arxiv-13200-172 | Generalized Uniformly Optimal Methods for Nonlinear Programming | http://arxiv.org/abs/1508.07384 | author:Saeed Ghadimi, Guanghui Lan, Hongchao Zhang category:math.OC stat.ML published:2015-08-29 summary:In this paper, we present a generic framework to extend existing uniformlyoptimal convex programming algorithms to solve more general nonlinear, possiblynonconvex, optimization problems. The basic idea is to incorporate a localsearch step (gradient descent or Quasi-Newton iteration) into these uniformlyoptimal convex programming methods, and then enforce a monotone decreasingproperty of the function values computed along the trajectory. Algorithms ofthese types will then achieve the best known complexity for nonconvex problems,and the optimal complexity for convex ones without requiring any problemparameters. As a consequence, we can have a unified treatment for a generalclass of nonlinear programming problems regardless of their convexity andsmoothness level. In particular, we show that the accelerated gradient andlevel methods, both originally designed for solving convex optimizationproblems only, can be used for solving both convex and nonconvex problemsuniformly. In a similar vein, we show that some well-studied techniques fornonlinear programming, e.g., Quasi-Newton iteration, can be embedded intooptimal convex optimization algorithms to possibly further enhance theirnumerical performance. Our theoretical and algorithmic developments arecomplemented by some promising numerical results obtained for solving a fewimportant nonconvex and nonlinear data analysis problems in the literature.
arxiv-13200-173 | Image Annotation Incorporating Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors | http://arxiv.org/abs/1508.07468 | author:Yuqing Hou category:cs.CV published:2015-08-29 summary:Tag-based image retrieval (TBIR) has drawn much attention in recent years dueto the explosive amount of digital images and crowdsourcing tags. However, TBIRis still suffering from the incomplete and inaccurate tags provided by users,posing a great challenge for tag-based image management applications. In thiswork, we proposed a novel method for image annotation, incorporating severalpriors: Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors.Highly representative CNN feature vectors are adopt to model the tag-visualcorrelation and narrow the semantic gap. And we extract word vectors for tagsto measure similarity between tags in the semantic level, which is moreaccurate than traditional frequency-based or graph-based methods. We utilizethe accelerated proximal gradient (APG) method to solve our model efficiently.Extensive experiments conducted on multiple benchmark datasets demonstrate theeffectiveness and robustness of the proposed method.
arxiv-13200-174 | Linked Component Analysis from Matrices to High Order Tensors: Applications to Biomedical Data | http://arxiv.org/abs/1508.07416 | author:Guoxu Zhou, Qibin Zhao, Yu Zhang, Tülay Adalı, Shengli Xie, Andrzej Cichocki category:cs.CE cs.LG cs.NA published:2015-08-29 summary:With the increasing availability of various sensor technologies, we now haveaccess to large amounts of multi-block (also called multi-set,multi-relational, or multi-view) data that need to be jointly analyzed toexplore their latent connections. Various component analysis methods haveplayed an increasingly important role for the analysis of such coupled data. Inthis paper, we first provide a brief review of existing matrix-based (two-way)component analysis methods for the joint analysis of such data with a focus onbiomedical applications. Then, we discuss their important extensions andgeneralization to multi-block multiway (tensor) data. We show how constrainedmulti-block tensor decomposition methods are able to extract similar orstatistically dependent common features that are shared by all blocks, byincorporating the multiway nature of data. Special emphasis is given to theflexible common and individual feature analysis of multi-block data with theaim to simultaneously extract common and individual latent components withdesired properties and types of diversity. Illustrative examples are given todemonstrate their effectiveness for biomedical data analysis.
arxiv-13200-175 | Multi-armed Bandit Problem with Known Trend | http://arxiv.org/abs/1508.07091 | author:Djallel Bouneffouf, Raphaël Feraud category:cs.LG I.2 published:2015-08-28 summary:We consider a variant of the multi-armed bandit model, which we callmulti-armed bandit problem with known trend, where the gambler knows the shapeof the reward function of each arm but not its distribution. This new problemis motivated by different online problems like active learning, music andinterface recommendation applications, where when an arm is sampled by themodel the received reward change according to a known trend. By adapting thestandard multi-armed bandit algorithm UCB1 to take advantage of this setting,we propose the new algorithm named A-UCB that assumes a stochastic model. Weprovide upper bounds of the regret which compare favourably with the ones ofUCB1. We also confirm that experimentally with different simulations
arxiv-13200-176 | Bilevel parameter learning for higher-order total variation regularisation models | http://arxiv.org/abs/1508.07243 | author:J. C. De los Reyes, C. -B. Schönlieb, T. Valkonen category:math.OC cs.CV published:2015-08-28 summary:We consider a bilevel optimisation approach for parameter learning inhigher-order total variation image reconstruction models. Apart from the leastsquares cost functional, naturally used in bilevel learning, we propose andanalyse an alternative cost, based on a Huber regularised TV-seminorm.Differentiability properties of the solution operator are verified and afirst-order optimality system is derived. Based on the adjoint information, aquasi-Newton algorithm is proposed for the numerical solution of the bilevelproblems. Numerical experiments are carried out to show the suitability of ourapproach and the improved performance of the new cost functional. Thanks to thebilevel optimisation framework, also a detailed comparison between TGV$^2$ andICTV is carried out, showing the advantages and shortcomings of bothregularisers, depending on the structure of the processed images and theirnoise level.
arxiv-13200-177 | Partitioning Large Scale Deep Belief Networks Using Dropout | http://arxiv.org/abs/1508.07096 | author:Yanping Huang, Sai Zhang category:stat.ML cs.LG cs.NE published:2015-08-28 summary:Deep learning methods have shown great promise in many practicalapplications, ranging from speech recognition, visual object recognition, totext processing. However, most of the current deep learning methods suffer fromscalability problems for large-scale applications, forcing researchers or usersto focus on small-scale problems with fewer parameters. In this paper, we consider a well-known machine learning model, deep beliefnetworks (DBNs) that have yielded impressive classification performance on alarge number of benchmark machine learning tasks. To scale up DBN, we proposean approach that can use the computing clusters in a distributed environment totrain large models, while the dense matrix computations within a single machineare sped up using graphics processors (GPU). When training a DBN, each machinerandomly drops out a portion of neurons in each hidden layer, for each trainingcase, making the remaining neurons only learn to detect features that aregenerally helpful for producing the correct answer. Within our approach, wehave developed four methods to combine outcomes from each machine to form aunified model. Our preliminary experiment on the mnst handwritten digitdatabase demonstrates that our approach outperforms the state of the art testerror rate.
arxiv-13200-178 | Regularized Kernel Recursive Least Square Algoirthm | http://arxiv.org/abs/1508.07103 | author:Songlin Zhao category:cs.LG stat.ML published:2015-08-28 summary:In most adaptive signal processing applications, system linearity is assumedand adaptive linear filters are thus used. The traditional class of supervisedadaptive filters rely on error-correction learning for their adaptivecapability. The kernel method is a powerful nonparametric modeling tool forpattern analysis and statistical signal processing. Through a nonlinearmapping, kernel methods transform the data into a set of points in aReproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fastconvergence rate in stationary scenario. However the good performance isobtained at a cost of high computation complexity. Sparsification in kernelmethods is know to related to less computational complexity and memoryconsumption.
arxiv-13200-179 | Parallel Dither and Dropout for Regularising Deep Neural Networks | http://arxiv.org/abs/1508.07130 | author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx published:2015-08-28 summary:Effective regularisation during training can mean the difference betweensuccess and failure for deep neural networks. Recently, dither has beensuggested as alternative to dropout for regularisation during batch-averagedstochastic gradient descent (SGD). In this article, we show that these methodsfail without batch averaging and we introduce a new, parallel regularisationmethod that may be used without batch averaging. Our results forparallel-regularised non-batch-SGD are substantially better than what ispossible with batch-SGD. Furthermore, our results demonstrate that dither anddropout are complimentary.
arxiv-13200-180 | Varying-coefficient models with isotropic Gaussian process priors | http://arxiv.org/abs/1508.07192 | author:Matthias Bussas, Christoph Sawade, Tobias Scheffer, Niels Landwehr category:cs.LG stat.ML I.2.6 published:2015-08-28 summary:We study learning problems in which the conditional distribution of theoutput given the input varies as a function of additional task variables. Invarying-coefficient models with Gaussian process priors, a Gaussian processgenerates the functional relationship between the task variables and theparameters of this conditional. Varying-coefficient models subsume hierarchicalBayesian multitask models, but also generalizations in which the conditionalvaries continuously, for instance, in time or space. However, Bayesianinference in varying-coefficient models is generally intractable. We show thatinference for varying-coefficient models with isotropic Gaussian process priorsresolves to standard inference for a Gaussian process that can be solvedefficiently. MAP inference in this model resolves to multitask learning usingtask and instance kernels, and inference for hierarchical Bayesian multitaskmodels can be carried out efficiently using graph-Laplacian kernels. We reporton experiments for geospatial prediction.
arxiv-13200-181 | Discrete Hashing with Deep Neural Network | http://arxiv.org/abs/1508.07148 | author:Thanh-Toan Do, Anh-Zung Doan, Ngai-Man Cheung category:cs.CV published:2015-08-28 summary:This paper addresses the problem of learning binary hash codes for largescale image search by proposing a novel hashing method based on deep neuralnetwork. The advantage of our deep model over previous deep model used inhashing is that our model contains necessary criteria for producing good codessuch as similarity preserving, balance and independence. Another advantage ofour method is that instead of relaxing the binary constraint of codes duringthe learning process as most previous works, in this paper, by introducing theauxiliary variable, we reformulate the optimization into two sub-optimizationsteps allowing us to efficiently solve binary constraints without anyrelaxation. The proposed method is also extended to the supervised hashing by leveragingthe label information such that the learned binary codes preserve the pairwiselabel of inputs. The experimental results on three benchmark datasets show the proposedmethods outperform state-of-the-art hashing methods.
arxiv-13200-182 | Competitive and Penalized Clustering Auto-encoder | http://arxiv.org/abs/1508.07175 | author:Zihao Wang, Yiuming Cheung category:cs.LG published:2015-08-28 summary:Auto-encoders (AE) has been widely applied in different fields of machinelearning. However, as a deep model, there are a large amount of learnableparameters in the AE, which would cause over-fitting and slow learning speed inpractice. Many researchers have been study the intrinsic structure of AE andshowed different useful methods to regularize those parameters. In this paper,we present a novel regularization method based on a clustering algorithm whichis able to classify the parameters into different groups. With thisregularization, parameters in a given group have approximate equivalent valuesand over-fitting problem could be alleviated. Moreover, due to the competitivebehavior of clustering algorithm, this model also overcomes some intrinsicproblems of clustering algorithms like the determination of number of clusters.Experiments on handwritten digits recognition verify the effectiveness of ournovel model.
arxiv-13200-183 | Understanding Editing Behaviors in Multilingual Wikipedia | http://arxiv.org/abs/1508.07266 | author:Suin Kim, Sungjoon Park, Scott A. Hale, Sooyoung Kim, Jeongmin Byun, Alice Oh category:cs.SI cs.CL cs.CY published:2015-08-28 summary:Multilingualism is common offline, but we have a more limited understandingof the ways multilingualism is displayed online and the roles thatmultilinguals play in the spread of content between speakers of differentlanguages. We take a computational approach to studying multilingualism usingone of the largest user-generated content platforms, Wikipedia. We studymultilingualism by collecting and analyzing a large dataset of the contentwritten by multilingual editors of the English, German, and Spanish editions ofWikipedia. This dataset contains over two million paragraphs edited by over15,000 multilingual users from July 8 to August 9, 2013. We analyze thesemultilingual editors in terms of their engagement, interests, and languageproficiency in their primary and non-primary (secondary) languages and findthat the English edition of Wikipedia displays different dynamics from theSpanish and German editions. Users primarily editing the Spanish and Germaneditions make more complex edits than users who edit these editions as a secondlanguage. In contrast, users editing the English edition as a second languagemake edits that are just as complex as the edits by users who primarily editthe English edition. In this way, English serves a special role bringingtogether content written by multilinguals from many language editions.Nonetheless, language remains a formidable hurdle to the spread of content: wefind evidence for a complexity barrier whereby editors are less likely to editcomplex content in a second language. In addition, we find that multilingualsare less engaged and show lower levels of language proficiency in their secondlanguages. We also examine the topical interests of multilingual editors andfind that there is no significant difference between primary and non-primaryeditors in each language.
arxiv-13200-184 | Rapid Exact Signal Scanning with Deep Convolutional Neural Networks | http://arxiv.org/abs/1508.06904 | author:Markus Thom, Franz Gritschneder category:cs.LG cs.CV cs.NE published:2015-08-27 summary:We introduce and analyze a rigorous formulation of the dynamics of a signalprocessing scheme aimed at exact dense signal scanning. Related methodsproposed in the recent past lack a satisfactory analysis whether they actuallyfulfill any exactness constraints. We improve on this through an exactcharacterization of the requirements for a sound sliding window approach. Thetools developed in this paper are especially beneficial if Convolutional NeuralNetworks are employed, but can also be used as a more general framework tovalidate related approaches to signal scanning. The contributed theory helps toeliminate redundant computations and renders special case treatmentunnecessary, resulting in a dramatic boost in efficiency particularly onmassively parallel processors. This is demonstrated both theoretically in acomputational complexity analysis and empirically on modern parallelprocessors.
arxiv-13200-185 | A biologically constrained model of the whole basal ganglia addressing the paradoxes of connections and selection | http://arxiv.org/abs/1512.00035 | author:Jean Liénard, Benoît Girard category:q-bio.NC cs.NE cs.RO published:2015-08-27 summary:The basal ganglia nuclei form a complex network of nuclei often assumed toperform selection, yet their individual roles and how they influence each otheris still largely unclear. In particular, the ties between the external andinternal parts of the globus pallidus are paradoxical, as anatomical datasuggest a potent inhibitory projection between them while electrophys-iologicalrecordings indicate that they have similar activities. Here we introduce atheoretical study that reconciles both views on the intra-pallidal projection,by providing a plausible characterization of the relationship between theexternal and internal globus pallidus. Specifically, we developed a mean-fieldmodel of the whole basal ganglia, whose parameterization is optimized torespect best a collection of numerous anatomical and electrophysiological data.We first obtained models respecting all our constraints, hence anatomical andelectrophysiological data on the intrapallidal projection are globallyconsistent. This model furthermore predicts that both aforementioned viewsabout the intra-pallidal projection may be reconciled when this projection isweakly inhibitory, thus making it possible to support similar neural activityin both nuclei and for the entire basal ganglia to select between actions.Second, we predicts that afferent projections are substantially unbalancedtowards the external segment, as it receives the strongest excitation from STNand the weakest inhibition from the striatum. Finally, our study stronglysuggest that the intrapallidal connection pattern is not focused but diffuse,as this latter pattern is more efficient for the overall selection performed inthe basal ganglia.
arxiv-13200-186 | Introducing Elitist Black-Box Models: When Does Elitist Selection Weaken the Performance of Evolutionary Algorithms? | http://arxiv.org/abs/1508.06802 | author:Carola Doerr, Johannes Lengler category:cs.NE cs.DS published:2015-08-27 summary:Black-box complexity theory provides lower bounds for the runtime ofblack-box optimizers like evolutionary algorithms and serves as an inspirationfor the design of new genetic algorithms. Several black-box models coveringdifferent classes of algorithms exist, each highlighting a different aspect ofthe algorithms under considerations. In this work we add to the existingblack-box notions a new \emph{elitist black-box model}, in which algorithms arerequired to base all decisions solely on (a fixed number of) the best searchpoints sampled so far. Our model combines features of the ranking-based and thememory-restricted black-box models with elitist selection. We provide several examples for which the elitist black-box complexity isexponentially larger than that the respective complexities in all previousblack-box models, thus showing that the elitist black-box complexity can bemuch closer to the runtime of typical evolutionary algorithms. We also introduce the concept of $p$-Monte Carlo black-box complexity, whichmeasures the time it takes to optimize a problem with failure probability atmost $p$. Even for small~$p$, the $p$-Monte Carlo black-box complexity of afunction class $\mathcal F$ can be smaller by an exponential factor than itstypically regarded Las Vegas complexity (which measures the \emph{expected}time it takes to optimize $\mathcal F$).
arxiv-13200-187 | Online Anomaly Detection via Class-Imbalance Learning | http://arxiv.org/abs/1508.06717 | author:Chandresh Kumar Maurya, Durga Toshniwal, Gopalan Vijendran Venkoparao category:cs.LG published:2015-08-27 summary:Anomaly detection is an important task in many real world applications suchas fraud detection, suspicious activity detection, health care monitoring etc.In this paper, we tackle this problem from supervised learning perspective inonline learning setting. We maximize well known \emph{Gmean} metric forclass-imbalance learning in online learning framework. Specifically, we showthat maximizing \emph{Gmean} is equivalent to minimizing a convex surrogateloss function and based on that we propose novel online learning algorithm foranomaly detection. We then show, by extensive experiments, that the performanceof the proposed algorithm with respect to $sum$ metric is as good as a recentlyproposed Cost-Sensitive Online Classification(CSOC) algorithm forclass-imbalance learning over various benchmarked data sets while keepingrunning time close to the perception algorithm. Our another conclusion is thatother competitive online algorithms do not perform consistently over data setsof varying size. This shows the potential applicability of our proposedapproach.
arxiv-13200-188 | Validation of neural spike sorting algorithms without ground-truth information | http://arxiv.org/abs/1508.06936 | author:Alex H. Barnett, Jeremy F. Magland, Leslie F. Greengard category:q-bio.NC cs.CV published:2015-08-27 summary:We describe a suite of validation metrics that assess the credibility of agiven automatic spike sorting algorithm applied to a given electrophysiologicalrecording, when ground-truth is unavailable. By rerunning the spike sorter twoor more times, the metrics measure stability under various perturbationsconsistent with variations in the data itself, making no assumptions about thenoise model, nor about the internal workings of the sorting algorithm. Suchstability is a prerequisite for reproducibility of results. We illustrate themetrics on standard sorting algorithms for both in vivo and ex vivo recordings.We believe that such metrics could reduce the significant human labor currentlyspent on validation, and should form an essential part of large-scale automatedspike sorting and systematic benchmarking of algorithms.
arxiv-13200-189 | Image Type Water Meter Character Recognition Based on Embedded DSP | http://arxiv.org/abs/1508.06725 | author:Ying Liu, Yan-bin Han, Yu-lin Zhang category:cs.CV published:2015-08-27 summary:In the paper, we combined DSP processor with image processing algorithm andstudied the method of water meter character recognition. We collected watermeter image through camera at a fixed angle, and the projection method is usedto recognize those digital images. The experiment results show that the methodcan recognize the meter characters accurately and artificial meter reading isreplaced by automatic digital recognition, which improves working efficiency.
arxiv-13200-190 | Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation | http://arxiv.org/abs/1508.06708 | author:Sijin Li, Weichen Zhang, Antoni B. Chan category:cs.CV published:2015-08-27 summary:This paper focuses on structured-output learning using deep neural networksfor 3D human pose estimation from monocular images. Our network takes an imageand 3D pose as inputs and outputs a score value, which is high when theimage-pose pair matches and low otherwise. The network structure consists of aconvolutional neural network for image feature extraction, followed by twosub-networks for transforming the image features and pose into a jointembedding. The score function is then the dot-product between the image andpose embeddings. The image-pose embedding and score function are jointlytrained using a maximum-margin cost function. Our proposed framework can beinterpreted as a special form of structured support vector machines where thejoint feature space is discriminatively learned using deep neural networks. Wetest our framework on the Human3.6m dataset and obtain state-of-the-art resultscompared to other recent methods. Finally, we present visualizations of theimage-pose embedding space, demonstrating the network has learned a high-levelembedding of body-orientation and pose-configuration.
arxiv-13200-191 | Continuous parameter working memory in a balanced chaotic neural network | http://arxiv.org/abs/1508.06944 | author:Nimrod Shaham, Yoram Burak category:cs.NE q-bio.NC published:2015-08-27 summary:Working memory, the ability to maintain information over time scales greaterthan those characterizing single neurons, is essential to many brain functions.It remains unclear whether neural networks in the balanced state, an importantmodel for activity in the cortex, can support a continuum of stable states thatwould make it possible to store a continuous variable in working memory whilealso accounting for the stochastic behavior of single neurons. Here we proposea simple neural architecture that achieves this goal. We show analytically thatin the limit of an infinite network a continuous parameter can be storedindefinitely on a continuum of balanced states. For finite networks wecalculate the diffusivity along the attractor driven by the chaotic noise inthe network, and show that it is inversely proportional to the system size.Thus, for large enough (but realistic) neural population sizes, and withsuitable tuning of the network connections, it is possible to maintaincontinuous parameter values over time scales larger by several orders ofmagnitude than the single neuron time scale.
arxiv-13200-192 | Compressive Sensing via Low-Rank Gaussian Mixture Models | http://arxiv.org/abs/1508.06901 | author:Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford category:stat.ML cs.LG stat.AP published:2015-08-27 summary:We develop a new compressive sensing (CS) inversion algorithm by utilizingthe Gaussian mixture model (GMM). While the compressive sensing is performedglobally on the entire image as implemented in our lensless camera, a low-rankGMM is imposed on the local image patches. This low-rank GMM is derived viaeigenvalue thresholding of the GMM trained on the projection of the measurementdata, thus learned {\em in situ}. The GMM and the projection of the measurementdata are updated iteratively during the reconstruction. Our GMM algorithmdegrades to the piecewise linear estimator (PLE) if each patch is representedby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is alsodeveloped for CS inversion, constituting an additional contribution of thispaper. Extensive results on both simulation data and real data captured by thelensless camera demonstrate the efficacy of the proposed algorithm.Furthermore, we compare the CS reconstruction results using our algorithm withthe JPEG compression. Simulation results demonstrate that when limitedbandwidth is available (a small number of measurements), our algorithm canachieve comparable results as JPEG.
arxiv-13200-193 | Shopper Analytics: a customer activity recognition system using a distributed RGB-D camera network | http://arxiv.org/abs/1508.06853 | author:Daniele Liciotti, Marco Contigiani, Emanuele Frontoni, Adriano Mancini, Primo Zingaretti, Valerio Placidi category:cs.CV published:2015-08-27 summary:The aim of this paper is to present an integrated system consisted of a RGB-Dcamera and a software able to monitor shoppers in intelligent retailenvironments. We propose an innovative low cost smart system that canunderstand the shoppers' behavior and, in particular, their interactions withthe products in the shelves, with the aim to develop an automatic RGB-Dtechnique for video analysis. The system of cameras detects the presence ofpeople and univocally identifies them. Through the depth frames, the systemdetects the interactions of the shoppers with the products on the shelf anddetermines if a product is picked up or if the product is taken and then putback and finally, if there is not contact with the products. The system is lowcost and easy to install, and experimental results demonstrated that itsperformances are satisfactory also in real environments.
arxiv-13200-194 | Encrypted statistical machine learning: new privacy preserving methods | http://arxiv.org/abs/1508.06845 | author:Louis J. M. Aslett, Pedro M. Esperança, Chris C. Holmes category:stat.ML cs.CR cs.LG stat.ME published:2015-08-27 summary:We present two new statistical machine learning methods designed to learn onfully homomorphic encrypted (FHE) data. The introduction of FHE schemesfollowing Gentry (2009) opens up the prospect of privacy preserving statisticalmachine learning analysis and modelling of encrypted data without compromisingsecurity constraints. We propose tailored algorithms for applying extremelyrandom forests, involving a new cryptographic stochastic fraction estimator,and na\"{i}ve Bayes, involving a semi-parametric model for the class decisionboundary, and show how they can be used to learn and predict from encrypteddata. We demonstrate that these techniques perform competitively on a varietyof classification data sets and provide detailed information about thecomputational practicalities of these and other FHE methods.
arxiv-13200-195 | A Comparative Analysis of Retrieval Techniques In Content Based Image Retrieval | http://arxiv.org/abs/1508.06728 | author:Mohini P. Sardey, G. K. Kharate category:cs.CV 68 published:2015-08-27 summary:Basic group of visual techniques such as color, shape, texture are used inContent Based Image Retrievals (CBIR) to retrieve query image or subregion ofimage to find similar images in image database. To improve query result,relevance feedback is used many times in CBIR to help user to express theirpreference and improve query results.In this paper, a new approach for imageretrieval is proposed which is based on the features such as Color Histogram,Eigen Values and Match Point. Images from various types of database are firstidentified by using edge detection techniques.Once the image is identified,then the image is searched in the particular database, then all related imagesare displayed. This will save the retrieval time. Further to retrieve theprecise query image, any of the three techniques are used and comparison isdone w.r.t. average retrieval time. Eigen value technique found to be the bestas compared with other two techniques
arxiv-13200-196 | Nucleosome positioning: resources and tools online | http://arxiv.org/abs/1508.06916 | author:Vladimir B. Teif category:q-bio.GN physics.bio-ph q-bio.BM stat.ML published:2015-08-27 summary:Nucleosome positioning is an important process required for proper genomepacking and its accessibility to execute the genetic program in acell-specific, timely manner. In the recent years hundreds of papers have beendevoted to the bioinformatics, physics and biology of nucleosome positioning.The purpose of this review is to cover a practical aspect of this field, namelyto provide a guide to the multitude of nucleosome positioning resourcesavailable online. These include almost 300 experimental datasets of genome-widenucleosome occupancy profiles determined in different cell types and more than40 computational tools for the analysis of experimental nucleosome positioningdata and prediction of intrinsic nucleosome formation probabilities from theDNA sequence. A manually curated, up to date list of these resources will bemaintained at http://generegulation.info.
arxiv-13200-197 | Using Genetic Algorithms to Benchmark the Cloud | http://arxiv.org/abs/1508.06705 | author:Jeff Kinnison, Sekou L. Remy category:cs.DC cs.NE cs.PF published:2015-08-27 summary:This paper presents a novel application of Genetic Algorithms(GAs) toquantify the performance of Platform as a Service (PaaS), a cloud service modelthat plays a critical role in both industry and academia. While Cloudbenchmarks are not new, in this novel concept, the authors use a GA to takeadvantage of the elasticity in Cloud services in a graceful manner that was notpreviously possible. Using Google App Engine, Heroku, and Python Anywhere withthree distinct classes of client computers running our GA codebase, wequantified the completion time for application of the GA to search for theparameters of controllers for dynamical systems. Our results show statisticallysignificant differences in PaaS performance by vendor, and also that theperformance of the PaaS performance is dependent upon the client that uses it.Results also show the effectiveness of our GA in determining the level ofservice of PaaS providers, and for determining if the level of service of onePaaS vendor is repeatable with another. Such a concept could then increase theappeal of PaaS Cloud services by making them more financially appealing.
arxiv-13200-198 | Alignment-based compositional semantics for instruction following | http://arxiv.org/abs/1508.06491 | author:Jacob Andreas, Dan Klein category:cs.CL published:2015-08-26 summary:This paper describes an alignment-based model for interpreting naturallanguage instructions in context. We approach instruction following as a searchover plans, scoring sequences of actions conditioned on structured observationsof text and the environment. By explicitly modeling both the low-levelcompositional structure of individual actions and the high-level structure offull plans, we are able to learn both grounded representations of sentencemeaning and pragmatic constraints on interpretation. To demonstrate the model'sflexibility, we apply it to a diverse set of benchmark tasks. On every task, weoutperform strong task-specific baselines, and achieve several newstate-of-the-art results.
arxiv-13200-199 | Financial Market Modeling with Quantum Neural Networks | http://arxiv.org/abs/1508.06586 | author:Carlos Pedro Gonçalves category:q-fin.CP cs.NE physics.soc-ph q-fin.GN published:2015-08-26 summary:Econophysics has developed as a research field that applies the formalism ofStatistical Mechanics and Quantum Mechanics to address Economics and Financeproblems. The branch of Econophysics that applies of Quantum Theory toEconomics and Finance is called Quantum Econophysics. In Finance, QuantumEconophysics' contributions have ranged from option pricing to market dynamicsmodeling, behavioral finance and applications of Game Theory, integrating theempirical finding, from human decision analysis, that shows that nonlinearupdate rules in probabilities, leading to non-additive decision weights, can becomputationally approached from quantum computation, with resulting quantuminterference terms explaining the non-additive probabilities. The current workdraws on these results to introduce new tools from Quantum ArtificialIntelligence, namely Quantum Artificial Neural Networks as a way to build andsimulate financial market models with adaptive selection of trading rules,leading to turbulence and excess kurtosis in the returns distributions for awide range of parameters.
arxiv-13200-200 | Character-Aware Neural Language Models | http://arxiv.org/abs/1508.06615 | author:Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush category:cs.CL cs.NE stat.ML published:2015-08-26 summary:We describe a simple neural language model that relies only oncharacter-level inputs. Predictions are still made at the word-level. Our modelemploys a convolutional neural network (CNN) and a highway network overcharacters, whose output is given to a long short-term memory (LSTM) recurrentneural network language model (RNN-LM). On the English Penn Treebank the modelis on par with the existing state-of-the-art despite having 60% fewerparameters. On languages with rich morphology (Arabic, Czech, French, German,Spanish, Russian), the model outperforms word-level/morpheme-level LSTMbaselines, again with fewer parameters. The results suggest that on manylanguages, character inputs are sufficient for language modeling. Analysis ofword representations obtained from the character composition part of the modelreveals that the model is able to encode, from characters only, both semanticand orthographic information.
arxiv-13200-201 | SPRIGHT: A Fast and Robust Framework for Sparse Walsh-Hadamard Transform | http://arxiv.org/abs/1508.06336 | author:Xiao Li, Joseph K. Bradley, Sameer Pawar, Kannan Ramchandran category:cs.IT cs.LG math.IT published:2015-08-26 summary:We consider the problem of computing the Walsh-Hadamard Transform (WHT) ofsome $N$-length input vector in the presence of noise, where the $N$-pointWalsh spectrum is $K$-sparse with $K = {O}(N^{\delta})$ scaling sub-linearly inthe input dimension $N$ for some $0<\delta<1$. Over the past decade, there hasbeen a resurgence in research related to the computation of Discrete FourierTransform (DFT) for some length-$N$ input signal that has a $K$-sparse Fourierspectrum. In particular, through a sparse-graph code design, our earlier workon the Fast Fourier Aliasing-based Sparse Transform (FFAST) algorithm computesthe $K$-sparse DFT in time ${O}(K\log K)$ by taking ${O}(K)$ noiseless samples.Inspired by the coding-theoretic design framework, Scheibler et al. proposedthe Sparse Fast Hadamard Transform (SparseFHT) algorithm that elegantlycomputes the $K$-sparse WHT in the absence of noise using ${O}(K\log N)$samples in time ${O}(K\log^2 N)$. However, the SparseFHT algorithm explicitlyexploits the noiseless nature of the problem, and is not equipped to deal withscenarios where the observations are corrupted by noise. Therefore, a questionof critical interest is whether this coding-theoretic framework can be maderobust to noise. Further, if the answer is yes, what is the extra price thatneeds to be paid for being robust to noise? In this paper, we show, quiteinterestingly, that there is {\it no extra price} that needs to be paid forbeing robust to noise other than a constant factor. In other words, we canmaintain the same sample complexity ${O}(K\log N)$ and the computationalcomplexity ${O}(K\log^2 N)$ as those of the noiseless case, using our SParseRobust Iterative Graph-based Hadamard Transform (SPRIGHT) algorithm.
arxiv-13200-202 | Gaussian Mixture Models with Component Means Constrained in Pre-selected Subspaces | http://arxiv.org/abs/1508.06388 | author:Mu Qiao, Jia Li category:stat.ML cs.LG published:2015-08-26 summary:We investigate a Gaussian mixture model (GMM) with component meansconstrained in a pre-selected subspace. Applications to classification andclustering are explored. An EM-type estimation algorithm is derived. We provethat the subspace containing the component means of a GMM with a commoncovariance matrix also contains the modes of the density and the class means.This motivates us to find a subspace by applying weighted principal componentanalysis to the modes of a kernel density and the class means. To circumventthe difficulty of deciding the kernel bandwidth, we acquire multiple subspacesfrom the kernel densities based on a sequence of bandwidths. The GMMconstrained by each subspace is estimated; and the model yielding the maximumlikelihood is chosen. A dimension reduction property is proved in the sense ofbeing informative for classification or clustering. Experiments on real andsimulated data sets are conducted to examine several ways of determining thesubspace and to compare with the reduced rank mixture discriminant analysis(MDA). Our new method with the simple technique of spanning the subspace onlyby class means often outperforms the reduced rank MDA when the subspacedimension is very low, making it particularly appealing for visualization.
arxiv-13200-203 | Component-Enhanced Chinese Character Embeddings | http://arxiv.org/abs/1508.06669 | author:Yanran Li, Wenjie Li, Fei Sun, Sujian Li category:cs.CL published:2015-08-26 summary:Distributed word representations are very useful for capturing semanticinformation and have been successfully applied in a variety of NLP tasks,especially on English. In this work, we innovatively develop twocomponent-enhanced Chinese character embedding models and their bigramextensions. Distinguished from English word embeddings, our models explore thecompositions of Chinese characters, which often serve as semantic indictorsinherently. The evaluations on both word similarity and text classificationdemonstrate the effectiveness of our models.
arxiv-13200-204 | Crossings as a side effect of dependency lengths | http://arxiv.org/abs/1508.06451 | author:Ramon Ferrer-i-Cancho, Carlos Gómez-Rodríguez category:cs.CL cs.SI physics.soc-ph published:2015-08-26 summary:The syntactic structure of sentences exhibits a striking regularity:dependencies tend to not cross when drawn above the sentence. Here weinvestigate two competing hypotheses for the origins of non-crossingdependencies. The traditional hypothesis is that the low frequency ofdependency crossings arises from an independent principle of syntax thatreduces crossings practically to zero. An alternative to this view is thehypothesis that crossings are a side effect of dependency lengths. According tothis view, sentences with shorter dependency lengths should tend to have fewercrossings. We recast the traditional view as a null hypothesis where one of thevariables, i.e. the number of crossings, is mean independent of the other, i.e.the sum of dependency lengths. The alternative view is then a positivecorrelation between these two variables. In spite of the rough estimation ofdependency crossings that this sum provides, we are able to reject thetraditional view in the majority of languages considered. The alternativehypothesis can lead to a more parsimonious theory of syntax.
arxiv-13200-205 | Population Synthesis via k-Nearest Neighbor Crossover Kernel | http://arxiv.org/abs/1508.06483 | author:Naoki Hamada, Katsumi Homma, Hiroyuki Higuchi, Hideyuki Kikuchi category:cs.NE published:2015-08-26 summary:The recent development of multi-agent simulations brings about a need forpopulation synthesis. It is a task of reconstructing the entire population froma sampling survey of limited size (1% or so), supplying the initial conditionsfrom which simulations begin. This paper presents a new kernel densityestimator for this task. Our method is an analogue of the classicalBreiman-Meisel-Purcell estimator, but employs novel techniques that harness thehuge degree of freedom which is required to model high-dimensional nonlinearlycorrelated datasets: the crossover kernel, the k-nearest neighbor restrictionof the kernel construction set and the bagging of kernels. The performance as astatistical estimator is examined through real and synthetic datasets. Weprovide an "optimization-free" parameter selection rule for our method, atheory of how our method works and a computational cost analysis. Todemonstrate the usefulness as a population synthesizer, our method is appliedto a household synthesis task for an urban micro-simulator.
arxiv-13200-206 | SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter | http://arxiv.org/abs/1508.06464 | author:Osamu Hirose, Shotaro Kawaguchi, Terumasa Tokunaga, Yu Toyoshima, Takayuki Teramoto, Sayuri Kuge, Takeshi Ishihara, Yuichi Iino, Ryo Yoshida category:cs.CV published:2015-08-26 summary:Tracking many cells in time-lapse 3D image sequences is an importantchallenging task of bioimage informatics. Motivated by a study of brain-wide 4Dimaging of neural activity in C. elegans, we present a new method of multi-celltracking. Data types to which the method is applicable are characterized asfollows: (i) cells are imaged as globular-like objects, (ii) it is difficult todistinguish cells based only on shape and size, (iii) the number of imagedcells ranges in several hundreds, (iv) moves of nearly-located cells arestrongly correlated and (v) cells do not divide. We developed a trackingsoftware suite which we call SPF-CellTracker. Incorporating dependency oncells' moves into prediction model is the key to reduce the tracking errors:cell-switching and coalescence of tracked positions. We model target cells'correlated moves as a Markov random field and we also derive a fast computationalgorithm, which we call spatial particle filter. With the live-imaging data ofnuclei of C. elegans neurons in which approximately 120 nuclei of neurons areimaged, we demonstrate an advantage of the proposed method over the standardparticle filter and a method developed by Tokunaga et al. (2014).
arxiv-13200-207 | A review of homomorphic encryption and software tools for encrypted statistical machine learning | http://arxiv.org/abs/1508.06574 | author:Louis J. M. Aslett, Pedro M. Esperança, Chris C. Holmes category:stat.ML cs.CR cs.LG published:2015-08-26 summary:Recent advances in cryptography promise to enable secure statisticalcomputation on encrypted data, whereby a limited set of operations can becarried out without the need to first decrypt. We review these homomorphicencryption schemes in a manner accessible to statisticians and machinelearners, focusing on pertinent limitations inherent in the current state ofthe art. These limitations restrict the kind of statistics and machine learningalgorithms which can be implemented and we review those which have beensuccessfully applied in the literature. Finally, we document a high performanceR package implementing a recent homomorphic scheme in a general framework.
arxiv-13200-208 | Towards universal neural nets: Gibbs machines and ACE | http://arxiv.org/abs/1508.06585 | author:Galin Georgiev category:cs.CV cs.LG cs.NE published:2015-08-26 summary:We study a class of neural nets - \emph{Gibbs machines} - which are a type ofvariational auto-encoders, designed for gradual learning. They offer anuniversal platform for incrementally adding newly learned features, includingphysical symmetries, and are directly connected to information geometry andthermodynamics. Combining them with classifiers, gives rise to a brand ofuniversal generative neural nets - stochastic auto-classifier-encoders (ACE).ACE have state-of-the-art performance in their class, both for classificationand density estimation for the MNIST data set.
arxiv-13200-209 | A Neural Algorithm of Artistic Style | http://arxiv.org/abs/1508.06576 | author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV cs.NE q-bio.NC published:2015-08-26 summary:In fine art, especially painting, humans have mastered the skill to createunique visual experiences through composing a complex interplay between thecontent and style of an image. Thus far the algorithmic basis of this processis unknown and there exists no artificial system with similar capabilities.However, in other key areas of visual perception such as object and facerecognition near-human performance was recently demonstrated by a class ofbiologically inspired vision models called Deep Neural Networks. Here weintroduce an artificial system based on a Deep Neural Network that createsartistic images of high perceptual quality. The system uses neuralrepresentations to separate and recombine content and style of arbitraryimages, providing a neural algorithm for the creation of artistic images.Moreover, in light of the striking similarities between performance-optimisedartificial neural networks and biological vision, our work offers a pathforward to an algorithmic understanding of how humans create and perceiveartistic imagery.
arxiv-13200-210 | Deep Convolutional Neural Networks for Smile Recognition | http://arxiv.org/abs/1508.06535 | author:Patrick O. Glauner category:cs.CV cs.LG cs.NE published:2015-08-26 summary:This thesis describes the design and implementation of a smile detector basedon deep convolutional neural networks. It starts with a summary of neuralnetworks, the difficulties of training them and new training methods, such asRestricted Boltzmann Machines or autoencoders. It then provides a literaturereview of convolutional neural networks and recurrent neural networks. In orderto select databases for smile recognition, comprehensive statistics ofdatabases popular in the field of facial expression recognition were generatedand are summarized in this thesis. It then proposes a model for smiledetection, of which the main part is implemented. The experimental results arediscussed in this thesis and justified based on a comprehensive model selectionperformed. All experiments were run on a Tesla K40c GPU benefiting from aspeedup of up to factor 10 over the computations on a CPU. A smile detectiontest accuracy of 99.45% is achieved for the Denver Intensity of SpontaneousFacial Action (DISFA) database, significantly outperforming existing approacheswith accuracies ranging from 65.55% to 79.67%. This experiment is re-run undervarious variations, such as retaining less neutral images or only the low orhigh intensities, of which the results are extensively compared.
arxiv-13200-211 | Greedy methods, randomization approaches and multi-arm bandit algorithms for efficient sparsity-constrained optimization | http://arxiv.org/abs/1508.06477 | author:A. Rakotomamonjy, S. Koço, L. Ralaivola category:cs.LG published:2015-08-26 summary:Several sparsity-constrained algorithms such as Orthogonal Matching Pursuitor the Frank-Wolfe algorithm with sparsity constraints work by iterativelyselecting a novel atom to add to the current non-zero set of variables. Thisselection step is usually performed by computing the gradient and then bylooking for the gradient component with maximal absolute entry. This step canbe computationally expensive especially for large-scale and high-dimensionaldata. In this work, we aim at accelerating these sparsity-constrainedoptimization algorithms by exploiting the key observation that, for thesealgorithms to work, one only needs the coordinate of the gradient's top entry.Hence, we introduce algorithms based on greedy methods and randomizationapproaches that aim at cheaply estimating the gradient and its top entry.Another of our contribution is to cast the problem of finding the best gradiententry as a best arm identification in a multi-armed bandit problem. Owing tothis novel insight, we are able to provide a bandit-based algorithm thatdirectly estimates the top entry in a very efficient way. Theoretical resultsstating that the resulting inexact Frank-Wolfe or Orthogonal Matching Pursuitalgorithms act, with high probability, similarly to their exact versions arealso given. We have carried out several experiments showing that the greedydeterministic and the bandit approaches we propose can achieve an accelerationof an order of magnitude while being as efficient as the exact gradient whenused in algorithms such as
arxiv-13200-212 | A fully data-driven method to identify (correlated) changes in diachronic corpora | http://arxiv.org/abs/1508.06374 | author:Alexander Koplenig category:cs.CL cs.IR stat.AP published:2015-08-26 summary:In this paper, a method for measuring synchronic corpus (dis-)similarity putforward by Kilgarriff (2001) is adapted and extended to identify trends andcorrelated changes in diachronic text data, using the Corpus of HistoricalAmerican English (Davies 2010a) and the Google Ngram Corpora (Michel et al.2010a). This paper shows that this fully data-driven method, which extractsword types that have undergone the most pronounced change in frequency in agiven period of time, is computationally very cheap and that it allowsinterpretations of diachronic trends that are both intuitively plausible andmotivated from the perspective of information theory. Furthermore, itdemonstrates that the method is able to identify correlated linguistic changesand diachronic shifts that can be linked to historical events. Finally, it canhelp to improve diachronic POS tagging and complement existing NLP approaches.This indicates that the approach can facilitate an improved understanding ofdiachronic processes in language change.
arxiv-13200-213 | Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric Admixture Modeling | http://arxiv.org/abs/1508.06446 | author:Lavanya Sita Tekumalla, Priyanka Agrawal, Indrajit Bhattacharya category:stat.ML cs.LG published:2015-08-26 summary:Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixturemodeling, where the number of mixture components grows with the number of dataitems. The Hierarchical Dirichlet Process (HDP), is an extension of DP forgrouped data, often used for non-parametric topic modeling, where each group isa mixture over shared mixture densities. The Nested Dirichlet Process (nDP), onthe other hand, is an extension of the DP for learning group leveldistributions from data, simultaneously clustering the groups. It allows grouplevel distributions to be shared across groups in a non-parametric setting,leading to a non-parametric mixture of mixtures. The nCRF extends the nDP formultilevel non-parametric mixture modeling, enabling modeling topichierarchies. However, the nDP and nCRF do not allow sharing of distributions asrequired in many applications, motivating the need for multi-levelnon-parametric admixture modeling. We address this gap by proposing multi-levelnested HDPs (nHDP) where the base distribution of the HDP is itself a HDP ateach level thereby leading to admixtures of admixtures at each level. Becauseof couplings between various HDP levels, scaling up is naturally a challengeduring inference. We propose a multi-level nested Chinese Restaurant Franchise(nCRF) representation for the nested HDP, with which we outline an inferencealgorithm based on Gibbs Sampling. We evaluate our model with the two levelnHDP for non-parametric entity topic modeling where an inner HDP creates acountably infinite set of topic mixtures and associates them with authorentities, while an outer HDP associates documents with these author entities.In our experiments on two real world research corpora, the nHDP is able togeneralize significantly better than existing models and detect missing authorentities with a reasonable level of accuracy.
arxiv-13200-214 | OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based Neural Training | http://arxiv.org/abs/1508.06095 | author:Rossella Cancelliere, Mario Gai, Patrick Gallinari, Luca Rubini category:cs.NE cs.LG stat.ML published:2015-08-25 summary:In this paper we consider the training of single hidden layer neural networksby pseudoinversion, which, in spite of its popularity, is sometimes affected bynumerical instability issues. Regularization is known to be effective in suchcases, so that we introduce, in the framework of Tikhonov regularization, amatricial reformulation of the problem which allows us to use the conditionnumber as a diagnostic tool for identification of instability. By imposingwell-conditioning requirements on the relevant matrices, our theoreticalanalysis allows the identification of an optimal value for the regularizationparameter from the standpoint of stability. We compare with the value derivedby cross-validation for overfitting control and optimisation of thegeneralization performance. We test our method for both regression andclassification tasks. The proposed method is quite effective in terms ofpredictivity, often with some improvement on performance with respect to thereference cases considered. This approach, due to analytical determination ofthe regularization parameter, dramatically reduces the computational loadrequired by many other techniques.
arxiv-13200-215 | Robot Language Learning, Generation, and Comprehension | http://arxiv.org/abs/1508.06161 | author:Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, Jeffrey Mark Siskind category:cs.RO cs.AI cs.CL cs.HC cs.LG published:2015-08-25 summary:We present a unified framework which supports grounding natural-languagesemantics in robotic driving. This framework supports acquisition (learninggrounded meanings of nouns and prepositions from human annotation of roboticdriving paths), generation (using such acquired meanings to generate sententialdescription of new robotic driving paths), and comprehension (using suchacquired meanings to support automated driving to accomplish navigational goalsspecified in natural language). We evaluate the performance of these threetasks by having independent human judges rate the semantic fidelity of thesentences associated with paths, achieving overall average correctness of 94.6%and overall average completeness of 85.6%.
arxiv-13200-216 | Visualizing NLP annotations for Crowdsourcing | http://arxiv.org/abs/1508.06044 | author:Hanchuan Li, Haichen Shen, Shengliang Xu, Congle Zhang category:cs.CL published:2015-08-25 summary:Visualizing NLP annotation is useful for the collection of training data forthe statistical NLP approaches. Existing toolkits either provide limited visualaid, or introduce comprehensive operators to realize sophisticated linguisticrules. Workers must be well trained to use them. Their audience thus can hardlybe scaled to large amounts of non-expert crowdsourced workers. In this paper,we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers toannotate two general categories of NLP problems: clustering and parsing.Workers can finish the tasks with simplified operators in an interactiveinterface, and fix errors conveniently. User studies show our toolkit is veryfriendly to NLP non-experts, and allow them to produce high quality labels forseveral sophisticated problems. We release our source code and toolkit to spurfuture research.
arxiv-13200-217 | ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution | http://arxiv.org/abs/1508.06013 | author:Zeinab Bahmani, Leopoldo Bertossi, Nikolaos Vasiloglou category:cs.DB cs.AI cs.LG published:2015-08-25 summary:Entity resolution (ER), an important and common data cleaning problem, isabout detecting data duplicate representations for the same external entities,and merging them into single representations. Relatively recently, declarativerules called matching dependencies (MDs) have been proposed for specifyingsimilarity conditions under which attribute values in database records aremerged. In this work we show the process and the benefits of integrating threecomponents of ER: (a) Classifiers for duplicate/non-duplicate record pairsbuilt using machine learning (ML) techniques, (b) MDs for supporting both theblocking phase of ML and the merge itself; and (c) The use of the declarativelanguage LogiQL -an extended form of Datalog supported by the LogicBloxplatform- for data processing, and the specification and enforcement of MDs.
arxiv-13200-218 | Better Summarization Evaluation with Word Embeddings for ROUGE | http://arxiv.org/abs/1508.06034 | author:Jun-Ping Ng, Viktoria Abrecht category:cs.CL cs.IR published:2015-08-25 summary:ROUGE is a widely adopted, automatic evaluation measure for textsummarization. While it has been shown to correlate well with human judgements,it is biased towards surface lexical similarities. This makes it unsuitable forthe evaluation of abstractive summarization, or summaries with substantialparaphrasing. We study the effectiveness of word embeddings to overcome thisdisadvantage of ROUGE. Specifically, instead of measuring lexical overlaps,word embeddings are used to compute the semantic similarity of the words usedin summaries instead. Our experimental results show that our proposal is ableto achieve better correlations with human judgements when measured with theSpearman and Kendall rank coefficients.
arxiv-13200-219 | BREN: Body Reflection Essence-Neuter Model for Separation of Reflection Components | http://arxiv.org/abs/1508.06171 | author:Changsoo Je, Hyung-Min Park category:cs.CV cs.GR physics.optics published:2015-08-25 summary:We propose a novel reflection color model consisting of body essence and(mixed) neuter, and present an effective method for separating dichromaticreflection components using a single image. Body essence is an entity invariantto interface reflection, and has two degrees of freedom unlike hue and maximumchromaticity. As a result, the proposed method is insensitive to noise andproper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red,green, and blue), contrary to the maximum chromaticity-based methods. Interfacereflection is separated by using a Gaussian function, which removes a criticalthresholding problem. Furthermore, the method does not require any regionsegmentation. Experimental results show the efficacy of the proposed model andmethod.
arxiv-13200-220 | Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm | http://arxiv.org/abs/1508.06235 | author:Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang category:stat.ML cs.AI cs.LG stat.CO published:2015-08-25 summary:In this paper, we propose a model-based clustering method (TVClust) thatrobustly incorporates noisy side information as soft-constraints and aims toseek a consensus between side information and the observed data. Our method isbased on a nonparametric Bayesian hierarchical model that combines theprobabilistic model for the data instance and the one for the side-information.An efficient Gibbs sampling algorithm is proposed for posterior inference.Using the small-variance asymptotics of our probabilistic model, we then derivea new deterministic clustering algorithm (RDP-means). It can be viewed as anextension of K-means that allows for the inclusion of side information and hasthe additional property that the number of clusters does not need to bespecified a priori. Empirical studies have been carried out to compare our workwith many constrained clustering algorithms from the literature on both avariety of data sets and under a variety of conditions such as using noisy sideinformation and erroneous k values. The results of our experiments show strongresults for our probabilistic and deterministic approaches under theseconditions when compared to other algorithms in the literature.
arxiv-13200-221 | An analysis of numerical issues in neural training by pseudoinversion | http://arxiv.org/abs/1508.06092 | author:R. Cancelliere, R. Deluca, M. Gai, P. Gallinari, L. Rubini category:cs.LG cs.NE published:2015-08-25 summary:Some novel strategies have recently been proposed for single hidden layerneural network training that set randomly the weights from input to hiddenlayer, while weights from hidden to output layer are analytically determined bypseudoinversion. These techniques are gaining popularity in spite of theirknown numerical issues when singular and/or almost singular matrices areinvolved. In this paper we discuss a critical use of Singular Value Analysisfor identification of these drawbacks and we propose an original use ofregularisation to determine the output weights, based on the concept ofcritical hidden layer size. This approach also allows to limit the trainingcomputational effort. Besides, we introduce a novel technique which relies aneffective determination of input weights to the hidden layer dimension. Thisapproach is tested for both regression and classification tasks, resulting in asignificant performance improvement with respect to alternative methods.
arxiv-13200-222 | Wavelet subspace decomposition of thermal infrared images for defect detection in artworks | http://arxiv.org/abs/1508.06010 | author:Muhammad Zubair Ahmad, Amir Ali Khan, Sihem Mezghani, Eric Perrin, Kamel Mouhoubi, Jean-Luc Bodnar, Valeriu Vrabie category:cs.CV published:2015-08-25 summary:Monitoring the health of ancient artworks requires adequate prudence becauseof the sensitive nature of these materials. Classical techniques foridentifying the development of faults rely on acoustic testing. Thesetechniques, being invasive, may result in causing permanent damage to thematerial, especially if the material is inspected periodically. Non destructivetesting has been carried out for different materials since long. In thisregard, non-invasive systems were developed based on infrared thermometryprinciple to identify the faults in artworks. The test artwork is heated andthe thermal response of the different layers is captured with the help of athermal infrared camera. However, prolonged heating risks overheating and thuscausing damage to artworks and an alternate approach is to use pseudo-randombinary sequence excitations. The faults in the artwork, though, cannot bedetected on the captured images, especially if their strength is weak. Theweaker faults are either masked by the stronger ones, by the pictorial layer ofthe artwork or by the non-uniform heating. This work addresses the detectionand localization of the faults through a wavelet based subspace decompositionscheme. The proposed scheme, on one hand, allows to remove the backgroundwhile, on the other hand, removes the undesired high frequency noise. It isshown that the detection parameter is proportional to the diameter and thedepth of the fault. A criterion is proposed to select the optimal wavelet basisalong with suitable level selection for wavelet decomposition andreconstruction. The proposed approach is tested on a laboratory developed testsample with known fault locations and dimensions as well as real artworks. Acomparison with a previously reported method demonstrates the efficacy of theproposed approach for fault detection in artworks.
arxiv-13200-223 | Inferring Passenger Type from Commuter Eigentravel Matrices | http://arxiv.org/abs/1509.01199 | author:Erika Fille Legara, Christopher Monterola category:physics.soc-ph cs.CY stat.AP stat.ML published:2015-08-25 summary:A sufficient knowledge of the demographics of a commuting public is essentialin formulating and implementing more targeted transportation policies, ascommuters exhibit different ways of traveling. With the advent of the AutomatedFare Collection system (AFC), probing the travel patterns of commuters hasbecome less invasive and more accessible. Consequently, numerous transportstudies related to human mobility have shown that these observed patterns allowone to pair individuals with locations and/or activities at certain times ofthe day. However, classifying commuters using their travel signatures is yet tobe thoroughly examined. Here, we contribute to the literature by demonstrating a procedure tocharacterize passenger types (Adult, Child/Student, and Senior Citizen) basedon their three-month travel patterns taken from a smart fare card system. Wefirst establish a method to construct distinct commuter matrices, which werefer to as eigentravel matrices, that capture the characteristic travelroutines of individuals. From the eigentravel matrices, we build classificationmodels that predict the type of passengers traveling. Among the modelsexplored, the gradient boosting method (GBM) gives the best prediction accuracyat 76%, which is 84% better than the minimum model accuracy (41%) requiredvis-\`a-vis the proportional chance criterion. In addition, we find that travelfeatures generated during weekdays have greater predictive power than those onweekends. This work should not only be useful for transport planners, but formarket researchers as well. With the awareness of which commuter types aretraveling, ads, service announcements, and surveys, among others, can be mademore targeted spatiotemporally. Finally, our framework should be effective increating synthetic populations for use in real-world simulations that involve ametropolitan's public transport system.
arxiv-13200-224 | Cooking in the kitchen: Recognizing and Segmenting Human Activities in Videos | http://arxiv.org/abs/1508.06073 | author:Hilde Kuehne, Juergen Gall, Thomas Serre category:cs.CV published:2015-08-25 summary:As research on action recognition matures, the focus is shifting away fromcategorizing basic task-oriented actions using hand-segmented video datasets tounderstanding complex goal-oriented daily human activities in real-worldsettings. Temporally structured models would seem obvious to tackle this set ofproblems, but so far, cases where these models have outperformed simplerunstructured bag-of-word types of models are scarce. With the increasingavailability of large human activity datasets, combined with the development ofnovel feature coding techniques that yield more compact representations, it istime to revisit structured generative approaches. Here, we describe an end-to-end generative approach from the encoding offeatures to the structural modeling of complex human activities by applyingFisher vectors and temporal models for the analysis of video sequences. We systematically evaluate the proposed approach on several availabledatasets (ADL, MPIICooking, and Breakfast datasets) using a variety ofperformance metrics. Through extensive system evaluations, we demonstrate thatcombining compact video representations based on Fisher Vectors with HMM-basedmodeling yields very significant gains in accuracy and when properly trainedwith sufficient training samples, structured temporal models outperformunstructured bag-of-word types of models by a large margin on the testedperformance metric.
arxiv-13200-225 | Multiple kernel multivariate performance learning using cutting plane algorithm | http://arxiv.org/abs/1508.06264 | author:Jingbin Wang, Haoxiang Wang, Yihua Zhou, Nancy McDonald category:cs.LG cs.CV published:2015-08-25 summary:In this paper, we propose a multi-kernel classifier learning algorithm tooptimize a given nonlinear and nonsmoonth multivariate classifier performancemeasure. Moreover, to solve the problem of kernel function selection and kernelparameter tuning, we proposed to construct an optimal kernel by weighted linearcombination of some candidate kernels. The learning of the classifier parameterand the kernel weight are unified in a single objective function considering tominimize the upper boundary of the given multivariate performance measure. Theobjective function is optimized with regard to classifier parameter and kernelweight alternately in an iterative algorithm by using cutting plane algorithm.The developed algorithm is evaluated on two different pattern classificationmethods with regard to various multivariate performance measure optimizationproblems. The experiment results show the proposed algorithm outperforms thecompeting methods.
arxiv-13200-226 | Accurate Urban Road Centerline Extraction from VHR Imagery via Multiscale Segmentation and Tensor Voting | http://arxiv.org/abs/1508.06163 | author:Guangliang Cheng, Feiyun Zhu, Shiming Xiang, Chunhong Pan category:cs.CV published:2015-08-25 summary:It is very useful and increasingly popular to extract accurate roadcenterlines from very-high-resolution (VHR) re- mote sensing imagery forvarious applications, such as road map generation and updating etc. There arethree shortcomings of current methods: (a) Due to the noise and occlusions(owing to vehicles and trees), most road extraction methods bring inheterogeneous classification results; (b) Morphological thinning algorithm iswidely used to extract road centerlines, while it pro- duces small spurs aroundthe centerlines; (c) Many methods are ineffective to extract centerlines aroundthe road intersections. To address the above three issues, we propose a novelmethod to ex- tract smooth and complete road centerlines via three techniques:the multiscale joint collaborative representation (MJCR) & graph cuts (GC),tensor voting (TV) & non-maximum suppression (NMS) and fitting based connectionalgorithm. Specifically, a MJCR-GC based road area segmentation method isproposed by incorporating mutiscale features and spatial information. In thisway, a homogenous road segmentation result is achieved. Then, to obtain asmooth and correct road centerline network, a TV-NMS based centerlineextraction method is introduced. This method not only extracts smooth roadcenterlines, but also connects the discontinuous road centerlines. Finally, toovercome the ineffectiveness of current methods in the road intersection, afitting based road centerline connection algorithm is proposed. As a result, wecan get a complete road centerline network. Extensive experiments on twodatasets demonstrate that our method achieves higher quantitative results, aswell as more satisfactory visual performances by comparing with state-of-the-art methods.
arxiv-13200-227 | AUC Optimisation and Collaborative Filtering | http://arxiv.org/abs/1508.06091 | author:Charanpal Dhanjal, Romaric Gaudel, Stephan Clemencon category:stat.ML cs.LG published:2015-08-25 summary:In recommendation systems, one is interested in the ranking of the predicteditems as opposed to other losses such as the mean squared error. Although avariety of ways to evaluate rankings exist in the literature, here we focus onthe Area Under the ROC Curve (AUC) as it widely used and has a strongtheoretical underpinning. In practical recommendation, only items at the top ofthe ranked list are presented to the users. With this in mind, we propose aclass of objective functions over matrix factorisations which primarilyrepresent a smooth surrogate for the real AUC, and in a special case we showhow to prioritise the top of the list. The objectives are differentiable andoptimised through a carefully designed stochastic gradient-descent-basedalgorithm which scales linearly with the size of the data. In the special caseof square loss we show how to improve computational complexity by leveragingpreviously computed measures. To understand theoretically the underlying matrixfactorisation approaches we study both the consistency of the loss functionswith respect to AUC, and generalisation using Rademacher theory. The resultinggeneralisation analysis gives strong motivation for the optimisation understudy. Finally, we provide computation results as to the efficacy of theproposed method using synthetic and real data.
arxiv-13200-228 | Morphometry-Based Longitudinal Neurodegeneration Simulation with MR Imaging | http://arxiv.org/abs/1508.05683 | author:Siqi Liu, Sidong Liu, Sonia Pujol, Ron Kikinis, Dagan Feng, Michael Fulham, Weidong Cai category:cs.CV published:2015-08-24 summary:We present a longitudinal MR simulation framework which simulates the futureneurodegenerative progression by outputting the predicted follow-up MR imageand the voxel-based morphometry (VBM) map. This framework expects the patientsto have at least 2 historical MR images available. The longitudinal andcross-sectional VBM maps are extracted to measure the affinity between thetarget subject and the template subjects collected for simulation. Then thefollow-up simulation is performed by resampling the latest available target MRimage with a weighted sum of non-linear transformations derived from thebest-matched templates. The leave-one-out strategy was used to comparedifferent simulation methods. Compared to the state-of-the-art voxel-basedmethod, our proposed morphometry-based simulation achieves better accuracy inmost cases.
arxiv-13200-229 | Causality, Information and Biological Computation: An algorithmic software approach to life, disease and the immune system | http://arxiv.org/abs/1508.06538 | author:Hector Zenil, Angelika Schmidt, Jesper Tegnér category:cs.NE cs.AI published:2015-08-24 summary:Biology has taken strong steps towards becoming a computer science aiming atreprogramming nature after the realisation that nature herself has reprogrammedorganisms by harnessing the power of natural selection and the digitalprescriptive nature of replicating DNA. Here we further unpack ideas related tocomputability, algorithmic information theory and software engineering, in thecontext of the extent to which biology can be (re)programmed, and with how wemay go about doing so in a more systematic way with all the tools and conceptsoffered by theoretical computer science in a translation exercise fromcomputing to molecular biology and back. These concepts provide a means to ahierarchical organization thereby blurring previously clear-cut lines betweenconcepts like matter and life, or between tumour types that are otherwise takenas different and may not have however a different cause. This does not diminishthe properties of life or make its components and functions less interesting.On the contrary, this approach makes for a more encompassing and integratedview of nature, one that subsumes observer and observed within the same system,and can generate new perspectives and tools with which to view complex diseaseslike cancer, approaching them afresh from a software-engineering viewpoint thatcasts evolution in the role of programmer, cells as computing machines, DNA andgenes as instructions and computer programs, viruses as hacking devices, theimmune system as a software debugging tool, and diseases as aninformation-theoretic battlefield where all these forces deploy. We show howinformation theory and algorithmic programming may explain fundamentalmechanisms of life and death.
arxiv-13200-230 | Another Look at DWD: Thrifty Algorithm and Bayes Risk Consistency in RKHS | http://arxiv.org/abs/1508.05913 | author:Boxiang Wang, Hui Zou category:stat.ML published:2015-08-24 summary:Distance weighted discrimination (DWD) is a margin-based classifier with aninteresting geometric motivation. DWD was originally proposed as a superioralternative to the support vector machine (SVM), however DWD is yet to bepopular compared with the SVM. The main reasons are twofold. First, thestate-of-the-art algorithm for solving DWD is based on the second-order-coneprogramming (SOCP), while the SVM is a quadratic programming problem which ismuch more efficient to solve. Second, the current statistical theory of DWDmainly focuses on the linear DWD for the high-dimension-low-sample-size settingand data-piling, while the learning theory for the SVM mainly focuses on theBayes risk consistency of the kernel SVM. In fact, the Bayes risk consistencyof DWD is presented as an open problem in the original DWD paper. In this work,we advance the current understanding of DWD from both computational andtheoretical perspectives. We propose a novel efficient algorithm for solvingDWD, and our algorithm can be several hundred times faster than the existingstate-of-the-art algorithm based on the SOCP. In addition, our algorithm canhandle the generalized DWD, while the SOCP algorithm only works well for aspecial DWD but not the generalized DWD. Furthermore, we consider a naturalkernel DWD in a reproducing kernel Hilbert space and then establish the Bayesrisk consistency of the kernel DWD. We compare DWD and the SVM on severalbenchmark data sets and show that the two have comparable classificationaccuracy, but DWD equipped with our new algorithm can be much faster to computethan the SVM.
arxiv-13200-231 | An evolutionary approach to the identification of Cellular Automata based on partial observations | http://arxiv.org/abs/1508.05752 | author:Witold Bołt, Jan M. Baetens, Bernard De Baets category:cs.NE nlin.CG published:2015-08-24 summary:In this paper we consider the identification problem of Cellular Automata(CAs). The problem is defined and solved in the context of partial observationswith time gaps of unknown length, i.e. pre-recorded, partial configurations ofthe system at certain, unknown time steps. A solution method based on amodified variant of a Genetic Algorithm (GA) is proposed and illustrated withbrief experimental results.
arxiv-13200-232 | A Framework for Comparing Groups of Documents | http://arxiv.org/abs/1508.05902 | author:Arun S. Maiya category:cs.CL cs.SI I.2.7 published:2015-08-24 summary:We present a general framework for comparing multiple groups of documents. Abipartite graph model is proposed where document groups are represented as onenode set and the comparison criteria are represented as the other node set.Using this model, we present basic algorithms to extract insights intosimilarities and differences among the document groups. Finally, we demonstratethe versatility of our framework through an analysis of NSF funding programsfor basic research.
arxiv-13200-233 | Searching for significant patterns in stratified data | http://arxiv.org/abs/1508.05803 | author:Felipe Llinares-Lopez, Laetitia Papaxanthos, Dean Bodenham, Karsten Borgwardt category:stat.ML cs.LG published:2015-08-24 summary:Significant pattern mining, the problem of finding itemsets that aresignificantly enriched in one class of objects, is statistically challenging,as the large space of candidate patterns leads to an enormous multiple testingproblem. Recently, the concept of testability was proposed as one approach tocorrect for multiple testing in pattern mining while retaining statisticalpower. Still, these strategies based on testability do not allow one tocondition the test of significance on the observed covariates, which severelylimits its utility in biomedical applications. Here we propose a strategy andan efficient algorithm to perform significant pattern mining in the presence ofcategorical covariates with K states.
arxiv-13200-234 | Echoes of Persuasion: The Effect of Euphony in Persuasive Communication | http://arxiv.org/abs/1508.05817 | author:Marco Guerini, Gözde Özbal, Carlo Strapparava category:cs.CL cs.CY cs.SI published:2015-08-24 summary:While the effect of various lexical, syntactic, semantic and stylisticfeatures have been addressed in persuasive language from a computational pointof view, the persuasive effect of phonetics has received little attention. Bymodeling a notion of euphony and analyzing four datasets comprising persuasiveand non-persuasive sentences in different domains (political speeches, moviequotes, slogans and tweets), we explore the impact of sounds on different formsof persuasiveness. We conduct a series of analyses and prediction experimentswithin and across datasets. Our results highlight the positive role of phoneticdevices on persuasion.
arxiv-13200-235 | Fast Asynchronous Parallel Stochastic Gradient Decent | http://arxiv.org/abs/1508.05711 | author:Shen-Yi Zhao, Wu-Jun Li category:stat.ML cs.LG published:2015-08-24 summary:Stochastic gradient descent~(SGD) and its variants have become more and morepopular in machine learning due to their efficiency and effectiveness. Tohandle large-scale problems, researchers have recently proposed severalparallel SGD methods for multicore systems. However, existing parallel SGDmethods cannot achieve satisfactory performance in real applications. In thispaper, we propose a fast asynchronous parallel SGD method, called AsySVRG, bydesigning an asynchronous strategy to parallelize the recently proposed SGDvariant called stochastic variance reduced gradient~(SVRG). Both theoreticaland empirical results show that AsySVRG can outperform existingstate-of-the-art parallel SGD methods like Hogwild! in terms of convergencerate and computation cost.
arxiv-13200-236 | Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for Stationary Gaussian Inputs and Slow Learning | http://arxiv.org/abs/1508.05873 | author:Jingen Ni, Jian Yang, Jie Chen, Cédric Richard, José Carlos M. Bermudez category:cs.NA cs.LG published:2015-08-24 summary:Some system identification problems impose nonnegativity constraints on theparameters to estimate due to inherent physical characteristics of the unknownsystem. The nonnegative least-mean-square (NNLMS) algorithm and its variantsallow to address this problem in an online manner. A nonnegative least meanfourth (NNLMF) algorithm has been recently proposed to improve the performanceof these algorithms in cases where the measurement noise is not Gaussian. Thispaper provides a first theoretical analysis of the stochastic behavior of theNNLMF algorithm for stationary Gaussian inputs and slow learning. Simulationresults illustrate the accuracy of the proposed analysis.
arxiv-13200-237 | Iterative Thresholded Bi-Histogram Equalization for Medical Image Enhancement | http://arxiv.org/abs/1508.05704 | author:Muhammad Ali Qadar, Yan Zhaowen, Li Hua category:cs.CV published:2015-08-24 summary:Enhancement of human vision to get an insight to information content is ofvital importance. The traditional histogram equalization methods have beensuffering from amplified contrast with the addition of artifacts and asurprising unnatural visibility of the processed images. In order to overcomethese drawbacks, this paper proposes interative, mean, and multi-thresholdselection criterion with plateau limits, which consist of histogramsegmentation, clipping and transformation modules. The histogram partitionconsists of multiple thresholding processes that divide the histogram into twoparts, whereas the clipping process nicely enhances the contrast by having acheck on the rate of enhancement that could be tuned. Histogram equalization toeach segmented sub-histogram provides the output image with preservedbrightness and enhanced contrast. Results of the present study showed that theproposed method efficiently handles the noise amplification. Further, it alsopreserves the brightness by retaining natural look of targeted image.
arxiv-13200-238 | An algorithm for Left Atrial Thrombi detection using Transesophageal Echocardiography | http://arxiv.org/abs/1508.05995 | author:Jianrui Ding, Min Xian, H. D. Cheng, Yang Li, Fei Xu, Yingtao Zhang category:cs.CV published:2015-08-24 summary:Transesophageal echocardiography (TEE) is widely used to detect left atrium(LA)/left atrial appendage (LAA) thrombi. In this paper, the local binarypattern variance (LBPV) features are extracted from region of interest (ROI).And the dynamic features are formed by using the information of its neighborframes in the sequence. The sequence is viewed as a bag, and the images in thesequence are considered as the instances. Multiple-instance learning (MIL)method is employed to solve the LAA thrombi detection. The experimental resultsshow that the proposed method can achieve better performance than that by usingother methods.
arxiv-13200-239 | Optical images-based edge detection in Synthetic Aperture Radar images | http://arxiv.org/abs/1508.05879 | author:Gilberto P. Silva Junior, Alejandro C. Frery, Sandra Sandri, Humberto Bustince, Edurne Barrenechea, Cédric Marco-Detchart category:cs.CV published:2015-08-24 summary:We address the issue of adapting optical images-based edge detectiontechniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.We modify the gravitational edge detection technique (inspired by the Law ofUniversal Gravity) proposed by Lopez-Molina et al, using the non-standardneighbourhood configuration proposed by Fu et al, to reduce the speckle noisein polarimetric SAR imagery. We compare the modified and unmodified versions ofthe gravitational edge detection technique with the well-established oneproposed by Canny, as well as with a recent multiscale fuzzy-based techniqueproposed by Lopez-Molina et Alejandro We also address the issues of aggregationof gray level images before and after edge detection and of filtering. Alltechniques addressed here are applied to a mosaic built using classdistributions obtained from a real scene, as well as to the true PolSAR image;the mosaic results are assessed using Baddeley's Delta Metric. Our experimentsshow that modifying the gravitational edge detection technique with anon-standard neighbourhood configuration produces better results than theoriginal technique, as well as the other techniques used for comparison. Theexperiments show that adapting edge detection methods from ComputationalIntelligence for use in PolSAR imagery is a new field worthy of exploration.
arxiv-13200-240 | Light Efficient Flutter Shutter | http://arxiv.org/abs/1509.01220 | author:Moshe Ben-Ezra category:cs.GR cs.CV published:2015-08-23 summary:Flutter shutter is a technique in which the exposure is chopped into segmentsand light is only integrated part of the time. By carefully selecting thechopping sequence it is possible to better condition the data forreconstruction problems such as motion deblurring, focal sweeping, andcompressed sensing. The partial exposure trades better conditioning for lessenergy. In problems such as motion deblurring the available energy is whatcaused the problem in the first place (as strong illumination allows shortexposure thus eliminates motion blur). It is still beneficial because thebenefit from the better conditioning outweighs the cost in energy. This documents is focused on light efficient flutter shutter that providesbetter conditioning and better energy utilization than conventional fluttershutter.
arxiv-13200-241 | Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery | http://arxiv.org/abs/1508.05565 | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG cs.CL cs.IR stat.ML published:2015-08-23 summary:We develop necessary and sufficient conditions and a novel provablyconsistent and efficient algorithm for discovering topics (latent factors) fromobservations (documents) that are realized from a probabilistic mixture ofshared latent factors that have certain properties. Our focus is on the classof topic models in which each shared latent factor contains a novel word thatis unique to that factor, a property that has come to be known as separability.Our algorithm is based on the key insight that the novel words correspond tothe extreme points of the convex hull formed by the row-vectors of a suitablynormalized word co-occurrence matrix. We leverage this geometric insight toestablish polynomial computation and sample complexity bounds based on a fewisotropic random projections of the rows of the normalized word co-occurrencematrix. Our proposed random-projections-based algorithm is naturally amenableto an efficient distributed implementation and is attractive for modernweb-scale distributed data mining applications.
arxiv-13200-242 | MultiView Diffusion Maps | http://arxiv.org/abs/1508.05550 | author:Ofir Lindenbaum, Arie Yeredor, Moshe Salhov, Amir Averbuch category:cs.LG stat.ML published:2015-08-23 summary:In this study we consider learning a reduced dimensionality representationfrom datasets obtained under multiple views. Such multiple views of datasetscan be obtained, for example, when the same underlying process is observedusing several different modalities, or measured with different instrumentation.Our goal is to effectively exploit the availability of such multiple views forvarious purposes, such as non-linear embedding, manifold learning, spectralclustering, anomaly detection and non-linear system identification. Ourproposed method exploits the intrinsic relation within each view, as well asthe mutual relations between views. We do this by defining a cross-view model,in which an implied Random Walk process between objects is restrained to hopbetween the different views. Our method is robust to scaling of each dataset,and is insensitive to small structural changes in the data. Within thisframework, we define new diffusion distances and analyze the spectra of theimplied kernels. We demonstrate the applicability of the proposed approach onboth artificial and real data sets.
arxiv-13200-243 | Learning Sampling Distributions for Efficient Object Detection | http://arxiv.org/abs/1508.05581 | author:Yanwei Pang, Jiale Cao, Xuelong Li category:cs.CV cs.LG published:2015-08-23 summary:Object detection is an important task in computer vision and learningsystems. Multistage particle windows (MPW), proposed by Gualdi et al., is analgorithm of fast and accurate object detection. By sampling particle windowsfrom a proposal distribution (PD), MPW avoids exhaustively scanning the image.Despite its success, it is unknown how to determine the number of stages andthe number of particle windows in each stage. Moreover, it has to generate toomany particle windows in the initialization step and it redraws unnecessary toomany particle windows around object-like regions. In this paper, we attempt tosolve the problems of MPW. An important fact we used is that there is largeprobability for a randomly generated particle window not to contain the objectbecause the object is a sparse event relevant to the huge number of candidatewindows. Therefore, we design the proposal distribution so as to efficientlyreject the huge number of non-object windows. Specifically, we propose theconcepts of rejection, acceptance, and ambiguity windows and regions. Thiscontrasts to MPW which utilizes only on region of support. The PD of MPW isacceptance-oriented whereas the PD of our method (called iPW) isrejection-oriented. Experimental results on human and face detectiondemonstrate the efficiency and effectiveness of the iPW algorithm. The sourcecode is publicly accessible.
arxiv-13200-244 | The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms | http://arxiv.org/abs/1508.05608 | author:Yahel David, Nahum Shimkin category:stat.ML cs.AI cs.LG published:2015-08-23 summary:We consider the Max $K$-Armed Bandit problem, where a learning agent is facedwith several sources (arms) of items (rewards), and interested in finding thebest item overall. At each time step the agent chooses an arm, and obtains arandom real valued reward. The rewards of each arm are assumed to be i.i.d.,with an unknown probability distribution that generally differs among the arms.Under the PAC framework, we provide lower bounds on the sample complexity ofany $(\epsilon,\delta)$-correct algorithm, and propose algorithms that attainthis bound up to logarithmic factors. We compare the performance of thismulti-arm algorithms to the variant in which the arms are not distinguishableby the agent and are chosen randomly at each stage. Interestingly, when themaximal rewards of the arms happen to be similar, the latter approach mayprovide better performance.
arxiv-13200-245 | Towards Neural Network-based Reasoning | http://arxiv.org/abs/1508.05508 | author:Baolin Peng, Zhengdong Lu, Hang Li, Kam-Fai Wong category:cs.AI cs.CL cs.LG cs.NE published:2015-08-22 summary:We propose Neural Reasoner, a framework for neural network-based reasoningover natural language sentences. Given a question, Neural Reasoner can inferover multiple supporting facts and find an answer to the question in specificforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,allowing it to examine multiple facts, and 2) a deep architecture, allowing itto model the complicated logical relations in reasoning tasks. Assuming noparticular structure exists in the question and facts, Neural Reasoner is ableto accommodate different types of reasoning and different forms of languageexpressions. Despite the model complexity, Neural Reasoner can still be trainedeffectively in an end-to-end manner. Our empirical studies show that NeuralReasoner can outperform existing neural reasoning systems with remarkablemargins on two difficult artificial tasks (Positional Reasoning and PathFinding) proposed in [8]. For example, it improves the accuracy on PathFinding(10K) from 33.4% [6] to over 98%.
arxiv-13200-246 | Bayesian Hypothesis Testing for Block Sparse Signal Recovery | http://arxiv.org/abs/1508.05495 | author:Mehdi Korki, Hadi Zayyani, Jingxin Zhang category:stat.ML cs.IT math.IT published:2015-08-22 summary:This letter presents a novel Block Bayesian Hypothesis Testing Algorithm(Block-BHTA) for reconstructing block sparse signals with unknown blockstructures. The Block-BHTA comprises the detection and recovery of thesupports, and the estimation of the amplitudes of the block sparse signal. Thesupport detection and recovery is performed using a Bayesian hypothesistesting. Then, based on the detected and reconstructed supports, the nonzeroamplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA isdemonstrated by numerical experiments.
arxiv-13200-247 | StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity | http://arxiv.org/abs/1508.05463 | author:Mohammad Javad Shafiee, Parthipan Siva, Alexander Wong category:cs.CV cs.LG cs.NE published:2015-08-22 summary:Deep neural networks is a branch in machine learning that has seen a meteoricrise in popularity due to its powerful abilities to represent and modelhigh-level abstractions in highly complex data. One area in deep neuralnetworks that is ripe for exploration is neural connectivity formation. Apivotal study on the brain tissue of rats found that synaptic formation forspecific functional connectivity in neocortical neural microcircuits can besurprisingly well modeled and predicted as a random formation. Motivated bythis intriguing finding, we introduce the concept of StochasticNet, where deepneural networks are formed via stochastic connectivity between neurons. As aresult, any type of deep neural networks can be formed as a StochasticNet byallowing the neuron connectivity to be stochastic. Stochastic synapticformations, in a deep neural network architecture, can allow for efficientutilization of neurons for performing specific tasks. To evaluate thefeasibility of such a deep neural network architecture, we train aStochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, andSTL-10). Experimental results show that a StochasticNet, using less than halfthe number of neural connections as a conventional deep neural network,achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST andSVHN dataset. Interestingly, StochasticNet with less than half the number ofneural connections, achieved a higher accuracy (relative improvement in testerror rate of ~6% compared to ConvNet) on the STL-10 dataset than aconventional deep neural network. Finally, StochasticNets have fasteroperational speeds while achieving better or similar accuracy performances.
arxiv-13200-248 | Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence | http://arxiv.org/abs/1508.05514 | author:Tohid Ardeshiri, Umut Orguner, Emre Özkan category:stat.ML cs.CV cs.LG cs.RO cs.SY published:2015-08-22 summary:We propose a greedy mixture reduction algorithm which is capable of pruningmixture components as well as merging them based on the Kullback-Leiblerdivergence (KLD). The algorithm is distinct from the well-known Runnalls' KLDbased method since it is not restricted to merging operations. The capabilityof pruning (in addition to merging) gives the algorithm the ability ofpreserving the peaks of the original mixture during the reduction. Analyticalapproximations are derived to circumvent the computational intractability ofthe KLD which results in a computationally efficient method. The proposedalgorithm is compared with Runnalls' and Williams' methods in two numericalexamples, using both simulated and real world data. The results indicate thatthe performance and computational complexity of the proposed approach make itan efficient alternative to existing mixture reduction methods.
arxiv-13200-249 | Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures | http://arxiv.org/abs/1508.05243 | author:Mario Lucic, Olivier Bachem, Andreas Krause category:stat.ML cs.LG published:2015-08-21 summary:Coresets are efficient representations of data sets such that models trainedon the coreset are provably competitive with models trained on the originaldata set. As such, they have been successfully used to scale up clusteringmodels such as K-Means and Gaussian mixture models to massive data sets.However, until now, the algorithms and the corresponding theory were usuallyspecific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for alarge class of hard and soft clustering problems based on Bregman divergences.This class includes hard clustering with popular distortion measures such asthe Squared Euclidean distance, the Mahalanobis distance, KL-divergence andItakura-Saito distance. The corresponding soft clustering problems are directlyrelated to popular mixture models due to a dual relationship between Bregmandivergences and Exponential family distributions. Our theoretical resultsfurther imply a randomized polynomial-time approximation scheme for hardclustering. We demonstrate the practicality of the proposed algorithm in anempirical evaluation.
arxiv-13200-250 | Dynamics of Human Cooperation in Economic Games | http://arxiv.org/abs/1508.05288 | author:Martin Spanknebel, Klaus Pawelzik category:physics.soc-ph cs.GT cs.LG math.DS published:2015-08-21 summary:Human decision behaviour is quite diverse. In many games humans on average donot achieve maximal payoff and the behaviour of individual players remainsinhomogeneous even after playing many rounds. For instance, in repeatedprisoner dilemma games humans do not always optimize their mean reward andfrequently exhibit broad distributions of cooperativity. The reasons for thesefailures of maximization are not known. Here we show that the dynamicsresulting from the tendency to shift choice probabilities towards previouslyrewarding choices in closed loop interaction with the strategy of the opponentcan not only explain systematic deviations from 'rationality', but alsoreproduce the diversity of choice behaviours. As a representative example weinvestigate the dynamics of choice probabilities in prisoner dilemma games withopponents using strategies with different degrees of extortion and generosity.We find that already a simple model for human learning can account for asurprisingly wide range of human decision behaviours. It reproduces suppressionof cooperation against extortionists and increasing cooperation when playingwith generous opponents, explains the broad distributions of individual choicesin ensembles of players, and predicts the evolution of individual subjects'cooperation rates over the course of the games. We conclude that importantaspects of human decision behaviours are rooted in elementary learningmechanisms realised in the brain.
arxiv-13200-251 | Posterior calibration and exploratory analysis for natural language processing models | http://arxiv.org/abs/1508.05154 | author:Khanh Nguyen, Brendan O'Connor category:cs.CL published:2015-08-21 summary:Many models in natural language processing define probabilistic distributionsover linguistic structures. We argue that (1) the quality of a model' sposterior distribution can and should be directly evaluated, as to whetherprobabilities correspond to empirical frequencies, and (2) NLP uncertainty canbe projected not only to pipeline components, but also to exploratory dataanalysis, telling a user when to trust and not trust the NLP analysis. Wepresent a method to analyze calibration, and apply it to compare themiscalibration of several commonly used models. We also contribute acoreference sampling algorithm that can create confidence intervals for apolitical event extraction task.
arxiv-13200-252 | On Monotonicity of the Optimal Transmission Policy in Cross-layer Adaptive m-QAM Modulation | http://arxiv.org/abs/1508.05383 | author:Ni Ding, Parastoo Sadeghi, Rodney A. Kennedy category:stat.ML cs.IT math.IT published:2015-08-21 summary:This paper considers a cross-layer adaptive modulation system that is modeledas a Markov decision process (MDP). We study how to utilize the monotonicity ofthe optimal transmission policy to relieve the computational complexity ofdynamic programming (DP). In this system, a scheduler controls the bit rate ofthe m-quadrature amplitude modulation (m-QAM) in order to minimize thelong-term losses incurred by the queue overflow in the data link layer and thetransmission power consumption in the physical layer. The work is done in twosteps. Firstly, we observe the L-natural-convexity and submodularity of DP toprove that the optimal policy is always nondecreasing in queue occupancy/stateand derive the sufficient condition for it to be nondecreasing in both queueand channel states. We also show that, due to the L-natural-convexity of DP,the variation of the optimal policy in queue state is restricted by a boundedmarginal effect: The increment of the optimal policy between adjacent queuestates is no greater than one. Secondly, we use the monotonicity results topresent two low complexity algorithms: monotonic policy iteration (MPI) basedon L-natural-convexity and discrete simultaneous perturbation stochasticapproximation (DSPSA). We run experiments to show that the time complexity ofMPI based on L-natural-convexity is much lower than that of DP and theconventional MPI that is based on submodularity and DSPSA is able to adaptivelytrack the optimal policy when the system parameters change.
arxiv-13200-253 | Recurrent Neural Network Based Modeling of Gene Regulatory Network Using Bat Algorithm | http://arxiv.org/abs/1509.03221 | author:Sudip Mandal, Goutam Saha, Rajat K. Pal category:cs.AI cs.NE published:2015-08-21 summary:Correct inference of genetic regulations inside a cell is one of the greatestchallenges in post genomic era for the biologist and researchers. Severalintelligent techniques and models were already proposed to identify theregulatory relations among genes from the biological database like time seriesmicroarray data. Recurrent Neural Network (RNN) is one of the most popular andsimple approach to model the dynamics as well as to infer correct dependenciesamong genes. In this paper, Bat Algorithm (BA) was applied to optimize themodel parameters of RNN model of Gene Regulatory Network (GRN). Initially theproposed method is tested against small artificial network without any noiseand the efficiency was observed in term of number of iteration, number ofpopulation and BA optimization parameters. The model was also validated inpresence of different level of random noise for the small artificial networkand that proved its ability to infer the correct inferences in presence ofnoise like real world dataset. In the next phase of this research, BA based RNNis applied to real world benchmark time series microarray dataset of E. Coli.The results shown that it can able to identify the maximum true positiveregulation but also include some false positive regulations. Therefore, BA isvery suitable for identifying biological plausible GRN with the help RNN model
arxiv-13200-254 | Adaptive Online Learning | http://arxiv.org/abs/1508.05170 | author:Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML published:2015-08-21 summary:We propose a general framework for studying adaptive regret bounds in theonline learning framework, including model selection bounds and data-dependentbounds. Given a data- or model-dependent bound we ask, "Does there exist somealgorithm achieving this bound?" We show that modifications to recentlyintroduced sequential complexity measures can be used to answer this questionby providing sufficient conditions under which adaptive rates can be achieved.In particular each adaptive rate induces a set of so-called offset complexitymeasures, and obtaining small upper bounds on these quantities is sufficient todemonstrate achievability. A cornerstone of our analysis technique is the useof one-sided tail inequalities to bound suprema of offset random processes. Our framework recovers and improves a wide variety of adaptive boundsincluding quantile bounds, second-order data-dependent bounds, and small lossbounds. In addition we derive a new type of adaptive bound for online linearoptimization based on the spectral norm, as well as a new online PAC-Bayestheorem that holds for countably infinite sets.
arxiv-13200-255 | A large annotated corpus for learning natural language inference | http://arxiv.org/abs/1508.05326 | author:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning category:cs.CL published:2015-08-21 summary:Understanding entailment and contradiction is fundamental to understandingnatural language, and inference about entailment and contradiction is avaluable testing ground for the development of semantic representations.However, machine learning research in this area has been dramatically limitedby the lack of large-scale resources. To address this, we introduce theStanford Natural Language Inference corpus, a new, freely available collectionof labeled sentence pairs, written by humans doing a novel grounded task basedon image captioning. At 570K pairs, it is two orders of magnitude larger thanall other resources of its type. This increase in scale allows lexicalizedclassifiers to outperform some sophisticated existing entailment models, and itallows a neural network-based model to perform competitively on naturallanguage inference benchmarks for the first time.
arxiv-13200-256 | Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation | http://arxiv.org/abs/1508.05151 | author:Christian Bailer, Bertram Taetz, Didier Stricker category:cs.CV I.4.8 published:2015-08-21 summary:Modern large displacement optical flow algorithms usually use aninitialization by either sparse descriptor matching techniques or denseapproximate nearest neighbor fields. While the latter have the advantage ofbeing dense, they have the major disadvantage of being very outlier prone asthey are not designed to find the optical flow, but the visually most similarcorrespondence. In this paper we present a dense correspondence field approachthat is much less outlier prone and thus much better suited for optical flowestimation than approximate nearest neighbor fields. Our approach isconceptually novel as it does not require explicit regularization, smoothing(like median filtering) or a new data term, but solely our novel purely databased search strategy that finds most inliers (even for small objects), whileit effectively avoids finding outliers. Moreover, we present novel enhancementsfor outlier filtering. We show that our approach is better suited for largedisplacement optical flow estimation than state-of-the-art descriptor matchingtechniques. We do so by initializing EpicFlow (so far the best method onMPI-Sintel) with our Flow Fields instead of their originally usedstate-of-the-art descriptor matching technique. We significantly outperform theoriginal EpicFlow on MPI-Sintel, KITTI and Middlebury.
arxiv-13200-257 | Representation of Quasi-Monotone Functionals by Families of Separating Hyperplanes | http://arxiv.org/abs/1508.05249 | author:Ingo Steinwart category:math.OC math.FA math.ST stat.ML stat.TH published:2015-08-21 summary:We characterize when the level sets of a continuous quasi-monotone functionaldefined on a suitable convex subset of a normed space can be uniquelyrepresented by a family of bounded continuous functionals. Furthermore, weinvestigate how regularly these functionals depend on the parameterizing level.Finally, we show how this question relates to the recent problem of propertyelicitation that simultaneously attracted interest in machine learning,statistical evaluation of forecasts, and finance.
arxiv-13200-258 | Exemplar Based Deep Discriminative and Shareable Feature Learning for Scene Image Classification | http://arxiv.org/abs/1508.05306 | author:Zhen Zuo, Gang Wang, Bing Shuai, Lifan Zhao, Qingxiong Yang category:cs.CV published:2015-08-21 summary:In order to encode the class correlation and class specific information inimage representation, we propose a new local feature learning approach namedDeep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims tohierarchically learn feature transformation filter banks to transform raw pixelimage patches to features. The learned filter banks are expected to: (1) encodecommon visual patterns of a flexible number of categories; (2) encodediscriminative information; and (3) hierarchically extract patterns atdifferent visual levels. Particularly, in each single layer of DDSFL, shareablefilters are jointly learned for classes which share the similar patterns.Discriminative power of the filters is achieved by enforcing the features fromthe same category to be close, while features from different categories to befar away from each other. Furthermore, we also propose two exemplar selectionmethods to iteratively select training data for more efficient and effectivelearning. Based on the experimental results, DDSFL can achieve very promisingperformance, and it also shows great complementary effect to thestate-of-the-art Caffe features.
arxiv-13200-259 | Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method | http://arxiv.org/abs/1508.05163 | author:Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra category:cs.CL cs.IR published:2015-08-21 summary:Text mining can be applied to many fields. One of the application is usingtext mining in digital newspaper to do politic sentiment analysis. In thispaper sentiment analysis is applied to get information from digital newsarticles about its positive or negative sentiment regarding particularpolitician. This paper suggests a simple model to analyze digital newspapersentiment polarity using naive Bayes classifier method. The model uses a set ofinitial data to begin with which will be updated when new information appears.The model showed promising result when tested and can be implemented to someother sentiment analysis problems.
arxiv-13200-260 | Multi-criteria Similarity-based Anomaly Detection using Pareto Depth Analysis | http://arxiv.org/abs/1508.04887 | author:Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, Alfred O. Hero III category:cs.CV cs.LG stat.ML published:2015-08-20 summary:We consider the problem of identifying patterns in a data set that exhibitanomalous behavior, often referred to as anomaly detection. Similarity-basedanomaly detection algorithms detect abnormally large amounts of similarity ordissimilarity, e.g.~as measured by nearest neighbor Euclidean distances betweena test sample and the training samples. In many application domains there maynot exist a single dissimilarity measure that captures all possible anomalouspatterns. In such cases, multiple dissimilarity measures can be defined,including non-metric measures, and one can test for anomalies by scalarizingusing a non-negative linear combination of them. If the relative importance ofthe different dissimilarity measures are not known in advance, as in manyanomaly detection applications, the anomaly detection algorithm may need to beexecuted multiple times with different choices of weights in the linearcombination. In this paper, we propose a method for similarity-based anomalydetection using a novel multi-criteria dissimilarity measure, the Pareto depth.The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses theconcept of Pareto optimality to detect anomalies under multiple criteriawithout having to run an algorithm multiple times with different choices ofweights. The proposed PDA approach is provably better than using linearcombinations of the criteria and shows superior performance on experiments withsynthetic and real data sets.
arxiv-13200-261 | DeepWriterID: An End-to-end Online Text-independent Writer Identification System | http://arxiv.org/abs/1508.04945 | author:Weixin Yang, Lianwen Jin, Manfei Liu category:cs.CV cs.LG stat.ML published:2015-08-20 summary:Owing to the rapid growth of touchscreen mobile terminals and pen-basedinterfaces, handwriting-based writer identification systems are attractingincreasing attention for personal authentication, digital forensics, and otherapplications. However, most studies on writer identification have not beensatisfying because of the insufficiency of data and difficulty of designinggood features under various conditions of handwritings. Hence, we introduce anend-to-end system, namely DeepWriterID, employed a deep convolutional neuralnetwork (CNN) to address these problems. A key feature of DeepWriterID is a newmethod we are proposing, called DropSegment. It designs to achieve dataaugmentation and improve the generalized applicability of CNN. For sufficientfeature representation, we further introduce path signature feature maps toimprove performance. Experiments were conducted on the NLPR handwritingdatabase. Even though we only use pen-position information in the pen-downstate of the given handwriting samples, we achieved new state-of-the-artidentification rates of 95.72% for Chinese text and 98.51% for English text.
arxiv-13200-262 | Seeing Behind the Camera: Identifying the Authorship of a Photograph | http://arxiv.org/abs/1508.05038 | author:Christopher Thomas, Adriana Kovashka category:cs.CV published:2015-08-20 summary:We introduce the novel problem of identifying the photographer behind thephotograph. To explore the feasibility of current computer vision techniques toaddress this problem, we created a new dataset of over 180,000 images taken by41 well-known photographers. Using this dataset, we examined the effectivenessof a variety of features (low and high-level, including CNN features) atidentifying the photographer. We also trained a new deep convolutional neuralnetwork for this task. Our results show that high-level features greatlyoutperform low-level features at this task. We provide qualitative resultsusing these learned models that give insight into our method's ability todistinguish between photographers, allow us to draw interesting conclusionsabout what specific photographers shoot, and demonstrate two applications ofour method.
arxiv-13200-263 | The ABACOC Algorithm: a Novel Approach for Nonparametric Classification of Data Streams | http://arxiv.org/abs/1508.04912 | author:Rocco De Rosa, Francesco Orabona, Nicolò Cesa-Bianchi category:stat.ML cs.LG published:2015-08-20 summary:Stream mining poses unique challenges to machine learning: predictive modelsare required to be scalable, incrementally trainable, must remain bounded insize (even when the data stream is arbitrarily long), and be nonparametric inorder to achieve high accuracy even in complex and dynamic environments.Moreover, the learning system must be parameterless ---traditional tuningmethods are problematic in streaming settings--- and avoid requiring priorknowledge of the number of distinct class labels occurring in the stream. Inthis paper, we introduce a new algorithmic approach for nonparametric learningin data streams. Our approach addresses all above mentioned challenges bylearning a model that covers the input space using simple local classifiers.The distribution of these classifiers dynamically adapts to the local (unknown)complexity of the classification problem, thus achieving a good balance betweenmodel complexity and predictive accuracy. We design four variants of ourapproach of increasing adaptivity. By means of an extensive empiricalevaluation against standard nonparametric baselines, we show state-of-the-artresults in terms of accuracy versus model size. For the variant that imposes astrict bound on the model size, we show better performance against all othermethods measured at the same model size value. Our empirical analysis iscomplemented by a theoretical performance guarantee which does not rely on anystochastic assumption on the source generating the stream.
arxiv-13200-264 | Semi-supervised Learning with Regularized Laplacian | http://arxiv.org/abs/1508.04906 | author:Konstantin Avrachenkov, Pavel Chebotarev, Alexey Mishenin category:cs.LG published:2015-08-20 summary:We study a semi-supervised learning method based on the similarity graph andRegularizedLaplacian. We give convenient optimization formulation of theRegularized Laplacian method and establishits various properties. Inparticular, we show that the kernel of the methodcan be interpreted in terms ofdiscrete and continuous time random walks and possesses severalimportantproperties of proximity measures. Both optimization and linear algebramethods can be used for efficientcomputation of the classification functions.We demonstrate on numerical examples that theRegularized Laplacian method iscompetitive with respect to the other state of the art semi-supervisedlearningmethods.
arxiv-13200-265 | Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection | http://arxiv.org/abs/1508.04843 | author:Kisuk Lee, Aleksandar Zlateski, Ashwin Vishwanathan, H. Sebastian Seung category:cs.CV published:2015-08-20 summary:Efforts to automate the reconstruction of neural circuits from 3D electronmicroscopic (EM) brain images are critical for the field of connectomics. Animportant computation for reconstruction is the detection of neuronalboundaries. Images acquired by serial section EM, a leading 3D EM technique,are highly anisotropic, with inferior quality along the third dimension. Forsuch images, the 2D max-pooling convolutional network has set the standard forperformance at boundary detection. Here we achieve a substantial gain inaccuracy through three innovations. Following the trend towards deeper networksfor object recognition, we use a much deeper network than previously employedfor boundary detection. Second, we incorporate 3D as well as 2D filters, toenable computations that use 3D context. Finally, we adopt a recursivelytrained architecture in which a first network generates a preliminary boundarymap that is provided as input along with the original image to a second networkthat generates a final boundary map. Backpropagation training is accelerated byZNN, a new implementation of 3D convolutional networks that uses multicore CPUparallelism for speed. Our hybrid 2D-3D architecture could be more generallyapplicable to other types of anisotropic 3D images, including video, and ourrecursive framework for any image labeling problem.
arxiv-13200-266 | Distributed Compressive Sensing: A Deep Learning Approach | http://arxiv.org/abs/1508.04924 | author:Hamid Palangi, Rabab Ward, Li Deng category:cs.LG cs.CV published:2015-08-20 summary:Various studies that address the compressed sensing problem with MultipleMeasurement Vectors (MMVs) have been recently carried. These studies assume thevectors of the different channels to be jointly sparse. In this paper, we relaxthis condition. Instead we assume that these sparse vectors depend on eachother but that this dependency is unknown. We capture this dependency bycomputing the conditional probability of each entry in each vector beingnon-zero, given the "residuals" of all previous vectors. To estimate theseprobabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], adata driven model for sequence modelling that is deep in time. To calculate themodel parameters, we minimize a cross entropy cost function. To reconstruct thesparse vectors at the decoder, we propose a greedy solver that uses the abovemodel to estimate the conditional probabilities. By performing extensiveexperiments on two real world datasets, we show that the proposed methodsignificantly outperforms the general MMV solver (the Simultaneous OrthogonalMatching Pursuit (SOMP)) and a number of the model-based Bayesian methods. Theproposed method does not add any complexity to the general compressive sensingencoder. The trained model is used just at the decoder. As the proposed methodis a data driven method, it is only applicable when training data is available.In many applications however, training data is indeed available, e.g. inrecorded images and videos.
arxiv-13200-267 | Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual Sentiment Prediction | http://arxiv.org/abs/1508.05056 | author:Victor Campos, Amaia Salvador, Brendan Jou, Xavier Giró-i-Nieto category:cs.MM cs.CV I.2.10; H.1.2 published:2015-08-20 summary:Visual media are powerful means of expressing emotions and sentiments. Theconstant generation of new content in social networks highlights the need ofautomated visual sentiment analysis tools. While Convolutional Neural Networks(CNNs) have established a new state-of-the-art in several vision problems,their application to the task of sentiment analysis is mostly unexplored andthere are few studies regarding how to design CNNs for this purpose. In thiswork, we study the suitability of fine-tuning a CNN for visual sentimentprediction as well as explore performance boosting techniques within this deeplearning setting. Finally, we provide a deep-dive analysis into a benchmark,state-of-the-art network architecture to gain insight about how to designpatterns for CNNs on the task of visual sentiment prediction.
arxiv-13200-268 | Review and Perspective for Distance Based Trajectory Clustering | http://arxiv.org/abs/1508.04904 | author:Philippe Besse, Brendan Guillouet, Jean-Michel Loubes, Royer François category:stat.ML cs.LG stat.AP published:2015-08-20 summary:In this paper we tackle the issue of clustering trajectories of geolocalizedobservations. Using clustering technics based on the choice of a distancebetween the observations, we first provide a comprehensive review of thedifferent distances used in the literature to compare trajectories. Then basedon the limitations of these methods, we introduce a new distance : SymmetrizedSegment-Path Distance (SSPD). We finally compare this new distance to theothers according to their corresponding clustering results obtained using bothhierarchical clustering and affinity propagation methods.
arxiv-13200-269 | Histogram of gradients of Time-Frequency Representations for Audio scene detection | http://arxiv.org/abs/1508.04909 | author:Alain Rakotomamonjy, Gilles Gasso category:cs.SD cs.LG published:2015-08-20 summary:This paper addresses the problem of audio scenes classification andcontributes to the state of the art by proposing a novel feature. We build thisfeature by considering histogram of gradients (HOG) of time-frequencyrepresentation of an audio scene. Contrarily to classical audio features likeMFCC, we make the hypothesis that histogram of gradients are able to encodesome relevant informations in a time-frequency {representation:} namely, thelocal direction of variation (in time and frequency) of the signal spectralpower. In addition, in order to gain more invariance and robustness, histogramof gradients are locally pooled. We have evaluated the relevance of {the novelfeature} by comparing its performances with state-of-the-art competitors, onseveral datasets, including a novel one that we provide, as part of ourcontribution. This dataset, that we make publicly available, involves $19$classes and contains about $900$ minutes of audio scene recording. We thusbelieve that it may be the next standard dataset for evaluating audio sceneclassification algorithms. Our comparison results clearly show that ourHOG-based features outperform its competitors
arxiv-13200-270 | Using User Generated Online Photos to Estimate and Monitor Air Pollution in Major Cities | http://arxiv.org/abs/1508.05028 | author:Yuncheng Li, Jifei Huang, Jiebo Luo category:cs.CV published:2015-08-20 summary:With the rapid development of economy in China over the past decade, airpollution has become an increasingly serious problem in major cities and causedgrave public health concerns in China. Recently, a number of studies have dealtwith air quality and air pollution. Among them, some attempt to predict andmonitor the air quality from different sources of information, ranging fromdeployed physical sensors to social media. These methods are either tooexpensive or unreliable, prompting us to search for a novel and effective wayto sense the air quality. In this study, we propose to employ the state of theart in computer vision techniques to analyze photos that can be easily acquiredfrom online social media. Next, we establish the correlation between the hazelevel computed directly from photos with the official PM 2.5 record of thetaken city at the taken time. Our experiments based on both synthetic and realphotos have shown the promise of this image-based approach to estimating andmonitoring air pollution.
arxiv-13200-271 | Introducing Geometry in Active Learning for Image Segmentation | http://arxiv.org/abs/1508.04955 | author:Ksenia Konyushkova, Raphael Sznitman, Pascal Fua category:cs.CV published:2015-08-20 summary:We propose an Active Learning approach to training a segmentation classifierthat exploits geometric priors to streamline the annotation process in 3D imagevolumes. To this end, we use these priors not only to select voxels most inneed of annotation but to guarantee that they lie on 2D planar patch, whichmakes it much easier to annotate than if they were randomly distributed in thevolume. A simplified version of this approach is effective in natural 2Dimages. We evaluated our approach on Electron Microscopy and Magnetic Resonanceimage volumes, as well as on natural images. Comparing our approach againstseveral accepted baselines demonstrates a marked performance increase.
arxiv-13200-272 | AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization | http://arxiv.org/abs/1508.05003 | author:Suvrit Sra, Adams Wei Yu, Mu Li, Alexander J. Smola category:stat.ML cs.LG math.OC published:2015-08-20 summary:We study distributed stochastic convex optimization under the delayedgradient model where the server nodes perform parameter updates, while theworker nodes compute stochastic gradients. We discuss, analyze, and experimentwith a setup motivated by the behavior of real-world distributed computationnetworks, where the machines are differently slow at different time. Therefore,we allow the parameter updates to be sensitive to the actual delaysexperienced, rather than to worst-case bounds on the maximum delay. Thissensitivity leads to larger stepsizes, that can help gain rapid initialconvergence without having to wait too long for slower machines, whilemaintaining the same asymptotic complexity. We obtain encouraging improvementsto overall convergence for distributed experiments on real datasets with up tobillions of examples and features.
arxiv-13200-273 | Auto-Sizing Neural Networks: With Applications to n-gram Language Models | http://arxiv.org/abs/1508.05051 | author:Kenton Murray, David Chiang category:cs.CL published:2015-08-20 summary:Neural networks have been shown to improve performance across a range ofnatural-language tasks. However, designing and training them can becomplicated. Frequently, researchers resort to repeated experimentation to pickoptimal settings. In this paper, we address the issue of choosing the correctnumber of units in hidden layers. We introduce a method for automaticallyadjusting network size by pruning out hidden units through $\ell_{\infty,1}$and $\ell_{2,1}$ regularization. We apply this method to language modeling anddemonstrate its ability to correctly choose the number of hidden units whilemaintaining perplexity. We also include these models in a machine translationdecoder and show that these smaller neural models maintain the significantimprovements of their unpruned versions.
arxiv-13200-274 | Improving Image Restoration with Soft-Rounding | http://arxiv.org/abs/1508.05046 | author:Xing Mei, Honggang Qi, Bao-Gang Hu, Siwei Lyu category:cs.CV published:2015-08-20 summary:Several important classes of images such as text, barcode and pattern imageshave the property that pixels can only take a distinct subset of values. Thisknowledge can benefit the restoration of such images, but it has not beenwidely considered in current restoration methods. In this work, we describe aneffective and efficient approach to incorporate the knowledge of distinct pixelvalues of the pristine images into the general regularized least squaresrestoration framework. We introduce a new regularizer that attains zero at thedesignated pixel values and becomes a quadratic penalty function in theintervals between them. When incorporated into the regularized least squaresrestoration framework, this regularizer leads to a simple and efficient stepthat resembles and extends the rounding operation, which we term assoft-rounding. We apply the soft-rounding enhanced solution to the restorationof binary text/barcode images and pattern images with multiple distinct pixelvalues. Experimental results show that soft-rounding enhanced restorationmethods achieve significant improvement in both visual quality and quantitativemeasures (PSNR and SSIM). Furthermore, we show that this regularizer can alsobenefit the restoration of general natural images.
arxiv-13200-275 | Steps Toward Deep Kernel Methods from Infinite Neural Networks | http://arxiv.org/abs/1508.05133 | author:Tamir Hazan, Tommi Jaakkola category:cs.LG cs.NE published:2015-08-20 summary:Contemporary deep neural networks exhibit impressive results on practicalproblems. These networks generalize well although their inherent capacity mayextend significantly beyond the number of training examples. We analyze thisbehavior in the context of deep, infinite neural networks. We show that deepinfinite layers are naturally aligned with Gaussian processes and kernelmethods, and devise stochastic kernels that encode the information of thesenetworks. We show that stability results apply despite the size, offering anexplanation for their empirical success.
arxiv-13200-276 | A Deep Bag-of-Features Model for Music Auto-Tagging | http://arxiv.org/abs/1508.04999 | author:Juhan Nam, Jorge Herrera, Kyogu Lee category:cs.LG cs.SD stat.ML published:2015-08-20 summary:Feature learning and deep learning have drawn great attention in recent yearsas a way of transforming input data into more effective representations usinglearning algorithms. Such interest has grown up in the area of musicinformation retrieval (MIR) as well, particularly in music classification taskssuch as auto-tagging. While a number of promising results have been shown, itis not well understood what acoustic sense the learned feature representationshave and how they are associated with semantic meaning of music. In this paper,we attempt to demystify the learned audio features using a bag-of-featuresmodel with two learning stages. The first stage learns to project localacoustic patterns of musical signals onto a high-dimensional sparse space in anunsupervised manner and summarizes an audio track as a bag-of-features. Thesecond stage maps the bag-of-features to semantic tags using deep neuralnetworks in a supervised manner. For the first stage, we focus on analyzing thelearned local audio features by quantitatively measuring the acousticproperties and interpreting the statistics in semantic context. For the secondstage, we examine training choices and tuning parameters for the neuralnetworks and show how the deep representations of bag-of-features become morediscriminative. Through this analysis, we not only provide better understandingof learned local audio features but also show the effectiveness of the deepbag-of-features model in the music auto-tagging task.
arxiv-13200-277 | Lifted Relational Neural Networks | http://arxiv.org/abs/1508.05128 | author:Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Ondrej Kuzelka category:cs.AI cs.LG cs.NE published:2015-08-20 summary:We propose a method combining relational-logic representations with neuralnetwork learning. A general lifted architecture, possibly reflecting somebackground domain knowledge, is described through relational rules which may behandcrafted or learned. The relational rule-set serves as a template forunfolding possibly deep neural networks whose structures also reflect thestructures of given training or testing relational examples. Different networkscorresponding to different examples share their weights, which co-evolve duringtraining by stochastic gradient descent algorithm. The framework allows forhierarchical relational modeling constructs and learning of latent relationalconcepts through shared hidden layers weights corresponding to the rules.Discovery of notable relational concepts and experiments on 78 relationallearning benchmarks demonstrate favorable performance of the method.
arxiv-13200-278 | High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging | http://arxiv.org/abs/1508.04981 | author:Changsoo Je, Sang Wook Lee, Rae-Hong Park category:cs.CV cs.GR physics.optics I.2.10; I.4.8 published:2015-08-20 summary:For structured-light range imaging, color stripes can be used for increasingthe number of distinguishable light patterns compared to binary BW stripes.Therefore, an appropriate use of color patterns can reduce the number of lightprojections and range imaging is achievable in single video frame or in "oneshot". On the other hand, the reliability and range resolution attainable fromcolor stripes is generally lower than those from multiply projected binary BWpatterns since color contrast is affected by object color reflectance andambient light. This paper presents new methods for selecting stripe colors anddesigning multiple-stripe patterns for "one-shot" and "two-shot" imaging. Weshow that maximizing color contrast between the stripes in one-shot imagingreduces the ambiguities resulting from colored object surfaces and limitationsin sensor/projector resolution. Two-shot imaging adds an extra video frame andmaximizes the color contrast between the first and second video frames todiminish the ambiguities even further. Experimental results demonstrate theeffectiveness of the presented one-shot and two-shot color-stripe imagingschemes.
arxiv-13200-279 | Dither is Better than Dropout for Regularising Deep Neural Networks | http://arxiv.org/abs/1508.04826 | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-08-19 summary:Regularisation of deep neural networks (DNN) during training is critical toperformance. By far the most popular method is known as dropout. Here, castthrough the prism of signal processing theory, we compare and contrast theregularisation effects of dropout with those of dither. We illustrate someserious inherent limitations of dropout and demonstrate that dither provides amore effective regulariser.
arxiv-13200-280 | Who are the Devils Wearing Prada in New York City? | http://arxiv.org/abs/1508.04785 | author:KuanTing Chen, Kezhen Chen, Peizhong Cong, Winston H. Hsu, Jiebo Luo category:cs.CV cs.CY published:2015-08-19 summary:Fashion is a perpetual topic in human social life, and the mass has thepenchant to emulate what large city residents and celebrities wear. Undeniably,New York City is such a bellwether large city with all kinds of fashionleadership. Consequently, to study what the fashion trends are during thisyear, it is very helpful to learn the fashion trends of New York City.Discovering fashion trends in New York City could boost many applications suchas clothing recommendation and advertising. Does the fashion trend in the NewYork Fashion Show actually influence the clothing styles on the public? Toanswer this question, we design a novel system that consists of three majorcomponents: (1) constructing a large dataset from the New York Fashion Showsand New York street chic in order to understand the likely clothing fashiontrends in New York, (2) utilizing a learning-based approach to discover fashionattributes as the representative characteristics of fashion trends, and (3)comparing the analysis results from the New York Fashion Shows and street-chicimages to verify whether the fashion shows have actual influence on the peoplein New York City. Through the preliminary experiments over a large clothingdataset, we demonstrate the effectiveness of our proposed system, and obtainuseful insights on fashion trends and fashion influence.
arxiv-13200-281 | Time Series Clustering via Community Detection in Networks | http://arxiv.org/abs/1508.04757 | author:Leonardo N. Ferreira, Liang Zhao category:stat.ML cs.LG cs.SI published:2015-08-19 summary:In this paper, we propose a technique for time series clustering usingcommunity detection in complex networks. Firstly, we present a method totransform a set of time series into a network using different distancefunctions, where each time series is represented by a vertex and the mostsimilar ones are connected. Then, we apply community detection algorithms toidentify groups of strongly connected vertices (called a community) and,consequently, identify time series clusters. Still in this paper, we make acomprehensive analysis on the influence of various combinations of time seriesdistance functions, network generation methods and community detectiontechniques on clustering results. Experimental study shows that the proposednetwork-based approach achieves better results than various classic orup-to-date clustering techniques under consideration. Statistical tests confirmthat the proposed method outperforms some classic clustering algorithms, suchas $k$-medoids, diana, median-linkage and centroid-linkage in various datasets. Interestingly, the proposed method can effectively detect shape patternspresented in time series due to the topological structure of the underlyingnetwork constructed in the clustering process. At the same time, othertechniques fail to identify such patterns. Moreover, the proposed method isrobust enough to group time series presenting similar pattern but with timeshifts and/or amplitude variations. In summary, the main point of the proposedmethod is the transformation of time series from time-space domain totopological domain. Therefore, we hope that our approach contributes not onlyfor time series clustering, but also for general time series analysis tasks.
arxiv-13200-282 | Fault Diagnosis of Helical Gear Box using Large Margin K-Nearest Neighbors Classifier using Sound Signals | http://arxiv.org/abs/1508.04734 | author:M. Amarnath, S. Arunav, Hemantha Kumar, V. Sugumaran, G. S Raghvendra category:cs.LG published:2015-08-19 summary:Gear drives are one of the most widely used transmission system in manymachinery. Sound signals of a rotating machine contain the dynamic informationabout its health conditions. Not much information available in the literaturereporting suitability of sound signals for fault diagnosis applications.Maximum numbers of literature are based on FFT (Fast Fourier Transform)analysis and have its own limitations with non-stationary signals like the onesfrom gears. In this paper, attempt has been made in using sound signalsacquired from gears in good and simulated faulty conditions for the purpose offault diagnosis through a machine learning approach. The descriptivestatistical features were extracted from the acquired sound signals and thepredominant features were selected using J48 decision tree technique. Theselected features were then used for classification using Large MarginK-nearest neighbor approach. The paper also discusses the effect of variousparameters on classification accuracy.
arxiv-13200-283 | Saliency maps on image hierarchies | http://arxiv.org/abs/1508.04586 | author:Verónica Vilaplana category:cs.CV published:2015-08-19 summary:In this paper we propose two saliency models for salient object segmentationbased on a hierarchical image segmentation, a tree-like structure thatrepresents regions at different scales from the details to the whole image(e.g. gPb-UCM, BPT). The first model is based on a hierarchy of imagepartitions. The saliency at each level is computed on a region basis, takinginto account the contrast between regions. The maps obtained for the differentpartitions are then integrated into a final saliency map. The second modeldirectly works on the structure created by the segmentation algorithm,computing saliency at each node and integrating these cues in a straightforwardmanner into a single saliency map. We show that the proposed models producehigh quality saliency maps. Objective evaluation demonstrates that the twomethods achieve state-of-the-art performance in several benchmark datasets.
arxiv-13200-284 | Learning to Predict Independent of Span | http://arxiv.org/abs/1508.04582 | author:Hado van Hasselt, Richard S. Sutton category:cs.LG published:2015-08-19 summary:We consider how to learn multi-step predictions efficiently. Conventionalalgorithms wait until observing actual outcomes before performing thecomputations to update their predictions. If predictions are made at a highrate or span over a large amount of time, substantial computation can berequired to store all relevant observations and to update all predictions whenthe outcome is finally observed. We show that the exact same predictions can belearned in a much more computationally congenial way, with uniform per-stepcomputation that does not depend on the span of the predictions. We apply thisidea to various settings of increasing generality, repeatedly adding desiredproperties and each time deriving an equivalent span-independent algorithm forthe conventional algorithm that satisfies these desiderata. Interestingly,along the way several known algorithmic constructs emerge spontaneously fromour derivations, including dutch eligibility traces, temporal differenceerrors, and averaging. This allows us to link these constructs one-to-one tothe corresponding desiderata, unambiguously connecting the `how' to the `why'.Each step, we make sure that the derived algorithm subsumes the previousalgorithms, thereby retaining their properties. Ultimately we arrive at asingle general temporal-difference algorithm that is applicable to the fullsetting of reinforcement learning.
arxiv-13200-285 | Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections | http://arxiv.org/abs/1508.04562 | author:Jingwei Zhang, Aaron Gerow, Jaan Altosaar, James Evans, Richard Jean So category:cs.CL cs.IR published:2015-08-19 summary:Weak topic correlation across document collections with different numbers oftopics in individual collections presents challenges for existingcross-collection topic models. This paper introduces two probabilistic topicmodels, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These addressproblems that can arise when analyzing large, asymmetric, and potentiallyweakly-related collections. Topic correlations in weakly-related collectionstypically lie in the tail of the topic distribution, where they would beoverlooked by models unable to fit large numbers of topics. To efficientlymodel this long tail for large-scale analysis, our models implement a parallelsampling algorithm based on the Metropolis-Hastings and alias methods (Yuan etal., 2015). The models are first evaluated on synthetic data, generated tosimulate various collection-level asymmetries. We then present a case study ofmodeling over 300k documents in collections of sciences and humanities researchfrom JSTOR.
arxiv-13200-286 | Introduction to Cross-Entropy Clustering The R Package CEC | http://arxiv.org/abs/1508.04559 | author:Jacek Tabor, Przemysław Spurek, Konrad Kamieniecki, Marek Śmieja, Krzysztof Misztal category:cs.LG stat.ME stat.ML published:2015-08-19 summary:The R Package CEC performs clustering based on the cross-entropy clustering(CEC) method, which was recently developed with the use of information theory.The main advantage of CEC is that it combines the speed and simplicity of$k$-means with the ability to use various Gaussian mixture models and reduceunnecessary clusters. In this work we present a practical tutorial to CEC basedon the R Package CEC. Functions are provided to encompass the whole process ofclustering.
arxiv-13200-287 | Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification | http://arxiv.org/abs/1508.04535 | author:Ruimao Zhang, Liang Lin, Rui Zhang, Wangmeng Zuo, Lei Zhang category:cs.CV published:2015-08-19 summary:Extracting informative image features and learning effective approximatehashing functions are two crucial steps in image retrieval . Conventionalmethods often study these two steps separately, e.g., learning hash functionsfrom a predefined hand-crafted feature space. Meanwhile, the bit lengths ofoutput hashing codes are preset in most previous methods, neglecting thesignificance level of different bits and restricting their practicalflexibility. To address these issues, we propose a supervised learningframework to generate compact and bit-scalable hashing codes directly from rawimages. We pose hashing learning as a problem of regularized similaritylearning. Specifically, we organize the training images into a batch of tripletsamples, each sample containing two images with the same label and one with adifferent label. With these triplet samples, we maximize the margin betweenmatched pairs and mismatched pairs in the Hamming space. In addition, aregularization term is introduced to enforce the adjacency consistency, i.e.,images of similar appearances should have similar codes. The deep convolutionalneural network is utilized to train the model in an end-to-end fashion, wherediscriminative image features and hash functions are simultaneously optimized.Furthermore, each bit of our hashing codes is unequally weighted so that we canmanipulate the code lengths by truncating the insignificant bits. Our frameworkoutperforms state-of-the-arts on public benchmarks of similar image search andalso achieves promising results in the application of person re-identificationin surveillance. It is also shown that the generated bit-scalable hashing codeswell preserve the discriminative powers with shorter code lengths.
arxiv-13200-288 | Spatio-temporal Spike and Slab Priors for Multiple Measurement Vector Problems | http://arxiv.org/abs/1508.04556 | author:Michael Riis Andersen, Ole Winther, Lars Kai Hansen category:stat.ML published:2015-08-19 summary:We are interested in solving the multiple measurement vector (MMV) problemfor instances, where the underlying sparsity pattern exhibit spatio-temporalstructure motivated by the electroencephalogram (EEG) source localizationproblem. We propose a probabilistic model that takes this structure intoaccount by generalizing the structured spike and slab prior and the associatedExpectation Propagation inference scheme. Based on numerical experiments, wedemonstrate the viability of the model and the approximate inference scheme.
arxiv-13200-289 | Mining Brain Networks using Multiple Side Views for Neurological Disorder Identification | http://arxiv.org/abs/1508.04554 | author:Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu, Ann B. Ragin category:cs.LG cs.CV cs.CY stat.AP stat.ML published:2015-08-19 summary:Mining discriminative subgraph patterns from graph data has attracted greatinterest in recent years. It has a wide variety of applications in diseasediagnosis, neuroimaging, etc. Most research on subgraph mining focuses on thegraph representation alone. However, in many real-world applications, the sideinformation is available along with the graph data. For example, forneurological disorder identification, in addition to the brain networks derivedfrom neuroimaging data, hundreds of clinical, immunologic, serologic andcognitive measures may also be documented for each subject. These measurescompose multiple side views encoding a tremendous amount of supplementalinformation for diagnostic purposes, yet are often ignored. In this paper, westudy the problem of discriminative subgraph selection using multiple sideviews and propose a novel solution to find an optimal set of subgraph featuresfor graph classification by exploring a plurality of side views. We derive afeature evaluation criterion, named gSide, to estimate the usefulness ofsubgraph patterns based upon side views. Then we develop a branch-and-boundalgorithm, called gMSV, to efficiently search for optimal subgraph features byintegrating the subgraph mining process and the procedure of discriminativefeature selection. Empirical studies on graph classification tasks forneurological disorders using brain networks demonstrate that subgraph patternsselected by the multi-side-view guided subgraph selection approach caneffectively boost graph classification performances and are relevant to diseasediagnosis.
arxiv-13200-290 | Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images | http://arxiv.org/abs/1508.04546 | author:Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang, Stefan Gumhold, Carsten Rother category:cs.CV 65-XX published:2015-08-19 summary:Analysis-by-synthesis has been a successful approach for many tasks incomputer vision, such as 6D pose estimation of an object in an RGB-D imagewhich is the topic of this work. The idea is to compare the observation withthe output of a forward process, such as a rendered image of the object ofinterest in a particular pose. Due to occlusion or complicated sensor noise, itcan be difficult to perform this comparison in a meaningful way. We propose anapproach that "learns to compare", while taking these difficulties intoaccount. This is done by describing the posterior density of a particularobject pose with a convolutional neural network (CNN) that compares an observedand rendered image. The network is trained with the maximum likelihoodparadigm. We observe empirically that the CNN does not specialize to thegeometry or appearance of specific objects, and it can be used with objects ofvastly different shapes and appearances, and in different backgrounds. Comparedto state-of-the-art, we demonstrate a significant improvement on two differentdatasets which include a total of eleven objects, cluttered background, andheavy occlusion.
arxiv-13200-291 | Exploring Metaphorical Senses and Word Representations for Identifying Metonyms | http://arxiv.org/abs/1508.04515 | author:Wei Zhang, Judith Gelernter category:cs.CL I.2.7 published:2015-08-19 summary:A metonym is a word with a figurative meaning, similar to a metaphor. Becausemetonyms are closely related to metaphors, we apply features that are usedsuccessfully for metaphor recognition to the task of detecting metonyms. On theACL SemEval 2007 Task 8 data with gold standard metonym annotations, our systemachieved 86.45% accuracy on the location metonyms. Our code can be found onGitHub.
arxiv-13200-292 | Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles | http://arxiv.org/abs/1508.04525 | author:Wei Zhang, Yang Yu, Osho Gupta, Judith Gelernter category:cs.CL cs.LG D.3.3 published:2015-08-19 summary:Precise geocoding and time normalization for text requires that location andtime phrases be identified. Many state-of-the-art geoparsers and temporalparsers suffer from low recall. Categories commonly missed by parsers are:nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases,prepositional phrases, and numerical phrases. We collected and annotated dataset by querying commercial web searches API with such spatiotemporalexpressions as were missed by state-of-the- art parsers. Due to the high costof sentence annotation, active learning was used to label training data, and anew strategy was designed to better select training examples to reduce labelingcost. For the learning algorithm, we applied an average perceptron trainedFeaturized Hidden Markov Model (FHMM). Five FHMM instances were used to createan ensemble, with the output phrase selected by voting. Our ensemble model wastested on a range of sequential labeling tasks, and has shown competitiveperformance. Our contributions include (1) an new dataset annotated with namedentities and expanded spatiotemporal expressions; (2) a comparison of inferencealgorithms for ensemble models showing the superior accuracy of BeliefPropagation over Viterbi Decoding; (3) a new example re-weighting method foractive ensemble learning that 'memorizes' the latest examples trained; (4) aspatiotemporal parser that jointly recognizes expanded spatiotemporalexpressions as well as named entities.
arxiv-13200-293 | End-to-End Attention-based Large Vocabulary Speech Recognition | http://arxiv.org/abs/1508.04395 | author:Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE published:2015-08-18 summary:Many of the current state-of-the-art Large Vocabulary Continuous SpeechRecognition Systems (LVCSR) are hybrids of neural networks and Hidden MarkovModels (HMMs). Most of these systems contain separate components that deal withthe acoustic modelling, language modelling and sequence decoding. Weinvestigate a more direct approach in which the HMM is replaced with aRecurrent Neural Network (RNN) that performs sequence prediction directly atthe character level. Alignment between the input features and the desiredcharacter sequence is learned automatically by an attention mechanism builtinto the RNN. For each predicted character, the attention mechanism scans theinput sequence and chooses relevant frames. We propose two methods to speed upthis operation: limiting the scan to a subset of most promising frames andpooling over time the information contained in neighboring frames, therebyreducing source sequence length. Integrating an n-gram language model into thedecoding process yields recognition accuracies similar to other HMM-freeRNN-based approaches.
arxiv-13200-294 | Preprint ARPPS Augmented Reality Pipeline Prospect System | http://arxiv.org/abs/1508.04238 | author:Xiaolei Zhang, Yong Han, DongSheng Hao, Zhihan Lv category:cs.CV published:2015-08-18 summary:This is the preprint version of our paper on ICONIP. Outdoor augmentedreality geographic information system (ARGIS) is the hot application ofaugmented reality over recent years. This paper concludes the key solutions ofARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),and respectively realizes the machine vision based pipeline prospect system(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With theMVBPPS's realization, this paper studies the neural network based 3D featuresmatching method.
arxiv-13200-295 | Image tag completion by local learning | http://arxiv.org/abs/1508.04224 | author:Jingyan Wang, Yihua Zhou, Haoxiang Wang, Xiaohong Yang, Feng Yang, Austin Peterson category:cs.CV published:2015-08-18 summary:The problem of tag completion is to learn the missing tags of an image. Inthis paper, we propose to learn a tag scoring vector for each image by locallinear learning. A local linear function is used in the neighborhood of eachimage to predict the tag scoring vectors of its neighboring images. Weconstruct a unified objective function for the learning of both tag scoringvectors and local linear function parame- ters. In the objective, we impose thelearned tag scoring vectors to be consistent with the known associations to thetags of each image, and also minimize the prediction error of each local linearfunction, while reducing the complexity of each local function. The objectivefunction is optimized by an alternate optimization strategy and gradientdescent methods in an iterative algorithm. We compare the proposed algorithmagainst different state-of-the-art tag completion methods, and the results showits advantages.
arxiv-13200-296 | Learning Meta-Embeddings by Using Ensembles of Embedding Sets | http://arxiv.org/abs/1508.04257 | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2015-08-18 summary:Word embeddings -- distributed representations of words -- in deep learningare beneficial for many tasks in natural language processing (NLP). However,different embedding sets vary greatly in quality and characteristics of thecaptured semantics. Instead of relying on a more advanced algorithm forembedding learning, this paper proposes an ensemble approach of combiningdifferent public embedding sets with the aim of learning meta-embeddings.Experiments on word similarity and analogy tasks and on part-of-speech taggingshow better performance of meta-embeddings compared to individual embeddingsets. One advantage of meta-embeddings is the increased vocabulary coverage. Wewill release our meta-embeddings publicly.
arxiv-13200-297 | Supervised learning of sparse context reconstruction coefficients for data representation and classification | http://arxiv.org/abs/1508.04221 | author:Xuejie Liu, Jingbin Wang, Ming Yin, Benjamin Edwards, Peijuan Xu category:cs.LG cs.CV published:2015-08-18 summary:Context of data points, which is usually defined as the other data points ina data set, has been found to play important roles in data representation andclassification. In this paper, we study the problem of using context of a datapoint for its classification problem. Our work is inspired by the observationthat actually only very few data points are critical in the context of a datapoint for its representation and classification. We propose to represent a datapoint as the sparse linear combination of its context, and learn the sparsecontext in a supervised way to increase its discriminative ability. To thisend, we proposed a novel formulation for context learning, by modeling thelearning of context parameter and classifier in a unified objective, andoptimizing it with an alternative strategy in an iterative algorithm.Experiments on three benchmark data set show its advantage overstate-of-the-art context-based data representation and classification methods.
arxiv-13200-298 | Scalable Bayesian Non-Negative Tensor Factorization for Massive Count Data | http://arxiv.org/abs/1508.04211 | author:Changwei Hu, Piyush Rai, Changyou Chen, Matthew Harding, Lawrence Carin category:stat.ML cs.LG published:2015-08-18 summary:We present a Bayesian non-negative tensor factorization model forcount-valued tensor data, and develop scalable inference algorithms (both batchand online) for dealing with massive tensors. Our generative model can handleoverdispersed counts as well as infer the rank of the decomposition. Moreover,leveraging a reparameterization of the Poisson distribution as a multinomialfacilitates conjugacy in the model and enables simple and efficient Gibbssampling and variational Bayes (VB) inference updates, with a computationalcost that only depends on the number of nonzeros in the tensor. The model alsoprovides a nice interpretability for the factors; in our model, each factorcorresponds to a "topic". We develop a set of online inference algorithms thatallow further scaling up the model to massive tensors, for which batchinference methods may be infeasible. We apply our framework on diversereal-world applications, such as \emph{multiway} topic modeling on a scientificpublications database, analyzing a political science data set, and analyzing amassive household transactions data set.
arxiv-13200-299 | A Dictionary Learning Approach for Factorial Gaussian Models | http://arxiv.org/abs/1508.04486 | author:Y. Cem Subakan, Johannes Traa, Paris Smaragdis, Noah Stein category:cs.LG stat.ML published:2015-08-18 summary:In this paper, we develop a parameter estimation method for factoriallyparametrized models such as Factorial Gaussian Mixture Model and FactorialHidden Markov Model. Our contributions are two-fold. First, we show that theemission matrix of the standard Factorial Model is unidentifiable even if thetrue assignment matrix is known. Secondly, we address the issue ofidentifiability by making a one component sharing assumption and derive aparameter learning algorithm for this case. Our approach is based on adictionary learning problem of the form $X = O R$, where the goal is to learnthe dictionary $O$ given the data matrix $X$. We argue that due to the specificstructure of the activation matrix $R$ in the shared component factorialmixture model, and an incoherence assumption on the shared component, it ispossible to extract the columns of the $O$ matrix without the need foralternating between the estimation of $O$ and $R$.
arxiv-13200-300 | Robust Subspace Clustering via Smoothed Rank Approximation | http://arxiv.org/abs/1508.04467 | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.IT cs.LG cs.NA math.IT stat.ML published:2015-08-18 summary:Matrix rank minimizing subject to affine constraints arises in manyapplication areas, ranging from signal processing to machine learning. Nuclearnorm is a convex relaxation for this problem which can recover the rank exactlyunder some restricted and theoretically interesting conditions. However, formany real-world applications, nuclear norm approximation to the rank functioncan only produce a result far from the optimum. To seek a solution of higheraccuracy than the nuclear norm, in this paper, we propose a rank approximationbased on Logarithm-Determinant. We consider using this rank approximation forsubspace clustering application. Our framework can model different kinds oferrors and noise. Effective optimization strategy is developed with theoreticalguarantee to converge to a stationary point. The proposed method givespromising results on face clustering and motion segmentation tasks compared tothe state-of-the-art subspace clustering algorithms.
