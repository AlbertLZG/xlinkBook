arxiv-13200-1 | Information retrieval in folktales using natural language processing | http://arxiv.org/pdf/1511.03012v1.pdf | author:Adrian Groza, Lidia Corde category:cs.CL cs.IR published:2015-11-10 summary:Our aim is to extract information about literary characters in unstructuredtexts. We employ natural language processing and reasoning on domainontologies. The first task is to identify the main characters and the parts ofthe story where these characters are described or act. We illustrate the systemin a scenario in the folktale domain. The system relies on a folktale ontologythat we have developed based on Propp's model for folktales morphology.
arxiv-13200-2 | Improvised Salient Object Detection and Manipulation | http://arxiv.org/pdf/1511.02999v1.pdf | author:Abhishek Maity category:cs.CV I.4 published:2015-11-10 summary:In case of salient subject recognition, computer algorithms have been heavilyrelied on scanning of images from top-left to bottom-right systematically andapply brute-force when attempting to locate objects of interest. Thus, theprocess turns out to be quite time consuming. Here a novel approach and asimple solution to the above problem is discussed. In this paper, we implementan approach to object manipulation and detection through segmentation map,which would help to desaturate or, in other words, wash out the background ofthe image. Evaluation for the performance is carried out using the Jaccardindex against the well-known Ground-truth target box technique.
arxiv-13200-3 | Traffic Sign Classification Using Deep Inception Based Convolutional Networks | http://arxiv.org/pdf/1511.02992v1.pdf | author:Mrinal Haloi category:cs.CV 68T45 published:2015-11-10 summary:In this work, we propose a novel deep networks for traffic signclassification that achieves outstanding performance on GTSRB surpassing allprevious methods. Our deep network consists of spatial transformer layers and amodified version of inception module specifically designed for capturing localand global features together. This features adoption allows our network toclassify precisely intra class samples even under deformations. Use of spatialtransformer layer make this network more robust to deformations such astranslation, rotation, scaling of input images. Unlike existing approaches thatare developed with hand crafted features, multiple deep networks with hugeparameters and data augmentations, our method addresses the concern ofexploding parameters and augmentations. We have achieved state-of-the-artperformance of 99.81% on GTSRB dataset.
arxiv-13200-4 | On the Generalization Properties of Differential Privacy | http://arxiv.org/pdf/1504.05800v2.pdf | author:Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR published:2015-04-22 summary:A new line of work, started with Dwork et al., studies the task of answeringstatistical queries using a sample and relates the problem to the concept ofdifferential privacy. By the Hoeffding bound, a sample of size $O(\logk/\alpha^2)$ suffices to answer $k$ non-adaptive queries within error $\alpha$,where the answers are computed by evaluating the statistical queries on thesample. This argument fails when the queries are chosen adaptively (and canhence depend on the sample). Dwork et al. showed that if the answers arecomputed with $(\epsilon,\delta)$-differential privacy then $O(\epsilon)$accuracy is guaranteed with probability $1-O(\delta^\epsilon)$. Using thePrivate Multiplicative Weights mechanism, they concluded that the sample sizecan still grow polylogarithmically with the $k$. Very recently, Bassily et al. presented an improved bound and showed that (avariant of) the private multiplicative weights algorithm can answer $k$adaptively chosen statistical queries using sample complexity that growslogarithmically in $k$. However, their results no longer hold for everydifferentially private algorithm, and require modifying the privatemultiplicative weights algorithm in order to obtain their high probabilitybounds. We greatly simplify the results of Dwork et al. and improve on the bound byshowing that differential privacy guarantees $O(\epsilon)$ accuracy withprobability $1-O(\delta\log(1/\epsilon)/\epsilon)$. It would be tempting toguess that an $(\epsilon,\delta)$-differentially private computation shouldguarantee $O(\epsilon)$ accuracy with probability $1-O(\delta)$. However, weshow that this is not the case, and that our bound is tight (up to logarithmicfactors).
arxiv-13200-5 | More General Queries and Less Generalization Error in Adaptive Data Analysis | http://arxiv.org/pdf/1503.04843v2.pdf | author:Raef Bassily, Adam Smith, Thomas Steinke, Jonathan Ullman category:cs.LG cs.DS published:2015-03-16 summary:Adaptivity is an important feature of data analysis---typically the choice ofquestions asked about a dataset depends on previous interactions with the samedataset. However, generalization error is typically bounded in a non-adaptivemodel, where all questions are specified before the dataset is drawn. Recentwork by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated theformal study of this problem, and gave the first upper and lower bounds on theachievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution $\mathcal{P}$ and aset of $n$ independent samples $x$ is drawn from $\mathcal{P}$. We seek analgorithm that, given $x$ as input, "accurately" answers a sequence ofadaptively chosen "queries" about the unknown distribution $\mathcal{P}$. Howmany samples $n$ must we draw from the distribution, as a function of the typeof queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions towards resolving this question: *We give upper bounds on the number of samples $n$ that are needed to answerstatistical queries that improve over the bounds of Dwork et al. *We prove the first upper bounds on the number of samples required to answermore general families of queries. These include arbitrary low-sensitivityqueries and the important class of convex risk minimization queries. As in Dwork et al., our algorithms are based on a connection betweendifferential privacy and generalization error, but we feel that our analysis issimpler and more modular, which may be useful for studying these questions inthe future.
arxiv-13200-6 | Dimension of Marginals of Kronecker Product Models | http://arxiv.org/pdf/1511.03570v1.pdf | author:Guido Montufar, Jason Morton category:stat.ML cs.NE math.AG math.CO math.PR 14T05, 52B05 published:2015-11-10 summary:A Kronecker product model is the set of visible marginal probabilitydistributions of an exponential family whose sufficient statistics matrixfactorizes as a Kronecker product of two matrices, one for the visiblevariables and one for the hidden variables. We estimate the dimension of thesemodels by the maximum rank of the Jacobian in the limit of large parameters.The limit is described by the tropical morphism; a piecewise linear map withpieces corresponding to slicings of the visible matrix by the normal fan of thehidden matrix. We obtain combinatorial conditions under which the model has theexpected dimension, equal to the minimum of the number of natural parametersand the dimension of the ambient probability simplex. Additionally, we provethat the binary restricted Boltzmann machine always has the expected dimension.
arxiv-13200-7 | Hyperspectral Image Recovery from Incomplete and Imperfect Measurements via Hybrid Regularization | http://arxiv.org/pdf/1511.02928v1.pdf | author:Reza Arablouei, Frank de Hoog category:cs.CV published:2015-11-09 summary:Natural images tend to mostly consist of smooth regions with individualpixels having highly correlated spectra. This information can be exploited torecover hyperspectral images of natural scenes from their incomplete and noisymeasurements. To perform the recovery while taking full advantage of the priorknowledge, we formulate a composite cost function containing a square-errordata-fitting term and two distinct regularization terms pertaining to spatialand spectral domains. The regularization for the spatial domain is the sum oftotal-variation of the image frames corresponding to all spectral bands. Theregularization for the spectral domain is the l_1-norm of the coefficientmatrix obtained by applying a suitable sparsifying transform to the spectra ofthe pixels. We use an accelerated proximal-subgradient method to minimize theformulated cost function. We analyse the performance of the proposed algorithmand prove its convergence. Numerical simulations using real hyperspectralimages exhibit that the proposed algorithm offers an excellent recoveryperformance with a number of measurements that is only a small fraction of thehyperspectral image data size. Simulation results also show that the proposedalgorithm significantly outperforms an accelerated proximal-gradient algorithmthat solves the classical basis-pursuit denoising problem to recover thehyperspectral image.
arxiv-13200-8 | Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries | http://arxiv.org/pdf/1507.05670v2.pdf | author:Yuke Zhu, Ce Zhang, Christopher RÃ©, Li Fei-Fei category:cs.CV cs.LG published:2015-07-20 summary:The complexity of the visual world creates significant challenges forcomprehensive visual understanding. In spite of recent successes in visualrecognition, today's vision systems would still struggle to deal with visualqueries that require a deeper reasoning. We propose a knowledge base (KB)framework to handle an assortment of visual queries, without the need to trainnew classifiers for new tasks. Building such a large-scale multimodal KBpresents a major challenge of scalability. We cast a large-scale MRF into a KBrepresentation, incorporating visual, textual and structured data, as well astheir diverse relations. We introduce a scalable knowledge base constructionsystem that is capable of building a KB with half billion variables andmillions of parameters in a few hours. Our system achieves competitive resultscompared to purpose-built models on standard recognition and retrieval tasks,while exhibiting greater flexibility in answering richer visual queries.
arxiv-13200-9 | Massive Online Crowdsourced Study of Subjective and Objective Picture Quality | http://arxiv.org/pdf/1511.02919v1.pdf | author:Deepti Ghadiyaram, Alan C. Bovik category:cs.CV published:2015-11-09 summary:Most publicly available image quality databases have been created underhighly controlled conditions by introducing graded simulated distortions ontohigh-quality photographs. However, images captured using typical real-worldmobile camera devices are usually afflicted by complex mixtures of multipledistortions, which are not necessarily well-modeled by the syntheticdistortions found in existing databases. The originators of existing legacydatabases usually conducted human psychometric studies to obtain statisticallymeaningful sets of human opinion scores on images in a stringently controlledvisual environment, resulting in small data collections relative to other kindsof image analysis databases. Towards overcoming these limitations, we designedand created a new database that we call the LIVE In the Wild Image QualityChallenge Database, which contains widely diverse authentic image distortionson a large number of images captured using a representative variety of modernmobile devices. We also designed and implemented a new online crowdsourcingsystem, which we have used to conduct a very large-scale, multi-month imagequality assessment subjective study. Our database consists of over 350000opinion scores on 1162 images evaluated by over 7000 unique human observers.Despite the lack of control over the experimental environments of the numerousstudy participants, we demonstrate excellent internal consistency of thesubjective dataset. We also evaluate several top-performing blind Image QualityAssessment algorithms on it and present insights on how mixtures of distortionschallenge both end users as well as automatic perceptual quality predictionmodels.
arxiv-13200-10 | Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders | http://arxiv.org/pdf/1511.02916v1.pdf | author:Zhouhan Lin, Yushi Chen, Xing Zhao, Gang Wang category:cs.CV cs.AI cs.LG published:2015-11-09 summary:Hyperspectral image (HSI) classification is a hot topic in the remote sensingcommunity. This paper proposes a new framework of spectral-spatial featureextraction for HSI classification, in which for the first time the concept ofdeep learning is introduced. Specifically, the model of autoencoder isexploited in our framework to extract various kinds of features. First weverify the eligibility of autoencoder by following classical spectralinformation based classification and use autoencoders with different depth toclassify hyperspectral image. Further in the proposed framework, we combine PCAon spectral dimension and autoencoder on the other two spatial dimensions toextract spectral-spatial information for classification. The experimentalresults show that this framework achieves the highest classification accuracyamong all methods, and outperforms classical classifiers such as SVM andPCA-based SVM.
arxiv-13200-11 | Efficient Construction of Local Parametric Reduced Order Models Using Machine Learning Techniques | http://arxiv.org/pdf/1511.02909v1.pdf | author:Azam Moosavi, Razvan Stefanescu, Adrian Sandu category:cs.LG published:2015-11-09 summary:Reduced order models are computationally inexpensive approximations thatcapture the important dynamical characteristics of large, high-fidelitycomputer models of physical systems. This paper applies machine learningtechniques to improve the design of parametric reduced order models.Specifically, machine learning is used to develop feasible regions in theparameter space where the admissible target accuracy is achieved with apredefined reduced order basis, to construct parametric maps, to chose the besttwo already existing bases for a new parameter configuration from accuracypoint of view and to pre-select the optimal dimension of the reduced basis suchas to meet the desired accuracy. By combining available information using basesconcatenation and interpolation as well as high-fidelity solutionsinterpolation we are able to build accurate reduced order models associatedwith new parameter settings. Promising numerical results with a viscous Burgersmodel illustrate the potential of machine learning approaches to help designbetter reduced order models.
arxiv-13200-12 | Visual Language Modeling on CNN Image Representations | http://arxiv.org/pdf/1511.02872v1.pdf | author:Hiroharu Kato, Tatsuya Harada category:cs.CV cs.AI cs.LG published:2015-11-09 summary:Measuring the naturalness of images is important to generate realistic imagesor to detect unnatural regions in images. Additionally, a method to measurenaturalness can be complementary to Convolutional Neural Network (CNN) basedfeatures, which are known to be insensitive to the naturalness of images.However, most probabilistic image models have insufficient capability ofmodeling the complex and abstract naturalness that we feel because they arebuilt directly on raw image pixels. In this work, we assume that naturalnesscan be measured by the predictability on high-level features during eyemovement. Based on this assumption, we propose a novel method to evaluate thenaturalness by building a variant of Recurrent Neural Network Language Modelson pre-trained CNN representations. Our method is applied to two tasks,demonstrating that 1) using our method as a regularizer enables us to generatemore understandable images from image features than existing approaches, and 2)unnaturalness maps produced by our method achieve state-of-the-art eye fixationprediction performance on two well-studied datasets.
arxiv-13200-13 | Tree-structured composition in neural networks without tree-structured architectures | http://arxiv.org/pdf/1506.04834v3.pdf | author:Samuel R. Bowman, Christopher D. Manning, Christopher Potts category:cs.CL cs.LG published:2015-06-16 summary:Tree-structured neural networks encode a particular tree geometry for asentence in the network design. However, these models have at best onlyslightly outperformed simpler sequence-based models. We hypothesize that neuralsequence models like LSTMs are in fact able to discover and implicitly userecursive compositional structure, at least for tasks with clear cues to thatstructure in the data. We demonstrate this possibility using an artificial datatask for which recursive compositional structure is crucial, and find anLSTM-based sequence model can indeed learn to exploit the underlying treestructure. However, its performance consistently lags behind that of treemodels, even on large training sets, suggesting that tree-structured models aremore effective at exploiting recursive structure.
arxiv-13200-14 | Bayesian Inference in Cumulative Distribution Fields | http://arxiv.org/pdf/1511.02796v1.pdf | author:Ricardo Silva category:stat.ML stat.ME published:2015-11-09 summary:One approach for constructing copula functions is by multiplication. Giventhat products of cumulative distribution functions (CDFs) are also CDFs, anadjustment to this multiplication will result in a copula model, as discussedby Liebscher (J Mult Analysis, 2008). Parameterizing models via products ofCDFs has some advantages, both from the copula perspective (e.g., it iswell-defined for any dimensionality) and from general multivariate analysis(e.g., it provides models where small dimensional marginal distributions can beeasily read-off from the parameters). Independently, Huang and Frey (J MachLearn Res, 2011) showed the connection between certain sparse graphical modelsand products of CDFs, as well as message-passing (dynamic programming) schemesfor computing the likelihood function of such models. Such schemes allowsmodels to be estimated with likelihood-based methods. We discuss anddemonstrate MCMC approaches for estimating such models in a Bayesian context,their application in copula modeling, and how message-passing can be stronglysimplified. Importantly, our view of message-passing opens up possibilities toscaling up such methods, given that even dynamic programming is not a scalablesolution for calculating likelihood functions in many models.
arxiv-13200-15 | Communication-Efficient False Discovery Rate Control via Knockoff Aggregation | http://arxiv.org/pdf/1506.05446v2.pdf | author:Weijie Su, Junyang Qian, Linxi Liu category:stat.ML stat.ME published:2015-06-17 summary:The false discovery rate (FDR)---the expected fraction of spuriousdiscoveries among all the discoveries---provides a popular statisticalassessment of the reproducibility of scientific studies in various disciplines.In this work, we introduce a new method for controlling the FDR inmeta-analysis of many decentralized linear models. Our method targets thescenario where many research groups---possibly the number of which israndom---are independently testing a common set of hypotheses and then sendingsummary statistics to a coordinating center in an online manner. Built on theknockoffs framework introduced by Barber and Candes (2015), our procedurestarts by applying the knockoff filter to each linear model and then aggregatesthe summary statistics via one-shot communication in a novel way. This methodgives exact FDR control non-asymptotically without any knowledge of the noisevariances or making any assumption about sparsity of the signal. In certainsettings, it has a communication complexity that is optimal up to a logarithmicfactor.
arxiv-13200-16 | A Survey of Algorithms and Analysis for Adaptive Online Learning | http://arxiv.org/pdf/1403.3465v3.pdf | author:H. Brendan McMahan category:cs.LG published:2014-03-14 summary:We present tools for the analysis of Follow-The-Regularized-Leader (FTRL),Dual Averaging, and Mirror Descent algorithms when the regularizer(equivalently, prox-function or learning rate schedule) is chosen adaptivelybased on the data. Adaptivity can be used to prove regret bounds that hold onevery round, and also allows for data-dependent regret bounds as inAdaGrad-style algorithms (e.g., Online Gradient Descent with adaptiveper-coordinate learning rates). We present results from a large number of priorworks in a unified manner, using a modular and tight analysis that isolates thekey arguments in easily re-usable lemmas. This approach strengthens pre-viouslyknown FTRL analysis techniques to produce bounds as tight as those achieved bypotential functions or primal-dual analysis. Further, we prove a general andexact equivalence between an arbitrary adaptive Mirror Descent algorithm and acorrespond- ing FTRL update, which allows us to analyze any Mirror Descentalgorithm in the same framework. The key to bridging the gap between DualAveraging and Mirror Descent algorithms lies in an analysis of theFTRL-Proximal algorithm family. Our regret bounds are proved in the mostgeneral form, holding for arbitrary norms and non-smooth regularizers withtime-varying weight.
arxiv-13200-17 | PAC-Bayesian High Dimensional Bipartite Ranking | http://arxiv.org/pdf/1511.02729v1.pdf | author:Benjamin Guedj, Sylvain Robbiano category:stat.ML math.ST stat.TH published:2015-11-09 summary:This paper is devoted to the bipartite ranking problem, a classicalstatistical learning task, in a high dimensional setting. We propose a scoringand ranking strategy based on the PAC-Bayesian approach. We consider nonlinearadditive scoring functions, and we derive non-asymptotic risk bounds under asparsity assumption. In particular, oracle inequalities in probability holdingunder a margin condition assess the performance of our procedure, and prove itsminimax optimality. An MCMC-flavored algorithm is proposed to implement ourmethod, along with its behavior on synthetic and real-life datasets.
arxiv-13200-18 | Learning Instrumental Variables with Non-Gaussianity Assumptions: Theoretical Limitations and Practical Algorithms | http://arxiv.org/pdf/1511.02722v1.pdf | author:Ricardo Silva, Shohei Shimizu category:stat.ML published:2015-11-09 summary:Learning a causal effect from observational data is not straightforward, asthis is not possible without further assumptions. If hidden common causesbetween treatment $X$ and outcome $Y$ cannot be blocked by other measurements,one possibility is to use an instrumental variable. In principle, it ispossible under some assumptions to discover whether a variable is structurallyinstrumental to a target causal effect $X \rightarrow Y$, but currentframeworks are somewhat lacking on how general these assumptions can be. Ainstrumental variable discovery problem is challenging, as no variable can betested as an instrument in isolation but only in groups, but differentvariables might require different conditions to be considered an instrument.Moreover, identification constraints might be hard to detect statistically. Inthis paper, we give a theoretical characterization of instrumental variablediscovery, highlighting identifiability problems and solutions, the need fornon-Gaussianity assumptions, and how they fit within existing methods.
arxiv-13200-19 | Biologically Inspired Dynamic Textures for Probing Motion Perception | http://arxiv.org/pdf/1511.02705v1.pdf | author:Jonathan Vacher, Andrew Meso, Laurent U Perrinet, Gabriel PeyrÃ© category:cs.CV math.ST stat.TH published:2015-11-09 summary:Perception is often described as a predictive process based on an optimalinference with respect to a generative model. We study here the principledconstruction of a generative model specifically crafted to probe motionperception. In that context, we first provide an axiomatic, biologically-drivenderivation of the model. This model synthesizes random dynamic textures whichare defined by stationary Gaussian distributions obtained by the randomaggregation of warped patterns. Importantly, we show that this model canequivalently be described as a stochastic partial differential equation. Usingthis characterization of motion in images, it allows us to recast motion-energymodels into a principled Bayesian inference framework. Finally, we apply thesetextures in order to psychophysically probe speed perception in humans. In thisframework, while the likelihood is derived from the generative model, the prioris estimated from the observed results and accounts for the perceptual bias ina principled fashion.
arxiv-13200-20 | On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions | http://arxiv.org/pdf/1502.06800v2.pdf | author:Francis Bach category:cs.LG math.NA stat.ML published:2015-02-24 summary:We show that kernel-based quadrature rules for computing integrals can beseen as a special case of random feature expansions for positive definitekernels, for a particular decomposition that always exists for such kernels. Weprovide a theoretical analysis of the number of required samples for a givenapproximation error, leading to both upper and lower bounds that are basedsolely on the eigenvalues of the associated integral operator and match up tologarithmic terms. In particular, we show that the upper bound may be obtainedfrom independent and identically distributed samples from a specificnon-uniform distribution, while the lower bound if valid for any set of points.Applying our results to kernel-based quadrature, while our results are fairlygeneral, we recover known upper and lower bounds for the special cases ofSobolev spaces. Moreover, our results extend to the more general problem offull function approximations (beyond simply computing an integral), withresults in L2- and L$\infty$-norm that match known results for special cases.Applying our results to random features, we show an improvement of the numberof random features needed to preserve the generalization guarantees forlearning with Lipschitz-continuous losses.
arxiv-13200-21 | A Lightened CNN for Deep Face Representation | http://arxiv.org/pdf/1511.02683v1.pdf | author:Xiang Wu, Ran He, Zhenan Sun category:cs.CV published:2015-11-09 summary:Convolution neural network (CNN) has significantly pushed forward thedevelopment of face recognition techniques. To achieve ultimate accuracy, CNNmodels tend to be deeper or multiple local facial patch ensemble, which resultin a waste of time and space. To alleviate this issue, this paper studies alightened CNN framework to learn a compact embedding for face representation.First, we introduce the concept of maxout in the fully connected layer to theconvolution layer, which leads to a new activation function, namedMax-Feature-Map (MFM). Compared with widely used ReLU, MFM can simultaneouslycapture compact representation and competitive information. Then, one shallowCNN model is constructed by 4 convolution layers and totally contains about 4Mparameters; and the other is constructed by reducing the kernel size ofconvolution layers and adding Network in Network (NIN) layers betweenconvolution layers based on the previous one. These models are trained on theCASIA-WebFace dataset and evaluated on the LFW and YTF datasets. Experimentalresults show that the proposed models achieve state-of-the-art results. At thesame time, a reduction of computational cost is reached by over 9 times incomparison with the released VGG model.
arxiv-13200-22 | Exploiting Egocentric Object Prior for 3D Saliency Detection | http://arxiv.org/pdf/1511.02682v1.pdf | author:Gedas Bertasius, Hyun Soo Park, Jianbo Shi category:cs.CV published:2015-11-09 summary:On a minute-to-minute basis people undergo numerous fluid interactions withobjects that barely register on a conscious level. Recent neuroscientificresearch demonstrates that humans have a fixed size prior for salient objects.This suggests that a salient object in 3D undergoes a consistent transformationsuch that people's visual system perceives it with an approximately fixed size.This finding indicates that there exists a consistent egocentric object priorthat can be characterized by shape, size, depth, and location in the firstperson view. In this paper, we develop an EgoObject Representation, which encodes thesecharacteristics by incorporating shape, location, size and depth features froman egocentric RGBD image. We empirically show that this representation canaccurately characterize the egocentric object prior by testing it on anegocentric RGBD dataset for three tasks: the 3D saliency detection, futuresaliency prediction, and interaction classification. This representation isevaluated on our new Egocentric RGBD Saliency dataset that includes variousactivities such as cooking, dining, and shopping. By using our EgoObjectrepresentation, we outperform previously proposed models for saliency detection(relative 30% improvement for 3D saliency detection task) on our dataset.Additionally, we demonstrate that this representation allows us to predictfuture salient objects based on the gaze cue and classify people's interactionswith objects.
arxiv-13200-23 | Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding | http://arxiv.org/pdf/1511.02680v1.pdf | author:Alex Kendall, Vijay Badrinarayanan, Roberto Cipolla category:cs.CV cs.NE published:2015-11-09 summary:We present a novel deep learning framework for probabilistic pixel-wisesemantic segmentation, which we term Bayesian SegNet. Pixel-wise semanticsegmentation is an important step for visual scene understanding. It is acomplex task requiring knowledge of support relationships and contextualinformation, as well as visual appearance. Our contribution is a practicalsystem which is able to predict pixel-wise class labels with a measure of modeluncertainty. We achieve this by Monte Carlo sampling with dropout at test timeto generate a posterior distribution of pixel class labels. We show thisBayesian neural network provides a significant performance improvement insegmentation, with no additional parameterisation. We set a new benchmark withstate-of-the-art performance on both the indoor SUN Scene Understanding andoutdoor CamVid driving scenes datasets. Bayesian SegNet also performscompetitively on Pascal VOC 2012 object segmentation challenge. For our webdemo and source code, see http://mi.eng.cam.ac.uk/projects/segnet/
arxiv-13200-24 | Semantic Segmentation with Boundary Neural Fields | http://arxiv.org/pdf/1511.02674v1.pdf | author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV published:2015-11-09 summary:The state-of-the-art in semantic segmentation is currently represented byfully convolutional networks (FCNs). However, FCNs use large receptive fieldsand many pooling layers, both of which cause blurring and low spatialresolution in the deep layers. As a result FCNs tend to produce segmentationsthat are poorly localized around object boundaries. Prior work has attempted toaddress this issue in post-processing steps, for example using a color-basedCRF on top of the FCN predictions. However, these approaches require additionalparameters and low-level features that are difficult to tune and integrate intothe original network architecture. Additionally, most CRFs use color-basedpixel affinities, which are not well suited for semantic segmentation and leadto spatially disjoint predictions. To overcome these problems, we introduce a Boundary Neural Field (BNF), whichis a global energy model integrating FCN predictions with boundary cues. Theboundary information is used to enhance semantic segment coherence and toimprove object localization. Specifically, we first show that the convolutionalfilters of semantic FCNs provide good features for boundary detection. We thenemploy the predicted boundaries to define pairwise potentials in our energy.Finally, we show that our energy decomposes semantic segmentation into multiplebinary problems, which can be relaxed for efficient global optimization. Wereport extensive experiments demonstrating that minimization of our globalboundary-based energy yields results superior to prior globalization methods,both quantitatively as well as qualitatively.
arxiv-13200-25 | Enacting textual entailment and ontologies for automated essay grading in chemical domain | http://arxiv.org/pdf/1511.02669v1.pdf | author:Adrian Groza, Roxana Szabo category:cs.AI cs.CL published:2015-11-09 summary:We propose a system for automated essay grading using ontologies and textualentailment. The process of textual entailment is guided by hypotheses, whichare extracted from a domain ontology. Textual entailment checks if the truth ofthe hypothesis follows from a given text. We enact textual entailment tocompare students answer to a model answer obtained from ontology. We validatedthe solution against various essays written by students in the chemistrydomain.
arxiv-13200-26 | An Efficient Multilinear Optimization Framework for Hypergraph Matching | http://arxiv.org/pdf/1511.02667v1.pdf | author:Quynh Nguyen, Francesco Tudisco, Antoine Gautier, Matthias Hein category:cs.CV cs.DS math.OC published:2015-11-09 summary:Hypergraph matching has recently become a popular approach for solvingcorrespondence problems in computer vision as it allows to integratehigher-order geometric information. Hypergraph matching can be formulated as athird-order optimization problem subject to the assignment constraints whichturns out to be NP-hard. In recent work, we have proposed an algorithm forhypergraph matching which first lifts the third-order problem to a fourth-orderproblem and then solves the fourth-order problem via optimization of thecorresponding multilinear form. This leads to a tensor block coordinate ascentscheme which has the guarantee of providing monotonic ascent in the originalmatching score function and leads to state-of-the-art performance both in termsof achieved matching score and accuracy. In this paper we show that the liftingstep to a fourth-order problem can be avoided yielding a third-order schemewith the same guarantees and performance but being two times faster. Moreover,we introduce a homotopy type method which further improves the performance.
arxiv-13200-27 | Learning Definite Horn Formulas from Closure Queries | http://arxiv.org/pdf/1503.09025v3.pdf | author:Marta Arias, JosÃ© L. BalcÃ¡zar, Cristina TÃ®rnÄucÄ category:cs.LG cs.LO published:2015-03-31 summary:A definite Horn theory is a set of n-dimensional Boolean vectors whosecharacteristic function is expressible as a definite Horn formula, that is, asconjunction of definite Horn clauses. The class of definite Horn theories isknown to be learnable under different query learning settings, such as learningfrom membership and equivalence queries or learning from entailment. We proposeyet a different type of query: the closure query. Closure queries are a naturalextension of membership queries and also a variant, appropriate in the contextof definite Horn formulas, of the so-called correction queries. We present analgorithm that learns conjunctions of definite Horn clauses in polynomial time,using closure and equivalence queries, and show how it relates to the canonicalGuigues-Duquenne basis for implicational systems. We also show how thedifferent query models mentioned relate to each other by either showingfull-fledged reductions by means of query simulation (where possible), or byshowing their connections in the context of particular algorithms that use themfor learning definite Horn formulas.
arxiv-13200-28 | Toward Biochemical Probabilistic Computation | http://arxiv.org/pdf/1511.02623v1.pdf | author:Jacques Droulez, David Colliaux, Audrey Houillon, Pierre BessiÃ¨re category:cs.ET cs.NE q-bio.MN published:2015-11-09 summary:Living organisms survive and multiply even though they have uncertain andincomplete information about their environment and imperfect models to predictthe consequences of their actions. Bayesian models have been proposed to facethis challenge. Indeed, Bayesian inference is a way to do optimal reasoningwhen only uncertain and incomplete information is available. Variousperceptive, sensory-motor, and cognitive functions have been successfullymodeled this way. However, the biological mechanisms allowing animals andhumans to represent and to compute probability distributions are not known. Ithas been proposed that neurons and assemblies of neurons could be theappropriate scale to search for clues to probabilistic reasoning. In contrast,in this paper, we propose that interacting populations of macromolecules anddiffusible messengers can perform probabilistic computation. This suggests thatprobabilistic reasoning, based on cellular signaling pathways, is a fundamentalskill of living organisms available to the simplest unicellular organisms aswell as the most complex brains.
arxiv-13200-29 | Decomposition Bounds for Marginal MAP | http://arxiv.org/pdf/1511.02619v1.pdf | author:Wei Ping, Qiang Liu, Alexander Ihler category:cs.LG cs.AI cs.IT math.IT stat.ML published:2015-11-09 summary:Marginal MAP inference involves making MAP predictions in systems definedwith latent variables or missing information. It is significantly moredifficult than pure marginalization and MAP tasks, for which a large class ofefficient and convergent variational algorithms, such as dual decomposition,exist. In this work, we generalize dual decomposition to a generic power suminference task, which includes marginal MAP, along with pure marginalizationand MAP, as special cases. Our method is based on a block coordinate descentalgorithm on a new convex decomposition bound, that is guaranteed to convergemonotonically, and can be parallelized efficiently. We demonstrate our approachon marginal MAP queries defined on real-world problems from the UAI approximateinference challenge, showing that our framework is faster and more reliablethan previous methods.
arxiv-13200-30 | A New Relaxation Approach to Normalized Hypergraph Cut | http://arxiv.org/pdf/1511.02595v1.pdf | author:Cong Xie, Wu-Jun Li, Zhihua Zhang category:cs.LG cs.DS published:2015-11-09 summary:Normalized graph cut (NGC) has become a popular research topic due to itswide applications in a large variety of areas like machine learning and verylarge scale integration (VLSI) circuit design. Most of traditional NGC methodsare based on pairwise relationships (similarities). However, in real-worldapplications relationships among the vertices (objects) may be more complexthan pairwise, which are typically represented as hyperedges in hypergraphs.Thus, normalized hypergraph cut (NHC) has attracted more and more attention.Existing NHC methods cannot achieve satisfactory performance in realapplications. In this paper, we propose a novel relaxation approach, which iscalled relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as anoptimization problem on the Stiefel manifold. To solve this problem, we resortto the Cayley transformation to devise a feasible learning algorithm.Experimental results on a set of large hypergraph benchmarks for clustering andpartitioning in VLSI domain show that RNHC can outperform the state-of-the-artmethods.
arxiv-13200-31 | Parkinson's disease patient rehabilitation using gaming platforms: lessons learnt | http://arxiv.org/pdf/1511.02589v1.pdf | author:Ioannis Pachoulakis, Nikolaos Papadopoulos, Cleanthe Spanaki category:cs.CY cs.CV I.6.3; I.6.8 published:2015-11-09 summary:Parkinson's disease (PD) is a progressive neurodegenerative movement disorderwhere motor dysfunction gradually increases as the disease progress. Inaddition to administering dopaminergic PD-specific drugs, attendingneurologists strongly recommend regular exercise combined with physiotherapy.However, because of the long-term nature of the disease, patients followingtraditional rehabilitation programs may get bored, lose interest and eventuallydrop out as a direct result of the repeatability and predictability of theprescribed exercises. Technology supported opportunities to liven up a dailyexercise schedule have appeared in the form of character-based, virtual realitygames which promote physical training in a non-linear and looser fashion andprovide an experience that varies from one game loop the next. Such"exergames", a word that results from the amalgamation of the words "exercise"and "game" challenge patients into performing movements of varying complexityin a playful and immersive virtual environment. Today's game consoles such asNintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present newopportunities to infuse motivation and variety to an otherwise mundanephysiotherapy routine. In this paper we present some of these approaches,discuss their suitability for these PD patients, mainly on the basis of demandsmade on balance, agility and gesture precision, and present design principlesthat exergame platforms must comply with in order to be suitable for PDpatients.
arxiv-13200-32 | Batch-normalized Maxout Network in Network | http://arxiv.org/pdf/1511.02583v1.pdf | author:Jia-Ren Chang, Yong-Sheng Chen category:cs.CV cs.LG published:2015-11-09 summary:This paper reports a novel deep architecture referred to as Maxout network InNetwork (MIN), which can enhance model discriminability and facilitate theprocess of information abstraction within the receptive field. The proposednetwork adopts the framework of the recently developed Network In Networkstructure, which slides a universal approximator, multilayer perceptron (MLP)with rectifier units, to exact features. Instead of MLP, we employ maxout MLPto learn a variety of piecewise linear activation functions and to mediate theproblem of vanishing gradients that can occur when using rectifier units.Moreover, batch normalization is applied to reduce the saturation of maxoutunits by pre-conditioning the model and dropout is applied to preventoverfitting. Finally, average pooling is used in all pooling layers toregularize maxout MLP in order to facilitate information abstraction in everyreceptive field while tolerating the change of object position. Because averagepooling preserves all features in the local patch, the proposed MIN model canenforce the suppression of irrelevant information during training. Ourexperiments demonstrated the state-of-the-art classification performance whenthe MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets andcomparable performance for SVHN dataset.
arxiv-13200-33 | How far can we go without convolution: Improving fully-connected networks | http://arxiv.org/pdf/1511.02580v1.pdf | author:Zhouhan Lin, Roland Memisevic, Kishore Konda category:cs.LG cs.NE published:2015-11-09 summary:We propose ways to improve the performance of fully connected networks. Wefound that two approaches in particular have a strong effect on performance:linear bottleneck layers and unsupervised pre-training using autoencoderswithout hidden unit biases. We show how both approaches can be related toimproving gradient flow and reducing sparsity in the network. We show that afully connected network can yield approximately 70% classification accuracy onthe permutation-invariant CIFAR-10 task, which is much higher than the currentstate-of-the-art. By adding deformations to the training data, the fullyconnected network achieves 78% accuracy, which is just 10% short of a decentconvolutional network.
arxiv-13200-34 | Scale-aware Fast R-CNN for Pedestrian Detection | http://arxiv.org/pdf/1510.08160v2.pdf | author:Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Shuicheng Yan category:cs.CV published:2015-10-28 summary:Intuitively, instances of the same object category with different spatialscales may exhibit dramatically different features. Thus, large variance ininstance scales, which results in undesirable large intra-category variance infeatures, may severely hurt the performance of modern object instance detectionmethods. We argue that this issue can be substantially alleviated by thedivide-and-conquer philosophy. Taking pedestrian detection as an example, weillustrate how we can leverage this philosophy to develop a Scale-Aware FastR-CNN (SAF R-CNN) framework. The model introduces multiple built-insub-networks which detect pedestrians with scales from disjoint ranges. Outputsfrom all the sub-networks are then adaptively combined to generate the finaldetection results that are shown to be robust to large variance in instancescales, via a gate function defined over the sizes of object proposals.Extensive evaluations on the challenging Caltech pedestrian detectiondataset~\cite{dollar2012pedestrian} well demonstrate the superiority of theproposed SAF R-CNN over the state-of-the-arts. Particularly, the miss rate isreduced to $9.68\%$, which is significantly smaller than $11.75\%$ byCompACT-Deep~\cite{compact}, $20.86\%$ by TA-CNN~\cite{ta_cnn} and $12.86\%$ bythe vanilla Fast R-CNN model~\cite{girshick2015fast}.
arxiv-13200-35 | A Century of Portraits: A Visual Historical Record of American High School Yearbooks | http://arxiv.org/pdf/1511.02575v1.pdf | author:Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Alexei A. Efros category:cs.CV published:2015-11-09 summary:Many details about our world are not captured in written records because theyare too mundane or too abstract to describe in words. Fortunately, since theinvention of the camera, an ever-increasing number of photographs capture muchof this otherwise lost information. This plethora of artifacts documenting our"visual culture" is a treasure trove of knowledge as yet untapped byhistorians. We present a dataset of 37,921 frontal-facing American high schoolyearbook photos that allow us to use computation to glimpse into the historicalvisual record too voluminous to be evaluated manually. The collected portraitsprovide a constant visual frame of reference with varying content. We cantherefore use them to consider issues such as a decade's defining styleelements, or trends in fashion and social norms over time. We demonstrate thatour historical image dataset may be used together with weakly-superviseddata-driven techniques to perform scalable historical analysis of large imagecorpora with minimal human effort, much in the same way that large text corporatogether with natural language processing revolutionized historians' workflow.Furthermore, we demonstrate the use of our dataset in dating grayscaleportraits using deep learning methods.
arxiv-13200-36 | Thresholding for Top-k Recommendation with Temporal Dynamics | http://arxiv.org/pdf/1506.02190v2.pdf | author:Lei Tang category:cs.IR cs.LG published:2015-06-06 summary:This work focuses on top-k recommendation in domains where underlying datadistribution shifts overtime. We propose to learn a time-dependent bias foreach item over whatever existing recommendation engine. Such a bias learningprocess alleviates data sparsity in constructing the engine, and at the sametime captures recent trend shift observed in data. We present an alternatingoptimization framework to resolve the bias learning problem, and developmethods to handle a variety of commonly used recommendation evaluationcriteria, as well as large number of items and users in practice. The proposedalgorithm is examined, both offline and online, using real world data setscollected from the largest retailer worldwide. Empirical results demonstratethat the bias learning can almost always boost recommendation performance. Weencourage other practitioners to adopt it as a standard component inrecommender systems where temporal dynamics is a norm.
arxiv-13200-37 | Sentiment Expression via Emoticons on Social Media | http://arxiv.org/pdf/1511.02556v1.pdf | author:Hao Wang, Jorge A. Castanon category:cs.CL cs.SI published:2015-11-09 summary:Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis andother NLP tasks as features to ma- chine learning algorithms or as entries ofsentiment lexicons. In this paper, we argue that while emoticons are strong andcommon signals of sentiment expression on social media the relationship betweenemoticons and sentiment polarity are not always clear. Thus, any algorithm thatdeals with sentiment polarity should take emoticons into account but extremecau- tion should be exercised in which emoticons to depend on. First, todemonstrate the prevalence of emoticons on social media, we analyzed thefrequency of emoticons in a large re- cent Twitter data set. Then we carriedout four analyses to examine the relationship between emoticons and sentimentpolarity as well as the contexts in which emoticons are used. The firstanalysis surveyed a group of participants for their perceived sentimentpolarity of the most frequent emoticons. The second analysis examinedclustering of words and emoti- cons to better understand the meaning conveyedby the emoti- cons. The third analysis compared the sentiment polarity ofmicroblog posts before and after emoticons were removed from the text. The lastanalysis tested the hypothesis that removing emoticons from text hurtssentiment classification by training two machine learning models with andwithout emoticons in the text respectively. The results confirms the argumentsthat: 1) a few emoticons are strong and reliable signals of sentiment polarityand one should take advantage of them in any senti- ment analysis; 2) a largegroup of the emoticons conveys com- plicated sentiment hence they should betreated with extreme caution.
arxiv-13200-38 | Sandwiching the marginal likelihood using bidirectional Monte Carlo | http://arxiv.org/pdf/1511.02543v1.pdf | author:Roger B. Grosse, Zoubin Ghahramani, Ryan P. Adams category:stat.ML cs.LG stat.CO published:2015-11-08 summary:Computing the marginal likelihood (ML) of a model requires marginalizing outall of the parameters and latent variables, a difficult high-dimensionalsummation or integration problem. To make matters worse, it is often hard tomeasure the accuracy of one's ML estimates. We present bidirectional MonteCarlo, a technique for obtaining accurate log-ML estimates on data simulatedfrom a model. This method obtains stochastic lower bounds on the log-ML usingannealed importance sampling or sequential Monte Carlo, and obtains stochasticupper bounds by running these same algorithms in reverse starting from an exactposterior sample. The true value can be sandwiched between these two stochasticbounds with high probability. Using the ground truth log-ML estimates obtainedfrom our method, we quantitatively evaluate a wide variety of existing MLestimators on several latent variable models: clustering, a low rankapproximation, and a binary attributes model. These experiments yield insightsinto how to accurately estimate marginal likelihoods.
arxiv-13200-39 | Speed learning on the fly | http://arxiv.org/pdf/1511.02540v1.pdf | author:Pierre-Yves MassÃ©, Yann Ollivier category:math.OC cs.LG stat.ML published:2015-11-08 summary:The practical performance of online stochastic gradient descent algorithms ishighly dependent on the chosen step size, which must be tediously hand-tuned inmany applications. The same is true for more advanced variants of stochasticgradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the stepsize by performing a gradient descent on the step size itself, viewing thewhole performance of the learning trajectory as a function of step size.Importantly, this adaptation can be computed online at little cost, withouthaving to iterate backward passes over the full data.
arxiv-13200-40 | Fast Randomized Kernel Methods With Statistical Guarantees | http://arxiv.org/pdf/1411.0306v3.pdf | author:Ahmed El Alaoui, Michael W. Mahoney category:stat.ML cs.LG stat.CO published:2014-11-02 summary:One approach to improving the running time of kernel-based machine learningmethods is to build a small sketch of the input and use it in lieu of the fullkernel matrix in the machine learning task of interest. Here, we describe aversion of this approach that comes with running time guarantees as well asimproved guarantees on its statistical performance. By extending the notion of\emph{statistical leverage scores} to the setting of kernel ridge regression,our main statistical result is to identify an importance sampling distributionthat reduces the size of the sketch (i.e., the required number of columns to besampled) to the \emph{effective dimensionality} of the problem. This quantityis often much smaller than previous bounds that depend on the \emph{maximaldegrees of freedom}. Our main algorithmic result is to present a fast algorithmto compute approximations to these scores. This algorithm runs in time that islinear in the number of samples---more precisely, the running time is$O(np^2)$, where the parameter $p$ depends only on the trace of the kernelmatrix and the regularization parameter---and it can be applied to the matrixof feature vectors, without having to form the full kernel matrix. This isobtained via a variant of length-squared sampling that we adapt to the kernelsetting in a way that is of independent interest. Lastly, we provide empiricalresults illustrating our theory, and we discuss how this new notion of thestatistical leverage of a data point captures in a fine way the difficulty ofthe original statistical learning problem.
arxiv-13200-41 | Algorithmic Stability for Adaptive Data Analysis | http://arxiv.org/pdf/1511.02513v1.pdf | author:Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman category:cs.LG cs.CR cs.DS published:2015-11-08 summary:Adaptivity is an important feature of data analysis---the choice of questionsto ask about a dataset often depends on previous interactions with the samedataset. However, statistical validity is typically studied in a nonadaptivemodel, where all questions are specified before the dataset is drawn. Recentwork by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiatedthe formal study of this problem, and gave the first upper and lower bounds onthe achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution $\mathbf{P}$ and a setof $n$ independent samples $\mathbf{x}$ is drawn from $\mathbf{P}$. We seek analgorithm that, given $\mathbf{x}$ as input, accurately answers a sequence ofadaptively chosen queries about the unknown distribution $\mathbf{P}$. How manysamples $n$ must we draw from the distribution, as a function of the type ofqueries, the number of queries, and the desired level of accuracy? In this work we make two new contributions: (i) We give upper bounds on the number of samples $n$ that are needed toanswer statistical queries. The bounds improve and simplify the work of Dworket al. (STOC, 2015), and have been applied in subsequent work by those authors(Science, 2015, NIPS, 2015). (ii) We prove the first upper bounds on the number of samples required toanswer more general families of queries. These include arbitrarylow-sensitivity queries and an important class of optimization queries. As in Dwork et al., our algorithms are based on a connection with algorithmicstability in the form of differential privacy. We extend their work by giving aquantitatively optimal, more general, and simpler proof of their main theoremthat stability implies low generalization error. We also study weaker stabilityguarantees such as bounded KL divergence and total variation distance.
arxiv-13200-42 | Towards Structured Deep Neural Network for Automatic Speech Recognition | http://arxiv.org/pdf/1511.02506v1.pdf | author:Yi-Hsiu Liao, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG cs.NE published:2015-11-08 summary:In this paper we propose the Structured Deep Neural Network (structured DNN)as a structured and deep learning framework. This approach can learn to findthe best structured object (such as a label sequence) given a structured input(such as a vector sequence) by globally considering the mapping relationshipsbetween the structures rather than item by item. When automatic speech recognition is viewed as a special case of such astructured learning problem, where we have the acoustic vector sequence as theinput and the phoneme label sequence as the output, it becomes possible tocomprehensively learn utterance by utterance as a whole, rather than frame byframe. Structured Support Vector Machine (structured SVM) was proposed to performASR with structured learning previously, but limited by the linear nature ofSVM. Here we propose structured DNN to use nonlinear transformations inmulti-layers as a structured and deep learning approach. This approach wasshown to beat structured SVM in preliminary experiments on TIMIT.
arxiv-13200-43 | Flowing ConvNets for Human Pose Estimation in Videos | http://arxiv.org/pdf/1506.02897v2.pdf | author:Tomas Pfister, James Charles, Andrew Zisserman category:cs.CV published:2015-06-09 summary:The objective of this work is human pose estimation in videos, where multipleframes are available. We investigate a ConvNet architecture that is able tobenefit from temporal context by combining information across the multipleframes using optical flow. To this end we propose a network architecture with the following novelties:(i) a deeper network than previously investigated for regressing heatmaps; (ii)spatial fusion layers that learn an implicit spatial model; (iii) optical flowis used to align heatmap predictions from neighbouring frames; and (iv) a finalparametric pooling layer which learns to combine the aligned heatmaps into apooled confidence map. We show that this architecture outperforms a number of others, including onethat uses optical flow solely at the input layers, one that regresses jointcoordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin onthree video pose estimation datasets, including the very challenging Poses inthe Wild dataset, and outperforms other deep methods that don't use a graphicalmodel on the single-image FLIC benchmark (and also Chen & Yuille and Tompson etal. in the high precision region).
arxiv-13200-44 | On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix Approximation | http://arxiv.org/pdf/1509.09236v2.pdf | author:Nicolas Gillis, Stephen A. Vavasis category:cs.LG cs.CC math.NA math.OC published:2015-09-30 summary:The low-rank matrix approximation problem with respect to the component-wise$\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principalcomponent analysis (PCA), has become a very popular tool in data mining andmachine learning. Robust PCA aims at recovering a low-rank matrix that wasperturbed with sparse noise, with applications for example inforeground-background video separation. Although $\ell_1$-LRA is stronglybelieved to be NP-hard, there is, to the best of our knowledge, no formal proofof this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already inthe rank-one case, using a reduction from MAX CUT. Our derivations drawinteresting connections between $\ell_1$-LRA and several other well-knownproblems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, aparticular densest bipartite subgraph problem, the computation of the cut normof $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove tobe NP-hard.
arxiv-13200-45 | An Egocentric Look at Video Photographer Identity | http://arxiv.org/pdf/1411.7591v3.pdf | author:Yedid Hoshen, Shmuel Peleg category:cs.CV published:2014-11-27 summary:Egocentric cameras are being worn by an increasing number of users, amongthem many security forces worldwide. GoPro cameras already penetrated the massmarket, reporting substantial increase in sales every year. As head-worncameras do not capture the photographer, it may seem that the anonymity of thephotographer is preserved even when the video is publicly distributed. We show that camera motion, as can be computed from the egocentric video,provides unique identity information. The photographer can be reliablyrecognized from a few seconds of video captured when walking. The proposedmethod achieves more than 90% recognition accuracy in cases where the randomsuccess rate is only 3%. Applications can include theft prevention by locking the camera when not wornby its rightful owner. Searching video sharing services (e.g. YouTube) foregocentric videos shot by a specific photographer may also become possible. Animportant message in this paper is that photographers should be aware thatsharing egocentric video will compromise their anonymity, even when their faceis not visible.
arxiv-13200-46 | Poisson Inverse Problems by the Plug-and-Play scheme | http://arxiv.org/pdf/1511.02500v1.pdf | author:Arie Rond, Raja Giryes, Michael Elad category:cs.CV math.OC published:2015-11-08 summary:The Anscombe transform offers an approximate conversion of a Poisson randomvariable into a Gaussian one. This transform is important and appealing, as itis easy to compute, and becomes handy in various inverse problems with Poissonnoise contamination. Solution to such problems can be done by first applyingthe Anscombe transform, then applying a Gaussian-noise-oriented restorationalgorithm of choice, and finally applying an inverse Anscombe transform. Theappeal in this approach is due to the abundance of high-performance restorationalgorithms designed for white additive Gaussian noise (we will refer to thesehereafter as "Gaussian-solvers"). This process is known to work well for highSNR images, where the Anscombe transform provides a rather accurateapproximation. When the noise level is high, the above path loses much of itseffectiveness, and the common practice is to replace it with a direct treatmentof the Poisson distribution. Naturally, with this we lose the ability toleverage on vastly available Gaussian-solvers. In this work we suggest a novel method for coupling Gaussian denoisingalgorithms to Poisson noisy inverse problems, which is based on a generalapproach termed "Plug-and-Play". Deploying the Plug-and-Play approach to suchproblems leads to an iterative scheme that repeats several key steps: 1) Aconvex programming task of simple form that can be easily treated; 2) Apowerful Gaussian denoising algorithm of choice; and 3) A simple update step. Such a modular method, just like the Anscombe transform, enables otherdevelopers to plug their own Gaussian denoising algorithms to our scheme in aneasy way. While the proposed method bares some similarity to the Anscombeoperation, it is in fact based on a different mathematical basis, which holdstrue for all SNR ranges.
arxiv-13200-47 | VideoStory Embeddings Recognize Events when Examples are Scarce | http://arxiv.org/pdf/1511.02492v1.pdf | author:Amirhossein Habibian, Thomas Mensink, Cees G. M. Snoek category:cs.CV cs.MM published:2015-11-08 summary:This paper aims for event recognition when video examples are scarce or evencompletely absent. The key in such a challenging setting is a semantic videorepresentation. Rather than building the representation from individualattribute detectors and their annotations, we propose to learn the entirerepresentation from freely available web videos and their descriptions using anembedding between video features and term vectors. In our proposed embedding,which we call VideoStory, the correlations between the terms are utilized tolearn a more effective representation by optimizing a joint objective balancingdescriptiveness and predictability.We show how learning the VideoStory using amultimodal predictability loss, including appearance, motion and audiofeatures, results in a better predictable representation. We also propose avariant of VideoStory to recognize an event in video from just the importantterms in a text query by introducing a term sensitive descriptiveness loss. Ourexperiments on three challenging collections of web videos from the NISTTRECVID Multimedia Event Detection and Columbia Consumer Videos datasetsdemonstrate: i) the advantages of VideoStory over representations usingattributes or alternative embeddings, ii) the benefit of fusing videomodalities by an embedding over common strategies, iii) the complementarity ofterm sensitive descriptiveness and multimodal predictability for eventrecognition without examples. By it abilities to improve predictability uponany underlying video feature while at the same time maximizing semanticdescriptiveness, VideoStory leads to state-of-the-art accuracy for both few-and zero-example recognition of events in video.
arxiv-13200-48 | A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model | http://arxiv.org/pdf/1511.02465v1.pdf | author:Jie Xu, Lianwen Jin, Lingyu Liang, Ziyong Feng, Duorui Xie category:cs.CV published:2015-11-08 summary:This paper proposes a deep leaning method to address the challenging facialattractiveness prediction problem. The method constructs a convolutional neuralnetwork of facial beauty prediction using a new deep cascaded fine-turningscheme with various face inputting channels, such as the original RGB faceimage, the detail layer image, and the lighting layer image. With a carefullydesigned CNN model of deep structure, large input size and small convolutionalkernels, we have achieved a high prediction correlation of 0.88. This resultconvinces us that the problem of facial attractiveness prediction can be solvedby deep learning approach, and it also shows the important roles of the facialsmoothness, lightness, and color information that were involved in facialbeauty perception, which is consistent with the result of recent psychologystudies. Furthermore, we analyze the high-level features learnt by CNN throughvisualization of its hidden layers, and some interesting phenomena wereobserved. It is found that the contours and appearance of facial features,especially eyes and moth, are the most significant facial attributes for facialattractiveness prediction, which is also consistent with the visual perceptionintuition of human.
arxiv-13200-49 | SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception | http://arxiv.org/pdf/1511.02459v1.pdf | author:Duorui Xie, Lingyu Liang, Lianwen Jin, Jie Xu, Mengru Li category:cs.CV published:2015-11-08 summary:In this paper, a novel face dataset with attractiveness ratings, namely, theSCUT-FBP dataset, is developed for automatic facial beauty perception. Thisdataset provides a benchmark to evaluate the performance of different methodsfor facial attractiveness prediction, including the state-of-the-art deeplearning method. The SCUT-FBP dataset contains face portraits of 500 Asianfemale subjects with attractiveness ratings, all of which have been verified interms of rating distribution, standard deviation, consistency, andself-consistency. Benchmark evaluations for facial attractiveness predictionwere performed with different combinations of facial geometrical features andtexture features using classical statistical learning methods and the deeplearning method. The best Pearson correlation (0.8187) was achieved by the CNNmodel. Thus, the results of our experiments indicate that the SCUT-FBP datasetprovides a reliable benchmark for facial beauty perception.
arxiv-13200-50 | Learn on Source, Refine on Target:A Model Transfer Learning Framework with Random Forests | http://arxiv.org/pdf/1511.01258v2.pdf | author:Noam Segev, Maayan Harel, Shie Mannor, Koby Crammer, Ran El-Yaniv category:cs.LG published:2015-11-04 summary:We propose novel model transfer-learning methods that refine a decisionforest model M learned within a "source" domain using a training set sampledfrom a "target" domain, assumed to be a variation of the source. We present tworandom forest transfer algorithms. The first algorithm searches greedily forlocally optimal modifications of each tree structure by trying to locallyexpand or reduce the tree around individual nodes. The second algorithm doesnot modify structure, but only the parameter (thresholds) associated withdecision nodes. We also propose to combine both methods by considering anensemble that contains the union of the two forests. The proposed methodsexhibit impressive experimental results over a range of problems.
arxiv-13200-51 | A Chinese POS Decision Method Using Korean Translation Information | http://arxiv.org/pdf/1511.02435v1.pdf | author:Son-Il Kwak, O-Chol Kown, Chang-Sin Kim, Yong-Il Pak, Gum-Chol Son, Chol-Jun Hwang, Hyon-Chol Kim, Hyok-Chol Sin, Gyong-Il Hyon, Sok-Min Han category:cs.CL published:2015-11-08 summary:In this paper we propose a method that imitates a translation expert usingthe Korean translation information and analyse the performance. Korean is goodat tagging than Chinese, so we can use this property in Chinese POS tagging.
arxiv-13200-52 | Max-Sum Diversification, Monotone Submodular Functions and Semi-metric Spaces | http://arxiv.org/pdf/1511.02402v1.pdf | author:Sepehr Abbasi Zadeh, Mehrdad Ghadiri category:cs.LG published:2015-11-07 summary:In many applications such as web-based search, document summarization,facility location and other applications, the results are preferable to be bothrepresentative and diversified subsets of documents. The goal of this study isto select a good "quality", bounded-size subset of a given set of items, whilemaintaining their diversity relative to a semi-metric distance function. Thisproblem was first studied by Borodin et al\cite{borodin}, but a crucialproperty used throughout their proof is the triangle inequality. In thismodified proof, we want to relax the triangle inequality and relate theapproximation ratio of max-sum diversification problem to the parameter of therelaxed triangle inequality in the normal form of the problem (i.e., a uniformmatroid) and also in an arbitrary matroid.
arxiv-13200-53 | Hierarchical Variational Models | http://arxiv.org/pdf/1511.02386v1.pdf | author:Rajesh Ranganath, Dustin Tran, David M. Blei category:stat.ML cs.LG stat.CO stat.ME published:2015-11-07 summary:Black box inference allows researchers to easily prototype and evaluate anarray of models. Recent advances in variational inference allow such algorithmsto scale to high dimensions. However, a central question remains: How tospecify an expressive variational distribution which maintains efficientcomputation? To address this, we develop hierarchical variational models. In ahierarchical variational model, the variational approximation is augmented witha prior on its parameters, such that the latent variables are conditionallyindependent given this shared structure. This preserves the computationalefficiency of the original approximation, while admitting hierarchicallycomplex distributions for both discrete and continuous latent variables. Westudy hierarchical variational models on a variety of deep discrete latentvariable models. Hierarchical variational models generalize other expressivevariational distributions and maintains higher fidelity to the posterior.
arxiv-13200-54 | Symmetry-invariant optimization in deep networks | http://arxiv.org/pdf/1511.01754v2.pdf | author:Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla category:cs.LG cs.AI cs.CV published:2015-11-05 summary:Recent works have highlighted scale invariance or symmetry that is present inthe weight space of a typical deep network and the adverse effect that it hason the Euclidean gradient based stochastic gradient descent optimization. Inthis work, we show that these and other commonly used deep networks, such asthose which use a max-pooling and sub-sampling layer, possess more complexforms of symmetry arising from scaling based reparameterization of the networkweights. We then propose two symmetry-invariant gradient based weight updatesfor stochastic gradient descent based learning. Our empirical evidence based onthe MNIST dataset shows that these updates improve the test performance withoutsacrificing the computational efficiency of the weight updates. We also showthe results of training with one of the proposed weight updates on an imagesegmentation problem.
arxiv-13200-55 | Review-Level Sentiment Classification with Sentence-Level Polarity Correction | http://arxiv.org/pdf/1511.02385v1.pdf | author:Sylvester Olubolu Orimaye, Saadat M. Alhashmi, Eu-Gene Siew, Sang Jung Kang category:cs.CL cs.AI cs.LG published:2015-11-07 summary:We propose an effective technique to solving review-level sentimentclassification problem by using sentence-level polarity correction. Ourpolarity correction technique takes into account the consistency of thepolarities (positive and negative) of sentences within each product reviewbefore performing the actual machine learning task. While sentences withinconsistent polarities are removed, sentences with consistent polarities areused to learn state-of-the-art classifiers. The technique achieved betterresults on different types of products reviews and outperforms baseline modelswithout the correction technique. Experimental results show an average of 82%F-measure on four different product review domains.
arxiv-13200-56 | Review of Person Re-identification Techniques | http://arxiv.org/pdf/1511.02319v1.pdf | author:Mohammad Ali Saghafi, Aini Hussain, Halimah Badioze Zaman, Mohamad Hanif Md Saad category:cs.CV published:2015-11-07 summary:Person re-identification across different surveillance cameras with disjointfields of view has become one of the most interesting and challenging subjectsin the area of intelligent video surveillance. Although several methods havebeen developed and proposed, certain limitations and unresolved issues remain.In all of the existing re-identification approaches, feature vectors areextracted from segmented still images or video frames. Different similarity ordissimilarity measures have been applied to these vectors. Some methods haveused simple constant metrics, whereas others have utilised models to obtainoptimised metrics. Some have created models based on local colour or textureinformation, and others have built models based on the gait of people. Ingeneral, the main objective of all these approaches is to achieve ahigher-accuracy rate and lowercomputational costs. This study summarisesseveral developments in recent literature and discusses the various availablemethods used in person re-identification. Specifically, their advantages anddisadvantages are mentioned and compared.
arxiv-13200-57 | A Sparse and Adaptive Prior for Time-Dependent Model Parameters | http://arxiv.org/pdf/1310.2627v2.pdf | author:Dani Yogatama, Bryan R. Routledge, Noah A. Smith category:stat.ML cs.AI cs.LG published:2013-10-09 summary:We consider the scenario where the parameters of a probabilistic model areexpected to vary over time. We construct a novel prior distribution thatpromotes sparsity and adapts the strength of correlation between parameters atsuccessive timesteps, based on the data. We derive approximate variationalinference procedures for learning and prediction with this prior. We test theapproach on two tasks: forecasting financial quantities from relevant text, andmodeling language contingent on time-varying financial measurements.
arxiv-13200-58 | Fingertip in the Eye: A cascaded CNN pipeline for the real-time fingertip detection in egocentric videos | http://arxiv.org/pdf/1511.02282v1.pdf | author:Xiaorui Liu, Yichao Huang, Xin Zhang, Lianwen Jin category:cs.CV published:2015-11-07 summary:We introduce a new pipeline for hand localization and fingertip detection.For RGB images captured from an egocentric vision mobile camera, hand andfingertip detection remains a challenging problem due to factors likebackground complexity and hand shape variety. To address these issuesaccurately and robustly, we build a large scale dataset named Ego-Fingertip andpropose a bi-level cascaded pipeline of convolutional neural networks, namely,Attention-based Hand Detector as well as Multi-point Fingertip Detector. Theproposed method significantly tackles challenges and achieves satisfactorilyaccurate prediction and real-time performance compared to previous hand andfingertip detection methods.
arxiv-13200-59 | Signed Support Recovery for Single Index Models in High-Dimensions | http://arxiv.org/pdf/1511.02270v1.pdf | author:Matey Neykov, Qian Lin, Jun S. Liu category:math.ST stat.ML stat.TH published:2015-11-07 summary:In this paper we study the support recovery problem for single index models$Y=f(\boldsymbol{X}^{\intercal} \boldsymbol{\beta},\varepsilon)$, where $f$ isan unknown link function, $\boldsymbol{X}\sim N_p(0,\mathbb{I}_{p})$ and$\boldsymbol{\beta}$ is an $s$-sparse unit vector such that$\boldsymbol{\beta}_{i}\in \{\pm\frac{1}{\sqrt{s}},0\}$. In particular, we lookinto the performance of two computationally inexpensive algorithms: (a) thediagonal thresholding sliced inverse regression (DT-SIR) introduced by Lin etal. (2015); and (b) a semi-definite programming (SDP) approach inspired byAmini & Wainwright (2008). When $s=O(p^{1-\delta})$ for some $\delta>0$, wedemonstrate that both procedures can succeed in recovering the support of$\boldsymbol{\beta}$ as long as the rescaled sample size$\kappa=\frac{n}{s\log(p-s)}$ is larger than a certain critical threshold. Onthe other hand, when $\kappa$ is smaller than a critical value, any algorithmfails to recover the support with probability at least $\frac{1}{2}$asymptotically. In other words, we demonstrate that both DT-SIR and the SDPapproach are optimal (up to a scalar) for recovering the support of$\boldsymbol{\beta}$ in terms of sample size.
arxiv-13200-60 | Bayesian Dark Knowledge | http://arxiv.org/pdf/1506.04416v3.pdf | author:Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling category:cs.LG stat.ML published:2015-06-14 summary:We consider the problem of Bayesian parameter estimation for deep neuralnetworks, which is important in problem settings where we may have little data,and/ or where we need accurate posterior predictive densities, e.g., forapplications involving bandits or active learning. One simple approach to thisis to use online Monte Carlo methods, such as SGLD (stochastic gradientLangevin dynamics). Unfortunately, such a method needs to store many copies ofthe parameters (which wastes memory), and needs to make predictions using manyversions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to theposterior predictive density into a more compact form, namely a single deepneural network. We compare to two very recent approaches to Bayesian neuralnetworks, namely an approach based on expectation propagation [Hernandez-Lobatoand Adams, 2015] and an approach based on variational Bayes [Blundell et al.,2015]. Our method performs better than both of these, is much simpler toimplement, and uses less computation at test time.
arxiv-13200-61 | Active Perceptual Similarity Modeling with Auxiliary Information | http://arxiv.org/pdf/1511.02254v1.pdf | author:Eric Heim, Matthew Berger, Lee Seversky, Milos Hauskrecht category:cs.LG stat.ML published:2015-11-06 summary:Learning a model of perceptual similarity from a collection of objects is afundamental task in machine learning underlying numerous applications. A commonway to learn such a model is from relative comparisons in the form of triplets:responses to queries of the form "Is object a more similar to b than it is toc?". If no consideration is made in the determination of which queries to ask,existing similarity learning methods can require a prohibitively large numberof responses. In this work, we consider the problem of actively learning fromtriplets -finding which queries are most useful for learning. Different fromprevious active triplet learning approaches, we incorporate auxiliaryinformation into our similarity model and introduce an active learning schemeto find queries that are informative for quickly learning both the relevantaspects of auxiliary data and the directly-learned similarity components.Compared to prior approaches, we show that we can learn just as effectivelywith much fewer queries. For evaluation, we introduce a new dataset ofexhaustive triplet comparisons obtained from humans and demonstrate improvedperformance for different types of auxiliary information.
arxiv-13200-62 | Learning Visual Features from Large Weakly Supervised Data | http://arxiv.org/pdf/1511.02251v1.pdf | author:Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache category:cs.CV published:2015-11-06 summary:Convolutional networks trained on large supervised dataset produce visualfeatures which form the basis for the state-of-the-art in many computer-visionproblems. Further improvements of these visual features will likely requireeven larger manually labeled data sets, which severely limits the pace at whichprogress can be made. In this paper, we explore the potential of leveragingmassive, weakly-labeled image collections for learning good visual features. Wetrain convolutional networks on a dataset of 100 million Flickr photos andcaptions, and show that these networks produce features that perform well in arange of vision problems. We also show that the networks appropriately captureword similarity, and learn correspondences between different languages.
arxiv-13200-63 | Seven ways to improve example-based single image super resolution | http://arxiv.org/pdf/1511.02228v1.pdf | author:Radu Timofte, Rasmus Rothe, Luc Van Gool category:cs.CV published:2015-11-06 summary:In this paper we present seven techniques that everybody should know toimprove example-based single image super resolution (SR): 1) augmentation ofdata, 2) use of large dictionaries with efficient search structures, 3)cascading, 4) image self-similarities, 5) back projection refinement, 6)enhanced prediction by consistency check, and 7) context reasoning. We validateour seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) andmethods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantialimprovements.The techniques are widely applicable and require no changes oronly minor adjustments of the SR methods. Moreover, our Improved A+ (IA) methodsets new state-of-the-art results outperforming A+ by up to 0.9dB on averagePSNR whilst maintaining a low time complexity.
arxiv-13200-64 | Deep Kernel Learning | http://arxiv.org/pdf/1511.02222v1.pdf | author:Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing category:cs.LG cs.AI stat.ME stat.ML published:2015-11-06 summary:We introduce scalable deep kernels, which combine the structural propertiesof deep learning architectures with the non-parametric flexibility of kernelmethods. Specifically, we transform the inputs of a spectral mixture basekernel with a deep architecture, using local kernel interpolation, inducingpoints, and structure exploiting (Kronecker and Toeplitz) algebra for ascalable kernel representation. These closed-form kernels can be used asdrop-in replacements for standard kernels, with benefits in expressive powerand scalability. We jointly learn the properties of these kernels through themarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$for $n$ training points, and predictions cost $O(1)$ per test point. On a largeand diverse collection of applications, including a dataset with 2 millionexamples, we show improved performance over scalable Gaussian processes withflexible kernel learning models, and stand-alone deep architectures.
arxiv-13200-65 | A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas | http://arxiv.org/pdf/1510.04781v2.pdf | author:Haohan Wang, Bhiksha Raj category:cs.LG cs.NE published:2015-10-16 summary:This report will show the history of deep learning evolves. It will traceback as far as the initial belief of connectionism modelling of brain, and comeback to look at its early stage realization: neural networks. With thebackground of neural network, we will gradually introduce how convolutionalneural network, as a representative of deep discriminative models, is developedfrom neural networks, together with many practical techniques that can help inoptimization of neural networks. On the other hand, we will also trace back tosee the evolution history of deep generative models, to see how researchersbalance the representation power and computation complexity to reach RestrictedBoltzmann Machine and eventually reach Deep Belief Nets. Further, we will alsolook into the development history of modelling time series data with neuralnetworks. We start with Time Delay Neural Networks and move further tocurrently famous model named Recurrent Neural Network and its extension LongShort Term Memory. We will also briefly look into how to construct deeprecurrent neural networks. Finally, we will conclude this report with someinteresting open-ended questions of deep neural networks.
arxiv-13200-66 | An Extended Frank-Wolfe Method with "In-Face" Directions, and its Application to Low-Rank Matrix Completion | http://arxiv.org/pdf/1511.02204v1.pdf | author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:math.OC stat.CO stat.ML 90C25 G.1.6 published:2015-11-06 summary:Motivated principally by the low-rank matrix completion problem, we presentan extension of the Frank-Wolfe method that is designed to induce near-optimalsolutions on low-dimensional faces of the feasible region. This is accomplishedby a new approach to generating ``in-face" directions at each iteration, aswell as through new choice rules for selecting between in-face and ``regular"Frank-Wolfe steps. Our framework for generating in-face directions generalizesthe notion of away-steps introduced by Wolfe. In particular, the in-facedirections always keep the next iterate within the minimal face containing thecurrent iterate. We present computational guarantees for the new method thattrade off efficiency in computing near-optimal solutions with upper bounds onthe dimension of minimal faces of iterates. We apply the new method to thematrix completion problem, where low-dimensional faces correspond to low-rankmatrices. We present computational results that demonstrate the effectivenessof our methodological approach at producing nearly-optimal solutions of verylow rank. On both artificial and real datasets, we demonstrate significantspeed-ups in computing very low-rank nearly-optimal solutions as compared toeither the Frank-Wolfe method or its traditional away-step variant.
arxiv-13200-67 | Evaluating Protein-protein Interaction Predictors with a Novel 3-Dimensional Metric | http://arxiv.org/pdf/1511.02196v1.pdf | author:Haohan Wang, Madhavi K. Ganapathiraju category:cs.LG published:2015-11-06 summary:In order for the predicted interactions to be directly adopted by biologists,the ma- chine learning predictions have to be of high precision, regardless ofrecall. This aspect cannot be evaluated or numerically represented well bytraditional metrics like accuracy, ROC, or precision-recall curve. In thiswork, we start from the alignment in sensitivity of ROC and recall ofprecision-recall curve, and propose an evaluation metric focusing on theability of a model to be adopted by biologists. This metric evaluates theability of a machine learning algorithm to predict only new interactions,meanwhile, it eliminates the influence of test dataset. In the experiment ofevaluating different classifiers with a same data set and evaluating the samepredictor with different datasets, our new metric fulfills the evaluation taskof our interest while two widely recognized metrics, ROC and precision-recallcurve fail the tasks for different reasons.
arxiv-13200-68 | Optimal Non-Asymptotic Lower Bound on the Minimax Regret of Learning with Expert Advice | http://arxiv.org/pdf/1511.02176v1.pdf | author:Francesco Orabona, David Pal category:stat.ML cs.LG published:2015-11-06 summary:We prove non-asymptotic lower bounds on the expectation of the maximum of $d$independent Gaussian variables and the expectation of the maximum of $d$independent symmetric random walks. Both lower bounds recover the optimalleading constant in the limit. A simple application of the lower bound forrandom walks is an (asymptotically optimal) non-asymptotic lower bound on theminimax regret of online learning with expert advice.
arxiv-13200-69 | Pooling the Convolutional Layers in Deep ConvNets for Action Recognition | http://arxiv.org/pdf/1511.02126v1.pdf | author:Shichao Zhao, Yanbin Liu, Yahong Han, Richang Hong category:cs.CV published:2015-11-06 summary:Deep ConvNets have shown its good performance in image classification tasks.However it still remains as a problem in deep video representation for actionrecognition. The problem comes from two aspects: on one hand, current videoConvNets are relatively shallow compared with image ConvNets, which limits itscapability of capturing the complex video action information; on the otherhand, temporal information of videos is not properly utilized to pool andencode the video sequences. Towards these issues, in this paper, we utilize twostate-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and thetemporal net from Two-Stream ConvNets, for action representation. Theconvolutional layers and the proposed new layer, called frame-diff layer, areextracted and pooled with two temporal pooling strategy: Trajectory pooling andline pooling. The pooled local descriptors are then encoded with VLAD to formthe video representations. In order to verify the effectiveness of the proposedframework, we conduct experiments on UCF101 and HMDB51 datasets. It achievesthe accuracy of 93.78\% on UCF101 which is the state-of-the-art and theaccuracy of 65.62\% on HMDB51 which is comparable to the state-of-the-art.
arxiv-13200-70 | Introducing SKYSET - a Quintuple Approach for Improving Instructions | http://arxiv.org/pdf/1511.02117v1.pdf | author:Kerry Fultz, Seth Filip category:cs.CL published:2015-11-06 summary:A new approach called SKYSET (Synthetic Knowledge Yield Social EntitiesTranslation) is proposed to validate completeness and to reduce ambiguity fromwritten instructional documentation. SKYSET utilizes a quintuple set ofstandardized categories, which differs from traditional approaches thattypically use triples. The SKYSET System defines the categories required toform a standard template for representing information that is portable acrossdifferent domains. It provides a standardized framework that enables sentencesfrom written instructions to be translated into sets of category typed entitieson a table or database. The SKYSET entities contain conceptual units or phrasesthat represent information from the original source documentation. SKYSETenables information concatenation where multiple documents from differentdomains can be translated and combined into a single common filterable andsearchable table of entities.
arxiv-13200-71 | Hierarchical Coupled Geometry Analysis for Neuronal Structure and Activity Pattern Discovery | http://arxiv.org/pdf/1511.02086v1.pdf | author:Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Uri Dubin, Ronald R. Coifman category:q-bio.QM q-bio.NC stat.ML published:2015-11-06 summary:In the wake of recent advances in experimental methods in neuroscience, theability to record in-vivo neuronal activity from awake animals has becomefeasible. The availability of such rich and detailed physiological measurementscalls for the development of advanced data analysis tools, as commonly usedtechniques do not suffice to capture the spatio-temporal network complexity. Inthis paper, we propose a new hierarchical coupled geometry analysis, whichexploits the hidden connectivity structures between neurons and the dynamicpatterns at multiple time-scales. Our approach gives rise to the jointorganization of neurons and dynamic patterns in data-driven hierarchical datastructures. These structures provide local to global data representations, fromlocal partitioning of the data in flexible trees through a new multiscalemetric to a global manifold embedding. The application of our techniques toin-vivo neuronal recordings demonstrate the capability of extracting neuronalactivity patterns and identifying temporal trends, associated with particularbehavioral events and manipulations introduced in the experiments.
arxiv-13200-72 | Texture Synthesis Using Convolutional Neural Networks | http://arxiv.org/pdf/1505.07376v3.pdf | author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV cs.NE q-bio.NC published:2015-05-27 summary:Here we introduce a new model of natural textures based on the feature spacesof convolutional neural networks optimised for object recognition. Samples fromthe model are of high perceptual quality demonstrating the generative power ofneural networks trained in a purely discriminative fashion. Within the model,textures are represented by the correlations between feature maps in severallayers of the network. We show that across layers the texture representationsincreasingly capture the statistical properties of natural images while makingobject information more and more explicit. The model provides a new tool togenerate stimuli for neuroscience and might offer insights into the deeprepresentations learned by convolutional neural networks.
arxiv-13200-73 | ALOJA: A Framework for Benchmarking and Predictive Analytics in Big Data Deployments | http://arxiv.org/pdf/1511.02037v1.pdf | author:Josep Ll. Berral, Nicolas Poggi, David Carrera, Aaron Call, Rob Reinauer, Daron Green category:cs.LG cs.DC C.4; I.2.6 published:2015-11-06 summary:This article presents the ALOJA project and its analytics tools, whichleverages machine learning to interpret Big Data benchmark performance data andtuning. ALOJA is part of a long-term collaboration between BSC and Microsoft toautomate the characterization of cost-effectiveness on Big Data deployments,currently focusing on Hadoop. Hadoop presents a complex run-time environment,where costs and performance depend on a large number of configuration choices.The ALOJA project has created an open, vendor-neutral repository, featuringover 40,000 Hadoop job executions and their performance details. The repositoryis accompanied by a test-bed and tools to deploy and evaluate thecost-effectiveness of different hardware configurations, parameters and Cloudservices. Despite early success within ALOJA, a comprehensive study requiresautomation of modeling procedures to allow an analysis of large andresource-constrained search spaces. The predictive analytics extension,ALOJA-ML, provides an automated system allowing knowledge discovery by modelingenvironments from observed executions. The resulting models can forecastexecution behaviors, predicting execution times for new configurations andhardware choices. That also enables model-based anomaly detection or efficientbenchmark guidance by prioritizing executions. In addition, the community canbenefit from ALOJA data-sets and framework to improve the design and deploymentof Big Data applications.
arxiv-13200-74 | ALOJA-ML: A Framework for Automating Characterization and Knowledge Discovery in Hadoop Deployments | http://arxiv.org/pdf/1511.02030v1.pdf | author:Josep Ll. Berral, Nicolas Poggi, David Carrera, Aaron Call, Rob Reinauer, Daron Green category:cs.LG cs.DC C.4; I.2.6 published:2015-11-06 summary:This article presents ALOJA-Machine Learning (ALOJA-ML) an extension to theALOJA project that uses machine learning techniques to interpret Hadoopbenchmark performance data and performance tuning; here we detail the approach,efficacy of the model and initial results. Hadoop presents a complex executionenvironment, where costs and performance depends on a large number of software(SW) configurations and on multiple hardware (HW) deployment choices. Theseresults are accompanied by a test bed and tools to deploy and evaluate thecost-effectiveness of the different hardware configurations, parameter tunings,and Cloud services. Despite early success within ALOJA from expert-guidedbenchmarking, it became clear that a genuinely comprehensive study requiresautomation of modeling procedures to allow a systematic analysis of large andresource-constrained search spaces. ALOJA-ML provides such an automated systemallowing knowledge discovery by modeling Hadoop executions from observedbenchmarks across a broad set of configuration parameters. The resultingperformance models can be used to forecast execution behavior of variousworkloads; they allow 'a-priori' prediction of the execution times for newconfigurations and HW choices and they offer a route to model-based anomalydetection. In addition, these models can guide the benchmarking explorationefficiently, by automatically prioritizing candidate future benchmark tests.Insights from ALOJA-ML's models can be used to reduce the operational time onclusters, speed-up the data acquisition and knowledge discovery process, andimportantly, reduce running costs. In addition to learning from the methodologypresented in this work, the community can benefit in general from ALOJAdata-sets, framework, and derived insights to improve the design and deploymentof Big Data applications.
arxiv-13200-75 | Towards a Better Understanding of Predict and Count Models | http://arxiv.org/pdf/1511.02024v1.pdf | author:S. Sathiya Keerthi, Tobias Schnabel, Rajiv Khanna category:cs.LG cs.CL published:2015-11-06 summary:In a recent paper, Levy and Goldberg pointed out an interesting connectionbetween prediction-based word embedding models and count models based onpointwise mutual information. Under certain conditions, they showed that bothmodels end up optimizing equivalent objective functions. This paper exploresthis connection in more detail and lays out the factors leading to differencesbetween these models. We find that the most relevant differences from anoptimization perspective are (i) predict models work in a low dimensional spacewhere embedding vectors can interact heavily; (ii) since predict models havefewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can beregularized in a principled manner and provide closed-form solutions for L1 andL2 regularization. Finally, we propose a new embedding model with a convexobjective and the additional benefit of being intelligible.
arxiv-13200-76 | Facial Expression Recognition Using Sparse Gaussian Conditional Random Field | http://arxiv.org/pdf/1511.02023v1.pdf | author:Mohammadamin Abbasnejad, Mohammad Ali Masnadi-Shirazi category:cs.CV published:2015-11-06 summary:The analysis of expression and facial Action Units (AUs) detection are veryimportant tasks in fields of computer vision and Human Computer Interaction(HCI) due to the wide range of applications in human life. Many works has beendone during the past few years which has their own advantages anddisadvantages. In this work we present a new model based on GaussianConditional Random Field. We solve our objective problem using ADMM and we showhow well the proposed model works. We train and test our work on two facialexpression datasets, CK+ and RU-FACS. Experimental evaluation shows that ourproposed approach outperform state of the art expression recognition.
arxiv-13200-77 | Next Generation Multicuts for Semi-Planar Graphs | http://arxiv.org/pdf/1511.01994v1.pdf | author:Julian Yarkony category:cs.CV cs.DS published:2015-11-06 summary:We study the problem of multicut segmentation. We introduce modified versionsof the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our workto natural image segmentation.
arxiv-13200-78 | Neutralized Empirical Risk Minimization with Generalization Neutrality Bound | http://arxiv.org/pdf/1511.01987v1.pdf | author:Kazuto Fukuchi, Jun Sakuma category:stat.ML published:2015-11-06 summary:Currently, machine learning plays an important role in the lives andindividual activities of numerous people. Accordingly, it has become necessaryto design machine learning algorithms to ensure that discrimination, biasedviews, or unfair treatment do not result from decision making or predictionsmade via machine learning. In this work, we introduce a novel empirical riskminimization (ERM) framework for supervised learning, neutralized ERM (NERM)that ensures that any classifiers obtained can be guaranteed to be neutral withrespect to a viewpoint hypothesis. More specifically, given a viewpointhypothesis, NERM works to find a target hypothesis that minimizes the empiricalrisk while simultaneously identifying a target hypothesis that is neutral tothe viewpoint hypothesis. Within the NERM framework, we derive a theoreticalbound on empirical and generalization neutrality risks. Furthermore, as arealization of NERM with linear classification, we derive a max-marginalgorithm, neutral support vector machine (SVM). Experimental results show thatour neutral SVM shows improved classification performance in real datasetswithout sacrificing the neutrality guarantee.
arxiv-13200-79 | Multi-lingual Geoparsing based on Machine Translation | http://arxiv.org/pdf/1511.01974v1.pdf | author:Xu Chen, Han Zhang, Judith Gelernter category:cs.CL cs.IR published:2015-11-06 summary:Our method for multi-lingual geoparsing uses monolingual tools and resourcesalong with machine translation and alignment to return location words in manylanguages. Not only does our method save the time and cost of developinggeoparsers for each language separately, but also it allows the possibility ofa wide range of language capabilities within a single interface. We evaluatedour method in our LanguageBridge prototype on location named entities usingnewswire, broadcast news and telephone conversations in English, Arabic andChinese data from the Linguistic Data Consortium (LDC). Our results forgeoparsing Chinese and Arabic text using our multi-lingual geoparsing methodare comparable to our results for geoparsing English text with our Englishtools. Furthermore, experiments using our machine translation approach resultsin accuracy comparable to results from the same data that was translatedmanually.
arxiv-13200-80 | Combinatorial Bandits Revisited | http://arxiv.org/pdf/1502.03475v3.pdf | author:Richard Combes, M. Sadegh Talebi, Alexandre Proutiere, Marc Lelarge category:cs.LG math.OC stat.ML published:2015-02-11 summary:This paper investigates stochastic and adversarial combinatorial multi-armedbandit problems. In the stochastic setting under semi-bandit feedback, wederive a problem-specific regret lower bound, and discuss its scaling with thedimension of the decision space. We propose ESCB, an algorithm that efficientlyexploits the structure of the problem and provide a finite-time analysis of itsregret. ESCB has better performance guarantees than existing algorithms, andsignificantly outperforms these algorithms in practice. In the adversarialsetting under bandit feedback, we propose \textsc{CombEXP}, an algorithm withthe same regret scaling as state-of-the-art algorithms, but with lowercomputational complexity for some combinatorial problems.
arxiv-13200-81 | Recovering hard-to-find object instances by sampling context-based object proposals | http://arxiv.org/pdf/1511.01954v1.pdf | author:Jose Oramas M., Tinne Tuytelaars category:cs.CV published:2015-11-05 summary:In this paper we focus on improving object detection performance in terms ofrecall. We propose a post-detection stage during which we explore the imagewith the objective of recovering missed detections. This exploration isperformed by sampling object proposals in the image. We analyse four differentstrategies to perform this sampling, giving special attention to strategiesthat exploit spatial relations between objects. In addition, we propose a novelmethod to discover higher-order relations between groups of objects.Experiments on the challenging KITTI dataset show that our proposedrelations-based proposal generation strategies can help improving recall at thecost of a relatively low amount of object proposals.
arxiv-13200-82 | Stop Wasting My Gradients: Practical SVRG | http://arxiv.org/pdf/1511.01942v1.pdf | author:Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub KoneÄnÃ½, Scott Sallinen category:cs.LG math.OC stat.CO stat.ML published:2015-11-05 summary:We present and analyze several strategies for improving the performance ofstochastic variance-reduced gradient (SVRG) methods. We first show that theconvergence rate of these methods can be preserved under a decreasing sequenceof errors in the control variate, and use this to derive variants of SVRG thatuse growing-batch strategies to reduce the number of gradient calculationsrequired in the early iterations. We further (i) show how to exploit supportvectors to reduce the number of gradient computations in the later iterations,(ii) prove that the commonly-used regularized SVRG iteration is justified andimproves the convergence rate, (iii) consider alternate mini-batch selectionstrategies, and (iv) consider the generalization error of the method.
arxiv-13200-83 | Rethinking LDA: moment matching for discrete ICA | http://arxiv.org/pdf/1507.01784v2.pdf | author:Anastasia Podosinnikova, Francis Bach, Simon Lacoste-Julien category:stat.ML cs.LG published:2015-07-07 summary:We consider moment matching techniques for estimation in Latent DirichletAllocation (LDA). By drawing explicit links between LDA and discrete versionsof independent component analysis (ICA), we first derive a new set ofcumulant-based tensors, with an improved sample complexity. Moreover, we reusestandard ICA techniques such as joint diagonalization of tensors to improveover existing methods based on the tensor power method. In an extensive set ofexperiments on both synthetic and real datasets, we show that our newcombination of tensors and orthogonal joint diagonalization techniquesoutperforms existing moment matching methods.
arxiv-13200-84 | Thoughts on Massively Scalable Gaussian Processes | http://arxiv.org/pdf/1511.01870v1.pdf | author:Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch category:cs.LG cs.AI stat.ME stat.ML published:2015-11-05 summary:We introduce a framework and early results for massively scalable Gaussianprocesses (MSGP), significantly extending the KISS-GP approach of Wilson andNickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)on billions of datapoints, without requiring distributed inference, or severeassumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GPlearning and inference to $O(n)$, and the standard $O(n^2)$ complexity per testpoint prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices asKronecker products of Toeplitz matrices approximated by circulant matrices.This multi-level circulant approximation allows one to unify the orthogonalcomputational benefits of fast Kronecker and Toeplitz approaches, and issignificantly faster than either approach in isolation; 2) local kernelinterpolation and inducing points to allow for arbitrarily located data inputs,and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-blockstructure (BTTB), which enables fast inference and learning whenmultidimensional Kronecker structure is not present; and 4) projections of theinput space to flexibly model correlated inputs and high dimensional data. Theability to handle many ($m \approx n$) inducing points allows for near-exactaccuracy and large scale kernel learning.
arxiv-13200-85 | Privacy Prediction of Images Shared on Social Media Sites Using Deep Features | http://arxiv.org/pdf/1510.08583v3.pdf | author:Ashwini Tonge, Cornelia Caragea category:cs.CV cs.CY published:2015-10-29 summary:Online image sharing in social media sites such as Facebook, Flickr, andInstagram can lead to unwanted disclosure and privacy violations, when privacysettings are used inappropriately. With the exponential increase in the numberof images that are shared online every day, the development of effective andefficient prediction methods for image privacy settings are highly needed. Theperformance of models critically depends on the choice of the featurerepresentation. In this paper, we present an approach to image privacyprediction that uses deep features and deep image tags as featurerepresentations. Specifically, we explore deep features at various neuralnetwork layers and use the top layer (probability) as an auto-annotationmechanism. The results of our experiments show that models trained on theproposed deep features and deep image tags substantially outperform baselinessuch as those based on SIFT and GIST as well as those that use "bag of tags" asfeatures.
arxiv-13200-86 | Autoregressive Model for Individual Consumption Data - LASSO Selection and Significance Test | http://arxiv.org/pdf/1511.01853v1.pdf | author:Pan Li, Baosen Zhang, Yang Weng, Ram Rajagopal category:stat.ML cs.SY math.OC published:2015-11-05 summary:Understanding user flexibility and behavior patterns is becoming increasinglyvital to the design of robust and efficient energy saving programs. Accurateprediction of consumption is a key part to this understanding. Existingprediction methods usually have high relative errors that can be larger than30\%. In this paper, we explore sparsity in users' past data and relationshipbetween different users to increase prediction accuracy. We show that usingLASSO and significance test techniques, prediction accuracy can besignificantly compared to standard existing algorithms. We use mean absolutepercentage error (MAPE) as the criteria.
arxiv-13200-87 | Sparse approximation by greedy algorithms | http://arxiv.org/pdf/1511.01846v1.pdf | author:Vladimir Temlyakov category:math.NA stat.ML published:2015-11-05 summary:It is a survey on recent results in constructive sparse approximation. Threedirections are discussed here: (1) Lebesgue-type inequalities for greedyalgorithms with respect to a special class of dictionaries, (2) constructivesparse approximation with respect to the trigonometric system, (3) sparseapproximation with respect to dictionaries with tensor product structure. Inall three cases constructive ways are provided for sparse approximation. Thetechnique used is based on fundamental results from the theory of greedyapproximation. In particular, results in the direction (1) are based on deepmethods developed recently in compressed sensing. We present some of theseresults with detailed proofs.
arxiv-13200-88 | Faster Convex Optimization: Simulated Annealing with an Efficient Universal Barrier | http://arxiv.org/pdf/1507.02528v2.pdf | author:Jacob Abernethy, Elad Hazan category:math.OC cs.LG published:2015-07-09 summary:This paper explores a surprising equivalence between two seemingly-distinctconvex optimization methods. We show that simulated annealing, a well-studiedrandom walk algorithms, is directly equivalent, in a certain sense, to thecentral path interior point algorithm for the the entropic universal barrierfunction. This connection exhibits several benefits. First, we are able improvethe state of the art time complexity for convex optimization under themembership oracle model. We improve the analysis of the randomized algorithm ofKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii thatunderly the central path following interior point algorithm. We are able totighten the temperature schedule for simulated annealing which gives animproved running time, reducing by square root of the dimension in certaininstances. Second, we get an efficient randomized interior point method with anefficiently computable universal barrier for any convex set described by amembership oracle. Previously, efficiently computable barriers were known onlyfor particular convex sets.
arxiv-13200-89 | Computational Intractability of Dictionary Learning for Sparse Representation | http://arxiv.org/pdf/1511.01776v1.pdf | author:Meisam Razaviyayn, Hung-Wei Tseng, Zhi-Quan Luo category:cs.LG stat.ML published:2015-11-05 summary:In this paper we consider the dictionary learning problem for sparserepresentation. We first show that this problem is NP-hard by polynomial timereduction of the densest cut problem. Then, using successive convexapproximation strategies, we propose efficient dictionary learning schemes tosolve several practical formulations of this problem to stationary points.Unlike many existing algorithms in the literature, such as K-SVD, our proposeddictionary learning scheme is theoretically guaranteed to converge to the setof stationary points under certain mild assumptions. For the image denoisingapplication, the performance and the efficiency of the proposed dictionarylearning scheme are comparable to that of K-SVD algorithm in simulation.
arxiv-13200-90 | Canonical Polyadic Decomposition with Auxiliary Information for Brain Computer Interface | http://arxiv.org/pdf/1410.6313v2.pdf | author:Junhua Li, Chao Li, Andrzej Cichocki category:cs.CV published:2014-10-23 summary:Physiological signals are often organized in the form of multiple dimensions(e.g., channel, time, task, and 3D voxel), so it is better to preserve originalorganization structure when processing. Unlike vector-based methods thatdestroy data structure, Canonical Polyadic Decomposition (CPD) aims to processphysiological signals in the form of multi-way array, which considersrelationships between dimensions and preserves structure information containedby the physiological signal. Nowadays, CPD is utilized as an unsupervisedmethod for feature extraction in a classification problem. After that, aclassifier, such as support vector machine, is required to classify thosefeatures. In this manner, classification task is achieved in two isolatedsteps. We proposed supervised Canonical Polyadic Decomposition by directlyincorporating auxiliary label information during decomposition, by which aclassification task can be achieved without an extra step of classifiertraining. The proposed method merges the decomposition and classifier learningtogether, so it reduces procedure of classification task compared with that ofrespective decomposition and classification. In order to evaluate theperformance of the proposed method, three different kinds of signals, syntheticsignal, EEG signal, and MEG signal, were used. The results based on evaluationsof synthetic and real signals demonstrated that the proposed method iseffective and efficient.
arxiv-13200-91 | Discrete RÃ©nyi Classifiers | http://arxiv.org/pdf/1511.01764v1.pdf | author:Meisam Razaviyayn, Farzan Farnia, David Tse category:cs.LG published:2015-11-05 summary:Consider the binary classification problem of predicting a target variable$Y$ from a discrete feature vector $X = (X_1,...,X_d)$. When the probabilitydistribution $\mathbb{P}(X,Y)$ is known, the optimal classifier, leading to theminimum misclassification rate, is given by the Maximum A-posterioriProbability decision rule. However, estimating the complete joint distribution$\mathbb{P}(X,Y)$ is computationally and statistically impossible for largevalues of $d$. An alternative approach is to first estimate some low ordermarginals of $\mathbb{P}(X,Y)$ and then design the classifier based on theestimated low order marginals. This approach is also helpful when the completetraining data instances are not available due to privacy concerns. In thiswork, we consider the problem of finding the optimum classifier based on someestimated low order marginals of $(X,Y)$. We prove that for a given set ofmarginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principleintroduced in [1] leads to a randomized classification rule which is shown tohave a misclassification rate no larger than twice the misclassification rateof the optimal classifier. Then, under a separability condition, we show thatthe proposed algorithm is equivalent to a randomized linear regressionapproach. In addition, this method naturally results in a robust featureselection method selecting a subset of features having the maximum worst caseHGR correlation with the target variable. Our theoretical upper-bound issimilar to the recent Discrete Chebyshev Classifier (DCC) approach [2], whilethe proposed algorithm has significant computational advantages since it onlyrequires solving a least square optimization problem. Finally, we numericallycompare our proposed algorithm with the DCC classifier and show that theproposed algorithm results in better misclassification rate over variousdatasets.
arxiv-13200-92 | "Pale as death" or "pÃ¢le comme la mort": Frozen similes used as literary clichÃ©s | http://arxiv.org/pdf/1511.01756v1.pdf | author:Suzanne Mpouli, Jean-Gabriel Ganascia category:cs.CL published:2015-11-05 summary:The present study is focused on the automatic identification and descriptionof frozen similes in British and French novels written between the 19 thcentury and the beginning of the 20 th century. Two main patterns of frozensimiles were considered: adjectival ground + simile marker + nominal vehicle(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.sleep like a top). All potential similes and their components were firstextracted using a rule-based algorithm. Then, frozen similes were identifiedbased on reference lists of existing similes and semantic distance between thetenor and the vehicle. The results obtained tend to confirm the fact thatfrozen similes are not used haphazardly in literary texts. In addition,contrary to how they are often presented, frozen similes often go beyond theground or the eventuality and the vehicle to also include the tenor.
arxiv-13200-93 | Multi-Target Tracking and Occlusion Handling with Learned Variational Bayesian Clusters and a Social Force Model | http://arxiv.org/pdf/1511.01726v1.pdf | author:Ata-ur-Rehman, Syed Mohsen Naqvi, Lyudmila Mihaylova, Jonathon Chambers category:cs.CV published:2015-11-05 summary:This paper considers the problem of multiple human target tracking in asequence of video data. A solution is proposed which is able to deal with thechallenges of a varying number of targets, interactions and when every targetgives rise to multiple measurements. The developed novel algorithm comprisesvariational Bayesian clustering combined with a social force model, integratedwithin a particle filter with an enhanced prediction step. It performsmeasurement-to-target association by automatically detecting the measurementrelevance. The performance of the developed algorithm is evaluated over severalsequences from publicly available data sets: AV16.3, CAVIAR and PETS2006, whichdemonstrates that the proposed algorithm successfully initializes and tracks avariable number of targets in the presence of complex occlusions. A comparisonwith state-of-the-art techniques due to Khan et al., Laet et al. and Czyz etal. shows improved tracking performance.
arxiv-13200-94 | Image classification based on support vector machine and the fusion of complementary features | http://arxiv.org/pdf/1511.01706v1.pdf | author:Huilin Gao, Wenjie Chen, Lihua Dou category:cs.CV published:2015-11-05 summary:Image Classification based on BOW (Bag-of-words) has broad applicationprospect in pattern recognition field but the shortcomings are existed becauseof single feature and low classification accuracy. To this end we combine threeingredients: (i) Three features with functions of mutual complementation areadopted to describe the images, including PHOW (Pyramid Histogram of Words),PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of OrientatedGradients). (ii) The improvement of traditional BOW model is presented by usingdense sample and an improved K-means clustering method for constructing thevisual dictionary. (iii) An adaptive feature-weight adjusted imagecategorization algorithm based on the SVM and the fusion of multiple featuresis adopted. Experiments carried out on Caltech 101 database confirm thevalidity of the proposed approach. From the experimental results can be seenthat the classification accuracy rate of the proposed method is improved by7%-17% higher than that of the traditional BOW methods. This algorithm makesfull use of global, local and spatial information and has significantimprovements to the classification accuracy.
arxiv-13200-95 | Comparing Writing Styles using Word Embedding and Dynamic Time Warping | http://arxiv.org/pdf/1511.01666v1.pdf | author:Abhinav Tushar, Abhinav Dahiya category:cs.CL published:2015-11-05 summary:The development of plot or story in novels is reflected in the content andthe words used. The flow of sentiments, which is one aspect of writing style,can be quantified by analyzing the flow of words. This study explores literaryworks as signals in word embedding space and tries to compare writing styles ofpopular classic novels using dynamic time warping.
arxiv-13200-96 | An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding | http://arxiv.org/pdf/1511.01665v1.pdf | author:Yiou Lin, Hang Lei, Jia Wu, Xiaoyu Li category:cs.CL published:2015-11-05 summary:In this article, how word embeddings can be used as features in Chinesesentiment classification is presented. Firstly, a Chinese opinion corpus isbuilt with a million comments from hotel review websites. Then the wordembeddings which represent each comment are used as input in different machinelearning methods for sentiment classification, including SVM, LogisticRegression, Convolutional Neural Network (CNN) and ensemble methods. Thesemethods get better performance compared with N-gram models using Naive Bayes(NB) and Maximum Entropy (ME). Finally, a combination of machine learningmethods is proposed which presents an outstanding performance in precision,recall and F1 score. After selecting the most useful methods to construct thecombinational model and testing over the corpus, the final F1 score is 0.920.
arxiv-13200-97 | Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model | http://arxiv.org/pdf/1511.01644v1.pdf | author:Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan category:stat.AP cs.LG stat.ML published:2015-11-05 summary:We aim to produce predictive models that are not only accurate, but are alsointerpretable to human experts. Our models are decision lists, which consist ofa series of if...then... statements (e.g., if high blood pressure, then stroke)that discretize a high-dimensional, multivariate feature space into a series ofsimple, readily interpretable decision statements. We introduce a generativemodel called Bayesian Rule Lists that yields a posterior distribution overpossible decision lists. It employs a novel prior structure to encouragesparsity. Our experiments show that Bayesian Rule Lists has predictive accuracyon par with the current top algorithms for prediction in machine learning. Ourmethod is motivated by recent developments in personalized medicine, and can beused to produce highly accurate and interpretable medical scoring systems. Wedemonstrate this by producing an alternative to the CHADS$_2$ score, activelyused in clinical practice for estimating the risk of stroke in patients thathave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but moreaccurate.
arxiv-13200-98 | Point Localization and Density Estimation from Ordinal kNN graphs using Synchronization | http://arxiv.org/pdf/1504.00722v2.pdf | author:Mihai Cucuringu, Joseph Woodworth category:stat.ML published:2015-04-03 summary:We consider the problem of embedding unweighted, directed k-nearest neighborgraphs in low-dimensional Euclidean space. The k-nearest neighbors of eachvertex provides ordinal information on the distances between points, but notthe distances themselves. We use this ordinal information along with thelow-dimensionality to recover the coordinates of the points up to arbitrarysimilarity transformations (rigid transformations and scaling). Furthermore, wealso illustrate the possibility of robustly recovering the underlying densityvia the Total Variation Maximum Penalized Likelihood Estimation (TV-MPLE)method. We make existing approaches scalable by using an instance of alocal-to-global algorithm based on group synchronization, recently proposed inthe literature in the context of sensor network localization and structuralbiology, which we augment with a scaling synchronization step. We demonstratethe scalability of our approach on large graphs, and show how it compares tothe Local Ordinal Embedding (LOE) algorithm, which was recently proposed forrecovering the configuration of a cloud of points from pairwise ordinalcomparisons between a sparse set of distances.
arxiv-13200-99 | Attention with Intention for a Neural Network Conversation Model | http://arxiv.org/pdf/1510.08565v3.pdf | author:Kaisheng Yao, Geoffrey Zweig, Baolin Peng category:cs.NE cs.AI cs.HC cs.LG published:2015-10-29 summary:In a conversation or a dialogue process, attention and intention playintrinsic roles. This paper proposes a neural network based approach thatmodels the attention and intention processes. It essentially consists of threerecurrent networks. The encoder network is a word-level model representingsource side sentences. The intention network is a recurrent network that modelsthe dynamics of the intention process. The decoder network is a recurrentnetwork produces responses to the input from the source side. It is a languagemodel that is dependent on the intention and has an attention mechanism toattend to particular source side words, when predicting a symbol in theresponse. The model is trained end-to-end without labeling data. Experimentsshow that this model generates natural responses to user inputs.
arxiv-13200-100 | Background Modeling Using Adaptive Pixelwise Kernel Variances in a Hybrid Feature Space | http://arxiv.org/pdf/1511.01631v1.pdf | author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV published:2015-11-05 summary:Recent work on background subtraction has shown developments on two majorfronts. In one, there has been increasing sophistication of probabilisticmodels, from mixtures of Gaussians at each pixel [7], to kernel densityestimates at each pixel [1], and more recently to joint domainrange densityestimates that incorporate spatial information [6]. Another line of work hasshown the benefits of increasingly complex feature representations, includingthe use of texture information, local binary patterns, and recentlyscale-invariant local ternary patterns [4]. In this work, we use jointdomain-range based estimates for background and foreground scores and show thatdynamically choosing kernel variances in our kernel estimates at eachindividual pixel can significantly improve results. We give a heuristic methodfor selectively applying the adaptive kernel calculations which is nearly asaccurate as the full procedure but runs much faster. We combine these modelingimprovements with recently developed complex features [4] and show significantimprovements on a standard backgrounding benchmark.
arxiv-13200-101 | Background subtraction - separating the modeling and the inference | http://arxiv.org/pdf/1511.01627v1.pdf | author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV published:2015-11-05 summary:In its early implementations, background modeling was a process of building amodel for the background of a video with a stationary camera, and identifyingpixels that did not conform well to this model. The pixels that were notwell-described by the background model were assumed to be moving objects. Manysystems today maintain models for the foreground as well as the background, andthese models compete to explain the pixels in a video. In this paper, we arguethat the logical endpoint of this evolution is to simply use Bayes' rule toclassify pixels. In particular, it is essential to have a backgroundlikelihood, a foreground likelihood, and a prior at each pixel. A simpleapplication of Bayes' rule then gives a posterior probability over the label.The only remaining question is the quality of the component models: thebackground likelihood, the foreground likelihood, and the prior. We describe amodel for the likelihoods that is built by using not only the past observationsat a given pixel location, but by also including observations in a spatialneighborhood around the location. This enables us to model the influencebetween neighboring pixels and is an improvement over earlier pixelwise modelsthat do not allow for such influence. Although similar in spirit to the jointdomain-range model, we show that our model overcomes certain deficiencies inthat model. We use a spatially dependent prior for the background andforeground. The background and foreground labels from the previous frame, afterspatial smoothing to account for movement of objects,are used to build theprior for the current frame.
arxiv-13200-102 | Douglas-Rachford splitting for nonconvex optimization with application to nonconvex feasibility problems | http://arxiv.org/pdf/1409.8444v5.pdf | author:Guoyin Li, Ting Kei Pong category:math.OC stat.ML published:2014-09-30 summary:We adapt the Douglas-Rachford (DR) splitting method to solve nonconvexfeasibility problems by studying this method for a class of nonconvexoptimization problem. While the convergence properties of the method for convexproblems have been well studied, far less is known in the nonconvex setting. Inthis paper, for the direct adaptation of the method to minimize the sum of aproper closed function $g$ and a smooth function $f$ with a Lipschitzcontinuous gradient, we show that if the step-size parameter is smaller than acomputable threshold and the sequence generated has a cluster point, then itgives a stationary point of the optimization problem. Convergence of the wholesequence and a local convergence rate are also established under the additionalassumption that $f$ and $g$ are semi-algebraic. We also give simple sufficientconditions guaranteeing the boundedness of the sequence generated. We thenapply our nonconvex DR splitting method to finding a point in the intersectionof a closed convex set $C$ and a general closed set $D$ by minimizing thesquared distance to $C$ subject to $D$. We show that if either set is boundedand the step-size parameter is smaller than a computable threshold, then thesequence generated from the DR splitting method is actually bounded.Consequently, the sequence generated will have cluster points that arestationary for an optimization problem, and the whole sequence is convergentunder an additional assumption that $C$ and $D$ are semi-algebraic. We achievethese results based on a new merit function constructed particularly for the DRsplitting method. Our preliminary numerical results indicate that our DRsplitting method usually outperforms the alternating projection method infinding a sparse solution of a linear system, in terms of both the solutionquality and the number of iterations taken.
arxiv-13200-103 | Coherent Motion Segmentation in Moving Camera Videos using Optical Flow Orientations | http://arxiv.org/pdf/1511.01619v1.pdf | author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV published:2015-11-05 summary:In moving camera videos, motion segmentation is commonly performed using theimage plane motion of pixels, or optical flow. However, objects that are atdifferent depths from the camera can exhibit different optical flows even ifthey share the same real-world motion. This can cause a depth-dependentsegmentation of the scene. Our goal is to develop a segmentation algorithm thatclusters pixels that have similar real-world motion irrespective of their depthin the scene. Our solution uses optical flow orientations instead of thecomplete vectors and exploits the well-known property that under cameratranslation, optical flow orientations are independent of object depth. Weintroduce a probabilistic model that automatically estimates the number ofobserved independent motions and results in a labeling that is consistent withreal-world motion in the scene. The result of our system is that static objectsare correctly identified as one segment, even if they are at different depths.Color features and information from previous frames in the video sequence areused to correct occasional errors due to the orientation-based segmentation. Wepresent results on more than thirty videos from different benchmarks. Thesystem is particularly robust on complex background scenes containing objectsat significantly different depths
arxiv-13200-104 | Color Aesthetics and Social Networks in Complete Tang Poems: Explorations and Discoveries | http://arxiv.org/pdf/1511.01559v1.pdf | author:Chao-Lin Liu, Hongsu Wang, Wen-Huei Cheng, Chu-Ting Hsu, Wei-Yun Chiu category:cs.CL cs.DL cs.IR published:2015-11-05 summary:The Complete Tang Poems (CTP) is the most important source to study Tangpoems. We look into CTP with computational tools from specific linguisticperspectives, including distributional semantics and collocational analysis.From such quantitative viewpoints, we compare the usage of "wind" and "moon" inthe poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,and play a crucial role in the imageries of poems. Thus, words for colors arestudied, and "white" is the main focus because it is the most frequent color inCTP. We also explore some cases of using colored words in antithesis pairs thatwere central for fostering the imageries of the poems. CTP also contains usefulhistorical information, and we extract person names in CTP to study the socialnetworks of the Tang poets. Such information can then be integrated with theChina Biographical Database of Harvard University.
arxiv-13200-105 | Ripple Down Rules for Question Answering | http://arxiv.org/pdf/1412.4160v4.pdf | author:Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham category:cs.CL cs.IR published:2014-12-12 summary:Recent years have witnessed a new trend of building ontology-based questionanswering systems. These systems use semantic web information to produce moreprecise answers to users' queries. However, these systems are mostly designedfor English. In this paper, we introduce an ontology-based question answeringsystem named KbQAS which, to the best of our knowledge, is the first one madefor Vietnamese. KbQAS employs our question analysis approach thatsystematically constructs a knowledge base of grammar rules to convert eachinput question into an intermediate representation element. KbQAS then takesthe intermediate representation element with respect to a target ontology andapplies concept-matching techniques to return an answer. On a wide range ofVietnamese questions, experimental results show that the performance of KbQASis promising with accuracies of 84.1% and 82.4% for analyzing input questionsand retrieving output answers, respectively. Furthermore, our question analysisapproach can easily be applied to new domains and new languages, thus savingtime and human effort.
arxiv-13200-106 | Mining Local Gazetteers of Literary Chinese with CRF and Pattern based Methods for Biographical Information in Chinese History | http://arxiv.org/pdf/1511.01556v1.pdf | author:Chao-Lin Liu, Chih-Kai Huang, Hongsu Wang, Peter K. Bol category:cs.CL cs.DL cs.IR cs.LG published:2015-11-04 summary:Person names and location names are essential building blocks for identifyingevents and social networks in historical documents that were written inliterary Chinese. We take the lead to explore the research on algorithmicallyrecognizing named entities in literary Chinese for historical studies withlanguage-model based and conditional-random-field based methods, and extend ourwork to mining the document structures in historical documents. Practicalevaluations were conducted with texts that were extracted from more than 220volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the singlemost important collection that contains information about officers who servedin local government in Chinese history. Our methods performed very well onthese realistic tests. Thousands of names and addresses were identified fromthe texts. A good portion of the extracted names match the biographicalinformation currently recorded in the China Biographical Database (CBDB) ofHarvard University, and many others can be verified by historians and willbecome as new additions to CBDB.
arxiv-13200-107 | Optimal Rates for Random Fourier Features | http://arxiv.org/pdf/1506.02155v2.pdf | author:Bharath K. Sriperumbudur, Zoltan Szabo category:math.ST cs.LG math.FA stat.ML stat.TH published:2015-06-06 summary:Kernel methods represent one of the most powerful tools in machine learningto tackle problems expressed in terms of function values and derivatives due totheir capability to represent and model complex relations. While these methodsshow good versatility, they are computationally intensive and have poorscalability to large data as they require operations on Gram matrices. In orderto mitigate this serious computational limitation, recently randomizedconstructions have been proposed in the literature, which allow the applicationof fast linear algorithms. Random Fourier features (RFF) are among the mostpopular and widely applied constructions: they provide an easily computable,low-dimensional feature representation for shift-invariant kernels. Despite thepopularity of RFFs, very little is understood theoretically about theirapproximation quality. In this paper, we provide a detailed finite-sampletheoretical analysis about the approximation quality of RFFs by (i)establishing optimal (in terms of the RFF dimension, and growing set size)performance guarantees in uniform norm, and (ii) presenting guarantees in $L^r$($1\le r<\infty$) norms. We also propose an RFF approximation to derivatives ofa kernel with a theoretical study on its approximation quality.
arxiv-13200-108 | Regularization and Bayesian Learning in Dynamical Systems: Past, Present and Future | http://arxiv.org/pdf/1511.01543v1.pdf | author:A. Chiuso category:cs.SY stat.ML published:2015-11-04 summary:Regularization and Bayesian methods for system identification have beenrepopularized in the recent years, and proved to be competitive w.r.t.classical parametric approaches. In this paper we shall make an attempt toillustrate how the use of regularization in system identification has evolvedover the years, starting from the early contributions both in the AutomaticControl as well as Econometrics and Statistics literature. In particular weshall discuss some fundamental issues such as compound estimation problems andexchangeability which play and important role in regularization and Bayesianapproaches, as also illustrated in early publications in Statistics. Thehistorical and foundational issues will be given more emphasis (and space), atthe expense of the more recent developments which are only briefly discussed.The main reason for such a choice is that, while the recent literature isreadily available, and surveys have already been published on the subject, inthe author's opinion a clear link with past work had not been completelyclarified.
arxiv-13200-109 | Mean-field inference of Hawkes point processes | http://arxiv.org/pdf/1511.01512v1.pdf | author:Emmanuel Bacry, StÃ©phane GaÃ¯ffas, Iacopo Mastromatteo, Jean-FranÃ§ois Muzy category:cs.LG published:2015-11-04 summary:We propose a fast and efficient estimation method that is able to accuratelyrecover the parameters of a d-dimensional Hawkes point-process from a set ofobservations. We exploit a mean-field approximation that is valid when thefluctuations of the stochastic intensity are small. We show that this isnotably the case in situations when interactions are sufficiently weak, whenthe dimension of the system is high or when the fluctuations are self-averagingdue to the large number of past events they involve. In such a regime theestimation of a Hawkes process can be mapped on a least-squares problem forwhich we provide an analytic solution. Though this estimator is biased, we showthat its precision can be comparable to the one of the Maximum LikelihoodEstimator while its computation speed is shown to be improved considerably. Wegive a theoretical control on the accuracy of our new approach and illustrateits efficiency using synthetic datasets, in order to assess the statisticalestimation error of the parameters.
arxiv-13200-110 | Enhancing Feature Tracking With Gyro Regularization | http://arxiv.org/pdf/1511.01508v1.pdf | author:Bryan Poling, Gilad Lerman category:cs.CV 68T45 published:2015-11-04 summary:We present a deeply integrated method of exploiting low-cost gyroscopes toimprove general purpose feature tracking. Most previous methods use gyroscopesto initialize and bound the search for features. In contrast, we use them toregularize the tracking energy function so that they can directly assist in thetracking of ambiguous and poor-quality features. We demonstrate that our simpletechnique offers significant improvements in performance over conventionaltemplate-based tracking methods, and is in fact competitive with more complexand computationally expensive state-of-the-art trackers, but at a fraction ofthe computational cost. Additionally, we show that the practice of initializingtemplate-based feature trackers like KLT (Kanade-Lucas-Tomasi) usinggyro-predicted optical flow offers no advantage over using a carefuloptical-only initialization method, suggesting that some deeper level ofintegration, like the method we propose, is needed in order to realize agenuine improvement in tracking performance from these inertial sensors.
arxiv-13200-111 | High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification | http://arxiv.org/pdf/1507.03003v2.pdf | author:Edgar Dobriban, Stefan Wager category:math.ST stat.ML stat.TH published:2015-07-10 summary:We provide a unified analysis of the predictive risk of ridge regression andregularized discriminant analysis in a dense random effects model. We work in ahigh-dimensional asymptotic regime where $p, n \to \infty$ and $p/n \to \gamma\in (0, \, \infty)$, and allow for arbitrary covariance among the features. Forboth methods, we provide an explicit and efficiently computable expression forthe limiting predictive risk, which depends only on the spectrum of thefeature-covariance matrix, the signal strength, and the aspect ratio $\gamma$.Especially in the case of regularized discriminant analysis, we find thatpredictive accuracy has a nuanced dependence on the eigenvalue distribution ofthe covariance matrix, suggesting that analyses based on the operator norm ofthe covariance matrix may not be sharp. Our results also uncover severalqualitative insights about both methods: for example, with ridge regression,there is an exact inverse relation between the limiting predictive risk and thelimiting estimation risk given a fixed signal strength. Our analysis builds onrecent advances in random matrix theory.
arxiv-13200-112 | Approximation of the truncated Zeta distribution and Zipf's law | http://arxiv.org/pdf/1511.01480v1.pdf | author:Maurizio Naldi category:stat.AP cs.CL cs.SI published:2015-11-04 summary:Zipf's law appears in many application areas but does not have a closed formexpression, which may make its use cumbersome. Since it coincides with thetruncated version of the Zeta distribution, in this paper we propose threeapproximate closed form expressions for the truncated Zeta distribution, whichmay be employed for Zipf's law as well. The three approximations are based onthe replacement of the sum occurring in Zipf's law with an integral, and arenamed respectively the integral approximation, the average integralapproximation, and the trapezoidal approximation. While the first one is shownto be of little use, the trapezoidal approximation exhibits an error which istypically lower than 1\%, but is as low as 0.1\% for the range of values of theZipf parameter below 1.
arxiv-13200-113 | Semi-supervised Sequence Learning | http://arxiv.org/pdf/1511.01432v1.pdf | author:Andrew M. Dai, Quoc V. Le category:cs.LG cs.CL published:2015-11-04 summary:We present two approaches that use unlabeled data to improve sequencelearning with recurrent networks. The first approach is to predict what comesnext in a sequence, which is a conventional language model in natural languageprocessing. The second approach is to use a sequence autoencoder, which readsthe input sequence into a vector and predicts the input sequence again. Thesetwo algorithms can be used as a "pretraining" step for a later supervisedsequence learning algorithm. In other words, the parameters obtained from theunsupervised step can be used as a starting point for other supervised trainingmodels. In our experiments, we find that long short term memory recurrentnetworks after being pretrained with the two approaches are more stable andgeneralize better. With pretraining, we are able to train long short termmemory recurrent networks up to a few hundred timesteps, thereby achievingstrong performance in many text classification tasks, such as IMDB, DBpedia and20 Newsgroups.
arxiv-13200-114 | Turing Computation with Recurrent Artificial Neural Networks | http://arxiv.org/pdf/1511.01427v1.pdf | author:Giovanni S Carmantini, Peter beim Graben, Mathieu Desroches, Serafim Rodrigues category:cs.NE published:2015-11-04 summary:We improve the results by Siegelmann & Sontag (1995) by providing a novel andparsimonious constructive mapping between Turing Machines and RecurrentArtificial Neural Networks, based on recent developments of Nonlinear DynamicalAutomata. The architecture of the resulting R-ANNs is simple and elegant,stemming from its transparent relation with the underlying NDAs. Thesecharacteristics yield promise for developments in machine learning methods andsymbolic computation with continuous time dynamical systems. A framework isprovided to directly program the R-ANNs from Turing Machine descriptions, inabsence of network training. At the same time, the network can potentially betrained to perform algorithmic tasks, with exciting possibilities in theintegration of approaches akin to Google DeepMind's Neural Turing Machines.
arxiv-13200-115 | Consistent Parameter Estimation for LASSO and Approximate Message Passing | http://arxiv.org/pdf/1511.01017v2.pdf | author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:math.ST cs.IT math.IT math.OC stat.ML stat.TH published:2015-11-03 summary:We consider the problem of recovering a vector $\beta_o \in \mathbb{R}^p$from $n$ random and noisy linear observations $y= X\beta_o + w$, where $X$ isthe measurement matrix and $w$ is noise. The LASSO estimate is given by thesolution to the optimization problem $\hat{\beta}_{\lambda} = \arg \min_{\beta}\frac{1}{2} \y-X\beta\_2^2 + \lambda \ \beta \_1$. Among the iterativealgorithms that have been proposed for solving this optimization problem,approximate message passing (AMP) has attracted attention for its fastconvergence. Despite significant progress in the theoretical analysis of theestimates of LASSO and AMP, little is known about their behavior as a functionof the regularization parameter $\lambda$, or the thereshold parameters$\tau^t$. For instance the following basic questions have not yet been studiedin the literature: (i) How does the size of the active set$\\hat{\beta}^\lambda\_0/p$ behave as a function of $\lambda$? (ii) How doesthe mean square error $\\hat{\beta}_{\lambda} - \beta_o\_2^2/p$ behave as afunction of $\lambda$? (iii) How does $\\beta^t - \beta_o \_2^2/p$ behave asa function of $\tau^1, \ldots, \tau^{t-1}$? Answering these questions will helpin addressing practical challenges regarding the optimal tuning of $\lambda$ or$\tau^1, \tau^2, \ldots$. This paper answers these questions in the asymptoticsetting and shows how these results can be employed in deriving simple andtheoretically optimal approaches for tuning the parameters $\tau^1, \ldots,\tau^t$ for AMP or $\lambda$ for LASSO. It also explores the connection betweenthe optimal tuning of the parameters of AMP and the optimal tuning of LASSO.
arxiv-13200-116 | Supervised Learning for Dynamical System Learning | http://arxiv.org/pdf/1505.05310v2.pdf | author:Ahmed Hefny, Carlton Downey, Geoffrey Gordon category:stat.ML cs.LG published:2015-05-20 summary:Recently there has been substantial interest in spectral methods for learningdynamical systems. These methods are popular since they often offer a goodtradeoff between computational and statistical efficiency. Unfortunately, theycan be difficult to use and extend in practice: e.g., they can make itdifficult to incorporate prior information such as sparsity or structure. Toaddress this problem, we present a new view of dynamical system learning: weshow how to learn dynamical systems by solving a sequence of ordinarysupervised learning problems, thereby allowing users to incorporate priorknowledge via standard techniques such as L1 regularization. Many existingspectral methods are special cases of this new framework, using linearregression as the supervised learner. We demonstrate the effectiveness of ourframework by showing examples where nonlinear regression or lasso let us learnbetter state representations than plain linear regression does; the correctnessof these instances follows directly from our general analysis.
arxiv-13200-117 | Operator-valued Kernels for Learning from Functional Response Data | http://arxiv.org/pdf/1510.08231v2.pdf | author:Hachem Kadri, Emmanuel Duflos, Philippe Preux, StÃ©phane Canu, Alain Rakotomamonjy, Julien Audiffren category:cs.LG stat.ML published:2015-10-28 summary:In this paper we consider the problems of supervised classification andregression in the case where attributes and labels are functions: a data isrepresented by a set of functions, and the label is also a function. We focuson the use of reproducing kernel Hilbert space theory to learn from suchfunctional data. Basic concepts and properties of kernel-based learning areextended to include the estimation of function-valued functions. In thissetting, the representer theorem is restated, a set of rigorously definedinfinite-dimensional operator-valued kernels that can be valuably applied whenthe data are functions is described, and a learning algorithm for nonlinearfunctional data analysis is introduced. The methodology is illustrated throughspeech and audio signal processing experiments.
arxiv-13200-118 | Dictionary descent in optimization | http://arxiv.org/pdf/1511.01304v1.pdf | author:Vladimir Temlyakov category:stat.ML math.NA published:2015-11-04 summary:The problem of convex optimization is studied. Usually in convex optimizationthe minimization is over a d-dimensional domain. Very often the convergencerate of an optimization algorithm depends on the dimension d. The algorithmsstudied in this paper utilize dictionaries instead of a canonical basis used inthe coordinate descent algorithms. We show how this approach allows us toreduce dimensionality of the problem. Also, we investigate which properties ofa dictionary are beneficial for the convergence rate of typical greedy-typealgorithms.
arxiv-13200-119 | A proposal project for a blind image quality assessment by learning distortions from the full reference image quality assessments | http://arxiv.org/pdf/1512.04354v1.pdf | author:StÃ©fane Paris category:cs.MM cs.CV published:2015-11-04 summary:This short paper presents a perspective plan to build a null reference imagequality assessment. Its main goal is to deliver both the objective score andthe distortion map for a given distorted image without the knowledge of itsreference image.
arxiv-13200-120 | Towards a tracking algorithm based on the clustering of spatio-temporal clouds of points | http://arxiv.org/pdf/1511.01293v1.pdf | author:Andrea Cavagna, Chiara Creato, Lorenzo Del Castello, Stefania Melillo, Leonardo Parisi, Massimiliano Viale category:cs.CV published:2015-11-04 summary:The interest in 3D dynamical tracking is growing in fields such as robotics,biology and fluid dynamics. Recently, a major source of progress in 3D trackinghas been the study of collective behaviour in biological systems, where thetrajectories of individual animals moving within large and dense groups need tobe reconstructed to understand the behavioural interaction rules. Experimentaldata in this field are generally noisy and at low spatial resolution, so thatindividuals appear as small featureless objects and trajectories must beretrieved by making use of epipolar information only. Moreover, opticalocclusions often occur: in a multi-camera system one or more objects becomeindistinguishable in one view, potentially jeopardizing the conservation ofidentity over long-time trajectories. The most advanced 3D tracking algorithmsovercome optical occlusions making use of set-cover techniques, which howeverhave to solve NP-hard optimization problems. Moreover, current methods are notable to cope with occlusions arising from actual physical proximity of objectsin 3D space. Here, we present a new method designed to work directly in 3Dspace and time, creating (3D+1) clouds of points representing the fullspatio-temporal evolution of the moving targets. We can then use a simpleconnected components labeling routine, which is linear in time, to solveoptical occlusions, hence lowering from NP to P the complexity of the problem.Finally, we use normalized cut spectral clustering to tackle 3D physicalproximity.
arxiv-13200-121 | Data-Driven Learning of a Union of Sparsifying Transforms Model for Blind Compressed Sensing | http://arxiv.org/pdf/1511.01289v1.pdf | author:Saiprasad Ravishankar, Yoram Bresler category:stat.ML cs.LG published:2015-11-04 summary:Compressed sensing is a powerful tool in applications such as magneticresonance imaging (MRI). It enables accurate recovery of images from highlyundersampled measurements by exploiting the sparsity of the images or imagepatches in a transform domain or dictionary. In this work, we focus on blindcompressed sensing (BCS), where the underlying sparse signal model is a prioriunknown, and propose a framework to simultaneously reconstruct the underlyingimage as well as the unknown model from highly undersampled measurements.Specifically, our model is that the patches of the underlying image(s) areapproximately sparse in a transform domain. We also extend this model to aunion of transforms model that better captures the diversity of features innatural images. The proposed block coordinate descent type algorithms for blindcompressed sensing are highly efficient, and are guaranteed to converge to atleast the partial global and partial local minimizers of the highly non-convexBCS problems. Our numerical experiments show that the proposed frameworkusually leads to better quality of image reconstructions in MRI compared toseveral recent image reconstruction methods. Importantly, the learning of aunion of sparsifying transforms leads to better image reconstructions than asingle adaptive transform.
arxiv-13200-122 | Lasso based feature selection for malaria risk exposure prediction | http://arxiv.org/pdf/1511.01284v1.pdf | author:Bienvenue KouwayÃ¨, NoÃ«l Fonton, Fabrice Rossi category:stat.ML published:2015-11-04 summary:In life sciences, the experts generally use empirical knowledge to recodevariables, choose interactions and perform selection by classical approach. Theaim of this work is to perform automatic learning algorithm for variablesselection which can lead to know if experts can be help in they decision orsimply replaced by the machine and improve they knowledge and results. TheLasso method can detect the optimal subset of variables for estimation andprediction under some conditions. In this paper, we propose a novel approachwhich uses automatically all variables available and all interactions. By adouble cross-validation combine with Lasso, we select a best subset ofvariables and with GLM through a simple cross-validation perform predictions.The algorithm assures the stability and the the consistency of estimators.
arxiv-13200-123 | Factorizing LambdaMART for cold start recommendations | http://arxiv.org/pdf/1511.01282v1.pdf | author:Phong Nguyen, Jun Wang, Alexandros Kalousis category:cs.LG cs.IR published:2015-11-04 summary:Recommendation systems often rely on point-wise loss metrics such as the meansquared error. However, in real recommendation settings only few items arepresented to a user. This observation has recently encouraged the use ofrank-based metrics. LambdaMART is the state-of-the-art algorithm in learning torank which relies on such a metric. Despite its success it does not have aprincipled regularization mechanism relying in empirical approaches to controlmodel complexity leaving it thus prone to overfitting. Motivated by the fact that very often the users' and items' descriptions aswell as the preference behavior can be well summarized by a small number ofhidden factors, we propose a novel algorithm, LambdaMART Matrix Factorization(LambdaMART-MF), that learns a low rank latent representation of users anditems using gradient boosted trees. The algorithm factorizes lambdaMART bydefining relevance scores as the inner product of the learned representationsof the users and items. The low rank is essentially a model complexitycontroller; on top of it we propose additional regularizers to constraint thelearned latent representations that reflect the user and item manifolds asthese are defined by their original feature based descriptors and thepreference behavior. Finally we also propose to use a weighted variant of NDCGto reduce the penalty for similar items with large rating discrepancy. We experiment on two very different recommendation datasets, meta-mining andmovies-users, and evaluate the performance of LambdaMART-MF, with and withoutregularization, in the cold start setting as well as in the simpler matrixcompletion setting. In both cases it outperforms in a significant mannercurrent state of the art algorithms.
arxiv-13200-124 | Co-Clustering Network-Constrained Trajectory Data | http://arxiv.org/pdf/1511.01281v1.pdf | author:Mohamed Khalil El Mahrsi, Romain GuigourÃ¨s, Fabrice Rossi, Marc BoullÃ© category:stat.ML cs.DB cs.LG published:2015-11-04 summary:Recently, clustering moving object trajectories kept gaining interest fromboth the data mining and machine learning communities. This problem, however,was studied mainly and extensively in the setting where moving objects can movefreely on the euclidean space. In this paper, we study the problem ofclustering trajectories of vehicles whose movement is restricted by theunderlying road network. We model relations between these trajectories and roadsegments as a bipartite graph and we try to cluster its vertices. Wedemonstrate our approaches on synthetic data and show how it could be useful ininferring knowledge about the flow dynamics and the behavior of the driversusing the road network.
arxiv-13200-125 | Study of a bias in the offline evaluation of a recommendation algorithm | http://arxiv.org/pdf/1511.01280v1.pdf | author:Arnaud De Myttenaere, Boris Golden, BÃ©nÃ©dicte Le Grand, Fabrice Rossi category:cs.IR cs.LG stat.ML published:2015-11-04 summary:Recommendation systems have been integrated into the majority of large onlinesystems to filter and rank information according to user profiles. It thusinfluences the way users interact with the system and, as a consequence, biasthe evaluation of the performance of a recommendation algorithm computed usinghistorical data (via offline evaluation). This paper describes this bias anddiscuss the relevance of a weighted offline evaluation to reduce this bias fordifferent classes of recommendation algorithms.
arxiv-13200-126 | Transforming Wikipedia into a Search Engine for Local Experts | http://arxiv.org/pdf/1511.01259v1.pdf | author:Gregory Grefenstette, Karima Rafes category:cs.IR cs.CL published:2015-11-04 summary:Finding experts for a given problem is recognized as a difficult task. Evenwhen a taxonomy of subject expertise exists, and is associated with a group ofexperts, it can be hard to exploit by users who have not internalized thetaxonomy. Here we present a method for both attaching experts to a domainontology, and hiding this fact from the end user looking for an expert. Bylinking Wikipedia to this same pivot ontology, we describe how a user canbrowse Wikipedia, as they normally do to search for information, and use thisbrowsing behavior to find experts. Experts are characterized by their textualproductions (webpages, publications, reports), and these textual productionsare attached to concepts in the pivot ontology. When the user finds theWikipedia page characterizing their need, a list of experts is displayed. Inthis way we transform Wikipedia into a search engine for experts.
arxiv-13200-127 | Face Aging Effect Simulation using Hidden Factor Analysis Joint Sparse Representation | http://arxiv.org/pdf/1511.01186v1.pdf | author:Hongyu Yang, Di Huang, Yunhong Wang, Heng Wang, Yuanyan Tang category:cs.CV published:2015-11-04 summary:Face aging simulation has received rising investigations nowadays, whereas itstill remains a challenge to generate convincing and natural age-progressedface images. In this paper, we present a novel approach to such an issue byusing hidden factor analysis joint sparse representation. In contrast to themajority of tasks in the literature that handle the facial texture integrally,the proposed aging approach separately models the person-specific facialproperties that tend to be stable in a relatively long period and theage-specific clues that change gradually over time. It then merely transformsthe age component to a target age group via sparse reconstruction, yieldingaging effects, which is finally combined with the identity component to achievethe aged face. Experiments are carried out on three aging databases, and theresults achieved clearly demonstrate the effectiveness and robustness of theproposed method in rendering a face with aging effects. Additionally, a seriesof evaluations prove its validity with respect to identity preservation andaging effect generation.
arxiv-13200-128 | Global convergence of splitting methods for nonconvex composite optimization | http://arxiv.org/pdf/1407.0753v6.pdf | author:Guoyin Li, Ting Kei Pong category:math.OC cs.LG math.NA stat.ML published:2014-07-03 summary:We consider the problem of minimizing the sum of a smooth function $h$ with abounded Hessian, and a nonsmooth function. We assume that the latter functionis a composition of a proper closed function $P$ and a surjective linear map$\cal M$, with the proximal mappings of $\tau P$, $\tau > 0$, simple tocompute. This problem is nonconvex in general and encompasses many importantapplications in engineering and machine learning. In this paper, we examinedtwo types of splitting methods for solving this nonconvex optimization problem:alternating direction method of multipliers and proximal gradient algorithm.For the direct adaptation of the alternating direction method of multipliers,we show that, if the penalty parameter is chosen sufficiently large and thesequence generated has a cluster point, then it gives a stationary point of thenonconvex problem. We also establish convergence of the whole sequence under anadditional assumption that the functions $h$ and $P$ are semi-algebraic.Furthermore, we give simple sufficient conditions to guarantee boundedness ofthe sequence generated. These conditions can be satisfied for a wide range ofapplications including the least squares problem with the $\ell_{1/2}$regularization. Finally, when $\cal M$ is the identity so that the proximalgradient algorithm can be efficiently applied, we show that any cluster pointis stationary under a slightly more flexible constant step-size rule than whatis known in the literature for a nonconvex $h$.
arxiv-13200-129 | Cell identification in whole-brain multiview images of neural activation | http://arxiv.org/pdf/1511.01168v1.pdf | author:Marco Paciscopi, Ludovico Silvestri, Francesco Saverio Pavone, Paolo Frasconi category:cs.CV J.3 published:2015-11-04 summary:We present a scalable method for brain cell identification in multiviewconfocal light sheet microscopy images. Our algorithmic pipeline includes ahierarchical registration approach and a novel multiview version of semanticdeconvolution that simultaneously enhance visibility of fluorescent cellbodies, equalize their contrast, and fuses adjacent views into a single 3Dimages on which cell identification is performed with mean shift. We present empirical results on a whole-brain image of an adult Arc-dVenusmouse acquired at 4micron resolution. Based on an annotated test volumecontaining 3278 cells, our algorithm achieves an $F_1$ measure of 0.89.
arxiv-13200-130 | Image based compensation for thickness variation in microscopy section series | http://arxiv.org/pdf/1511.01161v1.pdf | author:Philipp Hanslovsky, John A. Bogovic, C. Shan Xu, Kenneth J. Hayworth, Zhiyuan Lu, Harald F. Hess, Stephan Saalfeld category:cs.CV published:2015-11-03 summary:Serial block face scanning electron microscopy in combination with focusedion beam milling (FIB-SEM) has become a popular method for nanometer-resolutionisotropic imaging of neural and other cellular tissue with a planar field ofview of up to 100um. While FIB-SEM is particularly attractive for its highin-plane resolution, ion beam milling generates non-planar block faces andinhomogeneous z-spacing leading to distorted volume acquisitions. We extend ourprevious work on image-based z-spacing correction for serial section series todetermine a deformation field that varies within the xy-plane to account fornon-planarity. We show that our method identifies and corrects thesedistortions in real world FIB-SEM acquisitions and quantitatively assess itsprecision on virtual ground truth. Our method is available as an open sourceimplementation that is parallelized using the Spark framework enabling rapidprocessing of very large volumes.
arxiv-13200-131 | Robust Large-Scale Localization in 3D Point Clouds Revisited | http://arxiv.org/pdf/1511.01156v1.pdf | author:Fabian Tschopp, Marco Zorzi category:cs.CV published:2015-11-03 summary:We tackle the problem of getting a full 6-DOF pose estimation of a queryimage inside a given point cloud. This technical report re-evaluates thealgorithms proposed by Y. Li et al. "Worldwide Pose Estimation using 3D PointCloud". Our code computes poses from 3 or 4 points, with both known and unknownfocal length. The results can easily be displayed and analyzed with Meshlab. Wefound both advantages and shortcomings of the methods proposed. Furthermore,additional priors and parameters for point selection, RANSAC and pose qualityestimate (inlier test) are proposed and applied.
arxiv-13200-132 | Robust Registration of Calcium Images by Learned Contrast Synthesis | http://arxiv.org/pdf/1511.01154v1.pdf | author:John A. Bogovic, Philipp Hanslovsky, Allan Wong, Stephan Saalfeld category:cs.CV published:2015-11-03 summary:Multi-modal image registration is a challenging task that is vital to fusecomplementary signals for subsequent analyses. Despite much research into costfunctions addressing this challenge, there exist cases in which these areineffective. In this work, we show that (1) this is true for the registrationof in-vivo Drosophila brain volumes visualizing genetically encoded calciumindicators to an nc82 atlas and (2) that machine learning based contrastsynthesis can yield improvements. More specifically, the number of subjects forwhich the registration outright failed was greatly reduced (from 40% to 15%) byusing a synthesized image.
arxiv-13200-133 | Neuroevolution in Games: State of the Art and Open Challenges | http://arxiv.org/pdf/1410.7326v3.pdf | author:Sebastian Risi, Julian Togelius category:cs.NE published:2014-10-27 summary:This paper surveys research on applying neuroevolution (NE) to games. Inneuroevolution, artificial neural networks are trained through evolutionaryalgorithms, taking inspiration from the way biological brains evolved. Weanalyse the application of NE in games along five different axes, which are therole NE is chosen to play in a game, the different types of neural networksused, the way these networks are evolved, how the fitness is determined andwhat type of input the network receives. The article also highlights importantopen research challenges in the field.
arxiv-13200-134 | There is no fast lunch: an examination of the running speed of evolutionary algorithms in several languages | http://arxiv.org/pdf/1511.01088v1.pdf | author:Juan-J. Merelo, Pablo GarcÃ­a-SÃ¡nchez, Mario GarcÃ­a-Valdez, Israel Blancas category:cs.NE cs.PF published:2015-11-03 summary:It is quite usual when an evolutionary algorithm tool or library uses alanguage other than C, C++, Java or Matlab that a reviewer or the audiencequestions its usefulness based on the speed of those other languages,purportedly slower than the aforementioned ones. Despite speed being noteverything needed to design a useful evolutionary algorithm application, inthis paper we will measure the speed for several very basic evolutionaryalgorithm operations in several languages which use different virtual machinesand approaches, and prove that, in fact, there is no big difference in speedbetween interpreted and compiled languages, and that in some cases, interpretedlanguages such as JavaScript or Python can be faster than compiled languagessuch as Scala, making them worthy of use for evolutionary algorithmexperimentation.
arxiv-13200-135 | Understanding symmetries in deep networks | http://arxiv.org/pdf/1511.01029v1.pdf | author:Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla category:cs.LG cs.AI cs.CV published:2015-11-03 summary:Recent works have highlighted scale invariance or symmetry present in theweight space of a typical deep network and the adverse effect it has on theEuclidean gradient based stochastic gradient descent optimization. In thiswork, we show that a commonly used deep network, which uses convolution, batchnormalization, reLU, max-pooling, and sub-sampling pipeline, possess morecomplex forms of symmetry arising from scaling-based reparameterization of thenetwork weights. We propose to tackle the issue of the weight space symmetry byconstraining the filters to lie on the unit-norm manifold. Consequently,training the network boils down to using stochastic gradient descent updates onthe unit-norm manifold. Our empirical evidence based on the MNIST dataset showsthat the proposed updates improve the test performance beyond what is achievedwith batch normalization and without sacrificing the computational efficiencyof the weight updates.
arxiv-13200-136 | A Survey of Online Experiment Design with the Stochastic Multi-Armed Bandit | http://arxiv.org/pdf/1510.00757v4.pdf | author:Giuseppe Burtini, Jason Loeppky, Ramon Lawrence category:stat.ML cs.LG published:2015-10-02 summary:Adaptive and sequential experiment design is a well-studied area in numerousdomains. We survey and synthesize the work of the online statistical learningparadigm referred to as multi-armed bandits integrating the existing researchas a resource for a certain class of online experiments. We first explore thetraditional stochastic model of a multi-armed bandit, then explore a taxonomicscheme of complications to that model, for each complication relating it to aspecific requirement or consideration of the experiment design context.Finally, at the end of the paper, we present a table of known upper-bounds ofregret for all studied algorithms providing both perspectives for futuretheoretical work and a decision-making tool for practitioners looking fortheoretical guarantees.
arxiv-13200-137 | Highway Networks | http://arxiv.org/pdf/1505.00387v2.pdf | author:Rupesh Kumar Srivastava, Klaus Greff, JÃ¼rgen Schmidhuber category:cs.LG cs.NE 68T01 I.2.6; G.1.6 published:2015-05-03 summary:There is plenty of theoretical and empirical evidence that depth of neuralnetworks is a crucial ingredient for their success. However, network trainingbecomes more difficult with increasing depth and training of very deep networksremains an open problem. In this extended abstract, we introduce a newarchitecture designed to ease gradient-based training of very deep networks. Werefer to networks with this architecture as highway networks, since they allowunimpeded information flow across several layers on "information highways". Thearchitecture is characterized by the use of gating units which learn toregulate the flow of information through a network. Highway networks withhundreds of layers can be trained directly using stochastic gradient descentand with a variety of activation functions, opening up the possibility ofstudying extremely deep and efficient architectures.
arxiv-13200-138 | Basis Learning as an Algorithmic Primitive | http://arxiv.org/pdf/1411.1420v3.pdf | author:Mikhail Belkin, Luis Rademacher, James Voss category:cs.LG published:2014-11-05 summary:A number of important problems in theoretical computer science and machinelearning can be interpreted as recovering a certain basis. These includecertain tensor decompositions, Independent Component Analysis (ICA), spectralclustering and Gaussian mixture learning. Each of these problems reduces to aninstance of our general model, which we call a "Basis Encoding Function" (BEF).We show that learning a basis within this model can then be provably andefficiently achieved using a first order iteration algorithm (gradientiteration). Our algorithm goes beyond tensor methods, providing afunction-based generalization for a number of existing methods including theclassical matrix power method, the tensor power iteration as well ascumulant-based FastICA. Our framework also unifies the unusual phenomenonobserved in these domains that they can be solved using efficient non-convexoptimization. Specifically, we describe a class of BEFs such that their localmaxima on the unit sphere are in one-to-one correspondence with the basiselements. This description relies on a certain "hidden convexity" property ofthese functions. We provide a complete theoretical analysis of gradient iteration even whenthe BEF is perturbed. We show convergence and complexity bounds polynomial indimension and other relevant parameters, such as perturbation size. Ourperturbation results can be considered as a non-linear version of the classicalDavis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. Inaddition we show that our algorithm exhibits fast (superlinear) convergence andrelate the speed of convergence to the properties of the BEF. Moreover, thegradient iteration algorithm can be easily and efficiently implemented inpractice. Finally we apply our framework by providing the first provablealgorithm for recovery in a general perturbed ICA model.
arxiv-13200-139 | Convolutional Networks on Graphs for Learning Molecular Fingerprints | http://arxiv.org/pdf/1509.09292v2.pdf | author:David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael GÃ³mez-Bombarelli, Timothy Hirzel, AlÃ¡n Aspuru-Guzik, Ryan P. Adams category:cs.LG cs.NE stat.ML published:2015-09-30 summary:We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputsare graphs of arbitrary size and shape. The architecture we present generalizesstandard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have betterpredictive performance on a variety of tasks.
arxiv-13200-140 | Data Stream Classification using Random Feature Functions and Novel Method Combinations | http://arxiv.org/pdf/1511.00971v1.pdf | author:Diego MarrÃ³n, Jesse Read, Albert Bifet, Nacho Navarro category:cs.LG cs.NE published:2015-11-03 summary:Big Data streams are being generated in a faster, bigger, and morecommonplace. In this scenario, Hoeffding Trees are an established method forclassification. Several extensions exist, including high-performing ensemblesetups such as online and leveraging bagging. Also, $k$-nearest neighbors is apopular choice, with most extensions dealing with the inherent performancelimitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular,owing in part to the successes of deep learning. Although deep neural networkscan learn incrementally, they have so far proved too sensitive tohyper-parameter options and initial conditions to be considered an effective`off-the-shelf' data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbour,and gradient descent methods with a streaming preprocessing approach in theform of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which wetest on some large real-world datasets, and show the benefits of using GPUs fordata-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approachesthat we experiment with, highlighting important issues, and shed light onpromising future directions in approaches to data-stream classification.
arxiv-13200-141 | Face Detection with a 3D Model | http://arxiv.org/pdf/1404.3596v7.pdf | author:Adrian Barbu, Nathan Lay, Gary Gramajo category:cs.CV published:2014-04-14 summary:This paper presents a part-based face detection approach where the spatialrelationship between the face parts is represented by a hidden 3D model withsix parameters. The computational complexity of the search in the sixdimensional pose space is addressed by proposing meaningful 3D pose candidatesby image-based regression from detected face keypoint locations. The 3D posecandidates are evaluated using a parameter sensitive classifier based ondifference features relative to the 3D pose. A compatible subset of candidatesis then obtained by non-maximal suppression. Experiments on two standard facedetection datasets show that the proposed 3D model based approach obtainsresults comparable to or better than state of the art.
arxiv-13200-142 | Quantifying the Cognitive Extent of Science | http://arxiv.org/pdf/1511.00040v2.pdf | author:StaÅ¡a MilojeviÄ category:cs.DL astro-ph.IM cs.CL physics.soc-ph published:2015-10-30 summary:While the modern science is characterized by an exponential growth inscientific literature, the increase in publication volume clearly does notreflect the expansion of the cognitive boundaries of science. Nevertheless,most of the metrics for assessing the vitality of science or for making fundingand policy decisions are based on productivity. Similarly, the increasing levelof knowledge production by large science teams, whose results often enjoygreater visibility, does not necessarily mean that "big science" leads tocognitive expansion. Here we present a novel, big-data method to quantify theextents of cognitive domains of different bodies of scientific literatureindependently from publication volume, and apply it to 20 million articlespublished over 60-130 years in physics, astronomy, and biomedicine. The methodis based on the lexical diversity of titles of fixed quotas of researcharticles. Owing to large size of quotas, the method overcomes the inherentstochasticity of article titles to achieve <1% precision. We show that theperiods of cognitive growth do not necessarily coincide with the trends inpublication volume. Furthermore, we show that the articles produced by largerteams cover significantly smaller cognitive territory than (the same quota of)articles from smaller teams. Our findings provide a new perspective on the roleof small teams and individual researchers in expanding the cognitive boundariesof science. The proposed method of quantifying the extent of the cognitiveterritory can also be applied to study many other aspects of "science ofscience."
arxiv-13200-143 | Learning to Transduce with Unbounded Memory | http://arxiv.org/pdf/1506.02516v3.pdf | author:Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom category:cs.NE cs.CL cs.LG 68T05 published:2015-06-08 summary:Recently, strong results have been demonstrated by Deep Recurrent NeuralNetworks on natural language transduction problems. In this paper we explorethe representational power of these models using synthetic grammars designed toexhibit phenomena similar to those found in real transduction problems such asmachine translation. These experiments lead us to propose new memory-basedrecurrent networks that implement continuously differentiable analogues oftraditional data structures such as Stacks, Queues, and DeQues. We show thatthese architectures exhibit superior generalisation performance to Deep RNNsand are often able to learn the underlying generating algorithms in ourtransduction experiments.
arxiv-13200-144 | Properties of the Sample Mean in Graph Spaces and the Majorize-Minimize-Mean Algorithm | http://arxiv.org/pdf/1511.00871v1.pdf | author:Brijnesh J. Jain category:cs.CV cs.LG stat.ML published:2015-11-03 summary:One of the most fundamental concepts in statistics is the concept of samplemean. Properties of the sample mean that are well-defined in Euclidean spacesbecome unwieldy or even unclear in graph spaces. Open problems related to thesample mean of graphs include: non-existence, non-uniqueness, statisticalinconsistency, lack of convergence results of mean algorithms, non-existence ofmidpoints, and disparity to midpoints. We present conditions to resolve all sixproblems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments ongraph datasets representing images and molecules show that the MMM-Algorithmbest approximates a sample mean of graphs compared to six other meanalgorithms.
arxiv-13200-145 | PCA-Based Out-of-Sample Extension for Dimensionality Reduction | http://arxiv.org/pdf/1511.00831v1.pdf | author:Yariv Aizenbud, Amit Bermanis, Amir Averbuch category:stat.ML published:2015-11-03 summary:Dimensionality reduction methods are very common in the field of highdimensional data analysis. Typically, algorithms for dimensionality reductionare computationally expensive. Therefore, their applications for the analysisof massive amounts of data are impractical. For example, repeated computationsdue to accumulated data are computationally prohibitive. In this paper, anout-of-sample extension scheme, which is used as a complementary method fordimensionality reduction, is presented. We describe an algorithm which performsan out-of-sample extension to newly-arrived data points. Unlike other extensionalgorithms such as Nystr\"om algorithm, the proposed algorithm uses theintrinsic geometry of the data and properties for dimensionality reduction map.We prove that the error of the proposed algorithm is bounded. Additionally tothe out-of-sample extension, the algorithm provides a degree of the abnormalityof any newly-arrived data point.
arxiv-13200-146 | Explore no more: Improved high-probability regret bounds for non-stochastic bandits | http://arxiv.org/pdf/1506.03271v3.pdf | author:Gergely Neu category:cs.LG stat.ML published:2015-06-10 summary:This work addresses the problem of regret minimization in non-stochasticmulti-armed bandit problems, focusing on performance guarantees that hold withhigh probability. Such results are rather scarce in the literature sinceproving them requires a large deal of technical effort and significantmodifications to the standard, more intuitive algorithms that come only withguarantees that hold on expectation. One of these modifications is forcing thelearner to sample arms from the uniform distribution at least$\Omega(\sqrt{T})$ times over $T$ rounds, which can adversely affectperformance if many of the arms are suboptimal. While it is widely conjecturedthat this property is essential for proving high-probability regret bounds, weshow in this paper that it is possible to achieve such strong results withoutthis undesirable exploration component. Our result relies on a simple andintuitive loss-estimation strategy called Implicit eXploration (IX) that allowsa remarkably clean analysis. To demonstrate the flexibility of our technique,we derive several improved high-probability bounds for various extensions ofthe standard multi-armed bandit framework. Finally, we conduct a simpleexperiment that illustrates the robustness of our implicit explorationtechnique.
arxiv-13200-147 | Water Detection through Spatio-Temporal Invariant Descriptors | http://arxiv.org/pdf/1511.00472v2.pdf | author:Pascal Mettes, Robby T. Tan, Remco C. Veltkamp category:cs.CV published:2015-11-02 summary:In this work, we aim to segment and detect water in videos. Water detectionis beneficial for appllications such as video search, outdoor surveillance, andsystems such as unmanned ground vehicles and unmanned aerial vehicles. Thespecific problem, however, is less discussed compared to general texturerecognition. Here, we analyze several motion properties of water. First, wedescribe a video pre-processing step, to increase invariance against waterreflections and water colours. Second, we investigate the temporal and spatialproperties of water and derive corresponding local descriptors. The descriptorsare used to locally classify the presence of water and a binary water detectionmask is generated through spatio-temporal Markov Random Field regularization ofthe local classifications. Third, we introduce the Video Water Database,containing several hours of water and non-water videos, to validate ouralgorithm. Experimental evaluation on the Video Water Database and the DynTexdatabase indicates the effectiveness of the proposed algorithm, outperformingmultiple algorithms for dynamic texture recognition and material recognition byca. 5% and 15% respectively.
arxiv-13200-148 | Improved Deep Learning Baselines for Ubuntu Corpus Dialogs | http://arxiv.org/pdf/1510.03753v2.pdf | author:Rudolf Kadlec, Martin Schmid, Jan Kleindienst category:cs.CL published:2015-10-13 summary:This paper presents results of our experiments for the next utterance rankingon the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialogcorpus. First, we use an in-house implementation of previously reported modelsto do an independent evaluation using the same data. Second, we evaluate theperformances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, wecreate an ensemble by averaging predictions of multiple models. The ensemblefurther improves the performance and it achieves a state-of-the-art result forthe next utterance ranking on this dataset. Finally, we discuss our futureplans using this corpus.
arxiv-13200-149 | Artificial neural network approach for condition-based maintenance | http://arxiv.org/pdf/1601.03809v1.pdf | author:Mostafa Sayyed category:cs.NE cs.CY published:2015-11-03 summary:In this research, computerized maintenance management will be investigated.The rise of maintenance cost forced the research community to look for moreeffective ways to schedule maintenance operations. Using computerized models tocome up with optimal maintenance policy has led to better equipment utilizationand lower costs. This research adopts Condition-Based Maintenance model wherethe maintenance decision is generated based on equipment conditions. ArtificialNeural Network technique is proposed to capture and analyze equipment conditionsignals which lead to higher level of knowledge gathering. This knowledge isused to accurately estimate equipment failure time. Based on these estimations,an optimal maintenance management policy can be achieved.
arxiv-13200-150 | Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path | http://arxiv.org/pdf/1506.02903v3.pdf | author:Daniel Hsu, Aryeh Kontorovich, Csaba SzepesvÃ¡ri category:cs.LG stat.ML published:2015-06-09 summary:This article provides the first procedure for computing a fullydata-dependent interval that traps the mixing time $t_{\text{mix}}$ of a finitereversible ergodic Markov chain at a prescribed confidence level. The intervalis computed from a single finite-length sample path from the Markov chain, anddoes not require the knowledge of any parameters of the chain. This stands incontrast to previous approaches, which either only provide point estimates, orrequire a reset mechanism, or additional prior knowledge. The interval isconstructed around the relaxation time $t_{\text{relax}}$, which is stronglyrelated to the mixing time, and the width of the interval converges to zeroroughly at a $\sqrt{n}$ rate, where $n$ is the length of the sample path. Upperand lower bounds are given on the number of samples required to achieveconstant-factor multiplicative accuracy. The lower bounds indicate that, unlessfurther restrictions are placed on the chain, no procedure can achieve thisaccuracy level before seeing each state at least $\Omega(t_{\text{relax}})$times on the average. Finally, future directions of research are identified.
arxiv-13200-151 | A Practical Guide to Randomized Matrix Computations with MATLAB Implementations | http://arxiv.org/pdf/1505.07570v6.pdf | author:Shusen Wang category:cs.MS cs.LG published:2015-05-28 summary:Matrix operations such as matrix inversion, eigenvalue decomposition,singular value decomposition are ubiquitous in real-world applications.Unfortunately, many of these matrix operations so time and memory expensivethat they are prohibitive when the scale of data is large. In real-worldapplications, since the data themselves are noisy, machine-precision matrixoperations are not necessary at all, and one can sacrifice a reasonable amountof accuracy for computational efficiency. In recent years, a bunch of randomized algorithms have been devised to makematrix computations more scalable. Mahoney (2011) and Woodruff (2014) havewritten excellent but very technical reviews of the randomized algorithms.Differently, the focus of this manuscript is on intuition, algorithmderivation, and implementation. This manuscript should be accessible to peoplewith knowledge in elementary matrix algebra but unfamiliar with randomizedmatrix computations. The algorithms introduced in this manuscript are allsummarized in a user-friendly way, and they can be implemented in lines ofMATLAB code. The readers can easily follow the implementations even if they donot understand the maths and algorithms.
arxiv-13200-152 | PAC Learning-Based Verification and Model Synthesis | http://arxiv.org/pdf/1511.00754v1.pdf | author:Yu-Fang Chen, Chiao Hsieh, OndÅej LengÃ¡l, Tsung-Ju Lii, Ming-Hsien Tsai, Bow-Yaw Wang, Farn Wang category:cs.SE cs.LG cs.LO published:2015-11-03 summary:We introduce a novel technique for verification and model synthesis ofsequential programs. Our technique is based on learning a regular model of theset of feasible paths in a program, and testing whether this model contains anincorrect behavior. Exact learning algorithms require checking equivalencebetween the model and the program, which is a difficult problem, in generalundecidable. Our learning procedure is therefore based on the framework ofprobably approximately correct (PAC) learning, which uses sampling instead andprovides correctness guarantees expressed using the terms error probability andconfidence. Besides the verification result, our procedure also outputs themodel with the said correctness guarantees. Obtained preliminary experimentsshow encouraging results, in some cases even outperforming mature softwareverifiers.
arxiv-13200-153 | Data Generation as Sequential Decision Making | http://arxiv.org/pdf/1506.03504v3.pdf | author:Philip Bachman, Doina Precup category:cs.LG stat.ML published:2015-06-10 summary:We connect a broad class of generative models through their shared relianceon sequential decision making. Motivated by this view, we develop extensions toan existing model, and then explore the idea further in the context of dataimputation -- perhaps the simplest setting in which to investigate the relationbetween unconditional and conditional generative modelling. We formulate dataimputation as an MDP and develop models capable of representing effectivepolicies for it. We construct the models using neural networks and train themusing a form of guided policy search. Our models generate predictions throughan iterative process of feedback and refinement. We show that this approach canlearn effective policies for imputation problems of varying difficulty andacross multiple datasets.
arxiv-13200-154 | Learning Unfair Trading: a Market Manipulation Analysis From the Reinforcement Learning Perspective | http://arxiv.org/pdf/1511.00740v1.pdf | author:Enrique MartÃ­nez-Miranda, Peter McBurney, Matthew J. Howard category:q-fin.TR cs.LG published:2015-11-02 summary:Market manipulation is a strategy used by traders to alter the price offinancial securities. One type of manipulation is based on the process ofbuying or selling assets by using several trading strategies, among themspoofing is a popular strategy and is considered illegal by market regulators.Some promising tools have been developed to detect manipulation, but cases canstill be found in the markets. In this paper we model spoofing and pingingtrading, two strategies that differ in the legal background but share the sameelemental concept of market manipulation. We use a reinforcement learningframework within the full and partial observability of Markov decisionprocesses and analyse the underlying behaviour of the manipulators by findingthe causes of what encourages the traders to perform fraudulent activities.This reveals procedures to counter the problem that may be helpful to marketregulators as our model predicts the activity of spoofers.
arxiv-13200-155 | Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering | http://arxiv.org/pdf/1505.05612v3.pdf | author:Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu category:cs.CV cs.CL cs.LG published:2015-05-21 summary:In this paper, we present the mQA model, which is able to answer questionsabout the content of an image. The answer can be a sentence, a phrase or asingle word. Our model contains four components: a Long Short-Term Memory(LSTM) to extract the question representation, a Convolutional Neural Network(CNN) to extract the visual representation, an LSTM for storing the linguisticcontext in an answer, and a fusing component to combine the information fromthe first three components and generate the answer. We construct a FreestyleMultilingual Image Question Answering (FM-IQA) dataset to train and evaluateour mQA model. It contains over 150,000 images and 310,000 freestyle Chinesequestion-answer pairs and their English translations. The quality of thegenerated answers of our mQA model on this dataset is evaluated by human judgesthrough a Turing Test. Specifically, we mix the answers provided by humans andour model. The human judges need to distinguish our model from the human. Theywill also provide a score (i.e. 0, 1, 2, the larger the better) indicating thequality of the answer. We propose strategies to monitor the quality of thisevaluation process. The experiments show that in 64.7% of cases, the humanjudges cannot distinguish our model from humans. The average score is 1.454(1.918 for human). The details of this work, including the FM-IQA dataset, canbe found on the project page: http://idl.baidu.com/FM-IQA.html
arxiv-13200-156 | On the Number of Many-to-Many Alignments of N Sequences | http://arxiv.org/pdf/1511.00622v1.pdf | author:Steffen Eger category:math.CO cs.CL cs.DM published:2015-11-02 summary:We count the number of alignments of $N \ge 1$ sequences when match-up typesare from a specified set $S\subseteq \mathbb{N}^N$. Equivalently, we count thenumber of nonnegative integer matrices whose rows sum to a given fixed vectorand each of whose columns lie in $S$. We provide a new asymptotic formula forthe case $S=\{(s_1,\ldots,s_N) \:\: 1\le s_i\le 2\}$.
arxiv-13200-157 | From random walks to distances on unweighted graphs | http://arxiv.org/pdf/1511.00573v1.pdf | author:Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola category:stat.ML cs.AI cs.SI published:2015-11-02 summary:Large unweighted directed graphs are commonly used to capture relationsbetween entities. A fundamental problem in the analysis of such networks is toproperly define the similarity or dissimilarity between any two vertices.Despite the significance of this problem, statistical characterization of theproposed metrics has been limited. We introduce and develop a class oftechniques for analyzing random walks on graphs using stochastic calculus.Using these techniques we generalize results on the degeneracy of hitting timesand analyze a metric based on the Laplace transformed hitting time (LTHT). Themetric serves as a natural, provably well-behaved alternative to the expectedhitting time. We establish a general correspondence between hitting times ofthe Brownian motion and analogous hitting times on the graph. We show that theLTHT is consistent with respect to the underlying metric of a geometric graph,preserves clustering tendency, and remains robust against random addition ofnon-geometric edges. Tests on simulated and real-world data show that the LTHTmatches theoretical predictions and outperforms alternatives.
arxiv-13200-158 | Pixel-wise Segmentation of Street with Neural Networks | http://arxiv.org/pdf/1511.00513v1.pdf | author:Sebastian Bittel, Vitali Kaiser, Marvin Teichmann, Martin Thoma category:cs.CV published:2015-11-02 summary:Pixel-wise street segmentation of photographs taken from a driversperspective is important for self-driving cars and can also support otherobject recognition tasks. A framework called SST was developed to examine theaccuracy and execution time of different neural networks. The best neuralnetwork achieved an $F_1$-score of 89.5% with a simple feedforward neuralnetwork which trained to solve a regression task.
arxiv-13200-159 | Learning Sampling Distributions for Efficient Object Detection | http://arxiv.org/pdf/1508.05581v2.pdf | author:Yanwei Pang, Jiale Cao, Xuelong Li category:cs.CV cs.LG published:2015-08-23 summary:Object detection is an important task in computer vision and learningsystems. Multistage particle windows (MPW), proposed by Gualdi et al., is analgorithm of fast and accurate object detection. By sampling particle windowsfrom a proposal distribution (PD), MPW avoids exhaustively scanning the image.Despite its success, it is unknown how to determine the number of stages andthe number of particle windows in each stage. Moreover, it has to generate toomany particle windows in the initialization step and it redraws unnecessary toomany particle windows around object-like regions. In this paper, we attempt tosolve the problems of MPW. An important fact we used is that there is largeprobability for a randomly generated particle window not to contain the objectbecause the object is a sparse event relevant to the huge number of candidatewindows. Therefore, we design the proposal distribution so as to efficientlyreject the huge number of non-object windows. Specifically, we propose theconcepts of rejection, acceptance, and ambiguity windows and regions. Thiscontrasts to MPW which utilizes only on region of support. The PD of MPW isacceptance-oriented whereas the PD of our method (called iPW) isrejection-oriented. Experimental results on human and face detectiondemonstrate the efficiency and effectiveness of the iPW algorithm. The sourcecode is publicly accessible.
arxiv-13200-160 | Circle detection using isosceles triangles sampling | http://arxiv.org/pdf/1511.00461v1.pdf | author:Hanqing Zhang, Krister Wiklund, Magnus Andersson category:cs.CV I.5.4 published:2015-11-02 summary:Detection of circular objects in digital images is an important problem inseveral vision applications. Circle detection using randomized sampling hasbeen developed in recent years to reduce the computational intensity.Randomized sampling, however, is sensitive to noise that can lead to reducedaccuracy and false-positive candidates. This paper presents a new circledetection method based upon randomized isosceles triangles sampling to improvethe robustness of randomized circle detection in noisy conditions. It is shownthat the geometrical property of isosceles triangles provide a robust criterionto find relevant edge pixels and thereby efficiently provide an estimation ofthe circle center and radii. The estimated results given by the isoscelestriangles sampling from each connected component of edge map were analyzedusing a simple clustering approach for efficiency. To further improve on theaccuracy we applied a two-step refinement process using chords and linear errorcompensation with gradient information of the edge pixels. Extensiveexperiments using both synthetic and real images were presented and resultswere compared to leading state-of-the-art algorithms and showed that theproposed algorithm: are efficient in finding circles with a low number ofiterations; has high rejection rate of false-positive circle candidates; andhas high robustness against noise, making it adaptive and useful in many visionapplications.
arxiv-13200-161 | Reading Hidden Emotions: Spontaneous Micro-expression Spotting and Recognition | http://arxiv.org/pdf/1511.00423v1.pdf | author:Xiaobai Li, Xiaopeng Hong, Antti Moilanen, Xiaohua Huang, Tomas Pfister, Guoying Zhao, Matti PietikÃ¤inen category:cs.CV published:2015-11-02 summary:Micro-expressions (MEs) are rapid, involuntary facial expressions whichreveal emotions that people do not intend to show. Studying MEs is valuable asrecognizing them has many important applications, particularly in forensicscience and psychotherapy. However, analyzing spontaneous MEs is verychallenging due to their short duration and low intensity. Automatic MEanalysis includes two tasks: ME spotting and ME recognition.For ME spotting,previous studies have focused on posed rather than spontaneous videos. For MErecognition, the performance of previous studies is low. To address thesechallenges, we make the following contributions: (i) We propose the firstmethod for spotting spontaneous MEs in long videos (by exploiting featuredifference contrast). This method is training free and works on arbitraryunseen videos. (ii) We present an advanced ME recognition framework, whichoutperforms previous work by a large margin on two challenging spontaneous MEdatabases (SMIC and CASMEII). (iii) We propose the first automatic ME analysissystem (MESR), which can spot and recognize MEs from spontaneous video data.Finally, we show that our method achieves comparable performance to humans atthis very challenging task, and outperforms humans in the ME recognition taskby a large margin.
arxiv-13200-162 | Bidirectional Recurrent Neural Networks as Generative Models - Reconstructing Gaps in Time Series | http://arxiv.org/pdf/1504.01575v3.pdf | author:Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo KÃ¤rkkÃ¤inen, Akos Vetek, Juha Karhunen category:cs.LG cs.NE published:2015-04-07 summary:Bidirectional recurrent neural networks (RNN) are trained to predict both inthe positive and negative time directions simultaneously. They have not beenused commonly in unsupervised tasks, because a probabilistic interpretation ofthe model has been difficult. Recently, two different frameworks, GSN and NADE,provide a connection between reconstruction and probabilistic modeling, whichmakes the interpretation possible. As far as we know, neither GSN or NADE havebeen studied in the context of time series before. As an example of anunsupervised task, we study the problem of filling in gaps in high-dimensionaltime series with complex dynamics. Although unidirectional RNNs have recentlybeen trained successfully to model such time series, inference in the negativetime direction is non-trivial. We propose two probabilistic interpretations ofbidirectional RNNs that can be used to reconstruct missing gaps efficiently.Our experiments on text data show that both proposed methods are much moreaccurate than unidirectional reconstructions, although a bit less accurate thana computationally complex bidirectional Bayesian inference on theunidirectional RNN. We also provide results on music data for which theBayesian inference is computationally infeasible, demonstrating the scalabilityof the proposed methods.
arxiv-13200-163 | Automatic Prosody Prediction for Chinese Speech Synthesis using BLSTM-RNN and Embedding Features | http://arxiv.org/pdf/1511.00360v1.pdf | author:Chuang Ding, Lei Xie, Jie Yan, Weini Zhang, Yang Liu category:cs.CL cs.SD published:2015-11-02 summary:Prosody affects the naturalness and intelligibility of speech. However,automatic prosody prediction from text for Chinese speech synthesis is still agreat challenge and the traditional conditional random fields (CRF) basedmethod always heavily relies on feature engineering. In this paper, we proposeto use neural networks to predict prosodic boundary labels directly fromChinese characters without any feature engineering. Experimental results showthat stacking feed-forward and bidirectional long short-term memory (BLSTM)recurrent network layers achieves superior performance over the CRF-basedmethod. The embedding features learned from raw text further enhance theperformance.
arxiv-13200-164 | Stochastic Top-k ListNet | http://arxiv.org/pdf/1511.00271v1.pdf | author:Tianyi Luo, Dong Wang, Rong Liu, Yiqiao Pan category:cs.IR cs.LG published:2015-11-01 summary:ListNet is a well-known listwise learning to rank model and has gained muchattention in recent years. A particular problem of ListNet, however, is thehigh computation complexity in model training, mainly due to the large numberof object permutations involved in computing the gradients. This paper proposesa stochastic ListNet approach which computes the gradient within a boundedpermutation subset. It significantly reduces the computation complexity ofmodel training and allows extension to Top-k models, which is impossible withthe conventional implementation based on full-set permutations. Meanwhile, thenew approach utilizes partial ranking information of human labels, which helpsimprove model quality. Our experiments demonstrated that the stochastic ListNetmethod indeed leads to better ranking performance and speeds up the modeltraining remarkably.
arxiv-13200-165 | Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding | http://arxiv.org/pdf/1504.01255v3.pdf | author:Rie Johnson, Tong Zhang category:stat.ML cs.CL cs.LG published:2015-04-06 summary:This paper presents a new semi-supervised framework with convolutional neuralnetworks (CNNs) for text categorization. Unlike the previous approaches thatrely on word embeddings, our method learns embeddings of small text regionsfrom unlabeled data for integration into a supervised CNN. The proposed schemefor embedding learning is based on the idea of two-view semi-supervisedlearning, which is intended to be useful for the task of interest even thoughthe training is done on unlabeled data. Our models achieve better results thanprevious approaches on sentiment classification and topic classification tasks.
arxiv-13200-166 | LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization | http://arxiv.org/pdf/1511.00221v1.pdf | author:Ilya Loshchilov category:cs.NE math.OC published:2015-11-01 summary:The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is oftenconsidered to be the method of choice for continuous optimization when first-and/or second- order information is available. However, the use of L-BFGS canbe complicated in a black-box scenario where gradient information is notavailable and therefore should be numerically estimated. The accuracy of thisestimation, obtained by finite difference methods, is often problem-dependentthat may lead to premature convergence of the algorithm. In this paper, we demonstrate an alternative to L-BFGS, the limited memoryCovariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov(2014). The LM-CMA is a stochastic derivative-free algorithm for numericaloptimization of non-linear, non-convex optimization problems. Inspired by theL-BFGS, the LM-CMA samples candidate solutions according to a covariance matrixreproduced from $m$ direction vectors selected during the optimization process.The decomposition of the covariance matrix into Cholesky factors allows toreduce the memory complexity to $O(mn)$, where $n$ is the number of decisionvariables. The time complexity of sampling one candidate solution is also$O(mn)$, but scales as only about 25 scalar-vector multiplications in practice.The algorithm has an important property of invariance w.r.t. strictlyincreasing transformations of the objective function, such transformations donot compromise its ability to approach the optimum. The LM-CMA outperforms theoriginal CMA-ES and its large scale versions on non-separable ill-conditionedproblems with a factor increasing with problem dimension. Invariance propertiesof the algorithm do not prevent it from demonstrating a comparable performanceto L-BFGS on non-trivial large scale smooth and nonsmooth optimizationproblems.
arxiv-13200-167 | End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture | http://arxiv.org/pdf/1508.03398v2.pdf | author:Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, Li Deng category:cs.LG published:2015-08-14 summary:We develop a fully discriminative learning approach for supervised LatentDirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), whichmaximizes the posterior probability of the prediction variable given the inputdocument. Different from traditional variational learning or Gibbs samplingapproaches, the proposed learning method applies (i) the mirror descentalgorithm for maximum a posterior inference and (ii) back propagation over adeep architecture together with stochastic gradient/mirror descent for modelparameter estimation, leading to scalable and end-to-end discriminativelearning of the model. As a byproduct, we also apply this technique to developa new learning method for the traditional unsupervised LDA model (i.e.,BP-LDA). Experimental results on three real-world regression and classificationtasks show that the proposed methods significantly outperform the previoussupervised topic models, neural networks, and is on par with deep neuralnetworks.
arxiv-13200-168 | A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding | http://arxiv.org/pdf/1511.00215v1.pdf | author:Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao category:cs.CL published:2015-11-01 summary:Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) hasbeen shown to be very effective for modeling and predicting sequential data,e.g. speech utterances or handwritten documents. In this study, we propose touse BLSTM-RNN for a unified tagging solution that can be applied to varioustagging tasks including part-of-speech tagging, chunking and named entityrecognition. Instead of exploiting specific features carefully optimized foreach task, our solution only uses one set of task-independent features andinternal representations learnt from unlabeled text for all tasks.Requiring notask specific knowledge or sophisticated feature engineering, our approach getsnearly state-of-the-art performance in all these three tagging tasks.
arxiv-13200-169 | On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units | http://arxiv.org/pdf/1508.00330v2.pdf | author:Zhibin Liao, Gustavo Carneiro category:cs.CV cs.LG cs.NE published:2015-08-03 summary:Deep feedforward neural networks with piecewise linear activations arecurrently producing the state-of-the-art results in several public datasets.The combination of deep learning models and piecewise linear activationfunctions allows for the estimation of exponentially complex functions with theuse of a large number of subnetworks specialized in the classification ofsimilar input examples. During the training process, these subnetworks avoidoverfitting with an implicit regularization scheme based on the fact that theymust share their parameters with other subnetworks. Using this framework, wehave made an empirical observation that can improve even more the performanceof such models. We notice that these models assume a balanced initialdistribution of data points with respect to the domain of the piecewise linearactivation function. If that assumption is violated, then the piecewise linearactivation units can degenerate into purely linear activation units, which canresult in a significant reduction of their capacity to learn complex functions.Furthermore, as the number of model layers increases, this unbalanced initialdistribution makes the model ill-conditioned. Therefore, we propose theintroduction of batch normalisation units into deep feedforward neural networkswith piecewise linear activations, which drives a more balanced use of theseactivation units, where each region of the activation function is trained witha relatively large proportion of training samples. Also, this batchnormalisation promotes the pre-conditioning of very deep learning models. Weshow that by introducing maxout and batch normalisation units to the network innetwork model results in a model that produces classification results that arebetter than or comparable to the current state of the art in CIFAR-10,CIFAR-100, MNIST, and SVHN datasets.
arxiv-13200-170 | ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient | http://arxiv.org/pdf/1412.7419v5.pdf | author:Caglar Gulcehre, Marcin Moczulski, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2014-12-23 summary:Stochastic gradient algorithms have been the main focus of large-scalelearning problems and they led to important successes in machine learning. Theconvergence of SGD depends on the careful choice of learning rate and theamount of the noise in stochastic estimates of the gradients. In this paper, wepropose a new adaptive learning rate algorithm, which utilizes curvatureinformation for automatically tuning the learning rates. The information aboutthe element-wise curvature of the loss function is estimated from the localstatistics of the stochastic first order gradients. We further propose a newvariance reduction technique to speed up the convergence. In our preliminaryexperiments with deep neural networks, we obtained better performance comparedto the popular stochastic gradient algorithms.
arxiv-13200-171 | A Complete Recipe for Stochastic Gradient MCMC | http://arxiv.org/pdf/1506.04696v2.pdf | author:Yi-An Ma, Tianqi Chen, Emily B. Fox category:math.ST stat.ME stat.ML stat.TH published:2015-06-15 summary:Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuousdynamics to define a transition kernel that efficiently explores a targetdistribution. In tandem, a focus has been on devising scalable variants thatsubsample the data and use stochastic gradients in place of full-data gradientsin the dynamic simulations. However, such stochastic gradient MCMC samplershave lagged behind their full-data counterparts in terms of the complexity ofdynamics considered since proving convergence in the presence of the stochasticgradient noise is non-trivial. Even with simple dynamics, significant physicalintuition is often required to modify the dynamical system to account for thestochastic gradient noise. In this paper, we provide a general recipe forconstructing MCMC samplers--including stochastic gradient versions--based oncontinuous Markov processes specified via two matrices. We constructively provethat the framework is complete. That is, any continuous Markov process thatprovides samples from the target distribution can be written in our framework.We show how previous continuous-dynamic samplers can be trivially "reinvented"in our framework, avoiding the complicated sampler-specific proofs. We likewiseuse our recipe to straightforwardly propose a new state-adaptive sampler:stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experimentson simulated data and a streaming Wikipedia analysis demonstrate that theproposed SGRHMC sampler inherits the benefits of Riemann HMC, with thescalability of stochastic gradient methods.
arxiv-13200-172 | Dropout as a Bayesian Approximation: Appendix | http://arxiv.org/pdf/1506.02157v4.pdf | author:Yarin Gal, Zoubin Ghahramani category:stat.ML published:2015-06-06 summary:We show that a neural network with arbitrary depth and non-linearities, withdropout applied before every weight layer, is mathematically equivalent to anapproximation to a well known Bayesian model. This interpretation offers anexplanation to some of dropout's key properties, such as its robustness toover-fitting. Our interpretation allows us to reason about uncertainty in deeplearning, and allows the introduction of the Bayesian machinery into existingdeep learning frameworks in a principled way. This document is an appendix for the main paper "Dropout as a BayesianApproximation: Representing Model Uncertainty in Deep Learning" by Gal andGhahramani, 2015.
arxiv-13200-173 | Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning | http://arxiv.org/pdf/1506.02142v4.pdf | author:Yarin Gal, Zoubin Ghahramani category:stat.ML cs.LG published:2015-06-06 summary:Deep learning tools have gained tremendous attention in applied machinelearning. However such tools for regression and classification do not capturemodel uncertainty. In comparison, Bayesian models offer a mathematicallygrounded framework to reason about model uncertainty, but usually come with aprohibitive computational cost. In this paper we develop a new theoreticalframework casting dropout training in deep neural networks (NNs) as approximateBayesian inference in deep Gaussian processes. A direct result of this theorygives us tools to model uncertainty with dropout NNs -- extracting informationfrom existing models that has been thrown away so far. This mitigates theproblem of representing uncertainty in deep learning without sacrificing eithercomputational complexity or test accuracy. We perform an extensive study of theproperties of dropout's uncertainty. Various network architectures andnon-linearities are assessed on tasks of regression and classification, usingMNIST as an example. We show a considerable improvement in predictivelog-likelihood and RMSE compared to existing state-of-the-art methods, andfinish by using dropout's uncertainty in deep reinforcement learning.
arxiv-13200-174 | Support Vector Regression, Smooth Splines, and Time Series Prediction | http://arxiv.org/pdf/1511.00158v1.pdf | author:Raymundo Navarrete, Divakar Viswanath category:stat.ML cs.LG published:2015-10-31 summary:Delay coordinates and support vector regression are among the techniquescommonly used for time series prediction. We show that the combination of thesetwo techniques leads to systematic error that obstructs convergence. Apreliminary step of spline smoothing restores convergence and leads topredictions that are consistently more accurate, typically by about a factor of$2$ or so. Since the algorithm without spline smoothing is not convergent, theimprovement in accuracy can even be as high as a factor of $100$. Assuminglocal isotropy, the systematic error in the absence of spline smoothing isestimated to be $d\sigma^{2}L/2$, where $d$ is the embedding dimension,$\sigma^{2}$ is the variance of Gaussian noise in the signal, and $L$ is aglobal bound on the Hessian of the exact predictor. The smooth spline, althoughvery effective, is shown not to have even first order accuracy, unless thenoise is unusually mild. The lack of order of accuracy implies that attempts totake advantage of invariance in time to enhance fidelity of learning areunlikely to be successful.
arxiv-13200-175 | Blitzkriging: Kronecker-structured Stochastic Gaussian Processes | http://arxiv.org/pdf/1510.07965v2.pdf | author:Thomas Nickson, Tom Gunter, Chris Lloyd, Michael A Osborne, Stephen Roberts category:stat.ML published:2015-10-27 summary:We present Blitzkriging, a new approach to fast inference for Gaussianprocesses, applicable to regression, optimisation and classification.State-of-the-art (stochastic) inference for Gaussian processes on very largedatasets scales cubically in the number of 'inducing inputs', variablesintroduced to factorise the model. Blitzkriging shares state-of-the-art scalingwith data, but reduces the scaling in the number of inducing points toapproximately linear. Further, in contrast to other methods, Blitzkriging: doesnot force the data to conform to any particular structure (includinggrid-like); reduces reliance on error-prone optimisation of inducing pointlocations; and is able to learn rich (covariance) structure from the data. Wedemonstrate the benefits of our approach on real data in regression,time-series prediction and signal-interpolation experiments.
arxiv-13200-176 | Convergence of Proximal-Gradient Stochastic Variational Inference under Non-Decreasing Step-Size Sequence | http://arxiv.org/pdf/1511.00146v1.pdf | author:Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, Masashi Sugiyama category:stat.ML cs.LG stat.CO published:2015-10-31 summary:Stochastic approximation methods have recently gained popularity forvariational inference, but many existing approaches treat them as "black-box"tools. Thus, they often do not take advantage of the geometry of the posteriorand usually require a decreasing sequence of step-sizes (which converges slowlyin practice). We introduce a new stochastic-approximation method that uses aproximal-gradient framework. Our method exploits the geometry and structure ofthe variational lower bound, and contains many existing methods, such asstochastic variational inference, as a special case. We establish theconvergence of our method under a "non-decreasing" step-size schedule, whichhas both theoretical and practical advantages. We consider setting thestep-size based on the continuity of the objective and the geometry of theposterior, and show that our method gives a faster rate of convergence forvariational-Gaussian inference than existing stochastic methods.
arxiv-13200-177 | Efficient Face Alignment via Locality-constrained Representation for Robust Recognition | http://arxiv.org/pdf/1507.07073v2.pdf | author:Yandong Wen, Weiyang Liu, Meng Yang, Zhifeng Li category:cs.CV published:2015-07-25 summary:Practical face recognition has been studied in the past decades, but stillremains an open challenge. Current prevailing approaches have already achievedsubstantial breakthroughs in recognition accuracy. However, their performanceusually drops dramatically if face samples are severely misaligned. To addressthis problem, we propose a highly efficient misalignment-robustlocality-constrained representation (MRLR) algorithm for practical real-timeface recognition. Specifically, the locality constraint that activates the mostcorrelated atoms and suppresses the uncorrelated ones, is applied to constructthe dictionary for face alignment. Then we simultaneously align the warped faceand update the locality-constrained dictionary, eventually obtaining the finalalignment. Moreover, we make use of the block structure to accelerate thederived analytical solution. Experimental results on public data sets show thatMRLR significantly outperforms several state-of-the-art approaches in terms ofefficiency and scalability with even better performance.
arxiv-13200-178 | Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation | http://arxiv.org/pdf/1511.00111v1.pdf | author:M. Abdelsamea category:cs.CV published:2015-10-31 summary:Image segmentation is the problem of partitioning an image into differentsubsets, where each subset may have a different characterization in terms ofcolor, intensity, texture, and/or other features. Segmentation is a fundamentalcomponent of image processing, and plays a significant role in computer vision,object recognition, and object tracking. Active Contour Models (ACMs)constitute a powerful energy-based minimization framework for imagesegmentation, which relies on the concept of contour evolution. Starting froman initial guess, the contour is evolved with the aim of approximating betterand better the actual object boundary. Handling complex images in an efficient,effective, and robust way is a real challenge, especially in the presence ofintensity inhomogeneity, overlap between the foreground/background intensitydistributions, objects characterized by many different intensities, and/oradditive noise. In this thesis, to deal with these challenges, we propose anumber of image segmentation models relying on variational level set methodsand specific kinds of neural networks, to handle complex images in bothsupervised and unsupervised ways. Experimental results demonstrate the highaccuracy of the segmentation results, obtained by the proposed models onvarious benchmark synthetic and real images compared with state-of-the-artactive contour models.
arxiv-13200-179 | Fast Neuromimetic Object Recognition using FPGA Outperforms GPU Implementations | http://arxiv.org/pdf/1511.00100v1.pdf | author:Garrick Orchard, Jacob G. Martin, R. Jacob Vogelstein, Ralph Etienne-Cummings category:cs.CV published:2015-10-31 summary:Recognition of objects in still images has traditionally been regarded as adifficult computational problem. Although modern automated methods for visualobject recognition have achieved steadily increasing recognition accuracy, eventhe most advanced computational vision approaches are unable to obtainperformance equal to that of humans. This has led to the creation of manybiologically-inspired models of visual object recognition, among them the HMAXmodel. HMAX is traditionally known to achieve high accuracy in visual objectrecognition tasks at the expense of significant computational complexity.Increasing complexity, in turn, increases computation time, reducing the numberof images that can be processed per unit time. In this paper we describe howthe computationally intensive, biologically inspired HMAX model for visualobject recognition can be modified for implementation on a commercial FieldProgrammable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluationboard with XC6VLX240T FPGA. We show that with minor modifications to thetraditional HMAX model we can perform recognition on images of size 128x128pixels at a rate of 190 images per second with a less than 1% loss inrecognition accuracy in both binary and multi-class visual object recognitiontasks.
arxiv-13200-180 | Sketch-based Image Retrieval from Millions of Images under Rotation, Translation and Scale Variations | http://arxiv.org/pdf/1511.00099v1.pdf | author:Sarthak Parui, Anurag Mittal category:cs.CV cs.IR published:2015-10-31 summary:Proliferation of touch-based devices has made sketch-based image retrievalpractical. While many methods exist for sketch-based object detection/imageretrieval on small datasets, relatively less work has been done on large(web)-scale image retrieval. In this paper, we present an efficient approachfor image retrieval from millions of images based on user-drawn sketches.Unlike existing methods for this problem which are sensitive to eventranslation or scale variations, our method handles rotation, translation,scale (i.e. a similarity transformation) and small deformations. The objectboundaries are represented as chains of connected segments and the databaseimages are pre-processed to obtain such chains that have a high chance ofcontaining the object. This is accomplished using two approaches in this work:a) extracting long chains in contour segment networks and b) extractingboundaries of segmented object proposals. These chains are then represented bysimilarity-invariant variable length descriptors. Descriptor similarities arecomputed by a fast Dynamic Programming-based partial matching algorithm. Thismatching mechanism is used to generate a hierarchical k-medoids based indexingstructure for the extracted chains of all database images in an offline processwhich is used to efficiently retrieve a small set of possible matched imagesfor query chains. Finally, a geometric verification step is employed to testgeometric consistency of multiple chain matches to improve results. Qualitativeand quantitative results clearly demonstrate superiority of the approach overexisting methods.
arxiv-13200-181 | Semantic Cross-View Matching | http://arxiv.org/pdf/1511.00098v1.pdf | author:Francesco Castaldo, Amir Zamir, Roland Angst, Francesco Palmieri, Silvio Savarese category:cs.CV published:2015-10-31 summary:Matching cross-view images is challenging because the appearance andviewpoints are significantly different. While low-level features based ongradient orientations or filter responses can drastically vary with suchchanges in viewpoint, semantic information of images however shows an invariantcharacteristic in this respect. Consequently, semantically labeled regions canbe used for performing cross-view matching. In this paper, we therefore explorethis idea and propose an automatic method for detecting and representing thesemantic information of an RGB image with the goal of performing cross-viewmatching with a (non-RGB) geographic information system (GIS). A segmentedimage forms the input to our system with segments assigned to semantic conceptssuch as traffic signs, lakes, roads, foliage, etc. We design a descriptor torobustly capture both, the presence of semantic concepts and the spatial layoutof those segments. Pairwise distances between the descriptors extracted fromthe GIS map and the query image are then used to generate a shortlist of themost promising locations with similar semantic concepts in a consistent spatiallayout. An experimental evaluation with challenging query images and a largeurban area shows promising results.
arxiv-13200-182 | Bioinspired Visual Motion Estimation | http://arxiv.org/pdf/1511.00096v1.pdf | author:Garrick Orchard, Ralph Etienne-Cummings category:cs.CV published:2015-10-31 summary:Visual motion estimation is a computationally intensive, but important taskfor sighted animals. Replicating the robustness and efficiency of biologicalvisual motion estimation in artificial systems would significantly enhance thecapabilities of future robotic agents. 25 years ago, in this very journal,Carver Mead outlined his argument for replicating biological processing insilicon circuits. His vision served as the foundation for the field ofneuromorphic engineering, which has experienced a rapid growth in interest overrecent years as the ideas and technologies mature. Replicating biologicalvisual sensing was one of the first tasks attempted in the neuromorphic field.In this paper we focus specifically on the task of visual motion estimation. Wedescribe the task itself, present the progression of works from the early firstattempts through to the modern day state-of-the-art, and provide an outlook forfuture directions in the field.
arxiv-13200-183 | Copula variational inference | http://arxiv.org/pdf/1506.03159v2.pdf | author:Dustin Tran, David M. Blei, Edoardo M. Airoldi category:stat.ML cs.LG stat.CO stat.ME published:2015-06-10 summary:We develop a general variational inference method that preserves dependencyamong the latent variables. Our method uses copulas to augment the families ofdistributions used in mean-field and structured approximations. Copulas modelthe dependency that is not captured by the original variational distribution,and thus the augmented variational family guarantees better approximations tothe posterior. With stochastic optimization, inference on the augmenteddistribution is scalable. Furthermore, our strategy is generic: it can beapplied to any inference procedure that currently uses the mean-field orstructured approach. Copula variational inference has many advantages: itreduces bias; it is less sensitive to local optima; it is less sensitive tohyperparameters; and it helps characterize and interpret the dependency amongthe latent variables.
arxiv-13200-184 | Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm | http://arxiv.org/pdf/1508.06235v4.pdf | author:Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang category:stat.ML cs.AI cs.LG stat.CO published:2015-08-25 summary:In this paper, we propose a model-based clustering method (TVClust) thatrobustly incorporates noisy side information as soft-constraints and aims toseek a consensus between side information and the observed data. Our method isbased on a nonparametric Bayesian hierarchical model that combines theprobabilistic model for the data instance and the one for the side-information.An efficient Gibbs sampling algorithm is proposed for posterior inference.Using the small-variance asymptotics of our probabilistic model, we then derivea new deterministic clustering algorithm (RDP-means). It can be viewed as anextension of K-means that allows for the inclusion of side information and hasthe additional property that the number of clusters does not need to bespecified a priori. Empirical studies have been carried out to compare our workwith many constrained clustering algorithms from the literature on both avariety of data sets and under a variety of conditions such as using noisy sideinformation and erroneous k values. The results of our experiments show strongresults for our probabilistic and deterministic approaches under theseconditions when compared to other algorithms in the literature.
arxiv-13200-185 | Gaussian Process Random Fields | http://arxiv.org/pdf/1511.00054v1.pdf | author:David A. Moore, Stuart J. Russell category:cs.LG stat.ML published:2015-10-31 summary:Gaussian processes have been successful in both supervised and unsupervisedmachine learning tasks, but their computational complexity has constrainedpractical applications. We introduce a new approximation for large-scaleGaussian processes, the Gaussian Process Random Field (GPRF), in which localGPs are coupled via pairwise potentials. The GPRF likelihood is a simple,tractable, and parallelizeable approximation to the full GP marginallikelihood, enabling latent variable modeling and hyperparameter selection onlarge datasets. We demonstrate its effectiveness on synthetic spatial data aswell as a real-world application to seismic event location.
arxiv-13200-186 | The Pareto Regret Frontier for Bandits | http://arxiv.org/pdf/1511.00048v1.pdf | author:Tor Lattimore category:cs.LG published:2015-10-30 summary:Given a multi-armed bandit problem it may be desirable to achieve asmaller-than-usual worst-case regret for some special actions. I show that theprice for such unbalanced worst-case regret guarantees is rather high.Specifically, if an algorithm enjoys a worst-case regret of B with respect tosome action, then there must exist another action for which the worst-caseregret is at least {\Omega}(nK/B), where n is the horizon and K the number ofactions. I also give upper bounds in both the stochastic and adversarialsettings showing that this result cannot be improved. For the stochastic casethe pareto regret frontier is characterised exactly up to constant factors.
arxiv-13200-187 | Learning both Weights and Connections for Efficient Neural Networks | http://arxiv.org/pdf/1506.02626v3.pdf | author:Song Han, Jeff Pool, John Tran, William J. Dally category:cs.NE cs.CV cs.LG published:2015-06-08 summary:Neural networks are both computationally intensive and memory intensive,making them difficult to deploy on embedded systems. Also, conventionalnetworks fix the architecture before training starts; as a result, trainingcannot improve the architecture. To address these limitations, we describe amethod to reduce the storage and computation required by neural networks by anorder of magnitude without affecting their accuracy by learning only theimportant connections. Our method prunes redundant connections using athree-step method. First, we train the network to learn which connections areimportant. Next, we prune the unimportant connections. Finally, we retrain thenetwork to fine tune the weights of the remaining connections. On the ImageNetdataset, our method reduced the number of parameters of AlexNet by a factor of9x, from 61 million to 6.7 million, without incurring accuracy loss. Similarexperiments with VGG-16 found that the number of parameters can be reduced by13x, from 138 million to 10.3 million, again with no loss of accuracy.
arxiv-13200-188 | Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks | http://arxiv.org/pdf/1510.05198v2.pdf | author:Jiwei Li, Alan Ritter, Dan Jurafsky category:cs.SI cs.CL published:2015-10-18 summary:Inferring latent attributes of people online is an important social computingtask, but requires integrating the many heterogeneous sources of informationavailable on the web. We propose to learn individual representations of peopleusing neural nets to integrate information from social media. The algorithm isable to combine any kind of cues, such as the text a person writes, theperson's attributes (e.g. gender, employer, school, location) and socialrelations to other people (e.g., friendship, marriage), using global inferenceto infer missing attributes from noisy cues. The resulting latentrepresentations capture homophily: people who have similar attributes, arerelated socially, or write similar text are closer in vector space. We showthat these learned representations offer good performance at solving fourimportant tasks in social media inference on Twitter: predicting (1) gender,(2) occupation, (3) location, and (4) friendships for users, and that weachieve the best performance by integrating all these signals. Our approachscales to large datasets, using parallel stochastic gradient descent forlearning. The resulting representations can be used as general features in andhave the potential to benefit a large number of downstream tasks like linkprediction, community detection, or reasoning over social networks, discoveringfor example the high probability that a New York City resident is a fan of theNew York Knicks, or the greater preference for iPhones by computerprofessionals than legal professionals.
arxiv-13200-189 | Learning Causal Graphs with Small Interventions | http://arxiv.org/pdf/1511.00041v1.pdf | author:Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath category:cs.AI cs.IT cs.LG math.IT stat.ML published:2015-10-30 summary:We consider the problem of learning causal networks with interventions, wheneach intervention is limited in size under Pearl's Structural Equation Modelwith independent errors (SEM-IE). The objective is to minimize the number ofexperiments to discover the causal directions of all the edges in a causalgraph. Previous work has focused on the use of separating systems for completegraphs for this task. We prove that any deterministic adaptive algorithm needsto be a separating system in order to learn complete graphs in the worst case.In addition, we present a novel separating system construction, whose size isclose to optimal and is arguably simpler than previous work in combinatorics.We also develop a novel information theoretic lower bound on the number ofinterventions that applies in full generality, including for randomizedadaptive learning algorithms. For general chordal graphs, we derive worst case lower bounds on the numberof interventions. Building on observations about induced trees, we give a newdeterministic adaptive algorithm to learn directions on any chordal skeletoncompletely. In the worst case, our achievable scheme is an$\alpha$-approximation algorithm where $\alpha$ is the independence number ofthe graph. We also show that there exist graph classes for which the sufficientnumber of experiments is close to the lower bound. In the other extreme, thereare graph classes for which the required number of experiments ismultiplicatively $\alpha$ away from our lower bound. In simulations, our algorithm almost always performs very close to the lowerbound, while the approach based on separating systems for complete graphs issignificantly worse for random chordal graphs.
arxiv-13200-190 | Multicanonical Stochastic Variational Inference | http://arxiv.org/pdf/1411.1810v3.pdf | author:Stephan Mandt, James McInerney, Farhan Abrol, Rajesh Ranganath, David Blei category:stat.ML cs.LG published:2014-11-07 summary:Stochastic variational inference (SVI) enables approximate posteriorinference with large data sets for otherwise intractable models, but like allvariational inference algorithms it suffers from local optima. Deterministicannealing, which we formulate here for the generic class of conditionallyconjugate exponential family models, uses a temperature parameter thatdeterministically deforms the objective, and reduce this parameter over thecourse of the optimization to recover the original variational set-up. Awell-known drawback in annealing approaches is the choice of the annealingschedule. We therefore introduce multicanonical variational inference (MVI), avariational algorithm that operates at several annealing temperaturessimultaneously. This algorithm gives us adaptive annealing schedules. Comparedto the traditional SVI algorithm, both approaches find improved predictivelikelihoods on held-out data, with MVI being close to the best-tuned annealingschedule.
arxiv-13200-191 | Online Gradient Boosting | http://arxiv.org/pdf/1506.04820v2.pdf | author:Alina Beygelzimer, Elad Hazan, Satyen Kale, Haipeng Luo category:cs.LG published:2015-06-16 summary:We extend the theory of boosting for regression problems to the onlinelearning setting. Generalizing from the batch setting for boosting, the notionof a weak learning algorithm is modeled as an online learning algorithm withlinear loss functions that competes with a base class of regression functions,while a strong learning algorithm is an online learning algorithm with convexloss functions that competes with a larger class of regression functions. Ourmain result is an online gradient boosting algorithm which converts a weakonline learning algorithm into a strong one where the larger class of functionsis the linear span of the base class. We also give a simpler boosting algorithmthat converts a weak online learning algorithm into a strong one where thelarger class of functions is the convex hull of the base class, and prove itsoptimality.
arxiv-13200-192 | Submatrix localization via message passing | http://arxiv.org/pdf/1510.09219v1.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT cs.SI math.IT math.PR math.ST stat.TH published:2015-10-30 summary:The principal submatrix localization problem deals with recovering a $K\timesK$ principal submatrix of elevated mean $\mu$ in a large $n\times n$ symmetricmatrix subject to additive standard Gaussian noise. This problem serves as aprototypical example for community detection, in which the communitycorresponds to the support of the submatrix. The main result of this paper isthat in the regime $\Omega(\sqrt{n}) \leq K \leq o(n)$, the support of thesubmatrix can be weakly recovered (with $o(K)$ misclassification errors onaverage) by an optimized message passing algorithm if $\lambda = \mu^2K^2/n$,the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpandeand Montanari previously obtained for $K=\Theta(\sqrt{n}).$ In addition, thealgorithm can be extended to provide exact recovery wheneverinformation-theoretically possible and achieve the information limit of exactrecovery as long as $K \geq \frac{n}{\log n} (\frac{1}{8e} + o(1))$. The totalrunning time of the algorithm is $O(n^2\log n)$. Another version of the submatrix localization problem, known as noisybiclustering, aims to recover a $K_1\times K_2$ submatrix of elevated mean$\mu$ in a large $n_1\times n_2$ Gaussian matrix. The optimized message passingalgorithm and its analysis are adapted to the bicluster problem assuming$\Omega(\sqrt{n_i}) \leq K_i \leq o(n_i)$ and $K_1\asymp K_2.$ A sharpinformation-theoretic condition for the weak recovery of both clusters is alsoidentified.
arxiv-13200-193 | Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition | http://arxiv.org/pdf/1504.05477v4.pdf | author:Cameron Musco, Christopher Musco category:cs.DS cs.LG cs.NA published:2015-04-21 summary:Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko,Martinsson, and Tropp, randomized Simultaneous Power Iteration has become themethod of choice for approximate singular value decomposition. It is moreaccurate than simpler sketching algorithms, yet still converges quickly for anymatrix, independently of singular value gaps. After $\tilde{O}(1/\epsilon)$iterations, it gives a low-rank approximation within $(1+\epsilon)$ of optimalfor spectral norm error. We give the first provable runtime improvement on Simultaneous Iteration: asimple randomized block Krylov method, closely related to the classic BlockLanczos algorithm, gives the same guarantees in just$\tilde{O}(1/\sqrt{\epsilon})$ iterations and performs substantially betterexperimentally. Despite their long history, our analysis is the first of aKrylov subspace method that does not depend on singular value gaps, which areunreliable in practice. Furthermore, while it is a simple accuracy benchmark, even $(1+\epsilon)$error for spectral norm low-rank approximation does not imply that an algorithmreturns high quality principal components, a major issue for data applications.We address this problem for the first time by showing that both Block KrylovIteration and a minor modification of Simultaneous Iteration give nearlyoptimal PCA for any matrix. This result further justifies their strength overnon-iterative sketching methods. Finally, we give insight beyond the worst case, justifying why bothalgorithms can run much faster in practice than predicted. We clarify howsimple techniques can take advantage of common matrix properties tosignificantly improve runtime.
arxiv-13200-194 | Generating Text with Deep Reinforcement Learning | http://arxiv.org/pdf/1510.09202v1.pdf | author:Hongyu Guo category:cs.CL cs.LG cs.NE published:2015-10-30 summary:We introduce a novel schema for sequence to sequence learning with a DeepQ-Network (DQN), which decodes the output sequence iteratively. The aim here isto enable the decoder to first tackle easier portions of the sequences, andthen turn to cope with difficult parts. Specifically, in each iteration, anencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from theinput sequence, automatically create features to represent the internal statesof and formulate a list of potential actions for the DQN. Take rephrasing anatural sentence as an example. This list can contain ranked potential words.Next, the DQN learns to make decision on which action (e.g., word) will beselected from the list to modify the current decoded sequence. The newlymodified output sequence is subsequently used as the input to the DQN for thenext decoding iteration. In each iteration, we also bias the reinforcementlearning's attention to explore sequence portions which are previouslydifficult to be decoded. For evaluation, the proposed strategy was trained todecode ten thousands natural sentences. Our experiments indicate that, whencompared to a left-to-right greedy beam search LSTM decoder, the proposedmethod performed competitively well when decoding sentences from the trainingset, but significantly outperformed the baseline when decoding unseensentences, in terms of BLEU score obtained.
arxiv-13200-195 | Estimating Target Signatures with Diverse Density | http://arxiv.org/pdf/1510.09184v1.pdf | author:Taylor Glenn, Alina Zare category:cs.CV published:2015-10-30 summary:Hyperspectral target detection algorithms rely on knowing the desired targetsignature in advance. However, obtaining an effective target signature can bedifficult; signatures obtained from laboratory measurements orhand-spectrometers in the field may not transfer to airborne imageryeffectively. One approach to dealing with this difficulty is to learn aneffective target signature from training data. An approach for learning targetsignatures from training data is presented. The proposed approach addressesuncertainty and imprecision in groundtruth in the training data using amultiple instance learning, diverse density (DD) based objective function.After learning the target signature given data with uncertain and imprecisegroundtruth, target detection can be applied on test data. Results are shown onsimulated and real data.
arxiv-13200-196 | Sparse Variational Bayesian Approximations for Nonlinear Inverse Problems: applications in nonlinear elastography | http://arxiv.org/pdf/1412.0473v4.pdf | author:Isabell M. Franck, P. S. Koutsourelakis category:stat.AP math.NA stat.ML published:2014-12-01 summary:This paper presents an efficient Bayesian framework for solving nonlinear,high-dimensional model calibration problems. It is based on a VariationalBayesian formulation that aims at approximating the exact posterior by means ofsolving an optimization problem over an appropriately selected family ofdistributions. The goal is two-fold. Firstly, to find lower-dimensionalrepresentations of the unknown parameter vector that capture as much aspossible of the associated posterior density, and secondly to enable thecomputation of the approximate posterior density with as few forward calls aspossible. We discuss how these objectives can be achieved by using a fullyBayesian argumentation and employing the marginal likelihood or evidence as theultimate model validation metric for any proposed dimensionality reduction. Wedemonstrate the performance of the proposed methodology for problems innonlinear elastography where the identification of the mechanical properties ofbiological materials can inform non-invasive, medical diagnosis. An ImportanceSampling scheme is finally employed in order to validate the results and assessthe efficacy of the approximations provided.
arxiv-13200-197 | Accurate Vision-based Vehicle Localization using Satellite Imagery | http://arxiv.org/pdf/1510.09171v1.pdf | author:Hang Chu, Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.RO cs.CV published:2015-10-30 summary:We propose a method for accurately localizing ground vehicles with the aid ofsatellite imagery. Our approach takes a ground image as input, and outputs thelocation from which it was taken on a georeferenced satellite image. We performvisual localization by estimating the co-occurrence probabilities between theground and satellite images based on a ground-satellite feature dictionary. Themethod is able to estimate likelihoods over arbitrary locations without theneed for a dense ground image database. We present a ranking-loss basedalgorithm that learns location-discriminative feature projection matrices thatresult in further improvements in accuracy. We evaluate our method on theMalaga and KITTI public datasets and demonstrate significant improvements overa baseline that performs exhaustive search.
arxiv-13200-198 | Streaming, Distributed Variational Inference for Bayesian Nonparametrics | http://arxiv.org/pdf/1510.09161v1.pdf | author:Trevor Campbell, Julian Straub, John W. Fisher III, Jonathan P. How category:cs.LG stat.ML published:2015-10-30 summary:This paper presents a methodology for creating streaming, distributedinference algorithms for Bayesian nonparametric (BNP) models. In the proposedframework, processing nodes receive a sequence of data minibatches, compute avariational posterior for each, and make asynchronous streaming updates to acentral model. In contrast to previous algorithms, the proposed framework istruly streaming, distributed, asynchronous, learning-rate-free, andtruncation-free. The key challenge in developing the framework, arising fromthe fact that BNP models do not impose an inherent ordering on theircomponents, is finding the correspondence between minibatch and central BNPposterior components before performing each update. To address this, the paperdevelops a combinatorial optimization problem over component correspondences,and provides an efficient solution technique. The paper concludes with anapplication of the methodology to the DP mixture model, with experimentalresults demonstrating its practical scalability and performance.
arxiv-13200-199 | Learning Continuous Control Policies by Stochastic Value Gradients | http://arxiv.org/pdf/1510.09142v1.pdf | author:Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, Tom Erez category:cs.LG cs.NE published:2015-10-30 summary:We present a unified framework for learning continuous control policies usingbackpropagation. It supports stochastic control by treating stochasticity inthe Bellman equation as a deterministic function of exogenous noise. Theproduct is a spectrum of general policy gradient algorithms that range frommodel-free methods with value functions to model-based methods without valuefunctions. We use learned models but only require observations from theenvironment in- stead of observations from model-predicted trajectories,minimizing the impact of compounded model errors. We apply these algorithmsfirst to a toy stochastic control problem and then to several physics-basedcontrol problems in simulation. One of these variants, SVG(1), shows theeffectiveness of learning models, value functions, and policies simultaneouslyin continuous domains.
arxiv-13200-200 | Latent Bayesian melding for integrating individual and population models | http://arxiv.org/pdf/1510.09130v1.pdf | author:Mingjun Zhong, Nigel Goddard, Charles Sutton category:stat.ML cs.AI stat.AP stat.ME published:2015-10-30 summary:In many statistical problems, a more coarse-grained model may be suitable forpopulation-level behaviour, whereas a more detailed model is appropriate foraccurate modelling of individual behaviour. This raises the question of how tointegrate both types of models. Methods such as posterior regularization followthe idea of generalized moment matching, in that they allow matchingexpectations between two models, but sometimes both models are mostconveniently expressed as latent variable models. We propose latent Bayesianmelding, which is motivated by averaging the distributions over populationsstatistics of both the individual-level and the population-level models under alogarithmic opinion pool framework. In a case study on electricitydisaggregation, which is a type of single-channel blind source separationproblem, we show that latent Bayesian melding leads to significantly moreaccurate predictions than an approach based solely on generalized momentmatching.
arxiv-13200-201 | Subsampling in Smoothed Range Spaces | http://arxiv.org/pdf/1510.09123v1.pdf | author:Jeff M. Phillips, Yan Zheng category:cs.CG cs.LG published:2015-10-30 summary:We consider smoothed versions of geometric range spaces, so an element of theground set (e.g. a point) can be contained in a range with a non-binary valuein $[0,1]$. Similar notions have been considered for kernels; we extend them tomore general types of ranges. We then consider approximations of these rangespaces through $\varepsilon $-nets and $\varepsilon $-samples (aka$\varepsilon$-approximations). We characterize when size bounds for$\varepsilon $-samples on kernels can be extended to these more generalsmoothed range spaces. We also describe new generalizations for $\varepsilon$-nets to these range spaces and show when results from binary range spaces cancarry over to these smoothed ones.
arxiv-13200-202 | A Unified Framework for Representation-based Subspace Clustering of Out-of-sample and Large-scale Data | http://arxiv.org/pdf/1309.6487v2.pdf | author:Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, Shijie Xiao category:cs.LG cs.CV stat.ML published:2013-09-25 summary:Under the framework of spectral clustering, the key of subspace clustering isbuilding a similarity graph which describes the neighborhood relations amongdata points. Some recent works build the graph using sparse, low-rank, and$\ell_2$-norm-based representation, and have achieved state-of-the-artperformance. However, these methods have suffered from the following twolimitations. First, the time complexities of these methods are at leastproportional to the cube of the data size, which make those methods inefficientfor solving large-scale problems. Second, they cannot cope with out-of-sampledata that are not used to construct the similarity graph. To cluster eachout-of-sample datum, the methods have to recalculate the similarity graph andthe cluster membership of the whole data set. In this paper, we propose aunified framework which makes representation-based subspace clusteringalgorithms feasible to cluster both out-of-sample and large-scale data. Underour framework, the large-scale problem is tackled by converting it asout-of-sample problem in the manner of "sampling, clustering, coding, andclassifying". Furthermore, we give an estimation for the error bounds bytreating each subspace as a point in a hyperspace. Extensive experimentalresults on various benchmark data sets show that our methods outperform severalrecently-proposed scalable methods in clustering large-scale data set.
arxiv-13200-203 | SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis | http://arxiv.org/pdf/1510.09079v1.pdf | author:Lorenzo Gatti, Marco Guerini, Marco Turchi category:cs.CL published:2015-10-30 summary:Deriving prior polarity lexica for sentiment analysis - where positive ornegative scores are associated with words out of context - is a challengingtask. Usually, a trade-off between precision and coverage is hard to find, andit depends on the methodology used to build the lexicon. Manually annotatedlexica provide a high precision but lack in coverage, whereas automaticderivation from pre-existing knowledge guarantees high coverage at the cost ofa lower precision. Since the automatic derivation of prior polarities is lesstime consuming than manual annotation, there has been a great bloom of theseapproaches, in particular based on the SentiWordNet resource. In this paper, wecompare the most frequently used techniques based on SentiWordNet with newerones and blend them in a learning framework (a so called 'ensemble method'). Bytaking advantage of manually built prior polarity lexica, our ensemble methodis better able to predict the prior value of unseen words and to outperform allthe other SentiWordNet approaches. Using this technique we have builtSentiWords, a prior polarity lexicon of approximately 155,000 words, that hasboth a high precision and a high coverage. We finally show that in sentimentanalysis tasks, using our lexicon allows us to outperform both the singlemetrics derived from SentiWordNet and popular manually annotated sentimentlexica.
arxiv-13200-204 | A Study of the Spatio-Temporal Correlations in Mobile Calls Networks | http://arxiv.org/pdf/1510.09005v1.pdf | author:Romain GuigourÃ¨s, Marc BoullÃ©, Fabrice Rossi category:stat.ML cs.SI published:2015-10-30 summary:For the last few years, the amount of data has significantly increased in thecompanies. It is the reason why data analysis methods have to evolve to meetnew demands. In this article, we introduce a practical analysis of a largedatabase from a telecommunication operator. The problem is to segment aterritory and characterize the retrieved areas owing to their inhabitantbehavior in terms of mobile telephony. We have call detail records collectedduring five months in France. We propose a two stages analysis. The first oneaims at grouping source antennas which originating calls are similarlydistributed on target antennas and conversely for target antenna w.r.t. sourceantenna. A geographic projection of the data is used to display the results ona map of France. The second stage discretizes the time into periods betweenwhich we note changes in distributions of calls emerging from the clusters ofsource antennas. This enables an analysis of temporal changes of inhabitantsbehavior in every area of the country.
arxiv-13200-205 | Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets | http://arxiv.org/pdf/1509.08992v2.pdf | author:Justin Domke category:cs.LG stat.ML published:2015-09-30 summary:Inference is typically intractable in high-treewidth undirected graphicalmodels, making maximum likelihood learning a challenge. One way to overcomethis is to restrict parameters to a tractable set, most typically the set oftree-structured parameters. This paper explores an alternative notion of atractable set, namely a set of "fast-mixing parameters" where Markov chainMonte Carlo (MCMC) inference can be guaranteed to quickly converge to thestationary distribution. While it is common in practice to approximate thelikelihood gradient using samples obtained from MCMC, such procedures lacktheoretical guarantees. This paper proves that for any exponential family withbounded sufficient statistics, (not just graphical models) when parameters areconstrained to a fast-mixing set, gradient descent with gradients approximatedby sampling will approximate the maximum likelihood solution inside the setwith high-probability. When unregularized, to find a solution epsilon-accuratein log-likelihood requires a total amount of effort cubic in 1/epsilon,disregarding logarithmic factors. When ridge-regularized, strong convexityallows a solution epsilon-accurate in parameter distance with effort quadraticin 1/epsilon. Both of these provide of a fully-polynomial time randomizedapproximation scheme.
arxiv-13200-206 | A Unified Theory of Confidence Regions and Testing for High Dimensional Estimating Equations | http://arxiv.org/pdf/1510.08986v1.pdf | author:Matey Neykov, Yang Ning, Jun S. Liu, Han Liu category:math.ST stat.ME stat.ML stat.TH published:2015-10-30 summary:We propose a new inferential framework of constructing confidence regions andtesting hypotheses for statistical models specified by a system of highdimensional estimating equations. The key ingredient of this framework is aninfluence function constructed by projecting the fitted estimating equations toa sparse direction obtained by solving a large-scale linear program. The main feature of our framework which makes it different from the existingones is that the specification of the loglikelihood and other types of lossfunctions is not needed. The main theoretical contribution is to establish aunified Z-estimation theory of confidence regions for high dimensionalproblems. We further apply our general framework to a number of examples includingnoisy compressed sensing, undirected graphical models, discriminant analysisand vector autoregression models. We provide thorough numerical simulations toback up the developed theoretical results.
arxiv-13200-207 | Prediction-Adaptation-Correction Recurrent Neural Networks for Low-Resource Language Speech Recognition | http://arxiv.org/pdf/1510.08985v1.pdf | author:Yu Zhang, Ekapol Chuangsuwanich, James Glass, Dong Yu category:cs.CL cs.LG cs.NE published:2015-10-30 summary:In this paper, we investigate the use of prediction-adaptation-correctionrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. APAC-RNN is comprised of a pair of neural networks in which a {\it correction}network uses auxiliary information given by a {\it prediction} network to helpestimate the state probability. The information from the correction network isalso used by the prediction network in a recurrent loop. Our model outperformsother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.Moreover, transfer learning from a language that is similar to the targetlanguage can help improve performance further.
arxiv-13200-208 | CONQUER: Confusion Queried Online Bandit Learning | http://arxiv.org/pdf/1510.08974v1.pdf | author:Daniel Barsky, Koby Crammer category:cs.LG stat.ML published:2015-10-30 summary:We present a new recommendation setting for picking out two items from agiven set to be highlighted to a user, based on contextual input. These twoitems are presented to a user who chooses one of them, possibly stochastically,with a bias that favours the item with the higher value. We propose asecond-order algorithm framework that members of it use uses relativeupper-confidence bounds to trade off exploration and exploitation, and someexplore via sampling. We analyze one algorithm in this framework in anadversarial setting with only mild assumption on the data, and prove a regretbound of $O(Q_T + \sqrt{TQ_T\log T} + \sqrt{T}\log T)$, where $T$ is the numberof rounds and $Q_T$ is the cumulative approximation error of item values usinga linear model. Experiments with product reviews from 33 domains show theadvantage of our methods over algorithms designed for related settings, andthat UCB based algorithms are inferior to greed or sampling based algorithms.
arxiv-13200-209 | VISALOGY: Answering Visual Analogy Questions | http://arxiv.org/pdf/1510.08973v1.pdf | author:Fereshteh Sadeghi, C. Lawrence Zitnick, Ali Farhadi category:cs.CV published:2015-10-30 summary:In this paper, we study the problem of answering visual analogy questions.These questions take the form of image A is to image B as image C is to what.Answering these questions entails discovering the mapping from image A to imageB and then extending the mapping to image C and searching for the image D suchthat the relation from A to B holds for C to D. We pose this problem aslearning an embedding that encourages pairs of analogous images with similartransformations to be close together using convolutional neural networks with aquadruple Siamese architecture. We introduce a dataset of visual analogyquestions in natural images, and show first results of its kind on solvinganalogy questions on natural images.
arxiv-13200-210 | Robust Subspace Clustering via Tighter Rank Approximation | http://arxiv.org/pdf/1510.08971v1.pdf | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.AI cs.LG stat.ML published:2015-10-30 summary:Matrix rank minimization problem is in general NP-hard. The nuclear norm isused to substitute the rank function in many recent studies. Nevertheless, thenuclear norm approximation adds all singular values together and theapproximation error may depend heavily on the magnitudes of singular values.This might restrict its capability in dealing with many practical problems. Inthis paper, an arctangent function is used as a tighter approximation to therank function. We use it on the challenging subspace clustering problem. Forthis nonconvex minimization problem, we develop an effective optimizationprocedure based on a type of augmented Lagrange multipliers (ALM) method.Extensive experiments on face clustering and motion segmentation show that theproposed method is effective for rank approximation.
arxiv-13200-211 | Principal Differences Analysis: Interpretable Characterization of Differences between Distributions | http://arxiv.org/pdf/1510.08956v1.pdf | author:Jonas Mueller, Tommi Jaakkola category:stat.ML cs.LG stat.ME published:2015-10-30 summary:We introduce principal differences analysis (PDA) for analyzing differencesbetween high-dimensional distributions. The method operates by finding theprojection that maximizes the Wasserstein divergence between the resultingunivariate populations. Relying on the Cramer-Wold device, it requires noassumptions about the form of the underlying distributions, nor the nature oftheir inter-class differences. A sparse variant of the method is introduced toidentify features responsible for the differences. We provide algorithms forboth the original minimax formulation as well as its semidefinite relaxation.In addition to deriving some convergence results, we illustrate how theapproach may be applied to identify differences between cell populations in thesomatosensory cortex and hippocampus as manifested by single cell RNA-seq. Ourbroader framework extends beyond the specific choice of Wasserstein divergence.
arxiv-13200-212 | Testing Visual Attention in Dynamic Environments | http://arxiv.org/pdf/1510.08949v1.pdf | author:Philip Bachman, David Krueger, Doina Precup category:cs.LG published:2015-10-30 summary:We investigate attention as the active pursuit of useful information. Thiscontrasts with attention as a mechanism for the attenuation of irrelevantinformation. We also consider the role of short-term memory, whose use iscritical to any model incapable of simultaneously perceiving all information onwhich its output depends. We present several simple synthetic tasks, whichbecome considerably more interesting when we impose strong constraints on how amodel can interact with its input, and on how long it can take to produce itsoutput. We develop a model with a different structure from those seen inprevious work, and we train it using stochastic variational inference with alearned proposal distribution.
arxiv-13200-213 | Improving the Speed of Response of Learning Algorithms Using Multiple Models | http://arxiv.org/pdf/1510.05034v2.pdf | author:Kumpati S. Narendra, Snehasis Mukhopadyhay, Yu Wang category:cs.LG published:2015-10-16 summary:This is the first of a series of papers that the authors propose to write onthe subject of improving the speed of response of learning systems usingmultiple models. During the past two decades, the first author has worked onnumerous methods for improving the stability, robustness, and performance ofadaptive systems using multiple models and the other authors have collaboratedwith him on some of them. Independently, they have also worked on severallearning methods, and have considerable experience with their advantages andlimitations. In particular, they are well aware that it is common knowledgethat machine learning is in general very slow. Numerous attempts have been madeby researchers to improve the speed of convergence of algorithms in differentcontexts. In view of the success of multiple model based methods in improvingthe speed of convergence in adaptive systems, the authors believe that the sameapproach will also prove fruitful in the domain of learning. In this paper, afirst attempt is made to use multiple models for improving the speed ofresponse of the simplest learning schemes that have been studied. i.e. LearningAutomata.
arxiv-13200-214 | Robust Shift-and-Invert Preconditioning: Faster and More Sample Efficient Algorithms for Eigenvector Computation | http://arxiv.org/pdf/1510.08896v1.pdf | author:Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, Aaron Sidford category:cs.DS cs.LG math.NA math.OC published:2015-10-29 summary:We provide faster algorithms and improved sample complexities forapproximating the top eigenvector of a matrix. Offline Setting: Given an $n \times d$ matrix $A$, we show how to compute an$\epsilon$ approximate top eigenvector in time $\tilde O ( [nnz(A) + \frac{d\cdot sr(A)}{gap^2}]\cdot \log 1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4}(d \cdot sr(A))^{1/4}}{\sqrt{gap}}]\cdot \log1/\epsilon )$. Here $sr(A)$ is thestable rank and $gap$ is the multiplicative eigenvalue gap. By separating the$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczosmethods. We also improve prior work using fast subspace embeddings andstochastic optimization, giving significantly improved dependencies on $sr(A)$and $\epsilon$. Our second running time improves this further when $nnz(A) \le\frac{d\cdot sr(A)}{gap^2}$. Online Setting: Given a distribution $D$ with covariance matrix $\Sigma$ anda vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\Sigma$,we show how to refine to an $\epsilon$ approximation using $\tildeO(\frac{v(D)}{gap^2} + \frac{v(D)}{gap \cdot \epsilon})$ samples from $D$. Here$v(D)$ is a natural variance measure. Combining our algorithm with previouswork to initialize $x_0$, we obtain a number of improved sample complexity andruntime results. For general distributions, we achieve asymptotically optimalaccuracy as a function of sample size as the number of samples grows large. Our results center around a robust analysis of the classic method ofshift-and-invert preconditioning to reduce eigenvector computation toapproximately solving a sequence of linear systems. We then apply fast SVRGbased approximate system solvers to achieve our claims. We believe our resultssuggest the general effectiveness of shift-and-invert based approaches andimply that further computational gains may be reaped in practice.
arxiv-13200-215 | A Deep Siamese Network for Scene Detection in Broadcast Videos | http://arxiv.org/pdf/1510.08893v1.pdf | author:Lorenzo Baraldi, Costantino Grana, Rita Cucchiara category:cs.CV cs.MM published:2015-10-29 summary:We present a model that automatically divides broadcast videos into coherentscenes by learning a distance measure between shots. Experiments are performedto demonstrate the effectiveness of our approach by comparing our algorithmagainst recent proposals for automatic scene segmentation. We also propose animproved performance measure that aims to reduce the gap between numericalevaluation and expected results, and propose and release a new benchmarkdataset.
arxiv-13200-216 | Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications | http://arxiv.org/pdf/1510.08865v1.pdf | author:Kai Wei, Rishabh Iyer, Shengjie Wang, Wenruo Bai, Jeff Bilmes category:cs.DS cs.DM cs.LG published:2015-10-29 summary:We investigate two novel mixed robust/average-case submodular datapartitioning problems that we collectively call \emph{Submodular Partitioning}.These problems generalize purely robust instances of the problem, namely\emph{max-min submodular fair allocation} (SFA) and \emph{min-max submodularload balancing} (SLB), and also average-case instances, that is the\emph{submodular welfare problem} (SWP) and \emph{submodular multiwaypartition} (SMP). While the robust versions have been studied in the theorycommunity, existing work has focused on tight approximation guarantees, and theresultant algorithms are not generally scalable to large real-worldapplications. This is in contrast to the average case, where most of thealgorithms are scalable. In the present paper, we bridge this gap, by proposingseveral new algorithms (including greedy, majorization-minimization,minorization-maximization, and relaxation algorithms) that not only scale tolarge datasets but that also achieve theoretical approximation guaranteescomparable to the state-of-the-art. We moreover provide new scalable algorithmsthat apply to additive combinations of the robust and average-case objectives.We show that these problems have many applications in machine learning (ML),including data partitioning and load balancing for distributed ML, dataclustering, and image segmentation. We empirically demonstrate the efficacy ofour algorithms on real-world problems involving data partitioning fordistributed optimization (of convex and deep neural network objectives), andalso purely unsupervised image segmentation.
arxiv-13200-217 | Spiking Deep Networks with LIF Neurons | http://arxiv.org/pdf/1510.08829v1.pdf | author:Eric Hunsberger, Chris Eliasmith category:cs.LG cs.NE published:2015-10-29 summary:We train spiking deep networks using leaky integrate-and-fire (LIF) neurons,and achieve state-of-the-art results for spiking networks on the CIFAR-10 andMNIST datasets. This demonstrates that biologically-plausible spiking LIFneurons can be integrated into deep networks can perform as well as otherspiking models (e.g. integrate-and-fire). We achieved this result by softeningthe LIF response function, such that its derivative remains bounded, and bytraining the network with noise to provide robustness against the variabilityintroduced by spikes. Our method is general and could be applied to otherneuron types, including those used on modern neuromorphic hardware. Our workbrings more biological realism into modern image classification models, withthe hope that these models can inform how the brain performs this difficulttask. It also provides new methods for training deep networks to run onneuromorphic hardware, with the aim of fast, power-efficient imageclassification for robotics applications.
arxiv-13200-218 | Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization | http://arxiv.org/pdf/1506.08272v2.pdf | author:Xiangru Lian, Yijun Huang, Yuncheng Li, Ji Liu category:math.OC cs.NA stat.ML published:2015-06-27 summary:Asynchronous parallel implementations of stochastic gradient (SG) have beenbroadly used in solving deep neural network and received many successes inpractice recently. However, existing theories cannot explain their convergenceand speedup properties, mainly due to the nonconvexity of most deep learningformulations and the asynchronous parallel mechanism. To fill the gaps intheory and provide theoretical supports, this paper studies two asynchronousparallel implementations of SG: one is on the computer network and the other ison the shared memory system. We establish an ergodic convergence rate$O(1/\sqrt{K})$ for both algorithms and prove that the linear speedup isachievable if the number of workers is bounded by $\sqrt{K}$ ($K$ is the totalnumber of iterations). Our results generalize and improve existing analysis forconvex minimization.
arxiv-13200-219 | Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling | http://arxiv.org/pdf/1510.08692v1.pdf | author:Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, Amos J. Storkey category:stat.ML cs.LG published:2015-10-29 summary:Monte Carlo sampling for Bayesian posterior inference is a common approachused in machine learning. The Markov Chain Monte Carlo procedures that are usedare often discrete-time analogues of associated stochastic differentialequations (SDEs). These SDEs are guaranteed to leave invariant the requiredposterior distribution. An area of current research addresses the computationalbenefits of stochastic gradient methods in this setting. Existing techniquesrely on estimating the variance or covariance of the subsampling error, andtypically assume constant variance. In this article, we propose acovariance-controlled adaptive Langevin thermostat that can effectivelydissipate parameter-dependent noise while maintaining a desired targetdistribution. The proposed method achieves a substantial speedup over popularalternative schemes for large-scale machine learning applications.
arxiv-13200-220 | Visual Tracking via Nonnegative Regularization Multiple Locality Coding | http://arxiv.org/pdf/1510.01148v3.pdf | author:Fanghui Liu, Tao Zhou, Irene Y. H. Gu, Jie Yang category:cs.CV published:2015-10-05 summary:This paper presents a novel object tracking method based on approximatedLocality-constrained Linear Coding (LLC). Rather than using a non-negativityconstraint on encoding coefficients to guarantee these elements nonnegative, inthis paper, the non-negativity constraint is substituted for a conventional$\ell_2$ norm regularization term in approximated LLC to obtain the similarnonnegative effect. And we provide a detailed and adequate explanation intheoretical analysis to clarify the rationality of this replacement. Instead ofspecifying fixed K nearest neighbors to construct the local dictionary, aseries of different dictionaries with pre-defined numbers of nearest neighborsare selected. Weights of these various dictionaries are also learned fromapproximated LLC in the similar framework. In order to alleviate trackingdrifts, we propose a simple and efficient occlusion detection method. Theocclusion detection criterion mainly depends on whether negative templates areselected to represent the severe occluded target. Both qualitative andquantitative evaluations on several challenging sequences show that theproposed tracking algorithm achieves favorable performance compared with otherstate-of-the-art methods.
arxiv-13200-221 | Nonconvex Penalization in Sparse Estimation: An Approach Based on the Bernstein Function | http://arxiv.org/pdf/1510.08633v1.pdf | author:Zhihua Zhang category:stat.ML published:2015-10-29 summary:In this paper we study nonconvex penalization using Bernstein functions whosefirst-order derivatives are completely monotone. The Bernstein function caninduce a class of nonconvex penalty functions for high-dimensional sparseestimation problems. We derive a thresholding function based on the Bernsteinpenalty and discuss some important mathematical properties in sparsitymodeling. We show that a coordinate descent algorithm is especially appropriatefor regression problems penalized by the Bernstein function. We also considerthe application of the Bernstein penalty in classification problems and devisea proximal alternating linearized minimization method. Based on theory of theKurdyka-Lojasiewicz inequality, we conduct convergence analysis of thesealternating iteration procedures. We particularly exemplify a family ofBernstein nonconvex penalties based on a generalized Gamma measure and conductempirical analysis for this family.
arxiv-13200-222 | Higher-order asymptotics for the parametric complexity | http://arxiv.org/pdf/1510.00112v3.pdf | author:James G. Dowty category:cs.IT math.IT stat.ME stat.ML published:2015-10-01 summary:The parametric complexity is the key quantity in the minimum descriptionlength (MDL) approach to statistical model selection. Rissanen and others haveshown that the parametric complexity of a statistical model approaches a simplefunction of the Fisher information volume of the model as the sample size $n$goes to infinity. This paper derives higher-order asymptotic expansions for theparametric complexity, in the case of exponential families and independent andidentically distributed data. These higher-order approximations are calculatedfor some examples and are shown to have better finite-sample behaviour thanRissanen's approximation. The higher-order terms are given as expressionsinvolving cumulants (or, more naturally, the Amari-Chentsov tensors), and theseterms are likely to be interesting in themselves since they arise naturallyfrom the general information-theoretic principles underpinning MDL. Thederivation given here specializes to an alternative and arguably simpler proofof Rissanen's result (for the case considered here), proving for the first timethat his approximation is $O(n^{-1})$.
arxiv-13200-223 | The Singular Value Decomposition, Applications and Beyond | http://arxiv.org/pdf/1510.08532v1.pdf | author:Zhihua Zhang category:cs.LG published:2015-10-29 summary:The singular value decomposition (SVD) is not only a classical theory inmatrix computation and analysis, but also is a powerful tool in machinelearning and modern data analysis. In this tutorial we first study the basicnotion of SVD and then show the central role of SVD in matrices. Usingmajorization theory, we consider variational principles of singular values andeigenvalues. Built on SVD and a theory of symmetric gauge functions, we discussunitarily invariant norms, which are then used to formulate general results formatrix low rank approximation. We study the subdifferentials of unitarilyinvariant norms. These results would be potentially useful in many machinelearning problems such as matrix completion and matrix data classification.Finally, we discuss matrix low rank approximation and its recent developmentssuch as randomized SVD, approximate matrix multiplication, CUR decomposition,and Nystrom approximation. Randomized algorithms are important approaches tolarge scale SVD as well as fast matrix computations.
arxiv-13200-224 | Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso | http://arxiv.org/pdf/1510.08512v1.pdf | author:Eunho Yang, AurÃ©lie C. Lozano category:stat.ML published:2015-10-28 summary:Gaussian Graphical Models (GGMs) are popular tools for studying networkstructures. However, many modern applications such as gene network discoveryand social interactions analysis often involve high-dimensional noisy data withoutliers or heavier tails than the Gaussian distribution. In this paper, wepropose the Trimmed Graphical Lasso for robust estimation of sparse GGMs. Ourmethod guards against outliers by an implicit trimming mechanism akin to thepopular Least Trimmed Squares method used for linear regression. We provide arigorous statistical analysis of our estimator in the high-dimensional setting.In contrast, existing approaches for robust sparse GGMs estimation lackstatistical guarantees. Our theoretical results are complemented by experimentson simulated and real gene expression data which further demonstrate the valueof our approach.
arxiv-13200-225 | Emoticons vs. Emojis on Twitter: A Causal Inference Approach | http://arxiv.org/pdf/1510.08480v1.pdf | author:Umashanthi Pavalanathan, Jacob Eisenstein category:cs.CL published:2015-10-28 summary:Online writing lacks the non-verbal cues present in face-to-facecommunication, which provide additional contextual information about theutterance, such as the speaker's intention or affective state. To fill thisvoid, a number of orthographic features, such as emoticons, expressivelengthening, and non-standard punctuation, have become popular in social mediaservices including Twitter and Instagram. Recently, emojis have been introducedto social media, and are increasingly popular. This raises the question ofwhether these predefined pictographic characters will come to replace earlierorthographic methods of paralinguistic communication. In this abstract, weattempt to shed light on this question, using a matching approach from causalinference to test whether the adoption of emojis causes individual users toemploy fewer emoticons in their text on Twitter.
arxiv-13200-226 | Toward Long Distance, Sub-diffraction Imaging Using Coherent Camera Arrays | http://arxiv.org/pdf/1510.08470v1.pdf | author:Jason Holloway, M. Salman Asif, Manoj Kumar Sharma, Nathan Matsuda, Roarke Horstmeyer, Oliver Cossairt, Ashok Veeraraghavan category:cs.CV physics.optics published:2015-10-28 summary:In this work, we propose using camera arrays coupled with coherentillumination as an effective method of improving spatial resolution in longdistance images by a factor of ten and beyond. Recent advances in ptychographyhave demonstrated that one can image beyond the diffraction limit of theobjective lens in a microscope. We demonstrate a similar imaging system toimage beyond the diffraction limit in long range imaging. We emulate a cameraarray with a single camera attached to an X-Y translation stage. We show thatan appropriate phase retrieval based reconstruction algorithm can be used toeffectively recover the lost high resolution details from the multiple lowresolution acquired images. We analyze the effects of noise, required degree ofimage overlap, and the effect of increasing synthetic aperture size on thereconstructed image quality. We show that coherent camera arrays have thepotential to greatly improve imaging performance. Our simulations showresolution gains of 10x and more are achievable. Furthermore, experimentalresults from our proof-of-concept systems show resolution gains of 4x-7x forreal scenes. Finally, we introduce and analyze in simulation a new strategy tocapture macroscopic Fourier Ptychography images in a single snapshot, albeitusing a camera array.
arxiv-13200-227 | Priors on exchangeable directed graphs | http://arxiv.org/pdf/1510.08440v1.pdf | author:Diana Cai, Nathanael Ackerman, Cameron Freer category:math.ST stat.ME stat.ML stat.TH published:2015-10-28 summary:Directed graphs occur throughout statistical modeling of networks, andexchangeability is a natural assumption when the ordering of vertices does notmatter. There is a deep structural theory for exchangeable undirected graphs,which extends to the directed case, but with additional complexities that arisefrom the need to consider the joint distribution over both edge directions on apair of vertices. Exchangeable directed graphs are characterized by a samplingprocedure given by the Aldous-Hoover theorem, which can be described in termsof a distribution on measurable objects known as digraphons. Most existing workon exchangeable graph models has focused on undirected graphs, and littleattention has been placed on priors for exchangeable directed graphs.Currently, many directed network models generalize the undirected case bytreating each edge direction as independent, rather than considering both edgedirections jointly. By placing priors on digraphons one can capture dependencein the edge directions in exchangeable directed graphs, which we demonstrate isnot captured by models that consider the edge directions independently. Weconstruct priors on exchangeable directed graphs using digraphons, includingspecial cases such as tournaments, linear orderings, directed acyclic graphs,and partial orderings. We also present a Bayesian nonparametric block model forexchangeable directed graphs and demonstrate inference for these models onsynthetic data.
arxiv-13200-228 | Fast k-best Sentence Compression | http://arxiv.org/pdf/1510.08418v1.pdf | author:Katja Filippova, Enrique Alfonseca category:cs.CL published:2015-10-28 summary:A popular approach to sentence compression is to formulate the task as aconstrained optimization problem and solve it with integer linear programming(ILP) tools. Unfortunately, dependence on ILP may make the compressorprohibitively slow, and thus approximation techniques have been proposed whichare often complex and offer a moderate gain in speed. As an alternativesolution, we introduce a novel compression algorithm which generates k-bestcompressions relying on local deletion decisions. Our algorithm is two ordersof magnitude faster than a recent ILP-based method while producing bettercompressions. Moreover, an extensive evaluation demonstrates that the qualityof compressions does not degrade much as we move from single best to top-fiveresults.
arxiv-13200-229 | Communication Complexity of Distributed Convex Learning and Optimization | http://arxiv.org/pdf/1506.01900v2.pdf | author:Yossi Arjevani, Ohad Shamir category:cs.LG math.OC stat.ML published:2015-06-05 summary:We study the fundamental limits to communication-efficient distributedmethods for convex learning and optimization, under different assumptions onthe information available to individual machines, and the types of functionsconsidered. We identify cases where existing algorithms are already worst-caseoptimal, as well as cases where room for further improvement is still possible.Among other things, our results indicate that without similarity between thelocal objective functions (due to statistical data similarity or otherwise)many communication rounds may be required, even if the machines have unboundedcomputational power.
arxiv-13200-230 | Fast Landmark Subspace Clustering | http://arxiv.org/pdf/1510.08406v1.pdf | author:Xu Wang, Gilad Lerman category:stat.ML published:2015-10-28 summary:Kernel methods obtain superb performance in terms of accuracy for variousmachine learning tasks since they can effectively extract nonlinear relations.However, their time complexity can be rather large especially for clusteringtasks. In this paper we define a general class of kernels that can be easilyapproximated by randomization. These kernels appear in various applications, inparticular, traditional spectral clustering, landmark-based spectral clusteringand landmark-based subspace clustering. We show that for $n$ data points from$K$ clusters with $D$ landmarks, the randomization procedure results in analgorithm of complexity $O(KnD)$. Furthermore, we bound the error between theoriginal clustering scheme and its randomization. To illustrate the power ofthis framework, we propose a new fast landmark subspace (FLS) clusteringalgorithm. Experiments over synthetic and real datasets demonstrate thesuperior performance of FLS in accelerating subspace clustering with marginalsacrifice of accuracy.
arxiv-13200-231 | Efficient refinement of GPS-based localization in urban areas using visual information and sensor parameter | http://arxiv.org/pdf/1502.00319v3.pdf | author:Mahdi Salarian, Rashid Ansari category:cs.CV cs.IR published:2015-02-01 summary:An efficient method is proposed for refining GPS-acquired locationcoordinates in urban areas using camera images, Google Street View (GSV) andsensor parameters. The main goal is to compensate for GPS location imprecisionin dense area of cities due to proximity to walls and buildings. Avail-ablemethods for better localization often use visual information by using queryimages acquired with camera-equipped mobile devices and applying imageretrieval techniques to find the closest match in a GPS-referenced image dataset. The search areas required for reliable search are about 1-2 sq. Km and theaccuracy is typically 25-100 meters. Here we describe a method based on imageretrieval where a reliable search can be confined to areas of 0.01 sq. Km andthe accuracy in our experiments is less than 10 meters. To test our procedurewe created a database by acquiring all Google Street View images close to whatis seen by a pedestrian in a large region of downtown Chicago and saved allcoordinates and orientation data to be used for confining our search region.Prior knowledge from approximate position of query image is leveraged toaddress complexity and accuracy issues of our search in a large scalegeo-tagged data set. One key aspect that differentiates our work is that itutilizes the sensor information of GPS SOS and the camera orientation inimproving localization. Finally we demonstrate retrieval-based technique areless accurate in sparse open areas compared with purely GPS measurement. Theeffectiveness of our approach is discussed in detail and experimental resultsshow improved performance when compared with regular approaches.
arxiv-13200-232 | Universal Dependency Analysis | http://arxiv.org/pdf/1510.08389v1.pdf | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:Most data is multi-dimensional. Discovering whether any subset of dimensions,or subspaces, of such data is significantly correlated is a core task in datamining. To do so, we require a measure that quantifies how correlated asubspace is. For practical use, such a measure should be universal in the sensethat it captures correlation in subspaces of any dimensionality and allows tomeaningfully compare correlation scores across different subspaces, regardlesshow many dimensions they have and what specific statistical properties theirdimensions possess. Further, it would be nice if the measure cannon-parametrically and efficiently capture both linear and non-linearcorrelations. In this paper, we propose UDS, a multivariate correlation measure thatfulfills all of these desiderata. In short, we define \uds based on cumulativeentropy and propose a principled normalization scheme to bring its scoresacross different subspaces to the same domain, enabling universal correlationassessment. UDS is purely non-parametric as we make no assumption on datadistributions nor types of correlation. To compute it on empirical data, weintroduce an efficient and non-parametric method. Extensive experiments showthat UDS outperforms state of the art.
arxiv-13200-233 | Linear-time Detection of Non-linear Changes in Massively High Dimensional Time Series | http://arxiv.org/pdf/1510.08385v1.pdf | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:Change detection in multivariate time series has applications in manydomains, including health care and network monitoring. A common approach todetect changes is to compare the divergence between the distributions of areference window and a test window. When the number of dimensions is verylarge, however, the naive approach has both quality and efficiency issues: toensure robustness the window size needs to be large, which not only leads tomissed alarms but also increases runtime. To this end, we propose LIGHT, a linear-time algorithm for robustly detectingnon-linear changes in massively high dimensional time series. Importantly,LIGHT provides high flexibility in choosing the window size, allowing thedomain expert to fit the level of details required. To do such, we 1) performscalable PCA to reduce dimensionality, 2) perform scalable factorization of thejoint distribution, and 3) scalably compute divergences between these lowerdimensional distributions. Extensive empirical evaluation on both synthetic andreal-world data show that LIGHT outperforms state of the art with up to 100%improvement in both quality and efficiency.
arxiv-13200-234 | Flexibly Mining Better Subgroups | http://arxiv.org/pdf/1510.08382v1.pdf | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:In subgroup discovery, also known as supervised pattern mining, discoveringhigh quality one-dimensional subgroups and refinements of these is a crucialtask. For nominal attributes, this is relatively straightforward, as we canconsider individual attribute values as binary features. For numericalattributes, the task is more challenging as individual numeric values are notreliable statistics. Instead, we can consider combinations of adjacent values,i.e. bins. Existing binning strategies, however, are not tailored for subgroupdiscovery. That is, they do not directly optimize for the quality of subgroups,therewith potentially degrading the mining result. To address this issue, we propose FLEXI. In short, with FLEXI we propose touse optimal binning to find high quality binary features for both numeric andordinal attributes. We instantiate FLEXI with various quality measures and showhow to achieve efficiency accordingly. Experiments on both synthetic andreal-world data sets show that FLEXI outperforms state of the art with up to 25times improvement in subgroup quality.
arxiv-13200-235 | Canonical Divergence Analysis | http://arxiv.org/pdf/1510.08370v1.pdf | author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG published:2015-10-28 summary:We aim to analyze the relation between two random vectors that maypotentially have both different number of attributes as well as realizations,and which may even not have a joint distribution. This problem arises in manypractical domains, including biology and architecture. Existing techniquesassume the vectors to have the same domain or to be jointly distributed, andhence are not applicable. To address this, we propose Canonical DivergenceAnalysis (CDA). We introduce three instantiations, each of which permitspractical implementation. Extensive empirical evaluation shows the potential ofour method.
arxiv-13200-236 | A Flexible and Efficient Algorithmic Framework for Constrained Matrix and Tensor Factorization | http://arxiv.org/pdf/1506.04209v2.pdf | author:Kejun Huang, Nicholas D. Sidiropoulos, Athanasios P. Liavas category:stat.ML cs.LG math.OC stat.CO published:2015-06-13 summary:We propose a general algorithmic framework for constrained matrix and tensorfactorization, which is widely used in signal processing and machine learning.The new framework is a hybrid between alternating optimization (AO) and thealternating direction method of multipliers (ADMM): each matrix factor isupdated in turn, using ADMM, hence the name AO-ADMM. This combination cannaturally accommodate a great variety of constraints on the factor matrices,and almost all possible loss measures for the fitting. Computation caching andwarm start strategies are used to ensure that each update is evaluatedefficiently, while the outer AO framework exploits recent developments in blockcoordinate descent (BCD)-type methods which help ensure that every limit pointis a stationary point, as well as faster and more robust convergence inpractice. Three special cases are studied in detail: non-negative matrix/tensorfactorization, constrained matrix/tensor completion, and dictionary learning.Extensive simulations and experiments with real data are used to showcase theeffectiveness and broad applicability of the proposed framework.
arxiv-13200-237 | Extreme Compressive Sampling for Covariance Estimation | http://arxiv.org/pdf/1506.00898v2.pdf | author:Martin Azizyan, Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.IT math.IT published:2015-06-02 summary:This paper studies the problem of estimating the covariance of a collectionof vectors using only extremely compressed measurements of each vector. Anestimator based on back-projections of these compressive samples is proposedand analyzed. A distribution-free analysis shows that by observing just asingle compressive measurement of each vector, one can consistently estimatethe covariance matrix, in both infinity and spectral norm, and this sameanalysis leads to precise rates of convergence in both norms. Viainformation-theoretic techniques, lower bounds showing that this estimator isminimax-optimal for both infinity and spectral norm estimation problems areestablished. These results are also specialized to give matching upper andlower bounds for estimating the population covariance of a collection ofGaussian vectors, again in the compressive measurement model. The analysisconducted in this paper shows that the effective sample complexity for thisproblem is scaled by a factor of $m^2/d^2$ where $m$ is the compressiondimension and $d$ is the ambient dimension. Applications to subspace learning(Principal Components Analysis) and learning over distributed sensor networksare also discussed.
arxiv-13200-238 | Evaluating accuracy of community detection using the relative normalized mutual information | http://arxiv.org/pdf/1501.03844v2.pdf | author:Pan Zhang category:physics.soc-ph cs.SI stat.ML published:2015-01-15 summary:The Normalized Mutual Information (NMI) has been widely used to evaluate theaccuracy of community detection algorithms. However in this article we showthat the NMI is seriously affected by systematic errors due to finite size ofnetworks, and may give a wrong estimate of performance of algorithms in somecases. We give a simple theory to the finite-size effect of NMI and test ourtheory numerically. Then we propose a new metric for the accuracy of communitydetection, namely the relative Normalized Mutual Information (rNMI), whichconsiders statistical significance of the NMI by comparing it with the expectedNMI of random partitions. Our numerical experiments show that the rNMIovercomes the finite-size effect of the NMI.
arxiv-13200-239 | Qualitative Projection Using Deep Neural Networks | http://arxiv.org/pdf/1510.05711v2.pdf | author:Andrew J. R. Simpson category:cs.NE cs.LG 68Txx published:2015-10-19 summary:Deep neural networks (DNN) abstract by demodulating the output of linearfilters. In this article, we refine this definition of abstraction to show thatthe inputs of a DNN are abstracted with respect to the filters. Or, to restate,the abstraction is qualified by the filters. This leads us to introduce thenotion of qualitative projection. We use qualitative projection to abstractMNIST hand-written digits with respect to the various dogs, horses, planes andcars of the CIFAR dataset. We then classify the MNIST digits according to themagnitude of their dogness, horseness, planeness and carness qualities,illustrating the generality of qualitative projection.
arxiv-13200-240 | Fully adaptive density-based clustering | http://arxiv.org/pdf/1409.8437v4.pdf | author:Ingo Steinwart category:stat.ME stat.ML published:2014-09-30 summary:The clusters of a distribution are often defined by the connected componentsof a density level set. However, this definition depends on the user-specifiedlevel. We address this issue by proposing a simple, generic algorithm, whichuses an almost arbitrary level set estimator to estimate the smallest level atwhich there are more than one connected components. In the case where thisalgorithm is fed with histogram-based level set estimates, we provide a finitesample analysis, which is then used to show that the algorithm consistentlyestimates both the smallest level and the corresponding connected components.We further establish rates of convergence for the two estimation problems, andlast but not least, we present a simple, yet adaptive strategy for determiningthe width-parameter of the involved density estimator in a data-depending way.
arxiv-13200-241 | Beyond Convexity: Stochastic Quasi-Convex Optimization | http://arxiv.org/pdf/1507.02030v3.pdf | author:Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz category:cs.LG math.OC published:2015-07-08 summary:Stochastic convex optimization is a basic and well studied primitive inmachine learning. It is well known that convex and Lipschitz functions can beminimized efficiently using Stochastic Gradient Descent (SGD). The NormalizedGradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, whichupdates according to the direction of the gradients, rather than the gradientsthemselves. In this paper we analyze a stochastic version of NGD and prove itsconvergence to a global minimum for a wider class of functions: we require thefunctions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadensthe con- cept of unimodality to multidimensions and allows for certain types ofsaddle points, which are a known hurdle for first-order optimization methodssuch as gradient descent. Locally-Lipschitz functions are only required to beLipschitz in a small region around the optimum. This assumption circumventsgradient explosion, which is another known hurdle for gradient descentvariants. Interestingly, unlike the vanilla SGD algorithm, the stochasticnormalized gradient descent algorithm provably requires a minimal minibatchsize.
arxiv-13200-242 | Generalized Categorization Axioms | http://arxiv.org/pdf/1503.09082v11.pdf | author:Jian Yu category:cs.LG published:2015-03-31 summary:Categorization axioms have been proposed to axiomatizing clustering results,which offers a hint of bridging the difference between human recognition systemand machine learning through an intuitive observation: an object should beassigned to its most similar category. However, categorization axioms cannot begeneralized into a general machine learning system as categorization axiomsbecome trivial when the number of categories becomes one. In order togeneralize categorization axioms into general cases, categorization input andcategorization output are reinterpreted by inner and outer categoryrepresentation. According to the categorization reinterpretation, two categoryrepresentation axioms are presented. Category representation axioms andcategorization axioms can be combined into a generalized categorizationaxiomatic framework, which accurately delimit the theoretical categorizationconstraints and overcome the shortcoming of categorization axioms. The proposedaxiomatic framework not only discuses categorization test issue but alsoreinterprets many results in machine learning in a unified way, such asdimensionality reduction,density estimation, regression, clustering andclassification.
arxiv-13200-243 | Fast and Scalable Structural SVM with Slack Rescaling | http://arxiv.org/pdf/1510.06002v2.pdf | author:Heejin Choi, Ofer Meshi, Nathan Srebro category:cs.LG published:2015-10-20 summary:We present an efficient method for training slack-rescaled structural SVM.Although finding the most violating label in a margin-rescaled formulation isoften easy since the target function decomposes with respect to the structure,this is not the case for a slack-rescaled formulation, and finding the mostviolated label might be very difficult. Our core contribution is an efficientmethod for finding the most-violating-label in a slack-rescaled formulation,given an oracle that returns the most-violating-label in a (slightly modified)margin-rescaled formulation. We show that our method enables accurate andscalable training for slack-rescaled SVMs, reducing runtime by an order ofmagnitude compared to previous approaches to slack-rescaled SVMs.
arxiv-13200-244 | Spectral Convergence Rate of Graph Laplacian | http://arxiv.org/pdf/1510.08110v1.pdf | author:Xu Wang category:stat.ML published:2015-10-27 summary:Laplacian Eigenvectors of the graph constructed from a data set are used inmany spectral manifold learning algorithms such as diffusion maps and spectralclustering. Given a graph constructed from a random sample of a $d$-dimensionalcompact submanifold $M$ in $\mathbb{R}^D$, we establish the spectralconvergence rate of the graph Laplacian. It implies the consistency of thespectral clustering algorithm via a standard perturbation argument. A simplenumerical study indicates the necessity of a denoising step before applyingspectral algorithms.
arxiv-13200-245 | Online Learning with Gaussian Payoffs and Side Observations | http://arxiv.org/pdf/1510.08108v1.pdf | author:Yifan Wu, AndrÃ¡s GyÃ¶rgy, Csaba SzepesvÃ¡ri category:stat.ML cs.LG published:2015-10-27 summary:We consider a sequential learning problem with Gaussian payoffs and sideinformation: after selecting an action $i$, the learner receives informationabout the payoff of every action $j$ in the form of Gaussian observations whosemean is the same as the mean payoff, but the variance depends on the pair$(i,j)$ (and may be infinite). The setup allows a more refined informationtransfer from one action to another than previous partial monitoring setups,including the recently introduced graph-structured feedback case. For the firsttime in the literature, we provide non-asymptotic problem-dependent lowerbounds on the regret of any algorithm, which recover existing asymptoticproblem-dependent lower bounds and finite-time minimax lower bounds availablein the literature. We also provide algorithms that achieve theproblem-dependent lower bound (up to some universal constant factor) or theminimax lower bounds (up to logarithmic factors).
arxiv-13200-246 | A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights | http://arxiv.org/pdf/1503.01243v2.pdf | author:Weijie Su, Stephen Boyd, Emmanuel J. Candes category:stat.ML math.CA math.OC published:2015-03-04 summary:We derive a second-order ordinary differential equation (ODE) which is thelimit of Nesterov's accelerated gradient method. This ODE exhibits approximateequivalence to Nesterov's scheme and thus can serve as a tool for analysis. Weshow that the continuous time ODE allows for a better understanding ofNesterov's scheme. As a byproduct, we obtain a family of schemes with similarconvergence rates. The ODE interpretation also suggests restarting Nesterov'sscheme leading to an algorithm, which can be rigorously proven to converge at alinear rate whenever the objective is strongly convex.
arxiv-13200-247 | Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties | http://arxiv.org/pdf/1510.08039v1.pdf | author:Georg Poier, Konstantinos Roditakis, Samuel Schulter, Damien Michel, Horst Bischof, Antonis A. Argyros category:cs.CV published:2015-10-27 summary:Model-based approaches to 3D hand tracking have been shown to perform well ina wide range of scenarios. However, they require initialisation and cannotrecover easily from tracking failures that occur due to fast hand motions.Data-driven approaches, on the other hand, can quickly deliver a solution, butthe results often suffer from lower accuracy or missing anatomical validitycompared to those obtained from model-based approaches. In this work we proposea hybrid approach for hand pose estimation from a single depth image. First, alearned regressor is employed to deliver multiple initial hypotheses for the 3Dposition of each hand joint. Subsequently, the kinematic parameters of a 3Dhand model are found by deliberately exploiting the inherent uncertainty of theinferred joint proposals. This way, the method provides anatomically valid andaccurate solutions without requiring manual initialisation or suffering fromtrack losses. Quantitative results on several standard datasets demonstratethat the proposed method outperforms state-of-the-art representatives of themodel-based, data-driven and hybrid paradigms.
arxiv-13200-248 | ENFT: Efficient Non-Consecutive Feature Tracking for Robust Structure-from-Motion | http://arxiv.org/pdf/1510.08012v1.pdf | author:Guofeng Zhang, Haomin Liu, Zilong Dong, Jiaya Jia, Tien-Tsin Wong, Hujun Bao category:cs.CV published:2015-10-27 summary:Structure-from-motion (SfM) largely relies on the quality of featuretracking. In image sequences, if disjointed tracks caused by objects moving inand out of the view, occasional occlusion, or image noise, are not handledwell, the corresponding SfM could be significantly affected. This problembecomes more serious for accurate SfM of large-scale scenes, which typicallyrequires to capture multiple sequences to cover the whole scene. In this paper,we propose an efficient non-consecutive feature tracking (ENFT) framework tomatch the interrupted tracks distributed in different subsequences or even indifferent videos. Our framework consists of steps of solving the feature`dropout' problem when indistinctive structures, noise or even large imagedistortion exist, and of rapidly recognizing and joining common featureslocated in different subsequences. In addition, we contribute an effectivesegment-based coarse-to-fine SfM estimation algorithm for efficiently androbustly handling large datasets. Experimental results on several challengingand large video datasets demonstrate the effectiveness of the proposed system.
arxiv-13200-249 | Stochastic Online Shortest Path Routing: The Value of Feedback | http://arxiv.org/pdf/1309.7367v3.pdf | author:M. Sadegh Talebi, Zhenhua Zou, Richard Combes, Alexandre Proutiere, Mikael Johansson category:cs.NI cs.LG math.OC published:2013-09-27 summary:This paper studies online shortest path routing over multi-hop networks. Linkcosts or delays are time-varying and modeled by independent and identicallydistributed random processes, whose parameters are initially unknown. Theparameters, and hence the optimal path, can only be estimated by routingpackets through the network and observing the realized delays. Our aim is tofind a routing policy that minimizes the regret (the cumulative difference ofexpected delay) between the path chosen by the policy and the unknown optimalpath. We formulate the problem as a combinatorial bandit optimization problemand consider several scenarios that differ in where routing decisions are madeand in the information available when making the decisions. For each scenario,we derive a tight asymptotic lower bound on the regret that has to be satisfiedby any online routing policy. These bounds help us to understand theperformance improvements we can expect when (i) taking routing decisions ateach hop rather than at the source only, and (ii) observing per-link delaysrather than end-to-end path delays. In particular, we show that (i) is of nouse while (ii) can have a spectacular impact. Three algorithms, with atrade-off between computational complexity and performance, are proposed. Theregret upper bounds of these algorithms improve over those of the existingalgorithms, and they significantly outperform state-of-the-art algorithms innumerical experiments.
arxiv-13200-250 | Increasing Behavioral Complexity for Evolved Virtual Creatures with the ESP Method | http://arxiv.org/pdf/1510.07957v1.pdf | author:Dan Lessin, Don Fussell, Risto Miikkulainen, Sebastian Risi category:cs.NE published:2015-10-27 summary:Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs)have employed the coevolution of morphology and control to produce high-impactwork in multiple fields, including graphics, evolutionary computation,robotics, and artificial life. However, in contrast to fixed-morphologycreatures, there has been no clear increase in the behavioral complexity ofEVCs in those two decades. This paper describes a method for moving beyond thislimit, making use of high-level human input in the form of a syllabus ofintermediate learning tasks--along with mechanisms for preservation, reuse, andcombination of previously learned tasks. This method--named ESP for its threecomponents: encapsulation, syllabus, and pandemonium--is presented in twocomplementary versions: Fast ESP, which constrains later morphological changesto achieve linear growth in computation time as behavioral complexity is added,and General ESP, which allows this restriction to be removed when sufficientcomputational resources are available. Experiments demonstrate that the ESPmethod allows evolved virtual creatures to reach new levels of behavioralcomplexity in the co-evolution of morphology and control, approximatelydoubling the previous state of the art.
arxiv-13200-251 | Exclusive Sparsity Norm Minimization with Random Groups via Cone Projection | http://arxiv.org/pdf/1510.07925v1.pdf | author:Yijun Huang, Ji Liu category:stat.ML cs.LG published:2015-10-27 summary:Many practical applications such as gene expression analysis, multi-tasklearning, image recognition, signal processing, and medical data analysispursue a sparse solution for the feature selection purpose and particularlyfavor the nonzeros \emph{evenly} distributed in different groups. The exclusivesparsity norm has been widely used to serve to this purpose. However, it stilllacks systematical studies for exclusive sparsity norm optimization. This paperoffers two main contributions from the optimization perspective: 1) We provideseveral efficient algorithms to solve exclusive sparsity norm minimization witheither smooth loss or hinge loss (non-smooth loss). All algorithms achieve theoptimal convergence rate $O(1/k^2)$ ($k$ is the iteration number). To the bestof our knowledge, this is the first time to guarantee such convergence rate forthe general exclusive sparsity norm minimization; 2) When the group informationis unavailable to define the exclusive sparsity norm, we propose to use therandom grouping scheme to construct groups and prove that if the number ofgroups is appropriately chosen, the nonzeros (true features) would be groupedin the ideal way with high probability. Empirical studies validate theefficiency of proposed algorithms, and the effectiveness of random groupingscheme on the proposed exclusive SVM formulation.
arxiv-13200-252 | Automagically encoding Adverse Drug Reactions in MedDRA | http://arxiv.org/pdf/1506.08052v2.pdf | author:Carlo Combi, Riccardo Lora, Ugo Moretti, Marco Pagliarini, Margherita Zorzi category:cs.CL published:2015-06-26 summary:Pharmacovigilance is the field of science devoted to the collection, analysisand prevention of Adverse Drug Reactions (ADRs). Efficient strategies for theextraction of information about ADRs from free text resources are essential tosupport the work of experts, employed in the crucial task of detecting andclassifying unexpected pathologies possibly related to drug assumptions.Narrative ADR descriptions may be collected in several way, e.g. by monitoringsocial networks or through the so called spontaneous reporting, the main methodpharmacovigilance adopts in order to identify ADRs. The encoding of free-textADR descriptions according to MedDRA standard terminology is central for reportanalysis. It is a complex work, which has to be manually implemented by thepharmacovigilance experts. The manual encoding is expensive (in terms of time).Moreover, a problem about the accuracy of the encoding may occur, since thenumber of reports is growing up day by day. In this paper, we proposeMagiCoder, an efficient Natural Language Processing algorithm able toautomatically derive MedDRA terminologies from free-text ADR descriptions.MagiCoder is part of VigiWork, a web application for online ADR reporting andanalysis. From a practical view-point, MagiCoder radically reduces the revisiontime of ADR reports: the pharmacologist has simply to revise and validate theautomatic solution versus the hard task of choosing solutions in the 70k termsof MedDRA. This improvement of the expert work efficiency has a meaningfulimpact on the quality of data analysis. Moreover, our procedure is generalpurpose. We developed MagiCoder for the Italian pharmacovigilance language, butpreliminarily analyses show that it is robust to language and dictionarychanges.
arxiv-13200-253 | Standards for language resources in ISO -- Looking back at 13 fruitful years | http://arxiv.org/pdf/1510.07851v1.pdf | author:Laurent Romary category:cs.CL published:2015-10-27 summary:This paper provides an overview of the various projects carried out withinISO committee TC 37/SC 4 dealing with the management of language (digital)resources. On the basis of the technical experience gained in the committee andthe wider standardization landscape the paper identifies some possible trendsfor the future.
arxiv-13200-254 | Estimation Stability with Cross Validation (ESCV) | http://arxiv.org/pdf/1303.3128v2.pdf | author:Chinghway Lim, Bin Yu category:stat.ME stat.ML published:2013-03-13 summary:Cross-validation (CV) is often used to select the regularization parameter inhigh dimensional problems. However, when applied to the sparse modeling methodLasso, CV leads to models that are unstable in high-dimensions, andconsequently not suited for reliable interpretation. In this paper, we proposea model-free criterion ESCV based on a new estimation stability (ES) metric andCV. Our proposed ESCV finds a locally ES-optimal model smaller than the CVchoice so that the it fits the data and also enjoys estimation stabilityproperty. We demonstrate that ESCV is an effective alternative to CV at asimilar easily parallelizable computational cost. In particular, we compare thetwo approaches with respect to several performance measures when applied to theLasso on both simulated and real data sets. For dependent predictors common inpractice, our main finding is that, ESCV cuts down false positive rates oftenby a large margin, while sacrificing little of true positive rates. ESCVusually outperforms CV in terms of parameter estimation while giving similarperformance as CV in terms of prediction. For the two real data sets fromneuroscience and cell biology, the models found by ESCV are less than half ofthe model sizes by CV. Judged based on subject knowledge, they are moreplausible than those by CV as well. We also discuss some regularizationparameter alignment issues that come up in both approaches.
arxiv-13200-255 | Multimodal Task-Driven Dictionary Learning for Image Classification | http://arxiv.org/pdf/1502.01094v2.pdf | author:Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins category:stat.ML cs.CV cs.LG published:2015-02-04 summary:Dictionary learning algorithms have been successfully used for bothreconstructive and discriminative tasks, where an input signal is representedwith a sparse linear combination of dictionary atoms. While these methods aremostly developed for single-modality scenarios, recent studies havedemonstrated the advantages of feature-level fusion based on the joint sparserepresentation of the multimodal inputs. In this paper, we propose a multimodaltask-driven dictionary learning algorithm under the joint sparsity constraint(prior) to enforce collaborations among multiple homogeneous/heterogeneoussources of information. In this task-driven formulation, the multimodaldictionaries are learned simultaneously with their corresponding classifiers.The resulting multimodal dictionaries can generate discriminative latentfeatures (sparse codes) from the data that are optimized for a given task suchas binary or multiclass classification. Moreover, we present an extension ofthe proposed formulation using a mixed joint and independent sparsity priorwhich facilitates more flexible fusion of the modalities at feature level. Theefficacy of the proposed algorithms for multimodal classification isillustrated on four different applications -- multimodal face recognition,multi-view face recognition, multi-view action recognition, and multimodalbiometric recognition. It is also shown that, compared to the counterpartreconstructive-based dictionary learning algorithms, the task-drivenformulations are more computationally efficient in the sense that they can beequipped with more compact dictionaries and still achieve superior performance.
arxiv-13200-256 | An exploration of parameter redundancy in deep networks with circulant projections | http://arxiv.org/pdf/1502.03436v2.pdf | author:Yu Cheng, Felix X. Yu, Rogerio S. Feris, Sanjiv Kumar, Alok Choudhary, Shih-Fu Chang category:cs.CV published:2015-02-11 summary:We explore the redundancy of parameters in deep neural networks by replacingthe conventional linear projection in fully-connected layers with the circulantprojection. The circulant structure substantially reduces memory footprint andenables the use of the Fast Fourier Transform to speed up the computation.Considering a fully-connected neural network layer with d input nodes, and doutput nodes, this method improves the time complexity from O(d^2) to O(dlogd)and space complexity from O(d^2) to O(d). The space savings are particularlyimportant for modern deep convolutional neural network architectures, wherefully-connected layers typically contain more than 90% of the networkparameters. We further show that the gradient computation and optimization ofthe circulant projections can be performed very efficiently. Our experiments onthree standard datasets show that the proposed approach achieves thissignificant gain in storage and efficiency with minimal increase in error ratecompared to neural networks with unstructured projections.
arxiv-13200-257 | Community Detection and Classification in Hierarchical Stochastic Blockmodels | http://arxiv.org/pdf/1503.02115v3.pdf | author:Vince Lyzinski, Minh Tang, Avanti Athreya, Youngser Park, Carey E. Priebe category:stat.ML stat.AP published:2015-03-07 summary:We propose a robust, scalable, integrated methodology for community detectionand community comparison in graphs. In our procedure, we first embed a graphinto an appropriate Euclidean space to obtain a low-dimensional representation,and then cluster the vertices into communities. We next employ nonparametricgraph inference techniques to identify structural similarity among thesecommunities. These two steps are then applied recursively on the communities,allowing us to detect more fine-grained structure. We describe a hierarchicalstochastic blockmodel---namely, a stochastic blockmodel with a naturalhierarchical structure---and establish conditions under which our algorithmyields consistent estimates of model parameters and motifs, which we define tobe stochastically similar groups of subgraphs. Finally, we demonstrate theeffectiveness of our algorithm in both simulated and real data. Specifically,we address the problem of locating similar subcommunities in a partiallyreconstructed Drosophila connectome and in the social network Friendster.
arxiv-13200-258 | Computational models: Bottom-up and top-down aspects | http://arxiv.org/pdf/1510.07748v1.pdf | author:Laurent Itti, Ali Borji category:cs.CV published:2015-10-27 summary:Computational models of visual attention have become popular over the pastdecade, we believe primarily for two reasons: First, models make testablepredictions that can be explored by experimentalists as well as theoreticians,second, models have practical and technological applications of interest to theapplied science and engineering communities. In this chapter, we take acritical look at recent attention modeling efforts. We focus on {\emcomputational models of attention} as defined by Tsotsos \& Rothenstein\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus(typically, an image or video clip), which can possibly also be given some taskdefinition, and which make predictions that can be compared to human or animalbehavioral or physiological responses elicited by the same stimulus and task.Thus, we here place less emphasis on abstract models, phenomenological models,purely data-driven fitting or extrapolation models, or models specificallydesigned for a single task or for a restricted class of stimuli. Fortheoretical models, we refer the reader to a number of previous reviews thataddress attention theories and models more generally\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.
arxiv-13200-259 | Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm | http://arxiv.org/pdf/1503.05479v2.pdf | author:Qinqing Zheng, Ryota Tomioka category:cs.LG cs.AI stat.ML published:2015-03-18 summary:We consider the problem of recovering a low-rank tensor from its noisyobservation. Previous work has shown a recovery guarantee with signal to noiseratio $O(n^{\lceil K/2 \rceil /2})$ for recovering a $K$th order rank onetensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper,we first improve this bound to $O(n^{K/4})$ by a much simpler approach, butwith a more careful analysis. Then we propose a new norm called the subspacenorm, which is based on the Kronecker products of factors obtained by theproposed simple estimator. The imposed Kronecker structure allows us to show anearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound, in which the parameter $H$controls the blend from the non-convex estimator to mode-wise nuclear normminimization. Furthermore, we empirically demonstrate that the subspace normachieves the nearly ideal denoising performance even with $H=O(1)$.
arxiv-13200-260 | Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks | http://arxiv.org/pdf/1510.07641v1.pdf | author:Zachary C. Lipton, David C. Kale, Randall C. Wetzell category:cs.LG published:2015-10-26 summary:We present a novel application of LSTM recurrent neural networks tomultilabel classification of diagnoses given variable-length time series ofclinical measurements. Our method outperforms a strong baseline on a variety ofmetrics.
arxiv-13200-261 | Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction | http://arxiv.org/pdf/1510.07609v1.pdf | author:Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama category:stat.ML cs.LG published:2015-10-26 summary:We study the problem of reducing test-time acquisition costs inclassification systems. Our goal is to learn decision rules that adaptivelyselect sensors for each example as necessary to make a confident prediction. Wemodel our system as a directed acyclic graph (DAG) where internal nodescorrespond to sensor subsets and decision functions at each node choose whetherto acquire a new sensor or classify using the available measurements. Thisproblem can be naturally posed as an empirical risk minimization over trainingdata. Rather than jointly optimizing such a highly coupled and non-convexproblem over all decision nodes, we propose an efficient algorithm motivated bydynamic programming. We learn node policies in the DAG by reducing the globalobjective to a series of cost sensitive learning problems. Our approach iscomputationally efficient and has proven guarantees of convergence to theoptimal system for a fixed architecture. In addition, we present an extensionto map other budgeted learning problems with large number of sensors to our DAGarchitecture and demonstrate empirical performance exceeding state-of-the-artalgorithms for data composed of both few and many sensors.
arxiv-13200-262 | Parser for Abstract Meaning Representation using Learning to Search | http://arxiv.org/pdf/1510.07586v1.pdf | author:Sudha Rao, Yogarshi Vyas, Hal Daume III, Philip Resnik category:cs.CL published:2015-10-26 summary:We develop a novel technique to parse English sentences into Abstract MeaningRepresentation (AMR) using SEARN, a Learning to Search approach, by modelingthe concept and the relation learning in a unified framework. We evaluate ourparser on multiple datasets from varied domains and show an absoluteimprovement of 2% to 6% over the state-of-the-art. Additionally we show thatusing the most frequent concept gives us a baseline that is stronger than thestate-of-the-art for concept prediction. We plan to release our parser forpublic use.
arxiv-13200-263 | Parallelizing MCMC with Random Partition Trees | http://arxiv.org/pdf/1506.03164v2.pdf | author:Xiangyu Wang, Fangjian Guo, Katherine A. Heller, David B. Dunson category:stat.ML published:2015-06-10 summary:The modern scale of data has brought new challenges to Bayesian inference. Inparticular, conventional MCMC algorithms are computationally very expensive forlarge data sets. A promising approach to solve this problem is embarrassinglyparallel MCMC (EP-MCMC), which first partitions the data into multiple subsetsand runs independent sampling algorithms on each subset. The subset posteriordraws are then aggregated via some combining rules to obtain the finalapproximation. Existing EP-MCMC algorithms are limited by approximationaccuracy and difficulty in resampling. In this article, we propose a newEP-MCMC algorithm PART that solves these problems. The new algorithm appliesrandom partition trees to combine the subset posterior draws, which isdistribution-free, easy to resample from and can adapt to multiple scales. Weprovide theoretical justification and extensive experiments illustratingempirical performance.
arxiv-13200-264 | Generalized Regressive Motion: a Visual Cue to Collision | http://arxiv.org/pdf/1510.07573v1.pdf | author:Krzysztof Chalupka, Michael Dickinson, Pietro Perona category:cs.RO cs.CV cs.MA cs.SY published:2015-10-26 summary:Brains and sensory systems evolved to guide motion. Central to this task iscontrolling the approach to stationary obstacles and detecting movingorganisms. Looming has been proposed as the main monocular visual cue fordetecting the approach of other animals and avoiding collisions with stationaryobstacles. Elegant neural mechanisms for looming detection have been found inthe brain of insects and vertebrates. However, looming has not been analyzed inthe context of collisions between two moving animals. We propose an alternativestrategy, Generalized Regressive Motion (GRM), which is consistent withrecently observed behavior in fruit flies. Geometric analysis proves that GRMis a reliable cue to collision among conspecifics, whereas agent-based modelingsuggests that GRM is a better cue than looming as a means to detect approach,prevent collisions and maintain mobility.
arxiv-13200-265 | Active skeleton for bacteria modeling | http://arxiv.org/pdf/1507.06504v3.pdf | author:Jean-Pascal Jacob, Mariella Dimiccoli, Lionel Moisan category:cs.CV published:2015-07-23 summary:The investigation of spatio-temporal dynamics of bacterial cells and theirmolecular components requires automated image analysis tools to track cellshape properties and molecular component locations inside the cells. In thestudy of bacteria aging, the molecular components of interest are proteinaggregates accumulated near bacteria boundaries. This particular location makesvery ambiguous the correspondence between aggregates and cells, since computingaccurately bacteria boundaries in phase-contrast time-lapse imaging is achallenging task. This paper proposes an active skeleton formulation forbacteria modeling which provides several advantages: an easy computation ofshape properties (perimeter, length, thickness, orientation), an improvedboundary accuracy in noisy images, and a natural bacteria-centered coordinatesystem that permits the intrinsic location of molecular components inside thecell. Starting from an initial skeleton estimate, the medial axis of thebacterium is obtained by minimizing an energy function which incorporatesbacteria shape constraints. Experimental results on biological images andcomparative evaluation of the performances validate the proposed approach formodeling cigar-shaped bacteria like Escherichia coli. The Image-J plugin of theproposed method can be found online at http://fluobactracker.inrialpes.fr.
arxiv-13200-266 | Aggregating Deep Convolutional Features for Image Retrieval | http://arxiv.org/pdf/1510.07493v1.pdf | author:Artem Babenko, Victor Lempitsky category:cs.CV published:2015-10-26 summary:Several recent works have shown that image descriptors produced by deepconvolutional neural networks provide state-of-the-art performance for imageclassification and retrieval problems. It has also been shown that theactivations from the convolutional layers can be interpreted as local featuresdescribing particular image regions. These local features can be aggregatedusing aggregation approaches developed for local features (e.g. Fishervectors), thus providing new powerful global descriptors. In this paper we investigate possible ways to aggregate local deep featuresto produce compact global descriptors for image retrieval. First, we show thatdeep features and traditional hand-engineered features have quite differentdistributions of pairwise similarities, hence existing aggregation methods haveto be carefully re-evaluated. Such re-evaluation reveals that in contrast toshallow features, the simple aggregation method based on sum pooling providesarguably the best performance for deep convolutional features. This method isefficient, has few parameters, and bears little risk of overfitting when e.g.learning the PCA matrix. Overall, the new compact global descriptor improvesthe state-of-the-art on four common benchmarks considerably.
arxiv-13200-267 | A Markov Random Field and Active Contour Image Segmentation Model for Animal Spots Patterns | http://arxiv.org/pdf/1510.07474v1.pdf | author:Alexander GÃ³mez, German DÃ­ez, Jhony Giraldo, Augusto Salazar, Juan M. Daza category:cs.CV published:2015-10-26 summary:Non-intrusive biometrics of animals using images allows to analyze phenotypicpopulations and individuals with patterns like stripes and spots withoutaffecting the studied subjects. However, non-intrusive biometrics demand a welltrained subject or the development of computer vision algorithms that ease theidentification task. In this work, an analysis of classic segmentationapproaches that require a supervised tuning of their parameters such asthreshold, adaptive threshold, histogram equalization, and saturationcorrection is presented. In contrast, a general unsupervised algorithm usingMarkov Random Fields (MRF) for segmentation of spots patterns is proposed.Active contours are used to boost results using MRF output as seeds. As studysubject the Diploglossus millepunctatus lizard is used. The proposed methodachieved a maximum efficiency of $91.11\%$.
arxiv-13200-268 | A Parallel algorithm for $\mathcal{X}$-Armed bandits | http://arxiv.org/pdf/1510.07471v1.pdf | author:Cheng Chen, Shuang Liu, Zhihua Zhang, Wu-Jun Li category:stat.ML cs.LG published:2015-10-26 summary:The target of $\mathcal{X}$-armed bandit problem is to find the globalmaximum of an unknown stochastic function $f$, given a finite budget of $n$evaluations. Recently, $\mathcal{X}$-armed bandits have been widely used inmany situations. Many of these applications need to deal with large-scale datasets. To deal with these large-scale data sets, we study a distributed settingof $\mathcal{X}$-armed bandits, where $m$ players collaborate to find themaximum of the unknown function. We develop a novel anytime distributed$\mathcal{X}$-armed bandit algorithm. Compared with prior work on$\mathcal{X}$-armed bandits, our algorithm uses a quite different searchingstrategy so as to fit distributed learning scenarios. Our theoretical analysisshows that our distributed algorithm is $m$ times faster than the classicalsingle-player algorithm. Moreover, the number of communication rounds of ouralgorithm is only logarithmic in $mn$. The numerical results show that ourmethod can make effective use of every players to minimize the loss. Thus, ourdistributed approach is attractive and useful.
arxiv-13200-269 | Object Oriented Analysis using Natural Language Processing concepts: A Review | http://arxiv.org/pdf/1510.07439v1.pdf | author:Abinash Tripathy, Santanu Kumar Rath category:cs.SE cs.CL published:2015-10-26 summary:The Software Development Life Cycle (SDLC) starts with eliciting requirementsof the customers in the form of Software Requirement Specification (SRS). SRSdocument needed for software development is mostly written in NaturalLanguage(NL) convenient for the client. From the SRS document only, the classname, its attributes and the functions incorporated in the body of the classare traced based on pre-knowledge of analyst. The paper intends to present areview on Object Oriented (OO) analysis using Natural Language Processing (NLP)techniques. This analysis can be manual where domain expert helps to generatethe required diagram or automated system, where the system generates therequired diagram, from the input in the form of SRS.
arxiv-13200-270 | How good is good enough? Re-evaluating the bar for energy disaggregation | http://arxiv.org/pdf/1510.08713v1.pdf | author:Nipun Batra, Rishi Baijal, Amarjeet Singh, Kamin Whitehouse category:cs.LG published:2015-10-26 summary:Since the early 1980s, the research community has developed ever moresophisticated algorithms for the problem of energy disaggregation, but despitedecades of research, there is still a dearth of applications with demonstratedvalue. In this work, we explore a question that is highly pertinent to thisresearch community: how good does energy disaggregation need to be in order toinfer characteristics of a household? We present novel techniques that useunsupervised energy disaggregation to predict both household occupancy andstatic properties of the household such as size of the home and number ofoccupants. Results show that basic disaggregation approaches performs up to 30%better at occupancy estimation than using aggregate power data alone, and areup to 10% better at estimating static household characteristics. These resultsshow that even rudimentary energy disaggregation techniques are sufficient forimproved inference of household characteristics. To conclude, we re-evaluatethe bar set by the community for energy disaggregation accuracy and try toanswer the question "how good is good enough?"
arxiv-13200-271 | Vehicle Color Recognition using Convolutional Neural Network | http://arxiv.org/pdf/1510.07391v1.pdf | author:Reza Fuad Rachmadi, I Ketut Eddy Purnama category:cs.CV published:2015-10-26 summary:Vehicle color information is one of the important elements in ITS(Intelligent Traffic System). In this paper, we present a vehicle colorrecognition method using convolutional neural network (CNN). Naturally, CNN isdesigned to learn classification method based on shape information, but weproved that CNN can also learn classification based on color distribution. Inour method, we convert the input image to two different color spaces, HSV andCIE Lab, and run it to some CNN architecture. The training process followprocedure introduce by Krizhevsky, that learning rate is decreasing by factorof 10 after some iterations. To test our method, we use publicly vehicle colorrecognition dataset provided by Chen. The results, our model outperform theoriginal system provide by Chen with 2% higher overall accuracy.
arxiv-13200-272 | Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for Mobile Security Robots | http://arxiv.org/pdf/1510.07390v1.pdf | author:YongChol Sin, MyongSong Choe, GyongIl Ryang category:cs.RO cs.CV published:2015-10-26 summary:One of fundamental issues for security robots is to detect and track peoplein the surroundings. The main problems of this task are real-time constraints,a changing background, varying illumination conditions and a non-rigid shape ofthe person to be tracked. In this paper, we propose a solution for trackingwith a pan-tilt camera and a passive infrared range (PIR) sensor to detect themoving object based on consecutive frame difference. The proposed method isexcellent in real-time performance because it requires only a little memory andcomputation. Experiment results show that this method can detect the movingobject such as human efficiently and accurately in non-stationary and complexindoor environment.
arxiv-13200-273 | Neighbourhood NILM: A Big-data Approach to Household Energy Disaggregation | http://arxiv.org/pdf/1511.02900v1.pdf | author:Nipun Batra, Amarjeet Singh, Kamin Whitehouse category:cs.LG published:2015-10-26 summary:In this paper, we investigate whether "big-data" is more valuable than"precise" data for the problem of energy disaggregation: the process ofbreaking down aggregate energy usage on a per-appliance basis. Existingtechniques for disaggregation rely on energy metering at a resolution of 1minute or higher, but most power meters today only provide a reading once permonth, and at most once every 15 minutes. In this paper, we propose a newtechnique called Neighbourhood NILM that leverages data from 'neighbouring'homes to disaggregate energy given only a single energy reading per month. Thekey intuition behind our approach is that 'similar' homes have 'similar' energyconsumption on a per-appliance basis. Neighbourhood NILM matches every homewith a set of 'neighbours' that have direct submetering infrastructure, i.e.power meters on individual circuits or loads. Many such homes already exist.Then, it estimates the appliance-level energy consumption of the target home tobe the average of its K neighbours. We evaluate this approach using 25 homesand results show that our approach gives comparable or better disaggregation incomparison to state-of-the-art accuracy reported in the literature that dependon manual model training, high frequency power metering, or both. Results showthat Neighbourhood NILM can achieve 83% and 79% accuracy disaggregating fridgeand heating/cooling loads, compared to 74% and 73% for a technique called FHMM.Furthermore, it achieves up to 64% accuracy on washing machine, dryer,dishwasher, and lighting loads, which is higher than previously reportedresults. Many existing techniques are not able to disaggregate these loads atall. These results indicate a potentially substantial advantage to installingsubmetering infrastructure in a select few homes rather than installing newhigh-frequency smart metering infrastructure in all homes.
arxiv-13200-274 | How to merge three different methods for information filtering ? | http://arxiv.org/pdf/1510.07385v1.pdf | author:Jean-ValÃ¨re Cossu, Ludovic Bonnefoy, Xavier Bost, Marc El BÃ¨ze category:cs.CL cs.IR published:2015-10-26 summary:Twitter is now a gold marketing tool for entities concerned with onlinereputation. To automatically monitor online reputation of entities , systemshave to deal with ambiguous entity names, polarity detection and topicdetection. We propose three approaches to tackle the first issue: monitoringTwitter in order to find relevant tweets about a given entity. Evaluated withinthe framework of the RepLab-2013 Filtering task, each of them has been showncompetitive with state-of-the-art approaches. Mainly we investigate on how muchmerging strategies may impact performances on a filtering task according to theevaluation measure.
arxiv-13200-275 | Mining User Opinions in Mobile App Reviews: A Keyword-based Approach | http://arxiv.org/pdf/1505.04657v2.pdf | author:Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen category:cs.IR cs.CL published:2015-05-18 summary:User reviews of mobile apps often contain complaints or suggestions which arevaluable for app developers to improve user experience and satisfaction.However, due to the large volume and noisy-nature of those reviews, manuallyanalyzing them for useful opinions is inherently challenging. To address thisproblem, we propose MARK, a keyword-based framework for semi-automated reviewanalysis. MARK allows an analyst describing his interests in one or some mobileapps by a set of keywords. It then finds and lists the reviews most relevant tothose keywords for further analysis. It can also draw the trends over time ofthose keywords and detect their sudden changes, which might indicate theoccurrences of serious issues. To help analysts describe their interests moreeffectively, MARK can automatically extract keywords from raw reviews and rankthem by their associations with negative reviews. In addition, based on avector-based semantic representation of keywords, MARK can divide a large setof keywords into more cohesive subsets, or suggest keywords similar to theselected ones.
arxiv-13200-276 | Finding Temporally Consistent Occlusion Boundaries in Videos using Geometric Context | http://arxiv.org/pdf/1510.07323v1.pdf | author:S. Hussain Raza, Ahmad Humayun, Matthias Grundmann, David Anderson, Irfan Essa category:cs.CV published:2015-10-25 summary:We present an algorithm for finding temporally consistent occlusionboundaries in videos to support segmentation of dynamic scenes. We learnocclusion boundaries in a pairwise Markov random field (MRF) framework. Wefirst estimate the probability of an spatio-temporal edge being an occlusionboundary by using appearance, flow, and geometric features. Next, we enforceocclusion boundary continuity in a MRF model by learning pairwise occlusionprobabilities using a random forest. Then, we temporally smooth boundaries toremove temporal inconsistencies in occlusion boundary estimation. Our proposedframework provides an efficient approach for finding temporally consistentocclusion boundaries in video by utilizing causality, redundancy in videos, andsemantic layout of the scene. We have developed a dataset with fully annotatedground-truth occlusion boundaries of over 30 videos ($5000 frames). Thisdataset is used to evaluate temporal occlusion boundaries and provides a muchneeded baseline for future studies. We perform experiments to demonstrate therole of scene layout, and temporal information for occlusion reasoning indynamic scenes.
arxiv-13200-277 | Geometric Context from Videos | http://arxiv.org/pdf/1510.07320v1.pdf | author:S. Hussain Raza, Matthias Grundmann, Irfan Essa category:cs.CV published:2015-10-25 summary:We present a novel algorithm for estimating the broad 3D geometric structureof outdoor video scenes. Leveraging spatio-temporal video segmentation, wedecompose a dynamic scene captured by a video into geometric classes, based onpredictions made by region-classifiers that are trained on appearance andmotion features. By examining the homogeneity of the prediction, we combinepredictions across multiple segmentation hierarchy levels alleviating the needto determine the granularity a priori. We built a novel, extensive dataset ongeometric context of video to evaluate our method, consisting of over 100ground-truth annotated outdoor videos with over 20,000 frames. To further scalebeyond this dataset, we propose a semi-supervised learning framework to expandthe pool of labeled data with high confidence predictions obtained fromunlabeled data. Our system produces an accurate prediction of geometric contextof video achieving 96% accuracy across main geometric classes.
arxiv-13200-278 | Depth Extraction from Videos Using Geometric Context and Occlusion Boundaries | http://arxiv.org/pdf/1510.07317v1.pdf | author:S. Hussain Raza, Omar Javed, Aveek Das, Harpreet Sawhney, Hui Cheng, Irfan Essa category:cs.CV published:2015-10-25 summary:We present an algorithm to estimate depth in dynamic video scenes. We proposeto learn and infer depth in videos from appearance, motion, occlusionboundaries, and geometric context of the scene. Using our method, depth can beestimated from unconstrained videos with no requirement of camera poseestimation, and with significant background/foreground motions. We start bydecomposing a video into spatio-temporal regions. For each spatio-temporalregion, we learn the relationship of depth to visual appearance, motion, andgeometric classes. Then we infer the depth information of new scenes usingpiecewise planar parametrization estimated within a Markov random field (MRF)framework by combining appearance to depth learned mappings and occlusionboundary guided smoothness constraints. Subsequently, we perform temporalsmoothing to obtain temporally consistent depth maps. To evaluate our depthestimation algorithm, we provide a novel dataset with ground truth depth foroutdoor video scenes. We present a thorough evaluation of our algorithm on ournew dataset and the publicly available Make3d static image dataset.
arxiv-13200-279 | A Framework for Distributed Deep Learning Layer Design in Python | http://arxiv.org/pdf/1510.07303v1.pdf | author:Clay McLeod category:cs.LG published:2015-10-25 summary:In this paper, a framework for testing Deep Neural Network (DNN) design inPython is presented. First, big data, machine learning (ML), and ArtificialNeural Networks (ANNs) are discussed to familiarize the reader with theimportance of such a system. Next, the benefits and detriments of implementingsuch a system in Python are presented. Lastly, the specifics of the system areexplained, and some experimental results are presented to prove theeffectiveness of the system.
arxiv-13200-280 | Deep learning with Elastic Averaging SGD | http://arxiv.org/pdf/1412.6651v8.pdf | author:Sixin Zhang, Anna Choromanska, Yann LeCun category:cs.LG stat.ML published:2014-12-20 summary:We study the problem of stochastic optimization for deep learning in theparallel computing environment under communication constraints. A new algorithmis proposed in this setting where the communication and coordination of workamong concurrent processes (local workers), is based on an elastic force whichlinks the parameters they compute with a center variable stored by theparameter server (master). The algorithm enables the local workers to performmore exploration, i.e. the algorithm allows the local variables to fluctuatefurther from the center variable by reducing the amount of communicationbetween local workers and the master. We empirically demonstrate that in thedeep learning setting, due to the existence of many local optima, allowing moreexploration can lead to the improved performance. We propose synchronous andasynchronous variants of the new algorithm. We provide the stability analysisof the asynchronous variant in the round-robin scheme and compare it with themore common parallelized method ADMM. We show that the stability of EASGD isguaranteed when a simple stability condition is satisfied, which is not thecase for ADMM. We additionally propose the momentum-based version of ouralgorithm that can be applied in both synchronous and asynchronous settings.Asynchronous variant of the algorithm is applied to train convolutional neuralnetworks for image classification on the CIFAR and ImageNet datasets.Experiments demonstrate that the new algorithm accelerates the training of deeparchitectures compared to DOWNPOUR and other common baseline approaches andfurthermore is very communication efficient.
arxiv-13200-281 | Seam Puckering Objective Evaluation Method for Sewing Process | http://arxiv.org/pdf/1510.07234v1.pdf | author:Raluca Brad, Eugen HÄloiu, Remus Brad category:cs.CV cs.CE published:2015-10-25 summary:The paper presents an automated method for the assessment and classificationof puckering defects detected during the preproduction control stage of thesewing machine or product inspection. In this respect, we have presented thepossible causes and remedies of the wrinkle nonconformities. Subjective factorsrelated to the control environment and operators during the seams evaluationcan be reduced using an automated system whose operation is based on imageprocessing. Our implementation involves spectral image analysis using Fouriertransform and an unsupervised neural network, the Kohonen Map, employed toclassify material specimens, the input images, into five discrete degrees ofquality, from grade 5 (best) to grade 1 (the worst).
arxiv-13200-282 | Random Projections through multiple optical scattering: Approximating kernels at the speed of light | http://arxiv.org/pdf/1510.06664v2.pdf | author:Alaa Saade, Francesco Caltagirone, Igor Carron, Laurent Daudet, AngÃ©lique DrÃ©meau, Sylvain Gigan, Florent Krzakala category:cs.ET cs.LG physics.optics published:2015-10-22 summary:Random projections have proven extremely useful in many signal processing andmachine learning applications. However, they often require either to store avery large random matrix, or to use a different, structured matrix to reducethe computational and memory costs. Here, we overcome this difficulty byproposing an analog, optical device, that performs the random projectionsliterally at the speed of light without having to store any matrix in memory.This is achieved using the physical properties of multiple coherent scatteringof coherent light in random media. We use this device on a simple task ofclassification with a kernel machine, and we show that, on the MNIST database,the experimental results closely match the theoretical performance of thecorresponding kernel. This framework can help make kernel methods practical forapplications that have large training sets and/or require real-time prediction.We discuss possible extensions of the method in terms of a class of kernels,speed, memory consumption and different problems.
arxiv-13200-283 | Defect Detection Techniques for Airbag Production Sewing Stages | http://arxiv.org/pdf/1510.07905v1.pdf | author:Raluca Brad, Lavinia Barac, Remus Brad category:cs.CV published:2015-10-25 summary:Airbags are subject to strict quality control in order to ensure passengerssafety. The quality of fabric and sewing thread influence the final product andtherefore, sewing defects must be early and accurately detected, in order toremove the item from production. Airbag seams assembly can take various forms,using linear and circle primitives, with threads of different colors and lengthdensities, creating lockstitch or double threads chainstitch. The paperpresents a framework for the automatic detection of defects occurring duringthe airbag sewing stage. Types of defects as skipped stitch, missed stitch orsuperimposed seam for lockstitch and two threads chainstitch are detected andmarked. Using image processing methods, the proposed framework follows theseams path and determines if a color pattern of the considered stitches isvalid.
arxiv-13200-284 | On End-to-End Program Generation from User Intention by Deep Neural Networks | http://arxiv.org/pdf/1510.07211v1.pdf | author:Lili Mou, Rui Men, Ge Li, Lu Zhang, Zhi Jin category:cs.SE cs.LG published:2015-10-25 summary:This paper envisions an end-to-end program generation scenario usingrecurrent neural networks (RNNs): Users can express their intention in naturallanguage; an RNN then automatically generates corresponding code in acharacterby-by-character fashion. We demonstrate its feasibility through a casestudy and empirical analysis. To fully make such technique useful in practice,we also point out several cross-disciplinary challenges, including modelinguser intention, providing datasets, improving model architectures, etc.Although much long-term research shall be addressed in this new field, webelieve end-to-end program generation would become a reality in future decades,and we are looking forward to its practice.
arxiv-13200-285 | Vehicle Speed Prediction using Deep Learning | http://arxiv.org/pdf/1510.07208v1.pdf | author:Joe Lemieux, Yuan Ma category:cs.LG cs.NE published:2015-10-25 summary:Global optimization of the energy consumption of dual power source vehiclessuch as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug infuel cell electric vehicles requires knowledge of the complete routecharacteristics at the beginning of the trip. One of the main characteristicsis the vehicle speed profile across the route. The profile will translatedirectly into energy requirements for a given vehicle. However, the vehiclespeed that a given driver chooses will vary from driver to driver and from timeto time, and may be slower, equal to, or faster than the average traffic flow.If the specific driver speed profile can be predicted, the energy usage can beoptimized across the route chosen. The purpose of this paper is to research theapplication of Deep Learning techniques to this problem to identify at thebeginning of a drive cycle the driver specific vehicle speed profile for anindividual driver repeated drive cycle, which can be used in an optimizationalgorithm to minimize the amount of fossil fuel energy used during the trip.
arxiv-13200-286 | Structured Memory for Neural Turing Machines | http://arxiv.org/pdf/1510.03931v3.pdf | author:Wei Zhang, Yang Yu, Bowen Zhou category:cs.AI cs.NE I.2.6 published:2015-10-14 summary:Neural Turing Machines (NTM) contain memory component that simulates "workingmemory" in the brain to store and retrieve information to ease simplealgorithms learning. So far, only linearly organized memory is proposed, andduring experiments, we observed that the model does not always converge, andoverfits easily when handling certain tasks. We think memory component is keyto some faulty behaviors of NTM, and better organization of memory componentcould help fight those problems. In this paper, we propose several differentstructures of memory for NTM, and we proved in experiments that two of ourproposed structured-memory NTMs could lead to better convergence, in term ofspeed and prediction accuracy on copy task and associative recall task as in(Graves et al. 2014).
arxiv-13200-287 | Statistical Parsing by Machine Learning from a Classical Arabic Treebank | http://arxiv.org/pdf/1510.07193v1.pdf | author:Kais Dukes category:cs.CL published:2015-10-25 summary:Research into statistical parsing for English has enjoyed over a decade ofsuccessful results. However, adapting these models to other languages has metwith difficulties. Previous comparative work has shown that Modern Arabic isone of the most difficult languages to parse due to rich morphology and freeword order. Classical Arabic is the ancient form of Arabic, and is understudiedin computational linguistics, relative to its worldwide reach as the languageof the Quran. The thesis is based on seven publications that make significantcontributions to knowledge relating to annotating and parsing Classical Arabic. A central argument of this thesis is that using a hybrid representationclosely aligned to traditional grammar leads to improved parsing for Arabic. Totest this hypothesis, two approaches are compared. As a reference, a puredependency parser is adapted using graph transformations, resulting in an87.47% F1-score. This is compared to an integrated parsing model with anF1-score of 89.03%, demonstrating that joint dependency-constituency parsing isbetter suited to Classical Arabic.
arxiv-13200-288 | An Omnibus Nonparametric Test of Equality in Distribution for Unknown Functions | http://arxiv.org/pdf/1510.04195v2.pdf | author:Alexander R. Luedtke, Marco Carone, Mark J. van der Laan category:math.ST stat.ML stat.TH 62G10 published:2015-10-14 summary:We present a novel family of nonparametric omnibus tests of the hypothesisthat two unknown but estimable functions are equal in distribution when appliedto the observed data structure. We developed these tests, which represent ageneralization of the maximum mean discrepancy tests described in Gretton etal. [2006], using recent developments from the higher-order pathwisedifferentiability literature. Despite their complex derivation, the associatedtest statistics can be expressed rather simply as U-statistics. We study theasymptotic behavior of the proposed tests under the null hypothesis and underboth fixed and local alternatives. We provide examples to which our tests canbe applied and show that they perform well in a simulation study. As animportant special case, our proposed tests can be used to determine whether anunknown function, such as the conditional average treatment effect, is equal tozero almost surely.
arxiv-13200-289 | Computational models of attention | http://arxiv.org/pdf/1510.07182v1.pdf | author:Laurent Itti, Ali Borji category:cs.CV published:2015-10-24 summary:This chapter reviews recent computational models of visual attention. Webegin with models for the bottom-up or stimulus-driven guidance of attention tosalient visual items, which we examine in seven different broad categories. Wethen examine more complex models which address the top-down or goal-orientedguidance of attention towards items that are more relevant to the task at hand.
arxiv-13200-290 | Fast and Scalable Lasso via Stochastic Frank-Wolfe Methods with a Convergence Guarantee | http://arxiv.org/pdf/1510.07169v1.pdf | author:Emanuele Frandi, Ricardo Nanculef, Stefano Lodi, Claudio Sartori, Johan A. K. Suykens category:stat.ML cs.LG math.OC published:2015-10-24 summary:Frank-Wolfe (FW) algorithms have been often proposed over the last few yearsas efficient solvers for a variety of optimization problems arising in thefield of Machine Learning. The ability to work with cheap projection-freeiterations and the incremental nature of the method make FW a very effectivechoice for many large-scale problems where computing a sparse model isdesirable. In this paper, we present a high-performance implementation of the FW methodtailored to solve large-scale Lasso regression problems, based on a randomizediteration, and prove that the convergence guarantees of the standard FW methodare preserved in the stochastic setting. We show experimentally that ouralgorithm outperforms several existing state of the art methods, including theCoordinate Descent algorithm by Friedman et al. (one of the fastest known Lassosolvers), on several benchmark datasets with a very large number of features,without sacrificing the accuracy of the model. Our results illustrate that thealgorithm is able to generate the complete regularization path on problems ofsize up to four million variables in less than one minute.
arxiv-13200-291 | Evolutionary Landscape and Management of Population Diversity | http://arxiv.org/pdf/1510.07163v1.pdf | author:Maumita Bhattacharya category:cs.NE 68T99 published:2015-10-24 summary:The search ability of an Evolutionary Algorithm (EA) depends on the variationamong the individuals in the population [3, 4, 8]. Maintaining an optimal levelof diversity in the EA population is imperative to ensure that progress of theEA search is unhindered by premature convergence to suboptimal solutions.Clearer understanding of the concept of population diversity, in the context ofevolutionary search and premature convergence in particular, is the key todesigning efficient EAs. To this end, this paper first presents a briefanalysis of the EA population diversity issues. Next we present aninvestigation on a counter-niching EA technique [4] that introduces andmaintains constructive diversity in the population. The proposed approach usesinformed genetic operations to reach promising, but unexplored orunder-explored areas of the search space, while discouraging premature localconvergence. Simulation runs on a suite of standard benchmark test functionswith Genetic Algorithm (GA) implementation shows promising results.
arxiv-13200-292 | Data-driven detrending of nonstationary fractal time series with echo state networks | http://arxiv.org/pdf/1510.07146v1.pdf | author:Enrico Maiorino, Filippo Maria Bianchi, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.LG cs.NE published:2015-10-24 summary:In this paper, we propose a data-driven approach to the problem of detrendingfractal and multifractal time series. We consider a time series as themeasurements elaborated from a dynamical process over time. We assume that sucha dynamical process is predictable to a certain degree, by means of a class ofrecurrent networks called echo state networks. Such networks have been shown tobe able to predict the outcome of a number of dynamical processes. Here wepropose to perform a data-driven detrending of nonstationary, fractal andmultifractal time series by using an echo state network operating as a filter.Notably, we predict the trend component of a given input time series, which issuperimposed to the (multi)fractal component of interest. Such a (estimated)trend is then removed from the original time series and the residual signal isanalyzed with the Multifractal Detrended Fluctuation Analysis for aquantitative verification of the correctness of the proposed detrendingprocedure. In order to demonstrate the effectiveness of the proposed technique,we consider several synthetic time series having a self-similar noise componentwith known characteristics. Such synthetic time series contain different typesof trends. We also process a real-world dataset, the sunspot time series, whichis well-known for its multifractal features and it has recently gainedattention in the complex systems field. Results demonstrate the validity andgenerality of the proposed detrending method based on echo state networks.
arxiv-13200-293 | Higher-order MRFs based image super resolution: why not MAP? | http://arxiv.org/pdf/1410.7429v4.pdf | author:Yunjin Chen category:cs.CV published:2014-10-27 summary:A trainable filter-based higher-order Markov Random Fields (MRFs) model - theso called Fields of Experts (FoE), has proved a highly effective image priormodel for many classic image restoration problems. Generally, two options areavailable to incorporate the learned FoE prior in the inference procedure: (1)sampling-based minimum mean square error (MMSE) estimate, and (2) energyminimization-based maximum a posteriori (MAP) estimate. This letter is devotedto the FoE prior based single image super resolution (SR) problem, and wesuggest to make use of the MAP estimate for inference based on two facts: (I)It is well-known that the MAP inference has a remarkable advantage of highcomputational efficiency, while the sampling-based MMSE estimate is very timeconsuming. (II) Practical SR experiment results demonstrate that the MAPestimate works equally well compared to the MMSE estimate with exactly the sameFoE prior model. Moreover, it can lead to even further improvements byincorporating our discriminatively trained FoE prior model. In summary, we holdthat for higher-order natural image prior based SR problem, it is better toemploy the MAP estimate for inference.
arxiv-13200-294 | Image Parsing with a Wide Range of Classes and Scene-Level Context | http://arxiv.org/pdf/1510.07136v1.pdf | author:Marian George category:cs.CV published:2015-10-24 summary:This paper presents a nonparametric scene parsing approach that improves theoverall accuracy, as well as the coverage of foreground classes in sceneimages. We first improve the label likelihood estimates at superpixels bymerging likelihood scores from different probabilistic classifiers. This booststhe classification performance and enriches the representation ofless-represented classes. Our second contribution consists of incorporatingsemantic context in the parsing process through global label costs. Our methoddoes not rely on image retrieval sets but rather assigns a global likelihoodestimate to each label, which is plugged into the overall energy function. Weevaluate our system on two large-scale datasets, SIFTflow and LMSun. We achievestate-of-the-art performance on the SIFTflow dataset and near-record results onLMSun.
arxiv-13200-295 | On the Statistical Efficiency of $\ell_{1,p}$ Multi-Task Learning of Gaussian Graphical Models | http://arxiv.org/pdf/1207.4255v2.pdf | author:Jean Honorio, Tommi Jaakkola, Dimitris Samaras category:cs.LG stat.ML published:2012-07-18 summary:In this paper, we present $\ell_{1,p}$ multi-task structure learning forGaussian graphical models. We analyze the sufficient number of samples for thecorrect recovery of the support union and edge signs. We also analyze thenecessary number of samples for any conceivable method by providinginformation-theoretic lower bounds. We compare the statistical efficiency ofmulti-task learning versus that of single-task learning. For experiments, weuse a block coordinate descent method that is provably convergent and generatesa sequence of positive definite solutions. We provide experimental validationon synthetic data as well as on two publicly available real-world data sets,including functional magnetic resonance imaging and gene expression data.
arxiv-13200-296 | Predicting Face Recognition Performance Using Image Quality | http://arxiv.org/pdf/1510.07119v1.pdf | author:Abhishek Dutta, Raymond Veldhuis, Luuk Spreeuwers category:cs.CV published:2015-10-24 summary:This paper proposes a data driven model to predict the performance of a facerecognition system based on image quality features. We model the relationshipbetween image quality features (e.g. pose, illumination, etc.) and recognitionperformance measures using a probability density function. To address the issueof limited nature of practical training data inherent in most data drivenmodels, we have developed a Bayesian approach to model the distribution ofrecognition performance measures in small regions of the quality space. Sincethe model is based solely on image quality features, it can predict performanceeven before the actual recognition has taken place. We evaluate the performancepredictive capabilities of the proposed model for six face recognition systems(two commercial and four open source) operating on three independent data sets:MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model canaccurately predict performance using an accurate and unbiased Image QualityAssessor (IQA). Furthermore, our experiments highlight the impact of theunaccounted quality space -- the image quality features not considered by IQA-- in contributing to performance prediction errors.
arxiv-13200-297 | Predicting Performance of a Face Recognition System Based on Image Quality | http://arxiv.org/pdf/1510.07112v1.pdf | author:Abhishek Dutta category:cs.CV published:2015-10-24 summary:In this dissertation, we present a generative model to capture the relationbetween facial image quality features (like pose, illumination direction, etc)and face recognition performance. Such a model can be used to predict theperformance of a face recognition system. Since the model is based solely onimage quality features, performance predictions can be done even before theactual recognition has taken place thereby facilitating many preemptive action.A practical limitation of such a data driven generative model is the limitednature of training data set. To address this limitation, we have developed aBayesian approach to model the distribution of recognition performance measurebased on the number of match and non-match scores in small regions of the imagequality space. Random samples drawn from these models provide the initial dataessential for training the generative model. Experiment results based on sixface recognition systems operating on three independent data sets show that theproposed performance prediction model can accurately predict face recognitionperformance using an accurate and unbiased Image Quality Assessor (IQA).Furthermore, our results show that variability in the unaccounted quality space-- the image quality features not considered by the IQA -- is the major factorcausing inaccuracies in predicted performance.
arxiv-13200-298 | Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media | http://arxiv.org/pdf/1510.07099v1.pdf | author:Yao Yushi, Huang Zheng category:cs.CL published:2015-10-24 summary:In this paper, we propose a joint algorithm for the word segmentation onChinese social media. Previous work mainly focus on word segmentation for plainChinese text, in order to develop a Chinese social media processing tool, weneed to take the main features of social media into account, whose grammaticalstructure is not rigorous, and the tendency of using colloquial and Internetterms makes the existing Chinese-processing tools inefficient to obtain goodperformance on social media. In our approach, we combine CRF and MMSEG algorithm and extend features oftraditional CRF algorithm to train the model for word segmentation, We useInternet lexicon in order to improve the performance of our model on Chinesesocial media. Our experimental result on Sina Weibo shows that our approachoutperforms the state-of-the-art model.
arxiv-13200-299 | Generic decoding of seen and imagined objects using hierarchical visual features | http://arxiv.org/pdf/1510.06479v2.pdf | author:Tomoyasu Horikawa, Yukiyasu Kamitani category:q-bio.NC cs.CV published:2015-10-22 summary:Object recognition is a key function in both human and machine vision. Whilerecent studies have achieved fMRI decoding of seen and imagined contents, theprediction is limited to training examples. We present a decoding approach forarbitrary objects, using the machine vision principle that an object categoryis represented by a set of features rendered invariant through hierarchicalprocessing. We show that visual features including those from a convolutionalneural network can be predicted from fMRI patterns and that greater accuracy isachieved for low/high-level features with lower/higher-level visual areas,respectively. Predicted features are used to identify the target object(extending beyond decoder training) from a set of computed features fornumerous objects. Furthermore, we demonstrate the identification of imaginedobjects, suggesting the recruitment of intermediate image representations intop-down processing. Our results demonstrate a tight link between human andmachine vision and its utility for brain-based information retrieval.
arxiv-13200-300 | Unsupervised Incremental Learning and Prediction of Music Signals | http://arxiv.org/pdf/1502.00524v2.pdf | author:Ricard Marxer, Hendrik Purwins category:cs.SD cs.IR cs.LG stat.ML 68T05 I.2.6; H.5.5 published:2015-02-02 summary:A system is presented that segments, clusters and predicts musical audio inan unsupervised manner, adjusting the number of (timbre) clustersinstantaneously to the audio input. A sequence learning algorithm adapts itsstructure to a dynamically changing clustering tree. The flow of the system isas follows: 1) segmentation by onset detection, 2) timbre representation ofeach segment by Mel frequency cepstrum coefficients, 3) discretization byincremental clustering, yielding a tree of different sound classes (e.g.instruments) that can grow or shrink on the fly driven by the instantaneoussound events, resulting in a discrete symbol sequence, 4) extraction ofstatistical regularities of the symbol sequence, using hierarchical N-grams andthe newly introduced conceptual Boltzmann machine, and 5) prediction of thenext sound event in the sequence. The system's robustness is assessed withrespect to complexity and noisiness of the signal. Clustering in isolationyields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singingvoice and drums. Onset detection jointly with clustering achieve an ARI of81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /39.2%.
