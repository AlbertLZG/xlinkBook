arxiv-1211-3589 | A Truncated EM Approach for Spike-and-Slab Sparse Coding |  http://arxiv.org/abs/1211.3589  | author:Abdul-Saboor Sheikh, Jacquelyn A. Shelton, Jörg Lücke category:stat.ML published:2012-11-15 summary:We study inference and learning based on a sparse coding model with`spike-and-slab' prior. As in standard sparse coding, the model used assumesindependent latent sources that linearly combine to generate data points.However, instead of using a standard sparse prior such as a Laplacedistribution, we study the application of a more flexible `spike-and-slab'distribution which models the absence or presence of a source's contributionindependently of its strength if it contributes. We investigate two approachesto optimize the parameters of spike-and-slab sparse coding: a novel truncatedEM approach and, for comparison, an approach based on standard factoredvariational distributions. The truncated approach can be regarded as avariational approach with truncated posteriors as variational distributions. Inapplications to source separation we find that both approaches improve thestate-of-the-art in a number of standard benchmarks, which argues for the useof `spike-and-slab' priors for the corresponding data domains. Furthermore, wefind that the truncated EM approach improves on the standard factored approachin source separation tasks$-$which hints to biases introduced by assumingposterior independence in the factored variational approach. Likewise, on astandard benchmark for image denoising, we find that the truncated EM approachimproves on the factored variational approach. While the performance of thefactored approach saturates with increasing numbers of hidden dimensions, theperformance of the truncated approach improves the state-of-the-art for highernoise levels.
arxiv-1211-3601 | Statistical inference on errorfully observed graphs |  http://arxiv.org/abs/1211.3601  | author:Carey E. Priebe, Daniel L. Sussman, Minh Tang, Joshua T. Vogelstein category:stat.ML published:2012-11-15 summary:Statistical inference on graphs is a burgeoning field in the applied andtheoretical statistics communities, as well as throughout the wider world ofscience, engineering, business, etc. In many applications, we are faced withthe reality of errorfully observed graphs. That is, the existence of an edgebetween two vertices is based on some imperfect assessment. In this paper, weconsider a graph $G = (V,E)$. We wish to perform an inference task -- theinference task considered here is "vertex classification". However, we do notobserve $G$; rather, for each potential edge $uv \in {{V}\choose{2}}$ weobserve an "edge-feature" which we use to classify $uv$ as edge/not-edge. Thuswe errorfully observe $G$ when we observe the graph $\widetilde{G} =(V,\widetilde{E})$ as the edges in $\widetilde{E}$ arise from theclassifications of the "edge-features", and are expected to be errorful.Moreover, we face a quantity/quality trade-off regarding the edge-features weobserve -- more informative edge-features are more expensive, and hence thenumber of potential edges that can be assessed decreases with the quality ofthe edge-features. We studied this problem by formulating a quantity/qualitytradeoff for a simple class of random graphs model, namely the stochasticblockmodel. We then consider a simple but optimal vertex classifier forclassifying $v$ and we derive the optimal quantity/quality operating point forsubsequent graph inference in the face of this trade-off. The optimal operatingpoints for the quantity/quality trade-off are surprising and illustrate theissue that methods for intermediate tasks should be chosen to maximizeperformance for the ultimate inference task. Finally, we investigate thequantity/quality tradeoff for errorful obesrvations of the {\it C.\ elegans}connectome graph.
arxiv-1211-3760 | Mixed LICORS: A Nonparametric Algorithm for Predictive State Reconstruction |  http://arxiv.org/abs/1211.3760  | author:Georg M. Goerg, Cosma Rohilla Shalizi category:stat.ME stat.ML published:2012-11-15 summary:We introduce 'mixed LICORS', an algorithm for learning nonlinear,high-dimensional dynamics from spatio-temporal data, suitable for bothprediction and simulation. Mixed LICORS extends the recent LICORS algorithm(Goerg and Shalizi, 2012) from hard clustering of predictive distributions to anon-parametric, EM-like soft clustering. This retains the asymptotic predictiveoptimality of LICORS, but, as we show in simulations, greatly improvesout-of-sample forecasts with limited data. The new method is implemented in thepublicly-available R package "LICORS"(http://cran.r-project.org/web/packages/LICORS/).
arxiv-1211-3500 | Accelerated Canonical Polyadic Decomposition by Using Mode Reduction |  http://arxiv.org/abs/1211.3500  | author:Guoxu Zhou, Andrzej Cichocki, Shengli Xie category:cs.NA cs.LG math.NA published:2012-11-15 summary:Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widelyapplied to analyze high order tensors. Existing CPD methods use alternatingleast square (ALS) iterations and hence need to unfold tensors to each of the$N$ modes frequently, which is one major bottleneck of efficiency forlarge-scale data and especially when $N$ is large. To overcome this problem, inthis paper we proposed a new CPD method which converts the original $N$th($N>3$) order tensor to a 3rd-order tensor first. Then the full CPD is realizedby decomposing this mode reduced tensor followed by a Khatri-Rao productprojection procedure. This way is quite efficient as unfolding to each of the$N$ modes are avoided, and dimensionality reduction can also be easilyincorporated to further improve the efficiency. We show that, under mildconditions, any $N$th-order CPD can be converted into a 3rd-order case butwithout destroying the essential uniqueness, and theoretically gives the sameresults as direct $N$-way CPD methods. Simulations show that, compared withstate-of-the-art CPD methods, the proposed method is more efficient and escapefrom local solutions more easily.
arxiv-1211-3643 | A Principled Approach to Grammars for Controlled Natural Languages and Predictive Editors |  http://arxiv.org/abs/1211.3643  | author:Tobias Kuhn category:cs.CL published:2012-11-15 summary:Controlled natural languages (CNL) with a direct mapping to formal logic havebeen proposed to improve the usability of knowledge representation systems,query interfaces, and formal specifications. Predictive editors are a popularapproach to solve the problem that CNLs are easy to read but hard to write.Such predictive editors need to be able to "look ahead" in order to show allpossible continuations of a given unfinished sentence. Such lookahead features,however, are difficult to implement in a satisfying way with existing grammarframeworks, especially if the CNL supports complex nonlocal structures such asanaphoric references. Here, methods and algorithms are presented for a newgrammar notation called Codeco, which is specifically designed for controllednatural languages and predictive editors. A parsing approach for Codeco basedon an extended chart parsing algorithm is presented. A large subset of AttemptoControlled English (ACE) has been represented in Codeco. Evaluation of thisgrammar and the parser implementation shows that the approach is practical,adequate and efficient.
arxiv-1211-3212 | Distributed Non-Stochastic Experts |  http://arxiv.org/abs/1211.3212  | author:Varun Kanade, Zhenming Liu, Bozidar Radunovic category:cs.LG cs.AI published:2012-11-14 summary:We consider the online distributed non-stochastic experts problem, where thedistributed system consists of one coordinator node that is connected to $k$sites, and the sites are required to communicate with each other via thecoordinator. At each time-step $t$, one of the $k$ site nodes has to pick anexpert from the set ${1, ..., n}$, and the same site receives information aboutpayoffs of all experts for that round. The goal of the distributed system is tominimize regret at time horizon $T$, while simultaneously keeping communicationto a minimum. The two extreme solutions to this problem are: (i) Full communication: Thisessentially simulates the non-distributed setting to obtain the optimal$O(\sqrt{\log(n)T})$ regret bound at the cost of $T$ communication. (ii) Nocommunication: Each site runs an independent copy : the regret is$O(\sqrt{log(n)kT})$ and the communication is 0. This paper shows thedifficulty of simultaneously achieving regret asymptotically better than$\sqrt{kT}$ and communication better than $T$. We give a novel algorithm thatfor an oblivious adversary achieves a non-trivial trade-off: regret$O(\sqrt{k^{5(1+\epsilon)/6} T})$ and communication $O(T/k^{\epsilon})$, forany value of $\epsilon \in (0, 1/5)$. We also consider a variant of the model,where the coordinator picks the expert. In this model, we show that thelabel-efficient forecaster of Cesa-Bianchi et al. (2005) already gives usstrategy that is near optimal in regret vs communication trade-off.
arxiv-1211-3371 | A Comparison of Meta-heuristic Search for Interactive Software Design |  http://arxiv.org/abs/1211.3371  | author:C. L. Simons, J. E. Smith category:cs.AI cs.NE published:2012-11-14 summary:Advances in processing capacity, coupled with the desire to tackle problemswhere a human subjective judgment plays an important role in determining thevalue of a proposed solution, has led to a dramatic rise in the number ofapplications of Interactive Artificial Intelligence. Of particular note is thecoupling of meta-heuristic search engines with user-provided evaluation andrating of solutions, usually in the form of Interactive Evolutionary Algorithms(IEAs). These have a well-documented history of successes, but arguably thepreponderance of IEAs stems from this history, rather than as a consciousdesign choice of meta-heuristic based on the characteristics of the problem athand. This paper sets out to examine the basis for that assumption, taking as acase study the domain of interactive software design. We consider a range offactors that should affect the design choice including ease of use,scalability, and of course, performance, i.e. that ability to generate goodsolutions within the limited number of evaluations available in interactivework before humans lose focus. We then evaluate three methods, namely greedylocal search, an evolutionary algorithm and ant colony optimization, with avariety of representations for candidate solutions. Results show that aftersuitable parameter tuning, ant colony optimization is highly effective withininteractive search and out-performs evolutionary algorithms with respect toincreasing numbers of attributes and methods in the software design problem.However, when larger numbers of classes are present in the software design, anevolutionary algorithm using a naive grouping integer-based representationappears more scalable.
arxiv-1211-3711 | Sequence Transduction with Recurrent Neural Networks |  http://arxiv.org/abs/1211.3711  | author:Alex Graves category:cs.NE cs.LG stat.ML published:2012-11-14 summary:Many machine learning tasks can be expressed as the transformation---or\emph{transduction}---of input sequences into output sequences: speechrecognition, machine translation, protein secondary structure prediction andtext-to-speech to name but a few. One of the key challenges in sequencetransduction is learning to represent both the input and output sequences in away that is invariant to sequential distortions such as shrinking, stretchingand translating. Recurrent neural networks (RNNs) are a powerful sequencelearning architecture that has proven capable of learning such representations.However RNNs traditionally require a pre-defined alignment between the inputand output sequences to perform transduction. This is a severe limitation since\emph{finding} the alignment is the most difficult aspect of many sequencetransduction problems. Indeed, even determining the length of the outputsequence is often challenging. This paper introduces an end-to-end,probabilistic sequence transduction system, based entirely on RNNs, that is inprinciple able to transform any input sequence into any finite, discrete outputsequence. Experimental results for phoneme recognition are provided on theTIMIT speech corpus.
arxiv-1211-3295 | Order-independent constraint-based causal structure learning |  http://arxiv.org/abs/1211.3295  | author:Diego Colombo, Marloes H. Maathuis category:stat.ML cs.LG published:2012-11-14 summary:We consider constraint-based methods for causal structure learning, such asthe PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The firststep of all these algorithms consists of the PC-algorithm. This algorithm isknown to be order-dependent, in the sense that the output can depend on theorder in which the variables are given. This order-dependence is a minor issuein low-dimensional settings. We show, however, that it can be very pronouncedin high-dimensional settings, where it can lead to highly variable results. Wepropose several modifications of the PC-algorithm (and hence also of the otheralgorithms) that remove part or all of this order-dependence. All proposedmodifications are consistent in high-dimensional settings under the sameconditions as their original counterparts. We compare the PC-, FCI-, andRFCI-algorithms and their modifications in simulation studies and on a yeastgene expression data set. We show that our modifications yield similarperformance in low-dimensional settings and improved performance inhigh-dimensional settings. All software is implemented in the R-package pcalg.
arxiv-1211-3451 | Memory Capacity of a Random Neural Network |  http://arxiv.org/abs/1211.3451  | author:Matt Stowe category:cs.NE published:2012-11-14 summary:This paper considers the problem of information capacity of a random neuralnetwork. The network is represented by matrices that are square andsymmetrical. The matrices have a weight which determines the highest and lowestpossible value found in the matrix. The examined matrices are randomlygenerated and analyzed by a computer program. We find the surprising resultthat the capacity of the network is a maximum for the binary random neuralnetwork and it does not change as the number of quantization levels associatedwith the weights increases.
arxiv-1211-3444 | Spectral Clustering: An empirical study of Approximation Algorithms and its Application to the Attrition Problem |  http://arxiv.org/abs/1211.3444  | author:B. Cung, T. Jin, J. Ramirez, A. Thompson, C. Boutsidis, D. Needell category:cs.LG math.NA stat.ML published:2012-11-14 summary:Clustering is the problem of separating a set of objects into groups (calledclusters) so that objects within the same cluster are more similar to eachother than to those in different clusters. Spectral clustering is a nowwell-known method for clustering which utilizes the spectrum of the datasimilarity matrix to perform this separation. Since the method relies onsolving an eigenvector problem, it is computationally expensive for largedatasets. To overcome this constraint, approximation methods have beendeveloped which aim to reduce running time while maintaining accurateclassification. In this article, we summarize and experimentally evaluateseveral approximation methods for spectral clustering. From an applicationsstandpoint, we employ spectral clustering to solve the so-called attritionproblem, where one aims to identify from a set of employees those who arelikely to voluntarily leave the company from those who are not. Our study shedslight on the empirical performance of existing approximate spectral clusteringmethods and shows the applicability of these methods in an important businessoptimization related problem.
arxiv-1211-3412 | Network Sampling: From Static to Streaming Graphs |  http://arxiv.org/abs/1211.3412  | author:Nesreen K. Ahmed, Jennifer Neville, Ramana Kompella category:cs.SI cs.DS cs.LG physics.soc-ph stat.ML published:2012-11-14 summary:Network sampling is integral to the analysis of social, information, andbiological networks. Since many real-world networks are massive in size,continuously evolving, and/or distributed in nature, the network structure isoften sampled in order to facilitate study. For these reasons, a more thoroughand complete understanding of network sampling is critical to support the fieldof network science. In this paper, we outline a framework for the generalproblem of network sampling, by highlighting the different objectives,population and units of interest, and classes of network sampling methods. Inaddition, we propose a spectrum of computational models for network samplingmethods, ranging from the traditionally studied model based on the assumptionof a static domain to a more challenging model that is appropriate forstreaming domains. We design a family of sampling methods based on the conceptof graph induction that generalize across the full spectrum of computationalmodels (from static to streaming) while efficiently preserving many of thetopological properties of the input graphs. Furthermore, we demonstrate howtraditional static sampling algorithms can be modified for graph streams foreach of the three main classes of sampling methods: node, edge, andtopology-based sampling. Our experimental results indicate that our proposedfamily of sampling methods more accurately preserves the underlying propertiesof the graph for both static and streaming graphs. Finally, we study the impactof network sampling algorithms on the parameter estimation and performanceevaluation of relational classification algorithms.
arxiv-1211-3402 | Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship |  http://arxiv.org/abs/1211.3402  | author:Bohdan Pavlyshenko category:cs.IR cs.CL published:2012-11-14 summary:The genetic selection of keywords set, the text frequencies of which areconsidered as attributes in text classification analysis, has been analyzed.The genetic optimization was performed on a set of words, which is the fractionof the frequency dictionary with given frequency limits. The frequencydictionary was formed on the basis of analyzed text array of texts of Englishfiction. As the fitness function which is minimized by the genetic algorithm,the error of nearest k neighbors classifier was used. The obtained results showhigh precision and recall of texts classification by authorship categories onthe basis of attributes of keywords set which were selected by the geneticalgorithm from the frequency dictionary.
arxiv-1211-2863 | Multi-Sensor Fusion via Reduction of Dimensionality |  http://arxiv.org/abs/1211.2863  | author:Alon Schclar category:cs.CV published:2012-11-13 summary:Large high-dimensional datasets are becoming more and more popular in anincreasing number of research areas. Processing the high dimensional dataincurs a high computational cost and is inherently inefficient since many ofthe values that describe a data object are redundant due to noise and innercorrelations. Consequently, the dimensionality, i.e. the number of values thatare used to describe a data object, needs to be reduced prior to any otherprocessing of the data. The dimensionality reduction removes, in most cases,noise from the data and reduces substantially the computational cost ofalgorithms that are applied to the data. In this thesis, a novel coherent integrated methodology is introduced(theory, algorithm and applications) to reduce the dimensionality ofhigh-dimensional datasets. The method constructs a diffusion process among thedata coordinates via a random walk. The dimensionality reduction is obtainedbased on the eigen-decomposition of the Markov matrix that is associated withthe random walk. The proposed method is utilized for: (a) segmentation anddetection of anomalies in hyper-spectral images; (b) segmentation ofmulti-contrast MRI images; and (c) segmentation of video sequences. We also present algorithms for: (a) the characterization of materials usingtheir spectral signatures to enable their identification; (b) detection ofvehicles according to their acoustic signatures; and (c) classification ofvascular vessels recordings to detect hyper-tension and cardio-vasculardiseases. The proposed methodology and algorithms produce excellent results thatsuccessfully compete with current state-of-the-art algorithms.
arxiv-1211-2980 | Shattering-Extremal Systems |  http://arxiv.org/abs/1211.2980  | author:Shay Moran category:math.CO cs.CG cs.DM cs.LG published:2012-11-13 summary:The Shatters relation and the VC dimension have been investigated since theearly seventies. These concepts have found numerous applications in statistics,combinatorics, learning theory and computational geometry. Shattering extremalsystems are set-systems with a very rich structure and many differentcharacterizations. The goal of this thesis is to elaborate on the structure ofthese systems.
arxiv-1211-2958 | Study design in causal models |  http://arxiv.org/abs/1211.2958  | author:Juha Karvanen category:stat.ME stat.AP stat.ML G.3; G.2.2 published:2012-11-13 summary:The causal assumptions, the study design and the data are the elementsrequired for scientific inference in empirical research. The research isadequately communicated only if all of these elements and their relations aredescribed precisely. Causal models with design describe the study design andthe missing data mechanism together with the causal structure and allow thedirect application of causal calculus in the estimation of the causal effects.The flow of the study is visualized by ordering the nodes of the causal diagramin two dimensions by their causal order and the time of the observation.Conclusions whether a causal or observational relationship can be estimatedfrom the collected incomplete data can be made directly from the graph. Causalmodels with design offer a systematic and unifying view scientific inferenceand increase the clarity and speed of communication. Examples on the causalmodels for a case-control study, a nested case-control study, a clinical trialand a two-stage case-cohort study are presented.
arxiv-1211-2881 | Deep Attribute Networks |  http://arxiv.org/abs/1211.2881  | author:Junyoung Chung, Donghoon Lee, Youngjoo Seo, Chang D. Yoo category:cs.CV cs.LG stat.ML published:2012-11-13 summary:Obtaining compact and discriminative features is one of the major challengesin many of the real-world image classification tasks such as face verificationand object recognition. One possible approach is to represent input image onthe basis of high-level features that carry semantic meaning which humans canunderstand. In this paper, a model coined deep attribute network (DAN) isproposed to address this issue. For an input image, the model outputs theattributes of the input image without performing any classification. Theefficacy of the proposed model is evaluated on unconstrained face verificationand real-world object recognition tasks using the LFW and the a-PASCALdatasets. We demonstrate the potential of deep learning for attribute-basedclassification by showing comparable results with existing state-of-the-artresults. Once properly trained, the DAN is fast and does away with calculatinglow-level features which are maybe unreliable and computationally expensive.
arxiv-1211-3038 | Gradient density estimation in arbitrary finite dimensions using the method of stationary phase |  http://arxiv.org/abs/1211.3038  | author:Karthik S. Gurumoorthy, Anand Rangarajan, John Corring category:stat.ML published:2012-11-13 summary:We prove that the density function of the gradient of a sufficiently smoothfunction $S : \Omega \subset \mathbb{R}^d \rightarrow \mathbb{R}$, obtained viaa random variable transformation of a uniformly distributed random variable, isincreasingly closely approximated by the normalized power spectrum of$\phi=\exp\left(\frac{iS}{\tau}\right)$ as the free parameter $\tau \rightarrow0$. The result is shown using the stationary phase approximation and standardintegration techniques and requires proper ordering of limits. We highlight arelationship with the well-known characteristic function approach to densityestimation, and detail why our result is distinct from this approach.
arxiv-1211-3046 | Recovering the Optimal Solution by Dual Random Projection |  http://arxiv.org/abs/1211.3046  | author:Lijun Zhang, Mehrdad Mahdavi, Rong Jin, Tianbao Yang, Shenghuo Zhu category:cs.LG published:2012-11-13 summary:Random projection has been widely used in data classification. It mapshigh-dimensional data into a low-dimensional subspace in order to reduce thecomputational cost in solving the related optimization problem. While previousstudies are focused on analyzing the classification performance of using randomprojection, in this work, we consider the recovery problem, i.e., how toaccurately recover the optimal solution to the original optimization problem inthe high-dimensional space based on the solution learned from the subspacespanned by random projections. We present a simple algorithm, termed DualRandom Projection, that uses the dual solution of the low-dimensionaloptimization problem to recover the optimal solution to the original problem.Our theoretical analysis shows that with a high probability, the proposedalgorithm is able to accurately recover the optimal solution to the originalproblem, provided that the data matrix is of low rank or can be wellapproximated by a low rank matrix.
arxiv-1211-3010 | Time-series Scenario Forecasting |  http://arxiv.org/abs/1211.3010  | author:Sriharsha Veeramachaneni category:stat.ML cs.LG stat.AP published:2012-11-13 summary:Many applications require the ability to judge uncertainty of time-seriesforecasts. Uncertainty is often specified as point-wise error bars around amean or median forecast. Due to temporal dependencies, such a method obscuressome information. We would ideally have a way to query the posteriorprobability of the entire time-series given the predictive variables, or at aminimum, be able to draw samples from this distribution. We use a Bayesiandictionary learning algorithm to statistically generate an ensemble offorecasts. We show that the algorithm performs as well as a physics-basedensemble method for temperature forecasts for Houston. We conclude that themethod shows promise for scenario forecasting where physics-based methods areabsent.
arxiv-1211-2945 | The application of a perceptron model to classify an individual's response to a proposed loading dose regimen of Warfarin |  http://arxiv.org/abs/1211.2945  | author:Cen Wan, Irina V. Biktasheva, Steven Lane category:stat.AP cs.NE 68T05, 92C50 published:2012-11-13 summary:The dose regimen of Warfarin is separated into two phases. Firstly a loadingdose is given, which is designed to bring the International Normalisation Ratio(INR) to within therapeutic range. Then a stable maintenance dose is given tomaintain the INR within therapeutic range. In the United Kingdom (UK) theloading dose is usually given as three individual daily doses, the standardloading dose being 10mg on days one and two and 5mgs on day three, which can bevaried at the discretion of the clinician. However, due to the largeinter-individual variation in the response to Warfarin therapy, it is difficultto identify which patients will reach the narrow therapeutic window for targetINR, and which will be above or below the therapeutic window. The aim of thisresearch was to develop a methodology using a neural networks classificationalgorithm and data mining techniques to predict for a given loading dose andpatient characteristics if the patient is more likely to achieve target INR ormore likely to be above or below therapeutic range. Multilayer perceptron (MLP) and 10-fold stratified cross validationalgorithms were used to determine an artificial neural network to classifypatients' response to their initial Warfarin loading dose. The resulting neuralnetwork model correctly classifies an individual's response to their Warfarinloading dose over 80% of the time. As well as taking into account the initialloading dose, the final model also includes demographic, genetic and a numberof other potential confounding factors. With this model clinicians canpredetermine whether a given loading regimen, along with specific patientcharacteristics will achieve a therapeutic response for a particular patient.Thus tailoring the loading dose regimen to meet the individual needs of thepatient and reducing the risk of adverse drug reactions associated withWarfarin.
arxiv-1211-2891 | Boosting Simple Collaborative Filtering Models Using Ensemble Methods |  http://arxiv.org/abs/1211.2891  | author:Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, Alon Schclar category:cs.IR cs.LG stat.ML published:2012-11-13 summary:In this paper we examine the effect of applying ensemble learning to theperformance of collaborative filtering methods. We present several systematicapproaches for generating an ensemble of collaborative filtering models basedon a single collaborative filtering algorithm (single-model or homogeneousensemble). We present an adaptation of several popular ensemble techniques inmachine learning for the collaborative filtering domain, including bagging,boosting, fusion and randomness injection. We evaluate the proposed approach onseveral types of collaborative filtering base models: k- NN, matrixfactorization and a neighborhood matrix factorization model. Empiricalevaluation shows a prediction improvement compared to all base CF algorithms.In particular, we show that the performance of an ensemble of simple (weak) CFmodels such as k-NN is competitive compared with a single strong CF model (suchas matrix factorization) while requiring an order of magnitude lesscomputational cost.
arxiv-1211-2512 | Minimal cost feature selection of data with normal distribution measurement errors |  http://arxiv.org/abs/1211.2512  | author:Hong Zhao, Fan Min, William Zhu category:cs.AI cs.LG published:2012-11-12 summary:Minimal cost feature selection is devoted to obtain a trade-off between testcosts and misclassification costs. This issue has been addressed recently onnominal data. In this paper, we consider numerical data with measurement errorsand study minimal cost feature selection in this model. First, we build a datamodel with normal distribution measurement errors. Second, the neighborhood ofeach data item is constructed through the confidence interval. Comparing withdiscretized intervals, neighborhoods are more reasonable to maintain theinformation of data. Third, we define a new minimal total cost featureselection problem through considering the trade-off between test costs andmisclassification costs. Fourth, we proposed a backtracking algorithm withthree effective pruning techniques to deal with this problem. The algorithm istested on four UCI data sets. Experimental results indicate that the pruningtechniques are effective, and the algorithm is efficient for data sets withnearly one thousand objects.
arxiv-1211-2502 | New Edge Detection Technique based on the Shannon Entropy in Gray Level Images |  http://arxiv.org/abs/1211.2502  | author:Mohamed A. El-Sayed, Tarek Abd-El Hafeez category:cs.CV published:2012-11-12 summary:Edge detection is an important field in image processing. Edges characterizeobject boundaries and are therefore useful for segmentation, registration,feature extraction, and identification of objects in a scene. In this paper, anapproach utilizing an improvement of Baljit and Amar method which uses Shannonentropy other than the evaluation of derivatives of the image in detectingedges in gray level images has been proposed. The proposed method can reducethe CPU time required for the edge detection process and the quality of theedge detector of the output images is robust. A standard test images, thereal-world and synthetic images are used to compare the results of the proposededge detector with the Baljit and Amar edge detector method. In order tovalidate the results, the run time of the proposed method and the perviousmethod are presented. It has been observed that the proposed edge detectorworks effectively for different gray scale digital images. The performanceevaluation of the proposed technique in terms of the measured CPU time and thequality of edge detector method are presented. Experimental results demonstratethat the proposed method achieve better result than the relevant classicmethod.
arxiv-1211-2699 | A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete Wavelet Transform Domain using Two Subbands |  http://arxiv.org/abs/1211.2699  | author:Abdur Shahid, Shahriar Badsha, Md. Rethwan Kabeer, Junaid Ahsan, Mufti Mahmud category:cs.MM cs.CV published:2012-11-12 summary:Digital watermarking is the process to hide digital pattern directly into adigital content. Digital watermarking techniques are used to address digitalrights management, protect information and conceal secrets. An invisiblenon-blind watermarking approach for gray scale images is proposed in thispaper. The host image is decomposed into 3-levels using Discrete WaveletTransform. Based on the parent-child relationship between the waveletcoefficients the Set Partitioning in Hierarchical Trees (SPIHT) compressionalgorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out thesignificant coefficients. The most significant coefficients of LH2 and HL2bands are selected to embed a binary watermark image. The selected significantcoefficients are modulated using Noise Visibility Function, which is consideredas the best strength to ensure better imperceptibility. The approach is testedagainst various image processing attacks such as addition of noise, filtering,cropping, JPEG compression, histogram equalization and contrast adjustment. Theexperimental results reveal the high effectiveness of the method.
arxiv-1211-2717 | Proximal Stochastic Dual Coordinate Ascent |  http://arxiv.org/abs/1211.2717  | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG math.OC published:2012-11-12 summary:We introduce a proximal version of dual coordinate ascent method. Wedemonstrate how the derived algorithmic framework can be used for numerousregularized loss minimization problems, including $\ell_1$ regularization andstructured output SVM. The convergence rates we obtain match, and sometimesimprove, state-of-the-art results.
arxiv-1211-2741 | A Hindi Speech Actuated Computer Interface for Web Search |  http://arxiv.org/abs/1211.2741  | author:Kamlesh Sharma, S. V. A. V. Prasad, T. V. Prasad category:cs.CL cs.HC cs.IR published:2012-11-12 summary:Aiming at increasing system simplicity and flexibility, an audio evoked basedsystem was developed by integrating simplified headphone and user-friendlysoftware design. This paper describes a Hindi Speech Actuated ComputerInterface for Web search (HSACIWS), which accepts spoken queries in Hindilanguage and provides the search result on the screen. This system recognizesspoken queries by large vocabulary continuous speech recognition (LVCSR),retrieves relevant document by text retrieval, and provides the search resulton the Web by the integration of the Web and the voice systems. The LVCSR inthis system showed enough performance levels for speech with acoustic andlanguage models derived from a query corpus with target contents.
arxiv-1211-2556 | A Comparative Study of Gaussian Mixture Model and Radial Basis Function for Voice Recognition |  http://arxiv.org/abs/1211.2556  | author:Fatai Adesina Anifowose category:cs.LG cs.CV stat.ML published:2012-11-12 summary:A comparative study of the application of Gaussian Mixture Model (GMM) andRadial Basis Function (RBF) in biometric recognition of voice has been carriedout and presented. The application of machine learning techniques to biometricauthentication and recognition problems has gained a widespread acceptance. Inthis research, a GMM model was trained, using Expectation Maximization (EM)algorithm, on a dataset containing 10 classes of vowels and the model was usedto predict the appropriate classes using a validation dataset. For experimentalvalidity, the model was compared to the performance of two different versionsof RBF model using the same learning and validation datasets. The resultsshowed very close recognition accuracy between the GMM and the standard RBFmodel, but with GMM performing better than the standard RBF by less than 1% andthe two models outperformed similar models reported in literature. The DTREGversion of RBF outperformed the other two models by producing 94.8% recognitionaccuracy. In terms of recognition time, the standard RBF was found to be thefastest among the three models.
arxiv-1211-2500 | A New Algorithm Based Entropic Threshold for Edge Detection in Images |  http://arxiv.org/abs/1211.2500  | author:Mohamed A. El-Sayed category:cs.CV published:2012-11-12 summary:Edge detection is one of the most critical tasks in automatic image analysis.There exists no universal edge detection method which works well under allconditions. This paper shows the new approach based on the one of the mostefficient techniques for edge detection, which is entropy-based thresholding.The main advantages of the proposed method are its robustness and itsflexibility. We present experimental results for this method, and compareresults of the algorithm against several leading edge detection methods, suchas Canny, LOG, and Sobel. Experimental results demonstrate that the proposedmethod achieves better result than some classic methods and the quality of theedge detector of the output images is robust and decrease the computation time.
arxiv-1211-2532 | Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation |  http://arxiv.org/abs/1211.2532  | author:Dominique Guillot, Bala Rajaratnam, Benjamin T. Rolfs, Arian Maleki, Ian Wong category:stat.CO cs.LG stat.ML published:2012-11-12 summary:The L1-regularized maximum likelihood estimation problem has recently becomea topic of great interest within the machine learning, statistics, andoptimization communities as a method for producing sparse inverse covarianceestimators. In this paper, a proximal gradient method (G-ISTA) for performingL1-regularized covariance matrix estimation is presented. Although numerousalgorithms have been proposed for solving this problem, this simple proximalgradient method is found to have attractive theoretical and numericalproperties. G-ISTA has a linear rate of convergence, resulting in an O(log e)iteration complexity to reach a tolerance of e. This paper gives eigenvaluebounds for the G-ISTA iterates, providing a closed-form linear convergencerate. The rate is shown to be closely related to the condition number of theoptimal point. Numerical convergence results and timing comparisons for theproposed method are presented. G-ISTA is shown to perform very well, especiallywhen the optimal point is well-conditioned.
arxiv-1211-2742 | Sketch Recognition using Domain Classification |  http://arxiv.org/abs/1211.2742  | author:Vasudha Vashisht, Tanupriya Choudhury, T. V. Prasad category:cs.CV cs.HC published:2012-11-12 summary:Conceptualizing away the sketch processing details in a user interface willenable general users and domain experts to create more complex sketches. Thereare many domains for which sketch recognition systems are being developed. Butthey entail image-processing skill if they are to handle the details of eachdomain, and also they are lengthy to build. The implemented system goal is toenable user interface designers and domain experts who may not have proficiencyin sketch recognition to be able to construct these sketch systems. This sketchrecognition system takes in rough sketches from user drawn with the help ofmouse as its input. It then recognizes the sketch using segmentation and domainclassification, the properties of the user drawn sketch and segments aresearched heuristically in the domains and each figures of each domain, andfinally it shows its domain, the figure name and properties. It also draws thesketch smoothly. The work is resulted through extensive research and study ofmany existing image processing and pattern matching algorithms.
arxiv-1211-2378 | Hybrid methodology for hourly global radiation forecasting in Mediterranean area |  http://arxiv.org/abs/1211.2378  | author:Cyril Voyant, Marc Muselli, Christophe Paoli, Marie Laure Nivet category:cs.NE cs.LG physics.ao-ph stat.AP published:2012-11-11 summary:The renewable energies prediction and particularly global radiationforecasting is a challenge studied by a growing number of research teams. Thispaper proposes an original technique to model the insolation time series basedon combining Artificial Neural Network (ANN) and Auto-Regressive and MovingAverage (ARMA) model. While ANN by its non-linear nature is effective topredict cloudy days, ARMA techniques are more dedicated to sunny days withoutcloud occurrences. Thus, three hybrids models are suggested: the first proposessimply to use ARMA for 6 months in spring and summer and to use an optimizedANN for the other part of the year; the second model is equivalent to the firstbut with a seasonal learning; the last model depends on the error occurred theprevious hour. These models were used to forecast the hourly global radiationfor five places in Mediterranean area. The forecasting performance was comparedamong several models: the 3 above mentioned models, the best ANN and ARMA foreach location. In the best configuration, the coupling of ANN and ARMA allowsan improvement of more than 1%, with a maximum in autumn (3.4%) and a minimumin winter (0.9%) where ANN alone is the best.
arxiv-1211-2361 | Genetic Algorithm for Designing a Convenient Facility Layout for a Circular Flow Path |  http://arxiv.org/abs/1211.2361  | author:Hossein Jahandideh, Ardavan Asef-Vaziri, Mohammad Modarres category:cs.NE published:2012-11-11 summary:In this paper, we present a heuristic for designing facility layouts that areconvenient for designing a unidirectional loop for material handling. We usegenetic algorithm where the objective function and crossover and mutationoperators have all been designed specifically for this purpose. Our design ismade under flexible bay structure and comparisons are made with other layoutsfrom the literature that were designed under flexible bay structure.
arxiv-1211-2459 | Measures of Entropy from Data Using Infinitely Divisible Kernels |  http://arxiv.org/abs/1211.2459  | author:Luis G. Sanchez Giraldo, Murali Rao, Jose C. Principe category:cs.LG cs.IT math.IT stat.ML published:2012-11-11 summary:Information theory provides principled ways to analyze different inferenceand learning problems such as hypothesis testing, clustering, dimensionalityreduction, classification, among others. However, the use of informationtheoretic quantities as test statistics, that is, as quantities obtained fromempirical data, poses a challenging estimation problem that often leads tostrong simplifications such as Gaussian models, or the use of plug in densityestimators that are restricted to certain representation of the data. In thispaper, a framework to non-parametrically obtain measures of entropy directlyfrom data using operators in reproducing kernel Hilbert spaces defined byinfinitely divisible kernels is presented. The entropy functionals, which bearresemblance with quantum entropies, are defined on positive definite matricesand satisfy similar axioms to those of Renyi's definition of entropy.Convergence of the proposed estimators follows from concentration results onthe difference between the ordered spectrum of the Gram matrices and theintegral operators associated to the population quantities. In this way,capitalizing on both the axiomatic definition of entropy and on therepresentation power of positive definite kernels, the proposed measure ofentropy avoids the estimation of the probability distribution underlying thedata. Moreover, estimators of kernel-based conditional entropy and mutualinformation are also defined. Numerical experiments on independence testscompare favourably with state of the art.
arxiv-1211-2476 | Random Utility Theory for Social Choice |  http://arxiv.org/abs/1211.2476  | author:Hossein Azari Soufiani, David C. Parkes, Lirong Xia category:cs.MA cs.LG stat.ML published:2012-11-11 summary:Random utility theory models an agent's preferences on alternatives bydrawing a real-valued score on each alternative (typically independently) froma parameterized distribution, and then ranking the alternatives according toscores. A special case that has received significant attention is thePlackett-Luce model, for which fast inference methods for maximum likelihoodestimators are available. This paper develops conditions on general randomutility models that enable fast inference within a Bayesian framework throughMC-EM, providing concave loglikelihood functions and bounded sets of globalmaxima solutions. Results on both real-world and simulated data provide supportfor the scalability of the approach and capability for model selection amonggeneral random utility models including Plackett-Luce.
arxiv-1211-2290 | Dating Texts without Explicit Temporal Cues |  http://arxiv.org/abs/1211.2290  | author:Abhimanu Kumar, Jason Baldridge, Matthew Lease, Joydeep Ghosh category:cs.CL cs.AI published:2012-11-10 summary:This paper tackles temporal resolution of documents, such as determining whena document is about or when it was written, based only on its text. We applytechniques from information retrieval that predict dates via language modelsover a discretized timeline. Unlike most previous works, we rely {\it solely}on temporal cues implicit in the text. We consider both document-likelihood anddivergence based techniques and several smoothing methods for both of them. Ourbest model predicts the mid-point of individuals' lives with a median of 22 andmean error of 36 years for Wikipedia biographies from 3800 B.C. to the presentday. We also show that this approach works well when training on suchbiographies and predicting dates both for non-biographical Wikipedia pagesabout specific years (500 B.C. to 2010 A.D.) and for publication dates of shortstories (1798 to 2008). Together, our work shows that, even in absence oftemporal extraction resources, it is possible to achieve remarkable temporallocality across a diverse set of texts.
arxiv-1211-2315 | Efficient network-guided multi-locus association mapping with graph cuts |  http://arxiv.org/abs/1211.2315  | author:Chloé-Agathe Azencott, Dominik Grimm, Mahito Sugiyama, Yoshinobu Kawahara, Karsten M. Borgwardt category:stat.ML q-bio.QM published:2012-11-10 summary:As an increasing number of genome-wide association studies reveal thelimitations of attempting to explain phenotypic heritability by single geneticloci, there is growing interest for associating complex phenotypes with sets ofgenetic loci. While several methods for multi-locus mapping have been proposed,it is often unclear how to relate the detected loci to the growing knowledgeabout gene pathways and networks. The few methods that take biological pathwaysor networks into account are either restricted to investigating a limitednumber of predetermined sets of loci, or do not scale to genome-wide settings. We present SConES, a new efficient method to discover sets of genetic locithat are maximally associated with a phenotype, while being connected in anunderlying network. Our approach is based on a minimum cut reformulation of theproblem of selecting features under sparsity and connectivity constraints thatcan be solved exactly and rapidly. SConES outperforms state-of-the-art competitors in terms of runtime, scalesto hundreds of thousands of genetic loci, and exhibits higher power indetecting causal SNPs in simulation studies than existing methods. On floweringtime phenotypes and genotypes from Arabidopsis thaliana, SConES detects locithat enable accurate phenotype prediction and that are supported by theliterature. Matlab code for SConES is available athttp://webdav.tuebingen.mpg.de/u/karsten/Forschung/scones/
arxiv-1211-2304 | Probabilistic Combination of Classifier and Cluster Ensembles for Non-transductive Learning |  http://arxiv.org/abs/1211.2304  | author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Badrul Sarwar, Jean-David Ruvini category:cs.LG stat.ML published:2012-11-10 summary:Unsupervised models can provide supplementary soft constraints to helpclassify new target data under the assumption that similar objects in thetarget set are more likely to share the same class label. Such models can alsohelp detect possible differences between training and target distributions,which is useful in applications where concept drift may take place. This paperdescribes a Bayesian framework that takes as input class labels from existingclassifiers (designed based on labeled data from the source domain), as well ascluster labels from a cluster ensemble operating solely on the target data tobe classified, and yields a consensus labeling of the target data. Thisframework is particularly useful when the statistics of the target data driftor change from those of the training data. We also show that the proposedframework is privacy-aware and allows performing distributed learning whendata/models have sharing restrictions. Experiments show that our framework canyield superior results to those provided by applying classifier ensembles only.
arxiv-1211-2260 | No-Regret Algorithms for Unconstrained Online Convex Optimization |  http://arxiv.org/abs/1211.2260  | author:Matthew Streeter, H. Brendan McMahan category:cs.LG published:2012-11-09 summary:Some of the most compelling applications of online convex optimization,including online prediction and classification, are unconstrained: the naturalfeasible set is R^n. Existing algorithms fail to achieve sub-linear regret inthis setting unless constraints on the comparator point x^* are known inadvance. We present algorithms that, without such prior knowledge, offernear-optimal regret bounds with respect to any choice of x^*. In particular,regret with respect to x^* = 0 is constant. We then prove lower bounds showingthat our guarantees are near-optimal in this setting.
arxiv-1211-2264 | Calibrated Elastic Regularization in Matrix Completion |  http://arxiv.org/abs/1211.2264  | author:Tingni Sun, Cun-Hui Zhang category:math.ST stat.ML stat.TH published:2012-11-09 summary:This paper concerns the problem of matrix completion, which is to estimate amatrix from observations in a small subset of indices. We propose a calibratedspectrum elastic net method with a sum of the nuclear and Frobenius penaltiesand develop an iterative algorithm to solve the convex minimization problem.The iterative algorithm alternates between imputing the missing entries in theincomplete matrix by the current guess and estimating the matrix by a scaledsoft-thresholding singular value decomposition of the imputed matrix until theresulting matrix converges. A calibration step follows to correct the biascaused by the Frobenius penalty. Under proper coherence conditions and forsuitable penalties levels, we prove that the proposed estimator achieves anerror bound of nearly optimal order and in proportion to the noise level. Thisprovides a unified analysis of the noisy and noiseless matrix completionproblems. Simulation results are presented to compare our proposal withprevious ones.
arxiv-1211-2150 | NF-SAVO: Neuro-Fuzzy system for Arabic Video OCR |  http://arxiv.org/abs/1211.2150  | author:Mohamed Ben Halima, Hichem karray, Adel. M. Alimi, Ana Fernández Vila category:cs.CV published:2012-11-09 summary:In this paper we propose a robust approach for text extraction andrecognition from video clips which is called Neuro-Fuzzy system for ArabicVideo OCR. In Arabic video text recognition, a number of noise componentsprovide the text relatively more complicated to separate from the background.Further, the characters can be moving or presented in a diversity of colors,sizes and fonts that are not uniform. Added to this, is the fact that thebackground is usually moving making text extraction a more intricate process.Video include two kinds of text, scene text and artificial text. Scene text isusually text that becomes part of the scene itself as it is recorded at thetime of filming the scene. But artificial text is produced separately and awayfrom the scene and is laid over it at a later stage or during the postprocessing time. The emergence of artificial text is consequently vigilantlydirected. This type of text carries with it important information that helps invideo referencing, indexing and retrieval.
arxiv-1211-2190 | Efficient Monte Carlo Methods for Multi-Dimensional Learning with Classifier Chains |  http://arxiv.org/abs/1211.2190  | author:Jesse Read, Luca Martino, David Luengo category:cs.LG stat.CO stat.ML published:2012-11-09 summary:Multi-dimensional classification (MDC) is the supervised learning problemwhere an instance is associated with multiple classes, rather than with asingle class, as in traditional classification problems. Since these classesare often strongly correlated, modeling the dependencies between them allowsMDC methods to improve their performance - at the expense of an increasedcomputational cost. In this paper we focus on the classifier chains (CC)approach for modeling dependencies, one of the most popular and highest-performing methods for multi-label classification (MLC), a particular case ofMDC which involves only binary classes (i.e., labels). The original CCalgorithm makes a greedy approximation, and is fast but tends to propagateerrors along the chain. Here we present novel Monte Carlo schemes, both forfinding a good chain sequence and performing efficient inference. Ouralgorithms remain tractable for high-dimensional data sets and obtain the bestpredictive performance across several real data sets.
arxiv-1211-2227 | Efficient learning of simplices |  http://arxiv.org/abs/1211.2227  | author:Joseph Anderson, Navin Goyal, Luis Rademacher category:cs.LG cs.DS stat.ML published:2012-11-09 summary:We show an efficient algorithm for the following problem: Given uniformlyrandom points from an arbitrary n-dimensional simplex, estimate the simplex.The size of the sample and the number of arithmetic operations of our algorithmare polynomial in n. This answers a question of Frieze, Jerrum and Kannan[FJK]. Our result can also be interpreted as efficiently learning theintersection of n+1 half-spaces in R^n in the model where the intersection isbounded and we are given polynomially many uniform samples from it. Our proofuses the local search technique from Independent Component Analysis (ICA), alsoused by [FJK]. Unlike these previous algorithms, which were based on analyzingthe fourth moment, ours is based on the third moment. We also show a direct connection between the problem of learning a simplexand ICA: a simple randomized reduction to ICA from the problem of learning asimplex. The connection is based on a known representation of the uniformmeasure on a simplex. Similar representations lead to a reduction from theproblem of learning an affine transformation of an n-dimensional l_p ball toICA.
arxiv-1211-2037 | Time Complexity Analysis of Binary Space Partitioning Scheme for Image Compression |  http://arxiv.org/abs/1211.2037  | author:Rehna V. J., M. K. Jeyakumar category:cs.CV published:2012-11-09 summary:Segmentation-based image coding methods provide high compression ratios whencompared with traditional image coding approaches like the transform and subband coding for low bit-rate compression applications. In this paper, asegmentation-based image coding method, namely the Binary Space Partitionscheme, that divides the desired image using a recursive procedure for codingis presented. The BSP approach partitions the desired image recursively byusing bisecting lines, selected from a collection of discrete optional lines,in a hierarchical manner. This partitioning procedure generates a binary tree,which is referred to as the BSP-tree representation of the desired image. Thealgorithm is extremely complex in computation and has high execution time. Thetime complexity of the BSP scheme is explored in this work.
arxiv-1211-2073 | LAGE: A Java Framework to reconstruct Gene Regulatory Networks from Large-Scale Continues Expression Data |  http://arxiv.org/abs/1211.2073  | author:Yang Lu, Mengying Wang, Kenny Q. Zhu, Bo Yuan category:cs.LG cs.CE q-bio.QM stat.ML published:2012-11-09 summary:LAGE is a systematic framework developed in Java. The motivation of LAGE isto provide a scalable and parallel solution to reconstruct Gene RegulatoryNetworks (GRNs) from continuous gene expression data for very large amount ofgenes. The basic idea of our framework is motivated by the philosophy ofdivideand-conquer. Specifically, LAGE recursively partitions genes intomultiple overlapping communities with much smaller sizes, learnsintra-community GRNs respectively before merge them altogether. Besides, thecomplete information of overlapping communities serves as the byproduct, whichcould be used to mine meaningful functional modules in biological networks.
arxiv-1211-6340 | An Approach of Improving Students Academic Performance by using k means clustering algorithm and Decision tree |  http://arxiv.org/abs/1211.6340  | author:Md. Hedayetul Islam Shovon, Mahfuza Haque category:cs.LG published:2012-11-09 summary:Improving students academic performance is not an easy task for the academiccommunity of higher learning. The academic performance of engineering andscience students during their first year at university is a turning point intheir educational path and usually encroaches on their General PointAverage,GPA in a decisive manner. The students evaluation factors like classquizzes mid and final exam assignment lab work are studied. It is recommendedthat all these correlated information should be conveyed to the class teacherbefore the conduction of final exam. This study will help the teachers toreduce the drop out ratio to a significant level and improve the performance ofstudents. In this paper, we present a hybrid procedure based on Decision Treeof Data mining method and Data Clustering that enables academicians to predictstudents GPA and based on that instructor can take necessary step to improvestudent academic performance.
arxiv-1211-2116 | Localisation of Numerical Date Field in an Indian Handwritten Document |  http://arxiv.org/abs/1211.2116  | author:S Arunkumar, Pallab Kumar Sahu, Sudeep Gorai, Kalyan Ghosh category:cs.CV published:2012-11-09 summary:This paper describes a method to localise all those areas which mayconstitute the date field in an Indian handwritten document. Spatial patternsof the date field are studied from various handwritten documents and analgorithm is developed through statistical analysis to identify those sets ofconnected components which may constitute the date. Common date patternsfollowed in India are considered to classify the date formats in differentclasses. Reported results demonstrate promising performance of the proposedapproach
arxiv-1211-2082 | 3D Surface Reconstruction of Underwater Objects |  http://arxiv.org/abs/1211.2082  | author:C. J. Prabhakar, P. U. Praveen Kumar category:cs.CV published:2012-11-09 summary:In this paper, we propose a novel technique to reconstruct 3D surface of anunderwater object using stereo images. Reconstructing the 3D surface of anunderwater object is really a challenging task due to degraded quality ofunderwater images. There are various reason of quality degradation ofunderwater images i.e., non-uniform illumination of light on the surface ofobjects, scattering and absorption effects. Floating particles present inunderwater produces Gaussian noise on the captured underwater images whichdegrades the quality of images. The degraded underwater images are preprocessedby applying homomorphic, wavelet denoising and anisotropic filteringsequentially. The uncalibrated rectification technique is applied topreprocessed images to rectify the left and right images. The rectified leftand right image lies on a common plane. To find the correspondence points in aleft and right images, we have applied dense stereo matching technique i.e.,graph cut method. Finally, we estimate the depth of images using triangulationtechnique. The experimental result shows that the proposed method reconstruct3D surface of underwater objects accurately using captured underwater stereoimages.
arxiv-1211-1799 | Algorithm for Missing Values Imputation in Categorical Data with Use of Association Rules |  http://arxiv.org/abs/1211.1799  | author:Jiří Kaiser category:cs.LG published:2012-11-08 summary:This paper presents algorithm for missing values imputation in categoricaldata. The algorithm is based on using association rules and is presented inthree variants. Experimental shows better accuracy of missing values imputationusing the algorithm then using most common attribute value.
arxiv-1211-1752 | 3D Scene Grammar for Parsing RGB-D Pointclouds |  http://arxiv.org/abs/1211.1752  | author:Abhishek Anand, Sherwin Li category:cs.CV published:2012-11-08 summary:We pose 3D scene-understanding as a problem of parsing in a grammar. Agrammar helps us capture the compositional structure of real-word objects,e.g., a chair is composed of a seat, a back-rest and some legs. Having multiplerules for an object helps us capture structural variations in objects, e.g., achair can optionally also have arm-rests. Finally, having rules to capturecomposition at different levels helps us formulate the entire scene-processingpipeline as a single problem of finding most likely parse-tree---small segmentscombine to form parts of objects, parts to objects and objects to a scene. Weattach a generative probability model to our grammar by having afeature-dependent probability function for every rule. We evaluated it byextracting labels for every segment and comparing the results with thestate-of-the-art segment-labeling algorithm. Our algorithm was outperformed bythe state-or-the-art method. But, Our model can be trained very efficiently(within seconds), and it scales only linearly in with the number of rules inthe grammar. Also, we think that this is an important problem for the 3D visioncommunity. So, we are releasing our dataset and related code.
arxiv-1211-2007 | Multi-input Multi-output Beta Wavelet Network: Modeling of Acoustic Units for Speech Recognition |  http://arxiv.org/abs/1211.2007  | author:Ridha Ejbali, Mourad Zaied, Chokri Ben Amar category:cs.CV published:2012-11-08 summary:In this paper, we propose a novel architecture of wavelet network calledMulti-input Multi-output Wavelet Network MIMOWN as a generalization of the oldarchitecture of wavelet network. This newel prototype was applied to speechrecognition application especially to model acoustic unit of speech. Theoriginality of our work is the proposal of MIMOWN to model acoustic unit ofspeech. This approach was proposed to overcome limitation of old waveletnetwork model. The use of the multi-input multi-output architecture will allowstraining wavelet network on various examples of acoustic units.
arxiv-1211-1800 | A Comparative study of Arabic handwritten characters invariant feature |  http://arxiv.org/abs/1211.1800  | author:Hamdi Hassen, Maher khemakhem category:cs.CV published:2012-11-08 summary:This paper is practically interested in the unchangeable feature of Arabichandwritten character. It presents results of comparative study achieved oncertain features extraction techniques of handwritten character, based on Houghtransform, Fourier transform, Wavelet transform and Gabor Filter. Obtainedresults show that Hough Transform and Gabor filter are insensible to therotation and translation, Fourier Transform is sensible to the rotation butinsensible to the translation, in contrast to Hough Transform and Gabor filter,Wavelets Transform is sensitive to the rotation as well as to the translation.
arxiv-1211-1733 | Linear Antenna Array with Suppressed Sidelobe and Sideband Levels using Time Modulation |  http://arxiv.org/abs/1211.1733  | author:Swaprava Nath, Subrata Mitra category:cs.NE published:2012-11-08 summary:In this paper, the goal is to achieve an ultra low sidelobe level (SLL) andsideband levels (SBL) of a time modulated linear antenna array. The approachfollowed here is not to give fixed level of excitation to the elements of anarray, but to change it dynamically with time. The excitation levels of thedifferent array elements over time are varied to get the low sidelobe andsideband levels. The mathematics of getting the SLL and SBL furnished in detailand simulation is done using the mathematical results. The excitation patternover time is optimized using Genetic Algorithm (GA). Since, the amplitudes ofthe excitations of the elements are varied within a finite limit, results showit gives better sidelobe and sideband suppression compared to previous timemodulated arrays with uniform amplitude excitations.
arxiv-1211-1968 | Fourier-Bessel rotational invariant eigenimages |  http://arxiv.org/abs/1211.1968  | author:Zhizhen Zhao, Amit Singer category:cs.CV published:2012-11-08 summary:We present an efficient and accurate algorithm for principal componentanalysis (PCA) of a large set of two dimensional images, and, for each image,the set of its uniform rotations in the plane and its reflection. The algorithmstarts by expanding each image, originally given on a Cartesian grid, in theFourier-Bessel basis for the disk. Because the images are bandlimited in theFourier domain, we use a sampling criterion to truncate the Fourier-Besselexpansion such that the maximum amount of information is preserved without theeffect of aliasing. The constructed covariance matrix is invariant to rotationand reflection and has a special block diagonal structure. PCA is efficientlydone for each block separately. This Fourier-Bessel based PCA detects moremeaningful eigenimages and has improved denoising capability compared totraditional PCA for a finite number of noisy images.
arxiv-1211-1550 | A Riemannian geometry for low-rank matrix completion |  http://arxiv.org/abs/1211.1550  | author:B. Mishra, K. Adithya Apuroop, R. Sepulchre category:cs.LG cs.NA math.OC published:2012-11-07 summary:We propose a new Riemannian geometry for fixed-rank matrices that isspecifically tailored to the low-rank matrix completion problem. Exploiting thedegree of freedom of a quotient space, we tune the metric on our search spaceto the particular least square cost function. At one level, it illustrates in anovel way how to exploit the versatile framework of optimization on quotientmanifold. At another level, our algorithm can be considered as an improvedversion of LMaFit, the state-of-the-art Gauss-Seidel algorithm. We developnecessary tools needed to perform both first-order and second-orderoptimization. In particular, we propose gradient descent schemes (steepestdescent and conjugate gradient) and trust-region algorithms. We also show that,thanks to the simplicity of the cost function, it is numerically cheap toperform an exact linesearch given a search direction, which makes ouralgorithms competitive with the state-of-the-art on standard low-rank matrixcompletion instances.
arxiv-1211-1642 | Randomized Dimension Reduction on Massive Data |  http://arxiv.org/abs/1211.1642  | author:Stoyan Georgiev, Sayan Mukherjee category:stat.ML stat.ME published:2012-11-07 summary:Scalability of statistical estimators is of increasing importance in modernapplications and dimension reduction is often used to extract relevantinformation from data. A variety of popular dimension reduction approaches canbe framed as symmetric generalized eigendecomposition problems. In this paperwe outline how taking into account the low rank structure assumption implicitin these dimension reduction approaches provides both computational andstatistical advantages. We adapt recent randomized low-rank approximationalgorithms to provide efficient solutions to three dimension reduction methods:Principal Component Analysis (PCA), Sliced Inverse Regression (SIR), andLocalized Sliced Inverse Regression (LSIR). A key observation in this paper isthat randomization serves a dual role, improving both computational andstatistical performance. This point is highlighted in our experiments on realand simulated data.
arxiv-1211-1513 | K-Plane Regression |  http://arxiv.org/abs/1211.1513  | author:Naresh Manwani, P. S. Sastry category:cs.LG published:2012-11-07 summary:In this paper, we present a novel algorithm for piecewise linear regressionwhich can learn continuous as well as discontinuous piecewise linear functions.The main idea is to repeatedly partition the data and learn a liner model in ineach partition. While a simple algorithm incorporating this idea does not workwell, an interesting modification results in a good algorithm. The proposedalgorithm is similar in spirit to $k$-means clustering algorithm. We show thatour algorithm can also be viewed as an EM algorithm for maximum likelihoodestimation of parameters under a reasonable probability model. We empiricallydemonstrate the effectiveness of our approach by comparing its performance withthe state of art regression learning algorithms on some real world datasets.
arxiv-1211-1552 | Image denoising with multi-layer perceptrons, part 2: training trade-offs and analysis of their mechanisms |  http://arxiv.org/abs/1211.1552  | author:Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling category:cs.CV cs.LG published:2012-11-07 summary:Image denoising can be described as the problem of mapping from a noisy imageto a noise-free image. In another paper, we show that multi-layer perceptronscan achieve outstanding image denoising performance for various types of noise(additive white Gaussian noise, mixed Poisson-Gaussian noise, JPEG artifacts,salt-and-pepper noise and noise resembling stripes). In this work we discuss indetail which trade-offs have to be considered during the training procedure. Wewill show how to achieve good results and which pitfalls to avoid. By analysingthe activation patterns of the hidden units we are able to make observationsregarding the functioning principle of multi-layer perceptrons trained forimage denoising.
arxiv-1211-1716 | Blind Signal Separation in the Presence of Gaussian Noise |  http://arxiv.org/abs/1211.1716  | author:Mikhail Belkin, Luis Rademacher, James Voss category:cs.LG cs.DS stat.ML published:2012-11-07 summary:A prototypical blind signal separation problem is the so-called cocktailparty problem, with n people talking simultaneously and n different microphoneswithin a room. The goal is to recover each speech signal from the microphoneinputs. Mathematically this can be modeled by assuming that we are givensamples from an n-dimensional random variable X=AS, where S is a vector whosecoordinates are independent random variables corresponding to each speaker. Theobjective is to recover the matrix A^{-1} given random samples from X. A rangeof techniques collectively known as Independent Component Analysis (ICA) havebeen proposed to address this problem in the signal processing and machinelearning literature. Many of these techniques are based on using the kurtosisor other cumulants to recover the components. In this paper we propose a new algorithm for solving the blind signalseparation problem in the presence of additive Gaussian noise, when we aregiven samples from X=AS+\eta, where \eta is drawn from an unknown, notnecessarily spherical n-dimensional Gaussian distribution. Our approach isbased on a method for decorrelating a sample with additive Gaussian noise underthe assumption that the underlying distribution is a linear transformation of adistribution with independent components. Our decorrelation routine is based onthe properties of cumulant tensors and can be combined with any standardcumulant-based method for ICA to get an algorithm that is provably robust inthe presence of Gaussian noise. We derive polynomial bounds for the samplecomplexity and error propagation of our method.
arxiv-1211-1650 | Different Operating Systems Compatible for Image Prepress Process in Color Management: Analysis and Performance Testing |  http://arxiv.org/abs/1211.1650  | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-11-07 summary:Image computing has become a real catchphrase over the past few years and theinterpretations of the meaning of the term vary greatly. The Imagecomputingmarket is currently rapidly evolving with high growth prospects and almostdaily announcements of new devices and application platforms, which results inan increasing diversification of devices, operating system and developmentplatforms. Compared to more traditional information technology markets like theone of desktop computing, mobile computing is much less consolidated andneither standards nor even industry standards have yet been established. Thereare various platforms and interfaces which may be used to perform the desiredtasks through the device. We have tried to compare the various mobile operatingsystems and their trade-offs.
arxiv-1211-1544 | Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds |  http://arxiv.org/abs/1211.1544  | author:Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling category:cs.CV cs.LG published:2012-11-07 summary:Image denoising can be described as the problem of mapping from a noisy imageto a noise-free image. The best currently available denoising methodsapproximate this mapping with cleverly engineered algorithms. In this work weattempt to learn this mapping directly with plain multi layer perceptrons (MLP)applied to image patches. We will show that by training on large imagedatabases we are able to outperform the current state-of-the-art imagedenoising methods. In addition, our method achieves results that are superiorto one type of theoretical bound and goes a large way toward closing the gapwith a second type of theoretical bound. Our approach is easily adapted to lessextensively studied types of noise, such as mixed Poisson-Gaussian noise, JPEGartifacts, salt-and-pepper noise and noise resembling stripes, for which weachieve excellent results as well. We will show that combining a block-matchingprocedure with MLPs can further improve the results on certain images. In asecond paper, we detail the training trade-offs and the inner mechanisms of ourMLPs.
arxiv-1211-1654 | A New Randomness Evaluation Method with Applications to Image Shuffling and Encryption |  http://arxiv.org/abs/1211.1654  | author:Yue Wu, Sos Agaian, Joseph P. Noonan category:cs.CR cs.CV stat.AP published:2012-11-07 summary:This letter discusses the problem of testing the degree of randomness withinan image, particularly for a shuffled or encrypted image. Its key contributionsare: 1) a mathematical model of perfectly shuffled images; 2) the derivation ofthe theoretical distribution of pixel differences; 3) a new $Z$-test basedapproach to differentiate whether or not a test image is perfectly shuffled;and 4) a randomized algorithm to unbiasedly evaluate the degree of randomnesswithin a given image. Simulation results show that the proposed method isrobust and effective in evaluating the degree of randomness within an image,and may often be more suitable for image applications than commonly usedtesting schemes designed for binary data like NIST 800-22. The developed methodmay be also useful as a first step in determining whether or not a shuffling orencryption scheme is suitable for a particular cryptographic application.
arxiv-1211-1656 | James-Stein Type Center Pixel Weights for Non-Local Means Image Denoising |  http://arxiv.org/abs/1211.1656  | author:Yue Wu, Brian Tracey, Joseph P. Noonan category:cs.CV published:2012-11-07 summary:Non-Local Means (NLM) and variants have been proven to be effective androbust in many image denoising tasks. In this letter, we study the parameterselection problem of center pixel weights (CPW) in NLM. Our key contributionsare: 1) we give a novel formulation of the CPW problem from the statisticalshrinkage perspective; 2) we introduce the James-Stein type CPWs for NLM; and3) we propose a new adaptive CPW that is locally tuned for each image pixel.Our experimental results showed that compared to existing CPW solutions, thenew proposed CPWs are more robust and effective under various noise levels. Inparticular, the NLM with the James-Stein type CPWs attain higher means withsmaller variances in terms of the peak signal and noise ratio, implying theyimprove the NLM robustness and make it less sensitive to parameter selection.
arxiv-1211-1722 | Inverse problems in approximate uniform generation |  http://arxiv.org/abs/1211.1722  | author:Anindya De, Ilias Diakonikolas, Rocco A. Servedio category:cs.CC cs.DS cs.LG published:2012-11-07 summary:We initiate the study of \emph{inverse} problems in approximate uniformgeneration, focusing on uniform generation of satisfying assignments of varioustypes of Boolean functions. In such an inverse problem, the algorithm is givenuniform random satisfying assignments of an unknown function $f$ belonging to aclass $\C$ of Boolean functions, and the goal is to output a probabilitydistribution $D$ which is $\epsilon$-close, in total variation distance, to theuniform distribution over $f^{-1}(1)$. Positive results: We prove a general positive result establishing sufficientconditions for efficient inverse approximate uniform generation for a class$\C$. We define a new type of algorithm called a \emph{densifier} for $\C$, andshow (roughly speaking) how to combine (i) a densifier, (ii) an approximatecounting / uniform generation algorithm, and (iii) a Statistical Query learningalgorithm, to obtain an inverse approximate uniform generation algorithm. Weapply this general result to obtain a poly$(n,1/\eps)$-time algorithm for theclass of halfspaces; and a quasipoly$(n,1/\eps)$-time algorithm for the classof $\poly(n)$-size DNF formulas. Negative results: We prove a general negative result establishing that theexistence of certain types of signature schemes in cryptography implies thehardness of certain inverse approximate uniform generation problems. Thisimplies that there are no {subexponential}-time inverse approximate uniformgeneration algorithms for 3-CNF formulas; for intersections of two halfspaces;for degree-2 polynomial threshold functions; and for monotone 2-CNF formulas. Finally, we show that there is no general relationship between the complexityof the "forward" approximate uniform generation problem and the complexity ofthe inverse problem for a class $\C$ -- it is possible for either one to beeasy while the other is hard.
arxiv-1211-1526 | Explosion prediction of oil gas using SVM and Logistic Regression |  http://arxiv.org/abs/1211.1526  | author:Xiaofei Wang, Mingming Zhang, Liyong Shen, Suixiang Gao category:cs.CE cs.LG 62P30, 68T05 published:2012-11-07 summary:The prevention of dangerous chemical accidents is a primary problem ofindustrial manufacturing. In the accidents of dangerous chemicals, the oil gasexplosion plays an important role. The essential task of the explosionprevention is to estimate the better explosion limit of a given oil gas. Inthis paper, Support Vector Machines (SVM) and Logistic Regression (LR) are usedto predict the explosion of oil gas. LR can get the explicit probabilityformula of explosion, and the explosive range of the concentrations of oil gasaccording to the concentration of oxygen. Meanwhile, SVM gives higher accuracyof prediction. Furthermore, considering the practical requirements, the effectsof penalty parameter on the distribution of two types of errors are discussed.
arxiv-1211-1482 | Gender Recognition in Walk Gait through 3D Motion by Quadratic Bezier Curve and Statistical Techniques |  http://arxiv.org/abs/1211.1482  | author:Sajid Ali category:cs.CV published:2012-11-07 summary:Motion capture is the process of recording the movement of objects or people.It is used in military, entertainment, sports, and medical applications, andfor validation of computer vision[2] and robotics. In filmmaking and video gamedevelopment, it refers to recording actions of human actors, and using thatinformation to animate digital character models in 2D or 3D computer animation.When it includes face and fingers or captures subtle
arxiv-1211-1690 | Learning Monocular Reactive UAV Control in Cluttered Natural Environments |  http://arxiv.org/abs/1211.1690  | author:Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert category:cs.RO cs.CV cs.LG cs.SY published:2012-11-07 summary:Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairlystraight-forward, as expensive sensors and monitoring devices can be employed.In contrast, obstacle avoidance remains a challenging task for Micro AerialVehicles (MAVs) which operate at low altitude in cluttered environments. Unlikelarge vehicles, MAVs can only carry very light sensors, such as cameras, makingautonomous navigation through obstacles much more challenging. In this paper,we describe a system that navigates a small quadrotor helicopter autonomouslyat low altitude through natural forest environments. Using only a single cheapcamera to perceive the environment, we are able to maintain a constant velocityof up to 1.5m/s. Given a small set of human pilot demonstrations, we use recentstate-of-the-art imitation learning techniques to train a controller that canavoid trees by adapting the MAVs heading. We demonstrate the performance of oursystem in a more controlled environment indoors, and in real natural forestenvironments outdoors.
arxiv-1211-1893 | Tangent-based manifold approximation with locally linear models |  http://arxiv.org/abs/1211.1893  | author:Sofia Karygianni, Pascal Frossard category:cs.LG cs.CV published:2012-11-06 summary:In this paper, we consider the problem of manifold approximation with affinesubspaces. Our objective is to discover a set of low dimensional affinesubspaces that represents manifold data accurately while preserving themanifold's structure. For this purpose, we employ a greedy technique thatpartitions manifold samples into groups that can be each approximated by a lowdimensional subspace. We start by considering each manifold sample as adifferent group and we use the difference of tangents to determine appropriategroup mergings. We repeat this procedure until we reach the desired number ofsample groups. The best low dimensional affine subspaces corresponding to thefinal groups constitute our approximate manifold representation. Ourexperiments verify the effectiveness of the proposed scheme and show itssuperior performance compared to state-of-the-art methods for manifoldapproximation.
arxiv-1211-1265 | From Bits to Images: Inversion of Local Binary Descriptors |  http://arxiv.org/abs/1211.1265  | author:Emmanuel d'Angelo, Laurent jacques, Alexandre Alahi, Pierre Vandergheynst category:cs.CV cs.IT math.IT published:2012-11-06 summary:Local Binary Descriptors are becoming more and more popular for imagematching tasks, especially when going mobile. While they are extensivelystudied in this context, their ability to carry enough information in order toinfer the original image is seldom addressed. In this work, we leverage an inverse problem approach to show that it ispossible to directly reconstruct the image content from Local BinaryDescriptors. This process relies on very broad assumptions besides theknowledge of the pattern of the descriptor at hand. This generalizes previousresults that required either a prior learning database or non-binarizedfeatures. Furthermore, our reconstruction scheme reveals differences in the waydifferent Local Binary Descriptors capture and encode image information. Hence,the potential applications of our work are multiple, ranging from privacyissues caused by eavesdropping image keypoints streamed by mobile devices tothe design of better descriptors through the visualization and the analysis oftheir geometric content.
arxiv-1211-1127 | Visual Transfer Learning: Informal Introduction and Literature Overview |  http://arxiv.org/abs/1211.1127  | author:Erik Rodner category:cs.CV cs.LG published:2012-11-06 summary:Transfer learning techniques are important to handle small training sets andto allow for quick generalization even from only a few examples. The followingpaper is the introduction as well as the literature overview part of my thesisrelated to the topic of transfer learning for visual recognition problems.
arxiv-1211-1323 | Sample Size Planning for Classification Models |  http://arxiv.org/abs/1211.1323  | author:Claudia Beleites, Ute Neugebauer, Thomas Bocklitz, Christoph Krafft, Jürgen Popp category:stat.AP stat.ME stat.ML G.3 published:2012-11-06 summary:In biospectroscopy, suitably annotated and statistically independent samples(e. g. patients, batches, etc.) for classifier training and testing are scarceand costly. Learning curves show the model performance as function of thetraining sample size and can help to determine the sample size needed to traingood classifiers. However, building a good model is actually not enough: theperformance must also be proven. We discuss learning curves for typical smallsample size situations with 5 - 25 independent samples per class. Although theclassification models achieve acceptable performance, the learning curve can becompletely masked by the random testing uncertainty due to the equally limitedtest sample size. In consequence, we determine test sample sizes necessary toachieve reasonable precision in the validation and find that 75 - 100 sampleswill usually be needed to test a good but not perfect classifier. Such a dataset will then allow refined sample size planning on the basis of the achievedperformance. We also demonstrate how to calculate necessary sample sizes inorder to show the superiority of one classifier over another: this oftenrequires hundreds of statistically independent test samples or is eventheoretically impossible. We demonstrate our findings with a data set of ca.2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes andthree tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensivesimulation that allows precise determination of the actual performance of themodels in question.
arxiv-1211-1119 | A Survey on Techniques of Improving Generalization Ability of Genetic Programming Solutions |  http://arxiv.org/abs/1211.1119  | author:Vipul K. Dabhi, Sanjay Chaudhary category:cs.NE published:2012-11-06 summary:In the field of empirical modeling using Genetic Programming (GP), it isimportant to evolve solution with good generalization ability. Generalizationability of GP solutions get affected by two important issues: bloat andover-fitting. We surveyed and classified existing literature related todifferent techniques used by GP research community to deal with these issues.We also point out limitation of these techniques, if any. Moreover, theclassification of different bloat control approaches and measures for bloat andover-fitting are also discussed. We believe that this work will be useful to GPpractitioners in following ways: (i) to better understand concepts ofgeneralization in GP (ii) comparing existing bloat and over-fitting controltechniques and (iii) selecting appropriate approach to improve generalizationability of GP evolved solutions.
arxiv-1211-1255 | Handwritten digit recognition by bio-inspired hierarchical networks |  http://arxiv.org/abs/1211.1255  | author:Antonio G. Zippo, Giuliana Gelsomino, Sara Nencini, Gabriele E. M. Biella category:cs.LG cs.CV q-bio.NC published:2012-11-06 summary:The human brain processes information showing learning and predictionabilities but the underlying neuronal mechanisms still remain unknown.Recently, many studies prove that neuronal networks are able of bothgeneralizations and associations of sensory inputs. In this paper, following aset of neurophysiological evidences, we propose a learning framework with astrong biological plausibility that mimics prominent functions of corticalcircuitries. We developed the Inductive Conceptual Network (ICN), that is ahierarchical bio-inspired network, able to learn invariant patterns byVariable-order Markov Models implemented in its nodes. The outputs of thetop-most node of ICN hierarchy, representing the highest input generalization,allow for automatic classification of inputs. We found that the ICN clusterizedMNIST images with an error of 5.73% and USPS images with an error of 12.56%.
arxiv-1211-1082 | Active and passive learning of linear separators under log-concave distributions |  http://arxiv.org/abs/1211.1082  | author:Maria Florina Balcan, Philip M. Long category:cs.LG math.ST stat.ML stat.TH published:2012-11-06 summary:We provide new results concerning label efficient, polynomial time, passiveand active learning of linear separators. We prove that active learningprovides an exponential improvement over PAC (passive) learning of homogeneouslinear separators under nearly log-concave distributions. Building on this, weprovide a computationally efficient PAC algorithm with optimal (up to aconstant factor) sample complexity for such problems. This resolves an openquestion concerning the sample complexity of efficient PAC algorithms under theuniform distribution in the unit ball. Moreover, it provides the first boundfor a polynomial-time PAC algorithm that is tight for an interesting infiniteclass of hypothesis functions under a general and natural class ofdata-distributions, providing significant progress towards a longstanding openquestion. We also provide new bounds for active and passive learning in the case thatthe data might not be linearly separable, both in the agnostic case and andunder the Tsybakov low-noise condition. To derive our results, we provide newstructural results for (nearly) log-concave distributions, which might be ofindependent interest as well.
arxiv-1211-1275 | Kernelized Bayesian Matrix Factorization |  http://arxiv.org/abs/1211.1275  | author:Mehmet Gönen, Suleiman A. Khan, Samuel Kaski category:stat.ML published:2012-11-06 summary:We extend kernelized matrix factorization with a fully Bayesian treatment andwith an ability to work with multiple side information sources expressed asdifferent kernels. Kernel functions have been introduced to matrixfactorization to integrate side information about the rows and columns (e.g.,objects and users in recommender systems), which is necessary for makingout-of-matrix (i.e., cold start) predictions. We discuss specifically bipartitegraph inference, where the output matrix is binary, but extensions to moregeneral matrices are straightforward. We extend the state of the art in two keyaspects: (i) A fully conjugate probabilistic formulation of the kernelizedmatrix factorization problem enables an efficient variational approximation,whereas fully Bayesian treatments are not computationally feasible in theearlier approaches. (ii) Multiple side information sources are included,treated as different kernels in multiple kernel learning that additionallyreveals which side information sources are informative. Our method outperformsalternatives in predicting drug-protein interactions on two data sets. We thenshow that our framework can also be used for solving multilabel learningproblems by considering samples and labels as the two domains where matrixfactorization operates on. Our algorithm obtains the lowest Hamming loss valueson 10 out of 14 multilabel classification data sets compared to fivestate-of-the-art multilabel learning algorithms.
arxiv-1211-1328 | Random walk kernels and learning curves for Gaussian process regression on random graphs |  http://arxiv.org/abs/1211.1328  | author:Matthew Urry, Peter Sollich category:stat.ML cs.LG published:2012-11-06 summary:We consider learning on graphs, guided by kernels that encode similaritybetween vertices. Our focus is on random walk kernels, the analogues of squaredexponential kernels in Euclidean spaces. We show that on large, locallytreelike, graphs these have some counter-intuitive properties, specifically inthe limit of large kernel lengthscales. We consider using these kernels ascovariance matrices of e.g.\ Gaussian processes (GPs). In this situation onetypically scales the prior globally to normalise the average of the priorvariance across vertices. We demonstrate that, in contrast to the Euclideancase, this generically leads to significant variation in the prior varianceacross vertices, which is undesirable from the probabilistic modelling point ofview. We suggest the random walk kernel should be normalised locally, so thateach vertex has the same prior variance, and analyse the consequences of thisby studying learning curves for Gaussian process regression. Numericalcalculations as well as novel theoretical predictions for the learning curvesusing belief propagation make it clear that one obtains distinctly differentprobabilistic models depending on the choice of normalisation. Our method forpredicting the learning curves using belief propagation is significantly moreaccurate than previous approximations and should become exact in the limit oflarge random graphs.
arxiv-1211-1043 | Soft (Gaussian CDE) regression models and loss functions |  http://arxiv.org/abs/1211.1043  | author:Jose Hernandez-Orallo category:cs.LG stat.ML published:2012-11-05 summary:Regression, unlike classification, has lacked a comprehensive and effectiveapproach to deal with cost-sensitive problems by the reuse (and not are-training) of general regression models. In this paper, a wide variety ofcost-sensitive problems in regression (such as bids, asymmetric losses andrejection rules) can be solved effectively by a lightweight but powerfulapproach, consisting of: (1) the conversion of any traditional one-parametercrisp regression model into a two-parameter soft regression model, seen as anormal conditional density estimator, by the use of newly-introduced enrichmentmethods; and (2) the reframing of an enriched soft regression model to newcontexts by an instance-dependent optimisation of the expected loss derivedfrom the conditional normal distribution.
arxiv-1211-0817 | Discussion: Latent variable graphical model selection via convex optimization |  http://arxiv.org/abs/1211.0817  | author:Emmanuel J. Candés, Mahdi Soltanolkotabi category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1211-0801 | Discussion: Latent variable graphical model selection via convex optimization |  http://arxiv.org/abs/1211.0801  | author:Ming Yuan category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1211-0757 | Efficient Point-to-Subspace Query in $\ell^1$: Theory and Applications in Computer Vision |  http://arxiv.org/abs/1211.0757  | author:Ju Sun, Yuqian Zhang, John Wright category:stat.ML cs.CV stat.AP published:2012-11-05 summary:Motivated by vision tasks such as robust face and object recognition, weconsider the following general problem: given a collection of low-dimensionallinear subspaces in a high-dimensional ambient (image) space and a query point(image), efficiently determine the nearest subspace to the query in $\ell^1$distance. We show in theory that Cauchy random embedding of the objects intosignificantly-lower-dimensional spaces helps preserve the identity of thenearest subspace with constant probability. This offers the possibility ofefficiently selecting several candidates for accurate search. We sketchpreliminary experiments on robust face and digit recognition to corroborate ourtheory.
arxiv-1211-0879 | Comparing K-Nearest Neighbors and Potential Energy Method in classification problem. A case study using KNN applet by E.M. Mirkes and real life benchmark data sets |  http://arxiv.org/abs/1211.0879  | author:Yanshan Shi category:stat.ML cs.LG published:2012-11-05 summary:K-nearest neighbors (KNN) method is used in many supervised learningclassification problems. Potential Energy (PE) method is also developed forclassification problems based on its physical metaphor. The energy potentialused in the experiments are Yukawa potential and Gaussian Potential. In thispaper, I use both applet and MATLAB program with real life benchmark data toanalyze the performances of KNN and PE method in classification problems. Theresults show that in general, KNN and PE methods have similar performance. Inparticular, PE with Yukawa potential has worse performance than KNN when thedensity of the data is higher in the distribution of the database. When theGaussian potential is applied, the results from PE and KNN have similarbehavior. The indicators used are correlation coefficients and informationgain.
arxiv-1211-0932 | Kernels and Submodels of Deep Belief Networks |  http://arxiv.org/abs/1211.0932  | author:Guido F. Montufar, Jason Morton category:stat.ML published:2012-11-05 summary:We study the mixtures of factorizing probability distributions represented asvisible marginal distributions in stochastic layered networks. We take theperspective of kernel transitions of distributions, which gives a unifiedpicture of distributed representations arising from Deep Belief Networks (DBN)and other networks without lateral connections. We describe combinatorial andgeometric properties of the set of kernels and products of kernels realizableby DBNs as the network parameters vary. We describe explicit classes ofprobability distributions, including exponential families, that can be learnedby DBNs. We use these submodels to bound the maximal and the expectedKullback-Leibler approximation errors of DBNs from above depending on thenumber of hidden layers and units that they contain.
arxiv-1211-0806 | Discussion: Latent variable graphical model selection via convex optimization |  http://arxiv.org/abs/1211.0806  | author:Steffen Lauritzen, Nicolai Meinshausen category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1211-0919 | High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models |  http://arxiv.org/abs/1211.0919  | author:Majid Janzamin, Animashree Anandkumar category:stat.ML math.ST stat.TH 62H12 published:2012-11-05 summary:Fitting high-dimensional data involves a delicate tradeoff between faithfulrepresentation and the use of sparse models. Too often, sparsity assumptions onthe fitted model are too restrictive to provide a faithful representation ofthe observed data. In this paper, we present a novel framework incorporatingsparsity in different domains.We decompose the observed covariance matrix intoa sparse Gaussian Markov model (with a sparse precision matrix) and a sparseindependence model (with a sparse covariance matrix). Our frameworkincorporates sparse covariance and sparse precision estimation as special casesand thus introduces a richer class of high-dimensional models. We characterizesufficient conditions for identifiability of the two models, \viz Markov andindependence models. We propose an efficient decomposition method based on amodification of the popular $\ell_1$-penalized maximum-likelihood estimator($\ell_1$-MLE). We establish that our estimator is consistent in both thedomains, i.e., it successfully recovers the supports of both Markov andindependence models, when the number of samples $n$ scales as $n = \Omega(d^2\log p)$, where $p$ is the number of variables and $d$ is the maximum nodedegree in the Markov model. Our experiments validate these results and alsodemonstrate that our models have better inference accuracy under simplealgorithms such as loopy belief propagation.
arxiv-1211-0996 | Learning using Local Membership Queries |  http://arxiv.org/abs/1211.0996  | author:Pranjal Awasthi, Vitaly Feldman, Varun Kanade category:cs.LG cs.AI published:2012-11-05 summary:We introduce a new model of membership query (MQ) learning, where thelearning algorithm is restricted to query points that are \emph{close} torandom examples drawn from the underlying distribution. The learning model isintermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (wherethe queries are allowed to be arbitrary points). Membership query algorithms are not popular among machine learningpractitioners. Apart from the obvious difficulty of adaptively queryinglabelers, it has also been observed that querying \emph{unnatural} points leadsto increased noise from human labelers (Lang and Baum, 1992). This motivatesour study of learning algorithms that make queries that are close to examplesgenerated from the data distribution. We restrict our attention to functions defined on the $n$-dimensional Booleanhypercube and say that a membership query is local if its Hamming distance fromsome example in the (random) training data is at most $O(\log(n))$. We show thefollowing results in this model: (i) The class of sparse polynomials (with coefficients in R) over $\{0,1\}^n$is polynomial time learnable under a large class of \emph{locally smooth}distributions using $O(\log(n))$-local queries. This class also includes theclass of $O(\log(n))$-depth decision trees. (ii) The class of polynomial-sized decision trees is polynomial timelearnable under product distributions using $O(\log(n))$-local queries. (iii) The class of polynomial size DNF formulas is learnable under theuniform distribution using $O(\log(n))$-local queries in time$n^{O(\log(\log(n)))}$. (iv) In addition we prove a number of results relating the proposed model tothe traditional PAC model and the PAC+MQ model.
arxiv-1211-0835 | Rejoinder: Latent variable graphical model selection via convex optimization |  http://arxiv.org/abs/1211.0835  | author:Venkat Chandrasekaran, Pablo A. Parrilo, Alan S. Willsky category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Rejoinder to "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1211-1041 | Algorithms and Hardness for Robust Subspace Recovery |  http://arxiv.org/abs/1211.1041  | author:Moritz Hardt, Ankur Moitra category:cs.CC cs.DS cs.IT cs.LG math.IT published:2012-11-05 summary:We consider a fundamental problem in unsupervised learning called\emph{subspace recovery}: given a collection of $m$ points in $\mathbb{R}^n$,if many but not necessarily all of these points are contained in a$d$-dimensional subspace $T$ can we find it? The points contained in $T$ arecalled {\em inliers} and the remaining points are {\em outliers}. This problemhas received considerable attention in computer science and in statistics. Yetefficient algorithms from computer science are not robust to {\em adversarial}outliers, and the estimators from robust statistics are hard to compute in highdimensions. Are there algorithms for subspace recovery that are both robust to outliersand efficient? We give an algorithm that finds $T$ when it contains more than a$\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimatoris both easy to compute and well-behaved when there are a constant fraction ofoutliers. We prove that it is Small Set Expansion hard to find $T$ when thefraction of errors is any larger, thus giving evidence that our estimator is an{\em optimal} compromise between efficiency and robustness. As it turns out, this basic problem has a surprising number of connections toother areas including small set expansion, matroid theory and functionalanalysis that we make use of here.
arxiv-1211-0906 | Algorithm Runtime Prediction: Methods & Evaluation |  http://arxiv.org/abs/1211.0906  | author:Frank Hutter, Lin Xu, Holger H. Hoos, Kevin Leyton-Brown category:cs.AI cs.LG cs.PF stat.ML 68T20 I.2.8; I.2.6 published:2012-11-05 summary:Perhaps surprisingly, it is possible to predict how long an algorithm willtake to run on a previously unseen input, using machine learning techniques tobuild a model of the algorithm's runtime as a function of problem-specificinstance features. Such models have important applications to algorithmanalysis, portfolio-based algorithm selection, and the automatic configurationof parameterized algorithms. Over the past decade, a wide variety of techniqueshave been studied for building such models. Here, we describe extensions andimprovements of existing models, new families of models, and -- perhaps mostimportantly -- a much more thorough treatment of algorithm parameters as modelinputs. We also comprehensively describe new and existing features forpredicting algorithm runtime for propositional satisfiability (SAT), travellingsalesperson (TSP) and mixed integer programming (MIP) problems. We evaluatethese innovations through the largest empirical analysis of its kind, comparingto a wide range of runtime modelling techniques from the literature. Ourexperiments consider 11 algorithms and 35 instance distributions; they alsospan a very wide range of SAT, MIP, and TSP instances, with the leaststructured having been generated uniformly at random and the most structuredhaving emerged from real industrial applications. Overall, we demonstrate thatour new models yield substantially better runtime predictions than previousapproaches in terms of their generalization to new problem instances, to newalgorithms from a parameterized space, and to both simultaneously.
arxiv-1211-0808 | Discussion: Latent variable graphical model selection via convex optimization |  http://arxiv.org/abs/1211.0808  | author:Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1211-0660 | Generation of Two-Layer Monotonic Functions |  http://arxiv.org/abs/1211.0660  | author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE published:2012-11-04 summary:The problem of implementing a class of functions with particular conditionsby using monotonic multilayer functions is considered. A genetic algorithm isused to create monotonic functions of a certain class, and these areimplemented with two-layer monotonic functions. The existence of a solution tothe given problem suggests that from two monotone functions, a monotonicfunction with the same dimensions can be created. A new algorithm based on thegenetic algorithm is proposed, which easily implemented two-layer monotonicfunctions of a specific class for up to six variables.
arxiv-1211-0730 | Intelligent Algorithm for Optimum Solutions Based on the Principles of Bat Sonar |  http://arxiv.org/abs/1211.0730  | author:Mohammed Ali Tawfeeq category:cs.NE published:2012-11-04 summary:This paper presents a new intelligent algorithm that can solve the problemsof finding the optimum solution in the state space among which the desiredsolution resides. The algorithm mimics the principles of bat sonar in findingits targets. The algorithm introduces three search approaches. The first searchapproach considers a single sonar unit (SSU) with a fixed beam length and asingle starting point. In this approach, although the results converge towardthe optimum fitness, it is not guaranteed to find the global optimum solutionespecially for complex problems; it is satisfied with finding 'acceptably good'solutions to these problems. The second approach considers multisonar units(MSU) working in parallel in the same state space. Each unit has its ownstarting point and tries to find the optimum solution. In this approach theprobability that the algorithm converges toward the optimum solution issignificantly increased. It is found that this approach is suitable for complexfunctions and for problems of wide state space. In the third approach, a singlesonar unit with a moment (SSM) is used in order to handle the problem ofconvergence toward a local optimum rather than a global optimum. The momentumterm is added to the length of the transmitted beams. This will give the chanceto find the best fitness in a wider range within the state space. In this papera comparison between the proposed algorithm and genetic algorithm (GA) has beenmade. It showed that both of the algorithms can catch approximately the optimumsolutions for all of the testbed functions except for the function that has alocal minimum, in which the proposed algorithm's result is much better thanthat of the GA algorithm. On the other hand, the comparison showed that therequired execution time to obtain the optimum solution using the proposedalgorithm is much less than that of the GA algorithm.
arxiv-1211-0616 | The complexity of learning halfspaces using generalized linear methods |  http://arxiv.org/abs/1211.0616  | author:Amit Daniely, Nati Linial, Shai Shalev-Shwartz category:cs.LG cs.DS published:2012-11-03 summary:Many popular learning algorithms (E.g. Regression, Fourier-Transform basedalgorithms, Kernel SVM and Kernel ridge regression) operate by reducing theproblem to a convex optimization problem over a vector space of functions.These methods offer the currently best approach to several central problemssuch as learning half spaces and learning DNF's. In addition they are widelyused in numerous application domains. Despite their importance, there are stillvery few proof techniques to show limits on the power of these algorithms. We study the performance of this approach in the problem of (agnostically andimproperly) learning halfspaces with margin $\gamma$. Let $\mathcal{D}$ be adistribution over labeled examples. The $\gamma$-margin error of a hyperplane$h$ is the probability of an example to fall on the wrong side of $h$ or at adistance $\le\gamma$ from it. The $\gamma$-margin error of the best $h$ isdenoted $\mathrm{Err}_\gamma(\mathcal{D})$. An $\alpha(\gamma)$-approximationalgorithm receives $\gamma,\epsilon$ as input and, using i.i.d. samples of$\mathcal{D}$, outputs a classifier with error rate $\le\alpha(\gamma)\mathrm{Err}_\gamma(\mathcal{D}) + \epsilon$. Such an algorithmis efficient if it uses $\mathrm{poly}(\frac{1}{\gamma},\frac{1}{\epsilon})$samples and runs in time polynomial in the sample size. The best approximation ratio achievable by an efficient algorithm is$O\left(\frac{1/\gamma}{\sqrt{\log(1/\gamma)}}\right)$ and is achieved using analgorithm from the above class. Our main result shows that the approximationratio of every efficient algorithm from this family must be $\ge\Omega\left(\frac{1/\gamma}{\mathrm{poly}\left(\log\left(1/\gamma\right)\right)}\right)$,essentially matching the best known upper bound.
arxiv-1211-0613 | Application of Symmetric Uncertainty and Mutual Information to Dimensionality Reduction and Classification of Hyperspectral Images |  http://arxiv.org/abs/1211.0613  | author:ELkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV published:2012-11-03 summary:Remote sensing is a technology to acquire data for disatant substances,necessary to construct a model knowledge for applications as classification.Recently Hyperspectral Images (HSI) becomes a high technical tool that the maingoal is to classify the point of a region. The HIS is more than a hundredbidirectional measures, called bands (or simply images), of the same regioncalled Ground Truth Map (GT). But some bands are not relevant because they areaffected by different atmospheric effects; others contain redundantinformation; and high dimensionality of HSI features make the accuracy ofclassification lower. All these bands can be important for some applications;but for the classification a small subset of these is relevant. The problematicrelated to HSI is the dimensionality reduction. Many studies use mutualinformation (MI) to select the relevant bands. Others studies use the MInormalized forms, like Symmetric Uncertainty, in medical imagery applications.In this paper we introduce an algorithm based also on MI to select relevantbands and it apply the Symmetric Uncertainty coefficient to control redundancyand increase the accuracy of classification. This algorithm is featureselection tool and a Filter strategy. We establish this study on HSI AVIRIS92AV3C. This is an effectiveness, and fast scheme to control redundancy.
arxiv-1211-0587 | Partition Tree Weighting |  http://arxiv.org/abs/1211.0587  | author:Joel Veness, Martha White, Michael Bowling, András György category:cs.IT cs.LG math.IT stat.ML published:2012-11-03 summary:This paper introduces the Partition Tree Weighting technique, an efficientmeta-algorithm for piecewise stationary sources. The technique works byperforming Bayesian model averaging over a large class of possible partitionsof the data into locally stationary segments. It uses a prior, closely relatedto the Context Tree Weighting technique of Willems, that is well suited to datacompression applications. Our technique can be applied to any codingdistribution at an additional time and space cost only logarithmic in thesequence length. We provide a competitive analysis of the redundancy of ourmethod, and explore its application in a variety of settings. The order of theredundancy and the complexity of our algorithm matches those of the bestcompetitors available in the literature, and the new algorithm exhibits asuperior complexity-performance trade-off in our experiments.
arxiv-1211-0602 | Segmentation of ultrasound images of thyroid nodule for assisting fine needle aspiration cytology |  http://arxiv.org/abs/1211.0602  | author:Jie Zhao, Wei Zheng, Li Zhang, Hua Tian category:cs.CV published:2012-11-03 summary:The incidence of thyroid nodule is very high and generally increases with theage. Thyroid nodule may presage the emergence of thyroid cancer. The thyroidnodule can be completely cured if detected early. Fine needle aspirationcytology is a recognized early diagnosis method of thyroid nodule. There arestill some limitations in the fine needle aspiration cytology, and theultrasound diagnosis of thyroid nodule has become the first choice forauxiliary examination of thyroid nodular disease. If we could combine medicalimaging technology and fine needle aspiration cytology, the diagnostic rate ofthyroid nodule would be improved significantly. The properties of ultrasoundwill degrade the image quality, which makes it difficult to recognize the edgesfor physicians. Image segmentation technique based on graph theory has become aresearch hotspot at present. Normalized cut (Ncut) is a representative one,which is suitable for segmentation of feature parts of medical image. However,how to solve the normalized cut has become a problem, which needs large memorycapacity and heavy calculation of weight matrix. It always generates oversegmentation or less segmentation which leads to inaccurate in thesegmentation. The speckle noise in B ultrasound image of thyroid tumor makesthe quality of the image deteriorate. In the light of this characteristic, wecombine the anisotropic diffusion model with the normalized cut in this paper.After the enhancement of anisotropic diffusion model, it removes the noise inthe B ultrasound image while preserves the important edges and local details.This reduces the amount of computation in constructing the weight matrix of theimproved normalized cut and improves the accuracy of the final segmentationresults. The feasibility of the method is proved by the experimental results.
arxiv-1211-0632 | Stochastic ADMM for Nonsmooth Optimization |  http://arxiv.org/abs/1211.0632  | author:Hua Ouyang, Niao He, Alexander Gray category:cs.LG math.OC stat.ML published:2012-11-03 summary:We present a stochastic setting for optimization problems with nonsmoothconvex separable objective functions over linear equality constraints. To solvesuch problems, we propose a stochastic Alternating Direction Method ofMultipliers (ADMM) algorithm. Our algorithm applies to a more general class ofnonsmooth convex functions that does not necessarily have a closed-formsolution by minimizing the augmented function directly. We also demonstrate therates of convergence for our algorithm under various structural assumptions ofthe stochastic functions: $O(1/\sqrt{t})$ for convex functions and $O(\logt/t)$ for strongly convex functions. Compared to previous literature, weestablish the convergence rate of ADMM algorithm, for the first time, in termsof both the objective value and the feasibility violation.
arxiv-1211-0424 | Learning classifier systems with memory condition to solve non-Markov problems |  http://arxiv.org/abs/1211.0424  | author:Zhaoxiang Zang, Dehua Li, Junying Wang category:cs.NE cs.AI published:2012-11-02 summary:In the family of Learning Classifier Systems, the classifier system XCS hasbeen successfully used for many applications. However, the standard XCS has nomemory mechanism and can only learn optimal policy in Markov environments,where the optimal action is determined solely by the state of current sensoryinput. In practice, most environments are partially observable environments onagent's sensation, which are also known as non-Markov environments. Withinthese environments, XCS either fails, or only develops a suboptimal policy,since it has no memory. In this work, we develop a new classifier system basedon XCS to tackle this problem. It adds an internal message list to XCS as thememory list to record input sensation history, and extends a small number ofclassifiers with memory conditions. The classifier's memory condition, as afoothold to disambiguate non-Markov states, is used to sense a specifiedelement in the memory list. Besides, a detection method is employed torecognize non-Markov states in environments, to avoid these states controllingover classifiers' memory conditions. Furthermore, four sets of differentcomplex maze environments have been tested by the proposed method. Experimentalresults show that our system is one of the best techniques to solve partiallyobservable environments, compared with some well-known classifier systemsproposed for these environments.
arxiv-1211-0358 | Deep Gaussian Processes |  http://arxiv.org/abs/1211.0358  | author:Andreas C. Damianou, Neil D. Lawrence category:stat.ML cs.LG math.PR 60G15, 58E30 published:2012-11-02 summary:In this paper we introduce deep Gaussian process (GP) models. Deep GPs are adeep belief network based on Gaussian process mappings. The data is modeled asthe output of a multivariate GP. The inputs to that Gaussian process are thengoverned by another GP. A single layer model is equivalent to a standard GP orthe GP latent variable model (GP-LVM). We perform inference in the model byapproximate variational marginalization. This results in a strict lower boundon the marginal likelihood of the model which we use for model selection(number of layers and nodes per layer). Deep belief networks are typicallyapplied to relatively large data sets using stochastic gradient descent foroptimization. Our fully Bayesian treatment allows for the application of deepmodels even when data is scarce. Model selection by our variational bound showsthat a five layer hierarchy is justified even when modelling a digit data setcontaining only 150 examples.
arxiv-1211-0498 | Detecting English Writing Styles For Non-native Speakers |  http://arxiv.org/abs/1211.0498  | author:Rami Al-Rfou' category:cs.CL published:2012-11-02 summary:Analyzing writing styles of non-native speakers is a challenging task. Inthis paper, we analyze the comments written in the discussion pages of theEnglish Wikipedia. Using learning algorithms, we are able to detect nativespeakers' writing style with an accuracy of 74%. Given the diversity of theEnglish Wikipedia users and the large number of languages they speak, wemeasure the similarities among their native languages by comparing theinfluence they have on their English writing style. Our results show thatlanguages known to have the same origin and development path have similarfootprint on their speakers' English writing style. To enable further studies,the dataset we extracted from Wikipedia will be made available publicly.
arxiv-1211-0447 | Ordinal Rating of Network Performance and Inference by Matrix Completion |  http://arxiv.org/abs/1211.0447  | author:Wei Du, Yongjun Liao, and Pierre Geurts, Guy Leduc category:cs.NI cs.LG published:2012-11-02 summary:This paper addresses the large-scale acquisition of end-to-end networkperformance. We made two distinct contributions: ordinal rating of networkperformance and inference by matrix completion. The former reduces measurementcosts and unifies various metrics which eases their processing in applications.The latter enables scalable and accurate inference with no requirement ofstructural information of the network nor geometric constraints. By combiningboth, the acquisition problem bears strong similarities to recommender systems.This paper investigates the applicability of various matrix factorizationmodels used in recommender systems. We found that the simple regularized matrixfactorization is not only practical but also produces accurate results that arebeneficial for peer selection.
arxiv-1211-0889 | APPLE: Approximate Path for Penalized Likelihood Estimators |  http://arxiv.org/abs/1211.0889  | author:Yi Yu, Yang Feng category:stat.ML cs.LG published:2012-11-02 summary:In high-dimensional data analysis, penalized likelihood estimators are shownto provide superior results in both variable selection and parameterestimation. A new algorithm, APPLE, is proposed for calculating the ApproximatePath for Penalized Likelihood Estimators. Both the convex penalty (such asLASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered.The APPLE efficiently computes the solution path for the penalized likelihoodestimator using a hybrid of the modified predictor-corrector method and thecoordinate-descent algorithm. APPLE is compared with several well-knownpackages via simulation and analysis of two gene expression data sets.
arxiv-1211-0373 | Minimax sparse principal subspace estimation in high dimensions |  http://arxiv.org/abs/1211.0373  | author:Vincent Q. Vu, Jing Lei category:math.ST stat.ML stat.TH published:2012-11-02 summary:We study sparse principal components analysis in high dimensions, where $p$(the number of variables) can be much larger than $n$ (the number ofobservations), and analyze the problem of estimating the subspace spanned bythe principal eigenvectors of the population covariance matrix. We introducetwo complementary notions of $\ell_q$ subspace sparsity: row sparsity andcolumn sparsity. We prove nonasymptotic lower and upper bounds on the minimaxsubspace estimation error for $0\leq q\leq1$. The bounds are optimal for rowsparse subspaces and nearly optimal for column sparse subspaces, they apply togeneral classes of covariance matrices, and they show that $\ell_q$ constrainedestimates can achieve optimal minimax rates without restrictive spikedcovariance conditions. Interestingly, the form of the rates matches knownresults for sparse regression when the effective noise variance is definedappropriately. Our proof employs a novel variational $\sin\Theta$ theorem thatmay be useful in other regularized spectral estimation problems.
arxiv-1211-0439 | Learning curves for multi-task Gaussian process regression |  http://arxiv.org/abs/1211.0439  | author:Simon R. F. Ashton, Peter Sollich category:cs.LG stat.ML published:2012-11-02 summary:We study the average case performance of multi-task Gaussian process (GP)regression as captured in the learning curve, i.e. the average Bayes error fora chosen task versus the total number of examples $n$ for all tasks. For GPcovariances that are the product of an input-dependent covariance function anda free-form inter-task covariance matrix, we show that accurate approximationsfor the learning curve can be obtained for an arbitrary number of tasks $T$. Weuse these to study the asymptotic learning behaviour for large $n$.Surprisingly, multi-task learning can be asymptotically essentially useless, inthe sense that examples from other tasks help only when the degree ofinter-task correlation, $\rho$, is near its maximal value $\rho=1$. This effectis most extreme for learning of smooth target functions as described by e.g.squared exponential kernels. We also demonstrate that when learning many tasks,the learning curves separate into an initial phase, where the Bayes error oneach task is reduced down to a plateau value by "collective learning" eventhough most tasks have not seen examples, and a final decay that occurs oncethe number of examples is proportional to the number of tasks.
arxiv-1211-0418 | Verbalizing Ontologies in Controlled Baltic Languages |  http://arxiv.org/abs/1211.0418  | author:Normunds Grūzītis, Gunta Nešpore, Baiba Saulīte category:cs.CL cs.AI published:2012-11-02 summary:Controlled natural languages (mostly English-based) recently have emerged asseemingly informal supplementary means for OWL ontology authoring, if comparedto the formal notations that are used by professional knowledge engineers. Inthis paper we present by examples controlled Latvian language that has beendesigned to be compliant with the state of the art Attempto Controlled English.We also discuss relation with controlled Lithuanian language that is beingdesigned in parallel.
arxiv-1211-0074 | Transition-Based Dependency Parsing With Pluggable Classifiers |  http://arxiv.org/abs/1211.0074  | author:Alex Rudnick category:cs.CL published:2012-11-01 summary:In principle, the design of transition-based dependency parsers makes itpossible to experiment with any general-purpose classifier without otherchanges to the parsing algorithm. In practice, however, it often takessubstantial software engineering to bridge between the differentrepresentations used by two software packages. Here we present extensions toMaltParser that allow the drop-in use of any classifier conforming to theinterface of the Weka machine learning package, a wrapper for the TiMBLmemory-based learner to this interface, and experiments on multilingualdependency parsing with a variety of classifiers. While earlier work hadsuggested that memory-based learners might be a good choice for low-resourceparsing scenarios, we cannot support that hypothesis in this work. We observedthat support-vector machines give better parsing performance than thememory-based learner, regardless of the size of the training set.
arxiv-1211-0174 | Laplace approximation for logistic Gaussian process density estimation and regression |  http://arxiv.org/abs/1211.0174  | author:Jaakko Riihimäki, Aki Vehtari category:stat.CO stat.ME stat.ML published:2012-11-01 summary:Logistic Gaussian process (LGP) priors provide a flexible alternative formodelling unknown densities. The smoothness properties of the density estimatescan be controlled through the prior covariance structure of the LGP, but thechallenge is the analytically intractable inference. In this paper, we presentapproximate Bayesian inference for LGP density estimation in a grid usingLaplace's method to integrate over the non-Gaussian posterior distribution oflatent function values and to determine the covariance function parameters withtype-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace'smethod with MAP is sufficiently fast for practical interactive visualisation of1D and 2D densities. Our experiments with simulated and real 1D data sets showthat the estimation accuracy is close to a Markov chain Monte Carloapproximation and state-of-the-art hierarchical infinite Gaussian mixturemodels. We also construct a reduced-rank approximation to speed up thecomputations for dense 2D grids, and demonstrate density regression with theproposed Laplace approach.
arxiv-1211-0210 | Extension of TSVM to Multi-Class and Hierarchical Text Classification Problems With General Losses |  http://arxiv.org/abs/1211.0210  | author:Sathiya Keerthi Selvaraj, Sundararajan Sellamanickam, Shirish Shevade category:cs.LG published:2012-11-01 summary:Transductive SVM (TSVM) is a well known semi-supervised large margin learningmethod for binary text classification. In this paper we extend this method tomulti-class and hierarchical classification problems. We point out that thedetermination of labels of unlabeled examples with fixed classifier weights isa linear programming problem. We devise an efficient technique for solving it.The method is applicable to general loss functions. We demonstrate the value ofthe new method using large margin loss on a number of multi-class andhierarchical classification datasets. For maxent loss we show empirically thatour method is better than expectation regularization/constraint and posteriorregularization methods, and competitive with the version of entropyregularization method which uses label constraints.
arxiv-1211-0135 | Sampling and Reconstruction of Spatial Fields using Mobile Sensors |  http://arxiv.org/abs/1211.0135  | author:Jayakrishnan Unnikrishnan, Martin Vetterli category:cs.MM cs.CV cs.IT math.IT published:2012-11-01 summary:Spatial sampling is traditionally studied in a static setting where staticsensors scattered around space take measurements of the spatial field at theirlocations. In this paper we study the emerging paradigm of sampling andreconstructing spatial fields using sensors that move through space. We showthat mobile sensing offers some unique advantages over static sensing insensing time-invariant bandlimited spatial fields. Since a moving sensorencounters such a spatial field along its path as a time-domain signal, atime-domain anti-aliasing filter can be employed prior to sampling the signalreceived at the sensor. Such a filtering procedure, when used by aconfiguration of sensors moving at constant speeds along equispaced parallellines, leads to a complete suppression of spatial aliasing in the direction ofmotion of the sensors. We analytically quantify the advantage of using such asampling scheme over a static sampling scheme by computing the reduction insampling noise due to the filter. We also analyze the effects of non-uniformsensor speeds on the reconstruction accuracy. Using simulation examples wedemonstrate the advantages of mobile sampling over static sampling in practicalproblems. We extend our analysis to sampling and reconstruction schemes for monitoringtime-varying bandlimited fields using mobile sensors. We demonstrate that insome situations we require a lower density of sensors when using a mobilesensing scheme instead of the conventional static sensing scheme. The exactadvantage is quantified for a problem of sampling and reconstructing an audiofield.
arxiv-1210-8262 | On the Relation Between the Common Labelling and the Median Graph |  http://arxiv.org/abs/1210.8262  | author:Nicola Rebagliati, Albert Solé-Ribalta, Marcello Pelillo, Francesc Serratosa category:cs.CV published:2012-10-31 summary:In structural pattern recognition, given a set of graphs, the computation ofa Generalized Median Graph is a well known problem. Some methods approach theproblem by assuming a relation between the Generalized Median Graph and theCommon Labelling problem. However, this relation has still not been formallyproved. In this paper, we analyse such relation between both problems. The mainresult proves that the cost of the common labelling upper-bounds the cost ofthe median with respect to the given set. In addition, we show that the twoproblems are equivalent in some cases.
arxiv-1210-8436 | Optimal size, freshness and time-frame for voice search vocabulary |  http://arxiv.org/abs/1210.8436  | author:Maryam Kamvar, Ciprian Chelba category:cs.CL cs.IR published:2012-10-31 summary:In this paper, we investigate how to optimize the vocabulary for a voicesearch language model. The metric we optimize over is the out-of-vocabulary(OoV) rate since it is a strong indicator of user experience. In a departurefrom the usual way of measuring OoV rates, web search logs allow us to computethe per-session OoV rate and thus estimate the percentage of users thatexperience a given OoV rate. Under very conservative text normalization, wefind that a voice search vocabulary consisting of 2 to 2.5 million wordsextracted from 1 week of search query data will result in an aggregate OoV rateof 1%; at that size, the same OoV rate will also be experienced by 90% ofusers. The number of words included in the vocabulary is a stable indicator ofthe OoV rate. Altering the freshness of the vocabulary or the duration of thetime window over which the training data is gathered does not significantlychange the OoV rate. Surprisingly, a significantly larger vocabulary(approximately 10 million words) is required to guarantee OoV rates below 1%for 95% of the users.
arxiv-1210-8440 | Large Scale Language Modeling in Automatic Speech Recognition |  http://arxiv.org/abs/1210.8440  | author:Ciprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, Shankar Kumar category:cs.CL published:2012-10-31 summary:Large language models have been proven quite beneficial for a variety ofautomatic speech recognition tasks in Google. We summarize results on VoiceSearch and a few YouTube speech transcription tasks to highlight the impactthat one can expect from increasing both the amount of training data, and thesize of the language model estimated from such data. Depending on the task,availability and amount of training data used, language model size and amountof work and care put into integrating them in the lattice rescoring step weobserve reductions in word error rate between 6% and 10% relative, for systemson a wide range of operating points between 17% and 52% word error rate.
arxiv-1210-8385 | First Experiments with PowerPlay |  http://arxiv.org/abs/1210.8385  | author:Rupesh Kumar Srivastava, Bas R. Steunebrink, Jürgen Schmidhuber category:cs.AI cs.LG published:2012-10-31 summary:Like a scientist or a playing child, PowerPlay not only learns new skills tosolve given problems, but also invents new interesting problems by itself. Bydesign, it continually comes up with the fastest to find, initially novel, buteventually solvable tasks. It also continually simplifies or compresses orspeeds up solutions to previous tasks. Here we describe first experiments withPowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as ageneral computational problem solving architecture. Its connection weights canencode arbitrary, self-delimiting, halting or non-halting programs affectingboth environment (through effectors) and internal states encoding abstractionsof event sequences. Our PowerPlay-driven SLIM RNN learns to become anincreasingly general solver of self-invented problems, continually adding newproblem solving procedures to its growing skill repertoire. Extending a recentconference paper, we identify interesting, emerging, developmental stages ofour open-ended system. We also show how it automatically self-modularizes,frequently re-using code for previously invented skills, always trying toinvent novel tasks that can be quickly validated because they do not requiretoo many weight changes affecting too many previous tasks.
arxiv-1211-0028 | Understanding the Interaction between Interests, Conversations and Friendships in Facebook |  http://arxiv.org/abs/1211.0028  | author:Qirong Ho, Rong Yan, Rajat Raina, Eric P. Xing category:cs.SI cs.LG stat.ML published:2012-10-31 summary:In this paper, we explore salient questions about user interests,conversations and friendships in the Facebook social network, using a novellatent space model that integrates several data types. A key challenge ofstudying Facebook's data is the wide range of data modalities such as text,network links, and categorical labels. Our latent space model seamlesslycombines all three data modalities over millions of users, allowing us to studythe interplay between user friendships, interests, and higher-ordernetwork-wide social trends on Facebook. The recovered insights not only answerour initial questions, but also reveal surprising facts about user interests inthe context of Facebook's ecosystem. We also confirm that our results aresignificant with respect to evidential information from the study subjects.
arxiv-1211-0055 | Dimensionality Reduction and Classification Feature Using Mutual Information Applied to Hyperspectral Images: A Wrapper Strategy Algorithm Based on Minimizing the Error Probability Using the Inequality of Fano |  http://arxiv.org/abs/1211.0055  | author:Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV published:2012-10-31 summary:In the feature classification domain, the choice of data affects widely theresults. For the Hyperspectral image, the bands dont all contain theinformation; some bands are irrelevant like those affected by variousatmospheric effects, see Figure.4, and decrease the classification accuracy.And there exist redundant bands to complicate the learning system and productincorrect prediction [14]. Even the bands contain enough information about thescene they may can't predict the classes correctly if the dimension of spaceimages, see Figure.3, is so large that needs many cases to detect therelationship between the bands and the scene (Hughes phenomenon) [10]. We canreduce the dimensionality of hyperspectral images by selecting only therelevant bands (feature selection or subset selection methodology), orextracting, from the original bands, new bands containing the maximalinformation about the classes, using any functions, logical or numerical(feature extraction methodology) [11][9]. Here we focus on the featureselection using mutual information. Hyperspectral images have three advantagesregarding the multispectral images [6],
arxiv-1210-8442 | Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines |  http://arxiv.org/abs/1210.8442  | author:Louis Yuanlong Shao category:cs.AI cs.NE q-bio.NC stat.ML published:2012-10-31 summary:One conjecture in both deep learning and classical connectionist viewpoint isthat the biological brain implements certain kinds of deep networks as itsback-end. However, to our knowledge, a detailed correspondence has not yet beenset up, which is important if we want to bridge between neuroscience andmachine learning. Recent researches emphasized the biological plausibility ofLinear-Nonlinear-Poisson (LNP) neuron model. We show that with neurallyplausible settings, the whole network is capable of representing any Boltzmannmachine and performing a semi-stochastic Bayesian inference algorithm lyingbetween Gibbs sampling and variational inference.
arxiv-1211-0056 | Iterative Hard Thresholding Methods for $l_0$ Regularized Convex Cone Programming |  http://arxiv.org/abs/1211.0056  | author:Zhaosong Lu category:math.OC cs.LG math.NA stat.CO stat.ML published:2012-10-31 summary:In this paper we consider $l_0$ regularized convex cone programming problems.In particular, we first propose an iterative hard thresholding (IHT) method andits variant for solving $l_0$ regularized box constrained convex programming.We show that the sequence generated by these methods converges to a localminimizer. Also, we establish the iteration complexity of the IHT method forfinding an $\epsilon$-local-optimal solution. We then propose a method forsolving $l_0$ regularized convex cone programming by applying the IHT method toits quadratic penalty relaxation and establish its iteration complexity forfinding an $\epsilon$-approximate local minimizer. Finally, we propose avariant of this method in which the associated penalty parameter is dynamicallyupdated, and show that every accumulation point is a local minimizer of theproblem.
arxiv-1210-8429 | Anomaly Detection in Time Series of Graphs using Fusion of Graph Invariants |  http://arxiv.org/abs/1210.8429  | author:Youngser Park, Carey E. Priebe, Abdou Youssef category:stat.ML published:2012-10-31 summary:Given a time series of graphs G(t) = (V, E(t)), t = 1, 2, ..., where thefixed vertex set V represents "actors" and an edge between vertex u and vertexv at time t (uv \in E(t)) represents the existence of a communications eventbetween actors u and v during the tth time period, we wish to detect anomaliesand/or change points. We consider a collection of graph features, orinvariants, and demonstrate that adaptive fusion provides superior inferentialefficacy compared to naive equal weighting for a certain class of anomalydetection problems. Simulation results using a latent process model for timeseries of graphs, as well as illustrative experimental results for a timeseries of graphs derived from the Enron email data, show that a fusionstatistic can provide superior inference compared to individual invariantsalone. These results also demonstrate that an adaptive weighting scheme forfusion of invariants performs better than naive equal weighting.
arxiv-1210-8291 | Learning in the Model Space for Fault Diagnosis |  http://arxiv.org/abs/1210.8291  | author:Huanhuan Chen, Peter Tino, Xin Yao, Ali Rodan category:cs.LG cs.AI published:2012-10-31 summary:The emergence of large scaled sensor networks facilitates the collection oflarge amounts of real-time data to monitor and control complex engineeringsystems. However, in many cases the collected data may be incomplete orinconsistent, while the underlying environment may be time-varying orun-formulated. In this paper, we have developed an innovative cognitive faultdiagnosis framework that tackles the above challenges. This frameworkinvestigates fault diagnosis in the model space instead of in the signal space.Learning in the model space is implemented by fitting a series of models usinga series of signal segments selected with a rolling window. By investigatingthe learning techniques in the fitted model space, faulty models can bediscriminated from healthy models using one-class learning algorithm. Theframework enables us to construct fault library when unknown faults occur,which can be regarded as cognitive fault isolation. This paper alsotheoretically investigates how to measure the pairwise distance between twomodels in the model space and incorporates the model distance into the learningalgorithm in the model space. The results on three benchmark applications andone simulated model for the Barcelona water distribution network have confirmedthe effectiveness of the proposed framework.
arxiv-1210-8318 | Mugshot Identification from Manipulated Facial Images |  http://arxiv.org/abs/1210.8318  | author:H. R. Chennamma, Lalitha Rangarajan category:cs.CV cs.MM published:2012-10-31 summary:Editing on digital images is ubiquitous. Identification of deliberatelymodified facial images is a new challenge for face identification system. Inthis paper, we address the problem of identification of a face or person fromheavily altered facial images. In this face identification problem, the inputto the system is a manipulated or transformed face image and the system reportsback the determined identity from a database of known individuals. Such asystem can be useful in mugshot identification in which mugshot databasecontains two views (frontal and profile) of each criminal. We considered onlyfrontal view from the available database for face identification and the queryimage is a manipulated face generated by face transformation software toolavailable online. We propose SIFT features for efficient face identification inthis scenario. Further comparative analysis has been given with well knowneigenface approach. Experiments have been conducted with real case images toevaluate the performance of both methods.
arxiv-1210-8353 | Temporal Autoencoding Restricted Boltzmann Machine |  http://arxiv.org/abs/1210.8353  | author:Chris Häusler, Alex Susemihl category:stat.ML cs.AI cs.LG published:2012-10-31 summary:Much work has been done refining and characterizing the receptive fieldslearned by deep learning algorithms. A lot of this work has focused on thedevelopment of Gabor-like filters learned when enforcing sparsity constraintson a natural image dataset. Little work however has investigated how thesefilters might expand to the temporal domain, namely through training on naturalmovies. Here we investigate exactly this problem in established temporal deeplearning algorithms as well as a new learning paradigm suggested here, theTemporal Autoencoding Restricted Boltzmann Machine (TARBM).
arxiv-1211-0025 | Venn-Abers predictors |  http://arxiv.org/abs/1211.0025  | author:Vladimir Vovk, Ivan Petej category:cs.LG stat.ML 68T05, 68T10 published:2012-10-31 summary:This paper continues study, both theoretical and empirical, of the method ofVenn prediction, concentrating on binary prediction problems. Venn predictorsproduce probability-type predictions for the labels of test objects which areguaranteed to be well calibrated under the standard assumption that theobservations are generated independently from the same distribution. We give asimple formalization and proof of this property. We also introduce Venn-Aberspredictors, a new class of Venn predictors based on the idea of isotonicregression, and report promising empirical results both for Venn-Aberspredictors and for their more computationally efficient simplified version.
arxiv-1211-0053 | The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains |  http://arxiv.org/abs/1211.0053  | author:David I Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, Pierre Vandergheynst category:cs.DM cs.LG cs.SI published:2012-10-31 summary:In applications such as social, energy, transportation, sensor, and neuronalnetworks, high-dimensional data naturally reside on the vertices of weightedgraphs. The emerging field of signal processing on graphs merges algebraic andspectral graph theoretic concepts with computational harmonic analysis toprocess such signals on graphs. In this tutorial overview, we outline the mainchallenges of the area, discuss different ways to define graph spectraldomains, which are the analogues to the classical frequency domain, andhighlight the importance of incorporating the irregular structures of graphdata domains when processing signals on graphs. We then review methods togeneralize fundamental operations such as filtering, translation, modulation,dilation, and downsampling to the graph setting, and survey the localized,multiscale transforms that have been proposed to efficiently extractinformation from high-dimensional data on graphs. We conclude with a briefdiscussion of open issues and possible extensions.
arxiv-1210-8124 | Hierarchical Learning Algorithm for the Beta Basis Function Neural Network |  http://arxiv.org/abs/1210.8124  | author:Habib Dhahri, Mohamed Adel Alimi category:cs.NE cs.AI published:2012-10-30 summary:The paper presents a two-level learning method for the design of the BetaBasis Function Neural Network BBFNN. A Genetic Algorithm is employed at theupper level to construct BBFNN, while the key learning parameters :the width,the centers and the Beta form are optimised using the gradient algorithm at thelower level. In order to demonstrate the effectiveness of this hierarchicallearning algorithm HLABBFNN, we need to validate our algorithm for theapproximation of non-linear function.
arxiv-1210-7956 | Implementation of a Vision System for a Landmine Detecting Robot Using Artificial Neural Network |  http://arxiv.org/abs/1210.7956  | author:Roger Achkar, Michel Owayjan category:cs.NE cs.CV 68T45 I.2.6; I.5.1 published:2012-10-30 summary:Landmines, specifically anti-tank mines, cluster bombs, and unexplodedordnance form a serious problem in many countries. Several landmine sweepingtechniques are used for minesweeping. This paper presents the design and theimplementation of the vision system of an autonomous robot for landmineslocalization. The proposed work develops state-of-the-art techniques in digitalimage processing for pre-processing captured images of the contaminated area.After enhancement, Artificial Neural Network (ANN) is used in order toidentify, recognize and classify the landmines' make and model. TheBack-Propagation algorithm is used for training the network. The proposed workproved to be able to identify and classify different types of landmines undervarious conditions (rotated landmine, partially covered landmine) with asuccess rate of up to 90%.
arxiv-1210-7917 | The Model of Semantic Concepts Lattice For Data Mining Of Microblogs |  http://arxiv.org/abs/1210.7917  | author:Bohdan Pavlyshenko category:cs.CL cs.IR published:2012-10-30 summary:The model of semantic concept lattice for data mining of microblogs has beenproposed in this work. It is shown that the use of this model is effective forthe semantic relations analysis and for the detection of associative rules ofkey words.
arxiv-1210-7665 | Graph Estimation From Multi-attribute Data |  http://arxiv.org/abs/1210.7665  | author:Mladen Kolar, Han Liu, Eric P. Xing category:stat.ML published:2012-10-29 summary:Many real world network problems often concern multivariate nodal attributessuch as image, textual, and multi-view feature vectors on nodes, rather thansimple univariate nodal attributes. The existing graph estimation methods builton Gaussian graphical models and covariance selection algorithms can not handlesuch data, neither can the theories developed around such methods be directlyapplied. In this paper, we propose a new principled framework for estimatinggraphs from multi-attribute data. Instead of estimating the partial correlationas in current literature, our method estimates the partial canonicalcorrelations that naturally accommodate complex nodal features.Computationally, we provide an efficient algorithm which utilizes themulti-attribute structure. Theoretically, we provide sufficient conditionswhich guarantee consistent graph recovery. Extensive simulation studiesdemonstrate performance of our method under various conditions. Furthermore, weprovide illustrative applications to uncovering gene regulatory networks fromgene and protein profiles, and uncovering brain connectivity graph fromfunctional magnetic resonance imaging data.
arxiv-1210-7657 | Text Classification with Compression Algorithms |  http://arxiv.org/abs/1210.7657  | author:Antonio Giuliano Zippo category:cs.LG published:2012-10-29 summary:This work concerns a comparison of SVM kernel methods in text categorizationtasks. In particular I define a kernel function that estimates the similaritybetween two objects computing by their compressed lengths. In fact, compressionalgorithms can detect arbitrarily long dependencies within the text strings.Data text vectorization looses information in feature extractions and is highlysensitive by textual language. Furthermore, these methods are languageindependent and require no text preprocessing. Moreover, the accuracy computedon the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater thanGaussian, linear and polynomial kernels. The method limits are represented bycomputational time complexity of the Gram matrix and by very poor performanceon non-textual datasets.
arxiv-1210-7669 | Performance Evaluation of Different Techniques for texture Classification |  http://arxiv.org/abs/1210.7669  | author:Pooja Maknikar category:cs.CV published:2012-10-29 summary:Texture is the term used to characterize the surface of a given object orphenomenon and is an important feature used in image processing and patternrecognition. Our aim is to compare various Texture analyzing methods andcompare the results based on time complexity and accuracy of classification.The project describes texture classification using Wavelet Transform and Cooccurrence Matrix. Comparison of features of a sample texture with database ofdifferent textures is performed. In wavelet transform we use the Haar, Symletsand Daubechies wavelets. We find that, thee Haar wavelet proves to be the mostefficient method in terms of performance assessment parameters mentioned above.Comparison of Haar wavelet and Co-occurrence matrix method of classificationalso goes in the favor of Haar. Though the time requirement is high in thelater method, it gives excellent results for classification accuracy except ifthe image is rotated.
arxiv-1210-7631 | The fortresses of Ejin: an example of outlining a site from satellite images |  http://arxiv.org/abs/1210.7631  | author:Amelia Carolina Sparavigna category:cs.CV published:2012-10-29 summary:From 1960's to 1970's, the Chinese Army built some fortified artificialhills. Some of them are located in the Inner Mongolia, Western China. Theselarge fortresses are surrounded by moats. For some of them it is still possibleto see earthworks, trenches and ditches, the planning of which could have asymbolic meaning. We can argue this result form their digital outlining,obtained after an image processing of satellite images, based on edgedetection.
arxiv-1210-7599 | The automatic creation of concept maps from documents written using morphologically rich languages |  http://arxiv.org/abs/1210.7599  | author:Krunoslav Zubrinic, Damir Kalpic, Mario Milicevic category:cs.IR cs.AI cs.CL published:2012-10-29 summary:Concept map is a graphical tool for representing knowledge. They have beenused in many different areas, including education, knowledge management,business and intelligence. Constructing of concept maps manually can be acomplex task; an unskilled person may encounter difficulties in determining andpositioning concepts relevant to the problem area. An application thatrecommends concept candidates and their position in a concept map cansignificantly help the user in that situation. This paper gives an overview ofdifferent approaches to automatic and semi-automatic creation of concept mapsfrom textual and non-textual sources. The concept map mining process isdefined, and one method suitable for the creation of concept maps fromunstructured textual sources in highly inflected languages such as the Croatianlanguage is described in detail. Proposed method uses statistical and datamining techniques enriched with linguistic tools. With minor adjustments, thatmethod can also be used for concept map mining from textual sources in othermorphologically rich languages.
arxiv-1210-7559 | Tensor decompositions for learning latent variable models |  http://arxiv.org/abs/1210.7559  | author:Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, Matus Telgarsky category:cs.LG math.NA stat.ML published:2012-10-29 summary:This work considers a computationally and statistically efficient parameterestimation method for a wide class of latent variable models---includingGaussian mixture models, hidden Markov models, and latent Dirichletallocation---which exploits a certain tensor structure in their low-orderobservable moments (typically, of second- and third-order). Specifically,parameter estimation is reduced to the problem of extracting a certain(orthogonal) decomposition of a symmetric tensor derived from the moments; thisdecomposition can be viewed as a natural generalization of the singular valuedecomposition for matrices. Although tensor decompositions are generallyintractable to compute, the decomposition of these specially structured tensorscan be efficiently obtained by a variety of approaches, including poweriterations and maximization approaches (similar to the case of matrices). Adetailed analysis of a robust tensor power method is provided, establishing ananalogue of Wedin's perturbation theorem for the singular vectors of matrices.This implies a robust and computationally tractable estimation approach forseveral popular latent variable models.
arxiv-1210-7403 | Resolution Enhancement of Range Images via Color-Image Segmentation |  http://arxiv.org/abs/1210.7403  | author:Arnav Bhavsar category:cs.CV published:2012-10-28 summary:We report a method for super-resolution of range images. Our approachleverages the interpretation of LR image as sparse samples on the HR grid.Based on this interpretation, we demonstrate that our recently reportedapproach, which reconstructs dense range images from sparse range data byexploiting a registered colour image, can be applied for the task of resolutionenhancement of range images. Our method only uses a single colour image inaddition to the range observation in the super-resolution process. Using theproposed approach, we demonstrate super-resolution results for large factors(e.g. 4) with good localization accuracy.
arxiv-1210-7461 | Recognizing Static Signs from the Brazilian Sign Language: Comparing Large-Margin Decision Directed Acyclic Graphs, Voting Support Vector Machines and Artificial Neural Networks |  http://arxiv.org/abs/1210.7461  | author:César Roberto de Souza, Ednaldo Brigante Pizzolato, Mauro dos Santos Anjo category:cs.CV cs.LG stat.ML published:2012-10-28 summary:In this paper, we explore and detail our experiments in ahigh-dimensionality, multi-class image classification problem often found inthe automatic recognition of Sign Languages. Here, our efforts are directedtowards comparing the characteristics, advantages and drawbacks of creating andtraining Support Vector Machines disposed in a Directed Acyclic Graph andArtificial Neural Networks to classify signs from the Brazilian Sign Language(LIBRAS). We explore how the different heuristics, hyperparameters andmulti-class decision schemes affect the performance, efficiency and ease of usefor each classifier. We provide hyperparameter surface maps capturing accuracyand efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects ofheuristics when training ANNs with Resilient Backpropagation. We reportstatistically significant results using Cohen's Kappa statistic for contingencytables.
arxiv-1210-7477 | Parallel MCMC with Generalized Elliptical Slice Sampling |  http://arxiv.org/abs/1210.7477  | author:Robert Nishihara, Iain Murray, Ryan P. Adams category:stat.CO stat.ML published:2012-10-28 summary:Probabilistic models are conceptually powerful tools for finding structure indata, but their practical effectiveness is often limited by our ability toperform inference in them. Exact inference is frequently intractable, soapproximate inference is often performed using Markov chain Monte Carlo (MCMC).To achieve the best possible results from MCMC, we want to efficiently simulatemany steps of a rapidly mixing Markov chain which leaves the targetdistribution invariant. Of particular interest in this regard is how to takeadvantage of multi-core computing to speed up MCMC-based inference, both toimprove mixing and to distribute the computational load. In this paper, wepresent a parallelizable Markov chain Monte Carlo algorithm for efficientlysampling from continuous probability distributions that can take advantage ofhundreds of cores. This method shares information between parallel Markovchains to build a scale-mixture of Gaussians approximation to the densityfunction of the target distribution. We combine this approximation with arecent method known as elliptical slice sampling to create a Markov chain withno step-size parameters that can mix rapidly without requiring gradient orcurvature computations.
arxiv-1210-7282 | The Hangulphabet: A Descriptive Alphabet |  http://arxiv.org/abs/1210.7282  | author:Robert Bishop, Ruggero Micheletto category:cs.CL published:2012-10-27 summary:This paper describes the Hangulphabet, a new writing system that should proveuseful in a number of contexts. Using the Hangulphabet, a user can instantlysee voicing, manner and place of articulation of any phoneme found in humanlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axisrepresenting the place of articulation and the y-axis representing manner ofarticulation. Each individual grapheme contains radicals from both axes wherethe points intersect. The top radical represents manner of articulation wherethe bottom represents place of articulation. A horizontal line running throughthe middle of the bottom radical represents voicing. For vowels, place ofarticulation is located on a grid that represents the position of the tongue inthe mouth. This grid is similar to that of the IPA vowel chart (InternationalPhonetic Association, 1999). The difference with the Hangulphabet being thetrapezoid representing the vocal apparatus is on a slight tilt. Place ofarticulation for a vowel is represented by a breakout figure from the grid.This system can be used as an alternative to the International PhoneticAlphabet (IPA) or as a complement to it. Beginning students of linguistics mayfind it particularly useful. A Hangulphabet font has been created to facilitateswitching between the Hangulphabet and the IPA.
arxiv-1210-7362 | Discrete Energy Minimization, beyond Submodularity: Applications and Approximations |  http://arxiv.org/abs/1210.7362  | author:Shai Bagon category:cs.CV cs.LG math.OC stat.ML published:2012-10-27 summary:In this thesis I explore challenging discrete energy minimization problemsthat arise mainly in the context of computer vision tasks. This work motivatesthe use of such "hard-to-optimize" non-submodular functionals, and proposesmethods and algorithms to cope with the NP-hardness of their optimization.Consequently, this thesis revolves around two axes: applications andapproximations. The applications axis motivates the use of such"hard-to-optimize" energies by introducing new tasks. As the energies becomeless constrained and structured one gains more expressive power for theobjective function achieving more accurate models. Results show howchallenging, hard-to-optimize, energies are more adequate for certain computervision applications. To overcome the resulting challenging optimization tasksthe second axis of this thesis proposes approximation algorithms to cope withthe NP-hardness of the optimization. Experiments show that these new methodsyield good results for representative challenging problems.
arxiv-1210-7137 | Alberti's letter counts |  http://arxiv.org/abs/1210.7137  | author:Bernard Ycart category:math.HO cs.CL published:2012-10-26 summary:Four centuries before modern statistical linguistics was born, Leon BattistaAlberti (1404--1472) compared the frequency of vowels in Latin poems andorations, making the first quantified observation of a stylistic differenceever. Using a corpus of 20 Latin texts (over 5 million letters), Alberti'sobservations are statistically assessed. Letter counts prove that poets usedsignificantly more a's, e's, and y's, whereas orators used more of the othervowels. The sample sizes needed to justify the assertions are studied, andproved to be within reach for Alberti's scholarship.
arxiv-1210-7102 | 3D Face Recognition using Significant Point based SULD Descriptor |  http://arxiv.org/abs/1210.7102  | author:B. H. Shekar, N. Harivinod, M. Sharmila Kumari, K. Raghurama Holla category:cs.CV published:2012-10-26 summary:In this work, we present a new 3D face recognition method based on Speeded-UpLocal Descriptor (SULD) of significant points extracted from the range imagesof faces. The proposed model consists of a method for extracting distinctiveinvariant features from range images of faces that can be used to performreliable matching between different poses of range images of faces. For a given3D face scan, range images are computed and the potential interest points areidentified by searching at all scales. Based on the stability of the interestpoint, significant points are extracted. For each significant point we computethe SULD descriptor which consists of vector made of values from the convolvedHaar wavelet responses located on concentric circles centred on the significantpoint, and where the amount of Gaussian smoothing is proportional to the radiiof the circles. Experimental results show that the newly proposed methodprovides higher recognition rate compared to other existing contemporary modelsdeveloped for 3D face recognition.
arxiv-1210-7070 | A Multiscale Framework for Challenging Discrete Optimization |  http://arxiv.org/abs/1210.7070  | author:Shai Bagon, Meirav Galun category:cs.CV cs.LG math.OC stat.ML published:2012-10-26 summary:Current state-of-the-art discrete optimization methods struggle behind whenit comes to challenging contrast-enhancing discrete energies (i.e., favoringdifferent labels for neighboring variables). This work suggests a multiscaleapproach for these challenging problems. Deriving an algebraic representationallows us to coarsen any pair-wise energy using any interpolation in aprincipled algebraic manner. Furthermore, we propose an energy-awareinterpolation operator that efficiently exposes the multiscale landscape of theenergy yielding an effective coarse-to-fine optimization scheme. Results onchallenging contrast-enhancing energies show significant improvement overstate-of-the-art methods.
arxiv-1210-7038 | Full Object Boundary Detection by Applying Scale Invariant Features in a Region Merging Segmentation Algorithm |  http://arxiv.org/abs/1210.7038  | author:Reza Oji, Farshad Tajeripour category:cs.CV cs.AI published:2012-10-26 summary:Object detection is a fundamental task in computer vision and has manyapplications in image processing. This paper proposes a new approach for objectdetection by applying scale invariant feature transform (SIFT) in an automaticsegmentation algorithm. SIFT is an invariant algorithm respect to scale,translation and rotation. The features are very distinct and provide stablekeypoints that can be used for matching an object in different images. Atfirst, an object is trained with different aspects for finding best keypoints.The object can be recognized in the other images by using achieved keypoints.Then, a robust segmentation algorithm is used to detect the object with fullboundary based on SIFT keypoints. In segmentation algorithm, a merging role isdefined to merge the regions in image with the assistance of keypoints. Theresults show that the proposed approach is reliable for object detection andcan extract object boundary well.
arxiv-1210-7056 | Selective Transfer Learning for Cross Domain Recommendation |  http://arxiv.org/abs/1210.7056  | author:Zhongqi Lu, Erheng Zhong, Lili Zhao, Wei Xiang, Weike Pan, Qiang Yang category:cs.LG cs.IR stat.ML published:2012-10-26 summary:Collaborative filtering (CF) aims to predict users' ratings on itemsaccording to historical user-item preference data. In many real-worldapplications, preference data are usually sparse, which would make modelsoverfit and fail to give accurate predictions. Recently, several research worksshow that by transferring knowledge from some manually selected source domains,the data sparseness problem could be mitigated. However for most cases, partsof source domain data are not consistent with the observations in the targetdomain, which may misguide the target domain model building. In this paper, wepropose a novel criterion based on empirical prediction error and its varianceto better capture the consistency across domains in CF settings. Consequently,we embed this criterion into a boosting framework to perform selectiveknowledge transfer. Comparing to several state-of-the-art methods, we show thatour proposed selective transfer learning framework can significantly improvethe accuracy of rating prediction tasks on several real-world recommendationtasks.
arxiv-1210-7054 | Large-Scale Sparse Principal Component Analysis with Application to Text Data |  http://arxiv.org/abs/1210.7054  | author:Youwei Zhang, Laurent El Ghaoui category:stat.ML cs.LG math.OC published:2012-10-26 summary:Sparse PCA provides a linear combination of small number of features thatmaximizes variance across data. Although Sparse PCA has apparent advantagescompared to PCA, such as better interpretability, it is generally thought to becomputationally much more expensive. In this paper, we demonstrate thesurprising fact that sparse PCA can be easier than PCA in practice, and that itcan be reliably applied to very large data sets. This comes from a rigorousfeature elimination pre-processing result, coupled with the favorable fact thatfeatures in real-life data typically have exponentially decreasing variances,which allows for many features to be eliminated. We introduce a fast blockcoordinate ascent algorithm with much better computational complexity than theexisting first-order ones. We provide experimental results obtained on textcorpora involving millions of documents and hundreds of thousands of features.These results illustrate how Sparse PCA can help organize a large corpus oftext data in a user-interpretable way, providing an attractive alternativeapproach to topic models.
arxiv-1210-7053 | Managing sparsity, time, and quality of inference in topic models |  http://arxiv.org/abs/1210.7053  | author:Khoat Than, Tu Bao Ho category:stat.ML cs.AI cs.CV stat.ME published:2012-10-26 summary:Inference is an integral part of probabilistic topic models, but is oftennon-trivial to derive an efficient algorithm for a specific model. It is evenmuch more challenging when we want to find a fast inference algorithm whichalways yields sparse latent representations of documents. In this article, weintroduce a simple framework for inference in probabilistic topic models,denoted by FW. This framework is general and flexible enough to be easilyadapted to mixture models. It has a linear convergence rate, offers an easy wayto incorporate prior knowledge, and provides us an easy way to directly tradeoff sparsity against quality and time. We demonstrate the goodness andflexibility of FW over existing inference methods by a number of tasks.Finally, we show how inference in topic models with nonconjugate priors can bedone efficiently.
arxiv-1210-7047 | User-level Weibo Recommendation incorporating Social Influence based on Semi-Supervised Algorithm |  http://arxiv.org/abs/1210.7047  | author:Daifeng Li, Zhipeng Luo, Golden Guo-zheng Sun, Jie Tang, Jingwei Zhang category:cs.SI cs.CY cs.LG published:2012-10-26 summary:Tencent Weibo, as one of the most popular micro-blogging services in China,has attracted millions of users, producing 30-60 millions of weibo (similar astweet in Twitter) daily. With the overload problem of user generate content,Tencent users find it is more and more hard to browse and find valuableinformation at the first time. In this paper, we propose a Factor Graph basedweibo recommendation algorithm TSI-WR (Topic-Level Social Influence based WeiboRecommendation), which could help Tencent users to find most suitableinformation. The main innovation is that we consider both direct and indirectsocial influence from topic level based on social balance theory. The mainadvantages of adopting this strategy are that it could first build a moreaccurate description of latent relationship between two users with weakconnections, which could help to solve the data sparsity problem; secondprovide a more accurate recommendation for a certain user from a wider range.Other meaningful contextual information is also combined into our model, whichinclude: Users profile, Users influence, Content of weibos, Topic informationof weibos and etc. We also design a semi-supervised algorithm to further reducethe influence of data sparisty. The experiments show that all the selectedvariables are important and the proposed model outperforms several baselinemethods.
arxiv-1210-6766 | Structured Sparsity Models for Multiparty Speech Recovery from Reverberant Recordings |  http://arxiv.org/abs/1210.6766  | author:Afsaneh Asaei, Mohammad Golbabaee, Hervé Bourlard, Volkan Cevher category:cs.LG cs.SD published:2012-10-25 summary:We tackle the multi-party speech recovery problem through modeling theacoustic of the reverberant chambers. Our approach exploits structured sparsitymodels to perform room modeling and speech recovery. We propose a scheme forcharacterizing the room acoustic from the unknown competing speech sourcesrelying on localization of the early images of the speakers by sparseapproximation of the spatial spectra of the virtual sources in a free-spacemodel. The images are then clustered exploiting the low-rank structure of thespectro-temporal components belonging to each source. This enables us toidentify the early support of the room impulse response function and its uniquemap to the room geometry. To further tackle the ambiguity of the reflectionratios, we propose a novel formulation of the reverberation model and estimatethe absorption coefficients through a convex optimization exploiting jointsparsity model formulated upon spatio-spectral sparsity of concurrent speechrepresentation. The acoustic parameters are then incorporated for separatingindividual speech signals through either structured sparse recovery or inversefiltering the acoustic channels. The experiments conducted on real datarecordings demonstrate the effectiveness of the proposed approach formulti-party speech recovery and recognition.
arxiv-1210-6738 | Nested Hierarchical Dirichlet Processes |  http://arxiv.org/abs/1210.6738  | author:John Paisley, Chong Wang, David M. Blei, Michael I. Jordan category:stat.ML cs.LG published:2012-10-25 summary:We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We derive a stochasticvariational inference algorithm for the model, in addition to a greedy subtreeselection method for each document, which allows for efficient inference usingmassive collections of text documents. We demonstrate our algorithm on 1.8million documents from The New York Times and 3.3 million documents fromWikipedia.
arxiv-1210-6649 | Extended object reconstruction in adaptive-optics imaging: the multiresolution approach |  http://arxiv.org/abs/1210.6649  | author:Roberto Baena Gallé, Jorge Núñez, Szymon Gladysz category:astro-ph.IM cs.CV math.NA published:2012-10-25 summary:We propose the application of multiresolution transforms, such as wavelets(WT) and curvelets (CT), to the reconstruction of images of extended objectsthat have been acquired with adaptive optics (AO) systems. Such multichannelapproaches normally make use of probabilistic tools in order to distinguishsignificant structures from noise and reconstruction residuals. Furthermore, weaim to check the historical assumption that image-reconstruction algorithmsusing static PSFs are not suitable for AO imaging. We convolve an image ofSaturn taken with the Hubble Space Telescope (HST) with AO PSFs from the 5-mHale telescope at the Palomar Observatory and add both shot and readout noise.Subsequently, we apply different approaches to the blurred and noisy data inorder to recover the original object. The approaches include multi-frame blinddeconvolution (with the algorithm IDAC), myopic deconvolution withregularization (with MISTRAL) and wavelets- or curvelets-based static PSFdeconvolution (AWMLE and ACMLE algorithms). We used the mean squared error(MSE) and the structural similarity index (SSIM) to compare the results. Wediscuss the strengths and weaknesses of the two metrics. We found that CTproduces better results than WT, as measured in terms of MSE and SSIM.Multichannel deconvolution with a static PSF produces results which aregenerally better than the results obtained with the myopic/blind approaches(for the images we tested) thus showing that the ability of a method tosuppress the noise and to track the underlying iterative process is just ascritical as the capability of the myopic/blind approaches to update the PSF.
arxiv-1210-7014 | Computer vision tools for the non-invasive assessment of autism-related behavioral markers |  http://arxiv.org/abs/1210.7014  | author:Jordan Hashemi, Thiago Vallin Spina, Mariano Tepper, Amy Esler, Vassilios Morellas, Nikolaos Papanikolopoulos, Guillermo Sapiro category:cs.CV published:2012-10-25 summary:The early detection of developmental disorders is key to child outcome,allowing interventions to be initiated that promote development and improveprognosis. Research on autism spectrum disorder (ASD) suggests behavioralmarkers can be observed late in the first year of life. Many of these studiesinvolved extensive frame-by-frame video observation and analysis of a child'snatural behavior. Although non-intrusive, these methods are extremelytime-intensive and require a high level of observer training; thus, they areimpractical for clinical and large population research purposes. Diagnosticmeasures for ASD are available for infants but are only accurate when used byspecialists experienced in early diagnosis. This work is a first milestone in along-term multidisciplinary project that aims at helping clinicians and generalpractitioners accomplish this early detection/measurement task automatically.We focus on providing computer vision tools to measure and identify ASDbehavioral markers based on components of the Autism Observation Scale forInfants (AOSI). In particular, we develop algorithms to measure three criticalAOSI activities that assess visual attention. We augment these AOSI activitieswith an additional test that analyzes asymmetrical patterns in unsupportedgait. The first set of algorithms involves assessing head motion by trackingfacial features, while the gait analysis relies on joint foregroundsegmentation and 2D body pose estimation in video. We show results that provideinsightful knowledge to augment the clinician's behavioral observationsobtained from real in-clinic assessments.
arxiv-1210-6911 | Ancestor Sampling for Particle Gibbs |  http://arxiv.org/abs/1210.6911  | author:Fredrik Lindsten, Michael I. Jordan, Thomas B. Schön category:stat.CO stat.ML published:2012-10-25 summary:We present a novel method in the family of particle MCMC methods that werefer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to theexisting PG with backward simulation (PG-BS) procedure, we use backwardsampling to (considerably) improve the mixing of the PG kernel. Instead ofusing separate forward and backward sweeps as in PG-BS, however, we achieve thesame effect in a single forward sweep. We apply the PG-AS framework to thechallenging class of non-Markovian state-space models. We develop a truncationstrategy of these models that is applicable in principle to anybackward-simulation-based method, but which is particularly well suited to thePG-AS framework. In particular, as we show in a simulation study, PG-AS canyield an order-of-magnitude improved accuracy relative to PG-BS due to itsrobustness to the truncation error. Several application examples are discussed,including Rao-Blackwellized particle smoothing and inference in degeneratestate-space models.
arxiv-1210-6912 | Enhancing the functional content of protein interaction networks |  http://arxiv.org/abs/1210.6912  | author:Gaurav Pandey, Sahil Manocha, Gowtham Atluri, Vipin Kumar category:q-bio.MN cs.CE cs.LG q-bio.GN stat.ML published:2012-10-25 summary:Protein interaction networks are a promising type of data for studyingcomplex biological systems. However, despite the rich information embedded inthese networks, they face important data quality challenges of noise andincompleteness that adversely affect the results obtained from their analysis.Here, we explore the use of the concept of common neighborhood similarity(CNS), which is a form of local structure in networks, to address these issues.Although several CNS measures have been proposed in the literature, anunderstanding of their relative efficacies for the analysis of interactionnetworks has been lacking. We follow the framework of graph transformation toconvert the given interaction network into a transformed network correspondingto a variety of CNS measures evaluated. The effectiveness of each measure isthen estimated by comparing the quality of protein function predictionsobtained from its corresponding transformed network with those from theoriginal network. Using a large set of S. cerevisiae interactions, and a set of136 GO terms, we find that several of the transformed networks produce moreaccurate predictions than those obtained from the original network. Inparticular, the $HC.cont$ measure proposed here performs particularly well forthis task. Further investigation reveals that the two major factorscontributing to this improvement are the abilities of CNS measures, especially$HC.cont$, to prune out noisy edges and introduce new links betweenfunctionally related proteins.
arxiv-1211-0191 | Performance Evaluation of Random Set Based Pedestrian Tracking Algorithms |  http://arxiv.org/abs/1211.0191  | author:Branko Ristic, Jamie Sherrah, Ángel F. García-Fernández category:cs.CV published:2012-10-25 summary:The paper evaluates the error performance of three random finite set basedmulti-object trackers in the context of pedestrian video tracking. Theevaluation is carried out using a publicly available video dataset of 4500frames (town centre street) for which the ground truth is available. The inputto all pedestrian tracking algorithms is an identical set of head and bodydetections, obtained using the Histogram of Oriented Gradients (HOG) detector.The tracking error is measured using the recently proposed OSPA metric fortracks, adopted as the only known mathematically rigorous metric for measuringthe distance between two sets of tracks. A comparative analysis is presentedunder various conditions.
arxiv-1210-6465 | Black-Box Complexity: Breaking the $O(n \log n)$ Barrier of LeadingOnes |  http://arxiv.org/abs/1210.6465  | author:Benjamin Doerr, Carola Winzen category:cs.DS cs.NE published:2012-10-24 summary:We show that the unrestricted black-box complexity of the $n$-dimensionalXOR- and permutation-invariant LeadingOnes function class is $O(n \log (n) /\log \log n)$. This shows that the recent natural looking $O(n\log n)$ bound isnot tight. The black-box optimization algorithm leading to this bound can be implementedin a way that only 3-ary unbiased variation operators are used. Hence our boundis also valid for the unbiased black-box complexity recently introduced byLehre and Witt (GECCO 2010). The bound also remains valid if we impose theadditional restriction that the black-box algorithm does not have access to theobjective values but only to their relative order (ranking-based black-boxcomplexity).
arxiv-1210-6497 | Topic-Level Opinion Influence Model(TOIM): An Investigation Using Tencent Micro-Blogging |  http://arxiv.org/abs/1210.6497  | author:Daifeng Li, Ying Ding, Xin Shuai, Golden Guo-zheng Sun, Jie Tang, Zhipeng Luo, Jingwei Zhang, Guo Zhang category:cs.SI cs.CY cs.LG published:2012-10-24 summary:Mining user opinion from Micro-Blogging has been extensively studied on themost popular social networking sites such as Twitter and Facebook in the U.S.,but few studies have been done on Micro-Blogging websites in other countries(e.g. China). In this paper, we analyze the social opinion influence onTencent, one of the largest Micro-Blogging websites in China, endeavoring tounveil the behavior patterns of Chinese Micro-Blogging users. This paperproposes a Topic-Level Opinion Influence Model (TOIM) that simultaneouslyincorporates topic factor and social direct influence in a unifiedprobabilistic framework. Based on TOIM, two topic level opinion influencepropagation and aggregation algorithms are developed to consider the indirectinfluence: CP (Conservative Propagation) and NCP (None ConservativePropagation). Users' historical social interaction records are leveraged byTOIM to construct their progressive opinions and neighbors' opinion influencethrough a statistical learning process, which can be further utilized topredict users' future opinions on some specific topics. To evaluate and testthis proposed model, an experiment was designed and a sub-dataset from TencentMicro-Blogging was used. The experimental results show that TOIM outperformsbaseline methods on predicting users' opinion. The applications of CP and NCPhave no significant differences and could significantly improve recall andF1-measure of TOIM.
arxiv-1210-6891 | Predicting Near-Future Churners and Win-Backs in the Telecommunications Industry |  http://arxiv.org/abs/1210.6891  | author:Clifton Phua, Hong Cao, João Bártolo Gomes, Minh Nhut Nguyen category:cs.CE cs.LG published:2012-10-24 summary:In this work, we presented the strategies and techniques that we havedeveloped for predicting the near-future churners and win-backs for a telecomcompany. On a large-scale and real-world database containing customer profilesand some transaction data from a telecom company, we first analyzed the dataschema, developed feature computation strategies and then extracted a large setof relevant features that can be associated with the customer churning andreturning behaviors. Our features include both the original driver factors aswell as some derived features. We evaluated our features on the imbalancecorrected dataset, i.e. under-sampled dataset and compare a large number ofexisting machine learning tools, especially decision tree-based classifiers,for predicting the churners and win-backs. In general, we find RandomForest andSimpleCart learning algorithms generally perform well and tend to provide uswith highly competitive prediction performance. Among the top-15 driver factorsthat signal the churn behavior, we find that the service utilization, e.g. lasttwo months' download and upload volume, last three months' average upload anddownload, and the payment related factors are the most indicative features forpredicting if churn will happen soon. Such features can collectively telldiscrepancies between the service plans, payments and the dynamically changingutilization needs of the customers. Our proposed features and theircomputational strategy exhibit reasonable precision performance to predictchurn behavior in near future.
arxiv-1210-6707 | Clustering hidden Markov models with variational HEM |  http://arxiv.org/abs/1210.6707  | author:Emanuele Coviello, Antoni B. Chan, Gert R. G. Lanckriet category:cs.LG cs.CV stat.ML published:2012-10-24 summary:The hidden Markov model (HMM) is a widely-used generative model that copeswith sequential data, assuming that each observation is conditioned on thestate of a hidden Markov chain. In this paper, we derive a novel algorithm tocluster HMMs based on the hierarchical EM (HEM) algorithm. The proposedalgorithm i) clusters a given collection of HMMs into groups of HMMs that aresimilar, in terms of the distributions they represent, and ii) characterizeseach group by a "cluster center", i.e., a novel HMM that is representative forthe group, in a manner that is consistent with the underlying generative modelof the HMM. To cope with intractable inference in the E-step, the HEM algorithmis formulated as a variational optimization problem, and efficiently solved forthe HMM case by leveraging an appropriate variational approximation. Thebenefits of the proposed algorithm, which we call variational HEM (VHEM), aredemonstrated on several tasks involving time-series data, such as hierarchicalclustering of motion capture sequences, and automatic annotation and retrievalof music and of online hand-writing data, showing improvements over currentmethods. In particular, our variational HEM algorithm effectively leverageslarge amounts of data when learning annotation models by using an efficienthierarchical estimation procedure, which reduces learning times and memoryrequirements, while improving model robustness through better regularization.
arxiv-1210-6511 | Neural Networks for Complex Data |  http://arxiv.org/abs/1210.6511  | author:Marie Cottrell, Madalina Olteanu, Fabrice Rossi, Joseph Rynkiewicz, Nathalie Villa-Vialaneix category:cs.NE cs.LG stat.ML published:2012-10-24 summary:Artificial neural networks are simple and efficient machine learning tools.Defined originally in the traditional setting of simple vector data, neuralnetwork models have evolved to address more and more difficulties of complexreal world problems, ranging from time evolving data to sophisticated datastructures such as graphs and functions. This paper summarizes advances onthose themes from the last decade, with a focus on results obtained by membersof the SAMM team of Universit\'e Paris 1
arxiv-1210-6539 | Towards Swarm Calculus: Urn Models of Collective Decisions and Universal Properties of Swarm Performance |  http://arxiv.org/abs/1210.6539  | author:Heiko Hamann category:cs.NE cs.AI published:2012-10-24 summary:Methods of general applicability are searched for in swarm intelligence withthe aim of gaining new insights about natural swarms and to develop designmethodologies for artificial swarms. An ideal solution could be a `swarmcalculus' that allows to calculate key features of swarms such as expectedswarm performance and robustness based on only a few parameters. To worktowards this ideal, one needs to find methods and models with high degrees ofgenerality. In this paper, we report two models that might be examples ofexceptional generality. First, an abstract model is presented that describesswarm performance depending on swarm density based on the dichotomy betweencooperation and interference. Typical swarm experiments are given as examplesto show how the model fits to several different results. Second, we give anabstract model of collective decision making that is inspired by urn models.The effects of positive feedback probability, that is increasing over time in adecision making system, are understood by the help of a parameter that controlsthe feedback based on the swarm's current consensus. Several applicablemethods, such as the description as Markov process, calculation of splittingprobabilities, mean first passage times, and measurements of positive feedback,are discussed and applications to artificial and natural swarms are reported.
arxiv-1210-6119 | Time After Time: Notes on Delays In Spiking Neural P Systems |  http://arxiv.org/abs/1210.6119  | author:Francis George C. Cabarle, Kelvin C. Buño, Henry N. Adorna category:cs.NE cs.DC cs.ET 97P20 F.1 published:2012-10-23 summary:Spiking Neural P systems, SNP systems for short, are biologically inspiredcomputing devices based on how neurons perform computations. SNP systems useonly one type of symbol, the spike, in the computations. Information is encodedin the time differences of spikes or the multiplicity of spikes produced atcertain times. SNP systems with delays (associated with rules) and thosewithout delays are two of several Turing complete SNP system variants inliterature. In this work we investigate how restricted forms of SNP systemswith delays can be simulated by SNP systems without delays. We show thesimulations for the following spike routing constructs: sequential, iteration,join, and split.
arxiv-1210-6287 | Fast Exact Max-Kernel Search |  http://arxiv.org/abs/1210.6287  | author:Ryan R. Curtin, Parikshit Ram, Alexander G. Gray category:cs.DS cs.IR cs.LG published:2012-10-23 summary:The wide applicability of kernels makes the problem of max-kernel searchubiquitous and more general than the usual similarity search in metric spaces.We focus on solving this problem efficiently. We begin by characterizing theinherent hardness of the max-kernel search problem with a novel notion ofdirectional concentration. Following that, we present a method to use an $O(n\log n)$ algorithm to index any set of objects (points in $\Real^\dims$ orabstract objects) directly in the Hilbert space without any explicit featurerepresentations of the objects in this space. We present the first provably$O(\log n)$ algorithm for exact max-kernel search using this index. Empiricalresults for a variety of data sets as well as abstract objects demonstrate upto 4 orders of magnitude speedup in some cases. Extensions for approximatemax-kernel search are also presented.
arxiv-1210-6321 | High quality topic extraction from business news explains abnormal financial market volatility |  http://arxiv.org/abs/1210.6321  | author:Ryohei Hisano, Didier Sornette, Takayuki Mizuno, Takaaki Ohnishi, Tsutomu Watanabe category:stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST published:2012-10-23 summary:Understanding the mutual relationships between information flows and socialactivity in society today is one of the cornerstones of the social sciences. Infinancial economics, the key issue in this regard is understanding andquantifying how news of all possible types (geopolitical, environmental,social, financial, economic, etc.) affect trading and the pricing of firms inorganized stock markets. In this article, we seek to address this issue byperforming an analysis of more than 24 million news records provided byThompson Reuters and of their relationship with trading activity for 206 majorstocks in the S&P US stock index. We show that the whole landscape of news thataffect stock price movements can be automatically summarized via simpleregularized regressions between trading activity and news information piecesdecomposed, with the help of simple topic modeling techniques, into their"thematic" features. Using these methods, we are able to estimate and quantifythe impacts of news on trading. We introduce network-based visualizationtechniques to represent the whole landscape of news information associated witha basket of stocks. The examination of the words that are representative of thetopic distributions confirms that our method is able to extract the significantpieces of information influencing the stock market. Our results show that oneof the most puzzling stylized fact in financial economies, namely that atcertain times trading volumes appear to be "abnormally large," can be partiallyexplained by the flow of news. In this sense, our results prove that there isno "excess trading," when restricting to times when news are genuinely noveland provide relevant financial information.
arxiv-1210-6192 | Textural Approach to Palmprint Identification |  http://arxiv.org/abs/1210.6192  | author:Rachita Misra, Kasturika B ray category:cs.CV cs.CR cs.GR published:2012-10-23 summary:Biometrics which use of human physiological characteristics for identifyingan individual is now a widespread method of identification and authentication.Biometric identification is a technology which uses several image processingtechniques and describes the general procedure for identification andverification using feature extraction, storage and matching from the digitizedimage of biometric characters such as Finger Print, Face, Iris or Palm Print.The current paper uses palm print biometrics. Here we have presented anidentification approach using textural properties of palm print images. Theelegance of the method is that the conventional edge detection technique isextended to suitably describe the texture features. In this technique all thecharacteristics of the palm such as principal lines, edges and wrinkles areconsidered with equal importance.
arxiv-1210-6292 | A density-sensitive hierarchical clustering method |  http://arxiv.org/abs/1210.6292  | author:Álvaro Martínez-Pérez category:cs.LG 62H30, 68T10 published:2012-10-23 summary:We define a hierarchical clustering method: $\alpha$-unchaining singlelinkage or $SL(\alpha)$. The input of this algorithm is a finite metric spaceand a certain parameter $\alpha$. This method is sensitive to the density ofthe distribution and offers some solution to the so called chaining effect. Wealso define a modified version, $SL^*(\alpha)$, to treat the chaining throughpoints or small blocks. We study the theoretical properties of these methodsand offer some theoretical background for the treatment of chaining effects.
arxiv-1210-6157 | Novel Architecture for 3D model in virtual communities from detected face |  http://arxiv.org/abs/1210.6157  | author:Vibekananda Dutta, Dr Nishtha Kesswani, Deepti Gahalot category:cs.CV published:2012-10-23 summary:In this research paper we suggest how to extract a face from an image, modifyit, characterize it in terms of high-level properties, and apply it to thecreation of a personalized avatar. In this research work we tested, weimplemented the algorithm on several hundred facial images, including manytaken under uncontrolled acquisition conditions, and found to exhibitsatisfactory performance for immediate practical use.
arxiv-1210-6170 | Further properties of Gaussian Reproducing Kernel Hilbert Spaces |  http://arxiv.org/abs/1210.6170  | author:Minh Ha Quang category:stat.ML math.FA 68T05, 68P30 published:2012-10-23 summary:We generalize the orthonormal basis for the Gaussian RKHS described in\cite{MinhGaussian2010} to an infinite, continuously parametrized, family oforthonormal bases, along with some implications. The proofs are directgeneralizations of those in \cite{MinhGaussian2010}.
arxiv-1210-6293 | MLPACK: A Scalable C++ Machine Learning Library |  http://arxiv.org/abs/1210.6293  | author:Ryan R. Curtin, James R. Cline, N. P. Slagle, William B. March, Parikshit Ram, Nishant A. Mehta, Alexander G. Gray category:cs.MS cs.CV cs.LG published:2012-10-23 summary:MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learninglibrary released in late 2011 offering both a simple, consistent API accessibleto novice users and high performance and flexibility to expert users byleveraging modern features of C++. MLPACK provides cutting-edge algorithmswhose benchmarks exhibit far better performance than other leading machinelearning libraries. MLPACK version 1.0.3, licensed under the LGPL, is availableat http://www.mlpack.org.
arxiv-1210-6317 | On the geometric structure of fMRI searchlight-based information maps |  http://arxiv.org/abs/1210.6317  | author:Shivakumar Viswanathan, Matthew Cieslak, Scott T. Grafton category:q-bio.NC q-bio.QM stat.AP stat.ML published:2012-10-23 summary:Information mapping is a popular application of Multivoxel Pattern Analysis(MVPA) to fMRI. Information maps are constructed using the so calledsearchlight method, where the spherical multivoxel neighborhood of every voxel(i.e., a searchlight) in the brain is evaluated for the presence oftask-relevant response patterns. Despite their widespread use, information mapspresent several challenges for interpretation. One such challenge has to dowith inferring the size and shape of a multivoxel pattern from its signature onthe information map. To address this issue, we formally examined the geometricbasis of this mapping relationship. Based on geometric considerations, we showhow and why small patterns (i.e., having smaller spatial extents) can produce alarger signature on the information map as compared to large patterns,independent of the size of the searchlight radius. Furthermore, we show thatthe number of informative searchlights over the brain increase as a function ofsearchlight radius, even in the complete absence of any multivariate responsepatterns. These properties are unrelated to the statistical capabilities of thepattern-analysis algorithms used but are obligatory geometric propertiesarising from using the searchlight procedure.
arxiv-1210-6230 | A Self-Organized Neural Comparator |  http://arxiv.org/abs/1210.6230  | author:Guillermo A. Ludueña, Claudius Gros category:q-bio.NC cs.NE published:2012-10-23 summary:Learning algorithms need generally the possibility to compare several streamsof information. Neural learning architectures hence need a unit, a comparator,able to compare several inputs encoding either internal or externalinformation, like for instance predictions and sensory readings. Without thepossibility of comparing the values of prediction to actual sensory inputs,reward evaluation and supervised learning would not be possible. Comparators are usually not implemented explicitly, necessary comparisons arecommonly performed by directly comparing one-to-one the respective activities.This implies that the characteristics of the two input streams (like size andencoding) must be provided at the time of designing the system. It is however plausible that biological comparators emerge fromself-organizing, genetically encoded principles, which allow the system toadapt to the changes in the input and in the organism. We propose an unsupervised neural circuitry, where the function of inputcomparison emerges via self-organization only from the interaction of thesystem with the respective inputs, without external influence or supervision. The proposed neural comparator adapts, unsupervised, according to thecorrelations present in the input streams. The system consists of a multilayerfeed-forward neural network which follows a local output minimization(anti-Hebbian) rule for adaptation of the synaptic weights. The local output minimization allows the circuit to autonomously acquire thecapability of comparing the neural activities received from different neuralpopulations, which may differ in the size of the population and in the neuralencoding used. The comparator is able to compare objects never encounteredbefore in the sensory input streams and to evaluate a measure of theirsimilarity, even when differently encoded.
arxiv-1210-5992 | Strong oracle optimality of folded concave penalized estimation |  http://arxiv.org/abs/1210.5992  | author:Jianqing Fan, Lingzhou Xue, Hui Zou category:math.ST stat.CO stat.ML stat.TH published:2012-10-22 summary:Folded concave penalization methods have been shown to enjoy the strongoracle property for high-dimensional sparse estimation. However, a foldedconcave penalization problem usually has multiple local solutions and theoracle property is established only for one of the unknown local solutions. Achallenging fundamental issue still remains that it is not clear whether thelocal optimum computed by a given optimization algorithm possesses those nicetheoretical properties. To close this important theoretical gap in over adecade, we provide a unified theory to show explicitly how to obtain the oraclesolution via the local linear approximation algorithm. For a folded concavepenalized estimation problem, we show that as long as the problem islocalizable and the oracle estimator is well behaved, we can obtain the oracleestimator by using the one-step local linear approximation. In addition, oncethe oracle estimator is obtained, the local linear approximation algorithmconverges, namely it produces the same estimator in the next iteration. Thegeneral theory is demonstrated by using four classical sparse estimationproblems, that is, sparse linear regression, sparse logistic regression, sparseprecision matrix estimation and sparse quantile regression.
arxiv-1210-5806 | Multi-Stage Multi-Task Feature Learning |  http://arxiv.org/abs/1210.5806  | author:Pinghua Gong, Jieping Ye, Changshui Zhang category:stat.ML published:2012-10-22 summary:Multi-task sparse feature learning aims to improve the generalizationperformance by exploiting the shared features among tasks. It has beensuccessfully applied to many applications including computer vision andbiomedical informatics. Most of the existing multi-task sparse feature learningalgorithms are formulated as a convex sparse regularization problem, which isusually suboptimal, due to its looseness for approximating an $\ell_0$-typeregularizer. In this paper, we propose a non-convex formulation for multi-tasksparse feature learning based on a novel non-convex regularizer. To solve thenon-convex optimization problem, we propose a Multi-Stage Multi-Task FeatureLearning (MSMTFL) algorithm; we also provide intuitive interpretations,detailed convergence and reproducibility analysis for the proposed algorithm.Moreover, we present a detailed theoretical analysis showing that MSMTFLachieves a better parameter estimation error bound than the convex formulation.Empirical studies on both synthetic and real-world data sets demonstrate theeffectiveness of MSMTFL in comparison with the state of the art multi-tasksparse feature learning algorithms.
arxiv-1210-6082 | Interplay: Dispersed Activation in Neural Networks |  http://arxiv.org/abs/1210.6082  | author:Richard L. Churchill category:cs.NE q-bio.NC published:2012-10-22 summary:This paper presents a multi-point stimulation of a Hebbian neural networkwith investigation of the interplay between the stimulus waves through theneurons of the network. Equilibrium of the resulting memory is achieved forrecall of specific memory data at a rate faster than single point stimulus. Theinterplay of the intersecting stimuli appears to parallel the clarificationprocess of recall in biological systems.
arxiv-1210-5830 | Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation |  http://arxiv.org/abs/1210.5830  | author:Sylvain Arlot, Matthieu Lerasle category:math.ST cs.LG stat.TH published:2012-10-22 summary:This paper studies V-fold cross-validation for model selection inleast-squares density estimation. The goal is to provide theoretical groundsfor choosing V in order to minimize the least-squares loss of the selectedestimator. We first prove a non-asymptotic oracle inequality for V-foldcross-validation and its bias-corrected version (V-fold penalization). Inparticular, this result implies that V-fold penalization is asymptoticallyoptimal in the nonparametric case. Then, we compute the variance of V-foldcross-validation and related criteria, as well as the variance of keyquantities for model selection performance. We show that these variances dependon V like 1+4/(V-1), at least in some particular cases, suggesting that theperformance increases much from V=2 to V=5 or 10, and then is almost constant.Overall, this can explain the common advice to take V=5---at least in oursetting and when the computational power is limited---, as supported by somesimulation experiments. An oracle inequality and exact formulas for thevariance are also proved for Monte-Carlo cross-validation, also known asrepeated cross-validation, where the parameter V is replaced by the number B ofrandom splits of the data.
arxiv-1210-6001 | Reducing statistical time-series problems to binary classification |  http://arxiv.org/abs/1210.6001  | author:Daniil Ryabko, Jérémie Mary category:cs.LG stat.ML published:2012-10-22 summary:We show how binary classification methods developed to work on i.i.d. datacan be used for solving statistical problems that are seemingly unrelated toclassification and concern highly-dependent time series. Specifically, theproblems of time-series clustering, homogeneity testing and the three-sampleproblem are addressed. The algorithms that we construct for solving theseproblems are based on a new metric between time-series distributions, which canbe evaluated using binary classification methods. Universal consistency of theproposed algorithms is proven under most general assumptions. The theoreticalresults are illustrated with experiments on synthetic and real-world data.
arxiv-1210-5965 | Classification Analysis Of Authorship Fiction Texts in The Space Of Semantic Fields |  http://arxiv.org/abs/1210.5965  | author:Bohdan Pavlyshenko category:cs.CL published:2012-10-22 summary:The use of naive Bayesian classifier (NB) and the classifier by the k nearestneighbors (kNN) in classification semantic analysis of authors' texts ofEnglish fiction has been analysed. The authors' works are considered in thevector space the basis of which is formed by the frequency characteristics ofsemantic fields of nouns and verbs. Highly precise classification of authors'texts in the vector space of semantic fields indicates about the presence ofparticular spheres of author's idiolect in this space which characterizes theindividual author's style.
arxiv-1210-5898 | Some Chances and Challenges in Applying Language Technologies to Historical Studies in Chinese |  http://arxiv.org/abs/1210.5898  | author:Chao-Lin Liu, Guantao Jin, Qingfeng Liu, Wei-Yun Chiu, Yih-Soong Yu category:cs.CL cs.DL cs.IR published:2012-10-22 summary:We report applications of language technology to analyzing historicaldocuments in the Database for the Study of Modern Chinese Thoughts andLiterature (DSMCTL). We studied two historical issues with the reportedtechniques: the conceptualization of "huaren" (Chinese people) and the attemptto institute constitutional monarchy in the late Qing dynasty. We also discussresearch challenges for supporting sophisticated issues using our experiencewith DSMCTL, the Database of Government Officials of the Republic of China, andthe Dream of the Red Chamber. Advanced techniques and tools for lexical,syntactic, semantic, and pragmatic processing of language information, alongwith more thorough data collection, are needed to strengthen the collaborationbetween historians and computer scientists.
arxiv-1210-5873 | Initialization of Self-Organizing Maps: Principal Components Versus Random Initialization. A Case Study |  http://arxiv.org/abs/1210.5873  | author:A. A. Akinduko, E. M. Mirkes category:stat.ML cs.LG published:2012-10-22 summary:The performance of the Self-Organizing Map (SOM) algorithm is dependent onthe initial weights of the map. The different initialization methods canbroadly be classified into random and data analysis based initializationapproach. In this paper, the performance of random initialization (RI) approachis compared to that of principal component initialization (PCI) in which theinitial map weights are chosen from the space of the principal component.Performance is evaluated by the fraction of variance unexplained (FVU).Datasets were classified into quasi-linear and non-linear and it was observedthat RI performed better for non-linear datasets; however the performance ofPCI approach remains inconclusive for quasi-linear datasets.
arxiv-1210-5840 | Supervised Learning with Similarity Functions |  http://arxiv.org/abs/1210.5840  | author:Purushottam Kar, Prateek Jain category:cs.LG stat.ML published:2012-10-22 summary:We address the problem of general supervised learning when data can only beaccessed through an (indefinite) similarity function between data points.Existing work on learning with indefinite kernels has concentrated solely onbinary/multi-class classification problems. We propose a model that is genericenough to handle any supervised learning task and also subsumes the modelpreviously proposed for classification. We give a "goodness" criterion forsimilarity functions w.r.t. a given supervised learning task and then adapt awell-known landmarking technique to provide efficient algorithms for supervisedlearning using "good" similarity functions. We demonstrate the effectiveness ofour model on three important super-vised learning problems: a) real-valuedregression, b) ordinal regression and c) ranking where we show that our methodguarantees bounded generalization error. Furthermore, for the case ofreal-valued regression, we give a natural goodness definition that, when usedin conjunction with a recent result in sparse vector recovery, guarantees asparse predictor with bounded generalization error. Finally, we report resultsof our learning algorithms on regression and ordinal regression tasks usingnon-PSD similarity functions and demonstrate the effectiveness of ouralgorithms, especially that of the sparse landmark selection algorithm thatachieves significantly higher accuracies than the baseline methods whileoffering reduced computational costs.
arxiv-1210-5751 | Extraction of domain-specific bilingual lexicon from comparable corpora: compositional translation and ranking |  http://arxiv.org/abs/1210.5751  | author:Estelle Delpech, Béatrice Daille, Emmanuel Morin, Claire Lemaire category:cs.CL published:2012-10-21 summary:This paper proposes a method for extracting translations of morphologicallyconstructed terms from comparable corpora. The method is based on compositionaltranslation and exploits translation equivalences at the morpheme-level, whichallows for the generation of "fertile" translations (translation pairs in whichthe target term has more words than the source term). Ranking methods relyingon corpus-based and translation-based features are used to select the bestcandidate translation. We obtain an average precision of 91% on the Top1candidate translation. The method was tested on two language pairs(English-French and English-German) and with a small specialized comparablecorpora (400k words per language).
arxiv-1210-5732 | Developing ICC Profile Using Gray Level Control In Offset Printing Process |  http://arxiv.org/abs/1210.5732  | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-10-21 summary:In prepress department RGB image has to be converted to CMYK image. Tocontrol that amount of black, cyan, magenta and yellow has to be controlled byusing color separation method. Graycolor separation method is selected tocontrol the amounts of these colors because it increase the quality of printingalso. A single printer used for printing the same image on different paper alsoresults in different printed images. To remove this problem a different ICCprofile based on gray level control is developedand a sheet offset printer iscalibrated using that profile and a subjective evaluation shows satisfactoryresults for different quality papers.
arxiv-1210-5644 | Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials |  http://arxiv.org/abs/1210.5644  | author:Philipp Krähenbühl, Vladlen Koltun category:cs.CV cs.AI cs.LG published:2012-10-20 summary:Most state-of-the-art techniques for multi-class image segmentation andlabeling use conditional random fields defined over pixels or image regions.While region-level models often feature dense pairwise connectivity,pixel-level models are considerably larger and have only permitted sparse graphstructures. In this paper, we consider fully connected CRF models defined onthe complete set of pixels in an image. The resulting graphs have billions ofedges, making traditional inference algorithms impractical. Our maincontribution is a highly efficient approximate inference algorithm for fullyconnected CRF models in which the pairwise edge potentials are defined by alinear combination of Gaussian kernels. Our experiments demonstrate that denseconnectivity at the pixel level substantially improves segmentation andlabeling accuracy.
arxiv-1210-5653 | Identifications of concealed weapon in a Human Body |  http://arxiv.org/abs/1210.5653  | author:Prof. Samir K. Bandyopadhyay, Biswajita Datta, Sudipta Roy category:cs.CV published:2012-10-20 summary:The detection of weapons concealed underneath a person cloths is very muchimportant to the improvement of the security of the public as well as thesafety of public assets like airports, buildings and railway stations etc.
arxiv-1210-5631 | Content-boosted Matrix Factorization Techniques for Recommender Systems |  http://arxiv.org/abs/1210.5631  | author:Jennifer Nguyen, Mu Zhu category:stat.ML cs.LG published:2012-10-20 summary:Many businesses are using recommender systems for marketing outreach.Recommendation algorithms can be either based on content or driven bycollaborative filtering. We study different ways to incorporate contentinformation directly into the matrix factorization approach of collaborativefiltering. These content-boosted matrix factorization algorithms not onlyimprove recommendation accuracy, but also provide useful insights about thecontents, as well as make recommendations more easily interpretable.
arxiv-1210-5581 | Hidden Trends in 90 Years of Harvard Business Review |  http://arxiv.org/abs/1210.5581  | author:Chia-Chi Tsai, Chao-Lin Liu, Wei-Jie Huang, Man-Kwan Shan category:cs.CL cs.DL cs.IR published:2012-10-20 summary:In this paper, we demonstrate and discuss results of our mining the abstractsof the publications in Harvard Business Review between 1922 and 2012.Techniques for computing n-grams, collocations, basic sentiment analysis, andnamed-entity recognition were employed to uncover trends hidden in theabstracts. We present findings about international relationships, sentiment inHBR's abstracts, important international companies, influential technologicalinventions, renown researchers in management theories, US presidents viachronological analyses.
arxiv-1212-2516 | Learning Measurement Models for Unobserved Variables |  http://arxiv.org/abs/1212.2516  | author:Ricardo Silva, Richard Scheines, Clark Glymour, Peter L. Spirtes category:cs.LG stat.ML published:2012-10-19 summary:Observed associations in a database may be due in whole or part to variationsin unrecorded (latent) variables. Identifying such variables and their causalrelationships with one another is a principal goal in many scientific andpractical domains. Previous work shows that, given a partition of observedvariables such that members of a class share only a single latent common cause,standard search algorithms for causal Bayes nets can infer structural relationsbetween latent variables. We introduce an algorithm for discovering suchpartitions when they exist. Uniquely among available procedures, the algorithmis (asymptotically) correct under standard assumptions in causal Bayes netsearch algorithms, requires no prior knowledge of the number of latentvariables, and does not depend on the mathematical form of the relationshipsamong the latent variables. We evaluate the algorithm on a variety of simulateddata sets.
arxiv-1212-2490 | On the Convergence of Bound Optimization Algorithms |  http://arxiv.org/abs/1212.2490  | author:Ruslan R Salakhutdinov, Sam T Roweis, Zoubin Ghahramani category:cs.LG stat.ML published:2012-10-19 summary:Many practitioners who use the EM algorithm complain that it is sometimesslow. When does this happen, and what can be done about it? In this paper, westudy the general class of bound optimization algorithms - includingExpectation-Maximization, Iterative Scaling and CCCP - and their relationshipto direct optimization algorithms such as gradient-based methods for parameterlearning. We derive a general relationship between the updates performed bybound optimization methods and those of gradient and second-order methods andidentify analytic conditions under which bound optimization algorithms exhibitquasi-Newton behavior, and conditions under which they possess poor,first-order convergence. Based on this analysis, we consider several specificalgorithms, interpret and analyze their convergence properties and provide somerecipes for preprocessing input to these algorithms to yield faster convergencebehavior. We report empirical results supporting our analysis and showing thatsimple data preprocessing can result in dramatically improved performance ofbound optimizers in practice.
arxiv-1212-2491 | Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for Latent Models |  http://arxiv.org/abs/1212.2491  | author:Dmitry Rusakov, Dan Geiger category:cs.LG stat.ML published:2012-10-19 summary:We present and implement two algorithms for analytic asymptotic evaluation ofthe marginal likelihood of data given a Bayesian network with hidden nodes. Asshown by previous work, this evaluation is particularly hard for latentBayesian network models, namely networks that include hidden variables, whereasymptotic approximation deviates from the standard BIC score. Our algorithmssolve two central difficulties in asymptotic evaluation of marginal likelihoodintegrals, namely, evaluation of regular dimensionality drop for latentBayesian network models and computation of non-standard approximation formulasfor singular statistics for these models. The presented algorithms areimplemented in Matlab and Maple and their usage is demonstrated for marginallikelihood approximations for Bayesian networks with hidden variables.
arxiv-1212-2500 | On Local Optima in Learning Bayesian Networks |  http://arxiv.org/abs/1212.2500  | author:Jens D. Nielsen, Tomas Kocka, Jose M. Pena category:cs.LG cs.AI stat.ML published:2012-10-19 summary:This paper proposes and evaluates the k-greedy equivalence search algorithm(KES) for learning Bayesian networks (BNs) from complete data. The maincharacteristic of KES is that it allows a trade-off between greediness andrandomness, thus exploring different good local optima. When greediness is setat maximum, KES corresponds to the greedy equivalence search algorithm (GES).When greediness is kept at minimum, we prove that under mild assumptions KESasymptotically returns any inclusion optimal BN with nonzero probability.Experimental results for both synthetic and real data are reported showing thatKES often finds a better local optima than GES. Moreover, we use KES toexperimentally confirm that the number of different local optima is often huge.
arxiv-1212-2503 | Practically Perfect |  http://arxiv.org/abs/1212.2503  | author:Christopher Meek, David Maxwell Chickering category:cs.AI stat.ML published:2012-10-19 summary:The property of perfectness plays an important role in the theory of Bayesiannetworks. First, the existence of perfect distributions for arbitrary sets ofvariables and directed acyclic graphs implies that various methods for readingindependence from the structure of the graph (e.g., Pearl, 1988; Lauritzen,Dawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic reliabilityof various search methods is guaranteed under the assumption that thegenerating distribution is perfect (e.g., Spirtes, Glymour & Scheines, 2000;Chickering & Meek, 2002). We provide a lower-bound on the probability ofsampling a non-perfect distribution when using a fixed number of bits torepresent the parameters of the Bayesian network. This bound approaches zeroexponentially fast as one increases the number of bits used to represent theparameters. This result implies that perfect distributions with fixed-lengthrepresentations exist. We also provide a lower-bound on the number of bitsneeded to guarantee that a distribution sampled from a uniform Dirichletdistribution is perfect with probability greater than 1/2. This result isuseful for constructing randomized reductions for hardness proofs.
arxiv-1212-2512 | A Generalized Mean Field Algorithm for Variational Inference in Exponential Families |  http://arxiv.org/abs/1212.2512  | author:Eric P. Xing, Michael I. Jordan, Stuart Russell category:cs.LG stat.ML published:2012-10-19 summary:The mean field methods, which entail approximating intractable probabilitydistributions variationally with distributions from a tractable family, enjoyhigh efficiency, guaranteed convergence, and provide lower bounds on the truelikelihood. But due to requirement for model-specific derivation of theoptimization equations and unclear inference quality in various models, it isnot widely used as a generic approximate inference algorithm. In this paper, wediscuss a generalized mean field theory on variational approximation to a broadclass of intractable distributions using a rich set of tractable distributionsvia constrained optimization over distribution spaces. We present a class ofgeneralized mean field (GMF) algorithms for approximate inference in complexexponential family models, which entails limiting the optimization over theclass of cluster-factorizable distributions. GMF is a generic method requiringno model-specific derivations. It factors a complex model into a set ofdisjoint variable clusters, and uses a set of canonical fix-point equations toiteratively update the cluster distributions, and converge to locally optimalcluster marginals that preserve the original dependency structure within eachcluster, hence, fully decomposed the overall inference problem. We empiricallyanalyzed the effect of different tractable family (clusters of differentgranularity) on inference quality, and compared GMF with BP on severalcanonical models. Possible extension to higher-order MF approximation is alsodiscussed.
arxiv-1210-5500 | Modeling with Copulas and Vines in Estimation of Distribution Algorithms |  http://arxiv.org/abs/1210.5500  | author:Marta Soto, Yasser González-Fernández, Alberto Ochoa category:cs.NE stat.ME published:2012-10-19 summary:The aim of this work is studying the use of copulas and vines in theoptimization with Estimation of Distribution Algorithms (EDAs). Two EDAs arebuilt around the multivariate product and normal copulas, and other two arebased on pair-copula decomposition of vine models. Empirically we study theeffect of both marginal distributions and dependence structure separately, andshow that both aspects play a crucial role in the success of the optimization.The results show that the use of copulas and vines opens new opportunities to amore appropriate modeling of search distributions in EDAs.
arxiv-1212-2504 | Efficiently Inducing Features of Conditional Random Fields |  http://arxiv.org/abs/1212.2504  | author:Andrew McCallum category:cs.LG stat.ML published:2012-10-19 summary:Conditional Random Fields (CRFs) are undirected graphical models, a specialcase of which correspond to conditionally-trained finite state machines. A keyadvantage of these models is their great flexibility to include a wide array ofoverlapping, multi-granularity, non-independent features of the input. In faceof this freedom, an important question that remains is, what features should beused? This paper presents a feature induction method for CRFs. Founded on theprinciple of constructing only those feature conjunctions that significantlyincrease log-likelihood, the approach is based on that of Della Pietra et al[1997], but altered to work with conditional rather than joint probabilities,and with additional modifications for providing tractability specifically for asequence model. In comparison with traditional approaches, automated featureinduction offers both improved accuracy and more than an order of magnitudereduction in feature count; it enables the use of richer, higher-order Markovmodels, and offers more freedom to liberally guess about which atomic featuresmay be relevant to a task. The induction method applies to linear-chain CRFs,as well as to more arbitrary CRF structures, also known as Relational MarkovNetworks [Taskar & Koller, 2002]. We present experimental results on a namedentity extraction task.
arxiv-1212-2471 | Monte Carlo Matrix Inversion Policy Evaluation |  http://arxiv.org/abs/1212.2471  | author:Fletcher Lu, Dale Schuurmans category:cs.LG cs.AI cs.NA published:2012-10-19 summary:In 1950, Forsythe and Leibler (1950) introduced a statistical technique forfinding the inverse of a matrix by characterizing the elements of the matrixinverse as expected values of a sequence of random walks. Barto and Duff (1994)subsequently showed relations between this technique and standard dynamicprogramming and temporal differencing methods. The advantage of the Monte Carlomatrix inversion (MCMI) approach is that it scales better with respect tostate-space size than alternative techniques. In this paper, we introduce analgorithm for performing reinforcement learning policy evaluation using MCMI.We demonstrate that MCMI improves on runtime over a maximum likelihoodmodel-based policy evaluation approach and on both runtime and accuracy overthe temporal differencing (TD) policy evaluation approach. We further improveon MCMI policy evaluation by adding an importance sampling technique to ouralgorithm to reduce the variance of our estimator. Lastly, we illustratetechniques for scaling up MCMI to large state spaces in order to perform policyimprovement.
arxiv-1212-2472 | Budgeted Learning of Naive-Bayes Classifiers |  http://arxiv.org/abs/1212.2472  | author:Daniel J. Lizotte, Omid Madani, Russell Greiner category:cs.LG stat.ML published:2012-10-19 summary:Frequently, acquiring training data has an associated cost. We consider thesituation where the learner may purchase data during training, subject TO abudget. IN particular, we examine the CASE WHERE each feature label has anassociated cost, AND the total cost OF ALL feature labels acquired duringtraining must NOT exceed the budget.This paper compares methods FOR choosingwhich feature label TO purchase next, given the budget AND the CURRENT beliefstate OF naive Bayes model parameters.Whereas active learning has traditionallyfocused ON myopic(greedy) strategies FOR query selection, this paper presents atractable method FOR incorporating knowledge OF the budget INTO the decisionmaking process, which improves performance.
arxiv-1210-5323 | The performance of orthogonal multi-matching pursuit under RIP |  http://arxiv.org/abs/1210.5323  | author:Zhiqiang Xu category:cs.IT cs.LG math.IT math.NA published:2012-10-19 summary:The orthogonal multi-matching pursuit (OMMP) is a natural extension oforthogonal matching pursuit (OMP). We denote the OMMP with the parameter $M$ asOMMP(M) where $M\geq 1$ is an integer. The main difference between OMP andOMMP(M) is that OMMP(M) selects $M$ atoms per iteration, while OMP only addsone atom to the optimal atom set. In this paper, we study the performance oforthogonal multi-matching pursuit (OMMP) under RIP. In particular, we showthat, when the measurement matrix A satisfies $(9s, 1/10)$-RIP, there exists anabsolutely constant $M_0\leq 8$ so that OMMP(M_0) can recover $s$-sparse signalwithin $s$ iterations. We furthermore prove that, for slowly-decaying$s$-sparse signal, OMMP(M) can recover s-sparse signal within $O(\frac{s}{M})$iterations for a large class of $M$. In particular, for $M=s^a$ with $a\in[0,1/2]$, OMMP(M) can recover slowly-decaying $s$-sparse signal within$O(s^{1-a})$ iterations. The result implies that OMMP can reduce thecomputational complexity heavily.
arxiv-1212-2474 | Learning Riemannian Metrics |  http://arxiv.org/abs/1212.2474  | author:Guy Lebanon category:cs.LG stat.ML published:2012-10-19 summary:We propose a solution to the problem of estimating a Riemannian metricassociated with a given differentiable manifold. The metric learning problem isbased on minimizing the relative volume of a given set of points. We derive thedetails for a family of metrics on the multinomial simplex. The resultingmetric has applications in text classification and bears some similarity toTFIDF representation of text documents.
arxiv-1212-2475 | Efficient Gradient Estimation for Motor Control Learning |  http://arxiv.org/abs/1212.2475  | author:Gregory Lawrence, Noah Cowan, Stuart Russell category:cs.LG cs.SY published:2012-10-19 summary:The task of estimating the gradient of a function in the presence of noise iscentral to several forms of reinforcement learning, including policy searchmethods. We present two techniques for reducing gradient estimation errors inthe presence of observable input noise applied to the control signal. The firstmethod extends the idea of a reinforcement baseline by fitting a local linearmodel to the function whose gradient is being estimated; we show how to findthe linear model that minimizes the variance of the gradient estimate, and howto estimate the model from data. The second method improves this further bydiscounting components of the gradient vector that have high variance. Thesemethods are applied to the problem of motor control learning, where actuatornoise has a significant influence on behavior. In particular, we apply thetechniques to learn locally optimal controllers for a dart-throwing task usinga simulated three-link arm; we demonstrate that proposed methods significantlyimprove the reward function gradient estimate and, consequently, the learningcurve, over existing methods.
arxiv-1212-2510 | Markov Random Walk Representations with Continuous Distributions |  http://arxiv.org/abs/1212.2510  | author:Chen-Hsiang Yeang, Martin Szummer category:cs.LG stat.ML published:2012-10-19 summary:Representations based on random walks can exploit discrete data distributionsfor clustering and classification. We extend such representations from discreteto continuous distributions. Transition probabilities are now calculated usinga diffusion equation with a diffusion coefficient that inversely depends on thedata density. We relate this diffusion equation to a path integral and derivethe corresponding path probability measure. The framework is useful forincorporating continuous data densities and prior knowledge.
arxiv-1212-2477 | 1 Billion Pages = 1 Million Dollars? Mining the Web to Play "Who Wants to be a Millionaire?" |  http://arxiv.org/abs/1212.2477  | author:Shyong, K. Lam, David M Pennock, Dan Cosley, Steve Lawrence category:cs.IR cs.CL published:2012-10-19 summary:We exploit the redundancy and volume of information on the web to build acomputerized player for the ABC TV game show 'Who Wants To Be A Millionaire?'The player consists of a question-answering module and a decision-makingmodule. The question-answering module utilizes question transformationtechniques, natural language parsing, multiple information retrievalalgorithms, and multiple search engines; results are combined in the spirit ofensemble learning using an adaptive weighting scheme. Empirically, the systemcorrectly answers about 75% of questions from the Millionaire CD-ROM, 3rdedition - general-interest trivia questions often about popular culture andcommon knowledge. The decision-making module chooses from allowable actions inthe game in order to maximize expected risk-adjusted winnings, where theestimated probability of answering correctly is a function of past performanceand confidence in in correctly answering the current question. When given a sixquestion head start (i.e., when starting from the $2,000 level), we find thatthe system performs about as well on average as humans starting at thebeginning. Our system demonstrates the potential of simple but well-chosentechniques for mining answers from unstructured information such as the web.
arxiv-1210-5517 | Design of English-Hindi Translation Memory for Efficient Translation |  http://arxiv.org/abs/1210.5517  | author:Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-10-19 summary:Developing parallel corpora is an important and a difficult activity forMachine Translation. This requires manual annotation by Human Translators.Translating same text again is a useless activity. There are tools available toimplement this for European Languages, but no such tool is available for IndianLanguages. In this paper we present a tool for Indian Languages which not onlyprovides automatic translations of the previously available translation butalso provides multiple translations, in cases where a sentence has multipletranslations, in ranked list of suggestive translations for a sentence.Moreover this tool also lets translators have global and local saving optionsof their work, so that they may share it with others, which further lightensthe task.
arxiv-1212-2480 | Approximate Inference and Constrained Optimization |  http://arxiv.org/abs/1212.2480  | author:Tom Heskes, Kees Albers, Hilbert Kappen category:cs.LG cs.AI stat.ML published:2012-10-19 summary:Loopy and generalized belief propagation are popular algorithms forapproximate inference in Markov random fields and Bayesian networks. Fixedpoints of these algorithms correspond to extrema of the Bethe and Kikuchi freeenergy. However, belief propagation does not always converge, which explainsthe need for approaches that explicitly minimize the Kikuchi/Bethe free energy,such as CCCP and UPS. Here we describe a class of algorithms that solves thistypically nonconvex constrained minimization of the Kikuchi free energy througha sequence of convex constrained minimizations of upper bounds on the Kikuchifree energy. Intuitively one would expect tighter bounds to lead to fasteralgorithms, which is indeed convincingly demonstrated in our simulations.Several ideas are applied to obtain tight convex bounds that yield dramaticspeed-ups over CCCP.
arxiv-1212-2483 | Sufficient Dimensionality Reduction with Irrelevant Statistics |  http://arxiv.org/abs/1212.2483  | author:Amir Globerson, Gal Chechik, Naftali Tishby category:cs.LG stat.ML published:2012-10-19 summary:The problem of finding a reduced dimensionality representation of categoricalvariables while preserving their most relevant characteristics is fundamentalfor the analysis of complex data. Specifically, given a co-occurrence matrix oftwo variables, one often seeks a compact representation of one variable whichpreserves information about the other variable. We have recently introduced``Sufficient Dimensionality Reduction' [GT-2003], a method that extractscontinuous reduced dimensional features whose measurements (i.e., expectationvalues) capture maximal mutual information among the variables. However, suchmeasurements often capture information that is irrelevant for a given task.Widely known examples are illumination conditions, which are irrelevant asfeatures for face recognition, writing style which is irrelevant as a featurefor content classification, and intonation which is irrelevant as a feature forspeech recognition. Such irrelevance cannot be deduced apriori, since itdepends on the details of the task, and is thus inherently ill defined in thepurely unsupervised case. Separating relevant from irrelevant features can beachieved using additional side data that contains such irrelevant structures.This approach was taken in [CT-2002], extending the information bottleneckmethod, which uses clustering to compress the data. Here we use thisside-information framework to identify features whose measurements aremaximally informative for the original data set, but carry as littleinformation as possible on a side data set. In statistical terms this can beunderstood as extracting statistics which are maximally sufficient for theoriginal dataset, while simultaneously maximally ancillary for the sidedataset. We formulate this tradeoff as a constrained optimization problem andcharacterize its solutions. We then derive a gradient descent algorithm forthis problem, which is based on the Generalized Iterative Scaling method forfinding maximum entropy distributions. The method is demonstrated on syntheticdata, as well as on real face recognition datasets, and is shown to outperformstandard methods such as oriented PCA.
arxiv-1212-2487 | Locally Weighted Naive Bayes |  http://arxiv.org/abs/1212.2487  | author:Eibe Frank, Mark Hall, Bernhard Pfahringer category:cs.LG stat.ML published:2012-10-19 summary:Despite its simplicity, the naive Bayes classifier has surprised machinelearning researchers by exhibiting good performance on a variety of learningproblems. Encouraged by these results, researchers have looked to overcomenaive Bayes primary weakness - attribute independence - and improve theperformance of the algorithm. This paper presents a locally weighted version ofnaive Bayes that relaxes the independence assumption by learning local modelsat prediction time. Experimental results show that locally weighted naive Bayesrarely degrades accuracy compared to standard naive Bayes and, in many cases,improves accuracy dramatically. The main advantage of this method compared toother techniques for enhancing naive Bayes is its conceptual and computationalsimplicity.
arxiv-1212-2488 | A Distance-Based Branch and Bound Feature Selection Algorithm |  http://arxiv.org/abs/1212.2488  | author:Ari Frank, Dan Geiger, Zohar Yakhini category:cs.LG stat.ML published:2012-10-19 summary:There is no known efficient method for selecting k Gaussian features from nwhich achieve the lowest Bayesian classification error. We show an example ofhow greedy algorithms faced with this task are led to give results that are notoptimal. This motivates us to propose a more robust approach. We present aBranch and Bound algorithm for finding a subset of k independent Gaussianfeatures which minimizes the naive Bayesian classification error. Our algorithmuses additive monotonic distance measures to produce bounds for the Bayesianclassification error in order to exclude many feature subsets from evaluation,while still returning an optimal solution. We test our method on synthetic dataas well as data obtained from gene expression profiling.
arxiv-1212-2508 | Collaborative Ensemble Learning: Combining Collaborative and Content-Based Information Filtering via Hierarchical Bayes |  http://arxiv.org/abs/1212.2508  | author:Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying Ma, HongJiang Zhang category:cs.LG cs.IR stat.ML published:2012-10-19 summary:Collaborative filtering (CF) and content-based filtering (CBF) have widelybeen used in information filtering applications. Both approaches have theirstrengths and weaknesses which is why researchers have developed hybridsystems. This paper proposes a novel approach to unify CF and CBF in aprobabilistic framework, named collaborative ensemble learning. It usesprobabilistic SVMs to model each user's profile (as CBF does).At the predictionphase, it combines a society OF users profiles, represented by their respectiveSVM models, to predict an active users preferences(the CF idea).The combinationscheme is embedded in a probabilistic framework and retains an intuitiveexplanation.Moreover, collaborative ensemble learning does not require a globaltraining stage and thus can incrementally incorporate new data.We reportresults based on two data sets. For the Reuters-21578 text data set, wesimulate user ratings under the assumption that each user is interested in onlyone category. In the second experiment, we use users' opinions on a set of 642art images that were collected through a web-based survey. For both data sets,collaborative ensemble achieved excellent performance in terms ofrecommendation accuracy.
arxiv-1212-2460 | The Information Bottleneck EM Algorithm |  http://arxiv.org/abs/1212.2460  | author:Gal Elidan, Nir Friedman category:cs.LG stat.ML published:2012-10-19 summary:Learning with hidden variables is a central challenge in probabilisticgraphical models that has important implications for many real-life problems.The classical approach is using the Expectation Maximization (EM) algorithm.This algorithm, however, can get trapped in local maxima. In this paper weexplore a new approach that is based on the Information Bottleneck principle.In this approach, we view the learning problem as a tradeoff between twoinformation theoretic objectives. The first is to make the hidden variablesuninformative about the identity of specific instances. The second is to makethe hidden variables informative about the observed attributes. By exploringdifferent tradeoffs between these two objectives, we can gradually converge ona high-scoring solution. As we show, the resulting, Information BottleneckExpectation Maximization (IB-EM) algorithm, manages to find solutions that aresuperior to standard EM methods.
arxiv-1210-5486 | A Lightweight Stemmer for Gujarati |  http://arxiv.org/abs/1210.5486  | author:Juhi Ameta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-10-19 summary:Gujarati is a resource poor language with almost no language processing toolsbeing available. In this paper we have shown an implementation of a rule basedstemmer of Gujarati. We have shown the creation of rules for stemming and therichness in morphology that Gujarati possesses. We have also evaluated ourresults by verifying it with a human expert.
arxiv-1212-2462 | A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical Models for Marginal Independence |  http://arxiv.org/abs/1212.2462  | author:Mathias Drton, Thomas S. Richardson category:stat.ME cs.LG stat.ML published:2012-10-19 summary:Graphical models with bi-directed edges (<->) represent marginalindependence: the absence of an edge between two vertices indicates that thecorresponding variables are marginally independent. In this paper, we considermaximum likelihood estimation in the case of continuous variables with aGaussian joint distribution, sometimes termed a covariance graph model. Wepresent a new fitting algorithm which exploits standard regression techniquesand establish its convergence properties. Moreover, we contrast our procedureto existing estimation methods.
arxiv-1212-2464 | A Robust Independence Test for Constraint-Based Learning of Causal Structure |  http://arxiv.org/abs/1212.2464  | author:Denver Dash, Marek J. Druzdzel category:cs.AI cs.LG stat.ML published:2012-10-19 summary:Constraint-based (CB) learning is a formalism for learning a causal networkwith a database D by performing a series of conditional-independence tests toinfer structural information. This paper considers a new test of independencethat combines ideas from Bayesian learning, Bayesian network inference, andclassical hypothesis testing to produce a more reliable and robust test. Thenew test can be calculated in the same asymptotic time and space required forthe standard tests such as the chi-squared test, but it allows thespecification of a prior distribution over parameters and can be used when thedatabase is incomplete. We prove that the test is correct, and we demonstrateempirically that, when used with a CB causal discovery algorithm withnoninformative priors, it recovers structural features more reliably and itproduces networks with smaller KL-Divergence, especially as the number of nodesincreases or the number of records decreases. Another benefit is the dramaticreduction in the probability that a CB algorithm will stall during the search,providing a remedy for an annoying problem plaguing CB learning when thedatabase is small.
arxiv-1212-2466 | On Information Regularization |  http://arxiv.org/abs/1212.2466  | author:Adrian Corduneanu, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-10-19 summary:We formulate a principle for classification with the knowledge of themarginal distribution over the data points (unlabeled data). The principle iscast in terms of Tikhonov style regularization where the regularization penaltyarticulates the way in which the marginal density should constrain otherwiseunrestricted conditional distributions. Specifically, the regularizationpenalty penalizes any information introduced between the examples and labelsbeyond what is provided by the available labeled examples. The work extendsSzummer and Jaakkola's information regularization (NIPS 2002) to multipledimensions, providing a regularizer independent of the covering of the spaceused in the derivation. We show in addition how the information regularizer canbe used as a measure of complexity of the classification task with unlabeleddata and prove a relevant sample-complexity bound. We illustrate theregularization principle in practice by restricting the class of conditionaldistributions to be logistic regression models and constructing theregularization penalty from a finite set of unlabeled examples.
arxiv-1212-2468 | Large-Sample Learning of Bayesian Networks is NP-Hard |  http://arxiv.org/abs/1212.2468  | author:David Maxwell Chickering, Christopher Meek, David Heckerman category:cs.LG cs.AI stat.ML published:2012-10-19 summary:In this paper, we provide new complexity results for algorithms that learndiscrete-variable Bayesian networks from data. Our results apply whenever thelearning algorithm uses a scoring criterion that favors the simplest model ableto represent the generative distribution exactly. Our results therefore holdwhenever the learning algorithm uses a consistent scoring criterion and isapplied to a sufficiently large dataset. We show that identifying high-scoringstructures is hard, even when we are given an independence oracle, an inferenceoracle, and/or an information oracle. Our negative results also apply to thelearning of discrete-variable Bayesian networks in which each node has at mostk parents, for all k > 3.
arxiv-1212-2470 | Reasoning about Bayesian Network Classifiers |  http://arxiv.org/abs/1212.2470  | author:Hei Chan, Adnan Darwiche category:cs.LG cs.AI stat.ML published:2012-10-19 summary:Bayesian network classifiers are used in many fields, and one common class ofclassifiers are naive Bayes classifiers. In this paper, we introduce anapproach for reasoning about Bayesian network classifiers in which weexplicitly convert them into Ordered Decision Diagrams (ODDs), which are thenused to reason about the properties of these classifiers. Specifically, wepresent an algorithm for converting any naive Bayes classifier into an ODD, andwe show theoretically and experimentally that this algorithm can give us an ODDthat is tractable in size even given an intractable number of instances. SinceODDs are tractable representations of classifiers, our algorithm allows us toefficiently test the equivalence of two naive Bayes classifiers andcharacterize discrepancies between them. We also show a number of additionalresults including a count of distinct classifiers that can be induced bychanging some CPT in a naive Bayes classifier, and the range of allowablechanges to a CPT which keeps the current classifier unchanged.
arxiv-1212-2442 | Active Collaborative Filtering |  http://arxiv.org/abs/1212.2442  | author:Craig Boutilier, Richard S. Zemel, Benjamin Marlin category:cs.IR cs.LG stat.ML published:2012-10-19 summary:Collaborative filtering (CF) allows the preferences of multiple users to bepooled to make recommendations regarding unseen products. We consider in thispaper the problem of online and interactive CF: given the current ratingsassociated with a user, what queries (new ratings) would most improve thequality of the recommendations made? We cast this terms of expected value ofinformation (EVOI); but the online computational cost of computing optimalqueries is prohibitive. We show how offline prototyping and computation ofbounds on EVOI can be used to dramatically reduce the required onlinecomputation. The framework we develop is general, but we focus on derivationsand empirical study in the specific case of the multiple-cause vectorquantization model.
arxiv-1212-2447 | Bayesian Hierarchical Mixtures of Experts |  http://arxiv.org/abs/1212.2447  | author:Christopher M. Bishop, Markus Svensen category:cs.LG stat.ML published:2012-10-19 summary:The Hierarchical Mixture of Experts (HME) is a well-known tree-based modelfor regression and classification, based on soft probabilistic splits. In itsoriginal formulation it was trained by maximum likelihood, and is thereforeprone to over-fitting. Furthermore the maximum likelihood framework offers nonatural metric for optimizing the complexity and structure of the tree.Previous attempts to provide a Bayesian treatment of the HME model have reliedeither on ad-hoc local Gaussian approximations or have dealt with relatedmodels representing the joint distribution of both input and output variables.In this paper we describe a fully Bayesian treatment of the HME model based onvariational inference. By combining local and global variational methods weobtain a rigourous lower bound on the marginal probability of the data underthe model. This bound is optimized during the training phase, and its resultingvalue can be used for model order selection. We present results using thisapproach for a data set describing robot arm kinematics.
arxiv-1212-2511 | Stochastic complexity of Bayesian networks |  http://arxiv.org/abs/1212.2511  | author:Keisuke Yamazaki, Sumio Watanbe category:cs.LG stat.ML published:2012-10-19 summary:Bayesian networks are now being used in enormous fields, for example,diagnosis of a system, data mining, clustering and so on. In spite of theirwide range of applications, the statistical properties have not yet beenclarified, because the models are nonidentifiable and non-regular. In aBayesian network, the set of its parameter for a smaller model is an analyticset with singularities in the space of large ones. Because of thesesingularities, the Fisher information matrices are not positive definite. Inother words, the mathematical foundation for learning was not constructed. Inrecent years, however, we have developed a method to analyze non-regular modelsusing algebraic geometry. This method revealed the relation between the modelssingularities and its statistical properties. In this paper, applying thismethod to Bayesian networks with latent variables, we clarify the order of thestochastic complexities.Our result claims that the upper bound of those issmaller than the dimension of the parameter space. This means that the Bayesiangeneralization error is also far smaller than that of regular model, and thatSchwarzs model selection criterion BIC needs to be improved for Bayesiannetworks.
arxiv-1210-5474 | Disentangling Factors of Variation via Generative Entangling |  http://arxiv.org/abs/1210.5474  | author:Guillaume Desjardins, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2012-10-19 summary:Here we propose a novel model family with the objective of learning todisentangle the factors of variation in data. Our approach is based on thespike-and-slab restricted Boltzmann machine which we generalize to includehigher-order interactions among multiple latent variables. Seen from agenerative perspective, the multiplicative interactions emulates the entanglingof factors of variation. Inference in the model can be seen as disentanglingthese generative factors. Unlike previous attempts at disentangling latentfactors, the proposed model is trained using no supervised informationregarding the latent factors. We apply our model to the task of facialexpression classification.
arxiv-1212-2453 | Web-Based Question Answering: A Decision-Making Perspective |  http://arxiv.org/abs/1212.2453  | author:David Azari, Eric J. Horvitz, Susan Dumais, Eric Brill category:cs.IR cs.CL published:2012-10-19 summary:We describe an investigation of the use of probabilistic models andcost-benefit analyses to guide resource-intensive procedures used by aWeb-based question answering system. We first provide an overview of researchon question-answering systems. Then, we present details on AskMSR, a prototypeweb-based question answering system. We discuss Bayesian analyses of thequality of answers generated by the system and show how we can endow the systemwith the ability to make decisions about the number of queries issued to asearch engine, given the cost of queries and the expected value of queryresults in refining an ultimate answer. Finally, we review the results of a setof experiments.
arxiv-1210-5394 | Bayesian Estimation for Continuous-Time Sparse Stochastic Processes |  http://arxiv.org/abs/1210.5394  | author:Arash Amini, Ulugbek S. Kamilov, Emrah Bostan, Michael Unser category:cs.LG published:2012-10-19 summary:We consider continuous-time sparse stochastic processes from which we haveonly a finite number of noisy/noiseless samples. Our goal is to estimate thenoiseless samples (denoising) and the signal in-between (interpolationproblem). By relying on tools from the theory of splines, we derive the joint a prioridistribution of the samples and show how this probability density function canbe factorized. The factorization enables us to tractably implement the maximuma posteriori and minimum mean-square error (MMSE) criteria as two statisticalapproaches for estimating the unknowns. We compare the derived statisticalmethods with well-known techniques for the recovery of sparse signals, such asthe $\ell_1$ norm and Log ($\ell_1$-$\ell_0$ relaxation) regularizationmethods. The simulation results show that, under certain conditions, theperformance of the regularization techniques can be very close to that of theMMSE estimator.
arxiv-1210-5345 | Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions |  http://arxiv.org/abs/1210.5345  | author:Alexandra Carpentier, Rémi Munos category:stat.ML published:2012-10-19 summary:We consider the problem of adaptive stratified sampling for Monte Carlointegration of a differentiable function given a finite number of evaluationsto the function. We construct a sampling scheme that samples more often inregions where the function oscillates more, while allocating the samples suchthat they are well spread on the domain (this notion shares similitude with lowdiscrepancy). We prove that the estimate returned by the algorithm is almostsimilarly accurate as the estimate that an optimal oracle strategy (that wouldknow the variations of the function everywhere) would return, and provide afinite-sample analysis.
arxiv-1210-5321 | The origin of Mayan languages from Formosan language group of Austronesian |  http://arxiv.org/abs/1210.5321  | author:Koji Ohnishi category:cs.CL q-bio.PE published:2012-10-19 summary:Basic body-part names (BBPNs) were defined as body-part names in Swadeshbasic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensivelysearched for, by comparing with non-MY vocabulary, including ca.1300 basicwords of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs)in non-MY are listed in Table 1, as classified by language groups to which mostsimilar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23mutually unrelated CG-items, of which 17.5 CG-items have their MSCs inAustronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5,which consists of 10.33 MSCs in Formosan, 1.83 MSCs in WesternMalayo-Polynesian (W.MP), 0.33 in Central MP, 0.0 in SHWNG, and 5.0 in Oceanic[i.e., CSS(FORM)= 10.33, CSS(W.MP) = 1.88, ..., CSS(OC)= 5.0]. These CSSs forlanguage (sub)groups are also listed in the underline portion of every sectionof (Section1 - Section 6) in Table 1. Chi-squar test (degree of freedom = 1)using [Eq 1] and [Eqs.2] revealed that MSCs of MY BBPNs are distributed inFormosan in significantly higher frequency (P < 0.001) than in other subgroupsof AN, as well as than in non-AN languages. MY is thus concluded to have beenderived from Formosan of AN. Eskimo shows some BBPN similarities to FORM andMY.
arxiv-1212-2514 | Boltzmann Machine Learning with the Latent Maximum Entropy Principle |  http://arxiv.org/abs/1212.2514  | author:Shaojun Wang, Dale Schuurmans, Fuchun Peng, Yunxin Zhao category:cs.LG stat.ML published:2012-10-19 summary:We present a new statistical learning paradigm for Boltzmann machines basedon a new inference principle we have proposed: the latent maximum entropyprinciple (LME). LME is different both from Jaynes maximum entropy principleand from standard maximum likelihood estimation.We demonstrate the LMEprinciple BY deriving new algorithms for Boltzmann machine parameterestimation, and show how robust and fast new variant of the EM algorithm can bedeveloped.Our experiments show that estimation based on LME generally yieldsbetter results than maximum likelihood estimation, particularly when inferringhidden units from small amounts of data.
arxiv-1212-2517 | Learning Module Networks |  http://arxiv.org/abs/1212.2517  | author:Eran Segal, Dana Pe'er, Aviv Regev, Daphne Koller, Nir Friedman category:cs.LG cs.CE stat.ML published:2012-10-19 summary:Methods for learning Bayesian network structure can discover dependencystructure between observed variables, and have been shown to be useful in manyapplications. However, in domains that involve a large number of variables, thespace of possible network structures is enormous, making it difficult, for bothcomputational and statistical reasons, to identify a good model. In this paper,we consider a solution to this problem, suitable for domains where manyvariables have similar behavior. Our method is based on a new class of models,which we call module networks. A module network explicitly represents thenotion of a module - a set of variables that have the same parents in thenetwork and share the same conditional probability distribution. We define thesemantics of module networks, and describe an algorithm that learns a modulenetwork from data. The algorithm learns both the partitioning of the variablesinto modules and the dependency structure between the variables. We evaluateour algorithm on synthetic data, and on real data in the domains of geneexpression and the stock market. Our results show that module networksgeneralize better than Bayesian networks, and that the learned module networkstructure reveals regularities that are obscured in learned Bayesian networks.
arxiv-1210-5338 | Pairwise MRF Calibration by Perturbation of the Bethe Reference Point |  http://arxiv.org/abs/1210.5338  | author:Cyril Furtlehner, Yufei Han, Jean-Marc Lasgouttes, Victorin Martin category:cs.LG stat.ML published:2012-10-19 summary:We investigate different ways of generating approximate solutions to thepairwise Markov random field (MRF) selection problem. We focus mainly on theinverse Ising problem, but discuss also the somewhat related inverse Gaussianproblem because both types of MRF are suitable for inference tasks with thebelief propagation algorithm (BP) under certain conditions. Our approachconsists in to take a Bethe mean-field solution obtained with a maximumspanning tree (MST) of pairwise mutual information, referred to as the\emph{Bethe reference point}, for further perturbation procedures. We considerthree different ways following this idea: in the first one, we select andcalibrate iteratively the optimal links to be added starting from the Bethereference point; the second one is based on the observation that the naturalgradient can be computed analytically at the Bethe point; in the third one,assuming no local field and using low temperature expansion we develop a dualloop joint model based on a well chosen fundamental cycle basis. We indeedidentify a subclass of planar models, which we refer to as \emph{Bethe-dualgraph models}, having possibly many loops, but characterized by a singlyconnected dual factor graph, for which the partition function and the linearresponse can be computed exactly in respectively O(N) and $O(N^2)$ operations,thanks to a dual weight propagation (DWP) message passing procedure that we setup. When restricted to this subclass of models, the inverse Ising problem beingconvex, becomes tractable at any temperature. Experimental tests on variousdatasets with refined $L_0$ or $L_1$ regularization procedures indicate thatthese approaches may be competitive and useful alternatives to existing ones.
arxiv-1212-2513 | Efficient Parametric Projection Pursuit Density Estimation |  http://arxiv.org/abs/1212.2513  | author:Max Welling, Richard S. Zemel, Geoffrey E. Hinton category:cs.LG stat.ML published:2012-10-19 summary:Product models of low dimensional experts are a powerful way to avoid thecurse of dimensionality. We present the ``under-complete product of experts'(UPoE), where each expert models a one dimensional projection of the data. TheUPoE is fully tractable and may be interpreted as a parametric probabilisticmodel for projection pursuit. Its ML learning rules are identical to theapproximate learning rules proposed before for under-complete ICA. We alsoderive an efficient sequential learning algorithm and discuss its relationshipto projection pursuit density estimation and feature induction algorithms foradditive random field models.
arxiv-1210-5544 | Online Learning in Decentralized Multiuser Resource Sharing Problems |  http://arxiv.org/abs/1210.5544  | author:Cem Tekin, Mingyan Liu category:cs.LG published:2012-10-19 summary:In this paper, we consider the general scenario of resource sharing in adecentralized system when the resource rewards/qualities are time-varying andunknown to the users, and using the same resource by multiple users leads toreduced quality due to resource sharing. Firstly, we consider auser-independent reward model with no communication between the users, where auser gets feedback about the congestion level in the resource it uses.Secondly, we consider user-specific rewards and allow costly communicationbetween the users. The users have a cooperative goal of achieving the highestsystem utility. There are multiple obstacles in achieving this goal such as thedecentralized nature of the system, unknown resource qualities, communication,computation and switching costs. We propose distributed learning algorithmswith logarithmic regret with respect to the optimal allocation. Our logarithmicregret result holds under both i.i.d. and Markovian reward models, as well asunder communication, computation and switching costs.
arxiv-1212-2498 | Learning Continuous Time Bayesian Networks |  http://arxiv.org/abs/1212.2498  | author:Uri Nodelman, Christian R. Shelton, Daphne Koller category:cs.LG stat.ML published:2012-10-19 summary:Continuous time Bayesian networks (CTBNs) describe structured stochasticprocesses with finitely many states that evolve over continuous time. A CTBN isa directed (possibly cyclic) dependency graph over a set of variables, each ofwhich represents a finite state continuous time Markov process whose transitionmodel is a function of its parents. We address the problem of learningparameters and structure of a CTBN from fully observed data. We define aconjugate prior for CTBNs, and show how it can be used both for Bayesianparameter estimation and as the basis of a Bayesian score for structurelearning. Because acyclicity is not a constraint in CTBNs, we can show that thestructure learning problem is significantly easier, both in theory and inpractice, than structure learning for dynamic Bayesian networks (DBNs).Furthermore, as CTBNs can tailor the parameters and dependency structure to thedifferent time granularities of the evolution of different variables, they canprovide a better fit to continuous-time processes than DBNs with a fixed timegranularity.
arxiv-1212-2494 | Learning Generative Models of Similarity Matrices |  http://arxiv.org/abs/1212.2494  | author:Romer Rosales, Brendan J. Frey category:cs.LG stat.ML published:2012-10-19 summary:We describe a probabilistic (generative) view of affinity matrices along withinference algorithms for a subclass of problems associated with dataclustering. This probabilistic view is helpful in understanding differentmodels and algorithms that are based on affinity functions OF the data. INparticular, we show how(greedy) inference FOR a specific probabilistic model ISequivalent TO the spectral clustering algorithm.It also provides a frameworkFOR developing new algorithms AND extended models. AS one CASE, we present newgenerative data clustering models that allow us TO infer the underlyingdistance measure suitable for the clustering problem at hand. These models seemto perform well in a larger class of problems for which other clusteringalgorithms (including spectral clustering) usually fail. Experimentalevaluation was performed in a variety point data sets, showing excellentperformance.
arxiv-1210-5268 | Diffusion of Lexical Change in Social Media |  http://arxiv.org/abs/1210.5268  | author:Jacob Eisenstein, Brendan O'Connor, Noah A. Smith, Eric P. Xing category:cs.CL cs.SI physics.soc-ph published:2012-10-18 summary:Computer-mediated communication is driving fundamental changes in the natureof written language. We investigate these changes by statistical analysis of adataset comprising 107 million Twitter messages (authored by 2.7 million uniqueuser accounts). Using a latent vector autoregressive model to aggregate acrossthousands of words, we identify high-level patterns in diffusion of linguisticchange over the United States. Our model is robust to unpredictable changes inTwitter's sampling rate, and provides a probabilistic characterization of therelationship of macro-scale linguistic influence to a set of demographic andgeographic predictors. The results of this analysis offer support for priorarguments that focus on geographical proximity and population size. However,demographic similarity -- especially with regard to race -- plays an even morecentral role, as cities with similar racial demographics are far more likely toshare linguistic influence. Rather than moving towards a single unified"netspeak" dialect, language evolution in computer-mediated communicationreproduces existing fault lines in spoken American English.
arxiv-1210-5128 | A Novel Learning Algorithm for Bayesian Network and Its Efficient Implementation on GPU |  http://arxiv.org/abs/1210.5128  | author:Yu Wang, Weikang Qian, Shuchang Zhang, Bo Yuan category:cs.DC cs.LG published:2012-10-18 summary:Computational inference of causal relationships underlying complex networks,such as gene-regulatory pathways, is NP-complete due to its combinatorialnature when permuting all possible interactions. Markov chain Monte Carlo(MCMC) has been introduced to sample only part of the combinations while stillguaranteeing convergence and traversability, which therefore becomes widelyused. However, MCMC is not able to perform efficiently enough for networks thathave more than 15~20 nodes because of the computational complexity. In thispaper, we use general purpose processor (GPP) and general purpose graphicsprocessing unit (GPGPU) to implement and accelerate a novel Bayesian networklearning algorithm. With a hash-table-based memory-saving strategy and a noveltask assigning strategy, we achieve a 10-fold acceleration per iteration thanusing a serial GPP. Specially, we use a greedy method to search for the bestgraph from a given order. We incorporate a prior component in the currentscoring function, which further facilitates the searching. Overall, we are ableto apply this system to networks with more than 60 nodes, allowing inferencesand modeling of bigger and more complex networks than current methods.
arxiv-1210-5034 | Optimal Computational Trade-Off of Inexact Proximal Methods |  http://arxiv.org/abs/1210.5034  | author:Pierre Machart, Sandrine Anthoine, Luca Baldassarre category:cs.LG cs.CV cs.NA published:2012-10-18 summary:In this paper, we investigate the trade-off between convergence rate andcomputational cost when minimizing a composite functional withproximal-gradient methods, which are popular optimisation tools in machinelearning. We consider the case when the proximity operator is computed via aniterative procedure, which provides an approximation of the exact proximityoperator. In that case, we obtain algorithms with two nested loops. We showthat the strategy that minimizes the computational cost to reach a solutionwith a desired accuracy in finite time is to set the number of inner iterationsto a constant, which differs from the strategy indicated by a convergence rateanalysis. In the process, we also present a new procedure called SIP (that isSpeedy Inexact Proximal-gradient algorithm) that is both computationallyefficient and easy to implement. Our numerical experiments confirm thetheoretical findings and suggest that SIP can be a very competitive alternativeto the standard procedure.
arxiv-1210-5502 | OpenCFU, a New Free and Open-Source Software to Count Cell Colonies and Other Circular Objects |  http://arxiv.org/abs/1210.5502  | author:Quentin Geissmann category:q-bio.QM cs.CV published:2012-10-18 summary:Counting circular objects such as cell colonies is an important source ofinformation for biologists. Although this task is often time-consuming andsubjective, it is still predominantly performed manually. The aim of thepresent work is to provide a new tool to enumerate circular objects fromdigital pictures and video streams. Here, I demonstrate that the createdprogram, OpenCFU, is very robust, accurate and fast. In addition, it providescontrol over the processing parameters and is implemented in an in- tuitive andmodern interface. OpenCFU is a cross-platform and open-source software freelyavailable at http://opencfu.sourceforge.net.
arxiv-1210-5041 | Navigation domain representation for interactive multiview imaging |  http://arxiv.org/abs/1210.5041  | author:Thomas Maugey, Ismael Daribo, Gene Cheung, Pascal Frossard category:cs.MM cs.CV published:2012-10-18 summary:Enabling users to interactively navigate through different viewpoints of astatic scene is a new interesting functionality in 3D streaming systems. Whileit opens exciting perspectives towards rich multimedia applications, itrequires the design of novel representations and coding techniques in order tosolve the new challenges imposed by interactive navigation. Interactivityclearly brings new design constraints: the encoder is unaware of the exactdecoding process, while the decoder has to reconstruct information fromincomplete subsets of data since the server can generally not transmit imagesfor all possible viewpoints due to resource constrains. In this paper, wepropose a novel multiview data representation that permits to satisfy bandwidthand storage constraints in an interactive multiview streaming system. Inparticular, we partition the multiview navigation domain into segments, each ofwhich is described by a reference image and some auxiliary information. Theauxiliary information enables the client to recreate any viewpoint in thenavigation segment via view synthesis. The decoder is then able to navigatefreely in the segment without further data request to the server; it requestsadditional data only when it moves to a different segment. We discuss thebenefits of this novel representation in interactive navigation systems andfurther propose a method to optimize the partitioning of the navigation domaininto independent segments, under bandwidth and storage constraints.Experimental results confirm the potential of the proposed representation;namely, our system leads to similar compression performance as classicalinter-view coding, while it provides the high level of flexibility that isrequired for interactive streaming. Hence, our new framework represents apromising solution for 3D data representation in novel interactive multimediaservices.
arxiv-1210-5196 | Matrix reconstruction with the local max norm |  http://arxiv.org/abs/1210.5196  | author:Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov category:stat.ML cs.LG published:2012-10-18 summary:We introduce a new family of matrix norms, the "local max" norms,generalizing existing methods such as the max norm, the trace norm (nuclearnorm), and the weighted or smoothed weighted trace norms, which have beenextensively used in the literature as regularizers for matrix reconstructionproblems. We show that this new family can be used to interpolate between the(weighted or unweighted) trace norm and the more conservative max norm. We testthis interpolation on simulated data and on the large-scale Netflix andMovieLens ratings data, and find improved accuracy relative to the existingmatrix norms. We also provide theoretical results showing learning guaranteesfor some of the new norms.
arxiv-1210-5135 | LSBN: A Large-Scale Bayesian Structure Learning Framework for Model Averaging |  http://arxiv.org/abs/1210.5135  | author:Yang Lu, Mengying Wang, Menglu Li, Qili Zhu, Bo Yuan category:cs.LG stat.ML published:2012-10-18 summary:The motivation for this paper is to apply Bayesian structure learning usingModel Averaging in large-scale networks. Currently, Bayesian model averagingalgorithm is applicable to networks with only tens of variables, restrained byits super-exponential complexity. We present a novel framework, calledLSBN(Large-Scale Bayesian Network), making it possible to handle networks withinfinite size by following the principle of divide-and-conquer. The method ofLSBN comprises three steps. In general, LSBN first performs the partition byusing a second-order partition strategy, which achieves more robust results.LSBN conducts sampling and structure learning within each overlapping communityafter the community is isolated from other variables by Markov Blanket. FinallyLSBN employs an efficient algorithm, to merge structures of overlappingcommunities into a whole. In comparison with other four state-of-artlarge-scale network structure learning algorithms such as ARACNE, PC, GreedySearch and MMHC, LSBN shows comparable results in five common benchmarkdatasets, evaluated by precision, recall and f-score. What's more, LSBN makesit possible to learn large-scale Bayesian structure by Model Averaging whichused to be intractable. In summary, LSBN provides an scalable and parallelframework for the reconstruction of network structures. Besides, the completeinformation of overlapping communities serves as the byproduct, which could beused to mine meaningful clusters in biological networks, such asprotein-protein-interaction network or gene regulatory network, as well as insocial network.
arxiv-1210-4792 | Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear Multivariate Regression and Granger Causality |  http://arxiv.org/abs/1210.4792  | author:Vikas Sindhwani, Minh Ha Quang, Aurelie C. Lozano category:stat.ML cs.LG published:2012-10-17 summary:We propose a general matrix-valued multiple kernel learning framework forhigh-dimensional nonlinear multivariate regression problems. This frameworkallows a broad class of mixed norm regularizers, including those that inducesparsity, to be imposed on a dictionary of vector-valued Reproducing KernelHilbert Spaces. We develop a highly scalable and eigendecomposition-freealgorithm that orchestrates two inexact solvers for simultaneously learningboth the input and output components of separable matrix-valued kernels. As akey application enabled by our framework, we show how high-dimensional causalinference tasks can be naturally cast as sparse function estimation problems,leading to novel nonlinear extensions of a class of Graphical Granger Causalitytechniques. Our algorithmic developments and extensive empirical studies arecomplemented by theoretical analyses in terms of Rademacher generalizationbounds.
arxiv-1210-4695 | Regulating the information in spikes: a useful bias |  http://arxiv.org/abs/1210.4695  | author:David Balduzzi category:q-bio.NC cs.IT cs.LG math.IT published:2012-10-17 summary:The bias/variance tradeoff is fundamental to learning: increasing a model'scomplexity can improve its fit on training data, but potentially worsensperformance on future samples. Remarkably, however, the human braineffortlessly handles a wide-range of complex pattern recognition tasks. On thebasis of these conflicting observations, it has been argued that useful biasesin the form of "generic mechanisms for representation" must be hardwired intocortex (Geman et al). This note describes a useful bias that encourages cooperative learning whichis both biologically plausible and rigorously justified.
arxiv-1210-4762 | Mixture model for designs in high dimensional regression and the LASSO |  http://arxiv.org/abs/1210.4762  | author:Stéphane Chrétien category:math.ST stat.ML stat.TH published:2012-10-17 summary:The LASSO is a recent technique for variable selection in the regressionmodel \bean y & = & X\beta +\epsilon, \eean where $X\in \R^{n\times p}$ and$\epsilon$ is a centered gaussian i.i.d. noise vector $\mathcalN(0,\sigma^2I)$. The LASSO has been proved to perform exact support recoveryfor regression vectors when the design matrix satisfies certain algebraicconditions and $\beta$ is sufficiently sparse. Estimation of the vector$X\beta$ has also extensively been studied for the purpose of prediction underthe same algebraic conditions on $X$ and under sufficient sparsity of $\beta$.Among many other, the coherence is an index which can be used to study thesenice properties of the LASSO. More precisely, a small coherence implies thatmost sparse vectors, with less nonzero components than the order $n/\log(p)$,can be recovered with high probability if its nonzero components are largerthan the order $\sigma \sqrt{\log(p)}$. However, many matrices occuring inpractice do not have a small coherence and thus, most results which haveappeared in the litterature cannot be applied. The goal of this paper is tostudy a model for which precise results can be obtained. In the proposed model,the columns of the design matrix are drawn from a Gaussian mixture model andthe coherence condition is imposed on the much smaller matrix whose columns arethe mixture's centers, instead of on $X$ itself. Our main theorem states that$X\beta$ is as well estimated as in the case of small coherence up to acorrection parametrized by the maximal variance in the mixture model.
arxiv-1210-4657 | Mean-Field Learning: a Survey |  http://arxiv.org/abs/1210.4657  | author:Hamidou Tembine, Raul Tempone, Pedro Vilanova category:cs.LG cs.GT cs.MA math.DS stat.ML published:2012-10-17 summary:In this paper we study iterative procedures for stationary equilibria ingames with large number of players. Most of learning algorithms for games withcontinuous action spaces are limited to strict contraction best reply maps inwhich the Banach-Picard iteration converges with geometrical convergence rate.When the best reply map is not a contraction, Ishikawa-based learning isproposed. The algorithm is shown to behave well for Lipschitz continuous andpseudo-contractive maps. However, the convergence rate is still unsatisfactory.Several acceleration techniques are presented. We explain how cognitive userscan improve the convergence rate based only on few number of measurements. Themethodology provides nice properties in mean field games where the payofffunction depends only on own-action and the mean of the mean-field (firstmoment mean-field games). A learning framework that exploits the structure ofsuch games, called, mean-field learning, is proposed. The proposed mean-fieldlearning framework is suitable not only for games but also for non-convexglobal optimization problems. Then, we introduce mean-field learning withoutfeedback and examine the convergence to equilibria in beauty contest games,which have interesting applications in financial markets. Finally, we provide afully distributed mean-field learning and its speedup versions for satisfactorysolution in wireless networks. We illustrate the convergence rate improvementwith numerical examples.
arxiv-1210-4768 | Justice blocks and predictability of US Supreme Court votes |  http://arxiv.org/abs/1210.4768  | author:Roger Guimera, Marta Sales-Pardo category:physics.soc-ph stat.ML published:2012-10-17 summary:Successful attempts to predict judges' votes shed light into how legaldecisions are made and, ultimately, into the behavior and evolution of thejudiciary. Here, we investigate to what extent it is possible to makepredictions of a justice's vote based on the other justices' votes in the samecase. For our predictions, we use models and methods that have been developedto uncover hidden associations between actors in complex social networks. Weshow that these methods are more accurate at predicting justice's votes thanforecasts made by legal experts and by algorithms that take into considerationthe content of the cases. We argue that, within our framework, highpredictability is a quantitative proxy for stable justice (and case) blocks,which probably reflect stable a priori attitudes toward the law. We find thatU. S. Supreme Court justice votes are more predictable than one would expectfrom an ideal court composed of perfectly independent justices. Deviations fromideal behavior are most apparent in divided 5-4 decisions, where justice blocksseem to be most stable. Moreover, we find evidence that justice predictabilitydecreased during the 50-year period spanning from the Warren Court to theRehnquist Court, and that aggregate court predictability has been significantlylower during Democratic presidencies. More broadly, our results show that it ispossible to use methods developed for the analysis of complex social networksto quantitatively investigate historical questions related to politicaldecision-making.
arxiv-1210-4601 | A Direct Approach to Multi-class Boosting and Extensions |  http://arxiv.org/abs/1210.4601  | author:Chunhua Shen, Sakrapee Paisitkriangkrai, Anton van den Hengel category:cs.LG published:2012-10-17 summary:Boosting methods combine a set of moderately accurate weaklearners to form ahighly accurate predictor. Despite the practical importance of multi-classboosting, it has received far less attention than its binary counterpart. Inthis work, we propose a fully-corrective multi-class boosting formulation whichdirectly solves the multi-class problem without dividing it into multiplebinary classification problems. In contrast, most previous multi-class boostingalgorithms decompose a multi-boost problem into multiple binary boostingproblems. By explicitly deriving the Lagrange dual of the primal optimizationproblem, we are able to construct a column generation-based fully-correctiveapproach to boosting which directly optimizes multi-class classificationperformance. The new approach not only updates all weak learners' coefficientsat every iteration, but does so in a manner flexible enough to accommodatevarious loss functions and regularizations. For example, it enables us tointroduce structural sparsity through mixed-norm regularization to promotegroup sparsity and feature sharing. Boosting with shared features isparticularly beneficial in complex prediction problems where features can beexpensive to compute. Our experiments on various data sets demonstrate that ourdirect multi-class boosting generalizes as well as, or better than, a range ofcompeting multi-class boosting methods. The end result is a highly effectiveand compact ensemble classifier which can be trained in a distributed fashion.
arxiv-1210-4347 | Hilbert Space Embedding for Dirichlet Process Mixtures |  http://arxiv.org/abs/1210.4347  | author:Krikamol Muandet category:stat.ML cs.LG published:2012-10-16 summary:This paper proposes a Hilbert space embedding for Dirichlet Process mixturemodels via a stick-breaking construction of Sethuraman. Although Bayesiannonparametrics offers a powerful approach to construct a prior that avoids theneed to specify the model size/complexity explicitly, an exact inference isoften intractable. On the other hand, frequentist approaches such as kernelmachines, which suffer from the model selection/comparison problems, oftenbenefit from efficient learning algorithms. This paper discusses thepossibility to combine the best of both worlds by using the Dirichlet Processmixture model as a case study.
arxiv-1210-4846 | Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation |  http://arxiv.org/abs/1210.4846  | author:Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht category:cs.LG stat.ML published:2012-10-16 summary:In recent years, non-parametric methods utilizing random walks on graphs havebeen used to solve a wide range of machine learning problems, but in theirsimplest form they do not scale well due to the quadratic complexity. In thispaper, a new dual-tree based variational approach for approximating thetransition matrix and efficiently performing the random walk is proposed. Theapproach exploits a connection between kernel density estimation, mixturemodeling, and random walk on graphs in an optimization of the transition matrixfor the data graph that ties together edge transitions probabilities that aresimilar. Compared to the de facto standard approximation method based onk-nearestneighbors, we demonstrate order of magnitudes speedup withoutsacrificing accuracy for Label Propagation tasks on benchmark data sets insemi-supervised learning.
arxiv-1210-4920 | Factorized Multi-Modal Topic Model |  http://arxiv.org/abs/1210.4920  | author:Seppo Virtanen, Yangqing Jia, Arto Klami, Trevor Darrell category:cs.LG cs.IR stat.ML published:2012-10-16 summary:Multi-modal data collections, such as corpora of paired images and textsnippets, require analysis methods beyond single-view component and topicmodels. For continuous observations the current dominant approach is based onextensions of canonical correlation analysis, factorizing the variation intocomponents shared by the different modalities and those private to each ofthem. For count data, multiple variants of topic models attempting to tie themodalities together have been presented. All of these, however, lack theability to learn components private to one modality, and consequently will tryto force dependencies even between minimally correlating modalities. In thiswork we combine the two approaches by presenting a novel HDP-based topic modelthat automatically learns both shared and private topics. The model is shown tobe especially useful for querying the contents of one domain given samples ofthe other.
arxiv-1210-4917 | Fast Graph Construction Using Auction Algorithm |  http://arxiv.org/abs/1210.4917  | author:Jun Wang, Yinglong Xia category:cs.LG stat.ML published:2012-10-16 summary:In practical machine learning systems, graph based data representation hasbeen widely used in various learning paradigms, ranging from unsupervisedclustering to supervised classification. Besides those applications withnatural graph or network structure data, such as social network analysis andrelational learning, many other applications often involve a critical step inconverting data vectors to an adjacency graph. In particular, a sparse subgraphextracted from the original graph is often required due to both theoretic andpractical needs. Previous study clearly shows that the performance of differentlearning algorithms, e.g., clustering and classification, benefits from suchsparse subgraphs with balanced node connectivity. However, the existing graphconstruction methods are either computationally expensive or withunsatisfactory performance. In this paper, we utilize a scalable method calledauction algorithm and its parallel extension to recover a sparse yet nearlybalanced subgraph with significantly reduced computational cost. Empiricalstudy and comparison with the state-ofart approaches clearly demonstrate thesuperiority of the proposed method in both efficiency and accuracy.
arxiv-1210-4914 | Latent Structured Ranking |  http://arxiv.org/abs/1210.4914  | author:Jason Weston, John Blitzer category:cs.LG cs.IR stat.ML published:2012-10-16 summary:Many latent (factorized) models have been proposed for recommendation taskslike collaborative filtering and for ranking tasks like document or imageretrieval and annotation. Common to all those methods is that during inferencethe items are scored independently by their similarity to the query in thelatent embedding space. The structure of the ranked list (i.e. considering theset of items returned as a whole) is not taken into account. This can be aproblem because the set of top predictions can be either too diverse (containresults that contradict each other) or are not diverse enough. In this paper weintroduce a method for learning latent structured rankings that improves overexisting methods by providing the right blend of predictions at the top of theranked list. Particular emphasis is put on making this method scalable.Empirical results on large scale image annotation and music recommendationtasks show improvements over existing approaches.
arxiv-1210-4913 | An Improved Admissible Heuristic for Learning Optimal Bayesian Networks |  http://arxiv.org/abs/1210.4913  | author:Changhe Yuan, Brandon Malone category:cs.AI cs.LG stat.ML published:2012-10-16 summary:Recently two search algorithms, A* and breadth-first branch and bound(BFBnB), were developed based on a simple admissible heuristic for learningBayesian network structures that optimize a scoring function. The heuristicrepresents a relaxation of the learning problem such that each variable choosesoptimal parents independently. As a result, the heuristic may contain manydirected cycles and result in a loose bound. This paper introduces an improvedadmissible heuristic that tries to avoid directed cycles within small groups ofvariables. A sparse representation is also introduced to store only the uniqueoptimal parent choices. Empirical results show that the new techniquessignificantly improved the efficiency and scalability of A* and BFBnB on mostof datasets tested in this paper.
arxiv-1210-4910 | New Advances and Theoretical Insights into EDML |  http://arxiv.org/abs/1210.4910  | author:Khaled S. Refaat, Arthur Choi, Adnan Darwiche category:cs.AI cs.LG stat.ML published:2012-10-16 summary:EDML is a recently proposed algorithm for learning MAP parameters in Bayesiannetworks. In this paper, we present a number of new advances and insights onthe EDML algorithm. First, we provide the multivalued extension of EDML,originally proposed for Bayesian networks over binary variables. Next, weidentify a simplified characterization of EDML that further implies a simplefixed-point algorithm for the convex optimization problem that underlies it.This characterization further reveals a connection between EDML and EM: a fixedpoint of EDML is a fixed point of EM, and vice versa. We thus identify also anew characterization of EM fixed points, but in the semantics of EDML. Finally,we propose a hybrid EDML/EM algorithm that takes advantage of the improvedempirical convergence behavior of EDML, while maintaining the monotonicimprovement property of EM.
arxiv-1210-4909 | Active Learning with Distributional Estimates |  http://arxiv.org/abs/1210.4909  | author:Jens Roeder, Boaz Nadler, Kevin Kunzmann, Fred A. Hamprecht category:cs.LG stat.ML published:2012-10-16 summary:Active Learning (AL) is increasingly important in a broad range ofapplications. Two main AL principles to obtain accurate classification with fewlabeled data are refinement of the current decision boundary and exploration ofpoorly sampled regions. In this paper we derive a novel AL scheme that balancesthese two principles in a natural way. In contrast to many AL strategies, whichare based on an estimated class conditional probability ^p(yx), a keycomponent of our approach is to view this quantity as a random variable, henceexplicitly considering the uncertainty in its estimated value. Our maincontribution is a novel mathematical framework for uncertainty-based AL, and acorresponding AL scheme, where the uncertainty in ^p(yx) is modeled by asecond-order distribution. On the practical side, we show how to approximatesuch second-order distributions for kernel density classification. Finally, wefind that over a large number of UCI, USPS and Caltech4 datasets, our AL schemeachieves significantly better learning curves than popular AL methods such asuncertainty sampling and error reduction sampling, when all use the same kerneldensity classifier.
arxiv-1210-4905 | Latent Composite Likelihood Learning for the Structured Canonical Correlation Model |  http://arxiv.org/abs/1210.4905  | author:Ricardo Silva category:stat.ML cs.LG published:2012-10-16 summary:Latent variable models are used to estimate variables of interest quantitieswhich are observable only up to some measurement error. In many studies, suchvariables are known but not precisely quantifiable (such as "job satisfaction"in social sciences and marketing, "analytical ability" in educational testing,or "inflation" in economics). This leads to the development of measurementinstruments to record noisy indirect evidence for such unobserved variablessuch as surveys, tests and price indexes. In such problems, there arepostulated latent variables and a given measurement model. At the same time,other unantecipated latent variables can add further unmeasured confounding tothe observed variables. The problem is how to deal with unantecipated latentsvariables. In this paper, we provide a method loosely inspired by canonicalcorrelation that makes use of background information concerning the "known"latent variables. Given a partially specified structure, it provides astructure learning approach to detect "unknown unknowns," the confoundingeffect of potentially infinitely many other latent variables. This is donewithout explicitly modeling such extra latent factors. Because of the specialstructure of the problem, we are able to exploit a new variation of compositelikelihood fitting to efficiently learn this structure. Validation is providedwith experiments in synthetic data and the analysis of a large survey done witha sample of over 100,000 staff members of the National Health Service of theUnited Kingdom.
arxiv-1210-4902 | Efficiently Searching for Frustrated Cycles in MAP Inference |  http://arxiv.org/abs/1210.4902  | author:David Sontag, Do Kook Choe, Yitao Li category:cs.DS cs.LG stat.ML published:2012-10-16 summary:Dual decomposition provides a tractable framework for designing algorithmsfor finding the most probable (MAP) configuration in graphical models. However,for many real-world inference problems, the typical decomposition has a largeintegrality gap, due to frustrated cycles. One way to tighten the relaxation isto introduce additional constraints that explicitly enforce cycle consistency.Earlier work showed that cluster-pursuit algorithms, which iterativelyintroduce cycle and other higherorder consistency constraints, allows one toexactly solve many hard inference problems. However, these algorithmsexplicitly enumerate a candidate set of clusters, limiting them to triplets orother short cycles. We solve the search problem for cycle constraints, giving anearly linear time algorithm for finding the most frustrated cycle of arbitrarylength. We show how to use this search algorithm together with the dualdecomposition framework and clusterpursuit. The new algorithm exactly solvesMAP inference problems arising from relational classification and stereovision.
arxiv-1210-4899 | Fast Exact Inference for Recursive Cardinality Models |  http://arxiv.org/abs/1210.4899  | author:Daniel Tarlow, Kevin Swersky, Richard S. Zemel, Ryan Prescott Adams, Brendan J. Frey category:cs.LG stat.ML published:2012-10-16 summary:Cardinality potentials are a generally useful class of high order potentialthat affect probabilities based on how many of D binary variables are active.Maximum a posteriori (MAP) inference for cardinality potential models iswell-understood, with efficient computations taking O(DlogD) time. Yetefficient marginalization and sampling have not been addressed as thoroughly inthe machine learning community. We show that there exists a simple algorithmfor computing marginal probabilities and drawing exact joint samples that runsin O(Dlog2 D) time, and we show how to frame the algorithm as efficient beliefpropagation in a low order tree-structured model that includes additionalauxiliary variables. We then develop a new, more general class of models,termed Recursive Cardinality models, which take advantage of this efficiency.Finally, we show how to do efficient exact inference in models composed of atree structure and a cardinality potential. We explore the expressive power ofRecursive Cardinality models and empirically demonstrate their utility.
arxiv-1210-4898 | Value Function Approximation in Noisy Environments Using Locally Smoothed Regularized Approximate Linear Programs |  http://arxiv.org/abs/1210.4898  | author:Gavin Taylor, Ron Parr category:cs.LG stat.ML published:2012-10-16 summary:Recently, Petrik et al. demonstrated that L1Regularized Approximate LinearProgramming (RALP) could produce value functions and policies which comparedfavorably to established linear value function approximation techniques likeLSPI. RALP's success primarily stems from the ability to solve the featureselection and value function approximation steps simultaneously. RALP'sperformance guarantees become looser if sampled next states are used. For verynoisy domains, RALP requires an accurate model rather than samples, which canbe unrealistic in some practical scenarios. In this paper, we demonstrate thisweakness, and then introduce Locally Smoothed L1-Regularized Approximate LinearProgramming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuraciesstemming from noise even without an accurate model. We show that, given somesmoothness assumptions, as the number of samples increases, error from noiseapproaches zero, and provide experimental examples of LS-RALP's success oncommon reinforcement learning benchmark problems.
arxiv-1210-4896 | Closed-Form Learning of Markov Networks from Dependency Networks |  http://arxiv.org/abs/1210.4896  | author:Daniel Lowd category:cs.LG cs.AI stat.ML published:2012-10-16 summary:Markov networks (MNs) are a powerful way to compactly represent a jointprobability distribution, but most MN structure learning methods are very slow,due to the high cost of evaluating candidates structures. Dependency networks(DNs) represent a probability distribution as a set of conditional probabilitydistributions. DNs are very fast to learn, but the conditional distributionsmay be inconsistent with each other and few inference algorithms support DNs.In this paper, we present a closed-form method for converting a DN into an MN,allowing us to enjoy both the efficiency of DN learning and the convenience ofthe MN representation. When the DN is consistent, this conversion is exact. Forinconsistent DNs, we present averaging methods that significantly improve theapproximation. In experiments on 12 standard datasets, our methods are ordersof magnitude faster than and often more accurate than combining conditionaldistributions using weight learning.
arxiv-1210-4893 | Sparse Q-learning with Mirror Descent |  http://arxiv.org/abs/1210.4893  | author:Sridhar Mahadevan, Bo Liu category:cs.LG stat.ML published:2012-10-16 summary:This paper explores a new framework for reinforcement learning based ononline convex optimization, in particular mirror descent and relatedalgorithms. Mirror descent can be viewed as an enhanced gradient method,particularly suited to minimization of convex functions in highdimensionalspaces. Unlike traditional gradient methods, mirror descent undertakes gradientupdates of weights in both the dual space and primal space, which are linkedtogether using a Legendre transform. Mirror descent can be viewed as a proximalalgorithm where the distance generating function used is a Bregman divergence.A new class of proximal-gradient based temporal-difference (TD) methods arepresented based on different Bregman divergences, which are more powerful thanregular TD learning. Examples of Bregman divergences that are studied includep-norm functions, and Mahalanobis distance based on the covariance of samplegradients. A new family of sparse mirror-descent reinforcement learning methodsare proposed, which are able to find sparse fixed points of an l1-regularizedBellman equation at significantly less computational cost than previous methodsbased on second-order matrix methods. An experimental study of mirror-descentreinforcement learning is presented using discrete and continuous Markovdecision processes.
arxiv-1210-4892 | Unsupervised Joint Alignment and Clustering using Bayesian Nonparametrics |  http://arxiv.org/abs/1210.4892  | author:Marwan A. Mattar, Allen R. Hanson, Erik G. Learned-Miller category:cs.LG stat.ML published:2012-10-16 summary:Joint alignment of a collection of functions is the process of independentlytransforming the functions so that they appear more similar to each other.Typically, such unsupervised alignment algorithms fail when presented withcomplex data sets arising from multiple modalities or make restrictiveassumptions about the form of the functions or transformations, limiting theirgenerality. We present a transformed Bayesian infinite mixture model that cansimultaneously align and cluster a data set. Our model and associated learningscheme offer two key advantages: the optimal number of clusters is determinedin a data-driven fashion through the use of a Dirichlet process prior, and itcan accommodate any transformation function parameterized by a continuousparameter vector. As a result, it is applicable to a wide range of data types,and transformation functions. We present positive results on synthetictwo-dimensional data, on a set of one-dimensional curves, and on various imagedata sets, showing large improvements over previous work. We discuss severalvariations of the model and conclude with directions for future work.
arxiv-1211-1252 | Implementation of Radon Transformation for Electrical Impedance Tomography (EIT) |  http://arxiv.org/abs/1211.1252  | author:Md. Ali Hossain, Ahsan-Ul-Ambia, Md. Aktaruzzaman, Md. Ahaduzzaman Khan category:cs.CV published:2012-10-16 summary:Radon Transformation is generally used to construct optical image (like CTimage) from the projection data in biomedical imaging. In this paper, theconcept of Radon Transformation is implemented to reconstruct ElectricalImpedance Topographic Image (conductivity or resistivity distribution) of acircular subject. A parallel resistance model of a subject is proposed forElectrical Impedance Topography(EIT) or Magnetic Induction Tomography(MIT). Acircular subject with embedded circular objects is segmented into equal widthslices from different angles. For each angle, Conductance and Conductivity ofeach slice is calculated and stored in an array. A back projection method isused to generate a two-dimensional image from one-dimensional projections. As aback projection method, Inverse Radon Transformation is applied on thecalculated conductance and conductivity to reconstruct two dimensional images.These images are compared to the target image. In the time of imagereconstruction, different filters are used and these images are compared witheach other and target image.
arxiv-1210-4889 | Learning STRIPS Operators from Noisy and Incomplete Observations |  http://arxiv.org/abs/1210.4889  | author:Kira Mourao, Luke S. Zettlemoyer, Ronald P. A. Petrick, Mark Steedman category:cs.LG cs.AI stat.ML published:2012-10-16 summary:Agents learning to act autonomously in real-world domains must acquire amodel of the dynamics of the domain in which they operate. Learning domaindynamics can be challenging, especially where an agent only has partial accessto the world state, and/or noisy external sensors. Even in standard STRIPSdomains, existing approaches cannot learn from noisy, incomplete observationstypical of real-world domains. We propose a method which learns STRIPS actionmodels in such domains, by decomposing the problem into first learning atransition function between states in the form of a set of classifiers, andthen deriving explicit STRIPS rules from the classifiers' parameters. Weevaluate our approach on simulated standard planning domains from theInternational Planning Competition, and show that it learns useful domaindescriptions from noisy, incomplete observations.
arxiv-1210-4276 | Semi-Supervised Classification Through the Bag-of-Paths Group Betweenness |  http://arxiv.org/abs/1210.4276  | author:Bertrand Lebichot, Ilkka Kivimäki, Kevin Françoisse, Marco Saerens category:stat.ML cs.LG published:2012-10-16 summary:This paper introduces a novel, well-founded, betweenness measure, called theBag-of-Paths (BoP) betweenness, as well as its extension, the BoP groupbetweenness, to tackle semisupervised classification problems on weighteddirected graphs. The objective of semi-supervised classification is to assign alabel to unlabeled nodes using the whole topology of the graph and the labelednodes at our disposal. The BoP betweenness relies on a bag-of-paths frameworkassigning a Boltzmann distribution on the set of all possible paths through thenetwork such that long (high-cost) paths have a low probability of being pickedfrom the bag, while short (low-cost) paths have a high probability of beingpicked. Within that context, the BoP betweenness of node j is defined as thesum of the a posteriori probabilities that node j lies in-between two arbitrarynodes i, k, when picking a path starting in i and ending in k. Intuitively, anode typically receives a high betweenness if it has a large probability ofappearing on paths connecting two arbitrary nodes of the network. This quantitycan be computed in closed form by inverting a n x n matrix where n is thenumber of nodes. For the group betweenness, the paths are constrained to startand end in nodes within the same class, therefore defining a group betweennessfor each class. Unlabeled nodes are then classified according to the classshowing the highest group betweenness. Experiments on various real-world datasets show that BoP group betweenness outperforms all the tested stateof-the-art methods. The benefit of the BoP betweenness is particularlynoticeable when only a few labeled nodes are available.
arxiv-1210-4888 | Local Structure Discovery in Bayesian Networks |  http://arxiv.org/abs/1210.4888  | author:Teppo Niinimaki, Pekka Parviainen category:cs.LG cs.AI stat.ML published:2012-10-16 summary:Learning a Bayesian network structure from data is an NP-hard problem andthus exact algorithms are feasible only for small data sets. Therefore, networkstructures for larger networks are usually learned with various heuristics.Another approach to scaling up the structure learning is local learning. Inlocal learning, the modeler has one or more target variables that are ofspecial interest; he wants to learn the structure near the target variables andis not interested in the rest of the variables. In this paper, we present ascore-based local learning algorithm called SLL. We conjecture that ouralgorithm is theoretically sound in the sense that it is optimal in the limitof large sample size. Empirical results suggest that SLL is competitive whencompared to the constraint-based HITON algorithm. We also study the prospectsof constructing the network structure for the whole node set based on localresults by presenting two algorithms and comparing them to several heuristics.
arxiv-1210-4887 | Hilbert Space Embeddings of POMDPs |  http://arxiv.org/abs/1210.4887  | author:Yu Nishiyama, Abdeslam Boularias, Arthur Gretton, Kenji Fukumizu category:cs.LG cs.AI stat.ML published:2012-10-16 summary:A nonparametric approach for policy learning for POMDPs is proposed. Theapproach represents distributions over the states, observations, and actions asembeddings in feature spaces, which are reproducing kernel Hilbert spaces.Distributions over states given the observations are obtained by applying thekernel Bayes' rule to these distribution embeddings. Policies and valuefunctions are defined on the feature space over states, which leads to afeature space expression for the Bellman equation. Value iteration may then beused to estimate the optimal value function and associated policy. Experimentalresults confirm that the correct policy is learned using the feature spacerepresentation.
arxiv-1210-4854 | Semantic Understanding of Professional Soccer Commentaries |  http://arxiv.org/abs/1210.4854  | author:Hannaneh Hajishirzi, Mohammad Rastegari, Ali Farhadi, Jessica K. Hodgins category:cs.CL cs.AI published:2012-10-16 summary:This paper presents a novel approach to the problem of semantic parsing vialearning the correspondences between complex sentences and rich sets of events.Our main intuition is that correct correspondences tend to occur morefrequently. Our model benefits from a discriminative notion of similarity tolearn the correspondence between sentence and an event and a ranking machinerythat scores the popularity of each correspondence. Our method can discover agroup of events (called macro-events) that best describes a sentence. Weevaluate our method on our novel dataset of professional soccer commentaries.The empirical results show that our method significantly outperforms thestate-of-theart.
arxiv-1210-4884 | A Spectral Algorithm for Latent Junction Trees |  http://arxiv.org/abs/1210.4884  | author:Ankur P. Parikh, Le Song, Mariya Ishteva, Gabi Teodoru, Eric P. Xing category:cs.LG stat.ML published:2012-10-16 summary:Latent variable models are an elegant framework for capturing richprobabilistic dependencies in many applications. However, current approachestypically parametrize these models using conditional probability tables, andlearning relies predominantly on local search heuristics such as ExpectationMaximization. Using tensor algebra, we propose an alternative parameterizationof latent variable models (where the model structures are junction trees) thatstill allows for computation of marginals among observed variables. While thisnovel representation leads to a moderate increase in the number of parametersfor junction trees of low treewidth, it lets us design a local-minimum-freealgorithm for learning this parameterization. The main computation of thealgorithm involves only tensor operations and SVDs which can be orders ofmagnitude faster than EM algorithms for large datasets. To our knowledge, thisis the first provably consistent parameter learning technique for a large classof low-treewidth latent graphical models beyond trees. We demonstrate theadvantages of our method on synthetic and real datasets.
arxiv-1210-4883 | A Model-Based Approach to Rounding in Spectral Clustering |  http://arxiv.org/abs/1210.4883  | author:Leonard K. M. Poon, April H. Liu, Tengfei Liu, Nevin Lianwen Zhang category:cs.LG cs.NA stat.ML published:2012-10-16 summary:In spectral clustering, one defines a similarity matrix for a collection ofdata points, transforms the matrix to get the Laplacian matrix, finds theeigenvectors of the Laplacian matrix, and obtains a partition of the data usingthe leading eigenvectors. The last step is sometimes referred to as rounding,where one needs to decide how many leading eigenvectors to use, to determinethe number of clusters, and to partition the data points. In this paper, wepropose a novel method for rounding. The method differs from previous methodsin three ways. First, we relax the assumption that the number of clustersequals the number of eigenvectors used. Second, when deciding the number ofleading eigenvectors to use, we not only rely on information contained in theleading eigenvectors themselves, but also use subsequent eigenvectors. Third,our method is model-based and solves all the three subproblems of roundingusing a class of graphical models called latent tree models. We evaluate ourmethod on both synthetic and real-world data. The results show that our methodworks correctly in the ideal case where between-clusters similarity is 0, anddegrades gracefully as one moves away from the ideal case.
arxiv-1210-4881 | Tightening Fractional Covering Upper Bounds on the Partition Function for High-Order Region Graphs |  http://arxiv.org/abs/1210.4881  | author:Tamir Hazan, Jian Peng, Amnon Shashua category:cs.LG stat.ML published:2012-10-16 summary:In this paper we present a new approach for tightening upper bounds on thepartition function. Our upper bounds are based on fractional covering bounds onthe entropy function, and result in a concave program to compute these boundsand a convex program to tighten them. To solve these programs effectively forgeneral region graphs we utilize the entropy barrier method, thus decomposingthe original programs by their dual programs and solve them with dual blockoptimization scheme. The entropy barrier method provides an elegant frameworkto generalize the message-passing scheme to high-order region graph, as well asto solve the block dual steps in closed-form. This is a key for computationalrelevancy for large problems with thousands of regions.
arxiv-1210-4880 | Inferring Strategies from Limited Reconnaissance in Real-time Strategy Games |  http://arxiv.org/abs/1210.4880  | author:Jesse Hostetler, Ethan W. Dereszynski, Thomas G. Dietterich, Alan Fern category:cs.AI cs.GT cs.LG published:2012-10-16 summary:In typical real-time strategy (RTS) games, enemy units are visible only whenthey are within sight range of a friendly unit. Knowledge of an opponent'sdisposition is limited to what can be observed through scouting. Information iscostly, since units dedicated to scouting are unavailable for other purposes,and the enemy will resist scouting attempts. It is important to infer as muchas possible about the opponent's current and future strategy from the availableobservations. We present a dynamic Bayes net model of strategies in the RTSgame Starcraft that combines a generative model of how strategies relate toobservable quantities with a principled framework for incorporating evidencegained via scouting. We demonstrate the model's ability to infer unobservedaspects of the game from realistic observations.
arxiv-1210-4850 | Markov Determinantal Point Processes |  http://arxiv.org/abs/1210.4850  | author:Raja Hafiz Affandi, Alex Kulesza, Emily B. Fox category:cs.LG cs.IR stat.ML published:2012-10-16 summary:A determinantal point process (DPP) is a random process useful for modelingthe combinatorial problem of subset selection. In particular, DPPs encourage arandom subset Y to contain a diverse set of items selected from a base set Y.For example, we might use a DPP to display a set of news headlines that arerelevant to a user's interests while covering a variety of topics. Suppose,however, that we are asked to sequentially select multiple diverse sets ofitems, for example, displaying new headlines day-by-day. We might want thesesets to be diverse not just individually but also through time, offeringheadlines today that are unlike the ones shown yesterday. In this paper, weconstruct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. Theproposed M-DPP defines a stationary process that maintains DPP margins.Crucially, the induced union process Zt = Yt u Yt-1 is also marginallyDPP-distributed. Jointly, these properties imply that the sequence of randomsets are encouraged to be diverse both at a given time step as well as acrosstime steps. We describe an exact, efficient sampling procedure, and a methodfor incrementally learning a quality measure over items in the base set Y basedon external preferences. We apply the M-DPP to the task of sequentiallydisplaying diverse and relevant news articles to a user with topic preferences.
arxiv-1210-4879 | Causal Discovery of Linear Cyclic Models from Multiple Experimental Data Sets with Overlapping Variables |  http://arxiv.org/abs/1210.4879  | author:Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer category:stat.ME cs.AI stat.ML published:2012-10-16 summary:Much of scientific data is collected as randomized experiments intervening onsome and observing other variables of interest. Quite often, a given phenomenonis investigated in several studies, and different sets of variables areinvolved in each study. In this article we consider the problem of integratingsuch knowledge, inferring as much as possible concerning the underlying causalstructure with respect to the union of observed variables from suchexperimental or passive observational overlapping data sets. We do not assumeacyclicity or joint causal sufficiency of the underlying data generating model,but we do restrict the causal relationships to be linear and use only secondorder statistics of the data. We derive conditions for full modelidentifiability in the most generic case, and provide novel techniques forincorporating an assumption of faithfulness to aid in inference. In each casewe seek to establish what is and what is not determined by the data at hand.
arxiv-1210-4851 | Learning to Rank With Bregman Divergences and Monotone Retargeting |  http://arxiv.org/abs/1210.4851  | author:Sreangsu Acharyya, Oluwasanmi Koyejo, Joydeep Ghosh category:cs.LG stat.ML published:2012-10-16 summary:This paper introduces a novel approach for learning to rank (LETOR) based onthe notion of monotone retargeting. It involves minimizing a divergence betweenall monotonic increasing transformations of the training scores and aparameterized prediction function. The minimization is both over thetransformations as well as over the parameters. It is applied to Bregmandivergences, a large class of "distance like" functions that were recentlyshown to be the unique class that is statistically consistent with thenormalized discounted gain (NDCG) criterion [19]. The algorithm usesalternating projection style updates, in which one set of simultaneousprojections can be computed independent of the Bregman divergence and the otherreduces to parameter estimation of a generalized linear model. This results ineasily implemented, efficiently parallelizable algorithm for the LETOR taskthat enjoys global optimum guarantees under mild conditions. We presentempirical results on benchmark datasets showing that this approach canoutperform the state of the art NDCG consistent techniques.
arxiv-1210-4876 | Active Imitation Learning via Reduction to I.I.D. Active Learning |  http://arxiv.org/abs/1210.4876  | author:Kshitij Judah, Alan Fern, Thomas G. Dietterich category:cs.LG stat.ML published:2012-10-16 summary:In standard passive imitation learning, the goal is to learn a target policyby passively observing full execution trajectories of it. Unfortunately,generating such trajectories can require substantial expert effort and beimpractical in some cases. In this paper, we consider active imitation learningwith the goal of reducing this effort by querying the expert about the desiredaction at individual states, which are selected based on answers to pastqueries and the learner's interactions with an environment simulator. Weintroduce a new approach based on reducing active imitation learning to i.i.d.active learning, which can leverage progress in the i.i.d. setting. Our firstcontribution, is to analyze reductions for both non-stationary and stationarypolicies, showing that the label complexity (number of queries) of activeimitation learning can be substantially less than passive learning. Our secondcontribution, is to introduce a practical algorithm inspired by the reductions,which is shown to be highly effective in four test domains compared to a numberof alternatives.
arxiv-1210-4856 | Exploiting compositionality to explore a large space of model structures |  http://arxiv.org/abs/1210.4856  | author:Roger Grosse, Ruslan R Salakhutdinov, William T. Freeman, Joshua B. Tenenbaum category:cs.LG stat.ML published:2012-10-16 summary:The recent proliferation of richly structured probabilistic models raises thequestion of how to automatically determine an appropriate model for a dataset.We investigate this question for a space of matrix decomposition models whichcan express a variety of widely used models from unsupervised learning. Toenable model selection, we organize these models into a context-free grammarwhich generates a wide variety of structures through the compositionalapplication of a few simple rules. We use our grammar to generically andefficiently infer latent components and estimate predictive likelihood fornearly 2500 structures using a small toolbox of reusable algorithms. Using agreedy search over our grammar, we automatically choose the decompositionstructure from raw data by evaluating only a small fraction of all models. Theproposed method typically finds the correct structure for synthetic data andbacks off gracefully to simpler models under heavy noise. It learns sensiblestructures for datasets as diverse as image patches, motion capture, 20Questions, and U.S. Senate votes, all using exactly the same code.
arxiv-1210-4872 | Nested Dictionary Learning for Hierarchical Organization of Imagery and Text |  http://arxiv.org/abs/1210.4872  | author:Lingbo Li, XianXing Zhang, Mingyuan Zhou, Lawrence Carin category:cs.LG cs.CV stat.ML published:2012-10-16 summary:A tree-based dictionary learning model is developed for joint analysis ofimagery and associated text. The dictionary learning may be applied directly tothe imagery from patches, or to general feature vectors extracted from patchesor superpixels (using any existing method for image feature extraction). Eachimage is associated with a path through the tree (from root to a leaf), andeach of the multiple patches in a given image is associated with one node inthat path. Nodes near the tree root are shared between multiple paths,representing image characteristics that are common among different types ofimages. Moving toward the leaves, nodes become specialized, representingdetails in image classes. If available, words (text) are also jointly modeled,with a path-dependent probability over words. The tree structure is inferredvia a nested Dirichlet process, and a retrospective stick-breaking sampler isused to infer the tree depth and width.
arxiv-1210-4871 | Learning Mixtures of Submodular Shells with Application to Document Summarization |  http://arxiv.org/abs/1210.4871  | author:Hui Lin, Jeff A. Bilmes category:cs.LG cs.CL cs.IR stat.ML published:2012-10-16 summary:We introduce a method to learn a mixture of submodular "shells" in alarge-margin setting. A submodular shell is an abstract submodular functionthat can be instantiated with a ground set and a set of parameters to produce asubmodular function. A mixture of such shells can then also be so instantiatedto produce a more complex submodular function. What our algorithm learns arethe mixture weights over such shells. We provide a risk bound guarantee whenlearning in a large-margin structured-prediction setting using a projectedsubgradient method when only approximate submodular optimization is possible(such as with submodular function maximization). We apply this method to theproblem of multi-document summarization and produce the best results reportedso far on the widely used NIST DUC-05 through DUC-07 document summarizationcorpora.
arxiv-1210-4870 | Crowdsourcing Control: Moving Beyond Multiple Choice |  http://arxiv.org/abs/1210.4870  | author:Christopher H. Lin, Mausam, Daniel Weld category:cs.AI cs.LG published:2012-10-16 summary:To ensure quality results from crowdsourced tasks, requesters often aggregateworker responses and use one of a plethora of strategies to infer the correctanswer from the set of noisy responses. However, all current models assumeprior knowledge of all possible outcomes of the task. While not an unreasonableassumption for tasks that can be posited as multiple-choice questions (e.g.n-ary classification), we observe that many tasks do not naturally fit thisparadigm, but instead demand a free-response formulation where the outcomespace is of infinite size (e.g. audio transcription). We model such tasks witha novel probabilistic graphical model, and design and implement LazySusan, adecision-theoretic controller that dynamically requests responses as necessaryin order to infer answers to these tasks. We also design an EM algorithm tojointly learn the parameters of our model while inferring the correct answersto multiple tasks at a time. Live experiments on Amazon Mechanical Turkdemonstrate the superiority of LazySusan at solving SAT Math questions,eliminating 83.2% of the error and achieving greater net utility compared tothe state-ofthe-art strategy, majority-voting. We also show in live experimentsthat our EM algorithm outperforms majority-voting on a visualization task thatwe design.
arxiv-1210-4859 | Mechanism Design for Cost Optimal PAC Learning in the Presence of Strategic Noisy Annotators |  http://arxiv.org/abs/1210.4859  | author:Dinesh Garg, Sourangshu Bhattacharya, S. Sundararajan, Shirish Shevade category:cs.LG cs.GT stat.ML published:2012-10-16 summary:We consider the problem of Probably Approximate Correct (PAC) learning of abinary classifier from noisy labeled examples acquired from multiple annotators(each characterized by a respective classification noise rate). First, weconsider the complete information scenario, where the learner knows the noiserates of all the annotators. For this scenario, we derive sample complexitybound for the Minimum Disagreement Algorithm (MDA) on the number of labeledexamples to be obtained from each annotator. Next, we consider the incompleteinformation scenario, where each annotator is strategic and holds therespective noise rate as a private information. For this scenario, we design acost optimal procurement auction mechanism along the lines of Myerson's optimalauction design framework in a non-trivial manner. This mechanism satisfiesincentive compatibility property, thereby facilitating the learner to elicittrue noise rates of all the annotators.
arxiv-1210-4869 | Response Aware Model-Based Collaborative Filtering |  http://arxiv.org/abs/1210.4869  | author:Guang Ling, Haiqin Yang, Michael R. Lyu, Irwin King category:cs.LG cs.IR stat.ML published:2012-10-16 summary:Previous work on recommender systems mainly focus on fitting the ratingsprovided by users. However, the response patterns, i.e., some items are ratedwhile others not, are generally ignored. We argue that failing to observe suchresponse patterns can lead to biased parameter estimation and sub-optimal modelperformance. Although several pieces of work have tried to model users'response patterns, they miss the effectiveness and interpretability of thesuccessful matrix factorization collaborative filtering approaches. To bridgethe gap, in this paper, we unify explicit response models and PMF to establishthe Response Aware Probabilistic Matrix Factorization (RAPMF) framework. Weshow that RAPMF subsumes PMF as a special case. Empirically we demonstrate themerits of RAPMF from various aspects.
arxiv-1210-4867 | Lifted Relational Variational Inference |  http://arxiv.org/abs/1210.4867  | author:Jaesik Choi, Eyal Amir category:cs.LG stat.ML published:2012-10-16 summary:Hybrid continuous-discrete models naturally represent many real-worldapplications in robotics, finance, and environmental engineering. Inferencewith large-scale models is challenging because relational structuresdeteriorate rapidly during inference with observations. The main contributionof this paper is an efficient relational variational inference algorithm thatfactors largescale probability models into simpler variational models, composedof mixtures of iid (Bernoulli) random variables. The algorithm takesprobability relational models of largescale hybrid systems and converts them toa close-to-optimal variational models. Then, it efficiently calculates marginalprobabilities on the variational models by using a latent (or lifted) variableelimination or a lifted stochastic sampling. This inference is unique becauseit maintains the relational structure upon individual observations and duringinference steps.
arxiv-1210-4863 | DBN-Based Combinatorial Resampling for Articulated Object Tracking |  http://arxiv.org/abs/1210.4863  | author:Severine Dubuisson, Christophe Gonzales, Xuan Son NGuyen category:cs.CV published:2012-10-16 summary:Particle Filter is an effective solution to track objects in video sequencesin complex situations. Its key idea is to estimate the density over thepossible states of the object using a weighted sample whose elements are calledparticles. One of its crucial step is a resampling step in which particles areresampled to avoid some degeneracy problem. In this paper, we introduce a newresampling method called Combinatorial Resampling that exploits some featuresof articulated objects to resample over an implicitly created sample of anexponential size better representing the density to estimate. We prove that itis sound and, through experimentations both on challenging synthetic and realvideo sequences, we show that it outperforms all classical resampling methodsboth in terms of the quality of its results and in terms of response times.
arxiv-1210-4862 | Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits |  http://arxiv.org/abs/1210.4862  | author:Miroslav Dudik, Dumitru Erhan, John Langford, Lihong Li category:cs.LG stat.ML published:2012-10-16 summary:We present and prove properties of a new offline policy evaluator for anexploration learning setting which is superior to previous evaluators. Inparticular, it simultaneously and correctly incorporates techniques fromimportance weighting, doubly robust evaluation, and nonstationary policyevaluation approaches. In addition, our approach allows generating longerhistories by careful control of a bias-variance tradeoff, and further decreasesvariance by incorporating information about randomness of the target policy.Empirical evidence from synthetic and realworld exploration learning problemsshows the new evaluator successfully unifies previous approaches and usesinformation an order of magnitude more efficiently.
arxiv-1210-4839 | Leveraging Side Observations in Stochastic Bandits |  http://arxiv.org/abs/1210.4839  | author:Stephane Caron, Branislav Kveton, Marc Lelarge, Smriti Bhagat category:cs.LG stat.ML published:2012-10-16 summary:This paper considers stochastic bandits with side observations, a model thataccounts for both the exploration/exploitation dilemma and relationshipsbetween arms. In this setting, after pulling an arm i, the decision maker alsoobserves the rewards for some other actions related to i. We will see that thismodel is suited to content recommendation in social networks, where users'reactions may be endorsed or not by their friends. We provide efficientalgorithms based on upper confidence bounds (UCBs) to leverage this additionalinformation and derive new bounds improving on standard regret guarantees. Wealso evaluate these policies in the context of movie recommendation in socialnetworks: experiments on real datasets show substantial learning rate speedupsranging from 2.2x to 14x on dense networks.
arxiv-1210-4860 | Spectral Estimation of Conditional Random Graph Models for Large-Scale Network Data |  http://arxiv.org/abs/1210.4860  | author:Antonino Freno, Mikaela Keller, Gemma C. Garriga, Marc Tommasi category:cs.SI cs.LG physics.soc-ph stat.ML published:2012-10-16 summary:Generative models for graphs have been typically committed to strong priorassumptions concerning the form of the modeled distributions. Moreover, thevast majority of currently available models are either only suitable forcharacterizing some particular network properties (such as degree distributionor clustering coefficient), or they are aimed at estimating joint probabilitydistributions, which is often intractable in large-scale networks. In thispaper, we first propose a novel network statistic, based on the Laplacianspectrum of graphs, which allows to dispense with any parametric assumptionconcerning the modeled network properties. Second, we use the defined statisticto develop the Fiedler random graph model, switching the focus from theestimation of joint probability distributions to a more tractable conditionalestimation setting. After analyzing the dependence structure characterizingFiedler random graphs, we evaluate them experimentally in edge prediction overseveral real-world networks, showing that they allow to reach a much higherprediction accuracy than various alternative statistical models.
arxiv-1210-4855 | A Slice Sampler for Restricted Hierarchical Beta Process with Applications to Shared Subspace Learning |  http://arxiv.org/abs/1210.4855  | author:Sunil Kumar Gupta, Dinh Q. Phung, Svetha Venkatesh category:cs.LG cs.CV stat.ML published:2012-10-16 summary:Hierarchical beta process has found interesting applications in recent years.In this paper we present a modified hierarchical beta process prior withapplications to hierarchical modeling of multiple data sources. The novel useof the prior over a hierarchical factor model allows factors to be sharedacross different sources. We derive a slice sampler for this model, enablingtractable inference even when the likelihood and the prior over parameters arenon-conjugate. This allows the application of the model in much wider contextswithout restrictions. We present two different data generative models a linearGaussianGaussian model for real valued data and a linear Poisson-gamma modelfor count data. Encouraging transfer learning results are shown for two realworld applications text modeling and content based image retrieval.
arxiv-1210-4843 | Deterministic MDPs with Adversarial Rewards and Bandit Feedback |  http://arxiv.org/abs/1210.4843  | author:Raman Arora, Ofer Dekel, Ambuj Tewari category:cs.GT cs.LG published:2012-10-16 summary:We consider a Markov decision process with deterministic state transitiondynamics, adversarially generated rewards that change arbitrarily from round toround, and a bandit feedback model in which the decision maker only observesthe rewards it receives. In this setting, we present a novel and efficientonline decision making algorithm named MarcoPolo. Under mild assumptions on thestructure of the transition dynamics, we prove that MarcoPolo enjoys a regretof O(T^(3/4)sqrt(log(T))) against the best deterministic policy in hindsight.Specifically, our analysis does not rely on the stringent unichain assumption,which dominates much of the previous work on this topic.
arxiv-1210-4841 | An Efficient Message-Passing Algorithm for the M-Best MAP Problem |  http://arxiv.org/abs/1210.4841  | author:Dhruv Batra category:cs.AI cs.LG stat.ML published:2012-10-16 summary:Much effort has been directed at algorithms for obtaining the highestprobability configuration in a probabilistic random field model known as themaximum a posteriori (MAP) inference problem. In many situations, one couldbenefit from having not just a single solution, but the top M most probablesolutions known as the M-Best MAP problem. In this paper, we propose anefficient message-passing based algorithm for solving the M-Best MAP problem.Specifically, our algorithm solves the recently proposed Linear Programming(LP) formulation of M-Best MAP [7], while being orders of magnitude faster thana generic LP-solver. Our approach relies on studying a particular partialLagrangian relaxation of the M-Best MAP LP which exposes a naturalcombinatorial structure of the problem that we exploit.
arxiv-1210-4567 | Gender identity and lexical variation in social media |  http://arxiv.org/abs/1210.4567  | author:David Bamman, Jacob Eisenstein, Tyler Schnoebelen category:cs.CL published:2012-10-16 summary:We present a study of the relationship between gender, linguistic style, andsocial networks, using a novel corpus of 14,000 Twitter users. Priorquantitative work on gender often treats this social variable as a female/malebinary; we argue for a more nuanced approach. By clustering Twitter users, wefind a natural decomposition of the dataset into various styles and topicalinterests. Many clusters have strong gender orientations, but their use oflinguistic resources sometimes directly conflicts with the population-levellanguage statistics. We view these clusters as a more accurate reflection ofthe multifaceted nature of gendered language styles. Previous corpus-based workhas also had little to say about individuals whose linguistic styles defypopulation-level gender patterns. To identify such individuals, we train astatistical classifier, and measure the classifier confidence for eachindividual in the dataset. Examining individuals whose language does not matchthe classifier's model for their gender, we find that they have social networksthat include significantly fewer same-gender social connections and that, ingeneral, social network homophily is correlated with the use of same-genderlanguage markers. Pairing computational methods and social theory thus offers anew perspective on how gender emerges as individuals position themselvesrelative to audiences, topics, and mainstream gender norms.
arxiv-1210-4460 | Fast SVM-based Feature Elimination Utilizing Data Radius, Hard-Margin, Soft-Margin |  http://arxiv.org/abs/1210.4460  | author:Yaman Aksu category:stat.ML cs.LG published:2012-10-16 summary:Margin maximization in the hard-margin sense, proposed as feature eliminationcriterion by the MFE-LO method, is combined here with data radius utilizationto further aim to lower generalization error, as several published bounds andbound-related formulations pertaining to lowering misclassification risk (orerror) pertain to radius e.g. product of squared radius and weight vectorsquared norm. Additionally, we propose additional novel feature eliminationcriteria that, while instead being in the soft-margin sense, too can utilizedata radius, utilizing previously published bound-related formulations forapproaching radius for the soft-margin sense, whereby e.g. a focus was on theprinciple stated therein as "finding a bound whose minima are in a region withsmall leave-one-out values may be more important than its tightness". Theseadditional criteria we propose combine radius utilization with a novel andcomputationally low-cost soft-margin light classifier retraining approach wedevise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. Wecorrect an error in the MFE-LO description, find MFE-LO achieves the highestgeneralization accuracy among the previously published margin-based featureelimination (MFE) methods, discuss some limitations of MFE-LO, and find ournovel methods herein outperform MFE-LO, attain lower test set classificationerror rate. On several datasets that each both have a large number of featuresand fall into the `large features few samples' dataset category, and ondatasets with lower (low-to-intermediate) number of features, our novel methodsgive promising results. Especially, among our methods the tunable ones, that donot employ (the non-tunable) LO approach, can be tuned more aggressively in thefuture than herein, to aim to demonstrate for them even higher performance thanherein.
arxiv-1210-4918 | Dynamic Teaching in Sequential Decision Making Environments |  http://arxiv.org/abs/1210.4918  | author:Thomas J. Walsh, Sergiu Goschin category:cs.LG cs.AI stat.ML published:2012-10-16 summary:We describe theoretical bounds and a practical algorithm for teaching a modelby demonstration in a sequential decision making environment. Unlike previousefforts that have optimized learners that watch a teacher demonstrate a staticpolicy, we focus on the teacher as a decision maker who can dynamically choosedifferent policies to teach different parts of the environment. We developseveral teaching frameworks based on previously defined supervised protocols,such as Teaching Dimension, extending them to handle noise and sequences ofinputs encountered in an MDP.We provide theoretical bounds on the learnabilityof several important model classes in this setting and suggest a practicalalgorithm for dynamic teaching.
arxiv-1210-4919 | Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought Stressed Plants |  http://arxiv.org/abs/1210.4919  | author:Mirwaes Wahabzada, Kristian Kersting, Christian Bauckhage, Christoph Roemer, Agim Ballvora, Francisco Pinto, Uwe Rascher, Jens Leon, Lutz Ploemer category:cs.LG cs.CE stat.ML published:2012-10-16 summary:Understanding the adaptation process of plants to drought stress is essentialin improving management practices, breeding strategies as well as engineeringviable crops for a sustainable agriculture in the coming decades.Hyper-spectral imaging provides a particularly promising approach to gain suchunderstanding since it allows to discover non-destructively spectralcharacteristics of plants governed primarily by scattering and absorptioncharacteristics of the leaf internal structure and biochemical constituents.Several drought stress indices have been derived using hyper-spectral imaging.However, they are typically based on few hyper-spectral images only, rely oninterpretations of experts, and consider few wavelengths only. In this study,we present the first data-driven approach to discovering spectral droughtstress indices, treating it as an unsupervised labeling problem at massivescale. To make use of short range dependencies of spectral wavelengths, wedevelop an online variational Bayes algorithm for latent Dirichlet allocationwith convolved Dirichlet regularizer. This approach scales to massive datasetsand, hence, provides a more objective complement to plant physiologicalpractices. The spectral topics found conform to plant physiological knowledgeand can be computed in a fraction of the time compared to existing LDAapproaches.
arxiv-1210-4145 | A Biologically Realistic Model of Saccadic Eye Control with Probabilistic Population Codes |  http://arxiv.org/abs/1210.4145  | author:Sacha Sokoloski category:cs.NE q-bio.NC published:2012-10-15 summary:The posterior parietal cortex is believed to direct eye movements, especiallyin regards to target tracking tasks, and a number of debates exist over theprecise nature of the computations performed by the parietal cortex, with eachside supported by different sets of biological evidence. In this paper I willpresent my model which navigates a course between some of these debates,towards the end of presenting a model which can explain some of the competinginterpretations among the data sets. In particular, rather than assuming thatproprioception or efference copies form the key source of information forcomputing eye position information, I use a biological plausible implementationof a Kalman filter to optimally combine the two signals, and a simple gaincontrol mechanism in order to accommodate the latency of the proprioceptivesignal. Fitting within the Bayesian brain hypothesis, the result is a Bayesoptimal solution to the eye control problem, with a range of data supportingclaims of biological plausibility.
arxiv-1210-3865 | Opinion Mining for Relating Subjective Expressions and Annual Earnings in US Financial Statements |  http://arxiv.org/abs/1210.3865  | author:Chien-Liang Chen, Chao-Lin Liu, Yuan-Chen Chang, Hsiang-Ping Tsai category:cs.CL cs.AI cs.IR q-fin.GN published:2012-10-15 summary:Financial statements contain quantitative information and manager'ssubjective evaluation of firm's financial status. Using information released inU.S. 10-K filings. Both qualitative and quantitative appraisals are crucial forquality financial decisions. To extract such opinioned statements from thereports, we built tagging models based on the conditional random field (CRF)techniques, considering a variety of combinations of linguistic factorsincluding morphology, orthography, predicate-argument structure, syntax, andsimple semantics. Our results show that the CRF models are reasonably effectiveto find opinion holders in experiments when we adopted the popular MPQA corpusfor training and testing. The contribution of our paper is to identify opinionpatterns in multiword expressions (MWEs) forms rather than in single wordforms. We find that the managers of corporations attempt to use more optimisticwords to obfuscate negative financial performance and to accentuate thepositive financial performance. Our results also show that decreasing earningswere often accompanied by ambiguous and mild statements in the reporting yearand that increasing earnings were stated in assertive and positive way.
arxiv-1210-4184 | The Kernel Pitman-Yor Process |  http://arxiv.org/abs/1210.4184  | author:Sotirios P. Chatzis, Dimitrios Korkinof, Yiannis Demiris category:cs.LG cs.AI stat.ML published:2012-10-15 summary:In this work, we propose the kernel Pitman-Yor process (KPYP) fornonparametric clustering of data with general spatial or temporalinterdependencies. The KPYP is constructed by first introducing an infinitesequence of random locations. Then, based on the stick-breaking construction ofthe Pitman-Yor process, we define a predictor-dependent random probabilitymeasure by considering that the discount hyperparameters of theBeta-distributed random weights (stick variables) of the process are notuniform among the weights, but controlled by a kernel function expressing theproximity between the location assigned to each weight and the givenpredictors.
arxiv-1210-4006 | The Perturbed Variation |  http://arxiv.org/abs/1210.4006  | author:Maayan Harel, Shie Mannor category:cs.LG stat.ML published:2012-10-15 summary:We introduce a new discrepancy score between two distributions that gives anindication on their similarity. While much research has been done to determineif two samples come from exactly the same distribution, much less researchconsidered the problem of determining if two finite samples come from similardistributions. The new score gives an intuitive interpretation of similarity;it optimally perturbs the distributions so that they best fit each other. Thescore is defined between distributions, and can be efficiently estimated fromsamples. We provide convergence bounds of the estimated score, and develophypothesis testing procedures that test if two data sets come from similardistributions. The statistical power of this procedures is presented insimulations. We also compare the score's capacity to detect similarity withthat of other known measures on real data.
arxiv-1210-4021 | Local Optima Networks, Landscape Autocorrelation and Heuristic Search Performance |  http://arxiv.org/abs/1210.4021  | author:Francisco Chicano, Fabio Daolio, Gabriela Ochoa, Sébastien Verel, Marco Tomassini, Enrique Alba category:cs.AI cs.NE published:2012-10-15 summary:Recent developments in fitness landscape analysis include the study of LocalOptima Networks (LON) and applications of the Elementary Landscapes theory.This paper represents a first step at combining these two tools to exploretheir ability to forecast the performance of search algorithms. We base ouranalysis on the Quadratic Assignment Problem (QAP) and conduct a largestatistical study over 600 generated instances of different types. Our resultsreveal interesting links between the network measures, the autocorrelationmeasures and the performance of heuristic search algorithms.
arxiv-1210-4081 | Getting Feasible Variable Estimates From Infeasible Ones: MRF Local Polytope Study |  http://arxiv.org/abs/1210.4081  | author:Bogdan Savchynskyy, Stefan Schmidt category:cs.NA cs.CV cs.DS cs.LG math.OC published:2012-10-15 summary:This paper proposes a method for construction of approximate feasible primalsolutions from dual ones for large-scale optimization problems possessingcertain separability properties. Whereas infeasible primal estimates cantypically be produced from (sub-)gradients of the dual function, it is oftennot easy to project them to the primal feasible set, since the projectionitself has a complexity comparable to the complexity of the initial problem. Wepropose an alternative efficient method to obtain feasibility and show that itsproperties influencing the convergence to the optimum are similar to theproperties of the Euclidean projection. We apply our method to the localpolytope relaxation of inference problems for Markov Random Fields anddemonstrate its superiority over existing methods.
arxiv-1210-3926 | Learning Attitudes and Attributes from Multi-Aspect Reviews |  http://arxiv.org/abs/1210.3926  | author:Julian McAuley, Jure Leskovec, Dan Jurafsky category:cs.CL cs.IR cs.LG published:2012-10-15 summary:The majority of online reviews consist of plain-text feedback together with asingle numeric score. However, there are multiple dimensions to products andopinions, and understanding the `aspects' that contribute to users' ratings mayhelp us to better understand their individual preferences. For example, auser's impression of an audiobook presumably depends on aspects such as thestory and the narrator, and knowing their opinions on these aspects may help usto recommend better products. In this paper, we build models for rating systemsin which such dimensions are explicit, in the sense that users leave separateratings for each aspect of a product. By introducing new corpora consisting offive million reviews, rated with between three and six aspects, we evaluate ourmodels on three prediction tasks: First, we use our model to uncover whichparts of a review discuss which of the rated aspects. Second, we use our modelto summarize reviews, which for us means finding the sentences that bestexplain a user's rating. Finally, since aspect ratings are optional in many ofthe datasets we consider, we use our model to recover those ratings that aremissing from a user's evaluation. Our model matches state-of-the-art approacheson existing small-scale datasets, while scaling to the real-world datasets weintroduce. Moreover, our model is able to `disentangle' content and sentimentwords: we automatically learn content words that are indicative of a particularaspect as well as the aspect-specific sentiment words that are indicative of aparticular rating.
arxiv-1210-3831 | Graphical Modelling in Genetics and Systems Biology |  http://arxiv.org/abs/1210.3831  | author:Marco Scutari category:stat.ME q-bio.MN stat.ML published:2012-10-14 summary:Graphical modelling has a long history in statistics as a tool for theanalysis of multivariate data, starting from Wright's path analysis and Gibbs'applications to statistical physics at the beginning of the last century. Inits modern form, it was pioneered by Lauritzen and Wermuth and Pearl in the1980s, and has since found applications in fields as diverse as bioinformatics,customer satisfaction surveys and weather forecasts. Genetics and systems biology are unique among these fields in the dimensionof the data sets they study, which often contain several hundreds of variablesand only a few tens or hundreds of observations. This raises problems in bothcomputational complexity and the statistical significance of the resultingnetworks, collectively known as the "curse of dimensionality". Furthermore, thedata themselves are difficult to model correctly due to the limitedunderstanding of the underlying mechanisms. In the following, we willillustrate how such challenges affect practical graphical modelling and somepossible solutions.
arxiv-1210-3832 | Image Processing using Smooth Ordering of its Patches |  http://arxiv.org/abs/1210.3832  | author:Idan Ram, Michael Elad, Israel Cohen category:cs.CV published:2012-10-14 summary:We propose an image processing scheme based on reordering of its patches. Fora given corrupted image, we extract all patches with overlaps, refer to theseas coordinates in high-dimensional space, and order them such that they arechained in the "shortest possible path", essentially solving the travelingsalesman problem. The obtained ordering applied to the corrupted image, impliesa permutation of the image pixels to what should be a regular signal. Thisenables us to obtain good recovery of the clean image by applying relativelysimple 1D smoothing operations (such as filtering or interpolation) to thereordered set of pixels. We explore the use of the proposed approach to imagedenoising and inpainting, and show promising results in both cases.
arxiv-1210-3741 | Online computation of sparse representations of time varying stimuli using a biologically motivated neural network |  http://arxiv.org/abs/1210.3741  | author:Tao Hu, Dmitri B. Chklovskii category:q-bio.NC cs.NE published:2012-10-13 summary:Natural stimuli are highly redundant, possessing significant spatial andtemporal correlations. While sparse coding has been proposed as an efficientstrategy employed by neural systems to encode sensory stimuli, the underlyingmechanisms are still not well understood. Most previous approaches model theneural dynamics by the sparse representation dictionary itself and compute therepresentation coefficients offline. In reality, faced with the challenge ofconstantly changing stimuli, neurons must compute the sparse representationsdynamically in an online fashion. Here, we describe a leaky linearized Bregmaniteration (LLBI) algorithm which computes the time varying sparserepresentations using a biologically motivated network of leaky rectifyingneurons. Compared to previous attempt of dynamic sparse coding, LLBI exploitsthe temporal correlation of stimuli and demonstrate better performance both inrepresentation error and the smoothness of temporal evolution of sparsecoefficients.
arxiv-1210-3718 | On the Role of Contrast and Regularity in Perceptual Boundary Saliency |  http://arxiv.org/abs/1210.3718  | author:Mariano Tepper, Pablo Musé, Andrés Almansa category:cs.CV stat.AP published:2012-10-13 summary:Mathematical Morphology proposes to extract shapes from images as connectedcomponents of level sets. These methods prove very suitable for shaperecognition and analysis. We present a method to select the perceptuallysignificant (i.e., contrasted) level lines (boundaries of level sets), usingthe Helmholtz principle as first proposed by Desolneux et al. Contrarily to theclassical formulation by Desolneux et al. where level lines must be entirelysalient, the proposed method allows to detect partially salient level lines,thus resulting in more robust and more stable detections. We then tackle theproblem of combining two gestalts as a measure of saliency and propose a methodthat reinforces detections. Results in natural images show the good performanceof the proposed methods.
arxiv-1210-3709 | A Rank-Corrected Procedure for Matrix Completion with Fixed Basis Coefficients |  http://arxiv.org/abs/1210.3709  | author:Weimin Miao, Shaohua Pan, Defeng Sun category:math.OC cs.IT cs.NA math.IT stat.ML published:2012-10-13 summary:For the problems of low-rank matrix completion, the efficiency of thewidely-used nuclear norm technique may be challenged under many circumstances,especially when certain basis coefficients are fixed, for example, the low-rankcorrelation matrix completion in various fields such as the financial marketand the low-rank density matrix completion from the quantum state tomography.To seek a solution of high recovery quality beyond the reach of the nuclearnorm, in this paper, we propose a rank-corrected procedure using a nuclearsemi-norm to generate a new estimator. For this new estimator, we establish anon-asymptotic recovery error bound. More importantly, we quantify thereduction of the recovery error bound for this rank-corrected procedure.Compared with the one obtained for the nuclear norm penalized least squaresestimator, this reduction can be substantial (around 50%). We also providenecessary and sufficient conditions for rank consistency in the sense of Bach(2008). Very interestingly, these conditions are highly related to the conceptof constraint nondegeneracy in matrix optimization. As a byproduct, our resultsprovide a theoretical foundation for the majorized penalty method of Gao andSun (2010) and Gao (2010) for structured low-rank matrix optimization problems.Extensive numerical experiments demonstrate that our proposed rank-correctedprocedure can simultaneously achieve a high recovery accuracy and capture thelow-rank structure.
arxiv-1210-3729 | Inference of Fine-grained Attributes of Bengali Corpus for Stylometry Detection |  http://arxiv.org/abs/1210.3729  | author:Tanmoy Chakraborty, Sivaji Bandyopadhyay category:cs.CL cs.CV published:2012-10-13 summary:Stylometry, the science of inferring characteristics of the author from thecharacteristics of documents written by that author, is a problem with a longhistory and belongs to the core task of Text categorization that involvesauthorship identification, plagiarism detection, forensic investigation,computer security, copyright and estate disputes etc. In this work, we presenta strategy for stylometry detection of documents written in Bengali. We adopt aset of fine-grained attribute features with a set of lexical markers for theanalysis of the text and use three semi-supervised measures for makingdecisions. Finally, a majority voting approach has been taken for finalclassification. The system is fully automatic and language-independent.Evaluation results of our attempt for Bengali author's stylometry detectionshow reasonably promising accuracy in comparison to the baseline model.
arxiv-1210-3404 | A polygon-based interpolation operator for super-resolution imaging |  http://arxiv.org/abs/1210.3404  | author:Stéfan J. van der Walt, B. M. Herbst category:cs.CV published:2012-10-12 summary:We outline the super-resolution reconstruction problem posed as amaximization of probability. We then introduce an interpolation method based onpolygonal pixel overlap, express it as a linear operator, and use it to improvereconstruction. Polygon interpolation outperforms the simpler bilinearinterpolation operator and, unlike Gaussian modeling of pixels, requires noparameter estimation. A free software implementation that reproduces theresults shown is provided.
arxiv-1210-3634 | Quick Summary |  http://arxiv.org/abs/1210.3634  | author:Robert Wahlstedt category:cs.CL cs.AI published:2012-10-12 summary:Quick Summary is an innovate implementation of an automatic documentsummarizer that inputs a document in the English language and evaluates eachsentence. The scanner or evaluator determines criteria based on its grammaticalstructure and place in the paragraph. The program then asks the user to specifythe number of sentences the person wishes to highlight. For example should theuser ask to have three of the most important sentences, it would highlight thefirst and most important sentence in green. Commonly this is the sentencecontaining the conclusion. Then Quick Summary finds the second most importantsentence usually called a satellite and highlights it in yellow. This isusually the topic sentence. Then the program finds the third most importantsentence and highlights it in red. The implementations of this technology areuseful in a society of information overload when a person typically receives 42emails a day (Microsoft). The paper also is a candid look at difficulty thatmachine learning has in textural translating. However, it speaks on how toovercome the obstacles that historically prevented progress. This paperproposes mathematical meta-data criteria that justify the place of importanceof a sentence. Just as tools for the study of relational symmetry inbio-informatics, this tool seeks to classify words with greater clarity."Survey Finds Workers Average Only Three Productive Days per Week." MicrosoftNews Center. Microsoft. Web. 31 Mar. 2012.
