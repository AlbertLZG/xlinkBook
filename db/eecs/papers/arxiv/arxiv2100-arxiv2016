arxiv-2100-1 | Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization | http://arxiv.org/pdf/1209.1873v2.pdf | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG math.OC published:2012-09-10 summary:Stochastic Gradient Descent (SGD) has become popular for solving large scalesupervised machine learning optimization problems such as SVM, due to theirstrong theoretical guarantees. While the closely related Dual Coordinate Ascent(DCA) method has been implemented in various software packages, it has so farlacked good convergence analysis. This paper presents a new analysis ofStochastic Dual Coordinate Ascent (SDCA) showing that this class of methodsenjoy strong theoretical guarantees that are comparable or better than SGD.This analysis justifies the effectiveness of SDCA for practical applications.
arxiv-2100-2 | On the Geometry of Bayesian Graphical Models with Hidden Variables | http://arxiv.org/pdf/1301.7411v1.pdf | author:Raffaella Settimi, Jim Q. Smith category:cs.LG stat.ML published:2013-01-30 summary:In this paper we investigate the geometry of the likelihood of the unknownparameters in a simple class of Bayesian directed graphs with hidden variables.This enables us, before any numerical algorithms are employed, to obtaincertain insights in the nature of the unidentifiability inherent in suchmodels, the way posterior densities will be sensitive to prior densities andthe typical geometrical form these posterior densities might take. Many ofthese insights carry over into more complicated Bayesian networks withsystematic missing data.
arxiv-2100-3 | A Multivariate Discretization Method for Learning Bayesian Networks from Mixed Data | http://arxiv.org/pdf/1301.7403v1.pdf | author:Stefano Monti, Gregory F. Cooper category:cs.AI cs.LG published:2013-01-30 summary:In this paper we address the problem of discretization in the context oflearning Bayesian networks (BNs) from data containing both continuous anddiscrete variables. We describe a new technique for <EM>multivariate</EM>discretization, whereby each continuous variable is discretized while takinginto account its interaction with the other variables. The technique is basedon the use of a Bayesian scoring metric that scores the discretization policyfor a continuous variable given a BN structure and the observed data. Since themetric is relative to the BN structure currently being evaluated, thediscretization of a variable needs to be dynamically adjusted as the BNstructure changes.
arxiv-2100-4 | Mixture Representations for Inference and Learning in Boltzmann Machines | http://arxiv.org/pdf/1301.7393v1.pdf | author:Neil D. Lawrence, Christopher M. Bishop, Michael I. Jordan category:cs.LG stat.ML published:2013-01-30 summary:Boltzmann machines are undirected graphical models with two-state stochasticvariables, in which the logarithms of the clique potentials are quadraticfunctions of the node states. They have been widely studied in the neuralcomputing literature, although their practical applicability has been limitedby the difficulty of finding an effective learning algorithm. Onewell-established approach, known as mean field theory, represents thestochastic distribution using a factorized approximation. However, thecorresponding learning algorithm often fails to find a good solution. Weconjecture that this is due to the implicit uni-modality of the mean fieldapproximation which is therefore unable to capture multi-modality in the truedistribution. In this paper we use variational methods to approximate thestochastic distribution using multi-modal mixtures of factorized distributions.We present results for both inference and learning to demonstrate theeffectiveness of this approach.
arxiv-2100-5 | Large Deviation Methods for Approximate Probabilistic Inference | http://arxiv.org/pdf/1301.7392v1.pdf | author:Michael Kearns, Lawrence Saul category:cs.LG stat.ML published:2013-01-30 summary:We study two-layer belief networks of binary random variables in which theconditional probabilities Pr[childlparents] depend monotonically on weightedsums of the parents. In large networks where exact probabilistic inference isintractable, we show how to compute upper and lower bounds on manyprobabilities of interest. In particular, using methods from large deviationtheory, we derive rigorous bounds on marginal probabilities such asPr[children] and prove rates of convergence for the accuracy of our bounds as afunction of network size. Our results apply to networks with generic transferfunction parameterizations of the conditional probability tables, such assigmoid and noisy-OR. They also explicitly illustrate the types of averagingbehavior that can simplify the problem of inference in large networks.
arxiv-2100-6 | Hierarchical Mixtures-of-Experts for Exponential Family Regression Models with Generalized Linear Mean Functions: A Survey of Approximation and Consistency Results | http://arxiv.org/pdf/1301.7390v1.pdf | author:Wenxin Jiang, Martin A. Tanner category:cs.LG stat.ML published:2013-01-30 summary:We investigate a class of hierarchical mixtures-of-experts (HME) models whereexponential family regression models with generalized linear mean functions ofthe form psi(ga+fx^Tfgb) are mixed. Here psi(...) is the inverse link function.Suppose the true response y follows an exponential family regression model withmean function belonging to a class of smooth functions of the form psi(h(fx))where h(...)in W_2^infty (a Sobolev class over [0,1]^{s}). It is shown that theHME probability density functions can approximate the true density, at a rateof O(m^{-2/s}) in L_p norm, and at a rate of O(m^{-4/s}) in Kullback-Leiblerdivergence. These rates can be achieved within the family of HME structureswith no more than s-layers, where s is the dimension of the predictor fx. It isalso shown that likelihood-based inference based on HME is consistent inrecovering the truth, in the sense that as the sample size n and the number ofexperts m both increase, the mean square error of the predicted mean responsegoes to zero. Conditions for such results to hold are stated and discussed.
arxiv-2100-7 | Minimum Encoding Approaches for Predictive Modeling | http://arxiv.org/pdf/1301.7378v1.pdf | author:Peter D Grunwald, Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri category:cs.LG stat.ML published:2013-01-30 summary:We analyze differences between two information-theoretically motivatedapproaches to statistical inference and model selection: the MinimumDescription Length (MDL) principle, and the Minimum Message Length (MML)principle. Based on this analysis, we present two revised versions of MML: apointwise estimator which gives the MML-optimal single parameter model, and avolumewise estimator which gives the MML-optimal region in the parameter space.Our empirical results suggest that with small data sets, the MDL approachyields more accurate predictions than the MML estimators. The empirical resultsalso demonstrate that the revised MML estimators introduced here perform betterthan the original MML estimator suggested by Wallace and Freeman.
arxiv-2100-8 | Graphical Models and Exponential Families | http://arxiv.org/pdf/1301.7376v1.pdf | author:Dan Geiger, Christopher Meek category:cs.LG stat.ML published:2013-01-30 summary:We provide a classification of graphical models according to theirrepresentation as subfamilies of exponential families. Undirected graphicalmodels with no hidden variables are linear exponential families (LEFs),directed acyclic graphical models and chain graphs with no hidden variables,including Bayesian networks with several families of local distributions, arecurved exponential families (CEFs) and graphical models with hidden variablesare stratified exponential families (SEFs). An SEF is a finite union of CEFssatisfying a frontier condition. In addition, we illustrate how one canautomatically generate independence and non-independence constraints on thedistributions over the observable variables implied by a Bayesian network withhidden variables. The relevance of these results for model selection isexamined.
arxiv-2100-9 | Learning by Transduction | http://arxiv.org/pdf/1301.7375v1.pdf | author:Alex Gammerman, Volodya Vovk, Vladimir Vapnik category:cs.LG stat.ML published:2013-01-30 summary:We describe a method for predicting a classification of an object givenclassifications of the objects in the training set, assuming that the pairsobject/classification are generated by an i.i.d. process from a continuousprobability distribution. Our method is a modification of Vapnik'ssupport-vector machine; its main novelty is that it gives not only theprediction itself but also a practicable measure of the evidence found insupport of that prediction. We also describe a procedure for assigning degreesof confidence to predictions made by the support vector machine. Someexperimental results are presented, and possible extensions of the algorithmsare discussed.
arxiv-2100-10 | Learning the Structure of Dynamic Probabilistic Networks | http://arxiv.org/pdf/1301.7374v1.pdf | author:Nir Friedman, Kevin Murphy, Stuart Russell category:cs.AI cs.LG published:2013-01-30 summary:Dynamic probabilistic networks are a compact representation of complexstochastic processes. In this paper we examine how to learn the structure of aDPN from data. We extend structure scoring rules for standard probabilisticnetworks to the dynamic case, and show how to search for structure when some ofthe variables are hidden. Finally, we examine two applications where such atechnology might be useful: predicting and classifying dynamic behaviors, andlearning causal orderings in biological processes. We provide empirical resultsthat demonstrate the applicability of our methods in both domains.
arxiv-2100-11 | The Bayesian Structural EM Algorithm | http://arxiv.org/pdf/1301.7373v1.pdf | author:Nir Friedman category:cs.LG cs.AI stat.ML published:2013-01-30 summary:In recent years there has been a flurry of works on learning Bayesiannetworks from data. One of the hard problems in this area is how to effectivelylearn the structure of a belief network from incomplete data- that is, in thepresence of missing values or hidden variables. In a recent paper, I introducedan algorithm called Structural EM that combines the standard ExpectationMaximization (EM) algorithm, which optimizes parameters, with structure searchfor model selection. That algorithm learns networks based on penalizedlikelihood scores, which include the BIC/MDL score and various approximationsto the Bayesian score. In this paper, I extend Structural EM to deal directlywith Bayesian model selection. I prove the convergence of the resultingalgorithm and show how to apply it for learning a large class of probabilisticmodels, including Bayesian networks and some variants thereof.
arxiv-2100-12 | Empirical Analysis of Predictive Algorithms for Collaborative Filtering | http://arxiv.org/pdf/1301.7363v1.pdf | author:John S. Breese, David Heckerman, Carl Kadie category:cs.IR cs.LG published:2013-01-30 summary:Collaborative filtering or recommender systems use a database about userpreferences to predict additional topics or products a new user might like. Inthis paper we describe several algorithms designed for this task, includingtechniques based on correlation coefficients, vector-based similaritycalculations, and statistical Bayesian methods. We compare the predictiveaccuracy of the various methods in a set of representative problem domains. Weuse two basic classes of evaluation metrics. The first characterizes accuracyover a set of individual predictions in terms of average absolute deviation.The second estimates the utility of a ranked list of suggested items. Thismetric uses an estimate of the probability that a user will see arecommendation in an ordered list. Experiments were run for datasets associatedwith 3 application areas, 4 experimental protocols, and the 2 evaluationmetrics for the various algorithms. Results indicate that for a wide range ofconditions, Bayesian networks with decision trees at each node and correlationmethods outperform Bayesian-clustering and vector-similarity methods. Betweencorrelation and Bayesian networks, the preferred method depends on the natureof the dataset, nature of the application (ranked versus one-by-onepresentation), and the availability of votes with which to make predictions.Other considerations include the size of database, speed of predictions, andlearning time.
arxiv-2100-13 | A geometric analysis of subspace clustering with outliers | http://arxiv.org/pdf/1112.4258v5.pdf | author:Mahdi Soltanolkotabi, Emmanuel J. Candés category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2011-12-19 summary:This paper considers the problem of clustering a collection of unlabeled datapoints assumed to lie near a union of lower-dimensional planes. As is common incomputer vision or unsupervised learning applications, we do not know inadvance how many subspaces there are nor do we have any information about theirdimensions. We develop a novel geometric analysis of an algorithm named sparsesubspace clustering (SSC) [In IEEE Conference on Computer Vision and PatternRecognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantlybroadens the range of problems where it is provably effective. For instance, weshow that SSC can recover multiple subspaces, each of dimension comparable tothe ambient dimension. We also prove that SSC can correctly cluster data pointseven when the subspaces of interest intersect. Further, we develop an extensionof SSC that succeeds when the data set is corrupted with possiblyoverwhelmingly many outliers. Underlying our analysis are clear geometricinsights, which may bear on other sparse recovery problems. A numerical studycomplements our theoretical analysis and demonstrates the effectiveness ofthese methods.
arxiv-2100-14 | Multi-Step Regression Learning for Compositional Distributional Semantics | http://arxiv.org/pdf/1301.6939v2.pdf | author:Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, Marco Baroni category:cs.CL cs.LG 68T50 published:2013-01-29 summary:We present a model for compositional distributional semantics related to theframework of Coecke et al. (2010), and emulating formal semantics byrepresenting functions as tensors and arguments as vectors. We introduce a newlearning method for tensors, generalising the approach of Baroni and Zamparelli(2010). We evaluate it on two benchmark data sets, and find it to outperformexisting leading methods. We argue in our analysis that the nature of thislearning method also renders it suitable for solving more subtle problemscompositional distributional models might face.
arxiv-2100-15 | A note on selection stability: combining stability and prediction | http://arxiv.org/pdf/1301.7118v1.pdf | author:Yixin Fang, Junhui Wang, Wei Sun category:stat.ME stat.ML published:2013-01-30 summary:Recently, many regularized procedures have been proposed for variableselection in linear regression, but their performance depends on the tuningparameter selection. Here a criterion for the tuning parameter selection isproposed, which combines the strength of both stability selection andcross-validation and therefore is referred as the prediction and stabilityselection (PASS). The selection consistency is established assuming the datagenerating model is a subset of the full model, and the small sampleperformance is demonstrated through some simulation studies where theassumption is either held or violated.
arxiv-2100-16 | Statistical mechanics of complex neural systems and high dimensional data | http://arxiv.org/pdf/1301.7115v1.pdf | author:Madhu Advani, Subhaneil Lahiri, Surya Ganguli category:q-bio.NC stat.ML published:2013-01-30 summary:Recent experimental advances in neuroscience have opened new vistas into theimmense complexity of neuronal networks. This proliferation of data challengesus on two parallel fronts. First, how can we form adequate theoreticalframeworks for understanding how dynamical network processes cooperate acrosswidely disparate spatiotemporal scales to solve important computationalproblems? And second, how can we extract meaningful models of neuronal systemsfrom high dimensional datasets? To aid in these challenges, we give apedagogical review of a collection of ideas and theoretical methods arising atthe intersection of statistical physics, computer science and neurobiology. Weintroduce the interrelated replica and cavity methods, which originated instatistical physics as powerful ways to quantitatively analyze large highlyheterogeneous systems of many interacting degrees of freedom. We also introducethe closely related notion of message passing in graphical models, whichoriginated in computer science as a distributed algorithm capable of solvinglarge inference and optimization problems involving many coupled variables. Wethen show how both the statistical physics and computer science perspectivescan be applied in a wide diversity of contexts to problems arising intheoretical neuroscience and data analysis. Along the way we discuss spinglasses, learning theory, illusions of structure in noise, random matrices,dimensionality reduction, and compressed sensing, all within the unifiedformalism of the replica method. Moreover, we review recent conceptualconnections between message passing in graphical models, and neural computationand learning. Overall, these ideas illustrate how statistical physics andcomputer science might provide a lens through which we can uncover emergentcomputational functions buried deep within the dynamical complexities ofneuronal networks.
arxiv-2100-17 | Link prediction for partially observed networks | http://arxiv.org/pdf/1301.7047v1.pdf | author:Yunpeng Zhao, Elizaveta Levina, Ji Zhu category:stat.ML cs.LG cs.SI published:2013-01-29 summary:Link prediction is one of the fundamental problems in network analysis. Inmany applications, notably in genetics, a partially observed network may notcontain any negative examples of absent edges, which creates a difficulty formany existing supervised learning approaches. We develop a new method whichtreats the observed network as a sample of the true network with differentsampling rates for positive and negative examples. We obtain a relative rankingof potential links by their probabilities, utilizing information on nodecovariates as well as on network topology. Empirically, the method performswell under many settings, including when the observed network is sparse. Weapply the method to a protein-protein interaction network and a schoolfriendship network.
arxiv-2100-18 | Linearized Alternating Direction Method with Adaptive Penalty and Warm Starts for Fast Solving Transform Invariant Low-Rank Textures | http://arxiv.org/pdf/1205.5351v2.pdf | author:Xiang Ren, Zhouchen Lin category:cs.CV published:2012-05-24 summary:Transform Invariant Low-rank Textures (TILT) is a novel and powerful toolthat can effectively rectify a rich class of low-rank textures in 3D scenesfrom 2D images despite significant deformation and corruption. The existingalgorithm for solving TILT is based on the alternating direction method (ADM).It suffers from high computational cost and is not theoretically guaranteed toconverge to a correct solution. In this paper, we propose a novel algorithm tospeed up solving TILT, with guaranteed convergence. Our method is based on therecently proposed linearized alternating direction method with adaptive penalty(LADMAP). To further reduce computation, warm starts are also introduced toinitialize the variables better and cut the cost on singular valuedecomposition. Extensive experimental results on both synthetic and real datademonstrate that this new algorithm works much more efficiently and robustlythan the existing algorithm. It could be at least five times faster than theprevious method.
arxiv-2100-19 | Multi-Stage Classifier Design | http://arxiv.org/pdf/1205.4377v2.pdf | author:Kirill Trapeznikov, Venkatesh Saligrama, David Castanon category:cs.CV stat.ML published:2012-05-20 summary:In many classification systems, sensing modalities have different acquisitioncosts. It is often {\it unnecessary} to use every modality to classify amajority of examples. We study a multi-stage system in a prediction time costreduction setting, where the full data is available for training, but for atest example, measurements in a new modality can be acquired at each stage foran additional cost. We seek decision rules to reduce the average measurementacquisition cost. We formulate an empirical risk minimization problem (ERM) fora multi-stage reject classifier, wherein the stage $k$ classifier eitherclassifies a sample using only the measurements acquired so far or rejects itto the next stage where more attributes can be acquired for a cost. To solvethe ERM problem, we show that the optimal reject classifier at each stage is acombination of two binary classifiers, one biased towards positive examples andthe other biased towards negative examples. We use this parameterization toconstruct stage-by-stage global surrogate risk, develop an iterative algorithmin the boosting framework and present convergence and generalization results.We test our work on synthetic, medical and explosives detection datasets. Ourresults demonstrate that substantial cost reduction without a significantsacrifice in accuracy is achievable.
arxiv-2100-20 | PyXNAT: XNAT in Python | http://arxiv.org/pdf/1301.6952v1.pdf | author:Yannick Schwartz, Alexis Barbot, Benjamin Thyreau, Vincent Frouin, Gaël Varoquaux, Aditya Siram, Daniel Marcus, Jean-Baptiste Poline category:cs.DB cs.CV q-bio.QM published:2013-01-29 summary:As neuroimaging databases grow in size and complexity, the time researchersspend investigating and managing the data increases to the expense of dataanalysis. As a result, investigators rely more and more heavily on scriptingusing high-level languages to automate data management and processing tasks.For this, a structured and programmatic access to the data store is necessary.Web services are a first step toward this goal. They however lack infunctionality and ease of use because they provide only low level interfaces todatabases. We introduce here PyXNAT, a Python module that interacts with TheExtensible Neuroimaging Archive Toolkit (XNAT) through native Python callsacross multiple operating systems. The choice of Python enables PyXNAT toexpose the XNAT Web Services and unify their features with a higher level andmore expressive language. PyXNAT provides XNAT users direct access to all thescientific packages in Python. Finally PyXNAT aims to be efficient and easy touse, both as a backend library to build XNAT clients and as an alternativefrontend from the command line.
arxiv-2100-21 | On the Consistency of the Bootstrap Approach for Support Vector Machines and Related Kernel Based Methods | http://arxiv.org/pdf/1301.6944v1.pdf | author:Andreas Christmann, Robert Hable category:stat.ML cs.LG published:2013-01-29 summary:It is shown that bootstrap approximations of support vector machines (SVMs)based on a general convex and smooth loss function and on a general kernel areconsistent. This result is useful to approximate the unknown finite sampledistribution of SVMs by the bootstrap approach.
arxiv-2100-22 | An alternative text representation to TF-IDF and Bag-of-Words | http://arxiv.org/pdf/1301.6770v1.pdf | author:Zhixiang, Xu, Minmin Chen, Kilian Q. Weinberger, Fei Sha category:cs.IR cs.LG stat.ML published:2013-01-28 summary:In text mining, information retrieval, and machine learning, text documentsare commonly represented through variants of sparse Bag of Words (sBoW) vectors(e.g. TF-IDF). Although simple and intuitive, sBoW style representations sufferfrom their inherent over-sparsity and fail to capture word-level synonymy andpolysemy. Especially when labeled data is limited (e.g. in documentclassification), or the text documents are short (e.g. emails or abstracts),many features are rarely observed within the training corpus. This leads tooverfitting and reduced generalization accuracy. In this paper we propose DenseCohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoWdocument features. dCoT explicitly models absent words by removing andreconstructing random sub-sets of words in the unlabeled corpus. With thisapproach, dCoT learns to reconstruct frequent words from co-occurringinfrequent words and maps the high dimensional sparse sBoW vectors into alow-dimensional dense representation. We show that the feature removal can bemarginalized out and that the reconstruction can be solved for in closed-form.We demonstrate empirically, on several benchmark datasets, that dCoT featuressignificantly improve the classification accuracy across several documentclassification tasks.
arxiv-2100-23 | Latent Relation Representations for Universal Schemas | http://arxiv.org/pdf/1301.4293v2.pdf | author:Sebastian Riedel, Limin Yao, Andrew McCallum category:cs.LG stat.ML published:2013-01-18 summary:Traditional relation extraction predicts relations within some fixed andfinite target schema. Machine learning approaches to this task require eithermanual annotation or, in the case of distant supervision, existing structuredsources of the same schema. The need for existing datasets can be avoided byusing a universal schema: the union of all involved schemas (surface formpredicates as in OpenIE, and relations in the schemas of pre-existingdatabases). This schema has an almost unlimited set of relations (due tosurface forms), and supports integration with existing structured data (throughthe relation types of existing databases). To populate a database of suchschema we present a family of matrix factorization models that predict affinitybetween database tuples and relations. We show that this achieves substantiallyhigher accuracy than the traditional classification approach. More importantly,by operating simultaneously on relations observed in text and in pre-existingstructured DBs such as Freebase, we are able to reason about unstructured andstructured data in mutually-supporting ways. By doing so our approachoutperforms state-of-the-art distant supervision systems.
arxiv-2100-24 | Discriminative Feature Selection for Uncertain Graph Classification | http://arxiv.org/pdf/1301.6626v1.pdf | author:Xiangnan Kong, Philip S. Yu, Xue Wang, Ann B. Ragin category:cs.LG cs.DB stat.ML published:2013-01-28 summary:Mining discriminative features for graph data has attracted much attention inrecent years due to its important role in constructing graph classifiers,generating graph indices, etc. Most measurement of interestingness ofdiscriminative subgraph features are defined on certain graphs, where thestructure of graph objects are certain, and the binary edges within each graphrepresent the "presence" of linkages among the nodes. In many real-worldapplications, however, the linkage structure of the graphs is inherentlyuncertain. Therefore, existing measurements of interestingness based uponcertain graphs are unable to capture the structural uncertainty in theseapplications effectively. In this paper, we study the problem of discriminativesubgraph feature selection from uncertain graphs. This problem is challengingand different from conventional subgraph mining problems because both thestructure of the graph objects and the discrimination score of each subgraphfeature are uncertain. To address these challenges, we propose a noveldiscriminative subgraph feature selection method, DUG, which can finddiscriminative subgraph features in uncertain graphs based upon differentstatistical measures including expectation, median, mode and phi-probability.We first compute the probability distribution of the discrimination scores foreach subgraph feature based on dynamic programming. Then a branch-and-boundalgorithm is proposed to search for discriminative subgraphs efficiently.Extensive experiments on various neuroimaging applications (i.e., Alzheimer'sDisease, ADHD and HIV) have been performed to analyze the gain in performanceby taking into account structural uncertainties in identifying discriminativesubgraph features for graph classification.
arxiv-2100-25 | An improvement to k-nearest neighbor classifier | http://arxiv.org/pdf/1301.6324v1.pdf | author:T. Hitendra Sarma, P. Viswanath, D. Sai Koti Reddy, S. Sri Raghava category:cs.CV cs.LG stat.ML published:2013-01-27 summary:K-Nearest neighbor classifier (k-NNC) is simple to use and has little designtime like finding k values in k-nearest neighbor classifier, hence these aresuitable to work with dynamically varying data-sets. There exists somefundamental improvements over the basic k-NNC, like weighted k-nearestneighbors classifier (where weights to nearest neighbors are given based onlinear interpolation), using artificially generated training set calledbootstrapped training set, etc. These improvements are orthogonal to spacereduction and classification time reduction techniques, hence can be coupledwith any of them. The paper proposes another improvement to the basic k-NNCwhere the weights to nearest neighbors are given based on Gaussian distribution(instead of linear interpolation as done in weighted k-NNC) which is alsoindependent of any space reduction and classification time reduction technique.We formally show that our proposed method is closely related to non-parametricdensity estimation using a Gaussian kernel. We experimentally demonstrate usingvarious standard data-sets that the proposed method is better than the existingones in most cases.
arxiv-2100-26 | Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines | http://arxiv.org/pdf/1210.8442v3.pdf | author:Louis Yuanlong Shao category:cs.AI cs.NE q-bio.NC stat.ML published:2012-10-31 summary:One conjecture in both deep learning and classical connectionist viewpoint isthat the biological brain implements certain kinds of deep networks as itsback-end. However, to our knowledge, a detailed correspondence has not yet beenset up, which is important if we want to bridge between neuroscience andmachine learning. Recent researches emphasized the biological plausibility ofLinear-Nonlinear-Poisson (LNP) neuron model. We show that with neurallyplausible settings, the whole network is capable of representing any Boltzmannmachine and performing a semi-stochastic Bayesian inference algorithm lyingbetween Gibbs sampling and variational inference.
arxiv-2100-27 | Generalized double Pareto shrinkage | http://arxiv.org/pdf/1104.0861v4.pdf | author:Artin Armagan, David Dunson, Jaeyong Lee category:stat.ME math.ST stat.ML stat.TH published:2011-04-05 summary:We propose a generalized double Pareto prior for Bayesian shrinkageestimation and inferences in linear models. The prior can be obtained via ascale mixture of Laplace or normal distributions, forming a bridge between theLaplace and Normal-Jeffreys' priors. While it has a spike at zero like theLaplace density, it also has a Student's $t$-like tail behavior. Bayesiancomputation is straightforward via a simple Gibbs sampling algorithm. Weinvestigate the properties of the maximum a posteriori estimator, as sparseestimation plays an important role in many problems, reveal connections withsome well-established regularization procedures, and show some asymptoticresults. The performance of the prior is tested through simulations and anapplication.
arxiv-2100-28 | LA-LDA: A Limited Attention Topic Model for Social Recommendation | http://arxiv.org/pdf/1301.6277v1.pdf | author:Jeon-Hyung Kang, Kristina Lerman, Lise Getoor category:cs.SI cs.IR cs.LG published:2013-01-26 summary:Social media users have finite attention which limits the number of incomingmessages from friends they can process. Moreover, they pay more attention toopinions and recommendations of some friends more than others. In this paper,we propose LA-LDA, a latent topic model which incorporates limited,non-uniformly divided attention in the diffusion process by which opinions andinformation spread on the social network. We show that our proposed model isable to learn more accurate user models from users' social network and itemadoption behavior than models which do not take limited attention into account.We analyze voting on news items on the social news aggregator Digg and showthat our proposed model is better able to predict held out votes thanalternative models. Our study demonstrates that psycho-socially motivatedmodels have better ability to describe and predict observed behavior thanmodels which only consider topics.
arxiv-2100-29 | Transfer Topic Modeling with Ease and Scalability | http://arxiv.org/pdf/1301.5686v2.pdf | author:Jeon-Hyung Kang, Jun Ma, Yan Liu category:cs.CL cs.LG stat.ML published:2013-01-24 summary:The increasing volume of short texts generated on social media sites, such asTwitter or Facebook, creates a great demand for effective and efficient topicmodeling approaches. While latent Dirichlet allocation (LDA) can be applied, itis not optimal due to its weakness in handling short texts with fast-changingtopics and scalability concerns. In this paper, we propose a transfer learningapproach that utilizes abundant labeled documents from other domains (such asYahoo! News or Wikipedia) to improve topic modeling, with better model fittingand result interpretation. Specifically, we develop Transfer Hierarchical LDA(thLDA) model, which incorporates the label information from other domains viainformative priors. In addition, we develop a parallel implementation of ourmodel for large-scale applications. We demonstrate the effectiveness of ourthLDA model on both a microblogging dataset and standard text collectionsincluding AP and RCV1 datasets.
arxiv-2100-30 | Mixture Gaussian Process Conditional Heteroscedasticity | http://arxiv.org/pdf/1211.4410v4.pdf | author:Emmanouil A. Platanios, Sotirios P. Chatzis category:cs.LG stat.ML published:2012-11-19 summary:Generalized autoregressive conditional heteroscedasticity (GARCH) models havelong been considered as one of the most successful families of approaches forvolatility modeling in financial return series. In this paper, we propose analternative approach based on methodologies widely used in the field ofstatistical machine learning. Specifically, we propose a novel nonparametricBayesian mixture of Gaussian process regression models, each component of whichmodels the noise variance process that contaminates the observed data as aseparate latent Gaussian process driven by the observed data. This way, weessentially obtain a mixture Gaussian process conditional heteroscedasticity(MGPCH) model for volatility modeling in financial return series. We impose anonparametric prior with power-law nature over the distribution of the modelmixture components, namely the Pitman-Yor process prior, to allow for bettercapturing modeled data distributions with heavy tails and skewness. Finally, weprovide a copula- based approach for obtaining a predictive posterior for thecovariances over the asset returns modeled by means of a postulated MGPCHmodel. We evaluate the efficacy of our approach in a number of benchmarkscenarios, and compare its performance to state-of-the-art methodologies.
arxiv-2100-31 | The Neural Representation Benchmark and its Evaluation on Brain and Machine | http://arxiv.org/pdf/1301.3530v2.pdf | author:Charles F. Cadieu, Ha Hong, Dan Yamins, Nicolas Pinto, Najib J. Majaj, James J. DiCarlo category:cs.NE cs.CV cs.LG q-bio.NC published:2013-01-15 summary:A key requirement for the development of effective learning representationsis their evaluation and comparison to representations we know to be effective.In natural sensory domains, the community has viewed the brain as a source ofinspiration and as an implicit benchmark for success. However, it has not beenpossible to directly test representational learning algorithms directly againstthe representations contained in neural systems. Here, we propose a newbenchmark for visual representations on which we have directly tested theneural representation in multiple visual cortical areas in macaque (utilizingdata from [Majaj et al., 2012]), and on which any computer vision algorithmthat produces a feature space can be tested. The benchmark measures theeffectiveness of the neural or machine representation by computing theclassification loss on the ordered eigendecomposition of a kernel matrix[Montavon et al., 2011]. In our analysis we find that the neural representationin visual area IT is superior to visual area V4. In our analysis ofrepresentational learning algorithms, we find that three-layer models approachthe representational performance of V4 and the algorithm in [Le et al., 2012]surpasses the performance of V4. Impressively, we find that a recent supervisedalgorithm [Krizhevsky et al., 2012] achieves performance comparable to that ofIT for an intermediate level of image variation difficulty, and surpasses IT ata higher difficulty level. We believe this result represents a major milestone:it is the first learning algorithm we have found that exceeds our currentestimate of IT representation performance. We hope that this benchmark willassist the community in matching the representational performance of visualcortex and will serve as an initial rallying point for further correspondencebetween representations derived in brains and machines.
arxiv-2100-32 | Weighted Last-Step Min-Max Algorithm with Improved Sub-Logarithmic Regret | http://arxiv.org/pdf/1301.6058v1.pdf | author:Edward Moroshko, Koby Crammer category:cs.LG published:2013-01-25 summary:In online learning the performance of an algorithm is typically compared tothe performance of a fixed function from some class, with a quantity calledregret. Forster proposed a last-step min-max algorithm which was somewhatsimpler than the algorithm of Vovk, yet with the same regret. In fact thealgorithm he analyzed assumed that the choices of the adversary are bounded,yielding artificially only the two extreme cases. We fix this problem byweighing the examples in such a way that the min-max problem will be welldefined, and provide analysis with logarithmic regret that may have bettermultiplicative factor than both bounds of Forster and Vovk. We also derive anew bound that may be sub-logarithmic, as a recent bound of Orabona et.al, butmay have better multiplicative factor. Finally, we analyze the algorithm in aweak-type of non-stationary setting, and show a bound that is sub-linear if thenon-stationarity is sub-linear as well.
arxiv-2100-33 | Statistical Mechanics of Dictionary Learning | http://arxiv.org/pdf/1203.6178v3.pdf | author:Ayaka Sakata, Yoshiyuki Kabashima category:cs.IT cs.LG math.IT published:2012-03-28 summary:Finding a basis matrix (dictionary) by which objective signals arerepresented sparsely is of major relevance in various scientific andtechnological fields. We consider a problem to learn a dictionary from a set oftraining signals. We employ techniques of statistical mechanics of disorderedsystems to evaluate the size of the training set necessary to typically succeedin the dictionary learning. The results indicate that the necessary size ismuch smaller than previously estimated, which theoretically supports and/orencourages the use of dictionary learning in practical situations.
arxiv-2100-34 | Explorative Data Analysis for Changes in Neural Activity | http://arxiv.org/pdf/1301.6027v1.pdf | author:Duncan A. J. Blythe, Frank C. Meinecke, Paul von Buenau, Klaus-Robert Mueller category:q-bio.QM stat.ML published:2013-01-25 summary:Neural recordings are nonstationary time series, i.e. their propertiestypically change over time. Identifying specific changes, e.g. those induced bya learning task, can shed light on the underlying neural processes. However,such changes of interest are often masked by strong unrelated changes, whichcan be of physiological origin or due to measurement artifacts. We propose anovel algorithm for disentangling such different causes of non-stationarity andin this manner enable better neurophysiological interpretation for a wider setof experimental paradigms. A key ingredient is the repeated application ofStationary Subspace Analysis (SSA) using different temporal scales. Theusefulness of our explorative approach is demonstrated in simulations, theoryand EEG experiments with 80 Brain-Computer-Interfacing (BCI) subjects.
arxiv-2100-35 | Phase Diagram and Approximate Message Passing for Blind Calibration and Dictionary Learning | http://arxiv.org/pdf/1301.5898v1.pdf | author:Florent Krzakala, Marc Mézard, Lenka Zdeborová category:cs.IT cs.LG math.IT published:2013-01-24 summary:We consider dictionary learning and blind calibration for signals andmatrices created from a random ensemble. We study the mean-squared error in thelimit of large signal dimension using the replica method and unveil theappearance of phase transitions delimiting impossible, possible-but-hard andpossible inference regions. We also introduce an approximate message passingalgorithm that asymptotically matches the theoretical performance, and showthrough numerical tests that it performs very well, for the calibrationproblem, for tractable system sizes.
arxiv-2100-36 | Fitness Landscape-Based Characterisation of Nature-Inspired Algorithms | http://arxiv.org/pdf/1210.3210v2.pdf | author:Matthew Crossley, Andy Nisbet, Martyn Amos category:cs.NE published:2012-10-11 summary:A significant challenge in nature-inspired algorithmics is the identificationof specific characteristics of problems that make them harder (or easier) tosolve using specific methods. The hope is that, by identifying thesecharacteristics, we may more easily predict which algorithms are best-suited toproblems sharing certain features. Here, we approach this problem using fitnesslandscape analysis. Techniques already exist for measuring the "difficulty" ofspecific landscapes, but these are often designed solely with evolutionaryalgorithms in mind, and are generally specific to discrete optimisation. Inthis paper we develop an approach for comparing a wide range of continuousoptimisation algorithms. Using a fitness landscape generation technique, wecompare six different nature-inspired algorithms and identify which methodsperform best on landscapes exhibiting specific features.
arxiv-2100-37 | Reinforcement learning from comparisons: Three alternatives is enough, two is not | http://arxiv.org/pdf/1301.5734v1.pdf | author:Benoit Laslier, Jean-Francois Laslier category:math.OC cs.LG math.PR published:2013-01-24 summary:The paper deals with the problem of finding the best alternatives on thebasis of pairwise comparisons when these comparisons need not be transitive. Inthis setting, we study a reinforcement urn model. We prove convergence to theoptimal solution when reinforcement of a winning alternative occurs each timeafter considering three random alternatives. The simpler process, whichreinforces the winner of a random pair does not always converges: it may cycle.
arxiv-2100-38 | Improved Cheeger's Inequality: Analysis of Spectral Partitioning Algorithms through Higher Order Spectral Gap | http://arxiv.org/pdf/1301.5584v1.pdf | author:Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Shayan Oveis Gharan, Luca Trevisan category:cs.DS cs.DM math.CO math.SP stat.ML published:2013-01-23 summary:Let \phi(G) be the minimum conductance of an undirected graph G, and let0=\lambda_1 <= \lambda_2 <=... <= \lambda_n <= 2 be the eigenvalues of thenormalized Laplacian matrix of G. We prove that for any graph G and any k >= 2, \phi(G) = O(k) \lambda_2 / \sqrt{\lambda_k}, and this performance guaranteeis achieved by the spectral partitioning algorithm. This improves Cheeger'sinequality, and the bound is optimal up to a constant factor for any k. Ourresult shows that the spectral partitioning algorithm is a constant factorapproximation algorithm for finding a sparse cut if \lambda_k$ is a constantfor some constant k. This provides some theoretical justification to itsempirical performance in image segmentation and clustering problems. We extendthe analysis to other graph partitioning problems, including multi-waypartition, balanced separator, and maximum cut.
arxiv-2100-39 | Multi-Class Detection and Segmentation of Objects in Depth | http://arxiv.org/pdf/1301.5582v1.pdf | author:Cheng Zhang, Hedvig Kjellstrom category:cs.CV cs.RO published:2013-01-23 summary:The quality of life of many people could be improved by autonomous humanoidrobots in the home. To function in the human world, a humanoid household robotmust be able to locate itself and perceive the environment like a human; sceneperception, object detection and segmentation, and object spatial localizationin 3D are fundamental capabilities for such humanoid robots. This paperpresents a 3D multi-class object detection and segmentation method. Thecontributions are twofold. Firstly, we present a multi-class detection method,where a minimal joint codebook is learned in a principled manner. Secondly, weincorporate depth information using RGB-D imagery, which increases therobustness of the method and gives the 3D location of objects -- necessarysince the robot reasons in 3D space. Experiments show that the multi-classextension improves the detection efficiency with respect to the number ofclasses and the depth extension improves the detection robustness and givesufficient natural 3D location of the objects.
arxiv-2100-40 | Approximate Learning in Complex Dynamic Bayesian Networks | http://arxiv.org/pdf/1301.6738v1.pdf | author:Raffaella Settimi, Jim Q. Smith, A. S. Gargoum category:cs.LG stat.ML published:2013-01-23 summary:In this paper we extend the work of Smith and Papamichail (1999) and presentfast approximate Bayesian algorithms for learning in complex scenarios where atany time frame, the relationships between explanatory state space variables canbe described by a Bayesian network that evolve dynamically over time and theobservations taken are not necessarily Gaussian. It uses recent developments inapproximate Bayesian forecasting methods in combination with more familiarGaussian propagation algorithms on junction trees. The procedure for learningstate parameters from data is given explicitly for common samplingdistributions and the methodology is illustrated through a real application.The efficiency of the dynamic approximation is explored by using the Hellingerdivergence measure and theoretical bounds for the efficacy of such a procedureare discussed.
arxiv-2100-41 | Variational Learning in Mixed-State Dynamic Graphical Models | http://arxiv.org/pdf/1301.6731v1.pdf | author:Vladimir Pavlovic, Brendan J. Frey, Thomas S. Huang category:cs.LG stat.ML published:2013-01-23 summary:Many real-valued stochastic time-series are locally linear (Gassian), butglobally non-linear. For example, the trajectory of a human hand gesture can beviewed as a linear dynamic system driven by a nonlinear dynamic system thatrepresents muscle actions. We present a mixed-state dynamic graphical model inwhich a hidden Markov model drives a linear dynamic system. This combinationallows us to model both the discrete and continuous causes of trajectories suchas human gestures. The number of computations needed for exact inference isexponential in the sequence length, so we derive an approximate variationalinference technique that can also be used to learn the parameters of thediscrete and continuous models. We show how the mixed-state model and thevariational technique can be used to classify human hand gestures made with acomputer mouse.
arxiv-2100-42 | Accelerating EM: An Empirical Study | http://arxiv.org/pdf/1301.6730v1.pdf | author:Luis E. Ortiz, Leslie Pack Kaelbling category:cs.LG stat.ML published:2013-01-23 summary:Many applications require that we learn the parameters of a model from data.EM is a method used to learn the parameters of probabilistic models for whichthe data for some of the variables in the models is either missing or hidden.There are instances in which this method is slow to converge. Therefore,several accelerations have been proposed to improve the method. None of theproposed acceleration methods are theoretically dominant and experimentalcomparisons are lacking. In this paper, we present the different proposedaccelerations and try to compare them experimentally. From the results of theexperiments, we argue that some acceleration of EM is always possible, but thatwhich acceleration is superior depends on properties of the problem.
arxiv-2100-43 | Learning Bayesian Networks with Restricted Causal Interactions | http://arxiv.org/pdf/1301.6727v1.pdf | author:Julian R. Neil, Chris S. Wallace, Kevin B. Korb category:cs.AI cs.LG stat.ML published:2013-01-23 summary:A major problem for the learning of Bayesian networks (BNs) is theexponential number of parameters needed for conditional probability tables.Recent research reduces this complexity by modeling local structure in theprobability tables. We examine the use of log-linear local models. Whilelog-linear models in this context are not new (Whittaker, 1990; Buntine, 1991;Neal, 1992; Heckerman and Meek, 1997), for structure learning they aregenerally subsumed under a naive Bayes model. We describe an alternativeinterpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metricfor structure learning of networks exhibiting causal independence, which weterm first-order networks (FONs). We also investigate local model selection ona node-by-node basis.
arxiv-2100-44 | Learning Bayesian Networks from Incomplete Data with Stochastic Search Algorithms | http://arxiv.org/pdf/1301.6726v1.pdf | author:James W. Myers, Kathryn Blackmond Laskey, Tod S. Levitt category:cs.AI cs.LG published:2013-01-23 summary:This paper describes stochastic search approaches, including a new stochasticalgorithm and an adaptive mutation operator, for learning Bayesian networksfrom incomplete data. This problem is characterized by a huge solution spacewith a highly multimodal landscape. State-of-the-art approaches all involveusing deterministic approaches such as the expectation-maximization algorithm.These approaches are guaranteed to find local maxima, but do not explore thelandscape for other modes. Our approach evolves structure and the missing data.We compare our stochastic algorithms and show they all produce accurateresults.
arxiv-2100-45 | Loopy Belief Propagation for Approximate Inference: An Empirical Study | http://arxiv.org/pdf/1301.6725v1.pdf | author:Kevin Murphy, Yair Weiss, Michael I. Jordan category:cs.AI cs.LG published:2013-01-23 summary:Recently, researchers have demonstrated that loopy belief propagation - theuse of Pearls polytree algorithm IN a Bayesian network WITH loops OF error-correcting codes.The most dramatic instance OF this IS the near Shannon - limitperformance OF Turbo Codes codes whose decoding algorithm IS equivalent TOloopy belief propagation IN a chain - structured Bayesian network. IN thispaper we ask : IS there something special about the error - correcting codecontext, OR does loopy propagation WORK AS an approximate inference schemeIN amore general setting? We compare the marginals computed using loopy propagationTO the exact ones IN four Bayesian network architectures, including two real -world networks : ALARM AND QMR.We find that the loopy beliefs often convergeAND WHEN they do, they give a good approximation TO the correctmarginals.However,ON the QMR network, the loopy beliefs oscillated AND had noobvious relationship TO the correct posteriors. We present SOME initialinvestigations INTO the cause OF these oscillations, AND show that SOME simplemethods OF preventing them lead TO the wrong results.
arxiv-2100-46 | A Variational Approximation for Bayesian Networks with Discrete and Continuous Latent Variables | http://arxiv.org/pdf/1301.6724v1.pdf | author:Kevin Murphy category:cs.AI cs.LG stat.ML published:2013-01-23 summary:We show how to use a variational approximation to the logistic function toperform approximate inference in Bayesian networks containing discrete nodeswith continuous parents. Essentially, we convert the logistic function to aGaussian, which facilitates exact inference, and then iteratively adjust thevariational parameters to improve the quality of the approximation. Wedemonstrate experimentally that this approximation is faster and potentiallymore accurate than sampling. We also introduce a simple new technique forhandling evidence, which allows us to handle arbitrary distributions onobserved nodes, as well as achieving a significant speedup in networks withdiscrete variables of large cardinality.
arxiv-2100-47 | A Bayesian Network Classifier that Combines a Finite Mixture Model and a Naive Bayes Model | http://arxiv.org/pdf/1301.6723v1.pdf | author:Stefano Monti, Gregory F. Cooper category:cs.LG cs.AI stat.ML published:2013-01-23 summary:In this paper we present a new Bayesian network model for classification thatcombines the naive-Bayes (NB) classifier and the finite-mixture (FM)classifier. The resulting classifier aims at relaxing the strong assumptions onwhich the two component models are based, in an attempt to improve on theirclassification performance, both in terms of accuracy and in terms ofcalibration of the estimated probabilities. The proposed classifier is obtainedby superimposing a finite mixture model on the set of feature variables of anaive Bayes model. We present experimental results that compare the predictiveperformance on real datasets of the new classifier with the predictiveperformance of the NB classifier and the FM classifier.
arxiv-2100-48 | On Supervised Selection of Bayesian Networks | http://arxiv.org/pdf/1301.6710v1.pdf | author:Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri category:cs.LG stat.ML published:2013-01-23 summary:Given a set of possible models (e.g., Bayesian network structures) and a datasample, in the unsupervised model selection problem the task is to choose themost accurate model with respect to the domain joint probability distribution.In contrast to this, in supervised model selection it is a priori known thatthe chosen model will be used in the future for prediction tasks involving more``focused' predictive distributions. Although focused predictive distributionscan be produced from the joint probability distribution by marginalization, inpractice the best model in the unsupervised sense does not necessarily performwell in supervised domains. In particular, the standard marginal likelihoodscore is a criterion for the unsupervised task, and, although frequently usedfor supervised model selection also, does not perform well in such tasks. Inthis paper we study the performance of the marginal likelihood scoreempirically in supervised Bayesian network selection tasks by using a largenumber of publicly available classification data sets, and compare the resultsto those obtained by alternative model selection criteria, including empiricalcrossvalidation methods, an approximation of a supervised marginal likelihoodmeasure, and a supervised version of Dawids prequential(predictive sequential)principle.The results demonstrate that the marginal likelihood score does NOTperform well FOR supervised model selection, WHILE the best results areobtained BY using Dawids prequential r napproach.
arxiv-2100-49 | Probabilistic Latent Semantic Analysis | http://arxiv.org/pdf/1301.6705v1.pdf | author:Thomas Hofmann category:cs.LG cs.IR stat.ML published:2013-01-23 summary:Probabilistic Latent Semantic Analysis is a novel statistical technique forthe analysis of two-mode and co-occurrence data, which has applications ininformation retrieval and filtering, natural language processing, machinelearning from text, and in related areas. Compared to standard Latent SemanticAnalysis which stems from linear algebra and performs a Singular ValueDecomposition of co-occurrence tables, the proposed method is based on amixture decomposition derived from a latent class model. This results in a moreprincipled approach which has a solid foundation in statistics. In order toavoid overfitting, we propose a widely applicable generalization of maximumlikelihood model fitting by tempered EM. Our approach yields substantial andconsistent improvements over Latent Semantic Analysis in a number ofexperiments.
arxiv-2100-50 | Multi-objects association in perception of dynamical situation | http://arxiv.org/pdf/1301.6701v1.pdf | author:Dominique Gruyer, Veronique Berge-Cherfaoui category:cs.AI cs.CV published:2013-01-23 summary:In current perception systems applied to the rebuilding of the environmentfor intelligent vehicles, the part reserved to object association for thetracking is increasingly significant. This allows firstly to follow the objectstemporal evolution and secondly to increase the reliability of environmentperception. We propose in this communication the development of a multi-objectsassociation algorithm with ambiguity removal entering into the design of such adynamic perception system for intelligent vehicles. This algorithm uses thebelief theory and data modelling with fuzzy mathematics in order to be able tohandle inaccurate as well as uncertain information due to imperfect sensors.These theories also allow the fusion of numerical as well as symbolic data. Wedevelop in this article the problem of matching between known and perceivedobjects. This makes it possible to update a dynamic environment map for avehicle. The belief theory will enable us to quantify the belief in theassociation of each perceived object with each known object. Conflicts canappear in the case of object appearance or disappearance, or in the case of aconfused situation or bad perception. These conflicts are removed or solvedusing an assignment algorithm, giving a solution called the " best " and soensuring the tracking of some objects present in our environment.
arxiv-2100-51 | Learning Bayesian Network Structure from Massive Datasets: The "Sparse Candidate" Algorithm | http://arxiv.org/pdf/1301.6696v1.pdf | author:Nir Friedman, Iftach Nachman, Dana Pe'er category:cs.LG cs.AI stat.ML published:2013-01-23 summary:Learning Bayesian networks is often cast as an optimization problem, wherethe computational task is to find a structure that maximizes a statisticallymotivated score. By and large, existing learning tools address thisoptimization problem using standard heuristic search techniques. Since thesearch space is extremely large, such search procedures can spend most of thetime examining candidates that are extremely unreasonable. This problem becomescritical when we deal with data sets that are large either in the number ofinstances, or the number of attributes. In this paper, we introduce analgorithm that achieves faster learning by restricting the search space. Thisiterative algorithm restricts the parents of each variable to belong to a smallsubset of candidates. We then search for a network that satisfies theseconstraints. The learned network is then used for selecting better candidatesfor the next iteration. We evaluate this algorithm both on synthetic andreal-life data. Our results show that it is significantly faster thanalternative search procedures without loss of quality in the learnedstructures.
arxiv-2100-52 | Data Analysis with Bayesian Networks: A Bootstrap Approach | http://arxiv.org/pdf/1301.6695v1.pdf | author:Nir Friedman, Moises Goldszmidt, Abraham Wyner category:cs.LG cs.AI stat.ML published:2013-01-23 summary:In recent years there has been significant progress in algorithms and methodsfor inducing Bayesian networks from data. However, in complex data analysisproblems, we need to go beyond being satisfied with inducing networks with highscores. We need to provide confidence measures on features of these networks:Is the existence of an edge between two nodes warranted? Is the Markov blanketof a given node robust? Can we say something about the ordering of thevariables? We should be able to address these questions, even when the amountof data is not enough to induce a high scoring network. In this paper wepropose Efron's Bootstrap as a computationally efficient approach for answeringthese questions. In addition, we propose to use these confidence measures toinduce better structures from the data, and to detect the presence of latentvariables.
arxiv-2100-53 | Model-Based Bayesian Exploration | http://arxiv.org/pdf/1301.6690v1.pdf | author:Richard Dearden, Nir Friedman, David Andre category:cs.AI cs.LG published:2013-01-23 summary:Reinforcement learning systems are often concerned with balancing explorationof untested actions against exploitation of actions that are known to be good.The benefit of exploration can be estimated using the classical notion of Valueof Information - the expected improvement in future decision quality arisingfrom the information acquired by exploration. Estimating this quantity requiresan assessment of the agent's uncertainty about its current value estimates forstates. In this paper we investigate ways of representing and reasoning aboutthis uncertainty in algorithms where the system attempts to learn a model ofits environment. We explicitly represent uncertainty about the parameters ofthe model and build probability distributions over Q-values based on these.These distributions are used to compute a myopic approximation to the value ofinformation for each action and hence to select the action that best balancesexploration and exploitation.
arxiv-2100-54 | Learning Polytrees | http://arxiv.org/pdf/1301.6688v1.pdf | author:Sanjoy Dasgupta category:cs.AI cs.LG published:2013-01-23 summary:We consider the task of learning the maximum-likelihood polytree from data.Our first result is a performance guarantee establishing that the optimalbranching (or Chow-Liu tree), which can be computed very easily, constitutes agood approximation to the best polytree. We then show that it is not possibleto do very much better, since the learning problem is NP-hard even toapproximately solve within some constant factor.
arxiv-2100-55 | Comparing Bayesian Network Classifiers | http://arxiv.org/pdf/1301.6684v1.pdf | author:Jie Cheng, Russell Greiner category:cs.LG cs.AI stat.ML published:2013-01-23 summary:In this paper, we empirically evaluate algorithms for learning four types ofBayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BNaugmented Naive-Bayes and general BNs, where the latter two are learned usingtwo variants of a conditional-independence (CI) based BN-learning algorithm.Experimental results show the obtained classifiers, learned using the CI basedalgorithms, are competitive with (or superior to) the best known classifiers,based on both Bayesian networks and other formalisms; and that thecomputational time for learning and using these classifiers is relativelysmall. Moreover, these results also suggest a way to learn yet more effectiveclassifiers; we demonstrate empirically that this new algorithm does work asexpected. Collectively, these results argue that BN classifiers deserve moreattention in machine learning and data mining communities.
arxiv-2100-56 | Discovering the Hidden Structure of Complex Dynamic Systems | http://arxiv.org/pdf/1301.6683v1.pdf | author:Xavier Boyen, Nir Friedman, Daphne Koller category:cs.AI cs.LG published:2013-01-23 summary:Dynamic Bayesian networks provide a compact and natural representation forcomplex dynamic systems. However, in many cases, there is no expert availablefrom whom a model can be elicited. Learning provides an alternative approachfor constructing models of dynamic systems. In this paper, we address some ofthe crucial computational aspects of learning the structure of dynamic systems,particularly those where some relevant variables are partially observed or evenentirely unknown. Our approach is based on the Structural ExpectationMaximization (SEM) algorithm. The main computational cost of the SEM algorithmis the gathering of expected sufficient statistics. We propose a novelapproximation scheme that allows these sufficient statistics to be computedefficiently. We also investigate the fundamental problem of discovering theexistence of hidden variables without exhaustive and expensive search. Ourapproach is based on the observation that, in dynamic systems, ignoring ahidden variable typically results in a violation of the Markov property. Thus,our algorithm searches for such violations in the data, and introduces hiddenvariables to explain them. We provide empirical results showing that thealgorithm is able to learn the dynamics of complex systems in a computationallytractable way.
arxiv-2100-57 | Relative Loss Bounds for On-line Density Estimation with the Exponential Family of Distributions | http://arxiv.org/pdf/1301.6677v1.pdf | author:Katy S. Azoury, Manfred K. Warmuth category:cs.LG stat.ML published:2013-01-23 summary:We consider on-line density estimation with a parameterized density from theexponential family. The on-line algorithm receives one example at a time andmaintains a parameter that is essentially an average of the past examples.After receiving an example the algorithm incurs a loss which is the negativelog-likelihood of the example w.r.t. the past parameter of the algorithm. Anoff-line algorithm can choose the best parameter based on all the examples. Weprove bounds on the additional total loss of the on-line algorithm over thetotal loss of the off-line algorithm. These relative loss bounds hold for anarbitrary sequence of examples. The goal is to design algorithms with the bestpossible relative loss bounds. We use a certain divergence to derive andanalyze the algorithms. This divergence is a relative entropy between twoexponential distributions.
arxiv-2100-58 | Inferring Parameters and Structure of Latent Variable Models by Variational Bayes | http://arxiv.org/pdf/1301.6676v1.pdf | author:Hagai Attias category:cs.LG stat.ML published:2013-01-23 summary:Current methods for learning graphical models with latent variables and afixed structure estimate optimal values for the model parameters. Whereas thisapproach usually produces overfitting and suboptimal generalizationperformance, carrying out the Bayesian program of computing the full posteriordistributions over the parameters remains a difficult problem. Moreover,learning the structure of models with latent variables, for which the Bayesianapproach is crucial, is yet a harder problem. In this paper I present theVariational Bayes framework, which provides a solution to these problems. Thisapproach approximates full posterior distributions over model parameters andstructures, as well as latent variables, in an analytical manner withoutresorting to sampling methods. Unlike in the Laplace approximation, theseposteriors are generally non-Gaussian and no Hessian needs to be computed. Theresulting algorithm generalizes the standard Expectation Maximizationalgorithm, and its convergence is guaranteed. I demonstrate that this algorithmcan be applied to a large class of models in several domains, includingunsupervised clustering and blind source separation.
arxiv-2100-59 | ChESS - Quick and Robust Detection of Chess-board Features | http://arxiv.org/pdf/1301.5491v1.pdf | author:Stuart Bennett, Joan Lasenby category:cs.CV published:2013-01-23 summary:Localization of chess-board vertices is a common task in computer vision,underpinning many applications, but relatively little work focusses ondesigning a specific feature detector that is fast, accurate and robust. Inthis paper the `Chess-board Extraction by Subtraction and Summation' (ChESS)feature detector, designed to exclusively respond to chess-board vertices, ispresented. The method proposed is robust against noise, poor lighting and poorcontrast, requires no prior knowledge of the extent of the chess-board pattern,is computationally very efficient, and provides a strength measure of detectedfeatures. Such a detector has significant application both in the key field ofcamera calibration, as well as in Structured Light 3D reconstruction. Evidenceis presented showing its robustness, accuracy, and efficiency in comparison toother commonly used detectors both under simulation and in experimental 3Dreconstruction of flat plate and cylindrical objects
arxiv-2100-60 | Multi-class Generalized Binary Search for Active Inverse Reinforcement Learning | http://arxiv.org/pdf/1301.5488v1.pdf | author:Francisco Melo, Manuel Lopes category:cs.LG cs.AI stat.ML published:2013-01-23 summary:This paper addresses the problem of learning a task from demonstration. Weadopt the framework of inverse reinforcement learning, where tasks arerepresented in the form of a reward function. Our contribution is a novelactive learning algorithm that enables the learning agent to query the expertfor more informative demonstrations, thus leading to more sample-efficientlearning. For this novel algorithm (Generalized Binary Search for InverseReinforcement Learning, or GBS-IRL), we provide a theoretical bound on samplecomplexity and illustrate its applicability on several different tasks. To ourknowledge, GBS-IRL is the first active IRL algorithm with provable samplecomplexity bounds. We also discuss our method in light of other existingmethods in the literature and its general applicability in multi-classclassification problems. Finally, motivated by recent work on learning fromdemonstration in robots, we also discuss how different forms of human feedbackcan be integrated in a transparent manner in our learning framework.
arxiv-2100-61 | Spread spectrum compressed sensing MRI using chirp radio frequency pulses | http://arxiv.org/pdf/1301.5451v1.pdf | author:Xiaobo Qu, Ying Chen, Xiaoxing Zhuang, Zhiyu Yan, Di Guo, Zhong Chen category:cs.CV math.OC physics.med-ph published:2013-01-23 summary:Compressed sensing has shown great potential in reducing data acquisitiontime in magnetic resonance imaging (MRI). Recently, a spread spectrumcompressed sensing MRI method modulates an image with a quadratic phase. Itperforms better than the conventional compressed sensing MRI with variabledensity sampling, since the coherence between the sensing and sparsity basesare reduced. However, spread spectrum in that method is implemented via a shimcoil which limits its modulation intensity and is not convenient to operate. Inthis letter, we propose to apply chirp (linear frequency-swept) radio frequencypulses to easily control the spread spectrum. To accelerate the imagereconstruction, an alternating direction algorithm is modified by exploitingthe complex orthogonality of the quadratic phase encoding. Reconstruction onthe acquired data demonstrates that more image features are preserved using theproposed approach than those of conventional CS-MRI.
arxiv-2100-62 | Online Learning with Pairwise Loss Functions | http://arxiv.org/pdf/1301.5332v1.pdf | author:Yuyang Wang, Roni Khardon, Dmitry Pechyony, Rosie Jones category:stat.ML cs.LG published:2013-01-22 summary:Efficient online learning with pairwise loss functions is a crucial componentin building large-scale learning system that maximizes the area under theReceiver Operator Characteristic (ROC) curve. In this paper we investigate thegeneralization performance of online learning algorithms with pairwise lossfunctions. We show that the existing proof techniques for generalization boundsof online algorithms with a univariate loss can not be directly applied topairwise losses. In this paper, we derive the first result providingdata-dependent bounds for the average risk of the sequence of hypothesesgenerated by an arbitrary online learner in terms of an easily computablestatistic, and show how to extract a low risk hypothesis from the sequence. Wedemonstrate the generality of our results by applying it to two importantproblems in machine learning. First, we analyze two online algorithms forbipartite ranking; one being a natural extension of the perceptron algorithmand the other using online convex optimization. Secondly, we provide ananalysis for the risk bound for an online algorithm for supervised metriclearning.
arxiv-2100-63 | Stochastic ADMM for Nonsmooth Optimization | http://arxiv.org/pdf/1211.0632v2.pdf | author:Hua Ouyang, Niao He, Alexander Gray category:cs.LG math.OC stat.ML published:2012-11-03 summary:We present a stochastic setting for optimization problems with nonsmoothconvex separable objective functions over linear equality constraints. To solvesuch problems, we propose a stochastic Alternating Direction Method ofMultipliers (ADMM) algorithm. Our algorithm applies to a more general class ofnonsmooth convex functions that does not necessarily have a closed-formsolution by minimizing the augmented function directly. We also demonstrate therates of convergence for our algorithm under various structural assumptions ofthe stochastic functions: $O(1/\sqrt{t})$ for convex functions and $O(\logt/t)$ for strongly convex functions. Compared to previous literature, weestablish the convergence rate of ADMM algorithm, for the first time, in termsof both the objective value and the feasibility violation.
arxiv-2100-64 | Active Learning on Trees and Graphs | http://arxiv.org/pdf/1301.5112v1.pdf | author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG stat.ML published:2013-01-22 summary:We investigate the problem of active learning on a given tree whose nodes areassigned binary labels in an adversarial way. Inspired by recent results byGuillory and Bilmes, we characterize (up to constant factors) the optimalplacement of queries so to minimize the mistakes made on the non-queried nodes.Our query selection algorithm is extremely efficient, and the optimal number ofmistakes on the non-queried nodes is achieved by a simple and efficient mincutclassifier. Through a simple modification of the query selection algorithm wealso show optimality (up to constant factors) with respect to the trade-offbetween number of queries and number of mistakes on non-queried nodes. By usingspanning trees, our algorithms can be efficiently applied to general graphs,although the problem of finding optimal and efficient active learningalgorithms for general graphs remains open. Towards this end, we provide alower bound on the number of mistakes made on arbitrary graphs by any activelearning algorithm using a number of queries which is up to a constant fractionof the graph size.
arxiv-2100-65 | Piecewise Linear Multilayer Perceptrons and Dropout | http://arxiv.org/pdf/1301.5088v1.pdf | author:Ian J. Goodfellow category:stat.ML cs.LG published:2013-01-22 summary:We propose a new type of hidden layer for a multilayer perceptron, anddemonstrate that it obtains the best reported performance for an MLP on theMNIST dataset.
arxiv-2100-66 | Evaluation of a Supervised Learning Approach for Stock Market Operations | http://arxiv.org/pdf/1301.4944v1.pdf | author:Marcelo S. Lauretto, Barbara B. C. Silva, Pablo M. Andrade category:stat.ML cs.LG stat.AP published:2013-01-21 summary:Data mining methods have been widely applied in financial markets, with thepurpose of providing suitable tools for prices forecasting and automatictrading. Particularly, learning methods aim to identify patterns in time seriesand, based on such patterns, to recommend buy/sell operations. The objective ofthis work is to evaluate the performance of Random Forests, a supervisedlearning method based on ensembles of decision trees, for decision support instock markets. Preliminary results indicate good rates of successful operationsand good rates of return per operation, providing a strong motivation forfurther research in this topic.
arxiv-2100-67 | Dirichlet draws are sparse with high probability | http://arxiv.org/pdf/1301.4917v1.pdf | author:Matus Telgarsky category:cs.LG math.PR stat.ML published:2013-01-21 summary:This note provides an elementary proof of the folklore fact that draws from aDirichlet distribution (with parameters less than 1) are typically sparse (mostcoordinates are small).
arxiv-2100-68 | Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots | http://arxiv.org/pdf/1301.4862v1.pdf | author:Adrien Baranes, Pierre-Yves Oudeyer category:cs.LG cs.AI cs.CV cs.NE cs.RO published:2013-01-21 summary:We introduce the Self-Adaptive Goal Generation - Robust Intelligent AdaptiveCuriosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goalexploration mechanism which allows active learning of inverse models inhigh-dimensional redundant robots. This allows a robot to efficiently andactively learn distributions of parameterized motor skills/policies that solvea corresponding distribution of parameterized tasks/goals. The architecturemakes the robot sample actively novel parameterized tasks in the task space,based on a measure of competence progress, each of which triggers low-levelgoal-directed learning of the motor policy pa- rameters that allow to solve it.For both learning and generalization, the system leverages regressiontechniques which allow to infer the motor policy parameters corresponding to agiven novel parameterized task, and based on the previously learntcorrespondences between policy and task parameters. We present experiments withhigh-dimensional continuous sensorimotor spaces in three different roboticsetups: 1) learning the inverse kinematics in a highly-redundant robotic arm,2) learning omnidirectional locomotion with motor primitives in a quadrupedrobot, 3) an arm learning to control a fishing rod with a flexible wire. Weshow that 1) exploration in the task space can be a lot faster than explorationin the actuator space for learning inverse models in redundant robots; 2)selecting goals maximizing competence progress creates developmentaltrajectories driving the robot to progressively focus on tasks of increasingcomplexity and is statistically significantly more efficient than selectingtasks randomly, as well as more efficient than different standard active motorbabbling methods; 3) this architecture allows the robot to actively discoverwhich parts of its task space it can learn to reach and which part it cannot.
arxiv-2100-69 | Inverse Reinforcement Learning with Gaussian Process | http://arxiv.org/pdf/1208.2112v2.pdf | author:Qifeng Qiao, Peter A. Beling category:cs.LG published:2012-08-10 summary:We present new algorithms for inverse reinforcement learning (IRL, or inverseoptimal control) in convex optimization settings. We argue that finite-spaceIRL can be posed as a convex quadratic program under a Bayesian inferenceframework with the objective of maximum a posterior estimation. To deal withproblems in large or even infinite state space, we propose a Gaussian processmodel and use preference graphs to represent observations of decisiontrajectories. Our method is distinguished from other approaches to IRL in thatit makes no assumptions about the form of the reward function and yet itretains the promise of computationally manageable implementations for potentialreal-world applications. In comparison with an establish algorithm onsmall-scale numerical problems, our method demonstrated better accuracy inapprenticeship learning and a more robust dependence on the number ofobservations.
arxiv-2100-70 | Pattern Matching for Self- Tuning of MapReduce Jobs | http://arxiv.org/pdf/1301.4753v1.pdf | author:Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya category:cs.DC cs.AI cs.LG published:2013-01-21 summary:In this paper, we study CPU utilization time patterns of several MapReduceapplications. After extracting running patterns of several applications, theyare saved in a reference database to be later used to tweak system parametersto efficiently execute unknown applications in future. To achieve this goal,CPU utilization patterns of new applications are compared with the alreadyknown ones in the reference database to find/predict their most probableexecution patterns. Because of different patterns lengths, the Dynamic TimeWarping (DTW) is utilized for such comparison; a correlation analysis is thenapplied to DTWs outcomes to produce feasible similarity patterns. Three realapplications (WordCount, Exim Mainlog parsing and Terasort) are used toevaluate our hypothesis in tweaking system parameters in executing similarapplications. Results were very promising and showed effectiveness of ourapproach on pseudo-distributed MapReduce platforms.
arxiv-2100-71 | Joint Space Neural Probabilistic Language Model for Statistical Machine Translation | http://arxiv.org/pdf/1301.3614v2.pdf | author:Tsuyoshi Okita category:cs.CL published:2013-01-16 summary:A neural probabilistic language model (NPLM) provides an idea to achieve thebetter perplexity than n-gram language model and their smoothed languagemodels. This paper investigates application area in bilingual NLP, specificallyStatistical Machine Translation (SMT). We focus on the perspectives that NPLMhas potential to open the possibility to complement potentially `huge'monolingual resources into the `resource-constraint' bilingual resources. Weintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesianconstruction. In order to facilitate the application to various tasks, wepropose the joint space model of ngram-HMM language model. We show anexperiment of system combination in the area of SMT. One discovery was that ourtreatment of noise improved the results 0.20 BLEU points if NPLM is trained inrelatively small corpus, in our case 500,000 sentence pairs, which is often thecase due to the long training time of NPLM.
arxiv-2100-72 | Recurrent Neural Network Method in Arabic Words Recognition System | http://arxiv.org/pdf/1301.4662v1.pdf | author:Yusuf Perwej category:cs.NE published:2013-01-20 summary:The recognition of unconstrained handwriting continues to be a difficult taskfor computers despite active research for several decades. This is becausehandwritten text offers great challenges such as character and wordsegmentation, character recognition, variation between handwriting styles,different character size and no font constraints as well as the backgroundclarity. In this paper primarily discussed Online Handwriting Recognitionmethods for Arabic words which being often used among then across the MiddleEast and North Africa people. Because of the characteristic of the whole bodyof the Arabic words, namely connectivity between the characters, thereby thesegmentation of An Arabic word is very difficult. We introduced a recurrentneural network to online handwriting Arabic word recognition. The keyinnovation is a recently produce recurrent neural networks objective functionknown as connectionist temporal classification. The system consists of anadvanced recurrent neural network with an output layer designed for sequencelabeling, partially combined with a probabilistic language model. Experimentalresults show that unconstrained Arabic words achieve recognition rates about79%, which is significantly higher than the about 70% using a previouslydeveloped hidden markov model based recognition system.
arxiv-2100-73 | Switched linear encoding with rectified linear autoencoders | http://arxiv.org/pdf/1301.3753v2.pdf | author:Leif Johnson, Craig Corcoran category:cs.LG published:2013-01-16 summary:Several recent results in machine learning have established formalconnections between autoencoders---artificial neural network models thatattempt to reproduce their inputs---and other coding models like sparse codingand K-means. This paper explores in depth an autoencoder model that isconstructed using rectified linear activations on its hidden units. Ouranalysis builds on recent results to further unify the world of sparse linearcoding models. We provide an intuitive interpretation of the behavior of thesecoding models and demonstrate this intuition using small, artificial datasetswith known distributions.
arxiv-2100-74 | Lip Localization and Viseme Classification for Visual Speech Recognition | http://arxiv.org/pdf/1301.4558v1.pdf | author:Salah Werda, Walid Mahdi, Abdelmajid Ben Hamadou category:cs.CV published:2013-01-19 summary:The need for an automatic lip-reading system is ever increasing. Infact,today, extraction and reliable analysis of facial movements make up animportant part in many multimedia systems such as videoconference, lowcommunication systems, lip-reading systems. In addition, visual information isimperative among people with special needs. We can imagine, for example, adependent person ordering a machine with an easy lip movement or by a simplesyllable pronunciation. Moreover, people with hearing problems compensate fortheir special needs by lip-reading as well as listening to the person withwhome they are talking.
arxiv-2100-75 | Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means | http://arxiv.org/pdf/1207.4417v2.pdf | author:Jingwei Liu, Meizhi Xu category:cs.CV stat.CO published:2012-07-18 summary:A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithmis proposed with iterative reweighted least squares (IRLS) algorithm, andpenalty constraint and kernelization extensions of MFCM algorithms are alsodeveloped. Introducing penalty information to the object functions of MFCMalgorithms, the spatially constrained fuzzy C-means (SFCM) is extended topenalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclideandistance with kernel method, the MFCM and pMFCM algorithms are extended tokernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated inthree tasks: pattern recognition on 10 standard data sets from UCI MachineLearning databases, noise image segmentation performances on a synthetic image,a magnetic resonance brain image (MRI), and image segmentation of a standardimages from Berkeley Segmentation Dataset and Benchmark. The experimentalresults demonstrate the effectiveness of our proposed algorithms in patternrecognition and image segmentation.
arxiv-2100-76 | Efficient Sparse Group Feature Selection via Nonconvex Optimization | http://arxiv.org/pdf/1205.5075v2.pdf | author:Shuo Xiang, Xiaotong Shen, Jieping Ye category:cs.LG stat.ML published:2012-05-23 summary:Sparse feature selection has been demonstrated to be effective in handlinghigh-dimensional data. While promising, most of the existing works use convexmethods, which may be suboptimal in terms of the accuracy of feature selectionand parameter estimation. In this paper, we expand a nonconvex paradigm tosparse group feature selection, which is motivated by applications that requireidentifying the underlying group structure and performing feature selectionsimultaneously. The main contributions of this article are twofold: (1)statistically, we introduce a nonconvex sparse group feature selection modelwhich can reconstruct the oracle estimator. Therefore, consistent featureselection and parameter estimation can be achieved; (2) computationally, wepropose an efficient algorithm that is applicable to large-scale problems.Numerical results suggest that the proposed nonconvex method compares favorablyagainst its competitors on synthetic data and real-world applications, thusachieving desired goal of delivering high performance.
arxiv-2100-77 | Language learning from positive evidence, reconsidered: A simplicity-based approach | http://arxiv.org/pdf/1301.4432v1.pdf | author:Anne S. Hsu, Nick Chater, Paul M. B. Vitányi category:cs.CL published:2013-01-18 summary:Children learn their native language by exposure to their linguistic andcommunicative environment, but apparently without requiring that their mistakesare corrected. Such learning from positive evidence has been viewed as raisinglogical problems for language acquisition. In particular, without correction,how is the child to recover from conjecturing an over-general grammar, whichwill be consistent with any sentence that the child hears? There have been manyproposals concerning how this logical problem can be dissolved. Here, we reviewrecent formal results showing that the learner has sufficient data to learnsuccessfully from positive evidence, if it favours the simplest encoding of thelinguistic input. Results include the ability to learn a linguistic prediction,grammaticality judgements, language production, and form-meaning mappings. Thesimplicity approach can also be scaled-down to analyse the ability to learn aspecific linguistic constructions, and is amenable to empirical test as aframework for describing human language acquisition.
arxiv-2100-78 | Multiple models of Bayesian networks applied to offline recognition of Arabic handwritten city names | http://arxiv.org/pdf/1301.4377v1.pdf | author:Mohamed Ali Mahjoub, Nabil Ghanmy, Khlifia jayech, Ikram Miled category:cs.CV published:2013-01-18 summary:In this paper we address the problem of offline Arabic handwriting wordrecognition. Off-line recognition of handwritten words is a difficult task dueto the high variability and uncertainty of human writing. The majority of therecent systems are constrained by the size of the lexicon to deal with and thenumber of writers. In this paper, we propose an approach for multi-writersArabic handwritten words recognition using multiple Bayesian networks. First,we cut the image in several blocks. For each block, we compute a vector ofdescriptors. Then, we use K-means to cluster the low-level features includingZernik and Hu moments. Finally, we apply four variants of Bayesian networksclassifiers (Na\"ive Bayes, Tree Augmented Na\"ive Bayes (TAN), ForestAugmented Na\"ive Bayes (FAN) and DBN (dynamic bayesian network) to classifythe whole image of tunisian city name. The results demonstrate FAN and DBNoutperform good recognition rates
arxiv-2100-79 | A Study on Using Uncertain Time Series Matching Algorithms in MapReduce Applications | http://arxiv.org/pdf/1112.5505v5.pdf | author:Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya, Reza Moraveji category:cs.DC cs.AI cs.LG cs.PF published:2011-12-23 summary:In this paper, we study CPU utilization time patterns of several Map-Reduceapplications. After extracting running patterns of several applications, thepatterns with their statistical information are saved in a reference databaseto be later used to tweak system parameters to efficiently execute unknownapplications in future. To achieve this goal, CPU utilization patterns of newapplications along with its statistical information are compared with thealready known ones in the reference database to find/predict their mostprobable execution patterns. Because of different patterns lengths, the DynamicTime Warping (DTW) is utilized for such comparison; a statistical analysis isthen applied to DTWs' outcomes to select the most suitable candidates.Moreover, under a hypothesis, another algorithm is proposed to classifyapplications under similar CPU utilization patterns. Three widely used textprocessing applications (WordCount, Distributed Grep, and Terasort) and anotherapplication (Exim Mainlog parsing) are used to evaluate our hypothesis intweaking system parameters in executing similar applications. Results were verypromising and showed effectiveness of our approach on 5-node Map-Reduceplatform
arxiv-2100-80 | Ag-dependent (in silico) approach implies a deterministic kinetics for homeostatic memory cell turnover | http://arxiv.org/pdf/1111.2085v3.pdf | author:Alexandre de Castro category:q-bio.CB cs.NE published:2011-11-09 summary:Verhulst-like mathematical modeling has been used to investigate severalcomplex biological issues, such as immune memory equilibrium and cell-mediatedimmunity in mammals. The regulation mechanisms of both these processes arestill not sufficiently understood. In a recent paper, Choo et al. [J. Immunol.,v. 185, pp. 3436-44, 2010], used an Ag-independent approach to quantitativelyanalyze memory cell turnover from some empirical data, and concluded thatimmune homeostasis behaves stochastically, rather than deterministically. Inthe paper here presented, we use an in silico Ag-dependent approach to simulatethe process of antigenic mutation and study its implications for memorydynamics. Our results have suggested a deterministic kinetics for homeostaticequilibrium, what contradicts the Choo et al. findings. Accordingly, ourcalculations are an indication that a more extensive empirical protocol forstudying the homeostatic turnover should be considered.
arxiv-2100-81 | A Spectral Algorithm for Latent Dirichlet Allocation | http://arxiv.org/pdf/1204.6703v4.pdf | author:Animashree Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade, Yi-Kai Liu category:cs.LG stat.ML published:2012-04-30 summary:The problem of topic modeling can be seen as a generalization of theclustering problem, in that it posits that observations are generated due tomultiple latent factors (e.g., the words in each document are generated as amixture of several active topics, as opposed to just one). This increasedrepresentational power comes at the cost of a more challenging unsupervisedlearning problem of estimating the topic probability vectors (the distributionsover words for each topic), when only the words are observed and thecorresponding topics are hidden. We provide a simple and efficient learning procedure that is guaranteed torecover the parameters for a wide class of mixture models, including thepopular latent Dirichlet allocation (LDA) model. For LDA, the procedurecorrectly recovers both the topic probability vectors and the prior over thetopics, using only trigram statistics (i.e., third order moments, which may beestimated with documents containing just three words). The method, termedExcess Correlation Analysis (ECA), is based on a spectral decomposition of loworder moments (third and fourth order) via two singular value decompositions(SVDs). Moreover, the algorithm is scalable since the SVD operations arecarried out on $k\times k$ matrices, where $k$ is the number of latent factors(e.g. the number of topics), rather than in the $d$-dimensional observed space(typically $d \gg k$).
arxiv-2100-82 | Financial Portfolio Optimization: Computationally guided agents to investigate, analyse and invest!? | http://arxiv.org/pdf/1301.4194v1.pdf | author:Ankit Dangi category:q-fin.PM cs.CE cs.NE q-fin.CP stat.ML published:2013-01-17 summary:Financial portfolio optimization is a widely studied problem in mathematics,statistics, financial and computational literature. It adheres to determiningan optimal combination of weights associated with financial assets held in aportfolio. In practice, it faces challenges by virtue of varying math.formulations, parameters, business constraints and complex financialinstruments. Empirical nature of data is no longer one-sided; therebyreflecting upside and downside trends with repeated yet unidentifiable cyclicbehaviours potentially caused due to high frequency volatile movements in assettrades. Portfolio optimization under such circumstances is theoretically andcomputationally challenging. This work presents a novel mechanism to reach anoptimal solution by encoding a variety of optimal solutions in a solution bankto guide the search process for the global investment objective formulation. Itconceptualizes the role of individual solver agents that contribute optimalsolutions to a bank of solutions, a super-agent solver that learns from thesolution bank, and, thus reflects a knowledge-based computationally guidedagents approach to investigate, analyse and reach to optimal solution forinformed investment decisions. Conceptual understanding of classes of solver agents that represent varyingproblem formulations and, mathematically oriented deterministic solvers alongwith stochastic-search driven evolutionary and swarm-intelligence basedtechniques for optimal weights are discussed. Algorithmic implementation ispresented by an enhanced neighbourhood generation mechanism in SimulatedAnnealing algorithm. A framework for inclusion of heuristic knowledge and humanexpertise from financial literature related to investment decision makingprocess is reflected via introduction of controlled perturbation strategiesusing a decision matrix for neighbourhood generation.
arxiv-2100-83 | Affinity Weighted Embedding | http://arxiv.org/pdf/1301.4171v1.pdf | author:Jason Weston, Ron Weiss, Hector Yee category:cs.IR cs.LG stat.ML published:2013-01-17 summary:Supervised (linear) embedding models like Wsabie and PSI have provensuccessful at ranking, recommendation and annotation tasks. However, despitebeing scalable to large datasets they do not take full advantage of the extradata due to their linear nature, and typically underfit. We propose a new classof models which aim to provide improved performance while retaining many of thebenefits of the existing class of embedding models. Our new approach works byiteratively learning a linear embedding model where the next iteration'sfeatures and labels are reweighted as a function of the previous iteration. Wedescribe several variants of the family, and give some initial results.
arxiv-2100-84 | On the Product Rule for Classification Problems | http://arxiv.org/pdf/1301.4157v1.pdf | author:Marcelo Cicconet category:cs.LG cs.CV stat.ML published:2013-01-17 summary:We discuss theoretical aspects of the product rule for classificationproblems in supervised machine learning for the case of combining classifiers.We show that (1) the product rule arises from the MAP classifier supposingequivalent priors and conditional independence given a class; (2) under someconditions, the product rule is equivalent to minimizing the sum of the squareddistances to the respective centers of the classes related with differentfeatures, such distances being weighted by the spread of the classes; (3)observing some hypothesis, the product rule is equivalent to concatenating thevectors of features.
arxiv-2100-85 | Non-parametric Bayesian modelling of digital gene expression data | http://arxiv.org/pdf/1301.4144v1.pdf | author:Dimitrios V. Vavoulis, Julian Gough category:q-bio.QM q-bio.GN stat.AP stat.ML published:2013-01-17 summary:Next-generation sequencing technologies provide a revolutionary tool forgenerating gene expression data. Starting with a fixed RNA sample, theyconstruct a library of millions of differentially abundant short sequence tagsor "reads", which constitute a fundamentally discrete measure of the level ofgene expression. A common limitation in experiments using these technologies isthe low number or even absence of biological replicates, which complicates thestatistical analysis of digital gene expression data. Analysis of this type ofdata has often been based on modified tests originally devised for analysingmicroarrays; both these and even de novo methods for the analysis of RNA-seqdata are plagued by the common problem of low replication. We propose a novel,non-parametric Bayesian approach for the analysis of digital gene expressiondata. We begin with a hierarchical model for modelling over-dispersed countdata and a blocked Gibbs sampling algorithm for inferring the posteriordistribution of model parameters conditional on these counts. The algorithmcompensates for the problem of low numbers of biological replicates byclustering together genes with tag counts that are likely sampled from a commondistribution and using this augmented sample for estimating the parameters ofthis distribution. The number of clusters is not decided a priori, but it isinferred along with the remaining model parameters. We demonstrate the abilityof this approach to model biological data with high fidelity by applying thealgorithm on a public dataset obtained from cancerous and non-cancerous neuraltissues.
arxiv-2100-86 | Robust PCA and subspace tracking from incomplete observations using L0-surrogates | http://arxiv.org/pdf/1210.0805v2.pdf | author:Clemens Hage, Martin Kleinsteuber category:stat.ML published:2012-10-02 summary:Many applications in data analysis rely on the decomposition of a data matrixinto a low-rank and a sparse component. Existing methods that tackle this taskuse the nuclear norm and L1-cost functions as convex relaxations of the rankconstraint and the sparsity measure, respectively, or employ thresholdingtechniques. We propose a method that allows for reconstructing and tracking asubspace of upper-bounded dimension from incomplete and corrupted observations.It does not require any a priori information about the number of outliers. Thecore of our algorithm is an intrinsic Conjugate Gradient method on the set oforthogonal projection matrices, the so-called Grassmannian. Non-convex sparsitymeasures are used for outlier detection, which leads to improved performance interms of robustly recovering and tracking the low-rank matrix. In particular,our approach can cope with more outliers and with an underlying matrix ofhigher rank than other state-of-the-art methods.
arxiv-2100-87 | Evolutionary Algorithms and Dynamic Programming | http://arxiv.org/pdf/1301.4096v1.pdf | author:Benjamin Doerr, Anton Eremeev, Frank Neumann, Madeleine Theile, Christian Thyssen category:cs.NE cs.DS published:2013-01-17 summary:Recently, it has been proven that evolutionary algorithms produce goodresults for a wide range of combinatorial optimization problems. Some of theconsidered problems are tackled by evolutionary algorithms that use arepresentation which enables them to construct solutions in a dynamicprogramming fashion. We take a general approach and relate the construction ofsuch algorithms to the development of algorithms using dynamic programmingtechniques. Thereby, we give general guidelines on how to develop evolutionaryalgorithms that have the additional ability of carrying out dynamic programmingsteps. Finally, we show that for a wide class of the so-called DP-benevolentproblems (which are known to admit FPTAS) there exists a fully polynomial-timerandomized approximation scheme based on an evolutionary algorithm.
arxiv-2100-88 | Follow the Leader If You Can, Hedge If You Must | http://arxiv.org/pdf/1301.0534v2.pdf | author:Steven de Rooij, Tim van Erven, Peter D. Grünwald, Wouter M. Koolen category:cs.LG stat.ML published:2013-01-03 summary:Follow-the-Leader (FTL) is an intuitive sequential prediction strategy thatguarantees constant regret in the stochastic setting, but has terribleperformance for worst-case data. Other hedging strategies have betterworst-case guarantees but may perform much worse than FTL if the data are notmaximally adversarial. We introduce the FlipFlop algorithm, which is the firstmethod that provably combines the best of both worlds. As part of our construction, we develop AdaHedge, which is a new way ofdynamically tuning the learning rate in Hedge without using the doubling trick.AdaHedge refines a method by Cesa-Bianchi, Mansour and Stoltz (2007), yieldingslightly improved worst-case guarantees. By interleaving AdaHedge and FTL, theFlipFlop algorithm achieves regret within a constant factor of the FTL regret,without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance;moreover, unlike earlier methods, both have the intuitive property that theissued weights are invariant under rescaling and translation of the losses. Thelosses are also allowed to be negative, in which case they may be interpretedas gains.
arxiv-2100-89 | Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration | http://arxiv.org/pdf/1301.3966v1.pdf | author:Tingting Zhao, Hirotaka Hachiya, Voot Tangkaratt, Jun Morimoto, Masashi Sugiyama category:cs.LG stat.ML published:2013-01-17 summary:The policy gradient approach is a flexible and powerful reinforcementlearning method particularly for problems with continuous actions such as robotcontrol. A common challenge in this scenario is how to reduce the variance ofpolicy gradient estimates for reliable policy updates. In this paper, wecombine the following three ideas and give a highly effective policy gradientmethod: (a) the policy gradients with parameter based exploration, which is arecently proposed policy search method with low variance of gradient estimates,(b) an importance sampling technique, which allows us to reuse previouslygathered data in a consistent way, and (c) an optimal baseline, which minimizesthe variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance ofgradient estimates and show its usefulness through extensive experiments.
arxiv-2100-90 | Multiscale Discriminant Saliency for Visual Attention | http://arxiv.org/pdf/1301.3964v1.pdf | author:Anh Cat Le Ngo, Kenneth Ang Li-Minn, Guoping Qiu, Jasmine Seng Kah-Phooi category:cs.CV published:2013-01-17 summary:The bottom-up saliency, an early stage of humans' visual attention, can beconsidered as a binary classification problem between center and surroundclasses. Discriminant power of features for the classification is measured asmutual information between features and two classes distribution. The estimateddiscrepancy of two feature classes very much depends on considered scalelevels; then, multi-scale structure and discriminant power are integrated byemploying discrete wavelet features and Hidden markov tree (HMT). With waveletcoefficients and Hidden Markov Tree parameters, quad-tree like label structuresare constructed and utilized in maximum a posterior probability (MAP) of hiddenclass variables at corresponding dyadic sub-squares. Then, saliency value foreach dyadic square at each scale level is computed with discriminant powerprinciple and the MAP. Finally, across multiple scales is integrated the finalsaliency map by an information maximization rule. Both standard quantitativetools such as NSS, LCC, AUC and qualitative assessments are used for evaluatingthe proposed multiscale discriminant saliency method (MDIS) against thewell-know information-based saliency method AIM on its Bruce Database wityeye-tracking data. Simulation results are presented and analyzed to verify thevalidity of MDIS as well as point out its disadvantages for further researchdirection.
arxiv-2100-91 | Learning Output Kernels for Multi-Task Problems | http://arxiv.org/pdf/1301.3816v1.pdf | author:Francesco Dinuzzo category:cs.LG published:2013-01-16 summary:Simultaneously solving multiple related learning tasks is beneficial under avariety of circumstances, but the prior knowledge necessary to correctly modeltask relationships is rarely available in practice. In this paper, we develop anovel kernel-based multi-task learning technique that automatically revealsstructural inter-task relationships. Building over the framework of outputkernel learning (OKL), we introduce a method that jointly learns multiplefunctions and a low-rank multi-task kernel by solving a non-convexregularization problem. Optimization is carried out via a block coordinatedescent strategy, where each subproblem is solved using suitable conjugategradient (CG) type iterative methods for linear operator equations. Theeffectiveness of the proposed approach is demonstrated on pharmacological andcollaborative filtering data.
arxiv-2100-92 | On Multilabel Classification and Ranking with Partial Feedback | http://arxiv.org/pdf/1207.0166v3.pdf | author:Claudio Gentile, Francesco Orabona category:cs.LG published:2012-06-30 summary:We present a novel multilabel/ranking algorithm working in partialinformation settings. The algorithm is based on 2nd-order descent methods, andrelies on upper-confidence bounds to trade-off exploration and exploitation. Weanalyze this algorithm in a partial adversarial setting, where covariates canbe adversarial, but multilabel probabilities are ruled by (generalized) linearmodels. We show O(T^{1/2} log T) regret bounds, which improve in several wayson the existing results. We test the effectiveness of our upper-confidencescheme by contrasting against full-information baselines on real-worldmultilabel datasets, often obtaining comparable performance.
arxiv-2100-93 | Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models | http://arxiv.org/pdf/1301.3755v1.pdf | author:Derek Rose, Itamar Arel category:cs.CV published:2013-01-16 summary:Hyper-parameter selection remains a daunting task when building a patternrecognition architecture which performs well, particularly in recentlyconstructed visual pipeline models for feature extraction. We re-formulatepooling in an existing pipeline as a function of adjustable pooling map weightparameters and propose the use of supervised error signals from gradientdescent to tune the established maps within the model. This technique allows usto learn what would otherwise be a design choice within the model andspecialize the maps to aggregate areas of invariance for the task presented.Preliminary results show moderate potential gains in classification accuracyand highlight areas of importance within the intermediate featurerepresentation space.
arxiv-2100-94 | Variational Approximations between Mean Field Theory and the Junction Tree Algorithm | http://arxiv.org/pdf/1301.3901v1.pdf | author:Wim Wiegerinck category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Recently, variational approximations such as the mean field approximationhave received much interest. We extend the standard mean field method by usingan approximating distribution that factorises into cluster potentials. Thisincludes undirected graphs, directed acyclic graphs and junction trees. Wederive generalized mean field equations to optimize the cluster potentials. Weshow that the method bridges the gap between the standard mean fieldapproximation and the exact junction tree algorithm. In addition, we addressthe problem of how to choose the graphical structure of the approximatingdistribution. From the generalised mean field equations we derive rules tosimplify the structure of the approximating distribution in advance withoutaffecting the quality of the approximation. We also show how the method fitsinto some other variational approximations that are currently popular.
arxiv-2100-95 | Model-Based Hierarchical Clustering | http://arxiv.org/pdf/1301.3899v1.pdf | author:Shivakumar Vaithyanathan, Byron E Dom category:cs.LG cs.AI stat.ML published:2013-01-16 summary:We present an approach to model-based hierarchical clustering by formulatingan objective function based on a Bayesian analysis. This model organizes thedata into a cluster hierarchy while specifying a complex feature-setpartitioning that is a key component of our model. Features can have either aunique distribution in every cluster or a common distribution over some (oreven all) of the clusters. The cluster subsets over which these features havesuch a common distribution correspond to the nodes (clusters) of the treerepresenting the hierarchy. We apply this general model to the problem ofdocument clustering for which we use a multinomial likelihood function andDirichlet priors. Our algorithm consists of a two-stage process wherein wefirst perform a flat clustering followed by a modified hierarchicalagglomerative merging process that includes determining the features that willhave common distributions over the merged clusters. The regularization inducedby using the marginal likelihood automatically determines the optimal modelstructure including number of clusters, the depth of the tree and the subset offeatures to be modeled as having a common distribution at each node. We presentexperimental results on both synthetic data and a real document collection.
arxiv-2100-96 | A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks | http://arxiv.org/pdf/1301.3897v1.pdf | author:Jin Tian category:cs.AI cs.LG stat.ML published:2013-01-16 summary:This paper extends the work in [Suzuki, 1996] and presents an efficientdepth-first branch-and-bound algorithm for learning Bayesian networkstructures, based on the minimum description length (MDL) principle, for agiven (consistent) variable ordering. The algorithm exhaustively searchesthrough all network structures and guarantees to find the network with the bestMDL score. Preliminary experiments show that the algorithm is efficient, andthat the time complexity grows slowly with the sample size. The algorithm isuseful for empirically studying both the performance of suboptimal heuristicsearch algorithms and the adequacy of the MDL principle in learning Bayesiannetworks.
arxiv-2100-97 | An Uncertainty Framework for Classification | http://arxiv.org/pdf/1301.3896v1.pdf | author:Loo-Nin Teow, Kia-Fock Loe category:cs.LG stat.ML published:2013-01-16 summary:We define a generalized likelihood function based on uncertainty measures andshow that maximizing such a likelihood function for different measures inducesdifferent types of classifiers. In the probabilistic framework, we obtainclassifiers that optimize the cross-entropy function. In the possibilisticframework, we obtain classifiers that maximize the interclass margin.Furthermore, we show that the support vector machine is a sub-class of thesemaximum-margin classifiers.
arxiv-2100-98 | Dynamic Trees: A Structured Variational Method Giving Efficient Propagation Rules | http://arxiv.org/pdf/1301.3895v1.pdf | author:Amos J. Storkey category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Dynamic trees are mixtures of tree structured belief networks. They solvesome of the problems of fixed tree networks at the cost of making exactinference intractable. For this reason approximate methods such as sampling ormean field approaches have been used. However, mean field approximations assumea factorized distribution over node states. Such a distribution seems unlickelyin the posterior, as nodes are highly correlated in the prior. Here astructured variational approach is used, where the posterior distribution overthe non-evidential nodes is itself approximated by a dynamic tree. It turns outthat this form can be used tractably and efficiently. The result is a set ofupdate rules which can propagate information through the network to obtain botha full variational approximation, and the relevant marginals. The progagtionrules are more efficient than the mean field approach and give noticeablequantitative and qualitative improvement in the inference. The marginalscalculated give better approximations to the posterior than loopy propagationon a small toy problem.
arxiv-2100-99 | Combining Feature and Prototype Pruning by Uncertainty Minimization | http://arxiv.org/pdf/1301.3891v1.pdf | author:Marc Sebban, Richard Nock category:cs.LG stat.ML published:2013-01-16 summary:We focus in this paper on dataset reduction techniques for use in k-nearestneighbor classification. In such a context, feature and prototype selectionshave always been independently treated by the standard storage reductionalgorithms. While this certifying is theoretically justified by the fact thateach subproblem is NP-hard, we assume in this paper that a joint storagereduction is in fact more intuitive and can in practice provide better resultsthan two independent processes. Moreover, it avoids a lot of distancecalculations by progressively removing useless instances during the featurepruning. While standard selection algorithms often optimize the accuracy todiscriminate the set of solutions, we use in this paper a criterion based on anuncertainty measure within a nearest-neighbor graph. This choice comes fromrecent results that have proven that accuracy is not always the suitablecriterion to optimize. In our approach, a feature or an instance is removed ifits deletion improves information of the graph. Numerous experiments arepresented in this paper and a statistical analysis shows the relevance of ourapproach, and its tolerance in the presence of noise.
arxiv-2100-100 | Monte Carlo Inference via Greedy Importance Sampling | http://arxiv.org/pdf/1301.3890v1.pdf | author:Dale Schuurmans, Finnegan Southey category:cs.LG stat.CO stat.ML published:2013-01-16 summary:We present a new method for conducting Monte Carlo inference in graphicalmodels which combines explicit search with generalized importance sampling. Theidea is to reduce the variance of importance sampling by searching forsignificant points in the target distribution. We prove that it is possible tointroduce search and still maintain unbiasedness. We then demonstrate ourprocedure on a few simple inference tasks and show that it can improve theinference quality of standard MCMC methods, including Gibbs sampling,Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous workwhich showed how greedy importance sampling could be correctly realized in theone-dimensional case.
arxiv-2100-101 | Adaptive Importance Sampling for Estimation in Structured Domains | http://arxiv.org/pdf/1301.3882v1.pdf | author:Luis E. Ortiz, Leslie Pack Kaelbling category:cs.AI cs.LG stat.ML published:2013-01-16 summary:Sampling is an important tool for estimating large, complex sums andintegrals over high dimensional spaces. For instance, important sampling hasbeen used as an alternative to exact methods for inference in belief networks.Ideally, we want to have a sampling distribution that provides optimal-varianceestimators. In this paper, we present methods that improve the samplingdistribution by systematically adapting it as we obtain information from thesamples. We present a stochastic-gradient-descent method for sequentiallyupdating the sampling distribution based on the direct minization of thevariance. We also present other stochastic-gradient-descent methods based onthe minimizationof typical notions of distance between the current samplingdistribution and approximations of the target, optimal distribution. We finallyvalidate and compare the different methods empirically by applying them to theproblem of action evaluation in influence diagrams.
arxiv-2100-102 | PEGASUS: A Policy Search Method for Large MDPs and POMDPs | http://arxiv.org/pdf/1301.3878v1.pdf | author:Andrew Y. Ng, Michael I. Jordan category:cs.AI cs.LG published:2013-01-16 summary:We propose a new approach to the problem of searching a space of policies fora Markov decision process (MDP) or a partially observable Markov decisionprocess (POMDP), given a model. Our approach is based on the followingobservation: Any (PO)MDP can be transformed into an "equivalent" POMDP in whichall state transitions (given the current state and action) are deterministic.This reduces the general problem of policy search to one in which we need onlyconsider POMDPs with deterministic transitions. We give a natural way ofestimating the value of all policies in these transformed POMDPs. Policy searchis then simply performed by searching for a policy with high estimated value.We also establish conditions under which our value estimates will be good,recovering theoretical results similar to those of Kearns, Mansour and Ng(1999), but with "sample complexity" bounds that have only a polynomial ratherthan exponential dependence on the horizon time. Our method applies toarbitrary POMDPs, including ones with infinite state and action spaces. We alsopresent empirical results for our approach on a small discrete problem, and ona complex continuous state/continuous action problem involving learning to ridea bicycle.
arxiv-2100-103 | The Anchors Hierachy: Using the triangle inequality to survive high dimensional data | http://arxiv.org/pdf/1301.3877v1.pdf | author:Andrew Moore category:cs.LG cs.DS stat.ML published:2013-01-16 summary:This paper is about metric data structures in high-dimensional ornon-Euclidean space that permit cached sufficient statistics accelerations oflearning algorithms. It has recently been shown that for less than about 10 dimensions, decoratingkd-trees with additional "cached sufficient statistics" such as first andsecond moments and contingency tables can provide satisfying acceleration for avery wide range of statistical learning tasks such as kernel regression,locally weighted regression, k-means clustering, mixture modeling and Bayes Netlearning. In this paper, we begin by defining the anchors hierarchy - a fast datastructure and algorithm for localizing data based only on atriangle-inequality-obeying distance metric. We show how this, in its ownright, gives a fast and effective clustering of data. But more importantly weshow how it can produce a well-balanced structure similar to a Ball-Tree(Omohundro, 1991) or a kind of metric tree (Uhlmann, 1991; Ciaccia, Patella, &Zezula, 1997) in a way that is neither "top-down" nor "bottom-up" but instead"middle-out". We then show how this structure, decorated with cached sufficientstatistics, allows a wide variety of statistical learning algorithms to beaccelerated even in thousands of dimensions.
arxiv-2100-104 | Tractable Bayesian Learning of Tree Belief Networks | http://arxiv.org/pdf/1301.3875v1.pdf | author:Marina Meila, Tommi S. Jaakkola category:cs.LG cs.AI stat.ML published:2013-01-16 summary:In this paper we present decomposable priors, a family of priors overstructure and parameters of tree belief nets for which Bayesian learning withcomplete observations is tractable, in the sense that the posterior is alsodecomposable and can be completely determined analytically in polynomial time.This follows from two main results: First, we show that factored distributionsover spanning trees in a graph can be integrated in closed form. Second, weexamine priors over tree parameters and show that a set of assumptions similarto (Heckerman and al. 1995) constrain the tree parameter priors to be acompactly parameterized product of Dirichlet distributions. Beside allowing forexact Bayesian learning, these results permit us to formulate a new class oftractable latent variable models in which the likelihood of a data point iscomputed through an ensemble average over tree structures.
arxiv-2100-105 | Feature Selection and Dualities in Maximum Entropy Discrimination | http://arxiv.org/pdf/1301.3865v1.pdf | author:Tony S. Jebara, Tommi S. Jaakkola category:cs.LG stat.ML published:2013-01-16 summary:Incorporating feature selection into a classification or regression methodoften carries a number of advantages. In this paper we formalize featureselection specifically from a discriminative perspective of improvingclassification/regression accuracy. The feature selection method is developedas an extension to the recently proposed maximum entropy discrimination (MED)framework. We describe MED as a flexible (Bayesian) regularization approachthat subsumes, e.g., support vector classification, regression and exponentialfamily models. For brevity, we restrict ourselves primarily to featureselection in the context of linear classification/regression methods anddemonstrate that the proposed approach indeed carries substantial improvementsin practice. Moreover, we discuss and develop various extensions of featureselection, including the problem of dealing with example specific butunobserved degrees of freedom -- alignments or invariants.
arxiv-2100-106 | Dependency Networks for Collaborative Filtering and Data Visualization | http://arxiv.org/pdf/1301.3862v1.pdf | author:David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, Carl Kadie category:cs.AI cs.IR cs.LG published:2013-01-16 summary:We describe a graphical model for probabilistic relationships---analternative to the Bayesian network---called a dependency network. The graph ofa dependency network, unlike a Bayesian network, is potentially cyclic. Theprobability component of a dependency network, like a Bayesian network, is aset of conditional distributions, one for each node given its parents. Weidentify several basic properties of this representation and describe acomputationally efficient procedure for learning the graph and probabilitycomponents from data. We describe the application of this representation toprobabilistic inference, collaborative filtering (the task of predictingpreferences), and the visualization of acausal predictive relationships.
arxiv-2100-107 | Inference for Belief Networks Using Coupling From the Past | http://arxiv.org/pdf/1301.3861v1.pdf | author:Michael Harvey, Radford M. Neal category:cs.AI cs.LG published:2013-01-16 summary:Inference for belief networks using Gibbs sampling produces a distributionfor unobserved variables that differs from the correct distribution by a(usually) unknown error, since convergence to the right distribution occursonly asymptotically. The method of "coupling from the past" samples fromexactly the correct distribution by (conceptually) running dependent Gibbssampling simulations from every possible starting state from a time far enoughin the past that all runs reach the same state at time t=0. Explicitlyconsidering every possible state is intractable for large networks, however. Wepropose a method for layered noisy-or networks that uses a compact, but oftenimprecise, summary of a set of states. This method samples from exactly thecorrect distribution, and requires only about twice the time per step asordinary Gibbs sampling, but it may require more simulation steps than would beneeded if chains were tracked exactly.
arxiv-2100-108 | Gaussian Process Networks | http://arxiv.org/pdf/1301.3857v1.pdf | author:Nir Friedman, Iftach Nachman category:cs.AI cs.LG stat.ML published:2013-01-16 summary:In this paper we address the problem of learning the structure of a Bayesiannetwork in domains with continuous variables. This task requires a procedurefor comparing different candidate structures. In the Bayesian framework, thisis done by evaluating the {em marginal likelihood/} of the data given acandidate structure. This term can be computed in closed-form for standardparametric families (e.g., Gaussians), and can be approximated, at somecomputational cost, for some semi-parametric families (e.g., mixtures ofGaussians). We present a new family of continuous variable probabilistic networks thatare based on {em Gaussian Process/} priors. These priors are semi-parametric innature and can learn almost arbitrary noisy functional relations. Using thesepriors, we can directly compute marginal likelihoods for structure learning.The resulting method can discover a wide range of functional dependencies inmultivariate data. We develop the Bayesian score of Gaussian Process Networksand describe how to learn them from data. We present empirical results onartificial data as well as on real-life domains with non-linear dependencies.
arxiv-2100-109 | Being Bayesian about Network Structure | http://arxiv.org/pdf/1301.3856v1.pdf | author:Nir Friedman, Daphne Koller category:cs.LG cs.AI stat.ML published:2013-01-16 summary:In many domains, we are interested in analyzing the structure of theunderlying distribution, e.g., whether one variable is a direct parent of theother. Bayesian model-selection attempts to find the MAP model and use itsstructure to answer these questions. However, when the amount of available datais modest, there might be many models that have non-negligible posterior. Thus,we want compute the Bayesian posterior of a feature, i.e., the total posteriorprobability of all models that contain it. In this paper, we propose a newapproach for this task. We first show how to efficiently compute a sum over theexponential number of networks that are consistent with a fixed ordering overnetwork variables. This allows us to compute, for a given ordering, both themarginal probability of the data and the posterior of a feature. We then usethis result as the basis for an algorithm that approximates the Bayesianposterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC)method, but over orderings rather than over network structures. The space oforderings is much smaller and more regular than the space of structures, andhas a smoother posterior `landscape'. We present empirical results on syntheticand real-life datasets that compare our approach to full model averaging (whenpossible), to MCMC over network structures, and to a non-Bayesian bootstrapapproach.
arxiv-2100-110 | Learning Graphical Models of Images, Videos and Their Spatial Transformations | http://arxiv.org/pdf/1301.3854v1.pdf | author:Brendan J. Frey, Nebojsa Jojic category:cs.CV cs.LG stat.ML published:2013-01-16 summary:Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markovmodels are staples of static and dynamic data modeling and image and videomodeling in particular. We show how topographic transformations in the input,such as translation and shearing in images, can be accounted for in thesemodels by including a discrete transformation variable. The resulting modelsperform clustering, dimensionality reduction and time-series analysis in a waythat is invariant to transformations in the input. Using the EM algorithm,these transformation-invariant models can be fit to static data and timeseries. We give results on filtering microscopy images, face and facial poseclustering, handwritten digit modeling and recognition, video clustering,object tracking, and removal of distractions from video sequences.
arxiv-2100-111 | Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks | http://arxiv.org/pdf/1301.3853v1.pdf | author:Arnaud Doucet, Nando de Freitas, Kevin Murphy, Stuart Russell category:cs.LG cs.AI stat.CO published:2013-01-16 summary:Particle filters (PFs) are powerful sampling-based inference/learningalgorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in aprincipled way, any type of probability distribution, nonlinearity andnon-stationarity. They have appeared in several fields under such names as"condensation", "sequential Monte Carlo" and "survival of the fittest". In thispaper, we show how we can exploit the structure of the DBN to increase theefficiency of particle filtering, using a technique known asRao-Blackwellisation. Essentially, this samples some of the variables, andmarginalizes out the rest exactly, using the Kalman filter, HMM filter,junction tree algorithm, or any other finite dimensional optimal filter. Weshow that Rao-Blackwellised particle filters (RBPFs) lead to more accurateestimates than standard PFs. We demonstrate RBPFs on two problems, namelynon-stationary online regression with radial basis function networks and robotlocalization and map building. We also discuss other potential applicationareas and provide references to some finite dimensional optimal filters.
arxiv-2100-112 | Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed Continuous And Discrete Variables | http://arxiv.org/pdf/1301.3852v1.pdf | author:Scott Davies, Andrew Moore category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Recently developed techniques have made it possible to quickly learn accurateprobability density functions from data in low-dimensional continuous space. Inparticular, mixtures of Gaussians can be fitted to data very quickly using anaccelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999).In this paper, we propose a kind of Bayesian networks in which low-dimensionalmixtures of Gaussians over different subsets of the domain's variables arecombined into a coherent joint probability model over the entire domain. Thenetwork is also capable of modeling complex dependencies between discretevariables and continuous variables without requiring discretization of thecontinuous variables. We present efficient heuristic algorithms forautomatically learning these networks from data, and perform comparativeexperiments illustrated how well these networks model real scientific data andsynthetic data. We also briefly discuss some possible improvements to thenetworks, as well as possible applications.
arxiv-2100-113 | Minimum Message Length Clustering Using Gibbs Sampling | http://arxiv.org/pdf/1301.3851v1.pdf | author:Ian Davidson category:cs.LG stat.ML published:2013-01-16 summary:The K-Mean and EM algorithms are popular in clustering and mixture modeling,due to their simplicity and ease of implementation. However, they have severalsignificant limitations. Both coverage to a local optimum of their respectiveobjective functions (ignoring the uncertainty in the model space), require theapriori specification of the number of classes/clsuters, and are inconsistent.In this work we overcome these limitations by using the Minimum Message Length(MML) principle and a variation to the K-Means/EM observation assignment andparameter calculation scheme. We maintain the simplicity of these approacheswhile constructing a Bayesian mixture modeling tool that samples/searches themodel space using a Markov Chain Monte Carlo (MCMC) sampler known as a Gibbssampler. Gibbs sampling allows us to visit each model according to itsposterior probability. Therefore, if the model space is multi-modal we willvisit all models and not get stuck in local optima. We call our approachmultiple chains at equilibrium (MCE) MML sampling.
arxiv-2100-114 | A Two-round Variant of EM for Gaussian Mixtures | http://arxiv.org/pdf/1301.3850v1.pdf | author:Sanjoy Dasgupta, Leonard Schulman category:cs.LG stat.ML published:2013-01-16 summary:Given a set of possible models (e.g., Bayesian network structures) and a datasample, in the unsupervised model selection problem the task is to choose themost accurate model with respect to the domain joint probability distribution.In contrast to this, in supervised model selection it is a priori known thatthe chosen model will be used in the future for prediction tasks involving more``focused' predictive distributions. Although focused predictive distributionscan be produced from the joint probability distribution by marginalization, inpractice the best model in the unsupervised sense does not necessarily performwell in supervised domains. In particular, the standard marginal likelihoodscore is a criterion for the unsupervised task, and, although frequently usedfor supervised model selection also, does not perform well in such tasks. Inthis paper we study the performance of the marginal likelihood scoreempirically in supervised Bayesian network selection tasks by using a largenumber of publicly available classification data sets, and compare the resultsto those obtained by alternative model selection criteria, including empiricalcrossvalidation methods, an approximation of a supervised marginal likelihoodmeasure, and a supervised version of Dawids prequential(predictive sequential)principle.The results demonstrate that the marginal likelihood score does NOTperform well FOR supervised model selection, WHILE the best results areobtained BY using Dawids prequential r napproach.
arxiv-2100-115 | Experiments with Random Projection | http://arxiv.org/pdf/1301.3849v1.pdf | author:Sanjoy Dasgupta category:cs.LG stat.ML published:2013-01-16 summary:Recent theoretical work has identified random projection as a promisingdimensionality reduction technique for learning mixtures of Gausians. Here wesummarize these results and illustrate them by a wide variety of experiments onsynthetic and real data.
arxiv-2100-116 | Bayesian Classification and Feature Selection from Finite Data Sets | http://arxiv.org/pdf/1301.3843v1.pdf | author:Frans Coetzee, Steve Lawrence, C. Lee Giles category:cs.LG stat.ML published:2013-01-16 summary:Feature selection aims to select the smallest subset of features for aspecified level of performance. The optimal achievable classificationperformance on a feature subset is summarized by its Receiver Operating Curve(ROC). When infinite data is available, the Neyman- Pearson (NP) designprocedure provides the most efficient way of obtaining this curve. In practicethe design procedure is applied to density estimates from finite data sets. Weperform a detailed statistical analysis of the resulting error propagation onfinite alphabets. We show that the estimated performance curve (EPC) producedby the design procedure is arbitrarily accurate given sufficient data,independent of the size of the feature set. However, the underlying likelihoodranking procedure is highly sensitive to errors that reduces the probabilitythat the EPC is in fact the ROC. In the worst case, guaranteeing that the EPCis equal to the ROC may require data sizes exponential in the size of thefeature set. These results imply that in theory the NP design approach may onlybe valid for characterizing relatively small feature subsets, even when theperformance of any given classifier can be estimated very accurately. Wediscuss the practical limitations for on-line methods that ensures that the NPprocedure operates in a statistically valid region.
arxiv-2100-117 | Utilities as Random Variables: Density Estimation and Structure Discovery | http://arxiv.org/pdf/1301.3840v1.pdf | author:Urszula Chajewska, Daphne Koller category:cs.AI cs.LG published:2013-01-16 summary:Decision theory does not traditionally include uncertainty over utilityfunctions. We argue that the a person's utility value for a given outcome canbe treated as we treat other domain attributes: as a random variable with adensity function over its possible values. We show that we can applystatistical density estimation techniques to learn such a density function froma database of partially elicited utility functions. In particular, we define aBayesian learning framework for this problem, assuming the distribution overutilities is a mixture of Gaussians, where the mixture components representstatistically coherent subpopulations. We can also extend our techniques to theproblem of discovering generalized additivity structure in the utilityfunctions in the population. We define a Bayesian model selection criterion forutility function structure and a search procedure over structures. Thefactorization of the utilities in the learned model, and the generalizationobtained from density estimation, allows us to provide robust estimates ofutilities using a significantly smaller number of utility elicitationquestions. We experiment with our technique on synthetic utility data and on areal database of utility functions in the domain of prenatal diagnosis.
arxiv-2100-118 | Variational Relevance Vector Machines | http://arxiv.org/pdf/1301.3838v1.pdf | author:Christopher M. Bishop, Michael Tipping category:cs.LG stat.ML published:2013-01-16 summary:The Support Vector Machine (SVM) of Vapnik (1998) has become widelyestablished as one of the leading approaches to pattern recognition and machinelearning. It expresses predictions in terms of a linear combination of kernelfunctions centred on a subset of the training data, known as support vectors. Despite its widespread success, the SVM suffers from some importantlimitations, one of the most significant being that it makes point predictionsrather than generating predictive distributions. Recently Tipping (1999) hasformulated the Relevance Vector Machine (RVM), a probabilistic model whosefunctional form is equivalent to the SVM. It achieves comparable recognitionaccuracy to the SVM, yet provides a full predictive distribution, and alsorequires substantially fewer kernel functions. The original treatment of the RVM relied on the use of type II maximumlikelihood (the `evidence framework') to provide point estimates of thehyperparameters which govern model sparsity. In this paper we show how the RVMcan be formulated and solved within a completely Bayesian paradigm through theuse of variational inference, thereby giving a posterior distribution over bothparameters and hyperparameters. We demonstrate the practicality and performanceof the variational RVM using both synthetic and real world examples.
arxiv-2100-119 | Dynamic Bayesian Multinets | http://arxiv.org/pdf/1301.3837v1.pdf | author:Jeff A. Bilmes category:cs.LG cs.AI stat.ML published:2013-01-16 summary:In this work, dynamic Bayesian multinets are introduced where a Markov chainstate at time t determines conditional independence patterns between randomvariables lying within a local time window surrounding t. It is shown howinformation-theoretic criterion functions can be used to induce sparse,discriminative, and class-conditional network structures that yield an optimalapproximation to the class posterior probability, and therefore are useful forthe classification task. Using a new structure learning heuristic, theresulting models are tested on a medium-vocabulary isolated-word speechrecognition task. It is demonstrated that these discriminatively structureddynamic Bayesian multinets, when trained in a maximum likelihood setting usingEM, can outperform both HMMs and other dynamic Bayesian networks with a similarnumber of parameters.
arxiv-2100-120 | Reversible Jump MCMC Simulated Annealing for Neural Networks | http://arxiv.org/pdf/1301.3833v1.pdf | author:Christophe Andrieu, Nando de Freitas, Arnaud Doucet category:cs.LG cs.NE stat.ML published:2013-01-16 summary:We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulatedannealing algorithm to optimize radial basis function (RBF) networks. Thisalgorithm enables us to maximize the joint posterior distribution of thenetwork parameters and the number of basis functions. It performs a globalsearch in the joint space of the parameters and number of parameters, therebysurmounting the problem of local minima. We also show that by calibrating aBayesian model, we can obtain the classical AIC, BIC and MDL model selectioncriteria within a penalized likelihood framework. Finally, we showtheoretically and empirically that the algorithm converges to the modes of thefull posterior distribution in an efficient way.
arxiv-2100-121 | Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in DeSTIN | http://arxiv.org/pdf/1301.3385v2.pdf | author:Steven R. Young, Itamar Arel category:cs.CV published:2013-01-15 summary:This paper presents a basic enhancement to the DeSTIN deep learningarchitecture by replacing the explicitly calculated transition tables that areused to capture temporal features with a simpler, more scalable mechanism. Thismechanism uses feedback of state information to cluster over a space comprisedof both the spatial input and the current state. The resulting architectureachieves state-of-the-art results on the MNIST classification benchmark.
arxiv-2100-122 | Regularized Discriminant Embedding for Visual Descriptor Learning | http://arxiv.org/pdf/1301.3644v1.pdf | author:Kye-Hyeon Kim, Rui Cai, Lei Zhang, Seungjin Choi category:cs.CV cs.LG published:2013-01-16 summary:Images can vary according to changes in viewpoint, resolution, noise, andillumination. In this paper, we aim to learn representations for an image,which are robust to wide changes in such environmental conditions, usingtraining pairs of matching and non-matching local image patches that arecollected under various environmental conditions. We present a regularizeddiscriminant analysis that emphasizes two challenging categories among thegiven training pairs: (1) matching, but far apart pairs and (2) non-matching,but close pairs in the original feature space (e.g., SIFT feature space).Compared to existing work on metric learning and discriminant analysis, ourmethod can better distinguish relevant images from irrelevant, but look-alikeimages.
arxiv-2100-123 | Change-Point Detection in Time-Series Data by Relative Density-Ratio Estimation | http://arxiv.org/pdf/1203.0453v2.pdf | author:Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama category:stat.ML cs.LG stat.ME published:2012-03-02 summary:The objective of change-point detection is to discover abrupt propertychanges lying behind time-series data. In this paper, we present a novelstatistical change-point detection algorithm based on non-parametric divergenceestimation between time-series samples from two retrospective segments. Ourmethod uses the relative Pearson divergence as a divergence measure, and it isaccurately and efficiently estimated by a method of direct density-ratioestimation. Through experiments on artificial and real-world datasets includinghuman-activity sensing, speech, and Twitter messages, we demonstrate theusefulness of the proposed method.
arxiv-2100-124 | Tree structured sparse coding on cubes | http://arxiv.org/pdf/1301.3590v1.pdf | author:Arthur Szlam category:cs.IT cs.CV math.IT published:2013-01-16 summary:A brief description of tree structured sparse coding on the binary cube.
arxiv-2100-125 | Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative Clustering | http://arxiv.org/pdf/1301.3575v1.pdf | author:Boyi Xie, Shuheng Zheng category:cs.LG cs.CV stat.ML published:2013-01-16 summary:Large scale agglomerative clustering is hindered by computational burdens. Wepropose a novel scheme where exact inter-instance distance calculation isreplaced by the Hamming distance between Kernelized Locality-Sensitive Hashing(KLSH) hashed values. This results in a method that drastically decreasescomputation time. Additionally, we take advantage of certain labeled datapoints via distance metric learning to achieve a competitive precision andrecall comparing to K-Means but in much less computation time.
arxiv-2100-126 | A Nested HDP for Hierarchical Topic Models | http://arxiv.org/pdf/1301.3570v1.pdf | author:John Paisley, Chong Wang, David Blei, Michael I. Jordan category:stat.ML published:2013-01-16 summary:We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We demonstrate ouralgorithm on 1.8 million documents from The New York Times.
arxiv-2100-127 | Gender Recognition in Walk Gait through 3D Motion by Quadratic Bezier Curve and Statistical Techniques | http://arxiv.org/pdf/1211.1482v4.pdf | author:Sajid Ali category:cs.CV published:2012-11-07 summary:Motion capture is the process of recording the movement of objects or people.It is used in military, entertainment, sports, and medical applications, andfor validation of computer vision[2] and robotics. In filmmaking and video gamedevelopment, it refers to recording actions of human actors, and using thatinformation to animate digital character models in 2D or 3D computer animation.When it includes face and fingers or captures subtle
arxiv-2100-128 | Complexity of Representation and Inference in Compositional Models with Part Sharing | http://arxiv.org/pdf/1301.3560v1.pdf | author:Alan L. Yuille, Roozbeh Mottaghi category:cs.CV published:2013-01-16 summary:This paper describes serial and parallel compositional models of multipleobjects with part sharing. Objects are built by part-subpart compositions andexpressed in terms of a hierarchical dictionary of object parts. These partsare represented on lattices of decreasing sizes which yield an executivesummary description. We describe inference and learning algorithms for thesemodels. We analyze the complexity of this model in terms of computation time(for serial computers) and numbers of nodes (e.g., "neurons") for parallelcomputers. In particular, we compute the complexity gains by part sharing andits dependence on how the dictionary scales with the level of the hierarchy. Weexplore three regimes of scaling behavior where the dictionary size (i)increases exponentially with the level, (ii) is determined by an unsupervisedcompositional learning algorithm applied to real data, (iii) decreasesexponentially with scale. This analysis shows that in some regimes the use ofshared parts enables algorithms which can perform inference in time linear inthe number of levels for an exponential number of objects. In other regimespart sharing has little advantage for serial computers but can give linearprocessing on parallel computers.
arxiv-2100-129 | Model Selection for Gaussian Mixture Models | http://arxiv.org/pdf/1301.3558v1.pdf | author:Tao Huang, Heng Peng, Kun Zhang category:stat.ME math.ST stat.ML stat.TH published:2013-01-16 summary:This paper is concerned with an important issue in finite mixture modelling,the selection of the number of mixing components. We propose a new penalizedlikelihood method for model selection of finite multivariate Gaussian mixturemodels. The proposed method is shown to be statistically consistent indetermining of the number of components. A modified EM algorithm is developedto simultaneously select the number of components and to estimate the mixingweights, i.e. the mixing probabilities, and unknown parameters of Gaussiandistributions. Simulations and a real data analysis are presented to illustratethe performance of the proposed method.
arxiv-2100-130 | Stochastic Pooling for Regularization of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1301.3557v1.pdf | author:Matthew D. Zeiler, Rob Fergus category:cs.LG cs.NE stat.ML published:2013-01-16 summary:We introduce a simple and effective method for regularizing largeconvolutional neural networks. We replace the conventional deterministicpooling operations with a stochastic procedure, randomly picking the activationwithin each pooling region according to a multinomial distribution, given bythe activities within the pooling region. The approach is hyper-parameter freeand can be combined with other regularization approaches, such as dropout anddata augmentation. We achieve state-of-the-art performance on four imagedatasets, relative to other approaches that do not utilize data augmentation.
arxiv-2100-131 | A Rhetorical Analysis Approach to Natural Language Processing | http://arxiv.org/pdf/1301.3547v1.pdf | author:Benjamin Englard category:cs.CL stat.ML published:2013-01-16 summary:The goal of this research was to find a way to extend the capabilities ofcomputers through the processing of language in a more human way, and presentapplications which demonstrate the power of this method. This research presentsa novel approach, Rhetorical Analysis, to solving problems in Natural LanguageProcessing (NLP). The main benefit of Rhetorical Analysis, as opposed toprevious approaches, is that it does not require the accumulation of large setsof training data, but can be used to solve a multitude of problems within thefield of NLP. The NLP problems investigated with Rhetorical Analysis were theAuthor Identification problem - predicting the author of a piece of text basedon its rhetorical strategies, Election Prediction - predicting the winner of apresidential candidate's re-election campaign based on rhetorical strategieswithin that president's inaugural address, Natural Language Generation - havinga computer produce text containing rhetorical strategies, and DocumentSummarization. The results of this research indicate that an AuthorIdentification system based on Rhetorical Analysis could predict the correctauthor 100% of the time, that a re-election predictor based on RhetoricalAnalysis could predict the correct winner of a re-election campaign 55% of thetime, that a Natural Language Generation system based on Rhetorical Analysiscould output text with up to 87.3% similarity to Shakespeare in style, and thata Document Summarization system based on Rhetorical Analysis could extracthighly relevant sentences. Overall, this study demonstrated that RhetoricalAnalysis could be a useful approach to solving problems in NLP.
arxiv-2100-132 | Learning Features with Structure-Adapting Multi-view Exponential Family Harmoniums | http://arxiv.org/pdf/1301.3539v1.pdf | author:Yoonseop Kang, Seungjin Choi category:cs.LG published:2013-01-16 summary:We proposea graphical model for multi-view feature extraction thatautomatically adapts its structure to achieve better representation of datadistribution. The proposed model, structure-adapting multi-view harmonium(SA-MVH) has switch parameters that control the connection between hidden nodesand input views, and learn the switch parameter while training. Numericalexperiments on synthetic and a real-world dataset demonstrate the usefulbehavior of the SA-MVH, compared to existing multi-view feature extractionmethods.
arxiv-2100-133 | An Efficient Sufficient Dimension Reduction Method for Identifying Genetic Variants of Clinical Significance | http://arxiv.org/pdf/1301.3528v1.pdf | author:Momiao Xiong, Long Ma category:q-bio.GN cs.LG stat.ML published:2013-01-15 summary:Fast and cheaper next generation sequencing technologies will generateunprecedentedly massive and highly-dimensional genomic and epigenomic variationdata. In the near future, a routine part of medical record will include thesequenced genomes. A fundamental question is how to efficiently extract genomicand epigenomic variants of clinical utility which will provide information foroptimal wellness and interference strategies. Traditional paradigm foridentifying variants of clinical validity is to test association of thevariants. However, significantly associated genetic variants may or may not beusefulness for diagnosis and prognosis of diseases. Alternative to associationstudies for finding genetic variants of predictive utility is to systematicallysearch variants that contain sufficient information for phenotype prediction.To achieve this, we introduce concepts of sufficient dimension reduction andcoordinate hypothesis which project the original high dimensional data to verylow dimensional space while preserving all information on response phenotypes.We then formulate clinically significant genetic variant discovery problem intosparse SDR problem and develop algorithms that can select significant geneticvariants from up to or even ten millions of predictors with the aid of dividingSDR for whole genome into a number of subSDR problems defined for genomicregions. The sparse SDR is in turn formulated as sparse optimal scoringproblem, but with penalty which can remove row vectors from the basis matrix.To speed up computation, we develop the modified alternating direction methodfor multipliers to solve the sparse optimal scoring problem which can easily beimplemented in parallel. To illustrate its application, the proposed method isapplied to simulation data and the NHLBI's Exome Sequencing Project dataset
arxiv-2100-134 | How good is the Electricity benchmark for evaluating concept drift adaptation | http://arxiv.org/pdf/1301.3524v1.pdf | author:Indre Zliobaite category:cs.LG published:2013-01-15 summary:In this correspondence, we will point out a problem with testing adaptiveclassifiers on autocorrelated data. In such a case random change alarms mayboost the accuracy figures. Hence, we cannot be sure if the adaptation isworking well.
arxiv-2100-135 | Anomaly Classification with the Anti-Profile Support Vector Machine | http://arxiv.org/pdf/1301.3514v1.pdf | author:Wikum Dinalankara, Hector Corrada Bravo category:stat.ML q-bio.GN published:2013-01-15 summary:We introduce the anti-profile Support Vector Machine (apSVM) as a novelalgorithm to address the anomaly classification problem, an extension ofanomaly detection where the goal is to distinguish data samples from a numberof anomalous and heterogeneous classes based on their pattern of deviation froma normal stable class. We show that under heterogeneity assumptions definedhere that the apSVM can be solved as the dual of a standard SVM with anindirect kernel that measures similarity of anomalous samples throughsimilarity to the stable normal class. We characterize this indirect kernel asthe inner product in a Reproducing Kernel Hilbert Space between representersthat are projected to the subspace spanned by the representers of the normalsamples. We show by simulation and application to cancer genomics datasets thatthe anti-profile SVM produces classifiers that are more accurate and stablethan the standard SVM in the anomaly classification setting.
arxiv-2100-136 | Pooling-Invariant Image Feature Learning | http://arxiv.org/pdf/1302.5056v1.pdf | author:Yangqing Jia, Oriol Vinyals, Trevor Darrell category:cs.CV cs.LG published:2013-01-15 summary:Unsupervised dictionary learning has been a key component in state-of-the-artcomputer vision recognition architectures. While highly effective methods existfor patch-based dictionary learning, these methods may learn redundant featuresafter the pooling stage in a given early vision architecture. In this paper, weoffer a novel dictionary learning scheme to efficiently take into account theinvariance of learned features after the spatial pooling stage. The algorithmis built on simple clustering, and thus enjoys efficiency and scalability. Wediscuss the underlying mechanism that justifies the use of clusteringalgorithms, and empirically show that the algorithm finds better dictionariesthan patch-based methods with the same dictionary size.
arxiv-2100-137 | Multi-agent learning using Fictitious Play and Extended Kalman Filter | http://arxiv.org/pdf/1301.3347v1.pdf | author:Michalis Smyrnakis category:cs.MA cs.LG math.OC stat.ML published:2013-01-15 summary:Decentralised optimisation tasks are important components of multi-agentsystems. These tasks can be interpreted as n-player potential games: thereforegame-theoretic learning algorithms can be used to solve decentralisedoptimisation tasks. Fictitious play is the canonical example of thesealgorithms. Nevertheless fictitious play implicitly assumes that players havestationary strategies. We present a novel variant of fictitious play whereplayers predict their opponents' strategies using Extended Kalman filters anduse their predictions to update their strategies. We show that in 2 by 2 games with at least one pure Nash equilibrium and inpotential games where players have two available actions, the proposedalgorithm converges to the pure Nash equilibrium. The performance of theproposed algorithm was empirically tested, in two strategic form games and anad-hoc sensor network surveillance problem. The proposed algorithm performsbetter than the classic fictitious play algorithm in these games and thereforeimproves the performance of game-theoretical learning in decentralisedoptimisation.
arxiv-2100-138 | The Manifold of Human Emotions | http://arxiv.org/pdf/1301.3214v1.pdf | author:Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa category:cs.CL published:2013-01-15 summary:Sentiment analysis predicts the presence of positive or negative emotions ina text document. In this paper, we consider higher dimensional extensions ofthe sentiment concept, which represent a richer set of human emotions. Ourapproach goes beyond previous work in that our model contains a continuousmanifold rather than a finite set of human emotions. We investigate theresulting model, compare it to psychological observations, and explore itspredictive capabilities.
arxiv-2100-139 | Learning Graphical Model Parameters with Approximate Marginal Inference | http://arxiv.org/pdf/1301.3193v1.pdf | author:Justin Domke category:cs.LG cs.CV I.2.6; I.4.8 published:2013-01-15 summary:Likelihood based-learning of graphical models faces challenges ofcomputational-complexity and robustness to model mis-specification. This paperstudies methods that fit parameters directly to maximize a measure of theaccuracy of predicted marginals, taking into account both model and inferenceapproximations at training time. Experiments on imaging problems suggestmarginalization-based learning performs better than likelihood-basedapproximations on difficult problems where the model being fit is approximatein nature.
arxiv-2100-140 | Matrix Approximation under Local Low-Rank Assumption | http://arxiv.org/pdf/1301.3192v1.pdf | author:Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer category:cs.LG stat.ML I.2.6 published:2013-01-15 summary:Matrix approximation is a common tool in machine learning for buildingaccurate prediction models for recommendation systems, text mining, andcomputer vision. A prevalent assumption in constructing matrix approximationsis that the partially observed matrix is of low-rank. We propose a new matrixapproximation model where we assume instead that the matrix is only locally oflow-rank, leading to a representation of the observed matrix as a weighted sumof low-rank matrices. We analyze the accuracy of the proposed local low-rankmodeling. Our experiments show improvements in prediction accuracy inrecommendation tasks.
arxiv-2100-141 | On Automation and Medical Image Interpretation, With Applications for Laryngeal Imaging | http://arxiv.org/pdf/1212.6933v3.pdf | author:H. J. Moukalled category:cs.CV published:2012-12-31 summary:Indeed, these are exciting times. We are in the heart of a digitalrenaissance. Automation and computer technology allow engineers and scientiststo fabricate processes that amalgamate quality of life. We anticipate muchgrowth in medical image interpretation and understanding, due to the influx ofcomputer technologies. This work should serve as a guide to introduce thereader to core themes in theoretical computer science, as well as imagingapplications for understanding vocal-fold vibrations. In this work, we motivatethe use of automation, review some mathematical models of computation. Wepresent a proof of a classical problem in image analysis that cannot beautomated by means of algorithms. Furthermore, discuss some applications forprocessing medical images of the vocal folds, and discuss some of theexhilarating directions the art of automation will take vocal-fold imageinterpretation and quite possibly other areas of biomedical image analysis.
arxiv-2100-142 | Fano schemes of generic intersections and machine learning | http://arxiv.org/pdf/1301.3078v1.pdf | author:Franz Király, Paul Larsen category:math.AG stat.ML published:2013-01-14 summary:We investigate Fano schemes of conditionally generic intersections, i.e. ofhypersurfaces in projective space chosen generically up to additionalconditions. Via a correspondence between generic properties of algebraicvarieties and events in probability spaces that occur with probability one, weuse the obtained results on Fano schemes to solve a problem in machinelearning.
arxiv-2100-143 | Excess risk bounds for multitask learning with trace norm regularization | http://arxiv.org/pdf/1212.1496v2.pdf | author:Andreas Maurer, Massimiliano Pontil category:stat.ML cs.LG published:2012-12-06 summary:Trace norm regularization is a popular method of multitask learning. We giveexcess risk bounds with explicit dependence on the number of tasks, the numberof examples per task and properties of the data distribution. The bounds areindependent of the dimension of the input space, which may be infinite as inthe case of reproducing kernel Hilbert spaces. A byproduct of the proof arebounds on the expected norm of sums of random positive semidefinite matriceswith subexponential moments.
arxiv-2100-144 | Block-Coordinate Frank-Wolfe Optimization for Structural SVMs | http://arxiv.org/pdf/1207.4747v4.pdf | author:Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, Patrick Pletscher category:cs.LG math.OC stat.ML G.1.6; I.2.6 published:2012-07-19 summary:We propose a randomized block-coordinate variant of the classic Frank-Wolfealgorithm for convex optimization with block-separable constraints. Despite itslower iteration cost, we show that it achieves a similar convergence rate induality gap as the full Frank-Wolfe algorithm. We also show that, when appliedto the dual structural support vector machine (SVM) objective, this yields anonline algorithm that has the same low iteration complexity as primalstochastic subgradient methods. However, unlike stochastic subgradient methods,the block-coordinate Frank-Wolfe algorithm allows us to compute the optimalstep-size and yields a computable duality gap guarantee. Our experimentsindicate that this simple algorithm outperforms competing structural SVMsolvers.
arxiv-2100-145 | Wavelet-based Scale Saliency | http://arxiv.org/pdf/1301.2884v1.pdf | author:Anh Cat Le Ngo, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng, Guoping Qiu category:cs.CV published:2013-01-14 summary:Both pixel-based scale saliency (PSS) and basis project methods focus onmultiscale analysis of data content and structure. Their theoretical relationsand practical combination are previously discussed. However, no models haveever been proposed for calculating scale saliency on basis-projecteddescriptors since then. This paper extend those ideas into mathematical modelsand implement them in the wavelet-based scale saliency (WSS). While PSS usespixel-value descriptors, WSS treats wavelet sub-bands as basis descriptors. Thepaper discusses different wavelet descriptors: discrete wavelet transform(DWT), wavelet packet transform (DWPT), quaternion wavelet transform (QWT) andbest basis quaternion wavelet packet transform (QWPTBB). WSS saliency maps ofdifferent descriptors are generated and compared against other saliency methodsby both quantitative and quanlitative methods. Quantitative results, ROCcurves, AUC values and NSS values are collected from simulations on Bruce andKootstra image databases with human eye-tracking data as ground-truth.Furthermore, qualitative visual results of saliency maps are analyzed andcompared against each other as well as eye-tracking data inclusive in thedatabases.
arxiv-2100-146 | SpeedRead: A Fast Named Entity Recognition Pipeline | http://arxiv.org/pdf/1301.2857v1.pdf | author:Rami Al-Rfou', Steven Skiena category:cs.CL published:2013-01-14 summary:Online content analysis employs algorithmic methods to identify entities inunstructured text. Both machine learning and knowledge-base approaches lie atthe foundation of contemporary named entities extraction systems. However, theprogress in deploying these approaches on web-scale has been been hampered bythe computational cost of NLP over massive text corpora. We present SpeedRead(SR), a named entity recognition pipeline that runs at least 10 times fasterthan Stanford NLP pipeline. This pipeline consists of a high performance PennTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)tagger and knowledge-based named entity recognizer.
arxiv-2100-147 | A comparison of SVM and RVM for Document Classification | http://arxiv.org/pdf/1301.2785v1.pdf | author:Muhammad Rafi, Mohammad Shahid Shaikh category:cs.IR cs.LG published:2013-01-13 summary:Document classification is a task of assigning a new unclassified document toone of the predefined set of classes. The content based document classificationuses the content of the document with some weighting criteria to assign it toone of the predefined classes. It is a major task in library science,electronic document management systems and information sciences. This paperinvestigates document classification by using two different classificationtechniques (1) Support Vector Machine (SVM) and (2) Relevance Vector Machine(RVM). SVM is a supervised machine learning technique that can be used forclassification task. In its basic form, SVM represents the instances of thedata into space and tries to separate the distinct classes by a maximumpossible wide gap (hyper plane) that separates the classes. On the other handRVM uses probabilistic measure to define this separation space. RVM usesBayesian inference to obtain succinct solution, thus RVM uses significantlyfewer basis functions. Experimental studies on three standard textclassification datasets reveal that although RVM takes more training time, itsclassification is much better as compared to SVM.
arxiv-2100-148 | Robust High Dimensional Sparse Regression and Matching Pursuit | http://arxiv.org/pdf/1301.2725v1.pdf | author:Yudong Chen, Constantine Caramanis, Shie Mannor category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2013-01-12 summary:We consider high dimensional sparse regression, and develop strategies ableto deal with arbitrary -- possibly, severe or coordinated -- errors in thecovariance matrix $X$. These may come from corrupted data, persistentexperimental errors, or malicious respondents in surveys/recommender systems,etc. Such non-stochastic error-in-variables problems are notoriously difficultto treat, and as we demonstrate, the problem is particularly pronounced inhigh-dimensional settings where the primary goal is {\em support recovery} ofthe sparse regressor. We develop algorithms for support recovery in sparseregression, when some number $n_1$ out of $n+n_1$ total covariate/responsepairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interestedin understanding how many outliers, $n_1$, we can tolerate, while identifyingthe correct support. To the best of our knowledge, neither standard outlierrejection techniques, nor recently developed robust regression algorithms (thatfocus only on corrupted response variables), nor recent algorithms for dealingwith stochastic noise or erasures, can provide guarantees on support recovery.Perhaps surprisingly, we also show that the natural brute force algorithm thatsearches over all subsets of $n$ covariate/response pairs, and all subsets ofpossible support coordinates in order to minimize regression error, isremarkably poor, unable to correctly identify the support with even $n_1 =O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in thebasic setting we consider, where all authentic measurements and noise areindependent and sub-Gaussian. In this setting, we provide a simple algorithm --no more computationally taxing than OMP -- that gives stronger performanceguarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$corrupted points, where $p$ is the dimension of the signal to be recovered.
arxiv-2100-149 | Support Vector Regression for Right Censored Data | http://arxiv.org/pdf/1202.5130v2.pdf | author:Yair Goldberg, Michael R. Kosorok category:stat.ML math.ST stat.TH published:2012-02-23 summary:We develop a unified approach for classification and regression supportvector machines for data subject to right censoring. We provide finite samplebounds on the generalization error of the algorithm, prove risk consistency fora wide class of probability measures, and study the associated learning rates.We apply the general methodology to estimation of the (truncated) mean, median,quantiles, and for classification problems. We present a simulation study thatdemonstrates the performance of the proposed approach.
arxiv-2100-150 | Binocular disparity as an explanation for the moon illusion | http://arxiv.org/pdf/1301.2715v1.pdf | author:Joseph Antonides, Toshiro Kubota category:cs.CV physics.pop-ph published:2013-01-12 summary:We present another explanation for the moon illusion, in which the moon lookslarger near the horizon than near the zenith. In our model, the sky isconsidered a spatially contiguous and geometrically smooth surface. When anobject (like the moon) breaks the contiguity of the surface, humans perceive anocclusion of the surface rather than an object appearing through a hole.Binocular vision dictates that the moon is distant, but this perception modeldictates that the moon is closer than the sky. To solve the dilemma, the braindistorts the projections of the moon to increase the binocular disparity, whichresults in increase of the angular size of the moon. The degree of thedistortion depends upon the apparent distance to the sky, which is influencedby the surrounding objects and the condition of the sky. The closer the skyappears, the stronger the illusion. At the zenith, few distance cues arepresent, causing difficulty with distance estimation and weakening theillusion.
arxiv-2100-151 | Learning from Distributions via Support Measure Machines | http://arxiv.org/pdf/1202.6504v2.pdf | author:Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf category:stat.ML cs.LG published:2012-02-29 summary:This paper presents a kernel-based discriminative learning framework onprobability measures. Rather than relying on large collections of vectorialtraining examples, our framework learns using a collection of probabilitydistributions that have been constructed to meaningfully represent trainingdata. By representing these probability distributions as mean embeddings in thereproducing kernel Hilbert space (RKHS), we are able to apply many standardkernel-based learning techniques in straightforward fashion. To accomplishthis, we construct a generalization of the support vector machine (SVM) calleda support measure machine (SMM). Our analyses of SMMs provides several insightsinto their relationship to traditional SVMs. Based on such insights, we proposea flexible SVM (Flex-SVM) that places different kernel functions on eachtraining example. Experimental results on both synthetic and real-world datademonstrate the effectiveness of our proposed framework.
arxiv-2100-152 | A Triclustering Approach for Time Evolving Graphs | http://arxiv.org/pdf/1301.2659v1.pdf | author:Romain Guigourès, Marc Boullé, Fabrice Rossi category:cs.LG cs.SI stat.ML published:2013-01-12 summary:This paper introduces a novel technique to track structures in time evolvinggraphs. The method is based on a parameter free approach for three-dimensionalco-clustering of the source vertices, the target vertices and the time. Allthese features are simultaneously segmented in order to build time segments andclusters of vertices whose edge distributions are similar and evolve in thesame way over the time segments. The main novelty of this approach lies in thatthe time segments are directly inferred from the evolution of the edgedistribution between the vertices, thus not requiring the user to make an apriori discretization. Experiments conducted on a synthetic dataset illustratethe good behaviour of the technique, and a study of a real-life dataset showsthe potential of the proposed approach for exploratory data analysis.
arxiv-2100-153 | Multiple functional regression with both discrete and continuous covariates | http://arxiv.org/pdf/1301.2656v1.pdf | author:Hachem Kadri, Philippe Preux, Emmanuel Duflos, Stéphane Canu category:stat.ML cs.LG published:2013-01-12 summary:In this paper we present a nonparametric method for extending functionalregression methodology to the situation where more than one functionalcovariate is used to predict a functional response. Borrowing the idea fromKadri et al. (2010a), the method, which support mixed discrete and continuousexplanatory variables, is based on estimating a function-valued function inreproducing kernel Hilbert spaces by virtue of positive operator-valuedkernels.
arxiv-2100-154 | Functional Regularized Least Squares Classi cation with Operator-valued Kernels | http://arxiv.org/pdf/1301.2655v1.pdf | author:Hachem Kadri, Asma Rabaoui, Philippe Preux, Emmanuel Duflos, Alain Rakotomamonjy category:cs.LG stat.ML published:2013-01-12 summary:Although operator-valued kernels have recently received increasing interestin various machine learning and functional data analysis problems such asmulti-task learning or functional regression, little attention has been paid tothe understanding of their associated feature spaces. In this paper, we explorethe potential of adopting an operator-valued kernel feature space perspectivefor the analysis of functional data. We then extend the Regularized LeastSquares Classification (RLSC) algorithm to cover situations where there aremultiple functions per observation. Experiments on a sound recognition problemshow that the proposed method outperforms the classical RLSC algorithm.
arxiv-2100-155 | Computational Intelligence for Deepwater Reservoir Depositional Environments Interpretation | http://arxiv.org/pdf/1301.2638v1.pdf | author:Tina Yu, Dave Wilkinson, Julian Clark, Morgan Sullivan category:cs.NE physics.geo-ph published:2013-01-12 summary:Predicting oil recovery efficiency of a deepwater reservoir is a challengingtask. One approach to characterize a deepwater reservoir and to predict itsproducibility is by analyzing its depositional information. This researchproposes a deposition-based stratigraphic interpretation framework fordeepwater reservoir characterization. In this framework, one critical task isthe identification and labeling of the stratigraphic components in thereservoir, according to their depositional environments. This interpretationprocess is labor intensive and can produce different results depending on thestratigrapher who performs the analysis. To relieve stratigrapher's workloadand to produce more consistent results, we have developed a novel methodologyto automate this process using various computational intelligence techniques.Using a well log data set, we demonstrate that the developed methodology andthe designed workflow can produce finite state transducer models that interpretdeepwater reservoir depositional environments adequately.
arxiv-2100-156 | Information field theory | http://arxiv.org/pdf/1301.2556v1.pdf | author:Torsten Enßlin category:astro-ph.IM cs.IT math.IT stat.ML published:2013-01-11 summary:Non-linear image reconstruction and signal analysis deal with complex inverseproblems. To tackle such problems in a systematic way, I present informationfield theory (IFT) as a means of Bayesian, data based inference on spatiallydistributed signal fields. IFT is a statistical field theory, which permits theconstruction of optimal signal recovery algorithms even for non-linear andnon-Gaussian signal inference problems. IFT algorithms exploit spatialcorrelations of the signal fields and benefit from techniques developed toinvestigate quantum and statistical field theories, such as Feynman diagrams,re-normalisation calculations, and thermodynamic potentials. The theory can beused in many areas, and applications in cosmology and numerics are presented.
arxiv-2100-157 | Determining token sequence mistakes in responses to questions with open text answer | http://arxiv.org/pdf/1301.2466v1.pdf | author:Oleg Sychev, Dmitry Mamontov category:cs.CL cs.CY K.3.2 published:2013-01-11 summary:When learning grammar of the new language, a teacher should routinely checkstudent's exercises for grammatical correctness. The paper describes a methodof automatically detecting and reporting grammar mistakes, regarding an orderof tokens in the response. It could report extra tokens, missing tokens andmisplaced tokens. The method is useful when teaching language, where order oftokens is important, which includes most formal languages and some natural ones(like English). The method was implemented in a question type plug-inCorrectWriting for the widely used learning manage system Moodle.
arxiv-2100-158 | Backward-in-Time Selection of the Order of Dynamic Regression Prediction Model | http://arxiv.org/pdf/1301.2410v1.pdf | author:Ioannis Vlachos, Dimitris Kugiumtzis category:stat.AP stat.ME stat.ML published:2013-01-11 summary:We investigate the optimal structure of dynamic regression models used inmultivariate time series prediction and propose a scheme to form the laggedvariable structure called Backward-in-Time Selection (BTS) that takes intoaccount feedback and multi-collinearity, often present in multivariate timeseries. We compare BTS to other known methods, also in conjunction withregularization techniques used for the estimation of model parameters, namelyprincipal components, partial least squares and ridge regression estimation.The predictive efficiency of the different models is assessed by means of MonteCarlo simulations for different settings of feedback and multi-collinearity.The results show that BTS has consistently good prediction performance whileother popular methods have varying and often inferior performance. Theprediction performance of BTS was also found the best when tested on humanelectroencephalograms of an epileptic seizure, and to the prediction of returnsof indices of world financial markets.
arxiv-2100-159 | Dating medieval English charters | http://arxiv.org/pdf/1301.2405v1.pdf | author:Gelila Tilahun, Andrey Feuerverger, Michael Gervers category:stat.AP cs.CL published:2013-01-11 summary:Deeds, or charters, dealing with property rights, provide a continuousdocumentation which can be used by historians to study the evolution of social,economic and political changes. This study is concerned with charters (writtenin Latin) dating from the tenth through early fourteenth centuries in England.Of these, at least one million were left undated, largely due to administrativechanges introduced by William the Conqueror in 1066. Correctly dating suchcharters is of vital importance in the study of English medieval history. Thispaper is concerned with computer-automated statistical methods for dating suchdocument collections, with the goal of reducing the considerable effortsrequired to date them manually and of improving the accuracy of assigned dates.Proposed methods are based on such data as the variation over time of word andphrase usage, and on measures of distance between documents. The extensive (anddated) Documents of Early England Data Set (DEEDS) maintained at the Universityof Toronto was used for this purpose.
arxiv-2100-160 | Robust Image Analysis by L1-Norm Semi-supervised Learning | http://arxiv.org/pdf/1110.3109v2.pdf | author:Zhiwu Lu, Yuxin Peng category:cs.CV cs.LG published:2011-10-14 summary:This paper presents a novel L1-norm semi-supervised learning algorithm forrobust image analysis by giving new L1-norm formulation of Laplacianregularization which is the key step of graph-based semi-supervised learning.Since our L1-norm Laplacian regularization is defined directly over theeigenvectors of the normalized Laplacian matrix, we successfully formulatesemi-supervised learning as an L1-norm linear reconstruction problem which canbe effectively solved with sparse coding. By working with only a small subsetof eigenvectors, we further develop a fast sparse coding algorithm for ourL1-norm semi-supervised learning. Due to the sparsity induced by sparse coding,the proposed algorithm can deal with the noise in the data to some extent andthus has important applications to robust image analysis, such as noise-robustimage classification and noise reduction for visual and textual bag-of-words(BOW) models. In particular, this paper is the first attempt to obtain robustimage representation by sparse co-refinement of visual and textual BOW models.The experimental results have shown the promising performance of the proposedalgorithm.
arxiv-2100-161 | Application of Hopfield Network to Saccades | http://arxiv.org/pdf/1301.2351v1.pdf | author:Teruyoshi Washizawa category:cs.CV q-bio.NC published:2013-01-10 summary:Human eye movement mechanisms (saccades) are very useful for scene analysis,including object representation and pattern recognition. In this letter, aHopfield neural network to emulate saccades is proposed. The network uses anenergy function that includes location and identification tasks. Computersimulation shows that the network performs those tasks cooperatively. Theresult suggests that the network is applicable to shift-invariant patternrecognition.
arxiv-2100-162 | Planning by Prioritized Sweeping with Small Backups | http://arxiv.org/pdf/1301.2343v1.pdf | author:Harm van Seijen, Richard S. Sutton category:cs.AI cs.LG published:2013-01-10 summary:Efficient planning plays a crucial role in model-based reinforcementlearning. Traditionally, the main planning operation is a full backup based onthe current estimates of the successor states. Consequently, its computationtime is proportional to the number of successor states. In this paper, weintroduce a new planning backup that uses only the current value of a singlesuccessor state and has a computation time independent of the number ofsuccessor states. This new backup, which we call a small backup, opens the doorto a new class of model-based reinforcement learning methods that exhibit muchfiner control over their planning process than traditional methods. Weempirically demonstrate that this increased flexibility allows for moreefficient planning by showing that an implementation of prioritized sweepingbased on small backups achieves a substantial performance improvement overclassical implementations.
arxiv-2100-163 | Maximizing a Nonnegative, Monotone, Submodular Function Constrained to Matchings | http://arxiv.org/pdf/1212.6846v2.pdf | author:Sagar Kale category:cs.DS cs.AI cs.CC cs.LG stat.ML published:2012-12-31 summary:Submodular functions have many applications. Matchings have manyapplications. The bitext word alignment problem can be modeled as the problemof maximizing a nonnegative, monotone, submodular function constrained tomatchings in a complete bipartite graph where each vertex corresponds to a wordin the two input sentences and each edge represents a potential word-to-wordtranslation. We propose a more general problem of maximizing a nonnegative,monotone, submodular function defined on the edge set of a complete graphconstrained to matchings; we call this problem the CSM-Matching problem.CSM-Matching also generalizes the maximum-weight matching problem, which has apolynomial-time algorithm; however, we show that it is NP-hard to approximateCSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem toit. Our main result is a simple, greedy, 3-approximation algorithm forCSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,monotone, submodular function over two matroids, i.e., CSM-2-Matroids.CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We showthat we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.We extend this approach to similar problems.
arxiv-2100-164 | Determinantal point processes for machine learning | http://arxiv.org/pdf/1207.6083v4.pdf | author:Alex Kulesza, Ben Taskar category:stat.ML cs.IR cs.LG published:2012-07-25 summary:Determinantal point processes (DPPs) are elegant probabilistic models ofrepulsion that arise in quantum physics and random matrix theory. In contrastto traditional structured models like Markov random fields, which becomeintractable and hard to approximate in the presence of negative correlations,DPPs offer efficient and exact algorithms for sampling, marginalization,conditioning, and other inference tasks. We provide a gentle introduction toDPPs, focusing on the intuitions, algorithms, and extensions that are mostrelevant to the machine learning community, and show how DPPs can be applied toreal-world applications like finding diverse sets of high-quality searchresults, building informative summaries by selecting diverse sentences fromdocuments, modeling non-overlapping human poses in images or video, andautomatically building timelines of important news stories.
arxiv-2100-165 | Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Sampling in Kendall's Shape Space | http://arxiv.org/pdf/1212.5720v2.pdf | author:Yen-Yun Yu, P. Thomas Fletcher, Suyash P. Awate category:cs.CV published:2012-12-22 summary:This paper proposes a novel framework for multi-group shape analysis relyingon a hierarchical graphical statistical model on shapes within a population.Theframework represents individual shapes as point setsmodulo translation,rotation, and scale, following the notion in Kendall shape space.Whileindividual shapes are derived from their group shape model, each group shapemodel is derived from a single population shape model. The hierarchical modelfollows the natural organization of population data and the top level in thehierarchy provides a common frame of reference for multigroup shape analysis,e.g. classification and hypothesis testing. Unlike typical shape-modelingapproaches, the proposed model is a generative model that defines a jointdistribution of object-boundary data and the shape-model variables.Furthermore, it naturally enforces optimal correspondences during the processof model fitting and thereby subsumes the so-called correspondence problem. Theproposed inference scheme employs an expectation maximization (EM) algorithmthat treats the individual and group shape variables as hidden random variablesand integrates them out before estimating the parameters (population mean andvariance and the group variances). The underpinning of the EM algorithm is thesampling of pointsets, in Kendall shape space, from their posteriordistribution, for which we exploit a highly-efficient scheme based onHamiltonian Monte Carlo simulation. Experiments in this paper use the fittedhierarchical model to perform (1) hypothesis testing for comparison betweenpairs of groups using permutation testing and (2) classification for imageretrieval. The paper validates the proposed framework on simulated data anddemonstrates results on real data.
arxiv-2100-166 | Clusters and water flows: a novel approach to modal clustering through Morse theory | http://arxiv.org/pdf/1212.1384v2.pdf | author:José E. Chacón category:math.ST math.DG stat.ML stat.TH published:2012-12-06 summary:The problem of finding groups in data (cluster analysis) has been extensivelystudied by researchers from the fields of Statistics and Computer Science,among others. However, despite its popularity it is widely recognized that theinvestigation of some theoretical aspects of clustering has been relativelysparse. One of the main reasons for this lack of theoretical results is surelythe fact that, unlike the situation with other statistical problems asregression or classification, for some of the cluster methodologies it is quitedifficult to specify a population goal to which the data-based clusteringalgorithms should try to get close. This paper aims to provide some insightinto the theoretical foundations of the usual nonparametric approach toclustering, which understands clusters as regions of high density, bypresenting an explicit formulation for the ideal population clustering.
arxiv-2100-167 | Network-based clustering with mixtures of L1-penalized Gaussian graphical models: an empirical investigation | http://arxiv.org/pdf/1301.2194v1.pdf | author:Steven M. Hill, Sach Mukherjee category:stat.ML cs.LG stat.ME published:2013-01-10 summary:In many applications, multivariate samples may harbor previously unrecognizedheterogeneity at the level of conditional independence or network structure.For example, in cancer biology, disease subtypes may differ with respect tosubtype-specific interplay between molecular components. Then, both subtypediscovery and estimation of subtype-specific networks present important andrelated challenges. To enable such analyses, we put forward a mixture modelwhose components are sparse Gaussian graphical models. This brings togethermodel-based clustering and graphical modeling to permit simultaneous estimationof cluster assignments and cluster-specific networks. We carry out estimationwithin an L1-penalized framework, and investigate several specific penalizationregimes. We present empirical results on simulated data and provide generalrecommendations for the formulation and use of mixtures of L1-penalizedGaussian graphical models.
arxiv-2100-168 | Using Temporal Data for Making Recommendations | http://arxiv.org/pdf/1301.2320v1.pdf | author:Andrew Zimdars, David Maxwell Chickering, Christopher Meek category:cs.IR cs.AI cs.LG published:2013-01-10 summary:We treat collaborative filtering as a univariate time series estimationproblem: given a user's previous votes, predict the next vote. We describe twofamilies of methods for transforming data to encode time order in ways amenableto off-the-shelf classification and density estimation tools, and examine theresults of using these approaches on several real-world data sets. Theimprovements in predictive accuracy we realize recommend the use of otherpredictive algorithms that exploit the temporal order of data.
arxiv-2100-169 | Statistical Modeling in Continuous Speech Recognition (CSR)(Invited Talk) | http://arxiv.org/pdf/1301.2318v1.pdf | author:Steve Young category:cs.LG cs.AI stat.ML published:2013-01-10 summary:Automatic continuous speech recognition (CSR) is sufficiently mature that avariety of real world applications are now possible including large vocabularytranscription and interactive spoken dialogues. This paper reviews theevolution of the statistical modelling techniques which underlie current-daysystems, specifically hidden Markov models (HMMs) and N-grams. Starting from adescription of the speech signal and its parameterisation, the variousmodelling assumptions and their consequences are discussed. It then describesvarious techniques by which the effects of these assumptions can be mitigated.Despite the progress that has been made, the limitations of current modellingtechniques are still evident. The paper therefore concludes with a brief reviewof some of the more fundamental modelling work now in progress.
arxiv-2100-170 | Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation | http://arxiv.org/pdf/1301.2317v1.pdf | author:Max Welling, Yee Whye Teh category:cs.AI cs.LG published:2013-01-10 summary:We present a novel inference algorithm for arbitrary, binary, undirectedgraphs. Unlike loopy belief propagation, which iterates fixed point equations,we directly descend on the Bethe free energy. The algorithm consists of twophases, first we update the pairwise probabilities, given the marginalprobabilities at each unit,using an analytic expression. Next, we update themarginal probabilities, given the pairwise probabilities by following thenegative gradient of the Bethe free energy. Both steps are guaranteed todecrease the Bethe free energy, and since it is lower bounded, the algorithm isguaranteed to converge to a local minimum. We also show that the Bethe freeenergy is equal to the TAP free energy up to second order in the weights. Inexperiments we confirm that when belief propagation converges it usually findsidentical solutions as our belief optimization method. However, in cases wherebelief propagation fails to converge, belief optimization continues to convergeto reasonable beliefs. The stable nature of belief optimization makes itideally suited for learning graphical models from data.
arxiv-2100-171 | Cross-covariance modelling via DAGs with hidden variables | http://arxiv.org/pdf/1301.2316v1.pdf | author:Jacob A. Wegelin, Thomas S. Richardson category:cs.LG stat.ML published:2013-01-10 summary:DAG models with hidden variables present many difficulties that are notpresent when all nodes are observed. In particular, fully observed DAG modelsare identified and correspond to well-defined sets ofdistributions, whereasthis is not true if nodes are unobserved. Inthis paper we characterize exactlythe set of distributions given by a class of one-dimensional Gaussian latentvariable models. These models relate two blocks of observed variables, modelingonly the cross-covariance matrix. We describe the relation of this model to thesingular value decomposition of the cross-covariance matrix. We show that,although the model is underidentified, useful information may be extracted. Wefurther consider an alternative parametrization in which one latent variable isassociated with each block. Our analysis leads to some novel covarianceequivalence results for Gaussian hidden variable models.
arxiv-2100-172 | The Optimal Reward Baseline for Gradient-Based Reinforcement Learning | http://arxiv.org/pdf/1301.2315v1.pdf | author:Lex Weaver, Nigel Tao category:cs.LG cs.AI stat.ML published:2013-01-10 summary:There exist a number of reinforcement learning algorithms which learnbyclimbing the gradient of expected reward. Their long-runconvergence has beenproved, even in partially observableenvironments with non-deterministicactions, and without the need fora system model. However, the variance of thegradient estimator hasbeen found to be a significant practical problem. Recentapproacheshave discounted future rewards, introducing a bias-variancetrade-offinto the gradient estimate. We incorporate a reward baseline intothelearning system, and show that it affects variance withoutintroducingfurther bias. In particular, as we approach thezero-bias,high-variance parameterization, the optimal (or varianceminimizing)constant reward baseline is equal to the long-term averageexpectedreward. Modified policy-gradient algorithms are presented, and anumberof experiments demonstrate their improvement over previous work.
arxiv-2100-173 | Maximum Likelihood Bounded Tree-Width Markov Networks | http://arxiv.org/pdf/1301.2311v1.pdf | author:Nathan Srebro category:cs.LG cs.AI stat.ML published:2013-01-10 summary:Chow and Liu (1968) studied the problem of learning a maximumlikelihoodMarkov tree. We generalize their work to more complexMarkov networks byconsidering the problem of learning a maximumlikelihood Markov network ofbounded complexity. We discuss howtree-width is in many ways the appropriatemeasure of complexity andthus analyze the problem of learning a maximumlikelihood Markovnetwork of bounded tree-width.Similar to the work of Chow andLiu, we are able to formalize thelearning problem as a combinatorialoptimization problem on graphs. Weshow that learning a maximum likelihoodMarkov network of boundedtree-width is equivalent to finding a maximum weighthypertree. Thisequivalence gives rise to global, integer-programmingbased,approximation algorithms with provable performance guarantees, forthelearning problem. This contrasts with heuristic local-searchalgorithms whichwere previously suggested (e.g. by Malvestuto 1991).The equivalence also allowsus to study the computational hardness ofthe learning problem. We show thatlearning a maximum likelihoodMarkov network of bounded tree-width is NP-hard,and discuss thehardness of approximation.
arxiv-2100-174 | Policy Improvement for POMDPs Using Normalized Importance Sampling | http://arxiv.org/pdf/1301.2310v1.pdf | author:Christian R. Shelton category:cs.AI cs.LG published:2013-01-10 summary:We present a new method for estimating the expected return of a POMDP fromexperience. The method does not assume any knowledge of the POMDP and allowsthe experience to be gathered from an arbitrary sequence of policies. Thereturn is estimated for any new policy of the POMDP. We motivate the estimatorfrom function-approximation and importance sampling points-of-view and deriveits theoretical properties. Although the estimator is biased, it has lowvariance and the bias is often irrelevant when the estimator is used forpair-wise comparisons. We conclude by extending the estimator to policies withmemory and compare its performance in a greedy search algorithm to REINFORCEalgorithms showing an order of magnitude reduction in the number of trialsrequired.
arxiv-2100-175 | Symmetric Collaborative Filtering Using the Noisy Sensor Model | http://arxiv.org/pdf/1301.2309v1.pdf | author:Rita Sharma, David L Poole category:cs.IR cs.LG published:2013-01-10 summary:Collaborative filtering is the process of making recommendations regardingthe potential preference of a user, for example shopping on the Internet, basedon the preference ratings of the user and a number of other users for variousitems. This paper considers collaborative filtering based onexplicitmulti-valued ratings. To evaluate the algorithms, weconsider only {empure} collaborative filtering, using ratings exclusively, and no otherinformation about the people or items.Our approach is to predict a user'spreferences regarding a particularitem by using other people who rated thatitem and other items ratedby the user as noisy sensors. The noisy sensor modeluses Bayes' theorem to compute the probability distribution for theuser'srating of a new item. We give two variant models: in one, we learn a{emclassical normal linear regression} model of how users rate items; inanother,we assume different users rate items the same, but the accuracy ofthesensors needs to be learned. We compare these variant modelswithstate-of-the-art techniques and show how they are significantlybetter,whether a user has rated only two items or many. We reportempiricalresults using the EachMovie databasefootnote{http://research.compaq.com/SRC/eachmovie/} of movie ratings. Wealsoshow that by considering items similarity along with theusers similarity, theaccuracy of the prediction increases.
arxiv-2100-176 | Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments | http://arxiv.org/pdf/1301.2303v1.pdf | author:Alexandrin Popescul, Lyle H. Ungar, David M Pennock, Steve Lawrence category:cs.IR cs.LG stat.ML published:2013-01-10 summary:Recommender systems leverage product and community information to targetproducts to consumers. Researchers have developed collaborative recommenders,content-based recommenders, and (largely ad-hoc) hybrid systems. We propose aunified probabilistic framework for merging collaborative and content-basedrecommendations. We extend Hofmann's [1999] aspect model to incorporatethree-way co-occurrence data among users, items, and item content. The relativeinfluence of collaboration data versus content data is not imposed as anexogenous parameter, but rather emerges naturally from the given data sources.Global probabilistic models coupled with standard Expectation Maximization (EM)learning algorithms tend to drastically overfit in sparse-data situations, asis typical in recommendation applications. We show that secondary contentinformation can often be used to overcome sparsity. Experiments on data fromthe ResearchIndex library of Computer Science publications show thatappropriate mixture models incorporating secondary data produce significantlybetter quality recommenders than k-nearest neighbors (k-NN). Globalprobabilistic models also allow more general inferences than local methods likek-NN.
arxiv-2100-177 | Lattice Particle Filters | http://arxiv.org/pdf/1301.2298v1.pdf | author:Dirk Ormoneit, Christiane Lemieux, David J. Fleet category:cs.AI cs.CV published:2013-01-10 summary:A standard approach to approximate inference in state-space models isto applya particle filter, e.g., the Condensation Algorithm.However, the performance ofparticle filters often varies significantlydue to their stochastic nature.Wepresent a class of algorithms, called lattice particle filters, thatcircumventthis difficulty by placing the particles deterministicallyaccording to aQuasi-Monte Carlo integration rule.We describe a practical realization of thisidea, discuss itstheoretical properties, and its efficiency.Experimentalresults with a synthetic 2D tracking problem show that thelattice particlefilter is equivalent to a conventional particle filterthat has between 10 and60% more particles, depending ontheir "sparsity" in the state-space.We alsopresent results on inferring 3D human motion frommoving light displays.
arxiv-2100-178 | Expectation Propagation for approximate Bayesian inference | http://arxiv.org/pdf/1301.2294v1.pdf | author:Thomas P. Minka category:cs.AI cs.LG published:2013-01-10 summary:This paper presents a new deterministic approximation technique in Bayesiannetworks. This method, "Expectation Propagation", unifies two previoustechniques: assumed-density filtering, an extension of the Kalman filter, andloopy belief propagation, an extension of belief propagation in Bayesiannetworks. All three algorithms try to recover an approximate distribution whichis close in KL divergence to the true distribution. Loopy belief propagation,because it propagates exact belief states, is useful for a limited class ofbelief networks, such as those which are purely discrete. ExpectationPropagation approximates the belief states by only retaining certainexpectations, such as mean and variance, and iterates until these expectationsare consistent throughout the network. This makes it applicable to hybridnetworks with discrete and continuous nodes. Expectation Propagation alsoextends belief propagation in the opposite direction - it can propagate richerbelief states that incorporate correlations between nodes. Experiments withGaussian mixture models show Expectation Propagation to be convincingly betterthan methods with similar computational cost: Laplace's method, variationalBayes, and Monte Carlo. Expectation Propagation also provides an efficientalgorithm for training Bayes point machine classifiers.
arxiv-2100-179 | A Bayesian Multiresolution Independence Test for Continuous Variables | http://arxiv.org/pdf/1301.2292v1.pdf | author:Dimitris Margaritis, Sebastian Thrun category:cs.AI cs.LG published:2013-01-10 summary:In this paper we present a method ofcomputing the posterior probabilityofconditional independence of two or morecontinuous variables fromdata,examined at several resolutions. Ourapproach is motivated bytheobservation that the appearance ofcontinuous data varies widely atvariousresolutions, producing verydifferent independence estimatesbetween thevariablesinvolved. Therefore, it is difficultto ascertain independencewithoutexamining data at several carefullyselected resolutions. In our paper,weaccomplish this using the exactcomputation of the posteriorprobability ofindependence, calculatedanalytically given a resolution. Ateach examinedresolution, we assume amultinomial distribution with Dirichletpriors for thediscretized tableparameters, and compute the posteriorusing Bayesianintegration. Acrossresolutions, we use a search procedureto approximate theBayesian integral ofprobability over an exponential numberof possiblehistograms. Our methodgeneralizes to an arbitrary numbervariables in astraightforward manner.The test is suitable for Bayesiannetwork learningalgorithms that useindependence tests to infer the networkstructure, in domainsthat contain anymix of continuous, ordinal andcategorical variables.
arxiv-2100-180 | Iterative Markov Chain Monte Carlo Computation of Reference Priors and Minimax Risk | http://arxiv.org/pdf/1301.2286v1.pdf | author:John Lafferty, Larry A. Wasserman category:cs.LG stat.ML published:2013-01-10 summary:We present an iterative Markov chainMonte Carlo algorithm forcomputingreference priors and minimax risk forgeneral parametric families.Ourapproach uses MCMC techniques based onthe Blahut-Arimoto algorithmforcomputing channel capacity ininformation theory. We give astatisticalanalysis of the algorithm,bounding the number of samples requiredfor thestochastic algorithm to closelyapproximate the deterministic algorithmin eachiteration. Simulations arepresented for several examples fromexponentialfamilies. Although we focuson applications to reference priors andminimax risk,the methods and analysiswe develop are applicable to a muchbroader class ofoptimization problemsand iterative algorithms.
arxiv-2100-181 | Classifier Learning with Supervised Marginal Likelihood | http://arxiv.org/pdf/1301.2284v1.pdf | author:Petri Kontkanen, Petri Myllymaki, Henry Tirri category:cs.LG stat.ML published:2013-01-10 summary:It has been argued that in supervised classification tasks, in practice itmay be more sensible to perform model selection with respect to some morefocused model selection score, like the supervised (conditional) marginallikelihood, than with respect to the standard marginal likelihood criterion.However, for most Bayesian network models, computing the supervised marginallikelihood score takes exponential time with respect to the amount of observeddata. In this paper, we consider diagnostic Bayesian network classifiers wherethe significant model parameters represent conditional distributions for theclass variable, given the values of the predictor variables, in which case thesupervised marginal likelihood can be computed in linear time with respect tothe data. As the number of model parameters grows in this case exponentiallywith respect to the number of predictors, we focus on simple diagnostic modelswhere the number of relevant predictors is small, and suggest two approachesfor applying this type of models in classification. The first approach is basedon mixtures of simple diagnostic models, while in the second approach we applythe small predictor sets of the simple diagnostic models for augmenting theNaive Bayes classifier.
arxiv-2100-182 | Improved learning of Bayesian networks | http://arxiv.org/pdf/1301.2283v1.pdf | author:Tomas Kocka, Robert Castelo category:cs.LG cs.AI stat.ML published:2013-01-10 summary:The search space of Bayesian Network structures is usually defined as AcyclicDirected Graphs (DAGs) and the search is done by local transformations of DAGs.But the space of Bayesian Networks is ordered by DAG Markov model inclusion andit is natural to consider that a good search policy should take this intoaccount. First attempt to do this (Chickering 1996) was using equivalenceclasses of DAGs instead of DAGs itself. This approach produces better resultsbut it is significantly slower. We present a compromise between these twoapproaches. It uses DAGs to search the space in such a way that the ordering byinclusion is taken into account. This is achieved by repetitive usage of localmoves within the equivalence class of DAGs. We show that this new approachproduces better results than the original DAGs approach without substantialchange in time complexity. We present empirical results, within the frameworkof heuristic search and Markov Chain Monte Carlo, provided through the Alarmdataset.
arxiv-2100-183 | Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures | http://arxiv.org/pdf/1301.2280v1.pdf | author:Geoff A. Jarrad category:cs.LG cs.AI stat.ML published:2013-01-10 summary:A novel method for estimating Bayesian network (BN) parameters from data ispresented which provides improved performance on test data. Previous researchhas shown the value of representing conditional probability distributions(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)anddecision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of localdistributions,each having a different set of parents.This increases the spaceof possible structures which can be considered,enabling the CPDs to havefiner-grained dependencies.The resulting estimation procedure induces amodelthat is better able to emulate the underlying interactions occurring inthe data than conventional conditional Bernoulli network models.The results forartificially generated data indicate that overfitting is best reduced byrestricting the complexity of candidate mixture substructures local to eachnode. Furthermore, mixtures of very simple substructures can perform almost aswell as more complex ones.The BMN is also applied to data collected from anonline adventure game with an application to keyhole plan recognition. Theresults show that the BMN-based model brings a dramatic improvement inperformance over a conventional BN model.
arxiv-2100-184 | Discovering Multiple Constraints that are Frequently Approximately Satisfied | http://arxiv.org/pdf/1301.2278v1.pdf | author:Geoffrey E. Hinton, Yee Whye Teh category:cs.LG stat.ML published:2013-01-10 summary:Some high-dimensional data.sets can be modelled by assuming that there aremany different linear constraints, each of which is Frequently ApproximatelySatisfied (FAS) by the data. The probability of a data vector under the modelis then proportional to the product of the probabilities of its constraintviolations. We describe three methods of learning products of constraints usinga heavy-tailed probability distribution for the violations.
arxiv-2100-185 | Multivariate Information Bottleneck | http://arxiv.org/pdf/1301.2270v1.pdf | author:Nir Friedman, Ori Mosenzon, Noam Slonim, Naftali Tishby category:cs.LG cs.AI stat.ML published:2013-01-10 summary:The Information bottleneck method is an unsupervised non-parametric dataorganization technique. Given a joint distribution P(A,B), this methodconstructs a new variable T that extracts partitions, or clusters, over thevalues of A that are informative about B. The information bottleneck hasalready been applied to document classification, gene expression, neural code,and spectral analysis. In this paper, we introduce a general principledframework for multivariate extensions of the information bottleneck method.This allows us to consider multiple systems of data partitions that areinter-related. Our approach utilizes Bayesian networks for specifying thesystems of clusters and what information each captures. We show that thisconstruction provides insight about bottleneck variations and enables us tocharacterize solutions of these variations. We also present a general frameworkfor iterative algorithms for constructing solutions, and apply it to severalexamples.
arxiv-2100-186 | Learning the Dimensionality of Hidden Variables | http://arxiv.org/pdf/1301.2269v1.pdf | author:Gal Elidan, Nir Friedman category:cs.LG cs.AI stat.ML published:2013-01-10 summary:A serious problem in learning probabilistic models is the presence of hiddenvariables. These variables are not observed, yet interact with several of theobserved variables. Detecting hidden variables poses two problems: determiningthe relations to other variables in the model and determining the number ofstates of the hidden variable. In this paper, we address the latter problem inthe context of Bayesian networks. We describe an approach that utilizes ascore-based agglomerative state-clustering. As we show, this approach allows usto efficiently evaluate models with a range of cardinalities for the hiddenvariable. We show how to extend this procedure to deal with multipleinteracting hidden variables. We demonstrate the effectiveness of this approachby evaluating it on synthetic and real-life data. We show that our approachlearns models with hidden variables that generalize better and have betterstructure than previous approaches.
arxiv-2100-187 | Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables | http://arxiv.org/pdf/1301.2268v1.pdf | author:Tal El-Hay, Nir Friedman category:cs.AI cs.LG published:2013-01-10 summary:Global variational approximation methods in graphical models allow efficientapproximate inference of complex posterior distributions by using a simplermodel. The choice of the approximating model determines a tradeoff between thecomplexity of the approximation procedure and the quality of the approximation.In this paper, we consider variational approximations based on two classes ofmodels that are richer than standard Bayesian networks, Markov networks ormixture models. As such, these classes allow to find better tradeoffs in thespectrum of approximations. The first class of models are chain graphs, whichcapture distributions that are partially directed. The second class of modelsare directed graphs (Bayesian networks) with additional latent variables. Bothclasses allow representation of multi-variable dependencies that cannot beeasily represented within a Bayesian network.
arxiv-2100-188 | Variational MCMC | http://arxiv.org/pdf/1301.2266v1.pdf | author:Nando de Freitas, Pedro Hojen-Sorensen, Michael I. Jordan, Stuart Russell category:cs.LG stat.CO stat.ML published:2013-01-10 summary:We propose a new class of learning algorithms that combines variationalapproximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithmsthat use the variational approximation as proposal distribution can performpoorly because this approximation tends to underestimate the true variance andother features of the data. We solve this problem by introducing moresophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMCkernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH)kernel with a variational approximation as proposaldistribution. The MH kernelallows one to locate regions of high probability efficiently. The Metropoliskernel allows us to explore the vicinity of these regions. This algorithmoutperforms variationalapproximations because it yields slightly betterestimates of the mean and considerably better estimates of higher moments, suchas covariances. It also outperforms standard MCMC algorithms because it locatestheregions of high probability quickly, thus speeding up convergence. Wedemonstrate this algorithm on the problem of Bayesian parameter estimation forlogistic (sigmoid) belief networks.
arxiv-2100-189 | Conditions Under Which Conditional Independence and Scoring Methods Lead to Identical Selection of Bayesian Network Models | http://arxiv.org/pdf/1301.2262v1.pdf | author:Robert G. Cowell category:cs.AI cs.LG stat.ML published:2013-01-10 summary:It is often stated in papers tackling the task of inferring Bayesian networkstructures from data that there are these two distinct approaches: (i) Applyconditional independence tests when testing for the presence or otherwise ofedges; (ii) Search the model space using a scoring metric. Here I argue thatfor complete data and a given node ordering this division is a myth, by showingthat cross entropy methods for checking conditional independence aremathematically identical to methods based upon discriminating between models bytheir overall goodness-of-fit logarithmic scores.
arxiv-2100-190 | A Factorized Variational Technique for Phase Unwrapping in Markov Random Fields | http://arxiv.org/pdf/1301.2252v1.pdf | author:Kannan Achan, Brendan J. Frey, Ralf Koetter category:cs.CV published:2013-01-10 summary:Some types of medical and topographic imaging device produce images in whichthe pixel values are "phase-wrapped", i.e. measured modulus a known scalar.Phase unwrapping can be viewed as the problem of inferring the number of shiftsbetween each and every pair of neighboring pixels, subject to an a prioripreference for smooth surfaces, and subject to a zero curl constraint, whichrequires that the shifts must sum to 0 around every loop. We formulate phaseunwrapping as a mean field inference problem in a Markov network, where theprior favors the zero curl constraint. We compare our mean field technique withthe least squares method on a synthetic 100x100 image, and give results on a512x512 synthetic aperture radar image from Sandia National Laboratories.<LongText>
arxiv-2100-191 | Artificial Intelligence Framework for Simulating Clinical Decision-Making: A Markov Decision Process Approach | http://arxiv.org/pdf/1301.2158v1.pdf | author:Casey C. Bennett, Kris Hauser category:cs.AI stat.ML published:2013-01-10 summary:In the modern healthcare system, rapidly expanding costs/complexity, thegrowing myriad of treatment options, and exploding information streams thatoften do not effectively reach the front lines hinder the ability to chooseoptimal treatment decisions over time. The goal in this paper is to develop ageneral purpose (non-disease-specific) computational/artificial intelligence(AI) framework to address these challenges. This serves two potentialfunctions: 1) a simulation environment for exploring various healthcarepolicies, payment methodologies, etc., and 2) the basis for clinical artificialintelligence - an AI that can think like a doctor. This approach combinesMarkov decision processes and dynamic decision networks to learn from clinicaldata and develop complex plans via simulation of alternative sequentialdecision paths while capturing the sometimes conflicting, sometimes synergisticinteractions of various components in the healthcare system. It can operate inpartially observable environments (in the case of missing observations or data)by maintaining belief states about patient health status and functions as anonline agent that plans and re-plans. This framework was evaluated using realpatient data from an electronic health record. Such an AI framework easilyoutperforms the current treatment-as-usual (TAU) case-rate/fee-for-servicemodels of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a30-35% increase in patient outcomes. Tweaking certain model parameters furtherenhances this advantage, obtaining roughly 50% more improvement for roughlyhalf the costs. Given careful design and problem formulation, an AI simulationframework can approximate optimal decisions even in complex and uncertainenvironments. Future work is described that outlines potential lines ofresearch and integration of machine learning algorithms for personalizedmedicine.
arxiv-2100-192 | Enhancing the retrieval performance by combing the texture and edge features | http://arxiv.org/pdf/1301.2542v1.pdf | author:Mohamed Eisa, Amira Eletrebi, Ebrahim Elhenawy category:cs.CV cs.IR published:2013-01-10 summary:In this paper, anew algorithm which is based on geometrical moments and localbinary patterns (LBP) for content based image retrieval (CBIR) is proposed. Ingeometrical moments, each vector is compared with the all other vectors foredge map generation. The same concept is utilized at LBP calculation which isgenerating nine LBP patterns from a given 3x3 pattern. Finally, nine LBPhistograms are calculated which are used as a feature vector for imageretrieval. Moments are important features used in recognition of differenttypes of images. Two experiments have been carried out for proving the worth ofour algorithm. The results after being investigated shows a significantimprovement in terms of their evaluation measures as compared to LBP and otherexisting transform domain techniques.
arxiv-2100-193 | A remark on covering | http://arxiv.org/pdf/1301.3043v1.pdf | author:Vladimir Temlyakov category:math.MG math.FA math.NA stat.ML published:2013-01-10 summary:We discuss construction of coverings of the unit ball of a finite dimensionalBanach space. The well known technique of comparing volumes gives upper andlower bounds on covering numbers. This technique does not provide aconstruction of good coverings. Here we apply incoherent dictionaries forconstruction of good coverings. We use the following strategy. First, we builda good covering by balls with a radius close to one. Second, we iterate thisconstruction to obtain a good covering for any radius. We mostly concentrate onthe first step of this strategy.
arxiv-2100-194 | Domain Generalization via Invariant Feature Representation | http://arxiv.org/pdf/1301.2115v1.pdf | author:Krikamol Muandet, David Balduzzi, Bernhard Schölkopf category:stat.ML cs.LG published:2013-01-10 summary:This paper investigates domain generalization: How to take knowledge acquiredfrom an arbitrary number of related domains and apply it to previously unseendomains? We propose Domain-Invariant Component Analysis (DICA), a kernel-basedoptimization algorithm that learns an invariant transformation by minimizingthe dissimilarity across domains, whilst preserving the functional relationshipbetween input and output variables. A learning-theoretic analysis shows thatreducing dissimilarity improves the expected generalization ability ofclassifiers on new domains, motivating the proposed algorithm. Experimentalresults on synthetic and real-world datasets demonstrate that DICA successfullylearns invariant features and improves classifier performance in practice.
arxiv-2100-195 | Comparision and analysis of photo image forgery detection techniques | http://arxiv.org/pdf/1302.3119v1.pdf | author:S. Murali, Govindraj B. Chittapur, Prabhakara H. S, Basavaraj S. Anami category:cs.CV cs.CR cs.MM published:2013-01-10 summary:Digital Photo images are everywhere, on the covers of magazines, innewspapers, in courtrooms, and all over the Internet. We are exposed to themthroughout the day and most of the time. Ease with which images can bemanipulated; we need to be aware that seeing does not always imply believing.We propose methodologies to identify such unbelievable photo images andsucceeded to identify forged region by given only the forged image. Formats areadditive tag for every file system and contents are relatively expressed withextension based on most popular digital camera uses JPEG and Other imageformats like png, bmp etc. We have designed algorithm running behind with theconcept of abnormal anomalies and identify the forgery regions.
arxiv-2100-196 | A fair comparison of many max-tree computation algorithms (Extended version of the paper submitted to ISMM 2013 | http://arxiv.org/pdf/1212.1819v2.pdf | author:Edwin Carlinet, Thierry Géraud category:cs.CV published:2012-12-08 summary:With the development of connected filters for the last decade, manyalgorithms have been proposed to compute the max-tree. Max-tree allows tocompute the most advanced connected operators in a simple way. However, no faircomparison of algorithms has been proposed yet and the choice of an algorithmover an other depends on many parameters. Since the need of fast algorithms isobvious for production code, we present an in depth comparison of fivealgorithms and some variations of them in a unique framework. Finally, adecision tree will be proposed to help user in choosing the right algorithmwith respect to their data.
arxiv-2100-197 | Training Effective Node Classifiers for Cascade Classification | http://arxiv.org/pdf/1301.2032v1.pdf | author:Chunhua Shen, Peng Wang, Sakrapee Paisitkriangkrai, Anton van den Hengel category:cs.CV cs.LG stat.ML published:2013-01-10 summary:Cascade classifiers are widely used in real-time object detection. Differentfrom conventional classifiers that are designed for a low overallclassification error rate, a classifier in each node of the cascade is requiredto achieve an extremely high detection rate and moderate false positive rate.Although there are a few reported methods addressing this requirement in thecontext of object detection, there is no principled feature selection methodthat explicitly takes into account this asymmetric node learning objective. Weprovide such an algorithm here. We show that a special case of the biasedminimax probability machine has the same formulation as the linear asymmetricclassifier (LAC) of Wu et al (2005). We then design a new boosting algorithmthat directly optimizes the cost function of LAC. The resultingtotally-corrective boosting algorithm is implemented by the column generationtechnique in convex optimization. Experimental results on object detectionverify the effectiveness of the proposed boosting algorithm as a nodeclassifier in cascade object detection, and show performance better than thatof the current state-of-the-art.
arxiv-2100-198 | Heteroscedastic Relevance Vector Machine | http://arxiv.org/pdf/1301.2015v1.pdf | author:Daniel Khashabi, Mojtaba Ziyadi, Feng Liang category:stat.ML cs.LG published:2013-01-10 summary:In this work we propose a heteroscedastic generalization to RVM, a fastBayesian framework for regression, based on some recent similar works. We usevariational approximation and expectation propagation to tackle the problem.The work is still under progress and we are examining the results and comparingwith the previous works.
arxiv-2100-199 | Error Correction in Learning using SVMs | http://arxiv.org/pdf/1301.2012v1.pdf | author:Srivatsan Laxman, Sushil Mittal, Ramarathnam Venkatesan category:cs.LG published:2013-01-10 summary:This paper is concerned with learning binary classifiers under adversariallabel-noise. We introduce the problem of error-correction in learning where thegoal is to recover the original clean data from a label-manipulated version ofit, given (i) no constraints on the adversary other than an upper-bound on thenumber of errors, and (ii) some regularity properties for the original data. Wepresent a simple and practical error-correction algorithm called SubSVMs thatlearns individual SVMs on several small-size (log-size), class-balanced, randomsubsets of the data and then reclassifies the training points using a majorityvote. Our analysis reveals the need for the two main ingredients of SubSVMs,namely class-balanced sampling and subsampled bagging. Experimental results onsynthetic as well as benchmark UCI data demonstrate the effectiveness of ourapproach. In addition to noise-tolerance, log-size subsampled bagging alsoyields significant run-time benefits over standard SVMs.
arxiv-2100-200 | Spectral Clustering Based on Local PCA | http://arxiv.org/pdf/1301.2007v1.pdf | author:Ery Arias-Castro, Gilad Lerman, Teng Zhang category:stat.ML published:2013-01-09 summary:We propose a spectral clustering method based on local principal componentsanalysis (PCA). After performing local PCA in selected neighborhoods, thealgorithm builds a nearest neighbor graph weighted according to a discrepancybetween the principal subspaces in the neighborhoods, and then applies spectralclustering. As opposed to standard spectral methods based solely on pairwisedistances between points, our algorithm is able to resolve intersections. Weestablish theoretical guarantees for simpler variants within a prototypicalmathematical framework for multi-manifold clustering, and evaluate ouralgorithm on various simulated data sets.
arxiv-2100-201 | Syntactic Analysis Based on Morphological Characteristic Features of the Romanian Language | http://arxiv.org/pdf/1301.1950v1.pdf | author:Bogdan Patrut category:cs.CL cs.AI 68T50 published:2013-01-09 summary:This paper refers to the syntactic analysis of phrases in Romanian, as animportant process of natural language processing. We will suggest a real-timesolution, based on the idea of using some words or groups of words thatindicate grammatical category; and some specific endings of some parts ofsentence. Our idea is based on some characteristics of the Romanian language,where some prepositions, adverbs or some specific endings can provide a lot ofinformation about the structure of a complex sentence. Such characteristics canbe found in other languages, too, such as French. Using a special grammar, wedeveloped a system (DIASEXP) that can perform a dialogue in natural languagewith assertive and interogative sentences about a "story" (a set of sentencesdescribing some events from the real life).
arxiv-2100-202 | Risk-Aversion in Multi-armed Bandits | http://arxiv.org/pdf/1301.1936v1.pdf | author:Amir Sani, Alessandro Lazaric, Rémi Munos category:cs.LG published:2013-01-09 summary:Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma andultimately maximize the expected reward. Nonetheless, in many practicalproblems, maximizing the expected reward is not the most desirable objective.In this paper, we introduce a novel setting based on the principle ofrisk-aversion where the objective is to compete against the arm with the bestrisk-return trade-off. This setting proves to be intrinsically more difficultthan the standard multi-arm bandit setting due in part to an exploration riskwhich introduces a regret associated to the variability of an algorithm. Usingvariance as a measure of risk, we introduce two new algorithms, investigatetheir theoretical guarantees, and report preliminary empirical results.
arxiv-2100-203 | Nonparametric Reduced Rank Regression | http://arxiv.org/pdf/1301.1919v1.pdf | author:Rina Foygel, Michael Horrell, Mathias Drton, John Lafferty category:stat.ML published:2013-01-09 summary:We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models. An additive model isestimated for each dimension of a $q$-dimensional response, with a shared$p$-dimensional predictor variable. To control the complexity of the model, weemploy a functional form of the Ky-Fan or nuclear norm, resulting in a set offunction estimates that have low rank. Backfitting algorithms are derived andjustified using a nonparametric form of the nuclear norm subdifferential.Oracle inequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting. The methods areillustrated on gene expression data.
arxiv-2100-204 | Moon Search Algorithms for NASA's Dawn Mission to Asteroid Vesta | http://arxiv.org/pdf/1301.1907v1.pdf | author:Nargess Memarsadeghi, Lucy A. McFadden, David Skillman, Brian McLean, Max Mutchler, Uri Carsenty, Eric E. Palmer, the Dawn Mission's S category:astro-ph.IM astro-ph.EP cs.CV published:2013-01-09 summary:A moon or natural satellite is a celestial body that orbits a planetary bodysuch as a planet, dwarf planet, or an asteroid. Scientists seek understandingthe origin and evolution of our solar system by studying moons of these bodies.Additionally, searches for satellites of planetary bodies can be important toprotect the safety of a spacecraft as it approaches or orbits a planetary body.If a satellite of a celestial body is found, the mass of that body can also becalculated once its orbit is determined. Ensuring the Dawn spacecraft's safetyon its mission to the asteroid (4) Vesta primarily motivated the work of Dawn'sSatellite Working Group (SWG) in summer of 2011. Dawn mission scientists andengineers utilized various computational tools and techniques for Vesta'ssatellite search. The objectives of this paper are to 1) introduce the naturalsatellite search problem, 2) present the computational challenges, approaches,and tools used when addressing this problem, and 3) describe applications ofvarious image processing and computational algorithms for performing satellitesearches to the electronic imaging and computer science community. Furthermore,we hope that this communication would enable Dawn mission scientists to improvetheir satellite search algorithms and tools and be better prepared forperforming the same investigation in 2015, when the spacecraft is scheduled toapproach and orbit the dwarf planet (1) Ceres.
arxiv-2100-205 | Image Registration for Stability Testing of MEMS | http://arxiv.org/pdf/1301.1897v1.pdf | author:Nargess Memarsadeghi, Jacqueline Le Moigne, Peter N. Blake, Peter A. Morey, Wayne B. Landsman, Victor J. Chambers, Samuel H. Moseley category:cs.CV astro-ph.IM published:2013-01-09 summary:Image registration, or alignment of two or more images covering the samescenes or objects, is of great interest in many disciplines such as remotesensing, medical imaging, astronomy, and computer vision. In this paper, weintroduce a new application of image registration algorithms. We demonstratehow through a wavelet based image registration algorithm, engineers canevaluate stability of Micro-Electro-Mechanical Systems (MEMS). In particular,we applied image registration algorithms to assess alignment stability of theMicroShutters Subsystem (MSS) of the Near Infrared Spectrograph (NIRSpec)instrument of the James Webb Space Telescope (JWST). This work introduces a newmethodology for evaluating stability of MEMS devices to engineers as well as anew application of image registration algorithms to computer scientists.
arxiv-2100-206 | Short-time homomorphic wavelet estimation | http://arxiv.org/pdf/1209.0196v3.pdf | author:Roberto H. Herrera, Mirko Van der Baan category:physics.geo-ph cs.CV published:2012-09-02 summary:Successful wavelet estimation is an essential step for seismic methods likeimpedance inversion, analysis of amplitude variations with offset and fullwaveform inversion. Homomorphic deconvolution has long intrigued as apotentially elegant solution to the wavelet estimation problem. Yet asuccessful implementation has proven difficult. Associated disadvantages likephase unwrapping and restrictions of sparsity in the reflectivity functionlimit its application. We explore short-time homomorphic wavelet estimation asa combination of the classical homomorphic analysis and log-spectral averaging.The introduced method of log-spectral averaging using a short-term Fouriertransform increases the number of sample points, thus reducing estimationvariances. We apply the developed method on synthetic and real data examplesand demonstrate good performance.
arxiv-2100-207 | Linear Bandits in High Dimension and Recommendation Systems | http://arxiv.org/pdf/1301.1722v1.pdf | author:Yash Deshpande, Andrea Montanari category:cs.LG stat.ML published:2013-01-08 summary:A large number of online services provide automated recommendations to helpusers to navigate through a large collection of items. New items (products,videos, songs, advertisements) are suggested on the basis of the user's pasthistory and --when available-- her demographic profile. Recommendations have tosatisfy the dual goal of helping the user to explore the space of availableitems, while allowing the system to probe the user's preferences. We model this trade-off using linearly parametrized multi-armed bandits,propose a policy and prove upper and lower bounds on the cumulative "reward"that coincide up to constants in the data poor (high-dimensional) regime. Priorwork on linear bandits has focused on the data rich (low-dimensional) regimeand used cumulative "risk" as the figure of merit. For this data rich regime,we provide a simple modification for our policy that achieves near-optimal riskperformance under more restrictive assumptions on the geometry of the problem.We test (a variation of) the scheme used for establishing achievability on theNetflix and MovieLens datasets and obtain good agreement with the qualitativepredictions of the theory we develop.
arxiv-2100-208 | Causal graph-based video segmentation | http://arxiv.org/pdf/1301.1671v1.pdf | author:Camille Couprie, Clément Farabet, Yann LeCun category:cs.CV published:2013-01-08 summary:Numerous approaches in image processing and computer vision are making use ofsuper-pixels as a pre-processing step. Among the different methods producingsuch over-segmentation of an image, the graph-based approach of Felzenszwalband Huttenlocher is broadly employed. One of its interesting properties is thatthe regions are computed in a greedy manner in quasi-linear time. The algorithmmay be trivially extended to video segmentation by considering a video as a 3Dvolume, however, this can not be the case for causal segmentation, whensubsequent frames are unknown. We propose an efficient video segmentationapproach that computes temporally consistent pixels in a causal manner, fillingthe need for causal and real time applications.
arxiv-2100-209 | Distance Metric Learning for Kernel Machines | http://arxiv.org/pdf/1208.3422v2.pdf | author:Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle category:stat.ML cs.LG published:2012-08-16 summary:Recent work in metric learning has significantly improved thestate-of-the-art in k-nearest neighbor classification. Support vector machines(SVM), particularly with RBF kernels, are amongst the most popularclassification algorithms that uses distance metrics to compare examples. Thispaper provides an empirical analysis of the efficacy of three of the mostpopular Mahalanobis metric learning algorithms as pre-processing for SVMtraining. We show that none of these algorithms generate metrics that lead toparticularly satisfying improvements for SVM-RBF classification. As a remedy weintroduce support vector metric learning (SVML), a novel algorithm thatseamlessly combines the learning of a Mahalanobis metric with the training ofthe RBF-SVM parameters. We demonstrate the capabilities of SVML on ninebenchmark data sets of varying sizes and difficulties. In our study, SVMLoutperforms all alternative state-of-the-art metric learning algorithms interms of accuracy and establishes itself as a serious alternative to thestandard Euclidean metric with model selection by cross validation.
arxiv-2100-210 | The RNA Newton Polytope and Learnability of Energy Parameters | http://arxiv.org/pdf/1301.1608v1.pdf | author:Elmirasadat Forouzmand, Hamidreza Chitsaz category:q-bio.BM cs.CE cs.LG published:2013-01-08 summary:Despite nearly two scores of research on RNA secondary structure and RNA-RNAinteraction prediction, the accuracy of the state-of-the-art algorithms arestill far from satisfactory. Researchers have proposed increasingly complexenergy models and improved parameter estimation methods in anticipation ofendowing their methods with enough power to solve the problem. The output hasdisappointingly been only modest improvements, not matching the expectations.Even recent massively featured machine learning approaches were not able tobreak the barrier. In this paper, we introduce the notion of learnability ofthe parameters of an energy model as a measure of its inherent capability. Wesay that the parameters of an energy model are learnable iff there exists atleast one set of such parameters that renders every known RNA structure to datethe minimum free energy structure. We derive a necessary condition for thelearnability and give a dynamic programming algorithm to assess it. Ouralgorithm computes the convex hull of the feature vectors of all feasiblestructures in the ensemble of a given input sequence. Interestingly, thatconvex hull coincides with the Newton polytope of the partition function as apolynomial in energy parameters. We demonstrated the application of our theoryto a simple energy model consisting of a weighted count of A-U and C-G basepairs. Our results show that this simple energy model satisfies the necessarycondition for less than one third of the input unpseudoknottedsequence-structure pairs chosen from the RNA STRAND v2.0 database. For anotherone third, the necessary condition is barely violated, which suggests thataugmenting this simple energy model with more features such as the Turner loopsmay solve the problem. The necessary condition is severely violated for 8%,which provides a small set of hard cases that require further investigation.
arxiv-2100-211 | An Analysis of Gene Expression Data using Penalized Fuzzy C-Means Approach | http://arxiv.org/pdf/1302.3123v1.pdf | author:P. K. Nizar Banu, H. Hannah Inbarani category:cs.CV cs.CE published:2013-01-08 summary:With the rapid advances of microarray technologies, large amounts ofhigh-dimensional gene expression data are being generated, which posessignificant computational challenges. A first step towards addressing thischallenge is the use of clustering techniques, which is essential in the datamining process to reveal natural structures and identify interesting patternsin the underlying data. A robust gene expression clustering approach tominimize undesirable clustering is proposed. In this paper, Penalized FuzzyC-Means (PFCM) Clustering algorithm is described and compared with the mostrepresentative off-line clustering techniques: K-Means Clustering, RoughK-Means Clustering and Fuzzy C-Means clustering. These techniques areimplemented and tested for a Brain Tumor gene expression Dataset. Analysis ofthe performance of the proposed approach is presented through qualitativevalidation experiments. From experimental results, it can be observed thatPenalized Fuzzy C-Means algorithm shows a much higher usability than the otherprojected clustering algorithms used in our comparison study. Significant andpromising clustering results are presented using Brain Tumor Gene expressiondataset. Thus patterns seen in genome-wide expression experiments can beinterpreted as indications of the status of cellular processes. In theseclustering results, we find that Penalized Fuzzy C-Means algorithm providesuseful information as an aid to diagnosis in oncology.
arxiv-2100-212 | An Efficient Algorithm for Upper Bound on the Partition Function of Nucleic Acids | http://arxiv.org/pdf/1301.1590v1.pdf | author:Hamidreza Chitsaz, Elmirasadat Forouzmand, Gholamreza Haffari category:q-bio.BM cs.LG published:2013-01-08 summary:It has been shown that minimum free energy structure for RNAs and RNA-RNAinteraction is often incorrect due to inaccuracies in the energy parameters andinherent limitations of the energy model. In contrast, ensemble basedquantities such as melting temperature and equilibrium concentrations can bemore reliably predicted. Even structure prediction by sampling from theensemble and clustering those structures by Sfold [7] has proven to be morereliable than minimum free energy structure prediction. The main obstacle forensemble based approaches is the computational complexity of the partitionfunction and base pairing probabilities. For instance, the space complexity ofthe partition function for RNA-RNA interaction is $O(n^4)$ and the timecomplexity is $O(n^6)$ which are prohibitively large [4,12]. Our goal in thispaper is to give a fast algorithm, based on sparse folding, to calculate anupper bound on the partition function. Our work is based on the recentalgorithm of Hazan and Jaakkola [10]. The space complexity of our algorithm isthe same as that of sparse folding algorithms, and the time complexity of ouralgorithm is $O(MFE(n)\ell)$ for single RNA and $O(MFE(m, n)\ell)$ for RNA-RNAinteraction in practice, in which $MFE$ is the running time of sparse foldingand $\ell \leq n$ ($\ell \leq n + m$) is a sequence dependent parameter.
arxiv-2100-213 | A novel processing pipeline for optical multi-touch surfaces | http://arxiv.org/pdf/1301.1551v1.pdf | author:Philipp Ewerling category:cs.CV published:2013-01-08 summary:In this thesis a new approach for touch detection on optical multi-touchdevices is proposed that exploits the fact that the camera images reveal notonly the actual touch points but also objects above the screen such as the handor arm of a user. The touch processing relies on the Maximally Stable ExtremalRegions algorithm for finding the users' fingertips in the camera image. Thehierarchical structure of the generated extremal regions serves as a startingpoint for agglomerative clustering of the fingertips into hands. Furthermore, aheuristic is suggested that supports the identification of individual fingersas well as the distinction between left hands and right hands if all fivefingers of a hand are in contact with the touch surface. The evaluation confirmed that the system is robust against detection errorsresulting from non-uniform illumination and reliably assigns touch points toindividual hands based on the implicitly tracked context information. Theefficient multi-threaded implementation handles two-handed input from multipleusers in real-time.
arxiv-2100-214 | Adaptation of fictional and online conversations to communication media | http://arxiv.org/pdf/1301.1429v1.pdf | author:Christian M. Alis, May T. Lim category:physics.soc-ph cs.CL published:2013-01-08 summary:Conversations allow the quick transfer of short bits of information and it isreasonable to expect that changes in communication medium affect how weconverse. Using conversations in works of fiction and in an online socialnetworking platform, we show that the utterance length of conversations isslowly shortening with time but adapts more strongly to the constraints of thecommunication medium. This indicates that the introduction of any new medium ofcommunication can affect the way natural language evolves.
arxiv-2100-215 | PaFiMoCS: Particle Filtered Modified-CS and Applications in Visual Tracking across Illumination Change | http://arxiv.org/pdf/1301.1374v1.pdf | author:R. Sarkar, S. Das, N. Vaswani category:cs.CV published:2013-01-08 summary:We study the problem of tracking (causally estimating) a time sequence ofsparse spatial signals with changing sparsity patterns, as well as otherunknown states, from a sequence of nonlinear observations corrupted by(possibly) non-Gaussian noise. In many applications, particularly those invisual tracking, the unknown state can be split into a small dimensional part,e.g. global motion, and a spatial signal, e.g. illumination or shapedeformation. The spatial signal is often well modeled as being sparse in somedomain. For a long sequence, its sparsity pattern can change over time,although the changes are usually slow. To address the above problem, we proposea novel solution approach called Particle Filtered Modified-CS (PaFiMoCS). Thekey idea of PaFiMoCS is to importance sample for the small dimensional statevector, while replacing importance sampling by slow sparsity pattern changeconstrained posterior mode tracking for recovering the sparse spatial signal.We show that the problem of tracking moving objects across spatially varyingillumination change is an example of the above problem and explain how todesign PaFiMoCS for it. Experiments on both simulated data as well as on realvideos with significant illumination changes demonstrate the superiority of theproposed algorithm as compared with existing particle filter based trackingalgorithms.
arxiv-2100-216 | Automated Variational Inference in Probabilistic Programming | http://arxiv.org/pdf/1301.1299v1.pdf | author:David Wingate, Theophane Weber category:stat.ML cs.AI cs.LG published:2013-01-07 summary:We present a new algorithm for approximate inference in probabilisticprograms, based on a stochastic gradient for variational programs. This methodis efficient without restrictions on the probabilistic program; it isparticularly practical for distributions which are not analytically tractable,including highly structured distributions that arise in probabilistic programs.We show how to automatically derive mean-field probabilistic programs andoptimize them, and demonstrate that our perspective improves inferenceefficiency over other algorithms.
arxiv-2100-217 | Time-Frequency Representation of Microseismic Signals using the Synchrosqueezing Transform | http://arxiv.org/pdf/1301.1295v1.pdf | author:Roberto H. Herrera, Jean-Baptiste Tary, Mirko van der Baan category:physics.geo-ph cs.CE cs.CV published:2013-01-07 summary:Resonance frequencies can provide useful information on the deformationoccurring during fracturing experiments or $CO_2$ management, complementary tothe microseismic event distribution. An accurate time-frequency representationis of crucial importance prior to interpreting the cause of resonancefrequencies during microseismic experiments. The popular methods of Short-TimeFourier Transform (STFT) and wavelet analysis have limitations in representingclose frequencies and dealing with fast varying instantaneous frequencies andthis is often the nature of microseismic signals. The synchrosqueezingtransform (SST) is a promising tool to track these resonant frequencies andprovide a detailed time-frequency representation. Here we apply thesynchrosqueezing transform to microseismic signals and also show its potentialto general seismic signal processing applications.
arxiv-2100-218 | A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning | http://arxiv.org/pdf/1205.0288v2.pdf | author:Arash Afkanpour, András György, Csaba Szepesvári, Michael Bowling category:cs.LG stat.ML published:2012-05-01 summary:We consider the problem of simultaneously learning to linearly combine a verylarge number of kernels and learn a good predictor based on the learnt kernel.When the number of kernels $d$ to be combined is very large, multiple kernellearning methods whose computational cost scales linearly in $d$ areintractable. We propose a randomized version of the mirror descent algorithm toovercome this issue, under the objective of minimizing the group $p$-normpenalized empirical risk. The key to achieve the required exponential speed-upis the computationally efficient construction of low-variance estimates of thegradient. We propose importance sampling based estimates, and find that theideal distribution samples a coordinate with a probability proportional to themagnitude of the corresponding gradient. We show the surprising result that inthe case of learning the coefficients of a polynomial kernel, the combinatorialstructure of the base kernels to be combined allows the implementation ofsampling from this distribution to run in $O(\log(d))$ time, making the totalcomputational cost of the method to achieve an $\epsilon$-optimal solution tobe $O(\log(d)/\epsilon^2)$, thereby allowing our method to operate for verylarge values of $d$. Experiments with simulated and real data confirm that thenew algorithm is computationally more efficient than its state-of-the-artalternatives.
arxiv-2100-219 | Multi-criteria Anomaly Detection using Pareto Depth Analysis | http://arxiv.org/pdf/1110.3741v3.pdf | author:Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, Alfred O. Hero III category:cs.LG cs.CV cs.DB stat.ML published:2011-10-17 summary:We consider the problem of identifying patterns in a data set that exhibitanomalous behavior, often referred to as anomaly detection. In most anomalydetection algorithms, the dissimilarity between data samples is calculated by asingle criterion, such as Euclidean distance. However, in many cases there maynot exist a single dissimilarity measure that captures all possible anomalouspatterns. In such a case, multiple criteria can be defined, and one can testfor anomalies by scalarizing the multiple criteria using a linear combinationof them. If the importance of the different criteria are not known in advance,the algorithm may need to be executed multiple times with different choices ofweights in the linear combination. In this paper, we introduce a novelnon-parametric multi-criteria anomaly detection method using Pareto depthanalysis (PDA). PDA uses the concept of Pareto optimality to detect anomaliesunder multiple criteria without having to run an algorithm multiple times withdifferent choices of weights. The proposed PDA approach scales linearly in thenumber of criteria and is provably better than linear combinations of thecriteria.
arxiv-2100-220 | Dynamical Models and Tracking Regret in Online Convex Programming | http://arxiv.org/pdf/1301.1254v1.pdf | author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.LG published:2013-01-07 summary:This paper describes a new online convex optimization method whichincorporates a family of candidate dynamical models and establishes noveltracking regret bounds that scale with the comparator's deviation from the bestdynamical model in this family. Previous online optimization methods aredesigned to have a total accumulated loss comparable to that of the bestcomparator sequence, and existing tracking or shifting regret bounds scale withthe overall variation of the comparator sequence. In many practical scenarios,however, the environment is nonstationary and comparator sequences with smallvariation are quite weak, resulting in large losses. The proposed DynamicMirror Descent method, in contrast, can yield low regret relative to highlyvariable comparator sequences by both tracking the best dynamical model andforming predictions based on that model. This concept is demonstratedempirically in the context of sequential compressive observations of a dynamicscene and tracking a dynamic social network.
arxiv-2100-221 | Sparse Nonparametric Graphical Models | http://arxiv.org/pdf/1201.0794v2.pdf | author:John Lafferty, Han Liu, Larry Wasserman category:stat.ML cs.LG stat.ME published:2012-01-04 summary:We present some nonparametric methods for graphical modeling. In the discretecase, where the data are binary or drawn from a finite alphabet, Markov randomfields are already essentially nonparametric, since the cliques can take only afinite number of values. Continuous data are different. The Gaussian graphicalmodel is the standard parametric model for continuous data, but it makesdistributional assumptions that are often unrealistic. We discuss twoapproaches to building more flexible graphical models. One allows arbitrarygraphs and a nonparametric extension of the Gaussian; the other uses kerneldensity estimation and restricts the graphs to trees and forests. Examples ofboth methods are presented. We also discuss possible future research directionsfor nonparametric graphical modeling.
arxiv-2100-222 | Supervised, semi-supervised and unsupervised inference of gene regulatory networks | http://arxiv.org/pdf/1301.1083v1.pdf | author:Stefan R. Maetschke, Piyush B. Madhamshettiwar, Melissa J. Davis, Mark A. Ragan category:q-bio.MN q-bio.QM stat.ML published:2013-01-07 summary:Inference of gene regulatory network from expression data is a challengingtask. Many methods have been developed to this purpose but a comprehensiveevaluation that covers unsupervised, semi-supervised and supervised methods,and provides guidelines for their practical application, is lacking. We performed an extensive evaluation of inference methods on simulatedexpression data. The results reveal very low prediction accuracies forunsupervised techniques with the notable exception of the z-score method onknock-out data. In all other cases the supervised approach achieved the highestaccuracies and even in a semi-supervised setting with small numbers of onlypositive samples, outperformed the unsupervised techniques.
arxiv-2100-223 | Recklessly Approximate Sparse Coding | http://arxiv.org/pdf/1208.0959v2.pdf | author:Misha Denil, Nando de Freitas category:cs.LG cs.CV stat.ML published:2012-08-04 summary:It has recently been observed that certain extremely simple feature encodingtechniques are able to achieve state of the art performance on several standardimage classification benchmarks including deep belief networks, convolutionalnets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders andseveral others. Moreover, these "triangle" or "soft threshold" encodings areex- tremely efficient to compute. Several intuitive arguments have been putforward to explain this remarkable performance, yet no mathematicaljustification has been offered. The main result of this report is to show that these features are realized asan approximate solution to the a non-negative sparse coding problem. Using thisconnection we describe several variants of the soft threshold features anddemonstrate their effectiveness on two image classification benchmark tasks.
arxiv-2100-224 | Greedy Sparsity-Constrained Optimization | http://arxiv.org/pdf/1203.5483v3.pdf | author:Sohail Bahmani, Bhiksha Raj, Petros Boufounos category:stat.ML math.NA math.OC stat.CO 62FXX, 65KXX published:2012-03-25 summary:Sparsity-constrained optimization has wide applicability in machine learning,statistics, and signal processing problems such as feature selection andcompressive Sensing. A vast body of work has studied the sparsity-constrainedoptimization from theoretical, algorithmic, and application aspects in thecontext of sparse estimation in linear models where the fidelity of theestimate is measured by the squared error. In contrast, relatively less efforthas been made in the study of sparsity-constrained optimization in cases wherenonlinear models are involved or the cost function is not quadratic. In thispaper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), toapproximate sparse minima of cost functions of arbitrary form. Should a costfunction have a Stable Restricted Hessian (SRH) or a Stable RestrictedLinearization (SRL), both of which are introduced in this paper, our algorithmis guaranteed to produce a sparse vector within a bounded distance from thetrue sparse optimum. Our approach generalizes known results for quadratic costfunctions that arise in sparse linear regression and Compressive Sensing. Wealso evaluate the performance of GraSP through numerical simulations onsynthetic data, where the algorithm is employed for sparse logistic regressionwith and without $\ell_2$-regularization.
arxiv-2100-225 | Stratified SIFT Matching for Human Iris Recognition | http://arxiv.org/pdf/1301.0998v1.pdf | author:Sambit Bakshi, Hunny Mehrotra, Banshidhar Majhi category:cs.CV 68U10 published:2013-01-06 summary:This paper proposes an efficient three fold stratified SIFT matching for irisrecognition. The objective is to filter wrongly paired conventional SIFTmatches. In Strata I, the keypoints from gallery and probe iris images arepaired using traditional SIFT approach. Due to high image similarity atdifferent regions of iris there may be some impairments. These are detected andfiltered by finding gradient of paired keypoints in Strata II. Further, thescaling factor of paired keypoints is used to remove impairments in Strata III.The pairs retained after Strata III are likely to be potential matches for irisrecognition. The proposed system performs with an accuracy of 96.08% and 97.15%on publicly available CASIAV3 and BATH databases respectively. This markssignificant improvement of accuracy and FAR over the existing SIFT matching foriris.
arxiv-2100-226 | Graph 3-coloring with a hybrid self-adaptive evolutionary algorithm | http://arxiv.org/pdf/1301.0939v1.pdf | author:Iztok Fister, Marjan Mernik, Bogdan Filipič category:cs.NE published:2013-01-05 summary:This paper proposes a hybrid self-adaptive evolutionary algorithm for graphcoloring that is hybridized with the following novel elements: heuristicgenotype-phenotype mapping, a swap local search heuristic, and a neutralsurvivor selection operator. This algorithm was compared with the evolutionaryalgorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertzand de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. Theperformance of these algorithms were tested on a test suite consisting ofrandomly generated 3-colorable graphs of various structural features, such asgraph size, type, edge density, and variability in sizes of color classes.Furthermore, the test graphs were generated including the phase transitionwhere the graphs are hard to color. The purpose of the extensive experimentalwork was threefold: to investigate the behavior of the tested algorithms in thephase transition, to identify what impact hybridization with the DSaturtraditional heuristic has on the evolutionary algorithm, and to show how graphstructural features influence the performance of the graph-coloring algorithms.The results indicate that the performance of the hybrid self-adaptiveevolutionary algorithm is comparable with, or better than, the performance ofthe hybrid evolutionary algorithm which is one of the best graph-coloringalgorithms today. Moreover, the fact that all the considered algorithmsperformed poorly on flat graphs confirms that this type of graphs is really thehardest to color.
arxiv-2100-227 | Comparative Studies on Decentralized Multiloop PID Controller Design Using Evolutionary Algorithms | http://arxiv.org/pdf/1301.0930v1.pdf | author:Sayan Saha, Saptarshi Das, Anindya Pakhira, Sumit Mukherjee, Indranil Pan category:cs.SY cs.NE published:2013-01-05 summary:Decentralized PID controllers have been designed in this paper forsimultaneous tracking of individual process variables in multivariable systemsunder step reference input. The controller design framework takes into accountthe minimization of a weighted sum of Integral of Time multiplied Squared Error(ITSE) and Integral of Squared Controller Output (ISCO) so as to balance theoverall tracking errors for the process variables and required variation in thecorresponding manipulated variables. Decentralized PID gains are tuned usingthree popular Evolutionary Algorithms (EAs) viz. Genetic Algorithm (GA),Evolutionary Strategy (ES) and Cultural Algorithm (CA). Credible simulationcomparisons have been reported for four benchmark 2x2 multivariable processes.
arxiv-2100-228 | Chaotic multi-objective optimization based design of fractional order PIλDμ controller in AVR system | http://arxiv.org/pdf/1205.1765v2.pdf | author:Indranil Pan, Saptarshi Das category:cs.SY cs.NE published:2012-05-08 summary:In this paper, a fractional order (FO) PI{\lambda}D\mu controller is designedto take care of various contradictory objective functions for an AutomaticVoltage Regulator (AVR) system. An improved evolutionary Non-dominated SortingGenetic Algorithm II (NSGA II), which is augmented with a chaotic map forgreater effectiveness, is used for the multi-objective optimization problem.The Pareto fronts showing the trade-off between different design criteria areobtained for the PI{\lambda}D\mu and PID controller. A comparative analysis isdone with respect to the standard PID controller to demonstrate the merits anddemerits of the fractional order PI{\lambda}D\mu controller.
arxiv-2100-229 | Hybridization of Evolutionary Algorithms | http://arxiv.org/pdf/1301.0929v1.pdf | author:Iztok Fister, Marjan Mernik, Janez Brest category:cs.NE published:2013-01-05 summary:Evolutionary algorithms are good general problem solver but suffer from alack of domain specific knowledge. However, the problem specific knowledge canbe added to evolutionary algorithms by hybridizing. Interestingly, all theelements of the evolutionary algorithms can be hybridized. In this chapter, thehybridization of the three elements of the evolutionary algorithms isdiscussed: the objective function, the survivor selection operator and theparameter settings. As an objective function, the existing heuristic functionthat construct the solution of the problem in traditional way is used. However,this function is embedded into the evolutionary algorithm that serves as agenerator of new solutions. In addition, the objective function is improved bylocal search heuristics. The new neutral selection operator has been developedthat is capable to deal with neutral solutions, i.e. solutions that have thedifferent representation but expose the equal values of objective function. Theaim of this operator is to directs the evolutionary search into a newundiscovered regions of the search space. To avoid of wrong setting ofparameters that control the behavior of the evolutionary algorithm, theself-adaptation is used. Finally, such hybrid self-adaptive evolutionaryalgorithm is applied to the two real-world NP-hard problems: the graph3-coloring and the optimization of markers in the clothing industry. Extensiveexperiments shown that these hybridization improves the results of theevolutionary algorithms a lot. Furthermore, the impact of the particularhybridizations is analyzed in details as well.
arxiv-2100-230 | Sparse Recovery from Nonlinear Measurements with Applications in Bad Data Detection for Power Networks | http://arxiv.org/pdf/1112.6234v2.pdf | author:Weiyu Xu, Meng Wang, Jianfeng Cai, Ao Tang category:cs.IT cs.LG cs.SY math.IT published:2011-12-29 summary:In this paper, we consider the problem of sparse recovery from nonlinearmeasurements, which has applications in state estimation and bad data detectionfor power networks. An iterative mixed $\ell_1$ and $\ell_2$ convex program isused to estimate the true state by locally linearizing the nonlinearmeasurements. When the measurements are linear, through using the almostEuclidean property for a linear subspace, we derive a new performance bound forthe state estimation error under sparse bad data and additive observationnoise. As a byproduct, in this paper we provide sharp bounds on the almostEuclidean property of a linear subspace, using the "escape-through-the-mesh"theorem from geometric functional analysis. When the measurements arenonlinear, we give conditions under which the solution of the iterativealgorithm converges to the true state even though the locally linearizedmeasurements may not be the actual nonlinear measurements. We numericallyevaluate our iterative convex programming approach to perform bad datadetections in nonlinear electrical power networks problems. We are able to usesemidefinite programming to verify the conditions for convergence of theproposed iterative sparse recovery algorithms from nonlinear measurements.
arxiv-2100-231 | Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human Thought | http://arxiv.org/pdf/1206.1069v2.pdf | author:Diederik Aerts, Liane Gabora, Sandro Sozzo category:cs.AI cs.CL quant-ph published:2012-06-05 summary:We analyze different aspects of our quantum modeling approach of humanconcepts, and more specifically focus on the quantum effects of contextuality,interference, entanglement and emergence, illustrating how each of them makesits appearance in specific situations of the dynamics of human concepts andtheir combinations. We point out the relation of our approach, which is basedon an ontology of a concept as an entity in a state changing under influence ofa context, with the main traditional concept theories, i.e. prototype theory,exemplar theory and theory theory. We ponder about the question why quantumtheory performs so well in its modeling of human concepts, and shed light onthis question by analyzing the role of complex amplitudes, showing how theyallow to describe interference in the statistics of measurement outcomes, whilein the traditional theories statistics of outcomes originates in classicalprobability weights, without the possibility of interference. The relevance ofcomplex numbers, the appearance of entanglement, and the role of Fock space inexplaining contextual emergence, all as unique features of the quantummodeling, are explicitly revealed in this paper by analyzing human concepts andtheir dynamics.
arxiv-2100-232 | A New Geometric Approach to Latent Topic Modeling and Discovery | http://arxiv.org/pdf/1301.0858v1.pdf | author:Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama category:stat.ML published:2013-01-05 summary:A new geometrically-motivated algorithm for nonnegative matrix factorizationis developed and applied to the discovery of latent "topics" for text and image"document" corpora. The algorithm is based on robustly finding and clusteringextreme points of empirical cross-document word-frequencies that correspond tonovel "words" unique to each topic. In contrast to related approaches that arebased on solving non-convex optimization problems using suboptimalapproximations, locally-optimal methods, or heuristics, the new algorithm isconvex, has polynomial complexity, and has competitive qualitative andquantitative performance compared to the current state-of-the-art approaches onsynthetic and real-world datasets.
arxiv-2100-233 | Mahotas: Open source software for scriptable computer vision | http://arxiv.org/pdf/1211.4907v2.pdf | author:Luis Pedro Coelho category:cs.CV cs.SE published:2012-11-21 summary:Mahotas is a computer vision library for Python. It contains traditionalimage processing functionality such as filtering and morphological operationsas well as more modern computer vision functions for feature computation,including interest point detection and local descriptors. The interface is in Python, a dynamic programming language, which is veryappropriate for fast development, but the algorithms are implemented in C++ andare tuned for speed. The library is designed to fit in with the scientificsoftware ecosystem in this language and can leverage the existinginfrastructure developed in that language. Mahotas is released under a liberal open source license (MIT License) and isavailable from (http://github.com/luispedro/mahotas) and from the PythonPackage Index (http://pypi.python.org/pypi/mahotas).
arxiv-2100-234 | Content-boosted Matrix Factorization Techniques for Recommender Systems | http://arxiv.org/pdf/1210.5631v2.pdf | author:Jennifer Nguyen, Mu Zhu category:stat.ML cs.LG published:2012-10-20 summary:Many businesses are using recommender systems for marketing outreach.Recommendation algorithms can be either based on content or driven bycollaborative filtering. We study different ways to incorporate contentinformation directly into the matrix factorization approach of collaborativefiltering. These content-boosted matrix factorization algorithms not onlyimprove recommendation accuracy, but also provide useful insights about thecontents, as well as make recommendations more easily interpretable.
arxiv-2100-235 | Role Mining with Probabilistic Models | http://arxiv.org/pdf/1212.4775v3.pdf | author:Mario Frank, Joachim M. Buhmann, David Basin category:cs.CR cs.LG stat.ML published:2012-12-19 summary:Role mining tackles the problem of finding a role-based access control (RBAC)configuration, given an access-control matrix assigning users to accesspermissions as input. Most role mining approaches work by constructing a largeset of candidate roles and use a greedy selection strategy to iteratively picka small subset such that the differences between the resulting RBACconfiguration and the access control matrix are minimized. In this paper, weadvocate an alternative approach that recasts role mining as an inferenceproblem rather than a lossy compression problem. Instead of using combinatorialalgorithms to minimize the number of roles needed to represent theaccess-control matrix, we derive probabilistic models to learn the RBACconfiguration that most likely underlies the given matrix. Our models are generative in that they reflect the way that permissions areassigned to users in a given RBAC configuration. We additionally model howuser-permission assignments that conflict with an RBAC configuration emerge andwe investigate the influence of constraints on role hierarchies and on thenumber of assignments. In experiments with access-control matrices fromreal-world enterprises, we compare our proposed models with other role miningmethods. Our results show that our probabilistic models infer roles thatgeneralize well to new system users for a wide variety of data, while othermodels' generalization abilities depend on the dataset given.
arxiv-2100-236 | Adaptive Intelligent Cooperative Spectrum Sensing In Cognitive Radio | http://arxiv.org/pdf/1301.0785v1.pdf | author:Dilip S Aldar category:cs.NE published:2013-01-04 summary:Radio Spectrum is most precious and scarce resource and must be utilizedefficiently and effectively. Cognitive radio is the promising solutions for theoptimum utilization of the scared natural resource. The spectrum owned by theprimary user should be shared among the secondary user, but primary user shouldnot be interfered by the secondary user. In order to utilize the primary userspectrum, secondary user must detect accurately, the existence of primary inthe band of interest. In cooperative spectrum sensing, the channel between thesecondary users and the cognitive radio base station is non stationary andcauses interference in the decision in decision fusion and in information ininformation due to multipath fading. In this paper neural network basedcooperative spectrum sensing method is proposed, the performance of proposedmethod is evaluated and observed that, the neural network based schemeperformance improve significantly over the AND,OR and Majority rule
arxiv-2100-237 | The Sum-over-Forests density index: identifying dense regions in a graph | http://arxiv.org/pdf/1301.0725v1.pdf | author:Mathieu Senelle, Silvia Garcia-Diez, Amin Mantrach, Masashi Shimbo, Marco Saerens, François Fouss category:cs.LG stat.ML published:2013-01-04 summary:This work introduces a novel nonparametric density index defined on graphs,the Sum-over-Forests (SoF) density index. It is based on a clear and intuitiveidea: high-density regions in a graph are characterized by the fact that theycontain a large amount of low-cost trees with high outdegrees while low-densityregions contain few ones. Therefore, a Boltzmann probability distribution onthe countable set of forests in the graph is defined so that large (high-cost)forests occur with a low probability while short (low-cost) forests occur witha high probability. Then, the SoF density index of a node is defined as theexpected outdegree of this node in a non-trivial tree of the forest, thusproviding a measure of density around that node. Following the matrix-foresttheorem, and a statistical physics framework, it is shown that the SoF densityindex can be easily computed in closed form through a simple matrix inversion.Experiments on artificial and real data sets show that the proposed indexperforms well on finding dense regions, for graphs of various origins.
arxiv-2100-238 | Multi-Level Modeling of Quotation Families Morphogenesis | http://arxiv.org/pdf/1209.4277v2.pdf | author:Elisa Omodei, Thierry Poibeau, Jean-Philippe Cointet category:cs.CY cs.CL cs.SI physics.soc-ph published:2012-09-19 summary:This paper investigates cultural dynamics in social media by examining theproliferation and diversification of clearly-cut pieces of content: quotedtexts. In line with the pioneering work of Leskovec et al. and Simmons et al.on memes dynamics we investigate in deep the transformations that quotationspublished online undergo during their diffusion. We deliberately put aside thestructure of the social network as well as the dynamical patterns pertaining tothe diffusion process to focus on the way quotations are changed, how oftenthey are modified and how these changes shape more or less diverse families andsub-families of quotations. Following a biological metaphor, we try tounderstand in which way mutations can transform quotations at different scalesand how mutation rates depend on various properties of the quotations.
arxiv-2100-239 | Investigating the performance of Correspondence Algorithms in Vision based Driver-assistance in Indoor Environment | http://arxiv.org/pdf/1301.0435v1.pdf | author:F. Mahmood, Syed. M. B. Haider, F. Kunwar category:cs.CV cs.RO published:2013-01-03 summary:This paper presents the experimental comparison of fourteen stereo matchingalgorithms in variant illumination conditions. Different adaptations of globaland local stereo matching techniques are chosen for evaluation The variantstrength and weakness of the chosen correspondence algorithms are explored byemploying the methodology of the prediction error strategy. The algorithms aregauged on the basis of their performance on real world data set taken invarious indoor lighting conditions and at different times of the day
arxiv-2100-240 | A Self-Organizing Neural Scheme for Door Detection in Different Environments | http://arxiv.org/pdf/1301.0432v1.pdf | author:F. Mahmood, F. Kunwar category:cs.CV published:2013-01-03 summary:Doors are important landmarks for indoor mobile robot navigation and alsoassist blind people to independently access unfamiliar buildings. Most existingalgorithms of door detection are limited to work for familiar environmentsbecause of restricted assumptions about color, texture and shape. In this paperwe propose a novel approach which employs feature based classification and usesthe Kohonen Self-Organizing Map (SOM) for the purpose of door detection.Generic and stable features are used for the training of SOM that increase theperformance significantly: concavity, bottom-edge intensity profile and dooredges. To validate the robustness and generalizability of our method, wecollected a large dataset of real world door images from a variety ofenvironments and different lighting conditions. The algorithm achieves morethan 95% detection which demonstrates that our door detection method is genericand robust with variations of color, texture, occlusions, lighting condition,scales, and viewpoints.
arxiv-2100-241 | A Method for Finding Structured Sparse Solutions to Non-negative Least Squares Problems with Applications | http://arxiv.org/pdf/1301.0413v1.pdf | author:Ernie Esser, Yifei Lou, Jack Xin category:stat.ML stat.AP stat.CO stat.ME published:2013-01-03 summary:Demixing problems in many areas such as hyperspectral imaging anddifferential optical absorption spectroscopy (DOAS) often require findingsparse nonnegative linear combinations of dictionary elements that matchobserved data. We show how aspects of these problems, such as misalignment ofDOAS references and uncertainty in hyperspectral endmembers, can be modeled byexpanding the dictionary with grouped elements and imposing a structuredsparsity assumption that the combinations within each group should be sparse oreven 1-sparse. If the dictionary is highly coherent, it is difficult to obtaingood solutions using convex or greedy methods, such as non-negative leastsquares (NNLS) or orthogonal matching pursuit. We use penalties related to theHoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsitypenalties to be added to the objective in NNLS-type models. For solving theresulting nonconvex models, we propose a scaled gradient projection algorithmthat requires solving a sequence of strongly convex quadratic programs. Wediscuss its close connections to convex splitting methods and difference ofconvex programming. We also present promising numerical results for exampleDOAS analysis and hyperspectral demixing problems.
arxiv-2100-242 | A Geometric Blind Source Separation Method Based on Facet Component Analysis | http://arxiv.org/pdf/1301.0339v1.pdf | author:P. Yin, Y. Sun, J. Xin category:math.NA stat.ML published:2013-01-02 summary:Given a set of mixtures, blind source separation attempts to retrieve thesource signals without or with very little information of the the mixingprocess. We present a geometric approach for blind separation of nonnegativelinear mixtures termed {\em facet component analysis} (FCA). The approach isbased on facet identification of the underlying cone structure of the data.Earlier works focus on recovering the cone by locating its vertices (vertexcomponent analysis or VCA) based on a mutual sparsity condition which requireseach source signal to possess a stand-alone peak in its spectrum. We formulatealternative conditions so that enough data points fall on the facets of a coneinstead of accumulating around the vertices. To find a regime of uniquesolvability, we make use of both geometric and density properties of the datapoints, and develop an efficient facet identification method by combining dataclassification and linear regression. For noisy data, we show that denoisingmethods may be employed, such as the total variation technique in imagingprocessing, and principle component analysis. We show computational results onnuclear magnetic resonance spectroscopic data to substantiate our method.
arxiv-2100-243 | A Novel Design Specification Distance(DSD) Based K-Mean Clustering Performace Evluation on Engineering Materials Database | http://arxiv.org/pdf/1301.0179v1.pdf | author:Doreswamy, K. S. Hemanth category:cs.LG published:2013-01-02 summary:Organizing data into semantically more meaningful is one of the fundamentalmodes of understanding and learning. Cluster analysis is a formal study ofmethods for understanding and algorithm for learning. K-mean clusteringalgorithm is one of the most fundamental and simple clustering algorithms. Whenthere is no prior knowledge about the distribution of data sets, K-mean is thefirst choice for clustering with an initial number of clusters. In this paper anovel distance metric called Design Specification (DS) distance measurefunction is integrated with K-mean clustering algorithm to improve clusteraccuracy. The K-means algorithm with proposed distance measure maximizes thecluster accuracy to 99.98% at P = 1.525, which is determined through theiterative procedure. The performance of Design Specification (DS) distancemeasure function with K - mean algorithm is compared with the performances ofother standard distance functions such as Euclidian, squared Euclidean, CityBlock, and Chebshew similarity measures deployed with K-mean algorithm.Theproposed method is evaluated on the engineering materials database. Theexperiments on cluster analysis and the outlier profiling show that these is anexcellent improvement in the performance of the proposed method.
arxiv-2100-244 | Classifier Fusion Method to Recognize Handwritten Kannada Numerals | http://arxiv.org/pdf/1301.0167v1.pdf | author:H. R. Mamatha, S. Karthik, Murthy K. Srikanta category:cs.CV published:2013-01-02 summary:Optical Character Recognition (OCR) is one of the important fields in imageprocessing and pattern recognition domain. Handwritten character recognitionhas always been a challenging task. Only a little work can be traced towardsthe recognition of handwritten characters for the south Indian languages.Kannada is one such south Indian language which is also one of the officiallanguage of India. Accurate recognition of Kannada characters is a challengingtask because of the high degree of similarity between the characters. Hence,good quality features are to be extracted and better classifiers are needed toimprove the accuracy of the OCR for Kannada characters. This paper explores theeffectiveness of feature extraction method like run length count (RLC) anddirectional chain code (DCC) for the recognition of handwritten Kannadanumerals. In this paper, a classifier fusion method is implemented to improvethe recognition rate. For the classifier fusion, we have considered K-nearestneighbour (KNN) and Linear classifier (LC). The novelty of this method is toachieve better accuracy with few features using classifier fusion approach.Proposed method achieves an average recognition rate of 96%.
arxiv-2100-245 | Data complexity measured by principal graphs | http://arxiv.org/pdf/1212.5841v2.pdf | author:Andrei Zinovyev, Evgeny Mirkes category:cs.LG cs.IT math.IT published:2012-12-23 summary:How to measure the complexity of a finite set of vectors embedded in amultidimensional space? This is a non-trivial question which can be approachedin many different ways. Here we suggest a set of data complexity measures usinguniversal approximators, principal cubic complexes. Principal cubic complexesgeneralise the notion of principal manifolds for datasets with non-trivialtopologies. The type of the principal cubic complex is determined by itsdimension and a grammar of elementary graph transformations. The simplestgrammar produces principal trees. We introduce three natural types of data complexity: 1) geometric (deviationof the data's approximator from some "idealized" configuration, such asdeviation from harmonicity); 2) structural (how many elements of a principalgraph are needed to approximate the data), and 3) construction complexity (howmany applications of elementary graph transformations are needed to constructthe principal object starting from the simplest one). We compute these measures for several simulated and real-life datadistributions and show them in the "accuracy-complexity" plots, helping tooptimize the accuracy/complexity ratio. We discuss various issues connectedwith measuring data complexity. Software for computing data complexity measuresfrom principal cubic complexes is provided as well.
arxiv-2100-246 | Semi-Supervised Domain Adaptation with Non-Parametric Copulas | http://arxiv.org/pdf/1301.0142v1.pdf | author:David Lopez-Paz, José Miguel Hernández-Lobato, Bernhard Schölkopf category:stat.ML cs.LG published:2013-01-01 summary:A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes anymultivariate density into a product of marginal distributions and bivariatecop- ula functions. Therefore, changes in each of these factors can be detectedand corrected to adapt a density model accross different learning domains.Impor- tantly, we introduce a novel vine copula model, which allows for thisfactorization in a non-parametric manner. Experimental results on regressionproblems with real-world data illustrate the efficacy of the proposed approachwhen compared to state-of-the-art techniques.
arxiv-2100-247 | Policy Evaluation with Variance Related Risk Criteria in Markov Decision Processes | http://arxiv.org/pdf/1301.0104v1.pdf | author:Aviv Tamar, Dotan Di Castro, Shie Mannor category:cs.LG stat.ML published:2013-01-01 summary:In this paper we extend temporal difference policy evaluation algorithms toperformance criteria that include the variance of the cumulative reward. Suchcriteria are useful for risk management, and are important in domains such asfinance and process control. We propose both TD(0) and LSTD(lambda) variantswith linear function approximation, prove their convergence, and demonstratetheir utility in a 4-dimensional continuous state space problem.
arxiv-2100-248 | CloudSVM : Training an SVM Classifier in Cloud Computing Systems | http://arxiv.org/pdf/1301.0082v1.pdf | author:F. Ozgur Catak, M. Erdal Balaban category:cs.LG cs.DC published:2013-01-01 summary:In conventional method, distributed support vector machines (SVM) algorithmsare trained over pre-configured intranet/internet environments to find out anoptimal classifier. These methods are very complicated and costly for largedatasets. Hence, we propose a method that is referred as the Cloud SVM trainingmechanism (CloudSVM) in a cloud computing environment with MapReduce techniquefor distributed machine learning applications. Accordingly, (i) SVM algorithmis trained in distributed cloud storage servers that work concurrently; (ii)merge all support vectors in every trained cloud node; and (iii) iterate thesetwo steps until the SVM converges to the optimal classifier function. Largescale data sets are not possible to train using SVM algorithm on a singlecomputer. The results of this study are important for training of large scaledata sets for machine learning applications. We provided that iterativetraining of splitted data set in cloud computing environment using SVM willconverge to a global optimal classifier in finite iteration size.
arxiv-2100-249 | Generating High-Order Threshold Functions with Multiple Thresholds | http://arxiv.org/pdf/1301.0048v1.pdf | author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE published:2013-01-01 summary:In this paper, we consider situations in which a given logical function isrealized by a multithreshold threshold function. In such situations, constantfunctions can be easily obtained from multithreshold threshold functions, andtherefore, we can show that it becomes possible to optimize a class ofhigh-order neural networks. We begin by proposing a generating method forthreshold functions in which we use a vector that determines the boundarybetween the linearly separable function and the high-order threshold function.By applying this method to high-order threshold functions, we show thatfunctions with the same weight as, but a different threshold than, a thresholdfunction generated by the generation process can be easily obtained. We alsoshow that the order of the entire network can be extended while maintaining thestructure of given functions.
arxiv-2100-250 | On Distributed Online Classification in the Midst of Concept Drifts | http://arxiv.org/pdf/1301.0047v1.pdf | author:Zaid J. Towfic, Jianshu Chen, Ali H. Sayed category:math.OC cs.DC cs.LG cs.SI physics.soc-ph published:2013-01-01 summary:In this work, we analyze the generalization ability of distributed onlinelearning algorithms under stationary and non-stationary environments. We derivebounds for the excess-risk attained by each node in a connected network oflearners and study the performance advantage that diffusion strategies haveover individual non-cooperative processing. We conduct extensive simulations toillustrate the results.
arxiv-2100-251 | Bethe Bounds and Approximating the Global Optimum | http://arxiv.org/pdf/1301.0015v1.pdf | author:Adrian Weller, Tony Jebara category:cs.LG stat.ML published:2012-12-31 summary:Inference in general Markov random fields (MRFs) is NP-hard, thoughidentifying the maximum a posteriori (MAP) configuration of pairwise MRFs withsubmodular cost functions is efficiently solvable using graph cuts. Marginalinference, however, even for this restricted class, is in #P. We prove newformulations of derivatives of the Bethe free energy, provide bounds on thederivatives and bracket the locations of stationary points, introducing a newtechnique called Bethe bound propagation. Several results apply to pairwisemodels whether associative or not. Applying these to discretizedpseudo-marginals in the associative case we present a polynomial timeapproximation scheme for global optimization provided the maximum degree is$O(\log n)$, and discuss several extensions.
arxiv-2100-252 | Fast Solutions to Projective Monotone Linear Complementarity Problems | http://arxiv.org/pdf/1212.6958v1.pdf | author:Geoffrey J. Gordon category:cs.LG math.OC published:2012-12-31 summary:We present a new interior-point potential-reduction algorithm for solvingmonotone linear complementarity problems (LCPs) that have a particular specialstructure: their matrix $M\in{\mathbb R}^{n\times n}$ can be decomposed as$M=\Phi U + \Pi_0$, where the rank of $\Phi$ is $k<n$, and $\Pi_0$ denotesEuclidean projection onto the nullspace of $\Phi^\top$. We call such LCPsprojective. Our algorithm solves a monotone projective LCP to relative accuracy$\epsilon$ in $O(\sqrt n \ln(1/\epsilon))$ iterations, with each iterationrequiring $O(nk^2)$ flops. This complexity compares favorably withinterior-point algorithms for general monotone LCPs: these algorithms alsorequire $O(\sqrt n \ln(1/\epsilon))$ iterations, but each iteration needs tosolve an $n\times n$ system of linear equations, a much higher cost than ouralgorithm when $k\ll n$. Our algorithm works even though the solution to aprojective LCP is not restricted to lie in any low-rank subspace.
arxiv-2100-253 | Blind Analysis of EGM Signals: Sparsity-Aware Formulation | http://arxiv.org/pdf/1212.6936v1.pdf | author:David Luengo, Javier Via, Sandra Monzon, Tom Trigano, Antonio Artes-Rodriguez category:stat.ML published:2012-12-31 summary:This technical note considers the problems of blind sparse learning andinference of electrogram (EGM) signals under atrial fibrillation (AF)conditions. First of all we introduce a mathematical model for the observedsignals that takes into account the multiple foci typically appearing insidethe heart during AF. Then we propose a reconstruction model based on a fixeddictionary and discuss several alternatives for choosing the dictionary. Inorder to obtain a sparse solution that takes into account the biologicalrestrictions of the problem, a first alternative is using LASSO regularizationfollowed by a post-processing stage that removes low amplitude coefficientsviolating the refractory period characteristic of cardiac cells. As analternative we propose a novel regularization term, called cross products LASSO(CP-LASSO), that is able to incorporate the biological constraints directlyinto the optimization problem. Unfortunately, the resulting problem isnon-convex, but we show how it can be solved efficiently in an approximated waymaking use of successive convex approximations (SCA). Finally, spectralanalysis is performed on the clean activation sequence obtained from the sparselearning stage in order to estimate the number of latent foci and theirfrequencies. Simulations on synthetic and real data are provided to validatethe proposed approach.
arxiv-2100-254 | Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems | http://arxiv.org/pdf/1212.6922v1.pdf | author:Yana Mazwin Mohmad Hassim, Rozaida Ghazali category:cs.NE cs.LG published:2012-12-31 summary:Artificial Neural Networks have emerged as an important tool forclassification and have been widely used to classify a non-linear separablepattern. The most popular artificial neural networks model is a MultilayerPerceptron (MLP) as it is able to perform classification task with significantsuccess. However due to the complexity of MLP structure and also problems suchas local minima trapping, over fitting and weight interference have made neuralnetwork training difficult. Thus, the easy way to avoid these problems is toremove the hidden layers. This paper presents the ability of Functional LinkNeural Network (FLNN) to overcome the complexity structure of MLP by usingsingle layer architecture and propose an Artificial Bee Colony (ABC)optimization for training the FLNN. The proposed technique is expected toprovide better learning scheme for a classifier in order to get more accurateclassification result
arxiv-2100-255 | Autonomously Learning to Visually Detect Where Manipulation Will Succeed | http://arxiv.org/pdf/1212.6837v1.pdf | author:Hai Nguyen, Charles C. Kemp category:cs.RO cs.AI cs.CV published:2012-12-31 summary:Visual features can help predict if a manipulation behavior will succeed at agiven location. For example, the success of a behavior that flips lightswitches depends on the location of the switch. Within this paper, we presentmethods that enable a mobile manipulator to autonomously learn a function thattakes an RGB image and a registered 3D point cloud as input and returns a 3Dlocation at which a manipulation behavior is likely to succeed. Given a pair ofmanipulation behaviors that can change the state of the world between two sets(e.g., light switch up and light switch down), classifiers that detect wheneach behavior has been successful, and an initial hint as to where one of thebehaviors will be successful, the robot autonomously trains a pair of supportvector machine (SVM) classifiers by trying out the behaviors at locations inthe world and observing the results. When an image feature vector associatedwith a 3D location is provided as input to one of the SVMs, the SVM predicts ifthe associated manipulation behavior will be successful at the 3D location. Toevaluate our approach, we performed experiments with a PR2 robot from WillowGarage in a simulated home using behaviors that flip a light switch, push arocker-type light switch, and operate a drawer. By using active learning, therobot efficiently learned SVMs that enabled it to consistently succeed at thesetasks. After training, the robot also continued to learn in order to adapt inthe event of failure.
arxiv-2100-256 | Pilgrims Face Recognition Dataset -- HUFRD | http://arxiv.org/pdf/1205.4463v2.pdf | author:Salah A. Aly category:cs.CV cs.CY published:2012-05-20 summary:In this work, we define a new pilgrims face recognition dataset, called HUFRDdataset. The new developed dataset presents various pilgrims' images taken fromoutside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrahseasons. Such dataset will be used to test our developed facial recognition anddetection algorithms, as well as assess in the missing and found recognitionsystem \cite{crowdsensing}.
arxiv-2100-257 | Inference algorithms for pattern-based CRFs on sequence data | http://arxiv.org/pdf/1210.0508v4.pdf | author:Rustem Takhanov, Vladimir Kolmogorov category:cs.LG cs.DS published:2012-10-01 summary:We consider Conditional Random Fields (CRFs) with pattern-based potentialsdefined on a chain. In this model the energy of a string (labeling) $x_1...x_n$is the sum of terms over intervals $[i,j]$ where each term is non-zero only ifthe substring $x_i...x_j$ equals a prespecified pattern $\alpha$. Such CRFs canbe naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in aCRF, namely computing (i) the partition function, (ii) marginals, and (iii)computing the MAP. Their complexities are respectively $O(n L)$, $O(n L\ell_{max})$ and $O(n L \min\{D,\log (\ell_{max}+1)\})$ where $L$ is thecombined length of input patterns, $\ell_{max}$ is the maximum length of apattern, and $D$ is the input alphabet. This improves on the previousalgorithms of (Ye et al., 2009) whose complexities are respectively $O(n LD)$, $O(n \Gamma L^2 \ell_{max}^2)$ and $O(n L D)$, where $\Gamma$ isthe number of input patterns. In addition, we give an efficient algorithm for sampling. Finally, weconsider the case of non-positive weights. (Komodakis & Paragios, 2009) gave an$O(n L)$ algorithm for computing the MAP. We present a modification that hasthe same worst-case complexity but can beat it in the best case.
arxiv-2100-258 | Focus of Attention for Linear Predictors | http://arxiv.org/pdf/1212.6659v1.pdf | author:Raphael Pelossof, Zhiliang Ying category:stat.ML cs.AI cs.LG published:2012-12-29 summary:We present a method to stop the evaluation of a prediction process when theresult of the full evaluation is obvious. This trait is highly desirable inprediction tasks where a predictor evaluates all its features for every examplein large datasets. We observe that some examples are easier to classify thanothers, a phenomenon which is characterized by the event when most of thefeatures agree on the class of an example. By stopping the feature evaluationwhen encountering an easy- to-classify example, the predictor can achievesubstantial gains in computation. Our method provides a natural attentionmechanism for linear predictors where the predictor concentrates most of itscomputation on hard-to-classify examples and quickly discards easy-to-classifyones. By modifying a linear prediction algorithm such as an SVM or AdaBoost toinclude our attentive method we prove that the average number of featurescomputed is O(sqrt(n log 1/sqrt(delta))) where n is the original number offeatures, and delta is the error rate incurred due to early stopping. Wedemonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim,Gisette, and synthetic datasets.
arxiv-2100-259 | Discovering Basic Emotion Sets via Semantic Clustering on a Twitter Corpus | http://arxiv.org/pdf/1212.6527v1.pdf | author:Eugene Yuta Bann category:cs.AI cs.CL published:2012-12-28 summary:A plethora of words are used to describe the spectrum of human emotions, buthow many emotions are there really, and how do they interact? Over the past fewdecades, several theories of emotion have been proposed, each based around theexistence of a set of 'basic emotions', and each supported by an extensivevariety of research including studies in facial expression, ethology, neurologyand physiology. Here we present research based on a theory that people transmittheir understanding of emotions through the language they use surroundingemotion keywords. Using a labelled corpus of over 21,000 tweets, six of thebasic emotion sets proposed in existing literature were analysed using LatentSemantic Clustering (LSC), evaluating the distinctiveness of the semanticmeaning attached to the emotional label. We hypothesise that the more distinctthe language is used to express a certain emotion, then the more distinct theperception (including proprioception) of that emotion is, and thus more'basic'. This allows us to select the dimensions best representing the entirespectrum of emotion. We find that Ekman's set, arguably the most frequentlyused for classifying emotions, is in fact the most semantically distinctoverall. Next, taking all analysed (that is, previously proposed) emotion termsinto account, we determine the optimal semantically irreducible basic emotionset using an iterative LSC algorithm. Our newly-derived set (Accepting,Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful,Sad, Scared). We also demonstrate how using LSC data can help visualiseemotions. We introduce the concept of an Emotion Profile and briefly analysecompound emotions both visually and mathematically.
arxiv-2100-260 | Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes | http://arxiv.org/pdf/1212.1824v2.pdf | author:Ohad Shamir, Tong Zhang category:cs.LG math.OC stat.ML published:2012-12-08 summary:Stochastic Gradient Descent (SGD) is one of the simplest and most popularstochastic optimization methods. While it has already been theoreticallystudied for decades, the classical analysis usually required non-trivialsmoothness assumptions, which do not apply to many modern applications of SGDwith non-smooth objective functions such as support vector machines. In thispaper, we investigate the performance of SGD without such smoothnessassumptions, as well as a running average scheme to convert the SGD iterates toa solution with optimal optimization accuracy. In this framework, we prove thatafter T rounds, the suboptimality of the last SGD iterate scales asO(log(T)/\sqrt{T}) for non-smooth convex objective functions, and O(log(T)/T)in the non-smooth strongly convex case. To the best of our knowledge, these arethe first bounds of this kind, and almost match the minimax-optimal ratesobtainable by appropriate averaging schemes. We also propose a new and simpleaveraging scheme, which not only attains optimal rates, but can also be easilycomputed on-the-fly (in contrast, the suffix averaging scheme proposed inRakhlin et al. (2011) is not as simple to implement). Finally, we provide someexperimental illustrations.
arxiv-2100-261 | Fully scalable online-preprocessing algorithm for short oligonucleotide microarray atlases | http://arxiv.org/pdf/1212.5932v2.pdf | author:Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan Rung category:q-bio.QM cs.CE cs.LG q-bio.GN stat.AP stat.ML published:2012-12-24 summary:Accumulation of standardized data collections is opening up novelopportunities for holistic characterization of genome function. The limitedscalability of current preprocessing techniques has, however, formed abottleneck for full utilization of contemporary microarray collections. Whileshort oligonucleotide arrays constitute a major source of genome-wide profilingdata, scalable probe-level preprocessing algorithms have been available onlyfor few measurement platforms based on pre-calculated model parameters fromrestricted reference training sets. To overcome these key limitations, weintroduce a fully scalable online-learning algorithm that provides tools toprocess large microarray atlases including tens of thousands of arrays. Unlikethe alternatives, the proposed algorithm scales up in linear time with respectto sample size and is readily applicable to all short oligonucleotideplatforms. This is the only available preprocessing algorithm that can learnprobe-level parameters based on sequential hyperparameter updates at small,consecutive batches of data, thus circumventing the extensive memoryrequirements of the standard approaches and opening up novel opportunities totake full advantage of contemporary microarray data collections. Moreover,using the most comprehensive data collections to estimate probe-level effectscan assist in pinpointing individual probes affected by various biases andprovide new tools to guide array design and quality control. The implementationis freely available in R/Bioconductor athttp://www.bioconductor.org/packages/devel/bioc/html/RPA.html
arxiv-2100-262 | Localized Algorithm of Community Detection on Large-Scale Decentralized Social Networks | http://arxiv.org/pdf/1212.6323v1.pdf | author:Pili Hu, Wing Cheong Lau category:cs.SI physics.soc-ph stat.ML published:2012-12-27 summary:Despite the overwhelming success of the existing Social Networking Services(SNS), their centralized ownership and control have led to serious concerns inuser privacy, censorship vulnerability and operational robustness of theseservices. To overcome these limitations, Distributed Social Networks (DSN) haverecently been proposed and implemented. Under these new DSN architectures, nosingle party possesses the full knowledge of the entire social network. Whilethis approach solves the above problems, the lack of global knowledge for theDSN nodes makes it much more challenging to support some common but criticalSNS services like friends discovery and community detection. In this paper, wetackle the problem of community detection for a given user under the constraintof limited local topology information as imposed by common DSN architectures.By considering the Personalized Page Rank (PPR) approach as an ink spillingprocess, we justify its applicability for decentralized community detectionusing limited local topology information.Our proposed PPR-based solution has awide range of applications such as friends recommendation, targetedadvertisement, automated social relationship labeling and sybil defense. Usingdata collected from a large-scale SNS in practice, we demonstrate our adaptedversion of PPR can significantly outperform the basic PR as well as two othercommonly used heuristics. The inclusion of a few manually labeled friends inthe Escape Vector (EV) can boost the performance considerably (64.97% relativeimprovement in terms of Area Under the ROC Curve (AUC)).
arxiv-2100-263 | On-line relational SOM for dissimilarity data | http://arxiv.org/pdf/1212.6316v1.pdf | author:Madalina Olteanu, Nathalie Villa-Vialaneix, Marie Cottrell category:stat.ML cs.LG published:2012-12-27 summary:In some applications and in order to address real world situations better,data may be more complex than simple vectors. In some examples, they can beknown through their pairwise dissimilarities only. Several variants of the SelfOrganizing Map algorithm were introduced to generalize the original algorithmto this framework. Whereas median SOM is based on a rough representation of theprototypes, relational SOM allows representing these prototypes by a virtualcombination of all elements in the data set. However, this latter approachsuffers from two main drawbacks. First, its complexity can be large. Second,only a batch version of this algorithm has been studied so far and it oftenprovides results having a bad topographic organization. In this article, anon-line version of relational SOM is described and justified. The algorithm istested on several datasets, including categorical data and graphs, and comparedwith the batch version and with other SOM algorithms for non vector data.
arxiv-2100-264 | A brief experience on journey through hardware developments for image processing and its applications on Cryptography | http://arxiv.org/pdf/1212.6303v1.pdf | author:Sangeet Saha, Chandrajit pal, Rourab paul, Satyabrata Maity, Suman Sau category:cs.AR cs.CR cs.CV published:2012-12-27 summary:The importance of embedded applications on image and videoprocessing,communication and cryptography domain has been taking a larger spacein current research era. Improvement of pictorial information for betterment ofhuman perception like deblurring, de-noising in several fields such assatellite imaging, medical imaging etc are renewed research thrust.Specifically we would like to elaborate our experience on the significance ofcomputer vision as one of the domains where hardware implemented algorithmsperform far better than those implemented through software. So far embeddeddesign engineers have successfully implemented their designs by means ofApplication Specific Integrated Circuits (ASICs) and/or Digital SignalProcessors (DSP), however with the advancement of VLSI technology a verypowerful hardware device namely the Field Programmable Gate Array (FPGA)combining the key advantages of ASICs and DSPs was developed which have thepossibility of reprogramming making them a very attractive device for rapidprototyping.Communication of image and video data in multiple FPGA is no longerfar away from the thrust of secured transmission among them, and then therelevance of cryptography is indeed unavoidable. This paper shows how theXilinx hardware development platform as well Mathworks Matlab can be used todevelop hardware based computer vision algorithms and its corresponding cryptotransmission channel between multiple FPGA platform from a system levelapproach, making it favourable for developing a hardware-software co-designenvironment.
arxiv-2100-265 | Echo State Queueing Network: a new reservoir computing learning tool | http://arxiv.org/pdf/1212.6276v1.pdf | author:Sebastián Basterrech, Gerardo Rubino category:cs.NE cs.AI cs.LG published:2012-12-26 summary:In the last decade, a new computational paradigm was introduced in the fieldof Machine Learning, under the name of Reservoir Computing (RC). RC models areneural networks which a recurrent part (the reservoir) that does notparticipate in the learning process, and the rest of the system where norecurrence (no neural circuit) occurs. This approach has grown rapidly due toits success in solving learning tasks and other computational applications.Some success was also observed with another recently proposed neural networkdesigned using Queueing Theory, the Random Neural Network (RandNN). Bothapproaches have good properties and identified drawbacks. In this paper, wepropose a new RC model called Echo State Queueing Network (ESQN), where we useideas coming from RandNNs for the design of the reservoir. ESQNs consist inESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. Thepaper positions ESQNs in the global Machine Learning area, and providesexamples of their use and performances. We show on largely used benchmarks thatESQNs are very accurate tools, and we illustrate how they compare with standardESNs.
arxiv-2100-266 | Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals | http://arxiv.org/pdf/1212.6246v1.pdf | author:Chunyi Wang, Radford M. Neal category:stat.ML cs.LG published:2012-12-26 summary:Gaussian Process (GP) regression models typically assume that residuals areGaussian and have the same variance for all observations. However, applicationswith input-dependent noise (heteroscedastic residuals) frequently arise inpractice, as do applications in which the residuals do not have a Gaussiandistribution. In this paper, we propose a GP Regression model with a latentvariable that serves as an additional unobserved covariate for the regression.This model (which we call GPLC) allows for heteroscedasticity since it allowsthe function to have a changing partial derivative with respect to thisunobserved covariate. With a suitable covariance function, our GPLC model canhandle (a) Gaussian residuals with input-dependent variance, or (b)non-Gaussian residuals with input-dependent variance, or (c) Gaussian residualswith constant variance. We compare our model, using synthetic datasets, with amodel proposed by Goldberg, Williams and Bishop (1998), which we refer to asGPLV, which only deals with case (a), as well as a standard GP model which canhandle only case (c). Markov Chain Monte Carlo methods are developed for bothmodelsl. Experiments show that when the data is heteroscedastic, both GPLC andGPLV give better results (smaller mean squared error and negativelog-probability density) than standard GP regression. In addition, when theresidual are Gaussian, our GPLC model is generally nearly as good as GPLV,while when the residuals are non-Gaussian, our GPLC model is better than GPLV.
arxiv-2100-267 | Efficient Multiple Object Tracking Using Mutually Repulsive Active Membranes | http://arxiv.org/pdf/1212.6209v1.pdf | author:Yi Deng, Philip Coen, Mingzhai Sun, Joshua W. Shaevitz category:q-bio.QM cs.CV physics.bio-ph published:2012-12-26 summary:Studies of social and group behavior in interacting organisms requirehigh-throughput analysis of the motion of a large number of individualsubjects. Computer vision techniques offer solutions to specific trackingproblems, and allow automated and efficient tracking with minimal humanintervention. In this work, we adopt the open active contour model to track thetrajectories of moving objects at high density. We add repulsive interactionsbetween open contours to the original model, treat the trajectories as anextrusion in the temporal dimension, and show applications to two trackingproblems. The walking behavior of Drosophila is studied at different populationdensity and gender composition. We demonstrate that individual male flies havedistinct walking signatures, and that the social interaction between flies in amixed gender arena is gender specific. We also apply our model to studies oftrajectories of gliding Myxococcus xanthus bacteria at high density. We examinethe individual gliding behavioral statistics in terms of the gliding speeddistribution. Using these two examples at very distinctive spatial scales, weillustrate the use of our algorithm on tracking both short rigid bodies(Drosophila) and long flexible objects (Myxococcus xanthus). Our repulsiveactive membrane model reaches error rates better than $5\times 10^{-6}$ per flyper second for Drosophila tracking and comparable results for Myxococcusxanthus.
arxiv-2100-268 | Transfer Learning Using Logistic Regression in Credit Scoring | http://arxiv.org/pdf/1212.6167v1.pdf | author:Farid Beninel, Waad Bouaguel, Ghazi Belmufti category:cs.LG cs.CE published:2012-12-26 summary:The credit scoring risk management is a fast growing field due to consumer'scredit requests. Credit requests, of new and existing customers, are oftenevaluated by classical discrimination rules based on customers information.However, these kinds of strategies have serious limits and don't take intoaccount the characteristics difference between current customers and the futureones. The aim of this paper is to measure credit worthiness for non customersborrowers and to model potential risk given a heterogeneous population formedby borrowers customers of the bank and others who are not. We hold on previousworks done in generalized gaussian discrimination and transpose them into thelogistic model to bring out efficient discrimination rules for non customers'subpopulation. Therefore we obtain several simple models of connection between parameters ofboth logistic models associated respectively to the two subpopulations. TheGerman credit data set is selected to experiment and to compare these models.Experimental results show that the use of links between the two subpopulationsimprove the classification accuracy for the new loan applicants.
arxiv-2100-269 | Hyperplane Arrangements and Locality-Sensitive Hashing with Lift | http://arxiv.org/pdf/1212.6110v1.pdf | author:Makiko Konoshima, Yui Noma category:cs.LG cs.IR stat.ML H.3.3; H.3.m published:2012-12-26 summary:Locality-sensitive hashing converts high-dimensional feature vectors, such asimage and speech, into bit arrays and allows high-speed similarity calculationwith the Hamming distance. There is a hashing scheme that maps feature vectorsto bit arrays depending on the signs of the inner products between featurevectors and the normal vectors of hyperplanes placed in the feature space. Thishashing can be seen as a discretization of the feature space by hyperplanes. Iflabels for data are given, one can determine the hyperplanes by using learningalgorithms. However, many proposed learning methods do not consider thehyperplanes' offsets. Not doing so decreases the number of partitioned regions,and the correlation between Hamming distances and Euclidean distances becomessmall. In this paper, we propose a lift map that converts learning algorithmswithout the offsets to the ones that take into account the offsets. With thismethod, the learning methods without the offsets give the discretizations ofspaces as if it takes into account the offsets. For the proposed method, weinput several high-dimensional feature data sets and studied the relationshipbetween the statistical characteristics of data, the number of hyperplanes, andthe effect of the proposed method.
arxiv-2100-270 | Improved Total Variation based Image Compressive Sensing Recovery by Nonlocal Regularization | http://arxiv.org/pdf/1208.3716v2.pdf | author:Jian Zhang, Shaohui Liu, Debin Zhao, Ruiqin Xiong, Siwei Ma category:cs.CV published:2012-08-18 summary:Recently, total variation (TV) based minimization algorithms have achievedgreat success in compressive sensing (CS) recovery for natural images due toits virtue of preserving edges. However, the use of TV is not able to recoverthe fine details and textures, and often suffers from undesirable staircaseartifact. To reduce these effects, this letter presents an improved TV basedimage CS recovery algorithm by introducing a new nonlocal regularizationconstraint into CS optimization problem. The nonlocal regularization is builton the well known nonlocal means (NLM) filtering and takes advantage ofself-similarity in images, which helps to suppress the staircase effect andrestore the fine details. Furthermore, an efficient augmented Lagrangian basedalgorithm is developed to solve the above combined TV and nonlocalregularization constrained problem. Experimental results demonstrate that theproposed algorithm achieves significant performance improvements over thestate-of-the-art TV based algorithm in both PSNR and visual perception.
arxiv-2100-271 | Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval | http://arxiv.org/pdf/1212.6094v1.pdf | author:Chang Huang, Shenghuo Zhu, Kai Yu category:cs.CV published:2012-12-25 summary:Learning Mahanalobis distance metrics in a high- dimensional feature space isvery difficult especially when structural sparsity and low rank are enforced toimprove com- putational efficiency in testing phase. This paper addresses bothaspects by an ensemble metric learning approach that consists of sparse blockdiagonal metric ensembling and join- t metric learning as two consecutivesteps. The former step pursues a highly sparse block diagonal metric byselecting effective feature groups while the latter one further exploitscorrelations between selected feature groups to obtain an accurate and low rankmetric. Our algorithm considers all pairwise or triplet constraints generatedfrom training samples with explicit class labels, and possesses good scala-bility with respect to increasing feature dimensionality and growing datavolumes. Its applications to face verification and retrieval outperformexisting state-of-the-art methods in accuracy while retaining high efficiency.
arxiv-2100-272 | High Quality Image Interpolation via Local Autoregressive and Nonlocal 3-D Sparse Regularization | http://arxiv.org/pdf/1212.6058v1.pdf | author:Xinwei Gao, Jian Zhang, Feng Jiang, Xiaopeng Fan, Siwei Ma, Debin Zhao category:cs.MM cs.CV published:2012-12-25 summary:In this paper, we propose a novel image interpolation algorithm, which isformulated via combining both the local autoregressive (AR) model and thenonlocal adaptive 3-D sparse model as regularized constraints under theregularization framework. Estimating the high-resolution image by the local ARregularization is different from these conventional AR models, which weightedcalculates the interpolation coefficients without considering the roughstructural similarity between the low-resolution (LR) and high-resolution (HR)images. Then the nonlocal adaptive 3-D sparse model is formulated to regularizethe interpolated HR image, which provides a way to modify these pixels with theproblem of numerical stability caused by AR model. In addition, a newSplit-Bregman based iterative algorithm is developed to solve the aboveoptimization problem iteratively. Experiment results demonstrate that theproposed algorithm achieves significant performance improvements over thetraditional algorithms in terms of both objective quality and visual perception
arxiv-2100-273 | Tangent Bundle Manifold Learning via Grassmann&Stiefel Eigenmaps | http://arxiv.org/pdf/1212.6031v1.pdf | author:Alexander V. Bernstein, Alexander P. Kuleshov category:cs.LG 68T05 published:2012-12-25 summary:One of the ultimate goals of Manifold Learning (ML) is to reconstruct anunknown nonlinear low-dimensional manifold embedded in a high-dimensionalobservation space by a given set of data points from the manifold. We derive alocal lower bound for the maximum reconstruction error in a small neighborhoodof an arbitrary point. The lower bound is defined in terms of the distancebetween tangent spaces to the original manifold and the estimated manifold atthe considered point and reconstructed point, respectively. We propose anamplification of the ML, called Tangent Bundle ML, in which the proximity notonly between the original manifold and its estimator but also between theirtangent spaces is required. We present a new algorithm that solves this problemand gives a new solution for the ML also.
arxiv-2100-274 | Exponentially Weighted Moving Average Charts for Detecting Concept Drift | http://arxiv.org/pdf/1212.6018v1.pdf | author:Gordon J. Ross, Niall M. Adams, Dimitris K. Tasoulis, David J. Hand category:stat.ML cs.LG stat.AP published:2012-12-25 summary:Classifying streaming data requires the development of methods which arecomputationally efficient and able to cope with changes in the underlyingdistribution of the stream, a phenomenon known in the literature as conceptdrift. We propose a new method for detecting concept drift which uses anExponentially Weighted Moving Average (EWMA) chart to monitor themisclassification rate of an streaming classifier. Our approach is modular andcan hence be run in parallel with any underlying classifier to provide anadditional layer of concept drift detection. Moreover our method iscomputationally efficient with overhead O(1) and works in a fully online mannerwith no need to store data points in memory. Unlike many existing approaches toconcept drift detection, our method allows the rate of false positivedetections to be controlled and kept constant over time.
arxiv-2100-275 | Reconstructing Self Organizing Maps as Spider Graphs for better visual interpretation of large unstructured datasets | http://arxiv.org/pdf/1301.0289v1.pdf | author:Aaditya Prakash category:cs.GR stat.ML published:2012-12-24 summary:Self-Organizing Maps (SOM) are popular unsupervised artificial neural networkused to reduce dimensions and visualize data. Visual interpretation fromSelf-Organizing Maps (SOM) has been limited due to grid approach of datarepresentation, which makes inter-scenario analysis impossible. The paperproposes a new way to structure SOM. This model reconstructs SOM to showstrength between variables as the threads of a cobweb and illuminateinter-scenario analysis. While Radar Graphs are very crude representation ofspider web, this model uses more lively and realistic cobweb representation totake into account the difference in strength and length of threads. This modelallows for visualization of highly unstructured dataset with large number ofdimensions, common in Bigdata sources.
arxiv-2100-276 | Distributed optimization of deeply nested systems | http://arxiv.org/pdf/1212.5921v1.pdf | author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG cs.NE math.OC stat.ML published:2012-12-24 summary:In science and engineering, intelligent processing of complex signals such asimages, sound or language is often performed by a parameterized hierarchy ofnonlinear processing layers, sometimes biologically inspired. Hierarchicalsystems (or, more generally, nested systems) offer a way to generate complexmappings using simple stages. Each layer performs a different operation andachieves an ever more sophisticated representation of the input, as, forexample, in an deep artificial neural network, an object recognition cascade incomputer vision or a speech front-end processing. Joint estimation of theparameters of all the layers and selection of an optimal architecture is widelyconsidered to be a difficult numerical nonconvex optimization problem,difficult to parallelize for execution in a distributed computationenvironment, and requiring significant human expert effort, which leads tosuboptimal systems in practice. We describe a general mathematical strategy tolearn the parameters and, to some extent, the architecture of nested systems,called the method of auxiliary coordinates (MAC). This replaces the originalproblem involving a deeply nested function with a constrained problem involvinga different function in an augmented space without nesting. The constrainedproblem may be solved with penalty-based methods using alternating optimizationover the parameters and the auxiliary coordinates. MAC has provableconvergence, is easy to implement reusing existing algorithms for singlelayers, can be parallelized trivially and massively, applies even whenparameter derivatives are not available or not desirable, and is competitivewith state-of-the-art nonlinear optimizers even in the serial computationsetting, often providing reasonable models within a few iterations.
arxiv-2100-277 | A short note on the tail bound of Wishart distribution | http://arxiv.org/pdf/1212.5860v1.pdf | author:Shenghuo Zhu category:math.ST cs.LG stat.TH published:2012-12-24 summary:We study the tail bound of the emperical covariance of multivariate normaldistribution. Following the work of (Gittens & Tropp, 2011), we provide a tailbound with a small constant.
arxiv-2100-278 | Collaborating Robotics Using Nature-Inspired Meta-Heuristics | http://arxiv.org/pdf/1212.5777v1.pdf | author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.NE cs.RO published:2012-12-23 summary:This paper introduces collaborating robots which provide the possibility ofenhanced task performance, high reliability and decreased. Collaborating-botsare a collection of mobile robots able to self-assemble and to self-organize inorder to solve problems that cannot be solved by a single robot. These robotscombine the power of swarm intelligence with the flexibility ofself-reconfiguration as aggregate Collaborating-bots can dynamically changetheir structure to match environmental variations. Collaborating robots aremore than just networks of independent agents, they are potentiallyreconfigurable networks of communicating agents capable of coordinated sensingand interaction with the environment. Robots are going to be an important partof the future. Collaborating robots are limited in individual capability, butrobots deployed in large numbers can represent a strong force similar to acolony of ants or swarm of bees. We present a mechanism for collaboratingrobots based on swarm intelligence such as Ant colony optimization and Particleswarm Optimization
arxiv-2100-279 | ADADELTA: An Adaptive Learning Rate Method | http://arxiv.org/pdf/1212.5701v1.pdf | author:Matthew D. Zeiler category:cs.LG published:2012-12-22 summary:We present a novel per-dimension learning rate method for gradient descentcalled ADADELTA. The method dynamically adapts over time using only first orderinformation and has minimal computational overhead beyond vanilla stochasticgradient descent. The method requires no manual tuning of a learning rate andappears robust to noisy gradient information, different model architecturechoices, various data modalities and selection of hyperparameters. We showpromising results compared to other methods on the MNIST digit classificationtask using a single machine and on a large scale voice dataset in a distributedcluster environment.
arxiv-2100-280 | Revisiting the Training of Logic Models of Protein Signaling Networks with a Formal Approach based on Answer Set Programming | http://arxiv.org/pdf/1210.0690v2.pdf | author:Santiago Videla, Carito Guziolowski, Federica Eduati, Sven Thiele, Niels Grabe, Julio Saez-Rodriguez, Anne Siegel category:q-bio.QM cs.AI cs.CE cs.LG published:2012-10-02 summary:A fundamental question in systems biology is the construction and training todata of mathematical models. Logic formalisms have become very popular to modelsignaling networks because their simplicity allows us to model large systemsencompassing hundreds of proteins. An approach to train (Boolean) logic modelsto high-throughput phospho-proteomics data was recently introduced and solvedusing optimization heuristics based on stochastic methods. Here we demonstratehow this problem can be solved using Answer Set Programming (ASP), adeclarative problem solving paradigm, in which a problem is encoded as alogical program such that its answer sets represent solutions to the problem.ASP has significant improvements over heuristic methods in terms of efficiencyand scalability, it guarantees global optimality of solutions as well asprovides a complete set of solutions. We illustrate the application of ASP within silico cases based on realistic networks and data.
arxiv-2100-281 | High-precision camera distortion measurements with a "calibration harp" | http://arxiv.org/pdf/1212.5656v1.pdf | author:Zhongwei Tang, Rafael Grompone von Gioi, Pascal Monasse, Jean-Michel Morel category:cs.CV published:2012-12-22 summary:This paper addresses the high precision measurement of the distortion of adigital camera from photographs. Traditionally, this distortion is measuredfrom photographs of a flat pattern which contains aligned elements.Nevertheless, it is nearly impossible to fabricate a very flat pattern and tovalidate its flatness. This fact limits the attainable measurable precisions.In contrast, it is much easier to obtain physically very precise straight linesby tightly stretching good quality strings on a frame. Taking literally"plumb-line methods", we built a "calibration harp" instead of the classic flatpatterns to obtain a high precision measurement tool, demonstrably reaching2/100 pixel precisions. The harp is complemented with the algorithms computingautomatically from harp photographs two different and complementary lensdistortion measurements. The precision of the method is evaluated on imagescorrected by state-of-the-art distortion correction algorithms, and by popularsoftware. Three applications are shown: first an objective and reliablemeasurement of the result of any distortion correction. Second, the harppermits to control state-of-the art global camera calibration algorithms: Itpermits to select the right distortion model, thus avoiding internalcompensation errors inherent to these methods. Third, the method replacesmanual procedures in other distortion correction methods, makes them fullyautomatic, and increases their reliability and precision.
arxiv-2100-282 | Random Spanning Trees and the Prediction of Weighted Graphs | http://arxiv.org/pdf/1212.5637v1.pdf | author:Nicolo' Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG stat.ML published:2012-12-21 summary:We investigate the problem of sequentially predicting the binary labels onthe nodes of an arbitrary weighted graph. We show that, under a suitableparametrization of the problem, the optimal number of prediction mistakes canbe characterized (up to logarithmic factors) by the cutsize of a randomspanning tree of the graph. The cutsize is induced by the unknown adversariallabeling of the graph nodes. In deriving our characterization, we obtain asimple randomized algorithm achieving in expectation the optimal mistake boundon any polynomially connected weighted graph. Our algorithm draws a randomspanning tree of the original graph and then predicts the nodes of this tree inconstant expected amortized time and linear space. Experiments on real-worlddatasets show that our method compares well to both global (Perceptron) andlocal (label propagation) methods, while being generally faster in practice.
arxiv-2100-283 | Mixtures of Shifted Asymmetric Laplace Distributions | http://arxiv.org/pdf/1207.1727v3.pdf | author:Brian C. Franczak, Ryan P. Browne, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2012-07-06 summary:A mixture of shifted asymmetric Laplace distributions is introduced and usedfor clustering and classification. A variant of the EM algorithm is developedfor parameter estimation by exploiting the relationship with the generalinverse Gaussian distribution. This approach is mathematically elegant andrelatively computationally straightforward. Our novel mixture modellingapproach is demonstrated on both simulated and real data to illustrateclustering and classification applications. In these analyses, our mixture ofshifted asymmetric Laplace distributions performs favourably when compared tothe popular Gaussian approach. This work, which marks an important step in thenon-Gaussian model-based clustering and classification direction, concludeswith discussion as well as suggestions for future work.
arxiv-2100-284 | A Tutorial on Probabilistic Latent Semantic Analysis | http://arxiv.org/pdf/1212.3900v2.pdf | author:Liangjie Hong category:stat.ML cs.LG published:2012-12-17 summary:In this tutorial, I will discuss the details about how Probabilistic LatentSemantic Analysis (PLSA) is formalized and how different learning algorithmsare proposed to learn the model.
arxiv-2100-285 | In Vivo Quantification of Clot Formation in Extracorporeal Circuits | http://arxiv.org/pdf/1212.5454v1.pdf | author:Omid David, Rabin Gerrah category:cs.CV physics.med-ph published:2012-12-21 summary:Clot formation is a common complication in extracorporeal circuits. In thispaper we describe a novel method for clot formation analysis using imageprocessing. We assembled a closed extracorporeal circuit and circulated bloodat varying speeds. Blood filters were placed in downstream of the flow, andclotting agents were added to the circuit. Digital images of the filter weresubsequently taken, and image analysis was applied to calculate the density ofthe clot. Our results show a significant correlation between the cumulativesize of the clots, the density measure of the clot based on image analysis, andflow duration in the system.
arxiv-2100-286 | Black box modelling of HVAC system : improving the performances of neural networks | http://arxiv.org/pdf/1212.5594v1.pdf | author:Eric Fock, Thierry Alex Mara, Alfred Jean Philippe Lauret, Harry Boyer category:cs.NE cs.CE published:2012-12-21 summary:This paper deals with neural networks modelling of HVAC systems. In order toincrease the neural networks performances, a method based on sensitivityanalysis is applied. The same technique is also used to compute the relevanceof each input. To avoid the prediction errors in dry coil conditions, ametamodel for each capacity is derived from the neural networks. The regressioncoefficients of the polynomial forms are identified through the use of spectralanalysis. These methods based on sensitivity and spectral analysis lead to anoptimized neural network model, as regard to its architecture and predictions.
arxiv-2100-287 | Soft Set Based Feature Selection Approach for Lung Cancer Images | http://arxiv.org/pdf/1212.5391v1.pdf | author:G. Jothi, H. Hannah Inbarani category:cs.LG cs.CE published:2012-12-21 summary:Lung cancer is the deadliest type of cancer for both men and women. Featureselection plays a vital role in cancer classification. This paper investigatesthe feature selection process in Computed Tomographic (CT) lung cancer imagesusing soft set theory. We propose a new soft set based unsupervised featureselection algorithm. Nineteen features are extracted from the segmented lungimages using gray level co-occurence matrix (GLCM) and gray level differentmatrix (GLDM). In this paper, an efficient Unsupervised Soft Set based QuickReduct (SSUSQR) algorithm is presented. This method is used to select featuresfrom the data set and compared with existing rough set based unsupervisedfeature selection methods. Then K-Means and Self Organizing Map (SOM)clustering algorithms are used to cluster the data. The performance of thefeature selection algorithms is evaluated based on performance of clusteringtechniques. The results show that the proposed method effectively removesredundant features.
arxiv-2100-288 | Fuzzy soft rough K-Means clustering approach for gene expression data | http://arxiv.org/pdf/1212.5359v1.pdf | author:K. Dhanalakshmi, H. Hannah Inbarani category:cs.LG cs.CE published:2012-12-21 summary:Clustering is one of the widely used data mining techniques for medicaldiagnosis. Clustering can be considered as the most important unsupervisedlearning technique. Most of the clustering methods group data based on distanceand few methods cluster data based on similarity. The clustering algorithmsclassify gene expression data into clusters and the functionally related genesare grouped together in an efficient manner. The groupings are constructed suchthat the degree of relationship is strong among members of the same cluster andweak among members of different clusters. In this work, we focus on asimilarity relationship among genes with similar expression patterns so that aconsequential and simple analytical decision can be made from the proposedFuzzy Soft Rough K-Means algorithm. The algorithm is developed based on FuzzySoft sets and Rough sets. Comparative analysis of the proposed work is madewith bench mark algorithms like K-Means and Rough K-Means and efficiency of theproposed algorithm is illustrated in this work by using various clustervalidity measures such as DB index and Xie-Beni index.
arxiv-2100-289 | On the Adaptability of Neural Network Image Super-Resolution | http://arxiv.org/pdf/1212.5352v1.pdf | author:Kah Keong Chua, Yong Haur Tay category:cs.CV published:2012-12-21 summary:In this paper, we described and developed a framework for MultilayerPerceptron (MLP) to work on low level image processing, where MLP will be usedto perform image super-resolution. Meanwhile, MLP are trained with differenttypes of images from various categories, hence analyse the behaviour andperformance of the neural network. The tests are carried out using qualitativetest, in which Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) andStructural Similarity Index (SSIM). The results showed that MLP trained withsingle image category can perform reasonably well compared to methods proposedby other researchers.
arxiv-2100-290 | ET-LDA: Joint Topic Modeling For Aligning, Analyzing and Sensemaking of Public Events and Their Twitter Feeds | http://arxiv.org/pdf/1210.2164v3.pdf | author:Yuheng Hu, Ajita John, Fei Wang, Doree Duncan Seligmann, Subbarao Kambhampati category:cs.LG cs.AI cs.SI physics.soc-ph published:2012-10-08 summary:Social media channels such as Twitter have emerged as popular platforms forcrowds to respond to public events such as speeches, sports and debates. Whilethis promises tremendous opportunities to understand and make sense of thereception of an event from the social media, the promises come entwined withsignificant technical challenges. In particular, given an event and anassociated large scale collection of tweets, we need approaches to effectivelyalign tweets and the parts of the event they refer to. This in turn raisesquestions about how to segment the event into smaller yet meaningful parts, andhow to figure out whether a tweet is a general one about the entire event orspecific one aimed at a particular segment of the event. In this work, wepresent ET-LDA, an effective method for aligning an event and its tweetsthrough joint statistical modeling of topical influences from the events andtheir associated tweets. The model enables the automatic segmentation of theevents and the characterization of tweets into two categories: (1) episodictweets that respond specifically to the content in the segments of the events,and (2) steady tweets that respond generally about the events. We present anefficient inference method for this model, and a comprehensive evaluation ofits effectiveness over existing methods. In particular, through a user study,we demonstrate that users find the topics, the segments, the alignment, and theepisodic tweets discovered by ET-LDA to be of higher quality and moreinteresting as compared to the state-of-the-art, with improvements in the rangeof 18-41%.
arxiv-2100-291 | Towards the Evolution of Novel Vertical-Axis Wind Turbines | http://arxiv.org/pdf/1212.5271v1.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI published:2012-12-20 summary:Renewable and sustainable energy is one of the most important challengescurrently facing mankind. Wind has made an increasing contribution to theworld's energy supply mix, but still remains a long way from reaching its fullpotential. In this paper, we investigate the use of artificial evolution todesign vertical-axis wind turbine prototypes that are physically instantiatedand evaluated under approximated wind tunnel conditions. An artificial neuralnetwork is used as a surrogate model to assist learning and found to reduce thenumber of fabrications required to reach a higher aerodynamic efficiency,resulting in an important cost reduction. Unlike in other approaches, such ascomputational fluid dynamics simulations, no mathematical formulations are usedand no model assumptions are made.
arxiv-2100-292 | A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method | http://arxiv.org/pdf/1212.2002v2.pdf | author:Simon Lacoste-Julien, Mark Schmidt, Francis Bach category:cs.LG math.OC stat.ML G.1.6; I.2.6 published:2012-12-10 summary:In this note, we present a new averaging technique for the projectedstochastic subgradient method. By using a weighted average with a weight of t+1for each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)with both an easy proof and an easy implementation. The new scheme is comparedempirically to existing techniques, with similar performance behavior.
arxiv-2100-293 | The Twitter of Babel: Mapping World Languages through Microblogging Platforms | http://arxiv.org/pdf/1212.5238v1.pdf | author:Delia Mocanu, Andrea Baronchelli, Bruno Gonçalves, Nicola Perra, Alessandro Vespignani category:physics.soc-ph cs.CL cs.SI published:2012-12-20 summary:Large scale analysis and statistics of socio-technical systems that just afew short years ago would have required the use of consistent economic andhuman resources can nowadays be conveniently performed by mining the enormousamount of digital data produced by human activities. Although acharacterization of several aspects of our societies is emerging from the datarevolution, a number of questions concerning the reliability and the biasesinherent to the big data "proxies" of social life are still open. Here, wesurvey worldwide linguistic indicators and trends through the analysis of alarge-scale dataset of microblogging posts. We show that available data allowfor the study of language geography at scales ranging from country-levelaggregation to specific city neighborhoods. The high resolution and coverage ofthe data allows us to investigate different indicators such as the linguistichomogeneity of different countries, the touristic seasonal patterns withincountries and the geographical distribution of different languages inmultilingual regions. This work highlights the potential of geolocalizedstudies of open data sources to improve current analysis and develop indicatorsfor major social phenomena in specific communities.
arxiv-2100-294 | A Neural Network Approach to ECG Denoising | http://arxiv.org/pdf/1212.5217v1.pdf | author:Rui Rodrigues, Paula Couto category:cs.CE cs.NE published:2012-12-20 summary:We propose an ECG denoising method based on a feed forward neural networkwith three hidden layers. Particulary useful for very noisy signals, thisapproach uses the available ECG channels to reconstruct a noisy channel. Wetested the method, on all the records from Physionet MIT-BIH ArrhythmiaDatabase, adding electrode motion artifact noise. This denoising methodimproved the perfomance of publicly available ECG analysis programs on noisyECG signals. This is an offline method that can be used to remove noise fromvery corrupted Holter records.
arxiv-2100-295 | An Experiment with Hierarchical Bayesian Record Linkage | http://arxiv.org/pdf/1212.5203v1.pdf | author:Michael D. Larsen category:math.ST stat.AP stat.CO stat.ME stat.ML stat.TH G.3.0 published:2012-12-20 summary:In record linkage (RL), or exact file matching, the goal is to identify thelinks between entities with information on two or more files. RL is animportant activity in areas including counting the population, enhancing surveyframes and data, and conducting epidemiological and follow-up studies. RL ischallenging when files are very large, no accurate personal identification (ID)number is present on all files for all units, and some information is recordedwith error. Without an unique ID number one must rely on comparisons of names,addresses, dates, and other information to find the links. Latent class modelscan be used to automatically score the value of information for determiningmatch status. Data for fitting models come from comparisons made within groupsof units that pass initial file blocking requirements. Data distributions canvary across blocks. This article examines the use of prior information andhierarchical latent class models in the context of RL.
arxiv-2100-296 | Variational Optimization | http://arxiv.org/pdf/1212.4507v2.pdf | author:Joe Staines, David Barber category:stat.ML cs.LG cs.NA 65K10 G.1.6 published:2012-12-18 summary:We discuss a general technique that can be used to form a differentiablebound on the optima of non-differentiable or discrete objective functions. Weform a unified description of these methods and consider under whichcircumstances the bound is concave. In particular we consider two concreteapplications of the method, namely sparse learning and support vectorclassification.
arxiv-2100-297 | Hybrid Fuzzy-ART based K-Means Clustering Methodology to Cellular Manufacturing Using Operational Time | http://arxiv.org/pdf/1212.5101v1.pdf | author:Sourav Sengupta, Tamal Ghosh, Pranab K Dan, Manojit Chattopadhyay category:cs.LG published:2012-12-20 summary:This paper presents a new hybrid Fuzzy-ART based K-Means Clustering techniqueto solve the part machine grouping problem in cellular manufacturing systemsconsidering operational time. The performance of the proposed technique istested with problems from open literature and the results are compared to theexisting clustering models such as simple K-means algorithm and modified ART1algorithm using an efficient modified performance measure known as modifiedgrouping efficiency (MGE) as found in the literature. The results support thebetter performance of the proposed algorithm. The Novelty of this study lies inthe simple and efficient methodology to produce quick solutions for shop floormanagers with least computational efforts and time.
arxiv-2100-298 | Automatic landmark annotation and dense correspondence registration for 3D human facial images | http://arxiv.org/pdf/1212.4920v1.pdf | author:Jianya Guo, Xi Mei, Kun Tang category:cs.CV q-bio.QM published:2012-12-20 summary:Dense surface registration of three-dimensional (3D) human facial imagesholds great potential for studies of human trait diversity, disease genetics,and forensics. Non-rigid registration is particularly useful for establishingdense anatomical correspondences between faces. Here we describe a novelnon-rigid registration method for fully automatic 3D facial image mapping. Thismethod comprises two steps: first, seventeen facial landmarks are automaticallyannotated, mainly via PCA-based feature recognition following 3D-to-2D datatransformation. Second, an efficient thin-plate spline (TPS) protocol is usedto establish the dense anatomical correspondence between facial images, underthe guidance of the predefined landmarks. We demonstrate that this method isrobust and highly accurate, even for different ethnicities. The average face iscalculated for individuals of Han Chinese and Uyghur origins. While fullyautomatic and computationally efficient, this method enables high-throughputanalysis of human facial feature variation.
arxiv-2100-299 | SMML estimators for 1-dimensional continuous data | http://arxiv.org/pdf/1212.4906v1.pdf | author:James G. Dowty category:cs.IT math.IT math.ST stat.ML stat.TH published:2012-12-20 summary:A method is given for calculating the strict minimum message length (SMML)estimator for 1-dimensional exponential families with continuous sufficientstatistics. A set of $n$ equations are found that the $n$ cut-points of theSMML estimator must satisfy. These equations can be solved using Newton'smethod and this approach is used to produce new results and to replicateresults that C. S. Wallace obtained using his boundary rules for the SMMLestimator. A rigorous proof is also given that, despite being composed of stepfunctions, the posterior probability corresponding to the SMML estimator is acontinuous function of the data.
arxiv-2100-300 | Automatic post-picking using MAPPOS improves particle image detection from Cryo-EM micrographs | http://arxiv.org/pdf/1212.4871v1.pdf | author:Ramin Norousi, Stephan Wickles, Christoph Leidig, Thomas Becker, Volker J. Schmid, Roland Beckmann, Achim Tresch category:stat.ML cs.CV published:2012-12-19 summary:Cryo-electron microscopy (cryo-EM) studies using single particlereconstruction are extensively used to reveal structural information onmacromolecular complexes. Aiming at the highest achievable resolution, state ofthe art electron microscopes automatically acquire thousands of high-qualitymicrographs. Particles are detected on and boxed out from each micrograph usingfully- or semi-automated approaches. However, the obtained particles stillrequire laborious manual post-picking classification, which is one majorbottleneck for single particle analysis of large datasets. We introduce MAPPOS,a supervised post-picking strategy for the classification of boxed particleimages, as additional strategy adding to the already efficient automatedparticle picking routines. MAPPOS employs machine learning techniques to traina robust classifier from a small number of characteristic image features. Inorder to accurately quantify the performance of MAPPOS we used simulatedparticle and non-particle images. In addition, we verified our method byapplying it to an experimental cryo-EM dataset and comparing the results to themanual classification of the same dataset. Comparisons between MAPPOS andmanual post-picking classification by several human experts demonstrated thatmerely a few hundred sample images are sufficient for MAPPOS to classify anentire dataset with a human-like performance. MAPPOS was shown to greatlyaccelerate the throughput of large datasets by reducing the manual workload byorders of magnitude while maintaining a reliable identification of non-particleimages.
