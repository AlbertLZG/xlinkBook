arxiv-10800-1 | On the Prior Sensitivity of Thompson Sampling | http://arxiv.org/pdf/1506.03378v1.pdf | author:Che-Yu Liu, Lihong Li category:cs.LG cs.AI stat.ML published:2015-06-10 summary:The empirically successful Thompson Sampling algorithm for stochastic banditshas drawn much interest in understanding its theoretical properties. Oneimportant benefit of the algorithm is that it allows domain knowledge to beconveniently encoded as a prior distribution to balance exploration andexploitation more effectively. While it is generally believed that thealgorithm's regret is low (high) when the prior is good (bad), little is knownabout the exact dependence. In this paper, we fully characterize thealgorithm's worst-case dependence of regret on the choice of prior, focusing ona special yet representative case. These results also provide insights into thegeneral sensitivity of the algorithm to the choice of priors. In particular,with $p$ being the prior probability mass of the true reward-generating model,we prove $O(\sqrt{T/p})$ and $O(\sqrt{(1-p)T})$ regret upper bounds for thebad- and good-prior cases, respectively, as well as \emph{matching} lowerbounds. Our proofs rely on the discovery of a fundamental property of ThompsonSampling and make heavy use of martingale theory, both of which appear novel inthe literature, to the best of our knowledge.
arxiv-10800-2 | Contextual Bandits with Global Constraints and Objective | http://arxiv.org/pdf/1506.03374v1.pdf | author:Shipra Agrawal, Nikhil R. Devanur, Lihong Li category:cs.LG cs.AI stat.ML published:2015-06-10 summary:We consider the contextual version of a multi-armed bandit problem withglobal convex constraints and concave objective function. In each round, theoutcome of pulling an arm is a context-dependent vector, and the globalconstraints require the average of these vectors to lie in a certain convexset. The objective is a concave function of this average vector. The learningagent competes with an arbitrary set of context-dependent policies. Thisproblem is a common generalization of problems considered by Badanidiyuru etal. (2014) and Agrawal and Devanur (2014), with important applications. We givecomputationally efficient algorithms with near-optimal regret, generalizing theapproach of Agarwal et al. (2014) for the non-constrained version of theproblem. For the special case of budget constraints our regret bounds matchthose of Badanidiyuru et al. (2014), answering their main open question ofobtaining a computationally efficient algorithm.
arxiv-10800-3 | Genetic Algorithms for multimodal optimization: a review | http://arxiv.org/pdf/1508.05342v1.pdf | author:Noe Casas category:cs.NE published:2015-06-10 summary:In this article we provide a comprehensive review of the differentevolutionary algorithm techniques used to address multimodal optimizationproblems, classifying them according to the nature of their approach. On theone hand there are algorithms that address the issue of the early convergenceto a local optimum by differentiating the individuals of the population intogroups and limiting their interaction, hence having each group evolve with ahigh degree of independence. On the other hand other approaches are based ondirectly addressing the lack of genetic diversity of the population byintroducing elements into the evolutionary dynamics that promote new niches ofthe genotypical space to be explored. Finally, we study multi-objectiveoptimization genetic algorithms, that handle the situations where multiplecriteria have to be satisfied with no penalty for any of them. Very richliterature has arised over the years on these topics, and we aim at offering anoverview of the most important techniques of each branch of the field.
arxiv-10800-4 | Amoeba Techniques for Shape and Texture Analysis | http://arxiv.org/pdf/1411.3285v2.pdf | author:Martin Welk category:cs.CV published:2014-11-12 summary:Morphological amoebas are image-adaptive structuring elements formorphological and other local image filters introduced by Lerallut et al. Theirconstruction is based on combining spatial distance with contrast informationinto an image-dependent metric. Amoeba filters show interesting parallels toimage filtering methods based on partial differential equations (PDEs), whichcan be confirmed by asymptotic equivalence results. In computing amoebas, graphstructures are generated that hold information about local image texture. Thispaper reviews and summarises the work of the author and his coauthors onmorphological amoebas, particularly their relations to PDE filters and textureanalysis. It presents some extensions and points out directions for futureinvestigation on the subject.
arxiv-10800-5 | Optical Flow on Evolving Sphere-Like Surfaces | http://arxiv.org/pdf/1506.03358v1.pdf | author:Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2015-06-10 summary:In this work we consider optical flow on evolving Riemannian 2-manifoldswhich can be parametrised from the 2-sphere. Our main motivation is to estimatecell motion in time-lapse volumetric microscopy images depicting fluorescentlylabelled cells of a live zebrafish embryo. We exploit the fact that therecorded cells float on the surface of the embryo and allow for the extractionof an image sequence together with a sphere-like surface. We solve theresulting variational problem by means of a Galerkin method based on vectorspherical harmonics and present numerical results computed from theaforementioned microscopy data.
arxiv-10800-6 | Detecting Clusters of Anomalies on Low-Dimensional Feature Subsets with Application to Network Traffic Flow Data | http://arxiv.org/pdf/1511.01047v1.pdf | author:Zhicong Qiu, David J. Miller, George Kesidis category:cs.NI cs.CR cs.LG published:2015-06-10 summary:In a variety of applications, one desires to detect groups of anomalous datasamples, with a group potentially manifesting its atypicality (relative to areference model) on a low-dimensional subset of the full measured set offeatures. Samples may only be weakly atypical individually, whereas they may bestrongly atypical when considered jointly. What makes this group anomalydetection problem quite challenging is that it is a priori unknown which subsetof features jointly manifests a particular group of anomalies. Moreover, it isunknown how many anomalous groups are present in a given data batch. In thiswork, we develop a group anomaly detection (GAD) scheme to identify the subsetof samples and subset of features that jointly specify an anomalous cluster. Weapply our approach to network intrusion detection to detect BotNet andpeer-to-peer flow clusters. Unlike previous studies, our approach captures andexploits statistical dependencies that may exist between the measured features.Experiments on real world network traffic data demonstrate the advantage of ourproposed system, and highlight the importance of exploiting feature dependencystructure, compared to the feature (or test) independence assumption made inprevious studies.
arxiv-10800-7 | Boltzmann-machine learning of prior distributions of binarized natural images | http://arxiv.org/pdf/1412.7012v2.pdf | author:Tomoyuki Obuchi, Hirokazu Koma, Muneki Yasuda category:stat.ML cs.CV published:2014-12-16 summary:Prior distributions of binarized natural images are learned by usingBoltzmann machine. We find that there emerges a structure with two sublatticesin the interactions, and the nearest-neighbor and next-nearest-neighborinteractions correspondingly take two discriminative values, which reflectsindividual characteristics of three sets of pictures we treat. On the otherhand, in a longer spacial scale, a longer-range (though still rapidly-decaying)ferromagnetic interaction commonly appear in all the cases. The characteristiclength scale of the interactions is universally about up to four latticespacing $\xi \approx 4$. These results are derived by using the mean-fieldmethod which effectively reduces the computational time required in Boltzmannmachine. An improved mean-field method called the Bethe approximation alsogives the same result, which reinforces the validity of our analysis andfindings. Relations to criticality, frustration, and simple-cell receptivefields are also discussed.
arxiv-10800-8 | Wide baseline stereo matching with convex bounded-distortion constraints | http://arxiv.org/pdf/1506.03301v1.pdf | author:Meirav Galun, Tal Amir, Tal Hassner, Ronen Basri, Yaron Lipman category:cs.CV published:2015-06-10 summary:Finding correspondences in wide baseline setups is a challenging problem.Existing approaches have focused largely on developing better featuredescriptors for correspondence and on accurate recovery of epipolar lineconstraints. This paper focuses on the challenging problem of findingcorrespondences once approximate epipolar constraints are given. We introduce anovel method that integrates a deformation model. Specifically, we formulatethe problem as finding the largest number of corresponding points related by abounded distortion map that obeys the given epipolar constraints. We show that,while the set of bounded distortion maps is not convex, the subset of maps thatobey the epipolar line constraints is convex, allowing us to introduce anefficient algorithm for matching. We further utilize a robust cost function formatching and employ majorization-minimization for its optimization. Ourexperiments indicate that our method finds significantly more accurate mapsthan existing approaches.
arxiv-10800-9 | First-order regret bounds for combinatorial semi-bandits | http://arxiv.org/pdf/1502.06354v2.pdf | author:Gergely Neu category:cs.LG stat.ML published:2015-02-23 summary:We consider the problem of online combinatorial optimization undersemi-bandit feedback, where a learner has to repeatedly pick actions from acombinatorial decision set in order to minimize the total losses associatedwith its decisions. After making each decision, the learner observes the lossesassociated with its action, but not other losses. For this problem, there areseveral learning algorithms that guarantee that the learner's expected regretgrows as $\widetilde{O}(\sqrt{T})$ with the number of rounds $T$. In thispaper, we propose an algorithm that improves this scaling to$\widetilde{O}(\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the bestaction. Our algorithm is among the first to achieve such guarantees in apartial-feedback scheme, and the first one to do so in a combinatorial setting.
arxiv-10800-10 | Memory and information processing in neuromorphic systems | http://arxiv.org/pdf/1506.03264v1.pdf | author:Giacomo Indiveri, Shih-Chii Liu category:cs.NE published:2015-06-10 summary:A striking difference between brain-inspired neuromorphic processors andcurrent von Neumann processors architectures is the way in which memory andprocessing is organized. As Information and Communication Technologies continueto address the need for increased computational power through the increase ofcores within a digital processor, neuromorphic engineers and scientists cancomplement this need by building processor architectures where memory isdistributed with the processing. In this paper we present a survey ofbrain-inspired processor architectures that support models of cortical networksand deep neural networks. These architectures range from serial clockedimplementations of multi-neuron systems to massively parallel asynchronous onesand from purely digital systems to mixed analog/digital systems which implementmore biological-like models of neurons and synapses together with a suite ofadaptation and learning mechanisms analogous to the ones found in biologicalnervous systems. We describe the advantages of the different approaches beingpursued and present the challenges that need to be addressed for buildingartificial neural processing systems that can display the richness of behaviorsseen in biological systems.
arxiv-10800-11 | Combining Temporal Information and Topic Modeling for Cross-Document Event Ordering | http://arxiv.org/pdf/1506.03257v1.pdf | author:Borja Navarro-Colorado, Estela Saquete category:cs.CL published:2015-06-10 summary:Building unified timelines from a collection of written news articlesrequires cross-document event coreference resolution and temporal relationextraction. In this paper we present an approach event coreference resolutionaccording to: a) similar temporal information, and b) similar semanticarguments. Temporal information is detected using an automatic temporalinformation system (TIPSem), while semantic information is represented by meansof LDA Topic Modeling. The evaluation of our approach shows that it obtains thehighest Micro-average F-score results in the SemEval2015 Task 4: TimeLine:Cross-Document Event Ordering (25.36\% for TrackB, 23.15\% for SubtrackB), withan improvement of up to 6\% in comparison to the other systems. However, ourexperiment also showed some draw-backs in the Topic Modeling approach thatdegrades performance of the system.
arxiv-10800-12 | A Scale Mixture Perspective of Multiplicative Noise in Neural Networks | http://arxiv.org/pdf/1506.03208v1.pdf | author:Eric Nalisnick, Anima Anandkumar, Padhraic Smyth category:stat.ML published:2015-06-10 summary:Corrupting the input and hidden layers of deep neural networks (DNNs) withmultiplicative noise, often drawn from the Bernoulli distribution (or'dropout'), provides regularization that has significantly contributed to deeplearning's success. However, understanding how multiplicative corruptionsprevent overfitting has been difficult due to the complexity of a DNN'sfunctional form. In this paper, we show that when a Gaussian prior is placed ona DNN's weights, applying multiplicative noise induces a Gaussian scalemixture, which can be reparameterized to circumvent the problematic likelihoodfunction. Analysis can then proceed by using a type-II maximum likelihoodprocedure to derive a closed-form expression revealing how regularizationevolves as a function of the network's weights. Results show thatmultiplicative noise forces weights to become either sparse or invariant torescaling. We find our analysis has implications for model compression as itnaturally reveals a weight pruning rule that starkly contrasts with thecommonly used signal-to-noise ratio (SNR). While the SNR prunes weights withlarge variances, seeing them as noisy, our approach recognizes their robustnessand retains them. We empirically demonstrate our approach has a strongadvantage over the SNR heuristic and is competitive to retraining with softtargets produced from a teacher model.
arxiv-10800-13 | ICDAR 2015 Text Reading in the Wild Competition | http://arxiv.org/pdf/1506.03184v1.pdf | author:Xinyu Zhou, Shuchang Zhou, Cong Yao, Zhimin Cao, Qi Yin category:cs.CV published:2015-06-10 summary:Recently, text detection and recognition in natural scenes are becomingincreasing popular in the computer vision community as well as the documentanalysis community. However, majority of the existing ideas, algorithms andsystems are specifically designed for English. This technical report presentsthe final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015)competition, which aims at establishing a benchmark for assessing detection andrecognition algorithms devised for both Chinese and English scripts andproviding a playground for researchers from the community. In this article, wedescribe in detail the dataset, tasks, evaluation protocols and participants ofthis competition, and report the performance of the participating methods.Moreover, promising directions for future research are discussed.
arxiv-10800-14 | A Scheme for Molecular Computation of Maximum Likelihood Estimators for Log-Linear Models | http://arxiv.org/pdf/1506.03172v1.pdf | author:Manoj Gopalkrishnan category:cs.NE math.ST q-bio.MN stat.TH published:2015-06-10 summary:We propose a scheme for computing Maximum Likelihood Estimators forLog-Linear models using reaction networks, and prove its correctness. Ourscheme exploits the toric structure of equilibrium points of reaction networks.This allows an efficient encoding of the problem, and reveals how reactionnetworks are naturally suited to statistical inference tasks. Our scheme isrelevant to molecular programming, an emerging discipline that views molecularinteractions as computational primitives for the synthesis of sophisticatedbehaviors. In addition, such a scheme may provide a template to understand howbiochemical signaling pathways integrate extensive information about theirenvironment and history.
arxiv-10800-15 | Large-scale randomized-coordinate descent methods with non-separable linear constraints | http://arxiv.org/pdf/1409.2617v5.pdf | author:Sashank Reddi, Ahmed Hefny, Carlton Downey, Avinava Dubey, Suvrit Sra category:math.OC stat.ML published:2014-09-09 summary:We develop randomized (block) coordinate descent (CD) methods for linearlyconstrained convex optimization. Unlike most CD methods, we do not assume theconstraints to be separable, but let them be coupled linearly. To ourknowledge, ours is the first CD method that allows linear coupling constraints,without making the global iteration complexity have an exponential dependenceon the number of constraints. We present algorithms and analysis for four keyproblem scenarios: (i) smooth; (ii) smooth + nonsmooth separable; (iii)asynchronous parallel; and (iv) stochastic. We illustrate empirical behavior ofour algorithms by simulation experiments.
arxiv-10800-16 | Robust Subgraph Generation Improves Abstract Meaning Representation Parsing | http://arxiv.org/pdf/1506.03139v1.pdf | author:Keenon Werling, Gabor Angeli, Christopher Manning category:cs.CL published:2015-06-10 summary:The Abstract Meaning Representation (AMR) is a representation for open-domainrich semantics, with potential use in fields like event extraction and machinetranslation. Node generation, typically done using a simple dictionary lookup,is currently an important limiting factor in AMR parsing. We propose a smallset of actions that derive AMR subgraphs by transformations on spans of text,which allows for more robust learning of this stage. Our set of constructionactions generalize better than the previous approach, and can be learned with asimple classifier. We improve on the previous state-of-the-art result for AMRparsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 andLDC2014T12 datasets.
arxiv-10800-17 | Pointer Networks | http://arxiv.org/pdf/1506.03134v1.pdf | author:Oriol Vinyals, Meire Fortunato, Navdeep Jaitly category:stat.ML cs.CG cs.LG cs.NE published:2015-06-09 summary:We introduce a new neural architecture to learn the conditional probabilityof an output sequence with elements that are discrete tokens corresponding topositions in an input sequence. Such problems cannot be trivially addressed byexistent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in each step of the output depends on thelength of the input, which is variable. Problems such as sorting variable sizedsequences, and various combinatorial optimization problems belong to thisclass. Our model solves the problem of variable size output dictionaries usinga recently proposed mechanism of neural attention. It differs from the previousattention attempts in that, instead of using attention to blend hidden units ofan encoder to a context vector at each decoder step, it uses attention as apointer to select a member of the input sequence as the output. We call thisarchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learnapproximate solutions to three challenging geometric problems -- finding planarconvex hulls, computing Delaunay triangulations, and the planar TravellingSalesman Problem -- using training examples alone. Ptr-Nets not only improveover sequence-to-sequence with input attention, but also allow us to generalizeto variable size output dictionaries. We show that the learnt models generalizebeyond the maximum lengths they were trained on. We hope our results on thesetasks will encourage a broader exploration of neural learning for discreteproblems.
arxiv-10800-18 | Multiscale edge detection and parametric shape modeling for boundary delineation in optoacoustic images | http://arxiv.org/pdf/1506.03124v1.pdf | author:Subhamoy Mandal, Viswanath Pamulakanty Sudarshan, Yeshaswini Nagaraj, Xose Luis Dean Ben, Daniel Razansky category:physics.med-ph cs.CV published:2015-06-09 summary:In this article, we present a novel scheme for segmenting the image boundary(with the background) in optoacoustic small animal in vivo imaging systems. Themethod utilizes a multiscale edge detection algorithm to generate a binary edgemap. A scale dependent morphological operation is employed to clean spuriousedges. Thereafter, an ellipse is fitted to the edge map through constrainedparametric transformations and iterative goodness of fit calculations. Themethod delimits the tissue edges through the curve fitting model, which hasshown high levels of accuracy. Thus, this method enables segmentation ofoptoacoutic images with minimal human intervention, by eliminating need ofscale selection for multiscale processing and seed point determination forcontour mapping.
arxiv-10800-19 | Grammar as a Foreign Language | http://arxiv.org/pdf/1412.7449v3.pdf | author:Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton category:cs.CL cs.LG stat.ML published:2014-12-23 summary:Syntactic constituency parsing is a fundamental problem in natural languageprocessing and has been the subject of intensive research and engineering fordecades. As a result, the most accurate parsers are domain specific, complex,and inefficient. In this paper we show that the domain agnosticattention-enhanced sequence-to-sequence model achieves state-of-the-art resultson the most widely used syntactic constituency parsing dataset, when trained ona large synthetic corpus that was annotated using existing parsers. It alsomatches the performance of standard parsers when trained only on a smallhuman-annotated dataset, which shows that this model is highly data-efficient,in contrast to sequence-to-sequence models without the attention mechanism. Ourparser is also fast, processing over a hundred sentences per second with anunoptimized CPU implementation.
arxiv-10800-20 | Variational consensus Monte Carlo | http://arxiv.org/pdf/1506.03074v1.pdf | author:Maxim Rabinovich, Elaine Angelino, Michael I. Jordan category:stat.ML stat.CO published:2015-06-09 summary:Practitioners of Bayesian statistics have long depended on Markov chain MonteCarlo (MCMC) to obtain samples from intractable posterior distributions.Unfortunately, MCMC algorithms are typically serial, and do not scale to thelarge datasets typical of modern machine learning. The recently proposedconsensus Monte Carlo algorithm removes this limitation by partitioning thedata and drawing samples conditional on each partition in parallel (Scott etal, 2013). A fixed aggregation function then combines these samples, yieldingapproximate posterior samples. We introduce variational consensus Monte Carlo(VCMC), a variational Bayes algorithm that optimizes over aggregation functionsto obtain samples from a distribution that better approximates the target. Theresulting objective contains an intractable entropy term; we therefore derive arelaxation of the objective and show that the relaxed problem is blockwiseconcave under mild conditions. We illustrate the advantages of our algorithm onthree inference tasks from the literature, demonstrating both the superiorquality of the posterior approximation and the moderate overhead of theoptimization step. Our algorithm achieves a relative error reduction (measuredagainst serial MCMC) of up to 39% compared to consensus Monte Carlo on the taskof estimating 300-dimensional probit regression parameter expectations;similarly, it achieves an error reduction of 92% on the task of estimatingcluster comembership probabilities in a Gaussian mixture model with 8components in 8 dimensions. Furthermore, these gains come at moderate costcompared to the runtime of serial MCMC, achieving near-ideal speedup in someinstances.
arxiv-10800-21 | Clustering by transitive propagation | http://arxiv.org/pdf/1506.03072v1.pdf | author:Vijay Kumar, Dan Levy category:cs.LG stat.ML published:2015-06-09 summary:We present a global optimization algorithm for clustering data given theratio of likelihoods that each pair of data points is in the same cluster or indifferent clusters. To define a clustering solution in terms of pairwiserelationships, a necessary and sufficient condition is that belonging to thesame cluster satisfies transitivity. We define a global objective functionbased on pairwise likelihood ratios and a transitivity constraint over alltriples, assigning an equal prior probability to all clustering solutions. Wemaximize the objective function by implementing max-sum message passing on thecorresponding factor graph to arrive at an O(N^3) algorithm. Lastly, wedemonstrate an application inspired by mutational sequencing for decodingrandom binary words transmitted through a noisy channel.
arxiv-10800-22 | A Variance Reduced Stochastic Newton Method | http://arxiv.org/pdf/1503.08316v4.pdf | author:Aurelien Lucchi, Brian McWilliams, Thomas Hofmann category:cs.LG published:2015-03-28 summary:Quasi-Newton methods are widely used in practise for convex loss minimizationproblems. These methods exhibit good empirical performance on a wide variety oftasks and enjoy super-linear convergence to the optimal solution. Forlarge-scale learning problems, stochastic Quasi-Newton methods have beenrecently proposed. However, these typically only achieve sub-linear convergencerates and have not been shown to consistently perform well in practice sincenoisy Hessian approximations can exacerbate the effect of high-variancestochastic gradient estimates. In this work we propose Vite, a novel stochasticQuasi-Newton algorithm that uses an existing first-order technique to reducethis variance. Without exploiting the specific form of the approximate Hessian,we show that Vite reaches the optimum at a geometric rate with a constantstep-size when dealing with smooth strongly convex functions. Empirically, wedemonstrate improvements over existing stochastic Quasi-Newton and variancereduced stochastic gradient methods.
arxiv-10800-23 | The Wreath Process: A totally generative model of geometric shape based on nested symmetries | http://arxiv.org/pdf/1506.03041v1.pdf | author:Diana Borsa, Thore Graepel, Andrew Gordon category:cs.AI stat.ML 20-XX published:2015-06-09 summary:We consider the problem of modelling noisy but highly symmetric shapes thatcan be viewed as hierarchies of whole-part relationships in which higher levelobjects are composed of transformed collections of lower level objects. To thisend, we propose the stochastic wreath process, a fully generative probabilisticmodel of drawings. Following Leyton's "Generative Theory of Shape", werepresent shapes as sequences of transformation groups composed through awreath product. This representation emphasizes the maximization of transfer --- the idea thatthe most compact and meaningful representation of a given shape is achieved bymaximizing the re-use of existing building blocks or parts. The proposed stochastic wreath process extends Leyton's theory by defining aprobability distribution over geometric shapes in terms of noise processes thatare aligned with the generative group structure of the shape. We propose aninference scheme for recovering the generative history of given images in termsof the wreath process using reversible jump Markov chain Monte Carlo methodsand Approximate Bayesian Computation. In the context of sketching wedemonstrate the feasibility and limitations of this approach on model-generatedand real data.
arxiv-10800-24 | On the Uniform Convergence of Consistent Confidence Measures | http://arxiv.org/pdf/1506.03018v1.pdf | author:Yihan Gao, Aditya Parameswaran category:cs.LG published:2015-06-09 summary:Many classification algorithms produce confidence measures in the form ofconditional probability of labels given the features of the target instance. Itis desirable to be make these confidence measures calibrated or consistent, inthe sense that they correctly capture the belief of the algorithm in the labeloutput. For instance, if the algorithm outputs a label with confidence measure$p$ for $n$ times, then the output label should be correct approximately $np$times overall. Calibrated confidence measures lead to higher interpretabilityby humans and computers and enable downstream analysis or processing. In thispaper, we formally characterize the consistency of confidence measures andprove a PAC-style uniform convergence result for the consistency of confidencemeasures. We show that finite VC-dimension is sufficient for guaranteeing theconsistency of confidence measures produced by empirically consistentclassifiers. Our result also implies that we can calibrate confidence measuresproduced by any existing algorithms with monotonic functions, and still get thesame generalization guarantee on consistency.
arxiv-10800-25 | Active Sets Improves Learning for Mixture Models | http://arxiv.org/pdf/1506.02975v1.pdf | author:Vincent Zhao, Steven Zucker category:stat.ML cs.LG q-bio.QM published:2015-06-09 summary:We develop an algorithm to learn Bernoulli Mixture Models based on theprinciple that some variables are more informative than others. Working from aninformation-theoretic perspective, we propose both backward and forward schemesfor selecting the informative 'active' variables and using them to guide EM.The result is a stagewise EM algorithm, analogous to stagewise approaches tolinear regression, that should be applicable to neuroscience (and other)datasets with confounding (or irrelevant) variables. Results on synthetic andMNIST datasets illustrate the approach.
arxiv-10800-26 | A Linear-Time Particle Gibbs Sampler for Infinite Hidden Markov Models | http://arxiv.org/pdf/1505.00428v2.pdf | author:Nilesh Tripuraneni, Shane Gu, Hong Ge, Zoubin Ghahramani category:stat.ML published:2015-05-03 summary:Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametricgeneralization of the classical Hidden Markov Model which can automaticallyinfer the number of hidden states in the system. However, due to theinfinite-dimensional nature of transition dynamics performing inference in theiHMM is difficult. In this paper, we present an infinite-state Particle Gibbs(PG) algorithm to resample state trajectories for the iHMM. The proposedalgorithm uses an efficient proposal optimized for iHMMs and leverages ancestorsampling to suppress degeneracy of the standard PG algorithm. Our algorithmdemonstrates significant convergence improvements on synthetic and real worlddata sets. Additionally, the infinite-state PG algorithm has linear-timecomplexity in the number of states in the sampler, while competing methodsscale quadratically.
arxiv-10800-27 | Compact Shape Trees: A Contribution to the Forest of Shape Correspondences and Matching Methods | http://arxiv.org/pdf/1506.02923v1.pdf | author:Abdulrahman Oladipupo Ibraheem category:cs.CV published:2015-06-09 summary:We propose a novel technique, termed compact shape trees, for computingcorrespondences of single-boundary 2-D shapes in O(n2) time. Together with zeroor more features defined at each of n sample points on the shape's boundary,the compact shape tree of a shape comprises the O(n) collection of vectorsemanating from any of the sample points on the shape's boundary to the rest ofthe sample points on the boundary. As it turns out, compact shape trees have anumber of elegant properties both in the spatial and frequency domains. Inparticular, via a simple vector-algebraic argument, we show that the O(n)collection of vectors in a compact shape tree possesses at least the samediscriminatory power as the O(n2) collection of lines emanating from eachsample point to every other sample point on a shape's boundary. In addition, wedescribe neat approaches for achieving scale and rotation invariance withcompact shape trees in the spatial domain; by viewing compact shape trees asaperiodic discrete signals, we also prove scale and rotation invarianceproperties for them in the Fourier domain. Towards these, along the way, usingconcepts from differential geometry and the Calculus, we propose a novel theoryfor sampling 2-D shape boundaries in a scale and rotation invariant manner.Finally, we propose a number of shape recognition experiments to test theefficacy of our concept.
arxiv-10800-28 | An Ensemble method for Content Selection for Data-to-text Systems | http://arxiv.org/pdf/1506.02922v1.pdf | author:Dimitra Gkatzia, Helen Hastie category:cs.CL cs.AI published:2015-06-09 summary:We present a novel approach for automatic report generation from time-seriesdata, in the context of student feedback generation. Our proposed methodologytreats content selection as a multi-label classification (MLC) problem, whichtakes as input time-series data (students' learning data) and outputs a summaryof these data (feedback). Unlike previous work, this method considers all datasimultaneously using ensembles of classifiers, and therefore, it achieveshigher accuracy and F- score compared to meaningful baselines.
arxiv-10800-29 | Efficient Object Localization Using Convolutional Networks | http://arxiv.org/pdf/1411.4280v3.pdf | author:Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christopher Bregler category:cs.CV published:2014-11-16 summary:Recent state-of-the-art performance on human-body pose estimation has beenachieved with Deep Convolutional Networks (ConvNets). Traditional ConvNetarchitectures include pooling and sub-sampling layers which reducecomputational requirements, introduce invariance and prevent over-training.These benefits of pooling come at the cost of reduced localization accuracy. Weintroduce a novel architecture which includes an efficient `positionrefinement' model that is trained to estimate the joint offset location withina small region of the image. This refinement model is jointly trained incascade with a state-of-the-art ConvNet model to achieve improved accuracy inhuman joint location estimation. We show that the variance of our detectorapproaches the variance of human annotations on the FLIC dataset andoutperforms all existing approaches on the MPII-human-pose dataset.
arxiv-10800-30 | Times series averaging from a probabilistic interpretation of time-elastic kernel | http://arxiv.org/pdf/1505.06897v3.pdf | author:Pierre-François Marteau category:cs.LG cs.DS published:2015-05-26 summary:At the light of regularized dynamic time warping kernels, this paperreconsider the concept of time elastic centroid (TEC) for a set of time series.From this perspective, we show first how TEC can easily be addressed as apreimage problem. Unfortunately this preimage problem is ill-posed, may sufferfrom over-fitting especially for long time series and getting a sub-optimalsolution involves heavy computational costs. We then derive two new algorithmsbased on a probabilistic interpretation of kernel alignment matrices thatexpresses in terms of probabilistic distributions over sets of alignment paths.The first algorithm is an iterative agglomerative heuristics inspired from thestate of the art DTW barycenter averaging (DBA) algorithm proposed specificallyfor the Dynamic Time Warping measure. The second proposed algorithm achieves aclassical averaging of the aligned samples but also implements an averaging ofthe time of occurrences of the aligned samples. It exploits a straightforwardprogressive agglomerative heuristics. An experimentation that compares for 45time series datasets classification error rates obtained by first nearneighbors classifiers exploiting a single medoid or centroid estimate torepresent each categories show that: i) centroids based approachessignificantly outperform medoids based approaches, ii) on the consideredexperience, the two proposed algorithms outperform the state of the art DBAalgorithm, and iii) the second proposed algorithm that implements an averagingjointly in the sample space and along the time axes emerges as the mostsignificantly robust time elastic averaging heuristic with an interesting noisereduction capability. Index Terms-Time series averaging Time elastic kernelDynamic Time Warping Time series clustering and classification.
arxiv-10800-31 | Generalized Gradient Learning on Time Series under Elastic Transformations | http://arxiv.org/pdf/1502.04843v2.pdf | author:Brijnesh Jain category:cs.LG published:2015-02-17 summary:The majority of machine learning algorithms assumes that objects arerepresented as vectors. But often the objects we want to learn on are morenaturally represented by other data structures such as sequences and timeseries. For these representations many standard learning algorithms areunavailable. We generalize gradient-based learning algorithms to time seriesunder dynamic time warping. To this end, we introduce elastic functions, whichextend functions on time series to matrix spaces. Necessary conditions arepresented under which generalized gradient learning on time series isconsistent. We indicate how results carry over to arbitrary elastic distancefunctions and to sequences consisting of symbolic elements. Specifically, fourlinear classifiers are extended to time series under dynamic time warping andapplied to benchmark datasets. Results indicate that generalized gradientlearning via elastic functions have the potential to complement thestate-of-the-art in statistical pattern recognition on time series.
arxiv-10800-32 | Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages | http://arxiv.org/pdf/1503.02551v2.pdf | author:Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Sejdinovic, Zoltán Szabó category:stat.ML cs.LG G.3; I.2.6 published:2015-03-09 summary:We propose an efficient nonparametric strategy for learning a messageoperator in expectation propagation (EP), which takes as input the set ofincoming messages to a factor node, and produces an outgoing message as output.This learned operator replaces the multivariate integral required in classicalEP, which may not have an analytic expression. We use kernel-based regression,which is trained on a set of probability distributions representing theincoming messages, and the associated outgoing messages. The kernel approachhas two main advantages: first, it is fast, as it is implemented using a noveltwo-layer random feature representation of the input message distributions;second, it has principled uncertainty estimates, and can be cheaply updatedonline, meaning it can request and incorporate new training data when itencounters inputs on which it is uncertain. In experiments, our approach isable to solve learning problems where a single message operator is required formultiple, substantially different data sets (logistic regression for a varietyof classification problems), where it is essential to accurately assessuncertainty and to efficiently and robustly update the message operator.
arxiv-10800-33 | Distributed Stochastic Optimization of the Regularized Risk | http://arxiv.org/pdf/1406.4363v2.pdf | author:Shin Matsushima, Hyokun Yun, Xinhua Zhang, S. V. N. Vishwanathan category:stat.ML cs.LG published:2014-06-17 summary:Many machine learning algorithms minimize a regularized risk, and stochasticoptimization is widely used for this task. When working with massive data, itis desirable to perform stochastic optimization in parallel. Unfortunately,many existing stochastic optimization algorithms cannot be parallelizedefficiently. In this paper we show that one can rewrite the regularized riskminimization problem as an equivalent saddle-point problem, and propose anefficient distributed stochastic optimization (DSO) algorithm. We prove thealgorithm's rate of convergence; remarkably, our analysis shows that thealgorithm scales almost linearly with the number of processors. We also verifywith empirical evaluations that the proposed algorithm is competitive withother parallel, general purpose stochastic and batch optimization algorithmsfor regularized risk minimization.
arxiv-10800-34 | On the Error of Random Fourier Features | http://arxiv.org/pdf/1506.02785v1.pdf | author:Dougal J. Sutherland, Jeff Schneider category:cs.LG stat.ML published:2015-06-09 summary:Kernel methods give powerful, flexible, and theoretically grounded approachesto solving many problems in machine learning. The standard approach, however,requires pairwise evaluations of a kernel function, which can lead toscalability issues for very large datasets. Rahimi and Recht (2007) suggested apopular approach to handling this problem, known as random Fourier features.The quality of this approximation, however, is not well understood. We improvethe uniform error bound of that paper, as well as giving novel understandingsof the embedding's variance, approximation error, and use in some machinelearning methods. We also point out that surprisingly, of the two main variantsof those features, the more widely used is strictly higher-variance for theGaussian kernel and has worse bounds.
arxiv-10800-35 | Fast Geometric Fit Algorithm for Sphere Using Exact Solution | http://arxiv.org/pdf/1506.02776v1.pdf | author:Sumith YD category:cs.CV published:2015-06-09 summary:Sphere fitting is a common problem in almost all science and engineeringdisciplines. Most of methods available are iterative in behavior. This involvesfitting of the parameters in a least square sense or in a geometric sense. Herewe extend the methods of Thomas Chan and Landau who fitted the 2D data usingcircle. This work closely resemble their work in redefining the error estimateand solving the sphere fitting problem exactly. The solutions for center andradius of the sphere can be found exactly and the equations can be hard codedfor high performance. We have also shown some comparison with other popularmethods and how this method behaves.
arxiv-10800-36 | Self Organizing Maps Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using Conditional Rotations | http://arxiv.org/pdf/1506.02750v1.pdf | author:César A. Astudillo, B. John Oommen category:cs.NE cs.AI published:2015-06-09 summary:Numerous variants of Self-Organizing Maps (SOMs) have been proposed in theliterature, including those which also possess an underlying structure, and insome cases, this structure itself can be defined by the user Although theconcepts of growing the SOM and updating it have been studied, the whole issueof using a self-organizing Adaptive Data Structure (ADS) to further enhance theproperties of the underlying SOM, has been unexplored. In an earlier work, weimpose an arbitrary, user-defined, tree-like topology onto the codebooks, whichconsequently enforced a neighborhood phenomenon and the so-called tree-basedBubble of Activity. In this paper, we consider how the underlying tree itselfcan be rendered dynamic and adaptively transformed. To do this, we presentmethods by which a SOM with an underlying Binary Search Tree (BST) structurecan be adaptively re-structured using Conditional Rotations (CONROT). Theserotations on the nodes of the tree are local, can be done in constant time, andperformed so as to decrease the Weighted Path Length (WPL) of the entire tree.In doing this, we introduce the pioneering concept referred to as NeuralPromotion, where neurons gain prominence in the Neural Network (NN) as theirsignificance increases. We are not aware of any research which deals with theissue of Neural Promotion. The advantages of such a scheme is that the userneed not be aware of any of the topological peculiarities of the stochasticdata distribution. Rather, the algorithm, referred to as the TTOSOM withConditional Rotations (TTOCONROT), converges in such a manner that the neuronsare ultimately placed in the input space so as to represent its stochasticdistribution, and additionally, the neighborhood properties of the neurons suitthe best BST that represents the data. These properties have been confirmed byour experimental results on a variety of data sets.
arxiv-10800-37 | Sparse Representation Classification Beyond L1 Minimization and the Subspace Assumption | http://arxiv.org/pdf/1502.01368v2.pdf | author:Cencheng Shen, Li Chen, Carey E. Priebe category:stat.ML published:2015-02-04 summary:The sparse representation classifier (SRC) proposed in Wright et al. (2009)has recently gained much attention from the machine learning community. Itmakes use of L1 minimization, and is known to work well for data satisfying asubspace assumption. In this paper, we use the notion of class dominance aswell as a principal angle condition to investigate and validate theclassification performance of SRC, without relying on L1 minimization and thesubspace assumption. We prove that SRC can still work well using faster subsetregression methods such as orthogonal matching pursuit and marginal regression,and its applicability is not limited to data satisfying the subspaceassumption. We illustrate our theorems via various real data sets includingface images, text features, and network data.
arxiv-10800-38 | Empirical Studies on Symbolic Aggregation Approximation Under Statistical Perspectives for Knowledge Discovery in Time Series | http://arxiv.org/pdf/1506.02732v1.pdf | author:Wei Song, Zhiguang Wang, Yangdong Ye, Ming Fan category:cs.LG cs.IT math.IT published:2015-06-08 summary:Symbolic Aggregation approXimation (SAX) has been the de facto standardrepresentation methods for knowledge discovery in time series on a number oftasks and applications. So far, very little work has been done in empiricallyinvestigating the intrinsic properties and statistical mechanics in SAX words.In this paper, we applied several statistical measurements and proposed a newstatistical measurement, i.e. information embedding cost (IEC) to analyze thestatistical behaviors of the symbolic dynamics. Our experiments on thebenchmark datasets and the clinical signals demonstrate that SAX can alwaysreduce the complexity while preserving the core information embedded in theoriginal time series with significant embedding efficiency. Our proposed IECscore provide a priori to determine if SAX is adequate for specific dataset,which can be generalized to evaluate other symbolic representations. Our workprovides an analytical framework with several statistical tools to analyze,evaluate and further improve the symbolic dynamics for knowledge discovery intime series.
arxiv-10800-39 | Non-parametric Revenue Optimization for Generalized Second Price Auctions | http://arxiv.org/pdf/1506.02719v1.pdf | author:Mehryar Mohri, Andres Munoz Medina category:cs.LG cs.GT published:2015-06-08 summary:We present an extensive analysis of the key problem of learning optimalreserve prices for generalized second price auctions. We describe twoalgorithms for this task: one based on density estimation, and a novelalgorithm benefiting from solid theoretical guarantees and with a veryfavorable running-time complexity of $O(n S \log (n S))$, where $n$ is thesample size and $S$ the number of slots. Our theoretical guarantees are morefavorable than those previously presented in the literature. Additionally, weshow that even if bidders do not play at an equilibrium, our second algorithmis still well defined and minimizes a quantity of interest. To our knowledge,this is the first attempt to apply learning algorithms to the problem ofreserve price optimization in GSP auctions. Finally, we present the firstconvergence analysis of empirical equilibrium bidding functions to the uniquesymmetric Bayesian-Nash equilibrium of a GSP.
arxiv-10800-40 | Population Empirical Bayes | http://arxiv.org/pdf/1411.0292v2.pdf | author:Alp Kucukelbir, David M. Blei category:stat.ML cs.LG published:2014-11-02 summary:Bayesian predictive inference analyzes a dataset to make predictions aboutnew observations. When a model does not match the data, predictive accuracysuffers. We develop population empirical Bayes (POP-EB), a hierarchicalframework that explicitly models the empirical population distribution as partof Bayesian analysis. We introduce a new concept, the latent dataset, as ahierarchical variable and set the empirical population as its prior. This leadsto a new predictive density that mitigates model mismatch. We efficiently applythis method to complex models by proposing a stochastic variational inferencealgorithm, called bumping variational inference (BUMP-VI). We demonstrateimproved predictive accuracy over classical Bayesian inference in three models:a linear regression model of health data, a Bayesian mixture model of naturalimages, and a latent Dirichlet allocation topic model of scientific documents.
arxiv-10800-41 | A CMOS Spiking Neuron for Dense Memristor-Synapse Connectivity for Brain-Inspired Computing | http://arxiv.org/pdf/1506.01069v2.pdf | author:Xinyu Wu, Vishal Saxena, Kehan Zhu category:cs.NE cs.ET published:2015-06-02 summary:Neuromorphic systems that densely integrate CMOS spiking neurons andnano-scale memristor synapses open a new avenue of brain-inspired computing.Existing silicon neurons have molded neural biophysical dynamics but areincompatible with memristor synapses, or used extra training circuitry thuseliminating much of the density advantages gained by using memristors, or wereenergy inefficient. Here we describe a novel CMOS spiking leakyintegrate-and-fire neuron circuit. Building on a reconfigurable architecturewith a single opamp, the described neuron accommodates a large number ofmemristor synapses, and enables online spike timing dependent plasticity (STDP)learning with optimized power consumption. Simulation results of an 180nm CMOSdesign showed 97% power efficiency metric when realizing STDP learning in10,000 memristor synapses with a nominal 1M{\Omega} memristance, and only13{\mu}A current consumption when integrating input spikes. Therefore, thedescribed CMOS neuron contributes a generalized building block for large-scalebrain-inspired neuromorphic systems.
arxiv-10800-42 | Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition | http://arxiv.org/pdf/1506.01072v2.pdf | author:Xinyu Wu, Vishal Saxena, Kehan Zhu category:cs.NE cs.AI cs.CV cs.ET published:2015-06-02 summary:A neuromorphic chip that combines CMOS analog spiking neurons and memristivesynapses offers a promising solution to brain-inspired computing, as it canprovide massive neural network parallelism and density. Previous hybrid analogCMOS-memristor approaches required extensive CMOS circuitry for training, andthus eliminated most of the density advantages gained by the adoption ofmemristor synapses. Further, they used different waveforms for pre andpost-synaptic spikes that added undesirable circuit overhead. Here we describea hardware architecture that can feature a large number of memristor synapsesto learn real-world patterns. We present a versatile CMOS neuron that combinesintegrate-and-fire behavior, drives passive memristors and implementscompetitive learning in a compact circuit module, and enables in-situplasticity in the memristor synapses. We demonstrate handwritten-digitsrecognition using the proposed architecture using transistor-level circuitsimulations. As the described neuromorphic architecture is homogeneous, itrealizes a fundamental building block for large-scale energy-efficientbrain-inspired silicon chips that could lead to next-generation cognitivecomputing.
arxiv-10800-43 | The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal Prediction | http://arxiv.org/pdf/1506.02686v1.pdf | author:George D. Montanez, Cosma Rohilla Shalizi category:stat.ML cs.LG published:2015-06-08 summary:For the task of unsupervised spatio-temporal forecasting (e.g., learning topredict video data without labels), we propose two new nonparametric predictivestate algorithms, Moonshine and One Hundred Proof. The algorithms areconceptually simple and make few assumptions on the underlying spatio-temporalprocess yet have strong predictive performance and provide predictivedistributions over spatio-temporal data. The latter property allows forlikelihood estimation under the models, for classification and otherprobabilistic inference.
arxiv-10800-44 | A Topological Approach to Spectral Clustering | http://arxiv.org/pdf/1506.02633v1.pdf | author:Antonio Rieser category:cs.LG stat.ML published:2015-06-08 summary:We propose a clustering algorithm which, for input, takes data assumed to besampled from a uniform distribution supported on a metric space $X$, andoutputs a clustering of the data based on a topological estimate of theconnected components of $X$. The algorithm works by choosing a weighted graphon the samples from a natural one-parameter family of graphs using an errorbased on the heat operator on the graphs. The estimated connected components of$X$ are identified as the support of the eigenfunctions of the heat operatorwith eigenvalue $1$, which allows the algorithm to work without requiring thenumber of expected clusters as input.
arxiv-10800-45 | On Symmetric and Asymmetric LSHs for Inner Product Search | http://arxiv.org/pdf/1410.5518v3.pdf | author:Behnam Neyshabur, Nathan Srebro category:stat.ML cs.DS cs.IR cs.LG published:2014-10-21 summary:We consider the problem of designing locality sensitive hashes (LSH) forinner product similarity, and of the power of asymmetric hashes in thiscontext. Shrivastava and Li argue that there is no symmetric LSH for theproblem and propose an asymmetric LSH based on different mappings for query anddatabase points. However, we show there does exist a simple symmetric LSH thatenjoys stronger guarantees and better empirical performance than the asymmetricLSH they suggest. We also show a variant of the settings where asymmetry isin-fact needed, but there a different asymmetric LSH is required.
arxiv-10800-46 | An Information-Theoretic Analysis of Thompson Sampling | http://arxiv.org/pdf/1403.5341v2.pdf | author:Daniel Russo, Benjamin Van Roy category:cs.LG published:2014-03-21 summary:We provide an information-theoretic analysis of Thompson sampling thatapplies across a broad range of online optimization problems in which adecision-maker must learn from partial feedback. This analysis inherits thesimplicity and elegance of information theory and leads to regret bounds thatscale with the entropy of the optimal-action distribution. This strengthenspreexisting results and yields new insight into how information improvesperformance.
arxiv-10800-47 | Path-SGD: Path-Normalized Optimization in Deep Neural Networks | http://arxiv.org/pdf/1506.02617v1.pdf | author:Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.CV cs.NE stat.ML published:2015-06-08 summary:We revisit the choice of SGD for training deep neural networks byreconsidering the appropriate geometry in which to optimize the weights. Weargue for a geometry invariant to rescaling of weights that does not affect theoutput of the network, and suggest Path-SGD, which is an approximate steepestdescent method with respect to a path-wise regularizer related to max-normregularization. Path-SGD is easy and efficient to implement and leads toempirical gains over SGD and AdaGrad.
arxiv-10800-48 | Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection | http://arxiv.org/pdf/1506.02585v1.pdf | author:Zhimin Peng, Prudhvi Gurram, Heesung Kwon, Wotao Yin category:cs.LG published:2015-06-08 summary:In this paper, a novel framework of sparse kernel learning for Support VectorData Description (SVDD) based anomaly detection is presented. In this work,optimal sparse feature selection for anomaly detection is first modeled as aMixed Integer Programming (MIP) problem. Due to the prohibitively highcomputational complexity of the MIP, it is relaxed into a QuadraticallyConstrained Linear Programming (QCLP) problem. The QCLP problem can then bepractically solved by using an iterative optimization method, in which multiplesubsets of features are iteratively found as opposed to a single subset. TheQCLP-based iterative optimization problem is solved in a finite space calledthe \emph{Empirical Kernel Feature Space} (EKFS) instead of in the input spaceor \emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because ofthe fact that the geometrical properties of the EKFS and the corresponding RKHSremain the same. Now, an explicit nonlinear exploitation of the data in afinite EKFS is achievable, which results in optimal feature ranking.Experimental results based on a hyperspectral image show that the proposedmethod can provide improved performance over the current state-of-the-arttechniques.
arxiv-10800-49 | License Plate Recognition System Based on Color Coding Of License Plates | http://arxiv.org/pdf/1506.03128v1.pdf | author:Jani Biju Babjan category:cs.CV published:2015-06-08 summary:License Plate Recognition Systems are used to determine the license platenumber of a vehicle. The current system mainly uses Optical CharacterRecognition to recognize the number plate. There are several problems to thissystem. Some of them include interchanging of several letters or numbers(letter O with digit 0), difficulty in localizing the license plate, high errorrate, use of different fonts in license plates etc. So a new system torecognize the license plate number using color coding of license plates isproposed in this paper. Easier localization of license plate can be done bysearching for the start or stop patters of license plates. An eight segmentdisplay system along with traditional numbering with the first and lastsegments left for start or stop patterns is proposed in this paper. Practicalapplications include several areas under Internet of Things (IoT).
arxiv-10800-50 | Faster SGD Using Sketched Conditioning | http://arxiv.org/pdf/1506.02649v1.pdf | author:Alon Gonen, Shai Shalev-Shwartz category:cs.NA cs.LG published:2015-06-08 summary:We propose a novel method for speeding up stochastic optimization algorithmsvia sketching methods, which recently became a powerful tool for acceleratingalgorithms for numerical linear algebra. We revisit the method of conditioningfor accelerating first-order methods and suggest the use of sketching methodsfor constructing a cheap conditioner that attains a significant speedup withrespect to the Stochastic Gradient Descent (SGD) algorithm. While ourtheoretical guarantees assume convexity, we discuss the applicability of ourmethod to deep neural networks, and experimentally demonstrate its merits.
arxiv-10800-51 | Linear Convergence of the Randomized Feasible Descent Method Under the Weak Strong Convexity Assumption | http://arxiv.org/pdf/1506.02530v1.pdf | author:Chenxin Ma, Rachael Tappenden, Martin Takáč category:cs.LG stat.ML published:2015-06-08 summary:In this paper we generalize the framework of the feasible descent method(FDM) to a randomized (R-FDM) and a coordinate-wise random feasible descentmethod (RC-FDM) framework. We show that the famous SDCA algorithm foroptimizing the SVM dual problem, or the stochastic coordinate descent methodfor the LASSO problem, fits into the framework of RC-FDM. We prove linearconvergence for both R-FDM and RC-FDM under the weak strong convexityassumption. Moreover, we show that the duality gap converges linearly forRC-FDM, which implies that the duality gap also converges linearly for SDCAapplied to the SVM dual problem.
arxiv-10800-52 | Convex recovery of tensors using nuclear norm penalization | http://arxiv.org/pdf/1506.02520v1.pdf | author:Stephane Chretien, Tianwen Wei category:stat.ML published:2015-06-08 summary:The subdifferential of convex functions of the singular spectrum of realmatrices has been widely studied in matrix analysis, optimization and automaticcontrol theory. Convex analysis and optimization over spaces of tensors is nowgaining much interest due to its potential applications to signal processing,statistics and engineering. The goal of this paper is to present anapplications to the problem of low rank tensor recovery based on linear randommeasurement by extending the results of Tropp to the tensors setting.
arxiv-10800-53 | Learning Mixtures of Ising Models using Pseudolikelihood | http://arxiv.org/pdf/1506.02510v1.pdf | author:Onur Dikmen category:cs.LG stat.ML published:2015-06-08 summary:Maximum pseudolikelihood method has been among the most important methods forlearning parameters of statistical physics models, such as Ising models. Inthis paper, we study how pseudolikelihood can be derived for learningparameters of a mixture of Ising models. The performance of the proposedapproach is demonstrated for Ising and Potts models on both synthetic and realdata.
arxiv-10800-54 | SVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from ImageNet | http://arxiv.org/pdf/1506.02509v1.pdf | author:Lei Zhang, David Zhang category:cs.LG cs.CV published:2015-06-08 summary:Deep learning with a convolutional neural network (CNN) has been proved to bevery effective in feature extraction and representation of images. For imageclassification problems, this work aim at finding which classifier is morecompetitive based on high-level deep features of images. In this report, wehave discussed the nearest neighbor, support vector machines and extremelearning machines for image classification under deep convolutional activationfeature representation. Specifically, we adopt the benchmark object recognitiondataset from multiple sources with domain bias for evaluating differentclassifiers. The deep features of the object dataset are obtained by awell-trained CNN with five convolutional layers and three fully-connectedlayers on the challenging ImageNet. Experiments demonstrate that the ELMsoutperform SVMs in cross-domain recognition tasks. In particular,state-of-the-art results are obtained by kernel ELM which outperforms SVMs withabout 4% of the average accuracy. The features and codes are available inhttp://www.escience.cn/people/lei/index.html
arxiv-10800-55 | Gaussian Process Optimization with Mutual Information | http://arxiv.org/pdf/1311.4825v3.pdf | author:Emile Contal, Vianney Perchet, Nicolas Vayatis category:stat.ML cs.LG published:2013-11-19 summary:In this paper, we analyze a generic algorithm scheme for sequential globaloptimization using Gaussian processes. The upper bounds we derive on thecumulative regret for this generic algorithm improve by an exponential factorthe previously known bounds for algorithms like GP-UCB. We also introduce thenovel Gaussian Process Mutual Information algorithm (GP-MI), whichsignificantly improves further these upper bounds for the cumulative regret. Weconfirm the efficiency of this algorithm on synthetic and real tasks againstthe natural competitor, GP-UCB, and also the Expected Improvement heuristic.
arxiv-10800-56 | Trust Region Policy Optimization | http://arxiv.org/pdf/1502.05477v3.pdf | author:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel category:cs.LG published:2015-02-19 summary:In this article, we describe a method for optimizing control policies, withguaranteed monotonic improvement. By making several approximations to thetheoretically-justified scheme, we develop a practical algorithm, called TrustRegion Policy Optimization (TRPO). This algorithm is effective for optimizinglarge nonlinear policies such as neural networks. Our experiments demonstrateits robust performance on a wide variety of tasks: learning simulated roboticswimming, hopping, and walking gaits; and playing Atari games using images ofthe screen as input. Despite its approximations that deviate from the theory,TRPO tends to give monotonic improvement, with little tuning ofhyperparameters.
arxiv-10800-57 | Reflection Invariance: an important consideration of image orientation | http://arxiv.org/pdf/1506.02432v1.pdf | author:Craig Henderson, Ebroul Izquierdo category:cs.CV published:2015-06-08 summary:In this position paper, we consider the state of computer vision researchwith respect to invariance to the horizontal orientation of an image -- what weterm reflection invariance. We describe why we consider reflection invarianceto be an important property and provide evidence where the absence of thisinvariance produces surprising inconsistencies in state-of-the-art systems. Wedemonstrate inconsistencies in methods of object detection and sceneclassification when they are presented with images and the horizontal mirror ofthose images. Finally, we examine where some of the invariance is exhibited infeature detection and descriptors, and make a case for future consideration ofreflection invariance as a measure of quality in computer vision algorithms.
arxiv-10800-58 | Robust Regression via Hard Thresholding | http://arxiv.org/pdf/1506.02428v1.pdf | author:Kush Bhatia, Prateek Jain, Purushottam Kar category:cs.LG stat.ML published:2015-06-08 summary:We study the problem of Robust Least Squares Regression (RLSR) where severalresponse variables can be adversarially corrupted. More specifically, for adata matrix X \in R^{p x n} and an underlying model w*, the response vector isgenerated as y = X'w* + b where b \in R^n is the corruption vector supportedover at most C.n coordinates. Existing exact recovery results for RLSR focussolely on L1-penalty based convex formulations and impose relatively strictmodel assumptions such as requiring the corruptions b to be selectedindependently of X. In this work, we study a simple hard-thresholding algorithm called TORRENTwhich, under mild conditions on X, can recover w* exactly even if b corruptsthe response variables in an adversarial manner, i.e. both the support andentries of b are selected adversarially after observing X and w*. Our resultshold under deterministic assumptions which are satisfied if X is sampled fromany sub-Gaussian distribution. Finally unlike existing results that apply onlyto a fixed w*, generated independently of X, our results are universal and holdfor any w* \in R^p. Next, we propose gradient descent-based extensions of TORRENT that can scaleefficiently to large scale problems, such as high dimensional sparse recoveryand prove similar recovery guarantees for these extensions. Empirically we findTORRENT, and more so its extensions, offering significantly faster recoverythan the state-of-the-art L1 solvers. For instance, even on moderate-sizeddatasets (with p = 50K) with around 40% corrupted responses, a variant of ourproposed method called TORRENT-HYB is more than 20x faster than the best L1solver.
arxiv-10800-59 | Encoding Source Language with Convolutional Neural Network for Machine Translation | http://arxiv.org/pdf/1503.01838v5.pdf | author:Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, Qun Liu category:cs.CL cs.LG cs.NE published:2015-03-06 summary:The recently proposed neural network joint model (NNJM) (Devlin et al., 2014)augments the n-gram target language model with a heuristically chosen sourcecontext window, achieving state-of-the-art performance in SMT. In this paper,we give a more systematic treatment by summarizing the relevant sourceinformation through a convolutional architecture guided by the targetinformation. With different guiding signals during decoding, our specificallydesigned convolution+gating architectures can pinpoint the parts of a sourcesentence that are relevant to predicting a target word, and fuse them with thecontext of entire source sentence to form a unified representation. Thisrepresentation, together with target language words, are fed to a deep neuralnetwork (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-Englishtranslation tasks show that the proposed model can achieve significantimprovements over the previous NNJM by up to +1.08 BLEU points on average
arxiv-10800-60 | A Tensor-Based Dictionary Learning Approach to Tomographic Image Reconstruction | http://arxiv.org/pdf/1506.04954v1.pdf | author:Sara Soltani, Misha E. Kilmer, Per Christian Hansen category:cs.CV cs.NA math.NA published:2015-06-08 summary:We consider tomographic reconstruction using priors in the form of adictionary learned from training images. The reconstruction has two stages:first we construct a tensor dictionary prior from our training data, and thenwe pose the reconstruction problem in terms of recovering the expansioncoefficients in that dictionary. Our approach differs from past approaches inthat a) we use a third-order tensor representation for our images and b) werecast the reconstruction problem using the tensor formulation. The dictionarylearning problem is presented as a non-negative tensor factorization problemwith sparsity constraints. The reconstruction problem is formulated in a convexoptimization framework by looking for a solution with a sparse representationin the tensor dictionary. Numerical results show that our tensor formulationleads to very sparse representations of both the training images and thereconstructions due to the ability of representing repeated features compactlyin the dictionary.
arxiv-10800-61 | LOCO: Distributing Ridge Regression with Random Projections | http://arxiv.org/pdf/1406.3469v4.pdf | author:Christina Heinze, Brian McWilliams, Nicolai Meinshausen, Gabriel Krummenacher category:stat.ML published:2014-06-13 summary:We propose LOCO, an algorithm for large-scale ridge regression whichdistributes the features across workers on a cluster. Important dependenciesbetween variables are preserved using structured random projections which arecheap to compute and must only be communicated once. We show that LOCO obtainsa solution which is close to the exact ridge regression solution in the fixeddesign setting. We verify this experimentally in a simulation study as well asan application to climate prediction. Furthermore, we show that LOCO achievessignificant speedups compared with a state-of-the-art distributed algorithm ona large-scale regression problem.
arxiv-10800-62 | Video Inpainting of Complex Scenes | http://arxiv.org/pdf/1503.05528v2.pdf | author:Alasdair Newson, Andrés Almansa, Matthieu Fradet, Yann Gousseau, Patrick Pérez category:cs.CV cs.MM math.NA published:2015-03-18 summary:We propose an automatic video inpainting algorithm which relies on theoptimisation of a global, patch-based functional. Our algorithm is able to dealwith a variety of challenging situations which naturally arise in videoinpainting, such as the correct reconstruction of dynamic textures, multiplemoving objects and moving background. Furthermore, we achieve this in an orderof magnitude less execution time with respect to the state-of-the-art. We arealso able to achieve good quality results on high definition videos. Finally,we provide specific algorithmic details to make implementation of our algorithmas easy as possible. The resulting algorithm requires no segmentation or manualinput other than the definition of the inpainting mask, and can deal with awider variety of situations than is handled by previous work. 1. Introduction.Advanced image and video editing techniques are increasingly common in theimage processing and computer vision world, and are also starting to be used inmedia entertainment. One common and difficult task closely linked to the worldof video editing is image and video " inpainting ". Generally speaking, this isthe task of replacing the content of an image or video with some other contentwhich is visually pleasing. This subject has been extensively studied in thecase of images, to such an extent that commercial image inpainting productsdestined for the general public are available, such as Photoshop's " ContentAware fill " [1]. However, while some impressive results have been obtained inthe case of videos, the subject has been studied far less extensively thanimage inpainting. This relative lack of research can largely be attributed tohigh time complexity due to the added temporal dimension. Indeed, it has onlyvery recently become possible to produce good quality inpainting results onhigh definition videos, and this only in a semi-automatic manner. Nevertheless,high-quality video inpainting has many important and useful applications suchas film restoration, professional post-production in cinema and video editingfor personal use. For this reason, we believe that an automatic, generic videoinpainting algorithm would be extremely useful for both academic andprofessional communities.
arxiv-10800-63 | Microscopic approach of a time elapsed neural model | http://arxiv.org/pdf/1506.02361v1.pdf | author:Julien Chevallier, Maria J. Caceres, Marie Doumic, Patricia Reynaud-Bouret category:cs.NE published:2015-06-08 summary:The spike trains are the main components of the information processing in thebrain. To model spike trains several point processes have been investigated inthe literature. And more macroscopic approaches have also been studied, usingpartial differential equation models. The main aim of the present article is tobuild a bridge between several point processes models (Poisson, Wold, Hawkes)that have been proved to statistically fit real spike trains data andage-structured partial differential equations as introduced by Pakdaman,Perthame and Salort.
arxiv-10800-64 | Efficient Learning in Large-Scale Combinatorial Semi-Bandits | http://arxiv.org/pdf/1406.7443v3.pdf | author:Zheng Wen, Branislav Kveton, Azin Ashkan category:cs.LG cs.AI stat.ML published:2014-06-28 summary:A stochastic combinatorial semi-bandit is an online learning problem where ateach step a learning agent chooses a subset of ground items subject tocombinatorial constraints, and then observes stochastic weights of these itemsand receives their sum as a payoff. In this paper, we consider efficientlearning in large-scale combinatorial semi-bandits with linear generalization,and as a solution, propose two learning algorithms called Combinatorial LinearThompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Bothalgorithms are computationally efficient as long as the offline version of thecombinatorial problem can be solved efficiently. We establish that CombLinTSand CombLinUCB are also provably statistically efficient under reasonableassumptions, by developing regret bounds that are independent of the problemscale (number of items) and sublinear in time. We also evaluate CombLinTS on avariety of problems with thousands of items. Our experiment results demonstratethat CombLinTS is scalable, robust to the choice of algorithm parameters, andsignificantly outperforms the best of our baselines.
arxiv-10800-65 | Policy Gradient for Coherent Risk Measures | http://arxiv.org/pdf/1502.03919v2.pdf | author:Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor category:cs.AI cs.LG stat.ML published:2015-02-13 summary:Several authors have recently developed risk-sensitive policy gradientmethods that augment the standard expected cost minimization problem with ameasure of variability in cost. These studies have focused on specificrisk-measures, such as the variance or conditional value at risk (CVaR). Inthis work, we extend the policy gradient method to the whole class of coherentrisk measures, which is widely accepted in finance and operations research,among other fields. We consider both static and time-consistent dynamic riskmeasures. For static risk measures, our approach is in the spirit of policygradient algorithms and combines a standard sampling approach with convexprogramming. For dynamic risk measures, our approach is actor-critic style andinvolves explicit approximation of value function. Most importantly, ourcontribution presents a unified approach to risk-sensitive reinforcementlearning that generalizes and extends previous results.
arxiv-10800-66 | Convergence Rates of Active Learning for Maximum Likelihood Estimation | http://arxiv.org/pdf/1506.02348v1.pdf | author:Kamalika Chaudhuri, Sham Kakade, Praneeth Netrapalli, Sujay Sanghavi category:cs.LG stat.ML published:2015-06-08 summary:An active learner is given a class of models, a large set of unlabeledexamples, and the ability to interactively query labels of a subset of theseexamples; the goal of the learner is to learn a model in the class that fitsthe data well. Previous theoretical work has rigorously characterized label complexity ofactive learning, but most of this work has focused on the PAC or the agnosticPAC model. In this paper, we shift our attention to a more general setting --maximum likelihood estimation. Provided certain conditions hold on the modelclass, we provide a two-stage active learning algorithm for this problem. Theconditions we require are fairly general, and cover the widely popular class ofGeneralized Linear Models, which in turn, include models for binary andmulti-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and alower bound that matches it up to lower order terms. Our analysis shows thatunlike binary classification in the realizable case, just a single extra roundof interaction is sufficient to achieve near-optimal performance in maximumlikelihood estimation. On the empirical side, the recent work in~\cite{Zhang12} and~\cite{Zhang14} (on active linear and logistic regression)shows the promise of this approach.
arxiv-10800-67 | Wavelets and continuous wavelet transform for autostereoscopic multiview images | http://arxiv.org/pdf/1506.02345v1.pdf | author:Vladimir Saveljev category:cs.CV published:2015-06-08 summary:Recently, the reference functions for the synthesis and analysis of theautostereoscopic multiview and integral images in three-dimensional displays weintroduced. In the current paper, we propose the wavelets to analyze suchimages. The wavelets are built on the reference functions as on the scalingfunctions of the wavelet analysis. The continuous wavelet transform wassuccessfully applied to the testing wireframe binary objects. The restoredlocations correspond to the structure of the testing wireframe binary objects.
arxiv-10800-68 | A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features | http://arxiv.org/pdf/1506.02327v1.pdf | author:Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Yuan-ming Liou, Yen-Chen Wu, Yen-Ju Lu, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG cs.NE published:2015-06-07 summary:This paper summarizes the work done by the authors for the Zero ResourceSpeech Challenge organized in the technical program of Interspeech 2015. Thegoal of the challenge is to discover linguistic units directly from unlabeledspeech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this workautomatically discovers multiple sets of acoustic tokens from the given corpus.Each acoustic token set is specified by a set of hyperparameters that describethe model configuration. These sets of acoustic tokens carry differentcharacteristics of the given corpus and the language behind thus can bemutually reinforced. The multiple sets of token labels are then used as thetargets of a Multi-target DNN (MDNN) trained on low-level acoustic features.Bottleneck features extracted from the MDNN are used as feedback for the MATand the MDNN itself. We call this iterative system the Multi-layered AcousticTokenizing Deep Neural Network (MAT-DNN) which generates high quality featuresfor track 1 of the challenge and acoustic tokens for track 2 of the challenge.
arxiv-10800-69 | A Framework for Constrained and Adaptive Behavior-Based Agents | http://arxiv.org/pdf/1506.02312v1.pdf | author:Renato de Pontes Pereira, Paulo Martins Engel category:cs.AI cs.LG cs.RO cs.SY published:2015-06-07 summary:Behavior Trees are commonly used to model agents for robotics and games,where constrained behaviors must be designed by human experts in order toguarantee that these agents will execute a specific chain of actions given aspecific set of perceptions. In such application areas, learning is a desirablefeature to provide agents with the ability to adapt and improve interactionswith humans and environment, but often discarded due to its unreliability. Inthis paper, we propose a framework that uses Reinforcement Learning nodes aspart of Behavior Trees to address the problem of adding learning capabilitiesin constrained agents. We show how this framework relates to Options inHierarchical Reinforcement Learning, ensuring convergence of nested learningnodes, and we empirically show that the learning nodes do not affect theexecution of other nodes in the tree.
arxiv-10800-70 | SQUINKY! A Corpus of Sentence-level Formality, Informativeness, and Implicature | http://arxiv.org/pdf/1506.02306v1.pdf | author:Shibamouli Lahiri category:cs.CL published:2015-06-07 summary:We introduce a corpus of 7,032 sentences rated by human annotators forformality, informativeness, and implicature on a 1-7 scale. The corpus wasannotated using Amazon Mechanical Turk. Reliability in the obtained judgmentswas examined by comparing mean ratings across two MTurk experiments, andcorrelation with pilot annotations (on sentence formality) conducted in a morecontrolled setting. Despite the subjectivity and inherent difficulty of theannotation task, correlations between mean ratings were quite encouraging,especially on formality and informativeness. We further explored correlationbetween the three linguistic variables, genre-wise variation of ratings andcorrelations within genres, compatibility with automatic stylistic scoring, andsentential make-up of a document in terms of style. To date, our corpus is thelargest sentence-level annotated corpus released for formality,informativeness, and implicature.
arxiv-10800-71 | Winner-Take-All Autoencoders | http://arxiv.org/pdf/1409.2752v2.pdf | author:Alireza Makhzani, Brendan Frey category:cs.LG cs.NE published:2014-09-09 summary:In this paper, we propose a winner-take-all method for learning hierarchicalsparse representations in an unsupervised fashion. We first introducefully-connected winner-take-all autoencoders which use mini-batch statistics todirectly enforce a lifetime sparsity in the activations of the hidden units. Wethen propose the convolutional winner-take-all autoencoder which combines thebenefits of convolutional architectures and autoencoders for learningshift-invariant sparse representations. We describe a way to trainconvolutional autoencoders layer by layer, where in addition to lifetimesparsity, a spatial sparsity within each feature map is achieved usingwinner-take-all activation functions. We will show that winner-take-allautoencoders can be used to to learn deep sparse representations from theMNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,and achieve competitive classification performance.
arxiv-10800-72 | Optimal Ridge Detection using Coverage Risk | http://arxiv.org/pdf/1506.02278v1.pdf | author:Yen-Chi Chen, Christopher R. Genovese, Shirley Ho, Larry Wasserman category:stat.ME stat.ML published:2015-06-07 summary:We introduce the concept of coverage risk as an error measure for densityridge estimation. The coverage risk generalizes the mean integrated squareerror to set estimation. We propose two risk estimators for the coverage riskand we show that we can select tuning parameters by minimizing the estimatedrisk. We study the rate of convergence for coverage risk and prove consistencyof the risk estimators. We apply our method to three simulated datasets and tocosmology data. In all the examples, the proposed method successfully recoverthe underlying density structure.
arxiv-10800-73 | Randomized Structural Sparsity based Support Identification with Applications to Locating Activated or Discriminative Brain Areas: A Multi-center Reproducibility Study | http://arxiv.org/pdf/1506.02265v1.pdf | author:Yilun Wang, Sheng Zhang, Junjie Zheng, Heng Chen, Huafu Chen category:cs.CV 68T01 I.5.4 published:2015-06-07 summary:In this paper, we focus on how to locate the relevant or discriminative brainregions related with external stimulus or certain mental decease, which is alsocalled support identification, based on the neuroimaging data. The maindifficulty lies in the extremely high dimensional voxel space and relativelyfew training samples, easily resulting in an unstable brain region discovery(or called feature selection in context of pattern recognition). When thetraining samples are from different centers and have betweencenter variations,it will be even harder to obtain a reliable and consistent result.Corresponding, we revisit our recently proposed algorithm based on stabilityselection and structural sparsity. It is applied to the multi-center MRI dataanalysis for the first time. A consistent and stable result is achieved acrossdifferent centers despite the between-center data variation while many otherstate-of-the-art methods such as two sample t-test fail. Moreover, we haveempirically showed that the performance of this algorithm is robust andinsensitive to several of its key parameters. In addition, the supportidentification results on both functional MRI and structural MRI areinterpretable and can be the potential biomarkers.
arxiv-10800-74 | Knowledge Transfer Pre-training | http://arxiv.org/pdf/1506.02256v1.pdf | author:Zhiyuan Tang, Dong Wang, Yiqiao Pan, Zhiyong Zhang category:cs.LG cs.NE stat.ML published:2015-06-07 summary:Pre-training is crucial for learning deep neural networks. Most of existingpre-training methods train simple models (e.g., restricted Boltzmann machines)and then stack them layer by layer to form the deep structure. This layer-wisepre-training has found strong theoretical foundation and broad empiricalsupport. However, it is not easy to employ such method to pre-train modelswithout a clear multi-layer structure,e.g., recurrent neural networks (RNNs).This paper presents a new pre-training approach based on knowledge transferlearning. In contrast to the layer-wise approach which trains model componentsincrementally, the new approach trains the entire model as a whole but with aneasier objective function. This is achieved by utilizing soft targets producedby a prior trained model (teacher model). Compared to the conventionallayer-wise methods, this new method does not care about the model structure, socan be used to pre-train very complex models. Experiments on a speechrecognition task demonstrated that with this approach, complex RNNs can be welltrained with a weaker deep neural network (DNN) model. Furthermore, the newmethod can be combined with conventional layer-wise pre-training to deliveradditional gains.
arxiv-10800-75 | Reconstruction of recurrent synaptic connectivity of thousands of neurons from simulated spiking activity | http://arxiv.org/pdf/1502.04993v2.pdf | author:Yury V. Zaytsev, Abigail Morrison, Moritz Deger category:q-bio.NC q-bio.QM stat.ML published:2015-02-17 summary:Dynamics and function of neuronal networks are determined by their synapticconnectivity. Current experimental methods to analyze synaptic networkstructure on the cellular level, however, cover only small fractions offunctional neuronal circuits, typically without a simultaneous record ofneuronal spiking activity. Here we present a method for the reconstruction oflarge recurrent neuronal networks from thousands of parallel spike trainrecordings. We employ maximum likelihood estimation of a generalized linearmodel of the spiking activity in continuous time. For this model the pointprocess likelihood is concave, such that a global optimum of the parameters canbe obtained by gradient ascent. Previous methods, including those of the sameclass, did not allow recurrent networks of that order of magnitude to bereconstructed due to prohibitive computational cost and numericalinstabilities. We describe a minimal model that is optimized for large networksand an efficient scheme for its parallelized numerical optimization on genericcomputing clusters. For a simulated balanced random network of 1000 neurons,synaptic connectivity is recovered with a misclassification error rate of lessthan 1% under ideal conditions. We show that the error rate remains low in aseries of example cases under progressively less ideal conditions. Finally, wesuccessfully reconstruct the connectivity of a hidden synfire chain that isembedded in a random network, which requires clustering of the networkconnectivity to reveal the synfire groups. Our results demonstrate how synapticconnectivity could potentially be inferred from large-scale parallel spiketrain recordings.
arxiv-10800-76 | String Gaussian Process Kernels | http://arxiv.org/pdf/1506.02239v1.pdf | author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML published:2015-06-07 summary:We introduce a new class of nonstationary kernels, which we derive ascovariance functions of a novel family of stochastic processes we refer to asstring Gaussian processes (string GPs). We construct string GPs to allow formultiple types of local patterns in the data, while ensuring a mild globalregularity condition. In this paper, we illustrate the efficacy of the approachusing synthetic data and demonstrate that the model outperforms competingapproaches on well studied, real-life datasets that exhibit nonstationaryfeatures.
arxiv-10800-77 | Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex Losses | http://arxiv.org/pdf/1506.02227v1.pdf | author:Dominik Csiba, Peter Richtárik category:math.OC cs.DS cs.LG stat.ML published:2015-06-07 summary:In this work we develop a new algorithm for regularized empirical riskminimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],which enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.Moreover, our method is able to better utilize the information in the datadefining the ERM problem. For convex loss functions, our complexity resultsmatch those of QUARTZ, which is a primal-dual method also allowing forarbitrary mini-batching schemes. The advantage of a dual-free analysis comesfrom the fact that it guarantees convergence even for non-convex lossfunctions, as long as the average loss is convex. We illustrate throughexperiments the utility of being able to design arbitrary mini-batchingschemes.
arxiv-10800-78 | Randomized Structural Sparsity via Constrained Block Subsampling for Improved Sensitivity of Discriminative Voxel Identification | http://arxiv.org/pdf/1410.4650v2.pdf | author:Yilun Wang, Junjie Zheng, Sheng Zhang, Xujun Duan, Huafu Chen category:cs.CV stat.ML G.3, I.5.2 published:2014-10-17 summary:In this paper, we consider voxel selection for functional Magnetic ResonanceImaging (fMRI) brain data with the aim of finding a more complete set ofprobably correlated discriminative voxels, thus improving interpretation of thediscovered potential biomarkers. The main difficulty in doing this is anextremely high dimensional voxel space and few training samples, resulting inunreliable feature selection. In order to deal with the difficulty, stabilityselection has received a great deal of attention lately, especially due to itsfinite sample control of false discoveries and transparent principle forchoosing a proper amount of regularization. However, it fails to make explicituse of the correlation property or structural information of thesediscriminative features and leads to large false negative rates. In otherwords, many relevant but probably correlated discriminative voxels are missed.Thus, we propose a new variant on stability selection "randomized structuralsparsity", which incorporates the idea of structural sparsity. Numericalexperiments demonstrate that our method can be superior in controlling forfalse negatives while also keeping the control of false positives inheritedfrom stability selection.
arxiv-10800-79 | Boosting Optical Character Recognition: A Super-Resolution Approach | http://arxiv.org/pdf/1506.02211v1.pdf | author:Chao Dong, Ximei Zhu, Yubin Deng, Chen Change Loy, Yu Qiao category:cs.CV I.4.3; I.4.9 published:2015-06-07 summary:Text image super-resolution is a challenging yet open research problem in thecomputer vision community. In particular, low-resolution images hamper theperformance of typical optical character recognition (OCR) systems. In thisarticle, we summarize our entry to the ICDAR2015 Competition on Text ImageSuper-Resolution. Experiments are based on the provided ICDAR2015 TextSRdataset and the released Tesseract-OCR 3.02 system. We report that our winningentry of text image super-resolution framework has largely improved the OCRperformance with low-resolution images used as input, reaching an OCR accuracyscore of 77.19%, which is comparable with that of using the originalhigh-resolution images 78.80%.
arxiv-10800-80 | Describing Common Human Visual Actions in Images | http://arxiv.org/pdf/1506.02203v1.pdf | author:Matteo Ruggero Ronchi, Pietro Perona category:cs.CV published:2015-06-07 summary:Which common human actions and interactions are recognizable in monocularstill images? Which involve objects and/or other people? How many is a personperforming at a time? We address these questions by exploring the actions andinteractions that are detectable in the images of the MS COCO dataset. We maketwo main contributions. First, a list of 140 common `visual actions', obtainedby analyzing the largest on-line verb lexicon currently available for English(VerbNet) and human sentences used to describe images in MS COCO. Second, acomplete set of annotations for those `visual actions', composed ofsubject-object and associated verb, which we call COCO-a (a for `actions').COCO-a is larger than existing action datasets in terms of number of actionsand instances of these actions, and is unique because it is data-driven, ratherthan experimenter-biased. Other unique features are that it is exhaustive, andthat all subjects and objects are localized. A statistical analysis of theaccuracy of our annotations and of each action, interaction and subject-objectcombination is provided.
arxiv-10800-81 | Fast Mixing for Discrete Point Processes | http://arxiv.org/pdf/1506.02194v1.pdf | author:Patrick Rebeschini, Amin Karbasi category:stat.ML math.ST stat.TH published:2015-06-06 summary:We investigate the systematic mechanism for designing fast mixing Markovchain Monte Carlo algorithms to sample from discrete point processes under theDobrushin uniqueness condition for Gibbs measures. Discrete point processes aredefined as probability distributions $\mu(S)\propto \exp(\beta f(S))$ over allsubsets $S\in 2^V$ of a finite set $V$ through a bounded set function$f:2^V\rightarrow \mathbb{R}$ and a parameter $\beta>0$. A subclass of discretepoint processes characterized by submodular functions (which includelog-submodular distributions, submodular point processes, and determinantalpoint processes) has recently gained a lot of interest in machine learning andshown to be effective for modeling diversity and coverage. We show that if theset function (not necessarily submodular) displays a natural notion of decay ofcorrelation, then, for $\beta$ small enough, it is possible to design fastmixing Markov chain Monte Carlo methods that yield error bounds on marginalapproximations that do not depend on the size of the set $V$. The sufficientconditions that we derive involve a control on the (discrete) Hessian of setfunctions, a quantity that has not been previously considered in theliterature. We specialize our results for submodular functions, and we discusscanonical examples where the Hessian can be easily controlled.
arxiv-10800-82 | First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos | http://arxiv.org/pdf/1506.02184v1.pdf | author:Jun Ye, Hao Hu, Kai Li, Guo-Jun Qi, Kien A. Hua category:cs.CV published:2015-06-06 summary:With the prevalence of the commodity depth cameras, the new paradigm of userinterfaces based on 3D motion capturing and recognition have dramaticallychanged the way of interactions between human and computers. Human actionrecognition, as one of the key components in these devices, plays an importantrole to guarantee the quality of user experience. Although the model-drivenmethods have achieved huge success, they cannot provide a scalable solution forefficiently storing, retrieving and recognizing actions in the large-scaleapplications. These models are also vulnerable to the temporal translation andwarping, as well as the variations in motion scales and execution rates. Toaddress these challenges, we propose to treat the 3D human action recognitionas a video-level hashing problem and propose a novel First-Take-All (FTA)Hashing algorithm capable of hashing the entire video into hash codes of fixedlength. We demonstrate that this FTA algorithm produces a compactrepresentation of the video invariant to the above mentioned variations,through which action recognition can be solved by an efficient nearest neighborsearch by the Hamming distance between the FTA hash codes. Experiments on thepublic 3D human action datasets shows that the FTA algorithm can reach arecognition accuracy higher than 80%, with about 15 bits per frame consideringthere are 65 frames per video over the datasets.
arxiv-10800-83 | Hybridized Feature Extraction and Acoustic Modelling Approach for Dysarthric Speech Recognition | http://arxiv.org/pdf/1506.02170v1.pdf | author:Megha Rughani, D. Shivakrishna category:cs.SD cs.CL published:2015-06-06 summary:Dysarthria is malfunctioning of motor speech caused by faintness in the humannervous system. It is characterized by the slurred speech along with physicalimpairment which restricts their communication and creates the lack ofconfidence and affects the lifestyle. This paper attempt to increase theefficiency of Automatic Speech Recognition (ASR) system for unimpaired speechsignal. It describes state of art of research into improving ASR for speakerswith dysarthria by means of incorporated knowledge of their speech production.Hybridized approach for feature extraction and acoustic modelling techniquealong with evolutionary algorithm is proposed for increasing the efficiency ofthe overall system. Here number of feature vectors are varied and tested thesystem performance. It is observed that system performance is boosted bygenetic algorithm. System with 16 acoustic features optimized with geneticalgorithm has obtained highest recognition rate of 98.28% with training time of5:30:17.
arxiv-10800-84 | Riemannian preconditioning for tensor completion | http://arxiv.org/pdf/1506.02159v1.pdf | author:Hiroyuki Kasai, Bamdev Mishra category:cs.NA cs.LG math.OC published:2015-06-06 summary:We propose a novel Riemannian preconditioning approach for the tensorcompletion problem with rank constraint. A Riemannian metric or inner productis proposed that exploits the least-squares structure of the cost function andtakes into account the structured symmetry in Tucker decomposition. Thespecific metric allows to use the versatile framework of Riemannianoptimization on quotient manifolds to develop a preconditioned nonlinearconjugate gradient algorithm for the problem. To this end, concrete matrixrepresentations of various optimization-related ingredients are listed.Numerical comparisons suggest that our proposed algorithm robustly outperformsstate-of-the-art algorithms across different problem instances encompassingvarious synthetic and real-world datasets.
arxiv-10800-85 | ModDrop: adaptive multi-modal gesture recognition | http://arxiv.org/pdf/1501.00102v2.pdf | author:Natalia Neverova, Christian Wolf, Graham W. Taylor, Florian Nebout category:cs.CV cs.HC cs.LG published:2014-12-31 summary:We present a method for gesture detection and localisation based onmulti-scale and multi-modal deep learning. Each visual modality capturesspatial information at a particular spatial scale (such as motion of the upperbody or a hand), and the whole system operates at three temporal scales. Key toour technique is a training strategy which exploits: i) careful initializationof individual modalities; and ii) gradual fusion involving random dropping ofseparate channels (dubbed ModDrop) for learning cross-modality correlationswhile preserving uniqueness of each modality-specific representation. Wepresent experiments on the ChaLearn 2014 Looking at People Challenge gesturerecognition track, in which we placed first out of 17 teams. Fusing multiplemodalities at several spatial and temporal scales leads to a significantincrease in recognition rates, allowing the model to compensate for errors ofthe individual classifiers as well as noise in the separate channels.Futhermore, the proposed ModDrop training technique ensures robustness of theclassifier to missing signals in one or several channels to produce meaningfulpredictions from any number of available modalities. In addition, wedemonstrate the applicability of the proposed fusion scheme to modalities ofarbitrary nature by experiments on the same dataset augmented with audio.
arxiv-10800-86 | A Near-Optimal Dynamic Learning Algorithm for Online Matching Problems with Concave Returns | http://arxiv.org/pdf/1307.5934v3.pdf | author:Xiao Alison Chen, Zizhuo Wang category:cs.DS cs.LG math.OC published:2013-07-23 summary:We consider an online matching problem with concave returns. This problem isa significant generalization of the Adwords allocation problem and has vastapplications in online advertising. In this problem, a sequence of items arrivesequentially and each has to be allocated to one of the bidders, who bid acertain value for each item. At each time, the decision maker has to allocatethe current item to one of the bidders without knowing the future bids and theobjective is to maximize the sum of some concave functions of each bidder'saggregate value. In this work, we propose an algorithm that achievesnear-optimal performance for this problem when the bids arrive in a randomorder and the input data satisfies certain conditions. The key idea of ouralgorithm is to learn the input data pattern dynamically: we solve a sequenceof carefully chosen partial allocation problems and use their optimal solutionsto assist with the future decision. Our analysis belongs to the primal-dualparadigm, however, the absence of linearity of the objective function and thedynamic feature of the algorithm makes our analysis quite unique.
arxiv-10800-87 | On the consistency theory of high dimensional variable screening | http://arxiv.org/pdf/1502.06895v3.pdf | author:Xiangyu Wang, Chenlei Leng, David B. Dunson category:math.ST cs.LG stat.ML stat.TH published:2015-02-24 summary:Variable screening is a fast dimension reduction technique for assisting highdimensional feature selection. As a preselection method, it selects a moderatesize subset of candidate variables for further refining via feature selectionto produce the final model. The performance of variable screening depends onboth computational efficiency and the ability to dramatically reduce the numberof variables without discarding the important ones. When the data dimension $p$is substantially larger than the sample size $n$, variable screening becomescrucial as 1) Faster feature selection algorithms are needed; 2) Conditionsguaranteeing selection consistency might fail to hold. This article studies aclass of linear screening methods and establishes consistency theory for thisspecial class. In particular, we prove the restricted diagonally dominant (RDD)condition is a necessary and sufficient condition for strong screeningconsistency. As concrete examples, we show two screening methods $SIS$ and$HOLP$ are both strong screening consistent (subject to additional constraints)with large probability if $n > O((\rho s + \sigma/\tau)^2\log p)$ under randomdesigns. In addition, we relate the RDD condition to the irrepresentablecondition, and highlight limitations of $SIS$.
arxiv-10800-88 | Learning Multiple Tasks with Deep Relationship Networks | http://arxiv.org/pdf/1506.02117v1.pdf | author:Mingsheng Long, Jianmin Wang category:cs.LG published:2015-06-06 summary:Deep neural networks trained on large-scale dataset can learn transferablefeatures that promote learning multiple tasks for inductive transfer andlabeling mitigation. As deep features eventually transition from general tospecific along the network, a fundamental problem is how to exploit therelationship structure across different tasks while accounting for the featuretransferability in the task-specific layers. In this work, we propose a novelDeep Relationship Network (DRN) architecture for multi-task learning bydiscovering correlated tasks based on multiple task-specific layers of a deepconvolutional neural network. DRN models the task relationship by imposingmatrix normal priors over the network parameters of all task-specific layers,including higher feature layers and classifier layer that are not transferablesafely. By jointly learning the transferable features and task relationships,DRN is able to alleviate the dilemma of negative-transfer in the feature layersand under-transfer in the classifier layer. Empirical evidence shows that DRNyields state-of-the-art classification results on standard multi-domain objectrecognition datasets.
arxiv-10800-89 | Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks Using a Polynomial Number of Score Evaluations | http://arxiv.org/pdf/1506.02113v1.pdf | author:David Maxwell Chickering, Christopher Meek category:cs.LG cs.AI published:2015-06-06 summary:We introduce Selective Greedy Equivalence Search (SGES), a restricted versionof Greedy Equivalence Search (GES). SGES retains the asymptotic correctness ofGES but, unlike GES, has polynomial performance guarantees. In particular, weshow that when data are sampled independently from a distribution that isperfect with respect to a DAG ${\cal G}$ defined over the observable variablesthen, in the limit of large data, SGES will identify ${\cal G}$'s equivalenceclass after a number of score evaluations that is (1) polynomial in the numberof nodes and (2) exponential in various complexity measures includingmaximum-number-of-parents, maximum-clique-size, and a new measure called {\emv-width} that is at least as small as---and potentially much smaller than---theother two. More generally, we show that for any hereditary andequivalence-invariant property $\Pi$ known to hold in ${\cal G}$, we retain thelarge-sample optimality guarantees of GES even if we ignore any GES deletionoperator during the backward phase that results in a state for which $\Pi$ doesnot hold in the common-descendants subgraph.
arxiv-10800-90 | Low-Cost Learning via Active Data Procurement | http://arxiv.org/pdf/1502.05774v2.pdf | author:Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner category:cs.GT cs.AI cs.LG stat.ML J.4; I.2.6 published:2015-02-20 summary:We design mechanisms for online procurement of data held by strategic agentsfor machine learning tasks. The challenge is to use past data to actively pricefuture data and give learning guarantees even when an agent's cost forrevealing her data may depend arbitrarily on the data itself. We achieve thisgoal by showing how to convert a large class of no-regret algorithms intoonline posted-price and learning mechanisms. Our results in a sense parallelclassic sample complexity guarantees, but with the key resource being moneyrather than quantity of data: With a budget constraint $B$, we give robust risk(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use anactive approach, we can often guarantee to do significantly better byleveraging correlations between costs and data. Our algorithms and analysis go through a model of no-regret learning with $T$arriving pairs (cost, data) and a budget constraint of $B$. Our regret boundsfor this model are on the order of $T/\sqrt{B}$ and we give lower bounds on thesame order.
arxiv-10800-91 | Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks | http://arxiv.org/pdf/1502.00702v2.pdf | author:Shiliang Zhang, Hui Jiang category:cs.LG cs.NE published:2015-02-03 summary:In this paper, we propose a novel model for high-dimensional data, called theHybrid Orthogonal Projection and Estimation (HOPE) model, which combines alinear orthogonal projection and a finite mixture model under a unifiedgenerative modeling framework. The HOPE model itself can be learnedunsupervised from unlabelled data based on the maximum likelihood estimation aswell as discriminatively from labelled data. More interestingly, we have shownthe proposed HOPE models are closely related to neural networks (NNs) in asense that each hidden layer can be reformulated as a HOPE model. As a result,the HOPE framework can be used as a novel tool to probe why and how NNs work,more importantly, to learn NNs in either supervised or unsupervised ways. Inthis work, we have investigated the HOPE framework to learn NNs for severalstandard tasks, including image recognition on MNIST and speech recognition onTIMIT. Experimental results have shown that the HOPE framework yieldssignificant performance gains over the current state-of-the-art methods invarious types of NN learning problems, including unsupervised feature learning,supervised or semi-supervised learning.
arxiv-10800-92 | A Hierarchical Neural Autoencoder for Paragraphs and Documents | http://arxiv.org/pdf/1506.01057v2.pdf | author:Jiwei Li, Minh-Thang Luong, Dan Jurafsky category:cs.CL published:2015-06-02 summary:Natural language generation of coherent long texts like paragraphs or longerdocuments is a challenging problem for recurrent networks models. In thispaper, we explore an important step toward this generation task: training anLSTM (Long-short term memory) auto-encoder to preserve and reconstructmulti-sentence paragraphs. We introduce an LSTM model that hierarchicallybuilds an embedding for a paragraph from embeddings for sentences and words,then decodes this embedding to reconstruct the original paragraph. We evaluatethe reconstructed paragraph using standard metrics like ROUGE and Entity Grid,showing that neural models are able to encode texts in a way that preservesyntactic, semantic, and discourse coherence. While only a first step towardgenerating coherent text units from neural models, our work has the potentialto significantly impact natural language generation andsummarization\footnote{Code for the three models described in this paper can befound at www.stanford.edu/~jiweil/ .
arxiv-10800-93 | Global Gene Expression Analysis Using Machine Learning Methods | http://arxiv.org/pdf/1506.02087v1.pdf | author:Min Xu category:q-bio.QM cs.CE cs.LG stat.ML published:2015-06-05 summary:Microarray is a technology to quantitatively monitor the expression of largenumber of genes in parallel. It has become one of the main tools for globalgene expression analysis in molecular biology research in recent years. Thelarge amount of expression data generated by this technology makes the study ofcertain complex biological problems possible and machine learning methods areplaying a crucial role in the analysis process. At present, many machinelearning methods have been or have the potential to be applied to major areasof gene expression analysis. These areas include clustering, classification,dynamic modeling and reverse engineering. In this thesis, we focus our work on using machine learning methods to solvethe classification problems arising from microarray data. We first identify themajor types of the classification problems; then apply several machine learningmethods to solve the problems and perform systematic tests on real andartificial datasets. We propose improvement to existing methods. Specifically,we develop a multivariate and a hybrid feature selection method to obtain highclassification performance for high dimension classification problems. Usingthe hybrid feature selection method, we are able to identify small sets offeatures that give predictive accuracy that is as good as that from othermethods which require many more features.
arxiv-10800-94 | Gene selection for cancer classification using a hybrid of univariate and multivariate feature selection methods | http://arxiv.org/pdf/1506.02085v1.pdf | author:Min Xu, Rudy Setiono category:q-bio.QM cs.CE cs.LG stat.ML published:2015-06-05 summary:Various approaches to gene selection for cancer classification based onmicroarray data can be found in the literature and they may be grouped into twocategories: univariate methods and multivariate methods. Univariate methodslook at each gene in the data in isolation from others. They measure thecontribution of a particular gene to the classification without considering thepresence of the other genes. In contrast, multivariate methods measure therelative contribution of a gene to the classification by taking the other genesin the data into consideration. Multivariate methods select fewer genes ingeneral. However, the selection process of multivariate methods may besensitive to the presence of irrelevant genes, noises in the expression andoutliers in the training data. At the same time, the computational cost ofmultivariate methods is high. To overcome the disadvantages of the two types ofapproaches, we propose a hybrid method to obtain gene sets that are small andhighly discriminative. We devise our hybrid method from the univariate Maximum Likelihood method(LIK) and the multivariate Recursive Feature Elimination method (RFE). Weanalyze the properties of these methods and systematically test theeffectiveness of our proposed method on two cancer microarray datasets. Ourexperiments on a leukemia dataset and a small, round blue cell tumors datasetdemonstrate the effectiveness of our hybrid method. It is able to discover setsconsisting of fewer genes than those reported in the literature and at the sametime achieve the same or better prediction accuracy.
arxiv-10800-95 | Machine Assisted Authentication of Paper Currency: an Experiment on Indian Banknotes | http://arxiv.org/pdf/1401.0689v5.pdf | author:Ankush Roy, Biswajit Halder, Utpal Garain, David S. Doermann category:cs.CV published:2014-01-02 summary:Automatic authentication of paper money has been targeted. Indian bank notesare taken as reference to show how a system can be developed for discriminatingfake notes from genuine ones. Image processing and pattern recognitiontechniques are used to design the overall approach. The ability of the embeddedsecurity aspects is thoroughly analysed for detecting fake currencies. Realforensic samples are involved in the experiment that shows a high precisionmachine can be developed for authentication of paper money. The systemperformance is reported in terms of both accuracy and processing speed.Comparison with human subjects namely forensic experts and bank staffs clearlyshows its applicability for mass checking of currency notes in the real world.The analysis of security features to protect counterfeiting highlights somefacts that should be taken care of in future designing of currency notes.
arxiv-10800-96 | Automatic tracking of protein vesicles | http://arxiv.org/pdf/1506.02083v1.pdf | author:Min Xu category:q-bio.QM cs.CV published:2015-06-05 summary:With the advance of fluorescence imaging technologies, recently cellbiologists are able to record the movement of protein vesicles within a livingcell. Automatic tracking of the movements of these vesicles become key forqualitative analysis of dynamics of theses vesicles. In this thesis, weformulate such tracking problem as video object tracking problem, and design adynamic programming method for tracking single object. Our experiments onsimulation data show that the method can identify a track with high accuracywhich is robust to the choose of tracking parameters and presence of high levelnoise. We then extend this method to the tracking multiple objects using thetrack elimination strategy. In multiple object tracking, the above approachoften fails to correctly identify a track when two tracks cross. We solve thisproblem by incorporating the Kalman filter into the dynamic programmingframework. Our experiments on simulated data show that the tracking accuracy issignificantly improved.
arxiv-10800-97 | Local Nonstationarity for Efficient Bayesian Optimization | http://arxiv.org/pdf/1506.02080v1.pdf | author:Ruben Martinez-Cantin category:cs.LG stat.ML published:2015-06-05 summary:Bayesian optimization has shown to be a fundamental global optimizationalgorithm in many applications: ranging from automatic machine learning,robotics, reinforcement learning, experimental design, simulations, etc. Themost popular and effective Bayesian optimization relies on a surrogate model inthe form of a Gaussian process due to its flexibility to represent a prior overfunction. However, many algorithms and setups relies on the stationarityassumption of the Gaussian process. In this paper, we present a novelnonstationary strategy for Bayesian optimization that is able to outperform thestate of the art in Bayesian optimization both in stationary and nonstationaryproblems.
arxiv-10800-98 | Large-scale Simple Question Answering with Memory Networks | http://arxiv.org/pdf/1506.02075v1.pdf | author:Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston category:cs.LG cs.CL published:2015-06-05 summary:Training large-scale question answering systems is complicated becausetraining sources usually cover a small portion of the range of possiblequestions. This paper studies the impact of multitask and transfer learning forsimple question answering; a setting for which the reasoning required to answeris quite easy, as long as one can retrieve the correct evidence given aquestion, which can be difficult in large-scale conditions. To this end, weintroduce a new dataset of 100k questions that we use in conjunction withexisting benchmarks. We conduct our study within the framework of MemoryNetworks (Weston et al., 2015) because this perspective allows us to eventuallyscale up to more complex reasoning, and show that Memory Networks can besuccessfully trained to achieve excellent performance.
arxiv-10800-99 | Kernel Manifold Alignment | http://arxiv.org/pdf/1504.02338v3.pdf | author:Devis Tuia, Gustau Camps-Valls category:stat.ML cs.LG published:2015-04-09 summary:We introduce a kernel method for manifold alignment (KEMA) and domainadaptation that can match an arbitrary number of data sources without needingcorresponding pairs, just few labeled examples in all domains. KEMA hasinteresting properties: 1) it generalizes other manifold alignment methods, 2)it can align manifolds of very different complexities, performing a sort ofmanifold unfolding plus alignment, 3) it can define a domain-specific metric tocope with multimodal specificities, 4) it can align data spaces of differentdimensionality, 5) it is robust to strong nonlinear feature deformations, and6) it is closed-form invertible which allows transfer across-domains and datasynthesis. We also present a reduced-rank version for computational efficiencyand discuss the generalization performance of KEMA under Rademacher principlesof stability. KEMA exhibits very good performance over competing methods insynthetic examples, visual object recognition and recognition of facialexpressions tasks.
arxiv-10800-100 | Scalable Iterative Algorithm for Robust Subspace Clustering | http://arxiv.org/pdf/1503.01578v2.pdf | author:Sanghyuk Chun, Yung-Kyun Noh, Jinwoo Shin category:cs.DS cs.LG published:2015-03-05 summary:Subspace clustering (SC) is a popular method for dimensionality reduction ofhigh-dimensional data, where it generalizes Principal Component Analysis (PCA).Recently, several methods have been proposed to enhance the robustness of PCAand SC, while most of them are computationally very expensive, in particular,for high dimensional large-scale data. In this paper, we develop much fasteriterative algorithms for SC, incorporating robustness using a {\em non-squared}$\ell_2$-norm objective. The known implementations for optimizing the objectivewould be costly due to the alternative optimization of two separate objectives:optimal cluster-membership assignment and robust subspace selection, while thesubstitution of one process to a faster surrogate can cause failure inconvergence. To address the issue, we use a simplified procedure requiringefficient matrix-vector multiplications for subspace update instead of solvingan expensive eigenvector problem at each iteration, in addition to releasenested robust PCA loops. We prove that the proposed algorithm monotonicallyconverges to a local minimum with approximation guarantees, e.g., it achieves2-approximation for the robust PCA objective. In our experiments, the proposedalgorithm is shown to converge at an order of magnitude faster than knownalgorithms optimizing the same objective, and have outperforms prior subspaceclustering methods in accuracy and running time for MNIST dataset.
arxiv-10800-101 | No More Pesky Learning Rate Guessing Games | http://arxiv.org/pdf/1506.01186v2.pdf | author:Leslie N. Smith category:cs.CV cs.LG cs.NE published:2015-06-03 summary:It is known that the learning rate is the most important hyper-parameter totune for training deep convolutional neural networks (i.e., a "guessing game").This report describes a new method for setting the learning rate, namedcyclical learning rates, that eliminates the need to experimentally find thebest values and schedule for the learning rates. Instead of setting thelearning rate to fixed values, this method lets the learning rate cyclicallyvary within reasonable boundary values. This report shows that training withcyclical learning rates achieves near optimal classification accuracy withouttuning and often in many fewer iterations. This report also describes a simpleway to estimate "reasonable bounds" - by linearly increasing the learning ratein one training run of the network for only a few epochs. In addition, cyclicallearning rates are demonstrated on training with the CIFAR-10 dataset and theAlexNet and GoogLeNet architectures on the ImageNet dataset. These methods arepractical tools for everyone who trains convolutional neural networks.
arxiv-10800-102 | Multilingual Open Relation Extraction Using Cross-lingual Projection | http://arxiv.org/pdf/1503.06450v2.pdf | author:Manaal Faruqui, Shankar Kumar category:cs.CL published:2015-03-22 summary:Open domain relation extraction systems identify relation and argumentphrases in a sentence without relying on any underlying schema. However,current state-of-the-art relation extraction systems are available only forEnglish because of their heavy reliance on linguistic tools such aspart-of-speech taggers and dependency parsers. We present a cross-lingualannotation projection method for language independent relation extraction. Weevaluate our method on a manually annotated test set and present results onthree typologically different languages. We release these manual annotationsand extracted relations in 61 languages from Wikipedia.
arxiv-10800-103 | Sparse Overcomplete Word Vector Representations | http://arxiv.org/pdf/1506.02004v1.pdf | author:Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, Noah Smith category:cs.CL published:2015-06-05 summary:Current distributed representations of words show little resemblance totheories of lexical semantics. The former are dense and uninterpretable, thelatter largely based on familiar, discrete classes (e.g., supersenses) andrelations (e.g., synonymy and hypernymy). We propose methods that transformword vectors into sparse (and optionally binary) vectors. The resultingrepresentations are more similar to the interpretable features typically usedin NLP, though they are discovered automatically from raw corpora. Because thevectors are highly sparse, they are computationally easy to work with. Mostimportantly, we find that they outperform the original vectors on benchmarktasks.
arxiv-10800-104 | Efficient programmable learning to search | http://arxiv.org/pdf/1406.1837v4.pdf | author:Kai-Wei Chang, Hal Daumé III, John Langford, Stephane Ross category:cs.LG published:2014-06-07 summary:We improve "learning to search" approaches to structured prediction in twoways. First, we show that the search space can be defined by an arbitraryimperative program, reducing the number of lines of code required to developnew structured prediction tasks by orders of magnitude. Second, we makestructured prediction orders of magnitude faster through various algorithmicimprovements. We demonstrate the feasibility of our approach on threestructured prediction tasks: two variants of sequence labeling andentity-relation resolution. In all cases we obtain accuracies at least as highas alternative approaches, at drastically reduced execution and programmingtime.
arxiv-10800-105 | JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes | http://arxiv.org/pdf/1503.00332v3.pdf | author:Jonathan H. Huggins, Karthik Narasimhan, Ardavan Saeedi, Vikash K. Mansinghka category:stat.ML cs.LG published:2015-03-01 summary:Markov jump processes (MJPs) are used to model a wide range of phenomena fromdisease progression to RNA path folding. However, maximum likelihood estimationof parametric models leads to degenerate trajectories and inferentialperformance is poor in nonparametric models. We take a small-varianceasymptotics (SVA) approach to overcome these limitations. We derive thesmall-variance asymptotics for parametric and nonparametric MJPs for bothdirectly observed and hidden state models. In the parametric case we obtain anovel objective function which leads to non-degenerate trajectories. To derivethe nonparametric version we introduce the gamma-gamma process, a novelextension to the gamma-exponential process. We propose algorithms for each ofthese formulations, which we call \emph{JUMP-means}. Our experimentsdemonstrate that JUMP-means is competitive with or outperforms widely used MJPinference approaches in terms of both speed and reconstruction accuracy.
arxiv-10800-106 | Abstractive Multi-Document Summarization via Phrase Selection and Merging | http://arxiv.org/pdf/1506.01597v2.pdf | author:Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca J. Passonneau category:cs.CL cs.AI published:2015-06-04 summary:We propose an abstraction-based multi-document summarization framework thatcan construct new sentences by exploring more fine-grained syntactic units thansentences, namely, noun/verb phrases. Different from existing abstraction-basedapproaches, our method first constructs a pool of concepts and factsrepresented by phrases from the input documents. Then new sentences aregenerated by selecting and merging informative phrases to maximize the salienceof phrases and meanwhile satisfy the sentence construction constraints. Weemploy integer linear optimization for conducting phrase selection and mergingsimultaneously in order to achieve the global optimal solution for a summary.Experimental results on the benchmark data set TAC 2011 show that our frameworkoutperforms the state-of-the-art models under automated pyramid evaluationmetric, and achieves reasonably well results on manual linguistic qualityevaluation.
arxiv-10800-107 | BayesPy: Variational Bayesian Inference in Python | http://arxiv.org/pdf/1410.0870v3.pdf | author:Jaakko Luttinen category:stat.ML stat.CO published:2014-10-03 summary:BayesPy is an open-source Python software package for performing variationalBayesian inference. It is based on the variational message passing frameworkand supports conjugate exponential family models. By removing the tedious taskof implementing the variational Bayesian update equations, the user canconstruct models faster and in a less error-prone way. Simple syntax, flexiblemodel construction and efficient inference make BayesPy suitable for bothaverage and expert Bayesian users. It also supports some advanced methods suchas stochastic and collapsed variational inference.
arxiv-10800-108 | MADE: Masked Autoencoder for Distribution Estimation | http://arxiv.org/pdf/1502.03509v2.pdf | author:Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle category:cs.LG cs.NE stat.ML published:2015-02-12 summary:There has been a lot of recent interest in designing neural network models toestimate a distribution from a set of examples. We introduce a simplemodification for autoencoder neural networks that yields powerful generativemodels. Our method masks the autoencoder's parameters to respect autoregressiveconstraints: each input is reconstructed only from previous inputs in a givenordering. Constrained this way, the autoencoder outputs can be interpreted as aset of conditional probabilities, and their product, the full jointprobability. We can also train a single network that can decompose the jointprobability in multiple different orderings. Our simple framework can beapplied to multiple architectures, including deep ones. Vectorizedimplementations, such as on GPUs, are simple and fast. Experiments demonstratethat this approach is competitive with state-of-the-art tractable distributionestimators. At test time, the method is significantly faster and scales betterthan other autoregressive estimators.
arxiv-10800-109 | Content Translation: Computer-assisted translation tool for Wikipedia articles | http://arxiv.org/pdf/1506.01914v1.pdf | author:Niklas Laxström, Pau Giner, Santhosh Thottingal category:cs.CL published:2015-06-05 summary:The quality and quantity of articles in each Wikipedia language variesgreatly. Translating from another Wikipedia is a natural way to add morecontent, but the translation process is not properly supported in the softwareused by Wikipedia. Past computer-assisted translation tools built for Wikipediaare not commonly used. We created a tool that adapts to the specific needs ofan open community and to the kind of content in Wikipedia. Qualitative andquantitative data indicates that the new tool helps users translate articleseasier and faster.
arxiv-10800-110 | Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial Sentiment Analysis | http://arxiv.org/pdf/1506.01906v1.pdf | author:Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-06-05 summary:Although, the fair amount of works in sentiment analysis (SA) and opinionmining (OM) systems in the last decade and with respect to the performance ofthese systems, but it still not desired performance, especially formorphologically-Rich Language (MRL) such as Arabic, due to the complexities andchallenges exist in the nature of the languages itself. One of these challengesis the detection of idioms or proverbs phrases within the writer text orcomment. An idiom or proverb is a form of speech or an expression that ispeculiar to itself. Grammatically, it cannot be understood from the individualmeanings of its elements and can yield different sentiment when treats asseparate words. Consequently, In order to facilitate the task of detection andclassification of lexical phrases for automated SA systems, this paper presentsAIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic(MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentencelevel with semantic orientation (positive or negative). The efforts of manuallybuilding and annotating the lexicon are reported. Moreover, we build aclassifier that extracts idioms and proverbs, phrases from text using n-gramand similarity measure methods. Finally, several experiments were carried outon various data, including Arabic tweets and Arabic microblogs (hotelreservation, product reviews, and TV program comments) from publicly availableArabic online reviews websites (social media, blogs, forums, e-commerce websites) to evaluate the coverage and accuracy of AIPSeLEX.
arxiv-10800-111 | Semidefinite and Spectral Relaxations for Multi-Label Classification | http://arxiv.org/pdf/1506.01829v1.pdf | author:Rémi Lajugie, Piotr Bojanowski, Sylvain Arlot, Francis Bach category:cs.LG published:2015-06-05 summary:In this paper, we address the problem of multi-label classification. Weconsider linear classifiers and propose to learn a prior over the space oflabels to directly leverage the performance of such methods. This prior takesthe form of a quadratic function of the labels and permits to encode bothattractive and repulsive relations between labels. We cast this problem as astructured prediction one aiming at optimizing either the accuracies of thepredictors or the F 1-score. This leads to an optimization problem closelyrelated to the max-cut problem, which naturally leads to semidefinite andspectral relaxations. We show on standard datasets how such a general prior canimprove the performances of multi-label techniques.
arxiv-10800-112 | High-dimensional Ordinary Least-squares Projection for Screening Variables | http://arxiv.org/pdf/1506.01782v1.pdf | author:Xiangyu Wang, Chenlei Leng category:stat.ME math.ST stat.ML stat.TH published:2015-06-05 summary:Variable selection is a challenging issue in statistical applications whenthe number of predictors $p$ far exceeds the number of observations $n$. Inthis ultra-high dimensional setting, the sure independence screening (SIS)procedure was introduced to significantly reduce the dimensionality bypreserving the true model with overwhelming probability, before a refinedsecond stage analysis. However, the aforementioned sure screening propertystrongly relies on the assumption that the important variables in the modelhave large marginal correlations with the response, which rarely holds inreality. To overcome this, we propose a novel and simple screening techniquecalled the high-dimensional ordinary least-squares projection (HOLP). We showthat HOLP possesses the sure screening property and gives consistent variableselection without the strong correlation assumption, and has a lowcomputational complexity. A ridge type HOLP procedure is also discussed.Simulation study shows that HOLP performs competitively compared to many othermarginal correlation based methods. An application to a mammalian eye diseasedata illustrates the attractiveness of HOLP.
arxiv-10800-113 | Text to 3D Scene Generation with Rich Lexical Grounding | http://arxiv.org/pdf/1505.06289v2.pdf | author:Angel Chang, Will Monroe, Manolis Savva, Christopher Potts, Christopher D. Manning category:cs.CL cs.GR published:2015-05-23 summary:The ability to map descriptions of scenes to 3D geometric representations hasmany applications in areas such as art, education, and robotics. However, priorwork on the text to 3D scene generation task has used manually specified objectcategories and language that identifies them. We introduce a dataset of 3Dscenes annotated with natural language descriptions and learn from this datahow to ground textual descriptions to physical objects. Our method successfullygrounds a variety of lexical terms to concrete referents, and we showquantitatively that our method improves 3D scene generation over previous workusing purely rule-based methods. We evaluate the fidelity and plausibility of3D scenes generated with our grounding approach through human judgments. Toease evaluation on this task, we also introduce an automated metric thatstrongly correlates with human judgments.
arxiv-10800-114 | Spectral Learning of Large Structured HMMs for Comparative Epigenomics | http://arxiv.org/pdf/1506.01744v1.pdf | author:Chicheng Zhang, Jimin Song, Kevin C Chen, Kamalika Chaudhuri category:stat.ML cs.LG math.ST q-bio.GN stat.TH published:2015-06-04 summary:We develop a latent variable model and an efficient spectral algorithmmotivated by the recent emergence of very large data sets of chromatin marksfrom multiple human cell types. A natural model for chromatin data in one celltype is a Hidden Markov Model (HMM); we model the relationship between multiplecell types by connecting their hidden states by a fixed tree of knownstructure. The main challenge with learning parameters of such models is thatiterative methods such as EM are very slow, while naive spectral methods resultin time and space complexity exponential in the number of cell types. Weexploit properties of the tree structure of the hidden states to providespectral algorithms that are more computationally efficient for currentbiological datasets. We provide sample complexity bounds for our algorithm andevaluate it experimentally on biological data from nine human cell types.Finally, we show that beyond our specific model, some of our algorithmic ideascan be applied to other graphical models.
arxiv-10800-115 | Visual Causal Feature Learning | http://arxiv.org/pdf/1412.2309v2.pdf | author:Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI cs.CV cs.LG published:2014-12-07 summary:We provide a rigorous definition of the visual cause of a behavior that isbroadly applicable to the visually driven behavior in humans, animals, neurons,robots and other perceiving systems. Our framework generalizes standardaccounts of causal learning to settings in which the causal variables need tobe constructed from micro-variables. We prove the Causal Coarsening Theorem,which allows us to gain causal knowledge from observational data with minimalexperimental effort. The theorem provides a connection to standard inferencetechniques in machine learning that identify features of an image thatcorrelate with, but may not cause, the target behavior. Finally, we propose anactive learning scheme to learn a manipulator function that performs optimalmanipulations on the image to automatically identify the visual cause of atarget behavior. We illustrate our inference and learning algorithms inexperiments based on both synthetic and real data.
arxiv-10800-116 | Monocular SLAM Supported Object Recognition | http://arxiv.org/pdf/1506.01732v1.pdf | author:Sudeep Pillai, John Leonard category:cs.RO cs.CV published:2015-06-04 summary:In this work, we develop a monocular SLAM-aware object recognition systemthat is able to achieve considerably stronger recognition performance, ascompared to classical object recognition systems that function on aframe-by-frame basis. By incorporating several key ideas including multi-viewobject proposals and efficient feature encoding methods, our proposed system isable to detect and robustly recognize objects in its environment using a singleRGB camera in near-constant time. Through experiments, we illustrate theutility of using such a system to effectively detect and recognize objects,incorporating multiple object viewpoint detections into a unified predictionhypothesis. The performance of the proposed recognition system is evaluated onthe UW RGB-D Dataset, showing strong recognition performance and scalablerun-time performance compared to current state-of-the-art recognition systems.
arxiv-10800-117 | The Preference Learning Toolbox | http://arxiv.org/pdf/1506.01709v1.pdf | author:Vincent E. Farrugia, Héctor P. Martínez, Georgios N. Yannakakis category:stat.ML cs.IR cs.LG published:2015-06-04 summary:Preference learning (PL) is a core area of machine learning that handlesdatasets with ordinal relations. As the number of generated data of ordinalnature is increasing, the importance and role of the PL field becomes centralwithin machine learning research and practice. This paper introduces an opensource, scalable, efficient and accessible preference learning toolbox thatsupports the key phases of the data training process incorporating variouspopular data preprocessing, feature selection and preference learning methods.
arxiv-10800-118 | The Long-Short Story of Movie Description | http://arxiv.org/pdf/1506.01698v1.pdf | author:Anna Rohrbach, Marcus Rohrbach, Bernt Schiele category:cs.CV cs.CL published:2015-06-04 summary:Generating descriptions for videos has many applications including assistingblind people and human-robot interaction. The recent advances in imagecaptioning as well as the release of large-scale movie description datasetssuch as MPII Movie Description allow to study this task in more depth. Many ofthe proposed methods for image captioning rely on pre-trained object classifierCNNs and Long-Short Term Memory recurrent networks (LSTMs) for generatingdescriptions. While image description focuses on objects, we argue that it isimportant to distinguish verbs, objects, and places in the challenging settingof movie description. In this work we show how to learn robust visualclassifiers from the weak annotations of the sentence descriptions. Based onthese visual classifiers we learn how to generate a description using an LSTM.We explore different design choices to build and train the LSTM and achieve thebest performance to date on the challenging MPII-MD dataset. We compare andanalyze our approach and prior work along various dimensions to betterunderstand the key challenges of the movie description task.
arxiv-10800-119 | Stochastic Low-Rank Subspace Clustering by Auxiliary Variable Modeling | http://arxiv.org/pdf/1503.08356v2.pdf | author:Jie Shen, Ping Li, Huan Xu category:stat.ML published:2015-03-28 summary:Low-Rank Representation (LRR) has been a popular tool for identifying datagenerated from a union of subspaces. It is also known that LRR iscomputationally challenging. As the size of the nuclear norm regularized matrixof LRR is proportional to $n^2$ (where $n$ is the number of samples), itseriously hinders LRR for large scale problems. In this paper, we develop anovel algorithm to scale up the LRR method accurately and memory efficiently.In particular, we propose an online implementation of LRR that reduces thememory cost from $O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and$d$ being some estimated rank~($d < p \ll n$). Our proposed algorithm consists of two key technical components: (i) wereformulate the nuclear norm to an equivalent matrix factorization form, and(ii) we introduce an auxiliary variable which serves as a basis dictionary ofthe underlying data. Combing these two techniques makes the problem amenable tostochastic optimization. We establish the theoretical guarantee that thesequence of solutions produced by our algorithm converge to a stationary pointof the expected loss function asymptotically. Extensive experiments onsynthetic and realistic datasets further substantiate that our algorithm isfast, robust and memory efficient.
arxiv-10800-120 | Fine-Grained Visual Categorization via Multi-stage Metric Learning | http://arxiv.org/pdf/1402.0453v2.pdf | author:Qi Qian, Rong Jin, Shenghuo Zhu, Yuanqing Lin category:cs.CV cs.LG stat.ML published:2014-02-03 summary:Fine-grained visual categorization (FGVC) is to categorize objects intosubordinate classes instead of basic classes. One major challenge in FGVC isthe co-occurrence of two issues: 1) many subordinate classes are highlycorrelated and are difficult to distinguish, and 2) there exists the largeintra-class variation (e.g., due to object pose). This paper proposes toexplicitly address the above two issues via distance metric learning (DML). DMLaddresses the first issue by learning an embedding so that data points from thesame class will be pulled together while those from different classes should bepushed apart from each other; and it addresses the second issue by allowing theflexibility that only a portion of the neighbors (not all data points) from thesame class need to be pulled together. However, feature representation of animage is often high dimensional, and DML is known to have difficulty in dealingwith high dimensional feature vectors since it would require $\mathcal{O}(d^2)$for storage and $\mathcal{O}(d^3)$ for optimization. To this end, we proposed amulti-stage metric learning framework that divides the large-scale highdimensional learning problem to a series of simple subproblems, achieving$\mathcal{O}(d)$ computational complexity. The empirical study with FVGCbenchmark datasets verifies that our method is both effective and efficientcompared to the state-of-the-art FGVC approaches.
arxiv-10800-121 | Density-Based Diffusion for Soft Clustering | http://arxiv.org/pdf/1406.7130v2.pdf | author:Thomas Bonis, Steve Oudot category:stat.ML published:2014-06-27 summary:In this paper we advocate the use of diffusion processes guided by density toperform soft clustering tasks. Our approach interpolates between classical modeseeking and spectral clustering, being parametrized by a temperature parameter$\beta>0$ controlling the amount of random motion added to the gradient ascent.In practice we simulate the diffusion process in the continuous domain byrandom walks in neighborhood graphs built on the input data. We prove theconvergence of this scheme under mild sampling conditions, and we deriveguarantees for the clustering obtained in terms of the cluster membershipdistributions. Our theoretical results are cooroborated by preliminaryexperiments on manufactured data and on real data.
arxiv-10800-122 | Remarks on pointed digital homotopy | http://arxiv.org/pdf/1503.03016v2.pdf | author:Laurence Boxer, P. Christopher Staecker category:math.CO cs.CV math.GN 55P10, 68R10 I.4.m published:2015-03-10 summary:We present and explore in detail a pair of digital images with$c_u$-adjacencies that are homotopic but not pointed homotopic. For two digitalloops $f,g: [0,m]_Z \rightarrow X$ with the same basepoint, we introduce thenotion of {\em tight at the basepoint (TAB)} pointed homotopy, which is morerestrictive than ordinary pointed homotopy and yields some different results. We present a variant form of the digital fundamental group. Based on what wecall {\em eventually constant} loops, this version of the fundamental group isequivalent to that of Boxer (1999), but offers the advantage that eventuallyconstant maps are often easier to work with than the trivial extensions thatare key to the development of the fundamental group in Boxer (1999) and manysubsequent papers. We show that homotopy equivalent digital images have isomorphic fundamentalgroups, even when the homotopy equivalence does not preserve the basepoint.This assertion appeared in Boxer (2005), but there was an error in the proof;here, we correct the error.
arxiv-10800-123 | Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images | http://arxiv.org/pdf/1506.01596v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2015-06-04 summary:One of the challenges in hyperspectral data analysis is the presence of mixedpixels. Mixed pixels are the result of low spatial resolution of hyperspectralsensors. Spectral unmixing methods decompose a mixed pixel into a set ofendmembers and abundance fractions. Due to nonnegativity constraint onabundance fraction values, NMF based methods are well suited to this problem.In this paper multilayer NMF has been used to improve the results of NMFmethods for spectral unmixing of hyperspectral data under the linear mixingframework. Sparseness constraint on both spectral signatures and abundancefractions matrices are used in this paper. Evaluation of the proposed algorithmis done using synthetic and real datasets in terms of spectral angle andabundance angle distances. Results show that the proposed algorithm outperformsother previously proposed methods.
arxiv-10800-124 | Programs as Polypeptides | http://arxiv.org/pdf/1506.01573v1.pdf | author:Lance R. Williams category:cs.NE cs.ET cs.PL published:2015-06-04 summary:We describe a visual programming language for defining behaviors manifestedby reified actors in a 2D virtual world that can be compiled into programscomprised of sequences of combinators that are themselves reified as actors.This makes it possible to build programs that build programs from components ofa few fixed types delivered by diffusion using processes that resemblechemistry as much as computation.
arxiv-10800-125 | Feature selection and classification of high-dimensional normal vectors with possibly large number of classes | http://arxiv.org/pdf/1506.01567v1.pdf | author:Felix Abramovich, Marianna Pensky category:math.ST stat.ME stat.ML stat.TH published:2015-06-04 summary:We consider high-dimensional multi-class classification of normal vectors,where unlike standard assumptions, the number of classes may be also large. Wederive the (non-asymptotic) conditions on effects of significant features, andthe low and the upper bounds for distances between classes required forsuccessful feature selection and classification with a given accuracy. Inparticular, we present an interesting and, at first glance, somewhatcounter-intuitive phenomenon that the precision of classification can improveas a number of classes grows. This is due to more accurate feature selectionsince even weak significant features, which are not sufficiently strong to bemanifested in a coarse classification, can nevertheless have a strong impactwhen the number of classes is large. The presented simulation study illustratesthe performance of the procedure.
arxiv-10800-126 | A Deep Embedding Model for Co-occurrence Learning | http://arxiv.org/pdf/1504.02824v2.pdf | author:Yelong Shen, Ruoming Jin, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li Deng category:cs.LG published:2015-04-11 summary:Co-occurrence Data is a common and important information source in manyareas, such as the word co-occurrence in the sentences, friends co-occurrencein social networks and products co-occurrence in commercial transaction data,etc, which contains rich correlation and clustering information about theitems. In this paper, we study co-occurrence data using a general energy-basedprobabilistic model, and we analyze three different categories of energy-basedmodel, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capturedifferent levels of dependency in the co-occurrence data. We also discuss howseveral typical existing models are related to these three types of energymodels, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), MatrixFactorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the RestrictedBoltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model(DEM) (an $L_k$ model) from the energy model in a \emph{principled} manner.Furthermore, motivated by the observation that the partition function in theenergy model is intractable and the fact that the major objective of modelingthe co-occurrence data is to predict using the conditional probability, weapply the \emph{maximum pseudo-likelihood} method to learn DEM. In consequence,the developed model and its learning method naturally avoid the abovedifficulties and can be easily used to compute the conditional probability inprediction. Interestingly, our method is equivalent to learning a specialstructured deep neural network using back-propagation and a special samplingstrategy, which makes it scalable on large-scale datasets. Finally, in theexperiments, we show that the DEM can achieve comparable or better results thanstate-of-the-art methods on datasets across several application domains.
arxiv-10800-127 | Jointly Modeling Embedding and Translation to Bridge Video and Language | http://arxiv.org/pdf/1505.01861v3.pdf | author:Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui category:cs.CV cs.MM published:2015-05-07 summary:Automatically describing video content with natural language is a fundamentalchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequencedynamics, has attracted increasing attention on visual interpretation. However,most existing approaches generate a word locally with given previous words andthe visual content, while the relationship between sentence semantics andvisual content is not holistically exploited. As a result, the generatedsentences may be contextually correct but the semantics (e.g., subjects, verbsor objects) are not true. This paper presents a novel unified framework, named Long Short-Term Memorywith visual-semantic Embedding (LSTM-E), which can simultaneously explore thelearning of LSTM and visual-semantic embedding. The former aims to locallymaximize the probability of generating the next word given previous words andvisual content, while the latter is to create a visual-semantic embedding spacefor enforcing the relationship between the semantics of the entire sentence andvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or3-D deep convolutional neural networks for learning powerful videorepresentation, a deep RNN for generating sentences, and a joint embeddingmodel for exploring the relationships between visual content and sentencesemantics. The experiments on YouTube2Text dataset show that our proposedLSTM-E achieves to-date the best reported performance in generating naturalsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We alsodemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)triplets to several state-of-the-art techniques.
arxiv-10800-128 | A Novel Approach Towards Clustering Based Image Segmentation | http://arxiv.org/pdf/1506.01710v1.pdf | author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV published:2015-06-04 summary:In computer vision, image segmentation is always selected as a major researchtopic by researchers. Due to its vital rule in image processing, there alwaysarises the need of a better image segmentation method. Clustering is anunsupervised study with its application in almost every field of science andengineering. Many researchers used clustering in image segmentation process.But still there requires improvement of such approaches. In this paper, a novelapproach for clustering based image segmentation is proposed. Here, we giveimportance on color space and choose lab for this task. The famous hardclustering algorithm K-means is used, but as its performance is dependent onchoosing a proper distance measure, so, we go for cosine distance measure. Thenthe segmented image is filtered with sobel filter. The filtered image isanalyzed with marker watershed algorithm to have the final segmented result ofour original image. The MSE and PSNR values are evaluated to observe theperformance.
arxiv-10800-129 | Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to Color Image Segmentation | http://arxiv.org/pdf/1506.01472v1.pdf | author:Dibya Jyoti Bora, Anil Kumar Gupta, Fayaz Ahmad Khan category:cs.CV published:2015-06-04 summary:Color image segmentation is a very emerging topic for image processingresearch. Since it has the ability to present the result in a way that is muchmore close to the human yes perceive, so todays more research is going on thisarea. Choosing a proper color space is a very important issue for color imagesegmentation process. Generally LAB and HSV are the two frequently chosen colorspaces. In this paper a comparative analysis is performed between these twocolor spaces with respect to color image segmentation. For measuring theirperformance, we consider the parameters: mse and psnr . It is found that HSVcolor space is performing better than LAB.
arxiv-10800-130 | Robust Camera Location Estimation by Convex Programming | http://arxiv.org/pdf/1412.0165v2.pdf | author:Onur Ozyesil, Amit Singer category:cs.CV published:2014-11-29 summary:$3$D structure recovery from a collection of $2$D images requires theestimation of the camera locations and orientations, i.e. the camera motion.For large, irregular collections of images, existing methods for the locationestimation part, which can be formulated as the inverse problem of estimating$n$ locations $\mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_n$ in$\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions$\frac{\mathbf{t}_i - \mathbf{t}_j}{\\mathbf{t}_i - \mathbf{t}_j\}$, aresensitive to outliers in direction measurements. In this paper, we firstlyprovide a complete characterization of well-posed instances of the locationestimation problem, by presenting its relation to the existing theory ofparallel rigidity. For robust estimation of camera locations, we introduce atwo-step approach, comprised of a pairwise direction estimation method robustto outliers in point correspondences between image pairs, and a convex programto maintain robustness to outlier directions. In the presence of partiallycorrupted measurements, we empirically demonstrate that our convex formulationcan even recover the locations exactly. Lastly, we demonstrate the utility ofour formulations through experiments on Internet photo collections.
arxiv-10800-131 | Higher-order Spatial Accuracy in Diffeomorphic Image Registration | http://arxiv.org/pdf/1412.7504v2.pdf | author:Henry O. Jacobs, Stefan Sommer category:cs.CV math.DG math.OC published:2014-12-23 summary:We discretize a cost functional for image registration problems by derivingTaylor expansions for the matching term. Minima of the discretized costfunctionals can be computed with no spatial discretization error, and theoptimal solutions are equivalent to minimal energy curves in the space of$k$-jets. We show that the solutions convergence to optimal solutions of theoriginal cost functional as the number of particles increases with aconvergence rate of $O(h^{d+k})$ where $h$ is a resolution parameter. Theeffect of this approach over traditional particle methods is illustrated onsynthetic examples and real images.
arxiv-10800-132 | Implementation of Training Convolutional Neural Networks | http://arxiv.org/pdf/1506.01195v2.pdf | author:Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang category:cs.CV cs.LG cs.NE published:2015-06-03 summary:Deep learning refers to the shining branch of machine learning that is basedon learning levels of representations. Convolutional Neural Networks (CNN) isone kind of deep neural network. It can study concurrently. In this article, wegave a detailed analysis of the process of CNN algorithm both the forwardprocess and back propagation. Then we applied the particular convolutionalneural network to implement the typical face recognition problem by java. Then,a parallel strategy was proposed in section4. In addition, by measuring theactual time of forward and backward computing, we analysed the maximal speed upand parallel efficiency theoretically.
arxiv-10800-133 | Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM | http://arxiv.org/pdf/1506.01398v1.pdf | author:Jismy Alphonse, Biju V. G. category:cs.CV published:2015-06-03 summary:A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR(Synthetic aperture Radar) image change detection method is proposed in thispaper. It involves three steps: Difference Image (DI) generation by usingGauss-log ratio operator, speckle noise reduction by SRAD (Speckle ReducingAnisotropic Diffusion), and the detection of changed regions by using MRFFCM.The proposed method is compared with existing methods such as FCM and MRFFCMusing simulated and real SAR images. The measures used for evaluation includesOverall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient(KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). Theresults show that the proposed method is better compared to FCM and MRFFCMbased change detection method.
arxiv-10800-134 | A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture of k Gaussians, for any Constant k | http://arxiv.org/pdf/1506.01367v1.pdf | author:Jerry Li, Ludwig Schmidt category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2015-06-03 summary:Learning a Gaussian mixture model (GMM) is a fundamental problem in machinelearning, learning theory, and statistics. One notion of learning a GMM isproper learning: here, the goal is to find a mixture of $k$ Gaussians$\mathcal{M}$ that is close to the density $f$ of the unknown distribution fromwhich we draw samples. The distance between $\mathcal{M}$ and $f$ is typicallymeasured in the total variation or $L_1$-norm. We give an algorithm for learning a mixture of $k$ univariate Gaussians thatis nearly optimal for any fixed $k$. The sample complexity of our algorithm is$\tilde{O}(\frac{k}{\epsilon^2})$ and the running time is $(k \cdot\log\frac{1}{\epsilon})^{O(k^4)} + \tilde{O}(\frac{k}{\epsilon^2})$. It iswell-known that this sample complexity is optimal (up to logarithmic factors),and it was already achieved by prior work. However, the best known timecomplexity for proper learning a $k$-GMM was$\tilde{O}(\frac{1}{\epsilon^{3k-1}})$. In particular, the dependence between$\frac{1}{\epsilon}$ and $k$ was exponential. We significantly improve thisdependence by replacing the $\frac{1}{\epsilon}$ term with a $\log\frac{1}{\epsilon}$ while only increasing the exponent moderately. Hence, forany fixed $k$, the $\tilde{O} (\frac{k}{\epsilon^2})$ term dominates ourrunning time, and thus our algorithm runs in time which is nearly-linear in thenumber of samples drawn. Achieving a running time of $\textrm{poly}(k,\frac{1}{\epsilon})$ for proper learning of $k$-GMMs has recently been statedas an open problem by multiple researchers, and we make progress on thisquestion. Moreover, our approach offers an agnostic learning guarantee: our algorithmreturns a good GMM even if the distribution we are sampling from is not amixture of Gaussians. To the best of our knowledge, our algorithm is the firstagnostic proper learning algorithm for GMMs.
arxiv-10800-135 | Understanding Random Forests: From Theory to Practice | http://arxiv.org/pdf/1407.7502v3.pdf | author:Gilles Louppe category:stat.ML published:2014-07-28 summary:Data analysis and machine learning have become an integrative part of themodern scientific methodology, offering automated procedures for the predictionof a phenomenon based on past observations, unraveling underlying patterns indata and providing insights about the problem. Yet, caution should avoid usingmachine learning as a black-box tool, but rather consider it as a methodology,with a rational thought process that is entirely dependent on the problem understudy. In particular, the use of algorithms should ideally require a reasonableunderstanding of their mechanisms, properties and limitations, in order tobetter apprehend and interpret their results. Accordingly, the goal of this thesis is to provide an in-depth analysis ofrandom forests, consistently calling into question each and every part of thealgorithm, in order to shed new light on its learning capabilities, innerworkings and interpretability. The first part of this work studies theinduction of decision trees and the construction of ensembles of randomizedtrees, motivating their design and purpose whenever possible. Our contributionsfollow with an original complexity analysis of random forests, showing theirgood computational performance and scalability, along with an in-depthdiscussion of their implementation details, as contributed within Scikit-Learn. In the second part of this work, we analyse and discuss the interpretabilityof random forests in the eyes of variable importance measures. The core of ourcontributions rests in the theoretical characterization of the Mean Decrease ofImpurity variable importance measure, from which we prove and derive some ofits properties in the case of multiway totally randomized trees and inasymptotic conditions. In consequence of this work, our analysis demonstratesthat variable importances [...].
arxiv-10800-136 | Celeste: Variational inference for a generative model of astronomical images | http://arxiv.org/pdf/1506.01351v1.pdf | author:Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, Prabhat category:astro-ph.IM stat.ML G.3 published:2015-06-03 summary:We present a new, fully generative model of optical telescope image sets,along with a variational procedure for inference. Each pixel intensity istreated as a Poisson random variable, with a rate parameter dependent on latentproperties of stars and galaxies. Key latent properties are themselves random,with scientific prior distributions constructed from large ancillary data sets.We check our approach on synthetic images. We also run it on images from amajor sky survey, where it exceeds the performance of the currentstate-of-the-art method for locating celestial bodies and measuring theircolors.
arxiv-10800-137 | Bayesian optimization for materials design | http://arxiv.org/pdf/1506.01349v1.pdf | author:Peter I. Frazier, Jialei Wang category:stat.ML math.OC published:2015-06-03 summary:We introduce Bayesian optimization, a technique developed for optimizingtime-consuming engineering simulations and for fitting machine learning modelson large datasets. Bayesian optimization guides the choice of experimentsduring materials design and discovery to find good material designs in as fewexperiments as possible. We focus on the case when materials designs areparameterized by a low-dimensional vector. Bayesian optimization is built on astatistical technique called Gaussian process regression, which allowspredicting the performance of a new design based on previously tested designs.After providing a detailed introduction to Gaussian process regression, weintroduce two Bayesian optimization methods: expected improvement, for designproblems with noise-free evaluations; and the knowledge-gradient method, whichgeneralizes expected improvement and may be used in design problems with noisyevaluations. Both methods are derived using a value-of-information analysis,and enjoy one-step Bayes-optimality.
arxiv-10800-138 | Optimal change point detection in Gaussian processes | http://arxiv.org/pdf/1506.01338v1.pdf | author:Hossein Keshavarz, Clayton Scott, XuanLong Nguyen category:math.ST cs.IT cs.LG math.IT stat.ML stat.TH published:2015-06-03 summary:We study the problem of detecting a change in the mean of one-dimensionalGaussian process data. This problem is investigated in the setting ofincreasing domain (customarily employed in time series analysis) and in thesetting of fixed domain (typically arising in spatial data analysis). Wepropose a detection method based on generalized likelihood ratio test (GLRT),and show that our method achieves asymptotically optimal rate in the minimaxsense, in both settings. The salient feature of the proposed method is that itexploits in an efficient way the data dependence captured by the Gaussianprocess covariance structure. When the covariance is not known, we proposeplug-in GLRT method and derive conditions under which the method remainsasymptotically optimal. By contrast, the standard CUSUM method, which does notaccount for the covariance structure, is shown to be asymptotically optimalonly in the increasing domain. Our algorithms and accompanying theory areapplicable to a wide variety of covariance structures, including the Maternclass, the powered exponential class, and others. The plug-in GLRT method isshown to perform well for a number of covariance estimators, including maximumlikelihood estimators with a dense or a tapered covariance matrix.
arxiv-10800-139 | Unsupervised Feature Analysis with Class Margin Optimization | http://arxiv.org/pdf/1506.01330v1.pdf | author:Sen Wang, Feiping Nie, Xiaojun Chang, Lina Yao, Xue Li, Quan Z. Sheng category:cs.LG published:2015-06-03 summary:Unsupervised feature selection has been always attracting research attentionin the communities of machine learning and data mining for decades. In thispaper, we propose an unsupervised feature selection method seeking a featurecoefficient matrix to select the most distinctive features. Specifically, ourproposed algorithm integrates the Maximum Margin Criterion with asparsity-based model into a joint framework, where the class margin and featurecorrelation are taken into account at the same time. To maximize the total dataseparability while preserving minimized within-class scatter simultaneously, wepropose to embed Kmeans into the framework generating pseudo class labelinformation in a scenario of unsupervised feature selection. Meanwhile, asparsity-based model, ` 2 ,p-norm, is imposed to the regularization term toeffectively discover the sparse structures of the feature coefficient matrix.In this way, noisy and irrelevant features are removed by ruling out thosefeatures whose corresponding coefficients are zeros. To alleviate the localoptimum problem that is caused by random initializations of K-means, aconvergence guaranteed algorithm with an updating strategy for the clusteringindicator matrix, is proposed to iteractively chase the optimal solution.Performance evaluation is extensively conducted over six benchmark data sets.From plenty of experimental results, it is demonstrated that our method hassuperior performance against all other compared approaches.
arxiv-10800-140 | Probabilistic Numerics and Uncertainty in Computations | http://arxiv.org/pdf/1506.01326v1.pdf | author:Philipp Hennig, Michael A Osborne, Mark Girolami category:math.NA cs.AI cs.LG stat.CO stat.ML published:2015-06-03 summary:We deliver a call to arms for probabilistic numerical methods: algorithms fornumerical tasks, including linear algebra, integration, optimization andsolving differential equations, that return uncertainties in theircalculations. Such uncertainties, arising from the loss of precision induced bynumerical calculation with limited time or hardware, are important for muchcontemporary science and industry. Within applications such as climate scienceand astrophysics, the need to make decisions on the basis of computations withlarge and complex data has led to a renewed focus on the management ofnumerical uncertainty. We describe how several seminal classic numericalmethods can be interpreted naturally as probabilistic inference. We then showthat the probabilistic view suggests new algorithms that can flexibly beadapted to suit application specifics, while delivering improved empiricalperformance. We provide concrete illustrations of the benefits of probabilisticnumeric algorithms on real scientific problems from astrometry and astronomicalimaging, while highlighting open problems with these new algorithms. Finally,we describe how probabilistic numerical methods provide a coherent frameworkfor identifying the uncertainty in calculations performed with a combination ofnumerical algorithms (e.g. both numerical optimisers and differential equationsolvers), potentially allowing the diagnosis (and control) of error sources incomputations.
arxiv-10800-141 | Using PCA to Efficiently Represent State Spaces | http://arxiv.org/pdf/1505.00322v2.pdf | author:William Curran, Tim Brys, Matthew Taylor, William Smart category:cs.LG cs.RO published:2015-05-02 summary:Reinforcement learning algorithms need to deal with the exponential growth ofstates and actions when exploring optimal control in high-dimensional spaces.This is known as the curse of dimensionality. By projecting the agent's stateonto a low-dimensional manifold, we can represent the state space in a smallerand more efficient representation. By using this representation duringlearning, the agent can converge to a good policy much faster. We test thisapproach in the Mario Benchmarking Domain. When using dimensionality reductionin Mario, learning converges much faster to a good policy. But, there is acritical convergence-performance trade-off. By projecting onto alow-dimensional manifold, we are ignoring important data. In this paper, weexplore this trade-off of convergence and performance. We find that learning inas few as 4 dimensions (instead of 9), we can improve performance past learningin the full dimensional space at a faster convergence rate.
arxiv-10800-142 | Convex Denoising using Non-Convex Tight Frame Regularization | http://arxiv.org/pdf/1504.00976v2.pdf | author:Ankit Parekh, Ivan W. Selesnick category:cs.CV math.OC published:2015-04-04 summary:This paper considers the problem of signal denoising using a sparsetight-frame analysis prior. The L1 norm has been extensively used as aregularizer to promote sparsity; however, it tends to under-estimate non-zerovalues of the underlying signal. To more accurately estimate non-zero values,we propose the use of a non-convex regularizer, chosen so as to ensureconvexity of the objective function. The convexity of the objective function isensured by constraining the parameter of the non-convex penalty. We use ADMM toobtain a solution and show how to guarantee that ADMM converges to the globaloptimum of the objective function. We illustrate the proposed method for 1D and2D signal denoising.
arxiv-10800-143 | PeakSegJoint: fast supervised peak detection via joint segmentation of multiple count data samples | http://arxiv.org/pdf/1506.01286v1.pdf | author:Toby Dylan Hocking, Guillaume Bourque category:stat.ML q-bio.GN published:2015-06-03 summary:Joint peak detection is a central problem when comparing samples in genomicdata analysis, but current algorithms for this task are unsupervised andlimited to at most 2 sample types. We propose PeakSegJoint, a new constrainedmaximum likelihood segmentation model for any number of sample types. To selectthe number of peaks in the segmentation, we propose a supervised penaltylearning model. To infer the parameters of these two models, we propose to usea discrete optimization heuristic for the segmentation, and convex optimizationfor the penalty learning. In comparisons with state-of-the-art peak detectionalgorithms, PeakSegJoint achieves similar accuracy, faster speeds, and a moreinterpretable model with overlapping peaks that occur in exactly the samepositions across all samples.
arxiv-10800-144 | On the Stability of Deep Networks | http://arxiv.org/pdf/1412.5896v3.pdf | author:Raja Giryes, Guillermo Sapiro, Alex M. Bronstein category:stat.ML cs.IT cs.LG cs.NE math.IT math.MG published:2014-12-18 summary:In this work we study the properties of deep neural networks (DNN) withrandom weights. We formally prove that these networks perform adistance-preserving embedding of the data. Based on this we then drawconclusions on the size of the training data and the networks' structure. Alonger version of this paper with more results and details can be found in(Giryes et al., 2015). In particular, we formally prove in the longer versionthat DNN with random Gaussian weights perform a distance-preserving embeddingof the data, with a special treatment for in-class and out-of-class data.
arxiv-10800-145 | High Dynamic Range Imaging by Perceptual Logarithmic Exposure Merging | http://arxiv.org/pdf/1411.0326v2.pdf | author:Corneliu Florea, Constantin Vertan, Laura Florea category:cs.CV published:2014-11-02 summary:In this paper we emphasize a similarity between the Logarithmic-Type ImageProcessing (LTIP) model and the Naka-Rushton model of the Human Visual System(HVS). LTIP is a derivation of the Logarithmic Image Processing (LIP), whichfurther replaces the logarithmic function with a ratio of polynomial functions.Based on this similarity, we show that it is possible to present an unifyingframework for the High Dynamic Range (HDR) imaging problem, namely thatperforming exposure merging under the LTIP model is equivalent to standardirradiance map fusion. The resulting HDR algorithm is shown to provide highquality in both subjective and objective evaluations.
arxiv-10800-146 | Personalizing a Universal Recurrent Neural Network Language Model with User Characteristic Features by Crowdsouring over Social Networks | http://arxiv.org/pdf/1506.01192v1.pdf | author:Bo-Hsiang Tseng, Hung-Yi Lee, Lin-Shan Lee category:cs.CL cs.LG published:2015-06-03 summary:With the popularity of mobile devices, personalized speech recognizer becomesmore realizable today and highly attractive. Each mobile device is primarilyused by a single user, so it's possible to have a personalized recognizer wellmatching to the characteristics of individual user. Although acoustic modelpersonalization has been investigated for decades, much less work have beenreported on personalizing language model, probably because of the difficultiesin collecting enough personalized corpora. Previous work used the corporacollected from social networks to solve the problem, but constructing apersonalized model for each user is troublesome. In this paper, we propose auniversal recurrent neural network language model with user characteristicfeatures, so all users share the same model, except each with different usercharacteristic features. These user characteristic features can be obtained bycrowdsouring over social networks, which include huge quantity of texts postedby users with known friend relationships, who may share some subject topics andwording patterns. The preliminary experiments on Facebook corpus showed thatthis proposed approach not only drastically reduced the model perplexity, butoffered very good improvement in recognition accuracy in n-best rescoringtests. This approach also mitigated the data sparseness problem forpersonalized language models.
arxiv-10800-147 | Design of a Mobile Face Recognition System for Visually Impaired Persons | http://arxiv.org/pdf/1502.00756v2.pdf | author:Shonal Chaudhry, Rohitash Chandra category:cs.CY cs.CV cs.HC published:2015-02-03 summary:It is estimated that 285 million people globally are visually impaired. Amajority of these people live in developing countries and are among the elderlypopulation. One of the most difficult tasks faced by the visually impaired isidentification of people. While naturally, voice recognition is a common methodof identification, it is an intuitive and difficult process. The rise ofcomputation capability of mobile devices gives motivation to developapplications that can assist visually impaired persons. With the availabilityof mobile devices, these people can be assisted by an additional method ofidentification through intelligent software based on computer visiontechniques. In this paper, we present the design and implementation of a facedetection and recognition system for the visually impaired through the use ofmobile computing. This mobile system is assisted by a server-based supportsystem. The system was tested on a custom video database. Experiment resultsshow high face detection accuracy and promising face recognition accuracy insuitable conditions. The challenges of the system lie in better recognitiontechniques for difficult situations in terms of lighting and weather.
arxiv-10800-148 | Semantic Graph for Zero-Shot Learning | http://arxiv.org/pdf/1406.4112v2.pdf | author:Zhen-Yong Fu, Tao Xiang, Shaogang Gong category:cs.CV cs.LG published:2014-06-16 summary:Zero-shot learning aims to classify visual objects without any training datavia knowledge transfer between seen and unseen classes. This is typicallyachieved by exploring a semantic embedding space where the seen and unseenclasses can be related. Previous works differ in what embedding space is usedand how different classes and a test image can be related. In this paper, weutilize the annotation-free semantic word space for the former and focus onsolving the latter issue of modeling relatedness. Specifically, in contrast toprevious work which ignores the semantic relationships between seen classes andfocus merely on those between seen and unseen classes, in this paper a novelapproach based on a semantic graph is proposed to represent the relationshipsbetween all the seen and unseen class in a semantic word space. Based on thissemantic graph, we design a special absorbing Markov chain process, in whicheach unseen class is viewed as an absorbing state. After incorporating one testimage into the semantic graph, the absorbing probabilities from the test datato each unseen class can be effectively computed; and zero-shot classificationcan be achieved by finding the class label with the highest absorbingprobability. The proposed model has a closed-form solution which is linear withrespect to the number of test images. We demonstrate the effectiveness andcomputational efficiency of the proposed method over the state-of-the-arts onthe AwA (animals with attributes) dataset.
arxiv-10800-149 | A Hybrid Model for Enhancing Lexical Statistical Machine Translation (SMT) | http://arxiv.org/pdf/1506.01171v1.pdf | author:Ahmed G. M. ElSayed, Ahmed S. Salama, Alaa El-Din M. El-Ghazali category:cs.CL published:2015-06-03 summary:The interest in statistical machine translation systems increases currentlydue to political and social events in the world. A proposed Statistical MachineTranslation (SMT) based model that can be used to translate a sentence from thesource Language (English) to the target language (Arabic) automatically throughefficiently incorporating different statistical and Natural Language Processing(NLP) models such as language model, alignment model, phrase based model,reordering model, and translation model. These models are combined to enhancethe performance of statistical machine translation (SMT). Many implementationtools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, andBLEU. Based on the implementation, evaluation of this model, and comparing thegenerated translation with other implemented machine translation systems likeGoogle Translate, it was proved that this proposed model has enhanced theresults of the statistical machine translation, and forms a reliable andefficient model in this field of research.
arxiv-10800-150 | Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree | http://arxiv.org/pdf/1506.01166v1.pdf | author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3 published:2015-06-03 summary:This chapter approaches the image retrieval system on the base of the colorsof image. It creates fuzzy signature to describe the color of image on colorspace HSV and builds fuzzy Hamming distance (FHD) to evaluate the similaritybetween the images. In order to reduce the storage space and speed up thesearch of similar images, it aims to create S-tree to store fuzzy signaturerelies on FHD and builds image retrieval algorithm on S-tree. Then, it providesthe content-based image retrieval (CBIR) and an image retrieval method on FHDand S-tree. Last but not least, based on this theory, it also presents anapplication and experimental assessment of the process of querying similarimage on the database system over 10,000 images.
arxiv-10800-151 | Image Retrieval System Base on EMD Similarity Measure and S-Tree | http://arxiv.org/pdf/1506.01165v1.pdf | author:Thanh Manh Le, Thanh The Van category:cs.CV cs.IR H.2.8; H.3.3 published:2015-06-03 summary:The paper approaches the binary signature for each image based on thepercentage of the pixels in each color images, at the same time the paperbuilds a similar measure between images based on EMD (Earth Mover's Distance).Besides, the paper proceeded to create the S-tree based on the similar measureEMD to store the image's binary signatures to quickly query image signaturedata. From there, the paper build an image retrieval algorithm and CBIR(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.Based on this theory, the paper proceeded to build application and experimentalassessment of the process of querying image on the database system which haveover 10,000 images.
arxiv-10800-152 | Towards Structured Deep Neural Network for Automatic Speech Recognition | http://arxiv.org/pdf/1506.01163v1.pdf | author:Yi-Hsiu Liao, Hung-Yi Lee, Lin-shan Lee category:cs.LG published:2015-06-03 summary:In this paper we propose the Structured Deep Neural Network (Structured DNN)as a structured and deep learning algorithm, learning to find the beststructured object (such as a label sequence) given a structured input (such asa vector sequence) by globally considering the mapping relationships betweenthe structure rather than item by item. When automatic speech recognition is viewed as a special case of such astructured learning problem, where we have the acoustic vector sequence as theinput and the phoneme label sequence as the output, it becomes possible tocomprehensively learned utterance by utterance as a whole, rather than frame byframe. Structured Support Vector Machine (structured SVM) was proposed to performASR with structured learning previously, but limited by the linear nature ofSVM. Here we propose structured DNN to use nonlinear transformations inmulti-layers as a structured and deep learning algorithm. It was shown to beatstructured SVM in preliminary experiments on TIMIT.
arxiv-10800-153 | Do semidefinite relaxations solve sparse PCA up to the information limit? | http://arxiv.org/pdf/1306.3690v4.pdf | author:Robert Krauthgamer, Boaz Nadler, Dan Vilenchik category:math.ST stat.ML stat.TH published:2013-06-16 summary:Estimating the leading principal components of data, assuming they aresparse, is a central task in modern high-dimensional statistics. Manyalgorithms were developed for this sparse PCA problem, from simple diagonalthresholding to sophisticated semidefinite programming (SDP) methods. A keytheoretical question is under what conditions can such algorithms recover thesparse principal components? We study this question for a single-spike modelwith an $\ell_0$-sparse eigenvector, in the asymptotic regime as dimension $p$and sample size $n$ both tend to infinity. Amini and Wainwright [Ann. Statist.37 (2009) 2877-2921] proved that for sparsity levels $k\geq\Omega(n/\log p)$,no algorithm, efficient or not, can reliably recover the sparse eigenvector. Incontrast, for $k\leq O(\sqrt{n/\log p})$, diagonal thresholding is consistent.It was further conjectured that an SDP approach may close this gap betweencomputational and information limits. We prove that when$k\geq\Omega(\sqrt{n})$, the proposed SDP approach, at least in its standardusage, cannot recover the sparse spike. In fact, we conjecture that in thesingle-spike model, no computationally-efficient algorithm can recover a spikeof $\ell_0$-sparsity $k\geq\Omega(\sqrt{n})$. Finally, we present empiricalresults suggesting that up to sparsity levels $k=O(\sqrt{n})$, recovery ispossible by a simple covariance thresholding algorithm.
arxiv-10800-154 | Innovated interaction screening for high-dimensional nonlinear classification | http://arxiv.org/pdf/1501.01029v2.pdf | author:Yingying Fan, Yinfei Kong, Daoji Li, Zemin Zheng category:stat.ML published:2015-01-05 summary:This paper is concerned with the problems of interaction screening andnonlinear classification in a high-dimensional setting. We propose a two-stepprocedure, IIS-SQDA, where in the first step an innovated interaction screening(IIS) approach based on transforming the original $p$-dimensional featurevector is proposed, and in the second step a sparse quadratic discriminantanalysis (SQDA) is proposed for further selecting important interactions andmain effects and simultaneously conducting classification. Our IIS approachscreens important interactions by examining only $p$ features instead of alltwo-way interactions of order $O(p^2)$. Our theory shows that the proposedmethod enjoys sure screening property in interaction selection in thehigh-dimensional setting of $p$ growing exponentially with the sample size. Inthe selection and classification step, we establish a sparse inequality on theestimated coefficient vector for QDA and prove that the classification error ofour procedure can be upper-bounded by the oracle classification error plus somesmaller order term. Extensive simulation studies and real data analysis showthat our proposal compares favorably with existing methods in interactionselection and high-dimensional classification.
arxiv-10800-155 | Understanding deep features with computer-generated imagery | http://arxiv.org/pdf/1506.01151v1.pdf | author:Mathieu Aubry, Bryan Russell category:cs.CV published:2015-06-03 summary:We introduce an approach for analyzing the variation of features generated byconvolutional neural networks (CNNs) with respect to scene factors that occurin natural images. Such factors may include object style, 3D viewpoint, color,and scene lighting configuration. Our approach analyzes CNN feature responsescorresponding to different scene factors by controlling for them via renderingusing a large database of 3D CAD models. The rendered images are presented to atrained CNN and responses for different layers are studied with respect to theinput scene factors. We perform a decomposition of the responses based onknowledge of the input scene factors and analyze the resulting components. Inparticular, we quantify their relative importance in the CNN responses andvisualize them using principal component analysis. We show qualitative andquantitative results of our study on three CNNs trained on large imagedatasets: AlexNet, Places, and Oxford VGG. We observe important differencesacross the networks and CNN layers for different scene factors and objectcategories. Finally, we demonstrate that our analysis based oncomputer-generated imagery translates to the network representation of naturalimages.
arxiv-10800-156 | Robust and computationally feasible community detection in the presence of arbitrary outlier nodes | http://arxiv.org/pdf/1404.6000v4.pdf | author:T. Tony Cai, Xiaodong Li category:math.ST cs.IT math.IT math.OC stat.ML stat.TH published:2014-04-23 summary:Community detection, which aims to cluster $N$ nodes in a given graph into$r$ distinct groups based on the observed undirected edges, is an importantproblem in network data analysis. In this paper, the popular stochastic blockmodel (SBM) is extended to the generalized stochastic block model (GSBM) thatallows for adversarial outlier nodes, which are connected with the other nodesin the graph in an arbitrary way. Under this model, we introduce a procedureusing convex optimization followed by $k$-means algorithm with $k=r$. Boththeoretical and numerical properties of the method are analyzed. A theoreticalguarantee is given for the procedure to accurately detect the communities withsmall misclassification rate under the setting where the number of clusters cangrow with $N$. This theoretical result admits to the best-known result in theliterature of computationally feasible community detection in SBM withoutoutliers. Numerical results show that our method is both computationally fastand robust to different kinds of outliers, while some popular computationallyfast community detection algorithms, such as spectral clustering applied toadjacency matrices or graph Laplacians, may fail to retrieve the major clustersdue to a small portion of outliers. We apply a slight modification of ourmethod to a political blogs data set, showing that our method is competent inpractice and comparable to existing computationally feasible methods in theliterature. To the best of the authors' knowledge, our result is the first inthe literature in terms of clustering communities with fast growing numbersunder the GSBM where a portion of arbitrary outlier nodes exist.
arxiv-10800-157 | Unsupervised domain adaption dictionary learning for visual recognition | http://arxiv.org/pdf/1506.01125v1.pdf | author:Zhun Zhong, Zongmin Li, Runlin Li, Xiaoxia Sun category:cs.CV published:2015-06-03 summary:Over the last years, dictionary learning method has been extensively appliedto deal with various computer vision recognition applications, and producedstate-of-the-art results. However, when the data instances of a target domainhave a different distribution than that of a source domain, the dictionarylearning method may fail to perform well. In this paper, we address thecross-domain visual recognition problem and propose a simple but effectiveunsupervised domain adaption approach, where labeled data are only from sourcedomain. In order to bring the original data in source and target domain intothe same distribution, the proposed method forcing nearest coupled data betweensource and target domain to have identical sparse representations while jointlylearning dictionaries for each domain, where the learned dictionaries canreconstruct original data in source and target domain respectively. So thatsparse representations of original data can be used to perform visualrecognition tasks. We demonstrate the effectiveness of our approach on standarddatasets. Our method performs on par or better than competitivestate-of-the-art methods.
arxiv-10800-158 | Asymptotic normality and optimalities in estimation of large Gaussian graphical models | http://arxiv.org/pdf/1309.6024v3.pdf | author:Zhao Ren, Tingni Sun, Cun-Hui Zhang, Harrison H. Zhou category:math.ST stat.ME stat.ML stat.TH published:2013-09-24 summary:The Gaussian graphical model, a popular paradigm for studying relationshipamong variables in a wide range of applications, has attracted great attentionin recent years. This paper considers a fundamental question: When is itpossible to estimate low-dimensional parameters at parametric square-root ratein a large Gaussian graphical model? A novel regression approach is proposed toobtain asymptotically efficient estimation of each entry of a precision matrixunder a sparseness condition relative to the sample size. When the precisionmatrix is not sufficiently sparse, or equivalently the sample size is notsufficiently large, a lower bound is established to show that it is no longerpossible to achieve the parametric rate in the estimation of each entry. Thislower bound result, which provides an answer to the delicate sample sizequestion, is established with a novel construction of a subset of sparseprecision matrices in an application of Le Cam's lemma. Moreover, the proposedestimator is proven to have optimal convergence rate when the parametric ratecannot be achieved, under a minimal sample requirement. The proposed estimatoris applied to test the presence of an edge in the Gaussian graphical model orto recover the support of the entire model, to obtain adaptive rate-optimalestimation of the entire precision matrix as measured by the matrix $\ell_q$operator norm and to make inference in latent variables in the graphical model.All of this is achieved under a sparsity condition on the precision matrix anda side condition on the range of its spectrum. This significantly relaxes thecommonly imposed uniform signal strength condition on the precision matrix,irrepresentability condition on the Hessian tensor operator of the covariancematrix or the $\ell_1$ constraint on the precision matrix. Numerical resultsconfirm our theoretical findings. The ROC curve of the proposed algorithm,Asymptotic Normal Thresholding (ANT), for support recovery significantlyoutperforms that of the popular GLasso algorithm.
arxiv-10800-159 | Compound Rank-k Projections for Bilinear Analysis | http://arxiv.org/pdf/1411.6231v3.pdf | author:Xiaojun Chang, Feiping Nie, Sen Wang, Yi Yang, Xiaofang Zhou, Chengqi Zhang category:cs.LG published:2014-11-23 summary:In many real-world applications, data are represented by matrices orhigh-order tensors. Despite the promising performance, the existingtwo-dimensional discriminant analysis algorithms employ a single projectionmodel to exploit the discriminant information for projection, making the modelless flexible. In this paper, we propose a novel Compound Rank-k Projection(CRP) algorithm for bilinear analysis. CRP deals with matrices directly withouttransforming them into vectors, and it therefore preserves the correlationswithin the matrix and decreases the computation complexity. Different from theexisting two dimensional discriminant analysis algorithms, objective functionvalues of CRP increase monotonically.In addition, CRP utilizes multiple rank-kprojection models to enable a larger search space in which the optimal solutioncan be found. In this way, the discriminant ability is enhanced.
arxiv-10800-160 | Hyperspectral Image Classification and Clutter Detection via Multiple Structural Embeddings and Dimension Reductions | http://arxiv.org/pdf/1506.01115v1.pdf | author:Alexandros-Stavros Iliopoulos, Tiancheng Liu, Xiaobai Sun category:cs.CV published:2015-06-03 summary:We present a new and effective approach for Hyperspectral Image (HSI)classification and clutter detection, overcoming a few long-standing challengespresented by HSI data characteristics. Residing in a high-dimensional spectralattribute space, HSI data samples are known to be strongly correlated in theirspectral signatures, exhibit nonlinear structure due to several physical laws,and contain uncertainty and noise from multiple sources. In the presentedapproach, we generate an adaptive, structurally enriched representationenvironment, and employ the locally linear embedding (LLE) in it. There are twostructure layers external to LLE. One is feature space embedding: the HSI dataattributes are embedded into a discriminatory feature space wherespatio-spectral coherence and distinctive structures are distilled andexploited to mitigate various difficulties encountered in the nativehyperspectral attribute space. The other structure layer encloses the ranges ofalgorithmic parameters for LLE and feature embedding, and supports amultiplexing and integrating scheme for contending with multi-sourceuncertainty. Experiments on two commonly used HSI datasets with a small numberof learning samples have rendered remarkably high-accuracy classificationresults, as well as distinctive maps of detected clutter regions.
arxiv-10800-161 | Normal Bandits of Unknown Means and Variances: Asymptotic Optimality, Finite Horizon Regret Bounds, and a Solution to an Open Problem | http://arxiv.org/pdf/1504.05823v2.pdf | author:Wesley Cowan, Junya Honda, Michael N. Katehakis category:stat.ML cs.LG published:2015-04-22 summary:Consider the problem of sampling sequentially from a finite number of $N \geq2$ populations, specified by random variables $X^i_k$, $ i = 1,\ldots , N,$ and$k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from population $i$ the$k^{th}$ time it is sampled. It is assumed that for each fixed $i$, $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. normal random variables,with unknown mean $\mu_i$ and unknown variance $\sigma_i^2$. The objective is to have a policy $\pi$ for deciding from which of the $N$populations to sample form at any time $n=1,2,\ldots$ so as to maximize theexpected sum of outcomes of $n$ samples or equivalently to minimize the regretdue to lack on information of the parameters $\mu_i$ and $\sigma_i^2$. In thispaper, we present a simple inflated sample mean (ISM) index policy that isasymptotically optimal in the sense of Theorem 4 below. This resolves astanding open problem from Burnetas and Katehakis (1996). Additionally, finitehorizon regret bounds are given.
arxiv-10800-162 | Multi-view Machines | http://arxiv.org/pdf/1506.01110v1.pdf | author:Bokai Cao, Hucheng Zhou, Philip S. Yu category:cs.LG stat.ML H.2.8 published:2015-06-03 summary:For a learning task, data can usually be collected from different sources orbe represented from multiple views. For example, laboratory results fromdifferent medical examinations are available for disease diagnosis, and each ofthem can only reflect the health state of a person from a particularaspect/view. Therefore, different views provide complementary information forlearning tasks. An effective integration of the multi-view information isexpected to facilitate the learning performance. In this paper, we propose ageneral predictor, named multi-view machines (MVMs), that can effectivelyinclude all the possible interactions between features from multiple views. Ajoint factorization is embedded for the full-order interaction parameters whichallows parameter estimation under sparsity. Moreover, MVMs can work inconjunction with different loss functions for a variety of machine learningtasks. A stochastic gradient descent method is presented to learn the MVMmodel. We further illustrate the advantages of MVMs through comparison withother methods for multi-view classification, including support vector machines(SVMs), support tensor machines (STMs) and factorization machines (FMs).
arxiv-10800-163 | CIDEr: Consensus-based Image Description Evaluation | http://arxiv.org/pdf/1411.5726v2.pdf | author:Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh category:cs.CV cs.CL cs.IR published:2014-11-20 summary:Automatically describing an image with a sentence is a long-standingchallenge in computer vision and natural language processing. Due to recentprogress in object detection, attribute classification, action recognition,etc., there is renewed interest in this area. However, evaluating the qualityof descriptions has proven to be challenging. We propose a novel paradigm forevaluating image descriptions that uses human consensus. This paradigm consistsof three main parts: a new triplet-based method of collecting human annotationsto measure consensus, a new automated metric (CIDEr) that captures consensus,and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentencesdescribing each image. Our simple metric captures human judgment of consensusbetter than existing metrics across sentences generated by various sources. Wealso evaluate five state-of-the-art image description approaches using this newprotocol and provide a benchmark for future comparisons. A version of CIDErnamed CIDEr-D is available as a part of MS COCO evaluation server to enablesystematic evaluation and benchmarking.
arxiv-10800-164 | Bayesian Hierarchical Clustering with Exponential Family: Small-Variance Asymptotics and Reducibility | http://arxiv.org/pdf/1501.07430v2.pdf | author:Juho Lee, Seungjin Choi category:stat.ML cs.LG published:2015-01-29 summary:Bayesian hierarchical clustering (BHC) is an agglomerative clustering method,where a probabilistic model is defined and its marginal likelihoods areevaluated to decide which clusters to merge. While BHC provides a fewadvantages over traditional distance-based agglomerative clustering algorithms,successive evaluation of marginal likelihoods and careful hyperparameter tuningare cumbersome and limit the scalability. In this paper we relax BHC into anon-probabilistic formulation, exploring small-variance asymptotics inconjugate-exponential models. We develop a novel clustering algorithm, referredto as relaxed BHC (RBHC), from the asymptotic limit of the BHC model thatexhibits the scalability of distance-based agglomerative clustering algorithmsas well as the flexibility of Bayesian nonparametric models. We alsoinvestigate the reducibility of the dissimilarity measure emerged from theasymptotic limit of the BHC model, allowing us to use scalable algorithms suchas the nearest neighbor chain algorithm. Numerical experiments on bothsynthetic and real-world datasets demonstrate the validity and high performanceof our method.
arxiv-10800-165 | Bilinear Random Projections for Locality-Sensitive Binary Codes | http://arxiv.org/pdf/1506.01092v1.pdf | author:Saehoon Kim, Seungjin Choi category:cs.CV published:2015-06-03 summary:Locality-sensitive hashing (LSH) is a popular data-independent indexingmethod for approximate similarity search, where random projections followed byquantization hash the points from the database so as to ensure that theprobability of collision is much higher for objects that are close to eachother than for those that are far apart. Most of high-dimensional visualdescriptors for images exhibit a natural matrix structure. When visualdescriptors are represented by high-dimensional feature vectors and long binarycodes are assigned, a random projection matrix requires expensive complexitiesin both space and time. In this paper we analyze a bilinear random projectionmethod where feature matrices are transformed to binary codes by two smallerrandom projection matrices. We base our theoretical analysis on extendingRaginsky and Lazebnik's result where random Fourier features are composed withrandom binary quantizers to form locality sensitive binary codes. To this end,we answer the following two questions: (1) whether a bilinear random projectionalso yields similarity-preserving binary codes; (2) whether a bilinear randomprojection yields performance gain or loss, compared to a large linearprojection. Regarding the first question, we present upper and lower bounds onthe expected Hamming distance between binary codes produced by bilinear randomprojections. In regards to the second question, we analyze the upper and lowerbounds on covariance between two bits of binary codes, showing that thecorrelation between two bits is small. Numerical experiments on MNIST andFlickr45K datasets confirm the validity of our method.
arxiv-10800-166 | Probabilistic Network Metrics: Variational Bayesian Network Centrality | http://arxiv.org/pdf/1409.4141v2.pdf | author:Harold Soh category:stat.ML published:2014-09-15 summary:Network metrics form a fundamental part of the network analysis toolbox. Usedto quantitatively measure different aspects of the network, these metrics cangive insights into the underlying network structure and function. In this work,we connect network metrics to modern probabilistic machine learning. We focuson the centrality metric, which is used a wide variety of applications from websearch to gene-analysis. First, we formulate an eigenvector-based Bayesiancentrality model for determining node importance. Compared to existing methods,our probabilistic model allows for the assimilation of multiple edge weightobservations, the inclusion of priors and the extraction of uncertainties. Toenable tractable inference, we develop a variational lower bound (VBC) that isdemonstrated to be effective on a variety of networks (two synthetic and fivereal-world graphs). We then bridge this model to sparse Gaussian processes. Thesparse variational Bayesian centrality Gaussian process (VBC-GP) learns amapping between node attributes to latent centrality and hence, is capable ofpredicting centralities from node features and can potentially represent alarge number of nodes using only a limited number of inducing inputs.Experiments show that the VBC-GP learns high-quality mappings and comparesfavorably to a two-step baseline, i.e., a full GP trained on the nodeattributes and pre-computed centralities. Finally, we present two case-studiesusing the VBC-GP: first, to ascertain relevant features in a taxi transportnetwork and second, to distribute a limited number of vaccines to mitigate theseverity of a viral outbreak.
arxiv-10800-167 | On bicluster aggregation and its benefits for enumerative solutions | http://arxiv.org/pdf/1506.01077v1.pdf | author:Saullo Haniell Galvão de Oliveira, Rosana Veroneze, Fernando José Von Zuben category:cs.LG published:2015-06-02 summary:Biclustering involves the simultaneous clustering of objects and theirattributes, thus defining local two-way clustering models. Recently, efficientalgorithms were conceived to enumerate all biclusters in real-valued datasets.In this case, the solution composes a complete set of maximal and non-redundantbiclusters. However, the ability to enumerate biclusters revealed a challengingscenario: in noisy datasets, each true bicluster may become highly fragmentedand with a high degree of overlapping. It prevents a direct analysis of theobtained results. To revert the fragmentation, we propose here two approachesfor properly aggregating the whole set of enumerated biclusters: one based onsingle linkage and the other directly exploring the rate of overlapping. Bothproposals were compared with each other and with the actual state-of-the-art inseveral experiments, and they not only significantly reduced the number ofbiclusters but also consistently increased the quality of the solution.
arxiv-10800-168 | Multi-stage Multi-task feature learning via adaptive threshold | http://arxiv.org/pdf/1406.4465v2.pdf | author:Yaru Fan, Yilun Wang category:cs.LG cs.CV stat.ML 68T10 F.2.2 published:2014-06-16 summary:Multi-task feature learning aims to identity the shared features among tasksto improve generalization. It has been shown that by minimizing non-convexlearning models, a better solution than the convex alternatives can beobtained. Therefore, a non-convex model based on the capped-$\ell_{1},\ell_{1}$regularization was proposed in \cite{Gong2013}, and a corresponding efficientmulti-stage multi-task feature learning algorithm (MSMTFL) was presented.However, this algorithm harnesses a prescribed fixed threshold in thedefinition of the capped-$\ell_{1},\ell_{1}$ regularization and the lack ofadaptivity might result in suboptimal performance. In this paper we propose toemploy an adaptive threshold in the capped-$\ell_{1},\ell_{1}$ regularizedformulation, where the corresponding variant of MSMTFL will incorporate anadditional component to adaptively determine the threshold value. This variantis expected to achieve a better feature selection performance over the originalMSMTFL algorithm. In particular, the embedded adaptive threshold componentcomes from our previously proposed iterative support detection (ISD) method\cite{Wang2010}. Empirical studies on both synthetic and real-world data setsdemonstrate the effectiveness of this new variant over the original MSMTFL.
arxiv-10800-169 | Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases | http://arxiv.org/pdf/1506.00999v1.pdf | author:Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier, Yves Grandvalet category:cs.AI cs.CL cs.LG published:2015-06-02 summary:This paper tackles the problem of endogenous link prediction for KnowledgeBase completion. Knowledge Bases can be represented as directed graphs whosenodes correspond to entities and edges to relationships. Previous attemptseither consist of powerful systems with high capacity to model complexconnectivity patterns, which unfortunately usually end up overfitting on rarerelationships, or in approaches that trade capacity for simplicity in order tofairly model all relationships, frequent or not. In this paper, we proposeTatec a happy medium obtained by complementing a high-capacity model with asimpler one, both pre-trained separately and then combined. We present severalvariants of this model with different kinds of regularization and combinationstrategies and show that this approach outperforms existing methods ondifferent types of relationships by achieving state-of-the-art results on fourbenchmarks of the literature.
arxiv-10800-170 | Two step recovery of jointly sparse and low-rank matrices: theoretical guarantees | http://arxiv.org/pdf/1412.2669v2.pdf | author:Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob category:stat.ML cs.IT math.IT published:2014-12-05 summary:We introduce a two step algorithm with theoretical guarantees to recover ajointly sparse and low-rank matrix from undersampled measurements of itscolumns. The algorithm first estimates the row subspace of the matrix using aset of common measurements of the columns. In the second step, the subspaceaware recovery of the matrix is solved using a simple least square algorithm.The results are verified in the context of recovering CINE data fromundersampled measurements; we obtain good recovery when the sampling conditionsare satisfied.
arxiv-10800-171 | Subspace based low rank and joint sparse matrix recovery | http://arxiv.org/pdf/1412.2700v2.pdf | author:Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob category:cs.NA cs.CV published:2014-12-05 summary:We consider the recovery of a low rank and jointly sparse matrix from undersampled measurements of its columns. This problem is highly relevant in therecovery of dynamic MRI data with high spatio-temporal resolution, where eachcolumn of the matrix corresponds to a frame in the image time series; thematrix is highly low-rank since the frames are highly correlated. Similarly thenon-zero locations of the matrix in appropriate transform/frame domains (e.g.wavelet, gradient) are roughly the same in different frame. The superset of thesupport can be safely assumed to be jointly sparse. Unlike the classicalmultiple measurement vector (MMV) setup that measures all the snapshots usingthe same matrix, we consider each snapshot to be measured using a differentmeasurement matrix. We show that this approach reduces the total number ofmeasurements, especially when the rank of the matrix is much smaller than thanits sparsity. Our experiments in the context of dynamic imaging shows that thisapproach is very useful in realizing free breathing cardiac MRI.
arxiv-10800-172 | Discovering Valuable Items from Massive Data | http://arxiv.org/pdf/1506.00935v1.pdf | author:Hastagiri P. Vanchinathan, Andreas Marfurt, Charles-Antoine Robelin, Donald Kossmann, Andreas Krause category:cs.LG cs.AI cs.IT math.IT H.2.8 published:2015-06-02 summary:Suppose there is a large collection of items, each with an associated costand an inherent utility that is revealed only once we commit to selecting it.Given a budget on the cumulative cost of the selected items, how can we pick asubset of maximal value? This task generalizes several important problems suchas multi-arm bandits, active search and the knapsack problem. We present analgorithm, GP-Select, which utilizes prior knowledge about similarity be- tweenitems, expressed as a kernel function. GP-Select uses Gaussian processprediction to balance exploration (estimating the unknown value of items) andexploitation (selecting items of high value). We extend GP-Select to be able todiscover sets that simultaneously have high utility and are diverse. Ourpreference for diversity can be specified as an arbitrary monotone submodularfunction that quantifies the diminishing returns obtained when selectingsimilar items. Furthermore, we exploit the structure of the model updates toachieve an order of magnitude (up to 40X) speedup in our experiments withoutresorting to approximations. We provide strong guarantees on the performance ofGP-Select and apply it to three real-world case studies of industrialrelevance: (1) Refreshing a repository of prices in a Global DistributionSystem for the travel industry, (2) Identifying diverse, binding-affinepeptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scalerecommender system by recommending items to users.
arxiv-10800-173 | Facial Expressions Tracking and Recognition: Database Protocols for Systems Validation and Evaluation | http://arxiv.org/pdf/1506.00925v1.pdf | author:Catarina Runa Miranda, Pedro Mendes, Pedro Coelho, Xenxo Alvarez, João Freitas, Miguel Sales Dias, Verónica Costa Orvalho category:cs.CV published:2015-06-02 summary:Each human face is unique. It has its own shape, topology, and distinguishingfeatures. As such, developing and testing facial tracking systems arechallenging tasks. The existing face recognition and tracking algorithms inComputer Vision mainly specify concrete situations according to particulargoals and applications, requiring validation methodologies with data that fitstheir purposes. However, a database that covers all possible variations ofexternal and factors does not exist, increasing researchers' work in acquiringtheir own data or compiling groups of databases. To address this shortcoming, we propose a methodology for facial dataacquisition through definition of fundamental variables, such as subjectcharacteristics, acquisition hardware, and performance parameters. Followingthis methodology, we also propose two protocols that allow the capturing offacial behaviors under uncontrolled and real-life situations. As validation, weexecuted both protocols which lead to creation of two sample databases: FdMiee(Facial database with Multi input, expressions, and environments) and FACIA(Facial Multimodal database driven by emotional induced acting). Using different types of hardware, FdMiee captures facial information underenvironmental and facial behaviors variations. FACIA is an extension of FdMieeintroducing a pipeline to acquire additional facial behaviors and speech usingan emotion-acting method. Therefore, this work eases the creation of adaptabledatabase according to algorithm's requirements and applications, leading tosimplified validation and testing processes.
arxiv-10800-174 | The Influence of Context on Dialogue Act Recognition | http://arxiv.org/pdf/1506.00839v1.pdf | author:Eugénio Ribeiro, Ricardo Ribeiro, David Martins de Matos category:cs.CL published:2015-06-02 summary:This article presents a deep analysis of the influence of context informationon dialogue act recognition. We performed experiments on the annotated subsetsof three different corpora: the widely explored Switchboard Dialog Act Corpus,as well as the unexplored LEGO and Cambridge Restaurant corpora. In contrast with previous work, especially in what concerns the Switchboardcorpus, we used an event-based classification approach, using SVMs, instead ofthe more common sequential approaches, such as HMMs. We opted for such anapproach so that we could control the amount of provided context informationand, thus, explore its range of influence. Our base features consist ofn-grams, punctuation, and wh-words. Context information is obtained fromprevious utterances and provided in three ways -- n-grams, n-grams tagged withrelative position, and dialogue act classifications. A comparative study wasconducted to evaluate the performance of the three approaches. From it, we wereable to assess the importance of context information on dialogue actrecognition, as well as its range of influence for each of the three selectedrepresentations. In addition to the conclusions originated by the analysis, this work alsoproduced results that advance the state-of-the-art, especially consideringprevious work on the Switchboard corpus. Furthermore, since, to our knowledge,the remaining datasets had not been previously explored for this task, ourexperiments can be used as baselines for future work on those corpora.
arxiv-10800-175 | Average Convergence Rate of Evolutionary Algorithms | http://arxiv.org/pdf/1504.08117v3.pdf | author:Jun He, Guangming Lin category:cs.NE published:2015-04-30 summary:In evolutionary optimization, it is important to understand how fastevolutionary algorithms converge to the optimum per generation, or theirconvergence rate. This paper proposes a new measure of the convergence rate,called average convergence rate. It is a normalised geometric mean of thereduction ratio of the fitness difference per generation. The calculation ofthe average convergence rate is very simple and it is applicable for mostevolutionary algorithms on both continuous and discrete optimization. Atheoretical study of the average convergence rate is conducted for discreteoptimization. Lower bounds on the average convergence rate are derived. Thelimit of the average convergence rate is analysed and then the asymptoticaverage convergence rate is proposed.
arxiv-10800-176 | Learning Speech Rate in Speech Recognition | http://arxiv.org/pdf/1506.00799v1.pdf | author:Xiangyu Zeng, Shi Yin, Dong Wang category:cs.CL cs.LG published:2015-06-02 summary:A significant performance reduction is often observed in speech recognitionwhen the rate of speech (ROS) is too low or too high. Most of presentapproaches to addressing the ROS variation focus on the change of speechsignals in dynamic properties caused by ROS, and accordingly modify the dynamicmodel, e.g., the transition probabilities of the hidden Markov model (HMM).However, an abnormal ROS changes not only the dynamic but also the staticproperty of speech signals, and thus can not be compensated for purely bymodifying the dynamic model. This paper proposes an ROS learning approach basedon deep neural networks (DNN), which involves an ROS feature as the input ofthe DNN model and so the spectrum distortion caused by ROS can be learned andcompensated for. The experimental results show that this approach can deliverbetter performance for too slow and too fast utterances, demonstrating ourconjecture that ROS impacts both the dynamic and the static property of speech.In addition, the proposed approach can be combined with the conventional HMMtransition adaptation method, offering additional performance gains.
arxiv-10800-177 | Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays | http://arxiv.org/pdf/1506.00779v1.pdf | author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG published:2015-06-02 summary:We discuss a multiple-play multi-armed bandit (MAB) problem in which severalarms are selected at each round. Recently, Thompson sampling (TS), a randomizedalgorithm with a Bayesian spirit, has attracted much attention for itsempirically excellent performance, and it is revealed to have an optimal regretbound in the standard single-play MAB problem. In this paper, we propose themultiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to themultiple-play MAB problem, and discuss its regret analysis. We prove that MP-TSfor binary rewards has the optimal regret upper bound that matches the regretlower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the firstcomputationally efficient algorithm with optimal regret. A set of computersimulations was also conducted, which compared MP-TS with state-of-the-artalgorithms. We also propose a modification of MP-TS, which is shown to havebetter empirical performance.
arxiv-10800-178 | Labeled compression schemes for extremal classes | http://arxiv.org/pdf/1506.00165v2.pdf | author:Shay Moran, Manfred K. Warmuth category:cs.LG cs.DM math.CO published:2015-05-30 summary:It is a long-standing open problem whether there always exists a compressionscheme whose size is of the order of the Vapnik-Chervonienkis (VC) dimension$d$. Recently compression schemes of size exponential in $d$ have been foundfor any concept class of VC dimension $d$. Previously size $d$ unlabeledcompression scheme have been given for maximum classes, which are specialconcept classes whose size equals an upper bound due to Sauer-Shelah. Weconsider a natural generalization of the maximum classes called extremalclasses. Their definition is based on a generalization of the Sauer-Shelahbound called the Sandwich Theorem which has applications in many areas ofcombinatorics. The key result of the paper is the construction of a labeledcompression scheme for extremal classes of size equal to their VC dimension. Wealso give a number of open problems concerning the combinatorial structure ofextremal classes and the existence of unlabeled compression schemes for them.
arxiv-10800-179 | Soft Computing Techniques for Change Detection in remotely sensed images : A Review | http://arxiv.org/pdf/1506.00768v1.pdf | author:Madhu Khurana, Vikas Saxena category:cs.NE cs.CV published:2015-06-02 summary:With the advent of remote sensing satellites, a huge repository of remotelysensed images is available. Change detection in remotely sensed images has beenan active research area as it helps us understand the transitions that aretaking place on the Earths surface. This paper discusses the methods and theirclassifications proposed by various researchers for change detection. Since useof soft computing based techniques are now very popular among researchcommunity, this paper also presents a classification based on learningtechniques used in soft-computing methods for change detection.
arxiv-10800-180 | Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology | http://arxiv.org/pdf/1506.00765v1.pdf | author:Zheng Cai, Donglin Cao, Rongrong Ji category:cs.MM cs.CL cs.IR published:2015-06-02 summary:With faster connection speed, Internet users are now making social network ahuge reservoir of texts, images and video clips (GIF). Sentiment analysis forsuch online platform can be used to predict political elections, evaluateseconomic indicators and so on. However, GIF sentiment analysis is quitechallenging, not only because it hinges on spatio-temporal visualcontentabstraction, but also for the relationship between such abstraction andfinal sentiment remains unknown.In this paper, we dedicated to find out suchrelationship.We proposed a SentiPairSequence basedspatiotemporal visualsentiment ontology, which forms the midlevel representations for GIFsentiment.The establishment process of SentiPair contains two steps. First, we constructthe Synset Forest to define the semantic tree structure of visual sentimentlabel elements. Then, through theSynset Forest, we organically select andcombine sentiment label elements to form a mid-level visual sentimentrepresentation. Our experiments indicate that SentiPair outperforms othercompeting mid-level attributes. Using SentiPair, our analysis frameworkcanachieve satisfying prediction accuracy (72.6%). We also opened ourdataset(GSO-2015) to the research community. GSO-2015 contains more than 6,000manually annotated GIFs out of more than 40,000 candidates. Each is labeledwith both sentiment and SentiPair Sequence.
arxiv-10800-181 | Image Retrieval Based on Binary Signature ang S-kGraph | http://arxiv.org/pdf/1506.00761v1.pdf | author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3 published:2015-06-02 summary:In this paper, we introduce an optimum approach for querying similar imageson large digital-image databases. Our work is based on RBIR (region-based imageretrieval) method which uses multiple regions as the key to retrieval images.This method significantly improves the accuracy of queries. However, this alsoincreases the cost of computing. To reduce this expensive computational cost,we implement binary signature encoder which maps an image to its identificationin binary. In order to fasten the lookup, binary signatures of images areclassified by the help of S-kGraph. Finally, our work is evaluated on COREL'simages.
arxiv-10800-182 | Discriminative Neural Sentence Modeling by Tree-Based Convolution | http://arxiv.org/pdf/1504.01106v5.pdf | author:Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE published:2015-04-05 summary:This paper proposes a tree-based convolutional neural network (TBCNN) fordiscriminative sentence modeling. Our models leverage either constituency treesor dependency trees of sentences. The tree-based convolution process extractssentences' structural features, and these features are aggregated by maxpooling. Such architecture allows short propagation paths between the outputlayer and underlying feature detectors, which enables effective structuralfeature learning and extraction. We evaluate our models on two tasks: sentimentanalysis and question classification. In both experiments, TBCNN outperformsprevious state-of-the-art results, including existing neural networks anddedicated feature/rule engineering. We also make efforts to visualize thetree-based convolution process, shedding light on how our models work.
arxiv-10800-183 | What Makes Kevin Spacey Look Like Kevin Spacey | http://arxiv.org/pdf/1506.00752v1.pdf | author:Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, Steve Seitz category:cs.CV published:2015-06-02 summary:We reconstruct a controllable model of a person from a large photo collectionthat captures his or her {\em persona}, i.e., physical appearance and behavior.The ability to operate on unstructured photo collections enables modeling ahuge number of people, including celebrities and other well photographed peoplewithout requiring them to be scanned. Moreover, we show the ability to drive or{\em puppeteer} the captured person B using any other video of a differentperson A. In this scenario, B acts out the role of person A, but retainshis/her own personality and character. Our system is based on a novelcombination of 3D face reconstruction, tracking, alignment, and multi-texturemodeling, applied to the puppeteering problem. We demonstrate convincingresults on a large variety of celebrities derived from Internet imagery andvideo.
arxiv-10800-184 | 3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach | http://arxiv.org/pdf/1411.2942v4.pdf | author:Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, Kostas Daniilidis category:cs.CV published:2014-11-11 summary:We investigate the problem of estimating the 3D shape of an object, given aset of 2D landmarks in a single image. To alleviate the reconstructionambiguity, a widely-used approach is to confine the unknown 3D shape within ashape space built upon existing shapes. While this approach has proven to besuccessful in various applications, a challenging issue remains, i.e., thejoint estimation of shape parameters and camera-pose parameters requires tosolve a nonconvex optimization problem. The existing methods often adopt analternating minimization scheme to locally update the parameters, andconsequently the solution is sensitive to initialization. In this paper, wepropose a convex formulation to address this problem and develop an efficientalgorithm to solve the proposed convex program. We demonstrate the exactrecovery property of the proposed method, its merits compared to alternativemethods, and the applicability in human pose and car shape estimation.
arxiv-10800-185 | DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection | http://arxiv.org/pdf/1412.5661v2.pdf | author:Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, Xiaoou Tang category:cs.CV cs.NE published:2014-12-17 summary:In this paper, we propose deformable deep convolutional neural networks forgeneric object detection. This new deep learning object detection framework hasinnovations in multiple aspects. In the proposed new deep architecture, a newdeformation constrained pooling (def-pooling) layer models the deformation ofobject parts with geometric constraint and penalty. A new pre-training strategyis proposed to learn feature representations more suitable for the objectdetection task and with good generalization capability. By changing the netstructures, training strategies, adding and removing some key components in thedetection pipeline, a set of models with large diversity are obtained, whichsignificantly improves the effectiveness of model averaging. The proposedapproach improves the mean averaged precision obtained by RCNN\cite{girshick2014rich}, which was the state-of-the-art, from 31\% to 50.3\% onthe ILSVRC2014 detection test set. It also outperforms the winner ofILSVRC2014, GoogLeNet, by 6.1\%. Detailed component-wise analysis is alsoprovided through extensive experimental evaluation, which provide a global viewfor people to understand the deep learning object detection pipeline.
arxiv-10800-186 | Quantifying Creativity in Art Networks | http://arxiv.org/pdf/1506.00711v1.pdf | author:Ahmed Elgammal, Babak Saleh category:cs.AI cs.CV cs.CY cs.MM cs.SI published:2015-06-02 summary:Can we develop a computer algorithm that assesses the creativity of apainting given its context within art history? This paper proposes a novelcomputational framework for assessing the creativity of creative products, suchas paintings, sculptures, poetry, etc. We use the most common definition ofcreativity, which emphasizes the originality of the product and its influentialvalue. The proposed computational framework is based on constructing a networkbetween creative products and using this network to infer about the originalityand influence of its nodes. Through a series of transformations, we construct aCreativity Implication Network. We show that inference about creativity in thisnetwork reduces to a variant of network centrality problems which can be solvedefficiently. We apply the proposed framework to the task of quantifyingcreativity of paintings (and sculptures). We experimented on two datasets withover 62K paintings to illustrate the behavior of the proposed framework. Wealso propose a methodology for quantitatively validating the results of theproposed algorithm, which we call the "time machine experiment".
arxiv-10800-187 | Statistical Machine Translation Features with Multitask Tensor Networks | http://arxiv.org/pdf/1506.00698v1.pdf | author:Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar, Rabih Zbib, Richard Schwartz, John Makhoul category:cs.CL published:2015-06-01 summary:We present a three-pronged approach to improving Statistical MachineTranslation (SMT), building on recent success in the application of neuralnetworks to SMT. First, we propose new features based on neural networks tomodel various non-local translation phenomena. Second, we augment thearchitecture of the neural network with tensor layers that capture importanthigher-order interaction among the network units. Third, we apply multitasklearning to estimate the neural network parameters jointly. Each of ourproposed methods results in significant improvements that are complementary.The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English andChinese-English translation over a state-of-the-art system that alreadyincludes neural network features.
arxiv-10800-188 | Mutual Dependence: A Novel Method for Computing Dependencies Between Random Variables | http://arxiv.org/pdf/1506.00673v1.pdf | author:Rahul Agarwal, Pierre Sacre, Sridevi V. Sarma category:math.ST stat.ML stat.TH published:2015-06-01 summary:In data science, it is often required to estimate dependencies betweendifferent data sources. These dependencies are typically calculated usingPearson's correlation, distance correlation, and/or mutual information.However, none of these measures satisfy all the Granger's axioms for an "idealmeasure". One such ideal measure, proposed by Granger himself, calculates theBhattacharyya distance between the joint probability density function (pdf) andthe product of marginal pdfs. We call this measure the mutual dependence.However, to date this measure has not been directly computable from data. Inthis paper, we use our recently introduced maximum likelihood non-parametricestimator for band-limited pdfs, to compute the mutual dependence directly fromthe data. We construct the estimator of mutual dependence and compare itsperformance to standard measures (Pearson's and distance correlation) fordifferent known pdfs by computing convergence rates, computational complexity,and the ability to capture nonlinear dependencies. Our mutual dependenceestimator requires fewer samples to converge to theoretical values, is fasterto compute, and captures more complex dependencies than standard measures.
arxiv-10800-189 | Sample-Optimal Density Estimation in Nearly-Linear Time | http://arxiv.org/pdf/1506.00671v1.pdf | author:Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2015-06-01 summary:We design a new, fast algorithm for agnostically learning univariateprobability distributions whose densities are well approximated by piecewisepolynomial functions. Let $f$ be the density function of an arbitraryunivariate distribution, and suppose that $f$ is $\mathrm{OPT}$-close in$L_1$-distance to an unknown piecewise polynomial function with $t$ intervalpieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\epsilon^2)$ samplesfrom $f$, runs in time $\tilde{O}(n \cdot \mathrm{poly}(d))$, and withprobability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis$h$ that is $4 \cdot \mathrm{OPT} +\epsilon$ close to $f$. Our general algorithm yields (nearly) sample-optimal and nearly-linear timeestimators for a wide range of structured distribution families over bothcontinuous and discrete domains in a unified way. For most of our applications,these are the first sample-optimal and nearly-linear time estimators in theliterature. As a consequence, our work resolves the sample and computationalcomplexities of a broad class of inference tasks via a single "meta-algorithm".Moreover, we experimentally demonstrate that our algorithm performs very wellin practice. Our algorithm consists of three "levels": (i) At the top level, we employ aniterative greedy algorithm for finding a good partition of the real line intothe pieces of a piecewise polynomial. (ii) For each piece, we show that thesub-problem of finding a good polynomial fit on the current interval can besolved efficiently with a separation oracle method. (iii) We reduce the task offinding a separating hyperplane to a combinatorial problem and give anefficient algorithm for this problem. Combining these three procedures gives adensity estimation algorithm with the claimed guarantees.
arxiv-10800-190 | Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets | http://arxiv.org/pdf/1503.01007v4.pdf | author:Armand Joulin, Tomas Mikolov category:cs.NE cs.LG published:2015-03-03 summary:Despite the recent achievements in machine learning, we are still very farfrom achieving real artificial intelligence. In this paper, we discuss thelimitations of standard deep learning approaches and show that some of theselimitations can be overcome by learning how to grow the complexity of a modelin a structured way. Specifically, we study the simplest sequence predictionproblems that are beyond the scope of what is learnable with standard recurrentnetworks, algorithmically generated sequences which can only be learned bymodels which have the capacity to count and to memorize sequences. We show thatsome basic algorithms can be learned from sequential data using a recurrentnetwork associated with a trainable memory.
arxiv-10800-191 | Practice in Synonym Extraction at Large Scale | http://arxiv.org/pdf/1412.2197v3.pdf | author:Liangliang Cao, Chang Wang category:cs.CL published:2014-12-06 summary:Synonym extraction is an important task in natural language processing andoften used as a submodule in query expansion, question answering and otherapplications. Automatic synonym extractor is highly preferred for large scaleapplications. Previous studies in synonym extraction are most limited to smallscale datasets. In this paper, we build a large dataset with 3.4 millionsynonym/non-synonym pairs to capture the challenges in real world scenarios. Weproposed (1) a new cost function to accommodate the unbalanced learningproblem, and (2) a feature learning based deep neural network to model thecomplicated relationships in synonym pairs. We compare several differentapproaches based on SVMs and neural networks, and find out a novel featurelearning based neural network outperforms the methods with hand-assignedfeatures. Specifically, the best performance of our model surpasses the SVMbaseline with a significant 97\% relative improvement.
arxiv-10800-192 | Blocks and Fuel: Frameworks for deep learning | http://arxiv.org/pdf/1506.00619v1.pdf | author:Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2015-06-01 summary:We introduce two Python frameworks to train neural networks on largedatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compilerwith CUDA-support. It facilitates the training of complex neural network modelsby providing parametrized Theano operations, attaching metadata to Theano'ssymbolic computational graph, and providing an extensive set of utilities toassist training the networks, e.g. training algorithms, logging, monitoring,visualization, and serialization. Fuel provides a standard format for machinelearning datasets. It allows the user to easily iterate over large datasets,performing many types of pre-processing on the fly.
arxiv-10800-193 | On Quantum Generalizations of Information-Theoretic Measures and their Contribution to Distributional Semantics | http://arxiv.org/pdf/1506.00578v1.pdf | author:William Blacoe category:cs.IT cs.CL math.IT published:2015-06-01 summary:Information-theoretic measures such as relative entropy and correlation areextremely useful when modeling or analyzing the interaction of probabilisticsystems. We survey the quantum generalization of 5 such measures and point outsome of their commonalities and interpretations. In particular we find theapplication of information theory to distributional semantics useful. Bymodeling the distributional meaning of words as density operators rather thanvectors, more of their semantic structure may be exploited. Furthermore,properties of and interactions between words such as ambiguity, similarity andentailment can be simulated more richly and intuitively when using methods fromquantum information theory.
arxiv-10800-194 | Bootstrap Bias Corrections for Ensemble Methods | http://arxiv.org/pdf/1506.00553v1.pdf | author:Giles Hooker, Lucas Mentch category:stat.ML published:2015-06-01 summary:This paper examines the use of a residual bootstrap for bias correction inmachine learning regression methods. Accounting for bias is an importantobstacle in recent efforts to develop statistical inference for machinelearning methods. We demonstrate empirically that the proposed bootstrap biascorrection can lead to substantial improvements in both bias and predictiveaccuracy. In the context of ensembles of trees, we show that this correctioncan be approximated at only double the cost of training the original ensemblewithout introducing additional variance. Our method is shown to improvetest-set accuracy over random forests by up to 70\% on example problems fromthe UCI repository.
arxiv-10800-195 | Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection | http://arxiv.org/pdf/1506.00552v1.pdf | author:Julie Nutini, Mark Schmidt, Issam H. Laradji, Michael Friedlander, Hoyt Koepke category:math.OC cs.LG stat.CO stat.ML published:2015-06-01 summary:There has been significant recent work on the theory and application ofrandomized coordinate descent algorithms, beginning with the work of Nesterov[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selectionrule achieves the same convergence rate as the Gauss-Southwell selection rule.This result suggests that we should never use the Gauss-Southwell rule, as itis typically much more expensive than random selection. However, the empiricalbehaviours of these algorithms contradict this theoretical result: inapplications where the computational costs of the selection rules arecomparable, the Gauss-Southwell selection rule tends to perform substantiallybetter than random coordinate selection. We give a simple analysis of theGauss-Southwell rule showing that---except in extreme cases---it's convergencerate is faster than choosing random coordinates. Further, in this work we (i)show that exact coordinate optimization improves the convergence rate forcertain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule thatgives an even faster convergence rate given knowledge of the Lipschitzconstants of the partial derivatives, (iii) analyze the effect of approximateGauss-Southwell rules, and (iv) analyze proximal-gradient variants of theGauss-Southwell rule.
arxiv-10800-196 | Medical Synonym Extraction with Concept Space Models | http://arxiv.org/pdf/1506.00528v1.pdf | author:Chang Wang, Liangliang Cao, Bowen Zhou category:cs.CL published:2015-06-01 summary:In this paper, we present a novel approach for medical synonym extraction. Weaim to integrate the term embedding with the medical domain knowledge forhealthcare applications. One advantage of our method is that it is veryscalable. Experiments on a dataset with more than 1M term pairs show that theproposed approach outperforms the baseline approaches by a large margin.
arxiv-10800-197 | User Preferences Modeling and Learning for Pleasing Photo Collage Generation | http://arxiv.org/pdf/1506.00527v1.pdf | author:Simone Bianco, Gianluigi Ciocca category:cs.MM cs.CV cs.HC published:2015-06-01 summary:In this paper we consider how to automatically create pleasing photo collagescreated by placing a set of images on a limited canvas area. The task isformulated as an optimization problem. Differently from existingstate-of-the-art approaches, we here exploit subjective experiments to modeland learn pleasantness from user preferences. To this end, we design anexperimental framework for the identification of the criteria that need to betaken into account to generate a pleasing photo collage. Five differentthematic photo datasets are used to create collages using state-of-the-artcriteria. A first subjective experiment where several subjects evaluated thecollages, emphasizes that different criteria are involved in the subjectivedefinition of pleasantness. We then identify new global and local criteria anddesign algorithms to quantify them. The relative importance of these criteriaare automatically learned by exploiting the user preferences, and new collagesare generated. To validate our framework, we performed several psycho-visualexperiments involving different users. The results shows that the proposedframework allows to learn a novel computational model which effectively encodesan inter-user definition of pleasantness. The learned definition ofpleasantness generalizes well to new photo datasets of different themes andsizes not used in the learning. Moreover, compared with two state of the artapproaches, the collages created using our framework are preferred by themajority of the users.
arxiv-10800-198 | Robust Face Recognition with Structural Binary Gradient Patterns | http://arxiv.org/pdf/1506.00481v1.pdf | author:Weilin Huang, Hujun Yin category:cs.CV published:2015-06-01 summary:This paper presents a computationally efficient yet powerful binary frameworkfor robust facial representation based on image gradients. It is termed asstructural binary gradient patterns (SBGP). To discover underlying localstructures in the gradient domain, we compute image gradients from multipledirections and simplify them into a set of binary strings. The SBGP is derivedfrom certain types of these binary strings that have meaningful localstructures and are capable of resembling fundamental textural information. Theydetect micro orientational edges and possess strong orientation and localitycapabilities, thus enabling great discrimination. The SBGP also benefits fromthe advantages of the gradient domain and exhibits profound robustness againstillumination variations. The binary strategy realized by pixel correlations ina small neighborhood substantially simplifies the computational complexity andachieves extremely efficient processing with only 0.0032s in Matlab for atypical face image. Furthermore, the discrimination power of the SBGP can beenhanced on a set of defined orientational image gradient magnitudes, furtherenforcing locality and orientation. Results of extensive experiments on variousbenchmark databases illustrate significant improvements of the SBGP basedrepresentations over the existing state-of-the-art local descriptors in theterms of discrimination, robustness and complexity. Codes for the SBGP methodswill be available athttp://www.eee.manchester.ac.uk/research/groups/sisp/software/.
arxiv-10800-199 | Texture Retrieval via the Scattering Transform | http://arxiv.org/pdf/1501.02655v4.pdf | author:Alexander Sagel, Dominik Meyer, Hao Shen category:cs.IR cs.CV published:2015-01-12 summary:This work studies the problem of content-based image retrieval, specifically,texture retrieval. It focuses on feature extraction and similarity measure fortexture images. Our approach employs a recently developed method, the so-calledScattering transform, for the process of feature extraction in textureretrieval. It shares a distinctive property of providing a robustrepresentation, which is stable with respect to spatial deformations. Recentwork has demonstrated its capability for texture classification, and hence as apromising candidate for the problem of texture retrieval. Moreover, we adopt a common approach of measuring the similarity of texturesby comparing the subband histograms of a filterbank transform. To this end wederive a similarity measure based on the popular Bhattacharyya Kernel. Despitethe popularity of describing histograms using parametrized probability densityfunctions, such as the Generalized Gaussian Distribution, it is unfortunatelynot applicable for describing most of the Scattering transform subbands, due tothe complex modulus performed on each one of them. In this work, we propose touse the Weibull distribution to model the Scattering subbands of descendantlayers. Our numerical experiments demonstrated the effectiveness of the proposedapproach, in comparison with several state of the arts.
arxiv-10800-200 | Hierarchical structure-and-motion recovery from uncalibrated images | http://arxiv.org/pdf/1506.00395v1.pdf | author:Roberto Toldo, Riccardo Gherardi, Michela Farenzena, Andrea Fusiello category:cs.CV published:2015-06-01 summary:This paper addresses the structure-and-motion problem, that requires to findcamera motion and 3D struc- ture from point matches. A new pipeline, dubbedSamantha, is presented, that departs from the prevailing sequential paradigmand embraces instead a hierarchical approach. This method has severaladvantages, like a provably lower computational complexity, which is necessaryto achieve true scalability, and better error containment, leading to morestability and less drift. Moreover, a practical autocalibration procedureallows to process images without ancillary information. Experiments with realdata assess the accuracy and the computational efficiency of the method.
arxiv-10800-201 | RBIR using Interest Regions and Binary Signatures | http://arxiv.org/pdf/1506.00368v1.pdf | author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3 published:2015-06-01 summary:In this paper, we introduce an approach to overcome the low accuracy of theContent-Based Image Retrieval (CBIR) (when using the global features). Toincrease the accuracy, we use Harris-Laplace detector to identify the interestregions of image. Then, we build the Region-Based Image Retrieval (RBIR). Forthe efficient image storage and retrieval, we encode images into binarysignatures. The binary signature of a image is created from its interestregions. Furthermore, this paper also provides an algorithm for image retrievalon S-tree by comparing the images' signatures on a metric similarly to EMD(earth mover's distance). Finally, we evaluate the created models on COREL'simages.
arxiv-10800-202 | Generalized Twin Gaussian Processes using Sharma-Mittal Divergence | http://arxiv.org/pdf/1409.7480v5.pdf | author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.LG cs.CV stat.ML published:2014-09-26 summary:There has been a growing interest in mutual information measures due to theirwide range of applications in Machine Learning and Computer Vision. In thispaper, we present a generalized structured regression framework based onShama-Mittal divergence, a relative entropy measure, which is introduced to theMachine Learning community in this work. Sharma-Mittal (SM) divergence is ageneralized mutual information measure for the widely used R\'enyi, Tsallis,Bhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, westudy Sharma-Mittal divergence as a cost function in the context of the TwinGaussian Processes (TGP)~\citep{Bo:2010}, which generalizes over theKL-divergence without computational penalty. We show interesting properties ofSharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missinginsights in the traditional TGP formulation. However, we generalize this theorybased on SM-divergence instead of KL-divergence which is a special case.Experimentally, we evaluated the proposed SMTGP framework on several datasets.The results show that SMTGP reaches better predictions than KL-based TGP, sinceit offers a bigger class of models through its parameters that we learn fromthe data.
arxiv-10800-203 | A Fast and Flexible Algorithm for the Graph-Fused Lasso | http://arxiv.org/pdf/1505.06475v3.pdf | author:Wesley Tansey, James G. Scott category:stat.ML stat.CO published:2015-05-24 summary:We propose a new algorithm for solving the graph-fused lasso (GFL), a methodfor parameter estimation that operates under the assumption that the signaltends to be locally constant over a predefined graph structure. Our key insightis to decompose the graph into a set of trails which can then each be solvedefficiently using techniques for the ordinary (1D) fused lasso. We leveragethese trails in a proximal algorithm that alternates between closed form primalupdates and fast dual trail updates. The resulting techinque is both fasterthan previous GFL methods and more flexible in the choice of loss function andgraph structure. Furthermore, we present two algorithms for constructing trailsets and show empirically that they offer a tradeoff between preprocessing timeand convergence rate.
arxiv-10800-204 | Imaging Time-Series to Improve Classification and Imputation | http://arxiv.org/pdf/1506.00327v1.pdf | author:Zhiguang Wang, Tim Oates category:cs.LG cs.NE stat.ML published:2015-06-01 summary:Inspired by recent successes of deep learning in computer vision, we proposea novel framework for encoding time series as different types of images,namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and MarkovTransition Fields (MTF). This enables the use of techniques from computervision for time series classification and imputation. We used TiledConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learnhigh-level features from the individual and compound GASF-GADF-MTF images. Ourapproaches achieve highly competitive results when compared to nine of thecurrent best time series classification approaches. Inspired by the bijectionproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) onthe GASF images of four standard and one synthesized compound dataset. Theimputation MSE on test data is reduced by 12.18%-48.02% when compared to usingthe raw data. An analysis of the features and weights learned via tiled CNNsand DAs explains why the approaches work.
arxiv-10800-205 | Robust PCA: Optimization of the Robust Reconstruction Error over the Stiefel Manifold | http://arxiv.org/pdf/1506.00323v1.pdf | author:Anastasia Podosinnikova, Simon Setzer, Matthias Hein category:stat.ML cs.LG published:2015-06-01 summary:It is well known that Principal Component Analysis (PCA) is strongly affectedby outliers and a lot of effort has been put into robustification of PCA. Inthis paper we present a new algorithm for robust PCA minimizing the trimmedreconstruction error. By directly minimizing over the Stiefel manifold, weavoid deflation as often used by projection pursuit methods. In distinction toother methods for robust PCA, our method has no free parameter and iscomputationally very efficient. We illustrate the performance on variousdatasets including an application to background modeling and subtraction. Ourmethod performs better or similar to current state-of-the-art methods whilebeing faster.
arxiv-10800-206 | Copeland Dueling Bandits | http://arxiv.org/pdf/1506.00312v1.pdf | author:Masrour Zoghi, Zohar Karnin, Shimon Whiteson, Maarten de Rijke category:cs.LG published:2015-06-01 summary:A version of the dueling bandit problem is addressed in which a Condorcetwinner may not exist. Two algorithms are proposed that instead seek to minimizeregret with respect to the Copeland winner, which, unlike the Condorcet winner,is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designedfor small numbers of arms, while the second, Scalable Copeland Bandits (SCB),works better for large-scale problems. We provide theoretical results boundingthe regret accumulated by CCB and SCB, both substantially improving existingresults. Such existing results either offer bounds of the form $O(K \log T)$but require restrictive assumptions, or offer bounds of the form $O(K^2 \logT)$ without requiring such assumptions. Our results offer the best of bothworlds: $O(K \log T)$ bounds without restrictive assumptions.
arxiv-10800-207 | Automatic Inference for Inverting Software Simulators via Probabilistic Programming | http://arxiv.org/pdf/1506.00308v1.pdf | author:Ardavan Saeedi, Vlad Firoiu, Vikash Mansinghka category:stat.ML published:2015-05-31 summary:Models of complex systems are often formalized as sequential softwaresimulators: computationally intensive programs that iteratively build upprobable system configurations given parameters and initial conditions. Thesesimulators enable modelers to capture effects that are difficult tocharacterize analytically or summarize statistically. However, in manyreal-world applications, these simulations need to be inverted to match theobserved data. This typically requires the custom design, derivation andimplementation of sophisticated inversion algorithms. Here we give a frameworkfor inverting a broad class of complex software simulators via probabilisticprogramming and automatic inference, using under 20 lines of probabilisticcode. Our approach is based on a formulation of inversion as approximateinference in a simple sequential probabilistic model. We implement fourinference strategies, including Metropolis-Hastings, a sequentializedMetropolis-Hastings scheme, and a particle Markov chain Monte Carlo scheme,requiring 4 or fewer lines of probabilistic code each. We demonstrate ourframework by applying it to invert a real geological software simulator fromthe oil and gas industry.
arxiv-10800-208 | Interactive Knowledge Base Population | http://arxiv.org/pdf/1506.00301v1.pdf | author:Travis Wolfe, Mark Dredze, James Mayfield, Paul McNamee, Craig Harman, Tim Finin, Benjamin Van Durme category:cs.AI cs.CL published:2015-05-31 summary:Most work on building knowledge bases has focused on collecting entities andfacts from as large a collection of documents as possible. We argue for anddescribe a new paradigm where the focus is on a high-recall extraction over asmall collection of documents under the supervision of a human expert, that wecall Interactive Knowledge Base Population (IKBP).
arxiv-10800-209 | A Linear Dynamical System Model for Text | http://arxiv.org/pdf/1502.04081v2.pdf | author:David Belanger, Sham Kakade category:stat.ML cs.CL cs.LG published:2015-02-13 summary:Low dimensional representations of words allow accurate NLP models to betrained on limited annotated data. While most representations ignore words'local context, a natural way to induce context-dependent representations is toperform inference in a probabilistic latent-variable sequence model. Given therecent success of continuous vector space word representations, we provide suchan inference procedure for continuous states, where words' representations aregiven by the posterior mean of a linear dynamical system. Here, efficientinference can be performed using Kalman filtering. Our learning algorithm isextremely scalable, operating on simple cooccurrence counts for both parameterinitialization using the method of moments and subsequent iterations of EM. Inour experiments, we employ our inferred word embeddings as features in standardtagging tasks, obtaining significant accuracy improvements. Finally, the Kalmanfilter updates can be seen as a linear recurrent neural network. We demonstratethat using the parameters of our model to initialize a non-linear recurrentneural network language model reduces its training time by a day and yieldslower perplexity.
arxiv-10800-210 | Visual Madlibs: Fill in the blank Image Generation and Question Answering | http://arxiv.org/pdf/1506.00278v1.pdf | author:Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg category:cs.CV cs.CL published:2015-05-31 summary:In this paper, we introduce a new dataset consisting of 360,001 focusednatural language descriptions for 10,738 images. This dataset, the VisualMadlibs dataset, is collected using automatically produced fill-in-the-blanktemplates designed to gather targeted descriptions about: people and objects,their appearances, activities, and interactions, as well as inferences aboutthe general scene or its broader context. We provide several analyses of theVisual Madlibs dataset and demonstrate its applicability to two new descriptiongeneration tasks: focused description generation, and multiple-choicequestion-answering for images. Experiments using joint-embedding and deeplearning methods show promising results on these tasks.
arxiv-10800-211 | Spectral Convergence of the connection Laplacian from random samples | http://arxiv.org/pdf/1306.1587v3.pdf | author:Amit Singer, Hau-tieng Wu category:math.NA math.ST stat.ME stat.ML stat.TH published:2013-06-07 summary:Spectral methods that are based on eigenvectors and eigenvalues of discretegraph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps are often usedfor manifold learning and non-linear dimensionality reduction. It waspreviously shown by Belkin and Niyogi \cite{belkin_niyogi:2007} that theeigenvectors and eigenvalues of the graph Laplacian converge to theeigenfunctions and eigenvalues of the Laplace-Beltrami operator of the manifoldin the limit of infinitely many data points sampled independently from theuniform distribution over the manifold. Recently, we introduced VectorDiffusion Maps and showed that the connection Laplacian of the tangent bundleof the manifold can be approximated from random samples. In this paper, wepresent a unified framework for approximating other connection Laplacians overthe manifold by considering its principle bundle structure. We prove that theeigenvectors and eigenvalues of these Laplacians converge in the limit ofinfinitely many independent random samples. We generalize the spectralconvergence results to the case where the data points are sampled from anon-uniform distribution, and for manifolds with and without boundary.
arxiv-10800-212 | Parallel Spectral Clustering Algorithm Based on Hadoop | http://arxiv.org/pdf/1506.00227v1.pdf | author:Yajun Cui, Yang Zhao, Kafei Xiao, Chenglong Zhang, Lei Wang category:cs.DC cs.DS cs.LG published:2015-05-31 summary:Spectral clustering and cloud computing is emerging branch of computerscience or related discipline. It overcome the shortcomings of some traditionalclustering algorithm and guarantee the convergence to the optimal solution,thus have to the widespread attention. This article first introduced theparallel spectral clustering algorithm research background and significance,and then to Hadoop the cloud computing Framework has carried on the detailedintroduction, then has carried on the related to spectral clustering isintroduced, then introduces the spectral clustering arithmetic Method ofparallel and relevant steps, finally made the related experiments, and theexperiment are summarized.
arxiv-10800-213 | Recurrent Neural Networks with External Memory for Language Understanding | http://arxiv.org/pdf/1506.00195v1.pdf | author:Baolin Peng, Kaisheng Yao category:cs.CL cs.AI cs.LG cs.NE published:2015-05-31 summary:Recurrent Neural Networks (RNNs) have become increasingly popular for thetask of language understanding. In this task, a semantic tagger is deployed toassociate a semantic label to each word in an input sequence. The success ofRNN may be attributed to its ability to memorize long-term dependence thatrelates the current-time semantic label prediction to the observations manytime instances away. However, the memory capacity of simple RNNs is limitedbecause of the gradient vanishing and exploding problem. We propose to use anexternal memory to improve memorization capability of RNNs. We conductedexperiments on the ATIS dataset, and observed that the proposed model was ableto achieve the state-of-the-art results. We compare our proposed model withalternative models and report analysis results that may provide insights forfuture research.
arxiv-10800-214 | An Open Source Testing Tool for Evaluating Handwriting Input Methods | http://arxiv.org/pdf/1506.00176v1.pdf | author:Liquan Qiu, Lianwen Jin, Ruifen Dai, Yuxiang Zhang, Lei Li category:cs.HC cs.CV published:2015-05-30 summary:This paper presents an open source tool for testing the recognition accuracyof Chinese handwriting input methods. The tool consists of two modules, namelythe PC and Android mobile client. The PC client reads handwritten samples inthe computer, and transfers them individually to the Android client inaccordance with the socket communication protocol. After the Android clientreceives the data, it simulates the handwriting on screen of client device, andtriggers the corresponding handwriting recognition method. The recognitionaccuracy is recorded by the Android client. We present the design principlesand describe the implementation of the test platform. We construct several testdatasets for evaluating different handwriting recognition systems, and conductan objective and comprehensive test using six Chinese handwriting input methodswith five datasets. The test results for the recognition accuracy are thencompared and analyzed.
arxiv-10800-215 | Proximal Algorithms in Statistics and Machine Learning | http://arxiv.org/pdf/1502.03175v3.pdf | author:Nicholas G. Polson, James G. Scott, Brandon T. Willard category:stat.ML cs.LG stat.ME published:2015-02-11 summary:In this paper we develop proximal methods for statistical learning. Proximalpoint algorithms are useful in statistics and machine learning for obtainingoptimization solutions for composite functions. Our approach exploitsclosed-form solutions of proximal operators and envelope representations basedon the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes.Envelope representations lead to novel proximal algorithms for statisticaloptimisation of composite objective functions which include both non-smooth andnon-convex objectives. We illustrate our methodology with regularized Logisticand Poisson regression and non-convex bridge penalties with a fused lasso norm.We provide a discussion of convergence of non-descent algorithms withacceleration and for non-convex functions. Finally, we provide directions forfuture research.
arxiv-10800-216 | Using curvature to distinguish between surface reflections and vessel contents in computer vision based recognition of materials in transparent vessels | http://arxiv.org/pdf/1506.00168v1.pdf | author:Sagi Eppel category:cs.CV published:2015-05-30 summary:The recognition of materials and objects inside transparent containers usingcomputer vision has a wide range of applications, ranging from industrialbottles filling to the automation of chemistry laboratory. One of the mainchallenges in such recognition is the ability to distinguish between imagefeatures resulting from the vessels surface and image features resulting fromthe material inside the vessel. Reflections and the functional parts of avessels surface can create strong edges that can be mistakenly identified ascorresponding to the vessel contents, and cause recognition errors. The abilityto evaluate whether a specific edge in an image stems from the vessels surfaceor from its contents can considerably improve the ability to identify materialsinside transparent vessels. This work will suggest a method for suchevaluation, based on the following two assumptions: 1) Areas of high curvatureon the vessel surface are likely to cause strong edges due to changes inreflectivity, as is the appearance of functional parts (e.g. corks or valves).2) Most transparent vessels (bottles, glasses) have high symmetry(cylindrical). As a result the curvature angle of the vessels surface at eachpoint of the image is similar to the curvature angle of the contour line of thevessel in the same row in the image. These assumptions, allow theidentification of image regions with strong edges corresponding to the vesselsurface reflections. Combining this method with existing image analysis methodsfor detecting materials inside transparent containers allows considerableimprovement in accuracy.
arxiv-10800-217 | Addressing the Rare Word Problem in Neural Machine Translation | http://arxiv.org/pdf/1410.8206v4.pdf | author:Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech Zaremba category:cs.CL cs.LG cs.NE published:2014-10-30 summary:Neural Machine Translation (NMT) is a new approach to machine translationthat has shown promising results that are comparable to traditional approaches.A significant weakness in conventional NMT systems is their inability tocorrectly translate very rare words: end-to-end NMTs tend to have relativelysmall vocabularies with a single unk symbol that represents every possibleout-of-vocabulary (OOV) word. In this paper, we propose and implement aneffective technique to address this problem. We train an NMT system on datathat is augmented by the output of a word alignment algorithm, allowing the NMTsystem to emit, for each OOV word in the target sentence, the position of itscorresponding word in the source sentence. This information is later utilizedin a post-processing step that translates every OOV word using a dictionary.Our experiments on the WMT14 English to French translation task show that thismethod provides a substantial improvement of up to 2.8 BLEU points over anequivalent NMT system that does not use this technique. With 37.5 BLEU points,our NMT system is the first to surpass the best result achieved on a WMT14contest task.
arxiv-10800-218 | Deep Roto-Translation Scattering for Object Classification | http://arxiv.org/pdf/1412.8659v2.pdf | author:Edouard Oyallon, Stéphane Mallat category:cs.CV published:2014-12-30 summary:Dictionary learning algorithms or supervised deep convolution networks haveconsiderably improved the efficiency of predefined feature representations suchas SIFT. We introduce a deep scattering convolution network, with predefinedwavelet filters over spatial and angular variables. This representation bringsan important improvement to results previously obtained with predefinedfeatures over object image databases such as Caltech and CIFAR. The resultingaccuracy is comparable to results obtained with unsupervised deep learning anddictionary based representations. This shows that refining imagerepresentations by using geometric priors is a promising direction to improveimage classification and its understanding.
arxiv-10800-219 | Robust Anomaly Detection Using Semidefinite Programming | http://arxiv.org/pdf/1504.00905v2.pdf | author:Jose A. Lopez, Octavia Camps, Mario Sznaier category:math.OC cs.CV cs.LG cs.SY published:2015-04-03 summary:This paper presents a new approach, based on polynomial optimization and themethod of moments, to the problem of anomaly detection. The proposed techniqueonly requires information about the statistical moments of the normal-statedistribution of the features of interest and compares favorably with existingapproaches (such as Parzen windows and 1-class SVM). In addition, it provides asuccinct description of the normal state. Thus, it leads to a substantialsimplification of the the anomaly detection problem when working with higherdimensional datasets.
arxiv-10800-220 | Efficient combination of pairswise feature networks | http://arxiv.org/pdf/1506.00102v1.pdf | author:Pau Bellot, Patrick E. Meyer category:stat.ML cs.LG published:2015-05-30 summary:This paper presents a novel method for the reconstruction of a neural networkconnectivity using calcium fluorescence data. We introduce a fast unsupervisedmethod to integrate different networks that reconstructs structuralconnectivity from neuron activity. Our method improves the state-of-the-artreconstruction method General Transfer Entropy (GTE). We are able to bettereliminate indirect links, improving therefore the quality of the network via anormalization and ensemble process of GTE and three new informative features.The approach is based on a simple combination of networks, which is remarkablyfast. The performance of our approach is benchmarked on simulated time seriesprovided at the connectomics challenge and also submitted at the publiccompetition.
arxiv-10800-221 | A Review of Feature and Data Fusion with Medical Images | http://arxiv.org/pdf/1506.00097v1.pdf | author:Alex Pappachen James, Belur Dasarathy category:cs.CV published:2015-05-30 summary:The fusion techniques that utilize multiple feature sets to form new featuresthat are often more robust and contain useful information for future processingare referred to as feature fusion. The term data fusion is applied to the classof techniques used for combining decisions obtained from multiple feature setsto form global decisions. Feature and data fusion interchangeably represent twoimportant classes of techniques that have proved to be of practical importancein a wide range of medical imaging problems
arxiv-10800-222 | Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks | http://arxiv.org/pdf/1503.00075v3.pdf | author:Kai Sheng Tai, Richard Socher, Christopher D. Manning category:cs.CL cs.AI cs.LG published:2015-02-28 summary:Because of their superior ability to preserve sequence information over time,Long Short-Term Memory (LSTM) networks, a type of recurrent neural network witha more complex computational unit, have obtained strong results on a variety ofsequence modeling tasks. The only underlying LSTM structure that has beenexplored so far is a linear chain. However, natural language exhibits syntacticproperties that would naturally combine words to phrases. We introduce theTree-LSTM, a generalization of LSTMs to tree-structured network topologies.Tree-LSTMs outperform all existing systems and strong LSTM baselines on twotasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task1) and sentiment classification (Stanford Sentiment Treebank).
arxiv-10800-223 | Recognition of convolutional neural network based on CUDA Technology | http://arxiv.org/pdf/1506.00074v1.pdf | author:Yi-bin Huang, Kang Li, Ge Wang, Min Cao, Pin Li, Yu-jia Zhang category:cs.DC cs.NE published:2015-05-30 summary:For the problem whether Graphic Processing Unit(GPU),the stream processorwith high performance of floating-point computing is applicable to neuralnetworks, this paper proposes the parallel recognition algorithm ofConvolutional Neural Networks(CNNs).It adopts Compute Unified DeviceArchitecture(CUDA)technology, definite the parallel data structures, anddescribes the mapping mechanism for computing tasks on CUDA. It compares theparallel recognition algorithm achieved on GPU of GTX200 hardware architecturewith the serial algorithm on CPU. It improves speed by nearly 60 times. Resultshows that GPU based the stream processor architecture ate more applicable tosome related applications about neural networks than CPU.
arxiv-10800-224 | A Three-stage Approach for Segmenting Degraded Color Images: Smoothing, Lifting and Thresholding (SLaT) | http://arxiv.org/pdf/1506.00060v1.pdf | author:Xiaohao Cai, Raymond Chan, Mila Nikolova, Tieyong Zeng category:cs.CV math.NA 65F22 I.4.6 published:2015-05-30 summary:In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) methodwith three stages for multiphase segmentation of color images corrupted bydifferent degradations: noise, information loss, and blur. At the first stage,a convex variant of the Mumford-Shah model is applied to each channel to obtaina smooth image. We show that the model has unique solution under the differentdegradations. In order to properly handle the color information, the secondstage is dimension lifting where we consider a new vector-valued image composedof the restored image and its transform in the secondary color space withadditional information. This ensures that even if the first color space hashighly correlated channels, we can still have enough information to give goodsegmentation results. In the last stage, we apply multichannel thresholding tothe combined vector-valued image to find the segmentation. The number of phasesis only required in the last stage, so users can choose or change it allwithout the need of solving the previous stages again. Experiments demonstratethat our SLaT method gives excellent results in terms of segmentation qualityand CPU time in comparison with other state-of-the-art segmentation methods.
arxiv-10800-225 | Bag-of-Genres for Video Genre Retrieval | http://arxiv.org/pdf/1506.00051v1.pdf | author:Leonardo A. Duarte, Otávio A. B. Penatti, Jurandy Almeida category:cs.CV published:2015-05-30 summary:This paper presents a higher level representation for videos aiming at videogenre retrieval. In video genre retrieval, there is a challenge that videos maycomprise multiple categories, for instance, news videos may be composed ofsports, documentary, and action. Therefore, it is interesting to encode thedistribution of such genres in a compact and effective manner. We propose tocreate a visual dictionary using a genre classifier. Each visual word in theproposed model corresponds to a region in the classification space determinedby the classifier's model learned on the training frames. Therefore, the videofeature vector contains a summary of the activations of each genre in itscontents. We evaluate the bag-of-genres model for video genre retrieval, usingthe dataset of MediaEval Tagging Task of 2012. Results show that the proposedmodel increases the quality of the representation being more compact thanexisting features.
arxiv-10800-226 | Guaranteed Matrix Completion via Non-convex Factorization | http://arxiv.org/pdf/1411.8003v2.pdf | author:Ruoyu Sun, Zhi-Quan Luo category:cs.LG published:2014-11-28 summary:Matrix factorization is a popular approach for large-scale matrix completion.In this approach, the unknown low-rank matrix is expressed as the product oftwo much smaller matrices so that the low-rank property is automaticallyfulfilled. The resulting optimization problem, even with huge size, can besolved (to stationary points) very efficiently through standard optimizationalgorithms such as alternating minimization and stochastic gradient descent(SGD). However, due to the non-convexity caused by the factorization model,there is a limited theoretical understanding of whether these algorithms willgenerate a good solution. In this paper, we establish a theoretical guaranteefor the factorization based formulation to correctly recover the underlyinglow-rank matrix. In particular, we show that under similar conditions to thosein previous works, many standard optimization algorithms converge to the globaloptima of a factorization based formulation, and recover the true low-rankmatrix. A major difference of our work from the existing results is that we donot need resampling (i.e., using independent samples at each iteration) ineither the algorithm or its analysis. To the best of our knowledge, our resultis the first one that provides exact recovery guarantee for many standardalgorithms such as gradient descent, SGD and block coordinate gradient descent.
arxiv-10800-227 | Using Syntactic Features for Phishing Detection | http://arxiv.org/pdf/1506.00037v1.pdf | author:Gilchan Park, Julia M. Taylor category:cs.CL published:2015-05-29 summary:This paper reports on the comparison of the subject and object of verbs intheir usage between phishing emails and legitimate emails. The purpose of thisresearch is to explore whether the syntactic structures and subjects andobjects of verbs can be distinguishable features for phishing detection. Toachieve the objective, we have conducted two series of experiments: thesyntactic similarity for sentences, and the subject and object of verbcomparison. The results of the experiments indicated that both features can beused for some verbs, but more work has to be done for others.
arxiv-10800-228 | Feature Representation for Online Signature Verification | http://arxiv.org/pdf/1505.08153v1.pdf | author:Mohsen Fayyaz, Mohammad Hajizadeh_Saffar, Mohammad Sabokrou, Mahmood Fathy category:cs.CV cs.AI published:2015-05-29 summary:Biometrics systems have been used in a wide range of applications and haveimproved people authentication. Signature verification is one of the mostcommon biometric methods with techniques that employ various specifications ofa signature. Recently, deep learning has achieved great success in many fields,such as image, sounds and text processing. In this paper, deep learning methodhas been used for feature extraction and feature selection.
arxiv-10800-229 | Modeling of the meaning: computational interpreting and understanding of natural language fragments | http://arxiv.org/pdf/1505.08149v1.pdf | author:Michael Kapustin, Pavlo Kapustin category:cs.CL published:2015-05-29 summary:In this introductory article we present the basics of an approach toimplementing computational interpreting of natural language aiming to model themeanings of words and phrases. Unlike other approaches, we attempt to definethe meanings of text fragments in a composable and computer interpretable way.We discuss models and ideas for detecting different types of semanticincomprehension and choosing the interpretation that makes most sense in agiven context. Knowledge representation is designed for handlingcontext-sensitive and uncertain / imprecise knowledge, and for easyaccommodation of new information. It stores quantitative information capturingthe essence of the concepts, because it is crucial for working with naturallanguage understanding and reasoning. Still, the representation is generalenough to allow for new knowledge to be learned, and even generated by thesystem. The article concludes by discussing some reasoning-related topics:possible approaches to generation of new abstract concepts, and describingsituations and concepts in words (e.g. for specifying interpretationdifficulties).
arxiv-10800-230 | Learning to count with deep object features | http://arxiv.org/pdf/1505.08082v1.pdf | author:Santi Seguí, Oriol Pujol, Jordi Vitrià category:cs.CV published:2015-05-29 summary:Learning to count is a learning strategy that has been recently proposed inthe literature for dealing with problems where estimating the number of objectinstances in a scene is the final objective. In this framework, the task oflearning to detect and localize individual object instances is seen as a hardertask that can be evaded by casting the problem as that of computing aregression value from hand-crafted image features. In this paper we explore thefeatures that are learned when training a counting convolutional neural networkin order to understand their underlying representation. To this end we define acounting problem for MNIST data and show that the internal representation ofthe network is able to classify digits in spite of the fact that no directsupervision was provided for them during training. We also present preliminaryresults about a deep network that is able to count the number of pedestrians ina scene.
arxiv-10800-231 | Transition-Based Dependency Parsing with Stack Long Short-Term Memory | http://arxiv.org/pdf/1505.08075v1.pdf | author:Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A. Smith category:cs.CL cs.LG cs.NE published:2015-05-29 summary:We propose a technique for learning representations of parser states intransition-based dependency parsers. Our primary innovation is a new controlstructure for sequence-to-sequence neural networks---the stack LSTM. Like theconventional stack data structures used in transition-based parsing, elementscan be pushed to or popped from the top of the stack in constant time, but, inaddition, an LSTM maintains a continuous space embedding of the stack contents.This lets us formulate an efficient parsing model that captures three facets ofa parser's state: (i) unbounded look-ahead into the buffer of incoming words,(ii) the complete history of actions taken by the parser, and (iii) thecomplete contents of the stack of partially built tree fragments, includingtheir internal structures. Standard backpropagation techniques are used fortraining and yield state-of-the-art parsing performance.
arxiv-10800-232 | Signal Recovery on Graphs: Variation Minimization | http://arxiv.org/pdf/1411.7414v3.pdf | author:Siheng Chen, Aliaksei Sandryhaila, José M. F. Moura, Jelena Kovačević category:cs.SI cs.LG stat.ML published:2014-11-26 summary:We consider the problem of signal recovery on graphs as graphs model datawith complex structure as signals on a graph. Graph signal recovery impliesrecovery of one or multiple smooth graph signals from noisy, corrupted, orincomplete measurements. We propose a graph signal model and formulate signalrecovery as a corresponding optimization problem. We provide a general solutionby using the alternating direction methods of multipliers. We next show howsignal inpainting, matrix completion, robust principal component analysis, andanomaly detection all relate to graph signal recovery, and providecorresponding specific solutions and theoretical analysis. Finally, we validatethe proposed methods on real-world recovery problems, including online blogclassification, bridge condition identification, temperature estimation,recommender system, and expert opinion combination of online blogclassification.
arxiv-10800-233 | Geometry of Graph Edit Distance Spaces | http://arxiv.org/pdf/1505.08071v1.pdf | author:Brijnesh J. Jain category:cs.CV math.MG published:2015-05-29 summary:In this paper we study the geometry of graph spaces endowed with a specialclass of graph edit distances. The focus is on geometrical results useful forstatistical pattern recognition. The main result is the Graph RepresentationTheorem. It states that a graph is a point in some geometrical space, calledorbit space. Orbit spaces are well investigated and easier to explore than theoriginal graph space. We derive a number of geometrical results from the orbitspace representation, translate them to the graph space, and indicate theirsignificance and usefulness in statistical pattern recognition.
arxiv-10800-234 | General Deformations of Point Configurations Viewed By a Pinhole Model Camera | http://arxiv.org/pdf/1505.08070v1.pdf | author:Yirmeyahu Kaminski, Mike Werman category:cs.CV math.AG published:2015-05-29 summary:This paper is a theoretical study of the following Non-Rigid Structure fromMotion problem. What can be computed from a monocular view of a parametricallydeforming set of points? We treat various variations of this problem for affineand polynomial deformations with calibrated and uncalibrated cameras. We showthat in general at least three images with quasi-identical two deformations areneeded in order to have a finite set of solutions of the points' structure andcalculate some simple examples.
arxiv-10800-235 | Signal Recovery on Graphs: Random versus Experimentally Designed Sampling | http://arxiv.org/pdf/1504.05427v2.pdf | author:Siheng Chen, Rohan Varma, Aarti Singh, Jelena Kovačević category:cs.IT math.IT stat.ML published:2015-04-21 summary:We study signal recovery on graphs based on two sampling strategies: randomsampling and experimentally designed sampling. We propose a new class of smoothgraph signals, called approximately bandlimited, which generalizes thebandlimited class and is similar to the globally smooth class. We then proposetwo recovery strategies based on random sampling and experimentally designedsampling. The proposed recovery strategy based on experimentally designedsampling is similar to the leverage scores used in the matrix approximation. Weshow that while both strategies are unbiased estimators for the low-frequencycomponents, the convergence rate of experimentally designed sampling is muchfaster than that of random sampling when a graph is irregular. We validate theproposed recovery strategies on three specific graphs: a ring graph, anErd\H{o}s-R\'enyi graph, and a star graph. The simulation results support thetheoretical analysis.
arxiv-10800-236 | Research on the fast Fourier transform of image based on GPU | http://arxiv.org/pdf/1505.08019v1.pdf | author:Feifei Shen, Zhenjian Song, Congrui Wu, Jiaqi Geng, Qingyun Wang category:cs.MS cs.CV published:2015-05-29 summary:Study of general purpose computation by GPU (Graphics Processing Unit) canimprove the image processing capability of micro-computer system. This paperstudies the parallelism of the different stages of decimation in time radix 2FFT algorithm, designs the butterfly and scramble kernels and implements 2D FFTon GPU. The experiment result demonstrates the validity and advantage overgeneral CPU, especially in the condition of large input size. The approach canalso be generalized to other transforms alike.
arxiv-10800-237 | Unsupervised Feature Learning with C-SVDDNet | http://arxiv.org/pdf/1412.7259v3.pdf | author:Dong Wang, Xiaoyang Tan category:cs.CV cs.LG cs.NE published:2014-12-23 summary:In this paper, we investigate the problem of learning feature representationfrom unlabeled data using a single-layer K-means network. A K-means networkmaps the input data into a feature representation by finding the nearestcentroid for each input point, which has attracted researchers' great attentionrecently due to its simplicity, effectiveness, and scalability. However, onedrawback of this feature mapping is that it tends to be unreliable when thetraining data contains noise. To address this issue, we propose a SVDD basedfeature learning algorithm that describes the density and distribution of eachcluster from K-means with an SVDD ball for more robust feature representation.For this purpose, we present a new SVDD algorithm called C-SVDD that centersthe SVDD ball towards the mode of local density of each cluster, and we showthat the objective of C-SVDD can be solved very efficiently as a linearprogramming problem. Additionally, traditional unsupervised feature learningmethods usually take an average or sum of local representations to obtainglobal representation which ignore spatial relationship among them. To usespatial information we propose a global representation with a variant of SIFTdescriptor. The architecture is also extended with multiple receptive fieldscales and multiple pooling sizes. Extensive experiments on several popularobject recognition benchmarks, such as STL-10, MINST, Holiday and Copydaysshows that the proposed C-SVDDNet method yields comparable or betterperformance than that of the previous state of the art methods.
arxiv-10800-238 | Symbolic Segmentation Using Algorithm Selection | http://arxiv.org/pdf/1505.07934v1.pdf | author:Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama category:cs.CV published:2015-05-29 summary:In this paper we present an alternative approach to symbolic segmentation;instead of implementing a new method we approach symbolic segmentation as analgorithm selection problem. That is, let there be $n$ available algorithms forsymbolic segmentation, a selection mechanism forms a set of input features andimage attributes and selects on a case by case basis the best algorithm. Theselection mechanism is demonstrated from within an algorithm framework wherethe selection is done in a set of various algorithm networks. Two sets ofexperiments are performed and in both cases we demonstrate that the algorithmselection allows to increase the result of the symbolic segmentation by aconsiderable amount.
arxiv-10800-239 | Supervised Fine Tuning for Word Embedding with Integrated Knowledge | http://arxiv.org/pdf/1505.07931v1.pdf | author:Xuefeng Yang, Kezhi Mao category:cs.CL published:2015-05-29 summary:Learning vector representation for words is an important research field whichmay benefit many natural language processing tasks. Two limitations exist innearly all available models, which are the bias caused by the contextdefinition and the lack of knowledge utilization. They are difficult to tacklebecause these algorithms are essentially unsupervised learning approaches.Inspired by deep learning, the authors propose a supervised framework forlearning vector representation of words to provide additional supervised finetuning after unsupervised learning. The framework is knowledge rich approacherand compatible with any numerical vectors word representation. The authorsperform both intrinsic evaluation like attributional and relational similarityprediction and extrinsic evaluations like the sentence completion and sentimentanalysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets showthat the proposed fine tuning framework may significantly improve the qualityof the vector representation of words.
arxiv-10800-240 | Salient Object Detection via Augmented Hypotheses | http://arxiv.org/pdf/1505.07930v1.pdf | author:Tam V. Nguyen, Jose Sepulveda category:cs.CV published:2015-05-29 summary:In this paper, we propose using \textit{augmented hypotheses} which considerobjectness, foreground and compactness for salient object detection. Ouralgorithm consists of four basic steps. First, our method generates theobjectness map via objectness hypotheses. Based on the objectness map, weestimate the foreground margin and compute the corresponding foreground mapwhich prefers the foreground objects. From the objectness map and theforeground map, the compactness map is formed to favor the compact objects. Wethen derive a saliency measure that produces a pixel-accurate saliency mapwhich uniformly covers the objects of interest and consistently separates fore-and background. We finally evaluate the proposed framework on two challengingdatasets, MSRA-1000 and iCoSeg. Our extensive experimental results show thatour method outperforms state-of-the-art approaches.
arxiv-10800-241 | On the Computational Complexity of High-Dimensional Bayesian Variable Selection | http://arxiv.org/pdf/1505.07925v1.pdf | author:Yun Yang, Martin J. Wainwright, Michael I. Jordan category:math.ST cs.LG stat.CO stat.ME stat.ML stat.TH published:2015-05-29 summary:We study the computational complexity of Markov chain Monte Carlo (MCMC)methods for high-dimensional Bayesian linear regression under sparsityconstraints. We first show that a Bayesian approach can achievevariable-selection consistency under relatively mild conditions on the designmatrix. We then demonstrate that the statistical criterion of posteriorconcentration need not imply the computational desideratum of rapid mixing ofthe MCMC algorithm. By introducing a truncated sparsity prior for variableselection, we provide a set of conditions that guarantee bothvariable-selection consistency and rapid mixing of a particularMetropolis-Hastings algorithm. The mixing time is linear in the number ofcovariates up to a logarithmic factor. Our proof controls the spectral gap ofthe Markov chain by constructing a canonical path ensemble that is inspired bythe steps taken by greedy algorithms for variable selection.
arxiv-10800-242 | Fast Computation of PERCLOS and Saccadic Ratio | http://arxiv.org/pdf/1505.07923v1.pdf | author:Anirban Dasgupta, Aurobinda Routray category:cs.CV published:2015-05-29 summary:This thesis describes the development of fast algorithms for the computationof PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR). PERCLOS and SRare two ocular parameters reported to be measures of alertness levels in humanbeings. PERCLOS is the percentage of time in which at least 80% of the eyelidremains closed over the pupil. Saccades are fast and simultaneous movement ofboth the eyes in the same direction. SR is the ratio of peak saccadic velocityto the saccadic duration. This thesis addresses the issues of image basedestimation of PERCLOS and SR, prevailing in the literature such as illuminationvariation, poor illumination conditions, head rotations etc. In this work,algorithms for real-time PERCLOS computation has been developed and implementedon an embedded platform. The platform has been used as a case study forassessment of loss of attention in automotive drivers. The SR estimation hasbeen carried out offline as real-time implementation requires high frame ratesof processing which is difficult to achieve due to hardware limitations. Theaccuracy in estimation of the loss of attention using PERCLOS and SR has beenvalidated using brain signals, which are reported to be an authentic cue forestimating the state of alertness in human beings. The major contributions ofthis thesis include database creation, design and implementation of fastalgorithms for estimating PERCLOS and SR on embedded computing platforms.
arxiv-10800-243 | Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network | http://arxiv.org/pdf/1505.07922v1.pdf | author:Junshi Huang, Rogerio S. Feris, Qiang Chen, Shuicheng Yan category:cs.CV published:2015-05-29 summary:We address the problem of cross-domain image retrieval, considering thefollowing practical application: given a user photo depicting a clothing image,our goal is to retrieve the same or attribute-similar clothing items fromonline shopping stores. This is a challenging problem due to the largediscrepancy between online shopping images, usually taken in ideallighting/pose/background conditions, and user photos captured in uncontrolledconditions. To address this problem, we propose a Dual Attribute-aware RankingNetwork (DARN) for retrieval feature learning. More specifically, DARN consistsof two sub-networks, one for each domain, whose retrieval featurerepresentations are driven by semantic attribute learning. We show that thisattribute-guided learning is a key factor for retrieval accuracy improvement.In addition, to further align with the nature of the retrieval problem, weimpose a triplet visual similarity constraint for learning to rank across thetwo sub-networks. Another contribution of our work is a large-scale datasetwhich makes the network learning feasible. We exploit customer review websitesto crawl a large set of online shopping images and corresponding offline userphotos with fine-grained clothing attributes, i.e., around 450,000 onlineshopping images and about 90,000 exact offline counterpart images of thoseonline ones. All these images are collected from real-world consumer websitesreflecting the diversity of the data modality, which makes this dataset uniqueand rare in the academic community. We extensively evaluate the retrievalperformance of networks in different configurations. The top-20 retrievalaccuracy is doubled when using the proposed DARN other than the current popularsolution using pre-trained CNN features only (0.570 vs. 0.268).
arxiv-10800-244 | Coordinate Descent with Arbitrary Sampling II: Expected Separable Overapproximation | http://arxiv.org/pdf/1412.8063v2.pdf | author:Zheng Qu, Peter Richtárik category:math.OC cs.LG cs.NA math.NA math.PR published:2014-12-27 summary:The design and complexity analysis of randomized coordinate descent methods,and in particular of variants which update a random subset (sampling) ofcoordinates in each iteration, depends on the notion of expected separableoverapproximation (ESO). This refers to an inequality involving the objectivefunction and the sampling, capturing in a compact way certain smoothnessproperties of the function in a random subspace spanned by the sampledcoordinates. ESO inequalities were previously established for special classesof samplings only, almost invariably for uniform samplings. In this paper wedevelop a systematic technique for deriving these inequalities for a largeclass of functions and for arbitrary samplings. We demonstrate that one canrecover existing ESO results using our general approach, which is based on thestudy of eigenvalues associated with samplings and the data describing thefunction.
arxiv-10800-245 | Reply to Garcia et al.: Common mistakes in measuring frequency dependent word characteristics | http://arxiv.org/pdf/1505.06750v2.pdf | author:P. S. Dodds, E. M. Clark, S. Desu, M. R. Frank, A. J. Reagan, J. R. Williams, L. Mitchell, K. D. Harris, I. M. Kloumann, J. P. Bagrow, K. Megerdoomian, M. T. McMahon, B. F. Tivnan, C. M. Danforth category:physics.soc-ph cs.CL published:2015-05-25 summary:We demonstrate that the concerns expressed by Garcia et al. are misplaced,due to (1) a misreading of our findings in [1]; (2) a widespread failure toexamine and present words in support of asserted summary quantities based onword usage frequencies; and (3) a range of misconceptions about word usagefrequency, word rank, and expert-constructed word lists. In particular, we showthat the English component of our study compares well statistically with tworelated surveys, that no survey design influence is apparent, and thatestimates of measurement error do not explain the positivity biases reported inour work and that of others. We further demonstrate that for the frequencydependence of positivity---of which we explored the nuances in great detail in[1]---Garcia et al. did not perform a reanalysis of our data---they insteadcarried out an analysis of a different, statistically improper data set andintroduced a nonlinearity before performing linear regression.
arxiv-10800-246 | Query by String word spotting based on character bi-gram indexing | http://arxiv.org/pdf/1505.07778v1.pdf | author:Suman K. Ghosh, Ernest Valveny category:cs.CV published:2015-05-28 summary:In this paper we propose a segmentation-free query by string word spottingmethod. Both the documents and query strings are encoded using a recentlyproposed word representa- tion that projects images and strings into a commonatribute space based on a pyramidal histogram of characters(PHOC). Theseattribute models are learned using linear SVMs over the Fisher Vectorrepresentation of the images along with the PHOC labels of the correspondingstrings. In order to search through the whole page, document regions areindexed per character bi- gram using a similar attribute representation. On topof that, we propose an integral image representation of the document using asimplified version of the attribute model for efficient computation. Finally weintroduce a re-ranking step in order to boost retrieval performance. We showstate-of-the-art results for segmentation-free query by string word spotting insingle-writer and multi-writer standard datasets
arxiv-10800-247 | A Data-Driven Approach for Tag Refinement and Localization in Web Videos | http://arxiv.org/pdf/1407.0623v3.pdf | author:Lamberto Ballan, Marco Bertini, Giuseppe Serra, Alberto Del Bimbo category:cs.CV cs.IR cs.MM published:2014-07-02 summary:Tagging of visual content is becoming more and more widespread as web-basedservices and social networks have popularized tagging functionalities amongtheir users. These user-generated tags are used to ease browsing andexploration of media collections, e.g. using tag clouds, or to retrievemultimedia content. However, not all media are equally tagged by users. Usingthe current systems is easy to tag a single photo, and even tagging a part of aphoto, like a face, has become common in sites like Flickr and Facebook. On theother hand, tagging a video sequence is more complicated and time consuming, sothat users just tag the overall content of a video. In this paper we present amethod for automatic video annotation that increases the number of tagsoriginally provided by users, and localizes them temporally, associating tagsto keyframes. Our approach exploits collective knowledge embedded inuser-generated tags and web sources, and visual similarity of keyframes andimages uploaded to social sites like YouTube and Flickr, as well as web sourceslike Google and Bing. Given a keyframe, our method is able to select on the flyfrom these visual sources the training exemplars that should be the mostrelevant for this test sample, and proceeds to transfer labels across similarimages. Compared to existing video tagging approaches that require trainingclassifiers for each tag, our system has few parameters, is easy to implementand can deal with an open vocabulary scenario. We demonstrate the approach ontag refinement and localization on DUT-WEBV, a large dataset of web videos, andshow state-of-the-art results.
arxiv-10800-248 | Human Social Interaction Modeling Using Temporal Deep Networks | http://arxiv.org/pdf/1505.02137v2.pdf | author:Mohamed R. Amer, Behjat Siddiquie, Amir Tamrakar, David A. Salter, Brian Lande, Darius Mehri, Ajay Divakaran category:cs.CY cs.LG published:2015-05-06 summary:We present a novel approach to computational modeling of social interactionsbased on modeling of essential social interaction predicates (ESIPs) such asjoint attention and entrainment. Based on sound social psychological theory andmethodology, we collect a new "Tower Game" dataset consisting of audio-visualcapture of dyadic interactions labeled with the ESIPs. We expect this datasetto provide a new avenue for research in computational social interactionmodeling. We propose a novel joint Discriminative Conditional RestrictedBoltzmann Machine (DCRBM) model that combines a discriminative component withthe generative power of CRBMs. Such a combination enables us to uncoveractionable constituents of the ESIPs in two steps. First, we train the DCRBMmodel on the labeled data and get accurate (76\%-49\% across various ESIPs)detection of the predicates. Second, we exploit the generative capability ofDCRBMs to activate the trained model so as to generate the lower-level datacorresponding to the specific ESIP that closely matches the actual trainingdata (with mean square error 0.01-0.1 for generating 100 frames). We are thusable to decompose the ESIPs into their constituent actionable behaviors. Such apurely computational determination of how to establish an ESIP such asengagement is unprecedented.
arxiv-10800-249 | A Category Theory of Communication Theory | http://arxiv.org/pdf/1505.07712v1.pdf | author:Eric Werner category:cs.IT cs.CL cs.LO math.IT published:2015-05-28 summary:A theory of how agents can come to understand a language is presented. Ifunderstanding a sentence $\alpha$ is to associate an operator with $\alpha$that transforms the representational state of the agent as intended by thesender, then coming to know a language involves coming to know the operatorsthat correspond to the meaning of any sentence. This involves a higher orderoperator that operates on the possible transformations that operate on therepresentational capacity of the agent. We formalize these constructs usingconcepts and diagrams analogous to category theory.
arxiv-10800-250 | Invertible Orientation Scores of 3D Images | http://arxiv.org/pdf/1505.07690v1.pdf | author:Michiel Janssen, Remco Duits, Marcel Breeuwer category:math.NA cs.CV published:2015-05-28 summary:The enhancement and detection of elongated structures in noisy image data isrelevant for many biomedical applications. To handle complex crossingstructures in 2D images, 2D orientation scores were introduced, which alreadyshowed their use in a variety of applications. Here we extend this work to 3Dorientation scores. First, we construct the orientation score from a givendataset, which is achieved by an invertible coherent state type of transform.For this transformation we introduce 3D versions of the 2D cake-wavelets, whichare complex wavelets that can simultaneously detect oriented structures andoriented edges. For efficient implementation of the different steps in thewavelet creation we use a spherical harmonic transform. Finally, we show somefirst results of practical applications of 3D orientation scores.
arxiv-10800-251 | Improved Deep Convolutional Neural Network For Online Handwritten Chinese Character Recognition using Domain-Specific Knowledge | http://arxiv.org/pdf/1505.07675v1.pdf | author:Weixin Yang, Lianwen Jin, Zecheng Xie, Ziyong Feng category:cs.CV published:2015-05-28 summary:Deep convolutional neural networks (DCNNs) have achieved great success invarious computer vision and pattern recognition applications, including thosefor handwritten Chinese character recognition (HCCR). However, most currentDCNN-based HCCR approaches treat the handwritten sample simply as an imagebitmap, ignoring some vital domain-specific information that may be useful butthat cannot be learnt by traditional networks. In this paper, we propose anenhancement of the DCNN approach to online HCCR by incorporating a variety ofdomain-specific knowledge, including deformation, non-linear normalization,imaginary strokes, path signature, and 8-directional features. Our contributionis twofold. First, these domain-specific technologies are investigated andintegrated with a DCNN to form a composite network to achieve improvedperformance. Second, the resulting DCNNs with diversity in their domainknowledge are combined using a hybrid serial-parallel (HSP) strategy.Consequently, we achieve a promising accuracy of 97.20% and 96.87% onCASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the bestresults previously reported in the literature.
arxiv-10800-252 | A Generative Model of Natural Texture Surrogates | http://arxiv.org/pdf/1505.07672v1.pdf | author:Niklas Ludtke, Debapriya Das, Lucas Theis, Matthias Bethge category:cs.CV published:2015-05-28 summary:Natural images can be viewed as patchworks of different textures, where thelocal image statistics is roughly stationary within a small neighborhood butotherwise varies from region to region. In order to model this variability, wefirst applied the parametric texture algorithm of Portilla and Simoncelli toimage patches of 64X64 pixels in a large database of natural images such thateach image patch is then described by 655 texture parameters which specifycertain statistics, such as variances and covariances of wavelet coefficientsor coefficient magnitudes within that patch. To model the statistics of these texture parameters, we then developedsuitable nonlinear transformations of the parameters that allowed us to fittheir joint statistics with a multivariate Gaussian distribution. We find thatthe first 200 principal components contain more than 99% of the variance andare sufficient to generate textures that are perceptually extremely close tothose generated with all 655 components. We demonstrate the usefulness of themodel in several ways: (1) We sample ensembles of texture patches that can bedirectly compared to samples of patches from the natural image database and canto a high degree reproduce their perceptual appearance. (2) We furtherdeveloped an image compression algorithm which generates surprisingly accurateimages at bit rates as low as 0.14 bits/pixel. Finally, (3) We demonstrate howour approach can be used for an efficient and objective evaluation of samplesgenerated with probabilistic models of natural images.
arxiv-10800-253 | A trust-region method for stochastic variational inference with applications to streaming data | http://arxiv.org/pdf/1505.07649v1.pdf | author:Lucas Theis, Matthew D. Hoffman category:stat.ML stat.AP published:2015-05-28 summary:Stochastic variational inference allows for fast posterior inference incomplex Bayesian models. However, the algorithm is prone to local optima whichcan make the quality of the posterior approximation sensitive to the choice ofhyperparameters and initialization. We address this problem by replacing thenatural gradient step of stochastic varitional inference with a trust-regionupdate. We show that this leads to generally better results and reducedsensitivity to hyperparameters. We also describe a new strategy for variationalinference on streaming data and show that here our trust-region method iscrucial for getting good performance.
arxiv-10800-254 | Learning with Symmetric Label Noise: The Importance of Being Unhinged | http://arxiv.org/pdf/1505.07634v1.pdf | author:Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson category:cs.LG published:2015-05-28 summary:Convex potential minimisation is the de facto approach to binaryclassification. However, Long and Servedio [2010] proved that under symmetriclabel noise (SLN), minimisation of any convex potential over a linear functionclass can result in classification performance equivalent to random guessing.This ostensibly shows that convex losses are not SLN-robust. In this paper, wepropose a convex, classification-calibrated loss and prove that it isSLN-robust. The loss avoids the Long and Servedio [2010] result by virtue ofbeing negatively unbounded. The loss is a modification of the hinge loss, whereone does not clamp at zero; hence, we call it the unhinged loss. We show thatthe optimal unhinged solution is equivalent to that of a strongly regularisedSVM, and is the limiting solution for any convex potential; this implies thatstrong l2 regularisation makes most standard learners SLN-robust. Experimentsconfirm the SLN-robustness of the unhinged loss.
arxiv-10800-255 | Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons | http://arxiv.org/pdf/1504.07218v2.pdf | author:Yuxin Chen, Changho Suh category:cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH published:2015-04-27 summary:This paper explores the preference-based top-$K$ rank aggregation problem.Suppose that a collection of items is repeatedly compared in pairs, and onewishes to recover a consistent ordering that emphasizes the top-$K$ rankeditems, based on partially revealed preferences. We focus on theBradley-Terry-Luce (BTL) model that postulates a set of latent preferencescores underlying all items, where the odds of paired comparisons depend onlyon the relative scores of the items involved. We characterize the minimax limits on identifiability of top-$K$ rankeditems, in the presence of random and non-adaptive sampling. Our resultshighlight a separation measure that quantifies the gap of preference scoresbetween the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimumsample complexity required for reliable top-$K$ ranking scales inversely withthe separation measure irrespective of other preference distribution metrics.To approach this minimax limit, we propose a nearly linear-time ranking scheme,called \emph{Spectral MLE}, that returns the indices of the top-$K$ items inaccordance to a careful score estimate. In a nutshell, Spectral MLE starts withan initial score estimate with minimal squared loss (obtained via a spectralmethod), and then successively refines each component with the assistance ofcoordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ itemidentification under minimal sample complexity. The practical applicability ofSpectral MLE is further corroborated by numerical experiments.
arxiv-10800-256 | Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures | http://arxiv.org/pdf/1505.07522v1.pdf | author:Miriam Redi, Daniele Quercia, Lindsay T. Graham, Samuel D. Gosling category:cs.HC cs.CV cs.CY published:2015-05-28 summary:To choose restaurants and coffee shops, people are increasingly relying onsocial-networking sites. In a popular site such as Foursquare or Yelp, a placecomes with descriptions and reviews, and with profile pictures of people whofrequent them. Descriptions and reviews have been widely explored in theresearch area of data mining. By contrast, profile pictures have receivedlittle attention. Previous work showed that people are able to partly guess aplace's ambiance, clientele, and activities not only by observing the placeitself but also by observing the profile pictures of its visitors. Here wefurther that work by determining which visual cues people may have relied uponto make their guesses; showing that a state-of-the-art algorithm could makepredictions more accurately than humans at times; and demonstrating that thevisual cues people relied upon partly differ from those of the algorithm.
arxiv-10800-257 | A deep-structured fully-connected random field model for structured inference | http://arxiv.org/pdf/1412.6586v3.pdf | author:Alexander Wong, Mohammad Javad Shafiee, Parthipan Siva, Xiao Yu Wang category:stat.ML cs.IT cs.LG math.IT stat.ME published:2014-12-20 summary:There has been significant interest in the use of fully-connected graphicalmodels and deep-structured graphical models for the purpose of structuredinference. However, fully-connected and deep-structured graphical models havebeen largely explored independently, leaving the unification of these twoconcepts ripe for exploration. A fundamental challenge with unifying these twotypes of models is in dealing with computational complexity. In this study, weinvestigate the feasibility of unifying fully-connected and deep-structuredmodels in a computationally tractable manner for the purpose of structuredinference. To accomplish this, we introduce a deep-structured fully-connectedrandom field (DFRF) model that integrates a series of intermediate sparseauto-encoding layers placed between state layers to significantly reducecomputational complexity. The problem of image segmentation was used toillustrate the feasibility of using the DFRF for structured inference in acomputationally tractable manner. Results in this study show that it isfeasible to unify fully-connected and deep-structured models in acomputationally tractable manner for solving structured inference problems suchas image segmentation.
arxiv-10800-258 | Robots that can adapt like animals | http://arxiv.org/pdf/1407.3501v4.pdf | author:Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret category:cs.RO cs.AI cs.LG cs.NE q-bio.NC published:2014-07-13 summary:As robots leave the controlled environments of factories to autonomouslyfunction in more complex, natural environments, they will have to respond tothe inevitable fact that they will become damaged. However, while animals canquickly adapt to a wide variety of injuries, current robots cannot "thinkoutside the box" to find a compensatory behavior when damaged: they are limitedto their pre-specified self-sensing abilities, can diagnose only anticipatedfailure modes, and require a pre-programmed contingency plan for every type ofpotential damage, an impracticality for complex robots. Here we introduce anintelligent trial and error algorithm that allows robots to adapt to damage inless than two minutes, without requiring self-diagnosis or pre-specifiedcontingency plans. Before deployment, a robot exploits a novel algorithm tocreate a detailed map of the space of high-performing behaviors: This maprepresents the robot's intuitions about what behaviors it can perform and theirvalue. If the robot is damaged, it uses these intuitions to guide atrial-and-error learning algorithm that conducts intelligent experiments torapidly discover a compensatory behavior that works in spite of the damage.Experiments reveal successful adaptations for a legged robot injured in fivedifferent ways, including damaged, broken, and missing legs, and for a roboticarm with joints broken in 14 different ways. This new technique will enablemore robust, effective, autonomous robots, and suggests principles that animalsmay use to adapt to injury.
arxiv-10800-259 | Supervised detection of anomalous light-curves in massive astronomical catalogs | http://arxiv.org/pdf/1404.4888v3.pdf | author:Isadora Nun, Karim Pichara, Pavlos Protopapas, Dae-Won Kim category:cs.CE astro-ph.IM cs.LG published:2014-04-18 summary:The development of synoptic sky surveys has led to a massive amount of datafor which resources needed for analysis are beyond human capabilities. Toprocess this information and to extract all possible knowledge, machinelearning techniques become necessary. Here we present a new method toautomatically discover unknown variable objects in large astronomical catalogs.With the aim of taking full advantage of all the information we have aboutknown objects, our method is based on a supervised algorithm. In particular, wetrain a random forest classifier using known variability classes of objects andobtain votes for each of the objects in the training set. We then model thisvoting distribution with a Bayesian network and obtain the joint votingdistribution among the training objects. Consequently, an unknown object isconsidered as an outlier insofar it has a low joint probability. Our method issuitable for exploring massive datasets given that the training process isperformed offline. We tested our algorithm on 20 millions light-curves from theMACHO catalog and generated a list of anomalous candidates. We divided thecandidates into two main classes of outliers: artifacts and intrinsic outliers.Artifacts were principally due to air mass variation, seasonal variation, badcalibration or instrumental errors and were consequently removed from ouroutlier list and added to the training set. After retraining, we selected about4000 objects, which we passed to a post analysis stage by perfoming across-match with all publicly available catalogs. Within these candidates weidentified certain known but rare objects such as eclipsing Cepheids, bluevariables, cataclysmic variables and X-ray sources. For some outliers therewere no additional information. Among them we identified three unknownvariability types and few individual outliers that will be followed up for adeeper analysis.
arxiv-10800-260 | Compositional Vector Space Models for Knowledge Base Completion | http://arxiv.org/pdf/1504.06662v2.pdf | author:Arvind Neelakantan, Benjamin Roth, Andrew McCallum category:cs.CL stat.ML published:2015-04-24 summary:Knowledge base (KB) completion adds new facts to a KB by making inferencesfrom existing facts, for example by inferring with high likelihoodnationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hoprelational synonyms like this, or use as evidence a multi-hop relational pathtreated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paperpresents an approach that reasons about conjunctions of multi-hop relationsnon-atomically, composing the implications of a path using a recursive neuralnetwork (RNN) that takes as inputs vector embeddings of the binary relation inthe path. Not only does this allow us to generalize to paths unseen at trainingtime, but also, with a single high-capacity RNN, to predict new relation typesnot seen when the compositional model was trained (zero-shot learning). Weassemble a new dataset of over 52M relational triples, and show that our methodimproves over a traditional classifier by 11%, and a method leveragingpre-trained embeddings by 7%.
arxiv-10800-261 | Post-acquisition image based compensation for thickness variation in microscopy section series | http://arxiv.org/pdf/1411.6970v2.pdf | author:Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld category:cs.CV q-bio.QM stat.AP published:2014-11-25 summary:Serial section Microscopy is an established method for volumetric anatomyreconstruction. Section series imaged with Electron Microscopy are currentlyvital for the reconstruction of the synaptic connectivity of entire animalbrains such as that of Drosophila melanogaster. The process of removingultrathin layers from a solid block containing the specimen, however, is afragile procedure and has limited precision with respect to section thickness.We have developed a method to estimate the relative z-position of eachindividual section as a function of signal change across the section series.First experiments show promising results on both serial section TransmissionElectron Microscopy (ssTEM) data and Focused Ion Beam Scanning ElectronMicroscopy (FIB-SEM) series. We made our solution available as Open Sourceplugins for the TrakEM2 software and the ImageJ distribution Fiji.
arxiv-10800-262 | Training a Convolutional Neural Network for Appearance-Invariant Place Recognition | http://arxiv.org/pdf/1505.07428v1.pdf | author:Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier Gonzalez-Jimenez category:cs.CV cs.LG cs.RO published:2015-05-27 summary:Place recognition is one of the most challenging problems in computer vision,and has become a key part in mobile robotics and autonomous drivingapplications for performing loop closure in visual SLAM systems. Moreover, thedifficulty of recognizing a revisited location increases with appearancechanges caused, for instance, by weather or illumination variations, whichhinders the long-term application of such algorithms in real environments. Inthis paper we present a convolutional neural network (CNN), trained for thefirst time with the purpose of recognizing revisited locations under severeappearance changes, which maps images to a low dimensional space whereEuclidean distances represent place dissimilarity. In order for the network tolearn the desired invariances, we train it with triplets of images selectedfrom datasets which present a challenging variability in visual appearance. Thetriplets are selected in such way that two samples are from the same locationand the third one is taken from a different place. We validate our systemthrough extensive experimentation, where we demonstrate better performance thanstate-of-art algorithms in a number of popular datasets.
arxiv-10800-263 | Improving Spatial Codification in Semantic Segmentation | http://arxiv.org/pdf/1505.07409v1.pdf | author:Carles Ventura, Xavier Giró-i-Nieto, Verónica Vilaplana, Kevin McGuinness, Ferran Marqués, Noel E. O'Connor category:cs.CV published:2015-05-27 summary:This paper explores novel approaches for improving the spatial codificationfor the pooling of local descriptors to solve the semantic segmentationproblem. We propose to partition the image into three regions for each objectto be described: Figure, Border and Ground. This partition aims at minimizingthe influence of the image context on the object description and vice versa byintroducing an intermediate zone around the object contour. Furthermore, wealso propose a richer visual descriptor of the object by applying a SpatialPyramid over the Figure region. Two novel Spatial Pyramid configurations areexplored: Cartesian-based and crown-based Spatial Pyramids. We test theseapproaches with state-of-the-art techniques and show that they improve theFigure-Ground based pooling in the Pascal VOC 2011 and 2012 semanticsegmentation challenges.
arxiv-10800-264 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling | http://arxiv.org/pdf/1505.07293v1.pdf | author:Vijay Badrinarayanan, Ankur Handa, Roberto Cipolla category:cs.CV published:2015-05-27 summary:We propose a novel deep architecture, SegNet, for semantic pixel wise imagelabelling. SegNet has several attractive properties; (i) it only requiresforward evaluation of a fully learnt function to obtain smooth labelpredictions, (ii) with increasing depth, a larger context is considered forpixel labelling which improves accuracy, and (iii) it is easy to visualise theeffect of feature activation(s) in the pixel label space at any depth. SegNetis composed of a stack of encoders followed by a corresponding decoder stackwhich feeds into a soft-max classification layer. The decoders help map lowresolution feature maps at the output of the encoder stack to full input imagesize feature maps. This addresses an important drawback of recent deep learningapproaches which have adopted networks designed for object categorization forpixel wise labelling. These methods lack a mechanism to map deep layer featuremaps to input dimensions. They resort to ad hoc methods to upsample features,e.g. by replication. This results in noisy predictions and also restricts thenumber of pooling layers in order to avoid too much upsampling and thus reducesspatial context. SegNet overcomes these problems by learning to map encoderoutputs to image pixel labels. We test the performance of SegNet on outdoor RGBscenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our resultsshow that SegNet achieves state-of-the-art performance even without use ofadditional cues such as depth, video frames or post-processing with CRF models.
arxiv-10800-265 | A low variance consistent test of relative dependency | http://arxiv.org/pdf/1406.3852v3.pdf | author:Wacha Bounliphone, Arthur Gretton, Arthur Tenenhaus, Matthew Blaschko category:stat.ML cs.LG stat.CO published:2014-06-15 summary:We describe a novel non-parametric statistical hypothesis test of relativedependence between a source variable and two candidate target variables. Such atest enables us to determine whether one source variable is significantly moredependent on a first target variable or a second. Dependence is measured viathe Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair ofempirical dependence measures (source-target 1, source-target 2). We testwhether the first dependence measure is significantly larger than the second.Modeling the covariance between these HSIC statistics leads to a provably morepowerful test than the construction of independent HSIC statistics bysub-sampling. The resulting test is consistent and unbiased, and (being basedon U-statistics) has favorable convergence properties. The test can be computedin quadratic time, matching the computational complexity of standard empiricalHSIC estimators. The effectiveness of the test is demonstrated on severalreal-world problems: we identify language groups from a multilingual corpus,and we prove that tumor location is more dependent on gene expression thanchromosomal imbalances. Source code is available for download athttps://github.com/wbounliphone/reldep.
arxiv-10800-266 | Understanding Zipf's law of word frequencies through sample-space collapse in sentence formation | http://arxiv.org/pdf/1407.4610v2.pdf | author:Stefan Thurner, Rudolf Hanel, Bo Liu, Bernat Corominas-Murtra category:physics.soc-ph cs.CL published:2014-07-17 summary:The formation of sentences is a highly structured and history-dependentprocess. The probability of using a specific word in a sentence stronglydepends on the 'history' of word-usage earlier in that sentence. We study asimple history-dependent model of text generation assuming that thesample-space of word usage reduces along sentence formation, on average. Wefirst show that the model explains the approximate Zipf law found in wordfrequencies as a direct consequence of sample-space reduction. We thenempirically quantify the amount of sample-space reduction in the sentences often famous English books, by analysis of corresponding word-transition tablesthat capture which words can follow any given word in a text. We find a highlynested structure in these transition tables and show that this `nestedness' istightly related to the power law exponents of the observed word frequencydistributions. With the proposed model it is possible to understand that thenestedness of a text can be the origin of the actual scaling exponent, and thatdeviations from the exact Zipf law can be understood by variations of thedegree of nestedness on a book-by-book basis. On a theoretical level we areable to show that in case of weak nesting, Zipf's law breaks down in a fasttransition. Unlike previous attempts to understand Zipf's law in language thesample-space reducing model is not based on assumptions of multiplicative,preferential, or self-organised critical mechanisms behind language formation,but simply used the empirically quantifiable parameter 'nestedness' tounderstand the statistics of word frequencies.
arxiv-10800-267 | Global Convergence of Unmodified 3-Block ADMM for a Class of Convex Minimization Problems | http://arxiv.org/pdf/1505.04252v3.pdf | author:Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC cs.LG stat.ML published:2015-05-16 summary:The alternating direction method of multipliers (ADMM) has been successfullyapplied to solve structured convex optimization problems due to its superiorpractical performance. The convergence properties of the 2-block ADMM have beenstudied extensively in the literature. Specifically, it has been proven thatthe 2-block ADMM globally converges for any penalty parameter $\gamma>0$. Inthis sense, the 2-block ADMM allows the parameter to be free, i.e., there is noneed to restrict the value for the parameter when implementing this algorithmin order to ensure convergence. However, for the 3-block ADMM, Chen et al.recently constructed a counter-example showing that it can diverge if nofurther condition is imposed. The existing results on studying furthersufficient conditions on guaranteeing the convergence of the 3-block ADMMusually require $\gamma$ to be smaller than a certain bound, which is usuallyeither difficult to compute or too small to make it a practical algorithm. Inthis paper, we show that the 3-block ADMM still globally converges with anypenalty parameter $\gamma>0$ when applied to solve a class of commonlyencountered problems to be called regularized least squares decomposition(RLSD) in this paper, which covers many important applications in practice.
arxiv-10800-268 | Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews | http://arxiv.org/pdf/1412.5335v7.pdf | author:Grégoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio category:cs.CL cs.IR cs.LG cs.NE published:2014-12-17 summary:Sentiment analysis is a common task in natural language processing that aimsto detect polarity of a text document (typically a consumer review). In thesimplest settings, we discriminate only between positive and negativesentiment, turning the task into a standard binary classification problem. Wecompare several ma- chine learning approaches to this problem, and combine themto achieve the best possible results. We show how to use for this task thestandard generative lan- guage models, which are slightly complementary to thestate of the art techniques. We achieve strong results on a well-known datasetof IMDB movie reviews. Our results are easily reproducible, as we publish alsothe code needed to repeat the experiments. This should simplify further advanceof the state of the art, as other researchers can combine their techniques withours with little effort.
arxiv-10800-269 | New characterizations of minimum spanning trees and of saliency maps based on quasi-flat zones | http://arxiv.org/pdf/1505.07203v1.pdf | author:Jean Cousty, Laurent Najman, Yukiko Kenmochi, Silvio GuimarÃ£es category:cs.CV cs.DS published:2015-05-27 summary:We study three representations of hierarchies of partitions: dendrograms(direct representations), saliency maps, and minimum spanning trees. We providea new bijection between saliency maps and hierarchies based on quasi-flat zonesas used in image processing and characterize saliency maps and minimum spanningtrees as solutions to constrained minimization problems where the constraint isquasi-flat zones preservation. In practice, these results form a toolkit fornew hierarchical methods where one can choose the most convenientrepresentation. They also invite us to process non-image data withmorphological hierarchies.
arxiv-10800-270 | On using the Microsoft Kinect$^{\rm TM}$ sensors in the analysis of human motion | http://arxiv.org/pdf/1412.2032v4.pdf | author:M. J. Malinowski, E. Matsinos, S. Roth category:physics.med-ph cs.CV cs.RO published:2014-12-04 summary:The present paper aims at providing the theoretical background required forinvestigating the use of the Microsoft Kinect$^{\rm TM}$ (`Kinect', for short)sensors (original and upgraded) in the analysis of human motion. Ourmethodology is developed in such a way that its application be easily adaptableto comparative studies of other systems used in capturing human-motion data.Our future plans include the application of this methodology to two situations:first, in a comparative study of the performance of the two Kinect sensors;second, in pursuing their validation on the basis of comparisons with amarker-based system (MBS). One important feature in our approach is thetransformation of the MBS output into Kinect-output format, thus enabling theanalysis of the measurements, obtained from different systems, with the samesoftware application, i.e., the one we use in the analysis of Kinect-captureddata; one example of such a transformation, for one popular marker-placementscheme (`Plug-in Gait'), is detailed. We propose that the similarity of theoutput, obtained from the different systems, be assessed on the basis of thecomparison of a number of waveforms, representing the variation within the gaitcycle of quantities which are commonly used in the modelling of the humanmotion. The data acquisition may involve commercially-available treadmills anda number of velocity settings: for instance, walking-motion data may beacquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommendthat particular attention be called to systematic effects associated with thesubject's knee and lower leg, as well as to the ability of the Kinect sensorsin reliably capturing the details in the asymmetry of the motion for the leftand right parts of the human body. The previous versions of the study have beenwithdrawn due to the use of a non-representative database.
arxiv-10800-271 | Learning Transferable Features with Deep Adaptation Networks | http://arxiv.org/pdf/1502.02791v2.pdf | author:Mingsheng Long, Yue Cao, Jianmin Wang, Michael I. Jordan category:cs.LG published:2015-02-10 summary:Recent studies reveal that a deep neural network can learn transferablefeatures which generalize well to novel tasks for domain adaptation. However,as deep features eventually transition from general to specific along thenetwork, the feature transferability drops significantly in higher layers withincreasing domain discrepancy. Hence, it is important to formally reduce thedataset bias and enhance the transferability in task-specific layers. In thispaper, we propose a new Deep Adaptation Network (DAN) architecture, whichgeneralizes deep convolutional neural network to the domain adaptationscenario. In DAN, hidden representations of all task-specific layers areembedded in a reproducing kernel Hilbert space where the mean embeddings ofdifferent domain distributions can be explicitly matched. The domaindiscrepancy is further reduced using an optimal multi-kernel selection methodfor mean embedding matching. DAN can learn transferable features withstatistical guarantees, and can scale linearly by unbiased estimate of kernelembedding. Extensive empirical evidence shows that the proposed architectureyields state-of-the-art image classification error rates on standard domainadaptation benchmarks.
arxiv-10800-272 | Inner and Inter Label Propagation: Salient Object Detection in the Wild | http://arxiv.org/pdf/1505.07192v1.pdf | author:Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price category:cs.CV published:2015-05-27 summary:In this paper, we propose a novel label propagation based method for saliencydetection. A key observation is that saliency in an image can be estimated bypropagating the labels extracted from the most certain background and objectregions. For most natural images, some boundary superpixels serve as thebackground labels and the saliency of other superpixels are determined byranking their similarities to the boundary labels based on an inner propagationscheme. For images of complex scenes, we further deploy a 3-cue-center-biasedobjectness measure to pick out and propagate foreground labels. Aco-transduction algorithm is devised to fuse both boundary and objectnesslabels based on an inter propagation scheme. The compactness criterion decideswhether the incorporation of objectness labels is necessary, thus greatlyenhancing computational efficiency. Results on five benchmark datasets withpixel-wise accurate annotations show that the proposed method achieves superiorperformance compared with the newest state-of-the-arts in terms of differentevaluation metrics.
arxiv-10800-273 | Unsupervised Cross-Domain Word Representation Learning | http://arxiv.org/pdf/1505.07184v1.pdf | author:Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL published:2015-05-27 summary:Meaning of a word varies from one domain to another. Despite this importantdomain dependence in word semantics, existing word representation learningmethods are bound to a single domain. Given a pair of\emph{source}-\emph{target} domains, we propose an unsupervised method forlearning domain-specific word representations that accurately capture thedomain-specific aspects of word semantics. First, we select a subset offrequent words that occur in both domains as \emph{pivots}. Next, we optimizean objective function that enforces two constraints: (a) for both source andtarget domain documents, pivots that appear in a document must accuratelypredict the co-occurring non-pivots, and (b) word representations learnt forpivots must be similar in the two domains. Moreover, we propose a method toperform domain adaptation using the learnt word representations. Our proposedmethod significantly outperforms competitive baselines including thestate-of-the-art domain-insensitive word representations, and reports bestsentiment classification accuracies for all domain-pairs in a benchmarkdataset.
arxiv-10800-274 | Caffe con Troll: Shallow Ideas to Speed Up Deep Learning | http://arxiv.org/pdf/1504.04343v2.pdf | author:Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher Ré category:cs.LG cs.CV stat.ML published:2015-04-16 summary:We present Caffe con Troll (CcT), a fully compatible end-to-end version ofthe popular framework Caffe with rebuilt internals. We built CcT to examine theperformance characteristics of training and deploying general-purposeconvolutional neural networks across different hardware architectures. We findthat, by employing standard batching optimizations for CPU training, we achievea 4.5x throughput improvement over Caffe on popular networks like CaffeNet.Moreover, with these improvements, the end-to-end training time for CNNs isdirectly proportional to the FLOPS delivered by the CPU, which enables us toefficiently train hybrid CPU-GPU systems for CNNs.
arxiv-10800-275 | Achieving Exact Cluster Recovery Threshold via Semidefinite Programming: Extensions | http://arxiv.org/pdf/1502.07738v2.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.SI math.PR published:2015-02-26 summary:Resolving a conjecture of Abbe, Bandeira and Hall, the authors have recentlyshown that the semidefinite programming (SDP) relaxation of the maximumlikelihood estimator achieves the sharp threshold for exactly recovering thecommunity structure under the binary stochastic block model of two equal-sizedclusters. The same was shown for the case of a single cluster and outliers.Extending the proof techniques, in this paper it is shown that SDP relaxationsalso achieve the sharp recovery threshold in the following cases: (1) Binarystochastic block model with two clusters of sizes proportional to network sizebut not necessarily equal; (2) Stochastic block model with a fixed number ofequal-sized clusters; (3) Binary censored block model with the background graphbeing Erd\H{o}s-R\'enyi. Furthermore, a sufficient condition is given for anSDP procedure to achieve exact recovery for the general case of a fixed numberof clusters plus outliers. These results demonstrate the versatility of SDPrelaxation as a simple, general purpose, computationally feasible methodologyfor community detection.
arxiv-10800-276 | Belief Flows of Robust Online Learning | http://arxiv.org/pdf/1505.07067v1.pdf | author:Pedro A. Ortega, Koby Crammer, Daniel D. Lee category:stat.ML cs.LG published:2015-05-26 summary:This paper introduces a new probabilistic model for online learning whichdynamically incorporates information from stochastic gradients of an arbitraryloss function. Similar to probabilistic filtering, the model maintains aGaussian belief over the optimal weight parameters. Unlike traditional Bayesianupdates, the model incorporates a small number of gradient evaluations atlocations chosen using Thompson sampling, making it computationally tractable.The belief is then transformed via a linear flow field which optimally updatesthe belief distribution using rules derived from information theoreticprinciples. Several versions of the algorithm are shown using differentconstraints on the flow field and compared with conventional online learningalgorithms. Results are given for several classification tasks includinglogistic regression and multilayer neural networks.
arxiv-10800-277 | Understanding Image Virality | http://arxiv.org/pdf/1503.02318v3.pdf | author:Arturo Deza, Devi Parikh category:cs.SI cs.CV published:2015-03-08 summary:Virality of online content on social networking websites is an important butesoteric phenomenon often studied in fields like marketing, psychology and datamining. In this paper we study viral images from a computer vision perspective.We introduce three new image datasets from Reddit, and define a virality scoreusing Reddit metadata. We train classifiers with state-of-the-art imagefeatures to predict virality of individual images, relative virality in pairsof images, and the dominant topic of a viral image. We also compare machineperformance to human performance on these tasks. We find that computers performpoorly with low level features, and high level information is critical forpredicting virality. We encode semantic information through relativeattributes. We identify the 5 key visual attributes that correlate withvirality. We create an attribute-based characterization of images that canpredict relative virality with 68.10% accuracy (SVM+Deep Relative Attributes)-- better than humans at 60.12%. Finally, we study how human prediction ofimage virality varies with different `contexts' in which the images are viewed,such as the influence of neighbouring images, images recently viewed, as wellas the image title or caption. This work is a first step in understanding thecomplex but important phenomenon of image virality. Our datasets andannotations will be made publicly available.
arxiv-10800-278 | An Overview of the Asymptotic Performance of the Family of the FastICA Algorithms | http://arxiv.org/pdf/1505.07008v1.pdf | author:Tianwen Wei category:stat.ML cs.LG published:2015-05-26 summary:This contribution summarizes the results on the asymptotic performance ofseveral variants of the FastICA algorithm. A number of new closed-formexpressions are presented.
arxiv-10800-279 | Some Open Problems in Optimal AdaBoost and Decision Stumps | http://arxiv.org/pdf/1505.06999v1.pdf | author:Joshua Belanich, Luis E. Ortiz category:cs.LG stat.ML published:2015-05-26 summary:The significance of the study of the theoretical and practical properties ofAdaBoost is unquestionable, given its simplicity, wide practical use, andeffectiveness on real-world datasets. Here we present a few open problemsregarding the behavior of "Optimal AdaBoost," a term coined by Rudin,Daubechies, and Schapire in 2004 to label the simple version of the standardAdaBoost algorithm in which the weak learner that AdaBoost uses always outputsthe weak classifier with lowest weighted error among the respective hypothesisclass of weak classifiers implicit in the weak learner. We concentrate on thestandard, "vanilla" version of Optimal AdaBoost for binary classification thatresults from using an exponential-loss upper bound on the misclassificationtraining error. We present two types of open problems. One deals with generalweak hypotheses. The other deals with the particular case of decision stumps,as often and commonly used in practice. Answers to the open problems can haveimmediate significant impact to (1) cementing previously established results onasymptotic convergence properties of Optimal AdaBoost, for finite datasets,which in turn can be the start to any convergence-rate analysis; (2)understanding the weak-hypotheses class of effective decision stumps generatedfrom data, which we have empirically observed to be significantly smaller thanthe typically obtained class, as well as the effect on the weak learner'srunning time and previously established improved bounds on the generalizationperformance of Optimal AdaBoost classifiers; and (3) shedding some light on the"self control" that AdaBoost tends to exhibit in practice.
arxiv-10800-280 | Sequential Dimensionality Reduction for Extracting Localized Features | http://arxiv.org/pdf/1505.06957v1.pdf | author:Gabriella Casalino, Nicolas Gillis category:cs.CV cs.LG cs.NA math.NA stat.ML published:2015-05-26 summary:Linear dimensionality reduction techniques are powerful tools for imageanalysis as they allow the identification of important features in a data set.In particular, nonnegative matrix factorization (NMF) has become very popularas it is able to extract sparse, localized and easily interpretable features byimposing an additive combination of nonnegative basis elements. Nonnegativematrix underapproximation (NMU) is a closely related technique that has theadvantage to identify features sequentially. In this paper, we propose avariant of NMU that is particularly well suited for image analysis as itincorporates the spatial information, that is, it takes into account the factthat neighboring pixels are more likely to be contained in the same features,and favors the extraction of localized features by looking for sparse basiselements. We show that our new approach competes favorably with comparablestate-of-the-art techniques on several facial and hyperspectral image datasets.
arxiv-10800-281 | Approximate Joint Diagonalization and Geometric Mean of Symmetric Positive Definite Matrices | http://arxiv.org/pdf/1505.07343v1.pdf | author:Marco Congedo, Bijan Afsari, Alexandre Barachant, Maher Moakher category:math.DG stat.ML published:2015-05-26 summary:We explore the connection between two problems that have arisen independentlyin the signal processing and related fields: the estimation of the geometricmean of a set of symmetric positive definite (SPD) matrices and theirapproximate joint diagonalization (AJD). Today there is a considerable interestin estimating the geometric mean of a SPD matrix set in the manifold of SPDmatrices endowed with the Fisher information metric. The resulting mean hasseveral important invariance properties and has proven very useful in diverseengineering applications such as biomedical and image data processing. Whilefor two SPD matrices the mean has an algebraic closed form solution, for a setof more than two SPD matrices it can only be estimated by iterative algorithms.However, none of the existing iterative algorithms feature at the same timefast convergence, low computational complexity per iteration and guarantee ofconvergence. For this reason, recently other definitions of geometric meanbased on symmetric divergence measures, such as the Bhattacharyya divergence,have been considered. The resulting means, although possibly useful inpractice, do not satisfy all desirable invariance properties. In this paper weconsider geometric means of co-variance matrices estimated on high-dimensionaltime-series, assuming that the data is generated according to an instantaneousmixing model, which is very common in signal processing. We show that in thesecircumstances we can approximate the Fisher information geometric mean byemploying an efficient AJD algorithm. Our approximation is in general muchcloser to the Fisher information geometric mean as compared to its competitorsand verifies many invariance properties. Furthermore, convergence isguaranteed, the computational complexity is low and the convergence rate isquadratic. The accuracy of this new geometric mean approximation isdemonstrated by means of simulations.
arxiv-10800-282 | Fantasy Football Prediction | http://arxiv.org/pdf/1505.06918v1.pdf | author:Roman Lutz category:cs.LG I.2.6 published:2015-05-26 summary:The ubiquity of professional sports and specifically the NFL have lead to anincrease in popularity for Fantasy Football. Users have many tools at theirdisposal: statistics, predictions, rankings of experts and even recommendationsof peers. There are issues with all of these, though. Especially since manypeople pay money to play, the prediction tools should be enhanced as theyprovide unbiased and easy-to-use assistance for users. This paper provides anddiscusses approaches to predict Fantasy Football scores of Quarterbacks withrelatively limited data. In addition to that, it includes several suggestionson how the data could be enhanced to achieve better results. The datasetconsists only of game data from the last six NFL seasons. I used two differentmethods to predict the Fantasy Football scores of NFL players: Support VectorRegression (SVR) and Neural Networks. The results of both are promising giventhe limited data that was used.
arxiv-10800-283 | Large-scale Machine Learning for Metagenomics Sequence Classification | http://arxiv.org/pdf/1505.06915v1.pdf | author:Kévin Vervier, Pierre Mahé, Maud Tournoud, Jean-Baptiste Veyrieras, Jean-Philippe Vert category:q-bio.QM cs.CE cs.LG q-bio.GN stat.ML published:2015-05-26 summary:Metagenomics characterizes the taxonomic diversity of microbial communitiesby sequencing DNA directly from an environmental sample. One of the mainchallenges in metagenomics data analysis is the binning step, where eachsequenced read is assigned to a taxonomic clade. Due to the large volume ofmetagenomics datasets, binning methods need fast and accurate algorithms thatcan operate with reasonable computing requirements. While standardalignment-based methods provide state-of-the-art performance, compositionalapproaches that assign a taxonomic class to a DNA read based on the k-mers itcontains have the potential to provide faster solutions. In this work, weinvestigate the potential of modern, large-scale machine learningimplementations for taxonomic affectation of next-generation sequencing readsbased on their k-mers profile. We show that machine learning-basedcompositional approaches benefit from increasing the number of fragmentssampled from reference genome to tune their parameters, up to a coverage ofabout 10, and from increasing the k-mer size to about 12. Tuning these modelsinvolves training a machine learning model on about 10 8 samples in 10 7dimensions, which is out of reach of standard soft-wares but can be doneefficiently with modern implementations for large-scale machine learning. Theresulting models are competitive in terms of accuracy with well-establishedalignment tools for problems involving a small to moderate number of candidatespecies, and for reasonable amounts of sequencing errors. We show, however,that compositional approaches are still limited in their ability to deal withproblems involving a greater number of species, and more sensitive tosequencing errors. We finally confirm that compositional approach achievefaster prediction times, with a gain of 3 to 15 times with respect to theBWA-MEM short read mapper, depending on the number of candidate species and thelevel of sequencing noise.
arxiv-10800-284 | Using Dimension Reduction to Improve the Classification of High-dimensional Data | http://arxiv.org/pdf/1505.06907v1.pdf | author:Andreas Grünauer, Markus Vincze category:cs.LG cs.CV published:2015-05-26 summary:In this work we show that the classification performance of high-dimensionalstructural MRI data with only a small set of training examples is improved bythe usage of dimension reduction methods. We assessed two different dimensionreduction variants: feature selection by ANOVA F-test and featuretransformation by PCA. On the reduced datasets, we applied common learningalgorithms using 5-fold cross-validation. Training, tuning of thehyperparameters, as well as the performance evaluation of the classifiers wasconducted using two different performance measures: Accuracy, and ReceiverOperating Characteristic curve (AUC). Our hypothesis is supported byexperimental results.
arxiv-10800-285 | Monte Carlo Planning method estimates planning horizons during interactive social exchange | http://arxiv.org/pdf/1502.03696v7.pdf | author:Andreas Hula, P. Read Montague, Peter Dayan category:stat.ML published:2015-02-12 summary:Reciprocating interactions represent a central feature of all humanexchanges. They have been the target of various recent experiments, withhealthy participants and psychiatric populations engaging as dyads inmulti-round exchanges such as a repeated trust task. Behaviour in suchexchanges involves complexities related to each agent's preference for equitywith their partner, beliefs about the partner's appetite for equity, beliefsabout the partner's model of their partner, and so on. Agents may also plandifferent numbers of steps into the future. Providing a computationally preciseaccount of the behaviour is an essential step towards understanding whatunderlies choices. A natural framework for this is that of an interactivepartially observable Markov decision process (IPOMDP). However, the variouscomplexities make IPOMDPs inordinately computationally challenging. Here, weshow how to approximate the solution for the multi-round trust task using avariant of the Monte-Carlo tree search algorithm. We demonstrate that thealgorithm is efficient and effective, and therefore can be used to invertobservations of behavioural choices. We use generated behaviour to elucidatethe richness and sophistication of interactive inference.
arxiv-10800-286 | Discrete Independent Component Analysis (DICA) with Belief Propagation | http://arxiv.org/pdf/1505.06814v1.pdf | author:Francesco A. N. Palmieri, Amedeo Buonanno category:cs.CV cs.LG stat.ML published:2015-05-26 summary:We apply belief propagation to a Bayesian bipartite graph composed ofdiscrete independent hidden variables and discrete visible variables. Thenetwork is the Discrete counterpart of Independent Component Analysis (DICA)and it is manipulated in a factor graph form for inference and learning. A fullset of simulations is reported for character images from the MNIST dataset. Theresults show that the factorial code implemented by the sources contributes tobuild a good generative model for the data that can be used in variousinference modes.
arxiv-10800-287 | Surrogate Functions for Maximizing Precision at the Top | http://arxiv.org/pdf/1505.06813v1.pdf | author:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain category:stat.ML cs.LG published:2015-05-26 summary:The problem of maximizing precision at the top of a ranked list, often dubbedPrecision@k (prec@k), finds relevance in myriad learning applications such asranking, multi-label classification, and learning with severe label imbalance.However, despite its popularity, there exist significant gaps in ourunderstanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogatefor prec@k. We also lack scalable perceptron and stochastic gradient descentalgorithms for optimizing this performance measure. In this paper we make keycontributions in these directions. At the heart of our results is a family oftruly upper bounding surrogates for prec@k. These surrogates are motivated in aprincipled manner and enjoy attractive properties such as consistency to prec@kunder various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptronalgorithms for optimizing prec@k with provable mistake bounds. We also devisescalable stochastic gradient descent style methods for this problem withprovable convergence bounds. Our proofs rely on novel uniform convergencebounds which require an in-depth analysis of the structural properties ofprec@k and its surrogates. We conclude with experimental results comparing ouralgorithms with state-of-the-art cutting plane and stochastic gradientalgorithms for maximizing prec@k.
arxiv-10800-288 | Optimizing Non-decomposable Performance Measures: A Tale of Two Classes | http://arxiv.org/pdf/1505.06812v1.pdf | author:Harikrishna Narasimhan, Purushottam Kar, Prateek Jain category:stat.ML cs.LG published:2015-05-26 summary:Modern classification problems frequently present mild to severe labelimbalance as well as specific requirements on classification characteristics,and require optimizing performance measures that are non-decomposable over thedataset, such as F-measure. Such measures have spurred much interest and posespecific challenges to learning algorithms since their non-additive natureprecludes a direct application of well-studied large scale optimization methodssuch as stochastic gradient descent. In this paper we reveal that for two large families of performance measuresthat can be expressed as functions of true positive/negative rates, it isindeed possible to implement point stochastic updates. The families we considerare concave and pseudo-linear functions of TPR, TNR which cover severalpopularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families,using which we develop optimization techniques that enable truly point-basedstochastic updates. For concave performance measures we propose SPADE, astochastic primal dual solver; for pseudo-linear measures we propose STAMP, astochastic alternate maximization procedure. Both methods have crispconvergence guarantees, demonstrate significant speedups over existing methods- often by an order of magnitude or more, and give similar or more accuratepredictions on test data.
arxiv-10800-289 | MLlib: Machine Learning in Apache Spark | http://arxiv.org/pdf/1505.06807v1.pdf | author:Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, Ameet Talwalkar category:cs.LG cs.DC cs.MS stat.ML published:2015-05-26 summary:Apache Spark is a popular open-source platform for large-scale dataprocessing that is well-suited for iterative machine learning tasks. In thispaper we present MLlib, Spark's open-source distributed machine learninglibrary. MLlib provides efficient functionality for a wide range of learningsettings and includes several underlying statistical, optimization, and linearalgebra primitives. Shipped with Spark, MLlib supports several languages andprovides a high-level API that leverages Spark's rich ecosystem to simplify thedevelopment of end-to-end machine learning pipelines. MLlib has experienced arapid growth due to its vibrant open-source community of over 140 contributors,and includes extensive documentation to support further growth and to let usersquickly get up to speed.
arxiv-10800-290 | Boosting-like Deep Learning For Pedestrian Detection | http://arxiv.org/pdf/1505.06800v1.pdf | author:Lei Wang, Baochang Zhang category:cs.CV cs.LG cs.NE published:2015-05-26 summary:This paper proposes boosting-like deep learning (BDL) framework forpedestrian detection. Due to overtraining on the limited training samples,overfitting is a major problem of deep learning. We incorporate a boosting-liketechnique into deep learning to weigh the training samples, and thus preventovertraining in the iterative process. We theoretically give the details ofderivation of our algorithm, and report the experimental results on open datasets showing that BDL achieves a better stable performance than thestate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in theaverage miss rate compared with ACF and JointDeep on the largest Caltechbenchmark dataset, respectively.
arxiv-10800-291 | Location Recognition Over Large Time Lags | http://arxiv.org/pdf/1409.7556v3.pdf | author:Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars category:cs.CV published:2014-09-26 summary:Would it be possible to automatically associate ancient pictures to modernones and create fancy cultural heritage city maps? We introduce here the taskof recognizing the location depicted in an old photo given modern annotatedimages collected from the Internet. We present an extensive analysis ondifferent features, looking for the most discriminative and most robust to theimage variability induced by large time lags. Moreover, we show that thedescribed task benefits from domain adaptation.
arxiv-10800-292 | VeinPLUS: A Transillumination and Reflection-based Hand Vein Database | http://arxiv.org/pdf/1505.06769v1.pdf | author:Alexander Gruschina category:cs.CV published:2015-05-25 summary:This paper gives a short summary of work related to the creation of adepartment-hosted hand vein database. After the introducing section, specialproperties of the hand vein acquisition are explained, followed by a comparisontable, which shows key differences to existing well-known hand vein databases.At the end, the ROI extraction process is described and sample images and ROIsare presented.
arxiv-10800-293 | Agnostic Learning of Disjunctions on Symmetric Distributions | http://arxiv.org/pdf/1405.6791v2.pdf | author:Vitaly Feldman, Pravesh Kothari category:cs.LG cs.CC cs.DS published:2014-05-27 summary:We consider the problem of approximating and learning disjunctions (orequivalently, conjunctions) on symmetric distributions over $\{0,1\}^n$.Symmetric distributions are distributions whose PDF is invariant under anypermutation of the variables. We give a simple proof that for every symmetricdistribution $\mathcal{D}$, there exists a set of $n^{O(\log{(1/\epsilon)})}$functions $\mathcal{S}$, such that for every disjunction $c$, there is function$p$, expressible as a linear combination of functions in $\mathcal{S}$, suchthat $p$ $\epsilon$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$ or$\mathbf{E}_{x \sim \mathcal{D}}[ c(x)-p(x)] \leq \epsilon$. This directlygives an agnostic learning algorithm for disjunctions on symmetricdistributions that runs in time $n^{O( \log{(1/\epsilon)})}$. The best knownprevious bound is $n^{O(1/\epsilon^4)}$ and follows from approximation of themore general class of halfspaces (Wimmer, 2010). We also show that there existsa symmetric distribution $\mathcal{D}$, such that the minimum degree of apolynomial that $1/3$-approximates the disjunction of all $n$ variables is$\ell_1$ distance on $\mathcal{D}$ is $\Omega( \sqrt{n})$. Therefore thelearning result above cannot be achieved via $\ell_1$-regression with apolynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution$\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ ofdegree $O(\log{(1/\epsilon)})$ such that $p$ $\epsilon$-approximates $c$ in$\ell_1$ distance on $\mathcal{D}$. This was first proved by Blais et al.(2008) via a more involved argument.
arxiv-10800-294 | Stochastic Annealing for Variational Inference | http://arxiv.org/pdf/1505.06723v1.pdf | author:San Gultekin, Aonan Zhang, John Paisley category:stat.ML published:2015-05-25 summary:We empirically evaluate a stochastic annealing strategy for Bayesianposterior optimization with variational inference. Variational inference is adeterministic approach to approximate posterior inference in Bayesian models inwhich a typically non-convex objective function is locally optimized over theparameters of the approximating distribution. We investigate an annealingmethod for optimizing this objective with the aim of finding a better localoptimal solution and compare with deterministic annealing methods and noannealing. We show that stochastic annealing can provide clear improvement onthe GMM and HMM, while performance on LDA tends to favor deterministicannealing methods.
arxiv-10800-295 | Smooth and iteratively Restore: A simple and fast edge-preserving smoothing model | http://arxiv.org/pdf/1505.06702v1.pdf | author:Philipp Kniefacz, Walter Kropatsch category:cs.CV published:2015-05-25 summary:In image processing, it can be a useful pre-processing step to smooth awaysmall structures, such as noise or unimportant details, while retaining theoverall structure of the image by keeping edges, which separate objects, sharp.Typically this edge-preserving smoothing process is achieved using edge-awarefilters. However such filters may preserve unwanted small structures as well ifthey contain edges. In this work we present a novel framework foredge-preserving smoothing which separates the process into two different steps:First the image is smoothed using a blurring filter and in the second step theimportant edges are restored using a guided edge-aware filter. The presentedmethod proves to deliver very good results, compared to state-of-the-artedge-preserving smoothing filters, especially at removing unwanted smallstructures. Furthermore it is very versatile and can easily be adapted todifferent fields of applications while at the same time being very fast tocompute and therefore well-suited for real time applications.
arxiv-10800-296 | Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares -- ICML | http://arxiv.org/pdf/1505.06659v1.pdf | author:Garvesh Raskutti, Michael Mahoney category:stat.ML published:2015-05-25 summary:We consider statistical and algorithmic aspects of solving large-scaleleast-squares (LS) problems using randomized sketching algorithms. Priorresults show that, from an \emph{algorithmic perspective}, when using sketchingmatrices constructed from random projections and leverage-score sampling, ifthe number of samples $r$ much smaller than the original sample size $n$, thenthe worst-case (WC) error is the same as solving the original problem, up to avery small relative error. From a \emph{statistical perspective}, one typicallyconsiders the mean-squared error performance of randomized sketchingalgorithms, when data are generated according to a statistical linear model. Inthis paper, we provide a rigorous comparison of both perspectives leading toinsights on how they differ. To do this, we first develop a framework forassessing, in a unified manner, algorithmic and statistical aspects ofrandomized sketching methods. We then consider the statistical predictionefficiency (PE) and the statistical residual efficiency (RE) of the sketched LSestimator; and we use our framework to provide upper bounds for several typesof random projection and random sampling algorithms. Among other results, weshow that the RE can be upper bounded when $r$ is much smaller than $n$, whilethe PE typically requires the number of samples $r$ to be substantially larger.Lower bounds developed in subsequent work show that our upper bounds on PE cannot be improved.
arxiv-10800-297 | Recognition Confidence Analysis of Handwritten Chinese Character with CNN | http://arxiv.org/pdf/1505.06623v1.pdf | author:Meijun He, Shuye Zhang, Huiyun Mao, Lianwen Jin category:cs.CV published:2015-05-25 summary:In this paper, we present an effective method to analyze the recognitionconfidence of handwritten Chinese character, based on the softmax regressionscore of a high performance convolutional neural networks (CNN). Throughcareful and thorough statistics of 827,685 testing samples that randomlyselected from total 8836 different classes of Chinese characters, we find thatthe confidence measurement based on CNN is an useful metric to know howreliable the recognition results are. Furthermore, we find by experiments thatthe recognition confidence can be used to find out similar and confusablecharacter-pairs, to check wrongly or cursively written samples, and even todiscover and correct mis-labelled samples. Many interesting observations andstatistics are given and analyzed in this study.
arxiv-10800-298 | Machine learning based data mining for Milky Way filamentary structures reconstruction | http://arxiv.org/pdf/1505.06621v1.pdf | author:Giuseppe Riccio, Stefano Cavuoti, Eugenio Schisano, Massimo Brescia, Amata Mercurio, Davide Elia, Milena Benedettini, Stefano Pezzuto, Sergio Molinari, Anna Maria Di Giorgio category:astro-ph.IM cs.CV published:2015-05-25 summary:We present an innovative method called FilExSeC (Filaments Extraction,Selection and Classification), a data mining tool developed to investigate thepossibility to refine and optimize the shape reconstruction of filamentarystructures detected with a consolidated method based on the flux derivativeanalysis, through the column-density maps computed from Herschel infraredGalactic Plane Survey (Hi-GAL) observations of the Galactic plane. The presentmethodology is based on a feature extraction module followed by a machinelearning model (Random Forest) dedicated to select features and to classify thepixels of the input images. From tests on both simulations and realobservations the method appears reliable and robust with respect to thevariability of shape and distribution of filaments. In the cases of highlydefined filament structures, the presented method is able to bridge the gapsamong the detected fragments, thus improving their shape reconstruction. From apreliminary "a posteriori" analysis of derived filament physical parameters,the method appears potentially able to add a sufficient contribution tocomplete and refine the filament reconstruction.
arxiv-10800-299 | Electre Tri-Machine Learning Approach to the Record Linkage Problem | http://arxiv.org/pdf/1505.06614v1.pdf | author:Renato De Leone, Valentina Minnetti category:stat.ML cs.LG published:2015-05-25 summary:In this short paper, the Electre Tri-Machine Learning Method, generally usedto solve ordinal classification problems, is proposed for solving the RecordLinkage problem. Preliminary experimental results show that, using the ElectreTri method, high accuracy can be achieved and more than 99% of the matches andnonmatches were correctly identified by the procedure.
arxiv-10800-300 | Fast Detection of Curved Edges at Low SNR | http://arxiv.org/pdf/1505.06600v1.pdf | author:Nati Ofir, Meirav Galun, Boaz Nadler, Ronen Basri category:cs.CV published:2015-05-25 summary:Detecting edges is a fundamental problem in computer vision with manyapplications, some involving very noisy images. While most edge detectionmethods are fast, they perform well only on relatively clean images. Indeed,edges in such images can be reliably detected using only local filters.Detecting faint edges under high levels of noise cannot be done locally at theindividual pixel level, and requires more sophisticated global processing.Unfortunately, existing methods that achieve this goal are quite slow. In thispaper we develop a novel multiscale method to detect curved edges in noisyimages. While our algorithm searches for edges over a huge set of candidatecurves, it does so in a practical runtime, nearly linear in the total number ofimage pixels. As we demonstrate experimentally, our algorithm is orders ofmagnitude faster than previous methods designed to deal with high noise levels.Nevertheless, it obtains comparable, if not better, edge detection quality on avariety of challenging noisy images.
