arxiv-10800-1 | Structured Matrix Completion with Applications to Genomic Data Integration | http://arxiv.org/abs/1504.01823 | author:Tianxi Cai, T. Tony Cai, Anru Zhang category:stat.ME math.ST stat.ML stat.TH published:2015-04-08 summary:Matrix completion has attracted significant recent attention in many fieldsincluding statistics, applied mathematics and electrical engineering. Currentliterature on matrix completion focuses primarily on independent samplingmodels under which the individual observed entries are sampled independently.Motivated by applications in genomic data integration, we propose a newframework of structured matrix completion (SMC) to treat structured missingnessby design. Specifically, our proposed method aims at efficient matrix recoverywhen a subset of the rows and columns of an approximately low-rank matrix areobserved. We provide theoretical justification for the proposed SMC method andderive lower bound for the estimation errors, which together establish theoptimal rate of recovery over certain classes of approximately low-rankmatrices. Simulation studies show that the method performs well in finitesample under a variety of configurations. The method is applied to integrateseveral ovarian cancer genomic studies with different extent of genomicmeasurements, which enables us to construct more accurate prediction rules forovarian cancer survival.
arxiv-10800-2 | Image Subset Selection Using Gabor Filters and Neural Networks | http://arxiv.org/abs/1504.01954 | author:Heider K. Ali, Anthony Whitehead category:cs.CV published:2015-04-08 summary:An automatic method for the selection of subsets of images, both modern andhistoric, out of a set of landmark large images collected from the Internet ispresented in this paper. This selection depends on the extraction of dominantfeatures using Gabor filtering. Features are selected carefully from apreliminary image set and fed into a neural network as a training data. Themethod collects a large set of raw landmark images containing modern andhistoric landmark images and non-landmark images. The method then processesthese images to classify them as landmark and non-landmark images. Theclassification performance highly depends on the number of candidate featuresof the landmark.
arxiv-10800-3 | Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual Entailment in NTCIR RITE Evaluation Tasks | http://arxiv.org/abs/1504.02150 | author:Wei-Jie Huang, Chao-Lin Liu category:cs.CL cs.AI cs.DL I.2.7 published:2015-04-08 summary:We computed linguistic information at the lexical, syntactic, and semanticlevels for Recognizing Inference in Text (RITE) tasks for both traditional andsimplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,named-entity recognition, and near synonym recognition were employed, andfeatures like counts of common words, statement lengths, negation words, andantonyms were considered to judge the entailment relationships of twostatements, while we explored both heuristics-based functions andmachine-learning approaches. The reported systems showed robustness bysimultaneously achieving second positions in the binary-classification subtasksfor both simplified and traditional Chinese in NTCIR-10 RITE-2. We conductedmore experiments with the test data of NTCIR-9 RITE, with good results. We alsoextended our work to search for better configurations of our classifiers andinvestigated contributions of individual features. This extended work showedinteresting results and should encourage further discussion.
arxiv-10800-4 | Mining and discovering biographical information in Difangzhi with a language-model-based approach | http://arxiv.org/abs/1504.02148 | author:Peter K. Bol, Chao-Lin Liu, Hongsu Wang category:cs.CL cs.CY cs.DL I.2.7 published:2015-04-08 summary:We present results of expanding the contents of the China BiographicalDatabase by text mining historical local gazetteers, difangzhi. The goal of thedatabase is to see how people are connected together, through kinship, socialconnections, and the places and offices in which they served. The gazetteersare the single most important collection of names and offices covering the Songthrough Qing periods. Although we begin with local officials we shalleventually include lists of local examination candidates, people from thelocality who served in government, and notable local figures with biographies.The more data we collect the more connections emerge. The value of doingsystematic text mining work is that we can identify relevant connections thatare either directly informative or can become useful without deep historicalresearch. Academia Sinica is developing a name database for officials in thecentral governments of the Ming and Qing dynasties.
arxiv-10800-5 | Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction | http://arxiv.org/abs/1504.02147 | author:Tom Goldstein, Gavin Taylor, Kawika Barabin, Kent Sayre category:cs.DC cs.LG published:2015-04-08 summary:Recent approaches to distributed model fitting rely heavily on consensusADMM, where each node solves small sub-problems using only local data. Wepropose iterative methods that solve {\em global} sub-problems over an entiredistributed dataset. This is possible using transpose reduction strategies thatallow a single node to solve least-squares over massive datasets withoutputting all the data in one place. This results in simple iterative methodsthat avoid the expensive inner loops required for consensus methods. Todemonstrate the efficiency of this approach, we fit linear classifiers andsparse linear models to datasets over 5 Tb in size using a distributedimplementation with over 7000 cores in far less time than previous approaches.
arxiv-10800-6 | Decoupled Adapt-then-Combine diffusion networks with adaptive combiners | http://arxiv.org/abs/1504.01982 | author:Jesus Fernandez-Bes, Jerónimo Arenas-García, Magno T. M. Silva, Luis A. Azpicueta-Ruiz category:cs.SY cs.LG published:2015-04-08 summary:In this paper we analyze a novel diffusion strategy for adaptive networkscalled Decoupled Adapt-then-Combine, which keeps a fully local estimate of thesolution for the adaptation step. Our strategy, which is specially convenientfor heterogeneous networks, is compared with the standard Adapt-then-Combinescheme and theoretically analyzed using energy conservation arguments. Suchcomparison shows the need of implementing adaptive combiners for both schemesto obtain a good performance in case of heterogeneous networks. Therefore, wepropose two adaptive rules to learn the combination coefficients that areuseful for our diffusion strategy. Several experiments simulating bothstationary estimation and tracking problems show that our method outperformsstate-of-the-art techniques, becoming a competitive approach in differentscenarios.
arxiv-10800-7 | Robust real time face recognition and tracking on gpu using fusion of rgb and depth image | http://arxiv.org/abs/1504.01883 | author:Narmada Naik, G. N Rathna category:cs.CV published:2015-04-08 summary:This paper presents a real-time face recognition system using kinect sensor.The algorithm is implemented on GPU using opencl and significant speedimprovements are observed. We use kinect depth image to increase the robustnessand reduce computational cost of conventional LBP based face recognition. Themain objective of this paper was to perform robust, high speed fusion basedface recognition and tracking. The algorithm is mainly composed of three steps.First step is to detect all faces in the video using viola jones algorithm. Thesecond step is online database generation using a tracking window on the face.A modified LBP feature vector is calculated using fusion information from depthand greyscale image on GPU. This feature vector is used to train a svmclassifier. Third step involves recognition of multiple faces based on ourmodified feature vector.
arxiv-10800-8 | Pixel-wise Deep Learning for Contour Detection | http://arxiv.org/abs/1504.01989 | author:Jyh-Jing Hwang, Tyng-Luh Liu category:cs.CV cs.LG cs.NE published:2015-04-08 summary:We address the problem of contour detection via per-pixel classifications ofedge point. To facilitate the process, the proposed approach leverages withDenseNet, an efficient implementation of multiscale convolutional neuralnetworks (CNNs), to extract an informative feature vector for each pixel anduses an SVM classifier to accomplish contour detection. In the experiment ofcontour detection, we look into the effectiveness of combining per-pixelfeatures from different CNN layers and verify their performance on BSDS500.
arxiv-10800-9 | A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor Images | http://arxiv.org/abs/1504.01800 | author:Mohammed Khader, A. Ben Hamza category:cs.CV published:2015-04-08 summary:We propose a nonrigid registration approach for diffusion tensor images usinga multicomponent information-theoretic measure. Explicit orientationoptimization is enabled by incorporating tensor reorientation, which isnecessary for wrapping diffusion tensor images. Experimental results ondiffusion tensor images indicate the feasibility of the proposed approach and amuch better performance compared to the affine registration method based onmutual information in terms of registration accuracy in the presence ofgeometric distortion.
arxiv-10800-10 | A Chaotic Dynamical System that Paints | http://arxiv.org/abs/1504.02010 | author:Tuhin Sahai, George Mathew, Amit Surana category:nlin.CD cs.LG published:2015-04-08 summary:Can a dynamical system paint masterpieces such as Da Vinci's Mona Lisa orMonet's Water Lilies? Moreover, can this dynamical system be chaotic in thesense that although the trajectories are sensitive to initial conditions, thesame painting is created every time? Setting aside the creative aspect ofpainting a picture, in this work, we develop a novel algorithm to reproducepaintings and photographs. Combining ideas from ergodic theory and controltheory, we construct a chaotic dynamical system with predetermined statisticalproperties. If one makes the spatial distribution of colors in the picture thetarget distribution, akin to a human, the algorithm first captures large scalefeatures and then goes on to refine small scale features. Beyond reproducingpaintings, this approach is expected to have a wide variety of applicationssuch as uncertainty quantification, sampling for efficient inference inscalable machine learning for big data, and developing effective strategies forsearch and rescue. In particular, our preliminary studies demonstrate that thisalgorithm provides significant acceleration and higher accuracy than competingmethods for Markov Chain Monte Carlo (MCMC).
arxiv-10800-11 | Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space | http://arxiv.org/abs/1504.01840 | author:Yegor Tkachenko category:cs.LG published:2015-04-08 summary:The paper outlines a framework for autonomous control of a CRM (customerrelationship management) system. First, it explores how a modified version ofthe widely accepted Recency-Frequency-Monetary Value system of metrics can beused to define the state space of clients or donors. Second, it describes aprocedure to determine the optimal direct marketing action in discrete andcontinuous action space for the given individual, based on his position in thestate space. The procedure involves the use of model-free Q-learning to train adeep neural network that relates a client's position in the state space torewards associated with possible marketing actions. The estimated valuefunction over the client state space can be interpreted as customer lifetimevalue, and thus allows for a quick plug-in estimation of CLV for a givenclient. Experimental results are presented, based on KDD Cup 1998 mailingdataset of donation solicitations.
arxiv-10800-12 | Residential Demand Response Applications Using Batch Reinforcement Learning | http://arxiv.org/abs/1504.02125 | author:Frederik Ruelens, Bert Claessens, Stijn Vandael, Bart De Schutter, Robert Babuska, Ronnie Belmans category:cs.SY cs.LG published:2015-04-08 summary:Driven by recent advances in batch Reinforcement Learning (RL), this papercontributes to the application of batch RL to demand response. In contrast toconventional model-based approaches, batch RL techniques do not require asystem identification step, which makes them more suitable for a large-scaleimplementation. This paper extends fitted Q-iteration, a standard batch RLtechnique, to the situation where a forecast of the exogenous data is provided.In general, batch RL techniques do not rely on expert knowledge on the systemdynamics or the solution. However, if some expert knowledge is provided, it canbe incorporated by using our novel policy adjustment method. Finally, we tacklethe challenge of finding an open-loop schedule required to participate in theday-ahead market. We propose a model-free Monte-Carlo estimator method thatuses a metric to construct artificial trajectories and we illustrate thismethod by finding the day-ahead schedule of a heat-pump thermostat. Ourexperiments show that batch RL techniques provide a valuable alternative tomodel-based controllers and that they can be used to construct both closed-loopand open-loop policies.
arxiv-10800-13 | Detecting falls with X-Factor HMMs when the training data for falls is not available | http://arxiv.org/abs/1504.02141 | author:Shehroz S. Khan, Michelle E. Karg, Dana Kulic, Jesse Hoey category:cs.LG cs.AI published:2015-04-08 summary:Identification of falls while performing normal activities of daily living(ADL) is important to ensure personal safety and well-being. However, fallingis a short term activity that occurs infrequently. This poses a challenge totraditional classification algorithms, because there may be very littletraining data for falls (or none at all). This paper proposes an approach forthe identification of falls using a wearable device in the absence of trainingdata for falls but with plentiful data for normal ADL. We propose three`X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen fallsusing "inflated" output covariances (observation models). To estimate theinflated covariances, we propose a novel cross validation method to remove"outliers" from the normal ADL that serve as proxies for the unseen falls andallow learning the XHMMs using only normal activities. We tested the proposedXHMM approaches on two activity recognition datasets and show high detectionrates for falls in the absence of fall-specific training data. We show that thetraditional method of choosing a threshold based on maximum of negative oflog-likelihood to identify unseen falls is ill-posed for this problem. We alsoshow that supervised classification methods perform poorly when very limitedfall data are available during the training phase.
arxiv-10800-14 | Supporting Language Learners with the Meanings Of Closed Class Items | http://arxiv.org/abs/1504.02059 | author:Hayat Alrefaie, Allan Ramsay category:cs.AI cs.CL published:2015-04-08 summary:The process of language learning involves the mastery of countless tasks:making the constituent sounds of the language being learned, learning thegrammatical patterns, and acquiring the requisite vocabulary for reception andproduction. While a plethora of computational tools exist to facilitate thefirst and second of these tasks, a number of challenges arise with respect toenabling the third. This paper describes a tool that has been designed tosupport language learners with the challenge of understanding the use ofclosed-class lexical items. The process of learning the Arabic for office is(mktb) is relatively simple and should be possible by means of simplerepetition of the word. However, it is much more difficult to learn andcorrectly use the Arabic equivalent of the word on. The current paper describesa mechanism for the delivery of diagnostic information regarding specificlexical examples, with the aim of clearly demonstrating why a particulartranslation of a given closed-class item may be appropriate in certainsituations but not others, thereby helping learners to understand and use theterm correctly.
arxiv-10800-15 | On-line Handwritten Devanagari Character Recognition using Fuzzy Directional Features | http://arxiv.org/abs/1504.01488 | author:Sunil Kumar Kopparapu, Lajish VL category:cs.CV published:2015-04-07 summary:This paper describes a new feature set for use in the recognition of on-linehandwritten Devanagari script based on Fuzzy Directional Features. Experimentsare conducted for the automatic recognition of isolated handwritten characterprimitives (sub-character units). Initially we describe the proposed featureset, called the Fuzzy Directional Features (FDF) and then show how thesefeatures can be effectively utilized for writer independent characterrecognition. Experimental results show that FDF set perform well for writerindependent data set at stroke level recognition. The main contribution of thispaper is the introduction of a novel feature set and establish experimentallyits ability in recognition of handwritten Devanagari script.
arxiv-10800-16 | An Empirical Evaluation of Deep Learning on Highway Driving | http://arxiv.org/abs/1504.01716 | author:Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Mujica, Adam Coates, Andrew Y. Ng category:cs.RO cs.CV published:2015-04-07 summary:Numerous groups have applied a variety of deep learning techniques tocomputer vision problems in highway perception scenarios. In this paper, wepresented a number of empirical evaluations of recent deep learning advances.Computer vision, combined with deep learning, has the potential to bring abouta relatively inexpensive, robust solution to autonomous driving. To preparedeep learning for industry uptake and practical applications, neural networkswill require large data sets that represent all possible driving environmentsand scenarios. We collect a large data set of highway data and apply deeplearning and computer vision algorithms to problems such as car and lanedetection. We show how existing convolutional neural networks (CNNs) can beused to perform lane and vehicle detection while running at frame ratesrequired for a real-time system. Our results lend credence to the hypothesisthat deep learning holds promise for autonomous driving.
arxiv-10800-17 | Efficient SDP Inference for Fully-connected CRFs Based on Low-rank Decomposition | http://arxiv.org/abs/1504.01492 | author:Peng Wang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG stat.ML published:2015-04-07 summary:Conditional Random Fields (CRF) have been widely used in a variety ofcomputer vision tasks. Conventional CRFs typically define edges on neighboringimage pixels, resulting in a sparse graph such that efficient inference can beperformed. However, these CRFs fail to model long-range contextualrelationships. Fully-connected CRFs have thus been proposed. While there areefficient approximate inference methods for such CRFs, usually they aresensitive to initialization and make strong assumptions. In this work, wedevelop an efficient, yet general algorithm for inference on fully-connectedCRFs. The algorithm is based on a scalable SDP algorithm and the low- rankapproximation of the similarity/kernel matrix. The core of the proposedalgorithm is a tailored quasi-Newton method that takes advantage of thelow-rank matrix approximation when solving the specialized SDP dual problem.Experiments demonstrate that our method can be applied on fully-connected CRFsthat cannot be solved previously, such as pixel-level image co-segmentation.
arxiv-10800-18 | Bidirectional Recurrent Neural Networks as Generative Models - Reconstructing Gaps in Time Series | http://arxiv.org/abs/1504.01575 | author:Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo Kärkkäinen, Akos Vetek, Juha Karhunen category:cs.LG cs.NE published:2015-04-07 summary:Bidirectional recurrent neural networks (RNN) are trained to predict both inthe positive and negative time directions simultaneously. They have not beenused commonly in unsupervised tasks, because a probabilistic interpretation ofthe model has been difficult. Recently, two different frameworks, GSN and NADE,provide a connection between reconstruction and probabilistic modeling, whichmakes the interpretation possible. As far as we know, neither GSN or NADE havebeen studied in the context of time series before. As an example of anunsupervised task, we study the problem of filling in gaps in high-dimensionaltime series with complex dynamics. Although unidirectional RNNs have recentlybeen trained successfully to model such time series, inference in the negativetime direction is non-trivial. We propose two probabilistic interpretations ofbidirectional RNNs that can be used to reconstruct missing gaps efficiently.Our experiments on text data show that both proposed methods are much moreaccurate than unidirectional reconstructions, although a bit less accurate thana computationally complex bidirectional Bayesian inference on theunidirectional RNN. We also provide results on music data for which theBayesian inference is computationally infeasible, demonstrating the scalabilityof the proposed methods.
arxiv-10800-19 | Voice based self help System: User Experience Vs Accuracy | http://arxiv.org/abs/1504.01496 | author:Sunil Kumar Kopparapu category:cs.CL published:2015-04-07 summary:In general, self help systems are being increasingly deployed by servicebased industries because they are capable of delivering better customer serviceand increasingly the switch is to voice based self help systems because theyprovide a natural interface for a human to interact with a machine. A speechbased self help system ideally needs a speech recognition engine to convertspoken speech to text and in addition a language processing engine to take careof any misrecognitions by the speech recognition engine. Any off-the-shelfspeech recognition engine is generally a combination of acoustic processing andspeech grammar. While this is the norm, we believe that ideally a speechrecognition application should have in addition to a speech recognition enginea separate language processing engine to give the system better performance. Inthis paper, we discuss ways in which the speech recognition engine and thelanguage processing engine can be combined to give a better user experience.
arxiv-10800-20 | Transferring Knowledge from a RNN to a DNN | http://arxiv.org/abs/1504.01483 | author:William Chan, Nan Rosemary Ke, Ian Lane category:cs.LG cs.CL cs.NE stat.ML published:2015-04-07 summary:Deep Neural Network (DNN) acoustic models have yielded many state-of-the-artresults in Automatic Speech Recognition (ASR) tasks. More recently, RecurrentNeural Network (RNN) models have been shown to outperform DNNs counterparts.However, state-of-the-art DNN and RNN models tend to be impractical to deployon embedded systems with limited computational capacity. Traditionally, theapproach for embedded platforms is to either train a small DNN directly, or totrain a small DNN that learns the output distribution of a large DNN. In thispaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. Weuse the RNN model to generate soft alignments and minimize the Kullback-Leiblerdivergence against the small DNN. The small DNN trained on the soft RNNalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 taskcompared to a baseline 4.54 WER or more than 13% relative improvement.
arxiv-10800-21 | Deep Recurrent Neural Networks for Acoustic Modelling | http://arxiv.org/abs/1504.01482 | author:William Chan, Ian Lane category:cs.LG cs.CL cs.NE stat.ML published:2015-04-07 summary:We present a novel deep Recurrent Neural Network (RNN) model for acousticmodelling in Automatic Speech Recognition (ASR). We term our contribution as aTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) withTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory(BLSTM), and a final DNN. The first DNN acts as a feature processor to ourmodel, the BLSTM then generates a context from the sequence acoustic signal,and the final DNN takes the context and models the posterior probabilities ofthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)eval92 task or more than 8% relative improvement over the baseline DNN models.
arxiv-10800-22 | Separable time-causal and time-recursive spatio-temporal receptive fields | http://arxiv.org/abs/1504.01502 | author:Tony Lindeberg category:cs.CV q-bio.NC published:2015-04-07 summary:We present an improved model and theory for time-causal and time-recursivespatio-temporal receptive fields, obtained by a combination of Gaussianreceptive fields over the spatial domain and first-order integrators orequivalently truncated exponential filters coupled in cascade over the temporaldomain. Compared to previous spatio-temporal scale-space formulations in termsof non-enhancement of local extrema or scale invariance, these receptive fieldsare based on different scale-space axiomatics over time by ensuringnon-creation of new local extrema or zero-crossings with increasing temporalscale. Specifically, extensions are presented about parameterizing theintermediate temporal scale levels, analysing the resulting temporal dynamicsand transferring the theory to a discrete implementation in terms of recursivefilters over time.
arxiv-10800-23 | Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification | http://arxiv.org/abs/1504.01561 | author:Zuxuan Wu, Xi Wang, Yu-Gang Jiang, Hao Ye, Xiangyang Xue category:cs.CV cs.MM published:2015-04-07 summary:Classifying videos according to content semantics is an important problemwith a wide range of applications. In this paper, we propose a hybrid deeplearning framework for video classification, which is able to model staticspatial information, short-term motion, as well as long-term temporal clues inthe videos. Specifically, the spatial and the short-term motion features areextracted separately by two Convolutional Neural Networks (CNN). These twotypes of CNN-based features are then combined in a regularized feature fusionnetwork for classification, which is able to learn and utilize featurerelationships for improved performance. In addition, Long Short Term Memory(LSTM) networks are applied on top of the two features to further modellonger-term temporal clues. The main contribution of this work is the hybridlearning framework that can model several important aspects of the video data.We also show that (1) combining the spatial and the short-term motion featuresin the regularized fusion network is better than direct classification andfusion using the CNN with a softmax layer, and (2) the sequence-based LSTM ishighly complementary to the traditional classification strategy withoutconsidering the temporal frame orders. Extensive experiments are conducted ontwo popular and challenging benchmarks, the UCF-101 Human Actions and theColumbia Consumer Videos (CCV). On both benchmarks, our framework achievesto-date the best reported performance: $91.3\%$ on the UCF-101 and $83.5\%$ onthe CCV.
arxiv-10800-24 | Mobile Phone Based Vehicle License Plate Recognition for Road Policing | http://arxiv.org/abs/1504.01476 | author:Lajish V. L., Sunil Kumar Kopparapu category:cs.CV published:2015-04-07 summary:Identity of a vehicle is done through the vehicle license plate by trafficpolice in general. Au- tomatic vehicle license plate recognition has severalapplications in intelligent traffic management systems. The security situationacross the globe and particularly in India demands a need to equip the trafficpolice with a system that enables them to get instant details of a vehicle. Thesystem should be easy to use, should be mobile, and work 24 x 7. In this paper,we describe a mobile phone based, client-server architected, license platerecognition system. While we use the state of the art image processing andpattern recognition algorithms tuned for Indian conditions to automaticallyrecognize non-uniform license plates, the main contribution is in creating anend to end usable solution. The client application runs on a mobile device anda server application, with access to vehicle information database, is hostedcentrally. The solution enables capture of license plate image captured by thephone camera and passes to the server; on the server the license plate numberis recognized; the data associated with the number plate is then sent back tothe mobile device, instantaneously. We describe the end to end systemarchitecture in detail. A working prototype of the proposed system has beenimplemented in the lab environment.
arxiv-10800-25 | From Averaging to Acceleration, There is Only a Step-size | http://arxiv.org/abs/1504.01577 | author:Nicolas Flammarion, Francis Bach category:stat.ML math.OC published:2015-04-07 summary:We show that accelerated gradient descent, averaged gradient descent and theheavy-ball method for non-strongly-convex problems may be reformulated asconstant parameter second-order difference equation algorithms, where stabilityof the system is equivalent to convergence at rate O(1/n 2), where n is thenumber of iterations. We provide a detailed analysis of the eigenvalues of thecorresponding linear dynamical system , showing various oscillatory andnon-oscillatory behaviors, together with a sharp stability result with explicitconstants. We also consider the situation where noisy gradients are available,where we extend our general convergence result, which suggests an alternativealgorithm (i.e., with different step sizes) that exhibits the good aspects ofboth averaging and acceleration.
arxiv-10800-26 | Totally Corrective Boosting with Cardinality Penalization | http://arxiv.org/abs/1504.01446 | author:Vasil S. Denchev, Nan Ding, Shin Matsushima, S. V. N. Vishwanathan, Hartmut Neven category:cs.LG quant-ph published:2015-04-07 summary:We propose a totally corrective boosting algorithm with explicit cardinalityregularization. The resulting combinatorial optimization problems are not knownto be efficiently solvable with existing classical methods, but emergingquantum optimization technology gives hope for achieving sparser models inpractice. In order to demonstrate the utility of our algorithm, we use adistributed classical heuristic optimizer as a stand-in for quantum hardware.Even though this evaluation methodology incurs large time and resource costs onclassical computing machinery, it allows us to gauge the potential gains ingeneralization performance and sparsity of the resulting boosted ensembles. Ourexperimental results on public data sets commonly used for benchmarking ofboosting algorithms decidedly demonstrate the existence of such advantages. Ifactual quantum optimization were to be used with this algorithm in the future,we would expect equivalent or superior results at much smaller time and energycosts during training. Moreover, studying cardinality-penalized boosting alsosheds light on why unregularized boosting algorithms with early stopping oftenyield better results than their counterparts with explicit convexregularization: Early stopping performs suboptimal cardinality regularization.The results that we present here indicate it is beneficial to explicitly solvethe combinatorial problem still left open at early termination.
arxiv-10800-27 | Large Margin Nearest Neighbor Embedding for Knowledge Representation | http://arxiv.org/abs/1504.01684 | author:Miao Fan, Qiang Zhou, Thomas Fang Zheng, Ralph Grishman category:cs.AI cs.CL published:2015-04-07 summary:Traditional way of storing facts in triplets ({\it head\_entity, relation,tail\_entity}), abbreviated as ({\it h, r, t}), makes the knowledge intuitivelydisplayed and easily acquired by mankind, but hardly computed or even reasonedby AI machines. Inspired by the success in applying {\it DistributedRepresentations} to AI-related fields, recent studies expect to represent eachentity and relation with a unique low-dimensional embedding, which is differentfrom the symbolic and atomic framework of displaying knowledge in triplets. Inthis way, the knowledge computing and reasoning can be essentially facilitatedby means of a simple {\it vector calculation}, i.e. ${\bf h} + {\bf r} \approx{\bf t}$. We thus contribute an effective model to learn better embeddingssatisfying the formula by pulling the positive tail entities ${\bf t^{+}}$ toget together and close to {\bf h} + {\bf r} ({\it Nearest Neighbor}), andsimultaneously pushing the negatives ${\bf t^{-}}$ away from the positives${\bf t^{+}}$ via keeping a {\it Large Margin}. We also design a correspondinglearning algorithm to efficiently find the optimal solution based on {\itStochastic Gradient Descent} in iterative fashion. Quantitative experimentsillustrate that our approach can achieve the state-of-the-art performance,compared with several latest methods on some benchmark datasets for twoclassical applications, i.e. {\it Link prediction} and {\it Tripletclassification}. Moreover, we analyze the parameter complexities among all theevaluated models, and analytical results indicate that our model needs fewercomputational resources on outperforming the other methods.
arxiv-10800-28 | Tensor machines for learning target-specific polynomial features | http://arxiv.org/abs/1504.01697 | author:Jiyan Yang, Alex Gittens category:cs.LG stat.ML published:2015-04-07 summary:Recent years have demonstrated that using random feature maps cansignificantly decrease the training and testing times of kernel-basedalgorithms without significantly lowering their accuracy. Regrettably, becauserandom features are target-agnostic, typically thousands of such features arenecessary to achieve acceptable accuracies. In this work, we consider theproblem of learning a small number of explicit polynomial features. Ourapproach, named Tensor Machines, finds a parsimonious set of features byoptimizing over the hypothesis class introduced by Kar and Karnick for randomfeature maps in a target-specific manner. Exploiting a natural connectionbetween polynomials and tensors, we provide bounds on the generalization errorof Tensor Machines. Empirically, Tensor Machines behave favorably on severalreal-world datasets compared to other state-of-the-art techniques for learningpolynomial features, and deliver significantly more parsimonious models.
arxiv-10800-29 | Design and Implementation of a 3D Undersea Camera System | http://arxiv.org/abs/1504.01753 | author:Xida Chen, Steve Sutphen, Paul Macoun, Yee-Hong Yang category:cs.CV published:2015-04-07 summary:In this paper, we present the design and development of an undersea camerasystem. The goal of our system is to provide a 3D model of the undersea habitatin a long-term continuous manner. The most important feature of our system isthe use of multiple cameras and multiple projectors, which is able to provideaccurate 3D models with an accuracy of a millimeter. By introducing projectorsin our system, we can use many different structured light methods for differenttasks. There are two main advantages comparing our system with using ROVs orAUVs. First, our system can provide continuous monitoring of the underseahabitat. Second, our system has a low hardware cost. Comparing to existingdeployed camera systems, the advantage of our system is that it can provideaccurate 3D models and provides opportunities for future development ofinnovative algorithms for undersea research.
arxiv-10800-30 | A comparative study between proposed Hyper Kurtosis based Modified Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human Brain CT scan images | http://arxiv.org/abs/1505.06219 | author:Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Satyasaran Changdar, Ritwik Burman, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV published:2015-04-07 summary:In this paper, a comparative study between proposed hyper kurtosis basedmodified duo-histogram equalization (HKMDHE) algorithm and contrast limitedadaptive histogram enhancement (CLAHE) has been presented for theimplementation of contrast enhancement and brightness preservation of lowcontrast human brain CT scan images. In HKMDHE algorithm, contrast enhancementis done on the hyper-kurtosis based application. The results are very promisingof proposed HKMDHE technique with improved PSNR values and lesser AMMBE valuesthan CLAHE technique.
arxiv-10800-31 | Ego-Object Discovery | http://arxiv.org/abs/1504.01639 | author:Marc Bolaños, Petia Radeva category:cs.CV cs.AI published:2015-04-07 summary:Lifelogging devices are spreading faster everyday. This growth can representgreat benefits to develop methods for extraction of meaningful informationabout the user wearing the device and his/her environment. In this paper, wepropose a semi-supervised strategy for easily discovering objects relevant tothe person wearing a first-person camera. Given an egocentric video/imagessequence acquired by the camera, our algorithm uses both the appearanceextracted by means of a convolutional neural network and an object refillmethodology that allows to discover objects even in case of small amount ofobject appearance in the collection of images. An SVM filtering strategy isapplied to deal with the great part of the False Positive object candidatesfound by most of the state of the art object detectors. We validate our methodon a new egocentric dataset of 4912 daily images acquired by 4 persons as wellas on both PASCAL 2012 and MSRC datasets. We obtain for all of them resultsthat largely outperform the state of the art approach. We make public both theEDUB dataset and the algorithm code.
arxiv-10800-32 | Locally Non-rigid Registration for Mobile HDR Photography | http://arxiv.org/abs/1504.01441 | author:Orazio Gallo, Alejandro Troccoli, Jun Hu, Kari Pulli, Jan Kautz category:cs.CV published:2015-04-07 summary:Image registration for stack-based HDR photography is challenging. If notproperly accounted for, camera motion and scene changes result in artifacts inthe composite image. Unfortunately, existing methods to address this problemare either accurate, but too slow for mobile devices, or fast, but prone tofailing. We propose a method that fills this void: our approach is extremelyfast---under 700ms on a commercial tablet for a pair of 5MP images---andprevents the artifacts that arise from insufficient registration quality.
arxiv-10800-33 | Jointly Embedding Relations and Mentions for Knowledge Population | http://arxiv.org/abs/1504.01683 | author:Miao Fan, Kai Cao, Yifan He, Ralph Grishman category:cs.CL published:2015-04-07 summary:This paper contributes a joint embedding model for predicting relationsbetween a pair of entities in the scenario of relation inference. It differsfrom most stand-alone approaches which separately operate on either knowledgebases or free texts. The proposed model simultaneously learns low-dimensionalvector representations for both triplets in knowledge repositories and thementions of relations in free texts, so that we can leverage the evidence bothresources to make more accurate predictions. We use NELL to evaluate theperformance of our approach, compared with cutting-edge methods. Results ofextensive experiments show that our model achieves significant improvement onrelation extraction.
arxiv-10800-34 | Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing | http://arxiv.org/abs/1504.01515 | author:Paris Giampouras, Konstantinos Themelis, Athanasios Rontogiannis, Konstantinos Koutroumbas category:cs.CV math.OC stat.ML published:2015-04-07 summary:In a plethora of applications dealing with inverse problems, e.g. in imageprocessing, social networks, compressive sensing, biological data processingetc., the signal of interest is known to be structured in several ways at thesame time. This premise has recently guided the research to the innovative andmeaningful idea of imposing multiple constraints on the parameters involved inthe problem under study. For instance, when dealing with problems whoseparameters form sparse and low-rank matrices, the adoption of suitably combinedconstraints imposing sparsity and low-rankness, is expected to yieldsubstantially enhanced estimation results. In this paper, we address thespectral unmixing problem in hyperspectral images. Specifically, two novelunmixing algorithms are introduced, in an attempt to exploit both spatialcorrelation and sparse representation of pixels lying in homogeneous regions ofhyperspectral images. To this end, a novel convex mixed penalty term is firstdefined consisting of the sum of the weighted $\ell_1$ and the weighted nuclearnorm of the abundance matrix corresponding to a small area of the imagedetermined by a sliding square window. This penalty term is then used toregularize a conventional quadratic cost function and impose simultaneouslysparsity and row-rankness on the abundance matrix. The resulting regularizedcost function is minimized by a) an incremental proximal sparse and low-rankunmixing algorithm and b) an algorithm based on the alternating minimizationmethod of multipliers (ADMM). The effectiveness of the proposed algorithms isillustrated in experiments conducted both on simulated and real data.
arxiv-10800-35 | Heterogeneous Tensor Decomposition for Clustering via Manifold Optimization | http://arxiv.org/abs/1504.01777 | author:Yanfeng Sun, Junbin Gao, Xia Hong, Bamdev Mishra, Baocai Yin category:cs.CV published:2015-04-07 summary:Tensors or multiarray data are generalizations of matrices. Tensor clusteringhas become a very important research topic due to the intrinsically richstructures in real-world multiarray datasets. Subspace clustering based onvectorizing multiarray data has been extensively researched. However,vectorization of tensorial data does not exploit complete structureinformation. In this paper, we propose a subspace clustering algorithm withoutadopting any vectorization process. Our approach is based on a novelheterogeneous Tucker decomposition model. In contrast to existing techniques,we propose a new clustering algorithm that alternates between different modesof the proposed heterogeneous tensor model. All but the last mode haveclosed-form updates. Updating the last mode reduces to optimizing over theso-called multinomial manifold, for which we investigate second orderRiemannian geometry and propose a trust-region algorithm. Numerical experimentsshow that our proposed algorithm compete effectively with state-of-the-artclustering algorithms that are based on tensor factorization.
arxiv-10800-36 | PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent | http://arxiv.org/abs/1504.01365 | author:Cho-Jui Hsieh, Hsiang-Fu Yu, Inderjit S. Dhillon category:cs.LG published:2015-04-06 summary:Stochastic Dual Coordinate Descent (SDCD) has become one of the mostefficient ways to solve the family of $\ell_2$-regularized empirical riskminimization problems, including linear SVM, logistic regression, and manyothers. The vanilla implementation of DCD is quite slow; however, bymaintaining primal variables while updating dual variables, the time complexityof SDCD can be significantly reduced. Such a strategy forms the core algorithmin the widely-used LIBLINEAR package. In this paper, we parallelize the SDCDalgorithms in LIBLINEAR. In recent research, several synchronized parallel SDCDalgorithms have been proposed, however, they fail to achieve good speedup inthe shared memory multi-core setting. In this paper, we propose a family ofasynchronous stochastic dual coordinate descent algorithms (ASDCD). Each threadrepeatedly selects a random dual variable and conducts coordinate updates usingthe primal variables that are stored in the shared memory. We analyze theconvergence properties when different locking/atomic mechanisms are applied.For implementation with atomic operations, we show linear convergence undermild conditions. For implementation without any atomic operations or locking,we present the first {\it backward error analysis} for ASDCD under themulti-core environment, showing that the converged solution is the exactsolution for a primal problem with perturbed regularizer. Experimental resultsshow that our methods are much faster than previous parallel coordinate descentsolvers.
arxiv-10800-37 | Early Stopping is Nonparametric Variational Inference | http://arxiv.org/abs/1504.01344 | author:Dougal Maclaurin, David Duvenaud, Ryan P. Adams category:stat.ML cs.LG published:2015-04-06 summary:We show that unconverged stochastic gradient descent can be interpreted as aprocedure that samples from a nonparametric variational approximate posteriordistribution. This distribution is implicitly defined as the transformation ofan initial distribution by a sequence of optimization updates. By tracking thechange in entropy over this sequence of transformations during optimization, weform a scalable, unbiased estimate of the variational lower bound on the logmarginal likelihood. We can use this bound to optimize hyperparameters insteadof using cross-validation. This Bayesian interpretation of SGD suggestsimproved, overfitting-resistant optimization procedures, and gives atheoretical foundation for popular tricks such as early stopping andensembling. We investigate the properties of this marginal likelihood estimatoron neural network models.
arxiv-10800-38 | QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting Patterns | http://arxiv.org/abs/1504.01383 | author:Vlad Niculae, Caroline Suen, Justine Zhang, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.CL cs.SI physics.soc-ph published:2015-04-06 summary:Given the extremely large pool of events and stories available, media outletsneed to focus on a subset of issues and aspects to convey to their audience.Outlets are often accused of exhibiting a systematic bias in this selectionprocess, with different outlets portraying different versions of reality.However, in the absence of objective measures and empirical evidence, thedirection and extent of systematicity remains widely disputed. In this paper we propose a framework based on quoting patterns forquantifying and characterizing the degree to which media outlets exhibitsystematic bias. We apply this framework to a massive dataset of news articlesspanning the six years of Obama's presidency and all of his speeches, andreveal that a systematic pattern does indeed emerge from the outlet's quotingbehavior. Moreover, we show that this pattern can be successfully exploited inan unsupervised prediction setting, to determine which new quotes an outletwill select to broadcast. By encoding bias patterns in a low-rank space weprovide an analysis of the structure of political media coverage. This revealsa latent media bias space that aligns surprisingly well with political ideologyand outlet type. A linguistic analysis exposes striking differences acrossthese latent dimensions, showing how the different types of media outletsportray different realities even when reporting on the same events. Forexample, outlets mapped to the mainstream conservative side of the latent spacefocus on quotes that portray a presidential persona disproportionatelycharacterized by negativity.
arxiv-10800-39 | Knowledge driven Offline to Online Script Conversion | http://arxiv.org/abs/1504.01420 | author:Sunil Kopparapu, Devanuj, Akhilesh Srivastava, P. V. S. Rao category:cs.CV published:2015-04-06 summary:The problem of offline to online script conversion is a challenging and anill-posed problem. The interest in offline to online conversion exists becausethere are a plethora of robust algorithms in online script literature which cannot be used on offline scripts. In this paper, we propose a method, based onheuristics, to extract online script information from offline bitmap image. Weshow the performance of the proposed method on a real sample signature offlineimage, whose online information is known.
arxiv-10800-40 | Matching-CNN Meets KNN: Quasi-Parametric Human Parsing | http://arxiv.org/abs/1504.01220 | author:Si Liu, Xiaodan Liang, Luoqi Liu, Xiaohui Shen, Jianchao Yang, Changsheng Xu, Liang Lin, Xiaochun Cao, Shuicheng Yan category:cs.CV published:2015-04-06 summary:Both parametric and non-parametric approaches have demonstrated encouragingperformances in the human parsing task, namely segmenting a human image intoseveral semantic regions (e.g., hat, bag, left arm, face). In this work, we aimto develop a new solution with the advantages of both methodologies, namelysupervision from annotated data and the flexibility to use newly annotated(possibly uncommon) images, and present a quasi-parametric human parsing model.Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, theparametric Matching Convolutional Neural Network (M-CNN) is proposed to predictthe matching confidence and displacements of the best matched region in thetesting image for a particular semantic region in one KNN image. Given atesting image, we first retrieve its KNN images from theannotated/manually-parsed human image corpus. Then each semantic region in eachKNN image is matched with confidence to the testing image using M-CNN, and thematched regions from all KNN images are further fused, followed by a superpixelsmoothing procedure to obtain the ultimate human parsing result. The M-CNNdiffers from the classic CNN in that the tailored cross image matching filtersare introduced to characterize the matching between the testing image and thesemantic region of a KNN image. The cross image matching filters are defined atdifferent convolutional layers, each aiming to capture a particular range ofdisplacements. Comprehensive evaluations over a large dataset with 7,700annotated human images well demonstrate the significant performance gain fromthe quasi-parametric model over the state-of-the-arts, for the human parsingtask.
arxiv-10800-41 | A Metric to Classify Style of Spoken Speech | http://arxiv.org/abs/1504.01427 | author:Sunil Kopparapu, Saurabh Bhatnagar, K. Sahana, Sathyanarayana, Akhilesh Srivastava, P. V. S. Rao category:cs.CL published:2015-04-06 summary:The ability to classify spoken speech based on the style of speaking is animportant problem. With the advent of BPO's in recent times, specifically thosethat cater to a population other than the local population, it has becomenecessary for BPO's to identify people with certain style of speaking(American, British etc). Today BPO's employ accent analysts to identify peoplehaving the required style of speaking. This process while involving human bias,it is becoming increasingly infeasible because of the high attrition rate inthe BPO industry. In this paper, we propose a new metric, which robustly andaccurately helps classify spoken speech based on the style of speaking. Therole of the proposed metric is substantiated by using it to classify realspeech data collected from over seventy different people working in a BPO. Wecompare the performance of the metric against human experts who independentlycarried out the classification process. Experimental results show that theperformance of the system using the novel metric performs better than twodifferent human expert.
arxiv-10800-42 | A New Approach to Building the Input--Output Table | http://arxiv.org/abs/1504.01362 | author:Ryohei Hisano category:stat.ML published:2015-04-06 summary:I present a new approach to estimating the interdependence of industries inan economy by applying data science solutions. By exploiting interfirmbuyer--seller network data, I show that the problem of estimating theinterdependence of industries is similar to the problem of uncovering thelatent block structure in network science literature. To estimate theunderlying structure with greater accuracy, I propose an extension of thesparse block model that incorporates node textual information and an unboundednumber of industries and interactions among them. The latter task isaccomplished by extending the well-known Chinese restaurant process to twodimensions. Inference is based on collapsed Gibbs sampling, and the model isevaluated on both synthetic and real-world datasets. I show that the proposedmodel improves in predictive accuracy and successfully provides a satisfactorysolution to the motivated problem. I also discuss issues that affect the futureperformance of this approach.
arxiv-10800-43 | Information Recovery from Pairwise Measurements | http://arxiv.org/abs/1504.01369 | author:Yuxin Chen, Changho Suh, Andrea J. Goldsmith category:cs.IT cs.DM cs.LG math.IT math.ST stat.ML stat.TH published:2015-04-06 summary:This paper is concerned with jointly recovering $n$ node-variables $\left\{x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise differencemeasurements. Imagine we acquire a few observations taking the form of$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph$\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ isobserved if and only if $(i,j)\in\mathcal{E}$. To account for noisymeasurements in a general manner, we model the data acquisition process by aset of channels with given input/output transition measures. Employinginformation-theoretic tools applied to channel decoding problems, we develop a\emph{unified} framework to characterize the fundamental recovery criterion,which accommodates general graph structures, alphabet sizes, and channeltransition measures. In particular, our results isolate a family of\emph{minimum} \emph{channel divergence measures} to characterize the degree ofmeasurement corruption, which together with the size of the minimum cut of$\mathcal{G}$ dictates the feasibility of exact information recovery. Forvarious homogeneous graphs, the recovery condition depends almost only on theedge sparsity of the measurement graph irrespective of other graphical metrics;alternatively, the minimum sample complexity required for these graphs scaleslike \[ \text{minimum sample complexity }\asymp\frac{n\logn}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric$\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabetsize is not super-polynomial in $n$. We apply our general theory to threeconcrete applications, including the stochastic block model, the outlier model,and the haplotype assembly problem. Our theory leads to order-wise tightrecovery conditions for all these scenarios.
arxiv-10800-44 | Bengali to Assamese Statistical Machine Translation using Moses (Corpus Based) | http://arxiv.org/abs/1504.01182 | author:Nayan Jyoti Kalita, Baharul Islam category:cs.CL published:2015-04-06 summary:Machine dialect interpretation assumes a real part in encouraging man-machinecorrespondence and in addition men-men correspondence in Natural LanguageProcessing (NLP). Machine Translation (MT) alludes to utilizing machine tochange one dialect to an alternate. Statistical Machine Translation is a typeof MT consisting of Language Model (LM), Translation Model (TM) and decoder. Inthis paper, Bengali to Assamese Statistical Machine Translation Model has beencreated by utilizing Moses. Other translation tools like IRSTLM for LanguageModel and GIZA-PP-V1.0.7 for Translation model are utilized within thisframework which is accessible in Linux situations. The purpose of the LM is toencourage fluent output and the purpose of TM is to encourage similaritybetween input and output, the decoder increases the probability of translatedtext in target language. A parallel corpus of 17100 sentences in Bengali andAssamese has been utilized for preparing within this framework. Measurable MTprocedures have not so far been generally investigated for Indian dialects. Itmight be intriguing to discover to what degree these models can help theimmense continuous MT deliberations in the nation.
arxiv-10800-45 | Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding | http://arxiv.org/abs/1504.01255 | author:Rie Johnson, Tong Zhang category:stat.ML cs.CL cs.LG published:2015-04-06 summary:This paper presents a new semi-supervised framework with convolutional neuralnetworks (CNNs) for text categorization. Unlike the previous approaches thatrely on word embeddings, our method learns embeddings of small text regionsfrom unlabeled data for integration into a supervised CNN. The proposed schemefor embedding learning is based on the idea of two-view semi-supervisedlearning, which is intended to be useful for the task of interest even thoughthe training is done on unlabeled data. Our models achieve better results thanprevious approaches on sentiment classification and topic classification tasks.
arxiv-10800-46 | A Probabilistic $\ell_1$ Method for Clustering High Dimensional Data | http://arxiv.org/abs/1504.01294 | author:Tsvetan Asamov, Adi Ben-Israel category:math.ST cs.LG math.OC stat.ML stat.TH published:2015-04-06 summary:In general, the clustering problem is NP-hard, and global optimality cannotbe established for non-trivial instances. For high-dimensional data,distance-based methods for clustering or classification face an additionaldifficulty, the unreliability of distances in very high-dimensional spaces. Wepropose a distance-based iterative method for clustering data in veryhigh-dimensional space, using the $\ell_1$-metric that is less sensitive tohigh dimensionality than the Euclidean distance. For $K$ clusters in$\mathbb{R}^n$, the problem decomposes to $K$ problems coupled byprobabilities, and an iteration reduces to finding $Kn$ weighted medians ofpoints on a line. The complexity of the algorithm is linear in the dimension ofthe data space, and its performance was observed to improve significantly asthe dimension increases.
arxiv-10800-47 | Discriminative Neural Sentence Modeling by Tree-Based Convolution | http://arxiv.org/abs/1504.01106 | author:Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE published:2015-04-05 summary:This paper proposes a tree-based convolutional neural network (TBCNN) fordiscriminative sentence modeling. Our models leverage either constituency treesor dependency trees of sentences. The tree-based convolution process extractssentences' structural features, and these features are aggregated by maxpooling. Such architecture allows short propagation paths between the outputlayer and underlying feature detectors, which enables effective structuralfeature learning and extraction. We evaluate our models on two tasks: sentimentanalysis and question classification. In both experiments, TBCNN outperformsprevious state-of-the-art results, including existing neural networks anddedicated feature/rule engineering. We also make efforts to visualize thetree-based convolution process, shedding light on how our models work.
arxiv-10800-48 | Ultra-large alignments using Phylogeny-aware Profiles | http://arxiv.org/abs/1504.01142 | author:Nam-phuong Nguyen, Siavash Mirarab, Keerthana Kumar, Tandy Warnow category:q-bio.GN cs.CE cs.LG published:2015-04-05 summary:Many biological questions, including the estimation of deep evolutionaryhistories and the detection of remote homology between protein sequences, relyupon multiple sequence alignments (MSAs) and phylogenetic trees of largedatasets. However, accurate large-scale multiple sequence alignment is verydifficult, especially when the dataset contains fragmentary sequences. Wepresent UPP, an MSA method that uses a new machine learning technique - theEnsemble of Hidden Markov Models - that we propose here. UPP produces highlyaccurate alignments for both nucleotide and amino acid sequences, even onultra-large datasets or datasets containing fragmentary sequences. UPP isavailable at https://github.com/smirarab/sepp.
arxiv-10800-49 | EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by Noise and Interference | http://arxiv.org/abs/1504.01072 | author:Silvija Kokalj-Filipovic, Larry Greenstein category:cs.LG published:2015-04-05 summary:We propose a method for estimating channel parameters from RSSI measurementsand the lost packet count, which can work in the presence of losses due to bothinterference and signal attenuation below the noise floor. This is especiallyimportant in the wireless networks, such as vehicular, where propagation modelchanges with the density of nodes. The method is based on StochasticExpectation Maximization, where the received data is modeled as a mixture ofdistributions (no/low interference and strong interference), incomplete(censored) due to packet losses. The PDFs in the mixture are Gamma, accordingto the commonly accepted model for wireless signal and interference power. Thisapproach leverages the loss count as additional information, henceoutperforming maximum likelihood estimation, which does not use thisinformation (ML-), for a small number of received RSSI samples. Hence, itallows inexpensive on-line channel estimation from ad-hoc collected data. Themethod also outperforms ML- on uncensored data mixtures, as ML- assumes thatsamples are from a single-mode PDF.
arxiv-10800-50 | Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via Eigenvector and Semidefinite Programming Synchronization | http://arxiv.org/abs/1504.01070 | author:Mihai Cucuringu category:cs.LG cs.SI math.OC stat.ML published:2015-04-05 summary:We consider the classic problem of establishing a statistical ranking of aset of n items given a set of inconsistent and incomplete pairwise comparisonsbetween such items. Instantiations of this problem occur in numerousapplications in data analysis (e.g., ranking teams in sports data), computervision, and machine learning. We formulate the above problem of ranking withincomplete noisy information as an instance of the group synchronizationproblem over the group SO(2) of planar rotations, whose usefulness has beendemonstrated in numerous applications in recent years. Its least squaressolution can be approximated by either a spectral or a semidefinite programming(SDP) relaxation, followed by a rounding procedure. We perform extensivenumerical simulations on both synthetic and real-world data sets, showing thatour proposed method compares favorably to other algorithms from the recentliterature. Existing theoretical guarantees on the group synchronizationproblem imply lower bounds on the largest amount of noise permissible in theranking data while still achieving exact recovery. We propose a similarsynchronization-based algorithm for the rank-aggregation problem, whichintegrates in a globally consistent ranking pairwise comparisons given bydifferent rating systems on the same set of items. We also discuss the problemof semi-supervised ranking when there is available information on the groundtruth rank of a subset of players, and propose an algorithm based on SDP whichrecovers the ranks of the remaining players. Finally, synchronization-basedranking, combined with a spectral technique for the densest subgraph problem,allows one to extract locally-consistent partial rankings, in other words, toidentify the rank of a small subset of players whose pairwise comparisons areless noisy than the rest of the data, which other methods are not able toidentify.
arxiv-10800-51 | Heuristic algorithms for obtaining Polynomial Threshold Functions with low densities | http://arxiv.org/abs/1504.01167 | author:Can Eren Sezener, Erhan Oztop category:cs.CC cs.NE published:2015-04-05 summary:In this paper we present several heuristic algorithms, including a GeneticAlgorithm (GA), for obtaining polynomial threshold function (PTF)representations of Boolean functions (BFs) with small number of monomials. Wecompare these among each other and against the algorithm of Oztop viacomputational experiments. The results indicate that our heuristic algorithmsfind more parsimonious representations compared to the those of non-heuristicand GA-based algorithms.
arxiv-10800-52 | Efficient Dictionary Learning via Very Sparse Random Projections | http://arxiv.org/abs/1504.01169 | author:Farhad Pourkamali-Anaraki, Stephen Becker, Shannon M. Hughes category:stat.ML cs.LG published:2015-04-05 summary:Performing signal processing tasks on compressive measurements of data hasreceived great attention in recent years. In this paper, we extend previouswork on compressive dictionary learning by showing that more general randomprojections may be used, including sparse ones. More precisely, we examinecompressive K-means clustering as a special case of compressive dictionarylearning and give theoretical guarantees for its performance for a very generalclass of random projections. We then propose a memory and computation efficientdictionary learning algorithm, specifically designed for analyzing largevolumes of high-dimensional data, which learns the dictionary from very sparserandom projections. Experimental results demonstrate that our approach allowsfor reduction of computational complexity and memory/data access, withcontrollable loss in accuracy.
arxiv-10800-53 | Recursive Partitioning for Heterogeneous Causal Effects | http://arxiv.org/abs/1504.01132 | author:Susan Athey, Guido Imbens category:stat.ML published:2015-04-05 summary:In this paper we study the problems of estimating heterogeneity in causaleffects in experimental or observational studies and conducting inference aboutthe magnitude of the differences in treatment effects across subsets of thepopulation. In applications, our method provides a data-driven approach todetermine which subpopulations have large or small treatment effects and totest hypotheses about the differences in these effects. For experiments, ourmethod allows researchers to identify heterogeneity in treatment effects thatwas not specified in a pre-analysis plan, without concern about invalidatinginference due to multiple testing. In most of the literature on supervisedmachine learning (e.g. regression trees, random forests, LASSO, etc.), the goalis to build a model of the relationship between a unit's attributes and anobserved outcome. A prominent role in these methods is played bycross-validation which compares predictions to actual outcomes in test samples,in order to select the level of complexity of the model that provides the bestpredictive power. Our method is closely related, but it differs in that it istailored for predicting causal effects of a treatment rather than a unit'soutcome. The challenge is that the "ground truth" for a causal effect is notobserved for any individual unit: we observe the unit with the treatment, orwithout the treatment, but not both at the same time. Thus, it is not obvioushow to use cross-validation to determine whether a causal effect has beenaccurately predicted. We propose several novel cross-validation criteria forthis problem and demonstrate through simulations the conditions under whichthey perform better than standard methods for the problem of causal effects. Wethen apply the method to a large-scale field experiment re-ranking results on asearch engine.
arxiv-10800-54 | Discriminative and Efficient Label Propagation on Complementary Graphs for Multi-Object Tracking | http://arxiv.org/abs/1504.01124 | author:Amit Kumar K. C., Laurent Jacques, Christophe De Vleeschouwer category:cs.CV published:2015-04-05 summary:Given a set of detections, detected at each time instant independently, weinvestigate how to associate them across time. This is done by propagatinglabels on a set of graphs, each graph capturing how either the spatio-temporalor the appearance cues promote the assignment of identical or distinct labelsto a pair of detections. The graph construction is motivated by a locallylinear embedding of the detection features. Interestingly, the neighborhood ofa node in appearance graph is defined to include all the nodes for which theappearance feature is available (even if they are temporally distant). Thisgives our framework the uncommon ability to exploit the appearance featuresthat are available only sporadically. Once the graphs have been defined,multi-object tracking is formulated as the problem of finding a labelassignment that is consistent with the constraints captured each graph, whichresults into a difference of convex (DC) program. We propose to decompose theglobal objective function into node-wise sub-problems. This not only allows acomputationally efficient solution, but also supports an incremental andscalable construction of the graph, thereby making the framework applicable tolarge graphs and practical tracking scenarios. Moreover, it opens thepossibility of parallel implementation.
arxiv-10800-55 | Convex Denoising using Non-Convex Tight Frame Regularization | http://arxiv.org/abs/1504.00976 | author:Ankit Parekh, Ivan W. Selesnick category:cs.CV math.OC published:2015-04-04 summary:This paper considers the problem of signal denoising using a sparsetight-frame analysis prior. The L1 norm has been extensively used as aregularizer to promote sparsity; however, it tends to under-estimate non-zerovalues of the underlying signal. To more accurately estimate non-zero values,we propose the use of a non-convex regularizer, chosen so as to ensureconvexity of the objective function. The convexity of the objective function isensured by constraining the parameter of the non-convex penalty. We use ADMM toobtain a solution and show how to guarantee that ADMM converges to the globaloptimum of the objective function. We illustrate the proposed method for 1D and2D signal denoising.
arxiv-10800-56 | Concept Drift Detection for Streaming Data | http://arxiv.org/abs/1504.01044 | author:Heng Wang, Zubin Abraham category:stat.ML cs.LG published:2015-04-04 summary:Common statistical prediction models often require and assume stationarity inthe data. However, in many practical applications, changes in the relationshipof the response and predictor variables are regularly observed over time,resulting in the deterioration of the predictive performance of these models.This paper presents Linear Four Rates (LFR), a framework for detecting theseconcept drifts and subsequently identifying the data points that belong to thenew concept (for relearning the model). Unlike conventional concept driftdetection approaches, LFR can be applied to both batch and stream data; is notlimited by the distribution properties of the response variable (e.g., datasetswith imbalanced labels); is independent of the underlying statistical-model;and uses user-specified parameters that are intuitively comprehensible. Theperformance of LFR is compared to benchmark approaches using both simulated andcommonly used public datasets that span the gamut of concept drift types. Theresults show LFR significantly outperforms benchmark approaches in terms ofrecall, accuracy and delay in detection of concept drifts across datasets.
arxiv-10800-57 | ELM-Based Distributed Cooperative Learning Over Networks | http://arxiv.org/abs/1504.00981 | author:Wu Ai, Weisheng Chen category:cs.LG math.OC published:2015-04-04 summary:This paper investigates distributed cooperative learning algorithms for dataprocessing in a network setting. Specifically, the extreme learning machine(ELM) is introduced to train a set of data distributed across severalcomponents, and each component runs a program on a subset of the entire data.In this scheme, there is no requirement for a fusion center in the network dueto e.g., practical limitations, security, or privacy reasons. We firstreformulate the centralized ELM training problem into a separable form amongnodes with consensus constraints. Then, we solve the equivalent problem usingdistributed optimization tools. A new distributed cooperative learningalgorithm based on ELM, called DC-ELM, is proposed. The architecture of thisalgorithm differs from that of some existing parallel/distributed ELMs based onMapReduce or cloud computing. We also present an online version of the proposedalgorithm that can learn data sequentially in a one-by-one or chunk-by-chunkmode. The novel algorithm is well suited for potential applications such asartificial intelligence, computational biology, finance, wireless sensornetworks, and so on, involving datasets that are often extremely large,high-dimensional and located on distributed data sources. We show simulationresults on both synthetic and real-world data sets.
arxiv-10800-58 | An Online Approach to Dynamic Channel Access and Transmission Scheduling | http://arxiv.org/abs/1504.01050 | author:Yang Liu, Mingyan Liu category:cs.LG cs.SY published:2015-04-04 summary:Making judicious channel access and transmission scheduling decisions isessential for improving performance as well as energy and spectral efficiencyin multichannel wireless systems. This problem has been a subject of extensivestudy in the past decade, and the resulting dynamic and opportunistic channelaccess schemes can bring potentially significant improvement over traditionalschemes. However, a common and severe limitation of these dynamic schemes isthat they almost always require some form of a priori knowledge of the channelstatistics. A natural remedy is a learning framework, which has also beenextensively studied in the same context, but a typical learning algorithm inthis literature seeks only the best static policy, with performance measured byweak regret, rather than learning a good dynamic channel access policy. Thereis thus a clear disconnect between what an optimal channel access policy canachieve with known channel statistics that actively exploits temporal, spatialand spectral diversity, and what a typical existing learning algorithm aimsfor, which is the static use of a single channel devoid of diversity gain. Inthis paper we bridge this gap by designing learning algorithms that track knownoptimal or sub-optimal dynamic channel access and transmission schedulingpolicies, thereby yielding performance measured by a form of strong regret, theaccumulated difference between the reward returned by an optimal solution whena priori information is available and that by our online algorithm. We do so inthe context of two specific algorithms that appeared in [1] and [2],respectively, the former for a multiuser single-channel setting and the latterfor a single-user multichannel setting. In both cases we show that ouralgorithms achieve sub-linear regret uniform in time and outperforms thestandard weak-regret learning algorithms.
arxiv-10800-59 | Preprint Extending Touch-less Interaction on Vision Based Wearable Device | http://arxiv.org/abs/1504.01025 | author:Zhihan Lv, Liangbing Feng, Shengzhong Feng, Haibo Li category:cs.HC cs.CV cs.GR H.1.2; H.5.1 published:2015-04-04 summary:This is the preprint version of our paper on IEEE Virtual Reality Conference2015. A touch-less interaction technology on vision based wearable device isdesigned and evaluated. Users interact with the application with dynamichands/feet gestures in front of the camera. Several proof-of-concept prototypeswith eleven dynamic gestures are developed based on the touch-less interaction.At last, a comparing user study evaluation is proposed to demonstrate theusability of the touch-less approach, as well as the impact on user's emotion,running on a wearable framework or Google Glass.
arxiv-10800-60 | Fast algorithms for morphological operations using run-length encoded binary images | http://arxiv.org/abs/1504.01052 | author:Gregor Ehrensperger, Alexander Ostermann, Felix Schwitzer category:cs.CV cs.GR cs.IT math.IT published:2015-04-04 summary:This paper presents innovative algorithms to efficiently compute erosions anddilations of run-length encoded (RLE) binary images with arbitrary shapedstructuring elements. An RLE image is given by a set of runs, where a run is ahorizontal concatenation of foreground pixels. The proposed algorithms extractthe skeleton of the structuring element and build distance tables of the inputimage, which are storing the distance to the next background pixel on the leftand right hand sides. This information is then used to speed up thecalculations of the erosion and dilation operator by enabling the use oftechniques which allow to skip the analysis of certain pixels whenever a hit ormiss occurs. Additionally the input image gets trimmed during the preprocessingsteps on the base of two primitive criteria. Experimental results show theadvantages over other algorithms. The source code of our algorithms isavailable in C++.
arxiv-10800-61 | Watch and Learn: Optimizing from Revealed Preferences Feedback | http://arxiv.org/abs/1504.01033 | author:Aaron Roth, Jonathan Ullman, Zhiwei Steven Wu category:cs.DS cs.GT cs.LG published:2015-04-04 summary:A Stackelberg game is played between a leader and a follower. The leaderfirst chooses an action, then the follower plays his best response. The goal ofthe leader is to pick the action that will maximize his payoff given thefollower's best response. In this paper we present an approach to solving forthe leader's optimal strategy in certain Stackelberg games where the follower'sutility function (and thus the subsequent best response of the follower) isunknown. Stackelberg games capture, for example, the following interaction between aproducer and a consumer. The producer chooses the prices of the goods heproduces, and then a consumer chooses to buy a utility maximizing bundle ofgoods. The goal of the seller here is to set prices to maximize hisprofit---his revenue, minus the production cost of the purchased bundle. It isquite natural that the seller in this example should not know the buyer'sutility function. However, he does have access to revealed preferencefeedback---he can set prices, and then observe the purchased bundle and his ownprofit. We give algorithms for efficiently solving, in terms of bothcomputational and query complexity, a broad class of Stackelberg games in whichthe follower's utility function is unknown, using only "revealed preference"access to it. This class includes in particular the profit maximizationproblem, as well as the optimal tolling problem in nonatomic congestion games,when the latency functions are unknown. Surprisingly, we are able to solvethese problems even though the optimization problems are non-convex in theleader's actions.
arxiv-10800-62 | Efficient piecewise training of deep structured models for semantic segmentation | http://arxiv.org/abs/1504.01013 | author:Guosheng Lin, Chunhua Shen, Anton van dan Hengel, Ian Reid category:cs.CV published:2015-04-04 summary:Recent advances in semantic image segmentation have mostly been achieved bytraining deep convolutional neural networks (CNNs) for the task. We show how toimprove semantic segmentation through the use of contextual information.Specifically, we explore `patch-patch' context and `patch-background' contextwith deep CNNs. For learning the patch-patch context between image regions, weformulate Conditional Random Fields (CRFs) with CNN-based pairwise potentialfunctions to capture semantic correlations between neighboring patches.Efficient piecewise training of the proposed deep structured model is thenapplied to avoid repeated expensive CRF inference for back propagation. Inorder to capture the patch-background context, we show that a network designwith traditional multi-scale image input and sliding pyramid pooling iseffective for improving performance. Our experiment results set newstate-of-the-art performance on a number of popular semantic segmentationdatasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow.Particularly, we achieve an intersection-over-union score of $77.8$ on thechallenging PASCAL VOC 2012 dataset.
arxiv-10800-63 | Graph Connectivity in Noisy Sparse Subspace Clustering | http://arxiv.org/abs/1504.01046 | author:Yining Wang, Yu-Xiang Wang, Aarti Singh category:stat.ML cs.LG published:2015-04-04 summary:Subspace clustering is the problem of clustering data points into a union oflow-dimensional linear/affine subspaces. It is the mathematical abstraction ofmany important problems in computer vision, image processing and machinelearning. A line of recent work (4, 19, 24, 20) provided strong theoreticalguarantee for sparse subspace clustering (4), the state-of-the-art algorithmfor subspace clustering, on both noiseless and noisy data sets. It was shownthat under mild conditions, with high probability no two points from differentsubspaces are clustered together. Such guarantee, however, is not sufficientfor the clustering to be correct, due to the notorious "graph connectivityproblem" (15). In this paper, we investigate the graph connectivity problem fornoisy sparse subspace clustering and show that a simple post-processingprocedure is capable of delivering consistent clustering under certain "generalposition" or "restricted eigenvalue" assumptions. We also show that ourcondition is almost tight with adversarial noise perturbation by constructing acounter-example. These results provide the first exact clustering guarantee ofnoisy SSC for subspaces of dimension greater then 3.
arxiv-10800-64 | Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images | http://arxiv.org/abs/1504.00983 | author:Chen Sun, Sanketh Shetty, Rahul Sukthankar, Ram Nevatia category:cs.CV cs.MM I.2.10 published:2015-04-04 summary:We address the problem of fine-grained action localization from temporallyuntrimmed web videos. We assume that only weak video-level annotations areavailable for training. The goal is to use these weak labels to identifytemporal segments corresponding to the actions, and learn models thatgeneralize to unconstrained web videos. We find that web images queried byaction names serve as well-localized highlights for many actions, but arenoisily labeled. To solve this problem, we propose a simple yet effectivemethod that takes weak video labels and noisy image labels as input, andgenerates localized action frames as output. This is achieved by cross-domaintransfer between video frames and web images, using pre-trained deepconvolutional neural networks. We then use the localized action frames to trainaction recognition models with long short-term memory networks. We collect afine-grained sports action data set FGA-240 of more than 130,000 YouTubevideos. It has 240 fine-grained actions under 85 sports activities. Convincingresults are shown on the FGA-240 data set, as well as the THUMOS 2014localization data set with untrimmed training videos.
arxiv-10800-65 | Robust Anomaly Detection Using Semidefinite Programming | http://arxiv.org/abs/1504.00905 | author:Jose A. Lopez, Octavia Camps, Mario Sznaier category:math.OC cs.CV cs.LG cs.SY published:2015-04-03 summary:This paper presents a new approach, based on polynomial optimization and themethod of moments, to the problem of anomaly detection. The proposed techniqueonly requires information about the statistical moments of the normal-statedistribution of the features of interest and compares favorably with existingapproaches (such as Parzen windows and 1-class SVM). In addition, it provides asuccinct description of the normal state. Thus, it leads to a substantialsimplification of the the anomaly detection problem when working with higherdimensional datasets.
arxiv-10800-66 | Unsupervised Feature Selection with Adaptive Structure Learning | http://arxiv.org/abs/1504.00736 | author:Liang Du, Yi-Dong Shen category:cs.LG published:2015-04-03 summary:The problem of feature selection has raised considerable interests in thepast decade. Traditional unsupervised methods select the features which canfaithfully preserve the intrinsic structures of data, where the intrinsicstructures are estimated using all the input features of data. However, theestimated intrinsic structures are unreliable/inaccurate when the redundant andnoisy features are not removed. Therefore, we face a dilemma here: one need thetrue structures of data to identify the informative features, and one need theinformative features to accurately estimate the true structures of data. Toaddress this, we propose a unified learning framework which performs structurelearning and feature selection simultaneously. The structures are adaptivelylearned from the results of feature selection, and the informative features arereselected to preserve the refined structures of data. By leveraging theinteractions between these two essential tasks, we are able to capture accuratestructures and select more informative features. Experimental results on manybenchmark data sets demonstrate that the proposed method outperforms many stateof the art unsupervised feature selection methods.
arxiv-10800-67 | A Unified Deep Neural Network for Speaker and Language Recognition | http://arxiv.org/abs/1504.00923 | author:Fred Richardson, Douglas Reynolds, Najim Dehak category:cs.CL cs.CV cs.LG cs.NE stat.ML published:2015-04-03 summary:Learned feature representations and sub-phoneme posteriors from Deep NeuralNetworks (DNNs) have been used separately to produce significant performancegains for speaker and language recognition tasks. In this work we show howthese gains are possible using a single DNN for both speaker and languagerecognition. The unified DNN approach is shown to yield substantial performanceimprovements on the the 2013 Domain Adaptation Challenge speaker recognitiontask (55% reduction in EER for the out-of-domain condition) and on the NIST2011 Language Recognition Evaluation (48% reduction in EER for the 30s testcondition).
arxiv-10800-68 | Evaluation Evaluation a Monte Carlo study | http://arxiv.org/abs/1504.00854 | author:David M. W. Powers category:cs.AI cs.CL stat.ML published:2015-04-03 summary:Over the last decade there has been increasing concern about the biasesembodied in traditional evaluation methods for Natural LanguageProcessing/Learning, particularly methods borrowed from Information Retrieval.Without knowledge of the Bias and Prevalence of the contingency being tested,or equivalently the expectation due to chance, the simple conditionalprobabilities Recall, Precision and Accuracy are not meaningful as evaluationmeasures, either individually or in combinations such as F-factor. Theexistence of bias in NLP measures leads to the 'improvement' of systems byincreasing their bias, such as the practice of improving tagging and parsingscores by using most common value (e.g. water is always a Noun) rather than theattempting to discover the correct one. The measures Cohen Kappa and PowersInformedness are discussed as unbiased alternative to Recall and related to thepsychologically significant measure DeltaP. In this paper we will analyze bothbiased and unbiased measures theoretically, characterizing the preciserelationship between all these measures as well as evaluating the evaluationmeasures themselves empirically using a Monte Carlo simulation.
arxiv-10800-69 | Point Localization and Density Estimation from Ordinal kNN graphs using Synchronization | http://arxiv.org/abs/1504.00722 | author:Mihai Cucuringu, Joseph Woodworth category:stat.ML published:2015-04-03 summary:We consider the problem of embedding unweighted, directed k-nearest neighborgraphs in low-dimensional Euclidean space. The k-nearest neighbors of eachvertex provides ordinal information on the distances between points, but notthe distances themselves. We use this ordinal information along with thelow-dimensionality to recover the coordinates of the points up to arbitrarysimilarity transformations (rigid transformations and scaling). Furthermore, wealso illustrate the possibility of robustly recovering the underlying densityvia the Total Variation Maximum Penalized Likelihood Estimation (TV-MPLE)method. We make existing approaches scalable by using an instance of alocal-to-global algorithm based on group synchronization, recently proposed inthe literature in the context of sensor network localization and structuralbiology, which we augment with a scaling synchronization step. We demonstratethe scalability of our approach on large graphs, and show how it compares tothe Local Ordinal Embedding (LOE) algorithm, which was recently proposed forrecovering the configuration of a cloud of points from pairwise ordinalcomparisons between a sparse set of distances.
arxiv-10800-70 | The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth Selection in Univariate and Multivariate Kernel Density Estimations | http://arxiv.org/abs/1504.00781 | author:Dharmani Bhaveshkumar C category:cs.LG stat.CO stat.ME stat.ML 11Kxx I.5.0 published:2015-04-03 summary:The article derives a novel Gram-Charlier A (GCA) Series based ExtendedRule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation(KDE). There are existing various bandwidth selection rules achievingminimization of the Asymptotic Mean Integrated Square Error (AMISE) between theestimated probability density function (PDF) and the actual PDF. The rulesdiffer in a way to estimate the integration of the squared second orderderivative of an unknown PDF $(f(\cdot))$, identified as the roughness$R(f''(\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\cdot))$with an assumption that the density being estimated is Gaussian. Intuitively,better estimation of $R(f''(\cdot))$ and consequently better bandwidthselection rules can be derived, if the unknown PDF is approximated through aninfinite series expansion based on a more generalized density assumption. As ademonstration and verification to this concept, the ExROT derived in thearticle uses an extended assumption that the density being estimated is nearGaussian. This helps use of the GCA expansion as an approximation to theunknown near Gaussian PDF. The ExROT for univariate KDE is extended to that formultivariate KDE. The required multivariate AMISE criteria is re-derived usingelementary calculus of several variables, instead of Tensor calculus. Thederivation uses the Kronecker product and the vector differential operator toachieve the AMISE expression in vector notations. There is also derived ExROTfor kernel based density derivative estimator.
arxiv-10800-71 | Learning Mixed Membership Mallows Models from Pairwise Comparisons | http://arxiv.org/abs/1504.00757 | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG stat.ML published:2015-04-03 summary:We propose a novel parameterized family of Mixed Membership Mallows Models(M4) to account for variability in pairwise comparisons generated by aheterogeneous population of noisy and inconsistent users. M4 models individualpreferences as a user-specific probabilistic mixture of shared latent Mallowscomponents. Our key algorithmic insight for estimation is to establish astatistical connection between M4 and topic models by viewing pairwisecomparisons as words, and users as documents. This key insight leads us toexplore Mallows components with a separable structure and leverage recentadvances in separable topic discovery. While separability appears to be overlyrestrictive, we nevertheless show that it is an inevitable outcome of arelatively small number of latent Mallows components in a world of large numberof items. We then develop an algorithm based on robust extreme-pointidentification of convex polygons to learn the reference rankings, and isprovably consistent with polynomial sample complexity guarantees. Wedemonstrate that our new model is empirically competitive with the currentstate-of-the-art approaches in predicting real-world preferences.
arxiv-10800-72 | A Simple Way to Initialize Recurrent Networks of Rectified Linear Units | http://arxiv.org/abs/1504.00941 | author:Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton category:cs.NE cs.LG published:2015-04-03 summary:Learning long term dependencies in recurrent networks is difficult due tovanishing and exploding gradients. To overcome this difficulty, researchershave developed sophisticated optimization techniques and network architectures.In this paper, we propose a simpler solution that use recurrent neural networkscomposed of rectified linear units. Key to our solution is the use of theidentity matrix or its scaled version to initialize the recurrent weightmatrix. We find that our solution is comparable to LSTM on our four benchmarks:two toy problems involving long-range temporal structures, a large languagemodeling problem and a benchmark speech recognition problem.
arxiv-10800-73 | The Child is Father of the Man: Foresee the Success at the Early Stage | http://arxiv.org/abs/1504.00948 | author:Liangyue Li, Hanghang Tong category:cs.LG published:2015-04-03 summary:Understanding the dynamic mechanisms that drive the high-impact scientificwork (e.g., research papers, patents) is a long-debated research topic and hasmany important implications, ranging from personal career development andrecruitment search, to the jurisdiction of research resources. Recent advancesin characterizing and modeling scientific success have made it possible toforecast the long-term impact of scientific work, where data mining techniques,supervised learning in particular, play an essential role. Despite muchprogress, several key algorithmic challenges in relation to predictinglong-term scientific impact have largely remained open. In this paper, wepropose a joint predictive model to forecast the long-term scientific impact atthe early stage, which simultaneously addresses a number of these openchallenges, including the scholarly feature design, the non-linearity, thedomain-heterogeneity and dynamics. In particular, we formulate it as aregularized optimization problem and propose effective and scalable algorithmsto solve it. We perform extensive empirical evaluations on large, realscholarly data sets to validate the effectiveness and the efficiency of ourmethod.
arxiv-10800-74 | A Probabilistic Theory of Deep Learning | http://arxiv.org/abs/1504.00641 | author:Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk category:stat.ML cs.CV cs.LG cs.NE published:2015-04-02 summary:A grand challenge in machine learning is the development of computationalalgorithms that match or outperform humans in perceptual inference tasks thatare complicated by nuisance variation. For instance, visual object recognitioninvolves the unknown object position, orientation, and scale in objectrecognition while speech recognition involves the unknown voice pronunciation,pitch, and speed. Recently, a new breed of deep learning algorithms haveemerged for high-nuisance inference tasks that routinely yield patternrecognition systems with near- or super-human capabilities. But a fundamentalquestion remains: Why do they work? Intuitions abound, but a coherent frameworkfor understanding, analyzing, and synthesizing deep learning architectures hasremained elusive. We answer this question by developing a new probabilisticframework for deep learning based on the Deep Rendering Model: a generativeprobabilistic model that explicitly captures latent nuisance variation. Byrelaxing the generative model to a discriminative one, we can recover two ofthe current leading deep learning systems, deep convolutional neural networksand random decision forests, providing insights into their successes andshortcomings, as well as a principled route to their improvement.
arxiv-10800-75 | End-to-End Training of Deep Visuomotor Policies | http://arxiv.org/abs/1504.00702 | author:Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel category:cs.LG cs.CV cs.RO published:2015-04-02 summary:Policy search methods can allow robots to learn control policies for a widerange of tasks, but practical applications of policy search often requirehand-engineered components for perception, state estimation, and low-levelcontrol. In this paper, we aim to answer the following question: does trainingthe perception and control systems jointly end-to-end provide betterperformance than training each component separately? To this end, we develop amethod that can be used to learn policies that map raw image observationsdirectly to torques at the robot's motors. The policies are represented by deepconvolutional neural networks (CNNs) with 92,000 parameters, and are trainedusing a partially observed guided policy search method, which transforms policysearch into supervised learning, with supervision provided by a simpletrajectory-centric reinforcement learning method. We evaluate our method on arange of real-world manipulation tasks that require close coordination betweenvision and control, such as screwing a cap onto a bottle, and present simulatedcomparisons to a range of prior policy search methods.
arxiv-10800-76 | Antisocial Behavior in Online Discussion Communities | http://arxiv.org/abs/1504.00680 | author:Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.SI cs.CY stat.AP stat.ML published:2015-04-02 summary:User contributions in the form of posts, comments, and votes are essential tothe success of online communities. However, allowing user participation alsoinvites undesirable behavior such as trolling. In this paper, we characterizeantisocial behavior in three large online discussion communities by analyzingusers who were banned from these communities. We find that such users tend toconcentrate their efforts in a small number of threads, are more likely to postirrelevantly, and are more successful at garnering responses from other users.Studying the evolution of these users from the moment they join a community upto when they get banned, we find that not only do they write worse than otherusers over time, but they also become increasingly less tolerated by thecommunity. Further, we discover that antisocial behavior is exacerbated whencommunity feedback is overly harsh. Our analysis also reveals distinct groupsof users with different levels of antisocial behavior that can change overtime. We use these insights to identify antisocial users early on, a task ofhigh practical importance to community maintainers.
arxiv-10800-77 | The Approximation of the Dissimilarity Projection | http://arxiv.org/abs/1504.00593 | author:Emanuele Olivetti, Thien Bao Nguyen, Paolo Avesani category:stat.ML cs.CV published:2015-04-02 summary:Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3Dpathways of axons within the white matter of the brain as a tractography. Theanalysis of tractographies has drawn attention from the machine learning andpattern recognition communities providing novel challenges such as finding anappropriate representation space for the data. Many of the current learningalgorithms require the input to be from a vectorial space. This requirementcontrasts with the intrinsic nature of the tractography because its basicelements, called streamlines or tracks, have different lengths and differentnumber of points and for this reason they cannot be directly represented in acommon vectorial space. In this work we propose the adoption of thedissimilarity representation which is an Euclidean embedding technique definedby selecting a set of streamlines called prototypes and then mapping any newstreamline to the vector of distances from prototypes. We investigate thedegree of approximation of this projection under different prototype selectionpolicies and prototype set sizes in order to characterise its use ontractography data. Additionally we propose the use of a scalable approximationof the most effective prototype selection policy that provides fast andaccurate dissimilarity approximations of complete tractographies.
arxiv-10800-78 | Eliciting Disease Data from Wikipedia Articles | http://arxiv.org/abs/1504.00657 | author:Geoffrey Fairchild, Lalindra De Silva, Sara Y. Del Valle, Alberto M. Segre category:cs.IR cs.CL cs.SI q-bio.PE published:2015-04-02 summary:Traditional disease surveillance systems suffer from several disadvantages,including reporting lags and antiquated technology, that have caused a movementtowards internet-based disease surveillance systems. Internet systems areparticularly attractive for disease outbreaks because they can provide data innear real-time and can be verified by individuals around the globe. However,most existing systems have focused on disease monitoring and do not provide adata repository for policy makers or researchers. In order to fill this gap, weanalyzed Wikipedia article content. We demonstrate how a named-entity recognizer can be trained to tag casecounts, death counts, and hospitalization counts in the article narrative thatachieves an F1 score of 0.753. We also show, using the 2014 West African Ebolavirus disease epidemic article as a case study, that there are detailed timeseries data that are consistently updated that closely align with ground truthdata. We argue that Wikipedia can be used to create the first community-drivenopen-source emerging disease detection, monitoring, and repository system.
arxiv-10800-79 | Direct l_(2,p)-Norm Learning for Feature Selection | http://arxiv.org/abs/1504.00430 | author:Hanyang Peng, Yong Fan category:cs.LG cs.CV published:2015-04-02 summary:In this paper, we propose a novel sparse learning based feature selectionmethod that directly optimizes a large margin linear classification modelsparsity with l_(2,p)-norm (0 < p < 1)subject to data-fitting constraints,rather than using the sparsity as a regularization term. To solve the directsparsity optimization problem that is non-smooth and non-convex when 0<p<1, weprovide an efficient iterative algorithm with proved convergence by convertingit to a convex and smooth optimization problem at every iteration step. Theproposed algorithm has been evaluated based on publicly available datasets, andextensive comparison experiments have demonstrated that our algorithm couldachieve feature selection performance competitive to state-of-the-artalgorithms.
arxiv-10800-80 | Structure Learning of Partitioned Markov Networks | http://arxiv.org/abs/1504.00624 | author:Song Liu, Taiji Suzuki, Masashi Sugiyama, Kenji Fukumizu category:stat.ML published:2015-04-02 summary:We learn the structure of a Markov Network between two groups of randomvariables from joint observations. Since modelling and learning the full MNstructure may be hard, learning the links between two groups directly may be apreferable option. We introduce a novel concept called the \emph{partitionedratio} whose factorization directly associates with the Markovian properties ofrandom variables across two groups. A simple one-shot convex optimizationprocedure is proposed for learning the \emph{sparse} factorizations of thepartitioned ratio and it is theoretically guaranteed to recover the correctinter-group structure under mild conditions. The performance of the proposedmethod is experimentally compared with the state of the art MN structurelearning methods using ROC curves. Real applications on analyzingbipartisanship in US congress and pairwise DNA/time-series alignments are alsoreported.
arxiv-10800-81 | Quantum image classification using principal component analysis | http://arxiv.org/abs/1504.00580 | author:Mateusz Ostaszewski, Przemysław Sadowski, Piotr Gawron category:quant-ph cs.CV cs.LG published:2015-04-02 summary:We present a novel quantum algorithm for classification of images. Thealgorithm is constructed using principal component analysis and von Neumanquantum measurements. In order to apply the algorithm we present a new quantumrepresentation of grayscale images.
arxiv-10800-82 | Learning to Understand Phrases by Embedding the Dictionary | http://arxiv.org/abs/1504.00548 | author:Felix Hill, Kyunghyun Cho, Anna Korhonen, Yoshua Bengio category:cs.CL published:2015-04-02 summary:Distributional models that learn rich semantic word representations are asuccess story of recent NLP research. However, developing models that learnuseful representations of phrases and sentences has proved far harder. Wepropose using the definitions found in everyday dictionaries as a means ofbridging this gap between lexical and phrasal semantics. Neural languageembedding models can be effectively trained to map dictionary definitions(phrases) to (lexical) representations of the words defined by thosedefinitions. We present two applications of these architectures: "reversedictionaries" that return the name of a concept given a definition ordescription and general-knowledge crossword question answerers. On both tasks,neural language embedding models trained on definitions from a handful offreely-available lexical resources perform as well or better than existingcommercial systems that rely on significant task-specific engineering. Theresults highlight the effectiveness of both neural embedding architectures anddefinition-based training for developing models that understand phrases andsentences.
arxiv-10800-83 | A New Vision of Collaborative Active Learning | http://arxiv.org/abs/1504.00284 | author:Adrian Calma, Tobias Reitmaier, Bernhard Sick, Paul Lukowicz, Mark Embrechts category:cs.LG stat.ML published:2015-04-01 summary:Active learning (AL) is a learning paradigm where an active learner has totrain a model (e.g., a classifier) which is in principal trained in asupervised way, but in AL it has to be done by means of a data set withinitially unlabeled samples. To get labels for these samples, the activelearner has to ask an oracle (e.g., a human expert) for labels. The goal is tomaximize the performance of the model and to minimize the number of queries atthe same time. In this article, we first briefly discuss the state of the artand own, preliminary work in the field of AL. Then, we propose the concept ofcollaborative active learning (CAL). With CAL, we will overcome some of theharsh limitations of current AL. In particular, we envision scenarios where anexpert may be wrong for various reasons, there might be several or even manyexperts with different expertise, the experts may label not only samples butalso knowledge at a higher level such as rules, and we consider that thelabeling costs depend on many conditions. Moreover, in a CAL process humanexperts will profit by improving their own knowledge, too.
arxiv-10800-84 | A Theory of Feature Learning | http://arxiv.org/abs/1504.00083 | author:Brendan van Rooyen, Robert C. Williamson category:stat.ML cs.LG published:2015-04-01 summary:Feature Learning aims to extract relevant information contained in data setsin an automated fashion. It is driving force behind the current deep learningtrend, a set of methods that have had widespread empirical success. What islacking is a theoretical understanding of different feature learning schemes.This work provides a theoretical framework for feature learning and thencharacterizes when features can be learnt in an unsupervised fashion. We alsoprovide means to judge the quality of features via rate-distortion theory andits generalizations.
arxiv-10800-85 | Bayesian model comparison with un-normalised likelihoods | http://arxiv.org/abs/1504.00298 | author:Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina Evdemon-Hogan category:stat.CO stat.ME stat.ML published:2015-04-01 summary:Models for which the likelihood function can be evaluated only up to aparameter-dependent unknown normalising constant, such as Markov random fieldmodels, are used widely in computer science, statistical physics, spatialstatistics, and network analysis. However, Bayesian analysis of these modelsusing standard Monte Carlo methods is not possible due to the intractability oftheir likelihood functions. Several methods that permit exact, or close toexact, simulation from the posterior distribution have recently been developed.However, estimating the evidence and Bayes' factors (BFs) for these modelsremains challenging in general. This paper describes new random weightimportance sampling and sequential Monte Carlo methods for estimating BFs thatuse simulation to circumvent the evaluation of the intractable likelihood, andcompares them to existing methods. In some cases we observe an advantage in theuse of biased weight estimates. An initial investigation into the theoreticaland empirical properties of this class of methods is presented. Some supportfor the use of biased estimates is presented, but we advocate caution in theuse of such estimates.
arxiv-10800-86 | Learning in the Presence of Corruption | http://arxiv.org/abs/1504.00091 | author:Brendan van Rooyen, Robert C. Williamson category:stat.ML cs.LG published:2015-04-01 summary:In supervised learning one wishes to identify a pattern present in a jointdistribution $P$, of instances, label pairs, by providing a function $f$ frominstances to labels that has low risk $\mathbb{E}_{P}\ell(y,f(x))$. To do so,the learner is given access to $n$ iid samples drawn from $P$. In many realworld problems clean samples are not available. Rather, the learner is givenaccess to samples from a corrupted distribution $\tilde{P}$ from which tolearn, while the goal of predicting the clean pattern remains. There are manydifferent types of corruption one can consider, and as of yet there is nogeneral means to compare the relative ease of learning under these differentcorruption processes. In this paper we develop a general framework for tacklingsuch problems as well as introducing upper and lower bounds on the risk forlearning in the presence of corruption. Our ultimate goal is to be able to makeinformed economic decisions in regards to the acquisition of data sets. For acertain subclass of corruption processes (those that are\emph{reconstructible}) we achieve this goal in a particular sense. Our lowerbounds are in terms of the coefficient of ergodicity, a simple to calculateproperty of stochastic matrices. Our upper bounds proceed via a generalizationof the method of unbiased estimators appearing in recent work of Natarajan etal and implicit in the earlier work of Kearns.
arxiv-10800-87 | Signatures of Infinity: Nonergodicity and Resource Scaling in Prediction, Complexity, and Learning | http://arxiv.org/abs/1504.00386 | author:James P. Crutchfield, Sarah Marzen category:cs.IT cs.LG math.IT stat.ML published:2015-04-01 summary:We introduce a simple analysis of the structural complexity ofinfinite-memory processes built from random samples of stationary, ergodicfinite-memory component processes. Such processes are familiar from the wellknown multi-arm Bandit problem. We contrast our analysis withcomputation-theoretic and statistical inference approaches to understandingtheir complexity. The result is an alternative view of the relationship betweenpredictability, complexity, and learning that highlights the distinct ways inwhich informational and correlational divergences arise in complex ergodic andnonergodic processes. We draw out consequences for the resource divergencesthat delineate the structural hierarchy of ergodic processes and for processesthat are themselves hierarchical.
arxiv-10800-88 | Microsoft COCO Captions: Data Collection and Evaluation Server | http://arxiv.org/abs/1504.00325 | author:Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, C. Lawrence Zitnick category:cs.CV cs.CL published:2015-04-01 summary:In this paper we describe the Microsoft COCO Caption dataset and evaluationserver. When completed, the dataset will contain over one and a half millioncaptions describing over 330,000 images. For the training and validationimages, five independent human generated captions will be provided. To ensureconsistency in evaluation of automatic caption generation algorithms, anevaluation server is used. The evaluation server receives candidate captionsand scores them using several popular metrics, including BLEU, METEOR, ROUGEand CIDEr. Instructions for using the evaluation server are provided.
arxiv-10800-89 | Bayesian Clustering of Shapes of Curves | http://arxiv.org/abs/1504.00377 | author:Zhengwu Zhang, Debdeep Pati, Anuj Srivastava category:stat.ML cs.LG published:2015-04-01 summary:Unsupervised clustering of curves according to their shapes is an importantproblem with broad scientific applications. The existing model-based clusteringtechniques either rely on simple probability models (e.g., Gaussian) that arenot generally valid for shape analysis or assume the number of clusters. Wedevelop an efficient Bayesian method to cluster curve data using an elasticshape metric that is based on joint registration and comparison of shapes ofcurves. The elastic-inner product matrix obtained from the data is modeledusing a Wishart distribution whose parameters are assigned carefully chosenprior distributions to allow for automatic inference on the number of clusters.Posterior is sampled through an efficient Markov chain Monte Carlo procedurebased on the Chinese restaurant process to infer (1) the posterior distributionon the number of clusters, and (2) clustering configuration of shapes. Thismethod is demonstrated on a variety of synthetic data and real data examples onprotein structure analysis, cell shape analysis in microscopy images, andclustering of shaped from MPEG7 database.
arxiv-10800-90 | The Libra Toolkit for Probabilistic Models | http://arxiv.org/abs/1504.00110 | author:Daniel Lowd, Amirmohammad Rooshenas category:cs.LG cs.AI published:2015-04-01 summary:The Libra Toolkit is a collection of algorithms for learning and inferencewith discrete probabilistic models, including Bayesian networks, Markovnetworks, dependency networks, and sum-product networks. Compared to othertoolkits, Libra places a greater emphasis on learning the structure oftractable models in which exact inference is efficient. It also includes avariety of algorithms for learning graphical models in which inference ispotentially intractable, and for performing exact and approximate inference.Libra is released under a 2-clause BSD license to encourage broad use inacademia and industry.
arxiv-10800-91 | A New Repair Operator for Multi-objective Evolutionary Algorithm in Constrained Optimization Problems | http://arxiv.org/abs/1504.00154 | author:Zhun Fan, Wenji Li, Xinye Cai, Huibiao Lin, Shuxiang Xie, Erik Goodman category:cs.NE 68Q01 G.1.6 published:2015-04-01 summary:In this paper, we design a set of multi-objective constrained optimizationproblems (MCOPs) and propose a new repair operator to address them. Theproposed repair operator is used to fix the solutions that violate the boxconstraints. More specifically, it employs a reversed correction strategy thatcan effectively avoid the population falling into local optimum. In addition,we integrate the proposed repair operator into two classical multi-objectiveevolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator iscompared with other two kinds of commonly used repair operators on benchmarkproblems CTPs and MCOPs. The experiment results demonstrate that our proposedapproach is very effective in terms of convergence and diversity.
arxiv-10800-92 | Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons | http://arxiv.org/abs/1504.00064 | author:James Y. Zou, Kamalika Chaudhuri, Adam Tauman Kalai category:stat.ML cs.LG published:2015-03-31 summary:We introduce an unsupervised approach to efficiently discover the underlyingfeatures in a data set via crowdsourcing. Our queries ask crowd members toarticulate a feature common to two out of three displayed examples. In additionwe also ask the crowd to provide binary labels to the remaining examples basedon the discovered features. The triples are chosen adaptively based on thelabels of the previously discovered features on the data set. In two naturalmodels of features, hierarchical and independent, we show that a simpleadaptive algorithm, using "two-out-of-three" similarity queries, recovers allfeatures with less labor than any nonadaptive algorithm. Experimental resultsvalidate the theoretical findings.
arxiv-10800-93 | On the Projective Geometry of Kalman Filter | http://arxiv.org/abs/1503.09113 | author:Francesca Paola Carli, Rodolphe Sepulchre category:math.OC stat.ML published:2015-03-31 summary:Convergence of the Kalman filter is best analyzed by studying the contractionof the Riccati map in the space of positive definite (covariance) matrices. Inthis paper, we explore how this contraction property relates to a morefundamental non-expansiveness property of filtering maps in the space ofprobability distributions endowed with the Hilbert metric. This is viewed as apreliminary step towards improving the convergence analysis of filteringalgorithms over general graphical models.
arxiv-10800-94 | Towards Using Machine Translation Techniques to Induce Multilingual Lexica of Discourse Markers | http://arxiv.org/abs/1503.09144 | author:António Lopes, David Martins de Matos, Vera Cabarrão, Ricardo Ribeiro, Helena Moniz, Isabel Trancoso, Ana Isabel Mata category:cs.CL I.2.7 published:2015-03-31 summary:Discourse markers are universal linguistic events subject to languagevariation. Although an extensive literature has already reported languagespecific traits of these events, little has been said on their cross-languagebehavior and on building an inventory of multilingual lexica of discoursemarkers. This work describes new methods and approaches for the description,classification, and annotation of discourse markers in the specific domain ofthe Europarl corpus. The study of discourse markers in the context oftranslation is crucial due to the idiomatic nature of these structures.Multilingual lexica together with the functional analysis of such structuresare useful tools for the hard task of translating discourse markers intopossible equivalents from one language to another. Using Daniel Marcu'svalidated discourse markers for English, extracted from the Brown Corpus, ourpurpose is to build multilingual lexica of discourse markers for otherlanguages, based on machine translation techniques. The major assumption inthis study is that the usage of a discourse marker is independent of thelanguage, i.e., the rhetorical function of a discourse marker in a sentence inone language is equivalent to the rhetorical function of the same discoursemarker in another language.
arxiv-10800-95 | Learning Definite Horn Formulas from Closure Queries | http://arxiv.org/abs/1503.09025 | author:Marta Arias, José L. Balcázar, Cristina Tîrnăucă category:cs.LG cs.LO published:2015-03-31 summary:A definite Horn theory is a set of n-dimensional Boolean vectors whosecharacteristic function is expressible as a definite Horn formula, that is, asconjunction of definite Horn clauses. The class of definite Horn theories isknown to be learnable under different query learning settings, such as learningfrom membership and equivalence queries or learning from entailment. We proposeyet a different type of query: the closure query. Closure queries are a naturalextension of membership queries and also a variant, appropriate in the contextof definite Horn formulas, of the so-called correction queries. We present analgorithm that learns conjunctions of definite Horn clauses in polynomial time,using closure and equivalence queries, and show how it relates to the canonicalGuigues-Duquenne basis for implicational systems. We also show how thedifferent query models mentioned relate to each other by either showingfull-fledged reductions by means of query simulation (where possible), or byshowing their connections in the context of particular algorithms that use themfor learning definite Horn formulas.
arxiv-10800-96 | End-To-End Memory Networks | http://arxiv.org/abs/1503.08895 | author:Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus category:cs.NE cs.CL published:2015-03-31 summary:We introduce a neural network with a recurrent attention model over apossibly large external memory. The architecture is a form of Memory Network(Weston et al., 2015) but unlike the model in that work, it is trainedend-to-end, and hence requires significantly less supervision during training,making it more generally applicable in realistic settings. It can also be seenas an extension of RNNsearch to the case where multiple computational steps(hops) are performed per output symbol. The flexibility of the model allows usto apply it to tasks as diverse as (synthetic) question answering and tolanguage modeling. For the former our approach is competitive with MemoryNetworks, but with less supervision. For the latter, on the Penn TreeBank andText8 datasets our approach demonstrates comparable performance to RNNs andLSTMs. In both cases we show that the key concept of multiple computationalhops yields improved results.
arxiv-10800-97 | Beyond Short Snippets: Deep Networks for Video Classification | http://arxiv.org/abs/1503.08909 | author:Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici category:cs.CV published:2015-03-31 summary:Convolutional neural networks (CNNs) have been extensively applied for imagerecognition problems giving state-of-the-art results on recognition, detection,segmentation and retrieval. In this work we propose and evaluate several deepneural network architectures to combine image information across a video overlonger time periods than previously attempted. We propose two methods capableof handling full length videos. The first method explores various convolutionaltemporal feature pooling architectures, examining the various design choiceswhich need to be made when adapting a CNN for this task. The second proposedmethod explicitly models the video as an ordered sequence of frames. For thispurpose we employ a recurrent neural network that uses Long Short-Term Memory(LSTM) cells which are connected to the output of the underlying CNN. Our bestnetworks exhibit significant performance improvements over previously publishedresults on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101datasets with (88.6% vs. 88.0%) and without additional optical flow information(82.6% vs. 72.8%).
arxiv-10800-98 | Multi-label Classification using Labels as Hidden Nodes | http://arxiv.org/abs/1503.09022 | author:Jesse Read, Jaakko Hollmén category:stat.ML cs.LG published:2015-03-31 summary:Competitive methods for multi-label classification typically invest inlearning labels together. To do so in a beneficial way, analysis of labeldependence is often seen as a fundamental step, separate and prior toconstructing a classifier. Some methods invest up to hundreds of times morecomputational effort in building dependency models, than training the finalclassifier itself. We extend some recent discussion in the literature andprovide a deeper analysis, namely, developing the view that label dependence isoften introduced by an inadequate base classifier, rather than being inherentto the data or underlying concept; showing how even an exhaustive analysis oflabel dependence may not lead to an optimal classification structure. Viewinglabels as additional features (a transformation of the input), we createneural-network inspired novel methods that remove the emphasis of a priordependency structure. Our methods have an important advantage particular tomulti-label data: they leverage labels to create effective units in middlelayers, rather than learning these units from scratch in an unsupervisedfashion with gradient-based methods. Results are promising. The methods wepropose perform competitively, and also have very important qualities ofscalability.
arxiv-10800-99 | Iterative Regularization for Learning with Convex Loss Functions | http://arxiv.org/abs/1503.08985 | author:Junhong Lin, Lorenzo Rosasco, Ding-Xuan Zhou category:stat.ML math.OC published:2015-03-31 summary:We consider the problem of supervised learning with convex loss functions andpropose a new form of iterative regularization based on the subgradient method.Unlike other regularization approaches, in iterative regularization noconstraint or penalization is considered, and generalization is achieved by(early) stopping an empirical iteration. We consider a nonparametric setting,in the framework of reproducing kernel Hilbert spaces, and prove finite samplebounds on the excess risk under general regularity conditions. Our studyprovides a new class of efficient regularized learning algorithms and givesinsights on the interplay between statistics and optimization in machinelearning.
arxiv-10800-100 | Weakly Supervised Learning of Objects, Attributes and their Associations | http://arxiv.org/abs/1504.00045 | author:Zhiyuan Shi, Yongxin Yang, Timothy M. Hospedales, Tao Xiang category:cs.CV published:2015-03-31 summary:When humans describe images they tend to use combinations of nouns andadjectives, corresponding to objects and their associated attributesrespectively. To generate such a description automatically, one needs to modelobjects, attributes and their associations. Conventional methods require strongannotation of object and attribute locations, making them less scalable. Inthis paper, we model object-attribute associations from weakly labelled images,such as those widely available on media sharing sites (e.g. Flickr), where onlyimage-level labels (either object or attributes) are given, without theirlocations and associations. This is achieved by introducing a novel weaklysupervised non-parametric Bayesian model. Once learned, given a new image, ourmodel can describe the image, including objects, attributes and theirassociations, as well as their locations and segmentation. Extensiveexperiments on benchmark datasets demonstrate that our weakly supervised modelperforms at par with strongly supervised models on tasks such as imagedescription and retrieval based on object-attribute associations.
arxiv-10800-101 | Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning | http://arxiv.org/abs/1503.09105v11.pdf | author:Prasenjit Karmakar, Shalabh Bhatnagar category:math.DS cs.AI stat.ML published:2015-03-31 summary:We present for the first time an asymptotic convergence analysis of twotime-scale stochastic approximation driven by `controlled' Markov noise. Inparticular, both the faster and slower recursions have non-additive controlledMarkov noise components in addition to martingale difference noise. We analyzethe asymptotic behavior of our framework by relating it to limitingdifferential inclusions in both time-scales that are defined in terms of theergodic occupation measures associated with the controlled Markov processes.Finally, we present a solution to the off-policy convergence problem fortemporal difference learning with linear function approximation, using ourresults.
arxiv-10800-102 | Real-World Font Recognition Using Deep Network and Domain Adaptation | http://arxiv.org/abs/1504.00028 | author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV cs.LG published:2015-03-31 summary:We address a challenging fine-grain classification problem: recognizing afont style from an image of text. In this task, it is very easy to generatelots of rendered font examples but very hard to obtain real-world labeledimages. This real-to-synthetic domain gap caused poor generalization to newreal data in previous methods (Chen et al. (2014)). In this paper, we refer toConvolutional Neural Networks, and use an adaptation technique based on aStacked Convolutional Auto-Encoder that exploits unlabeled real-world imagescombined with synthetic data. The proposed method achieves an accuracy ofhigher than 80% (top-5) on a real-world dataset.
arxiv-10800-103 | Generalized Categorization Axioms | http://arxiv.org/abs/1503.09082v11.pdf | author:Jian Yu category:cs.LG published:2015-03-31 summary:Categorization axioms have been proposed to axiomatizing clustering results,which offers a hint of bridging the difference between human recognition systemand machine learning through an intuitive observation: an object should beassigned to its most similar category. However, categorization axioms cannot begeneralized into a general machine learning system as categorization axiomsbecome trivial when the number of categories becomes one. In order togeneralize categorization axioms into general cases, categorization input andcategorization output are reinterpreted by inner and outer categoryrepresentation. According to the categorization reinterpretation, two categoryrepresentation axioms are presented. Category representation axioms andcategorization axioms can be combined into a generalized categorizationaxiomatic framework, which accurately delimit the theoretical categorizationconstraints and overcome the shortcoming of categorization axioms. The proposedaxiomatic framework not only discuses categorization test issue but alsoreinterprets many results in machine learning in a unified way, such asdimensionality reduction,density estimation, regression, clustering andclassification.
arxiv-10800-104 | Improved Error Bounds Based on Worst Likely Assignments | http://arxiv.org/abs/1504.00052 | author:Eric Bax category:stat.ML cs.IT cs.LG math.IT math.PR published:2015-03-31 summary:Error bounds based on worst likely assignments use permutation tests tovalidate classifiers. Worst likely assignments can produce effective boundseven for data sets with 100 or fewer training examples. This paper introduces astatistic for use in the permutation tests of worst likely assignments thatimproves error bounds, especially for accurate classifiers, which are typicallythe classifiers of interest.
arxiv-10800-105 | Encoding Spike Patterns in Multilayer Spiking Neural Networks | http://arxiv.org/abs/1503.09129 | author:Brian Gardner, Ioana Sporea, André Grüning category:cs.NE published:2015-03-31 summary:Information encoding in the nervous system is supported through the precisespike-timings of neurons; however, an understanding of the underlying processesby which such representations are formed in the first place remains unclear.Here we examine how networks of spiking neurons can learn to encode for inputpatterns using a fully temporal coding scheme. To this end, we introduce alearning rule for spiking networks containing hidden neurons which optimizesthe likelihood of generating desired output spiking patterns. We show theproposed learning rule allows for a large number of accurate input-output spikepattern mappings to be learnt, which outperforms other existing learning rulesfor spiking neural networks: both in the number of mappings that can be learntas well as the complexity of spike train encodings that can be utilised. Thelearning rule is successful even in the presence of input noise, isdemonstrated to solve the linearly non-separable XOR computation andgeneralizes well on an example dataset. We further present a biologicallyplausible implementation of backpropagated learning in multilayer spikingnetworks, and discuss the neural mechanisms that might underlie its function.Our approach contributes both to a systematic understanding of how patternencodings might take place in the nervous system, and a learning rule thatdisplays strong technical capability.
arxiv-10800-106 | Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process | http://arxiv.org/abs/1503.08535 | author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML cs.IR cs.LG published:2015-03-30 summary:Incorporating the side information of text corpus, i.e., authors, timestamps, and emotional tags, into the traditional text mining models has gainedsignificant interests in the area of information retrieval, statistical naturallanguage processing, and machine learning. One branch of these works is theso-called Author Topic Model (ATM), which incorporates the authors's interestsas side information into the classical topic model. However, the existing ATMneeds to predefine the number of topics, which is difficult and inappropriatein many real-world settings. In this paper, we propose an Infinite Author Topic(IAT) model to resolve this issue. Instead of assigning a discrete probabilityon fixed number of topics, we use a stochastic process to determine the numberof topics from the data itself. To be specific, we extend a gamma-negativebinomial process to three levels in order to capture theauthor-document-keyword hierarchical structure. Furthermore, each document isassigned a mixed gamma process that accounts for the multi-author'scontribution towards this document. An efficient Gibbs sampling inferencealgorithm with each conditional distribution being closed-form is developed forthe IAT model. Experiments on several real-world datasets show the capabilitiesof our IAT model to learn the hidden topics, authors' interests on these topicsand the number of topics simultaneously.
arxiv-10800-107 | Average Distance Queries through Weighted Samples in Graphs and Metric Spaces: High Scalability with Tight Statistical Guarantees | http://arxiv.org/abs/1503.08528 | author:Shiri Chechik, Edith Cohen, Haim Kaplan category:cs.SI cs.LG published:2015-03-30 summary:The average distance from a node to all other nodes in a graph, or from aquery point in a metric space to a set of points, is a fundamental quantity indata analysis. The inverse of the average distance, known as the (classic)closeness centrality of a node, is a popular importance measure in the study ofsocial networks. We develop novel structural insights on the sparsifiability ofthe distance relation via weighted sampling. Based on that, we present highlypractical algorithms with strong statistical guarantees for fundamentalproblems. We show that the average distance (and hence the centrality) for allnodes in a graph can be estimated using $O(\epsilon^{-2})$ single-sourcedistance computations. For a set $V$ of $n$ points in a metric space, we showthat after preprocessing which uses $O(n)$ distance computations we can computea weighted sample $S\subset V$ of size $O(\epsilon^{-2})$ such that the averagedistance from any query point $v$ to $V$ can be estimated from the distancesfrom $v$ to $S$. Finally, we show that for a set of points $V$ in a metricspace, we can estimate the average pairwise distance using $O(n+\epsilon^{-2})$distance computations. The estimate is based on a weighted sample of$O(\epsilon^{-2})$ pairs of points, which is computed using $O(n)$ distancecomputations. Our estimates are unbiased with normalized mean square error(NRMSE) of at most $\epsilon$. Increasing the sample size by a $O(\log n)$factor ensures that the probability that the relative error exceeds $\epsilon$is polynomially small.
arxiv-10800-108 | Sparse plus low-rank autoregressive identification in neuroimaging time series | http://arxiv.org/abs/1503.08639 | author:Raphaël Liégeois, Bamdev Mishra, Mattia Zorzi, Rodolphe Sepulchre category:cs.LG cs.SY published:2015-03-30 summary:This paper considers the problem of identifying multivariate autoregressive(AR) sparse plus low-rank graphical models. Based on the corresponding problemformulation recently presented, we use the alternating direction method ofmultipliers (ADMM) to efficiently solve it and scale it to sizes encountered inneuroimaging applications. We apply this decomposition on synthetic and realneuroimaging datasets with a specific focus on the information encoded in thelow-rank structure of our model. In particular, we illustrate that thisinformation captures the spatio-temporal structure of the original data,generalizing classical component analysis approaches.
arxiv-10800-109 | Label-Embedding for Image Classification | http://arxiv.org/abs/1503.08677 | author:Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid category:cs.CV published:2015-03-30 summary:Attributes act as intermediate representations that enable parameter sharingbetween classes, a must when training data is scarce. We propose to viewattribute-based image classification as a label-embedding problem: each classis embedded in the space of attribute vectors. We introduce a function thatmeasures the compatibility between an image and a label embedding. Theparameters of this function are learned on a training set of labeled samples toensure that, given an image, the correct classes rank higher than the incorrectones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasetsshow that the proposed framework outperforms the standard Direct AttributePrediction baseline in a zero-shot learning scenario. Label embedding enjoys abuilt-in ability to leverage alternative sources of information instead of orin addition to attributes, such as e.g. class hierarchies or textualdescriptions. Moreover, label embedding encompasses the whole range of learningsettings from zero-shot learning to regular learning with a large number oflabeled examples.
arxiv-10800-110 | Visual Saliency Based on Multiscale Deep Features | http://arxiv.org/abs/1503.08663 | author:Guanbin Li, Yizhou Yu category:cs.CV published:2015-03-30 summary:Visual saliency is a fundamental problem in both cognitive and computationalsciences, including computer vision. In this CVPR 2015 paper, we discover thata high-quality visual saliency model can be trained with multiscale featuresextracted using a popular deep learning architecture, convolutional neuralnetworks (CNNs), which have had many successes in visual recognition tasks. Forlearning such saliency models, we introduce a neural network architecture,which has fully connected layers on top of CNNs responsible for extractingfeatures at three different scales. We then propose a refinement method toenhance the spatial coherence of our saliency results. Finally, aggregatingmultiple saliency maps computed for different levels of image segmentation canfurther boost the performance, yielding saliency maps better than thosegenerated from a single segmentation. To promote further research andevaluation of visual saliency models, we also construct a new large database of4447 challenging images and their pixelwise saliency annotation. Experimentalresults demonstrate that our proposed method is capable of achievingstate-of-the-art performance on all public benchmarks, improving the F-Measureby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectivelyon these two datasets.
arxiv-10800-111 | Globally Tuned Cascade Pose Regression via Back Propagation with Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images | http://arxiv.org/abs/1503.08843 | author:Peng Sun, James K. Min, Guanglei Xiong category:cs.CV published:2015-03-30 summary:Recently, a successful pose estimation algorithm, called Cascade PoseRegression (CPR), was proposed in the literature. Trained over Pose IndexFeature, CPR is a regressor ensemble that is similar to Boosting. In this paperwe show how CPR can be represented as a Neural Network. Specifically, we adopta Graph Transformer Network (GTN) representation and accordingly train CPR withBack Propagation (BP) that permits globally tuning. In contrast, previous CPRliterature only took a layer wise training without any post fine tuning. Weempirically show that global training with BP outperforms layer-wise(pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor,which utilized sparse connection to learn local image feature representation.We tested the proposed CPR-GTN on 2D face pose estimation problem as inprevious CPR literature. Besides, we also investigated the possibility ofextending CPR-GTN to 3D pose estimation by doing experiments using 3D ComputedTomography dataset for heart segmentation.
arxiv-10800-112 | A Parzen-based distance between probability measures as an alternative of summary statistics in Approximate Bayesian Computation | http://arxiv.org/abs/1503.08727 | author:Carlos D. Zuluaga, Edgar A. Valencia, Mauricio A. Álvarez category:stat.ML published:2015-03-30 summary:Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlomethods. ABC methods use a comparison between simulated data, using differentparameters drew from a prior distribution, and observed data. This comparisonprocess is based on computing a distance between the summary statistics fromthe simulated data and the observed data. For complex models, it is usuallydifficult to define a methodology for choosing or constructing the summarystatistics. Recently, a nonparametric ABC has been proposed, that uses adissimilarity measure between discrete distributions based on empirical kernelembeddings as an alternative for summary statistics. The nonparametric ABCoutperforms other methods including ABC, kernel ABC or synthetic likelihoodABC. However, it assumes that the probability distributions are discrete, andit is not robust when dealing with few observations. In this paper, we proposeto apply kernel embeddings using an smoother density estimator or Parzenestimator for comparing the empirical data distributions, and computing the ABCposterior. Synthetic data and real data were used to test the Bayesianinference of our method. We compare our method with respect to state-of-the-artmethods, and demonstrate that our method is a robust estimator of the posteriordistribution in terms of the number of observations.
arxiv-10800-113 | Fast Optimal Transport Averaging of Neuroimaging Data | http://arxiv.org/abs/1503.08596 | author:Alexandre Gramfort, Gabriel Peyré, Marco Cuturi category:cs.CV published:2015-03-30 summary:Knowing how the Human brain is anatomically and functionally organized at thelevel of a group of healthy individuals or patients is the primary goal ofneuroimaging research. Yet computing an average of brain imaging data definedover a voxel grid or a triangulation remains a challenge. Data are large, thegeometry of the brain is complex and the between subjects variability leads tospatially or temporally non-overlapping effects of interest. To address theproblem of variability, data are commonly smoothed before group linearaveraging. In this work we build on ideas originally introduced by Kantorovichto propose a new algorithm that can average efficiently non-normalized datadefined over arbitrary discrete domains using transportation metrics. We showhow Kantorovich means can be linked to Wasserstein barycenters in order to takeadvantage of an entropic smoothing approach. It leads to a smooth convexoptimization problem and an algorithm with strong convergence guarantees. Weillustrate the versatility of this tool and its empirical behavior onfunctional neuroimaging data, functional MRI and magnetoencephalography (MEG)source estimates, defined on voxel grids and triangulations of the foldedcortical surface.
arxiv-10800-114 | LSHTC: A Benchmark for Large-Scale Text Classification | http://arxiv.org/abs/1503.08581 | author:Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, Patrick Galinari category:cs.IR cs.CL cs.LG published:2015-03-30 summary:LSHTC is a series of challenges which aims to assess the performance ofclassification systems in large-scale classification in a a large number ofclasses (up to hundreds of thousands). This paper describes the dataset thathave been released along the LSHTC series. The paper details the constructionof the datsets and the design of the tracks as well as the evaluation measuresthat we implemented and a quick overview of the results. All of these datasetsare available online and runs may still be submitted on the online server ofthe challenges.
arxiv-10800-115 | Comparison of Bayesian predictive methods for model selection | http://arxiv.org/abs/1503.08650 | author:Juho Piironen, Aki Vehtari category:stat.ME cs.LG published:2015-03-30 summary:The goal of this paper is to compare several widely used Bayesian modelselection methods in practical model selection problems, highlight theirdifferences and give recommendations about the preferred approaches. We focuson the variable subset selection for regression and classification and performseveral numerical experiments using both simulated and real world data. Theresults show that the optimization of a utility estimate such as thecross-validation (CV) score is liable to finding overfitted models due torelatively high variance in the utility estimates when the data is scarce. Thiscan also lead to substantial selection induced bias and optimism in theperformance evaluation for the selected model. From a predictive viewpoint,best results are obtained by accounting for model uncertainty by forming thefull encompassing model, such as the Bayesian model averaging solution over thecandidate models. If the encompassing model is too complex, it can be robustlysimplified by the projection method, in which the information of the full modelis projected onto the submodels. This approach is substantially less prone tooverfitting than selection based on CV-score. Overall, the projection methodappears to outperform also the maximum a posteriori model and the selection ofthe most probable variables. The study also demonstrates that the modelselection can greatly benefit from using cross-validation outside the searchingprocess both for guiding the model size selection and assessing the predictiveperformance of the finally selected model.
arxiv-10800-116 | Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations | http://arxiv.org/abs/1503.08853 | author:Ali Borji, James Tanner category:cs.CV published:2015-03-30 summary:Predicting where people look in natural scenes has attracted a lot ofinterest in computer vision and computational neuroscience over the past twodecades. Two seemingly contrasting categories of cues have been proposed toinfluence where people look: \textit{low-level image saliency} and\textit{high-level semantic information}. Our first contribution is to take adetailed look at these cues to confirm the hypothesis proposed byHenderson~\cite{henderson1993eye} and Nuthmann \&Henderson~\cite{nuthmann2010object} that observers tend to look at the centerof objects. We analyzed fixation data for scene free-viewing over 17 observerson 60 fully annotated images with various types of objects. Images containeddifferent types of scenes, such as natural scenes, line drawings, and 3Drendered scenes. Our second contribution is to propose a simple combined modelof low-level saliency and object center-bias that outperforms each individualcomponent significantly over our data, as well as on the OSIE dataset by Xu etal.~\cite{xu2014predicting}. The results reconcile saliency with objectcenter-bias hypotheses and highlight that both types of cues are important inguiding fixations. Our work opens new directions to understand strategies thathumans use in observing scenes and objects, and demonstrates the constructionof combined models of low-level saliency and high-level object-basedinformation.
arxiv-10800-117 | Decentralized learning for wireless communications and networking | http://arxiv.org/abs/1503.08855 | author:Georgios B. Giannakis, Qing Ling, Gonzalo Mateos, Ioannis D. Schizas, Hao Zhu category:math.OC cs.IT cs.LG cs.MA cs.SY math.IT stat.ML published:2015-03-30 summary:This chapter deals with decentralized learning algorithms for in-networkprocessing of graph-valued data. A generic learning problem is formulated andrecast into a separable form, which is iteratively minimized using thealternating-direction method of multipliers (ADMM) so as to gain the desireddegree of parallelization. Without exchanging elements from the distributedtraining sets and keeping inter-node communications at affordable levels, thelocal (per-node) learners consent to the desired quantity inferred globally,meaning the one obtained if the entire training data set were centrallyavailable. Impact of the decentralized learning framework to contemporarywireless communications and networking tasks is illustrated through casestudies including target tracking using wireless sensor networks, unveilingInternet traffic anomalies, power system state estimation, as well as spectrumcartography for wireless cognitive radio networks.
arxiv-10800-118 | Fast Label Embeddings for Extremely Large Output Spaces | http://arxiv.org/abs/1503.08873 | author:Paul Mineiro, Nikos Karampatziakis category:cs.LG published:2015-03-30 summary:Many modern multiclass and multilabel problems are characterized byincreasingly large output spaces. For these problems, label embeddings havebeen shown to be a useful primitive that can improve computational andstatistical efficiency. In this work we utilize a correspondence between rankconstrained estimation and low dimensional label embeddings that uncovers afast label embedding algorithm which works in both the multiclass andmultilabel settings. The result is a randomized algorithm for partial leastsquares, whose running time is exponentially faster than naive algorithms. Wedemonstrate our techniques on two large-scale public datasets, from the LargeScale Hierarchical Text Challenge and the Open Directory Project, where weobtain state of the art results.
arxiv-10800-119 | Nonparametric Relational Topic Models through Dependent Gamma Processes | http://arxiv.org/abs/1503.08542 | author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML cs.CL cs.IR cs.LG published:2015-03-30 summary:Traditional Relational Topic Models provide a way to discover the hiddentopics from a document network. Many theoretical and practical tasks, such asdimensional reduction, document clustering, link prediction, benefit from thisrevealed knowledge. However, existing relational topic models are based on anassumption that the number of hidden topics is known in advance, and this isimpractical in many real-world applications. Therefore, in order to relax thisassumption, we propose a nonparametric relational topic model in this paper.Instead of using fixed-dimensional probability distributions in its generativemodel, we use stochastic processes. Specifically, a gamma process is assignedto each document, which represents the topic interest of this document.Although this method provides an elegant solution, it brings additionalchallenges when mathematically modeling the inherent network structure oftypical document network, i.e., two spatially closer documents tend to havemore similar topics. Furthermore, we require that the topics are shared by allthe documents. In order to resolve these challenges, we use a subsamplingstrategy to assign each document a different gamma process from the globalgamma process, and the subsampling probabilities of documents are assigned witha Markov Random Field constraint that inherits the document network structure.Through the designed posterior inference algorithm, we can discover the hiddentopics and its number simultaneously. Experimental results on both syntheticand real-world network datasets demonstrate the capabilities of learning thehidden topics and, more importantly, the number of topics.
arxiv-10800-120 | Founding Digital Currency on Imprecise Commodity | http://arxiv.org/abs/1503.08818 | author:Zimu Yuan, Zhiwei Xu category:cs.CY cs.LG published:2015-03-29 summary:Current digital currency schemes provide instantaneous exchange on precisecommodity, in which "precise" means a buyer can possibly verify the function ofthe commodity without error. However, imprecise commodities, e.g. statisticaldata, with error existing are abundant in digital world. Existing digitalcurrency schemes do not offer a mechanism to help the buyer for paymentdecision on precision of commodity, which may lead the buyer to a dilemmabetween having to buy and being unconfident. In this paper, we design acurrency schemes IDCS for imprecise digital commodity. IDCS completes a tradein three stages of handshake between a buyer and providers. We present an IDCSprototype implementation that assigns weights on the trustworthy of theproviders, and calculates a confidence level for the buyer to decide thequality of a imprecise commodity. In experiment, we characterize theperformance of IDCS prototype under varying impact factors.
arxiv-10800-121 | Cross-validation of matching correlation analysis by resampling matching weights | http://arxiv.org/abs/1503.08471 | author:Hidetoshi Shimodaira category:stat.ML cs.LG published:2015-03-29 summary:The strength of association between a pair of data vectors is represented bya nonnegative real number, called matching weight. For dimensionalityreduction, we consider a linear transformation of data vectors, and define amatching error as the weighted sum of squared distances between transformedvectors with respect to the matching weights. Given data vectors and matchingweights, the optimal linear transformation minimizing the matching error issolved by the spectral graph embedding of Yan et al. (2007). This method is ageneralization of the canonical correlation analysis, and will be called asmatching correlation analysis (MCA). In this paper, we consider a novelsampling scheme where the observed matching weights are randomly sampled fromunderlying true matching weights with small probability, whereas the datavectors are treated as constants. We then investigate a cross-validation byresampling the matching weights. Our asymptotic theory shows that thecross-validation, if rescaled properly, computes an unbiased estimate of thematching error with respect to the true matching weights. Existing ideas ofcross-validation for resampling data vectors, instead of resampling matchingweights, are not applicable here. MCA can be used for data vectors frommultiple domains with different dimensions via an embarrassingly simple idea ofcoding the data vectors. This method will be called as cross-domain matchingcorrelation analysis (CDMCA), and an interesting connection to the classicalassociative memory model of neural networks is also discussed.
arxiv-10800-122 | Active Authentication on Mobile Devices via Stylometry, Application Usage, Web Browsing, and GPS Location | http://arxiv.org/abs/1503.08479 | author:Lex Fridman, Steven Weber, Rachel Greenstadt, Moshe Kam category:cs.CR stat.ML published:2015-03-29 summary:Active authentication is the problem of continuously verifying the identityof a person based on behavioral aspects of their interaction with a computingdevice. In this study, we collect and analyze behavioral biometrics data from200subjects, each using their personal Android mobile device for a period of atleast 30 days. This dataset is novel in the context of active authenticationdue to its size, duration, number of modalities, and absence of restrictions ontracked activity. The geographical colocation of the subjects in the study isrepresentative of a large closed-world environment such as an organizationwhere the unauthorized user of a device is likely to be an insider threat:coming from within the organization. We consider four biometric modalities: (1)text entered via soft keyboard, (2) applications used, (3) websites visited,and (4) physical location of the device as determined from GPS (when outdoors)or WiFi (when indoors). We implement and test a classifier for each modalityand organize the classifiers as a parallel binary decision fusion architecture.We are able to characterize the performance of the system with respect tointruder detection time and to quantify the contribution of each modality tothe overall performance.
arxiv-10800-123 | Global Bandits | http://arxiv.org/abs/1503.08370 | author:Onur Atan, Cem Tekin, Mihaela van der Schaar category:cs.LG published:2015-03-29 summary:Standard multi-armed bandits model decision problems in which theconsequences of each action choice are unknown and independent of each other.But in a wide variety of decision problems - from drug dosage to dynamicpricing - the consequences (rewards) of different actions are correlated, sothat selecting one action provides information about the consequences (rewards)of other actions as well. We propose and analyze a class of models of suchdecision problems; we call this class of models global bandits. When rewardsacross actions (arms) are sufficiently correlated we construct a greedy policythat achieves bounded regret, with a bound that depends on the true parametersof the problem. In the special case in which rewards of all arms aredeterministic functions of a single unknown parameter, we construct a (moresophisticated) greedy policy that achieves bounded regret, with a bound thatdepends on the single true parameter of the problem. For this special case wealso obtain a bound on regret that is independent of the true parameter; thisbound is sub-linear, with an exponent that depends on the informativeness ofthe arms (which measures the strength of correlation between arm rewards).
arxiv-10800-124 | Towards Shockingly Easy Structured Classification: A Search-based Probabilistic Online Learning Framework | http://arxiv.org/abs/1503.08381 | author:Xu Sun category:cs.LG cs.AI published:2015-03-29 summary:There are two major approaches for structured classification. One is theprobabilistic gradient-based methods such as conditional random fields (CRF),which has high accuracy but with drawbacks: slow training, and no support ofsearch-based optimization (which is important in many cases). The other one isthe search-based learning methods such as perceptrons and margin infusedrelaxed algorithm (MIRA), which have fast training but also with drawbacks: lowaccuracy, no probabilistic information, and non-convergence in real-worldtasks. We propose a novel and "shockingly easy" solution, a search-basedprobabilistic online learning method, to address most of those issues. Thismethod searches the output candidates, derives probabilities, and conductefficient online learning. We show that this method is with fast training,support search-based optimization, very easy to implement, with top accuracy,with probabilities, and with theoretical guarantees of convergence. Experimentson well-known tasks show that our method has better accuracy than CRF andalmost as fast training speed as perceptron and MIRA. Results also show thatSAPO can easily beat the state-of-the-art systems on those highly-competitivetasks, achieving record-breaking accuracies.
arxiv-10800-125 | Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition | http://arxiv.org/abs/1503.08395 | author:Shusen Wang, Zhihua Zhang, Tong Zhang category:cs.LG published:2015-03-29 summary:Symmetric positive semi-definite (SPSD) matrix approximation methods havebeen extensively used to speed up large-scale eigenvalue computation and kernellearning methods. The sketching based method, which we call the prototypemodel, produces relatively accurate approximations. The prototype model iscomputationally efficient on skinny matrices where one of the matrix dimensionsis relatively small, but it is inefficient on large square matrices. TheNystr\"om method is highly efficient on SPSD matrices, but can only achieve lowmatrix approximation accuracy. In this paper we propose novel model which we call the {\it faster SPSDmatrix approximation model}. The faster model is nearly as efficient as theNystr\"om method and as accurate as the prototype model. We show that thefaster model can potentially solve eigenvalue problems and kernel learningproblems in linear time with respect to the matrix size to achieve $1+\epsilon$relative-error, whereas the prototype model and the Nystr\"om method cost atleast quadratic time to attain comparable error bound. We also contribute newunderstandings of the Nystro\"om method. The Nystr\"om method is a specialinstance of our faster SPSD matrix approximation model, and it is approximationto the prototype model. Our technique can be straightforwardly applied to makethe CUR matrix decomposition more efficiently computed without much affectingthe accuracy. Empirical experiments demonstrate the effectiveness of theproposed methods.
arxiv-10800-126 | CRF Learning with CNN Features for Image Segmentation | http://arxiv.org/abs/1503.08263 | author:Fayao Liu, Guosheng Lin, Chunhua Shen category:cs.CV published:2015-03-28 summary:Conditional Random Rields (CRF) have been widely applied in imagesegmentations. While most studies rely on hand-crafted features, we herepropose to exploit a pre-trained large convolutional neural network (CNN) togenerate deep features for CRF learning. The deep CNN is trained on theImageNet dataset and transferred to image segmentations here for constructingpotentials of superpixels. Then the CRF parameters are learnt using astructured support vector machine (SSVM). To fully exploit context informationin inference, we construct spatially related co-occurrence pairwise potentialsand incorporate them into the energy function. This prefers labelling of objectpairs that frequently co-occur in a certain spatial layout and at the same timeavoids implausible labellings during the inference. Extensive experiments onbinary and multi-class segmentation benchmarks demonstrate the promise of theproposed method. We thus provide new baselines for the segmentation performanceon the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC2011 datasets.
arxiv-10800-127 | Active Model Aggregation via Stochastic Mirror Descent | http://arxiv.org/abs/1503.08363 | author:Ravi Ganti category:stat.ML cs.AI cs.LG published:2015-03-28 summary:We consider the problem of learning convex aggregation of models, that is asgood as the best convex aggregation, for the binary classification problem.Working in the stream based active learning setting, where the active learnerhas to make a decision on-the-fly, if it wants to query for the label of thepoint currently seen in the stream, we propose a stochastic-mirror descentalgorithm, called SMD-AMA, with entropy regularization. We establish an excessrisk bounds for the loss of the convex aggregate returned by SMD-AMA to be ofthe order of $O\left(\sqrt{\frac{\log(M)}{{T^{1-\mu}}}}\right)$, where $\mu\in[0,1)$ is an algorithm dependent parameter, that trades-off the number oflabels queried, and excess risk.
arxiv-10800-128 | Sparse Linear Regression With Missing Data | http://arxiv.org/abs/1503.08348 | author:Ravi Ganti, Rebecca M. Willett category:stat.ML cs.LG stat.ME published:2015-03-28 summary:This paper proposes a fast and accurate method for sparse regression in thepresence of missing data. The underlying statistical model encapsulates thelow-dimensional structure of the incomplete data matrix and the sparsity of theregression coefficients, and the proposed algorithm jointly learns thelow-dimensional structure of the data and a linear regressor with sparsecoefficients. The proposed stochastic optimization method, Sparse LinearRegression with Missing Data (SLRM), performs an alternating minimizationprocedure and scales well with the problem size. Large deviation inequalitiesshed light on the impact of the various problem-dependent parameters on theexpected squared loss of the learned regressor. Extensive simulations on bothsynthetic and real datasets show that SLRM performs better than competingalgorithms in a variety of contexts.
arxiv-10800-129 | Some Further Evidence about Magnification and Shape in Neural Gas | http://arxiv.org/abs/1503.08322 | author:Giacomo Parigi, Andrea Pedrini, Marco Piastra category:cs.NE published:2015-03-28 summary:Neural gas (NG) is a robust vector quantization algorithm with a well-knownmathematical model. According to this, the neural gas samples the underlyingdata distribution following a power law with a magnification exponent thatdepends on data dimensionality only. The effects of shape in the input datadistribution, however, are not entirely covered by the NG model above, due tothe technical difficulties involved. The experimental work described here showsthat shape is indeed relevant in determining the overall NG behavior; inparticular, some experiments reveal richer and complex behaviors induced byshape that cannot be explained by the power law alone. Although a morecomprehensive analytical model remains to be defined, the evidence collected inthese experiments suggests that the NG algorithm has an interesting potentialfor detecting complex shapes in noisy datasets.
arxiv-10800-130 | Robust Bayesian compressive sensing with data loss recovery for structural health monitoring signals | http://arxiv.org/abs/1503.08272 | author:Yong Huang, James L. Beck, Stephen Wu, Hui Li category:stat.AP stat.CO stat.ML published:2015-03-28 summary:The application of compressive sensing (CS) to structural health monitoringis an emerging research topic. The basic idea in CS is to use aspecially-designed wireless sensor to sample signals that are sparse in somebasis (e.g. wavelet basis) directly in a compressed form, and then toreconstruct (decompress) these signals accurately using some inversionalgorithm after transmission to a central processing unit. However, mostsignals in structural health monitoring are only approximately sparse, i.e.only a relatively small number of the signal coefficients in some basis aresignificant, but the other coefficients are usually not exactly zero. In thiscase, perfect reconstruction from compressed measurements is not expected. Anew Bayesian CS algorithm is proposed in which robust treatment of theuncertain parameters is explored, including integration over theprediction-error precision parameter to remove it as a "nuisance" parameter.The performance of the new CS algorithm is investigated using compressed datafrom accelerometers installed on a space-frame structure and on a cable-stayedbridge. Compared with other state-of-the-art CS methods including ourpreviously-published Bayesian method which uses MAP (maximum a posteriori)estimation of the prediction-error precision parameter, the new algorithm showssuperior performance in reconstruction robustness and posterior uncertaintyquantification. Furthermore, our method can be utilized for recovery of lostdata during wireless transmission, regardless of the level of sparseness in thesignal.
arxiv-10800-131 | A Multi-signal Variant for the GPU-based Parallelization of Growing Self-Organizing Networks | http://arxiv.org/abs/1503.08294 | author:Giacomo Parigi, Angelo Stramieri, Danilo Pau, Marco Piastra category:cs.DC cs.NE published:2015-03-28 summary:Among the many possible approaches for the parallelization of self-organizingnetworks, and in particular of growing self-organizing networks, perhaps themost common one is producing an optimized, parallel implementation of thestandard sequential algorithms reported in the literature. In this paper weexplore an alternative approach, based on a new algorithm variant specificallydesigned to match the features of the large-scale, fine-grained parallelism ofGPUs, in which multiple input signals are processed at once. Comparative testshave been performed, using both parallel and sequential implementations of thenew algorithm variant, in particular for a growing self-organizing network thatreconstructs surfaces from point clouds. The experimental results show thatthis approach allows harnessing in a more effective way the intrinsicparallelism that the self-organizing networks algorithms seem intuitively tosuggest, obtaining better performances even with networks of smaller size.
arxiv-10800-132 | Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement and Retrieval | http://arxiv.org/abs/1503.08248 | author:Xirong Li, Tiberio Uricchio, Lamberto Ballan, Marco Bertini, Cees G. M. Snoek, Alberto Del Bimbo category:cs.IR cs.CV cs.MM cs.SI H.3.1; H.3.3 published:2015-03-28 summary:Where previous reviews on content-based image retrieval emphasize on what canbe seen in an image to bridge the semantic gap, this survey considers whatpeople tag about an image. A comprehensive treatise of three closely linkedproblems, i.e., image tag assignment, refinement, and tag-based image retrievalis presented. While existing works vary in terms of their targeted tasks andmethodology, they rely on the key functionality of tag relevance, i.e.estimating the relevance of a specific tag with respect to the visual contentof a given image and its social context. By analyzing what information aspecific method exploits to construct its tag relevance function and how suchinformation is exploited, this paper introduces a taxonomy to structure thegrowing literature, understand the ingredients of the main works, clarify theirconnections and difference, and recognize their merits and limitations. For ahead-to-head comparison between the state-of-the-art, a new experimentalprotocol is presented, with training sets containing 10k, 100k and 1m imagesand an evaluation on three test sets, contributed by various research groups.Eleven representative works are implemented and evaluated. Putting all thistogether, the survey aims to provide an overview of the past and fosterprogress for the near future.
arxiv-10800-133 | Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm | http://arxiv.org/abs/1503.08329 | author:Pascal Germain, Alexandre Lacasse, François Laviolette, Mario Marchand, Jean-Francis Roy category:stat.ML cs.LG published:2015-03-28 summary:We propose an extensive analysis of the behavior of majority votes in binaryclassification. In particular, we introduce a risk bound for majority votes,called the C-bound, that takes into account the average quality of the votersand their average disagreement. We also propose an extensive PAC-Bayesiananalysis that shows how the C-bound can be estimated from various observationscontained in the training data. The analysis intends to be self-contained andcan be used as introductory material to PAC-Bayesian statistical learningtheory. It starts from a general PAC-Bayesian perspective and ends withuncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leiblerdivergence and others allow kernel functions to be used as voters (via thesample compression setting). Finally, out of the analysis, we propose the MinCqlearning algorithm that basically minimizes the C-bound. MinCq reduces to asimple quadratic program. Aside from being theoretically grounded, MinCqachieves state-of-the-art performance, as shown in our extensive empiricalcomparison with both AdaBoost and the Support Vector Machine.
arxiv-10800-134 | Stochastic Low-Rank Subspace Clustering by Auxiliary Variable Modeling | http://arxiv.org/abs/1503.08356 | author:Jie Shen, Ping Li, Huan Xu category:stat.ML published:2015-03-28 summary:Low-Rank Representation (LRR) has been a popular tool for identifying datagenerated from a union of subspaces. It is also known that LRR iscomputationally challenging. As the size of the nuclear norm regularized matrixof LRR is proportional to $n^2$ (where $n$ is the number of samples), itseriously hinders LRR for large scale problems. In this paper, we develop anovel algorithm to scale up the LRR method accurately and memory efficiently.In particular, we propose an online implementation of LRR that reduces thememory cost from $O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and$d$ being some estimated rank~($d < p \ll n$). Our proposed algorithm consists of two key technical components: (i) wereformulate the nuclear norm to an equivalent matrix factorization form, and(ii) we introduce an auxiliary variable which serves as a basis dictionary ofthe underlying data. Combing these two techniques makes the problem amenable tostochastic optimization. We establish the theoretical guarantee that thesequence of solutions produced by our algorithm converge to a stationary pointof the expected loss function asymptotically. Extensive experiments onsynthetic and realistic datasets further substantiate that our algorithm isfast, robust and memory efficient.
arxiv-10800-135 | A Variance Reduced Stochastic Newton Method | http://arxiv.org/abs/1503.08316 | author:Aurelien Lucchi, Brian McWilliams, Thomas Hofmann category:cs.LG published:2015-03-28 summary:Quasi-Newton methods are widely used in practise for convex loss minimizationproblems. These methods exhibit good empirical performance on a wide variety oftasks and enjoy super-linear convergence to the optimal solution. Forlarge-scale learning problems, stochastic Quasi-Newton methods have beenrecently proposed. However, these typically only achieve sub-linear convergencerates and have not been shown to consistently perform well in practice sincenoisy Hessian approximations can exacerbate the effect of high-variancestochastic gradient estimates. In this work we propose Vite, a novel stochasticQuasi-Newton algorithm that uses an existing first-order technique to reducethis variance. Without exploiting the specific form of the approximate Hessian,we show that Vite reaches the optimum at a geometric rate with a constantstep-size when dealing with smooth strongly convex functions. Empirically, wedemonstrate improvements over existing stochastic Quasi-Newton and variancereduced stochastic gradient methods.
arxiv-10800-136 | Normalization of Non-Standard Words in Croatian Texts | http://arxiv.org/abs/1503.08167 | author:Slobodan Beliga, Miran Pobar, Sanda Martinčić-Ipšić category:cs.CL published:2015-03-27 summary:This paper presents text normalization which is an integral part of anytext-to-speech synthesis system. Text normalization is a set of methods with atask to write non-standard words, like numbers, dates, times, abbreviations,acronyms and the most common symbols, in their full expanded form arepresented. The whole taxonomy for classification of non-standard words inCroatian language together with rule-based normalization methods combined witha lookup dictionary are proposed. Achieved token rate for normalization ofCroatian texts is 95%, where 80% of expanded words are in correct morphologicalform.
arxiv-10800-137 | Competitive Distribution Estimation | http://arxiv.org/abs/1503.07940 | author:Alon Orlitsky, Ananda Theertha Suresh category:cs.IT cs.DS cs.LG math.IT math.ST stat.TH published:2015-03-27 summary:Estimating an unknown distribution from its samples is a fundamental problemin statistics. The common, min-max, formulation of this goal considers theperformance of the best estimator over all distributions in a class. It showsthat with $n$ samples, distributions over $k$ symbols can be learned to a KLdivergence that decreases to zero with the sample size $n$, but growsunboundedly with the alphabet size $k$. Min-max performance can be viewed as regret relative to an oracle that knowsthe underlying distribution. We consider two natural and modest limits on theoracle's power. One where it knows the underlying distribution only up tosymbol permutations, and the other where it knows the exact distribution but isrestricted to use natural estimators that assign the same probability tosymbols that appeared equally many times in the sample. We show that in both cases the competitive regret reduces to$\min(k/n,\tilde{\mathcal{O}}(1/\sqrt n))$, a quantity upper bounded uniformlyfor every alphabet size. This shows that distributions can be estimated nearlyas well as when they are essentially known in advance, and nearly as well aswhen they are completely known in advance but need to be estimated via anatural estimator. We also provide an estimator that runs in linear time andincurs competitive regret of $\tilde{\mathcal{O}}(\min(k/n,1/\sqrt n))$, andshow that for natural estimators this competitive regret is inevitable. We alsodemonstrate the effectiveness of competitive estimators using simulations.
arxiv-10800-138 | Discriminative Bayesian Dictionary Learning for Classification | http://arxiv.org/abs/1503.07989 | author:Naveed Akhtar, Faisal Shafait, Ajmal Mian category:cs.CV cs.LG 68T10 published:2015-03-27 summary:We propose a Bayesian approach to learn discriminative dictionaries forsparse representation of data. The proposed approach infers probabilitydistributions over the atoms of a discriminative dictionary using a BetaProcess. It also computes sets of Bernoulli distributions that associate classlabels to the learned dictionary atoms. This association signifies theselection probabilities of the dictionary atoms in the expansion ofclass-specific data. Furthermore, the non-parametric character of the proposedapproach allows it to infer the correct size of the dictionary. We exploit theaforementioned Bernoulli distributions in separately learning a linearclassifier. The classifier uses the same hierarchical Bayesian model as thedictionary, which we present along the analytical inference solution for Gibbssampling. For classification, a test instance is first sparsely encoded overthe learned dictionary and the codes are fed to the classifier. We performedexperiments for face and action recognition; and object and scene-categoryclassification using five public datasets and compared the results withstate-of-the-art discriminative sparse representation approaches. Experimentsshow that the proposed Bayesian approach consistently outperforms the existingapproaches.
arxiv-10800-139 | Bayesian Cross Validation and WAIC for Predictive Prior Design in Regular Asymptotic Theory | http://arxiv.org/abs/1503.07970 | author:Sumio Watanabe category:cs.LG stat.ML published:2015-03-27 summary:Prior design is one of the most important problems in both statistics andmachine learning. The cross validation (CV) and the widely applicableinformation criterion (WAIC) are predictive measures of the Bayesianestimation, however, it has been difficult to apply them to find the optimalprior because their mathematical properties in prior evaluation have beenunknown and the region of the hyperparameters is too wide to be examined. Inthis paper, we derive a new formula by which the theoretical relation among CV,WAIC, and the generalization loss is clarified and the optimal hyperparametercan be directly found. By the formula, three facts are clarified about predictive prior design.Firstly, CV and WAIC have the same second order asymptotic expansion, hencethey are asymptotically equivalent to each other as the optimizer of thehyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makesthe average generalization loss to be minimized asymptotically but does not therandom generalization loss. And lastly, by using the mathematical relationbetween priors, the variances of the optimized hyperparameters by CV and WAICare made smaller with small computational costs. Also we show that theoptimized hyperparameter by DIC or the marginal likelihood does not minimizethe average or random generalization loss in general.
arxiv-10800-140 | Estimation of a common covariance matrix for multiple classes with applications in meta- and discriminant analysis | http://arxiv.org/abs/1503.07990 | author:Anders Ellern Bilgrau, Poul Svante Eriksen, Karen Dybkær, Martin Bøgsted category:stat.ML q-bio.GN stat.ME published:2015-03-27 summary:We propose a hierarchical random effects model for a common covariance matrixin cases where multiple classes are present. It is applicable where the classesare believed to share a common covariance matrix of interest obscured byclass-dependent noise. As such, it provides a basis for integrative ormeta-analysis of covariance matrices where the classes are formed by datasets.Our approach is inspired by traditional meta-analysis using random effectsmodels but the model is also shown to be applicable as an intermediate betweenlinear and quadratic discriminant analysis. We derive basic properties andestimators of the model and compare their properties. Simple inference andinterpretation of the introduced parameter measuring the inter-classhomogeneity is suggested.
arxiv-10800-141 | Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings | http://arxiv.org/abs/1503.08195 | author:Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Krüger category:math.ST stat.ME stat.ML stat.TH published:2015-03-27 summary:In the practice of point prediction, it is desirable that forecasters receivea directive in the form of a statistical functional, such as the mean or aquantile of the predictive distribution. When evaluating and comparingcompeting forecasts, it is then critical that the scoring function used forthese purposes be consistent for the functional at hand, in the sense that theexpected score is minimized when following the directive. We show that any scoring function that is consistent for a quantile or anexpectile functional, respectively, can be represented as a mixture of extremalscoring functions that form a linearly parameterized family. Scoring functionsfor the mean value and probability forecasts of binary events constituteimportant examples. The quantile and expectile functionals along with therespective extremal scoring functions admit appealing economic interpretationsin terms of thresholds in decision making. The Choquet type mixture representations give rise to simple checks ofwhether a forecast dominates another in the sense that it is preferable underany consistent scoring function. In empirical settings it suffices to comparethe average scores for only a finite number of extremal elements. Plots of theaverage scores with respect to the extremal scoring functions, which we callMurphy diagrams, permit detailed comparisons of the relative merits ofcompeting forecasts.
arxiv-10800-142 | Real-time multi-view deconvolution | http://arxiv.org/abs/1503.07998 | author:Benjamin Schmid, Jan Huisken category:q-bio.QM cs.CV published:2015-03-27 summary:In light-sheet microscopy, overall image content and resolution are improvedby acquiring and fusing multiple views of the sample from different directions.State-of-the-art multi-view (MV) deconvolution employs the point spreadfunctions (PSF) of the different views to simultaneously fuse and deconvolvethe images in 3D, but processing takes a multiple of the acquisition time andconstitutes the bottleneck in the imaging pipeline. Here we show that MVdeconvolution in 3D can finally be achieved in real-time by reslicing theacquired data and processing cross-sectional planes individually on themassively parallel architecture of a graphics processing unit (GPU).
arxiv-10800-143 | Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories | http://arxiv.org/abs/1503.08155 | author:Miao Fan, Qiang Zhou, Thomas Fang Zheng category:cs.AI cs.CL published:2015-03-27 summary:This paper considers the problem of knowledge inference on large-scaleimperfect repositories with incomplete coverage by means of embedding entitiesand relations at the first attempt. We propose IIKE (Imperfect and IncompleteKnowledge Embedding), a probabilistic model which measures the probability ofeach belief, i.e. $\langle h,r,t\rangle$, in large-scale knowledge bases suchas NELL and Freebase, and our objective is to learn a better low-dimensionalvector representation for each entity ($h$ and $t$) and relation ($r$) in theprocess of minimizing the loss of fitting the corresponding confidence given bymachine learning (NELL) or crowdsouring (Freebase), so that we can use ${\bfh} + {\bf r} - {\bf t}$ to assess the plausibility of a belief whenconducting inference. We use subsets of those inexact knowledge bases to trainour model and test the performances of link prediction and tripletclassification on ground truth beliefs, respectively. The results of extensiveexperiments show that IIKE achieves significant improvement compared with thebaseline and state-of-the-art approaches.
arxiv-10800-144 | RankMap: A Platform-Aware Framework for Distributed Learning from Dense Datasets | http://arxiv.org/abs/1503.08169 | author:Azalia Mirhoseini, Eva. L. Dyer, Ebrahim. M. Songhori, Richard Baraniuk, Farinaz Koushanfar category:cs.DC cs.LG published:2015-03-27 summary:This paper introduces RankMap, a platform-aware end-to-end framework forefficient execution of a broad class of iterative learning algorithms formassive and dense datasets. In contrast to the existing dense (iterative) dataanalysis methods that are oblivious to the platform, for the first time, weintroduce novel scalable data transformation and mapping algorithms that enableoptimizing for the underlying computing platforms' cost/constraints. The costis defined by the number of arithmetic and (within-platform) message passingoperations incurred by the variable updates in each iteration, while theconstraints are set by the available memory resources. RankMap's transformationscalably factorizes data into an ensemble of lower dimensional subspaces, whileits mapping schedules the flow of iterative computation on the transformed dataonto the pertinent computing machine. We show a trade-off between the desiredlevel of accuracy for the learning algorithm and the achieved efficiency.RankMap provides two APIs, one matrix-based and one graph-based, whichfacilitate automated adoption of the framework for performing severalcontemporary iterative learning applications optimized to the platform. Todemonstrate the utility of RankMap, we solve sparse recovery and poweriteration problems on various real-world datasets with up to 1.8 billionnon-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlexplatforms using up to 244 cores. The results demonstrate up to 2 orders ofmagnitude improvements in memory usage, execution speed, and bandwidth comparedwith the best reported prior work.
arxiv-10800-145 | A System View of the Recognition and Interpretation of Observed Human Shape, Pose and Action | http://arxiv.org/abs/1503.08223 | author:David W. Arathorn category:cs.CV published:2015-03-27 summary:There is physiological evidence that our ability to interpret human pose andaction from 2D visual imagery (binocular or monocular) engages the circuitry ofthe motor cortices as well as the visual areas of the brain. This implies thatthe capability of the motor cortices to solve inverse kinematics is flexibleenough to apply to both motion planning as well as serving as a generativemodel for the visual processing of human figures, despite the differingfunctional requirements of the two tasks. This paper provides a computationalmodel of the cooperation between visual and motor areas: in other words, asystem view of an important class of brain computations. The model unifies thesolution of the separate inverse problems involved in the task, visualtransformation discovery, inverse kinematics, and adaptation to morphologyvariations, using several instances of the Map-seeking Circuit algorithm. Whilethe paper is weighted toward the exposition of a neurobiological hypothesis,from mathematical formalization of the problem to neuronal circuitry, thealgorithmic expression of the solution is also a functional machine visionsystem for human figure recognition, and 3D pose and body morphologyreconstruction from monocular, perspective-less input imagery. With an inversekinematic generative model capable of imposing a variety of endogenous andexogenous constraints the machine vision implementation acquirescharacteristics currently unique among such systems.
arxiv-10800-146 | Likelihood-free Model Choice | http://arxiv.org/abs/1503.07689 | author:Jean-Michel Marin, Pierre Pudlo, Christian P. Robert category:stat.ME stat.CO stat.ML published:2015-03-26 summary:This document is an invited chapter covering the specificities of ABC modelchoice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont(2015). Beyond exposing the potential pitfalls of ABC based posteriorprobabilities, the review emphasizes mostly the solution proposed by Pudlo etal. (2014) on the use of random forests for aggregating summary statistics andand for estimating the posterior probability of the most likely model via asecondary random fores.
arxiv-10800-147 | Breaking the News: First Impressions Matter on Online News | http://arxiv.org/abs/1503.07921 | author:Julio Reis, Fabrıcio Benevenuto, Pedro O. S. Vaz de Melo, Raquel Prates, Haewoon Kwak, Jisun An category:cs.CY cs.CL published:2015-03-26 summary:A growing number of people are changing the way they consume news, replacingthe traditional physical newspapers and magazines by their virtual onlineversions or/and weblogs. The interactivity and immediacy present in online newsare changing the way news are being produced and exposed by media corporations.News websites have to create effective strategies to catch people's attentionand attract their clicks. In this paper we investigate possible strategies usedby online news corporations in the design of their news headlines. We analyzethe content of 69,907 headlines produced by four major global mediacorporations during a minimum of eight consecutive months in 2014. In order todiscover strategies that could be used to attract clicks, we extracted featuresfrom the text of the news headlines related to the sentiment polarity of theheadline. We discovered that the sentiment of the headline is strongly relatedto the popularity of the news and also with the dynamics of the posted commentson that particular news.
arxiv-10800-148 | Transductive Multi-class and Multi-label Zero-shot Learning | http://arxiv.org/abs/1503.07884 | author:Yanwei Fu, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Shaogang Gong category:cs.LG cs.CV published:2015-03-26 summary:Recently, zero-shot learning (ZSL) has received increasing interest. The keyidea underpinning existing ZSL approaches is to exploit knowledge transfer viaan intermediate-level semantic representation which is assumed to be sharedbetween the auxiliary and target datasets, and is used to bridge between thesedomains for knowledge transfer. The semantic representation used in existingapproaches varies from visual attributes to semantic word vectors and semanticrelatedness. However, the overall pipeline is similar: a projection mappinglow-level features to the semantic representation is learned from the auxiliarydataset by either classification or regression models and applied directly tomap each instance into the same semantic representation space where a zero-shotclassifier is used to recognise the unseen target class instances with a singleknown 'prototype' of each target class. In this paper we discuss two relatedlines of work improving the conventional approach: exploiting transductivelearning ZSL, and generalising ZSL to the multi-label case.
arxiv-10800-149 | Generalized K-fan Multimodal Deep Model with Shared Representations | http://arxiv.org/abs/1503.07906 | author:Gang Chen, Sargur N. Srihari category:cs.LG stat.ML 68T10 I.2.6 published:2015-03-26 summary:Multimodal learning with deep Boltzmann machines (DBMs) is an generativeapproach to fuse multimodal inputs, and can learn the shared representation viaContrastive Divergence (CD) for classification and information retrieval tasks.However, it is a 2-fan DBM model, and cannot effectively handle multipleprediction tasks. Moreover, this model cannot recover the hiddenrepresentations well by sampling from the conditional distribution when morethan one modalities are missing. In this paper, we propose a K-fan deepstructure model, which can handle the multi-input and muti-output learningproblems effectively. In particular, the deep structure has K-branch fordifferent inputs where each branch can be composed of a multi-layer deep model,and a shared representation is learned in an discriminative manner to tacklemultimodal tasks. Given the deep structure, we propose two objective functionsto handle two multi-input and multi-output tasks: joint visual restoration andlabeling, and the multi-view multi-calss object recognition tasks. To estimatethe model parameters, we initialize the deep model parameters with CD tomaximize the joint distribution, and then we use backpropagation to update themodel according to specific objective function. The experimental resultsdemonstrate that the model can effectively leverages multi-source informationand predict multiple tasks well over competitive baselines.
arxiv-10800-150 | Gibbs Sampling with Low-Power Spiking Digital Neurons | http://arxiv.org/abs/1503.07793 | author:Srinjoy Das, Bruno Umbria Pedroni, Paul Merolla, John Arthur, Andrew S. Cassidy, Bryan L. Jackson, Dharmendra Modha, Gert Cauwenberghs, Ken Kreutz-Delgado category:cs.NE published:2015-03-26 summary:Restricted Boltzmann Machines and Deep Belief Networks have been successfullyused in a wide variety of applications including image classification andspeech recognition. Inference and learning in these algorithms uses a MarkovChain Monte Carlo procedure called Gibbs sampling. A sigmoidal function formsthe kernel of this sampler which can be realized from the firing statistics ofnoisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paperdemonstrates such an implementation on an array of digital spiking neurons withstochastic leak and threshold properties for inference tasks and presents somekey performance metrics for such a hardware-based sampler in both thegenerative and discriminative contexts.
arxiv-10800-151 | An Evolutionary Algorithm for Error-Driven Learning via Reinforcement | http://arxiv.org/abs/1503.07609 | author:Yanping Liu, Erik D. Reichle category:cs.AI cs.NE published:2015-03-26 summary:Although different learning systems are coordinated to afford complexbehavior, little is known about how this occurs. This article describes atheoretical framework that specifies how complex behaviors that might bethought to require error-driven learning might instead be acquired throughsimple reinforcement. This framework includes specific assumptions about themechanisms that contribute to the evolution of (artificial) neural networks togenerate topologies that allow the networks to learn large-scale complexproblems using only information about the quality of their performance. Thepractical and theoretical implications of the framework are discussed, as arepossible biological analogs of the approach.
arxiv-10800-152 | Unsupervised authorship attribution | http://arxiv.org/abs/1503.07613 | author:David Fifield, Torbjørn Follan, Emil Lunde category:cs.CL published:2015-03-26 summary:We describe a technique for attributing parts of a written text to a set ofunknown authors. Nothing is assumed to be known a priori about the writingstyles of potential authors. We use multiple independent clusterings of aninput text to identify parts that are similar and dissimilar to one another. Wedescribe algorithms necessary to combine the multiple clusterings into ameaningful output. We show results of the application of the technique on textshaving multiple writing styles.
arxiv-10800-153 | Multi-Labeled Classification of Demographic Attributes of Patients: a case study of diabetics patients | http://arxiv.org/abs/1503.07795 | author:Naveen Kumar Parachur Cotha, Marina Sokolova category:cs.LG published:2015-03-26 summary:Automated learning of patients demographics can be seen as multi-labelproblem where a patient model is based on different race and gender groups. Theresulting model can be further integrated into Privacy-Preserving Data Mining,where it can be used to assess risk of identification of different patientgroups. Our project considers relations between diabetes and demographics ofpatients as a multi-labelled problem. Most research in this area has been doneas binary classification, where the target class is finding if a person hasdiabetes or not. But very few, and maybe no work has been done in multi-labeledanalysis of the demographics of patients who are likely to be diagnosed withdiabetes. To identify such groups, we applied ensembles of several multi-labellearning algorithms.
arxiv-10800-154 | Robust Eye Centers Localization with Zero--Crossing Encoded Image Projections | http://arxiv.org/abs/1503.07697 | author:Laura Florea, Corneliu Florea, Constantin Vertan category:cs.CV published:2015-03-26 summary:This paper proposes a new framework for the eye centers localization by thejoint use of encoding of normalized image projections and a Multi LayerPerceptron (MLP) classifier. The encoding is novel and it consists inidentifying the zero-crossings and extracting the relevant parameters from theresulting modes. The compressed normalized projections produce featuredescriptors that are inputs to a properly-trained MLP, for discriminating amongvarious categories of image regions. The proposed framework forms a fast andreliable system for the eye centers localization, especially in the context offace expression analysis in unconstrained environments. We successfully testthe proposed method on a wide variety of databases including BioID,Cohn-Kanade, Extended Yale B and Labelled Faces in the Wild (LFW) databases.
arxiv-10800-155 | Pain Intensity Estimation by a Self--Taught Selection of Histograms of Topographical Features | http://arxiv.org/abs/1503.07706 | author:Corneliu Florea, Laura Florea, Raluca Boia, Alessandra Bandrabur, Constantin Vertan category:cs.CV published:2015-03-26 summary:Pain assessment through observational pain scales is necessary for specialcategories of patients such as neonates, patients with dementia, critically illpatients, etc. The recently introduced Prkachin-Solomon score allows painassessment directly from facial images opening the path for multiple assistiveapplications. In this paper, we introduce the Histograms of Topographical (HoT)features, which are a generalization of the topographical primal sketch, forthe description of the face parts contributing to the mentioned score. Wepropose a semi-supervised, clustering oriented self--taught learning proceduredeveloped on the emotion oriented Cohn-Kanade database. We use this procedureto improve the discrimination between different pain intensity levels and thegeneralization with respect to the monitored persons, while testing on the UNBCMcMaster Shoulder Pain database.
arxiv-10800-156 | Towards Learning free Naive Bayes Nearest Neighbor-based Domain Adaptation | http://arxiv.org/abs/1503.07783 | author:Faraz Saeedan, Barbara Caputo category:cs.CV published:2015-03-26 summary:As of today, object categorization algorithms are not able to achieve thelevel of robustness and generality necessary to work reliably in the realworld. Even the most powerful convolutional neural network we can train failsto perform satisfactorily when trained and tested on data from differentdatabases. This issue, known as domain adaptation and/or dataset bias in theliterature, is due to a distribution mismatch between data collections. Methodsaddressing it go from max-margin classifiers to learning how to modify thefeatures and obtain a more robust representation. Recent work showed that bycasting the problem into the image-to-class recognition framework, the domainadaptation problem is significantly alleviated \cite{danbnn}. Here we followthis approach, and show how a very simple, learning free Naive Bayes NearestNeighbor (NBNN)-based domain adaptation algorithm can significantly alleviatethe distribution mismatch among source and target data, especially when thenumber of classes and the number of sources grow. Experiments on standardbenchmarks used in the literature show that our approach (a) is competitivewith the current state of the art on small scale problems, and (b) achieves thecurrent state of the art as the number of classes and sources grows, withminimal computational requirements.
arxiv-10800-157 | Transductive Multi-label Zero-shot Learning | http://arxiv.org/abs/1503.07790 | author:Yanwei Fu, Yongxin Yang, Tim Hospedales, Tao Xiang, Shaogang Gong category:cs.LG cs.CV published:2015-03-26 summary:Zero-shot learning has received increasing interest as a means to alleviatethe often prohibitive expense of annotating training data for large scalerecognition problems. These methods have achieved great success via learningintermediate semantic representations in the form of attributes and morerecently, semantic word vectors. However, they have thus far been constrainedto the single-label case, in contrast to the growing popularity and importanceof more realistic multi-label data. In this paper, for the first time, weinvestigate and formalise a general framework for multi-label zero-shotlearning, addressing the unique challenge therein: how to exploit multi-labelcorrelation at test time with no training data for those classes? Inparticular, we propose (1) a multi-output deep regression model to project animage into a semantic word space, which explicitly exploits the correlations inthe intermediate semantic layer of word vectors; (2) a novel zero-shot learningalgorithm for multi-label data that exploits the unique compositionalityproperty of semantic word vector representations; and (3) a transductivelearning strategy to enable the regression model learned from seen classes togeneralise well to unseen classes. Our zero-shot learning experiments on anumber of standard multi-label datasets demonstrate that our method outperformsa variety of baselines.
arxiv-10800-158 | Interpretable Classification Models for Recidivism Prediction | http://arxiv.org/abs/1503.07810 | author:Jiaming Zeng, Berk Ustun, Cynthia Rudin category:stat.ML stat.AP published:2015-03-26 summary:We investigate a long-debated question, which is how to create predictivemodels of recidivism that are sufficiently accurate, transparent, andinterpretable to use for decision-making. This question is complicated as thesemodels are used to support different decisions, from sentencing, to determiningrelease on probation, to allocating preventative social services. Each use casemight have an objective other than classification accuracy, such as a desiredtrue positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair isa point on the receiver operator characteristic (ROC) curve. We use popularmachine learning methods to create models along the full ROC curve on a widerange of recidivism prediction problems. We show that many methods (SVM, RidgeRegression) produce equally accurate models along the full ROC curve. However,methods that designed for interpretability (CART, C5.0) cannot be tuned toproduce models that are accurate and/or interpretable. To handle thisshortcoming, we use a new method known as SLIM (Supersparse Linear IntegerModels) to produce accurate, transparent, and interpretable models along thefull ROC curve. These models can be used for decision-making for many differentuse cases, since they are just as accurate as the most powerful black-boxmachine learning models, but completely transparent, and highly interpretable.
arxiv-10800-159 | Using Latent Semantic Analysis to Identify Quality in Use (QU) Indicators from User Reviews | http://arxiv.org/abs/1503.07294 | author:Wendy Tan Wei Syn, Bong Chih How, Issa Atoum category:cs.CL cs.AI cs.IR published:2015-03-25 summary:The paper describes a novel approach to categorize users' reviews accordingto the three Quality in Use (QU) indicators defined in ISO: effectiveness,efficiency and freedom from risk. With the tremendous amount of reviewspublished each day, there is a need to automatically summarize user reviews toinform us if any of the software able to meet requirement of a companyaccording to the quality requirements. We implemented the method of LatentSemantic Analysis (LSA) and its subspace to predict QU indicators. We build areduced dimensionality universal semantic space from Information Systemjournals and Amazon reviews. Next, we projected set of indicators' measurementscales into the universal semantic space and represent them as subspace. In thesubspace, we can map similar measurement scales to the unseen reviews andpredict the QU indicators. Our preliminary study able to obtain the average ofF-measure, 0.3627.
arxiv-10800-160 | A Brief Survey of Recent Edge-Preserving Smoothing Algorithms on Digital Images | http://arxiv.org/abs/1503.07297 | author:Chandrajit Pal, Amlan Chakrabarti, Ranjan Ghosh category:cs.CV published:2015-03-25 summary:Edge preserving filters preserve the edges and its information while blurringan image. In other words they are used to smooth an image, while reducing theedge blurring effects across the edge like halos, phantom etc. They arenonlinear in nature. Examples are bilateral filter, anisotropic diffusionfilter, guided filter, trilateral filter etc. Hence these family of filters arevery useful in reducing the noise in an image making it very demanding incomputer vision and computational photography applications like denoising,video abstraction, demosaicing, optical-flow estimation, stereo matching, tonemapping, style transfer, relighting etc. This paper provides a concreteintroduction to edge preserving filters starting from the heat diffusionequation in olden to recent eras, an overview of its numerous applications, aswell as mathematical analysis, various efficient and optimized ways ofimplementation and their interrelationships, keeping focus on preserving theboundaries, spikes and canyons in presence of noise. Furthermore it provides arealistic notion for efficient implementation with a research scope forhardware realization for further acceleration.
arxiv-10800-161 | Quantized Nonparametric Estimation | http://arxiv.org/abs/1503.07368 | author:Yuancheng Zhu, John Lafferty category:math.ST stat.ML stat.TH published:2015-03-25 summary:We present an extension to Pinsker's theorem for nonparametric estimationover Sobolev ellipsoids when estimation is carried out under storage orcommunication constraints. Placing limits on the number of bits used to encodeany estimator, we give tight lower and upper bounds on the excess risk due toquantization in terms of the number of bits, the signal size, and the noiselevel. This establishes the Pareto optimal minimax tradeoff between storage andrisk under quantization constraints for Sobolev spaces. Our results and prooftechniques combine elements of rate distortion theory and minimax analysis.
arxiv-10800-162 | Compressed sensing MRI using masked DCT and DFT measurements | http://arxiv.org/abs/1503.07384 | author:Elma Hot, Petar Sekulić category:cs.CV published:2015-03-25 summary:This paper presents modification of the TwIST algorithm for CompressiveSensing MRI images reconstruction. Compressive Sensing is new approach insignal processing whose basic idea is recovering signal form small set ofavailable samples. The application of the Compressive Sensing in biomedicalimaging has found great importance. It allows significant lowering of theacquisition time, and therefore, save the patient from the negative impact ofthe MR apparatus. TwIST is commonly used algorithm for 2D signalsreconstruction using Compressive Sensing principle. It is based on the TotalVariation minimization. Standard version of the TwIST uses masked 2D DiscreteFourier Transform coefficients as Compressive Sensing measurements. In thispaper, different masks and different transformation domains for coefficientsselection are tested. Certain percent of the measurements is used from themask, as well as small number of coefficients outside the mask. Comparativeanalysis using 2D DFT and 2D DCT coefficients, with different mask shapes isperformed. The theory is proved with experimental results.
arxiv-10800-163 | A Survey of Classification Techniques in the Area of Big Data | http://arxiv.org/abs/1503.07477 | author:Praful Koturwar, Sheetal Girase, Debajyoti Mukhopadhyay category:cs.LG published:2015-03-25 summary:Big Data concern large-volume, growing data sets that are complex and havemultiple autonomous sources. Earlier technologies were not able to handlestorage and processing of huge data thus Big Data concept comes into existence.This is a tedious job for users unstructured data. So, there should be somemechanism which classify unstructured data into organized form which helps userto easily access required data. Classification techniques over bigtransactional database provide required data to the users from large datasetsmore simple way. There are two main classification techniques, supervised andunsupervised. In this paper we focused on to study of different supervisedclassification techniques. Further this paper shows a advantages andlimitations.
arxiv-10800-164 | Stable Feature Selection from Brain sMRI | http://arxiv.org/abs/1503.07508 | author:Bo Xin, Lingjing Hu, Yizhou Wang, Wen Gao category:cs.LG stat.ML published:2015-03-25 summary:Neuroimage analysis usually involves learning thousands or even millions ofvariables using only a limited number of samples. In this regard, sparsemodels, e.g. the lasso, are applied to select the optimal features and achievehigh diagnosis accuracy. The lasso, however, usually results in independentunstable features. Stability, a manifest of reproducibility of statisticalresults subject to reasonable perturbations to data and the model, is animportant focus in statistics, especially in the analysis of high dimensionaldata. In this paper, we explore a nonnegative generalized fused lasso model forstable feature selection in the diagnosis of Alzheimer's disease. In additionto sparsity, our model incorporates two important pathological priors: thespatial cohesion of lesion voxels and the positive correlation between thefeatures and the disease labels. To optimize the model, we propose an efficientalgorithm by proving a novel link between total variation and fast network flowalgorithms via conic duality. Experiments show that the proposed nonnegativemodel performs much better in exploring the intrinsic structure of data viaselecting stable features compared with other state-of-the-arts.
arxiv-10800-165 | A Bayesian Approach to Sparse plus Low rank Network Identification | http://arxiv.org/abs/1503.07340 | author:Mattia Zorzi, Alessandro Chiuso category:math.OC stat.ML published:2015-03-25 summary:We consider the problem of modeling multivariate time series withparsimonious dynamical models which can be represented as sparse dynamicBayesian networks with few latent nodes. This structure translates into asparse plus low rank model. In this paper, we propose a Gaussian regressionapproach to identify such a model.
arxiv-10800-166 | Regularized Minimax Conditional Entropy for Crowdsourcing | http://arxiv.org/abs/1503.07240 | author:Dengyong Zhou, Qiang Liu, John C. Platt, Christopher Meek, Nihar B. Shah category:cs.LG stat.ML published:2015-03-25 summary:There is a rapidly increasing interest in crowdsourcing for data labeling. Bycrowdsourcing, a large number of labels can be often quickly gathered at lowcost. However, the labels provided by the crowdsourcing workers are usually notof high quality. In this paper, we propose a minimax conditional entropyprinciple to infer ground truth from noisy crowdsourced labels. Under thisprinciple, we derive a unique probabilistic labeling model jointlyparameterized by worker ability and item difficulty. We also propose anobjective measurement principle, and show that our method is the only methodwhich satisfies this objective measurement principle. We validate our methodthrough a variety of real crowdsourcing datasets with binary, multiclass orordinal labels.
arxiv-10800-167 | Initialization Strategies of Spatio-Temporal Convolutional Neural Networks | http://arxiv.org/abs/1503.07274 | author:Elman Mansimov, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV cs.LG published:2015-03-25 summary:We propose a new way of incorporating temporal information present in videosinto Spatial Convolutional Neural Networks (ConvNets) trained on images, thatavoids training Spatio-Temporal ConvNets from scratch. We describe severalinitializations of weights in 3D Convolutional Layers of Spatio-TemporalConvNet using 2D Convolutional Weights learned from ImageNet. We show that itis important to initialize 3D Convolutional Weights judiciously in order tolearn temporal representations of videos. We evaluate our methods on theUCF-101 dataset and demonstrate improvement over Spatial ConvNets.
arxiv-10800-168 | Morphological Analyzer and Generator for Russian and Ukrainian Languages | http://arxiv.org/abs/1503.07283 | author:Mikhail Korobov category:cs.CL published:2015-03-25 summary:pymorphy2 is a morphological analyzer and generator for Russian and Ukrainianlanguages. It uses large efficiently encoded lexi- cons built from OpenCorporaand LanguageTool data. A set of linguistically motivated rules is developed toenable morphological analysis and generation of out-of-vocabulary wordsobserved in real-world documents. For Russian pymorphy2 providesstate-of-the-arts morphological analysis quality. The analyzer is implementedin Python programming language with optional C++ extensions. Emphasis is put onease of use, documentation and extensibility. The package is distributed undera permissive open-source license, encouraging its use in both academic andcommercial setting.
arxiv-10800-169 | Probabilistic Binary-Mask Cocktail-Party Source Separation in a Convolutional Deep Neural Network | http://arxiv.org/abs/1503.06962 | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-03-24 summary:Separation of competing speech is a key challenge in signal processing and afeat routinely performed by the human auditory brain. A long standing benchmarkof the spectrogram approach to source separation is known as the ideal binarymask. Here, we train a convolutional deep neural network, on a two-speakercocktail party problem, to make probabilistic predictions about binary masks.Our results approach ideal binary mask performance, illustrating thatrelatively simple deep neural networks are capable of robust binary maskprediction. We also illustrate the trade-off between prediction statistics andseparation quality.
arxiv-10800-170 | Fast keypoint detection in video sequences | http://arxiv.org/abs/1503.06959 | author:Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi category:cs.CV cs.MM published:2015-03-24 summary:A number of computer vision tasks exploit a succinct representation of thevisual content in the form of sets of local features. Given an input image,feature extraction algorithms identify a set of keypoints and assign to each ofthem a description vector, based on the characteristics of the visual contentsurrounding the interest point. Several tasks might require local features tobe extracted from a video sequence, on a frame-by-frame basis. Althoughtemporal downsampling has been proven to be an effective solution for mobileaugmented reality and visual search, high temporal resolution is a keyrequirement for time-critical applications such as object tracking, eventrecognition, pedestrian detection, surveillance. In recent years, more and morecomputationally efficient visual feature detectors and decriptors have beenproposed. Nonetheless, such approaches are tailored to still images. In thispaper we propose a fast keypoint detection algorithm for video sequences, thatexploits the temporal coherence of the sequence of keypoints. According to theproposed method, each frame is preprocessed so as to identify the parts of theinput frame for which keypoint detection and description need to be performed.Our experiments show that it is possible to achieve a reduction incomputational time of up to 40%, without significantly affecting the taskaccuracy.
arxiv-10800-171 | Comparing published multi-label classifier performance measures to the ones obtained by a simple multi-label baseline classifier | http://arxiv.org/abs/1503.06952 | author:Jean Metz, Newton Spolaôr, Everton A. Cherman, Maria C. Monard category:cs.LG published:2015-03-24 summary:In supervised learning, simple baseline classifiers can be constructed byonly looking at the class, i.e., ignoring any other information from thedataset. The single-label learning community frequently uses as a reference theone which always predicts the majority class. Although a classifier mightperform worse than this simple baseline classifier, this behaviour requires aspecial explanation. Aiming to motivate the community to compare experimentalresults with the ones provided by a multi-label baseline classifier, callingthe attention about the need of special explanations related to classifierswhich perform worse than the baseline, in this work we propose the use ofGeneral_B, a multi-label baseline classifier. General_B was evaluated incontrast to results published in the literature which were carefully selectedusing a systematic review process. It was found that a considerable number ofpublished results on 10 frequently used datasets are worse than or equal to theones obtained by General_B, and for one dataset it reaches up to 43% of thedataset published results. Moreover, although a simple baseline classifier wasnot considered in these publications, it was observed that even for very poorresults no special explanations were provided in most of them. We hope that thefindings of this work would encourage the multi-label community to consider theidea of using a simple baseline classifier, such that further explanations areprovided when a classifiers performs worse than a baseline.
arxiv-10800-172 | PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers | http://arxiv.org/abs/1503.06944 | author:Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant category:stat.ML cs.LG published:2015-03-24 summary:In this paper, we provide two main contributions in PAC-Bayesian theory fordomain adaptation where the objective is to learn, from a source distribution,a well-performing majority vote on a different target distribution. On the onehand, we propose an improvement of the previous approach proposed by Germain etal. (2013), that relies on a novel distribution pseudodistance based on adisagreement averaging, allowing us to derive a new tighter PAC-Bayesian domainadaptation bound for the stochastic Gibbs classifier. We specialize it tolinear classifiers, and design a learning algorithm which shows interestingresults on a synthetic problem and on a popular sentiment annotation task. Onthe other hand, we generalize these results to multisource domain adaptationallowing us to take into account different source domains. This study opens thedoor to tackle domain adaptation tasks by making use of all the PAC-Bayesiantools.
arxiv-10800-173 | Measuring Software Quality in Use: State-of-the-Art and Research Challenges | http://arxiv.org/abs/1503.06934 | author:Issa Atoum, Chih How Bong category:cs.SE cs.CL published:2015-03-24 summary:Software quality in use comprises quality from the user's perspective. It hasgained its importance in e-government applications, mobile-based applications,embedded systems, and even business process development. User's decisions onsoftware acquisitions are often ad hoc or based on preference due to difficultyin quantitatively measuring software quality in use. But, why is quality-in-usemeasurement difficult? Although there are many software quality models, to theauthors' knowledge no works survey the challenges related to softwarequality-in-use measurement. This article has two main contributions: 1) itidentifies and explains major issues and challenges in measuring softwarequality in use in the context of the ISO SQuaRE series and related softwarequality models and highlights open research areas; and 2) it sheds light on aresearch direction that can be used to predict software quality in use. Inshort, the quality-in-use measurement issues are related to the complexity ofthe current standard models and the limitations and incompleteness of thecustomized software quality models. A sentiment analysis of software reviews isproposed to deal with these issues.
arxiv-10800-174 | Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector | http://arxiv.org/abs/1503.06917 | author:Qiang Zhang, Yilin Wang, Baoxin Li category:cs.CV published:2015-03-24 summary:Visual saliency, which predicts regions in the field of view that draw themost visual attention, has attracted a lot of interest from researchers. It hasalready been used in several vision tasks, e.g., image classification, objectdetection, foreground segmentation. Recently, the spectrum analysis basedvisual saliency approach has attracted a lot of interest due to its simplicityand good performance, where the phase information of the image is used toconstruct the saliency map. In this paper, we propose a new approach fordetecting spatiotemporal visual saliency based on the phase spectrum of thevideos, which is easy to implement and computationally efficient. With theproposed algorithm, we also study how the spatiotemporal saliency can be usedin two important vision task, abnormality detection and spatiotemporal interestpoint detection. The proposed algorithm is evaluated on several commonly useddatasets with comparison to the state-of-art methods from the literature. Theexperiments demonstrate the effectiveness of the proposed approach tospatiotemporal visual saliency detection and its application to the abovevision tasks
arxiv-10800-175 | Convergence radius and sample complexity of ITKM algorithms for dictionary learning | http://arxiv.org/abs/1503.07027 | author:Karin Schnass category:cs.LG cs.IT math.IT published:2015-03-24 summary:In this work we show that iterative thresholding and K-means (ITKM)algorithms can recover a generating dictionary with K atoms from noisy $S$sparse signals up to an error $\tilde \varepsilon$ as long as theinitialisation is within a convergence radius, that is up to a $\log K$ factorinversely proportional to the dynamic range of the signals, and the sample sizeis proportional to $K \log K \tilde \varepsilon^{-2}$. The results are validfor arbitrary target errors if the sparsity level is of the order of the squareof the signal dimension $d$ and for target errors down to $K^{-\ell}$ if $S$scales as $S \leq d/(\ell \log K)$.
arxiv-10800-176 | Rotation-invariant convolutional neural networks for galaxy morphology prediction | http://arxiv.org/abs/1503.07077 | author:Sander Dieleman, Kyle W. Willett, Joni Dambre category:astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML published:2015-03-24 summary:Measuring the morphological parameters of galaxies is a key requirement forstudying their formation and evolution. Surveys such as the Sloan Digital SkySurvey (SDSS) have resulted in the availability of very large collections ofimages, which have permitted population-wide analyses of galaxy morphology.Morphological analysis has traditionally been carried out mostly via visualinspection by trained experts, which is time-consuming and does not scale tolarge ($\gtrsim10^4$) numbers of images. Although attempts have been made to build automated classification systems,these have not been able to achieve the desired level of accuracy. The GalaxyZoo project successfully applied a crowdsourcing strategy, inviting onlineusers to classify images by answering a series of questions. Unfortunately,even this approach does not scale well enough to keep up with the increasingavailability of galaxy images. We present a deep neural network model for galaxy morphology classificationwhich exploits translational and rotational symmetry. It was developed in thecontext of the Galaxy Challenge, an international competition to build the bestmodel for morphology classification based on annotated images from the GalaxyZoo project. For images with high agreement among the Galaxy Zoo participants, our modelis able to reproduce their consensus with near-perfect accuracy ($> 99\%$) formost questions. Confident model predictions are highly accurate, which makesthe model suitable for filtering large collections of images and forwardingchallenging images to experts for manual annotation. This approach greatlyreduces the experts' workload without affecting accuracy. The application ofthese algorithms to larger sets of training data will be critical for analysingresults from future surveys such as the LSST.
arxiv-10800-177 | Penalty, Shrinkage, and Preliminary Test Estimators under Full Model Hypothesis | http://arxiv.org/abs/1503.06910 | author:Enayetur Raheem, A. K. Md. Ehsanes Saleh category:math.ST stat.CO stat.ME stat.ML stat.TH published:2015-03-24 summary:This paper considers a multiple regression model and compares, under fullmodel hypothesis, analytically as well as by simulation, the performancecharacteristics of some popular penalty estimators such as ridge regression,LASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,restricted estimator, preliminary test estimator, and Stein-type estimatorswhen the dimension of the parameter space is smaller than the sample spacedimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE whileLASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it isobserved that neither penalty estimators nor Stein-type estimator dominate oneanother.
arxiv-10800-178 | A Note on Information-Directed Sampling and Thompson Sampling | http://arxiv.org/abs/1503.06902 | author:Li Zhou category:cs.LG cs.AI published:2015-03-24 summary:This note introduce three Bayesian style Multi-armed bandit algorithms:Information-directed sampling, Thompson Sampling and Generalized ThompsonSampling. The goal is to give an intuitive explanation for these threealgorithms and their regret bounds, and provide some derivations that areomitted in the original papers.
arxiv-10800-179 | Analysis of Spectrum Occupancy Using Machine Learning Algorithms | http://arxiv.org/abs/1503.07104 | author:Freeha Azmat, Yunfei Chen, Nigel Stocks category:cs.NI cs.LG published:2015-03-24 summary:In this paper, we analyze the spectrum occupancy using different machinelearning techniques. Both supervised techniques (naive Bayesian classifier(NBC), decision trees (DT), support vector machine (SVM), linear regression(LR)) and unsupervised algorithm (hidden markov model (HMM)) are studied tofind the best technique with the highest classification accuracy (CA). Adetailed comparison of the supervised and unsupervised algorithms in terms ofthe computational time and classification accuracy is performed. The classifiedoccupancy status is further utilized to evaluate the probability of secondaryuser outage for the future time slots, which can be used by system designers todefine spectrum allocation and spectrum sharing policies. Numerical resultsshow that SVM is the best algorithm among all the supervised and unsupervisedclassifiers. Based on this, we proposed a new SVM algorithm by combining itwith fire fly algorithm (FFA), which is shown to outperform all otheralgorithms.
arxiv-10800-180 | Universal Approximation of Markov Kernels by Shallow Stochastic Feedforward Networks | http://arxiv.org/abs/1503.07211 | author:Guido Montufar category:cs.LG stat.ML 82C32 published:2015-03-24 summary:We establish upper bounds for the minimal number of hidden units for which abinary stochastic feedforward network with sigmoid activation probabilities anda single hidden layer is a universal approximator of Markov kernels. We showthat each possible probabilistic assignment of the states of $n$ output units,given the states of $k\geq1$ input units, can be approximated arbitrarily wellby a network with $2^{k-1}(2^{n-1}-1)$ hidden units.
arxiv-10800-181 | Sample compression schemes for VC classes | http://arxiv.org/abs/1503.06960 | author:Shay Moran, Amir Yehudayoff category:cs.LG published:2015-03-24 summary:Sample compression schemes were defined by Littlestone and Warmuth (1986) asan abstraction of the structure underlying many learning algorithms. Roughlyspeaking, a sample compression scheme of size $k$ means that given an arbitrarylist of labeled examples, one can retain only $k$ of them in a way that allowsto recover the labels of all other examples in the list. They showed thatcompression implies PAC learnability for binary-labeled classes, and askedwhether the other direction holds. We answer their question and show that everyconcept class $C$ with VC dimension $d$ has a sample compression scheme of sizeexponential in $d$. The proof uses an approximate minimax phenomenon for binarymatrices of low VC dimension, which may be of interest in the context of gametheory.
arxiv-10800-182 | On Lower and Upper Bounds for Smooth and Strongly Convex Optimization Problems | http://arxiv.org/abs/1503.06833 | author:Yossi Arjevani, Shai Shalev-Shwartz, Ohad Shamir category:math.OC cs.LG published:2015-03-23 summary:We develop a novel framework to study smooth and strongly convex optimizationalgorithms, both deterministic and stochastic. Focusing on quadratic functionswe are able to examine optimization algorithms as a recursive application oflinear operators. This, in turn, reveals a powerful connection between a classof optimization algorithms and the analytic theory of polynomials whereby newlower and upper bounds are derived. Whereas existing lower bounds for thissetting are only valid when the dimensionality scales with the number ofiterations, our lower bound holds in the natural regime where thedimensionality is fixed. Lastly, expressing it as an optimal solution for thecorresponding optimization problem over polynomials, as formulated by ourframework, we present a novel systematic derivation of Nesterov's well-knownAccelerated Gradient Descent method. This rather natural interpretation of AGDcontrasts with earlier ones which lacked a simple, yet solid, motivation.
arxiv-10800-183 | Non-contact transmittance photoplethysmographic imaging (PPGI) for long-distance cardiovascular monitoring | http://arxiv.org/abs/1503.06775 | author:Robert Amelard, Christian Scharfenberger, Farnoud Kazemzadeh, Kaylen J. Pfisterer, Bill S. Lin, Alexander Wong, David A. Clausi category:physics.optics cs.CV published:2015-03-23 summary:Photoplethysmography (PPG) devices are widely used for monitoringcardiovascular function. However, these devices require skin contact, whichrestrict their use to at-rest short-term monitoring using single-pointmeasurements. Photoplethysmographic imaging (PPGI) has been recently proposedas a non-contact monitoring alternative by measuring blood pulse signals acrossa spatial region of interest. Existing systems operate in reflectance mode, ofwhich many are limited to short-distance monitoring and are prone to temporalchanges in ambient illumination. This paper is the first study to investigatethe feasibility of long-distance non-contact cardiovascular monitoring at thesupermeter level using transmittance PPGI. For this purpose, a novel PPGIsystem was designed at the hardware and software level using ambient correctionvia temporally coded illumination (TCI) and signal processing for PPGI signalextraction. Experimental results show that the processing steps yield asubstantially more pulsatile PPGI signal than the raw acquired signal,resulting in statistically significant increases in correlation to ground-truthPPG in both short- ($p \in [<0.0001, 0.040]$) and long-distance ($p \in[<0.0001, 0.056]$) monitoring. The results support the hypothesis thatlong-distance heart rate monitoring is feasible using transmittance PPGI,allowing for new possibilities of monitoring cardiovascular function in anon-contact manner.
arxiv-10800-184 | Using Generic Summarization to Improve Music Information Retrieval Tasks | http://arxiv.org/abs/1503.06666 | author:Francisco Raposo, Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.LG cs.SD H.5.5 published:2015-03-23 summary:In order to satisfy processing time constraints, many MIR tasks process onlya segment of the whole music signal. This practice may lead to decreasingperformance, since the most important information for the tasks may not be inthose processed segments. In this paper, we leverage generic summarizationalgorithms, previously applied to text and speech summarization, to summarizeitems in music datasets. These algorithms build summaries, that are bothconcise and diverse, by selecting appropriate segments from the input signalwhich makes them good candidates to summarize music as well. We evaluate thesummarization process on binary and multiclass music genre classificationtasks, by comparing the performance obtained using summarized datasets againstthe performances obtained using continuous segments (which is the traditionalmethod used for addressing the previously mentioned time constraints) and fullsongs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,MMR, and a Support Sets-based Centrality model improve classificationperformance when compared to selected 30-second baselines. We also show thatsummarized datasets lead to a classification performance whose difference isnot statistically significant from using full songs. Furthermore, we make anargument stating the advantages of sharing summarized datasets for future MIRresearch.
arxiv-10800-185 | Unsupervised POS Induction with Word Embeddings | http://arxiv.org/abs/1503.06760 | author:Chu-Cheng Lin, Waleed Ammar, Chris Dyer, Lori Levin category:cs.CL published:2015-03-23 summary:Unsupervised word embeddings have been shown to be valuable as features insupervised learning problems; however, their role in unsupervised problems hasbeen less thoroughly explored. In this paper, we show that embeddings canlikewise add value to the problem of unsupervised POS induction. In tworepresentative models of POS induction, we replace multinomial distributionsover the vocabulary with multivariate Gaussian distributions over wordembeddings and observe consistent improvements in eight languages. We alsoanalyze the effect of various choices while inducing word embeddings on"downstream" POS induction results.
arxiv-10800-186 | Online classifier adaptation for cost-sensitive learning | http://arxiv.org/abs/1503.06745 | author:Junlin Zhang, Jose Garcia category:cs.LG published:2015-03-23 summary:In this paper, we propose the problem of online cost-sensitive clas- sifieradaptation and the first algorithm to solve it. We assume we have a baseclassifier for a cost-sensitive classification problem, but it is trained withrespect to a cost setting different to the desired one. Moreover, we also havesome training data samples streaming to the algorithm one by one. The prob- lemis to adapt the given base classifier to the desired cost setting using thesteaming training samples online. To solve this problem, we propose to learn anew classifier by adding an adaptation function to the base classifier, andupdate the adaptation function parameter according to the streaming datasamples. Given a input data sample and the cost of misclassifying it, we up-date the adaptation function parameter by minimizing cost weighted hinge lossand respecting previous learned parameter simultaneously. The proposedalgorithm is compared to both online and off-line cost-sensitive algorithms ontwo cost-sensitive classification problems, and the experiments show that itnot only outperforms them one classification performances, but also requiressignificantly less running time.
arxiv-10800-187 | Yara Parser: A Fast and Accurate Dependency Parser | http://arxiv.org/abs/1503.06733 | author:Mohammad Sadegh Rasooli, Joel Tetreault category:cs.CL published:2015-03-23 summary:Dependency parsers are among the most crucial tools in natural languageprocessing as they have many important applications in downstream tasks such asinformation retrieval, machine translation and knowledge acquisition. Weintroduce the Yara Parser, a fast and accurate open-source dependency parserbased on the arc-eager algorithm and beam search. It achieves an unlabeledaccuracy of 93.32 on the standard WSJ test set which ranks it among the topdependency parsers. At its fastest, Yara can parse about 4000 sentences persecond when in greedy mode (1 beam). When optimizing for accuracy (using 64beams and Brown cluster features), Yara can parse 45 sentences per second. Theparser can be trained on any syntactic dependency treebank and differentoptions are provided in order to make it more flexible and tunable for specifictasks. It is released with the Apache version 2.0 license and can be used forboth commercial and academic purposes. The parser can be found athttps://github.com/yahoo/YaraParser.
arxiv-10800-188 | Video-Based Action Recognition Using Rate-Invariant Analysis of Covariance Trajectories | http://arxiv.org/abs/1503.06699 | author:Zhengwu Zhang, Jingyong Su, Eric Klassen, Huiling Le, Anuj Srivastava category:cs.CV published:2015-03-23 summary:Statistical classification of actions in videos is mostly performed byextracting relevant features, particularly covariance features, from imageframes and studying time series associated with temporal evolutions of thesefeatures. A natural mathematical representation of activity videos is in formof parameterized trajectories on the covariance manifold, i.e. the set ofsymmetric, positive-definite matrices (SPDMs). The variable execution-rates ofactions implies variable parameterizations of the resulting trajectories, andcomplicates their classification. Since action classes are invariant toexecution rates, one requires rate-invariant metrics for comparingtrajectories. A recent paper represented trajectories using their transportedsquare-root vector fields (TSRVFs), defined by parallel translatingscaled-velocity vectors of trajectories to a reference tangent space on themanifold. To avoid arbitrariness of selecting the reference and to reducedistortion introduced during this mapping, we develop a purely intrinsicapproach where SPDM trajectories are represented by redefining their TSRVFs atthe starting points of the trajectories, and analyzed as elements of a vectorbundle on the manifold. Using a natural Riemannain metric on vector bundles ofSPDMs, we compute geodesic paths and geodesic distances between trajectories inthe quotient space of this vector bundle, with respect to there-parameterization group. This makes the resulting comparison of trajectoriesinvariant to their re-parameterization. We demonstrate this framework on twoapplications involving video classification: visual speech recognition orlip-reading and hand-gesture recognition. In both cases we achieve resultseither comparable to or better than the current literature.
arxiv-10800-189 | Vehicle Local Position Estimation System | http://arxiv.org/abs/1503.06648 | author:Mrinal Haloi, Dinesh Babu Jayagopi category:cs.CV 68T45 published:2015-03-23 summary:In this paper, a robust vehicle local position estimation with the help ofsingle camera sensor and GPS is presented. A modified Inverse PerspectiveMapping, illuminant Invariant techniques and object detection based approach isused to localize the vehicle in the road. Vehicles current lane, its positionfrom road boundary and other cars are used to define its local position. Forthis purpose Lane markings are detected using a Laplacian edge feature, robustto shadowing. Effect of shadowing and extra sun light are removed using Labcolor space and illuminant invariant techniques. Lanes are assumed to be asparabolic model and fitted using robust RANSAC. This method can reliably detectall lanes of the road, estimate lane departure angle and local position ofvehicle relative to lanes, road boundary and other cars. Different type ofobstacle like pedestrians, vehicles are detected using HOG feature baseddeformable part model.
arxiv-10800-190 | A novel pLSA based Traffic Signs Classification System | http://arxiv.org/abs/1503.06643 | author:Mrinal Haloi category:cs.CV 68T45 published:2015-03-23 summary:In this work we developed a novel and fast traffic sign recognition system, avery important part for advanced driver assistance system and for autonomousdriving. Traffic signs play a very vital role in safe driving and avoidingaccident. We have used image processing and topic discovery model pLSA totackle this challenging multiclass classification problem. Our algorithm isconsist of two parts, shape classification and sign classification for improvedaccuracy. For processing and representation of image we have used bag offeatures model with SIFT local descriptor. Where a visual vocabulary of size300 words are formed using k-means codebook formation algorithm. We exploitedthe concept that every image is a collection of visual topics and images havingsame topics will belong to same category. Our algorithm is tested on Germantraffic sign recognition benchmark (GTSRB) and gives very promising result nearto existing state of the art techniques.
arxiv-10800-191 | Superpixelizing Binary MRF for Image Labeling Problems | http://arxiv.org/abs/1503.06642 | author:Junyan Wang, Sai-Kit Yeung category:cs.CV published:2015-03-23 summary:Superpixels have become prevalent in computer vision. They have been used toachieve satisfactory performance at a significantly smaller computational costfor various tasks. People have also combined superpixels with Markov randomfield (MRF) models. However, it often takes additional effort to formulate MRFon superpixel-level, and to the best of our knowledge there exists noprincipled approach to obtain this formulation. In this paper, we show howgeneric pixel-level binary MRF model can be solved in the superpixel space. Asthe main contribution of this paper, we show that a superpixel-level MRF can bederived from the pixel-level MRF by substituting the superpixel representationof the pixelwise label into the original pixel-level MRF energy. The resultantsuperpixel-level MRF energy also remains submodular for a submodularpixel-level MRF. The derived formula hence gives us a handy way to formulateMRF energy in superpixel-level. In the experiments, we demonstrate the efficacyof our approach on several computer vision problems.
arxiv-10800-192 | A Probabilistic Interpretation of Sampling Theory of Graph Signals | http://arxiv.org/abs/1503.06629 | author:Akshay Gadde, Antonio Ortega category:cs.LG published:2015-03-23 summary:We give a probabilistic interpretation of sampling theory of graph signals.To do this, we first define a generative model for the data using a pairwiseGaussian random field (GRF) which depends on the graph. We show that, undercertain conditions, reconstructing a graph signal from a subset of its samplesby least squares is equivalent to performing MAP inference on an approximationof this GRF which has a low rank covariance matrix. We then show that asampling set of given size with the largest associated cut-off frequency, whichis optimal from a sampling theoretic point of view, minimizes the worst casepredictive covariance of the MAP estimate on the GRF. This interpretation alsogives an intuitive explanation for the superior performance of the samplingtheoretic approach to active semi-supervised classification.
arxiv-10800-193 | Proficiency Comparison of LADTree and REPTree Classifiers for Credit Risk Forecast | http://arxiv.org/abs/1503.06608 | author:Lakshmi Devasena C category:cs.LG published:2015-03-23 summary:Predicting the Credit Defaulter is a perilous task of Financial Industrieslike Banks. Ascertaining non-payer before giving loan is a significant andconflict-ridden task of the Banker. Classification techniques are the betterchoice for predictive analysis like finding the claimant, whether he/she is anunpretentious customer or a cheat. Defining the outstanding classifier is arisky assignment for any industrialist like a banker. This allow computerscience researchers to drill down efficient research works through evaluatingdifferent classifiers and finding out the best classifier for such predictiveproblems. This research work investigates the productivity of LADTreeClassifier and REPTree Classifier for the credit risk prediction and comparestheir fitness through various measures. German credit dataset has been takenand used to predict the credit risk with a help of open source machine learningtool.
arxiv-10800-194 | A Machine Learning Approach to Predicting the Smoothed Complexity of Sorting Algorithms | http://arxiv.org/abs/1503.06572 | author:Bichen Shi, Michel Schellekens, Georgiana Ifrim category:cs.LG cs.AI cs.CC published:2015-03-23 summary:Smoothed analysis is a framework for analyzing the complexity of analgorithm, acting as a bridge between average and worst-case behaviour. Forexample, Quicksort and the Simplex algorithm are widely used in practicalapplications, despite their heavy worst-case complexity. Smoothed complexityaims to better characterize such algorithms. Existing theoretical bounds forthe smoothed complexity of sorting algorithms are still quite weak.Furthermore, empirically computing the smoothed complexity via its originaldefinition is computationally infeasible, even for modest input sizes. In thispaper, we focus on accurately predicting the smoothed complexity of sortingalgorithms, using machine learning techniques. We propose two regression modelsthat take into account various properties of sorting algorithms and some of theknown theoretical results in smoothed analysis to improve prediction quality.We show experimental results for predicting the smoothed complexity ofQuicksort, Mergesort, and optimized Bubblesort for large input sizes, thereforefilling the gap between known theoretical and empirical results.
arxiv-10800-195 | A Comparative Analysis of Tensor Decomposition Models Using Hyper Spectral Image | http://arxiv.org/abs/1503.06561 | author:Ankit Gupta, Ashish Oberoi category:cs.NA cs.CV published:2015-03-23 summary:Hyper spectral imaging is a remote sensing technology, providing variety ofapplications such as material identification, space object identification,planetary exploitation etc. It deals with capturing continuum of images of theearth surface from different angles. Due to the multidimensional nature of theimage, multi-way arrays are one of the possible solutions for analyzing hyperspectral data. This multi-way array is called tensor. Our approach deals withimplementing three decomposition models LMLRA, BTD and CPD to the sample datafor choosing the best decomposition of the data set. The results have provedthat Block Term Decomposition (BTD) is the best tensor model for decomposingthe hyper spectral image in to resultant factor matrices.
arxiv-10800-196 | Optimum Reject Options for Prototype-based Classification | http://arxiv.org/abs/1503.06549 | author:Lydia Fischer, Barbara Hammer, Heiko Wersing category:cs.LG published:2015-03-23 summary:We analyse optimum reject strategies for prototype-based classifiers andreal-valued rejection measures, using the distance of a data point to theclosest prototype or probabilistic counterparts. We compare reject schemes withglobal thresholds, and local thresholds for the Voronoi cells of theclassifier. For the latter, we develop a polynomial-time algorithm to computeoptimum thresholds based on a dynamic programming scheme, and we propose anintuitive linear time, memory efficient approximation thereof with competitiveaccuracy. Evaluating the performance in various benchmarks, we conclude thatlocal reject options are beneficial in particular for simple prototype-basedclassifiers, while the improvement is less pronounced for advanced models. Forthe latter, an accuracy-reject curve which is comparable to support vectormachine classifiers with state of the art reject options can be reached.
arxiv-10800-197 | Fusing Continuous-valued Medical Labels using a Bayesian Model | http://arxiv.org/abs/1503.06619 | author:Tingting Zhu, Nic Dunkley, Joachim Behar, David A. Clifton, Gari D. Clifford category:cs.LG published:2015-03-23 summary:With the rapid increase in volume of time series medical data availablethrough wearable devices, there is a need to employ automated algorithms tolabel data. Examples of labels include interventions, changes in activity (e.g.sleep) and changes in physiology (e.g. arrhythmias). However, automatedalgorithms tend to be unreliable resulting in lower quality care. Expertannotations are scarce, expensive, and prone to significant inter- andintra-observer variance. To address these problems, a BayesianContinuous-valued Label Aggregator(BCLA) is proposed to provide a reliableestimation of label aggregation while accurately infer the precision and biasof each algorithm. The BCLA was applied to QT interval (pro-arrhythmicindicator) estimation from the electrocardiogram using labels from the 2006PhysioNet/Computing in Cardiology Challenge database. It was compared to themean, median, and a previously proposed Expectation Maximization (EM) labelaggregation approaches. While accurately predicting each labelling algorithm'sbias and precision, the root-mean-square error of the BCLA was11.78$\pm$0.63ms, significantly outperforming the best Challenge entry(15.37$\pm$2.13ms) as well as the EM, mean, and median voting strategies(14.76$\pm$0.52ms, 17.61$\pm$0.55ms, and 14.43$\pm$0.57ms respectively with$p<0.0001$).
arxiv-10800-198 | Communication Efficient Distributed Kernel Principal Component Analysis | http://arxiv.org/abs/1503.06858 | author:Maria-Florina Balcan, Yingyu Liang, Le Song, David Woodruff, Bo Xie category:cs.LG published:2015-03-23 summary:Kernel Principal Component Analysis (KPCA) is a key machine learningalgorithm for extracting nonlinear features from data. In the presence of alarge volume of high dimensional data collected in a distributed fashion, itbecomes very costly to communicate all of this data to a single data center andthen perform kernel PCA. Can we perform kernel PCA on the entire dataset in adistributed and communication efficient fashion while maintaining provable andstrong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing acommunication efficient algorithm to perform kernel PCA in the distributedsetting. The algorithm is a clever combination of subspace embedding andadaptive sampling techniques, and we show that the algorithm can take as inputan arbitrary configuration of distributed datasets, and compute a set of globalkernel principal components with relative error guarantees independent of thedimension of the feature space or the total number of data points. Inparticular, computing $k$ principal components with relative error $\epsilon$over $s$ workers has communication cost $\tilde{O}(s \rho k/\epsilon+sk^2/\epsilon^3)$ words, where $\rho$ is the average number of nonzero entriesin each data point. Furthermore, we experimented the algorithm with large-scalereal world datasets and showed that the algorithm produces a high qualitykernel PCA solution while using significantly less communication thanalternative approaches.
arxiv-10800-199 | Study of all the periods of a Neuronal Recurrence Equation | http://arxiv.org/abs/1503.06866 | author:Serge Alain Ebélé, Renè Ndoundam category:cs.NE 92B20 F.1.1 published:2015-03-23 summary:We characterize the structure of the periods of a neuronal recurrenceequation. Firstly, we give a characterization of k-chains in 0-1 periodicsequences. Secondly, we characterize the periods of all cycles of some neuronalrecurrence equation. Thirdly, we explain how these results can be used todeduce the existence of the generalized period-halving bifurcation.
arxiv-10800-200 | On some provably correct cases of variational inference for topic models | http://arxiv.org/abs/1503.06567 | author:Pranjal Awasthi, Andrej Risteski category:cs.LG cs.DS stat.ML published:2015-03-23 summary:Variational inference is a very efficient and popular heuristic used invarious forms in the context of latent variable models. It's closely related toExpectation Maximization (EM), and is applied when exact EM is computationallyinfeasible. Despite being immensely popular, current theoretical understandingof the effectiveness of variaitonal inference based algorithms is very limited.In this work we provide the first analysis of instances where variationalinference algorithms converge to the global optimum, in the setting of topicmodels. More specifically, we show that variational inference provably learns theoptimal parameters of a topic model under natural assumptions on the topic-wordmatrix and the topic priors. The properties that the topic word matrix mustsatisfy in our setting are related to the topic expansion assumption introducedin (Anandkumar et al., 2013), as well as the anchor words assumption in (Aroraet al., 2012c). The assumptions on the topic priors are related to the wellknown Dirichlet prior, introduced to the area of topic modeling by (Blei etal., 2003). It is well known that initialization plays a crucial role in how wellvariational based algorithms perform in practice. The initializations that weuse are fairly natural. One of them is similar to what is currently used inLDA-c, the most popular implementation of variational inference for topicmodels. The other one is an overlapping clustering algorithm, inspired by awork by (Arora et al., 2014) on dictionary learning, which is very simple andefficient. While our primary goal is to provide insights into when variational inferencemight work in practice, the multiplicative, rather than the additive nature ofthe variational inference updates forces us to use fairly non-standard proofarguments, which we believe will be of general interest.
arxiv-10800-201 | Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation | http://arxiv.org/abs/1503.06813 | author:Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang category:cs.CV published:2015-03-23 summary:Due to large variations in shape, appearance, and viewing conditions, objectrecognition is a key precursory challenge in the fields of object manipulationand robotic/AI visual reasoning in general. Recognizing object categories,particular instances of objects and viewpoints/poses of objects are threecritical subproblems robots must solve in order to accurately grasp/manipulateobjects and reason about their environments. Multi-view images of the sameobject lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g.visual/depth descriptor spaces). These object manifolds share the same topologydespite being geometrically different. Each object manifold can be representedas a deformed version of a unified manifold. The object manifolds can thus beparameterized by its homeomorphic mapping/reconstruction from the unifiedmanifold. In this work, we develop a novel framework to jointly solve the threechallenging recognition sub-problems, by explicitly modeling the deformationsof object manifolds and factorizing it in a view-invariant space forrecognition. We perform extensive experiments on several challenging datasetsand achieve state-of-the-art results.
arxiv-10800-202 | Asymmetric Distributions from Constrained Mixtures | http://arxiv.org/abs/1503.06429 | author:Conrado S. Miranda, Fernando J. Von Zuben category:stat.ML cs.LG published:2015-03-22 summary:This paper introduces constrained mixtures for continuous distributions,characterized by a mixture of distributions where each distribution has a shapesimilar to the base distribution and disjoint domains. This new concept is usedto create generalized asymmetric versions of the Laplace and normaldistributions, which are shown to define exponential families, with knownconjugate priors, and to have maximum likelihood estimates for the originalparameters, with known closed-form expressions. The asymmetric and symmetricnormal distributions are compared in a linear regression example, showing thatthe asymmetric version performs at least as well as the symmetric one, and in areal world time-series problem, where a hidden Markov model is used to fit astock index, indicating that the asymmetric version provides higher likelihoodand may learn distribution models over states and transition distributions withconsiderably less entropy.
arxiv-10800-203 | Indian Buffet process for model selection in convolved multiple-output Gaussian processes | http://arxiv.org/abs/1503.06432 | author:Cristian Guarnizo, Mauricio A. Álvarez category:stat.ML published:2015-03-22 summary:Multi-output Gaussian processes have received increasing attention during thelast few years as a natural mechanism to extend the powerful flexibility ofGaussian processes to the setup of multiple output variables. The key pointhere is the ability to design kernel functions that allow exploiting thecorrelations between the outputs while fulfilling the positive definitenessrequisite for the covariance function. Alternatives to construct thesecovariance functions are the linear model of coregionalization and processconvolutions. Each of these methods demand the specification of the number oflatent Gaussian process used to build the covariance function for the outputs.We propose in this paper, the use of an Indian Buffet process as a way toperform model selection over the number of latent Gaussian processes. This typeof model is particularly important in the context of latent force models, wherethe latent forces are associated to physical quantities like protein profilesor latent forces in mechanical systems. We use variational inference toestimate posterior distributions over the variables involved, and show examplesof the model performance over artificial data, a motion capture dataset, and agene expression dataset.
arxiv-10800-204 | Modeling browser-based distributed evolutionary computation systems | http://arxiv.org/abs/1503.06424 | author:Juan Julián Merelo-Guervós, Pablo García-Sánchez category:cs.DC cs.NE cs.NI published:2015-03-22 summary:From the era of big science we are back to the "do it yourself", where you donot have any money to buy clusters or subscribe to grids but still havealgorithms that crave many computing nodes and need them to measurescalability. Fortunately, this coincides with the era of big data, cloudcomputing, and browsers that include JavaScript virtual machines. Those are thereasons why this paper will focus on two different aspects of volunteer orfreeriding computing: first, the pragmatic: where to find those resources,which ones can be used, what kind of support you have to give them; and then,the theoretical: how evolutionary algorithms can be adapted to an environmentin which nodes come and go, have different computing capabilities and operatein complete asynchrony of each other. We will examine the setup needed tocreate a very simple distributed evolutionary algorithm using JavaScript andthen find a model of how users react to it by collecting data from severalexperiments featuring different classical benchmark functions.
arxiv-10800-205 | What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes | http://arxiv.org/abs/1503.06410 | author:David M. W. Powers category:cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML published:2015-03-22 summary:The F-measure or F-score is one of the most commonly used single numbermeasures in Information Retrieval, Natural Language Processing and MachineLearning, but it is based on a mistake, and the flawed assumptions render itunsuitable for use in most contexts! Fortunately, there are betteralternatives.
arxiv-10800-206 | Large-scale Log-determinant Computation through Stochastic Chebyshev Expansions | http://arxiv.org/abs/1503.06394 | author:Insu Han, Dmitry Malioutov, Jinwoo Shin category:cs.DS cs.LG cs.NA published:2015-03-22 summary:Logarithms of determinants of large positive definite matrices appearubiquitously in machine learning applications including Gaussian graphical andGaussian process models, partition functions of discrete graphical models,minimum-volume ellipsoids, metric learning and kernel learning. Log-determinantcomputation involves the Cholesky decomposition at the cost cubic in the numberof variables, i.e., the matrix dimension, which makes it prohibitive forlarge-scale applications. We propose a linear-time randomized algorithm toapproximate log-determinants for very large-scale positive definite and generalnon-singular matrices using a stochastic trace approximation, called theHutchinson method, coupled with Chebyshev polynomial expansions that both relyon efficient matrix-vector multiplications. We establish rigorous additive andmultiplicative approximation error bounds depending on the condition number ofthe input matrix. In our experiments, the proposed algorithm can provide veryhigh accuracy solutions at orders of magnitude faster time than the Choleskydecomposition and Schur completion, and enables us to compute log-determinantsof matrices involving tens of millions of variables.
arxiv-10800-207 | Costing Generated Runtime Execution Plans for Large-Scale Machine Learning Programs | http://arxiv.org/abs/1503.06384 | author:Matthias Boehm category:cs.DC cs.LG published:2015-03-22 summary:Declarative large-scale machine learning (ML) aims at the specification of MLalgorithms in a high-level language and automatic generation of hybrid runtimeexecution plans ranging from single node, in-memory computations to distributedcomputations on MapReduce (MR) or similar frameworks like Spark. Thecompilation of large-scale ML programs exhibits many opportunities forautomatic optimization. Advanced cost-based optimization techniquesrequire---as a fundamental precondition---an accurate cost model for evaluatingthe impact of optimization decisions. In this paper, we share insights into asimple and robust yet accurate technique for costing alternative runtimeexecution plans of ML programs. Our cost model relies on generating and costingruntime plans in order to automatically reflect all successive optimizationphases. Costing runtime plans also captures control flow structures such asloops and branches, and a variety of cost factors like IO, latency, andcomputation costs. Finally, we linearize all these cost factors into a singlemeasure of expected execution time. Within SystemML, this cost model isleveraged by several advanced optimizers like resource optimization and globaldata flow optimization. We share our lessons learned in order to providefoundations for the optimization of ML programs.
arxiv-10800-208 | Real-time Dynamic MRI Reconstruction using Stacked Denoising Autoencoder | http://arxiv.org/abs/1503.06383 | author:Angshul Majumdar category:cs.CV cs.NE published:2015-03-22 summary:In this work we address the problem of real-time dynamic MRI reconstruction.There are a handful of studies on this topic; these techniques are either basedon compressed sensing or employ Kalman Filtering. These techniques cannotachieve the reconstruction speed necessary for real-time reconstruction. Inthis work, we propose a new approach to MRI reconstruction. We learn anon-linear mapping from the unstructured aliased images to the correspondingclean images using a stacked denoising autoencoder (SDAE). The training forSDAE is slow, but the reconstruction is very fast - only requiring a few matrixvector multiplications. In this work, we have shown that using SDAE one canreconstruct the MRI frame faster than the data acquisition rate, therebyachieving real-time reconstruction. The quality of reconstruction is of thesame order as a previous compressed sensing based online reconstructiontechnique.
arxiv-10800-209 | Unsupervised model compression for multilayer bootstrap networks | http://arxiv.org/abs/1503.06452 | author:Xiao-Lei Zhang category:cs.LG cs.NE stat.ML published:2015-03-22 summary:Recently, multilayer bootstrap network (MBN) has demonstrated promisingperformance in unsupervised dimensionality reduction. It can learn compactrepresentations in standard data sets, i.e. MNIST and RCV1. However, as abootstrap method, the prediction complexity of MBN is high. In this paper, wepropose an unsupervised model compression framework for this general problem ofunsupervised bootstrap methods. The framework compresses a large unsupervisedbootstrap model into a small model by taking the bootstrap model and itsapplication together as a black box and learning a mapping function from theinput of the bootstrap model to the output of the application by a supervisedlearner. To specialize the framework, we propose a new technique, namedcompressive MBN. It takes MBN as the unsupervised bootstrap model and deepneural network (DNN) as the supervised learner. Our initial result on MNISTshowed that compressive MBN not only maintains the high prediction accuracy ofMBN but also is over thousands of times faster than MBN at the predictionstage. Our result suggests that the new technique integrates the effectivenessof MBN on unsupervised learning and the effectiveness and efficiency of DNN onsupervised learning together for the effectiveness and efficiency ofcompressive MBN on unsupervised learning.
arxiv-10800-210 | Lifting Object Detection Datasets into 3D | http://arxiv.org/abs/1503.06465 | author:Joao Carreira, Sara Vicente, Lourdes Agapito, Jorge Batista category:cs.CV published:2015-03-22 summary:While data has certainly taken the center stage in computer vision in recentyears, it can still be difficult to obtain in certain scenarios. In particular,acquiring ground truth 3D shapes of objects pictured in 2D images remains achallenging feat and this has hampered progress in recognition-based objectreconstruction from a single image. Here we propose to bypass previoussolutions such as 3D scanning or manual design, that scale poorly, and insteadpopulate object category detection datasets semi-automatically with dense,per-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) groundtruth figure-ground segmentations and (iii) a small set of keypointannotations. Our proposed algorithm first estimates camera viewpoint usingrigid structure-from-motion and then reconstructs object shapes by optimizingover visual hull proposals guided by loose within-class shape similarityassumptions. The visual hull sampling process attempts to intersect an object'sprojection cone with the cones of minimal subsets of other similar objectsamong those pictured from certain vantage points. We show that our method isable to produce convincing per-object 3D reconstructions and to accuratelyestimate cameras viewpoints on one of the most challenging existingobject-category detection datasets, PASCAL VOC. We hope that our results willre-stimulate interest on joint object recognition and 3D reconstruction from asingle image.
arxiv-10800-211 | Multilingual Open Relation Extraction Using Cross-lingual Projection | http://arxiv.org/abs/1503.06450 | author:Manaal Faruqui, Shankar Kumar category:cs.CL published:2015-03-22 summary:Open domain relation extraction systems identify relation and argumentphrases in a sentence without relying on any underlying schema. However,current state-of-the-art relation extraction systems are available only forEnglish because of their heavy reliance on linguistic tools such aspart-of-speech taggers and dependency parsers. We present a cross-lingualannotation projection method for language independent relation extraction. Weevaluate our method on a manually annotated test set and present results onthree typologically different languages. We release these manual annotationsand extracted relations in 61 languages from Wikipedia.
arxiv-10800-212 | Machine Learning Methods for Attack Detection in the Smart Grid | http://arxiv.org/abs/1503.06468 | author:Mete Ozay, Inaki Esnaola, Fatos T. Yarman Vural, Sanjeev R. Kulkarni, H. Vincent Poor category:cs.LG cs.CR cs.SY published:2015-03-22 summary:Attack detection problems in the smart grid are posed as statistical learningproblems for different attack scenarios in which the measurements are observedin batch or online settings. In this approach, machine learning algorithms areused to classify measurements as being either secure or attacked. An attackdetection framework is provided to exploit any available prior knowledge aboutthe system and surmount constraints arising from the sparse structure of theproblem in the proposed approach. Well-known batch and online learningalgorithms (supervised and semi-supervised) are employed with decision andfeature level fusion to model the attack detection problem. The relationshipsbetween statistical and geometric properties of attack vectors employed in theattack scenarios and learning algorithms are analyzed to detect unobservableattacks using statistical learning methods. The proposed algorithms areexamined on various IEEE test systems. Experimental analyses show that machinelearning algorithms can detect attacks with performances higher than the attackdetection algorithms which employ state vector estimation methods in theproposed attack detection framework.
arxiv-10800-213 | Relaxed Leverage Sampling for Low-rank Matrix Completion | http://arxiv.org/abs/1503.06379 | author:Abhisek Kundu category:cs.IT cs.LG math.IT stat.ML published:2015-03-22 summary:We consider the problem of exact recovery of any $m\times n$ matrix of rank$\varrho$ from a small number of observed entries via the standard nuclear normminimization framework. Such low-rank matrices have degrees of freedom$(m+n)\varrho - \varrho^2$. We show that any arbitrary low-rank matrices can berecovered exactly from a $\Theta\left(((m+n)\varrho -\varrho^2)\log^2(m+n)\right)$ randomly sampled entries, thus matching the lowerbound on the required number of entries (in terms of degrees of freedom), withan additional factor of $O(\log^2(m+n))$. To achieve this bound on sample sizewe observe each entry with probabilities proportional to the sum ofcorresponding row and column leverage scores, minus their product. We show thatthis relaxation in sampling probabilities (as opposed to sum of leverage scoresin Chen et al, 2014) can give us an $O(\varrho^2\log^2(m+n))$ additiveimprovement on the (best known) sample size obtained by Chen et al, 2014, forthe nuclear norm minimization. Experiments on real data corroborate thetheoretical improvement on sample size. Further, exact recovery of $(a)$incoherent matrices (with restricted leverage scores), and $(b)$ matrices withonly one of the row or column spaces to be incoherent, can be performed usingour relaxed leverage score sampling, via nuclear norm minimization, withoutknowing the leverage scores a priori. In such settings also we can achieveimprovement on sample size.
arxiv-10800-214 | Adaptive Concentration of Regression Trees, with Application to Random Forests | http://arxiv.org/abs/1503.06388 | author:Stefan Wager, Guenther Walther category:math.ST stat.ML stat.TH published:2015-03-22 summary:We study the convergence of the predictive surface of regression trees andforests. To support our analysis we introduce a notion of adaptiveconcentration for regression trees. This approach breaks tree training into amodel selection phase in which we pick the tree splits, followed by a modelfitting phase where we find the best regression model consistent with thesesplits. We then show that the fitted regression tree concentrates around theoptimal predictor with the same splits: as d and n get large, the discrepancyis with high probability bounded on the order of sqrt(log(d) log(n)/k)uniformly over the whole regression surface, where d is the dimension of thefeature space, n is the number of training examples, and k is the minimum leafsize for each tree. We also provide rate-matching lower bounds for thisadaptive concentration statement. From a practical perspective, our resultenables us to prove consistency results for adaptively grown forests in highdimensions, and to carry out valid post-selection inference in the sense ofBerk et al. [2013] for subgroups defined by tree leaves.
arxiv-10800-215 | Skin Detection of Animation Characters | http://arxiv.org/abs/1503.06275 | author:Kazi Tanvir Ahmed Siddiqui, Abu Wasif category:cs.CV published:2015-03-21 summary:The increasing popularity of animes makes it vulnerable to unwanted usageslike copyright violations and pornography. That is why, we need to develop amethod to detect and recognize animation characters. Skin detection is one ofthe most important steps in this way. Though there are some methods to detecthuman skin color, but those methods do not work properly for anime characters.Anime skin varies greatly from human skin in color, texture, tone and indifferent kinds of lighting. They also vary greatly among themselves. Moreover,many other things (for example leather, shirt, hair etc.), which are not skin,can have color similar to skin. In this paper, we have proposed three methodsthat can identify an anime character skin more successfully as compared withKovac, Swift, Saleh and Osman methods, which are primarily designed for humanskin detection. Our methods are based on RGB values and their comparativerelations.
arxiv-10800-216 | Hierarchical sparse Bayesian learning: theory and application for inferring structural damage from incomplete modal data | http://arxiv.org/abs/1503.06267 | author:Yong Huang, James L. Beck category:stat.AP stat.ME stat.ML published:2015-03-21 summary:Structural damage due to excessive loading or environmental degradationtypically occurs in localized areas in the absence of collapse. This priorinformation about the spatial sparseness of structural damage is exploited hereby a hierarchical sparse Bayesian learning framework with the goal of reducingthe source of ill-conditioning in the stiffness loss inversion problem fordamage detection. Sparse Bayesian learning methodologies automatically pruneaway irrelevant or inactive features from a set of potential candidates, and sothey are effective probabilistic tools for producing sparse explanatorysubsets. We have previously proposed such an approach to establish theprobability of localized stiffness reductions that serve as a proxy for damageby using noisy incomplete modal data from before and after possible damage. Thecore idea centers on a specific hierarchical Bayesian model that promotesspatial sparseness in the inferred stiffness reductions in a way that isconsistent with the Bayesian Ockham razor. In this paper, we improve the theoryof our previously proposed sparse Bayesian learning approach by eliminating anapproximation and, more importantly, incorporating a constraint on stiffnessincreases. Our approach has many appealing features that are summarized at theend of the paper. We validate the approach by applying it to the Phase IIsimulated and experimental benchmark studies sponsored by the IASC-ASCE TaskGroup on Structural Health Monitoring. The results show that it can reliablydetect, locate and assess damage by inferring substructure stiffness lossesfrom the identified modal parameters. The occurrence of missed and false damagealerts is effectively suppressed.
arxiv-10800-217 | Fast Imbalanced Classification of Healthcare Data with Missing Values | http://arxiv.org/abs/1503.06250 | author:Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nick Marko category:stat.ML cs.LG published:2015-03-21 summary:In medical domain, data features often contain missing values. This cancreate serious bias in the predictive modeling. Typical standard data miningmethods often produce poor performance measures. In this paper, we propose anew method to simultaneously classify large datasets and reduce the effects ofmissing values. The proposed method is based on a multilevel framework of thecost-sensitive SVM and the expected maximization imputation method for missingvalues, which relies on iterated regression analyses. We compare classificationresults of multilevel SVM-based algorithms on public benchmark datasets withimbalanced classes and missing values as well as real data in healthapplications, and show that our multilevel SVM-based method produces fast, andmore accurate and robust classification results.
arxiv-10800-218 | Wavelet based approach for tissue fractal parameter measurement: Pre cancer detection | http://arxiv.org/abs/1503.06323 | author:Sabyasachi Mukhopadhyay, Nandan K. Das, Soham Mandal, Sawon Pratiher, Asish Mitra, Asima Pradhan, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV published:2015-03-21 summary:In this paper, we have carried out the detail studies of pre-cancer bywavelet coherency and multifractal based detrended fluctuation analysis (MFDFA)on differential interference contrast (DIC) images of stromal region amongdifferent grades of pre-cancer tissues. Discrete wavelet transform (DWT)through Daubechies basis has been performed for identifying fluctuations overpolynomial trends for clear characterization and differentiation of tissues.Wavelet coherence plots are performed for identifying the level of correlationin time scale plane between normal and various grades of DIC samples. ApplyingMFDFA on refractive index variations of cervical tissues, we have observed thatthe values of Hurst exponent (correlation) decreases from healthy (normal) topre-cancer tissues. The width of singularity spectrum has a sudden degradationat grade-I in comparison of healthy (normal) tissue but later on it increasesas cancer progresses from grade-II to grade-III.
arxiv-10800-219 | Boosting Convolutional Features for Robust Object Proposals | http://arxiv.org/abs/1503.06350 | author:Nikolaos Karianakis, Thomas J. Fuchs, Stefano Soatto category:cs.CV cs.AI cs.LG published:2015-03-21 summary:Deep Convolutional Neural Networks (CNNs) have demonstrated excellentperformance in image classification, but still show room for improvement inobject-detection tasks with many categories, in particular for cluttered scenesand occlusion. Modern detection algorithms like Regions with CNNs (Girshick etal., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regionswhich with high probability represent objects, where in turn CNNs are deployedfor classification. Selective Search represents a family of sophisticatedalgorithms that are engineered with multiple segmentation, appearance andsaliency cues, typically coming with a significant run-time overhead.Furthermore, (Hosang et al., 2014) have shown that most methods suffer from lowreproducibility due to unstable superpixels, even for slight imageperturbations. Although CNNs are subsequently used for classification intop-performing object-detection pipelines, current proposal methods areagnostic to how these models parse objects and their rich learnedrepresentations. As a result they may propose regions which may not resemblehigh-level objects or totally miss some of them. To overcome these drawbacks wepropose a boosting approach which directly takes advantage of hierarchical CNNfeatures for detecting regions of interest fast. We demonstrate its performanceon ImageNet 2013 detection benchmark and compare it with state-of-the-artmethods.
arxiv-10800-220 | Using novelty-biased GA to sample diversity in graphs satisfying constraints | http://arxiv.org/abs/1503.06342 | author:Peter Overbury, Luc Berthouze category:physics.soc-ph cs.NE cs.SI math.CO G.2.2 published:2015-03-21 summary:The structure of the network underlying many complex systems, whetherartificial or natural, plays a significant role in how these systems operate.As a result, much emphasis has been placed on accurately describing networksusing network theoretic metrics. When it comes to generating networks withsimilar properties, however, the set of available techniques and propertiesthat can be controlled for remains limited. Further, whilst it is becomingclear that some of the metrics currently used to control the generation of suchnetworks are not very prescriptive so that networks could potentially exhibitvery different higher-order structure within those constraints, networkgenerating algorithms typically produce fairly contrived networks and lackmechanisms by which to systematically explore the space of network solutions.In this paper, we explore the potential of a multi-objective novelty-biased GAto provide a viable alternative to these algorithms. We believe our resultsprovide the first proof of principle that (i) it is possible to use GAs togenerate graphs satisfying set levels of key classical graph theoreticproperties and (ii) it is possible to generate diverse solutions within theseconstraints. The paper is only a preliminary step, however, and we identify keyavenues for further development.
arxiv-10800-221 | Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies | http://arxiv.org/abs/1503.06169 | author:Shaojie Tang, Yaqin Zhou category:cs.LG published:2015-03-20 summary:In this paper, we investigate a largely extended version of classical MABproblem, called networked combinatorial bandit problems. In particular, weconsider the setting of a decision maker over a networked bandits as follows:each time a combinatorial strategy, e.g., a group of arms, is chosen, and thedecision maker receives a reward resulting from her strategy and also receivesa side bonus resulting from that strategy for each arm's neighbor. This ismotivated by many real applications such as on-line social networks wherefriends can provide their feedback on shared content, therefore if we promote aproduct to a user, we can also collect feedback from her friends on thatproduct. To this end, we consider two types of side bonus in this study: sideobservation and side reward. Upon the number of arms pulled at each time slot,we study two cases: single-play and combinatorial-play. Consequently, thisleaves us four scenarios to investigate in the presence of side bonus:Single-play with Side Observation, Combinatorial-play with Side Observation,Single-play with Side Reward, and Combinatorial-play with Side Reward. For eachcase, we present and analyze a series of \emph{zero regret} polices where theexpect of regret over time approaches zero as time goes to infinity. Extensivesimulations validate the effectiveness of our results.
arxiv-10800-222 | On measuring linguistic intelligence | http://arxiv.org/abs/1503.06151 | author:Maxim Litvak category:cs.CL published:2015-03-20 summary:This work addresses the problem of measuring how many languages a person"effectively" speaks given that some of the languages are close to each other.In other words, to assign a meaningful number to her language portfolio.Intuition says that someone who speaks fluently Spanish and Portuguese islinguistically less proficient compared to someone who speaks fluently Spanishand Chinese since it takes more effort for a native Spanish speaker to learnChinese than Portuguese. As the number of languages grows and their proficiencylevels vary, it gets even more complicated to assign a score to a languageportfolio. In this article we propose such a measure ("linguistic quotient" -LQ) that can account for these effects. We define the properties that such a measure should have. They are based onthe idea of coherent risk measures from the mathematical finance. Having laiddown the foundation, we propose one such a measure together with the algorithmthat works on languages classification tree as input. The algorithm together with the input is available online at lingvometer.com
arxiv-10800-223 | Block-Wise MAP Inference for Determinantal Point Processes with Application to Change-Point Detection | http://arxiv.org/abs/1503.06239 | author:Jinye Zhang, Zhijian Ou category:cs.LG cs.AI stat.ME stat.ML published:2015-03-20 summary:Existing MAP inference algorithms for determinantal point processes (DPPs)need to calculate determinants or conduct eigenvalue decomposition generally atthe scale of the full kernel, which presents a great challenge for real-worldapplications. In this paper, we introduce a class of DPPs, called BwDPPs, thatare characterized by an almost block diagonal kernel matrix and thus can allowefficient block-wise MAP inference. Furthermore, BwDPPs are successfullyapplied to address the difficulty of selecting change-points in the problem ofchange-point detection (CPD), which results in a new BwDPP-based CPD method,named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates isfirst created based on existing well-studied metrics. Then, these change-pointcandidates are treated as DPP items, and DPP-based subset selection isconducted to give the final estimate of the change-points that favours bothquality and diversity. The effectiveness of BwDppCpd is demonstrated throughextensive experiments on five real-world datasets.
arxiv-10800-224 | Country-scale Exploratory Analysis of Call Detail Records through the Lens of Data Grid Models | http://arxiv.org/abs/1503.06060 | author:Romain Guigourès, Dominique Gay, Marc Boullé, Fabrice Clérot, Fabrice Rossi category:cs.DB stat.ML published:2015-03-20 summary:Call Detail Records (CDRs) are data recorded by telecommunications companies,consisting of basic informations related to several dimensions of the callsmade through the network: the source, destination, date and time of calls. CDRsdata analysis has received much attention in the recent years since it mightreveal valuable information about human behavior. It has shown high added valuein many application domains like e.g., communities analysis or networkplanning. In this paper, we suggest a generic methodology for summarizinginformation contained in CDRs data. The method is based on a parameter-freeestimation of the joint distribution of the variables that describe the calls.We also suggest several well-founded criteria that allows one to browse thesummary at various granularities and to explore the summary by means ofinsightful visualizations. The method handles network graph data, temporalsequence data as well as user mobility data stemming from original CDRs data.We show the relevance of our methodology for various case studies on real-worldCDRs data from Ivory Coast.
arxiv-10800-225 | Deep Transform: Cocktail Party Source Separation via Probabilistic Re-Synthesis | http://arxiv.org/abs/1503.06046 | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-03-20 summary:In cocktail party listening scenarios, the human brain is able to separatecompeting speech signals. However, the signal processing implemented by thebrain to perform cocktail party listening is not well understood. Here, wetrained two separate convolutive autoencoder deep neural networks (DNN) toseparate monaural and binaural mixtures of two concurrent speech streams. Wethen used these DNNs as convolutive deep transform (CDT) devices to performprobabilistic re-synthesis. The CDTs operated directly in the time-domain. Oursimulations demonstrate that very simple neural networks are capable ofexploiting monaural and binaural information available in a cocktail partylistening scenario.
arxiv-10800-226 | Feeder Load Balancing using Neural Network | http://arxiv.org/abs/1503.06004 | author:A. Ukil, W. Siti, J. Jordaan category:cs.NE published:2015-03-20 summary:The distribution system problems, such as planning, loss minimization, andenergy restoration, usually involve the phase balancing or networkreconfiguration procedures. The determination of an optimal phase balance is,in general, a combinatorial optimization problem. This paper proposes optimalreconfiguration of the phase balancing using the neural network, to switch onand off the different switches, allowing the three phases supply by thetransformer to the end-users to be balanced. This paper presents theapplication examples of the proposed method using the real and simulated testdata.
arxiv-10800-227 | A Bennett Inequality for the Missing Mass | http://arxiv.org/abs/1503.06134 | author:Bahman Yari Saeed Khanloo category:stat.ML published:2015-03-20 summary:Novel concentration inequalities are obtained for the missing mass, i.e. thetotal probability mass of the outcomes not observed in the sample. We derivedistribution-free deviation bounds with sublinear exponents in deviation sizefor missing mass and improve the results of Berend and Kontorovich (2013) andYari Saeed Khanloo and Haffari (2015) for small deviations which is the mostimportant case in learning theory.
arxiv-10800-228 | Sequential Monte Carlo Methods for System Identification | http://arxiv.org/abs/1503.06058 | author:Thomas B. Schön, Fredrik Lindsten, Johan Dahlin, Johan Wågberg, Christian A. Naesseth, Andreas Svensson, Liang Dai category:stat.CO math.OC stat.ML published:2015-03-20 summary:One of the key challenges in identifying nonlinear and possibly non-Gaussianstate space models (SSMs) is the intractability of estimating the system state.Sequential Monte Carlo (SMC) methods, such as the particle filter (introducedmore than two decades ago), provide numerical solutions to the nonlinear stateestimation problems arising in SSMs. When combined with additionalidentification techniques, these algorithms provide solid solutions to thenonlinear system identification problem. We describe two general strategies forcreating such combinations and discuss why SMC is a natural tool forimplementing these strategies.
arxiv-10800-229 | Nonparametric Estimation of Band-limited Probability Density Functions | http://arxiv.org/abs/1503.06236 | author:Rahul Agarwal, Zhe Chen, Sridevi V. Sarma category:stat.ML math.ST stat.ME stat.TH published:2015-03-20 summary:In this paper, a nonparametric maximum likelihood (ML) estimator forband-limited (BL) probability density functions (pdfs) is proposed. The BLMLestimator is consistent and computationally efficient. To compute the BLMLestimator, three approximate algorithms are presented: a binary quadraticprogramming (BQP) algorithm for medium scale problems, a Trivial algorithm forlarge-scale problems that yields a consistent estimate if the underlying pdf isstrictly positive and BL, and a fast implementation of the Trivial algorithmthat exploits the band-limited assumption and the Nyquist sampling theorem("BLMLQuick"). All three BLML estimators outperform kernel density estimation(KDE) algorithms (adaptive and higher order KDEs) with respect to the meanintegrated squared error for data generated from both BL and infinite-bandpdfs. Further, the BLMLQuick estimate is remarkably faster than the KDalgorithms. Finally, the BLML method is applied to estimate the conditionalintensity function of a neuronal spike train (point process) recorded from arat's entorhinal cortex grid cell, for which it outperforms state-of-the-artestimators used in neuroscience.
arxiv-10800-230 | On Invariance and Selectivity in Representation Learning | http://arxiv.org/abs/1503.05938 | author:Fabio Anselmi, Lorenzo Rosasco, Tomaso Poggio category:cs.LG published:2015-03-19 summary:We discuss data representation which can be learned automatically from data,are invariant to transformations, and at the same time selective, in the sensethat two points have the same representation only if they are one thetransformation of the other. The mathematical results here sharpen some of thekey claims of i-theory -- a recent theory of feedforward processing in sensorycortex.
arxiv-10800-231 | Syntagma Lexical Database | http://arxiv.org/abs/1503.05907 | author:Daniel Christen category:cs.CL published:2015-03-19 summary:This paper discusses the structure of Syntagma's Lexical Database (focused onItalian). The basic database consists in four tables. Table Forms contains wordinflections, used by the POS-tagger for the identification of input-words.Forms is related to Lemma. Table Lemma stores all kinds of grammatical featuresof words, word-level semantic data and restrictions. In the table Meaningsmeaning-related data are stored: definition, examples, domain, and semanticinformation. Table Valency contains the argument structure of each meaning,with syntactic and semantic features for each argument. The extended version ofSLD contains the links to Syntagma's Semantic Net and to the WordNet synsets ofother languages.
arxiv-10800-232 | Building Statistical Shape Spaces for 3D Human Modeling | http://arxiv.org/abs/1503.05860 | author:Leonid Pishchulin, Stefanie Wuhrer, Thomas Helten, Christian Theobalt, Bernt Schiele category:cs.CV published:2015-03-19 summary:Statistical models of 3D human shape and pose learned from scan databaseshave developed into valuable tools to solve a variety of vision and graphicsproblems. Unfortunately, most publicly available models are of limitedexpressiveness as they were learned on very small databases that hardly reflectthe true variety in human body shapes. In this paper, we contribute byrebuilding a widely used statistical body representation from the largestcommercially available scan database, and making the resulting model availableto the community (visit http://humanshape.mpi-inf.mpg.de). As preprocessingseveral thousand scans for learning the model is a challenge in itself, wecontribute by developing robust best practice solutions for scan alignment thatquantitatively lead to the best learned models. We make implementations ofthese preprocessing steps also publicly available. We extensively evaluate theimproved accuracy and generality of our new model, and show its improvedperformance for human body reconstruction from sparse input data.
arxiv-10800-233 | Deep Transform: Time-Domain Audio Error Correction via Probabilistic Re-Synthesis | http://arxiv.org/abs/1503.05849 | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-03-19 summary:In the process of recording, storage and transmission of time-domain audiosignals, errors may be introduced that are difficult to correct in anunsupervised way. Here, we train a convolutional deep neural network tore-synthesize input time-domain speech signals at its output layer. We then usethis abstract transformation, which we call a deep transform (DT), to performprobabilistic re-synthesis on further speech (of the same speaker) which hasbeen degraded. Using the convolutive DT, we demonstrate the recovery of speechaudio that has been subject to extreme degradation. This approach may be usefulfor correction of errors in communications devices.
arxiv-10800-234 | Rank Subspace Learning for Compact Hash Codes | http://arxiv.org/abs/1503.05951 | author:Kai Li, Guojun Qi, Jun Ye, Kien A. Hua category:cs.LG cs.IR I.2.6; H.3.3 published:2015-03-19 summary:The era of Big Data has spawned unprecedented interests in developing hashingalgorithms for efficient storage and fast nearest neighbor search. Mostexisting work learn hash functions that are numeric quantizations of featurevalues in projected feature space. In this work, we propose a novel hashlearning framework that encodes feature's rank orders instead of numeric valuesin a number of optimal low-dimensional ranking subspaces. We formulate theranking subspace learning problem as the optimization of a piece-wise linearconvex-concave function and present two versions of our algorithm: one withindependent optimization of each hash bit and the other exploiting a sequentiallearning framework. Our work is a generalization of the Winner-Take-All (WTA)hash family and naturally enjoys all the numeric stability benefits of rankcorrelation measures while being optimized to achieve high precision at veryshort code length. We compare with several state-of-the-art hashing algorithmsin both supervised and unsupervised domain, showing superior performance in anumber of data sets.
arxiv-10800-235 | Neural Network-Based Active Learning in Multivariate Calibration | http://arxiv.org/abs/1503.05831 | author:A. Ukil, J. Bernasconi category:cs.NE cs.CE cs.LG published:2015-03-19 summary:In chemometrics, data from infrared or near-infrared (NIR) spectroscopy areoften used to identify a compound or to analyze the composition of amaterial.This involves the calibration of models that predict the concentrationofmaterial constituents from the measured NIR spectrum. An interesting aspectof multivariate calibration is to achieve a particular accuracy level with aminimum number of training samples, as this reduces the number of laboratorytests and thus the cost of model building. In these chemometric models, theinput refers to a proper representation of the spectra and the output to theconcentrations of the sample constituents. The search for a most informativenew calibration sample thus has to be performed in the output space of themodel, rather than in the input space as in conventionalmodeling problems. Inthis paper, we propose to solve the corresponding inversion problem byutilizing the disagreements of an ensemble of neural networks to represent theprediction error in the unexplored component space. The next calibration sampleis then chosen at a composition where the individual models of the ensembledisagree most. The results obtained for a realistic chemometric calibrationexample show that the proposed active learning can achieve a given calibrationaccuracy with less training samples than random sampling.
arxiv-10800-236 | Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression Algorithm | http://arxiv.org/abs/1503.05947 | author:Yanlai Chen category:math.NA cs.AI cs.CV cs.NA published:2015-03-19 summary:Dimension reduction is often needed in the area of data mining. The goal ofthese methods is to map the given high-dimensional data into a low-dimensionalspace preserving certain properties of the initial data. There are two kinds oftechniques for this purpose. The first, projective methods, builds an explicitlinear projection from the high-dimensional space to the low-dimensional one.On the other hand, the nonlinear methods utilizes nonlinear and implicitmapping between the two spaces. In both cases, the methods considered inliterature have usually relied on computationally very intensive matrixfactorizations, frequently the Singular Value Decomposition (SVD). Thecomputational burden of SVD quickly renders these dimension reduction methodsinfeasible thanks to the ever-increasing sizes of the practical datasets. In this paper, we present a new decomposition strategy, Reduced BasisDecomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given$X$ the high-dimensional data, the method approximates it by $Y \, T (\approxX)$ with $Y$ being the low-dimensional surrogate and $T$ the transformationmatrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. Infact, it is significantly faster than SVD with comparable accuracy. $T$ can becomputed on the fly. Moreover, unlike many compression algorithms, it easilyfinds the mapping for an arbitrary ``out-of-sample'' vector and it comes withan ``error indicator'' certifying the accuracy of the compression. Numericalresults are shown validating these claims.
arxiv-10800-237 | Sign Language Fingerspelling Classification from Depth and Color Images using a Deep Belief Network | http://arxiv.org/abs/1503.05830 | author:Lucas Rioux-Maldague, Philippe Giguère category:cs.CV published:2015-03-19 summary:Automatic sign language recognition is an open problem that has received alot of attention recently, not only because of its usefulness to signers, butalso due to the numerous applications a sign classifier can have. In thisarticle, we present a new feature extraction technique for hand poserecognition using depth and intensity images captured from a Microsoft Kinectsensor. We applied our technique to American Sign Language fingerspellingclassification using a Deep Belief Network, for which our feature extractiontechnique is tailored. We evaluated our results on a multi-user data set withtwo scenarios: one with all known users and one with an unseen user. Weachieved 99% recall and precision on the first, and 77% recall and 79%precision on the second. Our method is also capable of real-time signclassification and is adaptive to any environment or lightning intensity.
arxiv-10800-238 | On learning optimized reaction diffusion processes for effective image restoration | http://arxiv.org/abs/1503.05768 | author:Yunjin Chen, Wei Yu, Thomas Pock category:cs.CV published:2015-03-19 summary:For several decades, image restoration remains an active research topic inlow-level computer vision and hence new approaches are constantly emerging.However, many recently proposed algorithms achieve state-of-the-art performanceonly at the expense of very high computation time, which clearly limits theirpractical relevance. In this work, we propose a simple but effective approachwith both high computational efficiency and high restoration quality. We extendconventional nonlinear reaction diffusion models by several parametrized linearfilters as well as several parametrized influence functions. We propose totrain the parameters of the filters and the influence functions through a lossbased approach. Experiments show that our trained nonlinear reaction diffusionmodels largely benefit from the training of the parameters and finally lead tothe best reported performance on common test datasets for image restoration.Due to their structural simplicity, our trained models are highly efficient andare also well-suited for parallel computation on GPUs.
arxiv-10800-239 | Phrase database Approach to structural and semantic disambiguation in English-Korean Machine Translation | http://arxiv.org/abs/1503.05626 | author:Myong-Chol Pak category:cs.CL published:2015-03-19 summary:In machine translation it is common phenomenon that machine-readabledictionaries and standard parsing rules are not enough to ensure accuracy inparsing and translating English phrases into Korean language, which is revealedin misleading translation results due to consequent structural and semanticambiguities. This paper aims to suggest a solution to structural and semanticambiguities due to the idiomaticity and non-grammaticalness of phrases commonlyused in English language by applying bilingual phrase database inEnglish-Korean Machine Translation (EKMT). This paper firstly clarifies whatthe phrase unit in EKMT is based on the definition of the English phrase,secondly clarifies what kind of language unit can be the target of the phrasedatabase for EKMT, thirdly suggests a way to build the phrase database bypresenting the format of the phrase database with examples, and finallydiscusses briefly the method to apply this bilingual phrase database to theEKMT for structural and semantic disambiguation.
arxiv-10800-240 | A General Framework for Multi-focal Image Classification and Authentication: Application to Microscope Pollen Images | http://arxiv.org/abs/1503.05786 | author:François Chung, Tomás Rodríguez category:cs.CV published:2015-03-19 summary:In this article, we propose a general framework for multi-focal imageclassification and authentication, the methodology being demonstrated onmicroscope pollen images. The framework is meant to be generic and based on abrute force-like approach aimed to be efficient not only on any kind, and anynumber, of pollen images (regardless of the pollen type), but also on any kindof multi-focal images. All stages of the framework's pipeline are designed tobe used in an automatic fashion. First, the optimal focus is selected using theabsolute gradient method. Then, pollen grains are extracted using acoarse-to-fine approach involving both clustering and morphological techniques(coarse stage), and a snake-based segmentation (fine stage). Finally, featuresare extracted and selected using a generalized approach, and theirclassification is tested with four classifiers: Weighted Neighbor Distance,Neural Network, Decision Tree and Random Forest. The latter method, which hasshown the best and more robust classification accuracy results (above 97\% forany number of pollen types), is finally used for the authentication stage.
arxiv-10800-241 | Optimizing Neural Networks with Kronecker-factored Approximate Curvature | http://arxiv.org/abs/1503.05671 | author:James Martens, Roger Grosse category:cs.LG cs.NE stat.ML published:2015-03-19 summary:We propose an efficient method for approximating natural gradient descent inneural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).K-FAC is based on an efficiently invertible approximation of a neural network'sFisher information matrix which is neither diagonal nor low-rank, and in somecases is completely non-sparse. It is derived by approximating various largeblocks of the Fisher (corresponding to entire layers) as being the Kroneckerproduct of two much smaller matrices. While only several times more expensiveto compute than the plain stochastic gradient, the updates produced by K-FACmake much more progress optimizing the objective, which results in an algorithmthat can be much faster than stochastic gradient descent with momentum inpractice. And unlike some previously proposed approximatenatural-gradient/Newton methods which use high-quality non-diagonal curvaturematrices (such as Hessian-free optimization), K-FAC works very well in highlystochastic optimization regimes. This is because the cost of storing andinverting K-FAC's approximation to the curvature matrix does not depend on theamount of data used to estimate it, which is a feature typically associatedonly with diagonal or low-rank approximations to the curvature matrix.
arxiv-10800-242 | Learning Hypergraph-regularized Attribute Predictors | http://arxiv.org/abs/1503.05782 | author:Sheng Huang, Mohamed Elhoseiny, Ahmed Elgammal, Dan Yang category:cs.CV cs.LG published:2015-03-19 summary:We present a novel attribute learning framework named Hypergraph-basedAttribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict theattribute relations in the data. Then the attribute prediction problem iscasted as a regularized hypergraph cut problem in which HAP jointly learns acollection of attribute projections from the feature space to a hypergraphembedding space aligned with the attribute space. The learned projectionsdirectly act as attribute classifiers (linear and kernelized). This formulationleads to a very efficient approach. By considering our model as a multi-graphcut task, our framework can flexibly incorporate other available information,in particular class label. We apply our approach to attribute prediction,Zero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUBdatabases demonstrate the value of our methods in comparison with thestate-of-the-art approaches.
arxiv-10800-243 | Automatic Pollen Grain and Exine Segmentation from Microscope Images | http://arxiv.org/abs/1503.05767 | author:François Chung, Tomás Rodríguez category:cs.CV published:2015-03-19 summary:In this article, we propose an automatic method for the segmentation ofpollen grains from microscope images, followed by the automatic segmentation oftheir exine. The objective of exine segmentation is to separate the pollengrain in two regions of interest: exine and inner part. A coarse-to-fineapproach ensures a smooth and accurate segmentation of both structures. As arough stage, grain segmentation is performed by a procedure involvingclustering and morphological operations, while the exine is approximated by aniterative procedure consisting in consecutive cropping steps of the pollengrain. A snake-based segmentation is performed to refine the segmentation ofboth structures. Results have shown that our segmentation method is able todeal with different pollen types, as well as with different types of exine andinner part appearance. The proposed segmentation method aims to be generic andhas been designed as one of the core steps of an automatic pollenclassification framework.
arxiv-10800-244 | Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning | http://arxiv.org/abs/1503.05743 | author:Ken Miura, Tatsuya Harada category:cs.DC cs.LG cs.MS cs.NE stat.ML published:2015-03-19 summary:Deep learning can achieve outstanding results in various fields. However, itrequires so significant computational power that graphics processing units(GPUs) and/or numerous computers are often required for the practicalapplication. We have developed a new distributed calculation framework called"Sashimi" that allows any computer to be used as a distribution node only byaccessing a website. We have also developed a new JavaScript neural networkframework called "Sukiyaki" that uses general purpose GPUs with web browsers.Sukiyaki performs 30 times faster than a conventional JavaScript library fordeep convolutional neural networks (deep CNNs) learning. The combination ofSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates thedistributed deep learning of deep CNNs only with web browsers on variousdevices. The libraries that comprise the proposed methods are available underMIT license at http://mil-tokyo.github.io/.
arxiv-10800-245 | A Neural Transfer Function for a Smooth and Differentiable Transition Between Additive and Multiplicative Interactions | http://arxiv.org/abs/1503.05724 | author:Sebastian Urban, Patrick van der Smagt category:stat.ML cs.LG cs.NE published:2015-03-19 summary:Existing approaches to combine both additive and multiplicative neural unitseither use a fixed assignment of operations or require discrete optimization todetermine what function a neuron should perform. This leads either to aninefficient distribution of computational resources or an extensive increase inthe computational complexity of the training procedure. We present a novel, parameterizable transfer function based on themathematical concept of non-integer functional iteration that allows theoperation each neuron performs to be smoothly and, most importantly,differentiablely adjusted between addition and multiplication. This allows thedecision between addition and multiplication to be integrated into the standardbackpropagation training procedure.
arxiv-10800-246 | An approach to improving edge detection for facial and remotely sensed images using vector order statistics | http://arxiv.org/abs/1503.05692 | author:B O. Sadiq, S. M. Sani, S. Garba category:cs.CV published:2015-03-19 summary:This paper presents an improved edge detection algorithm for facial andremotely sensed images using vector order statistics. The developed algorithmprocesses colored images directly without been converted to gray scale. Anumber of the existing algorithms converts the colored images into gray scalebefore detection of edges. But this process leads to inaccurate precision ofrecognized edges, thus producing false and broken edges in the output edge map.Facial and remotely sensed images consist of curved edge lines which have to bedetected continuously to prevent broken edges. In order to deal with this, acollection of pixel approach is introduced with a view to minimizing the falseand broken edges that exists in the generated output edge map of facial andremotely sensed images.
arxiv-10800-247 | Edge Detection: A Collection of Pixel based Approach for Colored Images | http://arxiv.org/abs/1503.05689 | author:B. O Sadiq, S. M Sani, S Garba category:cs.CV published:2015-03-19 summary:The existing traditional edge detection algorithms process a single pixel onan image at a time, thereby calculating a value which shows the edge magnitudeof the pixel and the edge orientation. Most of these existing algorithmsconvert the coloured images into gray scale before detection of edges. However,this process leads to inaccurate precision of recognized edges, thus producingfalse and broken edges in the image. This paper presents a profile modellingscheme for collection of pixels based on the step and ramp edges, with a viewto reducing the false and broken edges present in the image. The collection ofpixel scheme generated is used with the Vector Order Statistics to reduce theimprecision of recognized edges when converting from coloured to gray scaleimages. The Pratt Figure of Merit (PFOM) is used as a quantitative comparisonbetween the existing traditional edge detection algorithm and the developedalgorithm as a means of validation. The PFOM value obtained for the developedalgorithm is 0.8480, which showed an improvement over the existing traditionaledge detection algorithms.
arxiv-10800-248 | Non-parametric Bayesian Models of Response Function in Dynamic Image Sequences | http://arxiv.org/abs/1503.05684 | author:Ondřej Tichý, Václav Šmídl category:stat.ML published:2015-03-19 summary:Estimation of response functions is an important task in dynamic medicalimaging. This task arises for example in dynamic renal scintigraphy, whereimpulse response or retention functions are estimated, or in functionalmagnetic resonance imaging where hemodynamic response functions are required.These functions can not be observed directly and their estimation iscomplicated because the recorded images are subject to superposition ofunderlying signals. Therefore, the response functions are estimated via blindsource separation and deconvolution. Performance of this algorithm heavilydepends on the used models of the response functions. Response functions inreal image sequences are rather complicated and finding a suitable parametricform is problematic. In this paper, we study estimation of the responsefunctions using non-parametric Bayesian priors. These priors were designed tofavor desirable properties of the functions, such as sparsity or smoothness.These assumptions are used within hierarchical priors of the blind sourceseparation and deconvolution algorithm. Comparison of the resulting algorithmswith these priors is performed on synthetic dataset as well as on real datasetsfrom dynamic renal scintigraphy. It is shown that flexible non-parametricpriors improve estimation of response functions in both cases. MATLABimplementation of the resulting algorithms is freely available for download.
arxiv-10800-249 | Learning to Search for Dependencies | http://arxiv.org/abs/1503.05615 | author:Kai-Wei Chang, He He, Hal Daumé III, John Langford category:cs.CL cs.LG published:2015-03-18 summary:We demonstrate that a dependency parser can be built using a creditassignment compiler which removes the burden of worrying about low-levelmachine learning details from the parser implementation. The result is a simpleparser which robustly applies to many languages that provides similarstatistical and computational performance with best-to-date transition-basedparsing approaches, while avoiding various downsides including randomization,extra feature requirements, and custom learning algorithms.
arxiv-10800-250 | What Properties are Desirable from an Electron Microscopy Segmentation Algorithm | http://arxiv.org/abs/1503.05430 | author:Toufiq Parag category:cs.CV published:2015-03-18 summary:The prospect of neural reconstruction from Electron Microscopy (EM) imageshas been elucidated by the automatic segmentation algorithms. Althoughsegmentation algorithms eliminate the necessity of tracing the neurons by hand,significant manual effort is still essential for correcting the mistakes theymake. A considerable amount of human labor is also required for annotatinggroundtruth volumes for training the classifiers of a segmentation framework.It is critically important to diminish the dependence on human interaction inthe overall reconstruction system. This study proposes a novel classifiertraining algorithm for EM segmentation aimed to reduce the amount of manualeffort demanded by the groundtruth annotation and error refinement tasks.Instead of using an exhaustive pixel level groundtruth, an active learningalgorithm is proposed for sparse labeling of pixel and boundaries ofsuperpixels. Because over-segmentation errors are in general more tolerable andeasier to correct than the under-segmentation errors, our algorithm is designedto prioritize minimization of false-merges over false-split mistakes. Ourexperiments on both 2D and 3D data suggest that the proposed method yieldssegmentation outputs that are more amenable to neural reconstruction than thoseof existing methods.
arxiv-10800-251 | The Knowledge Gradient Policy Using A Sparse Additive Belief Model | http://arxiv.org/abs/1503.05567 | author:Yan Li, Han Liu, Warren Powell category:stat.ML cs.IT cs.SY math.IT published:2015-03-18 summary:We propose a sequential learning policy for noisy discrete globaloptimization and ranking and selection (R\&S) problems with high dimensionalsparse belief functions, where there are hundreds or even thousands offeatures, but only a small portion of these features contain explanatory power.We aim to identify the sparsity pattern and select the best alternative beforethe finite budget is exhausted. We derive a knowledge gradient policy forsparse linear models (KGSpLin) with group Lasso penalty. This policy is aunique and novel hybrid of Bayesian R\&S with frequentist learning.Particularly, our method naturally combines B-spline basis expansion andgeneralizes to the nonparametric additive model (KGSpAM) and functional ANOVAmodel. Theoretically, we provide the estimation error bounds of the posteriormean estimate and the functional estimate. Controlled experiments show that thealgorithm efficiently learns the correct set of nonzero parameters even whenthe model is imbedded with hundreds of dummy parameters. Also it outperformsthe knowledge gradient for a linear model.
arxiv-10800-252 | Text Segmentation based on Semantic Word Embeddings | http://arxiv.org/abs/1503.05543 | author:Alexander A Alemi, Paul Ginsparg category:cs.CL cs.IR published:2015-03-18 summary:We explore the use of semantic word embeddings in text segmentationalgorithms, including the C99 segmentation algorithm and new algorithmsinspired by the distributed word vector representation. By developing a generalframework for discussing a class of segmentation objectives, we study theeffectiveness of greedy versus exact optimization approaches and suggest a newiterative refinement technique for improving the performance of greedystrategies. We compare our results to known benchmarks, using known metrics. Wedemonstrate state-of-the-art performance for an untrained method with ourContent Vector Segmentation (CVS) on the Choi test set. Finally, we apply thesegmentation procedure to an in-the-wild dataset consisting of text extractedfrom scholarly articles in the arXiv.org database.
arxiv-10800-253 | Differentiating the multipoint Expected Improvement for optimal batch design | http://arxiv.org/abs/1503.05509 | author:Sébastien Marmin, Clément Chevalier, David Ginsbourger category:stat.ML math.ST stat.TH published:2015-03-18 summary:This work deals with parallel optimization of expensive objective functionswhich are modeled as sample realizations of Gaussian processes. The study isformalized as a Bayesian optimization problem, or continuous multi-armed banditproblem, where a batch of q \textgreater{} 0 arms is pulled in parallel at eachiteration. Several algorithms have been developed for choosing batches bytrading off exploitation and exploration. As of today, the maximum ExpectedImprovement (EI) and Upper Confidence Bound (UCB) selection rules appear as themost prominent approaches for batch selection. Here, we build upon recent workon the multipoint Expected Improvement criterion, for which an analyticexpansion relying on Tallis' formula was recently established. Thecomputational burden of this selection rule being still an issue inapplication, we derive a closed-form expression for the gradient of themultipoint Expected Improvement, which aims at facilitating its maximizationusing gradient-based ascent algorithms. Substantial computational savings areshown in application. In addition, our algorithms are tested numerically andcompared to state-of-the-art UCB-based batch-sequential algorithms. Combiningstarting designs relying on UCB with gradient-based EI local optimizationfinally appears as a sound option for batch design in distributed GaussianProcess optimization.
arxiv-10800-254 | Interpretable Aircraft Engine Diagnostic via Expert Indicator Aggregation | http://arxiv.org/abs/1503.05526 | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG math.ST stat.AP stat.TH published:2015-03-18 summary:Detecting early signs of failures (anomalies) in complex systems is one ofthe main goal of preventive maintenance. It allows in particular to avoidactual failures by (re)scheduling maintenance operations in a way thatoptimizes maintenance costs. Aircraft engine health monitoring is onerepresentative example of a field in which anomaly detection is crucial.Manufacturers collect large amount of engine related data during flights whichare used, among other applications, to detect anomalies. This articleintroduces and studies a generic methodology that allows one to build automaticearly signs of anomaly detection in a way that builds upon human expertise andthat remains understandable by human operators who make the final maintenancedecision. The main idea of the method is to generate a very large number ofbinary indicators based on parametric anomaly scores designed by experts,complemented by simple aggregations of those scores. A feature selection methodis used to keep only the most discriminant indicators which are used as inputsof a Naive Bayes classifier. This give an interpretable classifier based oninterpretable anomaly detectors whose parameters have been optimized indirectlyby the selection process. The proposed methodology is evaluated on simulateddata designed to reproduce some of the anomaly types observed in real worldengines.
arxiv-10800-255 | Nonparametric Detection of Nonlinearly Mixed Pixels and Endmember Estimation in Hyperspectral Images | http://arxiv.org/abs/1503.05521 | author:Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard, Jean-Yves Tourneret category:cs.CV published:2015-03-18 summary:Mixing phenomena in hyperspectral images depend on a variety of factors suchas the resolution of observation devices, the properties of materials, and howthese materials interact with incident light in the scene. Different parametricand nonparametric models have been considered to address hyperspectral unmixingproblems. The simplest one is the linear mixing model. Nevertheless, it hasbeen recognized that mixing phenomena can also be nonlinear. The correspondingnonlinear analysis techniques are necessarily more challenging and complex thanthose employed for linear unmixing. Within this context, it makes sense todetect the nonlinearly mixed pixels in an image prior to its analysis, and thenemploy the simplest possible unmixing technique to analyze each pixel. In thispaper, we propose a technique for detecting nonlinearly mixed pixels. Thedetection approach is based on the comparison of the reconstruction errorsusing both a Gaussian process regression model and a linear regression model.The two errors are combined into a detection statistics for which a probabilitydensity function can be reasonably approximated. We also propose an iterativeendmember extraction algorithm to be employed in combination with the detectionalgorithm. The proposed Detect-then-Unmix strategy, which consists ofextracting endmembers, detecting nonlinearly mixed pixels and unmixing, istested with synthetic and real images.
arxiv-10800-256 | Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm | http://arxiv.org/abs/1503.05479 | author:Qinqing Zheng, Ryota Tomioka category:cs.LG cs.AI stat.ML published:2015-03-18 summary:We consider the problem of recovering a low-rank tensor from its noisyobservation. Previous work has shown a recovery guarantee with signal to noiseratio $O(n^{\lceil K/2 \rceil /2})$ for recovering a $K$th order rank onetensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper,we first improve this bound to $O(n^{K/4})$ by a much simpler approach, butwith a more careful analysis. Then we propose a new norm called the subspacenorm, which is based on the Kronecker products of factors obtained by theproposed simple estimator. The imposed Kronecker structure allows us to show anearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound, in which the parameter $H$controls the blend from the non-convex estimator to mode-wise nuclear normminimization. Furthermore, we empirically demonstrate that the subspace normachieves the nearly ideal denoising performance even with $H=O(1)$.
arxiv-10800-257 | Video Inpainting of Complex Scenes | http://arxiv.org/abs/1503.05528 | author:Alasdair Newson, Andrés Almansa, Matthieu Fradet, Yann Gousseau, Patrick Pérez category:cs.CV cs.MM math.NA published:2015-03-18 summary:We propose an automatic video inpainting algorithm which relies on theoptimisation of a global, patch-based functional. Our algorithm is able to dealwith a variety of challenging situations which naturally arise in videoinpainting, such as the correct reconstruction of dynamic textures, multiplemoving objects and moving background. Furthermore, we achieve this in an orderof magnitude less execution time with respect to the state-of-the-art. We arealso able to achieve good quality results on high definition videos. Finally,we provide specific algorithmic details to make implementation of our algorithmas easy as possible. The resulting algorithm requires no segmentation or manualinput other than the definition of the inpainting mask, and can deal with awider variety of situations than is handled by previous work. 1. Introduction.Advanced image and video editing techniques are increasingly common in theimage processing and computer vision world, and are also starting to be used inmedia entertainment. One common and difficult task closely linked to the worldof video editing is image and video " inpainting ". Generally speaking, this isthe task of replacing the content of an image or video with some other contentwhich is visually pleasing. This subject has been extensively studied in thecase of images, to such an extent that commercial image inpainting productsdestined for the general public are available, such as Photoshop's " ContentAware fill " [1]. However, while some impressive results have been obtained inthe case of videos, the subject has been studied far less extensively thanimage inpainting. This relative lack of research can largely be attributed tohigh time complexity due to the added temporal dimension. Indeed, it has onlyvery recently become possible to produce good quality inpainting results onhigh definition videos, and this only in a semi-automatic manner. Nevertheless,high-quality video inpainting has many important and useful applications suchas film restoration, professional post-production in cinema and video editingfor personal use. For this reason, we believe that an automatic, generic videoinpainting algorithm would be extremely useful for both academic andprofessional communities.
arxiv-10800-258 | Shared latent subspace modelling within Gaussian-Binary Restricted Boltzmann Machines for NIST i-Vector Challenge 2014 | http://arxiv.org/abs/1503.05471 | author:Danila Doroshin, Alexander Yamshinin, Nikolay Lubimov, Marina Nastasenko, Mikhail Kotov, Maxim Tkachenko category:cs.LG cs.NE cs.SD stat.ML 62M45 I.2.6; I.5.1 published:2015-03-18 summary:This paper presents a novel approach to speaker subspace modelling based onGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model isbased on the idea of shared factors as in the Probabilistic Linear DiscriminantAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,herein the speaker factor is shared over all vectors of the speaker. ThenMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.Various new scoring techniques for speaker verification using GRBM areproposed. The results for NIST i-vector Challenge 2014 dataset are presented.
arxiv-10800-259 | GSNs : Generative Stochastic Networks | http://arxiv.org/abs/1503.05571 | author:Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, Pascal Vincent category:cs.LG published:2015-03-18 summary:We introduce a novel training principle for probabilistic models that is analternative to maximum likelihood. The proposed Generative Stochastic Networks(GSN) framework is based on learning the transition operator of a Markov chainwhose stationary distribution estimates the data distribution. Because thetransition distribution is a conditional distribution generally involving asmall move, it has fewer dominant modes, being unimodal in the limit of smallmoves. Thus, it is easier to learn, more like learning to perform supervisedfunction approximation, with gradients that can be obtained byback-propagation. The theorems provided here generalize recent work on theprobabilistic interpretation of denoising auto-encoders and provide aninteresting justification for dependency networks and generalizedpseudolikelihood (along with defining an appropriate joint distribution andsampling mechanism, even when the conditionals are not consistent). We studyhow GSNs can be used with missing inputs and can be used to sample subsets ofvariables given the rest. Successful experiments are conducted, validatingthese theoretical results, on two image datasets and with a particulararchitecture that mimics the Deep Boltzmann Machine Gibbs sampler but allowstraining to proceed with backprop, without the need for layerwise pretraining.
arxiv-10800-260 | Efficient Machine Learning for Big Data: A Review | http://arxiv.org/abs/1503.05296 | author:O. Y. Al-Jarrah, P. D. Yoo, S Muhaidat, G. K. Karagiannidis, K. Taha category:cs.LG cs.AI published:2015-03-18 summary:With the emerging technologies and all associated devices, it is predictedthat massive amount of data will be created in the next few years, in fact, asmuch as 90% of current data were created in the last couple of years,a trendthat will continue for the foreseeable future. Sustainable computing studiesthe process by which computer engineer/scientist designs computers andassociated subsystems efficiently and effectively with minimal impact on theenvironment. However, current intelligent machine-learning systems areperformance driven, the focus is on the predictive/classification accuracy,based on known properties learned from the training samples. For instance, mostmachine-learning-based nonparametric models are known to require highcomputational cost in order to find the global optima. With the learning taskin a large dataset, the number of hidden nodes within the network willtherefore increase significantly, which eventually leads to an exponential risein computational complexity. This paper thus reviews the theoretical andexperimental data-modeling literature, in large-scale data-intensive fields,relating to: (1) model efficiency, including computational requirements inlearning, and data-intensive areas structure and design, and introduces (2) newalgorithmic approaches with the least memory requirements and processing tominimize computational cost, while maintaining/improving itspredictive/classification accuracy and stability.
arxiv-10800-261 | Improved Calibration of Near-Infrared Spectra by Using Ensembles of Neural Network Models | http://arxiv.org/abs/1503.05272 | author:A. Ukil, J. Bernasconi, H. Braendle, H. Buijs, S. Bonenfant category:cs.NE published:2015-03-18 summary:IR or near-infrared (NIR) spectroscopy is a method used to identify acompound or to analyze the composition of a material. Calibration of NIRspectra refers to the use of the spectra as multivariate descriptors to predictconcentrations of the constituents. To build a calibration model,state-of-the-art software predominantly uses linear regression techniques. Fornonlinear calibration problems, neural network-based models have proved to bean interesting alternative. In this paper, we propose a novel extension of theconventional neural network-based approach, the use of an ensemble of neuralnetwork models. The individual neural networks are obtained by resampling theavailable training data with bootstrapping or cross-validation techniques. Theresults obtained for a realistic calibration example show that theensemble-based approach produces a significantly more accurate and robustcalibration model than conventional regression methods.
arxiv-10800-262 | Improved LASSO | http://arxiv.org/abs/1503.05160 | author:A. K. Md. Ehsanes Saleh, Enayetur Raheem category:math.ST stat.AP stat.ML stat.TH published:2015-03-17 summary:We propose an improved LASSO estimation technique based on Stein-rule. Weshrink classical LASSO estimator using preliminary test, shrinkage, andpositive-rule shrinkage principle. Simulation results have been carried out forvarious configurations of correlation coefficients ($r$), size of the parametervector ($\beta$), error variance ($\sigma^2$) and number of non-zerocoefficients ($k$) in the model parameter vector. Several real data exampleshave been used to demonstrate the practical usefulness of the proposedestimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$Stein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformlyin the divergence parameter $\Delta^2$ as in the traditional case.
arxiv-10800-263 | ProtVec: A Continuous Distributed Representation of Biological Sequences | http://arxiv.org/abs/1503.05140 | author:Ehsaneddin Asgari, Mohammad R. K. Mofrad category:q-bio.QM cs.AI cs.LG q-bio.GN published:2015-03-17 summary:We propose a new approach for representing biological sequences. This method,named protein-vectors or ProtVec for short, can be utilized in bioinformaticsapplications such as family classification, protein visualization, structureprediction, disordered protein identification, and protein-protein interactionprediction. Using the Skip-gram neural networks, protein sequences arerepresented with a single dense n-dimensional vector. This method was evaluatedby classifying protein sequences obtained from Swiss-Prot belonging to 7,027protein families where an average family classification accuracy of $94\%\pm0.03\%$ was obtained, outperforming existing family classification methods. Inaddition, our model was used to predict disordered proteins from structuredproteins. Two databases of disordered sequences were used: the DisProt databaseas well as a database featuring the disordered regions of nucleoporins richwith phenylalanine-glycine repeats (FG-Nups). Using support vector machineclassifiers, FG-Nup sequences were distinguished from structured Protein DataBank (PDB) sequences with 99.81\% accuracy, and unstructured DisProt sequencesfrom structured DisProt sequences with 100.0\% accuracy. These results indicatethat by only providing sequence data for various proteins into this model,information about protein structure can be determined with high accuracy. Thisso-called embedding model needs to be trained only once and can then be used toascertain a diverse set of information regarding the proteins of interest. Inaddition, this representation can be considered as pre-training for variousapplications of deep learning in bioinformatics.
arxiv-10800-264 | Prediction Using Note Text: Synthetic Feature Creation with word2vec | http://arxiv.org/abs/1503.05123 | author:Manuel Amunategui, Tristan Markwell, Yelena Rozenfeld category:cs.CL published:2015-03-17 summary:word2vec affords a simple yet powerful approach of extracting quantitativevariables from unstructured textual data. Over half of healthcare data isunstructured and therefore hard to model without involved expertise in dataengineering and natural language processing. word2vec can serve as a bridge toquickly gather intelligence from such data sources. In this study, we ran 650 megabytes of unstructured, medical chart notes fromthe Providence Health & Services electronic medical record through word2vec. Weused two different approaches in creating predictive variables and tested themon the risk of readmission for patients with COPD (Chronic Obstructive LungDisease). As a comparative benchmark, we ran the same test using the LACE riskmodel (a single score based on length of stay, acuity, comorbid conditions, andemergency department visits). Using only free text and mathematical might, we found word2vec comparable toLACE in predicting the risk of readmission of COPD patients.
arxiv-10800-265 | Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks | http://arxiv.org/abs/1503.04949 | author:Varun Jampani, Martin Kiefel, Peter V. Gehler category:cs.CV published:2015-03-17 summary:Bilateral filters have wide spread use due to their edge-preservingproperties. The common use case is to manually choose a parametric filter type,usually a Gaussian filter. In this paper, we will generalize theparametrization and in particular derive a gradient descent algorithm so thefilter parameters can be learned from data. This derivation allows to learnhigh dimensional linear filters that operate in sparsely populated featurespaces. We build on the permutohedral lattice construction for efficientfiltering. The ability to learn more general forms of high-dimensional filterscan be used in several diverse applications. First, we demonstrate the use inapplications where single filter applications are desired for runtime reasons.Further, we show how this algorithm can be used to learn the pairwisepotentials in densely connected conditional random fields and apply these todifferent image segmentation tasks. Finally, we introduce layers of bilateralfilters in CNNs and propose bilateral neural networks for the use ofhigh-dimensional sparse data. This view provides new ways to encode modelstructure into network architectures. A diverse set of experiments empiricallyvalidates the usage of general forms of filters.
arxiv-10800-266 | Analysis of PCA Algorithms in Distributed Environments | http://arxiv.org/abs/1503.05214 | author:Tarek Elgamal, Mohamed Hefeeda category:cs.DC cs.LG cs.NA published:2015-03-17 summary:Classical machine learning algorithms often face scalability bottlenecks whenthey are applied to large-scale data. Such algorithms were designed to workwith small data that is assumed to fit in the memory of one machine. In thisreport, we analyze different methods for computing an important machine learingalgorithm, namely Principal Component Analysis (PCA), and we comment on itslimitations in supporting large datasets. The methods are analyzed and comparedacross two important metrics: time complexity and communication complexity. Weconsider the worst-case scenarios for both metrics, and we identify thesoftware libraries that implement each method. The analysis in this reporthelps researchers and engineers in (i) understanding the main bottlenecks forscalability in different PCA algorithms, (ii) choosing the most appropriatemethod and software library for a given application and data setcharacteristics, and (iii) designing new scalable PCA algorithms.
arxiv-10800-267 | Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits | http://arxiv.org/abs/1503.05087 | author:Gergely Neu, Gábor Bartók category:cs.LG stat.ML published:2015-03-17 summary:We propose a sample-efficient alternative for importance weighting forsituations where one only has sample access to the probability distributionthat generates the observations. Our new method, called Recurrence Weighting(RW), is described and analyzed in the context of online combinatorialoptimization under semi-bandit feedback, where a learner sequentially selectsits actions from a combinatorial decision set so as to minimize its cumulativeloss. In particular, we show that the well-known Follow-the-Perturbed-Leader(FPL) prediction method coupled with Recurrence Weighting yields the firstcomputationally efficient reduction from offline to online optimization in thissetting. We provide a thorough theoretical analysis for the resultingalgorithm, showing that its performance is on par with previous, inefficientsolutions. Our main contribution is showing that, despite the relatively largevariance induced by the RW procedure, our performance guarantees hold with highprobability rather than only in expectation. As a side result, we also improvethe best known regret bounds for FPL in online combinatorial optimization withfull feedback, closing the perceived performance gap between FPL andexponential weights in this setting.
arxiv-10800-268 | 3D Object Class Detection in the Wild | http://arxiv.org/abs/1503.05038 | author:Bojan Pepik, Michael Stark, Peter Gehler, Tobias Ritschel, Bernt Schiele category:cs.CV published:2015-03-17 summary:Object class detection has been a synonym for 2D bounding box localizationfor the longest time, fueled by the success of powerful statistical learningtechniques, combined with robust image representations. Only recently, therehas been a growing interest in revisiting the promise of computer vision fromthe early days: to precisely delineate the contents of a visual scene, objectby object, in 3D. In this paper, we draw from recent advances in objectdetection and 2D-3D object lifting in order to design an object class detectorthat is particularly tailored towards 3D object class detection. Our 3D objectclass detection method consists of several stages gradually enriching theobject detection output with object viewpoint, keypoints and 3D shapeestimates. Following careful design, in each stage it constantly improves theperformance and achieves state-ofthe-art performance in simultaneous 2Dbounding box and viewpoint estimation on the challenging Pascal3D+ dataset.
arxiv-10800-269 | $gen$CNN: A Convolutional Architecture for Word Sequence Prediction | http://arxiv.org/abs/1503.05034 | author:Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu category:cs.CL published:2015-03-17 summary:We propose a novel convolutional architecture, named $gen$CNN, for wordsequence prediction. Different from previous work on neural network-basedlanguage modeling and generation (e.g., RNN or LSTM), we choose not to greedilysummarize the history of words as a fixed length vector. Instead, we use aconvolutional neural network to predict the next word with the history of wordsof variable length. Also different from the existing feedforward networks forlanguage modeling, our model can effectively fuse the local correlation andglobal correlation in the word sequence, with a convolution-gating strategyspecifically designed for the task. We argue that our model can give adequaterepresentation of the history, and therefore can naturally exploit both theshort and long range dependencies. Our model is fast, easy to train, andreadily parallelized. Our extensive experiments on text generation and $n$-bestre-ranking in machine translation show that $gen$CNN outperforms thestate-of-the-arts with big margins.
arxiv-10800-270 | Ultra-Fast Shapelets for Time Series Classification | http://arxiv.org/abs/1503.05018 | author:Martin Wistuba, Josif Grabocka, Lars Schmidt-Thieme category:cs.LG published:2015-03-17 summary:Time series shapelets are discriminative subsequences and their similarity toa time series can be used for time series classification. Since the discoveryof time series shapelets is costly in terms of time, the applicability on longor multivariate time series is difficult. In this work we propose Ultra-FastShapelets that uses a number of random shapelets. It is shown that Ultra-FastShapelets yield the same prediction quality as current state-of-the-artshapelet-based time series classifiers that carefully select the shapelets bybeing by up to three orders of magnitudes. Since this method allows aultra-fast shapelet discovery, using shapelets for long multivariate timeseries classification becomes feasible. A method for using shapelets for multivariate time series is proposed andUltra-Fast Shapelets is proven to be successful in comparison tostate-of-the-art multivariate time series classifiers on 15 multivariate timeseries datasets from various domains. Finally, time series derivatives thathave proven to be useful for other time series classifiers are investigated forthe shapelet-based classifiers. It is shown that they have a positive impactand that they are easy to integrate with a simple preprocessing step, withoutthe need of adapting the shapelet discovery algorithm.
arxiv-10800-271 | An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests | http://arxiv.org/abs/1503.05187 | author:Khaled Fawagreh, Mohamad Medhat Gaber, Eyad Elyan category:cs.LG published:2015-03-17 summary:Random Forest (RF) is an ensemble classification technique that was developedby Breiman over a decade ago. Compared with other ensemble techniques, it hasproved its accuracy and superiority. Many researchers, however, believe thatthere is still room for enhancing and improving its performance in terms ofpredictive accuracy. This explains why, over the past decade, there have beenmany extensions of RF where each extension employed a variety of techniques andstrategies to improve certain aspect(s) of RF. Since it has been provenempirically that ensembles tend to yield better results when there is asignificant diversity among the constituent models, the objective of this paperis twofolds. First, it investigates how an unsupervised learning technique,namely, Local Outlier Factor (LOF) can be used to identify diverse trees in theRF. Second, trees with the highest LOF scores are then used to produce anextension of RF termed LOFB-DRF that is much smaller in size than RF, and yetperforms at least as good as RF, but mostly exhibits higher performance interms of accuracy. The latter refers to a known technique called ensemblepruning. Experimental results on 10 real datasets prove the superiority of ourproposed extension over the traditional RF. Unprecedented pruning levelsreaching 99% have been achieved at the time of boosting the predictive accuracyof the ensemble. The notably high pruning level makes the technique a goodcandidate for real-time applications.
arxiv-10800-272 | On Extreme Pruning of Random Forest Ensembles for Real-time Predictive Applications | http://arxiv.org/abs/1503.04996 | author:Khaled Fawagreh, Mohamad Medhat Gaber, Eyad Elyan category:cs.LG published:2015-03-17 summary:Random Forest (RF) is an ensemble supervised machine learning technique thatwas developed by Breiman over a decade ago. Compared with other ensembletechniques, it has proved its accuracy and superiority. Many researchers,however, believe that there is still room for enhancing and improving itsperformance accuracy. This explains why, over the past decade, there have beenmany extensions of RF where each extension employed a variety of techniques andstrategies to improve certain aspect(s) of RF. Since it has been provenempiricallthat ensembles tend to yield better results when there is asignificant diversity among the constituent models, the objective of this paperis twofold. First, it investigates how data clustering (a well known diversitytechnique) can be applied to identify groups of similar decision trees in an RFin order to eliminate redundant trees by selecting a representative from eachgroup (cluster). Second, these likely diverse representatives are then used toproduce an extension of RF termed CLUB-DRF that is much smaller in size thanRF, and yet performs at least as good as RF, and mostly exhibits higherperformance in terms of accuracy. The latter refers to a known technique calledensemble pruning. Experimental results on 15 real datasets from the UCIrepository prove the superiority of our proposed extension over the traditionalRF. Most of our experiments achieved at least 95% or above pruning level whileretaining or outperforming the RF accuracy.
arxiv-10800-273 | Energy Sharing for Multiple Sensor Nodes with Finite Buffers | http://arxiv.org/abs/1503.04964 | author:Sindhu Padakandla, Prabuchandran K. J, Shalabh Bhatnagar category:cs.NI cs.LG published:2015-03-17 summary:We consider the problem of finding optimal energy sharing policies thatmaximize the network performance of a system comprising of multiple sensornodes and a single energy harvesting (EH) source. Sensor nodes periodicallysense the random field and generate data, which is stored in the correspondingdata queues. The EH source harnesses energy from ambient energy sources and thegenerated energy is stored in an energy buffer. Sensor nodes receive energy fordata transmission from the EH source. The EH source has to efficiently sharethe stored energy among the nodes in order to minimize the long-run averagedelay in data transmission. We formulate the problem of energy sharing betweenthe nodes in the framework of average cost infinite-horizon Markov decisionprocesses (MDPs). We develop efficient energy sharing algorithms, namelyQ-learning algorithm with exploration mechanisms based on the $\epsilon$-greedymethod as well as upper confidence bound (UCB). We extend these algorithms byincorporating state and action space aggregation to tackle state-action spaceexplosion in the MDP. We also develop a cross entropy based method thatincorporates policy parameterization in order to find near optimal energysharing policies. Through simulations, we show that our algorithms yield energysharing policies that outperform the heuristic greedy method.
arxiv-10800-274 | How the symbol grounding of living organisms can be realized in artificial agents | http://arxiv.org/abs/1503.04941 | author:J. H. van Hateren category:cs.AI cs.NE cs.RO published:2015-03-17 summary:A system with artificial intelligence usually relies on symbol manipulation,at least partly and implicitly. However, the interpretation of the symbols -what they represent and what they are about - is ultimately left to humans, asdesigners and users of the system. How symbols can acquire meaning for thesystem itself, independent of external interpretation, is an unsolved problem.Some grounding of symbols can be obtained by embodiment, that is, by causallyconnecting symbols (or sub-symbolic variables) to the physical environment,such as in a robot with sensors and effectors. However, a causal connection assuch does not produce representation and aboutness of the kind that symbolshave for humans. Here I present a theory that explains how humans and otherliving organisms have acquired the capability to have symbols and sub-symbolicvariables that represent, refer to, and are about something else. The theoryshows how reference can be to physical objects, but also to abstract objects,and even how it can be misguided (errors in reference) or be about non-existingobjects. I subsequently abstract the primary components of the theory fromtheir biological context, and discuss how and under what conditions the theorycould be implemented in artificial agents. A major component of the theory isthe strong nonlinearity associated with (potentially unlimited)self-reproduction. The latter is likely not acceptable in artificial systems.It remains unclear if goals other than those inherently servingself-reproduction can have aboutness and if such goals could be stabilized.
arxiv-10800-275 | Hypoelliptic Diffusion Maps I: Tangent Bundles | http://arxiv.org/abs/1503.05459 | author:Tingran Gao category:math.ST stat.ML stat.TH I.2.6 published:2015-03-17 summary:We introduce the concept of Hypoelliptic Diffusion Maps (HDM), a frameworkgeneralizing Diffusion Maps in the context of manifold learning anddimensionality reduction. Standard non-linear dimensionality reduction methods(e.g., LLE, ISOMAP, Laplacian Eigenmaps, Diffusion Maps) focus on miningmassive data sets using weighted affinity graphs; Orientable Diffusion Maps andVector Diffusion Maps enrich these graphs by attaching to each node also somelocal geometry. HDM likewise considers a scenario where each node possessesadditional structure, which is now itself of interest to investigate.Virtually, HDM augments the original data set with attached structures, andprovides tools for studying and organizing the augmented ensemble. The goal isto obtain information on individual structures attached to the nodes and on therelationship between structures attached to nearby nodes, so as to study theunderlying manifold from which the nodes are sampled. In this paper, we analyzeHDM on tangent bundles, revealing its intimate connection with sub-Riemanniangeometry and a family of hypoelliptic differential operators. In a later paper,we shall consider more general fibre bundles.
arxiv-10800-276 | Long Short-Term Memory Over Tree Structures | http://arxiv.org/abs/1503.04881 | author:Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo category:cs.CL cs.LG cs.NE published:2015-03-16 summary:The chain-structured long short-term memory (LSTM) has showed to be effectivein a wide range of problems such as speech recognition and machine translation.In this paper, we propose to extend it to tree structures, in which a memorycell can reflect the history memories of multiple child cells or multipledescendant cells in a recursive process. We call the model S-LSTM, whichprovides a principled way of considering long-distance interaction overhierarchies, e.g., language or image parse structures. We leverage the modelsfor semantic composition to understand the meaning of text, a fundamentalproblem in natural language understanding, and show that it outperforms astate-of-the-art recursive model by replacing its composition layers with theS-LSTM memory blocks. We also show that utilizing the given structures ishelpful in achieving a performance better than that without considering thestructures.
arxiv-10800-277 | More General Queries and Less Generalization Error in Adaptive Data Analysis | http://arxiv.org/abs/1503.04843 | author:Raef Bassily, Adam Smith, Thomas Steinke, Jonathan Ullman category:cs.LG cs.DS published:2015-03-16 summary:Adaptivity is an important feature of data analysis---typically the choice ofquestions asked about a dataset depends on previous interactions with the samedataset. However, generalization error is typically bounded in a non-adaptivemodel, where all questions are specified before the dataset is drawn. Recentwork by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated theformal study of this problem, and gave the first upper and lower bounds on theachievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution $\mathcal{P}$ and aset of $n$ independent samples $x$ is drawn from $\mathcal{P}$. We seek analgorithm that, given $x$ as input, "accurately" answers a sequence ofadaptively chosen "queries" about the unknown distribution $\mathcal{P}$. Howmany samples $n$ must we draw from the distribution, as a function of the typeof queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions towards resolving this question: *We give upper bounds on the number of samples $n$ that are needed to answerstatistical queries that improve over the bounds of Dwork et al. *We prove the first upper bounds on the number of samples required to answermore general families of queries. These include arbitrary low-sensitivityqueries and the important class of convex risk minimization queries. As in Dwork et al., our algorithms are based on a connection betweendifferential privacy and generalization error, but we feel that our analysis issimpler and more modular, which may be useful for studying these questions inthe future.
arxiv-10800-278 | Phase and TV Based Convex Sets for Blind Deconvolution of Microscopic Images | http://arxiv.org/abs/1503.04776 | author:Mohammad Tofighi, Onur Yorulmaz, A. Enis Cetin category:math.OC cs.CV published:2015-03-16 summary:In this article, two closed and convex sets for blind deconvolution problemare proposed. Most blurring functions in microscopy are symmetric with respectto the origin. Therefore, they do not modify the phase of the Fourier transform(FT) of the original image. As a result blurred image and the original imagehave the same FT phase. Therefore, the set of images with a prescribed FT phasecan be used as a constraint set in blind deconvolution problems. Another convexset that can be used during the image reconstruction process is the epigraphset of Total Variation (TV) function. This set does not need a prescribed upperbound on the total variation of the image. The upper bound is automaticallyadjusted according to the current image of the restoration process. Both ofthese two closed and convex sets can be used as a part of any blinddeconvolution algorithm. Simulation examples are presented.
arxiv-10800-279 | Learning Mixed Membership Community Models in Social Tagging Networks through Tensor Methods | http://arxiv.org/abs/1503.04567 | author:Anima Anandkumar, Hanie Sedghi category:cs.LG cs.SI stat.ML published:2015-03-16 summary:Community detection in graphs has been extensively studied both in theory andin applications. However, detecting communities in hypergraphs is morechallenging. In this paper, we propose a tensor decomposition approach forguaranteed learning of communities in a special class of hypergraphs modelingsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniformhypergraph consisting of (user, tag, resource) hyperedges. We posit aprobabilistic mixed membership community model, and prove that the tensormethod consistently learns the communities under efficient sample complexityand separation requirements.
arxiv-10800-280 | Skilled Impostor Attacks Against Fingerprint Verification Systems And Its Remedy | http://arxiv.org/abs/1503.04729 | author:Carsten Gottschlich category:cs.CV cs.CR cs.CY published:2015-03-16 summary:Fingerprint verification systems are becoming ubiquitous in everyday life.This trend is propelled especially by the proliferation of mobile devices withfingerprint sensors such as smartphones and tablet computers, and fingerprintverification is increasingly applied for authenticating financial transactions.In this study we describe a novel attack vector against fingerprintverification systems which we coin skilled impostor attack. We show thatexisting protocols for performance evaluation of fingerprint verificationsystems are flawed and as a consequence of this, the system's realvulnerability is systematically underestimated. We examine a scenario in whicha fingerprint verification system is tuned to operate at false acceptance rateof 0.1% using the traditional verification protocols with random impostors(zero-effort attacks). We demonstrate that an active and intelligent attackercan achieve a chance of success in the area of 89% or more against this systemby performing skilled impostor attacks. We describe a new protocol forevaluating fingerprint verification performance in order to improve theassessment of potential and limitations of fingerprint recognition systems.This new evaluation protocol enables a more informed decision concerning theoperating threshold in practical applications and the respective trade-offbetween security (low false acceptance rates) and usability (low falserejection rates). The skilled impostor attack is a general attack concept whichis independent of specific databases or comparison algorithms. The proposedprotocol relying on skilled impostor attacks can directly be applied forevaluating the verification performance of other biometric modalities such ase.g. iris, face, ear, finger vein, gait or speaker recognition.
arxiv-10800-281 | Deep Feelings: A Massive Cross-Lingual Study on the Relation between Emotions and Virality | http://arxiv.org/abs/1503.04723 | author:Marco Guerini, Jacopo Staiano category:cs.SI cs.CL cs.CY published:2015-03-16 summary:This article provides a comprehensive investigation on the relations betweenvirality of news articles and the emotions they are found to evoke. Virality,in our view, is a phenomenon with many facets, i.e. under this generic termseveral different effects of persuasive communication are comprised. Byexploiting a high-coverage and bilingual corpus of documents containing metricsof their spread on social networks as well as a massive affective annotationprovided by readers, we present a thorough analysis of the interplay betweenevoked emotions and viral facets. We highlight and discuss our findings inlight of a cross-lingual approach: while we discover differences in evokedemotions and corresponding viral effects, we provide preliminary evidence of ageneralized explanatory model rooted in the deep structure of emotions: theValence-Arousal-Dominance (VAD) circumplex. We find that viral facets appear tobe consistently affected by particular VAD configurations, and theseconfigurations indicate a clear connection with distinct phenomena underlyingpersuasive communication.
arxiv-10800-282 | A Memcomputing Pascaline | http://arxiv.org/abs/1503.04673 | author:Y. V. Pershin, L. K. Castelano, F. Hartmann, V. Lopez-Richard, M. Di Ventra category:cs.ET cs.NE published:2015-03-16 summary:The original Pascaline was a mechanical calculator able to sum and subtractintegers. It encodes information in the angles of mechanical wheels and througha set of gears, and aided by gravity, could perform the calculations. Here, weshow that such a concept can be realized in electronics using memory elementssuch as memristive systems. By using memristive emulators we have demonstratedexperimentally the memcomputing version of the mechanical Pascaline, capable ofprocessing and storing the numerical results in the multiple levels of eachmemristive element. Our result is the first experimental demonstration ofmultidigit arithmetics with multi-level memory devices that further emphasizesthe versatility and potential of memristive systems for futuremassively-parallel high-density computing architectures.
arxiv-10800-283 | Template-based Monocular 3D Shape Recovery using Laplacian Meshes | http://arxiv.org/abs/1503.04643 | author:Dat Tien Ngo, Jonas Ostlund, Pascal Fua category:cs.CV published:2015-03-16 summary:We show that by extending the Laplacian formalism, which was first introducedin the Graphics community to regularize 3D meshes, we can turn the monocular 3Dshape reconstruction of a deformable surface given correspondences with areference image into a much better-posed problem. This allows us to quickly andreliably eliminate outliers by simply solving a linear least squares problem.This yields an initial 3D shape estimate, which is not necessarily accurate,but whose 2D projections are. The initial shape is then refined by aconstrained optimization problem to output the final surface reconstruction. Our approach allows us to reduce the dimensionality of the surfacereconstruction problem without sacrificing accuracy, thus allowing forreal-time implementations.
arxiv-10800-284 | PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and Multi-Illumination Images | http://arxiv.org/abs/1503.04598 | author:Reza Sabzevari, Vittori Murino, Alessio Del Bue category:cs.CV published:2015-03-16 summary:In this paper, we address the problem of dense 3D reconstruction frommultiple view images subject to strong lighting variations. In this regard, anew piecewise framework is proposed to explicitly take into account the changeof illumination across several wide-baseline images. Unlike multi-view stereoand multi-view photometric stereo methods, this pipeline deals withwide-baseline images that are uncalibrated, in terms of both camera parametersand lighting conditions. Such a scenario is meant to avoid use of any specificimaging setup and provide a tool for normal users without any expertise. To thebest of our knowledge, this paper presents the first work that deals with suchunconstrained setting. We propose a coarse-to-fine approach, in which a coarsemesh is first created using a set of geometric constraints and, then, finedetails are recovered by exploiting photometric properties of the scene.Augmenting the fine details on the coarse mesh is done via a final optimizationstep. Note that the method does not provide a generic solution for multi-viewphotometric stereo problem but it relaxes several common assumptions of thisproblem. The approach scales very well in size given its piecewise nature,dealing with large scale optimization and with severe missing data. Experimentson a benchmark dataset Robot data-set show the method performance against 3Dground truth.
arxiv-10800-285 | Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network | http://arxiv.org/abs/1503.04596 | author:Mark D. McDonnell, Tony Vladusich category:cs.NE cs.CV cs.LG published:2015-03-16 summary:We present a neural network architecture and training method designed toenable very rapid training and low implementation complexity. Due to itstraining speed and very few tunable parameters, the method has strong potentialfor applications requiring frequent retraining or online training. The approachis characterized by (a) convolutional filters based on biologically inspiredvisual processing filters, (b) randomly-valued classifier-stage input weights,(c) use of least squares regression to train the classifier output weights in asingle batch, and (d) linear classifier-stage output units. We demonstrate theefficacy of the method by applying it to image classification. Our resultsmatch existing state-of-the-art results on the MNIST (0.37% error) andNORB-small (2.2% error) image classification databases, but with very fasttraining times compared to standard deep network approaches. The network'sperformance on the Google Street View House Number (SVHN) (4% error) databaseis also competitive with state-of-the art methods.
arxiv-10800-286 | High-dimensional quadratic classifiers in non-sparse settings | http://arxiv.org/abs/1503.04549 | author:Makoto Aoshima, Kazuyoshi Yata category:stat.ML math.ST stat.TH published:2015-03-16 summary:We consider high-dimensional quadratic classifiers in non-sparse settings.The target of classification rules is not Bayes error rates in the context. Theclassifier based on the Mahalanobis distance does not always give a preferableperformance even if the populations are normal distributions having knowncovariance matrices. The quadratic classifiers proposed in this paper drawinformation about heterogeneity effectively through both the differences ofexpanding mean vectors and covariance matrices. We show that they hold aconsistency property in which misclassification rates tend to zero as thedimension goes to infinity under non-sparse settings. We verify that they areasymptotically distributed as a normal distribution under certain conditions.We also propose a quadratic classifier after feature selection by using boththe differences of mean vectors and covariance matrices. Finally, we discussperformances of the classifiers in actual data analyses. The proposedclassifiers achieve highly accurate classification with very low computationalcosts.
arxiv-10800-287 | Statistical Analysis of Loopy Belief Propagation in Random Fields | http://arxiv.org/abs/1503.04585 | author:Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka category:stat.ML cs.CV published:2015-03-16 summary:Loopy belief propagation (LBP), which is equivalent to the Betheapproximation in statistical mechanics, is a message-passing-type inferencemethod that is widely used to analyze systems based on Markov random fields(MRFs). In this paper, we propose a message-passing-type method to analyticallyevaluate the quenched average of LBP in random fields by using the replicacluster variation method. The proposed analytical method is applicable togeneral pair-wise MRFs with random fields whose distributions differ from eachother and can give the quenched averages of the Bethe free energies over randomfields, which are consistent with numerical results. The order of itscomputational cost is equivalent to that of standard LBP. In the latter part ofthis paper, we describe the application of the proposed method to Bayesianimage restoration, in which we observed that our theoretical results are ingood agreement with the numerical results for natural images.
arxiv-10800-288 | Pattern Recognition of Bearing Faults using Smoother Statistical Features | http://arxiv.org/abs/1503.04444 | author:Muhammad Masood Tahir, Ayyaz Hussain category:cs.CV published:2015-03-15 summary:A pattern recognition (PR) based diagnostic scheme is presented to identifybearing faults, using time domain features. Vibration data is acquired fromfaulty bearings using a test rig. The features are extracted from the data, andprocessed prior to utilize in the PR process. The processing involves smoothingof feature distributions. This reduces the undesired impact of vibrationrandomness on the PR process, and thus enhances the diagnostic accuracy of themodel.
arxiv-10800-289 | Statistical Estimation and Clustering of Group-invariant Orientation Parameters | http://arxiv.org/abs/1503.04474 | author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey Simmons, Alfred Hero category:stat.ML published:2015-03-15 summary:We treat the problem of estimation of orientation parameters whose values areinvariant to transformations from a spherical symmetry group. Previous work hasshown that any such group-invariant distribution must satisfy a restrictedfinite mixture representation, which allows the orientation parameter to beestimated using an Expectation Maximization (EM) maximum likelihood (ML)estimation algorithm. In this paper, we introduce two parametric models forthis spherical symmetry group estimation problem: 1) the hyperbolic Von MisesFisher (VMF) mixture distribution and 2) the Watson mixture distribution. Wealso introduce a new EM-ML algorithm for clustering samples that come frommixtures of group-invariant distributions with different parameters. We applythe models to the problem of mean crystal orientation estimation under thespherically symmetric group associated with the crystal form, e.g., cubic oroctahedral or hexahedral. Simulations and experiments establish the advantagesof the extended EM-VMF and EM-Watson estimators for data acquired by ElectronBackscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloysample.
arxiv-10800-290 | Separable and non-separable data representation for pattern discrimination | http://arxiv.org/abs/1503.04400 | author:Jarosław Adam Miszczak category:quant-ph cs.CV cs.LG I.5.2; I.4.1 published:2015-03-15 summary:We provide a complete work-flow, based on the language of quantum informationtheory, suitable for processing data for the purpose of pattern recognition.The main advantage of the introduced scheme is that it can be easilyimplemented and applied to process real-world data using modest computationresources. At the same time it can be used to investigate the difference in thepattern recognition resulting from the utilization of the tensor productstructure of the space of quantum states. We illustrate this difference byproviding a simple example based on the classification of 2D data.
arxiv-10800-291 | Simulation of Genetic Algorithm: Traffic Light Efficiency | http://arxiv.org/abs/1503.04475 | author:Eric Lienert category:cs.NE published:2015-03-15 summary:Traffic is a problem in many urban areas worldwide. Traffic flow is dictatedby certain devices such as traffic lights. The traffic lights signal when eachlane is able to pass through the intersection. Often, static schedulesinterfere with ideal traffic flow. The purpose of this project was to find away to make intersections controlled with traffic lights more efficient. Thisgoal was accomplished through the creation of a genetic algorithm, whichenhances an input algorithm through genetic principles to produce the fittestalgorithm. The program was comprised of two major elements: coding in Java andcoding in Simulation of Urban Mobility (SUMO), which is an environment thatsimulates real traffic. The Java code called upon the SUMO simulation via acommand prompt which ran the simulation, received the output, altered thealgorithm, and looped. The SUMO component initialized a simulation in which a 1x 1 street layout was created, each intersection with its own traffic light.Each loop enhanced the input algorithm by altering the scheduling string(dictates the light changes). After the looped simulations were executed, thedata was then analyzed. This was accomplished by creating an algorithm basedupon regular practice, timed traffic lights, and comparing the output which wascomprised of the total time it took for all vehicles to exit the system and theaverage time it took each individual vehicle to exit the system. Thesedifferent variables: the time it took the average vehicle to exit the systemand total time for all vehicles to exit the system, where then graphed togetherto provide a visual aid. The genetic algorithm did improve traffic light andtraffic flow efficiency in comparison to traditional scheduling methods.
arxiv-10800-292 | Towards radio astronomical imaging using an arbitrary basis | http://arxiv.org/abs/1503.04338 | author:Matthias Petschow category:astro-ph.IM cs.CV published:2015-03-14 summary:The new generation of radio telescopes, such as the Square Kilometer Array(SKA), requires dramatic advances in computer hardware and software, in orderto process the large amounts of produced data efficiently. In this document, weexplore a new approach to wide-field imaging. By generalizing the imagereconstruction, which is performed by an inverse Fourier transform, toarbitrary transformations, we gain enormous new possibilities. In particular,we outline an approach that might allow to obtain a sky image of size P times Qin (optimal) O(PQ) time. This could be a step in the direction of real-time,wide-field sky imaging for future telescopes.
arxiv-10800-293 | A Dictionary-based Approach for Estimating Shape and Spatially-Varying Reflectance | http://arxiv.org/abs/1503.04265 | author:Zhuo Hui, Aswin C. Sankaranarayanan category:cs.CV published:2015-03-14 summary:We present a technique for estimating the shape and reflectance of an objectin terms of its surface normals and spatially-varying BRDF. We assume thatmultiple images of the object are obtained under fixed view-point and varyingillumination, i.e, the setting of photometric stereo. Assuming that the BRDF ateach pixel lies in the non-negative span of a known BRDF dictionary, we derivea per-pixel surface normal and BRDF estimation framework that requires neitheriterative optimization techniques nor careful initialization, both of which areendemic to most state-of-the-art techniques. We showcase the performance of ourtechnique on a wide range of simulated and real scenes where we outperformcompeting methods.
arxiv-10800-294 | Metric Localization using Google Street View | http://arxiv.org/abs/1503.04287 | author:Pratik Agarwal, Wolfram Burgard, Luciano Spinello category:cs.RO cs.CV published:2015-03-14 summary:Accurate metrical localization is one of the central challenges in mobilerobotics. Many existing methods aim at localizing after building a map with therobot. In this paper, we present a novel approach that instead uses geotaggedpanoramas from the Google Street View as a source of global positioning. Wemodel the problem of localization as a non-linear least squares estimation intwo phases. The first estimates the 3D position of tracked feature points fromshort monocular camera sequences. The second computes the rigid bodytransformation between the Street View panoramas and the estimated points. Theonly input of this approach is a stream of monocular camera images and odometryestimates. We quantified the accuracy of the method by running the approach ona robotic platform in a parking lot by using visual fiducials as ground truth.Additionally, we applied the approach in the context of personal localizationin a real urban scenario by using data from a Google Tango tablet.
arxiv-10800-295 | An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning | http://arxiv.org/abs/1503.04269 | author:Richard S. Sutton, A. Rupam Mahmood, Martha White category:cs.LG published:2015-03-14 summary:In this paper we introduce the idea of improving the performance ofparametric temporal-difference (TD) learning algorithms by selectivelyemphasizing or de-emphasizing their updates on different time steps. Inparticular, we show that varying the emphasis of linear TD($\lambda$)'s updatesin a particular way causes its expected update to become stable underoff-policy training. The only prior model-free TD methods to achieve this withper-step computation linear in the number of function approximation parametersare the gradient-TD family of methods including TDC, GTD($\lambda$), andGQ($\lambda$). Compared to these methods, our _emphatic TD($\lambda$)_ issimpler and easier to use; it has only one learned parameter vector and onestep-size parameter. Our treatment includes general state-dependent discountingand bootstrapping functions, and a way of specifying varying degrees ofinterest in accurately valuing different states.
arxiv-10800-296 | Novel Super-Resolution Method Based on High Order Nonlocal-Means | http://arxiv.org/abs/1503.04253 | author:Kang Yong-Rim, Kim Yong-Jin category:cs.IT cs.CV math.IT published:2015-03-14 summary:Super-resolution without explicit sub-pixel motion estimation is a veryactive subject of image reconstruction containing general motion. The Non-LocalMeans (NLM) method is a simple image reconstruction method without explicitmotion estimation. In this paper we generalize NLM method to higher ordersusing kernel regression can apply to super-resolution reconstruction. Theperformance of the generalized method is compared with other methods.
arxiv-10800-297 | LiSens --- A Scalable Architecture for Video Compressive Sensing | http://arxiv.org/abs/1503.04267 | author:Jian Wang, Mohit Gupta, Aswin C. Sankaranarayanan category:cs.CV published:2015-03-14 summary:The measurement rate of cameras that take spatially multiplexed measurementsby using spatial light modulators (SLM) is often limited by the switching speedof the SLMs. This is especially true for single-pixel cameras where thephotodetector operates at a rate that is many orders-of-magnitude greater thanthe SLM. We study the factors that determine the measurement rate for suchspatial multiplexing cameras (SMC) and show that increasing the number ofpixels in the device improves the measurement rate, but there is an optimumnumber of pixels (typically, few thousands) beyond which the measurement ratedoes not increase. This motivates the design of LiSens, a novel imagingarchitecture, that replaces the photodetector in the single-pixel camera with a1D linear array or a line-sensor. We illustrate the optical architectureunderlying LiSens, build a prototype, and demonstrate results of a range ofindoor and outdoor scenes. LiSens delivers on the promise of SMCs: imaging at amegapixel resolution, at video rate, using an inexpensive low-resolutionsensor.
arxiv-10800-298 | Content-Based Bird Retrieval using Shape context, Color moments and Bag of Features | http://arxiv.org/abs/1503.07816 | author:Bahri Abdelkhalak, Hamid Zouaki category:cs.CV cs.IR published:2015-03-14 summary:In this paper we propose a new descriptor for birds search. First, our workwas carried on the choice of a descriptor. This choice is usually driven by theapplication requirements such as robustness to noise, stability with respect tobias, the invariance to geometrical transformations or tolerance to occlusions.In this context, we introduce a descriptor which combines the shape and colordescriptors to have an effectiveness description of birds. The proposeddescriptor is an adaptation of a descriptor based on the contours defined inarticle Belongie et al. [5] combined with color moments [19]. Specifically,points of interest are extracted from each image and information's in theregion in the vicinity of these points are represented by descriptors of shapecontext concatenated with color moments. Thus, the approach bag of visual wordsis applied to the latter. The experimental results show the effectiveness ofour descriptor for the bird search by content.
arxiv-10800-299 | Communication-efficient sparse regression: a one-shot approach | http://arxiv.org/abs/1503.04337 | author:Jason D. Lee, Yuekai Sun, Qiang Liu, Jonathan E. Taylor category:stat.ML cs.LG published:2015-03-14 summary:We devise a one-shot approach to distributed sparse regression in thehigh-dimensional setting. The key idea is to average "debiased" or"desparsified" lasso estimators. We show the approach converges at the samerate as the lasso as long as the dataset is not split across too many machines.We also extend the approach to generalized linear models.
arxiv-10800-300 | The YLI-MED Corpus: Characteristics, Procedures, and Plans | http://arxiv.org/abs/1503.04250 | author:Julia Bernd, Damian Borth, Benjamin Elizalde, Gerald Friedland, Heather Gallagher, Luke Gottlieb, Adam Janin, Sara Karabashlieva, Jocelyn Takahashi, Jennifer Won category:cs.MM cs.CL published:2015-03-13 summary:The YLI Multimedia Event Detection corpus is a public-domain index of videoswith annotations and computed features, specialized for research in multimediaevent detection (MED), i.e., automatically identifying what's happening in avideo by analyzing the audio and visual content. The videos indexed in theYLI-MED corpus are a subset of the larger YLI feature corpus, which is beingdeveloped by the International Computer Science Institute and LawrenceLivermore National Laboratory based on the Yahoo Flickr Creative Commons 100Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depictingone of ten target events, or no target event, and are annotated for additionalattributes like language spoken and whether the video has a musical score. Theannotations also include degree of annotator agreement and average annotatorconfidence scores for the event categorization of each video. Version 1.0 ofYLI-MED includes 1823 "positive" videos that depict the target events and48,138 "negative" videos, as well as 177 supplementary videos that are similarto event videos but are not positive examples. Our goal in producing YLI-MED isto be as open about our data and procedures as possible. This report describesthe procedures used to collect the corpus; gives detailed descriptivestatistics about the corpus makeup (and how video attributes affectedannotators' judgments); discusses possible biases in the corpus introduced byour procedural choices and compares it with the most similar existing dataset,TRECVID MED's HAVIC corpus; and gives an overview of our future plans forexpanding the annotation effort.
