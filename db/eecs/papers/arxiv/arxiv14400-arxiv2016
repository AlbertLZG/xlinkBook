arxiv-14400-1 | Automatic 3D object detection of Proteins in Fluorescent labeled microscope images with spatial statistical analysis | http://arxiv.org/pdf/1601.01216v1.pdf | author:Ramin Norousi, Volker J. Schmid category:cs.CV published:2016-01-06 summary:Since manual object detection is very inaccurate and time consuming, someautomatic object detection tools have been developed in recent years. At themoment, there is no image analysis software available which provides anautomatic, objective assessment of 3D foci which is generally applicable.Complications arise from discrete foci which are very close or even come incontact to other foci, moreover they are of variable sizes and show variablesignal-to-noise, and must be analyzed fully in 3D. Therefore we introduce the3D-OSCOS (3D-Object Segmentation and Colocalization Analysis based on Spatialstatistics) algorithm which is implemented as a user-friendly toolbox forinteractive detection of 3D objects and visualization of labeled images.
arxiv-14400-2 | A Riemannian low-rank method for optimization over semidefinite matrices with block-diagonal constraints | http://arxiv.org/pdf/1506.00575v2.pdf | author:Nicolas Boumal category:math.OC cs.CV stat.CO published:2015-06-01 summary:We propose a new algorithm to solve optimization problems of the form $\minf(X)$ for a smooth function $f$ under the constraints that $X$ is positivesemidefinite and the diagonal blocks of $X$ are small identity matrices. Suchproblems often arise as the result of relaxing a rank constraint (lifting). Inparticular, many estimation tasks involving phases, rotations, orthonormalbases or permutations fit in this framework, and so do certain relaxations ofcombinatorial problems such as Max-Cut. The proposed algorithm exploits thefacts that (1) such formulations admit low-rank solutions, and (2) theirrank-restricted versions are smooth optimization problems on a Riemannianmanifold. Combining insights from both the Riemannian and the convex geometriesof the problem, we characterize when second-order critical points of the smoothproblem reveal KKT points of the semidefinite problem. We compare against stateof the art, mature software and find that, on certain interesting probleminstances, what we call the staircase method is orders of magnitude faster, ismore accurate and scales better. Code is available.
arxiv-14400-3 | Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference | http://arxiv.org/pdf/1510.07482v3.pdf | author:Effi Levi, Roi Reichart, Ari Rappoport category:cs.CL published:2015-10-26 summary:The run time complexity of state-of-the-art inference algorithms ingraph-based dependency parsing is super-linear in the number of input words(n). Recently, pruning algorithms for these models have shown to cut a largeportion of the graph edges, with minimal damage to the resulting parse trees.Solving the inference problem in run time complexity determined solely by thenumber of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodesthe problem as a minimum spanning tree (MST) problem in an undirected graph.This allows us to utilize state-of-the-art undirected MST algorithms whose runtime is O(m) at expectation and with a very high probability. A directed parsetree is then inferred from the undirected MST and is subsequently improved withrespect to the directed parsing model through local greedy updates, both stepsrunning in O(n) time. In experiments with 18 languages, a variant of thefirst-order MSTParser (McDonald et al., 2005b) that employs our algorithmperforms very similarly to the original parser that runs an O(n^2) directed MSTinference.
arxiv-14400-4 | Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON 2015 | http://arxiv.org/pdf/1601.01195v1.pdf | author:Kamal Sarkar category:cs.CL 68T50 published:2016-01-06 summary:This paper discusses the experiments carried out by us at Jadavpur Universityas part of the participation in ICON 2015 task: POS Tagging for Code-mixedIndian Social Media Text. The tool that we have developed for the task is basedon Trigram Hidden Markov Model that utilizes information from dictionary aswell as some other word level features to enhance the observation probabilitiesof the known tokens as well as unknown tokens. We submitted runs forBengali-English, Hindi-English and Tamil-English Language pairs. Our system hasbeen trained and tested on the datasets released for ICON 2015 shared task: POSTagging For Code-mixed Indian Social Media Text. In constrained mode, oursystem obtains average overall accuracy (averaged over all three languagepairs) of 75.60% which is very close to other participating two systems (76.79%for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. Inunconstrained mode, our system obtains average overall accuracy of 70.65% whichis also close to the system (72.85% for AMRITA_CEN) which obtains the highestaverage overall accuracy.
arxiv-14400-5 | On Bayesian index policies for sequential resource allocation | http://arxiv.org/pdf/1601.01190v1.pdf | author:Emilie Kaufmann category:stat.ML published:2016-01-06 summary:This paper is about index policies for minimizing (frequentist) regret in astochastic multi-armed bandit model, that are inspired by a Bayesian view onthe problem. Our main contribution is to prove the asymptotic optimality ofBayes-UCB, an algorithm based on quantiles of posterior distributions, when therewards distributions belong to a one-dimensional exponential family, for alarge class of prior distributions. We also show that the Bayesian literaturegives new insight on what kind of exploration rates could be used infrequentist, UCB-type algorithms. Indeed, approximations of the Bayesianoptimal solution or the Finite Horizon Gittins indices suggest the introductionof two algorithms, KL-UCB + and KL-UCB-H + , whose asymptotic optimality isalso established.
arxiv-14400-6 | A brief survey on deep belief networks and introducing a new object oriented toolbox (DeeBNet) | http://arxiv.org/pdf/1408.3264v7.pdf | author:Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour category:cs.CV cs.LG cs.MS cs.NE 68T01 published:2014-08-14 summary:Nowadays, this is very popular to use the deep architectures in machinelearning. Deep Belief Networks (DBNs) are deep architectures that use stack ofRestricted Boltzmann Machines (RBM) to create a powerful generative model usingtraining data. DBNs have many ability like feature extraction andclassification that are used in many applications like image processing, speechprocessing and etc. This paper introduces a new object oriented MATLAB toolboxwith most of abilities needed for the implementation of DBNs. In the newversion, the toolbox can be used in Octave. According to the results of theexperiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups(text) datasets, it was shown that the toolbox can learn automatically a goodrepresentation of the input from unlabeled data with better discriminationbetween different classes. Also on all datasets, the obtained classificationerrors are comparable to those of state of the art classifiers. In addition,the toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and ournew FEPCD method), different sparsity methods (quadratic, rate distortion andour new normal method), different RBM types (generative and discriminative),using GPU, etc. The toolbox is a user-friendly open source software and isfreely available on the websitehttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html .
arxiv-14400-7 | A simple technique for improving multi-class classification with neural networks | http://arxiv.org/pdf/1601.01157v1.pdf | author:Thomas Kopinski, Alexander Gepperth, Uwe Handmann category:cs.LG published:2016-01-06 summary:We present a novel method to perform multi-class pattern classification withneural networks and test it on a challenging 3D hand gesture recognitionproblem. Our method consists of a standard one-against-all (OAA)classification, followed by another network layer classifying the resultingclass scores, possibly augmented by the original raw input vector. This allowsthe network to disambiguate hard-to-separate classes as the distribution ofclass scores carries considerable information as well, and is in fact oftenused for assessing the confidence of a decision. We show that by this approachwe are able to significantly boost our results, overall as well as forparticular difficult cases, on the hard 10-class gesture classification task.
arxiv-14400-8 | Vehicle Classification using Transferable Deep Neural Network Features | http://arxiv.org/pdf/1601.01145v1.pdf | author:Yiren Zhou, Ngai-Man Cheung category:cs.CV published:2016-01-06 summary:We address vehicle detection on rear view vehicle images captured from adistance along multi-lane highways, and vehicle classification usingtransferable features from Deep Neural Network. We address the followingproblems that are specific to our application: how to utilize dash lanemarkings to assist vehicle detection, what features are useful forclassification on vehicle categories, and how to utilize Deep Neural Networkwhen the size of the labelled data is limited. Experiment results suggest ourapproach outperforms other state-of-the-art.
arxiv-14400-9 | Streaming Gibbs Sampling for LDA Model | http://arxiv.org/pdf/1601.01142v1.pdf | author:Yang Gao, Jianfei Chen, Jun Zhu category:cs.LG stat.ML published:2016-01-06 summary:Streaming variational Bayes (SVB) is successful in learning LDA models in anonline manner. However previous attempts toward developing online Monte-Carlomethods for LDA have little success, often by having much worse perplexity thantheir batch counterparts. We present a streaming Gibbs sampling (SGS) method,an online extension of the collapsed Gibbs sampling (CGS). Our empirical studyshows that SGS can reach similar perplexity as CGS, much better than SVB. Ourdistributed version of SGS, DSGS, is much more scalable than SVB mainly becausethe updates' communication complexity is small.
arxiv-14400-10 | A pragmatic approach to multi-class classification | http://arxiv.org/pdf/1601.01121v1.pdf | author:Thomas Kopinski, Stéphane Magand, Uwe Handmann, Alexander Gepperth category:cs.LG published:2016-01-06 summary:We present a novel hierarchical approach to multi-class classification whichis generic in that it can be applied to different classification models (e.g.,support vector machines, perceptrons), and makes no explicit assumptions aboutthe probabilistic structure of the problem as it is usually done in multi-classclassification. By adding a cascade of additional classifiers, each of whichreceives the previous classifier's output in addition to regular input data,the approach harnesses unused information that manifests itself in the form of,e.g., correlations between predicted classes. Using multilayer perceptrons as aclassification model, we demonstrate the validity of this approach by testingit on a complex ten-class 3D gesture recognition task.
arxiv-14400-11 | Low-Rank Representation over the Manifold of Curves | http://arxiv.org/pdf/1601.00732v2.pdf | author:Stephen Tierney, Junbin Gao, Yi Guo, Zhengwu Zhang category:cs.CV cs.LG published:2016-01-05 summary:In machine learning it is common to interpret each data point as a vector inEuclidean space. However the data may actually be functional i.e.\ each datapoint is a function of some variable such as time and the function isdiscretely sampled. The naive treatment of functional data as traditionalmultivariate data can lead to poor performance since the algorithms areignoring the correlation in the curvature of each function. In this paper wepropose a method to analyse subspace structure of the functional data by usingthe state of the art Low-Rank Representation (LRR). Experimental evaluation onsynthetic and real data reveals that this method massively outperformsconventional LRR in tasks concerning functional data.
arxiv-14400-12 | INRIASAC: Simple Hypernym Extraction Methods | http://arxiv.org/pdf/1502.01271v2.pdf | author:Gregory Grefenstette category:cs.CL published:2015-02-04 summary:Given a set of terms from a given domain, how can we structure them into ataxonomy without manual intervention? This is the task 17 of SemEval 2015. Herewe present our simple taxonomy structuring techniques which, despite theirsimplicity, ranked first in this 2015 benchmark. We use large quantities oftext (English Wikipedia) and simple heuristics such as term overlap anddocument and sentence co-occurrence to produce hypernym lists. We describethese techniques and pre-sent an initial evaluation of results.
arxiv-14400-13 | A Planning based Framework for Essay Generation | http://arxiv.org/pdf/1512.05919v2.pdf | author:Bing Qin, Duyu Tang, Xinwei Geng, Dandan Ning, Jiahao Liu, Ting Liu category:cs.CL published:2015-12-18 summary:Generating an article automatically with computer program is a challengingtask in artificial intelligence and natural language processing. In this paper,we target at essay generation, which takes as input a topic word in mind andgenerates an organized article under the theme of the topic. We follow the ideaof text planning \cite{Reiter1997} and develop an essay generation framework.The framework consists of three components, including topic understanding,sentence extraction and sentence reordering. For each component, we studiedseveral statistical algorithms and empirically compared between them in termsof qualitative or quantitative analysis. Although we run experiments on Chinesecorpus, the method is language independent and can be easily adapted to otherlanguage. We lay out the remaining challenges and suggest avenues for futureresearch.
arxiv-14400-14 | Memory Matters: Convolutional Recurrent Neural Network for Scene Text Recognition | http://arxiv.org/pdf/1601.01100v1.pdf | author:Guo Qiang, Tu Dan, Li Guohui, Lei Jun category:cs.CV published:2016-01-06 summary:Text recognition in natural scene is a challenging problem due to the manyfactors affecting text appearance. In this paper, we presents a method thatdirectly transcribes scene text images to text without needing of sophisticatedcharacter segmentation. We leverage recent advances of deep neural networks tomodel the appearance of scene text images with temporal dynamics. Specifically,we integrates convolutional neural network (CNN) and recurrent neural network(RNN) which is motivated by observing the complementary modeling capabilitiesof the two models. The main contribution of this work is investigating howtemporal memory helps in an segmentation free fashion for this specificproblem. By using long short-term memory (LSTM) blocks as hidden units, ourmodel can retain long-term memory compared with HMMs which only maintainshort-term state dependences. We conduct experiments on Street View HouseNumber dataset containing highly variable number images. The resultsdemonstrate the superiority of the proposed method over traditional HMM basedmethods.
arxiv-14400-15 | Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | http://arxiv.org/pdf/1506.01497v3.pdf | author:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun category:cs.CV published:2015-06-04 summary:State-of-the-art object detection networks depend on region proposalalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNNhave reduced the running time of these detection networks, exposing regionproposal computation as a bottleneck. In this work, we introduce a RegionProposal Network (RPN) that shares full-image convolutional features with thedetection network, thus enabling nearly cost-free region proposals. An RPN is afully convolutional network that simultaneously predicts object bounds andobjectness scores at each position. The RPN is trained end-to-end to generatehigh-quality region proposals, which are used by Fast R-CNN for detection. Wefurther merge RPN and Fast R-CNN into a single network by sharing theirconvolutional features---using the recently popular terminology of neuralnetworks with 'attention' mechanisms, the RPN component tells the unifiednetwork where to look. For the very deep VGG-16 model, our detection system hasa frame rate of 5fps (including all steps) on a GPU, while achievingstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MSCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015competitions, Faster R-CNN and RPN are the foundations of the 1st-place winningentries in several tracks. Code has been made publicly available.
arxiv-14400-16 | Learning Multi-Domain Convolutional Neural Networks for Visual Tracking | http://arxiv.org/pdf/1510.07945v2.pdf | author:Hyeonseob Nam, Bohyung Han category:cs.CV published:2015-10-27 summary:We propose a novel visual tracking algorithm based on the representationsfrom a discriminatively trained Convolutional Neural Network (CNN). Ouralgorithm pretrains a CNN using a large set of videos with trackingground-truths to obtain a generic target representation. Our network iscomposed of shared layers and multiple branches of domain-specific layers,where domains correspond to individual training sequences and each branch isresponsible for binary classification to identify the target in each domain. Wetrain the network with respect to each domain iteratively to obtain generictarget representations in the shared layers. When tracking a target in a newsequence, we construct a new network by combining the shared layers in thepretrained CNN with a new binary classification layer, which is updated online.Online tracking is performed by evaluating the candidate windows randomlysampled around the previous target state. The proposed algorithm illustratesoutstanding performance compared with state-of-the-art methods in existingtracking benchmarks.
arxiv-14400-17 | Incorporating Structural Alignment Biases into an Attentional Neural Translation Model | http://arxiv.org/pdf/1601.01085v1.pdf | author:Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, Gholamreza Haffari category:cs.CL published:2016-01-06 summary:Neural encoder-decoder models of machine translation have achieved impressiveresults, rivalling traditional translation models. However their modellingformulation is overly simplistic, and omits several key inductive biases builtinto traditional models. In this paper we extend the attentional neuraltranslation model to include structural biases from word based alignmentmodels, including positional bias, Markov conditioning, fertility and agreementover translation directions. We show improvements over a baseline attentionalmodel and standard phrase-based model over several language pairs, evaluatingon difficult languages in a low resource setting.
arxiv-14400-18 | Skopus: Exact discovery of the most interesting sequential patterns under Leverage | http://arxiv.org/pdf/1506.08009v2.pdf | author:Francois Petitjean, Tao Li, Nikolaj Tatti, Geoffrey I. Webb category:cs.AI cs.LG stat.ML published:2015-06-26 summary:This paper presents a framework for exact discovery of the most interestingsequential patterns. It combines (1) a novel definition of the expected supportfor a sequential pattern - a concept on which most interestingness measuresdirectly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exactdiscovery of top-k sequential patterns under a given measure of interest. Ourinterestingness measure is based on comparing the pattern support with theaverage support of its sister patterns, obtained by permuting (to certainextent) the items of the pattern. The larger the support compared to theexpectation, the more interesting is the pattern. We build on these twoelements to exactly extract the k sequential patterns with highest leverage,consistent with our definition of expected support. We conduct experiments onboth synthetic data with known patterns and real-world datasets; bothexperiments confirm the consistency and relevance of our approach with regardto the state of the art.
arxiv-14400-19 | Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism | http://arxiv.org/pdf/1601.01073v1.pdf | author:Orhan Firat, Kyunghyun Cho, Yoshua Bengio category:cs.CL stat.ML published:2016-01-06 summary:We propose multi-way, multilingual neural machine translation. The proposedapproach enables a single neural translation model to translate betweenmultiple languages, with a number of parameters that grows only linearly withthe number of languages. This is made possible by having a single attentionmechanism that is shared across all language pairs. We train the proposedmulti-way, multilingual model on ten language pairs from WMT'15 simultaneouslyand observe clear performance improvements over models trained on only onelanguage pair. In particular, we observe that the proposed model significantlyimproves the translation quality of low-resource language pairs.
arxiv-14400-20 | Low-rank Matrix Factorization under General Mixture Noise Distributions | http://arxiv.org/pdf/1601.01060v1.pdf | author:Xiangyong Cao, Qian Zhao, Deyu Meng, Yang Chen, Zongben Xu category:cs.CV published:2016-01-06 summary:Many computer vision problems can be posed as learning a low-dimensionalsubspace from high dimensional data. The low rank matrix factorization (LRMF)represents a commonly utilized subspace learning strategy. Most of the currentLRMF techniques are constructed on the optimization problems using L1-norm andL2-norm losses, which mainly deal with Laplacian and Gaussian noises,respectively. To make LRMF capable of adapting more complex noise, this paperproposes a new LRMF model by assuming noise as Mixture of Exponential Power(MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combiningthe penalized likelihood method with MoEP distributions. Such settingfacilitates the learned LRMF model capable of automatically fitting the realnoise through MoEP distributions. Each component in this mixture is adaptedfrom a series of preliminary super- or sub-Gaussian candidates. Moreover, byfacilitating the local continuity of noise components, we embed Markov randomfield into the PMoEP model and further propose the advanced PMoEP-MRF model. AnExpectation Maximization (EM) algorithm and a variational EM (VEM) algorithmare also designed to infer the parameters involved in the proposed PMoEP andthe PMoEP-MRF model, respectively. The superseniority of our methods isdemonstrated by extensive experiments on synthetic data, face modeling,hyperspectral image restoration and background subtraction.
arxiv-14400-21 | Achieving Exact Cluster Recovery Threshold via Semidefinite Programming | http://arxiv.org/pdf/1412.6156v2.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.DS math.PR published:2014-11-24 summary:The binary symmetric stochastic block model deals with a random graph of $n$vertices partitioned into two equal-sized clusters, such that each pair ofvertices is connected independently with probability $p$ within clusters and$q$ across clusters. In the asymptotic regime of $p=a \log n/n$ and $q=b \logn/n$ for fixed $a,b$ and $n \to \infty$, we show that the semidefiniteprogramming relaxation of the maximum likelihood estimator achieves the optimalthreshold for exactly recovering the partition from the graph with probabilitytending to one, resolving a conjecture of Abbe et al. \cite{Abbe14}.Furthermore, we show that the semidefinite programming relaxation also achievesthe optimal recovery threshold in the planted dense subgraph model containing asingle cluster of size proportional to $n$.
arxiv-14400-22 | L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs | http://arxiv.org/pdf/1511.08102v2.pdf | author:Matey Neykov, Jun S. Liu, Tianxi Cai category:math.ST stat.ML stat.TH published:2015-11-25 summary:It is known that for a certain class of single index models (SIM) $Y =f(\boldsymbol{X}^\intercal\boldsymbol{\beta}_0, \varepsilon)$, support recoveryis impossible when $\boldsymbol{X} \sim \mathcal{N}(0, \mathbb{I}_{p \timesp})$ and a model complexity adjusted sample size is below a critical threshold.Recently, optimal algorithms based on Sliced Inverse Regression (SIR) weresuggested. These algorithms work provably under the assumption that the designmatrix $\boldsymbol{X}$ comes from an i.i.d. Gaussian distribution. In thepresent paper we analyze algorithms based on covariance screening and leastsquares with $L_1$ penalization (i.e. LASSO) and demonstrate that they can alsoenjoy optimal (up to a scalar) rescaled sample size in terms of supportrecovery, albeit under slightly different assumptions on $f$ and $\varepsilon$compared to the SIR based algorithms. Furthermore, we show more generally, thatLASSO succeeds in recovering the signed support of $\boldsymbol{\beta}_0$ if$\boldsymbol{X} \sim \mathcal{N}(0, \boldsymbol{\Sigma})$, and the covariance$\boldsymbol{\Sigma}$ satisfies the irrepresentable condition. Our work extendsexisting results on the support recovery of LASSO for the linear model, to acertain class of SIM.
arxiv-14400-23 | Forecasting Social Navigation in Crowded Complex Scenes | http://arxiv.org/pdf/1601.00998v1.pdf | author:Alexandre Robicquet, Alexandre Alahi, Amir Sadeghian, Bryan Anenberg, John Doherty, Eli Wu, Silvio Savarese category:cs.CV cs.RO cs.SI published:2016-01-05 summary:When humans navigate a crowed space such as a university campus or thesidewalks of a busy street, they follow common sense rules based on socialetiquette. In this paper, we argue that in order to enable the design of newalgorithms that can take fully advantage of these rules to better solve taskssuch as target tracking or trajectory forecasting, we need to have access tobetter data in the first place. To that end, we contribute the very first largescale dataset (to the best of our knowledge) that collects images and videos ofvarious types of targets (not just pedestrians, but also bikers, skateboarders,cars, buses, golf carts) that navigate in a real-world outdoor environment suchas a university campus. We present an extensive evaluation where differentmethods for trajectory forecasting are evaluated and compared. Moreover, wepresent a new algorithm for trajectory prediction that exploits the complexityof our new dataset and allows to: i) incorporate inter-class interactions intotrajectory prediction models (e.g, pedestrian vs bike) as opposed to justintra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degreeto which the social forces are regulating an interaction. We call the latter"social sensitivity"and it captures the sensitivity to which a target isresponding to a certain interaction. An extensive experimental evaluationdemonstrates the effectiveness of our novel approach.
arxiv-14400-24 | Crater Detection via Convolutional Neural Networks | http://arxiv.org/pdf/1601.00978v1.pdf | author:Joseph Paul Cohen, Henry Z. Lo, Tingting Lu, Wei Ding category:cs.CV published:2016-01-05 summary:Craters are among the most studied geomorphic features in the Solar Systembecause they yield important information about the past and present geologicalprocesses and provide information about the relative ages of observed geologicformations. We present a method for automatic crater detection using advancedmachine learning to deal with the large amount of satellite imagery collected.The challenge of automatically detecting craters comes from their is complexsurface because their shape erodes over time to blend into the surface.Bandeira provided a seminal dataset that embodied this challenge that is stillan unsolved pattern recognition problem to this day. There has been work tosolve this challenge based on extracting shape and contrast features and thenapplying classification models on those features. The limiting factor in thisexisting work is the use of hand crafted filters on the image such as Gabor orSobel filters or Haar features. These hand crafted methods rely on domainknowledge to construct. We would like to learn the optimal filters and featuresbased on training examples. In order to dynamically learn filters and featureswe look to Convolutional Neural Networks (CNNs) which have shown theirdominance in computer vision. The power of CNNs is that they can learn imagefilters which generate features for high accuracy classification.
arxiv-14400-25 | Optimally Pruning Decision Tree Ensembles With Feature Cost | http://arxiv.org/pdf/1601.00955v1.pdf | author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.LG published:2016-01-05 summary:We consider the problem of learning decision rules for prediction withfeature budget constraint. In particular, we are interested in pruning anensemble of decision trees to reduce expected feature cost while maintaininghigh prediction accuracy for any test example. We propose a novel 0-1 integerprogram formulation for ensemble pruning. Our pruning formulation is general -it takes any ensemble of decision trees as input. By explicitly accounting forfeature-sharing across trees together with accuracy/cost trade-off, our methodis able to significantly reduce feature cost by pruning subtrees that introducemore loss in terms of feature cost than benefit in terms of prediction accuracygain. Theoretically, we prove that a linear programming relaxation produces theexact solution of the original integer program. This allows us to use efficientconvex optimization tools to obtain an optimally pruned ensemble for any givenbudget. Empirically, we see that our pruning algorithm significantly improvesthe performance of the state of the art ensemble method BudgetRF.
arxiv-14400-26 | Gradient Estimation Using Stochastic Computation Graphs | http://arxiv.org/pdf/1506.05254v3.pdf | author:John Schulman, Nicolas Heess, Theophane Weber, Pieter Abbeel category:cs.LG published:2015-06-17 summary:In a variety of problems originating in supervised, unsupervised, andreinforcement learning, the loss function is defined by an expectation over acollection of random variables, which might be part of a probabilistic model orthe external world. Estimating the gradient of this loss function, usingsamples, lies at the core of gradient-based learning algorithms for theseproblems. We introduce the formalism of stochastic computationgraphs---directed acyclic graphs that include both deterministic functions andconditional probability distributions---and describe how to easily andautomatically derive an unbiased estimator of the loss function's gradient. Theresulting algorithm for computing the gradient estimator is a simplemodification of the standard backpropagation algorithm. The generic scheme wepropose unifies estimators derived in variety of prior work, along withvariance-reduction techniques therein. It could assist researchers indeveloping intricate models involving a combination of stochastic anddeterministic operations, enabling, for example, attention, memory, and controlactions.
arxiv-14400-27 | Complex Decomposition of the Negative Distance kernel | http://arxiv.org/pdf/1601.00925v1.pdf | author:Tim vor der Brück, Steffen Eger, Alexander Mehler category:cs.LG published:2016-01-05 summary:A Support Vector Machine (SVM) has become a very popular machine learningmethod for text classification. One reason for this relates to the range ofexisting kernels which allow for classifying data that is not linearlyseparable. The linear, polynomial and RBF (Gaussian Radial Basis Function)kernel are commonly used and serve as a basis of comparison in our study. Weshow how to derive the primal form of the quadratic Power Kernel (PK) -- alsocalled the Negative Euclidean Distance Kernel (NDK) -- by means of complexnumbers. We exemplify the NDK in the framework of text categorization using theDewey Document Classification (DDC) as the target scheme. Our evaluation showsthat the power kernel produces F-scores that are comparable to the referencekernels, but is -- except for the linear kernel -- faster to compute. Finally,we show how to extend the NDK-approach by including the Mahalanobis distance.
arxiv-14400-28 | The high-conductance state enables neural sampling in networks of LIF neurons | http://arxiv.org/pdf/1601.00909v1.pdf | author:Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE physics.bio-ph published:2016-01-05 summary:The apparent stochasticity of in-vivo neural circuits has long beenhypothesized to represent a signature of ongoing stochastic inference in thebrain. More recently, a theoretical framework for neural sampling has beenproposed, which explains how sample-based inference can be performed bynetworks of spiking neurons. One particular requirement of this approach isthat the neural response function closely follows a logistic curve. Analytical approaches to calculating neural response functions have been thesubject of many theoretical studies. In order to make the problem tractable,particular assumptions regarding the neural or synaptic parameters are usuallymade. However, biologically significant activity regimes exist which are notcovered by these approaches: Under strong synaptic bombardment, as is often thecase in cortex, the neuron is shifted into a high-conductance state (HCS)characterized by a small membrane time constant. In this regime, synaptic timeconstants and refractory periods dominate membrane dynamics. The core idea of our approach is to separately consider two different "modes"of spiking dynamics: burst spiking and transient quiescence, in which theneuron does not spike for longer periods. We treat the former by propagatingthe PDF of the effective membrane potential from spike to spike within a burst,while using a diffusion approximation for the latter. We find that ourprediction of the neural response function closely matches simulation data.Moreover, in the HCS scenario, we show that the neural response functionbecomes symmetric and can be well approximated by a logistic function, therebyproviding the correct dynamics in order to perform neural sampling. We herebyprovide not only a normative framework for Bayesian inference in cortex, butalso powerful applications of low-power, accelerated neuromorphic systems torelevant machine learning tasks.
arxiv-14400-29 | Joint learning of ontology and semantic parser from text | http://arxiv.org/pdf/1601.00901v1.pdf | author:Janez Starc, Dunja Mladenić category:cs.AI cs.CL published:2016-01-05 summary:Semantic parsing methods are used for capturing and representing semanticmeaning of text. Meaning representation capturing all the concepts in the textmay not always be available or may not be sufficiently complete. Ontologiesprovide a structured and reasoning-capable way to model the content of acollection of texts. In this work, we present a novel approach to jointlearning of ontology and semantic parser from text. The method is based onsemi-automatic induction of a context-free grammar from semantically annotatedtext. The grammar parses the text into semantic trees. Both, the grammar andthe semantic trees are used to learn the ontology on several levels -- classes,instances, taxonomic and non-taxonomic relations. The approach was evaluated onthe first sentences of Wikipedia pages describing people.
arxiv-14400-30 | The Role of Context Types and Dimensionality in Learning Word Embeddings | http://arxiv.org/pdf/1601.00893v1.pdf | author:Oren Melamud, David McClosky, Siddharth Patwardhan, Mohit Bansal category:cs.CL published:2016-01-05 summary:We provide the first extensive evaluation of how using different types ofcontext to learn skip-gram word embeddings affects performance on a wide rangeof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsictasks tend to exhibit a clear preference to particular types of contexts andhigher dimensionality, more careful tuning is required for finding the optimalsettings for most of the extrinsic tasks that we considered. Furthermore, forthese extrinsic tasks, we find that once the benefit from increasing theembedding dimensionality is mostly exhausted, simple concatenation of wordembeddings, learned with different context types, can yield further performancegains. As an additional contribution, we propose a new variant of the skip-grammodel that learns word embeddings from weighted contexts of substitute words.
arxiv-14400-31 | MMSE of probabilistic low-rank matrix estimation: Universality with respect to the output channel | http://arxiv.org/pdf/1507.03857v2.pdf | author:Thibault Lesieur, Florent Krzakala, Lenka Zdeborová category:cs.IT math.IT stat.ML published:2015-07-14 summary:This paper considers probabilistic estimation of a low-rank matrix fromnon-linear element-wise measurements of its elements. We derive thecorresponding approximate message passing (AMP) algorithm and its stateevolution. Relying on non-rigorous but standard assumptions motivated bystatistical physics, we characterize the minimum mean squared error (MMSE)achievable information theoretically and with the AMP algorithm. Unlike inrelated problems of linear estimation, in the present setting the MMSE dependson the output channel only trough a single parameter - its Fisher information.We illustrate this striking finding by analysis of submatrix localization, andof detection of communities hidden in a dense stochastic block model. For thisexample we locate the computational and statistical boundaries that are notequal for rank larger than four.
arxiv-14400-32 | On the convergence of cycle detection for navigational reinforcement learning | http://arxiv.org/pdf/1511.08724v2.pdf | author:Tom J. Ameloot, Jan Van den Bussche category:cs.LG cs.AI published:2015-11-27 summary:We consider a reinforcement learning framework where agents have to navigatefrom start states to goal states. We prove convergence of a cycle-detectionlearning algorithm on a class of tasks that we call reducible. Reducible taskshave an acyclic solution. We also syntactically characterize the form of thefinal policy. This characterization can be used to precisely detect theconvergence point in a simulation. Our result demonstrates that even simplealgorithms can be successful in learning a large class of nontrivial tasks. Inaddition, our framework is elementary in the sense that we only use basicconcepts to formally prove convergence.
arxiv-14400-33 | Gamifying Video Object Segmentation | http://arxiv.org/pdf/1601.00825v1.pdf | author:Simone Palazzo, Concetto Spampinato, Daniela Giordano category:cs.CV published:2016-01-05 summary:Video object segmentation can be considered as one of the most challengingcomputer vision problems. Indeed, so far, no existing solution is able toeffectively deal with the peculiarities of real-world videos, especially incases of articulated motion and object occlusions; limitations that appear moreevident when we compare their performance with the human one. However, manuallysegmenting objects in videos is largely impractical as it requires a lot ofhuman time and concentration. To address this problem, in this paper we proposean interactive video object segmentation method, which exploits, on one hand,the capability of humans to identify correctly objects in visual scenes, and onthe other hand, the collective human brainpower to solve challenging tasks. Inparticular, our method relies on a web game to collect human inputs on objectlocations, followed by an accurate segmentation phase achieved by optimizing anenergy function encoding spatial and temporal constraints between objectregions as well as human-provided input. Performance analysis carried out onchallenging video datasets with some users playing the game demonstrated thatour method shows a better trade-off between annotation times and segmentationaccuracy than interactive video annotation and automated video objectsegmentation approaches.
arxiv-14400-34 | Geometry-Aware Neighborhood Search for Learning Local Models for Image Reconstruction | http://arxiv.org/pdf/1505.01429v3.pdf | author:Julio Cesar Ferreira, Elif Vural, Christine Guillemot category:cs.CV cs.IT math.IT math.OC published:2015-05-06 summary:Local learning of sparse image models has proven to be very effective tosolve inverse problems in many computer vision applications. To learn suchmodels, the data samples are often clustered using the K-means algorithm withthe Euclidean distance as a dissimilarity metric. However, the Euclideandistance may not always be a good dissimilarity measure for comparing datasamples lying on a manifold. In this paper, we propose two algorithms fordetermining a local subset of training samples from which a good local modelcan be computed for reconstructing a given input test sample, where we takeinto account the underlying geometry of the data. The first algorithm, calledAdaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive schemewhich can be seen as an out-of-sample extension of the replicator graphclustering method for local model learning. The second method, calledGeometry-driven Overlapping Clusters (GOC), is a less complex nonadaptivealternative for training subset selection. The proposed AGNN and GOC methodsare evaluated in image super-resolution, deblurring and denoising applicationsand shown to outperform spectral clustering, soft clustering, and geodesicdistance based subset selection in most settings.
arxiv-14400-35 | Open challenges in understanding development and evolution of speech forms: The roles of embodied self-organization, motivation and active exploration | http://arxiv.org/pdf/1601.00816v1.pdf | author:Pierre-Yves Oudeyer category:cs.AI cs.CL cs.CY cs.LG published:2016-01-05 summary:This article discusses open scientific challenges for understandingdevelopment and evolution of speech forms, as a commentary to Moulin-Frier etal. (Moulin-Frier et al., 2015). Based on the analysis of mathematical modelsof the origins of speech forms, with a focus on their assumptions , we studythe fundamental question of how speech can be formed out of non--speech, atboth developmental and evolutionary scales. In particular, we emphasize theimportance of embodied self-organization , as well as the role of mechanisms ofmotivation and active curiosity-driven exploration in speech formation. Finally, we discuss an evolutionary-developmental perspective of the origins ofspeech.
arxiv-14400-36 | Probabilistic Programming with Gaussian Process Memoization | http://arxiv.org/pdf/1512.05665v2.pdf | author:Ulrich Schaechtle, Ben Zinberg, Alexey Radul, Kostas Stathis, Vikash K. Mansinghka category:cs.LG cs.AI stat.ML published:2015-12-17 summary:Gaussian Processes (GPs) are widely used tools in statistics, machinelearning, robotics, computer vision, and scientific computation. However,despite their popularity, they can be difficult to apply; all but the simplestclassification or regression applications require specification and inferenceover complex covariance functions that do not admit simple analyticalposteriors. This paper shows how to embed Gaussian processes in anyhigher-order probabilistic programming language, using an idiom based onmemoization, and demonstrates its utility by implementing and extending classicand state-of-the-art GP applications. The interface to Gaussian processes,called gpmem, takes an arbitrary real-valued computational process as input andreturns a statistical emulator that automatically improve as the originalprocess is invoked and its input-output behavior is recorded. The flexibilityof gpmem is illustrated via three applications: (i) robust GP regression withhierarchical hyper-parameter learning, (ii) discovering symbolic expressionsfrom time-series data by fully Bayesian structure learning over kernelsgenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesianoptimization with automatic inference and action selection. All applicationsshare a single 50-line Python library and require fewer than 20 lines ofprobabilistic code each.
arxiv-14400-37 | Robust Method of Vote Aggregation and Proposition Verification for Invariant Local Features | http://arxiv.org/pdf/1601.00781v1.pdf | author:Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas category:cs.CV published:2016-01-05 summary:This paper presents a method for analysis of the vote space created from thelocal features extraction process in a multi-detection system. The method isopposed to the classic clustering approach and gives a high level of controlover the clusters composition for further verification steps. Proposed methodcomprises of the graphical vote space presentation, the proposition generation,the two-pass iterative vote aggregation and the cascade filters forverification of the propositions. Cascade filters contain all of the minoralgorithms needed for effective object detection verification. The new approachdoes not have the drawbacks of the classic clustering approaches and gives asubstantial control over process of detection. Method exhibits an exceptionallyhigh detection rate in conjunction with a low false detection chance incomparison to alternative methods.
arxiv-14400-38 | Deconstructing the Ladder Network Architecture | http://arxiv.org/pdf/1511.06430v3.pdf | author:Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, Yoshua Bengio category:cs.LG published:2015-11-19 summary:The Manual labeling of data is and will remain a costly endeavor. For thisreason, semi-supervised learning remains a topic of practical importance. Therecently proposed Ladder Network is one such approach that has proven to bevery successful. In addition to the supervised objective, the Ladder Networkalso adds an unsupervised objective corresponding to the reconstruction costsof a stack of denoising autoencoders. Although the empirical results areimpressive, the Ladder Network has many components intertwined, whosecontributions are not obvious in such a complex architecture. In order to helpelucidate and disentangle the different ingredients in the Ladder Networkrecipe, this paper presents an extensive experimental investigation of variantsof the Ladder Network in which we replace or remove individual components togain more insight into their relative importance. We find that all of thecomponents are necessary for achieving optimal performance, but they do notcontribute equally. For semi-supervised tasks, we conclude that the mostimportant contribution is made by the lateral connection, followed by theapplication of noise, and finally the choice of what we refer to as the`combinator function' in the decoder path. We also find that as the number oflabeled training examples increases, the lateral connections and reconstructioncriterion become less important, with most of the improvement in generalizationbeing due to the injection of noise in each layer. Furthermore, we present anew type of combinator function that outperforms the original design in bothfully- and semi-supervised tasks, reducing record test error rates onPermutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97%and 1.0% for semi-supervised settings with 1000 and 100 labeled examplesrespectively.
arxiv-14400-39 | Estimating Absolute-Phase Maps Using ESPIRiT and Virtual Conjugate Coils | http://arxiv.org/pdf/1509.03557v2.pdf | author:Martin Uecker, Michael Lustig category:cs.CV cs.CE physics.med-ph published:2015-07-17 summary:Purpose: To develop an ESPIRiT-based method to estimate coil sensitivitieswith image phase as a building block for efficient and robust imagereconstruction with phase constraints. Theory and Methods: ESPIRiT is a newframework for calibration of the coil sensitivities and reconstruction inparallel Magnetic Resonance Imaging (MRI). Applying ESPIRiT to a combined setof physical and virtual conjugate coils (VCC-ESPIRiT) implicitly exploitsconjugate symmetry in k-space similar to VCC-GRAPPA. Based on this method, anew post-processing step is proposed for the explicit computation of coilsensitivities that include the absolute phase of the image. The accuracy of thecomputed maps is directly validated using a test based on projection onto fullysampled coil images and also indirectly in phase-constrained parallel-imagingreconstructions. Results: The proposed method can estimate accuratesensitivities which include low-resolution image phase. In case ofhigh-frequency phase variations VCC-ESPIRiT yields an additional set of mapsthat indicates the existence of a high-frequency phase component. Taking thisadditional set of maps into account can improve the robustness ofphase-constrained parallel imaging. Conclusion: The extended VCC-ESPIRiT is auseful tool for phase-constrained imaging.
arxiv-14400-40 | Efficient Background Modeling Based on Sparse Representation and Outlier Iterative Removal | http://arxiv.org/pdf/1401.6013v2.pdf | author:Linhao Li, Ping Wang, Qinghua Hu, Sijia Cai category:cs.CV published:2014-01-23 summary:Background modeling is a critical component for various vision-basedapplications. Most traditional methods tend to be inefficient when solvinglarge-scale problems. In this paper, we introduce sparse representation intothe task of large scale stable background modeling, and reduce the video sizeby exploring its 'discriminative' frames. A cyclic iteration process is thenproposed to extract the background from the discriminative frame set. The twoparts combine to form our Sparse Outlier Iterative Removal (SOIR) algorithm.The algorithm operates in tensor space to obey the natural data structure ofvideos. Experimental results show that a few discriminative frames determinethe performance of the background extraction. Further, SOIR can achieve highaccuracy and high speed simultaneously when dealing with real video sequences.Thus, SOIR has an advantage in solving large-scale tasks.
arxiv-14400-41 | Sharp Time--Data Tradeoffs for Linear Inverse Problems | http://arxiv.org/pdf/1507.04793v2.pdf | author:Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi category:cs.IT cs.LG math.IT math.OC math.ST stat.TH published:2015-07-16 summary:In this paper we characterize sharp time-data tradeoffs for optimizationproblems used for solving linear inverse problems. We focus on the minimizationof a least-squares objective subject to a constraint defined as the sub-levelset of a penalty function. We present a unified convergence analysis of thegradient projection algorithm applied to such problems. We sharply characterizethe convergence rate associated with a wide variety of random measurementensembles in terms of the number of measurements and structural complexity ofthe signal with respect to the chosen penalty function. The results apply toboth convex and nonconvex constraints, demonstrating that a linear convergencerate is attainable even though the least squares objective is not stronglyconvex in these settings. When specialized to Gaussian measurements our resultsshow that such linear convergence occurs when the number of measurements ismerely 4 times the minimal number required to recover the desired signal at all(a.k.a. the phase transition). We also achieve a slower but geometric rate ofconvergence precisely above the phase transition point. Extensive numericalresults suggest that the derived rates exactly match the empirical performance.
arxiv-14400-42 | Learning Preferences for Manipulation Tasks from Online Coactive Feedback | http://arxiv.org/pdf/1601.00741v1.pdf | author:Ashesh Jain, Shikhar Sharma, Thorsten Joachims, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2016-01-05 summary:We consider the problem of learning preferences over trajectories for mobilemanipulators such as personal robots and assembly line robots. The preferenceswe learn are more intricate than simple geometric constraints on trajectories;they are rather governed by the surrounding context of various objects andhuman interactions in the environment. We propose a coactive online learningframework for teaching preferences in contextually rich environments. The keynovelty of our approach lies in the type of feedback expected from the user:the human user does not need to demonstrate optimal trajectories as trainingdata, but merely needs to iteratively provide trajectories that slightlyimprove over the trajectory currently proposed by the system. We argue thatthis coactive preference feedback can be more easily elicited thandemonstrations of optimal trajectories. Nevertheless, theoretical regret boundsof our algorithm match the asymptotic rates of optimal trajectory algorithms. We implement our algorithm on two high degree-of-freedom robots, PR2 andBaxter, and present three intuitive mechanisms for providing such incrementalfeedback. In our experimental evaluation we consider two context rich settings-- household chores and grocery store checkout -- and show that users are ableto train the robot with just a few feedbacks (taking only a fewminutes).\footnote{Parts of this work has been published at NIPS and ISRRconferences~\citep{Jain13,Jain13b}. This journal submission presents aconsistent full paper, and also includes the proof of regret bounds, moredetails of the robotic system, and a thorough related work.}
arxiv-14400-43 | PlanIt: A Crowdsourcing Approach for Learning to Plan Paths from Large Scale Preference Feedback | http://arxiv.org/pdf/1406.2616v3.pdf | author:Ashesh Jain, Debarghya Das, Jayesh K Gupta, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2014-06-10 summary:We consider the problem of learning user preferences over robot trajectoriesfor environments rich in objects and humans. This is challenging because thecriterion defining a good trajectory varies with users, tasks and interactionsin the environment. We represent trajectory preferences using a cost functionthat the robot learns and uses it to generate good trajectories in newenvironments. We design a crowdsourcing system - PlanIt, where non-expert userslabel segments of the robot's trajectory. PlanIt allows us to collect a largeamount of user feedback, and using the weak and noisy labels from PlanIt welearn the parameters of our model. We test our approach on 122 differentenvironments for robotic navigation and manipulation tasks. Our extensiveexperiments show that the learned cost function generates preferredtrajectories in human environments. Our crowdsourcing system is publiclyavailable for the visualization of the learned costs and for providingpreference feedback: \url{http://planit.cs.cornell.edu}
arxiv-14400-44 | Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture | http://arxiv.org/pdf/1601.00740v1.pdf | author:Ashesh Jain, Hema S Koppula, Shane Soh, Bharad Raghavan, Avi Singh, Ashutosh Saxena category:cs.RO cs.CV cs.LG published:2016-01-05 summary:Advanced Driver Assistance Systems (ADAS) have made driving safer over thelast decade. They prepare vehicles for unsafe road conditions and alert driversif they perform a dangerous maneuver. However, many accidents are unavoidablebecause by the time drivers are alerted, it is already too late. Anticipatingmaneuvers beforehand can alert drivers before they perform the maneuver andalso give ADAS more time to avoid or prepare for the danger. In this work we propose a vehicular sensor-rich platform and learningalgorithms for maneuver anticipation. For this purpose we equip a car withcameras, Global Positioning System (GPS), and a computing device to capture thedriving context from both inside and outside of the car. In order to anticipatemaneuvers, we propose a sensory-fusion deep learning architecture which jointlylearns to anticipate and fuse multiple sensory streams. Our architectureconsists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory(LSTM) units to capture long temporal dependencies. We propose a novel trainingprocedure which allows the network to predict the future given only a partialtemporal context. We introduce a diverse data set with 1180 miles of naturalfreeway and city driving, and show that we can anticipate maneuvers 3.5 secondsbefore they occur in real-time with a precision and recall of 90.5\% and 87.4\%respectively.
arxiv-14400-45 | Matrix Variate RBM and Its Applications | http://arxiv.org/pdf/1601.00722v1.pdf | author:Guanglei Qi, Yanfeng Sun, Junbin Gao, Yongli Hu, Jinghua Li category:cs.CV published:2016-01-05 summary:Restricted Boltzmann Machine (RBM) is an importan- t generative modelmodeling vectorial data. While applying an RBM in practice to images, the datahave to be vec- torized. This results in high-dimensional data and valu- ablespatial information has got lost in vectorization. In this paper, aMatrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed bygeneralizing the classic RBM to explicitly model matrix data. In the new RBMmodel, both input and hidden variables are in matrix forms which are connectedby bilinear transforms. The MVRBM has much less model parameters, resulting ina faster train- ing algorithm while retaining comparable performance as theclassic RBM. The advantages of the MVRBM have been demonstrated on tworeal-world applications: Image super- resolution and handwritten digitrecognition.
arxiv-14400-46 | Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency | http://arxiv.org/pdf/1512.08301v2.pdf | author:Shiliang Zhang, Cong Liu, Hui Jiang, Si Wei, Lirong Dai, Yu Hu category:cs.NE published:2015-12-28 summary:In this paper, we propose a novel neural network structure, namely\emph{feedforward sequential memory networks (FSMN)}, to model long-termdependency in time series without using recurrent feedback. The proposed FSMNis a standard fully-connected feedforward neural network equipped with somelearnable memory blocks in its hidden layers. The memory blocks use atapped-delay line structure to encode the long context information into afixed-size representation as short-term memory mechanism. We have evaluated theproposed FSMNs in several standard benchmark tasks, including speechrecognition and language modelling. Experimental results have shown FSMNssignificantly outperform the conventional recurrent neural networks (RNN),including LSTMs, in modeling sequential signals like speech or language.Moreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMsdue to the inherent non-recurrent model structure.
arxiv-14400-47 | Event Specific Multimodal Pattern Mining with Image-Caption Pairs | http://arxiv.org/pdf/1601.00022v2.pdf | author:Hongzhi Li, Joseph G. Ellis, Shih-Fu Chang category:cs.CV published:2015-12-31 summary:In this paper we describe a novel framework and algorithms for discoveringimage patch patterns from a large corpus of weakly supervised image-captionpairs generated from news events. Current pattern mining techniques attempt tofind patterns that are representative and discriminative, we stipulate that ourdiscovered patterns must also be recognizable by humans and preferably withmeaningful names. We propose a new multimodal pattern mining approach thatleverages the descriptive captions often accompanying news images to learnsemantically meaningful image patch patterns. The mutltimodal patterns are thennamed using words mined from the associated image captions for each pattern. Anovel evaluation framework is provided that demonstrates our patterns are 26.2%more semantically meaningful than those discovered by the state of the artvision only pipeline, and that we can provide tags for the discovered imagespatches with 54.5% accuracy with no direct supervision. Our methods alsodiscover named patterns beyond those covered by the existing image datasetslike ImageNet. To the best of our knowledge this is the first algorithmdeveloped to automatically mine image patch patterns that have strong semanticmeaning specific to high-level news events, and then evaluate these patternsbased on that criteria.
arxiv-14400-48 | Sentiment/Subjectivity Analysis Survey for Languages other than English | http://arxiv.org/pdf/1601.00087v2.pdf | author:Mohammed Korayem category:cs.CL published:2016-01-01 summary:Subjective and sentiment analysis has gained considerable attention recently.Most of the resources and systems built so far are done for English. The needfor designing systems for other languages is increasing. This paper surveysdifferent ways used for building systems for subjective and sentiment analysisfor languages other than English. There are three different types of systemsused for building these systems. The first (and the best) one is the languagespecific systems. The second type of systems involves reusing or transferringsentiment resources from English to the target language. The third type ofmethods is based on using language independent methods. The paper presents aseparate section devoted to Arabic sentiment analysis.
arxiv-14400-49 | Multi-Source Neural Translation | http://arxiv.org/pdf/1601.00710v1.pdf | author:Barret Zoph, Kevin Knight category:cs.CL published:2016-01-05 summary:We build a multi-source machine translation model and train it to maximizethe probability of a target English string given French and German sources.Using the neural encoder-decoder framework, we explore several combinationmethods and report up to +4.8 Bleu increases on top of a very strongattention-based neural translation model.
arxiv-14400-50 | Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis | http://arxiv.org/pdf/1601.00706v1.pdf | author:Jimei Yang, Scott Reed, Ming-Hsuan Yang, Honglak Lee category:cs.LG cs.AI cs.CV published:2016-01-05 summary:An important problem for both graphics and vision is to synthesize novelviews of a 3D object from a single image. This is particularly challenging dueto the partial observability inherent in projecting a 3D object onto the imagespace, and the ill-posedness of inferring object shape and pose. However, wecan train a neural network to address the problem if we restrict our attentionto specific object categories (in our case faces and chairs) for which we cangather ample training data. In this paper, we propose a novel recurrentconvolutional encoder-decoder network that is trained end-to-end on the task ofrendering rotated objects starting from a single image. The recurrent structureallows our model to capture long-term dependencies along a sequence oftransformations. We demonstrate the quality of its predictions for human faceson the Multi-PIE dataset and for a dataset of 3D chair models, and also showits ability to disentangle latent factors of variation (e.g., identity andpose) without using full supervision.
arxiv-14400-51 | Nonlinear Hebbian learning as a unifying principle in receptive field formation | http://arxiv.org/pdf/1601.00701v1.pdf | author:Carlos S. N. Brito, Wulfram Gerstner category:q-bio.NC cs.LG published:2016-01-04 summary:The development of sensory receptive fields has been modeled in the past by avariety of models including normative models such as sparse coding orindependent component analysis and bottom-up models such as spike-timingdependent plasticity or the Bienenstock-Cooper-Munro model of synapticplasticity. Here we show that the above variety of approaches can all beunified into a single common principle, namely Nonlinear Hebbian Learning. WhenNonlinear Hebbian Learning is applied to natural images, receptive field shapeswere strongly constrained by the input statistics and preprocessing, butexhibited only modest variation across different choices of nonlinearities inneuron models or synaptic plasticity rules. Neither overcompleteness nor sparsenetwork activity are necessary for the development of localized receptivefields. The analysis of alternative sensory modalities such as auditory modelsor V2 development lead to the same conclusions. In all examples, receptivefields can be predicted a priori by reformulating an abstract model asnonlinear Hebbian learning. Thus nonlinear Hebbian learning and naturalstatistics can account for many aspects of receptive field formation acrossmodels and sensory modalities.
arxiv-14400-52 | Scalable Models for Computing Hierarchies in Information Networks | http://arxiv.org/pdf/1601.00626v1.pdf | author:Baoxu Shi, Tim Weninger category:cs.AI cs.DL cs.LG published:2016-01-04 summary:Information hierarchies are organizational structures that often used toorganize and present large and complex information as well as provide amechanism for effective human navigation. Fortunately, many statistical andcomputational models exist that automatically generate hierarchies; however,the existing approaches do not consider linkages in information {\em networks}that are increasingly common in real-world scenarios. Current approaches alsotend to present topics as an abstract probably distribution over words, etcrather than as tangible nodes from the original network. Furthermore, thestatistical techniques present in many previous works are not yet capable ofprocessing data at Web-scale. In this paper we present the HierarchicalDocument Topic Model (HDTM), which uses a distributed vertex-programmingprocess to calculate a nonparametric Bayesian generative model. Experiments onthree medium size data sets and the entire Wikipedia dataset show that HDTM caninfer accurate hierarchies even over large information networks.
arxiv-14400-53 | Distant IE by Bootstrapping Using Lists and Document Structure | http://arxiv.org/pdf/1601.00620v1.pdf | author:Lidong Bing, Mingyang Ling, Richard C. Wang, William W. Cohen category:cs.CL published:2016-01-04 summary:Distant labeling for information extraction (IE) suffers from noisy trainingdata. We describe a way of reducing the noise associated with distant IE byidentifying coupling constraints between potential instance labels. As oneexample of coupling, items in a list are likely to have the same label. Asecond example of coupling comes from analysis of document structure: in somecorpora, sections can be identified such that items in the same section arelikely to have the same label. Such sections do not exist in all corpora, butwe show that augmenting a large corpus with coupling constraints from even asmall, well-structured corpus can improve performance substantially, doublingF1 on one task.
arxiv-14400-54 | Performing Highly Accurate Predictions Through Convolutional Networks for Actual Telecommunication Challenges | http://arxiv.org/pdf/1511.04906v2.pdf | author:Jaime Zaratiegui, Ana Montoro, Federico Castanedo category:cs.LG cs.CV published:2015-11-16 summary:We investigated how the application of deep learning, specifically the use ofconvolutional networks trained with GPUs, can help to build better predictivemodels in telecommunication business environments, and fill this gap. Inparticular, we focus on the non-trivial problem of predicting customer churn intelecommunication operators. Our model, called WiseNet, consists of aconvolutional network and a novel encoding method that transforms customeractivity data and Call Detail Records (CDRs) into images. Experimentalevaluation with several machine learning classifiers supports the ability ofWiseNet for learning features when using structured input data. For this typeof telecommunication business problems, we found that WiseNet outperformsmachine learning models with hand-crafted features, and does not require thelabor-intensive step of feature engineering. Furthermore, the same model hasbeen applied without retraining to a different market, achieving consistentresults. This confirms the generalization property of WiseNet and the abilityto extract useful representations.
arxiv-14400-55 | Multimodal Classification of Events in Social Media | http://arxiv.org/pdf/1601.00599v1.pdf | author:Matthias Zeppelzauer, Daniel Schopfhauser category:cs.CV cs.IR cs.MM published:2016-01-04 summary:A large amount of social media hosted on platforms like Flickr and Instagramis related to social events. The task of social event classification refers tothe distinction of event and non-event-related content as well as theclassification of event types (e.g. sports events, concerts, etc.). In thispaper, we provide an extensive study of textual, visual, as well as multimodalrepresentations for social event classification. We investigate strengths andweaknesses of the modalities and study synergy effects between the modalities.Experimental results obtained with our multimodal representation outperformstate-of-the-art methods and provide a new baseline for future research.
arxiv-14400-56 | Interacting Behavior and Emerging Complexity | http://arxiv.org/pdf/1512.07450v3.pdf | author:Alyssa Adams, Hector Zenil, Eduardo Hermo Reyes, Joost Joosten category:cs.NE cs.CC nlin.CG q-bio.PE published:2015-12-23 summary:Can we quantify the change of complexity throughout evolutionary processes?We attempt to address this question through an empirical approach. In verygeneral terms, we simulate two simple organisms on a computer that compete overlimited available resources. We implement Global Rules that determine theinteraction between two Elementary Cellular Automata on the same grid. GlobalRules change the complexity of the state evolution output which suggests thatsome complexity is intrinsic to the interaction rules themselves. The largestincreases in complexity occurred when the interacting elementary rules had verylittle complexity, suggesting that they are able to accept complexity throughinteraction only. We also found that some Class 3 or 4 CA rules are morefragile than others to Global Rules, while others are more robust, hencesuggesting some intrinsic properties of the rules independent of the GlobalRule choice. We provide statistical mappings of Elementary Cellular Automataexposed to Global Rules and different initial conditions onto differentcomplexity classes.
arxiv-14400-57 | Robust non-linear regression analysis: A greedy approach employing kernels and application to image denoising | http://arxiv.org/pdf/1601.00595v1.pdf | author:George Papageorgiou, Pantelis Bouboulis, Sergios Theodoridis category:cs.LG stat.ML published:2016-01-04 summary:We consider the task of robust non-linear estimation in the presence of bothbounded noise and outliers. Assuming that the unknown non-linear functionbelongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to accuratelyestimate the coefficients of the kernel regression matrix. Due to the existenceof outliers, common techniques such as the Kernel Ridge Regression (KRR), orthe Support Vector Regression (SVR) turn out to be inadequate. Instead, weemploy sparse modeling arguments to model and estimate the outliers, adopting agreedy approach. In particular, the proposed robust scheme, i.e., Kernel GreedyAlgorithm for Robust Denoising (KGARD), is a modification of the classicalOrthogonal Matching Pursuit (OMP) algorithm. In a nutshell, the proposed schemealternates between a KRR task and an OMP-like selection step. Convergenceproperties as well as theoretical results concerning the identification of theoutliers are provided. Moreover, KGARD is compared against other cutting edgemethods (using toy examples) to demonstrate its performance and verify theaforementioned theoretical results. Finally, the proposed robust estimationframework is applied to the task of image denoising, showing that it canenhance the denoising process significantly, when outliers are present.
arxiv-14400-58 | NFL Play Prediction | http://arxiv.org/pdf/1601.00574v1.pdf | author:Brendan Teich, Roman Lutz, Valentin Kassarnig category:cs.LG published:2016-01-04 summary:Based on NFL game data we try to predict the outcome of a play in multipledifferent ways. An application of this is the following: by plugging in variousplay options one could determine the best play for a given situation in realtime. While the outcome of a play can be described in many ways we had the mostpromising results with a newly defined measure that we call "progress". We seethis work as a first step to include predictive analysis into NFL playcalling.
arxiv-14400-59 | Semi-supervised Tuning from Temporal Coherence | http://arxiv.org/pdf/1511.03163v3.pdf | author:Davide Maltoni, Vincenzo Lomonaco category:cs.LG stat.ML published:2015-11-10 summary:Recent works demonstrated the usefulness of temporal coherence to regularizesupervised training or to learn invariant features with deep architectures. Inparticular, enforcing smooth output changes while presenting temporally-closedframes from video sequences, proved to be an effective strategy. In this paperwe prove the efficacy of temporal coherence for semi-supervised incrementaltuning. We show that a deep architecture, just mildly trained in a supervisedmanner, can progressively improve its classification accuracy, if exposed tovideo sequences of unlabeled data. The extent to which, in some cases, asemi-supervised tuning allows to improve classification accuracy (approachingthe supervised one) is somewhat surprising. A number of control experimentspointed out the fundamental role of temporal coherence.
arxiv-14400-60 | Approximate Message Passing with Nearest Neighbor Sparsity Pattern Learning | http://arxiv.org/pdf/1601.00543v1.pdf | author:Xiangming Meng, Sheng Wu, Linling Kuang, Defeng, Huang, Jianhua Lu category:cs.IT cs.LG math.IT published:2016-01-04 summary:We consider the problem of recovering clustered sparse signals with no priorknowledge of the sparsity pattern. Beyond simple sparsity, signals of interestoften exhibits an underlying sparsity pattern which, if leveraged, can improvethe reconstruction performance. However, the sparsity pattern is usuallyunknown a priori. Inspired by the idea of k-nearest neighbor (k-NN) algorithm,we propose an efficient algorithm termed approximate message passing withnearest neighbor sparsity pattern learning (AMP-NNSPL), which learns thesparsity pattern adaptively. AMP-NNSPL specifies a flexible spike and slabprior on the unknown signal and, after each AMP iteration, sets the sparseratios as the average of the nearest neighbor estimates via expectationmaximization (EM). Experimental results on both synthetic and real datademonstrate the superiority of our proposed algorithm both in terms ofreconstruction performance and computational complexity.
arxiv-14400-61 | Denoising Criterion for Variational Auto-Encoding Framework | http://arxiv.org/pdf/1511.06406v2.pdf | author:Daniel Jiwoong Im, Sungjin Ahn, Roland Memisevic, Yoshua Bengio category:cs.LG published:2015-11-19 summary:Denoising autoencoders (DAE) are trained to reconstruct their clean inputswith noise injected at the input level, while variational autoencoders (VAE)are trained with noise injected in their stochastic hidden layer, with aregularizer that encourages this noise injection. In this paper, we show thatinjecting noise both in input and in the stochastic hidden layer can beadvantageous and we propose a modified variational lower bound as an improvedobjective function in this setup. When input is corrupted, then the standardVAE lower bound involves marginalizing the encoder conditional distributionover the input noise, which makes the training criterion intractable. Instead,we propose a modified training criterion which corresponds to a tractable boundwhen input is corrupted. Experimentally, we find that the proposed denoisingvariational autoencoder (DVAE) yields better average log-likelihood than theVAE and the importance weighted autoencoder on the MNIST and Frey Facedatasets.
arxiv-14400-62 | Modelling-based experiment retrieval: A case study with gene expression clustering | http://arxiv.org/pdf/1505.05007v4.pdf | author:Paul Blomstedt, Ritabrata Dutta, Sohan Seth, Alvis Brazma, Samuel Kaski category:stat.ML cs.IR cs.LG published:2015-05-19 summary:Motivation: Public and private repositories of experimental data are growingto sizes that require dedicated methods for finding relevant data. To improveon the state of the art of keyword searches from annotations, methods forcontent-based retrieval have been proposed. In the context of gene expressionexperiments, most methods retrieve gene expression profiles, requiring eachexperiment to be expressed as a single profile, typically of case vs. control.A more general, recently suggested alternative is to retrieve experiments whosemodels are good for modelling the query dataset. However, for very noisy andhigh-dimensional query data, this retrieval criterion turns out to be verynoisy as well. Results: We propose doing retrieval using a denoised model of the querydataset, instead of the original noisy dataset itself. To this end, weintroduce a general probabilistic framework, where each experiment is modelledseparately and the retrieval is done by finding related models. For retrievalof gene expression experiments, we use a probabilistic model called productpartition model, which induces a clustering of genes that show similarexpression patterns across a number of samples. The suggested metric forretrieval using clusterings is the normalized information distance. Empiricalresults finally suggest that inference for the full probabilistic model can beapproximated with good performance using computationally faster heuristicclustering approaches (e.g. $k$-means). The method is highly scalable andstraightforward to apply to construct a general-purpose gene expressionexperiment retrieval method. Availability: The method can be implemented using standard clusteringalgorithms and normalized information distance, available in many statisticalsoftware packages.
arxiv-14400-63 | Learning relationships between data obtained independently | http://arxiv.org/pdf/1601.00504v1.pdf | author:Alexandra Carpentier, Teresa Schlueter category:stat.ML published:2016-01-04 summary:The aim of this paper is to provide a new method for learning therelationships between data that have been obtained independently. Unlikeexisting methods like matching, the proposed technique does not require anycontextual information, provided that the dependency between the variables ofinterest is monotone. It can therefore be easily combined with matching inorder to exploit the advantages of both methods. This technique can bedescribed as a mix between quantile matching, and deconvolution. We provide forit a theoretical and an empirical validation.
arxiv-14400-64 | Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data | http://arxiv.org/pdf/1601.00496v1.pdf | author:Søren F. V. Nielsen, Kristoffer H. Madsen, Rasmus Røge, Mikkel N. Schmidt, Morten Mørup category:stat.AP q-bio.NC stat.ML published:2016-01-04 summary:Dynamic functional connectivity (FC) has in recent years become a topic ofinterest in the neuroimaging community. Several models and methods exist forboth functional magnetic resonance imaging (fMRI) and electroencephalography(EEG), and the results point towards the conclusion that FC exhibits dynamicchanges. The existing approaches modeling dynamic connectivity have primarilybeen based on time-windowing the data and k-means clustering. We propose anon-parametric generative model for dynamic FC in fMRI that does not rely onspecifying window lengths and number of dynamic states. Rooted in Bayesianstatistical modeling we use the predictive likelihood to investigate if themodel can discriminate between a motor task and rest both within and acrosssubjects. We further investigate what drives dynamic states using the model onthe entire data collated across subjects and task/rest. We find that the numberof states extracted are driven by subject variability and preprocessingdifferences while the individual states are almost purely defined by eithertask or rest. This questions how we in general interpret dynamic FC and pointsto the need for more research on what drives dynamic FC.
arxiv-14400-65 | Fitting Spectral Decay with the $k$-Support Norm | http://arxiv.org/pdf/1601.00449v1.pdf | author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:cs.LG stat.ML published:2016-01-04 summary:The spectral $k$-support norm enjoys good estimation properties in low rankmatrix learning problems, empirically outperforming the trace norm. Its unitball is the convex hull of rank $k$ matrices with unit Frobenius norm. In thispaper we generalize the norm to the spectral $(k,p)$-support norm, whoseadditional parameter $p$ can be used to tailor the norm to the decay of thespectrum of the underlying model. We characterize the unit ball and weexplicitly compute the norm. We further provide a conditional gradient methodto solve regularization problems with the norm, and we derive an efficientalgorithm to compute the Euclidean projection on the unit ball in the case$p=\infty$. In numerical experiments, we show that allowing $p$ to varysignificantly improves performance over the spectral $k$-support norm onvarious matrix completion benchmarks, and better captures the spectral decay ofthe underlying model.
arxiv-14400-66 | Kernel Sparse Subspace Clustering on Symmetric Positive Definite Manifolds | http://arxiv.org/pdf/1601.00414v1.pdf | author:Ming Yin, Yi Guo, Junbin Gao, Zhaoshui He, Shengli Xie category:cs.CV published:2016-01-04 summary:Sparse subspace clustering (SSC), as one of the most successful subspaceclustering methods, has achieved notable clustering accuracy in computer visiontasks. However, SSC applies only to vector data in Euclidean space. As such,there is still no satisfactory approach to solve subspace clustering by ${\itself-expressive}$ principle for symmetric positive definite (SPD) matriceswhich is very useful in computer vision. In this paper, by embedding the SPDmatrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspaceclustering method is constructed on the SPD manifold through an appropriateLog-Euclidean kernel, termed as kernel sparse subspace clustering on the SPDRiemannian manifold (KSSCR). By exploiting the intrinsic Riemannian geometrywithin data, KSSCR can effectively characterize the geodesic distance betweenSPD matrices to uncover the underlying subspace structure. Experimental resultson two famous database demonstrate that the proposed method achieves betterclustering results than the state-of-the-art approaches.
arxiv-14400-67 | Convergence of Stochastic Gradient Descent for PCA | http://arxiv.org/pdf/1509.09002v2.pdf | author:Ohad Shamir category:cs.LG math.OC stat.ML published:2015-09-30 summary:We consider the problem of principal component analysis (PCA) in a streamingstochastic setting, where our goal is to find a direction of approximatemaximal variance, based on a stream of i.i.d. data points in $\reals^d$. Asimple and computationally cheap algorithm for this is stochastic gradientdescent (SGD), which incrementally updates its estimate based on each new datapoint. However, due to the non-convex nature of the problem, analyzing itsperformance has been a challenge. In particular, existing guarantees rely on anon-trivial eigengap assumption on the covariance matrix, which is intuitivelyunnecessary. In this paper, we provide (to the best of our knowledge) the firsteigengap-free convergence guarantees for SGD in the context of PCA. This alsopartially resolves an open problem posed in \cite{hardt2014noisy}. Moreover,under an eigengap assumption, we show that the same techniques lead to new SGDconvergence guarantees with better dependence on the eigengap.
arxiv-14400-68 | Multi-task CNN Model for Attribute Prediction | http://arxiv.org/pdf/1601.00400v1.pdf | author:Abrar H. Abdulnabi, Gang Wang, Jiwen Lu, Kui Jia category:cs.CV published:2016-01-04 summary:This paper proposes a joint multi-task learning algorithm to better predictattributes in images using deep convolutional neural networks (CNN). Weconsider learning binary semantic attributes through a multi-task CNN model,where each CNN will predict one binary attribute. The multi-task learningallows CNN models to simultaneously share visual knowledge among differentattribute categories. Each CNN will generate attribute-specific featurerepresentations, and then we apply multi-task learning on the features topredict their attributes. In our multi-task framework, we propose a method todecompose the overall model's parameters into a latent task matrix andcombination matrix. Furthermore, under-sampled classifiers can leverage sharedstatistics from other classifiers to improve their performance. Naturalgrouping of attributes is applied such that attributes in the same group areencouraged to share more knowledge. Meanwhile, attributes in different groupswill generally compete with each other, and consequently share less knowledge.We show the effectiveness of our method on two popular attribute datasets.
arxiv-14400-69 | Automatic Detection and Decoding of Photogrammetric Coded Targets | http://arxiv.org/pdf/1601.00396v1.pdf | author:Udaya Wijenayake, Sung-In Choi, Soon-Yong Park category:cs.CV published:2016-01-04 summary:Close-range Photogrammetry is widely used in many industries because of thecost effectiveness and efficiency of the technique. In this research, weintroduce an automated coded target detection method which can be used toenhance the efficiency of the Photogrammetry.
arxiv-14400-70 | On the Reducibility of Submodular Functions | http://arxiv.org/pdf/1601.00393v1.pdf | author:Jincheng Mei, Hao Zhang, Bao-Liang Lu category:cs.LG stat.ML published:2016-01-04 summary:The scalability of submodular optimization methods is critical for theirusability in practice. In this paper, we study the reducibility of submodularfunctions, a property that enables us to reduce the solution space ofsubmodular optimization problems without performance loss. We introduce theconcept of reducibility using marginal gains. Then we show that by addingperturbation, we can endow irreducible functions with reducibility, based onwhich we propose the perturbation-reduction optimization framework. Ourtheoretical analysis proves that given the perturbation scales, thereducibility gain could be computed, and the performance loss has additiveupper bounds. We further conduct empirical studies and the results demonstratethat our proposed framework significantly accelerates existing optimizationmethods for irreducible submodular functions with a cost of only smallperformance losses.
arxiv-14400-71 | Unsupervised Learning of Video Representations using LSTMs | http://arxiv.org/pdf/1502.04681v3.pdf | author:Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov category:cs.LG cs.CV cs.NE published:2015-02-16 summary:We use multilayer Long Short Term Memory (LSTM) networks to learnrepresentations of video sequences. Our model uses an encoder LSTM to map aninput sequence into a fixed length representation. This representation isdecoded using single or multiple decoder LSTMs to perform different tasks, suchas reconstructing the input sequence, or predicting the future sequence. Weexperiment with two kinds of input sequences - patches of image pixels andhigh-level representations ("percepts") of video frames extracted using apretrained convolutional net. We explore different design choices such aswhether the decoder LSTMs should condition on the generated output. We analyzethe outputs of the model qualitatively to see how well the model canextrapolate the learned video representation into the future and into the past.We try to visualize and interpret the learned features. We stress test themodel by running it on longer time scales and on out-of-domain data. We furtherevaluate the representations by finetuning them for a supervised learningproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We showthat the representations help improve classification accuracy, especially whenthere are only a few training examples. Even models pretrained on unrelateddatasets (300 hours of YouTube videos) can help action recognition performance.
arxiv-14400-72 | Training Bidirectional Helmholtz Machines | http://arxiv.org/pdf/1506.03877v4.pdf | author:Jorg Bornschein, Samira Shabanian, Asja Fischer, Yoshua Bengio category:cs.LG stat.ML published:2015-06-12 summary:Efficient unsupervised training and inference in deep generative modelsremains a challenging problem. One basic approach, called Helmholtz machine,involves training a top-down directed generative model together with abottom-up auxiliary model that is trained to help perform approximateinference. Recent results indicate that better results can be obtained withbetter approximate inference procedures. Instead of employing more powerfulprocedures, we here propose to regularize the generative model to stay close tothe class of distributions that can be efficiently inverted by the approximateinference model. We achieve this by interpreting both the top-down and thebottom-up directed models as approximate inference distributions and bydefining the model distribution to be the geometric mean of these two. Wepresent a lower-bound for the likelihood of this model and we show thatoptimizing this bound regularizes the model so that the Bhattacharyya distancebetween the bottom-up and top-down approximate distributions is minimized. Wedemonstrate that we can use this approach to fit generative models with manylayers of hidden binary stochastic variables to complex training distributionsand hat this method prefers significantly deeper architectures while itsupports orders of magnitude more efficient approximate inference than otherapproaches.
arxiv-14400-73 | Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in Wireless Sensor Networks | http://arxiv.org/pdf/1601.00350v1.pdf | author:Hadi Zayyani, Mehdi Korki, Farrokh Marvasti category:stat.ML cs.IT cs.LG math.IT published:2016-01-03 summary:This letter proposes a sparse diffusion steepest-descent algorithm for onebit compressed sensing in wireless sensor networks. The approach exploits thediffusion strategy from distributed learning in the one bit compressed sensingframework. To estimate a common sparse vector cooperatively from only the signof measurements, steepest-descent is used to minimize the suitable global andlocal convex cost functions. A diffusion strategy is suggested for distributivelearning of the sparse vector. Simulation results show the effectiveness of theproposed distributed algorithm compared to the state-of-the-art nondistributive algorithms in the one bit compressed sensing framework.
arxiv-14400-74 | Backward and Forward Language Modeling for Constrained Sentence Generation | http://arxiv.org/pdf/1512.06612v2.pdf | author:Lili Mou, Rui Yan, Ge Li, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE published:2015-12-21 summary:Recent language models, especially those based on recurrent neural networks(RNNs), make it possible to generate natural language from a learnedprobability. Language generation has wide applications including machinetranslation, summarization, question answering, conversation systems, etc.Existing methods typically learn a joint probability of words conditioned onadditional information, which is (either statically or dynamically) fed toRNN's hidden layer. In many applications, we are likely to impose hardconstraints on the generated texts, i.e., a particular word must appear in thesentence. Unfortunately, existing approaches could not solve this problem. Inthis paper, we propose a novel backward and forward language model. Provided aspecific word, we use RNNs to generate previous words and future words, eithersimultaneously or asynchronously, resulting in two model variants. In this way,the given word could appear at any position in the sentence. Experimentalresults show that the generated texts are comparable to sequential LMs inquality.
arxiv-14400-75 | Reducing the Training Time of Neural Networks by Partitioning | http://arxiv.org/pdf/1511.02954v2.pdf | author:Conrado S. Miranda, Fernando J. Von Zuben category:cs.NE cs.LG published:2015-11-10 summary:This paper presents a new method for pre-training neural networks that candecrease the total training time for a neural network while maintaining thefinal performance, which motivates its use on deep neural networks. Bypartitioning the training task in multiple training subtasks with sub-models,which can be performed independently and in parallel, it is shown that the sizeof the sub-models reduces almost quadratically with the number of subtaskscreated, quickly scaling down the sub-models used for the pre-training. Thesub-models are then merged to provide a pre-trained initial set of weights forthe original model. The proposed method is independent of the other aspects ofthe training, such as architecture of the neural network, training method, andobjective, making it compatible with a wide range of existing approaches. Thespeedup without loss of performance is validated experimentally on MNIST and onCIFAR10 data sets, also showing that even performing the subtasks sequentiallycan decrease the training time. Moreover, we show that larger models maypresent higher speedups and conjecture about the benefits of the method indistributed learning systems.
arxiv-14400-76 | Image Resolution Enhancement by Using Interpolation Followed by Iterative Back Projection | http://arxiv.org/pdf/1601.00260v1.pdf | author:Pejman Rasti, Hasan Demirel, Gholamreza Anbarjafari category:cs.CV published:2016-01-03 summary:In this paper, we propose a new super resolution technique based on theinterpolation followed by registering them using iterative back projection(IBP). Low resolution images are being interpolated and then the interpolatedimages are being registered in order to generate a sharper high resolutionimage. The proposed technique has been tested on Lena, Elaine, Pepper, andBaboon. The quantitative peak signal-to-noise ratio (PSNR) and structuralsimilarity index (SSIM) results as well as the visual results show thesuperiority of the proposed technique over the conventional and state-of-artimage super resolution techniques. For Lena's image, the PSNR is 6.52 dB higherthan the bicubic interpolation.
arxiv-14400-77 | Supervised Dimensionality Reduction via Distance Correlation Maximization | http://arxiv.org/pdf/1601.00236v1.pdf | author:Praneeth Vepakomma, Chetan Tonde, Ahmed Elgammal category:cs.LG stat.ML published:2016-01-03 summary:In our work, we propose a novel formulation for supervised dimensionalityreduction based on a nonlinear dependency criterion called Statistical DistanceCorrelation, Szekely et. al. (2007). We propose an objective which is free ofdistributional assumptions on regression variables and regression modelassumptions. Our proposed formulation is based on learning a low-dimensionalfeature representation $\mathbf{z}$, which maximizes the squared sum ofDistance Correlations between low dimensional features $\mathbf{z}$ andresponse $y$, and also between features $\mathbf{z}$ and covariates$\mathbf{x}$. We propose a novel algorithm to optimize our proposed objectiveusing the Generalized Minimization Maximizaiton method of \Parizi et. al.(2015). We show superior empirical results on multiple datasets proving theeffectiveness of our proposed approach over several relevant state-of-the-artsupervised dimensionality reduction methods.
arxiv-14400-78 | Supervised Texture Segmentation: A Comparative Study | http://arxiv.org/pdf/1601.00212v1.pdf | author:Omar S. Al-Kadi category:cs.CV published:2016-01-02 summary:This paper aims to compare between four different types of feature extractionapproaches in terms of texture segmentation. The feature extraction methodsthat were used for segmentation are Gabor filters (GF), Gaussian Markov randomfields (GMRF), run-length matrix (RLM) and co-occurrence matrix (GLCM). It wasshown that the GF performed best in terms of quality of segmentation while theGLCM localises the texture boundaries better as compared to the other methods.
arxiv-14400-79 | A fractal dimension based optimal wavelet packet analysis technique for classification of meningioma brain tumours | http://arxiv.org/pdf/1601.00211v1.pdf | author:Omar S. Al-Kadi category:cs.CV published:2016-01-02 summary:With the heterogeneous nature of tissue texture, using a single resolutionapproach for optimum classification might not suffice. In contrast, amultiresolution wavelet packet analysis can decompose the input signal into aset of frequency subbands giving the opportunity to characterise the texture atthe appropriate frequency channel. An adaptive best bases algorithm for optimalbases selection for meningioma histopathological images is proposed, viaapplying the fractal dimension (FD) as the bases selection criterion in atree-structured manner. Thereby, the most significant subband that betteridentifies texture discontinuities will only be chosen for furtherdecomposition, and its fractal signature would represent the extracted featurevector for classification. The best basis selection using the FD outperformedthe energy based selection approaches, achieving an overall classificationaccuracy of 91.25% as compared to 83.44% and 73.75% for the co-occurrencematrix and energy texture signatures; respectively.
arxiv-14400-80 | Susceptibility of texture measures to noise: an application to lung tumor CT images | http://arxiv.org/pdf/1601.00210v1.pdf | author:O. S. Al-Kadi, D. Watson category:cs.CV published:2016-01-02 summary:Five different texture methods are used to investigate their susceptibilityto subtle noise occurring in lung tumor Computed Tomography (CT) images causedby acquisition and reconstruction deficiencies. Noise of Gaussian and Rayleighdistributions with varying mean and variance was encountered in the analyzed CTimages. Fisher and Bhattacharyya distance measures were used to differentiatebetween an original extracted lung tumor region of interest (ROI) with afiltered and noisy reconstructed versions. Through examining the texturecharacteristics of the lung tumor areas by five different texture measures, itwas determined that the autocovariance measure was least affected and the graylevel co-occurrence matrix was the most affected by noise. Depending on theselected ROI size, it was concluded that the number of extracted features fromeach texture measure increases susceptibility to noise.
arxiv-14400-81 | A Unified Framework for Compositional Fitting of Active Appearance Models | http://arxiv.org/pdf/1601.00199v1.pdf | author:Joan Alabort-i-Medina, Stefanos Zafeiriou category:cs.CV published:2016-01-02 summary:Active Appearance Models (AAMs) are one of the most popular andwell-established techniques for modeling deformable objects in computer vision.In this paper, we study the problem of fitting AAMs using CompositionalGradient Descent (CGD) algorithms. We present a unified and complete view ofthese algorithms and classify them with respect to three main characteristics:i) cost function; ii) type of composition; and iii) optimization method.Furthermore, we extend the previous view by: a) proposing a novel Bayesian costfunction that can be interpreted as a general probabilistic formulation of thewell-known project-out loss; b) introducing two new types of composition,asymmetric and bidirectional, that combine the gradients of both image andappearance model to derive better conver- gent and more robust CGD algorithms;and c) providing new valuable insights into existent CGD algorithms byreinterpreting them as direct applications of the Schur complement and theWiberg method. Finally, in order to encourage open research and facilitatefuture comparisons with our work, we make the implementa- tion of thealgorithms studied in this paper publicly available as part of the MenpoProject.
arxiv-14400-82 | An Improved Intelligent Agent for Mining Real-Time Databases Using Modified Cortical Learning Algorithms | http://arxiv.org/pdf/1601.00191v1.pdf | author:N. E. Osegi category:cs.NE published:2016-01-02 summary:Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTMhave been developed by Numenta Incorporation from which variations andmodifications are currently being investigated upon. HTM offers better promisesas a future computational model of the neocortex the seat of intelligence inthe brain. Currently, intelligent agents are embedded in almost every modernday electronic system found in homes, offices and industries worldwide. In thispaper, we present a first step in realising useful HTM like applicationsspecifically for mining a synthetic and real time dataset based on a novelintelligent agent framework, and demonstrate how a modified version of thisvery important computational technique will lead to improved recognition.
arxiv-14400-83 | Adaptive Independent Sticky MCMC algorithms | http://arxiv.org/pdf/1308.3779v4.pdf | author:L. Martino, R. Casarin, F. Leisen, D. Luengo category:stat.CO stat.ML published:2013-08-17 summary:In this work, we introduce a novel class of adaptive Monte Carlo methods,called adaptive independent sticky MCMC algorithms, for efficient sampling froma generic target probability density function (pdf). The new class ofalgorithms employs adaptive non-parametric proposal densities which becomecloser and closer to the target as the number of iterations increases. Theproposal pdf is built using interpolation procedures based on a set of supportpoints which is constructed iteratively based on previously drawn samples. Thealgorithm's efficiency is ensured by a test that controls the evolution of theset of support points. This extra stage controls the computational cost and theconvergence of the proposal density to the target. Each part of the novelfamily of algorithms is discussed and several examples are provided. Althoughthe novel algorithms are presented for univariate target densities, we showthat they can be easily extended to the multivariate context within aGibbs-type sampler. The ergodicity is ensured and discussed. Exhaustivenumerical examples illustrate the efficiency of sticky schemes, both as astand-alone methods to sample from complicated one-dimensional pdfs and withinGibbs in order to draw from multi-dimensional target distributions.
arxiv-14400-84 | Joint Estimation of Precision Matrices in Heterogeneous Populations | http://arxiv.org/pdf/1601.00142v1.pdf | author:Takumi Saegusa, Ali Shojaie category:stat.ML published:2016-01-02 summary:We introduce a general framework for estimation of inverse covariance, orprecision, matrices from heterogeneous populations. The proposed framework usesa Laplacian shrinkage penalty to encourage similarity among estimates fromdisparate, but related, subpopulations, while allowing for differences amongmatrices. We propose an efficient alternating direction method of multipliers(ADMM) algorithm for parameter estimation, as well as its extension for fastercomputation in high dimensions by thresholding the empirical covariance matrixto identify the joint block diagonal structure in the estimated precisionmatrices. We establish both variable selection and norm consistency of theproposed estimator for distributions with exponential or polynomial tails.Further, to extend the applicability of the method to the settings with unknownpopulations structure, we propose a Laplacian penalty based on hierarchicalclustering, and discuss conditions under which this data-driven choice resultsin consistent estimation of precision matrices in heterogenous populations.Extensive numerical studies and applications to gene expression data fromsubtypes of cancer with distinct clinical outcomes indicate the potentialadvantages of the proposed method over existing approaches.
arxiv-14400-85 | Real-Time Anomalous Behavior Detection and Localization in Crowded Scenes | http://arxiv.org/pdf/1511.07425v2.pdf | author:Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hosseini category:cs.CV published:2015-11-21 summary:In this paper, we propose an accurate and real-time anomaly detection andlocalization in crowded scenes, and two descriptors for representing anomalousbehavior in video are proposed. We consider a video as being a set of cubicpatches. Based on the low likelihood of an anomaly occurrence, and theredundancy of structures in normal patches in videos, two (global and local)views are considered for modeling the video. Our algorithm has two components,for (1) representing the patches using local and global descriptors, and for(2) modeling the training patches using a new representation. We have twoGaussian models for all training patches respect to global and localdescriptors. The local and global features are based on structure similaritybetween adjacent patches and the features that are learned in an unsupervisedway. We propose a fusion strategy to combine the two descriptors as the outputof our system. Experimental results show that our algorithm performs like astate-of-the-art method on several standard datasets, but even is moretime-efficient.
arxiv-14400-86 | Discriminative Sparsity for Sonar ATR | http://arxiv.org/pdf/1601.00119v1.pdf | author:John McKay, Raghu Raj, Vishal Monga, Jason Isaacs category:cs.CV published:2016-01-01 summary:Advancements in Sonar image capture have enabled researchers to applysophisticated object identification algorithms in order to locate targets ofinterest in images such as mines. Despite progress in this field, modern sonarautomatic target recognition (ATR) approaches lack robustness to the amount ofnoise one would expect in real-world scenarios, the capability to handleblurring incurred from the physics of image capture, and the ability to excelwith relatively few training samples. We address these challenges by adaptingmodern sparsity-based techniques with dictionaries comprising of training fromeach class. We develop new discriminative (as opposed to generative) sparserepresentations which can help automatically classify targets in Sonar imaging.Using a simulated SAS data set from the Naval Surface Warfare Center (NSWC), weobtained compelling classification rates for multi-class problems even in caseswith considerable noise and sparsity in training samples.
arxiv-14400-87 | Demystifying Symmetric Smoothing Filters | http://arxiv.org/pdf/1601.00088v1.pdf | author:Stanley H. Chan, Todd Zickler, Yue M. Lu category:cs.CV published:2016-01-01 summary:Many patch-based image denoising algorithms can be formulated as applying asmoothing filter to the noisy image. Expressed as matrices, the smoothingfilters must be row normalized so that each row sums to unity. Surprisingly, ifwe apply a column normalization before the row normalization, the performanceof the smoothing filter can often be significantly improved. Prior works showedthat such performance gain is related to the Sinkhorn-Knopp balancingalgorithm, an iterative procedure that symmetrizes a row-stochastic matrix to adoubly-stochastic matrix. However, a complete understanding of the performancegain phenomenon is still lacking. In this paper, we study the performance gain phenomenon from a statisticallearning perspective. We show that Sinkhorn-Knopp is equivalent to anExpectation-Maximization (EM) algorithm of learning a Product of Gaussians(PoG) prior of the image patches. By establishing the correspondence betweenthe steps of Sinkhorn-Knopp and the EM algorithm, we provide a geometricalinterpretation of the symmetrization process. The new PoG model also allows usto develop a new denoising algorithm called Product of Gaussian Non-Local-Means(PoG-NLM). PoG-NLM is an extension of the Sinkhorn-Knopp and is ageneralization of the classical non-local means. Despite its simpleformulation, PoG-NLM outperforms many existing smoothing filters and has asimilar performance compared to BM3D.
arxiv-14400-88 | Computational Pathology: Challenges and Promises for Tissue Analysis | http://arxiv.org/pdf/1601.00027v1.pdf | author:Thomas J. Fuchs, Joachim M. Buhmann category:cs.CV cs.AI published:2015-12-31 summary:The histological assessment of human tissue has emerged as the key challengefor detection and treatment of cancer. A plethora of different data sourcesranging from tissue microarray data to gene expression, proteomics ormetabolomics data provide a detailed overview of the health status of apatient. Medical doctors need to assess these information sources and they relyon data driven automatic analysis tools. Methods for classification, groupingand segmentation of heterogeneous data sources as well as regression of noisydependencies and estimation of survival probabilities enter the processingworkflow of a pathology diagnosis system at various stages. This paper reportson state-of-the-art of the design and effectiveness of computational pathologyworkflows and it discusses future research directions in this emergent field ofmedical informatics and diagnostic machine learning.
arxiv-14400-89 | Write a Classifier: Predicting Visual Classifiers from Unstructured Text Descriptions | http://arxiv.org/pdf/1601.00025v1.pdf | author:Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh category:cs.CV cs.CL cs.LG published:2015-12-31 summary:People typically learn through exposure to visual concepts associated withlinguistic descriptions. For instance, teaching visual object categories tochildren is often accompanied by descriptions in text or speech. In a machinelearning context, these observations motivates us to ask whether this learningprocess could be computationally modeled to learn visual classifiers. Morespecifically, the main question of this work is how to utilize purely textualdescription of visual classes with no training images, to learn explicit visualclassifiers for them. We propose and investigate two baseline formulations,based on regression and domain transfer that predict a linear classifier. Then,we propose a new constrained optimization formulation that combines aregression function and a knowledge transfer function with additionalconstraints to predict the linear classifier parameters for new classes. Wealso propose a generic kernelized models where a kernel classifier, in the formdefined by the representer theorem, is predicted. The kernelized models allowdefining any two RKHS kernel functions in the visual space and text space,respectively, and could be useful for other applications. We finally propose akernel function between unstructured text descriptions that builds ondistributional semantics, which shows an advantage in our setting and could beuseful for other applications. We applied all the studied models to predictvisual classifiers for two fine-grained categorization datasets, and theresults indicate successful predictions of our final model against severalbaselines that we designed.
arxiv-14400-90 | Selecting Near-Optimal Learners via Incremental Data Allocation | http://arxiv.org/pdf/1601.00024v1.pdf | author:Ashish Sabharwal, Horst Samulowitz, Gerald Tesauro category:cs.LG stat.ML published:2015-12-31 summary:We study a novel machine learning (ML) problem setting of sequentiallyallocating small subsets of training data amongst a large set of classifiers.The goal is to select a classifier that will give near-optimal accuracy whentrained on all data, while also minimizing the cost of misallocated samples.This is motivated by large modern datasets and ML toolkits with manycombinations of learning algorithms and hyper-parameters. Inspired by theprinciple of "optimism under uncertainty," we propose an innovative strategy,Data Allocation using Upper Bounds (DAUB), which robustly achieves theseobjectives across a variety of real-world datasets. We further develop substantial theoretical support for DAUB in an idealizedsetting where the expected accuracy of a classifier trained on $n$ samples canbe known exactly. Under these conditions we establish a rigorous sub-linearbound on the regret of the approach (in terms of misallocated data), as well asa rigorous bound on suboptimality of the selected classifier. Our accuracyestimates using real-world datasets only entail mild violations of thetheoretical scenario, suggesting that the practical behavior of DAUB is likelyto approach the idealized behavior.
arxiv-14400-91 | Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server | http://arxiv.org/pdf/1512.09327v1.pdf | author:Yee Whye Teh, Leonard Hasenclever, Thibaut Lienart, Sebastian Vollmer, Stefan Webb, Balaji Lakshminarayanan, Charles Blundell category:cs.LG stat.ML published:2015-12-31 summary:This paper makes two contributions to Bayesian machine learning algorithms.Firstly, we propose stochastic natural gradient expectation propagation (SNEP),a novel alternative to expectation propagation (EP), a popular variationalinference algorithm. SNEP is a black box variational algorithm, in that it doesnot require any simplifying assumptions on the distribution of interest, beyondthe existence of some Monte Carlo sampler for estimating the moments of the EPtilted distributions. Further, as opposed to EP which has no guarantee ofconvergence, SNEP can be shown to be convergent, even when using Monte Carlomoment estimates. Secondly, we propose a novel architecture for distributedBayesian learning which we call the posterior server. The posterior serverallows scalable and robust Bayesian learning in cases where a dataset is storedin a distributed manner across a cluster, with each compute node containing adisjoint subset of data. An independent Markov chain Monte Carlo (MCMC) sampleris run on each compute node, with direct access only to the local data subset,but which targets an approximation to the global posterior distribution givenall data across the whole cluster. This is achieved by using a distributedasynchronous implementation of SNEP to pass messages across the cluster. Wedemonstrate SNEP and the posterior server on distributed Bayesian learning oflogistic regression and neural networks.
arxiv-14400-92 | A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data | http://arxiv.org/pdf/1409.3970v3.pdf | author:Yin Zheng, Yu-Jin Zhang, Hugo Larochelle category:cs.CV cs.IR cs.LG cs.NE published:2014-09-13 summary:Topic modeling based on latent Dirichlet allocation (LDA) has been aframework of choice to deal with multimodal data, such as in image annotationtasks. Another popular approach to model the multimodal data is through deepneural networks, such as the deep Boltzmann machine (DBM). Recently, a new typeof topic model called the Document Neural Autoregressive Distribution Estimator(DocNADE) was proposed and demonstrated state-of-the-art performance for textdocument modeling. In this work, we show how to successfully apply and extendthis model to multimodal data, such as simultaneous image classification andannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,that increases the discriminative power of the learned hidden topic featuresand show how to employ it to learn a joint representation from image visualwords, annotation words and class label information. We test our model on theLabelMe and UIUC-Sports data sets and show that it compares favorably to othertopic models. Second, we propose a deep extension of our model and provide anefficient way of training the deep model. Experimental results show that ourdeep model outperforms its shallow version and reaches state-of-the-artperformance on the Multimedia Information Retrieval (MIR) Flickr data set.
arxiv-14400-93 | Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models | http://arxiv.org/pdf/1501.03291v3.pdf | author:Michael U. Gutmann, Jukka Corander category:stat.ML stat.CO stat.ME published:2015-01-14 summary:Our paper deals with inferring simulator-based statistical models given someobserved data. A simulator-based model is a parametrized mechanism whichspecifies how data are generated. It is thus also referred to as generativemodel. We assume that only a finite number of parameters are of interest andallow the generative process to be very general; it may be a noisy nonlineardynamical system with an unrestricted number of hidden variables. This weakassumption is useful for devising realistic models but it renders statisticalinference very difficult. The main challenge is the intractability of thelikelihood function. Several likelihood-free inference methods have beenproposed which share the basic idea of identifying the parameters by findingvalues for which the discrepancy between simulated and observed data is small.A major obstacle to using these methods is their computational cost. The costis largely due to the need to repeatedly simulate data sets and the lack ofknowledge about how the parameters affect the discrepancy. We propose astrategy which combines probabilistic modeling of the discrepancy withoptimization to facilitate likelihood-free inference. The strategy isimplemented using Bayesian optimization and is shown to accelerate theinference through a reduction in the number of required simulations by severalorders of magnitude.
arxiv-14400-94 | Linear Convergence of Proximal Gradient Algorithm with Extrapolation for a Class of Nonconvex Nonsmooth Minimization Problems | http://arxiv.org/pdf/1512.09302v1.pdf | author:Bo Wen, Xiaojun Chen, Ting Kei Pong category:math.OC stat.ML published:2015-12-31 summary:In this paper, we study the proximal gradient algorithm with extrapolationfor minimizing the sum of a Lipschitz differentiable function and a properclosed convex function. Under the error bound condition used in [19] foranalyzing the convergence of the proximal gradient algorithm, we show thatthere exists a threshold such that if the extrapolation coefficients are chosenbelow this threshold, then the sequence generated converges $R$-linearly to astationary point of the problem. Moreover, the corresponding sequence ofobjective values is also $R$-linearly convergent. In addition, the thresholdreduces to $1$ for convex problems and, as a consequence, we obtain the$R$-linear convergence of the sequence generated by the FISTA with the fixedrestart. Finally, again for convex problems, we show that the successivechanges of the iterates vanish for many choices of sequences of extrapolationcoefficients that approach the threshold. In particular, this conclusion can beshown to hold for the sequence generated by the FISTA.
arxiv-14400-95 | Strategies and Principles of Distributed Machine Learning on Big Data | http://arxiv.org/pdf/1512.09295v1.pdf | author:Eric P. Xing, Qirong Ho, Pengtao Xie, Wei Dai category:stat.ML cs.DC cs.LG published:2015-12-31 summary:The rise of Big Data has led to new demands for Machine Learning (ML) systemsto learn complex models with millions to billions of parameters, that promiseadequate capacity to digest massive datasets and offer powerful predictiveanalytics thereupon. In order to run ML algorithms at such scales, on adistributed cluster with 10s to 1000s of machines, it is often the case thatsignificant engineering efforts are required --- and one might fairly ask ifsuch engineering truly falls within the domain of ML research or not. Takingthe view that Big ML systems can benefit greatly from ML-rooted statistical andalgorithmic insights --- and that ML researchers should therefore not shy awayfrom such systems design --- we discuss a series of principles and strategiesdistilled from our recent efforts on industrial-scale ML solutions. Theseprinciples and strategies span a continuum from application, to engineering,and to theoretical research and development of Big ML systems andarchitectures, with the goal of understanding how to make them efficient,generally-applicable, and supported with convergence and scaling guarantees.They concern four key questions which traditionally receive little attention inML research: How to distribute an ML program over a cluster? How to bridge MLcomputation with inter-machine communication? How to perform suchcommunication? What should be communicated between machines? By exposingunderlying statistical and algorithmic characteristics unique to ML programsbut not typically seen in traditional computer programs, and by dissectingsuccessful cases to reveal how we have harnessed these principles to design anddevelop both high-performance distributed ML software as well asgeneral-purpose ML frameworks, we present opportunities for ML researchers andpractitioners to further shape and grow the area that lies between ML andsystems.
arxiv-14400-96 | Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions | http://arxiv.org/pdf/1512.09272v1.pdf | author:B G Vijay Kumar, Gustavo Carneiro, Ian Reid category:cs.CV published:2015-12-31 summary:Recent innovations in training deep convolutional neural network (ConvNet)models have motivated the design of new methods to automatically learn localimage descriptors. The latest deep ConvNets proposed for this task consist of asiamese network that is trained by penalising misclassification of pairs oflocal image patches. Current results from machine learning show that replacingthis siamese by a triplet network can improve the classification accuracy inseveral problems, but this has yet to be demonstrated for local imagedescriptor learning. Moreover, current siamese and triplet networks have beentrained with stochastic gradient descent that computes the gradient fromindividual pairs or triplets of local image patches, which can make them proneto overfitting. In this paper, we first propose the use of triplet networks forthe problem of local image descriptor learning. Furthermore, we also proposethe use of a global loss that minimises the overall classification error in thetraining set, which can improve the generalisation capability of the model.Using the UBC benchmark dataset for comparing local image descriptors, we showthat the triplet network produces a more accurate embedding than the siamesenetwork in terms of the UBC dataset errors. Moreover, we also demonstrate thata combination of the triplet and global losses produces the best embedding inthe field, using this triplet network. Finally, we also show that the use ofthe central-surround siamese network trained with the global loss produces thebest result of the field on the UBC dataset.
arxiv-14400-97 | Solving the G-problems in less than 500 iterations: Improved efficient constrained optimization by surrogate modeling and adaptive parameter control | http://arxiv.org/pdf/1512.09251v1.pdf | author:Samineh Bagheri, Wolfgang Konen, Michael Emmerich, Thomas Bäck category:math.OC cs.NE stat.ML published:2015-12-31 summary:Constrained optimization of high-dimensional numerical problems plays animportant role in many scientific and industrial applications. Functionevaluations in many industrial applications are severely limited and noanalytical information about objective function and constraint functions isavailable. For such expensive black-box optimization tasks, the constraintoptimization algorithm COBRA was proposed, making use of RBF surrogate modelingfor both the objective and the constraint functions. COBRA has shown remarkablesuccess in solving reliably complex benchmark problems in less than 500function evaluations. Unfortunately, COBRA requires careful adjustment ofparameters in order to do so. In this work we present a new self-adjusting algorithm SACOBRA, which isbased on COBRA and capable to achieve high-quality results with very fewfunction evaluations and no parameter tuning. It is shown with the help ofperformance profiles on a set of benchmark problems (G-problems, MOPTA08) thatSACOBRA consistently outperforms any COBRA algorithm with fixed parametersetting. We analyze the importance of the several new elements in SACOBRA andfind that each element of SACOBRA plays a role to boost up the overalloptimization performance. We discuss the reasons behind and get in this way abetter understanding of high-quality RBF surrogate modeling.
arxiv-14400-98 | Denoising and Completion of 3D Data via Multidimensional Dictionary Learning | http://arxiv.org/pdf/1512.09227v1.pdf | author:Zemin Zhang, Shuchin Aeron category:cs.LG cs.CV cs.DS published:2015-12-31 summary:In this paper a new dictionary learning algorithm for multidimensional datais proposed. Unlike most conventional dictionary learning methods which arederived for dealing with vectors or matrices, our algorithm, named KTSVD,learns a multidimensional dictionary directly via a novel algebraic approachfor tensor factorization as proposed in [3, 12, 13]. Using this approach onecan define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-Ddata to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, basedon the idea of sparse coding (using group-sparsity over multidimensionalcoefficient vectors), alternates between estimating a compact representationand dictionary learning. We analyze our KTSVD algorithm and demonstrate itsresult on video completion and multispectral image denoising.
arxiv-14400-99 | Recovery of Sparse Signals Using Multiple Orthogonal Least Squares | http://arxiv.org/pdf/1410.2505v2.pdf | author:Jian Wang, Ping Li category:stat.ME cs.IT cs.LG math.IT published:2014-10-09 summary:We study the problem of recovering sparse signals from compressed linearmeasurements. This problem, often referred to as sparse recovery or sparsereconstruction, has generated a great deal of interest in recent years. Torecover the sparse signals, we propose a new method called multiple orthogonalleast squares (MOLS), which extends the well-known orthogonal least squares(OLS) algorithm by allowing multiple $L$ indices to be chosen per iteration.Owing to inclusion of multiple support indices in each selection, the MOLSalgorithm converges in much fewer iterations and improves the computationalefficiency over the conventional OLS algorithm. Theoretical analysis shows thatMOLS ($L > 1$) performs exact recovery of all $K$-sparse signals within $K$iterations if the measurement matrix satisfies the restricted isometry property(RIP) with isometry constant $\delta_{LK} < \frac{\sqrt{L}}{\sqrt{K} + 2\sqrt{L}}.$ The recovery performance of MOLS in the noisy scenario is alsostudied. It is shown that stable recovery of sparse signals can be achievedwith the MOLS algorithm when the signal-to-noise ratio (SNR) scales linearlywith the sparsity level of input signals.
arxiv-14400-100 | Nonparametric mixture of Gaussian graphical models | http://arxiv.org/pdf/1512.09206v1.pdf | author:Kevin Lee, Lingzhou Xue category:stat.ME stat.ML published:2015-12-31 summary:Graphical model has been widely used to investigate the complex dependencestructure of high-dimensional data, and it is common to assume that observeddata follow a homogeneous graphical model. However, observations usually comefrom different resources and have heterogeneous hidden commonality inreal-world applications. Thus, it is of great importance to estimateheterogeneous dependencies and discover subpopulation with certain commonalityacross the whole population. In this work, we introduce a novel regularizedestimation scheme for learning nonparametric mixture of Gaussian graphicalmodels, which extends the methodology and applicability of Gaussian graphicalmodels and mixture models. We propose a unified penalized likelihood approachto effectively estimate nonparametric functional parameters and heterogeneousgraphical parameters. We further design an efficient generalized effective EMalgorithm to address three significant challenges: high-dimensionality,non-convexity, and label switching. Theoretically, we study both thealgorithmic convergence of our proposed algorithm and the asymptotic propertiesof our proposed estimators. Numerically, we demonstrate the performance of ourmethod in simulation studies and a real application to estimate human brainfunctional connectivity from ADHD imaging data, where two heterogeneousconditional dependencies are explained through profiling demographic variablesand supported by existing scientific findings.
arxiv-14400-101 | Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies | http://arxiv.org/pdf/1512.09204v1.pdf | author:Weici Hu, Peter I. Frazier category:cs.LG cs.AI stat.ML published:2015-12-31 summary:We consider effort allocation in crowdsourcing, where we wish to assignlabeling tasks to imperfect homogeneous crowd workers to maximize overallaccuracy in a continuous-time Bayesian setting, subject to budget and timeconstraints. The Bayes-optimal policy for this problem is the solution to apartially observable Markov decision process, but the curse of dimensionalityrenders the computation infeasible. Based on the Lagrangian Relaxationtechnique in Adelman & Mersereau (2008), we provide a computationally tractableinstance-specific upper bound on the value of this Bayes-optimal policy, whichcan in turn be used to bound the optimality gap of any other sub-optimalpolicy. In an approach similar in spirit to the Whittle index for restlessmultiarmed bandits, we provide an index policy for effort allocation incrowdsourcing and demonstrate numerically that it outperforms other stateof-arts and performs close to optimal solution.
arxiv-14400-102 | Statistical Query Algorithms for Stochastic Convex Optimization | http://arxiv.org/pdf/1512.09170v1.pdf | author:Vitaly Feldman, Cristobal Guzman, Santosh Vempala category:cs.LG cs.DS published:2015-12-30 summary:Stochastic convex optimization, where the objective is the expectation of arandom convex function, is an important and widely used method with numerousapplications in machine learning, statistics, operations research and otherareas. We study the complexity of stochastic convex optimization given onlystatistical query (SQ) access to the objective function. We show thatwell-known and popular methods, including first-order iterative methods andpolynomial-time methods, can be implemented using only statistical queries. Formany cases of interest we derive nearly matching upper and lower bounds on theestimation (sample) complexity including linear optimization in the mostgeneral setting. We then present several consequences for machine learning,differential privacy and proving concrete lower bounds on the power of convexoptimization based methods. A new technical ingredient of our work is SQ algorithms for estimating themean vector of a distribution over vectors in $\mathbb{R}^d$ with optimalestimation complexity. This is a natural problem and we show that our solutionscan be used to get substantially improved SQ versions of Perceptron and otheronline algorithms for learning halfspaces.
arxiv-14400-103 | Low rank approximation and decomposition of large matrices using error correcting codes | http://arxiv.org/pdf/1512.09156v1.pdf | author:Shashanka Ubaru, Arya Mazumdar, Yousef Saad category:cs.IT cs.LG cs.NA math.IT published:2015-12-30 summary:Low rank approximation is an important tool used in many applications ofsignal processing and machine learning. Recently, randomized sketchingalgorithms were proposed to effectively construct low rank approximations andobtain approximate singular value decompositions of large matrices. Similarideas were used to solve least squares regression problems. In this paper, weshow how matrices from error correcting codes can be used to find such low rankapproximations and matrix decompositions, and extend the framework to linearleast squares regression problems. The benefits of using these code matricesare the following: (i) They are easy to generate and they reduce randomnesssignificantly. (ii) Code matrices with mild properties satisfy the subspaceembedding property, and have a better chance of preserving the geometry of anentire subspace of vectors. (iii) For parallel and distributed applications,code matrices have significant advantages over structured random matrices andGaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,which require sampling $O(k\log k)$ columns for a rank-$k$ approximation, thelog factor is not necessary for certain types of code matrices. That is,$(1+\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$approximation with $O(k/\epsilon)$ samples. (v) Fast multiplication is possiblewith structured code matrices, so fast approximations can be achieved forgeneral dense input matrices. (vi) For least squares regression problem$\min\Ax-b\_2$ where $A\in \mathbb{R}^{n\times d}$, the $(1+\epsilon)$relative error approximation can be achieved with $O(d/\epsilon)$ samples, withhigh probability, when certain code matrices are used.
arxiv-14400-104 | LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing | http://arxiv.org/pdf/1512.09049v1.pdf | author:Chenliang Xu, Jason J. Corso category:cs.CV published:2015-12-30 summary:Supervoxel segmentation has strong potential to be incorporated into earlyvideo analysis as superpixel segmentation has in image analysis. However, thereare many plausible supervoxel methods and little understanding as to when andwhere each is most appropriate. Indeed, we are not aware of a singlecomparative study on supervoxel segmentation. To that end, we study sevensupervoxel algorithms, including both off-line and streaming methods, in thecontext of what we consider to be a good supervoxel: namely, spatiotemporaluniformity, object/region boundary detection, region compression and parsimony.For the evaluation we propose a comprehensive suite of seven quality metrics tomeasure these desirable supervoxel characteristics. In addition, we evaluatethe methods in a supervoxel classification task as a proxy for subsequenthigh-level uses of the supervoxels in video analysis. We use six existingbenchmark video datasets with a variety of content-types and dense humanannotations. Our findings have led us to conclusive evidence that thehierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) andtemporal superpixels (TSP) methods are the top-performers among the sevenmethods. They all perform well in terms of segmentation accuracy, but vary inregard to the other desiderata: GBH captures object boundaries best; SWA hasthe best potential for region compression; and TSP achieves the bestundersegmentation error.
arxiv-14400-105 | Actor-Action Semantic Segmentation with Grouping Process Models | http://arxiv.org/pdf/1512.09041v1.pdf | author:Chenliang Xu, Jason J. Corso category:cs.CV published:2015-12-30 summary:Actor-action semantic segmentation made an important step toward advancedvideo understanding problems: what action is happening; who is performing theaction; and where is the action in space-time. Current models for this problemare local, based on layered CRFs, and are unable to capture long-ranginginteraction of video parts. We propose a new model that combines these locallabeling CRFs with a hierarchical supervoxel decomposition. The supervoxelsprovide cues for possible groupings of nodes, at various scales, in the CRFs toencourage adaptive, high-order groups for more effective labeling. Our model isdynamic and continuously exchanges information during inference: the local CRFsinfluence what supervoxels in the hierarchy are active, and these active nodesinfluence the connectivity in the CRF; we hence call it a grouping processmodel. The experimental results on a recent large-scale video dataset show alarge margin of 60% relative improvement over the state of the art, whichdemonstrates the effectiveness of the dynamic, bidirectional flow betweenlabeling and grouping.
arxiv-14400-106 | Recursive Partitioning for Heterogeneous Causal Effects | http://arxiv.org/pdf/1504.01132v3.pdf | author:Susan Athey, Guido Imbens category:stat.ML published:2015-04-05 summary:In this paper we study the problems of estimating heterogeneity in causaleffects in experimental or observational studies and conducting inference aboutthe magnitude of the differences in treatment effects across subsets of thepopulation. In applications, our method provides a data-driven approach todetermine which subpopulations have large or small treatment effects and totest hypotheses about the differences in these effects. For experiments, ourmethod allows researchers to identify heterogeneity in treatment effects thatwas not specified in a pre-analysis plan, without concern about invalidatinginference due to multiple testing. In most of the literature on supervisedmachine learning (e.g. regression trees, random forests, LASSO, etc.), the goalis to build a model of the relationship between a unit's attributes and anobserved outcome. A prominent role in these methods is played bycross-validation which compares predictions to actual outcomes in test samples,in order to select the level of complexity of the model that provides the bestpredictive power. Our method is closely related, but it differs in that it istailored for predicting causal effects of a treatment rather than a unit'soutcome. The challenge is that the "ground truth" for a causal effect is notobserved for any individual unit: we observe the unit with the treatment, orwithout the treatment, but not both at the same time. Thus, it is not obvioushow to use cross-validation to determine whether a causal effect has beenaccurately predicted. We propose several novel cross-validation criteria forthis problem and demonstrate through simulations the conditions under whichthey perform better than standard methods for the problem of causal effects. Wethen apply the method to a large-scale field experiment re-ranking results on asearch engine.
arxiv-14400-107 | Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices | http://arxiv.org/pdf/1512.08996v1.pdf | author:Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou category:stat.ML stat.AP stat.ME published:2015-12-30 summary:A gamma process dynamic Poisson factor analysis model is proposed tofactorize a dynamic count matrix, whose columns are sequentially observed countvectors. The model builds a novel Markov chain that sends the latent gammarandom variables at time $(t-1)$ as the shape parameters of those at time $t$,which are linked to observed or latent counts under the Poisson likelihood. Thesignificant challenge of inferring the gamma shape parameters is fullyaddressed, using unique data augmentation and marginalization techniques forthe negative binomial distribution. The same nonparametric Bayesian model alsoapplies to the factorization of a dynamic binary matrix, via aBernoulli-Poisson link that connects a binary observation to a latent count,with closed-form conditional posteriors for the latent counts and efficientcomputation for sparse observations. We apply the model to text and musicanalysis, with state-of-the-art results.
arxiv-14400-108 | Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction | http://arxiv.org/pdf/1501.06218v2.pdf | author:Mingyuan Zhou category:stat.ML cs.SI published:2015-01-25 summary:A hierarchical gamma process infinite edge partition model is proposed tofactorize the binary adjacency matrix of an unweighted undirected relationalnetwork under a Bernoulli-Poisson link. The model describes both homophily andstochastic equivalence, and is scalable to big sparse networks by focusing itscomputation on pairs of linked nodes. It can not only discover overlappingcommunities and inter-community interactions, but also predict missing edges. Asimplified version omitting inter-community interactions is also provided andwe reveal its interesting connections to existing models. The number ofcommunities is automatically inferred in a nonparametric Bayesian manner, andefficient inference via Gibbs sampling is derived using novel data augmentationtechniques. Experimental results on four real networks demonstrate the models'scalability and state-of-the-art performance.
arxiv-14400-109 | Technical Report: a tool for measuring Prosodic Accommodation | http://arxiv.org/pdf/1512.08982v1.pdf | author:Sucheta Ghosh category:cs.SD cs.CL published:2015-12-30 summary:Social interaction is a dynamic and joint activity where all participants areengaged and coordinate their behaviour in the co-construction of meaning. It isobserved that conversational partners adapt their pitch, intensity and timingbehaviour to their in- terlocutors. The majority of research has focused on itslinear manifestation over the course of an interaction. De Looze et alhypothesised that it evolves dynamically with functional social aspects. In thework of De Looze et al, they proposed through their praat based featureextraction and matlab based visualisation that one can visualise prosodicaccommodation at the positive correlation threshold values. and the capture ofits dynamic manifestation. Here we seek to build a complete system formeasuring prosodic accommodation with matlab. This work uses data collected ina pilot training scenario where senior pilot and co-pilot are engaged inconversation during the flight. Additionally, we use the data from a ship wrecktask by two pilots. We also attempt to evaluate this measures of accommodationwith ground truth labels given by a trainer of Crew (or sometimes Crisis)Resource Management (CRM).
arxiv-14400-110 | The Poisson Gamma Belief Network | http://arxiv.org/pdf/1511.02199v2.pdf | author:Mingyuan Zhou, Yulai Cong, Bo Chen category:stat.ML stat.ME published:2015-11-06 summary:To infer a multilayer representation of high-dimensional count vectors, wepropose the Poisson gamma belief network (PGBN) that factorizes each of itslayers into the product of a connection weight matrix and the nonnegative realhidden units of the next layer. The PGBN's hidden layers are jointly trainedwith an upward-downward Gibbs sampler, each iteration of which upward samplesDirichlet distributed connection weight vectors starting from the first layer(bottom data layer), and then downward samples gamma distributed hidden unitsstarting from the top hidden layer. The gamma-negative binomial processcombined with a layer-wise training strategy allows the PGBN to infer the widthof each layer given a fixed budget on the width of the first layer. The PGBNwith a single hidden layer reduces to Poisson factor analysis. Example resultson text analysis illustrate interesting relationships between the width of thefirst layer and the inferred network structure, and demonstrate that the PGBN,whose hidden units are imposed with correlated gamma priors, can add morelayers to increase its performance gains over Poisson factor analysis, giventhe same limit on the width of the first layer.
arxiv-14400-111 | Assessing binary classifiers using only positive and unlabeled data | http://arxiv.org/pdf/1504.06837v2.pdf | author:Marc Claesen, Jesse Davis, Frank De Smet, Bart De Moor category:stat.ML cs.IR cs.LG I.5.2 published:2015-04-26 summary:Assessing the performance of a learned model is a crucial part of machinelearning. However, in some domains only positive and unlabeled examples areavailable, which prohibits the use of most standard evaluation metrics. Wepropose an approach to estimate any metric based on contingency tables,including ROC and PR curves, using only positive and unlabeled data. Estimatingthese performance metrics is essentially reduced to estimating the fraction of(latent) positives in the unlabeled set, assuming known positives are a randomsample of all positives. We provide theoretical bounds on the quality of ourestimates, illustrate the importance of estimating the fraction of positives inthe unlabeled set and demonstrate empirically that we are able to reliablyestimate ROC and PR curves on real data.
arxiv-14400-112 | Online Keyword Spotting with a Character-Level Recurrent Neural Network | http://arxiv.org/pdf/1512.08903v1.pdf | author:Kyuyeon Hwang, Minjae Lee, Wonyong Sung category:cs.CL cs.LG cs.NE published:2015-12-30 summary:In this paper, we propose a context-aware keyword spotting model employing acharacter-level recurrent neural network (RNN) for spoken term detection incontinuous speech. The RNN is end-to-end trained with connectionist temporalclassification (CTC) to generate the probabilities of character andword-boundary labels. There is no need for the phonetic transcription, senonemodeling, or system dictionary in training and testing. Also, keywords caneasily be added and modified by editing the text based keyword list withoutretraining the RNN. Moreover, the unidirectional RNN processes an infinitelylong input audio streams without pre-segmentation and keywords are detectedwith low-latency before the utterance is finished. Experimental results showthat the proposed keyword spotter significantly outperforms the deep neuralnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model evenwith less computations.
arxiv-14400-113 | Estimation of the sample covariance matrix from compressive measurements | http://arxiv.org/pdf/1512.08887v1.pdf | author:Farhad Pourkamali-Anaraki category:stat.ML cs.LG published:2015-12-30 summary:This paper focuses on the estimation of the sample covariance matrix fromlow-dimensional random projections of data known as compressive measurements.In particular, we present an unbiased estimator to extract the covariancestructure from compressive measurements obtained by a general class of randomprojection matrices consisting of i.i.d. zero-mean entries and finite firstfour moments. In contrast to previous works, we make no structural assumptionsabout the underlying covariance matrix such as being low-rank. In fact, ouranalysis is based on a non-Bayesian data setting which requires nodistributional assumptions on the set of data samples. Furthermore, inspired bythe generality of the projection matrices, we propose an approach to covarianceestimation that utilizes very sparse random projections with Bernoulli entries.Therefore, our algorithm can be used to estimate the covariance matrix inapplications with limited memory and computation power at the acquisitiondevices. Experimental results demonstrate that our approach allows for accurateestimation of the sample covariance matrix on several real-world data sets,including video data.
arxiv-14400-114 | Learning Meta-Embeddings by Using Ensembles of Embedding Sets | http://arxiv.org/pdf/1508.04257v2.pdf | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2015-08-18 summary:Word embeddings -- distributed representations of words -- in deep learningare beneficial for many tasks in natural language processing (NLP). However,different embedding sets vary greatly in quality and characteristics of thecaptured semantics. Instead of relying on a more advanced algorithm forembedding learning, this paper proposes an ensemble approach of combiningdifferent public embedding sets with the aim of learning meta-embeddings.Experiments on word similarity and analogy tasks and on part-of-speech taggingshow better performance of meta-embeddings compared to individual embeddingsets. One advantage of meta-embeddings is the increased vocabulary coverage. Wewill release our meta-embeddings publicly.
arxiv-14400-115 | Post-Regularization Inference for Dynamic Nonparanormal Graphical Models | http://arxiv.org/pdf/1512.08298v2.pdf | author:Junwei Lu, Mladen Kolar, Han Liu category:stat.ML published:2015-12-28 summary:We propose a novel class of dynamic nonparanormal graphical models, whichallows us to model high dimensional heavy-tailed systems and the evolution oftheir latent network structures. Under this model we develop statistical testsfor presence of edges both locally at a fixed index value and globally over arange of values. The tests are developed for a high-dimensional regime, arerobust to model selection mistakes and do not require commonly assumed minimumsignal strength. The testing procedures are based on a high dimensional,debiasing-free moment estimator, which uses a novel kernel smoothed Kendall'stau correlation matrix as an input statistic. The estimator consistentlyestimates the latent inverse Pearson correlation matrix uniformly in both indexvariable and kernel bandwidth. Its rate of convergence is shown to be minimaxoptimal. Thorough numerical simulations and an application to a neural imagingdataset support the usefulness of our method.
arxiv-14400-116 | Sharp Computational-Statistical Phase Transitions via Oracle Computational Model | http://arxiv.org/pdf/1512.08861v1.pdf | author:Zhaoran Wang, Quanquan Gu, Han Liu category:stat.ML published:2015-12-30 summary:We study the fundamental tradeoffs between computational tractability andstatistical accuracy for a general family of hypothesis testing problems withcombinatorial structures. Based upon an oracle model of computation, whichcaptures the interactions between algorithms and data, we establish a generallower bound that explicitly connects the minimum testing risk undercomputational budget constraints with the intrinsic probabilistic andcombinatorial structures of statistical problems. This lower bound mirrors theclassical statistical lower bound by Le Cam (1986) and allows us to quantifythe optimal statistical performance achievable given limited computationalbudgets in a systematic fashion. Under this unified framework, we sharplycharacterize the statistical-computational phase transition for two testingproblems, namely, normal mean detection and sparse principal componentdetection. For normal mean detection, we consider two combinatorial structures,namely, sparse set and perfect matching. For these problems we identifysignificant gaps between the optimal statistical accuracy that is achievableunder computational tractability constraints and the classical statisticallower bounds. Compared with existing works on computational lower bounds forstatistical problems, which consider general polynomial-time algorithms onTuring machines, and rely on computational hardness hypotheses on problems likeplanted clique detection, we focus on the oracle computational model, whichcovers a broad range of popular algorithms, and do not rely on unprovenhypotheses. Moreover, our result provides an intuitive and concreteinterpretation for the intrinsic computational intractability ofhigh-dimensional statistical problems. One byproduct of our result is a lowerbound for a strict generalization of the matrix permanent problem, which is ofindependent interest.
arxiv-14400-117 | Learning Natural Language Inference with LSTM | http://arxiv.org/pdf/1512.08849v1.pdf | author:Shuohang Wang, Jing Jiang category:cs.CL cs.AI cs.NE published:2015-12-30 summary:Natural language inference (NLI) is a fundamentally important task in naturallanguage processing that has many applications. The recently released StanfordNatural Language Inference (SNLI) corpus has made it possible to develop andevaluate learning-centered methods such as deep neural networks for the NLItask. In this paper, we propose a special long short-term memory (LSTM)architecture for NLI. Our model builds on top of a recently proposed neutralattention model for NLI but is based on a significantly different idea. Insteadof deriving sentence embeddings for the premise and the hypothesis to be usedfor classification, our solution uses a matching-LSTM that performsword-by-word matching of the hypothesis with the premise. This LSTM is able toplace more emphasis on important word-level matching results. In particular, weobserve that this LSTM remembers important mismatches that are critical forpredicting the contradiction or the neutral relationship label. Our experimentson the SNLI corpus show that our model outperforms the state of the art,achieving an accuracy of 86.1% on the test data.
arxiv-14400-118 | Learning to Filter with Predictive State Inference Machines | http://arxiv.org/pdf/1512.08836v1.pdf | author:Wen Sun, Arun Venkatraman, Byron Boots, J. Andrew Bagnell category:cs.LG published:2015-12-30 summary:Latent state space models are one of the most fundamental and widely usedtools for modeling dynamical systems. Traditional Maximum Likelihood Estimation(MLE) based approaches aim to maximize the likelihood objective, which isnon-convex due to latent states. While non-convex optimization methods like EMcan learn models that locally optimize the likelihood objective, using thelocally optimal model for an inference task such as Bayesian filtering usuallydoes not have performance guarantees. In this work, we propose a method thatconsiders the inference procedure on the dynamical system as a composition ofpredictors. Instead of optimizing a given parametrization of latent states, welearn predictors for inference in predictive belief space, where we can usesufficient features of observations for supervision of our learning algorithm.We further show that our algorithm, the Predictive State Inference Machine, hastheoretical performance guarantees on the inference task. Empiricalverification across several of dynamical system benchmarks ranging from asimulated helicopter to recorded telemetry traces from a robot showcase theabilities of training Inference Machines.
arxiv-14400-119 | Learning with a Wasserstein Loss | http://arxiv.org/pdf/1506.05439v3.pdf | author:Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, Tomaso Poggio category:cs.LG cs.CV stat.ML published:2015-06-17 summary:Learning to predict multi-label outputs is challenging, but in many problemsthere is a natural metric on the outputs that can be used to improvepredictions. In this paper we develop a loss function for multi-label learning,based on the Wasserstein distance. The Wasserstein distance provides a naturalnotion of dissimilarity for probability measures. Although optimizing withrespect to the exact Wasserstein distance is costly, recent work has describeda regularized approximation that is efficiently computed. We describe anefficient learning algorithm based on this regularization, as well as a novelextension of the Wasserstein distance from probability measures to unnormalizedmeasures. We also describe a statistical learning bound for the loss. TheWasserstein loss can encourage smoothness of the predictions with respect to achosen metric on the output space. We demonstrate this property on a real-datatag prediction problem, using the Yahoo Flickr Creative Commons dataset,outperforming a baseline that doesn't use the metric.
arxiv-14400-120 | Joint limiting laws for high-dimensional independence tests | http://arxiv.org/pdf/1512.08819v1.pdf | author:Danning Li, Lingzhou Xue category:math.ST stat.ME stat.ML stat.OT stat.TH 62H12, 60F05 published:2015-12-30 summary:Testing independence is of significant interest in many important areas oflarge-scale inference. Using extreme-value form statistics to test againstsparse alternatives and using quadratic form statistics to test against densealternatives are two important testing procedures for high-dimensionalindependence. However, quadratic form statistics suffer from low power againstsparse alternatives, and extreme-value form statistics suffer from low poweragainst dense alternatives with small disturbances and may have sizedistortions due to its slow convergence. For real-world applications, it isimportant to derive powerful testing procedures against more generalalternatives. Based on intermediate limiting distributions, we derive(model-free) joint limiting laws of extreme-value form and quadratic formstatistics, and surprisingly, we prove that they are asymptoticallyindependent. Given such asymptotic independencies, we propose (model-free)testing procedures to boost the power against general alternatives and alsoretain the correct asymptotic size. Under the high-dimensional setting, wederive the closed-form limiting null distributions, and obtain their explicitrates of uniform convergence. We prove their consistent statistical powersagainst general alternatives. We demonstrate the performance of our proposedtest statistics in simulation studies. Our work provides very helpful insightsto high-dimensional independence tests, and fills an important gap.
arxiv-14400-121 | Combined statistical and model based texture features for improved image classification | http://arxiv.org/pdf/1512.08814v1.pdf | author:Omar Al-Kadi category:cs.CV published:2015-12-29 summary:This paper aims to improve the accuracy of texture classification based onextracting texture features using five different texture methods andclassifying the patterns using a naive Bayesian classifier. Threestatistical-based and two model-based methods are used to extract texturefeatures from eight different texture images, then their accuracy is rankedafter using each method individually and in pairs. The accuracy improved up to97.01% when model based -Gaussian Markov random field (GMRF) and fractionalBrownian motion (fBm) - were used together for classification as compared tothe highest achieved using each of the five different methods alone; and provedto be better in classifying as compared to statistical methods. Also, usingGMRF with statistical based methods, such as Gray level co-occurrence (GLCM)and run-length (RLM) matrices, improved the overall accuracy to 96.94% and96.55%; respectively.
arxiv-14400-122 | Matrix Completion Under Monotonic Single Index Models | http://arxiv.org/pdf/1512.08787v1.pdf | author:Ravi Ganti, Laura Balzano, Rebecca Willett category:stat.ML cs.LG published:2015-12-29 summary:Most recent results in matrix completion assume that the matrix underconsideration is low-rank or that the columns are in a union of low-ranksubspaces. In real-world settings, however, the linear structure underlyingthese models is distorted by a (typically unknown) nonlinear transformation.This paper addresses the challenge of matrix completion in the face of suchnonlinearities. Given a few observations of a matrix that are obtained byapplying a Lipschitz, monotonic function to a low rank matrix, our task is toestimate the remaining unobserved entries. We propose a novel matrix completionmethod that alternates between low-rank matrix estimation and monotonicfunction estimation to estimate the missing matrix elements. Mean squared errorbounds provide insight into how well the matrix can be estimated based on thesize, rank of the matrix and properties of the nonlinear transformation.Empirical results on synthetic and real-world datasets demonstrate thecompetitiveness of the proposed approach.
arxiv-14400-123 | Memory vectors for similarity search in high-dimensional spaces | http://arxiv.org/pdf/1412.3328v5.pdf | author:Ahmet Iscen, Teddy Furon, Vincent Gripon, Michael Rabbat, Hervé Jégou category:cs.CV cs.DB published:2014-12-10 summary:We study an indexing architecture to store and search in a database ofhigh-dimensional vectors. This architecture is composed of several memoryunits, each of which summarizes a fraction of the database by a singlerepresentative vector.The potential similarity of the query to one of thevectors stored in the memory unit is gauged by a simple correlation with thememory unit's representative vector. This representative optimizes the test ofthe following hypothesis: the query is independent from any vector in thememory unit vs. the query is a simple perturbation of one of the storedvectors. Compared to exhaustive search, our approach finds the most similardatabase vectors significantly faster without a noticeable reduction in searchquality. Interestingly, the reduction of complexity is provably better inhigh-dimensional spaces. We empirically demonstrate its practical interest in alarge-scale image search scenario with off-the-shelf state-of-the-artdescriptors.
arxiv-14400-124 | Error Bounds for Compressed Sensing Algorithms With Group Sparsity: A Unified Approach | http://arxiv.org/pdf/1512.08673v1.pdf | author:M. Eren Ahsen, M. Vidyasagar category:stat.ML 62J99 published:2015-12-29 summary:In compressed sensing, in order to recover a sparse or nearly sparse vectorfrom possibly noisy measurements, the most popular approach is $\ell_1$-normminimization. Upper bounds for the $\ell_2$- norm of the error between the trueand estimated vectors are given in [1] and reviewed in [2], while bounds forthe $\ell_1$-norm are given in [3]. When the unknown vector is notconventionally sparse but is "group sparse" instead, a variety of alternativesto the $\ell_1$-norm have been proposed in the literature, including the groupLASSO, sparse group LASSO, and group LASSO with tree structured overlappinggroups. However, no error bounds are available for any of these modifiedobjective functions. In the present paper, a unified approach is presented forderiving upper bounds on the error between the true vector and itsapproximation, based on the notion of decomposable and $\gamma$-decomposablenorms. The bounds presented cover all of the norms mentioned above, and alsoprovide a guideline for choosing norms in future to accommodate alternate formsof sparsity.
arxiv-14400-125 | Robust Scene Text Recognition Using Sparse Coding based Features | http://arxiv.org/pdf/1512.08669v1.pdf | author:Da-Han Wang, Hanzi Wang, Dong Zhang, Jonathan Li, David Zhang category:cs.CV published:2015-12-29 summary:In this paper, we propose an effective scene text recognition method usingsparse coding based features, called Histograms of Sparse Codes (HSC) features.For character detection, we use the HSC features instead of using theHistograms of Oriented Gradients (HOG) features. The HSC features are extractedby computing sparse codes with dictionaries that are learned from data usingK-SVD, and aggregating per-pixel sparse codes to form local histograms. Forword recognition, we integrate multiple cues including character detectionscores and geometric contexts in an objective function. The final recognitionresults are obtained by searching for the words which correspond to the maximumvalue of the objective function. The parameters in the objective function arelearned using the Minimum Classification Error (MCE) training method.Experiments on several challenging datasets demonstrate that the proposedHSC-based scene text recognition method outperforms HOG-based methodssignificantly and outperforms most state-of-the-art methods.
arxiv-14400-126 | A framework for robust object multi-detection with a vote aggregation and a cascade filtering | http://arxiv.org/pdf/1512.08648v1.pdf | author:Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas category:cs.CV published:2015-12-29 summary:This paper presents a framework designed for the multi-object detectionpurposes and adjusted for the application of product search on the marketshelves. The framework uses a single feedback loop and a pattern resizingmechanism to demonstrate the top effectiveness of the state-of-the-art localfeatures. A high detection rate with a low false detection chance can beachieved with use of only one pattern per object and no manual parametersadjustments. The method incorporates well known local features and a basicmatching process to create a reliable voting space. Further steps comprise ofmetric transformations, graphical vote space representation, two-phase voteaggregation process and a cascade of verifying filters.
arxiv-14400-127 | Hypothesis Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity | http://arxiv.org/pdf/1512.08643v1.pdf | author:Eugene Belilovsky, Gaël Varoquaux, Matthew B. Blaschko category:stat.ML published:2015-12-29 summary:Functional brain networks are well described and estimated from data withGaussian Graphical Models (GGMs), e.g. using sparse inverse covarianceestimators. Comparing functional connectivity of subjects in two populationcalls for comparing these estimated GGMs. We study the problem of identifyingdifferences in Gaussian Graphical Models (GGMs) known to have similarstructure. We aim to characterize the uncertainty of differences withconfidence intervals obtained using a para-metric distribution on parameters ofa sparse estimator. Sparse penalties enable statistical guarantees andinterpretable models even in high-dimensional and low-number-of-samplessettings. Quantifying the uncertainty of the parameters selected by the sparsepenalty is an important question in applications such as neuroimaging orbioinformatics. Indeed, selected variables can be interpreted to buildtheoretical understanding or to make therapeutic decisions. Characterizing thedistributions of sparse regression models is inherently challenging since thepenalties produce a biased estimator. Recent work has shown how one can invokethe sparsity assumptions to effectively remove the bias from a sparse estimatorsuch as the lasso. These distributions can be used to give us confidenceintervals on edges in GGMs, and by extension their differences. However, in thecase of comparing GGMs, these estimators do not make use of any assumed jointstructure among the GGMs. Inspired by priors from brain functional connectivitywe focus on deriving the distribution of parameter differences under a jointpenalty when parameters are known to be sparse in the difference. This leads usto introduce the debiased multi-task fused lasso. We show that we can debiasand characterize the distribution in an efficient manner. We then go on to showhow the debiased lasso and multi-task fused lasso can be used to obtainconfidence intervals on edge differences in Gaussian graphical models. Wevalidate the techniques proposed on a set of synthetic examples as well asneuro-imaging dataset created for the study of autism.
arxiv-14400-128 | Triplet Spike Time Dependent Plasticity: A floating-gate Implementation | http://arxiv.org/pdf/1512.00961v3.pdf | author:Roshan Gopalakrishnan, Arindam Basu category:cs.NE published:2015-12-03 summary:Synapse plays an important role of learning in a neural network; the learningrules which modify the synaptic strength based on the timing difference betweenthe pre- and post-synaptic spike occurrence is termed as Spike Time DependentPlasticity (STDP). The most commonly used rule posits weight change based ontime difference between one pre- and one post spike and is hence termed doubletSTDP (DSTDP). However, D-STDP could not reproduce results of many biologicalexperiments; a triplet STDP (T-STDP) that considers triplets of spikes as thefundamental unit has been proposed recently to explain these observations. Thispaper describes the compact implementation of a synapse using singlefloating-gate (FG) transistor that can store a weight in a nonvolatile mannerand demonstrate the triplet STDP (T-STDP) learning rule by modifying drainvoltages according to triplets of spikes. We describe a mathematical procedureto obtain control voltages for the FG device for T-STDP and also showmeasurement results from a FG synapse fabricated in TSMC 0.35um CMOS process tosupport the theory. Possible VLSI implementation of drain voltage waveformgenerator circuits are also presented with simulation results.
arxiv-14400-129 | Tight Bounds for Approximate Carathéodory and Beyond | http://arxiv.org/pdf/1512.08602v1.pdf | author:Vahab Mirrokni, Renato Paes Leme, Adrian Vladu, Sam Chiu-wai Wong category:cs.DS cs.LG math.OC published:2015-12-29 summary:We give a deterministic nearly-linear time algorithm for approximating anypoint inside a convex polytope with a sparse convex combination of thepolytope's vertices. Our result provides a constructive proof for theApproximate Carath\'{e}odory Problem, which states that any point inside apolytope contained in the $\ell_p$ ball of radius $D$ can be approximated towithin $\epsilon$ in $\ell_p$ norm by a convex combination of only $O\left(D^2p/\epsilon^2\right)$ vertices of the polytope for $p \geq 2$. We also show thatthis bound is tight, using an argument based on anti-concentration for thebinomial distribution. Along the way of establishing the upper bound, we develop a technique forminimizing norms over convex sets with complicated geometry; this is achievedby running Mirror Descent on a dual convex function obtained via Sion'sTheorem. As simple extensions of our method, we then provide new algorithms forsubmodular function minimization and SVM training. For submodular functionminimization we obtain a simplification and (provable) speed-up over Wolfe'salgorithm, the method commonly found to be the fastest in practice. For SVMtraining, we obtain $O(1/\epsilon^2)$ convergence for arbitrary kernels; eachiteration only requires matrix-vector operations involving the kernel matrix,so we overcome the obstacle of having to explicitly store the kernel or computeits Cholesky factorization.
arxiv-14400-130 | Optimal Selective Attention in Reactive Agents | http://arxiv.org/pdf/1512.08575v1.pdf | author:Roy Fox, Naftali Tishby category:cs.LG cs.IT math.IT published:2015-12-29 summary:In POMDPs, information about the hidden state, delivered throughobservations, is both valuable to the agent, allowing it to base its actions onbetter informed internal states, and a "curse", exploding the size anddiversity of the internal state space. One attempt to deal with this is tofocus on reactive policies, that only base their actions on the most recentobservation. However, even reactive policies can be demanding on resources, andagents need to pay selective attention to only some of the informationavailable to them in observations. In this report we present theminimum-information principle for selective attention in reactive agents. Wefurther motivate this approach by reducing the general problem of optimalcontrol in POMDPs, to reactive control with complex observations. Lastly, weexplore a newly discovered phenomenon of this optimization process - perioddoubling bifurcations. This necessitates periodic policies, and raises manymore questions regarding stability, periodicity and chaos in optimal control.
arxiv-14400-131 | Structured Pruning of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1512.08571v1.pdf | author:Sajid Anwar, Kyuyeon Hwang, Wonyong Sung category:cs.NE cs.LG stat.ML published:2015-12-29 summary:Real time application of deep learning algorithms is often hindered by highcomputational complexity and frequent memory accesses. Network pruning is apromising technique to solve this problem. However, pruning usually results inirregular network connections that not only demand extra representation effortsbut also do not fit well on parallel computation. We introduce structuredsparsity at various scales for convolutional neural networks, which are channelwise, kernel wise and intra kernel strided sparsity. This structured sparsityis very advantageous for direct computational resource savings on embeddedcomputers, parallel computing environments and hardware based systems. Todecide the importance of network connections and paths, the proposed methoduses a particle filtering approach. The importance weight of each particle isassigned by computing the misclassification rate with correspondingconnectivity pattern. The pruned network is re-trained to compensate for thelosses due to pruning. While implementing convolutions as matrix products, weparticularly show that intra kernel strided sparsity with a simple constraintcan significantly reduce the size of kernel and feature map matrices. Thepruned network is finally fixed point optimized with reduced word lengthprecision. This results in significant reduction in the total storage sizeproviding advantages for on-chip memory based implementations of deep neuralnetworks.
arxiv-14400-132 | Analyzing Walter Skeat's Forty-Five Parallel Extracts of William Langland's Piers Plowman | http://arxiv.org/pdf/1512.08569v1.pdf | author:Roger Bilisoly category:stat.AP cs.CL published:2015-12-29 summary:Walter Skeat published his critical edition of William Langland's 14thcentury alliterative poem, Piers Plowman, in 1886. In preparation for this helocated forty-five manuscripts, and to compare dialects, he published excerptsfrom each of these. This paper does three statistical analyses using theseexcerpts, each of which mimics a task he did in writing his critical edition.First, he combined multiple versions of a poetic line to create a best line,which is compared to the mean string that is computed by a generalization ofthe arithmetic mean that uses edit distance. Second, he claims that a certainsubset of manuscripts varies little. This is quantified by computing a stringvariance, which is closely related to the above generalization of the mean.Third, he claims that the manuscripts fall into three groups, which is aclustering problem that is addressed by using edit distance. The overall goalis to develop methodology that would be of use to a literary critic.
arxiv-14400-133 | G-Learning: Taming the Noise in Reinforcement Learning via Soft Updates | http://arxiv.org/pdf/1512.08562v1.pdf | author:Roy Fox, Ari Pakman, Naftali Tishby category:cs.LG cs.IT math.IT published:2015-12-28 summary:Model-free reinforcement learning algorithms such as Q-learning performpoorly in the early stages of learning in noisy environments, because mucheffort is spent on unlearning biased estimates of the state-action function.The bias comes from selecting, among several noisy estimates, the apparentoptimum, which may actually be suboptimal. We propose G-learning, a newoff-policy learning algorithm that regularizes the noise in the space ofoptimal actions by penalizing deterministic policies at the beginning of thelearning. Moreover, it enables naturally incorporating prior distributions overoptimal actions when available. The stochastic nature of G-learning also makesit more cost-effective than Q-learning in noiseless but exploration-riskydomains. We illustrate these ideas in several examples where G-learning resultsin significant improvements of the learning rate and the learning cost.
arxiv-14400-134 | Phase Transitions for High Dimensional Clustering and Related Problems | http://arxiv.org/pdf/1502.06952v3.pdf | author:Jiashun Jin, Zheng Tracy Ke, Wanjie Wang category:math.ST stat.ML stat.TH published:2015-02-24 summary:Consider a two-class clustering problem where we observe $X_i = \ell_i \mu +Z_i$, $Z_i \stackrel{iid}{\sim} N(0, I_p)$, $1 \leq i \leq n$. The featurevector $\mu\in R^p$ is unknown but is presumably sparse. The class labels$\ell_i\in\{-1, 1\}$ are also unknown and the main interest is to estimatethem. We are interested in the statistical limits. In the two-dimensional phasespace calibrating the rarity and strengths of useful features, we find theprecise demarcation for the Region of Impossibility and Region of Possibility.In the former, useful features are too rare/weak for successful clustering. Inthe latter, useful features are strong enough to allow successful clustering.The results are extended to the case of colored noise using Le Cam's idea oncomparison of experiments. We also extend the study on statistical limits for clustering to that forsignal recovery and that for hypothesis testing. We compare the statisticallimits for three problems and expose some interesting insight. We propose classical PCA and Important Features PCA (IF-PCA) for clustering.For a threshold $t > 0$, IF-PCA clusters by applying classical PCA to allcolumns of $X$ with an $L^2$-norm larger than $t$. We also propose twoaggregation methods. For any parameter in the Region of Possibility, some ofthese methods yield successful clustering. We find an interesting phasetransition for IF-PCA. Our results require delicate analysis, especially on post-selection RandomMatrix Theory and on lower bound arguments.
arxiv-14400-135 | A Simple Baseline for Travel Time Estimation using Large-Scale Trip Data | http://arxiv.org/pdf/1512.08580v1.pdf | author:Hongjian Wang, Zhenhui Li, Yu-Hsuan Kuo, Dan Kifer category:cs.LG cs.CY H.2.8; I.2.6 published:2015-12-28 summary:The increased availability of large-scale trajectory data around the worldprovides rich information for the study of urban dynamics. For example, NewYork City Taxi Limousine Commission regularly releases source-destinationinformation about trips in the taxis they regulate. Taxi data provideinformation about traffic patterns, and thus enable the study of urban flow --what will traffic between two locations look like at a certain date and time inthe future? Existing big data methods try to outdo each other in terms ofcomplexity and algorithmic sophistication. In the spirit of "big data beatsalgorithms", we present a very simple baseline which outperformsstate-of-the-art approaches, including Bing Maps and Baidu Maps (whose APIspermit large scale experimentation). Such a travel time estimation baseline hasseveral important uses, such as navigation (fast travel time estimates canserve as approximate heuristics for A search variants for path finding) andtrip planning (which uses operating hours for popular destinations along withtravel time estimates to create an itinerary).
arxiv-14400-136 | MRF-based multispectral image fusion using an adaptive approach based on edge-guided interpolation | http://arxiv.org/pdf/1512.08475v1.pdf | author:Mohammad Reza Khosravi, Suleiman Mansouri, Ahmad Keshavarz, Habib Rostami category:cs.CV published:2015-12-28 summary:In interpretation of remote sensing images, it is possible that the imageswhich are supplied by different sensors wouldn't be understandable or we couldnot get vital information from them. For better visual perception of images, itis essential to operate series of pre-processing and elementary corrections andthen operate a series of main processing for more precise analysis on theimage. There are several approaches for processing which depend on type ofremote sensing image. The discussed approach in this article, i.e. imagefusion, is using natural colors of an optical image for adding color togray-scale satellite image which gives us the ability to better observe the HRimage of the OLI sensor of Landsat. This process previously with emphasis ondetails of fusion technique was performed, but we are going to relieve conceptof interpolation process that did not have suitable attentions in past. In factwe see many important software tools such as ENVI and ERDAS as most famousremote sensing image processing software tools have only classicalinterpolation techniques (such as BL and CC). Therefore ENVI-based andERDAS-based researches in image fusion area and even other fusion researchesoften do not use new and better interpolations and only are concentrating onfusion details for achievement of better quality, so we only focus oninterpolation impact in fusion quality in a specific application, i.e. Landsatmulti-spectral images. The important feature of this approach is using astatistical, adaptive, edge-guided and MRF-based interpolation method forimproving color quality in MRF-based images with maintenance of high resolutionin practice. Numerical Simulations show selection of suitable interpolationtechnique in MRF-based images creates better quality rather than classicalinterpolations.
arxiv-14400-137 | Approximate Hubel-Wiesel Modules and the Data Structures of Neural Computation | http://arxiv.org/pdf/1512.08457v1.pdf | author:Joel Z. Leibo, Julien Cornebise, Sergio Gómez, Demis Hassabis category:cs.NE q-bio.NC published:2015-12-28 summary:This paper describes a framework for modeling the interface betweenperception and memory on the algorithmic level of analysis. It is consistentwith phenomena associated with many different brain regions. These includeview-dependence (and invariance) effects in visual psychophysics andinferotemporal cortex physiology, as well as episodic memory recallinterference effects associated with the medial temporal lobe. The perspectivedeveloped here relies on a novel interpretation of Hubel and Wiesel'sconjecture for how receptive fields tuned to complex objects, and invariant todetails, could be achieved. It complements existing accounts of two-speedlearning systems in neocortex and hippocampus (e.g., McClelland et al. 1995)while significantly expanding their scope to encompass a unified view of theentire pathway from V1 to hippocampus.
arxiv-14400-138 | Graph entropies in texture segmentation of images | http://arxiv.org/pdf/1512.08424v1.pdf | author:Martin Welk category:cs.CV published:2015-12-28 summary:We study the applicability of a set of texture descriptors introduced inrecent work by the author to texture-based segmentation of images. The texturedescriptors under investigation result from applying graph indices fromquantitative graph theory to graphs encoding the local structure of images. Theunderlying graphs arise from the computation of morphological amoebas asstructuring elements for adaptive morphology, either as weighted or unweightedDijkstra search trees or as edge-weighted pixel graphs within structuringelements. In the present paper we focus on texture descriptors in which thegraph indices are entropy-based, and use them in a geodesic active contourframework for image segmentation. Experiments on several synthetic and onereal-world image are shown to demonstrate texture segmentation by thisapproach. Forthermore, we undertake an attempt to analyse selectedentropy-based texture descriptors with regard to what information about texturethey actually encode. Whereas this analysis uses some heuristic assumptions, itindicates that the graph-based texture descriptors are related to fractaldimension measures that have been proven useful in texture analysis.
arxiv-14400-139 | Ordered Decompositional DAG Kernels Enhancements | http://arxiv.org/pdf/1507.03372v2.pdf | author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG published:2015-07-13 summary:In this paper, we show how the Ordered Decomposition DAGs (ODD) kernelframework, a framework that allows the definition of graph kernels from treekernels, allows to easily define new state-of-the-art graph kernels. Here weconsider a fast graph kernel based on the Subtree kernel (ST), and we proposevarious enhancements to increase its expressiveness. The proposed DAG kernelhas the same worst-case complexity as the one based on ST, but an improvedexpressivity due to an augmented set of features. Moreover, we propose a novelweighting scheme for the features, which can be applied to other kernels of theODD framework. These improvements allow the proposed kernels to improve on theclassification performances of the ST-based kernel for several real-worlddatasets, reaching state-of-the-art performances.
arxiv-14400-140 | Outlier Detection In Large-scale Traffic Data By Naïve Bayes Method and Gaussian Mixture Model Method | http://arxiv.org/pdf/1512.08413v1.pdf | author:Philip Lam, Lili Wang, Henry Y. T. Ngan, Nelson H. C. Yung, Anthony G. O. Yeh category:cs.CV published:2015-12-28 summary:It is meaningful to detect outliers in traffic data for traffic management.However, this is a massive task for people from large-scale database todistinguish outliers. In this paper, we present two methods: Kernel SmoothingNa\"ive Bayes (NB) method and Gaussian Mixture Model (GMM) method toautomatically detect any hardware errors as well as abnormal traffic events intraffic data collected at a four-arm junction in Hong Kong. Traffic data wasrecorded in a video format, and converted to spatial-temporal (ST) trafficsignals by statistics. The ST signals are then projected to a two-dimensional(2D) (x,y)-coordinate plane by Principal Component Analysis (PCA) for dimensionreduction. We assume that inlier data are normal distributed. As such, the NBand GMM methods are successfully applied in outlier detection (OD) for trafficdata. The kernel smooth NB method assumes the existence of kernel distributionsin traffic data and uses Bayes' Theorem to perform OD. In contrast, the GMMmethod believes the traffic data is formed by the mixture of Gaussiandistributions and exploits confidence region for OD. This paper would addressthe modeling of each method and evaluate their respective performances.Experimental results show that the NB algorithm with Triangle kernel and GMMmethod achieve up to 93.78% and 94.50% accuracies, respectively.
arxiv-14400-141 | Automatic Incident Classification for Big Traffic Data by Adaptive Boosting SVM | http://arxiv.org/pdf/1512.04392v2.pdf | author:Li-Li Wang, Henry Y. T. Ngan, Nelson H. C. Yung category:cs.LG published:2015-12-14 summary:Modern cities experience heavy traffic flows and congestions regularly acrossspace and time. Monitoring traffic situations becomes an important challengefor the Traffic Control and Surveillance Systems (TCSS). In advanced TCSS, itis helpful to automatically detect and classify different traffic incidentssuch as severity of congestion, abnormal driving pattern, abrupt or illegalstop on road, etc. Although most TCSS are equipped with basic incidentdetection algorithms, they are however crude to be really useful as anautomated tool for further classification. In literature, there is a lack ofresearch for Automated Incident Classification (AIC). Therefore, a novel AICmethod is proposed in this paper to tackle such challenges. In the proposedmethod, traffic signals are firstly extracted from captured videos andconverted as spatial-temporal (ST) signals. Based on the characteristics of theST signals, a set of realistic simulation data are generated to construct anextended big traffic database to cover a variety of traffic situations. Next, aMean-Shift filter is introduced to suppress the effect of noise and extractsignificant features from the ST signals. The extracted features are thenassociated with various types of traffic data: one normal type (inliers) andmultiple abnormal types (outliers). For the classification, an adaptiveboosting classifier is trained to detect outliers in traffic dataautomatically. Further, a Support Vector Machine (SVM) based method is adoptedto train the model for identifying the categories of outliers. In short, thishybrid approach is called an Adaptive Boosting Support Vector Machines (AB-SVM)method. Experimental results show that the proposed AB-SVM method achieves asatisfied result with more than 92% classification accuracy on average.
arxiv-14400-142 | Linear-time Online Action Detection From 3D Skeletal Data Using Bags of Gesturelets | http://arxiv.org/pdf/1502.01228v6.pdf | author:Moustafa Meshry, Mohamed E. Hussein, Marwan Torki category:cs.CV published:2015-02-04 summary:Sliding window is one direct way to extend a successful recognition system tohandle the more challenging detection problem. While action recognition decidesonly whether or not an action is present in a pre-segmented video sequence,action detection identifies the time interval where the action occurred in anunsegmented video stream. Sliding window approaches for action detection canhowever be slow as they maximize a classifier score over all possiblesub-intervals. Even though new schemes utilize dynamic programming to speed upthe search for the optimal sub-interval, they require offline processing on thewhole video sequence. In this paper, we propose a novel approach for onlineaction detection based on 3D skeleton sequences extracted from depth data. Itidentifies the sub-interval with the maximum classifier score in linear time.Furthermore, it is invariant to temporal scale variations and is suitable forreal-time applications with low latency.
arxiv-14400-143 | Using Causal Discovery to Track Information Flow in Spatio-Temporal Data - A Testbed and Experimental Results Using Advection-Diffusion Simulations | http://arxiv.org/pdf/1512.08279v1.pdf | author:Imme Ebert-Uphoff, Yi Deng category:cs.LG published:2015-12-27 summary:Causal discovery algorithms based on probabilistic graphical models haveemerged in geoscience applications for the identification and visualization ofdynamical processes. The key idea is to learn the structure of a graphicalmodel from observed spatio-temporal data, which indicates information flow,thus pathways of interactions, in the observed physical system. Studying thosepathways allows geoscientists to learn subtle details about the underlyingdynamical mechanisms governing our planet. Initial studies using this approachon real-world atmospheric data have shown great potential for scientificdiscovery. However, in these initial studies no ground truth was available, sothat the resulting graphs have been evaluated only by whether a domain expertthinks they seemed physically plausible. This paper seeks to fill this gap. Wedevelop a testbed that emulates two dynamical processes dominant in manygeoscience applications, namely advection and diffusion, in a 2D grid. Then weapply the causal discovery based information tracking algorithms to thesimulation data to study how well the algorithms work for different scenariosand to gain a better understanding of the physical meaning of the graphresults, in particular of instantaneous connections. We make all data sets usedin this study available to the community as a benchmark. Keywords: Information flow, graphical model, structure learning, causaldiscovery, geoscience.
arxiv-14400-144 | Statistical and Computational Guarantees for the Baum-Welch Algorithm | http://arxiv.org/pdf/1512.08269v1.pdf | author:Fanny Yang, Sivaraman Balakrishnan, Martin J. Wainwright category:stat.ML cs.IT math.IT math.ST stat.TH published:2015-12-27 summary:The Hidden Markov Model (HMM) is one of the mainstays of statistical modelingof discrete time series, with applications including speech recognition,computational biology, computer vision and econometrics. Estimating an HMM fromits observation process is often addressed via the Baum-Welch algorithm, whichis known to be susceptible to local optima. In this paper, we first give ageneral characterization of the basin of attraction associated with any globaloptimum of the population likelihood. By exploiting this characterization, weprovide non-asymptotic finite sample guarantees on the Baum-Welch updates,guaranteeing geometric convergence to a small ball of radius on the order ofthe minimax rate around a global optimum. As a concrete example, we prove alinear rate of convergence for a hidden Markov mixture of two isotropicGaussians given a suitable mean separation and an initialization within a ballof large radius around (one of) the true parameters. To our knowledge, theseare the first rigorous local convergence guarantees to global optima for theBaum-Welch algorithm in a setting where the likelihood function is nonconvex.We complement our theoretical results with thorough numerical simulationsstudying the convergence of the Baum-Welch algorithm and illustrating theaccuracy of our predictions.
arxiv-14400-145 | Robust Semi-supervised Least Squares Classification by Implicit Constraints | http://arxiv.org/pdf/1512.08240v1.pdf | author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG published:2015-12-27 summary:We introduce the implicitly constrained least squares (ICLS) classifier, anovel semi-supervised version of the least squares classifier. This classifierminimizes the squared loss on the labeled data among the set of parametersimplied by all possible labelings of the unlabeled data. Unlike otherdiscriminative semi-supervised methods, this approach does not introduceexplicit additional assumptions into the objective function, but leveragesimplicit assumptions already present in the choice of the supervised leastsquares classifier. This method can be formulated as a quadratic programmingproblem and its solution can be found using a simple gradient descentprocedure. We prove that, in a limited 1-dimensional setting, this approachnever leads to performance worse than the supervised classifier. Experimentalresults show that also in the general multidimensional case performanceimprovements can be expected, both in terms of the squared loss that isintrinsic to the classifier, as well as in terms of the expected classificationerror.
arxiv-14400-146 | Sequence to Sequence Learning for Optical Character Recognition | http://arxiv.org/pdf/1511.04176v2.pdf | author:Devendra Kumar Sahu, Mohak Sukhwani category:cs.CV published:2015-11-13 summary:We propose an end-to-end recurrent encoder-decoder based sequence learningapproach for printed text Optical Character Recognition (OCR). In contrast topresent day existing state-of-art OCR solution which uses connectionisttemporal classification (CTC) output layer, our approach makes minimalisticassumptions on the structure and length of the sequence. We use a two stepencoder-decoder approach -- (a) A recurrent encoder reads a variable lengthprinted text word image and encodes it to a fixed dimensional embedding. (b)This fixed dimensional embedding is subsequently comprehended by decoderstructure which converts it into a variable length text output. Ourarchitecture gives competitive performance relative to connectionist temporalclassification (CTC) output layer while being executed in more naturalsettings. The learnt deep word image embedding from encoder can be used forprinted text based retrieval systems. The expressive fixed dimensionalembedding for any variable length input expedites the task of retrieval andmakes it more efficient which is not possible with other recurrent neuralnetwork architectures. We empirically investigate the expressiveness and thelearnability of long short term memory (LSTMs) in the sequence to sequencelearning regime by training our network for prediction tasks in segmentationfree printed text OCR. The utility of the proposed architecture for printedtext is demonstrated by quantitative and qualitative evaluation of two tasks --word prediction and retrieval.
arxiv-14400-147 | Improving Facial Analysis and Performance Driven Animation through Disentangling Identity and Expression | http://arxiv.org/pdf/1512.08212v1.pdf | author:David Rim, Sina Honari, Md Kamrul Hasan, Chris Pal category:cs.CV published:2015-12-27 summary:We present techniques for improving performance driven facial animation,emotion recognition, and facial key-point or landmark prediction techniquesusing learned identity invariant representations. Established approaches tothese problems can work well if sufficient examples and labels for a particularidentity are available and factors of variation are highly controlled. However,labeled examples of facial expressions, emotions and key-points for newindividuals are difficult and costly to obtain. In this paper we improve theability of techniques to generalize to new and unseen individuals by explicitlymodeling previously seen variations related to identity and expression. We usea weakly-supervised approach in which identity labels are used to learn thedifferent factors of variation linked to identity separately from factorsrelated to expression. We show how probabilistic modeling of these sources ofvariation allows one to learn identity-invariant representations forexpressions which can then used to identity-normalize various procedures forfacial expression analysis and animation control. We also show how to extendthe widely used techniques of active appearance models and constrained localmodels through replacing the underlying point distribution models which aretypically constructed using principal component analysis withidentity-expression factorized representations. We present a wide variety ofexperiments in which we consistently improve performance on emotionrecognition, markerless performance-driven facial animation and facialkey-point tracking.
arxiv-14400-148 | New Perspectives on $k$-Support and Cluster Norms | http://arxiv.org/pdf/1512.08204v1.pdf | author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:cs.LG stat.ML published:2015-12-27 summary:We study a regularizer which is defined as a parameterized infimum ofquadratics, and which we call the box-norm. We show that the k-support norm, aregularizer proposed by [Argyriou et al, 2012] for sparse vector predictionproblems, belongs to this family, and the box-norm can be generated as aperturbation of the former. We derive an improved algorithm to compute theproximity operator of the squared box-norm, and we provide a method to computethe norm. We extend the norms to matrices, introducing the spectral k-supportnorm and spectral box-norm. We note that the spectral box-norm is essentiallyequivalent to the cluster norm, a multitask learning regularizer introduced by[Jacob et al. 2009a], and which in turn can be interpreted as a perturbation ofthe spectral k-support norm. Centering the norm is important for multitasklearning and we also provide a method to use centered versions of the norms asregularizers. Numerical experiments indicate that the spectral k-support andbox-norms and their centered variants provide state of the art performance inmatrix completion and multitask learning problems respectively.
arxiv-14400-149 | Advanced statistical methods for eye movement analysis and modeling: a gentle introduction | http://arxiv.org/pdf/1506.07194v3.pdf | author:Giuseppe Boccignone category:cs.CV q-bio.NC G.3; I.5; I.4 published:2015-06-23 summary:In this Chapter we show that by considering eye movements, and in particular,the resulting sequence of gaze shifts, a stochastic process, a wide variety oftools become available for analyses and modelling beyond conventionalstatistical methods. Such tools encompass random walk analyses and more complextechniques borrowed from the pattern recognition and machine learning fields. After a brief, though critical, probabilistic tour of current computationalmodels of eye movements and visual attention, we lay down the basis for gazeshift pattern analysis. To this end, the concepts of Markov Processes, theWiener process and related random walks within the Gaussian framework of theCentral Limit Theorem will be introduced. Then, we will deliberately violatefundamental assumptions of the Central Limit Theorem to elicit a largerperspective, rooted in statistical physics, for analysing and modelling eyemovements in terms of anomalous, non-Gaussian, random walks and modern foragingtheory. Eventually, by resorting to machine learning techniques, we discuss how theanalyses of movement patterns can develop into the inference of hidden patternsof the mind: inferring the observer's task, assessing cognitive impairments,classifying expertise.
arxiv-14400-150 | Mixed Gaussian-Impulse Noise Removal from Highly Corrupted Images via Adaptive Local and Nonlocal Statistical Priors | http://arxiv.org/pdf/1508.07415v2.pdf | author:Nasser Eslahi, Hami Mahdavinataj, Ali Aghagolzadeh category:cs.CV 62F15 published:2015-08-29 summary:The motivation of this paper is to introduce a novel framework for therestoration of images corrupted by mixed Gaussian-impulse noise. To this aim,first, an adaptive curvelet thresholding criterion is proposed which tries toadaptively remove the perturbations appeared during denoising process. Then, anew statistical regularization term, called joint adaptive statistical prior(JASP), is established which enforces both the local and nonlocal statisticalconsistencies, simultaneously, in a unified manner. Furthermore, a noveltechnique for mixed Gaussian plus impulse noise removal using JASP in avariational scheme is developed--we refer to it as De-JASP. To efficientlysolve the above variational scheme, an efficient alternating minimizationalgorithm based on split Bregman iterative framework is developed. Extensiveexperimental results manifest the effectiveness of the proposed methodcomparing with the current state-of-the-art methods in mixed Gaussian-impulsenoise removal.
arxiv-14400-151 | Electricity Demand Forecasting by Multi-Task Learning | http://arxiv.org/pdf/1512.08178v1.pdf | author:Jean-Baptiste Fiot, Francesco Dinuzzo category:cs.LG published:2015-12-27 summary:We explore the application of kernel-based multi-task learning techniques toforecast the demand of electricity in multiple nodes of a distribution network.We show that recently developed output kernel learning techniques areparticularly well suited to solve this problem, as they allow to flexibly modelthe complex seasonal effects that characterize electricity demand data, whilelearning and exploiting correlations between multiple demand profiles. We alsodemonstrate that kernels with a multiplicative structure yield superiorpredictive performance with respect to the widely adopted (generalized)additive models. Our study is based on residential and industrial smart meterdata provided by the Irish Commission for Energy Regulation (CER).
arxiv-14400-152 | TransG : A Generative Mixture Model for Knowledge Graph Embedding | http://arxiv.org/pdf/1509.05488v4.pdf | author:Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu category:cs.CL published:2015-09-18 summary:Recently, knowledge graph embedding, which projects symbolic entities andrelations into continuous vector space, has become a new, hot topic inartificial intelligence. This paper addresses a new issue of \textbf{multiplerelation semantics} that a relation may have multiple meanings revealed by theentity pairs associated with the corresponding triples, and proposes a novelGaussian mixture model for embedding, \textbf{TransG}. The new model candiscover latent semantics for a relation and leverage a mixture of relationcomponent vectors for embedding a fact triple. To the best of our knowledge,this is the first generative model for knowledge graph embedding, which is ableto deal with multiple relation semantics. Extensive experiments show that theproposed model achieves substantial improvements against the state-of-the-artbaselines.
arxiv-14400-153 | Self-Excitation: An Enabler for Online Thermal Estimation and Model Predictive Control of Buildings | http://arxiv.org/pdf/1512.08169v1.pdf | author:Peter Radecki, Brandon Hencey category:cs.SY cs.LG published:2015-12-27 summary:This paper investigates a method to improve buildings' thermal predictivecontrol performance via online identification and excitation (active learningprocess) that minimally disrupts normal operations. In previous studies we havedemonstrated scalable methods to acquire multi-zone thermal models of passivebuildings using a gray-box approach that leverages building topology andmeasurement data. Here we extend the method to multi-zone actively controlledbuildings and examine how to improve the thermal model estimation by using thecontroller to excite unknown portions of the building's dynamics. Comparingagainst a baseline thermostat controller, we demonstrate the utility of boththe initially acquired and improved thermal models within a Model PredictiveControl (MPC) framework, which anticipates weather uncertainty and time-varyingtemperature set-points. A simulation study demonstrates self-excitationimproves model estimation, which corresponds to improved MPC energy savings andoccupant comfort. By coupling building topology, estimation, and controlroutines into a single online framework, we have demonstrated the potential forlow-cost scalable methods to actively learn and control buildings to ensureoccupant comfort and minimize energy usage, all while using the existingbuilding's HVAC sensors and hardware.
arxiv-14400-154 | Online Model Estimation for Predictive Thermal Control of Buildings | http://arxiv.org/pdf/1601.02947v1.pdf | author:Peter Radecki, Brandon Hencey category:cs.SY cs.LG published:2015-12-27 summary:This study proposes a general, scalable method to learn control-orientedthermal models of buildings that could enable wide-scale deployment ofcost-effective predictive controls. An Unscented Kalman Filter augmented forparameter and disturbance estimation is shown to accurately learn and predict abuilding's thermal response. Recent studies of heating, ventilating, and airconditioning (HVAC) systems have shown significant energy savings with advancedmodel predictive control (MPC). A scalable cost-effective method to readilyacquire accurate, robust models of individual buildings' unique thermalenvelopes has historically been elusive and hindered the widespread deploymentof prediction-based control systems. Continuous commissioning and lifetimeperformance of these thermal models requires deployment of on-line data-drivensystem identification and parameter estimation routines. We propose a novelgray-box approach using an Unscented Kalman Filter based on a multi-zonethermal network and validate it with EnergyPlus simulation data. The filterquickly learns parameters of a thermal network during periods of known orconstrained loads and then characterizes unknown loads in order to provideaccurate 24+ hour energy predictions. This study extends our initialinvestigation by formalizing parameter and disturbance estimation routines anddemonstrating results across a year-long study.
arxiv-14400-155 | Large-Scale Optimization Algorithms for Sparse Conditional Gaussian Graphical Models | http://arxiv.org/pdf/1509.04681v2.pdf | author:Calvin McCarter, Seyoung Kim category:stat.ML published:2015-09-15 summary:This paper addresses the problem of scalable optimization for L1-regularizedconditional Gaussian graphical models. Conditional Gaussian graphical modelsgeneralize the well-known Gaussian graphical models to conditionaldistributions to model the output network influenced by conditioning inputvariables. While highly scalable optimization methods exist for sparse Gaussiangraphical model estimation, state-of-the-art methods for conditional Gaussiangraphical models are not efficient enough and more importantly, fail due tomemory constraints for very large problems. In this paper, we propose a newoptimization procedure based on a Newton method that efficiently iterates overtwo sub-problems, leading to drastic improvement in computation time comparedto the previous methods. We then extend our method to scale to large problemsunder memory constraints, using block coordinate descent to limit memory usagewhile achieving fast convergence. Using synthetic and genomic data, we showthat our methods can solve one million dimensional problems to high accuracy ina little over a day on a single machine.
arxiv-14400-156 | The Utility of Abstaining in Binary Classification | http://arxiv.org/pdf/1512.08133v1.pdf | author:Akshay Balsubramani category:cs.LG published:2015-12-26 summary:We explore the problem of binary classification in machine learning, with atwist - the classifier is allowed to abstain on any datum, professing ignoranceabout the true class label without committing to any prediction. This isdirectly motivated by applications like medical diagnosis and fraud riskassessment, in which incorrect predictions have potentially calamitousconsequences. We focus on a recent spate of theoretically driven work in thisarea that characterizes how allowing abstentions can lead to fewer errors invery general settings. Two areas are highlighted: the surprising possibility ofzero-error learning, and the fundamental tradeoff between predictingsufficiently often and avoiding incorrect predictions. We review efficientalgorithms with provable guarantees for each of these areas. We also discussconnections to other scenarios, notably active learning, as they suggestpromising directions of further inquiry in this emerging field.
arxiv-14400-157 | K2-ABC: Approximate Bayesian Computation with Kernel Embeddings | http://arxiv.org/pdf/1502.02558v4.pdf | author:Mijung Park, Wittawat Jitkrittum, Dino Sejdinovic category:stat.ML cs.LG published:2015-02-09 summary:Complicated generative models often result in a situation where computing thelikelihood of observed data is intractable, while simulating from theconditional density given a parameter value is relatively easy. ApproximateBayesian Computation (ABC) is a paradigm that enables simulation-basedposterior inference in such cases by measuring the similarity between simulatedand observed data in terms of a chosen set of summary statistics. However,there is no general rule to construct sufficient summary statistics for complexmodels. Insufficient summary statistics will "leak" information, which leads toABC algorithms yielding samples from an incorrect (partial) posterior. In thispaper, we propose a fully nonparametric ABC paradigm which circumvents the needfor manually selecting summary statistics. Our approach, K2-ABC, uses maximummean discrepancy (MMD) as a dissimilarity measure between the distributionsover observed and simulated data. MMD is easily estimated as the squareddifference between their empirical kernel embeddings. Experiments on asimulated scenario and a real-world biological problem illustrate theeffectiveness of the proposed algorithm.
arxiv-14400-158 | Data Driven Robust Image Guided Depth Map Restoration | http://arxiv.org/pdf/1512.08103v1.pdf | author:Wei Liu, Yun Gu, Chunhua Shen, Xiaogang Chen, Qiang Wu, Jie Yang category:cs.CV published:2015-12-26 summary:Depth maps captured by modern depth cameras such as Kinect and Time-of-Flight(ToF) are usually contaminated by missing data, noises and suffer from being oflow resolution. In this paper, we present a robust method for high-qualityrestoration of a degraded depth map with the guidance of the correspondingcolor image. We solve the problem in an energy optimization framework thatconsists of a novel robust data term and smoothness term. To accommodate notonly the noise but also the inconsistency between depth discontinuities and thecolor edges, we model both the data term and smoothness term with a robustexponential error norm function. We propose to use Iteratively Re-weightedLeast Squares (IRLS) methods for efficiently solving the resulting highlynon-convex optimization problem. More importantly, we further develop adata-driven adaptive parameter selection scheme to properly determine theparameter in the model. We show that the proposed approach can preserve finedetails and sharp depth discontinuities even for a large upsampling factor($8\times$ for example). Experimental results on both simulated and realdatasets demonstrate that the proposed method outperforms recentstate-of-the-art methods in coping with the heavy noise, preserving sharp depthdiscontinuities and suppressing the texture copy artifacts.
arxiv-14400-159 | Part-Stacked CNN for Fine-Grained Visual Categorization | http://arxiv.org/pdf/1512.08086v1.pdf | author:Shaoli Huang, Zhe Xu, Dacheng Tao, Ya Zhang category:cs.CV published:2015-12-26 summary:In the context of fine-grained visual categorization, the ability tointerpret models as human-understandable visual manuals is sometimes asimportant as achieving high classification accuracy. In this paper, we proposea novel Part-Stacked CNN architecture that explicitly explains the fine-grainedrecognition process by modeling subtle differences from object parts. Based onmanually-labeled strong part annotations, the proposed architecture consists ofa fully convolutional network to locate multiple object parts and a two-streamclassification network that en- codes object-level and part-level cuessimultaneously. By adopting a set of sharing strategies between the computationof multiple object parts, the proposed architecture is very efficient runningat 20 frames/sec during inference. Experimental results on the CUB-200-2011dataset reveal the effectiveness of the proposed architecture, from both theperspective of classification accuracy and model interpretability.
arxiv-14400-160 | The Improvement of Negative Sentences Translation in English-to-Korean Machine Translation | http://arxiv.org/pdf/1512.08066v1.pdf | author:Chung-Hyok Jang, Kwang-Hyok Kim category:cs.CL published:2015-12-26 summary:This paper describes the algorithm for translating English negative sentencesinto Korean in English-Korean Machine Translation (EKMT). The proposedalgorithm is based on the comparative study of English and Korean negativesentences. The earlier translation software cannot translate English negativesentences into accurate Korean equivalents. We established a new algorithm forthe negative sentence translation and evaluated it.
arxiv-14400-161 | Inverse Reinforcement Learning via Deep Gaussian Process | http://arxiv.org/pdf/1512.08065v1.pdf | author:Ming Jin, Costas Spanos category:cs.LG cs.RO stat.ML published:2015-12-26 summary:The report proposes a new approach for inverse reinforcement learning basedon deep Gaussian process (GP), which is capable of learning complicated rewardstructures with few demonstrations. The model stacks multiple latent GP layersto learn abstract representations of the state feature space, which is linkedto the demonstrations through the Maximum Entropy learning framework. Asanalytic derivation of the model evidence is prohibitive due to thenonlinearity of latent variables, variational inference is employed forapproximate inference, based on a special choice of variational distributions.This guards the model from over training, achieving the Automatic Occam'sRazor. Experiments on the benchmark test, i.e., object world, as well as a newsetup, i.e., binary world, are performed, where the proposed method outperformsstate-of-the-art approaches.
arxiv-14400-162 | Statistical Learning under Nonstationary Mixing Processes | http://arxiv.org/pdf/1512.08064v1.pdf | author:Steve Hanneke, Tommi Jaakkola, Liu Yang category:cs.LG stat.ML published:2015-12-26 summary:We study a special case of the problem of statistical learning without thei.i.d. assumption. Specifically, we suppose a learning method is presented witha sequence of data points, and required to make a prediction (e.g., aclassification) for each one, and can then observe the loss incurred by thisprediction. We go beyond traditional analyses, which have focused on stationarymixing processes or nonstationary product processes, by combining these tworelaxations to allow nonstationary mixing processes. We are particularlyinterested in the case of $\beta$-mixing processes, with the sum of changes inmarginal distributions growing sublinearly in the number of samples. Underthese conditions, we propose a learning method, and establish that for boundedVC subgraph classes, the cumulative excess risk grows sublinearly in the numberof predictions, at a quantified rate.
arxiv-14400-163 | A Multiresolution Clinical Decision Support System Based on Fractal Model Design for Classification of Histological Brain Tumours | http://arxiv.org/pdf/1512.08051v1.pdf | author:Omar S. Al-Kadi category:cs.CV published:2015-12-25 summary:Tissue texture is known to exhibit a heterogeneous or non-stationary nature,therefore using a single resolution approach for optimum classification mightnot suffice. A clinical decision support system that exploits the subbandtextural fractal characteristics for best bases selection of meningioma brainhistopathological image classification is proposed. Each subband is analysedusing its fractal dimension instead of energy, which has the advantage of beingless sensitive to image intensity and abrupt changes in tissue texture. Themost significant subband that best identifies texture discontinuities will bechosen for further decomposition, and its fractal characteristics wouldrepresent the optimal feature vector for classification. The performance wastested using the support vector machine (SVM), Bayesian and k-nearest neighbour(kNN) classifiers and a leave-one-patient-out method was employed forvalidation. Our method outperformed the classical energy based selectionapproaches, achieving for SVM, Bayesian and kNN classifiers an overallclassification accuracy of 94.12%, 92.50% and 79.70%, as compared to 86.31%,83.19% and 51.63% for the co-occurrence matrix, and 76.01%, 73.50% and 50.69%for the energy texture signatures, respectively. These results indicate thepotential usefulness as a decision support system that could complementradiologists diagnostic capability to discriminate higher order statisticaltextural information, for which it would be otherwise difficult via ordinaryhuman vision.
arxiv-14400-164 | Texture measures combination for improved meningioma classification of histopathological images | http://arxiv.org/pdf/1512.08049v1.pdf | author:Omar S. Al-Kadi category:cs.CV published:2015-12-25 summary:Providing an improved technique which can assist pathologists in correctlyclassifying meningioma tumours with a significant accuracy is our mainobjective. The proposed technique, which is based on optimum texture measurecombination, inspects the separability of the RGB colour channels and selectsthe channel which best segments the cell nuclei of the histopathologicalimages. The morphological gradient was applied to extract the region ofinterest for each subtype and for elimination of possible noise (e.g. cracks)which might occur during biopsy preparation. Meningioma texture features areextracted by four different texture measures (two model-based and twostatistical-based) and then corresponding features are fused together indifferent combinations after excluding highly correlated features, and aBayesian classifier was used for meningioma subtype discrimination. Thecombined Gaussian Markov random field and run-length matrix texture measuresoutperformed all other combinations in terms of quantitatively characterisingthe meningioma tissue, achieving an overall classification accuracy of 92.50%,improving from 83.75% which is the best accuracy achieved if the texturemeasures are used individually.
arxiv-14400-165 | Assessment of texture measures susceptibility to noise in conventional and contrast enhanced computed tomography lung tumour images | http://arxiv.org/pdf/1512.08047v1.pdf | author:Omar Sultan Al-Kadi category:cs.CV published:2015-12-25 summary:Noise is one of the major problems that hinder an effective texture analysisof disease in medical images, which may cause variability in the reporteddiagnosis. In this paper seven texture measurement methods (two wavelet, twomodel and three statistical based) were applied to investigate theirsusceptibility to subtle noise caused by acquisition and reconstructiondeficiencies in computed tomography (CT) images. Features of lung tumours wereextracted from two different conventional and contrast enhanced CT imagedata-sets under filtered and noisy conditions. When measuring the noise in thebackground open-air region of the analysed CT images, noise of Gaussian andRayleigh distributions with varying mean and variance was encountered, andFisher distance was used to differentiate between an original extracted lungtumour region of interest (ROI) with the filtered and noisy reconstructedversions. It was determined that the wavelet packet (WP) and fractal dimensionmeasures were the least affected, while the Gaussian Markov random field,run-length and co-occurrence matrices were the most affected by noise.Depending on the selected ROI size, it was concluded that texture measures withfewer extracted features can decrease susceptibility to noise, with the WP andthe Gabor filter having a stable performance in both filtered and noisy CTversions and for both data-sets. Knowing how robust each texture measure undernoise presence is can assist physicians using an automated lung textureclassification system in choosing the appropriate feature extraction algorithmfor a more accurate diagnosis.
arxiv-14400-166 | Larger-Context Language Modelling | http://arxiv.org/pdf/1511.03729v2.pdf | author:Tian Wang, Kyunghyun Cho category:cs.CL published:2015-11-11 summary:In this work, we propose a novel method to incorporate corpus-level discourseinformation into language modelling. We call this larger-context languagemodel. We introduce a late fusion approach to a recurrent language model basedon long short-term memory units (LSTM), which helps the LSTM unit keepintra-sentence dependencies and inter-sentence dependencies separate from eachother. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank),we demon- strate that the proposed model improves perplexity significantly. Inthe experi- ments, we evaluate the proposed approach while varying the numberof context sentences and observe that the proposed late fusion is superior tothe usual way of incorporating additional inputs to the LSTM. By analyzing thetrained larger- context language model, we discover that content words,including nouns, adjec- tives and verbs, benefit most from an increasing numberof context sentences. This analysis suggests that larger-context language modelimproves the unconditional language model by capturing the theme of a documentbetter and more easily.
arxiv-14400-167 | Discovering topic structures of a temporally evolving document corpus | http://arxiv.org/pdf/1512.08008v1.pdf | author:Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh category:cs.IR cs.LG published:2015-12-25 summary:In this paper we describe a novel framework for the discovery of the topicalcontent of a data corpus, and the tracking of its complex structural changesacross the temporal dimension. In contrast to previous work our model does notimpose a prior on the rate at which documents are added to the corpus nor doesit adopt the Markovian assumption which overly restricts the type of changesthat the model can capture. Our key technical contribution is a framework basedon (i) discretization of time into epochs, (ii) epoch-wise topic discoveryusing a hierarchical Dirichlet process-based model, and (iii) a temporalsimilarity graph which allows for the modelling of complex topic changes:emergence and disappearance, evolution, splitting, and merging. The power ofthe proposed framework is demonstrated on two medical literature corporaconcerned with the autism spectrum disorder (ASD) and the metabolic syndrome(MetS) -- both increasingly important research subjects with significant socialand healthcare consequences. In addition to the collected ASD and metabolicsyndrome literature corpora which we made freely available, our contributionalso includes an extensive empirical analysis of the proposed framework. Wedescribe a detailed and careful examination of the effects that ouralgorithms's free parameters have on its output, and discuss the significanceof the findings both in the context of the practical application of ouralgorithm as well as in the context of the existing body of work on temporaltopic analysis. Our quantitative analysis is followed by several qualitativecase studies highly relevant to the current research on ASD and MetS, on whichour algorithm is shown to capture well the actual developments in these fields.
arxiv-14400-168 | Inducing Generalized Multi-Label Rules with Learning Classifier Systems | http://arxiv.org/pdf/1512.07982v1.pdf | author:Fani A. Tzima, Miltiadis Allamanis, Alexandros Filotheou, Pericles A. Mitkas category:cs.NE cs.LG published:2015-12-25 summary:In recent years, multi-label classification has attracted a significant bodyof research, motivated by real-life applications, such as text classificationand medical diagnoses. Although sparsely studied in this context, LearningClassifier Systems are naturally well-suited to multi-label classificationproblems, whose search space typically involves multiple highly specificniches. This is the motivation behind our current work that introduces ageneralized multi-label rule format -- allowing for flexible label-dependencymodeling, with no need for explicit knowledge of which correlations to searchfor -- and uses it as a guide for further adapting the general Michigan-stylesupervised Learning Classifier System framework. The integration of theaforementioned rule format and framework adaptations results in a novelalgorithm for multi-label classification whose behavior is studied through aset of properly defined artificial problems. The proposed algorithm is alsothoroughly evaluated on a set of multi-label datasets and found competitive toother state-of-the-art multi-label classification methods.
arxiv-14400-169 | Micro-Differential Evolution: Diversity Enhancement and Comparative Study | http://arxiv.org/pdf/1512.07980v1.pdf | author:Hojjat Salehinejad, Shahryar Rahnamayan, Hamid R. Tizhoosh category:cs.NE published:2015-12-25 summary:The differential evolution (DE) algorithm suffers from high computationaltime due to slow nature of evaluation. In contrast, micro-DE (MDE) algorithmsemploy a very small population size, which can converge faster to a reasonablesolution. However, these algorithms are vulnerable to a premature convergenceas well as to high risk of stagnation. In this paper, MDE algorithm withvectorized random mutation factor (MDEVM) is proposed, which utilizes the smallsize population benefit while empowers the exploration ability of mutationfactor through randomizing it in the decision variable level. The idea issupported by analyzing mutation factor using Monte-Carlo based simulations. Tofacilitate the usage of MDE algorithms with very-small population sizes, newmutation schemes for population sizes less than four are also proposed.Furthermore, comprehensive comparative simulations and analysis on performanceof the MDE algorithms over various mutation schemes, population sizes, problemtypes (i.e. uni-modal, multi-modal, and composite), problem dimensionalities,and mutation factor ranges are conducted by considering population diversityanalysis for stagnation and trapping in local optimum situations. The studiesare conducted on 28 benchmark functions provided for the IEEE CEC-2013competition. Experimental results demonstrate high performance and convergencespeed of the proposed MDEVM algorithm.
arxiv-14400-170 | Histogram Meets Topic Model: Density Estimation by Mixture of Histograms | http://arxiv.org/pdf/1512.07960v1.pdf | author:Hideaki Kim, Hiroshi Sawada category:stat.ML published:2015-12-25 summary:The histogram method is a powerful non-parametric approach for estimating theprobability density function of a continuous variable. But the construction ofa histogram, compared to the parametric approaches, demands a large number ofobservations to capture the underlying density function. Thus it is notsuitable for analyzing a sparse data set, a collection of units with a smallsize of data. In this paper, by employing the probabilistic topic model, wedevelop a novel Bayesian approach to alleviating the sparsity problem in theconventional histogram estimation. Our method estimates a unit's densityfunction as a mixture of basis histograms, in which the number of bins for eachbasis, as well as their heights, is determined automatically. The estimationprocedure is performed by using the fast and easy-to-implement collapsed Gibbssampling. We apply the proposed method to synthetic data, showing that itperforms well.
arxiv-14400-171 | Relation Classification via Recurrent Neural Network | http://arxiv.org/pdf/1508.01006v2.pdf | author:Dongxu Zhang, Dong Wang category:cs.CL cs.LG cs.NE published:2015-08-05 summary:Deep learning has gained much success in sentence-level relationclassification. For example, convolutional neural networks (CNN) have deliveredcompetitive performance without much effort on feature engineering as theconventional pattern-based methods. Thus a lot of works have been producedbased on CNN structures. However, a key issue that has not been well addressedby the CNN-based method is the lack of capability to learn temporal features,especially long-distance dependency between nominal pairs. In this paper, wepropose a simple framework based on recurrent neural networks (RNN) and compareit with CNN-based model. To show the limitation of popular used SemEval-2010Task 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,2014). Experiments on two different datasets strongly indicates that theRNN-based model can deliver better performance on relation classification, andit is particularly capable of learning long-distance relation patterns. Thismakes it suitable for real-world applications where complicated expressions areoften involved.
arxiv-14400-172 | A Combined Deep-Learning and Deformable-Model Approach to Fully Automatic Segmentation of the Left Ventricle in Cardiac MRI | http://arxiv.org/pdf/1512.07951v1.pdf | author:M. R. Avendi, A. Kheradvar, H. Jafarkhani category:cs.CV published:2015-12-25 summary:Segmentation of the left ventricle (LV) from cardiac magnetic resonanceimaging (MRI) datasets is an essential step for calculation of clinical indicessuch as ventricular volume and ejection fraction. In this work, we employ deeplearning algorithms combined with deformable models to develop and evaluate afully automatic segmentation tool for the LV from short-axis cardiac MRIdatasets. The method employs deep learning algorithms to learn the segmentationtask from the ground true data. Convolutional networks are employed toautomatically detect the LV chamber in MRI dataset. Stacked autoencoders areutilized to infer the shape of the LV. The inferred shape is incorporated intodeformable models to improve the accuracy and robustness of the segmentation.We validated our method using 45 cardiac MR datasets taken from the MICCAI 2009LV segmentation challenge and showed that it outperforms the state-of-the artmethods. Excellent agreement with the ground truth was achieved. Validationmetrics, percentage of good contours, Dice metric, average perpendiculardistance and conformity, were computed as 96.69%, 0.94, 1.81mm and 0.86, versusthose of 79.2%-95.62%, 0.87-0.9, 1.76-2.97mm and 0.67-0.78, obtained by othermethods, respectively.
arxiv-14400-173 | Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain Stochastically Fully Connected Conditional Random Fields | http://arxiv.org/pdf/1512.07947v1.pdf | author:Edward Li, Farzad Khalvati, Mohammad Javad Shafiee, Masoom A. Haider, Alexander Wong category:cs.CV physics.med-ph stat.ME published:2015-12-25 summary:Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology forthe screening and diagnosis of frequently occurring cancers. However imagequality may suffer by long acquisition times for MRIs due to patient motion, aswell as result in great patient discomfort. Reducing MRI acquisition time canreduce patient discomfort and as a result reduces motion artifacts from theacquisition process. Compressive sensing strategies, when applied to MRI, havebeen demonstrated to be effective at decreasing acquisition times significantlyby sparsely sampling the \emph{k}-space during the acquisition process.However, such a strategy requires advanced reconstruction algorithms to producehigh quality and reliable images from compressive sensing MRI. This paperproposes a new reconstruction approach based on cross-domain stochasticallyfully connected conditional random fields (CD-SFCRF) for compressive sensingMRI. The CD-SFCRF introduces constraints in both \emph{k}-space and spatialdomains within a stochastically fully connected graphical model to produceimproved MRI reconstruction. Experimental results using T2-weighted (T2w)imaging and diffusion-weighted imaging (DWI) of the prostate show strongperformance in preserving fine details and tissue structures in thereconstructed images when compared to other tested methods even at low samplingrates.
arxiv-14400-174 | Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization | http://arxiv.org/pdf/1512.01110v2.pdf | author:Yang Song, Jun Zhu category:cs.NA cs.AI cs.LG published:2015-12-03 summary:Bayesian matrix completion has been studied based on a low-rank matrixfactorization formulation with promising results. However, little work has beendone on Bayesian matrix completion based on the more direct spectralregularization formulation. We fill this gap by presenting a novel Bayesianmatrix completion method based on spectral regularization. In order tocircumvent the difficulties of dealing with the orthonormality constraints ofsingular vectors, we derive a new equivalent form with relaxed constraints,which then leads us to design an adaptive version of spectral regularizationfeasible for Bayesian inference. Our Bayesian method requires no parametertuning and can infer the number of latent factors automatically. Experiments onsynthetic and real datasets demonstrate encouraging results on rank recoveryand collaborative filtering, with notably good results for very sparsematrices.
arxiv-14400-175 | Multi-Level Cause-Effect Systems | http://arxiv.org/pdf/1512.07942v1.pdf | author:Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI published:2015-12-25 summary:We present a domain-general account of causation that applies to settings inwhich macro-level causal relations between two systems are of interest, but therelevant causal features are poorly understood and have to be aggregated fromvast arrays of micro-measurements. Our approach generalizes that of Chalupka etal. (2015) to the setting in which the macro-level effect is not specified. Weformalize the connection between micro- and macro-variables in such situationsand provide a coherent framework describing causal relations at multiple levelsof analysis. We present an algorithm that discovers macro-variable causes andeffects from micro-level measurements obtained from an experiment. We furthershow how to design experiments to discover macro-variables from observationalmicro-variable data. Finally, we show that under specific conditions, one canidentify multiple levels of causal structure. Throughout the article, we use asimulated neuroscience multi-unit recording experiment to illustrate the ideasand the algorithms.
arxiv-14400-176 | Sufficient Forecasting Using Factor Models | http://arxiv.org/pdf/1505.07414v2.pdf | author:Jianqing Fan, Lingzhou Xue, Jiawei Yao category:math.ST stat.ME stat.ML stat.TH published:2015-05-27 summary:We consider forecasting a single time series when there is a large number ofpredictors and a possible nonlinear effect. The dimensionality was firstreduced via a high-dimensional (approximate) factor model implemented by theprincipal component analysis. Using the extracted factors, we develop a novelforecasting method called the sufficient forecasting, which provides a set ofsufficient predictive indices, inferred from high-dimensional predictors, todeliver additional predictive power. The projected principal component analysiswill be employed to enhance the accuracy of inferred factors when asemi-parametric (approximate) factor model is assumed. Our method is alsoapplicable to cross-sectional sufficient regression using extracted factors.The connection between the sufficient forecasting and the deep learningarchitecture is explicitly stated. The sufficient forecasting correctlyestimates projection indices of the underlying factors even in the presence ofa nonparametric forecasting function. The proposed method extends thesufficient dimension reduction to high-dimensional regimes by condensing thecross-sectional information through factor models. We derive asymptoticproperties for the estimate of the central subspace spanned by these projectiondirections as well as the estimates of the sufficient predictive indices. Wefurther show that the natural method of running multiple regression of targeton estimated factors yields a linear estimate that actually falls into thiscentral subspace. Our method and theory allow the number of predictors to belarger than the number of observations. We finally demonstrate that thesufficient forecasting improves upon the linear forecasting in both simulationstudies and an empirical study of forecasting macroeconomic variables.
arxiv-14400-177 | Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network | http://arxiv.org/pdf/1512.07928v1.pdf | author:Seunghoon Hong, Junhyuk Oh, Bohyung Han, Honglak Lee category:cs.CV published:2015-12-24 summary:We propose a novel weakly-supervised semantic segmentation algorithm based onDeep Convolutional Neural Network (DCNN). Contrary to existingweakly-supervised approaches, our algorithm exploits auxiliary segmentationannotations available for different categories to guide segmentations on imageswith only image-level class labels. To make the segmentation knowledgetransferrable across categories, we design a decoupled encoder-decoderarchitecture with attention model. In this architecture, the model generatesspatial highlights of each category presented in an image using an attentionmodel, and subsequently generates foreground segmentation for each highlightedregion using decoder. Combining attention model, we show that the decodertrained with segmentation annotations in different categories can boost theperformance of weakly-supervised semantic segmentation. The proposed algorithmdemonstrates substantially improved performance compared to thestate-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012dataset when our model is trained with the annotations in 60 exclusivecategories in Microsoft COCO dataset.
arxiv-14400-178 | Feature Elimination in Kernel Machines in moderately high dimensions | http://arxiv.org/pdf/1304.5245v2.pdf | author:Sayan Dasgupta, Yair Goldberg, Michael Kosorok category:stat.ML 68T05, 62G08 published:2013-04-18 summary:We develop an approach for feature elimination in statistical learning withkernel machines, based on recursive elimination of features.We presenttheoretical properties of this method and show that it is uniformly consistentin finding the correct feature space under certain generalized assumptions.Wepresent four case studies to show that the assumptions are met in mostpractical situations and present simulation results to demonstrate performanceof the proposed approach.
arxiv-14400-179 | Truncated Max-of-Convex Models | http://arxiv.org/pdf/1512.07815v1.pdf | author:Pankaj Pansari, M. Pawan Kumar category:cs.CV published:2015-12-24 summary:Truncated convex models (TCM) are special cases of pairwise random fieldsthat have been widely used in computer vision. However, by restricting theorder of the potentials to be at most two, they fail to capture useful imagestatistics. We propose a natural generalization of TCM to high-order randomfields, which we call truncated max-of-convex models (TMCM). The energyfunction of TMCM consists of two types of potentials: (i) unary potentials,which have no restriction on their form; and (ii) high-order potentials, whichare the sum of the truncation of the m largest convex distances over disjointpairs of random variables in an arbitrary size clique. The use of a convexdistance function encourages smoothness, while truncation allows fordiscontinuities in the labeling. By using m > 1, TMCM provides robustnesstowards errors in the clique definition. In order to minimize the energyfunction of a TMCM over all possible labelings, we design an efficientst-mincut based range expansion algorithm. We prove the accuracy of ouralgorithm by establishing strong multiplicative bounds for several specialcases of interest.
arxiv-14400-180 | The Lovász Hinge: A Convex Surrogate for Submodular Losses | http://arxiv.org/pdf/1512.07797v1.pdf | author:Jiaqian Yu, Matthew Blaschko category:stat.ML cs.LG published:2015-12-24 summary:Learning with non-modular losses is an important problem when sets ofpredictions are made simultaneously. The main tools for constructing convexsurrogate loss functions for set prediction are margin rescaling and slackrescaling. In this work, we show that these strategies lead to tight convexsurrogates iff the underlying loss function is increasing in the number ofincorrect predictions. However, gradient or cutting-plane computation for thesefunctions is NP-hard for non-supermodular loss functions. We propose instead anovel surrogate loss function for submodular losses, the Lov{\'a}sz hinge,which leads to O(p log p) complexity with O(p) oracle accesses to the lossfunction to compute a gradient or cutting-plane. We prove that the Lov{\'a}szhinge is convex and yields an extension. As a result, we have developed thefirst tractable convex surrogates in the literature for submodular losses. Wedemonstrate the utility of this novel convex surrogate through several setprediction tasks, including on the PASCAL VOC and Microsoft COCO datasets.
arxiv-14400-181 | Distinguishing cause from effect using observational data: methods and benchmarks | http://arxiv.org/pdf/1412.3773v3.pdf | author:Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, Bernhard Schölkopf category:cs.LG cs.AI stat.ML stat.OT published:2014-12-11 summary:The discovery of causal relationships from purely observational data is afundamental problem in science. The most elementary form of such a causaldiscovery problem is to decide whether X causes Y or, alternatively, Y causesX, given joint observations of two variables X, Y. An example is to decidewhether altitude causes temperature, or vice versa, given only jointmeasurements of both variables. Even under the simplifying assumptions of noconfounding, no feedback loops, and no selection bias, such bivariate causaldiscovery problems are challenging. Nevertheless, several approaches foraddressing those problems have been proposed in recent years. We review twofamilies of such methods: Additive Noise Methods (ANM) and InformationGeometric Causal Inference (IGCI). We present the benchmark CauseEffectPairsthat consists of data for 100 different cause-effect pairs selected from 37datasets from various domains (e.g., meteorology, biology, medicine,engineering, economy, etc.) and motivate our decisions regarding the "groundtruth" causal directions of all pairs. We evaluate the performance of severalbivariate causal discovery methods on these real-world benchmark data and inaddition on artificially simulated data. Our empirical results on real-worlddata indicate that certain methods are indeed able to distinguish cause fromeffect using only purely observational data, although more benchmark data wouldbe needed to obtain statistically significant conclusions. One of the bestperforming methods overall is the additive-noise method originally proposed byHoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of0.74+-0.05 on the real-world benchmark. As the main theoretical contribution ofthis work we prove the consistency of that method.
arxiv-14400-182 | Hardware Architecture for Large Parallel Array of Random Feature Extractors applied to Image Recognition | http://arxiv.org/pdf/1512.07783v1.pdf | author:Aakash Patil, Shanlan Shen, Enyi Yao, Arindam Basu category:cs.ET cs.NE published:2015-12-24 summary:We demonstrate a low-power and compact hardware implementation of RandomFeature Extractor (RFE) core. With complex tasks like Image Recognitionrequiring a large set of features, we show how weight reuse technique can allowto virtually expand the random features available from RFE core. Further, weshow how to avoid computation cost wasted for propagating "incognizant" orredundant random features. For proof of concept, we validated our approach byusing our RFE core as the first stage of Extreme Learning Machine (ELM)--a twolayer neural network--and were able to achieve $>97\%$ accuracy on MNISTdatabase of handwritten digits. ELM's first stage of RFE is done on an analogASIC occupying $5$mm$\times5$mm area in $0.35\mu$m CMOS and consuming $5.95$$\mu$J/classify while using $\approx 5000$ effective hidden neurons. The ELMsecond stage consisting of just adders can be implemented as digital circuitwith estimated power consumption of $20.9$ nJ/classify. With a total energyconsumption of only $5.97$ $\mu$J/classify, this low-power mixed signal ASICcan act as a co-processor in portable electronic gadgets with cameras.
arxiv-14400-183 | Low-Rank Approximation of Weighted Tree Automata | http://arxiv.org/pdf/1511.01442v2.pdf | author:Guillaume Rabusseau, Borja Balle, Shay B. Cohen category:cs.LG cs.FL published:2015-11-04 summary:We describe a technique to minimize weighted tree automata (WTA), a powerfulformalisms that subsumes probabilistic context-free grammars (PCFGs) andlatent-variable PCFGs. Our method relies on a singular value decomposition ofthe underlying Hankel matrix defined by the WTA. Our main theoretical result isan efficient algorithm for computing the SVD of an infinite Hankel matriximplicitly represented as a WTA. We provide an analysis of the approximationerror induced by the minimization, and we evaluate our method on real-worlddata originating in newswire treebank. We show that the model achieves lowerperplexity than previous methods for PCFG minimization, and also is much morestable due to the absence of local optima.
arxiv-14400-184 | Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips | http://arxiv.org/pdf/1512.07748v1.pdf | author:Tomohiko Nakamura, Eita Nakamura, Shigeki Sagayama category:cs.SD cs.LG cs.MM published:2015-12-24 summary:This paper discusses real-time alignment of audio signals of musicperformance to the corresponding score (a.k.a. score following) which canhandle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)in performances. This type of score following is particularly useful inautomatic accompaniment for practices and rehearsals, where errors andrepeats/skips are often made. Simple extensions of the algorithms previouslyproposed in the literature are not applicable in these situations for scores ofpractical length due to the problem of large computational complexity. To copewith this problem, we present two hidden Markov models of monophonicperformance with errors and arbitrary repeats/skips, and derive efficientscore-following algorithms with an assumption that the prior probabilitydistributions of score positions before and after repeats/skips are independentfrom each other. We confirmed real-time operation of the algorithms with musicscores of practical length (around 10000 notes) on a modern laptop and theirtracking ability to the input performance within 0.7 s on average afterrepeats/skips in clarinet performance data. Further improvements and extensionfor polyphonic signals are also discussed.
arxiv-14400-185 | Differential Evolution with Event-Triggered Impulsive Control | http://arxiv.org/pdf/1512.05449v2.pdf | author:Wei Du, Sunney Yung Sun Leung, Yang Tang, Athanasios V. Vasilakos category:cs.NE cs.SY math.OC published:2015-12-17 summary:Differential evolution (DE) is a simple but powerful evolutionary algorithm,which has been widely and successfully used in various areas. In this paper, anevent-triggered impulsive control scheme (ETI) is introduced to improve theperformance of DE. Impulsive control, the concept of which derives from controltheory, aims at regulating the states of a network by instantly adjusting thestates of a fraction of nodes at certain instants, and these instants aredetermined by event-triggered mechanism (ETM). By introducing impulsive controland ETM into DE, we hope to change the search performance of the population ina positive way after revising the positions of some individuals at certainmoments. At the end of each generation, the impulsive control operation istriggered when the update rate of the population declines or equals to zero. Indetail, inspired by the concepts of impulsive control, two types of impulsesare presented within the framework of DE in this paper: stabilizing impulsesand destabilizing impulses. Stabilizing impulses help the individuals withlower rankings instantly move to a desired state determined by the individualswith better fitness values. Destabilizing impulses randomly alter the positionsof inferior individuals within the range of the current population. By means ofintelligently modifying the positions of a part of individuals with these twokinds of impulses, both exploitation and exploration abilities of the wholepopulation can be meliorated. In addition, the proposed ETI is flexible to beincorporated into several state-of-the-art DE variants. Experimental resultsover the CEC 2014 benchmark functions exhibit that the developed scheme issimple yet effective, which significantly improves the performance of theconsidered DE algorithms.
arxiv-14400-186 | Fast Parallel SVM using Data Augmentation | http://arxiv.org/pdf/1512.07716v1.pdf | author:Hugh Perkins, Minjie Xu, Jun Zhu, Bo Zhang category:cs.LG published:2015-12-24 summary:As one of the most popular classifiers, linear SVMs still have challenges indealing with very large-scale problems, even though linear or sub-linearalgorithms have been developed recently on single machines. Parallel computingmethods have been developed for learning large-scale SVMs. However, existingmethods rely on solving local sub-optimization problems. In this paper, wedevelop a novel parallel algorithm for learning large-scale linear SVM. Ourapproach is based on a data augmentation equivalent formulation, which caststhe problem of learning SVM as a Bayesian inference problem, for which we candevelop very efficient parallel sampling methods. We provide empirical resultsfor this parallel sampling SVM, and provide extensions for SVR, non-linearkernels, and provide a parallel implementation of the Crammer and Singer model.This approach is very promising in its own right, and further is a very usefultechnique to parallelize a broader family of general maximum-margin models.
arxiv-14400-187 | Harnessing the Deep Net Object Models for Enhancing Human Action Recognition | http://arxiv.org/pdf/1512.06498v2.pdf | author:O. V. Ramana Murthy, Roland Goecke category:cs.CV published:2015-12-21 summary:In this study, the influence of objects is investigated in the scenario ofhuman action recognition with large number of classes. We hypothesize that theobjects the humans are interacting will have good say in determining the actionbeing performed. Especially, if the objects are non-moving, such as objectsappearing in the background, features such as spatio-temporal interest points,dense trajectories may fail to detect them. Hence we propose to detect objectsusing pre-trained object detectors in every frame statically. Trained Deepnetwork models are used as object detectors. Information from different layersin conjunction with different encoding techniques is extensively studied toobtain the richest feature vectors. This technique is observed to yieldstate-of-the-art performance on HMDB51 and UCF101 datasets.
arxiv-14400-188 | Fast Acquisition for Quantitative MRI Maps: Sparse Recovery from Non-linear Measurements | http://arxiv.org/pdf/1512.07712v1.pdf | author:Anupriya Gogna, Angshul Majumdar category:cs.CV published:2015-12-24 summary:This work addresses the problem of estimating proton density and T1 maps fromtwo partially sampled K-space scans such that the total acquisition timeremains approximately the same as a single scan. Existing multi parametric nonlinear curve fitting techniques require a large number (8 or more) of echoes toestimate the maps resulting in prolonged (clinically infeasible) acquisitiontimes. Our simulation results show that our method yields very accurate androbust results from only two partially sampled scans (total scan time being thesame as a single echo MRI). We model PD and T1 maps to be sparse in sometransform domain. The PD map is recovered via standard Compressed Sensing basedrecovery technique. Estimating the T1 map requires solving an analysis priorsparse recovery problem from non linear measurements, since the relationshipbetween T1 values and intensity values or K space samples is not linear. Forthe first time in this work, we propose an algorithm for analysis prior sparserecovery for non linear measurements. We have compared our approach with theonly existing technique based on matrix factorization from non linearmeasurements; our method yields considerably superior results.
arxiv-14400-189 | Service Choreography, SBVR, and Time | http://arxiv.org/pdf/1512.07685v1.pdf | author:Nurulhuda A. Manaf, Sotiris Moschoyiannis, Paul Krause category:cs.SE cs.CL published:2015-12-24 summary:We propose the use of structured natural language (English) in specifyingservice choreographies, focusing on the what rather than the how of therequired coordination of participant services in realising a businessapplication scenario. The declarative approach we propose uses the OMG standardSemantics of Business Vocabulary and Rules (SBVR) as a modelling language. Theservice choreography approach has been proposed for describing the globalorderings of the invocations on interfaces of participant services. Wetherefore extend SBVR with a notion of time which can capture the coordinationof the participant services, in terms of the observable message exchangesbetween them. The extension is done using existing modelling constructs inSBVR, and hence respects the standard specification. The idea is that users -domain specialists rather than implementation specialists - can verify therequested service composition by directly reading the structured English usedby SBVR. At the same time, the SBVR model can be represented in formal logic soit can be parsed and executed by a machine.
arxiv-14400-190 | Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks | http://arxiv.org/pdf/1512.07666v1.pdf | author:Chunyuan Li, Changyou Chen, David Carlson, Lawrence Carin category:stat.ML published:2015-12-23 summary:Effective training of deep neural networks suffers from two main issues. Thefirst is that the parameter spaces of these models exhibit pathologicalcurvature. Recent methods address this problem by using adaptivepreconditioning for Stochastic Gradient Descent (SGD). These methods improveconvergence by adapting to the local geometry of parameter space. A secondissue is overfitting, which is typically addressed by early stopping. However,recent work has demonstrated that Bayesian model averaging mitigates thisproblem. The posterior can be sampled by using Stochastic Gradient LangevinDynamics (SGLD). However, the rapidly changing curvature renders default SGLDmethods inefficient. Here, we propose combining adaptive preconditioners withSGLD. In support of this idea, we give theoretical properties on asymptoticconvergence and predictive risk. We also provide empirical results for LogisticRegression, Feedforward Neural Nets, and Convolutional Neural Nets,demonstrating that our preconditioned SGLD method gives state-of-the-artperformance on these models.
arxiv-14400-191 | High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep Models | http://arxiv.org/pdf/1512.07662v1.pdf | author:Chunyuan Li, Changyou Chen, Kai Fan, Lawrence Carin category:stat.ML published:2015-12-23 summary:Learning in deep models using Bayesian methods has generated significantattention recently. This is largely because of the feasibility of modernBayesian methods to yield scalable learning and inference, while maintaining ameasure of uncertainty in the model parameters. Stochastic gradient MCMCalgorithms (SG-MCMC) are a family of diffusion-based sampling methods forlarge-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradientthermostats (mSGNHT) augment each parameter of interest, with a momentum and athermostat variable to maintain stationary distributions as target posteriordistributions. As the number of variables in a continuous-time diffusionincreases, its numerical approximation error becomes a practical bottleneck, sobetter use of a numerical integrator is desirable. To this end, we propose useof an efficient symmetric splitting integrator in mSGNHT, instead of thetraditional Euler integrator. We demonstrate that the proposed scheme is moreaccurate, robust, and converges faster. These properties are demonstrated to bedesirable in Bayesian deep learning. Extensive experiments on two canonicalmodels and their deep extensions demonstrate that the proposed scheme improvesgeneral Bayesian posterior sampling, particularly for deep models.
arxiv-14400-192 | The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms | http://arxiv.org/pdf/1512.07650v1.pdf | author:Yahel David, Nahum Shimkin category:stat.ML cs.AI cs.LG published:2015-12-23 summary:We consider the Max $K$-Armed Bandit problem, where a learning agent is facedwith several stochastic arms, each a source of i.i.d. rewards of unknowndistribution. At each time step the agent chooses an arm, and observes thereward of the obtained sample. Each sample is considered here as a separateitem with the reward designating its value, and the goal is to find an itemwith the highest possible value. Our basic assumption is a known lower bound onthe {\em tail function} of the reward distributions. Under the PAC framework,we provide a lower bound on the sample complexity of any$(\epsilon,\delta)$-correct algorithm, and propose an algorithm that attainsthis bound up to logarithmic factors. We analyze the robustness of the proposedalgorithm and in addition, we compare the performance of this algorithm to thevariant in which the arms are not distinguishable by the agent and are chosenrandomly at each stage. Interestingly, when the maximal rewards of the armshappen to be similar, the latter approach may provide better performance.
arxiv-14400-193 | Satisficing in multi-armed bandit problems | http://arxiv.org/pdf/1512.07638v1.pdf | author:Paul Reverdy, Vaibhav Srivastava, Naomi Ehrich Leonard category:cs.LG math.OC stat.ML published:2015-12-23 summary:Satisficing is a relaxation of maximizing and allows for less riskydecision-making in the face of uncertainty. We propose two sets of satisficingobjectives for the multi-armed bandit problem, where the objective is toachieve reward-based decision-making performance above a given threshold. Weshow that these new problems are equivalent to various standard multi-armedbandit problems with maximizing objectives and use the equivalence to findbounds on performance. The different objectives can result in qualitativelydifferent behavior; for example, agents explore their options continually inone case and only a finite number of times in another. For the case of Gaussianrewards we show an additional equivalence between the two sets of satisficingobjectives that allows algorithms developed for one set to be applied to theother. We then develop variants of the Upper Credible Limit (UCL) algorithmthat solve the problems with satisficing objectives and show that thesemodified UCL algorithms achieve efficient satisficing performance.
arxiv-14400-194 | Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes | http://arxiv.org/pdf/1506.04088v2.pdf | author:Ryan Giordano, Tamara Broderick, Michael Jordan category:stat.ML published:2015-06-12 summary:Mean field variational Bayes (MFVB) is a popular posterior approximationmethod due to its fast runtime on large-scale data sets. However, it is wellknown that a major failing of MFVB is that it underestimates the uncertainty ofmodel variables (sometimes severely) and provides no information about modelvariable covariance. We generalize linear response methods from statistical physics to deliveraccurate uncertainty estimates for model variables---both for individualvariables and coherently across variables. We call our method linear responsevariational Bayes (LRVB). When the MFVB posterior approximation is in theexponential family, LRVB has a simple, analytic form, even for non-conjugatemodels. Indeed, we make no assumptions about the form of the true posterior. Wedemonstrate the accuracy and scalability of our method on a range of models forboth simulated and real data.
arxiv-14400-195 | k-Means Clustering Is Matrix Factorization | http://arxiv.org/pdf/1512.07548v1.pdf | author:Christian Bauckhage category:stat.ML published:2015-12-23 summary:We show that the objective function of conventional k-means clustering can beexpressed as the Frobenius norm of the difference of a data matrix and a lowrank approximation of that data matrix. In short, we show that k-meansclustering is a matrix factorization problem. These notes are meant as areference and intended to provide a guided tour towards a result that is oftenmentioned but seldom made explicit in the literature.
arxiv-14400-196 | Connectivity Preserving Multivalued Functions in Digital Topology | http://arxiv.org/pdf/1504.02174v2.pdf | author:Laurence Boxer, P. Christopher Staecker category:cs.CV math.GN I.4.m published:2015-04-09 summary:We study connectivity preserving multivalued functions between digitalimages. This notion generalizes that of continuous multivalued functionsstudied mostly in the setting of the digital plane $Z^2$. We show thatconnectivity preserving multivalued functions, like continuous multivaluedfunctions, are appropriate models for digital morpholological operations.Connectivity preservation, unlike continuity, is preserved by compositions, andgeneralizes easily to higher dimensions and arbitrary adjacency relations.
arxiv-14400-197 | Putting Things in Context: Community-specific Embedding Projections for Sentiment Analysis | http://arxiv.org/pdf/1511.06052v2.pdf | author:Yi Yang, Jacob Eisenstein category:cs.CL cs.AI cs.SI published:2015-11-19 summary:Variation in language is ubiquitous, and is particularly evident in newerforms of writing such as social media. Fortunately, variation is not random,but is usually linked to social factors. By exploiting linguistic homophily ---the tendency of socially linked individuals to use language similarly --- it ispossible to build models that are more robust to variation. In this paper, wefocus on social network communities, which make it possible to generalizesociolinguistic properties from authors in the training set to authors in thetest sets, without requiring demographic author metadata. We detect communitiesvia standard graph clustering algorithms, and then exploit these communities bylearning community-specific projections of word embeddings. These projectionscapture shifts in word meaning in different social groups; by modeling them, weare able to improve the overall accuracy of Twitter sentiment analysis by asignificant margin over competitive prior work.
arxiv-14400-198 | Convolutional Architecture Exploration for Action Recognition and Image Classification | http://arxiv.org/pdf/1512.07502v1.pdf | author:J. T. Turner, David Aha, Leslie Smith, Kalyan Moy Gupta category:cs.CV published:2015-12-23 summary:Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is asoftware package for the training, classifying, and feature extraction ofimages. The UCF Sports Action dataset is a widely used machine learning datasetthat has 200 videos taken in 720x480 resolution of 9 different sportingactivities: diving, golf, swinging, kicking, lifting, horseback riding,running, skateboarding, swinging (various gymnastics), and walking. In thisreport we report on a caffe feature extraction pipeline of images taken fromthe videos of the UCF Sports Action dataset. A similar test was performed onoverfeat, and results were inferior to caffe. This study is intended to explorethe architecture and hyper parameters needed for effective static analysis ofaction in videos and classification over a variety of image datasets.
arxiv-14400-199 | Adaptive Ensemble Learning with Confidence Bounds | http://arxiv.org/pdf/1512.07446v1.pdf | author:Cem Tekin, Jinsung Yoon. Mihael category:cs.LG stat.ML published:2015-12-23 summary:Extracting actionable intelligence from distributed, heterogeneous,correlated and high-dimensional data sources requires run-time processing andlearning both locally and globally. In the last decade, a large number ofmeta-learning techniques have been proposed in which local learners make onlinepredictions based on their locally-collected data instances, and feed thesepredictions to an ensemble learner, which fuses them and issues a globalprediction. However, most of these works do not provide performance guaranteesor, when they do, these guarantees are asymptotic. None of these existing worksprovide confidence estimates about the issued predictions or rate of learningguarantees for the ensemble learner. In this paper, we provide a systematicensemble learning method called Hedged Bandits, which comes with both long run(asymptotic) and short run (rate of learning) performance guarantees. Moreover,we show that our proposed method outperforms all existing ensemble learningtechniques, even in the presence of concept drift. We illustrate theperformance of Hedged Bandits in the context of medical informatics. However,the proposed methods have numerous other applications, including networkmonitoring and security, online recommendation systems, social networks, smartcities, etc.
arxiv-14400-200 | Adaptive Algorithms for Online Convex Optimization with Long-term Constraints | http://arxiv.org/pdf/1512.07422v1.pdf | author:Rodolphe Jenatton, Jim Huang, Cédric Archambeau category:stat.ML cs.LG math.OC published:2015-12-23 summary:We present an adaptive online gradient descent algorithm to solve onlineconvex optimization problems with long-term constraints , which are constraintsthat need to be satisfied when accumulated over a finite number of rounds T ,but can be violated in intermediate rounds. For some user-defined trade-offparameter $\beta$ $\in$ (0, 1), the proposed algorithm achieves cumulativeregret bounds of O(T^max{$\beta$,1--$\beta$}) and O(T^(1--$\beta$/2)) for theloss and the constraint violations respectively. Our results hold for convexlosses and can handle arbitrary convex constraints without requiring knowledgeof the number of rounds in advance. Our contributions improve over the bestknown cumulative regret bounds by Mahdavi, et al. (2012) that are respectivelyO(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) andO(T^2/3) when further restricting to polyhedral domains. We supplement theanalysis with experiments validating the performance of our algorithm inpractice.
arxiv-14400-201 | Convex Factorization Machine for Regression | http://arxiv.org/pdf/1507.01073v3.pdf | author:Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Suleiman A Khan, Samuel Kaski, Hiroshi Mamitsuka, Yi Chang category:stat.ML cs.LG published:2015-07-04 summary:We propose the convex factorization machine (CFM), which is a convex variantof the widely used Factorization Machines (FMs). Specifically, we employ alinear+quadratic model and regularize the linear term with the$\ell_2$-regularizer and the quadratic term with the trace norm regularizer.Then, we formulate the CFM optimization as a semidefinite programming problemand propose an efficient optimization procedure with Hazan's algorithm. A keyadvantage of CFM over existing FMs is that it can find a globally optimalsolution, while FMs may get a poor locally optimal solution since the objectivefunction of FMs is non-convex. In addition, the proposed algorithm is simpleyet effective and can be implemented easily. Finally, CFM is a generalfactorization method and can also be used for other factorization problemsincluding multi-view matrix factorization problems. Through synthetic andmovielens datasets, we first show that the proposed CFM achieves resultscompetitive to FMs. Furthermore, in a toxicogenomics prediction task, we showthat CFM outperforms a state-of-the-art tensor factorization method.
arxiv-14400-202 | A Deep Generative Deconvolutional Image Model | http://arxiv.org/pdf/1512.07344v1.pdf | author:Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin category:cs.CV cs.LG stat.ML published:2015-12-23 summary:A deep generative model is developed for representation and analysis ofimages, based on a hierarchical convolutional dictionary-learning framework.Stochastic {\em unpooling} is employed to link consecutive layers in the model,yielding top-down image generation. A Bayesian support vector machine is linkedto the top-layer features, yielding max-margin discrimination. Deepdeconvolutional inference is employed when testing, to infer the latentfeatures, and the top-layer features are connected with the max-marginclassifier for discrimination tasks. The model is efficiently trained using aMonte Carlo expectation-maximization (MCEM) algorithm, with implementation ongraphical processor units (GPUs) for efficient large-scale learning, and fasttesting. Excellent results are obtained on several benchmark datasets,including ImageNet, demonstrating that the proposed model achieves results thatare highly competitive with similarly sized convolutional neural networks.
arxiv-14400-203 | Latent Variable Modeling with Diversity-Inducing Mutual Angular Regularization | http://arxiv.org/pdf/1512.07336v1.pdf | author:Pengtao Xie, Yuntian Deng, Eric Xing category:cs.LG stat.ML published:2015-12-23 summary:Latent Variable Models (LVMs) are a large family of machine learning modelsproviding a principled and effective way to extract underlying patterns,structure and knowledge from observed data. Due to the dramatic growth ofvolume and complexity of data, several new challenges have emerged and cannotbe effectively addressed by existing LVMs: (1) How to capture long-tailpatterns that carry crucial information when the popularity of patterns isdistributed in a power-law fashion? (2) How to reduce model complexity andcomputational cost without compromising the modeling power of LVMs? (3) How toimprove the interpretability and reduce the redundancy of discovered patterns?To addresses the three challenges discussed above, we develop a novelregularization technique for LVMs, which controls the geometry of the latentspace during learning to enable the learned latent components of LVMs to bediverse in the sense that they are favored to be mutually different from eachother, to accomplish long-tail coverage, low redundancy, and betterinterpretability. We propose a mutual angular regularizer (MAR) to encouragethe components in LVMs to have larger mutual angles. The MAR is non-convex andnon-smooth, entailing great challenges for optimization. To cope with thisissue, we derive a smooth lower bound of the MAR and optimize the lower boundinstead. We show that the monotonicity of the lower bound is closely alignedwith the MAR to qualify the lower bound as a desirable surrogate of the MAR.Using neural network (NN) as an instance, we analyze how the MAR affects thegeneralization performance of NN. On two popular latent variable models ---restricted Boltzmann machine and distance metric learning, we demonstrate thatMAR can effectively capture long-tail patterns, reduce model complexity withoutsacrificing expressivity and improve interpretability.
arxiv-14400-204 | Marginal likelihood and model selection for Gaussian latent tree and forest models | http://arxiv.org/pdf/1412.8285v2.pdf | author:Mathias Drton, Shaowei Lin, Luca Weihs, Piotr Zwiernik category:stat.ME math.ST stat.ML stat.TH published:2014-12-29 summary:Gaussian latent tree models, or more generally, Gaussian latent forest modelshave Fisher-information matrices that become singular along interestingsubmodels, namely, models that correspond to subforests. For thesesingularities, we compute the real log-canonical thresholds (also known asstochastic complexities or learning coefficients) that quantify thelarge-sample behavior of the marginal likelihood in Bayesian inference. Thisprovides the information needed for a recently introduced generalization of theBayesian information criterion. Our mathematical developments treat the generalsetting of Laplace integrals whose phase functions are sums of squareddifferences between monomials and constants. We clarify how in this case reallog-canonical thresholds can be computed using polyhedral geometry, and we showhow to apply the general theory to the Laplace integrals associated withGaussian latent tree and forest models. In simulations and a data example, wedemonstrate how the mathematical knowledge can be applied in model selection.
arxiv-14400-205 | Plug-and-Play Priors for Bright Field Electron Tomography and Sparse Interpolation | http://arxiv.org/pdf/1512.07331v1.pdf | author:Suhas Sreehari, S. V. Venkatakrishnan, Brendt Wohlberg, Lawrence F. Drummy, Jeffrey P. Simmons, Charles A. Bouman category:cs.CV published:2015-12-23 summary:Many material and biological samples in scientific imaging are characterizedby non-local repeating structures. These are studied using scanning electronmicroscopy and electron tomography. Sparse sampling of individual pixels in a2D image acquisition geometry, or sparse sampling of projection images withlarge tilt increments in a tomography experiment, can enable high speed dataacquisition and minimize sample damage caused by the electron beam. In this paper, we present an algorithm for electron tomographicreconstruction and sparse image interpolation that exploits the non-localredundancy in images. We adapt a framework, termed plug-and-play (P&P) priors,to solve these imaging problems in a regularized inversion setting. The powerof the P&P approach is that it allows a wide array of modern denoisingalgorithms to be used as a "prior model" for tomography and imageinterpolation. We also present sufficient mathematical conditions that ensureconvergence of the P&P approach, and we use these insights to design a newnon-local means denoising algorithm. Finally, we demonstrate that the algorithmproduces higher quality reconstructions on both simulated and real electronmicroscope data, along with improved convergence properties compared to othermethods.
arxiv-14400-206 | Mid-level Representation for Visual Recognition | http://arxiv.org/pdf/1512.07314v1.pdf | author:Moin Nabi category:cs.CV published:2015-12-23 summary:Visual Recognition is one of the fundamental challenges in AI, where the goalis to understand the semantics of visual data. Employing mid-levelrepresentation, in particular, shifted the paradigm in visual recognition. Themid-level image/video representation involves discovering and training a set ofmid-level visual patterns (e.g., parts and attributes) and represent a givenimage/video utilizing them. The mid-level patterns can be extracted from imagesand videos using the motion and appearance information of visual phenomenas.This thesis targets employing mid-level representations for differenthigh-level visual recognition tasks, namely (i)image understanding and(ii)video understanding. In the case of image understanding, we focus on object detection/recognitiontask. We investigate on discovering and learning a set of mid-level patches tobe used for representing the images of an object category. We specificallyemploy the discriminative patches in a subcategory-aware webly-supervisedfashion. We, additionally, study the outcomes provided by employing thesubcategory-based models for undoing dataset bias.
arxiv-14400-207 | Topical differences between Chinese language Twitter and Sina Weibo | http://arxiv.org/pdf/1512.07281v1.pdf | author:Qian Zhang, Bruno Gonçalves category:cs.SI cs.CL physics.soc-ph published:2015-12-22 summary:Sina Weibo, China's most popular microblogging platform, is currently used byover $500M$ users and is considered to be a proxy of Chinese social life. Inthis study, we contrast the discussions occurring on Sina Weibo and on Chineselanguage Twitter in order to observe two different strands of Chinese culture:people within China who use Sina Weibo with its government imposed restrictionsand those outside that are free to speak completely anonymously. We firstpropose a simple ad-hoc algorithm to identify topics of Tweets and Weibo.Different from previous works on micro-message topic detection, our algorithmconsiders topics of the same contents but with different \#tags. Our algorithmcan also detect topics for Tweets and Weibos without any \#tags. Using a largecorpus of Weibo and Chinese language tweets, covering the period from January$1$ to December $31$, $2012$, we obtain a list of topics using clustered \#tagsthat we can then use to compare the two platforms. Surprisingly, we find thatthere are no common entries among the Top $100$ most popular topics.Furthermore, only $9.2\%$ of tweets correspond to the Top $1000$ topics on SinaWeibo platform, and conversely only $4.4\%$ of weibos were found to discuss themost popular Twitter topics. Our results reveal significant differences insocial attention on the two platforms, with most popular topics on Sina Weiborelating to entertainment while most tweets corresponded to cultural orpolitical contents that is practically non existent in Sina Weibo.
arxiv-14400-208 | Relating Cascaded Random Forests to Deep Convolutional Neural Networks for Semantic Segmentation | http://arxiv.org/pdf/1507.07583v2.pdf | author:David L. Richmond, Dagmar Kainmueller, Michael Y. Yang, Eugene W. Myers, Carsten Rother category:cs.CV published:2015-07-27 summary:We consider the task of pixel-wise semantic segmentation given a small set oflabeled training images. Among two of the most popular techniques to addressthis task are Random Forests (RF) and Neural Networks (NN). The maincontribution of this work is to explore the relationship between two specialforms of these techniques: stacked RFs and deep Convolutional Neural Networks(CNN). We show that there exists a mapping from stacked RF to deep CNN, and anapproximate mapping back. This insight gives two major practical benefits:Firstly, deep CNNs can be intelligently constructed and initialized, which iscrucial when dealing with a limited amount of training data. Secondly, it canbe utilized to create a new stacked RF with improved performance. Furthermore,this mapping yields a new CNN architecture, that is well suited for pixel-wisesemantic labeling. We experimentally verify these practical benefits for twodifferent application scenarios in computer vision and biology, where thelayout of parts is important: Kinect-based body part labeling from depthimages, and somite segmentation in microscopy images of developing zebrafish.
arxiv-14400-209 | Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components | http://arxiv.org/pdf/1411.3972v4.pdf | author:Philipp Geiger, Kun Zhang, Mingming Gong, Dominik Janzing, Bernhard Schölkopf category:stat.ML published:2014-11-14 summary:A widely applied approach to causal inference from a non-experimental timeseries $X$, often referred to as "(linear) Granger causal analysis", is toregress present on past and interpret the regression matrix $\hat{B}$ causally.However, if there is an unmeasured time series $Z$ that influences $X$, thenthis approach can lead to wrong causal conclusions, i.e., distinct from thoseone would draw if one had additional information such as $Z$. In this paper wetake a different approach: We assume that $X$ together with some hidden $Z$forms a first order vector autoregressive (VAR) process with transition matrix$A$, and argue why it is more valid to interpret $A$ causally instead of$\hat{B}$. Then we examine under which conditions the most important parts of$A$ are identifiable or almost identifiable from only $X$. Essentially,sufficient conditions are (1) non-Gaussian, independent noise or (2) noinfluence from $X$ to $Z$. We present two estimation algorithms that aretailored towards conditions (1) and (2), respectively, and evaluate them onsynthetic and real-world data. We discuss how to check the model using $X$.
arxiv-14400-210 | Probabilistic Label Relation Graphs with Ising Models | http://arxiv.org/pdf/1503.01428v3.pdf | author:Nan Ding, Jia Deng, Kevin Murphy, Hartmut Neven category:cs.LG published:2015-03-04 summary:We consider classification problems in which the label space has structure. Acommon example is hierarchical label spaces, corresponding to the case whereone label subsumes another (e.g., animal subsumes dog). But labels can also bemutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). Tojointly model hierarchy and exclusion relations, the notion of a HEX (hierarchyand exclusion) graph was introduced in [7]. This combined a conditional randomfield (CRF) with a deep neural network (DNN), resulting in state of the artresults when applied to visual object classification problems where thetraining labels were drawn from different levels of the ImageNet hierarchy(e.g., an image might be labeled with the basic level category "dog", ratherthan the more specific label "husky"). In this paper, we extend the HEX modelto allow for soft or probabilistic relations between labels, which is usefulwhen there is uncertainty about the relationship between two labels (e.g., anantelope is "sort of" furry, but not to the same degree as a grizzly bear). Wecall our new model pHEX, for probabilistic HEX. We show that the pHEX graph canbe converted to an Ising model, which allows us to use existing off-the-shelfinference methods (in contrast to the HEX method, which needed specializedinference algorithms). Experimental results show significant improvements in anumber of large-scale visual object classification tasks, outperforming theprevious HEX model.
arxiv-14400-211 | Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web | http://arxiv.org/pdf/1512.07155v1.pdf | author:Shugao Ma, Sarah Adel Bargal, Jianming Zhang, Leonid Sigal, Stan Sclaroff category:cs.CV published:2015-12-22 summary:Recently, attempts have been made to collect millions of videos to train CNNmodels for action recognition in videos. However, curating such large-scalevideo datasets requires immense human labor, and training CNNs on millions ofvideos demands huge computational resources. In contrast, collecting actionimages from the Web is much easier and training on images requires much lesscomputation. In addition, labeled web images tend to contain discriminativeaction poses, which highlight discriminative portions of a video's temporalprogression. We explore the question of whether we can utilize web actionimages to train better CNN models for action recognition in videos. We collect23.8K manually filtered images from the Web that depict the 101 actions in theUCF101 action video dataset. We show that by utilizing web action images alongwith videos in training, significant performance boosts of CNN models can beachieved. We then investigate the scalability of the process by leveragingcrawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2Mvideo frames by 393K unfiltered images and get comparable performance.
arxiv-14400-212 | Refined Error Bounds for Several Learning Algorithms | http://arxiv.org/pdf/1512.07146v1.pdf | author:Steve Hanneke category:cs.LG math.ST stat.ML stat.TH published:2015-12-22 summary:This article studies the achievable guarantees on the error rates of certainlearning algorithms, with particular focus on refining logarithmic factors.Many of the results are based on a general technique for obtaining bounds onthe error rates of sample-consistent classifiers with monotonic error regions,in the realizable case. We prove bounds of this type expressed in terms ofeither the VC dimension or the sample compression size. This general techniquealso enables us to derive several new bounds on the error rates of generalsample-consistent learning algorithms, as well as refined bounds on the labelcomplexity of the CAL active learning algorithm. Additionally, we establish asimple necessary and sufficient condition for the existence of adistribution-free bound on the error rates of all sample-consistent learningrules, converging at a rate inversely proportional to the sample size. We alsostudy learning in the presence of classification noise, deriving a new excesserror rate guarantee for general VC classes under Tsybakov's noise condition,and establishing a simple and general necessary and sufficient condition forthe minimax excess risk under bounded noise to converge at a rate inverselyproportional to the sample size.
arxiv-14400-213 | SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation | http://arxiv.org/pdf/1512.07143v1.pdf | author:Mariella Dimiccoli, Marc Bolaños, Estefania Talavera, Maedeh Aghaei, Stavri G. Nikolov, Petia Radeva category:cs.AI cs.CV published:2015-12-22 summary:While wearable cameras are becoming increasingly popular, locating relevantinformation in large unstructured collections of egocentric images is still atedious and time consuming processes. This paper addresses the problem oforganizing egocentric photo streams acquired by a wearable camera intosemantically meaningful segments. First, contextual and semantic information isextracted for each image by employing a Convolutional Neural Networks approach.Later, by integrating language processing, a vocabulary of concepts is definedin a semantic space. Finally, by exploiting the temporal coherence in photostreams, images which share contextual and semantic attributes are groupedtogether. The resulting temporal segmentation is particularly suited forfurther analysis, ranging from activity and event recognition to semanticindexing and summarization. Experiments over egocentric sets of nearly 17,000images, show that the proposed approach outperforms state-of-the-art methods.
arxiv-14400-214 | A New Vision of Collaborative Active Learning | http://arxiv.org/pdf/1504.00284v3.pdf | author:Adrian Calma, Tobias Reitmaier, Bernhard Sick, Paul Lukowicz, Mark Embrechts category:cs.LG stat.ML published:2015-04-01 summary:Active learning (AL) is a learning paradigm where an active learner has totrain a model (e.g., a classifier) which is in principal trained in asupervised way, but in AL it has to be done by means of a data set withinitially unlabeled samples. To get labels for these samples, the activelearner has to ask an oracle (e.g., a human expert) for labels. The goal is tomaximize the performance of the model and to minimize the number of queries atthe same time. In this article, we first briefly discuss the state of the artand own, preliminary work in the field of AL. Then, we propose the concept ofcollaborative active learning (CAL). With CAL, we will overcome some of theharsh limitations of current AL. In particular, we envision scenarios where anexpert may be wrong for various reasons, there might be several or even manyexperts with different expertise, the experts may label not only samples butalso knowledge at a higher level such as rules, and we consider that thelabeling costs depend on many conditions. Moreover, in a CAL process humanexperts will profit by improving their own knowledge, too.
arxiv-14400-215 | A Comprehensive Approach to Mode Clustering | http://arxiv.org/pdf/1406.1780v4.pdf | author:Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman category:stat.ME stat.ML published:2014-06-06 summary:Mode clustering is a nonparametric method for clustering that definesclusters using the basins of attraction of a density estimator's modes. Weprovide several enhancements to mode clustering: (i) a soft variant of clusterassignment, (ii) a measure of connectivity between clusters, (iii) a techniquefor choosing the bandwidth, (iv) a method for denoising small clusters, and (v)an approach to visualizing the clusters. Combining all these enhancements givesus a complete procedure for clustering in multivariate problems. We alsocompare mode clustering to other clustering methods in several examples
arxiv-14400-216 | Recent Advances in Convolutional Neural Networks | http://arxiv.org/pdf/1512.07108v1.pdf | author:Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang category:cs.CV cs.LG cs.NE published:2015-12-22 summary:In the last few years, deep learning has lead to very good performance on avariety of problems, such as object recognition, speech recognition and naturallanguage processing. Among different types of deep neural networks,convolutional neural networks have been most extensively studied. Due to thelack of training data and computing power in early days, it is hard to train alarge high-capacity convolutional neural network without overfitting. Recently,with the rapid growth of data size and the increasing power of graphicsprocessor unit, many researchers have improved the convolutional neuralnetworks and achieved state-of-the-art results on various tasks. In this paper,we provide a broad survey of the recent advances in convolutional neuralnetworks. Besides, we also introduce some applications of convolutional neuralnetworks in computer vision.
arxiv-14400-217 | Diffusion Methods for Classification with Pairwise Relationships | http://arxiv.org/pdf/1505.06072v3.pdf | author:Pedro F. Felzenszwalb, Benar F. Svaiter category:cs.AI cs.CV published:2015-05-22 summary:We define two algorithms for propagating information in classificationproblems with pairwise relationships. The algorithms are based on contractionmaps and are related to non-linear diffusion and random walks on graphs. Theapproach is also related to message passing algorithms, including beliefpropagation and mean field methods. The algorithms we describe are guaranteedto converge on graphs with arbitrary topology. Moreover they always converge toa unique fixed point, independent of initialization. We prove that the fixedpoints of the algorithms under consideration define lower-bounds on the energyfunction and the max-marginals of a Markov random field. The theoreticalresults also illustrate a relationship between message passing algorithms andvalue iteration for an infinite horizon Markov decision process. We illustratethe practical application of the algorithms under study with numericalexperiments in image restoration, stereo depth estimation and binaryclassification on a grid.
arxiv-14400-218 | DeepWriterID: An End-to-end Online Text-independent Writer Identification System | http://arxiv.org/pdf/1508.04945v2.pdf | author:Weixin Yang, Lianwen Jin, Manfei Liu category:cs.CV cs.LG stat.ML published:2015-08-20 summary:Owing to the rapid growth of touchscreen mobile terminals and pen-basedinterfaces, handwriting-based writer identification systems are attractingincreasing attention for personal authentication, digital forensics, and otherapplications. However, most studies on writer identification have not beensatisfying because of the insufficiency of data and difficulty of designinggood features under various conditions of handwritings. Hence, we introduce anend-to-end system, namely DeepWriterID, employed a deep convolutional neuralnetwork (CNN) to address these problems. A key feature of DeepWriterID is a newmethod we are proposing, called DropSegment. It designs to achieve dataaugmentation and improve the generalized applicability of CNN. For sufficientfeature representation, we further introduce path signature feature maps toimprove performance. Experiments were conducted on the NLPR handwritingdatabase. Even though we only use pen-position information in the pen-downstate of the given handwriting samples, we achieved new state-of-the-artidentification rates of 95.72% for Chinese text and 98.51% for English text.
arxiv-14400-219 | The Multiverse Loss for Robust Transfer Learning | http://arxiv.org/pdf/1511.09033v2.pdf | author:Etai Littwin, Lior Wolf category:cs.CV published:2015-11-29 summary:Deep learning techniques are renowned for supporting effective transferlearning. However, as we demonstrate, the transferred representations supportonly a few modes of separation and much of its dimensionality is unutilized. Inthis work, we suggest to learn, in the source domain, multiple orthogonalclassifiers. We prove that this leads to a reduced rank representation, which,however, supports more discriminative directions. Interestingly, the softmaxprobabilities produced by the multiple classifiers are likely to be identical.Experimental results, on CIFAR-100 and LFW, further demonstrate theeffectiveness of our method.
arxiv-14400-220 | Cost-based Feature Transfer for Vehicle Occupant Classification | http://arxiv.org/pdf/1512.07080v1.pdf | author:Toby Perrett, Majid Mirmehdi, Eduardo Dias category:cs.CV I.4.9 published:2015-12-22 summary:Knowledge of human presence and interaction in a vehicle is of growinginterest to vehicle manufacturers for design and safety purposes. We present aframework to perform the tasks of occupant detection and occupantclassification for automatic child locks and airbag suppression. It operatesfor all passenger seats, using a single overhead camera. A transfer learningtechnique is introduced to make full use of training data from all seats whilststill maintaining some control over the bias, necessary for a system designedto penalize certain misclassifications more than others. An evaluation isperformed on a challenging dataset with both weighted and unweightedclassifiers, demonstrating the effectiveness of the transfer process.
arxiv-14400-221 | Move from Perturbed scheme to exponential weighting average | http://arxiv.org/pdf/1512.07074v1.pdf | author:Chunyang Xiao category:cs.LG published:2015-12-22 summary:In an online decision problem, one makes decisions often with a pool ofdecision sequence called experts but without knowledge of the future. Aftereach step, one pays a cost based on the decision and observed rate. Onereasonal goal would be to perform as well as the best expert in the pool. Themodern and well-known way to attain this goal is the algorithm of exponentialweighting. However, recently, another algorithm called follow the perturbedleader is developed and achieved about the same performance. In our work, wefirst show the properties shared in common by the two algorithms which explainthe similarities on the performance. Next we will show that for a specificperturbation, the two algorithms are identical. Finally, we show with someexamples that follow-the-leader style algorithms extend naturally to a largeclass of structured online problems for which the exponential algorithms areinefficient.
arxiv-14400-222 | Partial Functional Correspondence | http://arxiv.org/pdf/1506.05274v2.pdf | author:Emanuele Rodolà, Luca Cosmo, Michael M. Bronstein, Andrea Torsello, Daniel Cremers category:cs.CV published:2015-06-17 summary:In this paper, we propose a method for computing partial functionalcorrespondence between non-rigid shapes. We use perturbation analysis to showhow removal of shape parts changes the Laplace-Beltrami eigenfunctions, andexploit it as a prior on the spectral representation of the correspondence.Corresponding parts are optimization variables in our problem and are used toweight the functional correspondence; we are looking for the largest and mostregular (in the Mumford-Shah sense) parts that minimize correspondencedistortion. We show that our approach can cope with very challengingcorrespondence settings.
arxiv-14400-223 | News Across Languages - Cross-Lingual Document Similarity and Event Tracking | http://arxiv.org/pdf/1512.07046v1.pdf | author:Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz Skraba, Blaz Fortuna, Marko Grobelnik category:cs.IR cs.CL published:2015-12-22 summary:In today's world, we follow news which is distributed globally. Significantevents are reported by different sources and in different languages. In thiswork, we address the problem of tracking of events in a large multilingualstream. Within a recently developed system Event Registry we examine twoaspects of this problem: how to compare articles in different languages and howto link collections of articles in different languages which refer to the sameevent. Taking a multilingual stream and clusters of articles from eachlanguage, we compare different cross-lingual document similarity measures basedon Wikipedia. This allows us to compute the similarity of any two articlesregardless of language. Building on previous work, we show there are methodswhich scale well and can compute a meaningful similarity between articles fromlanguages with little or no direct overlap in the training data. Using thiscapability, we then propose an approach to link clusters of articles acrosslanguages which represent the same event. We provide an extensive evaluation ofthe system as a whole, as well as an evaluation of the quality and robustnessof the similarity measure and the linking algorithm.
arxiv-14400-224 | Implementation of deep learning algorithm for automatic detection of brain tumors using intraoperative IR-thermal mapping data | http://arxiv.org/pdf/1512.07041v1.pdf | author:A. V. Makarenko, M. G. Volovik category:cs.CV cs.LG q-bio.QM stat.ML published:2015-12-22 summary:The efficiency of deep machine learning for automatic delineation of tumorareas has been demonstrated for intraoperative neuronavigation using activeIR-mapping with the use of the cold test. The proposed approach employs amatrix IR-imager to remotely register the space-time distribution of surfacetemperature pattern, which is determined by the dynamics of local cerebralblood flow. The advantages of this technique are non-invasiveness, zero risksfor the health of patients and medical staff, low implementation andoperational costs, ease and speed of use. Traditional IR-diagnostic techniquehas a crucial limitation - it involves a diagnostician who determines theboundaries of tumor areas, which gives rise to considerable uncertainty, whichcan lead to diagnosis errors that are difficult to control. The current studydemonstrates that implementing deep learning algorithms allows to eliminate theexplained drawback.
arxiv-14400-225 | Deep Learning with S-shaped Rectified Linear Activation Units | http://arxiv.org/pdf/1512.07030v1.pdf | author:Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, Shuicheng Yan category:cs.CV published:2015-12-22 summary:Rectified linear activation units are important components forstate-of-the-art deep convolutional networks. In this paper, we propose a novelS-shaped rectified linear activation unit (SReLU) to learn both convex andnon-convex functions, imitating the multiple function forms given by the twofundamental laws, namely the Webner-Fechner law and the Stevens law, inpsychophysics and neural sciences. Specifically, SReLU consists of threepiecewise linear functions, which are formulated by four learnable parameters.The SReLU is learned jointly with the training of the whole deep networkthrough back propagation. During the training phase, to initialize SReLU indifferent layers, we propose a "freezing" method to degenerate SReLU into apredefined leaky rectified linear unit in the initial several training epochsand then adaptively learn the good initial values. SReLU can be universallyused in the existing deep networks with negligible additional parameters andcomputation cost. Experiments with two popular CNN architectures, Network inNetwork and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100,MNIST and ImageNet demonstrate that SReLU achieves remarkable improvementcompared to other activation functions.
arxiv-14400-226 | Towards automating the generation of derivative nouns in Sanskrit by simulating Panini | http://arxiv.org/pdf/1512.05670v2.pdf | author:Amrith Krishna, Pawan Goyal category:cs.CL published:2015-12-17 summary:About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal withgeneration of derivative nouns, making it one of the largest topical sectionsin Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76.This section is a systematic arrangement of rules that enumerates variousaffixes that are used in the derivation under specific semantic relations. Wepropose a system that automates the process of generation of derivative nounsas per the rules in Astadhyayi. The proposed system follows a completely objectoriented approach, that models each rule as a class of its own and then groupsthem as rule groups. The rule groups are decided on the basis of selectivegrouping of rules by virtue of anuvrtti. The grouping of rules results in aninheritance network of rules which is a directed acyclic graph. Every rulegroup has a head rule and the head rule notifies all the direct member rules ofthe group about the environment which contains all the details about dataentities, participating in the derivation process. The system implements thismechanism using multilevel inheritance and observer design patterns. The systemfocuses not only on generation of the desired final form, but also on thecorrectness of sequence of rules applied to make sure that the derivation hastaken place in strict adherence to Astadhyayi. The proposed system's designallows to incorporate various conflict resolution methods mentioned inauthentic texts and hence the effectiveness of those rules can be validatedwith the results from the system. We also present cases where we have checkedthe applicability of the system with the rules which are not specificallyapplicable to derivation of derivative nouns, in order to see the effectivenessof the proposed schema as a generic system for modeling Astadhyayi.
arxiv-14400-227 | FAASTA: A fast solver for total-variation regularization of ill-conditioned problems with application to brain imaging | http://arxiv.org/pdf/1512.06999v1.pdf | author:Gaël Varoquaux, Michael Eickenberg, Elvis Dohmatob, Bertand Thirion category:q-bio.NC cs.LG stat.CO stat.ML published:2015-12-22 summary:The total variation (TV) penalty, as many other analysis-sparsity problems,does not lead to separable factors or a proximal operatorwith a closed-formexpression, such as soft thresholding for the $\ell\_1$ penalty. As a result,in a variational formulation of an inverse problem or statisticallearningestimation, it leads to challenging non-smooth optimization problemsthat areoften solved with elaborate single-step first-order methods. When thedata-fitterm arises from empirical measurements, as in brain imaging, it isoften veryill-conditioned and without simple structure. In this situation, in proximalsplitting methods, the computation cost of thegradient step can easily dominateeach iteration. Thus it is beneficialto minimize the number of gradientsteps.We present fAASTA, a variant of FISTA, that relies on an internal solverforthe TV proximal operator, and refines its tolerance to balancecomputationalcost of the gradient and the proximal steps. We give benchmarksandillustrations on "brain decoding": recovering brain maps fromnoisymeasurements to predict observed behavior. The algorithm as well astheempirical study of convergence speed are valuable for any non-exactproximaloperator, in particular analysis-sparsity problems.
arxiv-14400-228 | On the Differential Privacy of Bayesian Inference | http://arxiv.org/pdf/1512.06992v1.pdf | author:Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis category:cs.AI cs.CR cs.LG math.ST stat.ML stat.TH published:2015-12-22 summary:We study how to communicate findings of Bayesian inference to third parties,while preserving the strong guarantee of differential privacy. Our maincontributions are four different algorithms for private Bayesian inference onproba-bilistic graphical models. These include two mechanisms for adding noiseto the Bayesian updates, either directly to the posterior parameters, or totheir Fourier transform so as to preserve update consistency. We also utilise arecently introduced posterior sampling mechanism, for which we prove bounds forthe specific but general case of discrete Bayesian networks; and we introduce amaximum-a-posteriori private mechanism. Our analysis includes utility andprivacy bounds, with a novel focus on the influence of graph structure onprivacy. Worked examples and experiments with Bayesian na{\"i}ve Bayes andBayesian linear regression illustrate the application of our mechanisms.
arxiv-14400-229 | Multi-Instance Visual-Semantic Embedding | http://arxiv.org/pdf/1512.06963v1.pdf | author:Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, Alan Yuille category:cs.CV published:2015-12-22 summary:Visual-semantic embedding models have been recently proposed and shown to beeffective for image classification and zero-shot learning, by mapping imagesinto a continuous semantic label space. Although several approaches have beenproposed for single-label embedding tasks, handling images with multiple labels(which is a more general setting) still remains an open problem, mainly due tothe complex underlying corresponding relationship between image and its labels.In this work, we present Multi-Instance visual-semantic Embedding model (MIE)for embedding images associated with either single or multiple labels. Ourmodel discovers and maps semantically-meaningful image subregions to theircorresponding labels. And we demonstrate the superiority of our method over thestate-of-the-art on two tasks, including multi-label image annotation andzero-shot learning.
arxiv-14400-230 | A Survey of Available Corpora for Building Data-Driven Dialogue Systems | http://arxiv.org/pdf/1512.05742v2.pdf | author:Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau category:cs.CL cs.AI cs.HC cs.LG stat.ML published:2015-12-17 summary:During the past decade, several areas of speech and language understandinghave witnessed substantial breakthroughs from the use of data-driven models. Inthe area of dialogue systems, the trend is less obvious, and most practicalsystems are still built through significant engineering and expert knowledge.Nevertheless, several recent results suggest that data-driven approaches arefeasible and quite promising. To facilitate research in this area, we havecarried out a wide survey of publicly available datasets suitable fordata-driven learning of dialogue systems. We discuss important characteristicsof these datasets and how they can be used to learn diverse dialoguestrategies. We also describe other potential uses of these datasets, such asmethods for transfer learning between datasets and the use of externalknowledge, and discuss appropriate choice of evaluation metrics for thelearning objective.
arxiv-14400-231 | Action-Conditional Video Prediction using Deep Networks in Atari Games | http://arxiv.org/pdf/1507.08750v2.pdf | author:Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh category:cs.LG cs.AI cs.CV published:2015-07-31 summary:Motivated by vision-based reinforcement learning (RL) problems, in particularAtari games from the recent benchmark Aracade Learning Environment (ALE), weconsider spatio-temporal prediction problems where future (image-)frames aredependent on control variables or actions as well as previous frames. While notcomposed of natural scenes, frames in Atari games are high-dimensional in size,can involve tens of objects with one or more objects being controlled by theactions directly and many other objects being influenced indirectly, caninvolve entry and departure of objects, and can involve deep partialobservability. We propose and evaluate two deep neural network architecturesthat consist of encoding, action-conditional transformation, and decodinglayers based on convolutional neural networks and recurrent neural networks.Experimental results show that the proposed architectures are able to generatevisually-realistic frames that are also useful for control over approximately100-step action-conditional futures in some games. To the best of ourknowledge, this paper is the first to make and evaluate long-term predictionson high-dimensional video conditioned by control inputs.
arxiv-14400-232 | Facility Deployment Decisions through Warp Optimizaton of Regressed Gaussian Processes | http://arxiv.org/pdf/1512.06929v1.pdf | author:Anthony Scopatz category:math.OC stat.ML published:2015-12-22 summary:A method for quickly determining deployment schedules that meet a given fuelcycle demand is presented here. This algorithm is fast enough to perform insitu within low-fidelity fuel cycle simulators. It uses Gaussian processregression models to predict the production curve as a function of time and thenumber of deployed facilities. Each of these predictions is measured againstthe demand curve using the dynamic time warping distance. The minimum distancedeployment schedule is evaluated in a full fuel cycle simulation, whosegenerated production curve then informs the model on the next optimizationiteration. The method converges within five to ten iterations to a distancethat is less than one percent of the total deployable production. Arepresentative once-through fuel cycle is used to demonstrate the methodologyfor reactor deployment.
arxiv-14400-233 | Transformed Residual Quantization for Approximate Nearest Neighbor Search | http://arxiv.org/pdf/1512.06925v1.pdf | author:Jiangbo Yuan, Xiuwen Liu category:cs.CV published:2015-12-22 summary:The success of product quantization (PQ) for fast nearest neighbor searchdepends on the exponentially reduced complexities of both storage andcomputation with respect to the codebook size. Recent efforts have been focusedon employing sophisticated optimization strategies, or seeking more effectivemodels. Residual quantization (RQ) is such an alternative that holds the sameproperty as PQ in terms of the aforementioned complexities. In addition tobeing a direct replacement of PQ, hybrids of PQ and RQ can yield more gains forapproximate nearest neighbor search. This motivated us to propose a novelapproach to optimizing RQ and the related hybrid models. With an observation ofthe general randomness increase in a residual space, we propose a new strategythat jointly learns a local transformation per residual cluster with anultimate goal to reduce overall quantization errors. We have shown that ourapproach can achieve significantly better accuracy on nearest neighbor searchthan both the original and the optimized PQ on several very large scalebenchmarks.
arxiv-14400-234 | Predicting the Co-Evolution of Event and Knowledge Graphs | http://arxiv.org/pdf/1512.06900v1.pdf | author:Cristóbal Esteban, Volker Tresp, Yinchong Yang, Stephan Baier, Denis Krompaß category:cs.LG published:2015-12-21 summary:Embedding learning, a.k.a. representation learning, has been shown to be ableto model large-scale semantic knowledge graphs. A key concept is a mapping ofthe knowledge graph to a tensor representation whose entries are predicted bymodels using latent representations of generalized entities. Knowledge graphsare typically treated as static: A knowledge graph grows more links when morefacts become available but the ground truth values associated with links isconsidered time invariant. In this paper we address the issue of knowledgegraphs where triple states depend on time. We assume that changes in theknowledge graph always arrive in form of events, in the sense that the eventsare the gateway to the knowledge graph. We train an event prediction modelwhich uses both knowledge graph background information and information onrecent events. By predicting future events, we also predict likely changes inthe knowledge graph and thus obtain a model for the evolution of the knowledgegraph as well. Our experiments demonstrate that our approach performs well in aclinical application, a recommendation engine and a sensor network application.
arxiv-14400-235 | Black-Box Policy Search with Probabilistic Programs | http://arxiv.org/pdf/1507.04635v3.pdf | author:Jan-Willem van de Meent, David Tolpin, Brooks Paige, Frank Wood category:stat.ML cs.AI published:2015-07-16 summary:In this work, we explore how probabilistic programs can be used to representpolicies in sequential decision problems. In this formulation, a probabilisticprogram is a black-box stochastic simulator for both the problem domain and theagent. We relate classic policy gradient techniques to recently introducedblack-box variational methods which generalize to probabilistic programinference. We present case studies in the Canadian traveler problem, RockSample, and a benchmark for optimal diagnosis inspired by Guess Who. Each studyillustrates how programs can efficiently represent policies using moderatenumbers of parameters.
arxiv-14400-236 | Noncrossing Ordinal Classification | http://arxiv.org/pdf/1505.03442v3.pdf | author:Xingye Qiao category:stat.ML stat.CO 62H30 published:2015-05-13 summary:Ordinal data are often seen in real applications. Regular multicategoryclassification methods are not designed for this data type and a more propertreatment is needed. We consider a framework of ordinal classification whichpools the results from binary classifiers together. An inherent difficulty ofthis framework is that the class prediction can be ambiguous due to boundarycrossing. To fix this issue, we propose a noncrossing ordinal classificationmethod which materializes the framework by imposing noncrossing constraints. Anasymptotic study of the proposed method is conducted. We show by simulated anddata examples that the proposed method can improve the classificationperformance for ordinal data without the ambiguity caused by boundarycrossings.
arxiv-14400-237 | Car Segmentation and Pose Estimation using 3D Object Models | http://arxiv.org/pdf/1512.06790v1.pdf | author:Siddharth Mahendran, René Vidal category:cs.CV published:2015-12-21 summary:Image segmentation and 3D pose estimation are two key cogs in any algorithmfor scene understanding. However, state-of-the-art CRF-based models for imagesegmentation rely mostly on 2D object models to construct top-down high-orderpotentials. In this paper, we propose new top-down potentials for imagesegmentation and pose estimation based on the shape and volume of a 3D objectmodel. We show that these complex top-down potentials can be easily decomposedinto standard forms for efficient inference in both the segmentation and poseestimation tasks. Experiments on a car dataset show that knowledge ofsegmentation helps perform pose estimation better and vice versa.
arxiv-14400-238 | Information-Theoretic Bounded Rationality | http://arxiv.org/pdf/1512.06789v1.pdf | author:Pedro A. Ortega, Daniel A. Braun, Justin Dyer, Kee-Eung Kim, Naftali Tishby category:stat.ML cs.AI cs.SY math.OC published:2015-12-21 summary:Bounded rationality, that is, decision-making and planning under resourcelimitations, is widely regarded as an important open problem in artificialintelligence, reinforcement learning, computational neuroscience and economics.This paper offers a consolidated presentation of a theory of boundedrationality based on information-theoretic ideas. We provide a conceptualjustification for using the free energy functional as the objective functionfor characterizing bounded-rational decisions. This functional possesses threecrucial properties: it controls the size of the solution space; it has MonteCarlo planners that are exact, yet bypass the need for exhaustive search; andit captures model uncertainty arising from lack of evidence or from interactingwith other agents having unknown intentions. We discuss the single-stepdecision-making case, and show how to extend it to sequential decisions usingequivalence transformations. This extension yields a very general class ofdecision problems that encompass classical decision rules (e.g. EXPECTIMAX andMINIMAX) as limit cases, as well as trust- and risk-sensitive planning.
arxiv-14400-239 | Beyond Classification: Latent User Interests Profiling from Visual Contents Analysis | http://arxiv.org/pdf/1512.06785v1.pdf | author:Longqi Yang, Cheng-Kang Hsieh, Deborah Estrin category:cs.IR cs.CV cs.SI published:2015-12-21 summary:User preference profiling is an important task in modern online socialnetworks (OSN). With the proliferation of image-centric social platforms, suchas Pinterest, visual contents have become one of the most informative datastreams for understanding user preferences. Traditional approaches usuallytreat visual content analysis as a general classification problem where one ormore labels are assigned to each image. Although such an approach simplifiesthe process of image analysis, it misses the rich context and visual cues thatplay an important role in people's perception of images. In this paper, weexplore the possibilities of learning a user's latent visual preferencesdirectly from image contents. We propose a distance metric learning methodbased on Deep Convolutional Neural Networks (CNN) to directly extractsimilarity information from visual contents and use the derived distance metricto mine individual users' fine-grained visual preferences. Through ourpreliminary experiments using data from 5,790 Pinterest users, we show thateven for the images within the same category, each user possesses distinct andindividually-identifiable visual preferences that are consistent over theirlifetime. Our results underscore the untapped potential of finer-grained visualpreference profiling in understanding users' preferences.
arxiv-14400-240 | Multilinear Subspace Clustering | http://arxiv.org/pdf/1512.06730v1.pdf | author:Eric Kernfeld, Nathan Majumder, Shuchin Aeron, Misha Kilmer category:cs.IT cs.CV cs.LG math.IT stat.ML published:2015-12-21 summary:In this paper we present a new model and an algorithm for unsupervisedclustering of 2-D data such as images. We assume that the data comes from aunion of multilinear subspaces (UOMS) model, which is a specific structuredcase of the much studied union of subspaces (UOS) model. For segmentation underthis model, we develop Multilinear Subspace Clustering (MSC) algorithm andevaluate its performance on the YaleB and Olivietti image data sets. We showthat MSC is highly competitive with existing algorithms employing the UOS modelin terms of clustering performance while enjoying improvement in computationalcomplexity.
arxiv-14400-241 | Sparse Coding with Fast Image Alignment via Large Displacement Optical Flow | http://arxiv.org/pdf/1512.06709v1.pdf | author:Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV published:2015-12-21 summary:Sparse representation-based classifiers have shown outstanding accuracy androbustness in image classification tasks even with the presence of intensenoise and occlusion. However, it has been discovered that the performancedegrades significantly either when test image is not aligned with thedictionary atoms or the dictionary atoms themselves are not aligned with eachother, in which cases the sparse linear representation assumption fails. Inthis paper, having both training and test images misaligned, we introduce anovel sparse coding framework that is able to efficiently adapt the dictionaryatoms to the test image via large displacement optical flow. In the proposedalgorithm, every dictionary atom is automatically aligned with the input imageand the sparse code is then recovered using the adapted dictionary atoms. Acorresponding supervised dictionary learning algorithm is also developed forthe proposed framework. Experimental results on digit datasets recognitionverify the efficacy and robustness of the proposed algorithm.
arxiv-14400-242 | Weakly-Supervised Alignment of Video With Text | http://arxiv.org/pdf/1505.06027v2.pdf | author:Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid category:cs.CV cs.CL published:2015-05-22 summary:Suppose that we are given a set of videos, along with natural languagedescriptions in the form of multiple sentences (e.g., manual annotations, moviescripts, sport summaries etc.), and that these sentences appear in the sametemporal order as their visual counterparts. We propose in this paper a methodfor aligning the two modalities, i.e., automatically providing a time stamp forevery sentence. Given vectorial features for both video and text, we propose tocast this task as a temporal assignment problem, with an implicit linearmapping between the two feature modalities. We formulate this problem as aninteger quadratic program, and solve its continuous convex relaxation using anefficient conditional gradient algorithm. Several rounding procedures areproposed to construct the final integer solution. After demonstratingsignificant improvements over the state of the art on the related task ofaligning video with symbolic labels [7], we evaluate our method on achallenging dataset of videos with associated textual descriptions [36], usingboth bag-of-words and continuous representations for text.
arxiv-14400-243 | The 2015 Sheffield System for Transcription of Multi-Genre Broadcast Media | http://arxiv.org/pdf/1512.06643v1.pdf | author:Oscar Saz, Mortaza Doulaty, Salil Deena, Rosanna Milner, Raymond W. M. Ng, Madina Hasan, Yulan Liu, Thomas Hain category:cs.CL published:2015-12-21 summary:We describe the University of Sheffield system for participation in the 2015Multi-Genre Broadcast (MGB) challenge task of transcribing multi-genrebroadcast shows. Transcription was one of four tasks proposed in the MGBchallenge, with the aim of advancing the state of the art of automatic speechrecognition, speaker diarisation and automatic alignment of subtitles forbroadcast media. Four topics are investigated in this work: Data selectiontechniques for training with unreliable data, automatic speech segmentation ofbroadcast media shows, acoustic modelling and adaptation in highly variableenvironments, and language modelling of multi-genre shows. The final systemoperates in multiple passes, using an initial unadapted decoding stage torefine segmentation, followed by three adapted passes: a hybrid DNN pass withinput features normalised by speaker-based cepstral normalisation, anotherhybrid stage with input features normalised by speaker feature-MLLRtransformations, and finally a bottleneck-based tandem stage with noise andspeaker factorisation. The combination of these three system outputs provides afinal error rate of 27.5% on the official development set, consisting of 47multi-genre shows.
arxiv-14400-244 | Local and global gestalt laws: A neurally based spectral approach | http://arxiv.org/pdf/1512.06566v1.pdf | author:Marta Favali, Giovanna Citti, Alessandro Sarti category:cs.CV published:2015-12-21 summary:A mathematical model of figure-ground articulation is presented, taking intoaccount both local and global gestalt laws. The model is compatible with thefunctional architecture of the primary visual cortex (V1). Particularly thelocal gestalt law of good continuity is described by means of suitableconnectivity kernels, that are derived from Lie group theory and are neurallyimplemented in long range connectivity in V1. Different kernels are compatiblewith the geometric structure of cortical connectivity and they are derived asthe fundamental solutions of the Fokker Planck, the Sub-Riemannian Laplacianand the isotropic Laplacian equations. The kernels are used to constructmatrices of connectivity among the features present in a visual stimulus.Global gestalt constraints are then introduced in terms of spectral analysis ofthe connectivity matrix, showing that this processing can be corticallyimplemented in V1 by mean field neural equations. This analysis performsgrouping of local features and individuates perceptual units with the highestsaliency. Numerical simulations are performed and results are obtained applyingthe technique to a number of stimuli.
arxiv-14400-245 | Analysis of Vessel Connectivities in Retinal Images by Cortically Inspired Spectral Clustering | http://arxiv.org/pdf/1512.06559v1.pdf | author:Marta Favali, Samaneh Abbasi-Sureshjani, Bart ter Haar Romeny, Alessandro Sarti category:cs.CV published:2015-12-21 summary:Retinal images provide early signs of diabetic retinopathy, glaucoma andhypertension. These signs can be investigated based on microaneurysms orsmaller vessels. The diagnostic biomarkers are the change of vessel widths andangles especially at junctions, which are investigated using the vesselsegmentation or tracking. Vessel paths may also be interrupted, crossings andbifurcations may be disconnected. This paper addresses a novel contextualmethod based on the geometry of the primary visual cortex (V1) to study thesedifficulties. We have analysed the specific problems at junctions with aconnectivity kernel obtained as the fundamental solution of the Fokker-Planckequation, which is usually used to represent the geometrical structure ofmulti-orientation cortical connectivity. By using the spectral clustering on alarge local affinity matrix constructed by both the connectivity kernel and thefeature of intensity, the vessels are identified successfully in a hierarchicaltopology each representing an individual perceptual unit.
arxiv-14400-246 | Spatial Phase-Sweep: Increasing temporal resolution of transient imaging using a light source array | http://arxiv.org/pdf/1512.06539v1.pdf | author:Ryuichi Tadano, Adithya Kumar Pediredla, Kaushik Mitra, Ashok Veeraraghavan category:cs.CV published:2015-12-21 summary:Transient imaging or light-in-flight techniques capture the propagation of anultra-short pulse of light through a scene, which in effect captures theoptical impulse response of the scene. Recently, it has been shown that we cancapture transient images using commercially available Time-of-Flight (ToF)systems such as Photonic Mixer Devices (PMD). In this paper, we propose`spatial phase-sweep', a technique that exploits the speed of light to increasethe temporal resolution beyond the 100 picosecond limit imposed by currentelectronics. Spatial phase-sweep uses a linear array of light sources withspatial separation of about 3 mm between them, thereby resulting in a timeshift of about 10 picoseconds, which translates into 100 Gfps of transientimaging in theory. We demonstrate a prototype and transient imaging resultsusing spatial phase-sweep.
arxiv-14400-247 | ADAAPT: A Deep Architecture for Adaptive Policy Transfer from Multiple Sources | http://arxiv.org/pdf/1510.02879v2.pdf | author:Janarthanan Rajendran, P Prasanna, Balaraman Ravindran, Mitesh M. Khapra category:cs.AI cs.LG published:2015-10-10 summary:The ability to transfer knowledge from learnt source tasks to a new targettask can be very useful in speeding up the learning process of a ReinforcementLearning agent. This has been receiving a lot of attention, but the applicationof transfer poses two serious challenges which have not been adequatelyaddressed in the past. First, the agent should be able to avoid negativetransfer, which happens when the transfer hampers or slows down the learninginstead of speeding it up. Secondly, the agent should be able to do selectivetransfer which is the ability to select and transfer from different andmultiple source tasks for different parts of the state space of the targettask. We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer,which addresses these challenges. We test ADAAPT using two differentinstantiations: One as ADAAPTive REINFORCE algorithm for direct policy searchand another as ADAAPTive Actor-Critic where the actor uses ADAAPT. Empiricalevaluations on simulated domains show that ADAAPT can be effectively used forpolicy transfer from multiple source MDPs sharing the same state and actionspace.
arxiv-14400-248 | Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect | http://arxiv.org/pdf/1512.06492v1.pdf | author:Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy category:cs.CV cs.AI published:2015-12-21 summary:This paper summarizes the recent progress we have made for the computervision technologies in physical therapy with the accessible and affordabledevices. We first introduce the remote health coaching system we build withMicrosoft Kinect. Since the motion data captured by Kinect is noisy, weinvestigate the data accuracy of Kinect with respect to the high accuracymotion capture system. We also propose an outlier data removal algorithm basedon the data distribution. In order to generate the kinematic parameter from thenoisy data captured by Kinect, we propose a kinematic filtering algorithm basedon Unscented Kalman Filter and the kinematic model of human skeleton. Theproposed algorithm can obtain smooth kinematic parameter with reduced noisecompared to the kinematic parameter generated from the raw motion data fromKinect.
arxiv-14400-249 | Optimal Cluster Recovery in the Labeled Stochastic Block Model | http://arxiv.org/pdf/1510.05956v5.pdf | author:Se-Young Yun, Alexandre Proutiere category:math.PR cs.LG cs.SI stat.ML published:2015-10-20 summary:We consider the problem of community detection or clustering in the labeledStochastic Block Model (labeled SBM) with a finite number $K$ of clusters ofsizes linearly growing with the global population of items $n$. Every pair ofitems is labeled independently at random, and label $\ell$ appears withprobability $p(i,j,\ell)$ between two items in clusters indexed by $i$ and $j$,respectively. The objective is to reconstruct the clusters from the observationof these random labels. Clustering under the SBM and their extensions has attracted much attentionrecently. Most existing work aimed at characterizing the set of parameters suchthat it is possible to infer clusters either positively correlated with thetrue clusters, or with a vanishing proportion of misclassified items, orexactly matching the true clusters. We address the finer and more challenging question of determining, under thegeneral LSBM and for any $s$, the set of parameters such that there exists apolynomial-time clustering algorithm with at most $s$ misclassified items inaverage. We prove that in the regime where it is possible to recover theclusters with a vanishing proportion of misclassified items, a necessary andsufficient condition to get $s=o(n)$ misclassified items in average is $\frac{nD(\alpha,p)}{ \log (n/s)} \ge 1$, where $D(\alpha,p)$ is an appropriatelydefined function of the parameters $p=(p(i,j,\ell), i,j, \ell)$, and $\alpha$defining the sizes of the clusters. We further develop an algorithm, based onsimple spectral methods, that achieves this fundamental performance limit. Theanalysis presented in this paper allows us to recover existing results forasymptotically accurate and exact cluster recovery in the SBM, but has muchbroader applications. For example, it implies that the minimal number ofmisclassified items under the LSBM considered scales as$n\exp(-nD(\alpha,p)(1+o(1)))$.
arxiv-14400-250 | Reading Scene Text in Deep Convolutional Sequences | http://arxiv.org/pdf/1506.04395v2.pdf | author:Pan He, Weilin Huang, Yu Qiao, Chen Change Loy, Xiaoou Tang category:cs.CV published:2015-06-14 summary:We develop a Deep-Text Recurrent Network (DTRN) that regards scene textreading as a sequence labelling problem. We leverage recent advances of deepconvolutional neural networks to generate an ordered high-level sequence from awhole word image, avoiding the difficult character segmentation problem. Then adeep recurrent model, building on long short-term memory (LSTM), is developedto robustly recognize the generated CNN sequences, departing from most existingapproaches recognising each character independently. Our model has a number ofappealing properties in comparison to existing scene text recognition methods:(i) It can recognise highly ambiguous words by leveraging meaningful contextinformation, allowing it to work reliably without either pre- orpost-processing; (ii) the deep CNN feature is robust to various imagedistortions; (iii) it retains the explicit order information in word image,which is essential to discriminate word strings; (iv) the model does not dependon pre-defined dictionary, and it can process unknown words and arbitrarystrings. Codes for the DTRN will be available.
arxiv-14400-251 | Behavioral Modeling for Churn Prediction: Early Indicators and Accurate Predictors of Custom Defection and Loyalty | http://arxiv.org/pdf/1512.06430v1.pdf | author:Muhammad R. Khan, Johua Manoj, Anikate Singh, Joshua Blumenstock category:cs.LG published:2015-12-20 summary:Churn prediction, or the task of identifying customers who are likely todiscontinue use of a service, is an important and lucrative concern of firms inmany different industries. As these firms collect an increasing amount oflarge-scale, heterogeneous data on the characteristics and behaviors ofcustomers, new methods become possible for predicting churn. In this paper, wepresent a unified analytic framework for detecting the early warning signs ofchurn, and assigning a "Churn Score" to each customer that indicates thelikelihood that the particular individual will churn within a predefined amountof time. This framework employs a brute force approach to feature engineering,then winnows the set of relevant attributes via feature selection, beforefeeding the final feature-set into a suite of supervised learning algorithms.Using several terabytes of data from a large mobile phone network, our methodidentifies several intuitive - and a few surprising - early warning signs ofchurn, and our best model predicts whether a subscriber will churn with 89.4%accuracy.
arxiv-14400-252 | Variational Dropout and the Local Reparameterization Trick | http://arxiv.org/pdf/1506.02557v2.pdf | author:Diederik P. Kingma, Tim Salimans, Max Welling category:stat.ML cs.LG stat.CO published:2015-06-08 summary:We investigate a local reparameterizaton technique for greatly reducing thevariance of stochastic gradients for variational Bayesian inference (SGVB) of aposterior over model parameters, while retaining parallelizability. This localreparameterization translates uncertainty about global parameters into localnoise that is independent across datapoints in the minibatch. Suchparameterizations can be trivially parallelized and have variance that isinversely proportional to the minibatch size, generally leading to much fasterconvergence. Additionally, we explore a connection with dropout: Gaussiandropout objectives correspond to SGVB with local reparameterization, ascale-invariant prior and proportionally fixed posterior variance. Our methodallows inference of more flexibly parameterized posteriors; specifically, wepropose variational dropout, a generalization of Gaussian dropout where thedropout rates are learned, often leading to better models. The method isdemonstrated through several experiments.
arxiv-14400-253 | Revisiting Differentially Private Regression: Lessons From Learning Theory and their Consequences | http://arxiv.org/pdf/1512.06388v1.pdf | author:Xi Wu, Matthew Fredrikson, Wentao Wu, Somesh Jha, Jeffrey F. Naughton category:cs.CR cs.DB cs.LG published:2015-12-20 summary:Private regression has received attention from both database and securitycommunities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzedthe functional mechanism (Zhang et al. VLDB 2012) for training linearregression models over medical data. Unfortunately, they found that modelaccuracy is already unacceptable with differential privacy when $\varepsilon =5$. We address this issue, presenting an explicit connection betweendifferential privacy and stable learning theory through which a substantiallybetter privacy/utility tradeoff can be obtained. Perhaps more importantly, ourtheory reveals that the most basic mechanism in differential privacy, outputperturbation, can be used to obtain a better tradeoff for allconvex-Lipschitz-bounded learning tasks. Since output perturbation is simple toimplement, it means that our approach is potentially widely applicable inpractice. We go on to apply it on the same medical data as used by Fredriksonet al. Encouragingly, we achieve accurate models even for $\varepsilon = 0.1$.In the last part of this paper, we study the impact of our improveddifferentially private mechanisms on model inversion attacks, a privacy attackintroduced by Fredrikson et al. We observe that the improved tradeoff makes theresulting differentially private model more susceptible to inversion attacks.We analyze this phenomenon formally.
arxiv-14400-254 | Tensorizing Neural Networks | http://arxiv.org/pdf/1509.06569v2.pdf | author:Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, Dmitry Vetrov category:cs.LG cs.NE published:2015-09-22 summary:Deep neural networks currently demonstrate state-of-the-art performance inseveral domains. At the same time, models of this class are very demanding interms of computational resources. In particular, a large amount of memory isrequired by commonly used fully-connected layers, making it hard to use themodels on low-end devices and stopping the further increase of the model size.In this paper we convert the dense weight matrices of the fully-connectedlayers to the Tensor Train format such that the number of parameters is reducedby a huge factor and at the same time the expressive power of the layer ispreserved. In particular, for the Very Deep VGG networks we report thecompression factor of the dense weight matrix of a fully-connected layer up to200000 times leading to the compression factor of the whole network up to 7times.
arxiv-14400-255 | Kernel principal component analysis network for image classification | http://arxiv.org/pdf/1512.06337v1.pdf | author:Dan Wu, Jiasong Wu, Rui Zeng, Longyu Jiang, Lotfi Senhadji, Huazhong Shu category:cs.LG cs.CV published:2015-12-20 summary:In order to classify the nonlinear feature with linear classifier and improvethe classification accuracy, a deep learning network named kernel principalcomponent analysis network (KPCANet) is proposed. First, mapping the data intohigher space with kernel principal component analysis to make the data linearlyseparable. Then building a two-layer KPCANet to obtain the principal componentsof image. Finally, classifying the principal components with linearlyclassifier. Experimental results show that the proposed KPCANet is effective inface recognition, object recognition and hand-writing digits recognition, italso outperforms principal component analysis network (PCANet) generally aswell. Besides, KPCANet is invariant to illumination and stable to occlusion andslight deformation.
arxiv-14400-256 | Detecting the large entries of a sparse covariance matrix in sub-quadratic time | http://arxiv.org/pdf/1505.03001v2.pdf | author:Ofer Shwartz, Boaz Nadler category:stat.CO cs.LG stat.ML published:2015-05-12 summary:The covariance matrix of a $p$-dimensional random variable is a fundamentalquantity in data analysis. Given $n$ i.i.d. observations, it is typicallyestimated by the sample covariance matrix, at a computational cost of$O(np^{2})$ operations. When $n,p$ are large, this computation may beprohibitively slow. Moreover, in several contemporary applications, thepopulation matrix is approximately sparse, and only its few large entries areof interest. This raises the following question, at the focus of our work:Assuming approximate sparsity of the covariance matrix, can its large entriesbe detected much faster, say in sub-quadratic time, without explicitlycomputing all its $p^{2}$ entries? In this paper, we present and theoreticallyanalyze two randomized algorithms that detect the large entries of anapproximately sparse sample covariance matrix using only $O(np\text{ poly log }p)$ operations. Furthermore, assuming sparsity of the population matrix, wederive sufficient conditions on the underlying random variable and on thenumber of samples $n$, for the sample covariance matrix to satisfy ourapproximate sparsity requirements. Finally, we illustrate the performance ofour algorithms via several simulations.
arxiv-14400-257 | Inference and Mixture Modeling with the Elliptical Gamma Distribution | http://arxiv.org/pdf/1410.4812v2.pdf | author:Reshad Hosseini, Suvrit Sra, Lucas Theis, Matthias Bethge category:stat.CO math.OC stat.ML published:2014-10-17 summary:We study modeling and inference with the Elliptical Gamma Distribution (EGD).We consider maximum likelihood (ML) estimation for EGD scatter matrices, a taskfor which we develop new fixed-point algorithms. Our algorithms are efficientand converge to global optima despite nonconvexity. Moreover, they turn out tobe much faster than both a well-known iterative algorithm of Kent & Tyler(1991) and sophisticated manifold optimization algorithms. Subsequently, weinvoke our ML algorithms as subroutines for estimating parameters of a mixtureof EGDs. We illustrate our methods by applying them to model natural imagestatistics---the proposed EGD mixture model yields the most parsimonious modelamong several competing approaches.
arxiv-14400-258 | On non-iterative training of a neural classifier | http://arxiv.org/pdf/1512.04509v2.pdf | author:K. Eswaran, K. Damodhar Rao category:cs.CV cs.LG cs.NE 62M45 published:2015-12-14 summary:Recently an algorithm, was discovered, which separates points in n-dimensionby planes in such a manner that no two points are left un-separated by at leastone plane{[}1-3{]}. By using this new algorithm we show that there are two waysof classification by a neural network, for a large dimension feature space,both of which are non-iterative and deterministic. To demonstrate the power ofboth these methods we apply them exhaustively to the classical patternrecognition problem: The Fisher-Anderson's, IRIS flower data set and presentthe results. It is expected these methods will now be widely used for the training ofneural networks for Deep Learning not only because of their non-iterative anddeterministic nature but also because of their efficiency and speed and willsupersede other classification methods which are iterative in nature and relyon error minimization.
arxiv-14400-259 | A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction | http://arxiv.org/pdf/1512.06293v1.pdf | author:Thomas Wiatowski, Helmut Bölcskei category:cs.IT cs.AI cs.LG math.FA math.IT stat.ML published:2015-12-19 summary:Deep convolutional neural networks have led to breakthrough results inpractical feature extraction applications. The mathematical analysis of suchnetworks was initiated by Mallat, 2012. Specifically, Mallat consideredso-called scattering networks based on semi-discrete shift-invariant waveletframes and modulus non-linearities in each network layer, and provedtranslation invariance (asymptotically in the wavelet scale parameter) anddeformation stability of the corresponding feature extractor. The purpose ofthis paper is to develop Mallat's theory further by allowing for generalconvolution kernels, or in more technical parlance, general semi-discreteshift-invariant frames (including Weyl-Heisenberg, curvelet, shearlet,ridgelet, and wavelet frames) and general Lipschitz-continuous non-linearities(e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents,and modulus functions), as well as pooling through sub-sampling, all of whichcan be different in different network layers. The resulting generalized networkenables extraction of significantly wider classes of features than thoseresolved by Mallat's wavelet-modulus scattering network. We prove deformationstability for a larger class of deformations than those considered by Mallat,and we establish a new translation invariance result which is of verticalnature in the sense of the network depth determining the amount of invariance.Moreover, our results establish that deformation stability and verticaltranslation invariance are guaranteed by the network structure per se ratherthan the specific convolution kernels and non-linearities. This offers anexplanation for the tremendous success of deep convolutional neural networks ina wide variety of practical feature extraction applications. The mathematicaltechniques we employ are based on continuous frame theory.
arxiv-14400-260 | Neutro-Connectedness Cut | http://arxiv.org/pdf/1512.06285v1.pdf | author:Min Xian, Yingtao Zhang, H. D. Cheng, Fei Xu, Jianrui Ding category:cs.CV published:2015-12-19 summary:Interactive image segmentation is a challenging task and received increasingattention recently; however, two major drawbacks exist in interactivesegmentation approaches. First, the segmentation performance of ROI-basedmethods is sensitive to the initial ROI: different ROIs may produce resultswith great difference. Second, most seed-based methods need intenseinteractions, and are not applicable in many cases. In this work, we generalizethe Neutro-Connectedness (NC) to be independent of top-down priors of objectsand to model image topology with indeterminacy measurement on image regions,propose a novel method for determining object and background regions, which isapplied to exclude isolated background regions and enforce label consistency,and put forward a hybrid interactive segmentation method, Neutro-ConnectednessCut (NC-Cut), which can overcome the above two problems by utilizing bothpixel-wise appearance information and region-based NC properties. We evaluatethe proposed NC-Cut by employing two image datasets (265 images), anddemonstrate that the proposed approach outperforms state-of-the-art interactiveimage segmentation methods (Grabcut, MILCut, One-Cut, {{GC}_max}^sum and pPBC).
arxiv-14400-261 | Estimating network edge probabilities by neighborhood smoothing | http://arxiv.org/pdf/1509.08588v2.pdf | author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML published:2015-09-29 summary:The problem of estimating probabilities of network edges from the observedadjacency matrix has important applications to predicting missing links andnetwork denoising. It has usually been addressed by estimating the graphon, afunction that determines the matrix of edge probabilities, but is ill-definedwithout strong assumptions on the network structure. Here we propose a novelcomputationally efficient method based on neighborhood smoothing to estimatethe expectation of the adjacency matrix directly, without making the strongstructural assumptions graphon estimation requires. The neighborhood smoothingmethod requires little tuning, has a competitive mean-squared error rate, andoutperforms many benchmark methods on the task of link prediction in bothsimulated and real networks.
arxiv-14400-262 | A dense subgraph based algorithm for compact salient image region detection | http://arxiv.org/pdf/1511.06545v2.pdf | author:Souradeep Chakraborty, Pabitra Mitra category:cs.CV published:2015-11-20 summary:We present an algorithm for graph based saliency computation that utilizesthe underlying dense subgraphs in finding visually salient regions in an image.To compute the salient regions, the model first obtains a saliency map usingrandom walks on a Markov chain. Next, k-dense subgraphs are detected to furtherenhance the salient regions in the image. Dense subgraphs convey moreinformation about local graph structure than simple centrality measures. Togenerate the Markov chain, intensity and color features of an image in additionto region compactness is used. For evaluating the proposed model, we doextensive experiments on benchmark image data sets. The proposed methodperforms comparable to well-known algorithms in salient region detection.
arxiv-14400-263 | Multistage SFM: A Coarse-to-Fine Approach for 3D Reconstruction | http://arxiv.org/pdf/1512.06235v1.pdf | author:Rajvi Shah, Aditya Deshpande, P J Narayanan category:cs.CV published:2015-12-19 summary:Several methods have been proposed for large-scale 3D reconstruction fromlarge, unorganized image collections. A large reconstruction problem istypically divided into multiple components which are reconstructedindependently using structure from motion (SFM) and later merged together.Incremental SFM methods are most popular for the basic structure recovery of asingle component. They are robust and effective but are strictly sequential innature. We present a multistage approach for SFM reconstruction of a singlecomponent that breaks the sequential nature of the incremental SFM methods. Ourapproach begins with quickly building a coarse 3D model using only a fractionof features from given images. The coarse model is then enriched by localizingremaining images and matching and triangulating remaining features insubsequent stages. These stages are made efficient and highly parallel byleveraging the geometry of the coarse model. Our method produces similarquality models as compared to incremental SFM methods while being notably fastand parallel.
arxiv-14400-264 | Using machine learning for medium frequency derivative portfolio trading | http://arxiv.org/pdf/1512.06228v1.pdf | author:Abhijit Sharang, Chetan Rao category:q-fin.TR cs.LG stat.ML published:2015-12-19 summary:We use machine learning for designing a medium frequency trading strategy fora portfolio of 5 year and 10 year US Treasury note futures. We formulate thisas a classification problem where we predict the weekly direction of movementof the portfolio using features extracted from a deep belief network trained ontechnical indicators of the portfolio constituents. The experimentation showsthat the resulting pipeline is effective in making a profitable trade.
arxiv-14400-265 | A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging | http://arxiv.org/pdf/1412.4021v5.pdf | author:Dat Quoc Nguyen, Dai Quoc Nguyen, Dang Duc Pham, Son Bao Pham category:cs.CL published:2014-12-12 summary:In this paper, we propose a new approach to construct a system oftransformation rules for the Part-of-Speech (POS) tagging task. Our approach isbased on an incremental knowledge acquisition method where rules are stored inan exception structure and new rules are only added to correct the errors ofexisting rules; thus allowing systematic control of the interaction between therules. Experimental results on 13 languages show that our approach is fast interms of training time and tagging speed. Furthermore, our approach obtainsvery competitive accuracy in comparison to state-of-the-art POS andmorphological taggers.
arxiv-14400-266 | Combining patch-based strategies and non-rigid registration-based label fusion methods | http://arxiv.org/pdf/1512.06223v1.pdf | author:Carlos Platero, M. Carmen Tobar category:cs.CV published:2015-12-19 summary:The objective of this study is to develop a patch-based labeling method thatcooperates with a label fusion using non-rigid registrations. We present anovel patch-based label fusion method, whose selected patches and their weightsare calculated from a combination of similarity measures between patches usingintensity-based distances and labeling-based distances, where a previouslabeling of the target image is inferred through a label fusion method usingnon-rigid registrations. These combined similarity measures result in betterselection of the patches, and their weights are more robust, which improves thesegmentation results compared to other label fusion methods, including theconventional patch-based labeling method. To evaluate the performance and therobustness of the proposed label fusion method, we employ two availabledatabases of T1-weighted (T1W) magnetic resonance imaging (MRI) of humanbrains. We compare our approach with other label fusion methods in theautomatic hippocampal segmentation from T1W-MRI. Our label fusion method yields mean Dice coefficients of 0.847 and 0.798 forthe two databases used with mean times of approximately 180 and 320 seconds,respectively. The collaboration between the patch-based labeling method and thelabel fusion using non-rigid registrations is given in the several levels: (a)The pre-selection of the patches in the atlases are improved, (b) The weightsof our selected patches are also more robust, (c) our approach imposesgeometrical restrictions, such as shape priors, and (d) the work-flow is veryefficient. We show that the proposed approach is very competitive with respectto recently reported methods.
arxiv-14400-267 | A new robust adaptive algorithm for underwater acoustic channel equalization | http://arxiv.org/pdf/1512.06222v1.pdf | author:Dariush Kari, Muhammed Omer Sayin, Suleyman Serdar Kozat category:cs.SD cs.IT cs.LG math.IT published:2015-12-19 summary:We introduce a novel family of adaptive robust equalizers for highlychallenging underwater acoustic (UWA) channel equalization. Since theunderwater environment is highly non-stationary and subjected to impulsivenoise, we use adaptive filtering techniques based on a relative logarithmiccost function inspired by the competitive methods from the online learningliterature. To improve the convergence performance of the conventional linearequalization methods, while mitigating the stability issues, we intrinsicallycombine different norms of the error in the cost function, using logarithmicfunctions. Hence, we achieve a comparable convergence performance to least meanfourth (LMF) equalizer, while significantly enhancing the stability performancein such an adverse communication medium. We demonstrate the performance of ouralgorithms through highly realistic experiments performed on accuratelysimulated underwater acoustic channels.
arxiv-14400-268 | Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines | http://arxiv.org/pdf/1512.06216v1.pdf | author:Hao Zhang, Zhiting Hu, Jinliang Wei, Pengtao Xie, Gunhee Kim, Qirong Ho, Eric Xing category:cs.LG cs.CV cs.DC published:2015-12-19 summary:Deep learning (DL) has achieved notable successes in many machine learningtasks. A number of frameworks have been developed to expedite the process ofdesigning and training deep neural networks (DNNs), such as Caffe, Torch andTheano. Currently they can harness multiple GPUs on a single machine, but areunable to use GPUs that are distributed across multiple machines; as evenaverage-sized DNNs can take days to train on a single GPU with 100s of GBs toTBs of data, distributed GPUs present a prime opportunity for scaling up DL.However, the limited bandwidth available on commodity Ethernet networkspresents a bottleneck to distributed GPU training, and prevents its trivialrealization. To investigate how to adapt existing frameworks to efficiently supportdistributed GPUs, we propose Poseidon, a scalable system architecture fordistributed inter-machine communication in existing DL frameworks. We integratePoseidon with Caffe and evaluate its performance at training DNNs for objectrecognition. Poseidon features three key contributions that accelerate DNNtraining on clusters: (1) a three-level hybrid architecture that allowsPoseidon to support both CPU-only and GPU-equipped clusters, (2) a distributedwait-free backpropagation (DWBP) algorithm to improve GPU utilization and tobalance communication, and (3) a structure-aware communication protocol (SACP)to minimize communication overheads. We empirically show that Poseidonconverges to same objectives as a single machine, and achieves state-of-arttraining speedup across multiple models and well-established datasets using acommodity GPU cluster of 8 nodes (e.g. 4.5x speedup on AlexNet, 4x onGoogLeNet, 4x on CIFAR-10). On the much larger ImageNet22K dataset, Poseidonwith 8 nodes achieves better speedup and competitive accuracy to recentCPU-based distributed systems such as Adam and Le et al., which use 10s to1000s of nodes.
arxiv-14400-269 | Discriminative Subnetworks with Regularized Spectral Learning for Global-state Network Data | http://arxiv.org/pdf/1512.06173v1.pdf | author:Xuan Hong Dang, Ambuj K. Singh, Petko Bogdanov, Hongyuan You, Bayyuan Hsu category:cs.LG published:2015-12-19 summary:Data mining practitioners are facing challenges from data with networkstructure. In this paper, we address a specific class of global-state networkswhich comprises of a set of network instances sharing a similar structure yethaving different values at local nodes. Each instance is associated with aglobal state which indicates the occurrence of an event. The objective is touncover a small set of discriminative subnetworks that can optimally classifyglobal network values. Unlike most existing studies which explore anexponential subnetwork space, we address this difficult problem by adopting aspace transformation approach. Specifically, we present an algorithm thatoptimizes a constrained dual-objective function to learn a low-dimensionalsubspace that is capable of discriminating networks labelled by differentglobal states, while reconciling with common network topology sharing acrossinstances. Our algorithm takes an appealing approach from spectral graphlearning and we show that the globally optimum solution can be achieved viamatrix eigen-decomposition.
arxiv-14400-270 | Regularized Estimation of Piecewise Constant Gaussian Graphical Models: The Group-Fused Graphical Lasso | http://arxiv.org/pdf/1512.06171v1.pdf | author:Alexander J. Gibberd, James D. B. Nelson category:stat.ME stat.CO stat.ML published:2015-12-19 summary:The time-evolving precision matrix of a piecewise-constant Gaussian graphicalmodel encodes the dynamic conditional dependency structure of a multivariatetime-series. Traditionally, graphical models are estimated under the assumptionthat data is drawn identically from a generating distribution. Introducingsparsity and sparse-difference inducing priors we relax these assumptions andpropose a novel regularized M-estimator to jointly estimate both the graph andchangepoint structure. The resulting estimator possesses the ability totherefore favor sparse dependency structures and/or smoothly evolving graphstructures, as required. Moreover, our approach extends current methods toallow estimation of changepoints that are grouped across multiple dependenciesin a system. An efficient algorithm for estimating structure is proposed. Westudy the empirical recovery properties in a synthetic setting. The qualitativeeffect of grouped changepoint estimation is then demonstrated by applying themethod on two real-world data-sets.
arxiv-14400-271 | The Mechanism of Additive Composition | http://arxiv.org/pdf/1511.08407v2.pdf | author:Ran Tian, Naoaki Okazaki, Kentaro Inui category:cs.CL cs.LG published:2015-11-26 summary:We prove an upper bound for the bias of additive composition (Foltz et al.,1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely usedmethod for computing meanings of phrases by averaging the vectorrepresentations of their constituent words. The result endorses additivecomposition as a reasonable operation for calculating meanings of phrases,which is the first theoretical analysis on compositional frameworks from amachine learning point of view. The theory also suggests ways to improveadditive compositionality, including: transforming entries of distributionalword vectors by a function that meets a specific condition, constructing anovel type of vector representations to make additive composition sensitive toword order, and utilizing singular value decomposition to train word vectors.
arxiv-14400-272 | What is India speaking: The "Hinglish" invasion | http://arxiv.org/pdf/1406.4824v2.pdf | author:Rana D. Parshad, Vineeta Chand, Neha Sinha, Nitu Kumari category:cs.CL math.DS published:2014-06-12 summary:While language competition models of diachronic language shift areincreasingly sophisticated, drawing on sociolinguistic components like variablelanguage prestige, distance from language centers and intermediate bilingualtransitionary populations, in one significant way they fall short. They fail toconsider contact-based outcomes resulting in mixed language practices, e.g.outcome scenarios such as creoles or unmarked code switching as an emergentcommunicative norm. On these lines something very interesting is uncovered inIndia, where traditionally there have been monolingual Hindi speakers andHindi/English bilinguals, but virtually no monolingual English speakers. Whilethe Indian census data reports a sharp increase in the proportion ofHindi/English bilinguals, we argue that the number of Hindi/English bilingualsin India is inaccurate, given a new class of urban individuals speaking a mixedlect of Hindi and English, popularly known as "Hinglish". Based onpredator-prey, sociolinguistic theories, salient local ecological factors andthe rural-urban divide in India, we propose a new mathematical model ofinteracting monolingual Hindi speakers, Hindi/English bilinguals and Hinglishspeakers. The model yields globally asymptotic stable states of coexistence, aswell as bilingual extinction. To validate our model, sociolinguistic data fromdifferent Indian classes are contrasted with census reports: We see thatpurported urban Hindi/English bilinguals are unable to maintain fluent Hindispeech and instead produce Hinglish, whereas rural speakers evidencemonolingual Hindi. Thus we present evidence for the first time where anunrecognized mixed lect involving English but not "English", has possibly takenover a sizeable faction of a large global population.
arxiv-14400-273 | Expectation propagation for diffusion processes by moment closure approximations | http://arxiv.org/pdf/1512.06098v1.pdf | author:Botond Cseke, David Schnoerr, Manfred Opper, Guido Sanguinetti category:stat.ML published:2015-12-18 summary:We consider the inverse problem of reconstructing the trajectory of adiffusion process from discrete and continuous time (soft) observations. Wecast the problem in a Bayesian framework and derive approximations to theposterior distributions of state space marginals using variational approximateinference. The resulting optimisation algorithm is a hybrid expectationpropagation/variational message passing algorithm. We then show how theapproximation can be extended to a wide class of discrete-state Markovian jumpprocesses by making use of the chemical Langevin equation. Our empiricalresults show that this is a computationally feasible and accurate method toapproach these intractable classes of inverse problems.
arxiv-14400-274 | Lossy Compression via Sparse Linear Regression: Performance under Minimum-distance Encoding | http://arxiv.org/pdf/1202.0840v4.pdf | author:Ramji Venkataramanan, Antony Joseph, Sekhar Tatikonda category:cs.IT math.IT stat.ML published:2012-02-03 summary:We study a new class of codes for lossy compression with the squared-errordistortion criterion, designed using the statistical framework ofhigh-dimensional linear regression. Codewords are linear combinations ofsubsets of columns of a design matrix. Called a Sparse Superposition or SparseRegression codebook, this structure is motivated by an analogous constructionproposed recently by Barron and Joseph for communication over an AWGN channel.For i.i.d Gaussian sources and minimum-distance encoding, we show that such acode can attain the Shannon rate-distortion function with the optimal errorexponent, for all distortions below a specified value. It is also shown thatsparse regression codes are robust in the following sense: a codebook designedto compress an i.i.d Gaussian source of variance $\sigma^2$ with(squared-error) distortion $D$ can compress any ergodic source of variance lessthan $\sigma^2$ to within distortion $D$. Thus the sparse regression ensembleretains many of the good covering properties of the i.i.d random Gaussianensemble, while having having a compact representation in terms of a matrixwhose size is a low-order polynomial in the block-length.
arxiv-14400-275 | Bayesian anti-sparse coding | http://arxiv.org/pdf/1512.06086v1.pdf | author:Clément Elvira, Pierre Chainais, Nicolas Dobigeon category:stat.ML stat.ME published:2015-12-18 summary:Sparse representations have proven their efficiency in solving a wide classof inverse problems encountered in signal and image processing. Conversely,enforcing the information to be spread uniformly over representationcoefficients exhibits relevant properties in various applications such asdigital communications. Anti-sparse regularization can be naturally expressedthrough an $\ell_{\infty}$-norm penalty. This paper derives a probabilisticformulation of such representations. A new probability distribution, referredto as the democratic prior, is first introduced. Its main properties as well asthree random variate generators for this distribution are derived. Then thisprobability distribution is used as a prior to promote anti-sparsity in aGaussian linear inverse problem, yielding a fully Bayesian formulation ofanti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposedto generate samples according to the posterior distribution. The first one is astandard Gibbs sampler. The second one uses Metropolis-Hastings moves thatexploit the proximity mapping of the log-posterior distribution. These samplesare used to approximate maximum a posteriori and minimum mean square errorestimators of both parameters and hyperparameters. Simulations on syntheticdata illustrate the performances of the two proposed samplers, for bothcomplete and over-complete dictionaries. All results are compared to the recentdeterministic variational FITRA algorithm.
arxiv-14400-276 | Modeling Colors of Single Attribute Variations with Application to Food Appearance | http://arxiv.org/pdf/1512.06075v1.pdf | author:Yaser Yacoob category:cs.CV published:2015-12-18 summary:This paper considers the intra-image color-space of an object or a scene whenthese are subject to a dominant single-source of variation. The source ofvariation can be intrinsic or extrinsic (i.e., imaging conditions) to theobject. We observe that the quantized colors for such objects typically lie ona planar subspace of RGB, and in some cases linear or polynomial curves on thisplane are effective in capturing these color variations. We also observe thatthe inter-image color sub-spaces are robust as long as drastic illuminationchange is not involved. We illustrate the use of this analysis for: discriminating betweenshading-change and reflectance-change for patches, and object detection,segmentation and recognition based on a single exemplar. We focus on images offood items to illustrate the effectiveness of the proposed approach.
arxiv-14400-277 | Asymptotic Behavior of Mean Partitions in Consensus Clustering | http://arxiv.org/pdf/1512.06061v1.pdf | author:Brijnesh Jain category:cs.LG stat.ML published:2015-12-18 summary:Although consistency is a minimum requirement of any estimator, little isknown about consistency of the mean partition approach in consensus clustering.This contribution studies the asymptotic behavior of mean partitions. We showthat under normal assumptions, the mean partition approach is consistent andasymptotic normal. To derive both results, we represent partitions as points ofsome geometric space, called orbit space. Then we draw on results from thetheory of Fr\'echet means and stochastic programming. The asymptotic propertieshold for continuous extensions of standard cluster criteria (indices). Theresults justify consensus clustering using finite but sufficiently large samplesizes. Furthermore, the orbit space framework provides a mathematicalfoundation for studying further statistical, geometrical, and analyticalproperties of sets of partitions.
arxiv-14400-278 | Multiregion Bilinear Convolutional Neural Networks for Person Re-Identification | http://arxiv.org/pdf/1512.05300v2.pdf | author:Evgeniya Ustinova, Yaroslav Ganin, Victor Lempitsky category:cs.CV published:2015-12-16 summary:In this work we explore the applicability of the recently proposed CNNarchitecture, called Bilinear CNN, and its new modification that we callmulti-region Bilinear CNN to the person re-identification problem. Originally,Bilinear CNNs were introduced for fine-grained classification and proved to beboth simple and high-performing architectures. Bilinear CNN allows to build anorderless descriptor for an image using outer product of features outputtedfrom two separate feature extractors. Based on this approach, MultiregionBilinear CNN, apply bilinear pooling over multiple regions for extracting richand useful descriptors that retain some spatial information. We show than when embedded into a standard "siamese" type learning, bilinearCNNs and in particular their multi-region variants can improvere-identification performance compared to standard CNNs and achievestate-of-the-art accuracy on the largest person re-identification datasetsavailable at the moment, namely CUHK03 and Market-1501.
arxiv-14400-279 | High dimensional errors-in-variables models with dependent measurements | http://arxiv.org/pdf/1502.02355v2.pdf | author:Mark Rudelson, Shuheng Zhou category:math.ST stat.ML stat.TH published:2015-02-09 summary:Suppose that we observe $y \in \mathbb{R}^f$ and $X \in \mathbb{R}^{f \timesm}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & X_0\beta^* + \epsilon \\ X & = & X_0 + W \end{eqnarray*} where $X_0$ is a $f\times m$ design matrix with independent subgaussian row vectors, $\epsilon \in\mathbb{R}^f$ is a noise vector and $W$ is a mean zero $f \times m$ randomnoise matrix with independent subgaussian column vectors, independent of $X_0$and $\epsilon$. This model is significantly different from those analyzed inthe literature in the sense that we allow the measurement error for eachcovariate to be a dependent vector across its $f$ observations. Such errorstructures appear in the science literature when modeling the trial-to-trialfluctuations in response strength shared across a set of neurons. Under sparsity and restrictive eigenvalue type of conditions, we show thatone is able to recover a sparse vector $\beta^* \in \mathbb{R}^m$ from themodel given a single observation matrix $X$ and the response vector $y$. Weestablish consistency in estimating $\beta^*$ and obtain the rates ofconvergence in the $\ell_q$ norm, where $q = 1, 2$ for the Lasso-typeestimator, and for $q \in [1, 2]$ for a Dantzig-type conic programmingestimator. We show error bounds which approach that of the regular Lasso andthe Dantzig selector in case the errors in $W$ are tending to 0.
arxiv-14400-280 | Multiclass Classification of Cervical Cancer Tissues by Hidden Markov Model | http://arxiv.org/pdf/1512.06014v1.pdf | author:Sabyasachi Mukhopadhyay, Sanket Nandan, Indrajit Kurmi category:cs.CV published:2015-12-18 summary:In this paper, we report a hidden Markov model based multiclassclassification of cervical cancer tissues. This model has been validateddirectly over time series generated by the medium refractive index fluctuationsextracted from differential interference contrast images of healthy anddifferent stages of cancer tissues. The method shows promising results formulticlass classification with higher accuracy.
arxiv-14400-281 | Face Hallucination using Linear Models of Coupled Sparse Support | http://arxiv.org/pdf/1512.06009v1.pdf | author:Reuben Farrugia, Christine Guillemot category:cs.CV published:2015-12-18 summary:Most face super-resolution methods assume that low-resolution andhigh-resolution manifolds have similar local geometrical structure, hence learnlocal models on the lowresolution manifolds (e.g. sparse or locally linearembedding models), which are then applied on the high-resolution manifold.However, the low-resolution manifold is distorted by the oneto-manyrelationship between low- and high- resolution patches. This paper presents amethod which learns linear models based on the local geometrical structure onthe high-resolution manifold rather than on the low-resolution manifold. Forthis, in a first step, the low-resolution patch is used to derive a globallyoptimal estimate of the high-resolution patch. The approximated solution isshown to be close in Euclidean space to the ground-truth but is generallysmooth and lacks the texture details needed by state-ofthe-art facerecognizers. This first estimate allows us to find the support of thehigh-resolution manifold using sparse coding (SC), which are then used assupport for learning a local projection (or upscaling) model between thelow-resolution and the highresolution manifolds using Multivariate RidgeRegression (MRR). Experimental results show that the proposed methodoutperforms six face super-resolution methods in terms of both recognition andquality. These results also reveal that the recognition and quality aresignificantly affected by the method used for stitching all super-resolvedpatches together, where quilting was found to better preserve the texturedetails which helps to achieve higher recognition rates.
arxiv-14400-282 | Deformable Distributed Multiple Detector Fusion for Multi-Person Tracking | http://arxiv.org/pdf/1512.05990v1.pdf | author:Andy J Ma, Pong C Yuen, Suchi Saria category:cs.CV published:2015-12-18 summary:This paper addresses fully automated multi-person tracking in complexenvironments with challenging occlusion and extensive pose variations. Oursolution combines multiple detectors for a set of different regions of interest(e.g., full-body and head) for multi-person tracking. The use of multipledetectors leads to fewer miss detections as it is able to exploit thecomplementary strengths of the individual detectors. While the number of falsepositives may increase with the increased number of bounding boxes detectedfrom multiple detectors, we propose to group the detection outputs by boundingbox location and depth information. For robustness to significant posevariations, deformable spatial relationship between detectors are learnt in ourmulti-person tracking system. On RGBD data from a live Intensive Care Unit(ICU), we show that the proposed method significantly improves multi-persontracking performance over state-of-the-art methods.
arxiv-14400-283 | Can Pretrained Neural Networks Detect Anatomy? | http://arxiv.org/pdf/1512.05986v1.pdf | author:Vlado Menkovski, Zharko Aleksovski, Axel Saalbach, Hannes Nickisch category:cs.CV cs.AI cs.NE published:2015-12-18 summary:Convolutional neural networks demonstrated outstanding empirical results incomputer vision and speech recognition tasks where labeled training data isabundant. In medical imaging, there is a huge variety of possible imagingmodalities and contrasts, where annotated data is usually very scarce. Wepresent two approaches to deal with this challenge. A network pretrained in adifferent domain with abundant data is used as a feature extractor, while asubsequent classifier is trained on a small target dataset; and a deeparchitecture trained with heavy augmentation and equipped with sophisticatedregularization methods. We test the approaches on a corpus of X-ray images todesign an anatomy detection system.
arxiv-14400-284 | Complexity and Approximation of the Fuzzy K-Means Problem | http://arxiv.org/pdf/1512.05947v1.pdf | author:Johannes Blömer, Sascha Brauer, Kathrin Bujna category:cs.LG cs.DS published:2015-12-18 summary:The fuzzy $K$-means problem is a generalization of the classical $K$-meansproblem to soft clusterings, i.e. clusterings where each points belongs to eachcluster to some degree. Although popular in practice, prior to this work thefuzzy $K$-means problem has not been studied from a complexity theoretic oralgorithmic perspective. We show that optimal solutions for fuzzy $K$-meanscannot, in general, be expressed by radicals over the input points.Surprisingly, this already holds for very simple inputs in one-dimensionalspace. Hence, one cannot expect to compute optimal solutions exactly. We givethe first $(1+\epsilon)$-approximation algorithms for the fuzzy $K$-meansproblem. First, we present a deterministic approximation algorithm whoseruntime is polynomial in $N$ and linear in the dimension $D$ of the input set,given that $K$ is constant, i.e. a polynomial time approximation algorithmgiven a fixed $K$. We achieve this result by showing that for each softclustering there exists a hard clustering with comparable properties. Second,by using techniques known from coreset constructions for the $K$-means problem,we develop a deterministic approximation algorithm that runs in time almostlinear in $N$ but exponential in the dimension $D$. We complement these resultswith a randomized algorithm which imposes some natural restrictions on theinput set and whose runtime is comparable to some of the most efficientapproximation algorithms for $K$-means, i.e. linear in the number of points andthe dimension, but exponential in the number of clusters.
arxiv-14400-285 | Decentralized Joint-Sparse Signal Recovery: A Sparse Bayesian Learning Approach | http://arxiv.org/pdf/1507.02387v2.pdf | author:Saurabh Khanna, Chandra R. Murthy category:cs.LG cs.IT math.IT published:2015-07-09 summary:This work proposes a decentralized, iterative, Bayesian algorithm calledCB-DSBL for in-network estimation of multiple jointly sparse vectors by anetwork of nodes, using noisy and underdetermined linear measurements. Theproposed algorithm exploits the network wide joint sparsity of the un- knownsparse vectors to recover them from significantly fewer number of localmeasurements compared to standalone sparse signal recovery schemes. To reducethe amount of inter-node communication and the associated overheads, the nodesexchange messages with only a small subset of their single hop neighbors. Underthis communication scheme, we separately analyze the convergence of theunderlying Alternating Directions Method of Multipliers (ADMM) iterations usedin our proposed algorithm and establish its linear convergence rate. Thefindings from the convergence analysis of decentralized ADMM are used toaccelerate the convergence of the proposed CB-DSBL algorithm. Using Monte Carlosimulations, we demonstrate the superior signal reconstruction as well assupport recovery performance of our proposed algorithm compared to existingdecentralized algorithms: DRL-1, DCOMP and DCSP.
arxiv-14400-286 | Bayesian Inference of Online Social Network Statistics via Lightweight Random Walk Crawls | http://arxiv.org/pdf/1510.05407v2.pdf | author:Konstantin Avrachenkov, Bruno Ribeiro, Jithin K. Sreedharan category:cs.SI physics.soc-ph stat.ML published:2015-10-19 summary:Online social networks (OSN) contain extensive amount of information aboutthe underlying society that is yet to be explored. One of the most feasibletechnique to fetch information from OSN, crawling through ApplicationProgramming Interface (API) requests, poses serious concerns over the theguarantees of the estimates. In this work, we focus on making reliablestatistical inference with limited API crawls. Based on regenerative propertiesof the random walks, we propose an unbiased estimator for the aggregated sum offunctions over edges and proved the connection between variance of theestimator and spectral gap. In order to facilitate Bayesian inference on thetrue value of the estimator, we derive the approximate posterior distributionof the estimate. Later the proposed ideas are validated with numericalexperiments on inference problems in real-world networks.
arxiv-14400-287 | Experimental robustness of Fourier Ptychography phase retrieval algorithms | http://arxiv.org/pdf/1511.02986v2.pdf | author:Li-Hao Yeh, Jonathan Dong, Jingshan Zhong, Lei Tian, Michael Chen, Gongguo Tang, Mahdi Soltanolkotabi, Laura Waller category:physics.optics cs.CV published:2015-11-10 summary:Fourier ptychography is a new computational microscopy technique thatprovides gigapixel-scale intensity and phase images with both widefield-of-view and high resolution. By capturing a stack of low-resolutionimages under different illumination angles, a nonlinear inverse algorithm canbe used to computationally reconstruct the high-resolution complex field. Here,we compare and classify multiple proposed inverse algorithms in terms ofexperimental robustness. We find that the main sources of error are noise,aberrations and mis-calibration (i.e. model mis-match). Using simulations andexperiments, we demonstrate that the choice of cost function plays a criticalrole, with amplitude-based cost functions performing better thanintensity-based ones. The reason for this is that Fourier ptychography datasetsconsist of images from both brightfield and darkfield illumination,representing a large range of measured intensities. Both noise (e.g. Poissonnoise) and model mis-match errors are shown to scale with intensity. Hence,algorithms that use an appropriate cost function will be more tolerant to bothnoise and model mis-match. Given these insights, we propose a global Newton'smethod algorithm which is robust and computationally efficient. Finally, wediscuss the impact of procedures for algorithmic correction of aberrations andmis-calibration.
arxiv-14400-288 | Domain Adaptation and Transfer Learning in StochasticNets | http://arxiv.org/pdf/1512.05844v1.pdf | author:Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, Alexander Wong category:cs.CV stat.ML published:2015-12-18 summary:Transfer learning is a recent field of machine learning research that aims toresolve the challenge of dealing with insufficient training data in the domainof interest. This is a particular issue with traditional deep neural networkswhere a large amount of training data is needed. Recently, StochasticNets wasproposed to take advantage of sparse connectivity in order to decrease thenumber of parameters that needs to be learned, which in turn may relax trainingdata size requirements. In this paper, we study the efficacy of transferlearning on StochasticNet frameworks. Experimental results show ~7% improvementon StochasticNet performance when the transfer learning is applied in trainingstep.
arxiv-14400-289 | Scene-adaptive Coded Apertures Imaging | http://arxiv.org/pdf/1506.05942v2.pdf | author:Xuehui Wang, Jinli Suo, Jingyi Yu, Yongdong Zhang, Qionghai Dai category:cs.CV published:2015-06-19 summary:Coded aperture imaging systems have recently shown great success inrecovering scene depth and extending the depth-of-field. The ideal pattern,however, would have to serve two conflicting purposes: 1) be broadband toensure robust deconvolution and 2) has sufficient zero-crossings for a highdepth discrepancy. This paper presents a simple but effective scene-adaptivecoded aperture solution to bridge this gap. We observe that the geometricstructures in a natural scene often exhibit only a few edge directions, and thesuccessive frames are closely correlated. Therefore we adopt a spatialpartitioning and temporal propagation scheme. In each frame, we address oneprincipal direction by applying depth-discriminative codes along it andbroadband codes along its orthogonal direction. Since within a frame only theregions with edge direction corresponding to its aperture code behaves well, weutilize the close among-frame correlation to propagate the high quality singleframe results temporally to obtain high performance over the whole imagelattice. To physically implement this scheme, we use a Liquid Crystal onSilicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly,we capture the scene with a pinhole and analyze the scene content to determineprimary edge orientations. Secondly, we sequentially apply the proposed codingscheme with these orientations in the following frames. Experiments on bothsynthetic and real scenes show that our technique is able to combine advantagesof the state-of-the-art patterns for recovering better quality depth map andall-focus images.
arxiv-14400-290 | Deep Poisson Factorization Machines: factor analysis for mapping behaviors in journalist ecosystem | http://arxiv.org/pdf/1512.05840v1.pdf | author:Pau Perng-Hwa Kung category:cs.CY cs.LG stat.ML published:2015-12-18 summary:Newsroom in online ecosystem is difficult to untangle. With prevalence ofsocial media, interactions between journalists and individuals become visible,but lack of understanding to inner processing of information feedback loop inpublic sphere leave most journalists baffled. Can we provide an organized viewto characterize journalist behaviors on individual level to know better of theecosystem? To this end, I propose Poisson Factorization Machine (PFM), aBayesian analogue to matrix factorization that assumes Poisson distribution forgenerative process. The model generalizes recent studies on Poisson MatrixFactorization to account temporal interaction which involves tensor-likestructure, and label information. Two inference procedures are designed, onebased on batch variational EM and another stochastic variational inferencescheme that efficiently scales with data size. An important novelty in thisnote is that I show how to stack layers of PFM to introduce a deeparchitecture. This work discusses some potential results applying the model andexplains how such latent factors may be useful for analyzing latent behaviorsfor data exploration.
arxiv-14400-291 | Monocular Object Instance Segmentation and Depth Ordering with CNNs | http://arxiv.org/pdf/1505.03159v2.pdf | author:Ziyu Zhang, Alexander G. Schwing, Sanja Fidler, Raquel Urtasun category:cs.CV published:2015-05-12 summary:In this paper we tackle the problem of instance-level segmentation and depthordering from a single monocular image. Towards this goal, we take advantage ofconvolutional neural nets and train them to directly predict instance-levelsegmentations where the instance ID encodes the depth ordering within imagepatches. To provide a coherent single explanation of an image we develop aMarkov random field which takes as input the predictions of convolutionalneural nets applied at overlapping patches of different resolutions, as well asthe output of a connected component algorithm. It aims to predict accurateinstance-level segmentation and depth ordering. We demonstrate theeffectiveness of our approach on the challenging KITTI benchmark and show goodperformance on both tasks.
arxiv-14400-292 | Successive Ray Refinement and Its Application to Coordinate Descent for LASSO | http://arxiv.org/pdf/1512.05808v1.pdf | author:Jun Liu, Zheng Zhao, Ruiwen Zhang category:cs.LG published:2015-12-17 summary:Coordinate descent is one of the most popular approaches for solving Lassoand its extensions due to its simplicity and efficiency. When applyingcoordinate descent to solving Lasso, we update one coordinate at a time whilefixing the remaining coordinates. Such an update, which is usually easy tocompute, greedily decreases the objective function value. In this paper, we aimto improve its computational efficiency by reducing the number of coordinatedescent iterations. To this end, we propose a novel technique called SuccessiveRay Refinement (SRR). SRR makes use of the following ray continuation propertyon the successive iterations: for a particular coordinate, the value obtainedin the next iteration almost always lies on a ray that starts at its previousiteration and passes through the current iteration. Motivated by thisray-continuation property, we propose that coordinate descent be performed notdirectly on the previous iteration but on a refined search point that has thefollowing properties: on one hand, it lies on a ray that starts at a historysolution and passes through the previous iteration, and on the other hand, itachieves the minimum objective function value among all the points on the ray.We propose two schemes for defining the search point and show that the refinedsearch point can be efficiently obtained. Empirical results for real andsynthetic data sets show that the proposed SRR can significantly reduce thenumber of coordinate descent iterations, especially for small Lassoregularization parameters.
arxiv-14400-293 | Macau: Scalable Bayesian Multi-relational Factorization with Side Information using MCMC | http://arxiv.org/pdf/1509.04610v2.pdf | author:Jaak Simm, Adam Arany, Pooya Zakeri, Tom Haber, Jörg K. Wegner, Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau category:stat.ML published:2015-09-15 summary:We propose Macau, a powerful and flexible Bayesian factorization method forheterogeneous data. Our model can factorize any set of entities and relationsthat can be represented by a relational model, including tensors and alsomultiple relations for each entity. Macau can also incorporate sideinformation, specifically entity and relation features, which are crucial forpredicting sparsely observed relations. Macau scales to millions of entityinstances, hundred millions of observations, and sparse entity features withmillions of dimensions. To achieve the scale up, we specially designed samplingprocedure for entity and relation features that relies primarily on noiseinjection in linear regressions. We show performance and advanced features ofMacau in a set of experiments, including challenging drug-protein activityprediction task.
arxiv-14400-294 | Synthesis of recurrent neural networks for dynamical system simulation | http://arxiv.org/pdf/1512.05702v1.pdf | author:Adam P Trischler, Gabriele MT D'Eleuterio category:cs.NE 68T01 published:2015-12-17 summary:We review several of the most widely used techniques for training recurrentneural networks to approximate dynamical systems, then describe a novelalgorithm for this task. The algorithm is based on an earlier theoreticalresult that guarantees the quality of the network approximation. We show that afeedforward neural network can be trained on the vector field representation ofa given dynamical system using backpropagation, then recast, using matrixmanipulations, as a recurrent network that replicates the original system'sdynamics. After detailing this algorithm and its relation to earlierapproaches, we present numerical examples that demonstrate its capabilities.One of the distinguishing features of our approach is that both the originaldynamical systems and the recurrent networks that simulate them operate incontinuous time.
arxiv-14400-295 | Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences | http://arxiv.org/pdf/1506.04089v4.pdf | author:Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.CL cs.AI cs.LG cs.NE cs.RO published:2015-06-12 summary:We propose a neural sequence-to-sequence model for direction following, atask that is essential to realizing effective autonomous agents. Ouralignment-based encoder-decoder model with long short-term memory recurrentneural networks (LSTM-RNN) translates natural language instructions to actionsequences based upon a representation of the observable world state. Weintroduce a multi-level aligner that empowers our model to focus on sentence"regions" salient to the current world state by using multiple abstractions ofthe input sentence. In contrast to existing methods, our model uses nospecialized linguistic resources (e.g., parsers) or task-specific annotations(e.g., seed lexicons). It is therefore generalizable, yet still achieves thebest results reported to-date on a benchmark single-sentence dataset andcompetitive results for the limited-training multi-sentence setting. We analyzeour model through a series of ablations that elucidate the contributions of theprimary components of our model.
arxiv-14400-296 | Oracle inequalities for ranking and U-processes with Lasso penalty | http://arxiv.org/pdf/1512.05698v1.pdf | author:Wojciech Rejchel category:stat.ML published:2015-12-17 summary:We investigate properties of estimators obtained by minimization ofU-processes with the Lasso penalty in high-dimensional settings. Our attentionis focused on the ranking problem that is popular in machine learning. It isrelated to guessing the ordering between objects on the basis of their observedpredictors. We prove the oracle inequality for the excess risk of theconsidered estimator as well as the bound for the l1 distance between theestimator and the oracle.
arxiv-14400-297 | Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost Sure, Arbitrarily Slow Growing Regret | http://arxiv.org/pdf/1505.02865v2.pdf | author:Wesley Cowan, Michael N. Katehakis category:stat.ML cs.LG 62L10 published:2015-05-12 summary:The purpose of this paper is to provide further understanding into thestructure of the sequential allocation ("stochastic multi-armed bandit", orMAB) problem by establishing probability one finite horizon bounds andconvergence rates for the sample (or "pseudo") regret associated with twosimple classes of allocation policies $\pi$. For any slowly increasing function $g$, subject to mild regularityconstraints, we construct two policies (the $g$-Forcing, and the $g$-InflatedSample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surelyas $n \to \infty$, bound from above and below. Additionally, almost sure upperand lower bounds on the remainder term are established. In the constructionsherein, the function $g$ effectively controls the "exploration" of theclassical "exploration/exploitation" tradeoff.
arxiv-14400-298 | rnn : Recurrent Library for Torch | http://arxiv.org/pdf/1511.07889v2.pdf | author:Nicholas Léonard, Sagar Waghmare, Yang Wang, Jin-Hwa Kim category:cs.NE published:2015-11-24 summary:The rnn package provides components for implementing a wide range ofRecurrent Neural Networks. It is built withing the framework of the Torchdistribution for use with the nn package. The components have evolved from 3iterations, each adding to the flexibility and capability of the package. Allcomponent modules inherit either the AbstractRecurrent or AbstractSequencerclasses. Strong unit testing, continued backwards compatibility and access tosupporting material are the principles followed during its development. Thepackage is compared against existing implementations of two published papers.
arxiv-14400-299 | Asymptotically Optimal Sequential Experimentation Under Generalized Ranking | http://arxiv.org/pdf/1510.02041v3.pdf | author:Wesley Cowan, Michael N. Katehakis category:stat.ML published:2015-10-07 summary:We consider the \mnk{classical} problem of a controller activating (orsampling) sequentially from a finite number of $N \geq 2$ populations,specified by unknown distributions. Over some time horizon, at each time $n =1, 2, \ldots$, the controller wishes to select a population to sample, with thegoal of sampling from a population that optimizes some "score" function of itsdistribution, e.g., maximizing the expected sum of outcomes or minimizingvariability. We define a class of \textit{Uniformly Fast (UF)} samplingpolicies and show, under mild regularity conditions, that there is anasymptotic lower bound for the expected total number of sub-optimal populationactivations. Then, we provide sufficient conditions under which a UCB policy isUF and asymptotically optimal, since it attains this lower bound. Explicitsolutions are provided for a number of examples of interest, including generalscore functionals on unconstrained Pareto distributions (of potentiallyinfinite mean), and uniform distributions of unknown support. Additionalresults on bandits of Normal distributions are also provided.
arxiv-14400-300 | Asymptotically Optimal Multi-Armed Bandit Policies under a Cost Constraint | http://arxiv.org/pdf/1509.02857v3.pdf | author:Apostolos N. Burnetas, Odysseas Kanavetas, Michael N. Katehakis category:stat.ML math.OC published:2015-09-09 summary:We develop asymptotically optimal policies for the multi armed bandit (MAB),problem, under a cost constraint. This model is applicable in situations whereeach sample (or activation) from a population (bandit) incurs a known banditdependent cost. Successive samples from each population are iid randomvariables with unknown distribution. The objective is to design a feasiblepolicy for deciding from which population to sample from, so as to maximize theexpected sum of outcomes of $n$ total samples or equivalently to minimize theregret due to lack on information on sample distributions, For this problem weconsider the class of feasible uniformly fast (f-UF) convergent policies, thatsatisfy the cost constraint sample-path wise. We first establish a necessaryasymptotic lower bound for the rate of increase of the regret function of f-UFpolicies. Then we construct a class of f-UF policies and provide conditionsunder which they are asymptotically optimal within the class of f-UF policies,achieving this asymptotic lower bound. At the end we provide the explicit formof such policies for the case in which the unknown distributions are Normalwith unknown means and known variances.
