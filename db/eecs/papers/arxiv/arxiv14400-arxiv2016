arxiv-14400-1 | Semi-supervised Collaborative Ranking with Push at Top | http://arxiv.org/abs/1511.05266 | author:Iman Barjasteh, Rana Forsati, Abdol-Hossein Esfahanian, Hayder Radha category:cs.LG cs.IR published:2015-11-17 summary:Existing collaborative ranking based recommender systems tend to perform bestwhen there is enough observed ratings for each user and the observation is madecompletely at random. Under this setting recommender systems can properlysuggest a list of recommendations according to the user interests. However,when the observed ratings are extremely sparse (e.g. in the case of cold-startusers where no rating data is available), and are not sampled uniformly atrandom, existing ranking methods fail to effectively leverage side informationto transduct the knowledge from existing ratings to unobserved ones. We proposea semi-supervised collaborative ranking model, dubbed \texttt{S$^2$COR}, toimprove the quality of cold-start recommendation. \texttt{S$^2$COR} mitigatesthe sparsity issue by leveraging side information about both observed andmissing ratings by collaboratively learning the ranking model. This enables itto deal with the case of missing data not at random, but to also effectivelyincorporate the available side information in transduction. We experimentallyevaluated our proposed algorithm on a number of challenging real-world datasetsand compared against state-of-the-art models for cold-start recommendation. Wereport significantly higher quality recommendations with our algorithm comparedto the state-of-the-art.
arxiv-14400-2 | Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning | http://arxiv.org/abs/1511.05286 | author:Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey category:cs.CV q-bio.SC stat.ML published:2015-11-17 summary:Convolutional neural networks (CNN) have achieved state of the artperformance on both classification and segmentation tasks. Applying CNNs tomicroscopy images is challenging due to the lack of datasets labeled at thesingle cell level. We extend the application of CNNs to microscopy imageclassification and segmentation using multiple instance learning (MIL). Wepresent the adaptive Noisy-AND MIL pooling function, a new MIL operator that isrobust to outliers. Combining CNNs with MIL enables training CNNs using fullresolution microscopy images with global labels. We base our approach on thesimilarity between the aggregation function used in MIL and pooling layers usedin CNNs. We show that training MIL CNNs end-to-end outperforms several previousmethods on both mammalian and yeast microscopy images without requiring anysegmentation steps.
arxiv-14400-3 | Optimized Linear Imputation | http://arxiv.org/abs/1511.05309 | author:Yehezkel S. Resheff, Daphna Weinshall category:stat.ML stat.AP stat.CO stat.ME published:2015-11-17 summary:Often in real-world datasets, especially in high dimensional data, somefeature values are missing. Since most data analysis and statistical methods donot handle gracefully missing values, the ?rst step in the analysis requiresthe imputation of missing values. Indeed, there has been a long standinginterest in methods for the imputation of missing values as a pre-processingstep. One recent and e?ective approach, the IRMI stepwise regression imputationmethod, uses a linear regression model for each real-valued feature on thebasis of all other features in the dataset. However, the proposed iterativeformulation lacks convergence guarantee. Here we propose a closely relatedmethod, stated as a single optimization problem and a block coordinate-descentsolution which is guaranteed to converge to a local minimum. Experiments showresults on both synthetic and benchmark datasets, which are comparable to theresults of the IRMI method whenever it converges. However, while in the set ofexperiments described here IRMI often does not converge, the performance of ourmethods is shown to be markedly superior in comparison with other methods.
arxiv-14400-4 | Return of Frustratingly Easy Domain Adaptation | http://arxiv.org/abs/1511.05547 | author:Baochen Sun, Jiashi Feng, Kate Saenko category:cs.CV cs.AI cs.LG cs.NE published:2015-11-17 summary:Unlike human learning, machine learning often fails to handle changes betweentraining (source) and test (target) input distributions. Such domain shifts,common in practical scenarios, severely damage the performance of conventionalmachine learning methods. Supervised domain adaptation methods have beenproposed for the case when the target data have labels, including some thatperform very well despite being "frustratingly easy" to implement. However, inpractice, the target domain is often unlabeled, requiring unsupervisedadaptation. We propose a simple, effective, and efficient method forunsupervised domain adaptation called CORrelation ALignment (CORAL). CORALminimizes domain shift by aligning the second-order statistics of source andtarget distributions, without requiring any target labels. Even though it isextraordinarily simple--it can be implemented in four lines of Matlabcode--CORAL performs remarkably well in extensive evaluations on standardbenchmark datasets.
arxiv-14400-5 | On the interplay of network structure and gradient convergence in deep learning | http://arxiv.org/abs/1511.05297 | author:Vamsi K Ithapu, Sathya Ravi, Vikas Singh category:cs.LG stat.ML published:2015-11-17 summary:The regularization and output consistency behavior of dropout and layer-wisepretraining for learning deep networks have been fairly well studied. However,our understanding of how the asymptotic convergence of backpropagation in deeparchitectures is related to the structural properties of the network and otherdesign choices (like denoising and dropout rate) is less clear at this time. Aninteresting question one may ask is whether the network architecture and inputdata statistics may guide the choices of learning parameters and vice versa. Inthis work, we explore the association between such structural, distributionaland learnability aspects vis-\`a-vis their interaction with parameterconvergence rates. We present a framework to address these questions based onthe backpropagation convergence for general nonconvex objectives usingfirst-order information. This analysis suggests an interesting relationshipbetween feature denoising and dropout. Building upon the results, we obtain asetup that provides systematic guidance regarding the choice of learningparameters and network sizes that achieve a certain level of convergence (inthe optimization sense) often mediated by statistical attributes of the inputs.Our results are supported by a set of experiments we conducted as well asindependent empirical observations reported by other groups in recent papers.
arxiv-14400-6 | The Use of Machine Learning Algorithms in Recommender Systems: A Systematic Review | http://arxiv.org/abs/1511.05263 | author:Ivens Portugal, Paulo Alencar, Donald Cowan category:cs.SE cs.IR cs.LG published:2015-11-17 summary:Recommender systems use algorithms to provide users with product or servicerecommendations. Recently, these systems have been using machine learningalgorithms from the field of artificial intelligence. However, choosing asuitable machine learning algorithm for a recommender system is difficultbecause of the number of algorithms described in the literature. Researchersand practitioners developing recommender systems are left with littleinformation about the current approaches in algorithm usage. Moreover, thedevelopment of a recommender system using a machine learning algorithm oftenhas problems and open questions that must be evaluated, so software engineersknow where to focus research efforts. This paper presents a systematic reviewof the literature that analyzes the use of machine learning algorithms inrecommender systems and identifies research opportunities for softwareengineering research. The study concludes that Bayesian and decision treealgorithms are widely used in recommender systems because of their relativesimplicity, and that requirement and design phases of recommender systemdevelopment appear to offer opportunities for further research.
arxiv-14400-7 | Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data | http://arxiv.org/abs/1511.05284 | author:Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell category:cs.CV cs.CL published:2015-11-17 summary:While recent deep neural network models have achieved promising results onthe image captioning task, they rely largely on the availability of corporawith paired image and sentence captions to describe objects in context. In thiswork, we propose the Deep Compositional Captioner (DCC) to address the task ofgenerating descriptions of novel objects which are not present in pairedimage-sentence datasets. Our method achieves this by leveraging large objectrecognition datasets and external text corpora and by transferring knowledgebetween semantically similar concepts. Current deep caption models can onlydescribe objects contained in paired image-sentence corpora, despite the factthat they are pre-trained with large object recognition datasets, namelyImageNet. In contrast, our model can compose sentences that describe novelobjects and their interactions with other objects. We demonstrate our model'sability to describe novel concepts by empirically evaluating its performance onMSCOCO and show qualitative results on ImageNet images of objects for which nopaired image-caption data exist. Further, we extend our approach to generatedescriptions of objects in video clips. Our results show that DCC has distinctadvantages over existing image and video captioning approaches for generatingdescriptions of new objects in context.
arxiv-14400-8 | Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression | http://arxiv.org/abs/1511.04901 | author:Zhiao Huang, Erjin Zhou, Zhimin Cao category:cs.CV published:2015-11-16 summary:Facial landmark localization plays an important role in face recognition andanalysis applications. In this paper, we give a brief introduction to acoarse-to-fine pipeline with neural networks and sequential regression. First,a global convolutional network is applied to the holistic facial image to givean initial landmark prediction. A pyramid of multi-scale local image patches isthen cropped to feed to a new network for each landmark to refine theprediction. As the refinement network outputs a more accurate positionestimation than the input, such procedure could be repeated several times untilthe estimation converges. We evaluate our system on the 300-W dataset [11] andit outperforms the recent state-of-the-arts.
arxiv-14400-9 | Fast clustering for scalable statistical analysis on structured images | http://arxiv.org/abs/1511.04898 | author:Bertrand Thirion, Andrés Hoyos-Idrobo, Jonas Kahn, Gael Varoquaux category:stat.ML cs.CV published:2015-11-16 summary:The use of brain images as markers for diseases or behavioral differences ischallenged by the small effects size and the ensuing lack of power, an issuethat has incited researchers to rely more systematically on large cohorts.Coupled with resolution increases, this leads to very large datasets. Astriking example in the case of brain imaging is that of the Human ConnectomeProject: 20 Terabytes of data and growing. The resulting data deluge posessevere challenges regarding the tractability of some processing steps(discriminant analysis, multivariate models) due to the memory demands posed bythese data. In this work, we revisit dimension reduction approaches, such asrandom projections, with the aim of replacing costly function evaluations bycheaper ones while decreasing the memory requirements. Specifically, weinvestigate the use of alternate schemes, based on fast clustering, that arewell suited for signals exhibiting a strong spatial structure, such asanatomical and functional brain images. Our contribution is twofold: i) wepropose a linear-time clustering scheme that bypasses the percolation issuesinherent in these algorithms and thus provides compressions nearly as good astraditional quadratic-complexity variance-minimizing clustering schemes, ii) weshow that cluster-based compression can have the virtuous effect of removinghigh-frequency noise, actually improving subsequent estimations steps. As aconsequence, the proposed approach yields very accurate models on severallarge-scale problems yet with impressive gains in computational efficiency,making it possible to analyze large datasets.
arxiv-14400-10 | Deep Learning for steganalysis is better than a Rich Model with an Ensemble Classifier, and is natively robust to the cover source-mismatch | http://arxiv.org/abs/1511.04855 | author:Lionel Pibre, Pasquet Jérôme, Dino Ienco, Marc Chaumont category:cs.MM cs.CV cs.LG cs.NE published:2015-11-16 summary:Since the BOSS competition, in 2010, most steganalysis approaches use alearning methodology involving two steps: feature extraction, such as the RichModels (RM), for the image representation, and use of the Ensemble Classifier(EC) for the learning step. In 2015, Qian et al. have shown that the use of adeep learning approach that jointly learns and computes the features, is verypromising for the steganalysis. In this paper, we follow-up the study of Qianet al., and show that, due to intrinsic joint minimization, the resultsobtained from a Convolutional Neural Network (CNN) or a Fully Connected NeuralNetwork (FNN), if well parameterized, surpass the conventional use of a RM withan EC. First, numerous experiments were conducted in order to find the best "shape " of the CNN. Second, experiments were carried out in the clairvoyantscenario in order to compare the CNN and FNN to an RM with an EC. The resultsshow more than 16% reduction in the classification error with our CNN or FNN.Third, experiments were also performed in a cover-source mismatch setting. Theresults show that the CNN and FNN are naturally robust to the mismatch problem.In Addition to the experiments, we provide discussions on the internalmechanisms of a CNN, and weave links with some previously stated ideas, inorder to understand the impressive results we obtained.
arxiv-14400-11 | Graph-based denoising for time-varying point clouds | http://arxiv.org/abs/1511.04902 | author:Yann Schoenenberger, Johan Paratte, Pierre Vandergheynst category:cs.CV cs.GR I.5.4 published:2015-11-16 summary:Noisy 3D point clouds arise in many applications. They may be due to errorswhen constructing a 3D model from images or simply to imprecise depth sensors.Point clouds can be given geometrical structure using graphs created from thesimilarity information between points. This paper introduces a technique thatuses this graph structure and convex optimization methods to denoise 3D pointclouds. A short discussion presents how those methods naturally generalize totime-varying inputs such as 3D point cloud time series.
arxiv-14400-12 | Probabilistic Segmentation via Total Variation Regularization | http://arxiv.org/abs/1511.04817 | author:Matt Wytock, J. Zico Kolter category:stat.ML published:2015-11-16 summary:We present a convex approach to probabilistic segmentation and modeling oftime series data. Our approach builds upon recent advances in multivariatetotal variation regularization, and seeks to learn a separate set of parametersfor the distribution over the observations at each time point, but with anadditional penalty that encourages the parameters to remain constant over time.We propose efficient optimization methods for solving the resulting (large)optimization problems, and a two-stage procedure for estimating recurringclusters under such models, based upon kernel density estimation. Finally, weshow on a number of real-world segmentation tasks, the resulting methods oftenperform as well or better than existing latent variable models, while beingsubstantially easier to train.
arxiv-14400-13 | Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition | http://arxiv.org/abs/1511.05204 | author:Mengyi Liu, Shiguang Shan, Ruiping Wang, Xilin Chen category:cs.CV published:2015-11-16 summary:Facial expression is temporally dynamic event which can be decomposed into aset of muscle motions occurring in different facial regions over various timeintervals. For dynamic expression recognition, two key issues, temporalalignment and semantics-aware dynamic representation, must be taken intoaccount. In this paper, we attempt to solve both problems via manifold modelingof videos based on a novel mid-level representation, i.e.\textbf{expressionlet}. Specifically, our method contains three key stages: 1)each expression video clip is characterized as a spatial-temporal manifold(STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM)is learned over all low-level features and represented as a set of local modesto statistically unify all the STMs. 3) the local modes on each STM can beinstantiated by fitting to UMM, and the corresponding expressionlet isconstructed by modeling the variations in each local mode. With above strategy,expression videos are naturally aligned both spatially and temporally. Toenhance the discriminative power, the expressionlet-based STM representation isfurther processed with discriminant embedding. Our method is evaluated on fourpublic expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, ourmethod outperforms the known state-of-the-art by a large margin.
arxiv-14400-14 | Identification and Counting White Blood Cells and Red Blood Cells using Image Processing Case Study of Leukemia | http://arxiv.org/abs/1511.04934 | author:Esti Suryani, Wiharto Wiharto, Nizomjon Polvonov category:cs.CV published:2015-11-16 summary:Leukemia is diagnosed with complete blood counts which is by calculating allblood cells and compare the number of white blood cells (White Blood Cells /WBC) and red blood cells (Red Blood Cells / RBC). Information obtained from acomplete blood count, has become a cornerstone in the hematology laboratory fordiagnostic purposes and monitoring of hematological disorders. However, thetraditional procedure for counting blood cells manually requires effort and along time, therefore this method is one of the most expensive routine tests inlaboratory hematology clinic. Solution for such kind of time consuming task andnecessity of data tracability can be found in image processing techniques basedon blood cell morphology . This study aims to identify Acute LymphocyticLeukemia (ALL) and Acute Myeloid Leukemia type M3 (AML M3) using Fuzzy RuleBased System based on morphology of white blood cells. Characteristicparameters witch extractedare WBC Area, Nucleus and Granule Ratio of whiteblood cells. Image processing algorithms such as thresholding, Canny edgedetection and color identification filters are used.Then for identification ofALL, AML M3 and Healthy cells used Fuzzy Rule Based System with Sugeno method.In the testing process used 104 images out of which 29 ALL - Positive, 50 AMLM3 - Positive and 25 Healthy cells. Test results showed 83.65 % accuracy .
arxiv-14400-15 | Learning Spanish dialects through Twitter | http://arxiv.org/abs/1511.04970 | author:Bruno Gonçalves, David Sánchez category:stat.ML cs.CL cs.CY physics.soc-ph stat.AP published:2015-11-16 summary:We map the large-scale variation of the Spanish language by employing acorpus based on geographically tagged Twitter messages. Lexical dialects areextracted from an analysis of variants of tens of concepts. The resulting mapsshow linguistic variations on an unprecedented scale across the globe. Wediscuss the properties of the main dialects within a machine learning approachand find that varieties spoken in urban areas have an international characterin contrast to country areas where dialects show a more regional uniformity.
arxiv-14400-16 | An Empirical Study of Recent Face Alignment Methods | http://arxiv.org/abs/1511.05049 | author:Heng Yang, Xuhui Jia, Chen Change Loy, Peter Robinson category:cs.CV published:2015-11-16 summary:The problem of face alignment has been intensively studied in the past years.A large number of novel methods have been proposed and reported very goodperformance on benchmark dataset such as 300W. However, the differences in theexperimental setting and evaluation metric, missing details in the descriptionof the methods make it hard to reproduce the results reported and evaluate therelative merits. For instance, most recent face alignment methods are built ontop of face detection but from different face detectors. In this paper, wecarry out a rigorous evaluation of these methods by making the followingcontributions: 1) we proposes a new evaluation metric for face alignment on aset of images, i.e., area under error distribution curve within a threshold,AUC$_\alpha$, given the fact that the traditional evaluation measure (meanerror) is very sensitive to big alignment error. 2) we extend the 300W databasewith more practical face detections to make fair comparison possible. 3) wecarry out face alignment sensitivity analysis w.r.t. face detection, on bothsynthetic and real data, using both off-the-shelf and re-retrained models. 4)we study factors that are particularly important to achieve good performanceand provide suggestions for practical applications. Most of the conclusionsdrawn from our comparative analysis cannot be inferred from the originalpublications.
arxiv-14400-17 | An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family | http://arxiv.org/abs/1511.05042 | author:Alexandre de Brébisson, Pascal Vincent category:cs.NE cs.LG stat.ML published:2015-11-16 summary:In a multi-class classification problem, it is standard to model the outputof a neural network as a categorical distribution conditioned on the inputs.The output must therefore be positive and sum to one, which is traditionallyenforced by a softmax. This probabilistic mapping allows to use the maximumlikelihood principle, which leads to the well-known log-softmax loss. Howeverthe choice of the softmax function seems somehow arbitrary as there are manyother possible normalizing functions. It is thus unclear why the log-softmaxloss would perform better than other loss alternatives. In particular Vincentet al. (2015) recently introduced a class of loss functions, called thespherical family, for which there exists an efficient algorithm to compute theupdates of the output weights irrespective of the output size. In this paper,we explore several loss functions from this family as possible alternatives tothe traditional log-softmax. In particular, we focus our investigation onspherical bounds of the log-softmax loss and on two spherical log-likelihoodlosses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) andthe log-Taylor Softmax that we introduce. Although these alternatives do notyield as good results as the log-softmax loss on two language modeling tasks,they surprisingly outperform it in our experiments on MNIST and CIFAR-10,suggesting that they might be relevant in a broad range of applications.
arxiv-14400-18 | Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation | http://arxiv.org/abs/1511.05076 | author:Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain category:cs.CL published:2015-11-16 summary:This paper presents a new method for the discovery of latent domains indiverse speech data, for the use of adaptation of Deep Neural Networks (DNNs)for Automatic Speech Recognition. Our work focuses on transcription ofmulti-genre broadcast media, which is often only categorised broadly in termsof high level genres such as sports, news, documentary, etc. However, in termsof acoustic modelling these categories are coarse. Instead, it is expected thata mixture of latent domains can better represent the complex and diversebehaviours within a TV show, and therefore lead to better and more robustperformance. We propose a new method, whereby these latent domains arediscovered with Latent Dirichlet Allocation, in an unsupervised manner. Theseare used to adapt DNNs using the Unique Binary Code (UBIC) representation forthe LDA domains. Experiments conducted on a set of BBC TV broadcasts, with morethan 2,000 shows for training and 47 shows for testing, show that the use ofLDA-UBIC DNNs reduces the error up to 13% relative compared to the baselinehybrid DNN models.
arxiv-14400-19 | Learning Mid-level Words on Riemannian Manifold for Action Recognition | http://arxiv.org/abs/1511.04808 | author:Mengyi Liu, Ruiping Wang, Shiguang Shan, Xilin Chen category:cs.CV published:2015-11-16 summary:Human action recognition remains a challenging task due to the varioussources of video data and large intra-class variations. It thus becomes one ofthe key issues in recent research to explore effective and robustrepresentation to handle such challenges. In this paper, we propose a novelrepresentation approach by constructing mid-level words in videos and encodingthem on Riemannian manifold. Specifically, we first conduct a global alignmenton the densely extracted low-level features to build a bank of correspondingfeature groups, each of which can be statistically modeled as a mid-level wordlying on some specific Riemannian manifold. Based on these mid-level words, weconstruct intrinsic Riemannian codebooks by employing K-Karcher-meansclustering and Riemannian Gaussian Mixture Model, and consequently extend theRiemannian manifold version of three well studied encoding methods in Euclideanspace, i.e. Bag of Visual Words (BoVW), Vector of Locally AggregatedDescriptors (VLAD), and Fisher Vector (FV), to obtain the final action videorepresentations. Our method is evaluated in two tasks on four popular realisticdatasets: action recognition on YouTube, UCF50, HMDB51 databases, and actionsimilarity labeling on ASLAN database. In all cases, the reported resultsachieve very competitive performance with those most recent state-of-the-artworks.
arxiv-14400-20 | Topic Modeling of Behavioral Modes Using Sensor Data | http://arxiv.org/abs/1511.05082 | author:Yehezkel S. Resheff, Shay Rotics, Ran Nathan, Daphna Weinshall category:cs.LG published:2015-11-16 summary:The field of Movement Ecology, like so many other fields, is experiencing aperiod of rapid growth in availability of data. As the volume rises,traditional methods are giving way to machine learning and data science, whichare playing an increasingly large part it turning this data intoscience-driving insights. One rich and interesting source is the bio-logger.These small electronic wearable devices are attached to animals free to roam intheir natural habitats, and report back readings from multiple sensors,including GPS and accelerometer bursts. A common use of accelerometer data isfor supervised learning of behavioral modes. However, we need unsupervisedanalysis tools as well, in order to overcome the inherent difficulties ofobtaining a labeled dataset, which in some cases is either infeasible or doesnot successfully encompass the full repertoire of behavioral modes of interest.Here we present a matrix factorization based topic-model method foraccelerometer bursts, derived using a linear mixture property of patchfeatures. Our method is validated via comparison to a labeled dataset, and isfurther compared to standard clustering algorithms.
arxiv-14400-21 | Controlling Bias in Adaptive Data Analysis Using Information Theory | http://arxiv.org/abs/1511.05219 | author:Daniel Russo, James Zou category:stat.ML cs.LG published:2015-11-16 summary:Modern data is messy and high-dimensional, and it is often not clear a prioriwhat are the right questions to ask. Instead, the analyst typically needs touse the data to search for interesting analyses to perform and hypotheses totest. This is an adaptive process, where the choice of analysis to be performednext depends on the results of the previous analyses on the same data. It'swidely recognized that this process, even if well-intentioned, can lead tobiases and false discoveries, contributing to the crisis of reproducibility inscience. But while adaptivity renders standard statistical theory invalid,folklore and experience suggest that not all types of adaptive analysis areequally at risk for false discoveries. In this paper, we propose a generalinformation-theoretic framework to quantify and provably bound the bias andother statistics of an arbitrary adaptive analysis process. We prove that ourmutual information based bound is tight in natural models, and then use it togive rigorous insights into when commonly used procedures do or do not lead tosubstantially biased estimation. We first consider several popular featureselection protocols, like rank selection or variance-based selection. We thenconsider the practice of adding random noise to the observations or to thereported statistics, which is advocated by related ideas from differentialprivacy and blinded data analysis. We discuss the connections between thesetechniques and our framework, and supplement our results with illustrativesimulations.
arxiv-14400-22 | Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization | http://arxiv.org/abs/1511.04798 | author:Baohan Xu, Yanwei Fu, Yu-Gang Jiang, Boyang Li, Leonid Sigal category:cs.CV cs.AI cs.MM published:2015-11-16 summary:Emotional content is a key element in user-generated videos. However, it isdifficult to understand emotions conveyed in such videos due to the complex andunstructured nature of user-generated content and the sparsity of video framesthat express emotion. In this paper, for the first time, we study the problemof transferring knowledge from heterogeneous external sources, including imageand textual data, to facilitate three related tasks in video emotionunderstanding: emotion recognition, emotion attribution and emotion-orientedsummarization. Specifically, our framework (1) learns a video encoding from anauxiliary emotional image dataset in order to improve supervised video emotionrecognition, and (2) transfers knowledge from an auxiliary textual corpus forzero-shot \pl{recognition} of emotion classes unseen during training. Theproposed technique for knowledge transfer facilitates novel applications ofemotion attribution and emotion-oriented summarization. A comprehensive set ofexperiments on multiple datasets demonstrate the effectiveness of ourframework.
arxiv-14400-23 | How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? | http://arxiv.org/abs/1511.05101 | author:Ferenc Huszár category:stat.ML cs.AI cs.IT cs.LG math.IT published:2015-11-16 summary:Modern applications and progress in deep learning research have createdrenewed interest for generative models of text and of images. However, eventoday it is unclear what objective functions one should use to train andevaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-arttraining method that contributed to the winning entry to the MSCOCO imagecaptioning benchmark in 2015. Here we show that despite this impressiveempirical performance, the objective function underlying scheduled sampling isimproper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant toaddress, and present an alternative interpretation. We argue that maximumlikelihood is an inappropriate training objective when the end-goal is togenerate natural-looking samples. We go on to derive an ideal objectivefunction to use in this situation instead. We introduce a generalisation ofadversarial training, and show how such method can interpolate between maximumlikelihood training and our ideal training objective. To our knowledge this isthe first theoretical analysis that explains why adversarial training tends toproduce samples with higher perceived quality.
arxiv-14400-24 | Sparse-promoting Full Waveform Inversion based on Online Orthonormal Dictionary Learning | http://arxiv.org/abs/1511.05194 | author:Lingchen Zhu, Entao Liu, James H. McClellan category:physics.geo-ph cs.LG cs.NA math.NA published:2015-11-16 summary:Full waveform inversion (FWI) delivers high-resolution images of a subsurfacemedium model by minimizing iteratively the least-squares misfit between theobserved and simulated seismic data. Due to the limited accuracy of thestarting model and the inconsistency of the seismic waveform data, the FWIproblem is inherently ill-posed, so that regularization techniques aretypically applied to obtain better models. FWI is also a computationallyexpensive problem because modern seismic surveys cover very large areas ofinterest and collect massive volumes of data. The dimensionality of the problemand the heterogeneity of the medium both stress the need for faster algorithmsand sparse regularization techniques to accelerate and improve imaging results. This paper reaches these goals by developing a compressive sensing approachfor the FWI problem, where the sparsity of model perturbations is exploitedwithin learned dictionaries. Based on stochastic approximations, thedictionaries are updated iteratively to adapt to dynamic model perturbations.Meanwhile, the dictionaries are kept orthonormal in order to maintain thecorresponding transform in a fast and compact manner without introducing extracomputational overhead to FWI. Such a sparsity regularization on modelperturbations enables us to take randomly subsampled data for computation andthus significantly reduce the cost. Compared with other approaches that employsparsity constraints in the fixed curvelet transform domain, our approach canachieve more robust inversion results with better model fit and visual quality.
arxiv-14400-25 | Binary Classifier Calibration using an Ensemble of Near Isotonic Regression Models | http://arxiv.org/abs/1511.05191 | author:Mahdi Pakdaman Naeini, Gregory F. Cooper category:cs.LG stat.ML published:2015-11-16 summary:Learning accurate probabilistic models from data is crucial in many practicaltasks in data mining. In this paper we present a new non-parametric calibrationmethod called \textit{ensemble of near isotonic regression} (ENIR). The methodcan be considered as an extension of BBQ, a recently proposed calibrationmethod, as well as the commonly used calibration method based on isotonicregression. ENIR is designed to address the key limitation of isotonicregression which is the monotonicity assumption of the predictions. Similar toBBQ, the method post-processes the output of a binary classifier to obtaincalibrated probabilities. Thus it can be combined with many existingclassification models. We demonstrate the performance of ENIR on synthetic andreal datasets for the commonly used binary classification models. Experimentalresults show that the method outperforms several common binary classifiercalibration methods. In particular on the real data, ENIR commonly performsstatistically significantly better than the other methods, and never worse. Itis able to improve the calibration power of classifiers, while retaining theirdiscrimination power. The method is also computationally tractable for largescale datasets, as it is $O(N \log N)$ time, where $N$ is the number ofsamples.
arxiv-14400-26 | Cross-scale predictive dictionaries | http://arxiv.org/abs/1511.05174 | author:Vishwanath Saragadam, Aswin Sankaranarayanan, Xin Li category:cs.CV stat.ML published:2015-11-16 summary:We propose a novel signal model, based on sparse representations, thatcaptures cross-scale features for visual signals. We show that cross-scalepredictive model enables faster solutions to sparse approximation problems.This is achieved by first solving the sparse approximation problem for thedownsampled signal and using the support of the solution to constrain thesupport at the original resolution. The speedups obtained are especiallycompelling for high-dimensional signals that require large dictionaries toprovide precise sparse approximations. We demonstrate speedups in the order of10-100x for denoising and up to 15x speedups for compressive sensing of images,videos, hyperspectral images and light-field images.
arxiv-14400-27 | Diversity Networks | http://arxiv.org/abs/1511.05077 | author:Zelda Mariet, Suvrit Sra category:cs.LG cs.NE published:2015-11-16 summary:We introduce Divnet, a flexible technique for learning networks with diverseneurons. Divnet models neuronal diversity by placing a Determinantal PointProcess (DPP) over neurons in a given layer. It uses this DPP to select asubset of diverse neurons and subsequently fuses the redundant neurons into theselected ones. Compared with previous approaches, Divnet offers a moreprincipled, flexible technique for capturing neuronal diversity and thusimplicitly enforcing regularization. This enables effective auto-tuning ofnetwork architecture and leads to smaller network sizes without hurtingperformance. Moreover, through its focus on diversity and neuron fusing, Divnetremains compatible with other procedures that seek to reduce memory footprintsof networks. We present experimental results to corroborate our claims: forpruning neural networks, Divnet is seen to be notably superior to competingapproaches.
arxiv-14400-28 | Resolving the Geometric Locus Dilemma for Support Vector Learning Machines | http://arxiv.org/abs/1511.05102 | author:Denise M. Reeves category:cs.LG stat.ML published:2015-11-16 summary:Capacity control, the bias/variance dilemma, and learning unknown functionsfrom data, are all concerned with identifying effective and consistent fits ofunknown geometric loci to random data points. A geometric locus is a curve orsurface formed by points, all of which possess some uniform property. Ageometric locus of an algebraic equation is the set of points whose coordinatesare solutions of the equation. Any given curve or surface must pass througheach point on a specified locus. This paper argues that it is impossible to fitrandom data points to algebraic equations of partially configured geometricloci that reference arbitrary Cartesian coordinate systems. It also argues thatthe fundamental curve of a linear decision boundary is actually a principaleigenaxis. It is shown that learning principal eigenaxes of linear decisionboundaries involves finding a point of statistical equilibrium for whicheigenenergies of principal eigenaxis components are symmetrically balanced witheach other. It is demonstrated that learning linear decision boundariesinvolves strong duality relationships between a statistical eigenlocus ofprincipal eigenaxis components and its algebraic forms, in primal and dual,correlated Hilbert spaces. Locus equations are introduced and developed thatdescribe principal eigen-coordinate systems for lines, planes, and hyperplanes.These equations are used to introduce and develop primal and dual statisticaleigenlocus equations of principal eigenaxes of linear decision boundaries.Important generalizations for linear decision boundaries are shown to beencoded within a dual statistical eigenlocus of principal eigenaxis components.Principal eigenaxes of linear decision boundaries are shown to encode Bayes'likelihood ratio for common covariance data and a robust likelihood ratio forall other data.
arxiv-14400-29 | Nonlinear Local Metric Learning for Person Re-identification | http://arxiv.org/abs/1511.05169 | author:Siyuan Huang, Jiwen Lu, Jie Zhou, Anil K. Jain category:cs.CV published:2015-11-16 summary:Person re-identification aims at matching pedestrians observed fromnon-overlapping camera views. Feature descriptor and metric learning are twosignificant problems in person re-identification. A discriminative metriclearning method should be capable of exploiting complex nonlineartransformations due to the large variations in feature space. In this paper, wepropose a nonlinear local metric learning (NLML) method to improve thestate-of-the-art performance of person re-identification on public datasets.Motivated by the fact that local metric learning has been introduced to handlethe data which varies locally and deep neural network has presented outstandingcapability in exploiting the nonlinearity of samples, we utilize the merits ofboth local metric learning and deep neural network to learn multiple sets ofnonlinear transformations. By enforcing a margin between the distances ofpositive pedestrian image pairs and distances of negative pairs in thetransformed feature subspace, discriminative information can be effectivelyexploited in the developed neural networks. Our experiments show that theproposed NLML method achieves the state-of-the-art results on the widely usedVIPeR, GRID, and CUHK 01 datasets.
arxiv-14400-30 | Adversarial Manipulation of Deep Representations | http://arxiv.org/abs/1511.05122 | author:Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet category:cs.CV cs.LG cs.NE published:2015-11-16 summary:We show that the representation of an image in a deep neural network (DNN)can be manipulated to mimic those of other natural images, with only minor,imperceptible perturbations to the original image. Previous methods forgenerating adversarial images focused on image perturbations designed toproduce erroneous class labels, while we concentrate on the internal layers ofDNN representations. In this way our new class of adversarial images differsqualitatively from others. While the adversary is perceptually similar to oneimage, its internal representation appears remarkably similar to a differentimage, one from a different class, bearing little if any apparent similarity tothe input; they appear generic and consistent with the space of natural images.This phenomenon raises questions about DNN representations, as well as theproperties of natural images themselves.
arxiv-14400-31 | Deep Kalman Filters | http://arxiv.org/abs/1511.05121 | author:Rahul G. Krishnan, Uri Shalit, David Sontag category:stat.ML cs.LG published:2015-11-16 summary:Kalman Filters are one of the most influential models of time-varyingphenomena. They admit an intuitive probabilistic interpretation, have a simplefunctional form, and enjoy widespread adoption in a variety of disciplines.Motivated by recent variational methods for learning deep generative models, weintroduce a unified algorithm to efficiently learn a broad spectrum of Kalmanfilters. Of particular interest is the use of temporal generative models forcounterfactual inference. We investigate the efficacy of such models forcounterfactual inference, and to that end we introduce the "Healing MNIST"dataset where long-term structure, noise and actions are applied to sequencesof digits. We show the efficacy of our method for modeling this dataset. Wefurther show how our model can be used for counterfactual inference forpatients, based on electronic health record data of 8,000 patients over 4.5years.
arxiv-14400-32 | Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering | http://arxiv.org/abs/1511.04960 | author:Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, Lars Petersson category:cs.CV published:2015-11-16 summary:Scene parsing has attracted a lot of attention in computer vision. Whileparametric models have proven effective for this task, they cannot easilyincorporate new training data. By contrast, nonparametric approaches, whichbypass any learning phase and directly transfer the labels from the trainingdata to the query images, can readily exploit new labeled samples as theybecome available. Unfortunately, because of the computational cost of theirlabel transfer procedures, state-of-the-art nonparametric methods typicallyfilter out most training images to only keep a few relevant ones to label thequery. As such, these methods throw away many images that still containvaluable information and generally obtain an unbalanced set of labeled samples.In this paper, we introduce a nonparametric approach to scene parsing thatfollows a sample-and-filter strategy. More specifically, we propose to samplelabeled superpixels according to an image similarity score, which allows us toobtain a balanced set of samples. We then formulate label transfer as anefficient filtering procedure, which lets us exploit more labeled samples thanexisting techniques. Our experiments evidence the benefits of our approach overstate-of-the-art nonparametric methods on two benchmark datasets.
arxiv-14400-33 | Yin and Yang: Balancing and Answering Binary Visual Questions | http://arxiv.org/abs/1511.05099 | author:Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh category:cs.CL cs.CV cs.LG published:2015-11-16 summary:The complex compositional structure of language makes problems at theintersection of vision and language challenging. But language also provides astrong prior that can result in good superficial performance, without theunderlying models truly understanding the visual content. This can hinderprogress in pushing state of art in the computer vision aspects of multi-modalAI. In this paper, we address binary Visual Question Answering (VQA) onabstract scenes. We formulate this problem as visual verification of conceptsinquired in the questions. Specifically, we convert the question to a tuplethat concisely summarizes the visual concept to be detected in the image. Ifthe concept can be found in the image, the answer to the question is "yes", andotherwise "no". Abstract scenes play two roles (1) They allow us to focus onthe high-level semantics of the VQA task as opposed to the low-levelrecognition problems, and perhaps more importantly, (2) They provide us themodality to balance the dataset such that language priors are controlled, andthe role of vision is essential. In particular, we collect fine-grained pairsof scenes for every question, such that the answer to the question is "yes" forone scene, and "no" for the other for the exact same question. Indeed, languagepriors alone do not perform better than chance on our balanced dataset.Moreover, our proposed approach matches the performance of a state-of-the-artVQA approach on the unbalanced dataset, and outperforms it on the balanceddataset.
arxiv-14400-34 | Visualizing and Understanding Deep Texture Representations | http://arxiv.org/abs/1511.05197 | author:Tsung-Yu Lin, Subhransu Maji category:cs.CV published:2015-11-16 summary:A number of recent approaches have used deep convolutional neural networks(CNNs) to build texture representations. Nevertheless, it is still unclear howthese models represent texture and invariances to categorical variations. Thiswork conducts a systematic evaluation of recent CNN-based texture descriptorsfor recognition and attempts to understand the nature of invariances capturedby these representations. First we show that the recently proposed bilinear CNNmodel [25] is an excellent general-purpose texture descriptor and comparesfavorably to other CNN-based descriptors on various texture and scenerecognition benchmarks. The model is translationally invariant and obtainsbetter accuracy on the ImageNet dataset without requiring spatial jittering ofdata compared to corresponding models trained with spatial jittering. Based onrecent work [13, 28] we propose a technique to visualize pre-images, providinga means for understanding categorical properties that are captured by theserepresentations. Finally, we show preliminary results on how a unifiedparametric model of texture analysis and synthesis can be used forattribute-based image manipulation, e.g. to make an image more swirly,honeycombed, or knitted. The source code and additional visualizations areavailable at http://vis-www.cs.umass.edu/texture
arxiv-14400-35 | Random sampling of bandlimited signals on graphs | http://arxiv.org/abs/1511.05118 | author:Gilles Puy, Nicolas Tremblay, Rémi Gribonval, Pierre Vandergheynst category:cs.SI cs.LG stat.ML published:2015-11-16 summary:We study the problem of sampling k-bandlimited signals on graphs. We proposetwo sampling strategies that consist in selecting a small subset of nodes atrandom. The first strategy is non-adaptive, i.e., independent of the graphstructure, and its performance depends on a parameter called the graphcoherence. On the contrary, the second strategy is adaptive but yields optimalresults. Indeed, no more than O(k log(k)) measurements are sufficient to ensurean accurate and stable recovery of all k-bandlimited signals. This secondstrategy is based on a careful choice of the sampling distribution, which canbe estimated quickly. Then, we propose a computationally efficient decoder toreconstruct k-bandlimited signals from their samples. We prove that it yieldsaccurate reconstructions and that it is also stable to noise. Finally, weconduct several experiments to test these techniques.
arxiv-14400-36 | An Online Sequence-to-Sequence Model Using Partial Conditioning | http://arxiv.org/abs/1511.04868 | author:Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, Samy Bengio category:cs.LG cs.CL cs.NE published:2015-11-16 summary:Sequence-to-sequence models have achieved impressive results on varioustasks. However, they are unsuitable for tasks that require incrementalpredictions to be made as more data arrives. This is because they generate anoutput sequence conditioned on an entire input sequence. In this paper, wepresent a new model that can make incremental predictions as more inputarrives, without redoing the entire computation. Unlike sequence-to-sequencemodels, our method computes the next-step distribution conditioned on thepartial input sequence observed and the partial sequence generated. Itaccomplishes this goal using an encoder recurrent neural network (RNN) thatcomputes features at the same frame rate as the input, and a transducer RNNthat operates over blocks of input steps. The transducer RNN extends thesequence produced so far using a local sequence-to-sequence model. Duringtraining, our method uses alignment information to generate supervised targetsfor each block. Approximate alignment is easily available for tasks such asspeech recognition, action recognition in videos, etc. During inference(decoding), beam search is used to find the most likely output sequence for aninput sequence. This decoding is performed online - at the end of each block,the best candidates from the previous block are extended through the localsequence-to-sequence model. On TIMIT, our online method achieves 19.8% phoneerror rate (PER). For comparison with published sequence-to-sequence methods,we used a bidirectional encoder and achieved 18.7% PER compared to 17.6% fromthe best reported sequence-to-sequence model. Importantly, unlikesequence-to-sequence our model is minimally impacted by the length of theinput. On artificially created longer utterances, it achieves 20.9% with aunidirectional model, compared to 20% from the best bidirectionalsequence-to-sequence models.
arxiv-14400-37 | Performing Highly Accurate Predictions Through Convolutional Networks for Actual Telecommunication Challenges | http://arxiv.org/abs/1511.04906 | author:Jaime Zaratiegui, Ana Montoro, Federico Castanedo category:cs.LG cs.CV published:2015-11-16 summary:We investigated how the application of deep learning, specifically the use ofconvolutional networks trained with GPUs, can help to build better predictivemodels in telecommunication business environments, and fill this gap. Inparticular, we focus on the non-trivial problem of predicting customer churn intelecommunication operators. Our model, called WiseNet, consists of aconvolutional network and a novel encoding method that transforms customeractivity data and Call Detail Records (CDRs) into images. Experimentalevaluation with several machine learning classifiers supports the ability ofWiseNet for learning features when using structured input data. For this typeof telecommunication business problems, we found that WiseNet outperformsmachine learning models with hand-crafted features, and does not require thelabor-intensive step of feature engineering. Furthermore, the same model hasbeen applied without retraining to a different market, achieving consistentresults. This confirms the generalization property of WiseNet and the abilityto extract useful representations.
arxiv-14400-38 | Convolutional Models for Joint Object Categorization and Pose Estimation | http://arxiv.org/abs/1511.05175 | author:Mohamed Elhoseiny, Tarek El-Gaaly, Amr Bakry, Ahmed Elgammal category:cs.CV cs.AI cs.LG published:2015-11-16 summary:In the task of Object Recognition, there exists a dichotomy between thecategorization of objects and estimating object pose, where the formernecessitates a view-invariant representation, while the latter requires arepresentation capable of capturing pose information over different categoriesof objects. With the rise of deep architectures, the prime focus has been onobject category recognition. Deep learning methods have achieved wide successin this task. In contrast, object pose regression using these approaches hasreceived relatively much less attention. In this paper we show how deeparchitectures, specifically Convolutional Neural Networks (CNN), can be adaptedto the task of simultaneous categorization and pose estimation of objects. Weinvestigate and analyze the layers of various CNN models and extensivelycompare between them with the goal of discovering how the layers of distributedrepresentations of CNNs represent object pose information and how thiscontradicts with object category representations. We extensively experiment ontwo recent large and challenging multi-view datasets. Our models achieve betterthan state-of-the-art performance on both datasets.
arxiv-14400-39 | A genetic algorithm to discover flexible motifs with support | http://arxiv.org/abs/1511.04986 | author:Joan Serrà, Aleksandar Matic, Josep Luis Arcos, Alexandros Karatzoglou category:cs.LG cs.NE published:2015-11-16 summary:Finding repeated patterns or motifs in a time series is an importantunsupervised task that has still a number of open issues, starting by thedefinition of motif. In this paper, we revise the notion of motif support,characterizing it as the number of patterns or repetitions that define a motif.We then propose GENMOTIF, a genetic algorithm to discover motifs with supportwhich, at the same time, is flexible enough to accommodate other motifspecifications and task characteristics. GENMOTIF is an anytime algorithm thateasily adapts to many situations: searching in a range of segment lengths,applying uniform scaling, dealing with multiple dimensions, using differentsimilarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: ithas only two intuitive parameters which, if set within reasonable bounds, donot substantially affect its performance. We demonstrate the value of ourapproach in a number of synthetic and real-world settings, considering trafficvolume measurements, accelerometer signals, and telephone call records.
arxiv-14400-40 | Understanding learned CNN features through Filter Decoding with Substitution | http://arxiv.org/abs/1511.05084 | author:Ivet Rafegas, Maria Vanrell category:cs.CV published:2015-11-16 summary:In parallel with the success of CNNs to solve vision problems, there is agrowing interest in developing methodologies to understand and visualize theinternal representations of these networks. How the responses of a trained CNNencode the visual information is a fundamental question both for computer andhuman vision research. Image representations provided by the firstconvolutional layer as well as the resolution change provided by themax-polling operation are easy to understand, however, as soon as a second andfurther convolutional layers are added in the representation, any intuition islost. A usual way to deal with this problem has been to define deconvolutionalnetworks that somehow allow to explore the internal representations of the mostimportant activations towards the image space, where deconvolution is assumedas a convolution with the transposed filter. However, this assumption is notthe best approximation of an inverse convolution. In this paper we propose anew assumption based on filter substitution to reverse the encoding of aconvolutional layer. This provides us with a new tool to directly visualize anyCNN single neuron as a filter in the first layer, this is in terms of the imagespace.
arxiv-14400-41 | Handcrafted Local Features are Convolutional Neural Networks | http://arxiv.org/abs/1511.05045 | author:Zhenzhong Lan, Shoou-I Yu, Ming Lin, Bhiksha Raj, Alexander G. Hauptmann category:cs.CV published:2015-11-16 summary:Image and video classification research has made great progress through thedevelopment of handcrafted local features and learning based features. Thesetwo architectures were proposed roughly at the same time and have flourished atoverlapping stages of history. However, they are typically viewed as distinctapproaches. In this paper, we emphasize their structural similarities and showhow such a unified view helps us in designing features that balance efficiencyand effectiveness. As an example, we study the problem of designing efficientvideo feature learning algorithms for action recognition. We approach this problem by first showing that local handcrafted features andConvolutional Neural Networks (CNNs) share the same convolution-pooling networkstructure. We then propose a two-stream Convolutional ISA (ConvISA) that adoptsthe convolution-pooling structure of the state-of-the-art handcrafted videofeature with greater modeling capacities and a cost-effective trainingalgorithm. Through custom designed network structures for pixels and opticalflow, our method also reflects distinctive characteristics of these two datasources. Our experimental results on standard action recognition benchmarks show thatby focusing on the structure of CNNs, rather than end-to-end training methods,we are able to design an efficient and powerful video feature learningalgorithm.
arxiv-14400-42 | MuProp: Unbiased Backpropagation for Stochastic Neural Networks | http://arxiv.org/abs/1511.05176 | author:Shixiang Gu, Sergey Levine, Ilya Sutskever, Andriy Mnih category:cs.LG published:2015-11-16 summary:Deep neural networks are powerful parametric models that can be trainedefficiently using the backpropagation algorithm. Stochastic neural networkscombine the power of large parametric functions with that of graphical models,which makes it possible to learn very complex distributions. However, asbackpropagation is not directly applicable to stochastic networks that includediscrete sampling operations within their computational graph, training suchnetworks remains difficult. We present MuProp, an unbiased gradient estimatorfor stochastic networks, designed to make this task easier. MuProp improves onthe likelihood-ratio estimator by reducing its variance using a control variatebased on the first-order Taylor expansion of a mean-field network. Crucially,unlike prior attempts at using backpropagation for training stochasticnetworks, the resulting estimator is unbiased and well behaved. Our experimentson structured output prediction and discrete latent variable modelingdemonstrate that MuProp yields consistently good performance across a range ofdifficult tasks.
arxiv-14400-43 | Jet-Images -- Deep Learning Edition | http://arxiv.org/abs/1511.05190 | author:Luke de Oliveira, Michael Kagan, Lester Mackey, Benjamin Nachman, Ariel Schwartzman category:hep-ph stat.ML published:2015-11-16 summary:Building on the notion of a particle physics detector as a camera and thecollimated streams of high energy particles, or jets, it measures as an image,we investigate the potential of machine learning techniques based on deeplearning architectures to identify highly boosted W bosons. Modern deeplearning algorithms trained on jet images can out-perform standardphysically-motivated feature driven approaches to jet tagging. We developtechniques for visualizing how these features are learned by the network andwhat additional information is used to improve performance. This interplaybetween physically-motivated feature driven tools and supervised learningalgorithms is general and can be used to significantly increase the sensitivityto discover new particles and new forces, and gain a deeper understanding ofthe physics within jets.
arxiv-14400-44 | Proposal Flow | http://arxiv.org/abs/1511.05065 | author:Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce category:cs.CV published:2015-11-16 summary:Finding image correspondences remains a challenging problem in the presenceof intra-class variations and large changes in scene layout, typical in sceneflow computation. We introduce a novel approach to this problem, dubbedproposal flow, that establishes reliable correspondences using objectproposals. Unlike prevailing scene flow approaches that operate on pixels orregularly sampled local regions, proposal flow benefits from thecharacteristics of modern object proposals, that exhibit high repeatability atmultiple scales, and can take advantage of both local and geometric consistencyconstraints among proposals. We also show that proposal flow can effectively betransformed into a conventional dense flow field. We introduce a new datasetthat can be used to evaluate both general scene flow techniques andregion-based approaches such as proposal flow. We use this benchmark to comparedifferent matching algorithms, object proposals, and region features withinproposal flow with the state of the art in scene flow. This comparison, alongwith experiments on standard datasets, demonstrates that proposal flowsignificantly outperforms existing scene flow methods in various settings.
arxiv-14400-45 | Neural Programmer: Inducing Latent Programs with Gradient Descent | http://arxiv.org/abs/1511.04834 | author:Arvind Neelakantan, Quoc V. Le, Ilya Sutskever category:cs.LG cs.CL stat.ML published:2015-11-16 summary:Deep neural networks have achieved impressive supervised classificationperformance in many tasks including image recognition, speech recognition, andsequence to sequence learning. However, this success has not been translated toapplications like question answering that may involve complex arithmetic andlogic reasoning. A major limitation of these models is in their inability tolearn even simple arithmetic and logic operations. For example, it has beenshown that neural networks fail to learn to add two binary numbers reliably. Inthis work, we propose Neural Programmer, an end-to-end differentiable neuralnetwork augmented with a small set of basic arithmetic and logic operations.Neural Programmer can call these augmented operations over several steps,thereby inducing compositional programs that are more complex than the built-inoperations. The model learns from a weak supervision signal which is the resultof execution of the correct program, hence it does not require expensiveannotation of the correct program itself. The decisions of what operations tocall, and what data segments to apply to are inferred by Neural Programmer.Such decisions, during training, are done in a differentiable fashion so thatthe entire network can be trained jointly by gradient descent. We find thattraining the model is difficult, but it can be greatly improved by addingrandom noise to the gradient. On a fairly complex synthetic table-comprehensiondataset, traditional recurrent networks and attentional models perform poorlywhile Neural Programmer typically obtains nearly perfect accuracy.
arxiv-14400-46 | Efficient AUC Optimization for Information Ranking Applications | http://arxiv.org/abs/1511.05202 | author:Sean J. Welleck category:cs.IR stat.ML published:2015-11-16 summary:Adequate evaluation of an information retrieval system to estimate futureperformance is a crucial task. Area under the ROC curve (AUC) is widely used toevaluate the generalization of a retrieval system. However, the objectivefunction optimized in many retrieval systems is the error rate and not the AUCvalue. This paper provides an efficient and effective non-linear approach tooptimize AUC using additive regression trees, with a special emphasis on theuse of multi-class AUC (MAUC) because multiple relevance levels are widely usedin many ranking applications. Compared to a conventional linear approach, theperformance of the non-linear approach is comparable on binary-relevancebenchmark datasets and is better on multi-relevance benchmark datasets.
arxiv-14400-47 | Budget Online Multiple Kernel Learning | http://arxiv.org/abs/1511.04813 | author:Jing Lu, Steven C. H. Hoi, Doyen Sahoo, Peilin Zhao category:cs.LG published:2015-11-16 summary:Online learning with multiple kernels has gained increasing interests inrecent years and found many applications. For classification tasks, OnlineMultiple Kernel Classification (OMKC), which learns a kernel based classifierby seeking the optimal linear combination of a pool of single kernelclassifiers in an online fashion, achieves superior accuracy and enjoys greatflexibility compared with traditional single-kernel classifiers. Despite beingstudied extensively, existing OMKC algorithms suffer from high computationalcost due to their unbounded numbers of support vectors. To overcome thisdrawback, we present a novel framework of Budget Online Multiple KernelLearning (BOMKL) and propose a new Sparse Passive Aggressive learning toperform effective budget online learning. Specifically, we adopt a simple yeteffective Bernoulli sampling to decide if an incoming instance should be addedto the current set of support vectors. By limiting the number of supportvectors, our method can significantly accelerate OMKC while maintainingsatisfactory accuracy that is comparable to that of the existing OMKCalgorithms. We theoretically prove that our new method achieves an optimalregret bound in expectation, and empirically found that the proposed algorithmoutperforms various OMKC algorithms and can easily scale up to large-scaledatasets.
arxiv-14400-48 | Efficient Likelihood Learning of a Generic CNN-CRF Model for Semantic Segmentation | http://arxiv.org/abs/1511.05067 | author:Alexander Kirillov, Dmitrij Schlesinger, Walter Forkel, Anatoly Zelenin, Shuai Zheng, Philip Torr, Carsten Rother category:cs.CV published:2015-11-16 summary:Deep Models, such as Convolutional Neural Networks (CNNs), are omnipresent incomputer vision, as well as, structured models, such as Conditional RandomFields (CRFs). Combining them brings many advantages, foremost the ability toin-cooperate prior knowledge into CNNs, e.g. by explicitly modelling thedependencies between output variables. In this work we present a CRF model wereunary factors are dependent on a CNN. Our main contribution is an efficient andscalable, maximum likelihood-based, learning procedure to infer all modelparameters jointly. Previous work either concentrated on piecewise-training, ormaximum likelihood learning of restricted model families, such as Gaussian CRFsor CRFs with a few variables only. In contrast, we are the first to performmaximum likelihood learning for large-sized factor graphs with non-parametricpotentials. We have applied our model to the task of semantic labeling of bodyparts in depth images. We show that it is superior to selected competing modelsand learning strategies. Furthermore, we empirically observe that our model cancapture shape and context information of relating body parts.
arxiv-14400-49 | Sherlock: Scalable Fact Learning in Images | http://arxiv.org/abs/1511.04891 | author:Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal category:cs.CV cs.CL cs.LG published:2015-11-16 summary:We study scalable and uniform understanding of facts in images. Existingvisual recognition systems are typically modeled differently for each fact typesuch as objects, actions, and interactions. We propose a setting where allthese facts can be modeled simultaneously with a capacity to understandunbounded number of facts in a structured way. The training data comes asstructured facts in images, including (1) objects (e.g., $<$boy$>$), (2)attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and(4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semanticlanguage view (e.g., $<$ boy, playing$>$) and a visual view (an image with thisfact). We show that learning visual facts in a structured way enables not onlya uniform but also generalizable visual understanding. We propose andinvestigate recent and strong approaches from the multiview learning literatureand also introduce two learning representation models as potential baselines.We applied the investigated methods on several datasets that we augmented withstructured facts and a large scale dataset of more than 202,000 facts and814,000 images. Our experiments show the advantage of relating facts by thestructure by the proposed models compared to the designed baselines onbidirectional fact retrieval.
arxiv-14400-50 | Binary embeddings with structured hashed projections | http://arxiv.org/abs/1511.05212 | author:Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, Yann LeCun category:cs.LG published:2015-11-16 summary:We consider the hashing mechanism for constructing binary embeddings, thatinvolves pseudo-random projections followed by nonlinear (sign function)mappings. The pseudo-random projection is described by a matrix, where not allentries are independent random variables but instead a fixed "budget ofrandomness" is distributed across the matrix. Such matrices can be efficientlystored in sub-quadratic or even linear space, provide reduction in randomnessusage (i.e. number of required random values), and very often lead tocomputational speed ups. We prove several theoretical results showing thatprojections via various structured matrices followed by nonlinear mappingsaccurately preserve the angular distance between input high-dimensionalvectors. To the best of our knowledge, these results are the first that givetheoretical ground for the use of general structured matrices in the nonlinearsetting. In particular, they generalize previous extensions of theJohnson-Lindenstrauss lemma and prove the plausibility of the approach that wasso far only heuristically confirmed for some special structured matrices.Consequently, we show that many structured matrices can be used as an efficientinformation compression mechanism. Our findings build a better understanding ofcertain deep architectures, which contain randomly weighted and untrainedlayers, and yet achieve high performance on different learning tasks. Weempirically verify our theoretical findings and show the dependence of learningvia structured hashed projections on the performance of neural network as wellas nearest neighbor classifier.
arxiv-14400-51 | Nonparametric Canonical Correlation Analysis | http://arxiv.org/abs/1511.04839 | author:Tomer Michaeli, Weiran Wang, Karen Livescu category:cs.LG stat.ML published:2015-11-16 summary:Canonical correlation analysis (CCA) is a classical representation learningtechnique for finding correlated variables in multi-view data. Severalnonlinear extensions of the original linear CCA have been proposed, includingkernel and deep neural network methods. These approaches seek maximallycorrelated projections among families of functions, which the user specifies(by choosing a kernel or neural network structure), and are computationallydemanding. Interestingly, the theory of nonlinear CCA, without functionalrestrictions, had been studied in the population setting by Lancaster alreadyin the 1950s, but these results have not inspired practical algorithms. Werevisit Lancaster's theory to devise a practical algorithm for nonparametricCCA (NCCA). Specifically, we show that the solution can be expressed in termsof the singular value decomposition of a certain operator associated with thejoint density of the views. Thus, by estimating the population density fromdata, NCCA reduces to solving an eigenvalue system, superficially like kernelCCA but, importantly, without requiring the inversion of any kernel matrix. Wealso derive a partially linear CCA (PLCCA) variant in which one of the viewsundergoes a linear projection while the other is nonparametric. Using a kerneldensity estimate based on a small number of nearest neighbors, our NCCA andPLCCA algorithms are memory-efficient, often run much faster, and performbetter than kernel CCA and comparable to deep CCA.
arxiv-14400-52 | An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors | http://arxiv.org/abs/1511.04695 | author:Linxiao Yang, Jun Fang, Hongbin Li, Bing Zeng category:cs.NA cs.LG published:2015-11-15 summary:We consider the problem of low-rank decomposition of incomplete multiwaytensors. Since many real-world data lie on an intrinsically low dimensionalsubspace, tensor low-rank decomposition with missing entries has applicationsin many data analysis problems such as recommender systems and imageinpainting. In this paper, we focus on Tucker decomposition which represents anNth-order tensor in terms of N factor matrices and a core tensor viamultilinear operations. To exploit the underlying multilinear low-rankstructure in high-dimensional datasets, we propose a group-based log-sumpenalty functional to place structural sparsity over the core tensor, whichleads to a compact representation with smallest core tensor. The method forTucker decomposition is developed by iteratively minimizing a surrogatefunction that majorizes the original objective function, which results in aniterative reweighted process. In addition, to reduce the computationalcomplexity, an over-relaxed monotone fast iterative shrinkage-thresholdingtechnique is adapted and embedded in the iterative reweighted process. Theproposed method is able to determine the model complexity (i.e. multilinearrank) in an automatic way. Simulation results show that the proposed algorithmoffers competitive performance compared with other existing algorithms.
arxiv-14400-53 | Deep Neural Network for Real-Time Autonomous Indoor Navigation | http://arxiv.org/abs/1511.04668 | author:Dong Ki Kim, Tsuhan Chen category:cs.CV published:2015-11-15 summary:Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) possesses manychallenges. One main reason is that GPS has limited precision in indoorenvironments. The additional fact that MAVs are not able to carry heavy weightor power consuming sensors, such as range finders, makes indoor autonomousnavigation a challenging task. In this paper, we propose a practical system inwhich a quadcopter autonomously navigates indoors and finds a specific target,i.e., a book bag, by using a single camera. A deep learning model,Convolutional Neural Network (ConvNet), is used to learn a controller strategythat mimics an expert pilot's choice of action. We show our system'sperformance through real-time experiments in diverse indoor locations. Tounderstand more about our trained network, we use several visualizationtechniques.
arxiv-14400-54 | Expressive recommender systems through normalized nonnegative models | http://arxiv.org/abs/1511.04775 | author:Cyril Stark category:cs.LG stat.ML published:2015-11-15 summary:We introduce normalized nonnegative models (NNM) for explorative dataanalysis. NNMs are partial convexifications of models from probability theory.We demonstrate their value at the example of item recommendation. We show thatNNM-based recommender systems satisfy three criteria that all recommendersystems should ideally satisfy: high predictive power, computationaltractability, and expressive representations of users and items. Expressiveuser and item representations are important in practice to succinctly summarizethe pool of customers and the pool of items. In NNMs, user representations areexpressive because each user's preference can be regarded as normalized mixtureof preferences of stereotypical users. The interpretability of item and userrepresentations allow us to arrange properties of items (e.g., genres of moviesor topics of documents) or users (e.g., personality traits) hierarchically.
arxiv-14400-55 | Learning Representations of Affect from Speech | http://arxiv.org/abs/1511.04747 | author:Sayan Ghosh, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer category:cs.CL cs.LG published:2015-11-15 summary:There has been a lot of prior work on representation learning for speechrecognition applications, but not much emphasis has been given to aninvestigation of effective representations of affect from speech, where theparalinguistic elements of speech are separated out from the verbal content. Inthis paper, we explore denoising autoencoders for learning paralinguisticattributes i.e. categorical and dimensional affective traits from speech. Weshow that the representations learnt by the bottleneck layer of the autoencoderare highly discriminative of activation intensity and at separating outnegative valence (sadness and anger) from positive valence (happiness). Weexperiment with different input speech features (such as FFT and log-melspectrograms with temporal context windows), and different autoencoderarchitectures (such as stacked and deep autoencoders). We also learn utterancespecific representations by a combination of denoising autoencoders and BLSTMbased recurrent autoencoders. Emotion classification is performed with thelearnt temporal/dynamic representations to evaluate the quality of therepresentations. Experiments on a well-established real-life speech dataset(IEMOCAP) show that the learnt representations are comparable to state of theart feature extractors (such as voice quality features and MFCCs) and arecompetitive with state-of-the-art approaches at emotion and dimensional affectrecognition.
arxiv-14400-56 | Separation Surfaces in the Spectral TV Domain for Texture Decomposition | http://arxiv.org/abs/1511.04687 | author:Dikla Horesh, Guy Gilboa category:cs.CV math.SP published:2015-11-15 summary:In this paper we introduce a novel notion of separation surfaces for imagedecomposition. A surface is embedded in the spectral total-variation (TV) threedimensional domain and encodes a spatially-varying separation scale. The methodallows good separation of textures with gradually varying pattern-size,pattern-contrast or illumination. The recently proposed total variationspectral framework is used to decompose the image into a continuum of texturalscales. A desired texture, within a scale range, is found by fitting a surfaceto the local maximal responses in the spectral domain. A band above and belowthe surface, referred to as the \textit{Texture Stratum}, defines for eachpixel the adaptive scale-range of the texture. Based on the decomposition anapplication is proposed which can attenuate or enhance textures in the image ina very natural and visually convincing manner.
arxiv-14400-57 | Semi-Inner-Products for Convex Functionals and Their Use in Image Decomposition | http://arxiv.org/abs/1511.04685 | author:Guy Gilboa category:math.NA cs.CV math.SP published:2015-11-15 summary:Semi-inner-products in the sense of Lumer are extended to convex functionals.This yields a Hilbert-space like structure to convex functionals in Banachspaces. In particular, a general expression for semi-inner-products withrespect to one homogeneous functionals is given. Thus one can use the newoperator for the analysis of total variation and higher order functionals liketotal-generalized-variation (TGV). Having a semi-inner-product, an anglebetween functions can be defined in a straightforward manner. It is shown thatin the one homogeneous case the Bregman distance can be expressed in terms ofthis newly defined angle. In addition, properties of the semi-inner-product ofnonlinear eigenfunctions induced by the functional are derived. We use thisconstruction to state a sufficient condition for a perfect decomposition of twosignals and suggest numerical measures which indicate when those conditions areapproximately met.
arxiv-14400-58 | Uncovering Temporal Context for Video Question and Answering | http://arxiv.org/abs/1511.04670 | author:Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G. Hauptmann category:cs.CV published:2015-11-15 summary:In this work, we introduce Video Question Answering in temporal domain toinfer the past, describe the present and predict the future. We present anencoder-decoder approach using Recurrent Neural Networks to learn temporalstructures of videos and introduce a dual-channel ranking loss to answermultiple-choice questions. We explore approaches for finer understanding ofvideo content using question form of "fill-in-the-blank", and managed tocollect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD,MEDTest 14 datasets, while the corresponding 390,744 questions are generatedfrom annotations. Extensive experiments demonstrate that our approachsignificantly outperforms the compared baselines.
arxiv-14400-59 | Deep Activity Recognition Models with Triaxial Accelerometers | http://arxiv.org/abs/1511.04664 | author:Mohammad Abu Alsheikh, Ahmed Selim, Dusit Niyato, Linda Doyle, Shaowei Lin, Hwee-Pink Tan category:cs.LG cs.HC cs.NE published:2015-11-15 summary:Despite the widespread installation of accelerometers in almost all mobilephones and wearable devices, activity recognition using accelerometers is stillimmature due to the poor recognition accuracy of existing recognition methodsand the scarcity of labeled training data. We consider the problem of humanactivity recognition using triaxial accelerometers and deep learning paradigms.This paper shows that deep activity recognition models (a) provide betterrecognition accuracy of human activities, (b) avoid the expensive design ofhandcrafted features in existing systems, and (c) utilize the massive unlabeledacceleration samples for unsupervised feature extraction. Moreover, a hybridapproach of deep learning and hidden Markov models (DL-HMM) is presented forsequential activity recognition. This hybrid approach integrates thehierarchical representations of deep activity recognition models with thestochastic modeling of temporal sequences in the hidden Markov models. We showsubstantial recognition improvement on real world datasets overstate-of-the-art methods of human activity recognition using triaxialaccelerometers.
arxiv-14400-60 | A System for Extracting Sentiment from Large-Scale Arabic Social Data | http://arxiv.org/abs/1511.04661 | author:Hao Wang, Vijay R. Bommireddipalli, Ayman Hanafy, Mohamed Bahgat, Sara Noeman, Ossama S. Emam category:cs.CL published:2015-11-15 summary:Social media data in Arabic language is becoming more and more abundant. Itis a consensus that valuable information lies in social media data. Mining thisdata and making the process easier are gaining momentum in the industries. Thispaper describes an enterprise system we developed for extracting sentiment fromlarge volumes of social data in Arabic dialects. First, we give an overview ofthe Big Data system for information extraction from multilingual social datafrom a variety of sources. Then, we focus on the Arabic sentiment analysiscapability that was built on top of the system including normalizing writtenArabic dialects, building sentiment lexicons, sentiment classification, andperformance evaluation. Lastly, we demonstrate the value of enriching sentimentresults with user profiles in understanding sentiments of a specific usergroup.
arxiv-14400-61 | Complete Dictionary Recovery over the Sphere II: Recovery by Riemannian Trust-region Method | http://arxiv.org/abs/1511.04777 | author:Ju Sun, Qing Qu, John Wright category:cs.IT cs.CV math.IT math.OC stat.ML published:2015-11-15 summary:We consider the problem of recovering a complete (i.e., square andinvertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb{R}^{n \times p}$with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ issufficiently sparse. This recovery problem is central to the theoreticalunderstanding of dictionary learning, which seeks a sparse representation for acollection of input signals, and finds numerous applications in modern signalprocessing and machine learning. We give the first efficient algorithm thatprovably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros percolumn, under suitable probability model for $\mathbf X_0$. Our algorithmic pipeline centers around solving a certain nonconvexoptimization problem with a spherical constraint, and hence is naturallyphrased in the language of manifold optimization. In a companion paper(arXiv:1511.03607), we have showed that with high probability our nonconvexformulation has no "spurious" local minimizers and any saddle point present issecond-order. In this paper, we take advantage of the particular geometricstructure and design a Riemannian trust region algorithm over the sphere thatprovably converges to a local minimizer with an arbitrary initialization. Suchminimizers give excellent approximations to rows of $\mathbf X_0$. The rows arerecovered by linear programming rounding and deflation.
arxiv-14400-62 | Implementation and comparative quantitative assessment of different multispectral image pansharpening approches | http://arxiv.org/abs/1511.04659 | author:Shailesh Panchal, Rajesh Thakker category:cs.CV published:2015-11-15 summary:In remote sensing, images acquired by various earth observation satellitestend to have either a high spatial and low spectral resolution or vice versa.Pansharpening is a technique which aims to improve spatial resolution ofmultispectral image. The challenges involve in the pansharpening are not onlyto improve the spatial resolution but also to preserve spectral quality of themultispectral image. In this paper, various pansharpening algorithms arediscussed and classified based on approaches they have adopted. Using MATLABimage processing toolbox, several state-of-art pan-sharpening algorithms areimplemented. Quality of pansharpened images are assessed visually andquantitatively. Correlation coefficient (CC), Root mean square error (RMSE),Relative average spectral error (RASE) and Universal quality index (Q) indicesare used to easure spectral quality while to spatial-CC (SCC) quantitativeparameter is used for spatial quality measurement. Finally, the paper isconcluded with useful remarks.
arxiv-14400-63 | Mixtures of Sparse Autoregressive Networks | http://arxiv.org/abs/1511.04776 | author:Marc Goessling, Yali Amit category:stat.ML cs.LG published:2015-11-15 summary:We consider high-dimensional distribution estimation through autoregressivenetworks. By combining the concepts of sparsity, mixtures and parameter sharingwe obtain a simple model which is fast to train and which achievesstate-of-the-art or better results on several standard benchmark datasets.Specifically, we use an L1-penalty to regularize the conditional distributionsand introduce a procedure for automatic parameter sharing between mixturecomponents. Moreover, we propose a simple distributed representation whichpermits exact likelihood evaluations since the latent variables are interleavedwith the observable variables and can be easily integrated out. Our modelachieves excellent generalization performance and scales well to extremely highdimensions.
arxiv-14400-64 | Causal interpretation rules for encoding and decoding models in neuroimaging | http://arxiv.org/abs/1511.04780 | author:Sebastian Weichwald, Timm Meyer, Ozan Özdenizci, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML cs.LG q-bio.NC stat.AP published:2015-11-15 summary:Causal terminology is often introduced in the interpretation of encoding anddecoding models trained on neuroimaging data. In this article, we investigatewhich causal statements are warranted and which ones are not supported byempirical evidence. We argue that the distinction between encoding and decodingmodels is not sufficient for this purpose: relevant features in encoding anddecoding models carry a different meaning in stimulus- and in response-basedexperimental paradigms. We show that only encoding models in the stimulus-basedsetting support unambiguous causal interpretations. By combining encoding anddecoding models trained on the same data, however, we obtain insights intocausal relations beyond those that are implied by each individual model type.We illustrate the empirical relevance of our theoretical findings on EEG datarecorded during a visuo-motor learning task.
arxiv-14400-65 | Word Embedding based Correlation Model for Question/Answer Matching | http://arxiv.org/abs/1511.04646 | author:Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng, Jie Tang, Zhang Xiong category:cs.CL cs.AI published:2015-11-15 summary:With the development of community based question answering (Q\&A) services, alarge scale of Q\&A archives have been accumulated and are an importantinformation and knowledge resource on the web. Question and answer matching hasbeen attached much importance to for its ability to reuse knowledge stored inthese systems: it can be useful in enhancing user experience with recurrentquestions. In this paper, we try to improve the matching accuracy by overcomingthe lexical gap between question and answer pairs. A Word Embedding basedCorrelation (WEC) model is proposed by integrating advantages of both thetranslation model and word embedding, given a random pair of words, WEC canscore their co-occurrence probability in Q\&A pairs and it can also leveragethe continuity and smoothness of continuous space word representation to dealwith new pairs of words that are rare in the training parallel text. Anexperimental study on Yahoo! Answers dataset and Baidu Zhidao dataset showsthis new method's promising potential.
arxiv-14400-66 | Large-Scale Approximate Kernel Canonical Correlation Analysis | http://arxiv.org/abs/1511.04773 | author:Weiran Wang, Karen Livescu category:cs.LG published:2015-11-15 summary:Kernel canonical correlation analysis (KCCA) is a nonlinear multi-viewrepresentation learning technique with broad applicability in statistics andmachine learning. Although there is a closed-form solution for the KCCAobjective, it involves solving an $N\times N$ eigenvalue system where $N$ isthe training set size, making its computational requirements in both memory andtime prohibitive for large-scale problems. Various approximation techniqueshave been developed for KCCA. A commonly used approach is to first transformthe original inputs to an $M$-dimensional random feature space so that innerproducts in the feature space approximate kernel evaluations, and then applylinear CCA to the transformed inputs. In many applications, however, thedimensionality $M$ of the random feature space may need to be very large inorder to obtain a sufficiently good approximation; it then becomes challengingto perform the linear CCA step on the resulting very high-dimensional datamatrices. We show how to use a stochastic optimization algorithm, recentlyproposed for linear CCA and its neural-network extension, to further alleviatethe computation requirements of approximate KCCA. This approach allows us torun approximate KCCA on a speech dataset with $1.4$ million training samplesand a random feature space of dimensionality $M=100000$ on a typicalworkstation.
arxiv-14400-67 | Deep Linear Discriminant Analysis | http://arxiv.org/abs/1511.04707 | author:Matthias Dorfer, Rainer Kelz, Gerhard Widmer category:cs.LG published:2015-11-15 summary:We introduce Deep Linear Discriminant Analysis (DeepLDA) which learnslinearly separable latent representations in an end-to-end fashion. Classic LDAextracts features which preserve class separability and is used fordimensionality reduction for many classification problems. The central idea ofthis paper is to put LDA on top of a deep neural network. This can be seen as anon-linear extension of classic LDA. Instead of maximizing the likelihood oftarget labels for individual samples, we propose an objective function thatpushes the network to produce feature distributions which: (a) have lowvariance within the same class and (b) high variance between different classes.Our objective is derived from the general LDA eigenvalue problem and stillallows to train with stochastic gradient descent and back-propagation. Forevaluation we test our approach on three different benchmark datasets (MNIST,CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST andCIFAR-10 and outperforms a network trained with categorical cross entropy (samearchitecture) on a supervised setting of STL-10.
arxiv-14400-68 | Robust Elastic Net Regression | http://arxiv.org/abs/1511.04690 | author:Weiyang Liu, Rongmei Lin, Meng Yang category:cs.LG stat.ML published:2015-11-15 summary:We propose a robust elastic net (REN) model for high-dimensional sparseregression and give its performance guarantees (both the statistical errorbound and the optimization bound). A simple idea of trimming the inner productis applied to the elastic net model. Specifically, we robustify the covariancematrix by trimming the inner product based on the intuition that the trimmedinner product can not be significant affected by a bounded number ofarbitrarily corrupted points (outliers). The REN model can also derive twointeresting special cases: robust Lasso and robust soft thresholding.Comprehensive experimental results show that the robustness of the proposedmodel consistently outperforms the original elastic net and matches theperformance guarantees nicely.
arxiv-14400-69 | Accurate Image Super-Resolution Using Very Deep Convolutional Networks | http://arxiv.org/abs/1511.04587 | author:Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee category:cs.CV published:2015-11-14 summary:We present a highly accurate single-image super-resolution (SR) method. Ourmethod uses a very deep convolutional network inspired by VGG-net used forImageNet classification \cite{simonyan2015very}. We find increasing our networkdepth shows a significant improvement in accuracy. Our final model uses 20weight layers. By cascading small filters many times in a deep networkstructure, contextual information over large image regions is exploited in anefficient way. With very deep networks, however, convergence speed becomes acritical issue during training. We propose a simple yet effective trainingprocedure. We learn residuals only and use extremely high learning rates($10^4$ times higher than SRCNN \cite{dong2015image}) enabled by adjustablegradient clipping. Our proposed method performs better than existing methods inaccuracy and visual improvements in our results are easily noticeable.
arxiv-14400-70 | Fast Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting | http://arxiv.org/abs/1511.05133 | author:Canyi Lu, Huan Li, Zhouchen Lin, Shuicheng Yan category:math.OC cs.LG cs.NA published:2015-11-14 summary:The Augmented Lagragian Method (ALM) and Alternating Direction Method ofMultiplier (ADMM) have been powerful optimization methods for general convexprogramming subject to linear constraint. We consider the convex problem whoseobjective consists of a smooth part and a nonsmooth but simple part. We proposethe Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves theconvergence rate $O(1/K^2)$, compared with $O(1/K)$ by the traditional PALM. Inorder to further reduce the per-iteration complexity and handle themulti-blocks problem, we propose the Fast Proximal ADMM with Parallel Splitting(Fast PL-ADMM-PS) method. It also partially improves the rate related to thesmooth part of the objective function. Experimental results on both synthesizedand real world data demonstrate that our fast methods significantly improve theprevious PALM and ADMM.
arxiv-14400-71 | Learning Fine-grained Features via a CNN Tree for Large-scale Classification | http://arxiv.org/abs/1511.04534 | author:Zhenhua Wang, Xingxing Wang, Gang Wang category:cs.CV published:2015-11-14 summary:We propose a novel approach to enhance the discriminability of ConvolutionalNeural Networks (CNN). The key idea is to build a tree structure that couldprogressively learn fine-grained features to distinguish a subset of classes,by learning features only among these classes. Such features are expected to bemore discriminative, compared to features learned for all the classes. Wedevelop a new algorithm to effectively learn the tree structure among a largenumber of classes. Experiments on large-scale image classification tasksdemonstrate that our method could boost the performance of a given basic CNNmodel. Our method is quite general, hence it can potentially be used incombination with many other deep learning models.
arxiv-14400-72 | Sparse Nonlinear Regression: Parameter Estimation and Asymptotic Inference | http://arxiv.org/abs/1511.04514 | author:Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, Tong Zhang category:stat.ML cs.IT cs.LG math.IT math.OC published:2015-11-14 summary:We study parameter estimation and asymptotic inference for sparse nonlinearregression. More specifically, we assume the data are given by $y = f( x^\top\beta^* ) + \epsilon$, where $f$ is nonlinear. To recover $\beta^*$, we proposean $\ell_1$-regularized least-squares estimator. Unlike classical linearregression, the corresponding optimization problem is nonconvex because of thenonlinearity of $f$. In spite of the nonconvexity, we prove that under mildconditions, every stationary point of the objective enjoys an optimalstatistical rate of convergence. In addition, we provide an efficient algorithmthat provably converges to a stationary point. We also access the uncertaintyof the obtained estimator. Specifically, based on any stationary point of theobjective, we construct valid hypothesis tests and confidence intervals for thelow dimensional components of the high-dimensional parameter $\beta^*$.Detailed numerical results are provided to back up our theory.
arxiv-14400-73 | BING++: A Fast High Quality Object Proposal Generator at 100fps | http://arxiv.org/abs/1511.04511 | author:Ziming Zhang, Yun Liu, Tolga Bolukbasi, Ming-Ming Cheng, Venkatesh Saligrama category:cs.CV published:2015-11-14 summary:We are motivated by the need for an object proposal generation algorithm thatachieves a good balance between proposal localization quality, object recalland computational efficiency. We propose a novel object proposal algorithm {\emBING++} which inherits the good computational efficiency of BING\cite{BingObj2014} but significantly improves its proposal localizationquality. Central to our success is based on the observation that good boundingboxes are those that tightly cover objects. Edge features, which can becomputed efficiently, play a critical role in this context. We propose a newalgorithm that recursively improves BING's proposals by exploiting the factthat edges in images are typically associated with object boundaries. BING++improves proposals recursively by incorporating nearest edge points (toproposal boundary pixels) to obtain a tighter bounding box. This operation haslinear computational complexity in number of pixels and can be done efficientlyusing distance transform. Superpixel merging techniques are then employed aspost-processing to further improve the proposal quality. Empirically on theVOC2007 dataset, using $10^3$ proposals and IoU threshold 0.5, our methodachieves 95.3\% object detection recall (DR), 79.2\% mean average best overlap(MABO), and 68.7\% mean average precision (mAP) on object detection over 20object classes within an average time of {\bf 0.009} seconds per image.
arxiv-14400-74 | Semantic Object Parsing with Local-Global Long Short-Term Memory | http://arxiv.org/abs/1511.04510 | author:Xiaodan Liang, Xiaohui Shen, Donglai Xiang, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV published:2015-11-14 summary:Semantic object parsing is a fundamental task for understanding objects indetail in computer vision community, where incorporating multi-level contextualinformation is critical for achieving such fine-grained pixel-levelrecognition. Prior methods often leverage the contextual information throughpost-processing predicted confidence maps. In this work, we propose a noveldeep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlesslyincorporate short-distance and long-distance spatial dependencies into thefeature learning over all pixel positions. In each LG-LSTM layer, localguidance from neighboring positions and global guidance from the whole imageare imposed on each position to better exploit complex local and globalcontextual information. Individual LSTMs for distinct spatial dimensions arealso utilized to intrinsically capture various spatial layouts of semanticparts in the images, yielding distinct hidden and memory cells of each positionfor each dimension. In our parsing approach, several LG-LSTM layers are stackedand appended to the intermediate convolutional layers to directly enhancevisual features, allowing network parameters to be learned in an end-to-endway. The long chains of sequential computation by stacked LG-LSTM layers alsoenable each pixel to sense a much larger region for inference benefiting fromthe memorization of previous dependencies in all positions along alldimensions. Comprehensive evaluations on three public datasets well demonstratethe significant superiority of our LG-LSTM over other state-of-the-art methods.
arxiv-14400-75 | Jointly Learning Non-negative Projection and Dictionary with Discriminative Graph Constraints for Classification | http://arxiv.org/abs/1511.04601 | author:Weiyang Liu, Zhiding Yu, Yingzhen Yang, Meng Yang, Thomas S. Huang category:cs.CV published:2015-11-14 summary:Dictionary learning (DL) for sparse coding has shown impressive performancein classification tasks. But how to select a feature that can best work withthe learned dictionary remains an open question. Current prevailing DL methodsusually adopt existing well-performing features, ignoring the innerrelationship between dictionaries and features. To address the problem, wepropose a joint non-negative projection and dictionary learning (JNPDL) method.Non-negative projection learning and dictionary learning are complementary toeach other, since the former leads to the intrinsic discriminative parts-basedfeatures for objects while the latter searches a suitable representation in theprojected feature space. Specifically, discrimination of projection anddictionary is achieved by imposing to both projection and coding coefficients agraph constraint that maximizes the intra-class compactness and inter-classseparability. Experimental results on both image classification and image setclassification show the excellent performance of JNPDL by being comparable oroutperforming many state-of-the-art approaches.
arxiv-14400-76 | Deeply-Recursive Convolutional Network for Image Super-Resolution | http://arxiv.org/abs/1511.04491 | author:Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee category:cs.CV published:2015-11-14 summary:We propose an image super-resolution method (SR) using a deeply-recursiveconvolutional network (DRCN). Our network has a very deep recursive layer (upto 16 recursions). Increasing recursion depth can improve performance withoutintroducing new parameters for additional convolutions. Albeit advantages,learning a DRCN is very hard with a standard gradient descent method due toexploding/vanishing gradients. To ease the difficulty of training, we proposetwo extensions: recursive-supervision and skip-connection. Our methodoutperforms previous methods by a large margin.
arxiv-14400-77 | Reversible Recursive Instance-level Object Segmentation | http://arxiv.org/abs/1511.04517 | author:Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Zequn Jie, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV published:2015-11-14 summary:In this work, we propose a novel Reversible Recursive Instance-level ObjectSegmentation (R2-IOS) framework to address the challenging instance-levelobject segmentation task. R2-IOS consists of a reversible proposal refinementsub-network that predicts bounding box offsets for refining the object proposallocations, and an instance-level segmentation sub-network that generates theforeground mask of the dominant object instance in each proposal. By beingrecursive, R2-IOS iteratively optimizes the two sub-networks during jointtraining, in which the refined object proposals and improved segmentationpredictions are alternately fed into each other to progressively increase thenetwork capabilities. By being reversible, the proposal refinement sub-networkadaptively determines an optimal number of refinement iterations required foreach proposal during both training and testing. Furthermore, to handle multipleoverlapped instances within a proposal, an instance-aware denoising autoencoderis introduced into the segmentation sub-network to distinguish the dominantobject from other distracting instances. Extensive experiments on thechallenging PASCAL VOC 2012 benchmark well demonstrate the superiority ofR2-IOS over other state-of-the-art methods. In particular, the $\text{AP}^r$over $20$ classes at $0.5$ IoU achieves $66.7\%$, which significantlyoutperforms the results of $58.7\%$ by PFN~\cite{PFN} and $46.3\%$by~\cite{liu2015multi}.
arxiv-14400-78 | Unsupervised Learning in Synaptic Sampling Machines | http://arxiv.org/abs/1511.04484 | author:Emre O. Neftci, Bruno U. Pedroni, Siddharth Joshi, Maruan Al-Shedivat, Gert Cauwenberghs category:cs.NE published:2015-11-14 summary:Recent studies have shown that synaptic unreliability is a robust andsufficient mechanism for inducing the stochasticity observed in cortex. Here,we introduce the Synaptic Sampling Machine (SSM), a stochastic neural networkmodel that uses synaptic unreliability as a means to stochasticity forsampling. Synaptic unreliability plays the dual role of an efficient mechanismfor sampling in neuromorphic hardware, and a regularizer during learning akinto DropConnect. Similar to the original formulation of Boltzmann machines, theSSM can be viewed as a stochastic counterpart of Hopfield networks, but wherestochasticity is induced by a random mask over the connections. The SSM istrained to learn generative models with a synaptic plasticity rule implementingan event-driven form of contrastive divergence. We demonstrate this by learninga model of MNIST hand-written digit dataset, and by testing it in recognitionand inference tasks. We find that SSMs outperform restricted Boltzmann machines(4.4% error rate vs. 5%), they are more robust to overfitting, and tend tolearn sparser representations. SSMs are remarkably robust to weight pruning:removal of more than 80% of the weakest connections followed by cursoryre-learning causes only a negligible performance loss on the MNIST task (4.8%error rate). These results show that SSMs offer substantial improvements interms of performance, power and complexity over existing methods forunsupervised learning in spiking neural networks, and are thus promising modelsfor machine learning in neuromorphic execution platforms.
arxiv-14400-79 | Learning to Represent Words in Context with Multilingual Supervision | http://arxiv.org/abs/1511.04623 | author:Kazuya Kawakami, Chris Dyer category:cs.CL published:2015-11-14 summary:We present a neural network architecture based on bidirectional LSTMs tocompute representations of words in the sentential contexts. Thesecontext-sensitive word representations are suitable for, e.g., distinguishingdifferent word senses and other context-modulated variations in meaning. Tolearn the parameters of our model, we use cross-lingual supervision,hypothesizing that a good representation of a word in context will be one thatis sufficient for selecting the correct translation into a second language. Weevaluate the quality of our representations as features in three downstreamtasks: prediction of semantic supersenses (which assign nouns and verbs into afew dozen semantic classes), low resource machine translation, and a lexicalsubstitution task, and obtain state-of-the-art results on all of these.
arxiv-14400-80 | A Test of Relative Similarity For Model Selection in Generative Models | http://arxiv.org/abs/1511.04581 | author:Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, Arthur Gretton category:stat.ML cs.LG published:2015-11-14 summary:Probabilistic generative models provide a powerful framework for representingdata that avoids the expense of manual annotation typically needed bydiscriminative approaches. Model selection in this generative setting can bechallenging, however, particularly when likelihoods are not easily accessible.To address this issue, we introduce a statistical test of relative similarity,which is used to determine which of two models generates samples that aresignificantly closer to a real-world reference dataset of interest. We use asour test statistic the difference in maximum mean discrepancies (MMDs) betweenthe reference dataset and each model dataset, and derive a powerful,low-variance test based on the joint asymptotic distribution of the MMDsbetween each reference-model pair. In experiments on deep generative models,including the variational auto-encoder and generative moment matching network,the tests provide a meaningful ranking of model performance as a function ofparameter and training settings.
arxiv-14400-81 | Character-based Neural Machine Translation | http://arxiv.org/abs/1511.04586 | author:Wang Ling, Isabel Trancoso, Chris Dyer, Alan W Black category:cs.CL published:2015-11-14 summary:We introduce a neural machine translation model that views the input andoutput sentences as sequences of characters rather than words. Since word-levelinformation provides a crucial source of bias, our input model composesrepresentations of character sequences into representations of words (asdetermined by whitespace boundaries), and then these are translated using ajoint attention/translation model. In the target language, the translation ismodeled as a sequence of word vectors, but each word is generated one characterat a time, conditional on the previous character generations in each word. Asthe representation and generation of words is performed at the character level,our model is capable of interpreting and generating unseen word forms. Asecondary benefit of this approach is that it alleviates much of the challengesassociated with preprocessing/tokenization of the source and target languages.We show that our model can achieve translation results that are on par withconventional word-based models.
arxiv-14400-82 | Zero-Shot Learning via Joint Latent Similarity Embedding | http://arxiv.org/abs/1511.04512 | author:Ziming Zhang, Venkatesh Saligrama category:cs.CV published:2015-11-14 summary:Zero-shot recognition (ZSR) deals with the problem of predicting class labelsfor target domain instances based on source domain side information (e.g.attributes) of unseen classes. We formulate ZSR as a binary prediction problem.Our resulting classifier is class-independent. It takes an arbitrary pair ofsource and target domain instances as input and predicts whether or not theycome from the same class, i.e. whether there is a match. We model the posteriorprobability of a match since it is a sufficient statistic and propose a latentprobabilistic model in this context. We develop a joint discriminative learningframework based on dictionary learning to jointly learn the parameters of ourmodel for both domains, which ultimately leads to our class-independentclassifier. Many of the existing embedding methods can be viewed as specialcases of our probabilistic model. On ZSR our method shows 4.90\% improvementover the state-of-the-art in accuracy averaged across four benchmark datasets.We also adapt ZSR method for zero-shot retrieval and show 22.45\% improvementaccordingly in mean average precision (mAP).
arxiv-14400-83 | Deep Reinforcement Learning with an Action Space Defined by Natural Language | http://arxiv.org/abs/1511.04636 | author:Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf category:cs.AI cs.CL cs.LG published:2015-11-14 summary:In this paper, we propose the deep reinforcement relevance network (DRRN), anovel deep architecture, to design a model for handling an action spacecharacterized using natural language with applications to text-based games. Fora particular class of games, a user must choose among a number of actionsdescribed by text, with the goal of maximizing long-term reward. In thesegames, the best action is typically what fits the current situation best(modeled as a state in the DRRN), also described by text. Because of theexponential complexity of natural language with respect to sentence length,there is typically an unbounded set of unique actions. Even with a constrainedvocabulary, the action space is very large and sparse, posing challenges forlearning. To address this challenge, the DRRN extracts separate high-levelembedding vectors from the texts that describe states and actions,respectively, using a general interaction function, such as inner product,bilinear, and DNN interaction, between these embedding vectors to approximatethe Q-function. We evaluate the DRRN on two popular text games, showingsuperior performance over other deep Q-learning architectures.
arxiv-14400-84 | 8-Bit Approximations for Parallelism in Deep Learning | http://arxiv.org/abs/1511.04561 | author:Tim Dettmers category:cs.NE cs.LG published:2015-11-14 summary:The creation of practical deep learning data-products often requiresparallelization across processors and computers to make deep learning feasibleon large data sets, but bottlenecks in communication bandwidth make itdifficult to attain good speedups through parallelism. Here we develop and test8-bit approximation algorithms which make better use of the available bandwidthby compressing 32-bit gradients and nonlinear activations to 8-bitapproximations. We show that these approximations do not decrease predictiveperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelismand provide a data transfer speedup of 2x relative to 32-bit parallelism. Webuild a predictive model for speedups based on our experimental data, verifyits validity on known speedup data, and show that we can obtain a speedup of50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. Wecompare our data types with other methods and show that 8-bit approximationsachieve state-of-the-art speedups for model parallelism. Thus 8-bitapproximation is an efficient method to parallelize convolutional networks onvery large systems of GPUs.
arxiv-14400-85 | DeepFool: a simple and accurate method to fool deep neural networks | http://arxiv.org/abs/1511.04599 | author:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard category:cs.LG published:2015-11-14 summary:State-of-the-art deep neural networks have achieved impressive results onmany image classification tasks. However, these same architectures have beenshown to be unstable to small, well sought, perturbations of the images.Despite the importance of this phenomenon, no effective methods have beenproposed to accurately compute the robustness of state-of-the-art deepclassifiers to such perturbations on large-scale datasets. In this paper, wefill this gap and propose the DeepFool algorithm to efficiently computeperturbations that fool deep networks, and thus reliably quantify therobustness of these classifiers. Extensive experimental results show that ourapproach outperforms recent methods in the task of computing adversarialperturbations and making classifiers more robust.
arxiv-14400-86 | Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks | http://arxiv.org/abs/1511.04508 | author:Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami category:cs.CR cs.LG cs.NE stat.ML published:2015-11-14 summary:Deep learning algorithms have been shown to perform extremely well on manyclassical machine learning problems. However, recent studies have shown thatdeep learning, like other machine learning techniques, is vulnerable toadversarial samples: inputs crafted to force a deep neural network (DNN) toprovide adversary-selected outputs. Such attacks can seriously undermine thesecurity of the system supported by the DNN, sometimes with devastatingconsequences. For example, autonomous vehicles can be crashed, illicit orillegal content can bypass content filters, or biometric authentication systemscan be manipulated to allow improper access. In this work, we introduce adefensive mechanism called defensive distillation to reduce the effectivenessof adversarial samples on DNNs. We analytically investigate thegeneralizability and robustness properties granted by the use of defensivedistillation when training DNNs. We also empirically study the effectiveness ofour defense mechanisms on two DNNs placed in adversarial settings. The studyshows that defensive distillation can reduce effectiveness of sample creationfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can beexplained by the fact that distillation leads gradients used in adversarialsample creation to be reduced by a factor of 10^30. We also find thatdistillation increases the average minimum number of features that need to bemodified to create adversarial samples by about 800% on one of the DNNs wetested.
arxiv-14400-87 | Efficient Training of Very Deep Neural Networks for Supervised Hashing | http://arxiv.org/abs/1511.04524 | author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV cs.LG cs.NE published:2015-11-14 summary:In this paper, we propose training very deep neural networks (DNNs) forsupervised learning of hash codes. Existing methods in this context trainrelatively "shallow" networks limited by the issues arising in back propagation(e.e. vanishing gradients) as well as computational efficiency. We propose anovel and efficient training algorithm inspired by alternating direction methodof multipliers (ADMM) that overcomes some of these limitations. Our methoddecomposes the training process into independent layer-wise local updatesthrough auxiliary variables. Empirically we observe that our training algorithmalways converges and its computational complexity is linearly proportional tothe number of edges in the networks. Empirically we manage to train DNNs with64 hidden layers and 1024 nodes per layer for supervised hashing in about 3hours using a single GPU. Our proposed very deep supervised hashing (VDSH)method significantly outperforms the state-of-the-art on several benchmarkdatasets.
arxiv-14400-88 | Empirical performance upper bounds for image and video captioning | http://arxiv.org/abs/1511.04590 | author:Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio category:cs.CV cs.CL stat.ML published:2015-11-14 summary:The task of associating images and videos with a natural language descriptionhas attracted a great amount of attention recently. Rapid progress has beenmade in terms of both developing novel algorithms and releasing new datasets.Indeed, the state-of-the-art results on some of the standard datasets have beenpushed into the regime where it has become more and more difficult to makesignificant improvements. Instead of proposing new models, this workinvestigates the possibility of empirically establishing performance upperbounds on various visual captioning datasets without extra data labellingeffort or human evaluation. In particular, it is assumed that visual captioningis decomposed into two steps: from visual inputs to visual concepts, and fromvisual concepts to natural language descriptions. One would be able to obtainan upper bound when assuming the first step is perfect and only requiringtraining a conditional language model for the second step. We demonstrate theconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combinationof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we usedfor visual concept extraction in the first step and the simplicity of thelanguage model for the second step, we show that current state-of-the-artmodels fall short when being compared with the learned upper bounds.Furthermore, with such a bound, we quantify several important factorsconcerning image and video captioning: the number of visual concepts capturedby different models, the trade-off between the amount of visual elementscaptured and their accuracy, and the intrinsic difficulty and blessing ofdifferent datasets.
arxiv-14400-89 | Dynamic Sum Product Networks for Tractable Inference on Sequence Data | http://arxiv.org/abs/1511.04412 | author:Mazen Melibari, Pascal Poupart, Prashant Doshi category:cs.LG cs.AI stat.ML published:2015-11-13 summary:Sum-Product Networks (SPN) have recently emerged as a new class of tractableprobabilistic graphical models. Unlike Bayesian networks and Markov networkswhere inference may be exponential in the size of the network, inference inSPNs is in time linear in the size of the network. Since SPNs representdistributions over a fixed set of variables only, we propose dynamic sumproduct networks (DSPNs) as a generalization of SPNs for sequence data ofvarying length. A DSPN consists of a template network that is repeated as manytimes as needed to model data sequences of any length. We present a localsearch technique to learn the structure of the template network. In contrast todynamic Bayesian networks for which inference is generally exponential in thenumber of variables per time slice, DSPNs inherit the linear inferencecomplexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and othermodels on several datasets of sequence data.
arxiv-14400-90 | Scalable Gaussian Processes for Characterizing Multidimensional Change Surfaces | http://arxiv.org/abs/1511.04408 | author:William Herlands, Andrew Wilson, Hannes Nickisch, Seth Flaxman, Daniel Neill, Wilbert van Panhuis, Eric Xing category:stat.ML published:2015-11-13 summary:We present a scalable Gaussian process model for identifying andcharacterizing smooth multidimensional changepoints, and automatically learningchanges in expressive covariance structure. We use Random Kitchen Sink featuresto flexibly define a change surface in combination with expressive spectralmixture kernels to capture the complex statistical structure. Finally, throughthe use of novel methods for additive non-separable kernels, we can scale themodel to large datasets. We demonstrate the model on numerical and real worlddata, including a large spatio-temporal disease dataset where we identifypreviously unknown heterogeneous changes in space and time.
arxiv-14400-91 | Deep Reinforcement Learning in Parameterized Action Space | http://arxiv.org/abs/1511.04143 | author:Matthew Hausknecht, Peter Stone category:cs.AI cs.LG cs.MA cs.NE published:2015-11-13 summary:Recent work has shown that deep neural networks are capable of approximatingboth value functions and policies in reinforcement learning domains featuringcontinuous state and action spaces. However, to the best of our knowledge noprevious work has succeeded at using deep neural networks in structured(parameterized) continuous action spaces. To fill this gap, this paper focuseson learning within the domain of simulated RoboCup soccer, which features asmall set of discrete action types, each of which is parameterized withcontinuous variables. The best learned agent can score goals more reliably thanthe 2012 RoboCup champion agent. As such, this paper represents a successfulextension of deep reinforcement learning to the class of parameterized actionspace MDPs.
arxiv-14400-92 | Learning to Assign Orientations to Feature Points | http://arxiv.org/abs/1511.04273 | author:Kwang Moo Yi, Yannick Verdie, Pascal Fua, Vincent Lepetit category:cs.CV published:2015-11-13 summary:We show how to train a Convolutional Neural Network to assign a canonicalorientation to feature points given an image patch centered on the featurepoint. Our method improves feature point matching upon the state-of-the art andcan be used in conjunction with any existing rotation sensitive descriptors. Toavoid the tedious and almost impossible task of finding a target orientation tolearn, we propose to use Siamese networks which implicitly find the optimalorientations during training. We also propose a new type of activation functionfor Neural Networks that generalizes the popular ReLU, maxout, and PReLUactivation functions. This novel activation performs better for our task. Wevalidate the effectiveness of our method extensively with four existingdatasets, including two non-planar datasets, as well as our own dataset. Weshow that we outperform the state-of-the-art without the need of retraining foreach dataset.
arxiv-14400-93 | On the Quality of the Initial Basin in Overspecified Neural Networks | http://arxiv.org/abs/1511.04210 | author:Itay Safran, Ohad Shamir category:cs.LG stat.ML published:2015-11-13 summary:Deep learning, in the form of artificial neural networks, has achievedremarkable practical success in recent years, for a variety of difficultmachine learning applications. However, a theoretical explanation for thisremains a major open problem, since training neural networks involvesoptimizing a highly non-convex objective function, and is known to becomputationally hard in the worst case. In this work, we study the\emph{geometric} structure of the associated non-convex objective function, inthe context of ReLU networks and starting from a random initialization of thenetwork parameters. We identify some conditions under which it becomes morefavorable to optimization, in the sense of (i) High probability of initializingat a point from which there is a monotonically decreasing path to a globalminimum; and (ii) High probability of initializing at a basin (suitablydefined) with a small minimal objective value. A common theme in our results isthat such properties are more likely to hold for larger ("overspecified")networks, which accords with some recent empirical and theoreticalobservations.
arxiv-14400-94 | A Continuous-time Mutually-Exciting Point Process Framework for Prioritizing Events in Social Media | http://arxiv.org/abs/1511.04145 | author:Mehrdad Farajtabar, Safoora Yousefi, Long Q. Tran, Le Song, Hongyuan Zha category:cs.SI cs.LG published:2015-11-13 summary:The overwhelming amount and rate of information update in online social mediais making it increasingly difficult for users to allocate their attention totheir topics of interest, thus there is a strong need for prioritizing newsfeeds. The attractiveness of a post to a user depends on many complexcontextual and temporal features of the post. For instance, the contents of thepost, the responsiveness of a third user, and the age of the post may all haveimpact. So far, these static and dynamic features has not been incorporated ina unified framework to tackle the post prioritization problem. In this paper,we propose a novel approach for prioritizing posts based on a feature modulatedmulti-dimensional point process. Our model is able to simultaneously capturetextual and sentiment features, and temporal features such as self-excitation,mutual-excitation and bursty nature of social interaction. As an evaluation, wealso curated a real-world conversational benchmark dataset crawled fromFacebook. In our experiments, we demonstrate that our algorithm is able toachieve the-state-of-the-art performance in terms of analyzing, predicting, andprioritizing events. In terms of interpretability of our method, we observethat features indicating individual user profile and linguistic characteristicsof the events work best for prediction and prioritization of new events.
arxiv-14400-95 | Natural Language Object Retrieval | http://arxiv.org/abs/1511.04164 | author:Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell category:cs.CV cs.CL published:2015-11-13 summary:In this paper, we address the task of natural language object retrieval, tolocalize a target object within a given image based on a natural language queryof the object. Natural language object retrieval differs from text-based imageretrieval task as it involves spatial information about objects within thescene and global scene context. To address this issue, we propose a novelSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidateboxes for object retrieval, integrating spatial configurations and globalscene-level contextual information into the network. Our model processes querytext, local image descriptors, spatial configurations and global contextfeatures through a recurrent network, outputs the probability of the query textconditioned on each candidate box as a score for the box, and can transfervisual-linguistic knowledge from image captioning domain to our task.Experimental results demonstrate that our method effectively utilizes bothlocal and global information, outperforming previous baseline methodssignificantly on different datasets and scenarios, and can exploit large scalevision and language datasets for knowledge transfer.
arxiv-14400-96 | DISC: Deep Image Saliency Computing via Progressive Representation Learning | http://arxiv.org/abs/1511.04192 | author:Tianshui Chen, Liang Lin, Lingbo Liu, Xiaonan Luo, Xuelong Li category:cs.CV published:2015-11-13 summary:Salient object detection increasingly receives attention as an importantcomponent or step in several pattern recognition and image processing tasks.Although a variety of powerful saliency models have been intensively proposed,they usually involve heavy feature (or model) engineering based on priors (orassumptions) about the properties of objects and backgrounds. Inspired by theeffectiveness of recently developed feature learning, we provide a novel DeepImage Saliency Computing (DISC) framework for fine-grained image saliencycomputing. In particular, we model the image saliency from both the coarse- andfine-level observations, and utilize the deep convolutional neural network(CNN) to learn the saliency representation in a progressive manner.Specifically, our saliency model is built upon two stacked CNNs. The first CNNgenerates a coarse-level saliency map by taking the overall image as the input,roughly identifying saliency regions in the global context. Furthermore, weintegrate superpixel-based local context information in the first CNN to refinethe coarse-level saliency map. Guided by the coarse saliency map, the secondCNN focuses on the local context to produce fine-grained and accurate saliencymap while preserving object details. For a testing image, the two CNNscollaboratively conduct the saliency computing in one shot. Our DISC frameworkis capable of uniformly highlighting the objects-of-interest from complexbackground while preserving well object details. Extensive experiments onseveral standard benchmarks suggest that DISC outperforms otherstate-of-the-art methods and it also generalizes well across datasets withoutadditional training. The executable version of DISC is available online:http://vision.sysu.edu.cn/projects/DISC.
arxiv-14400-97 | Deep Mean Maps | http://arxiv.org/abs/1511.04150 | author:Junier B. Oliva, Dougal J. Sutherland, Barnabás Póczos, Jeff Schneider category:stat.ML cs.CV cs.LG published:2015-11-13 summary:The use of distributions and high-level features from deep architecture hasbecome commonplace in modern computer vision. Both of these methodologies haveseparately achieved a great deal of success in many computer vision tasks.However, there has been little work attempting to leverage the power of theseto methodologies jointly. To this end, this paper presents the Deep Mean Maps(DMMs) framework, a novel family of methods to non-parametrically representdistributions of features in convolutional neural network models. DMMs are able to both classify images using the distribution of top-levelfeatures, and to tune the top-level features for performing this task. We showhow to implement DMMs using a special mean map layer composed of typical CNNoperations, making both forward and backward propagation simple. We illustrate the efficacy of DMMs at analyzing distributional patterns inimage data in a synthetic data experiment. We also show that we extendingexisting deep architectures with DMMs improves the performance of existing CNNson several challenging real-world datasets.
arxiv-14400-98 | Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling | http://arxiv.org/abs/1511.04137 | author:Lin Chen, Forrest W. Crawford, Amin Karbasi category:cs.SI cs.AI cs.LG published:2015-11-13 summary:Learning about the social structure of hidden and hard-to-reach populations--- such as drug users and sex workers --- is a major goal of epidemiologicaland public health research on risk behaviors and disease prevention.Respondent-driven sampling (RDS) is a peer-referral process widely used by manyhealth organizations, where research subjects recruit other subjects from theirsocial network. In such surveys, researchers observe who recruited whom, alongwith the time of recruitment and the total number of acquaintances (networkdegree) of respondents. However, due to privacy concerns, the identities ofacquaintances are not disclosed. In this work, we show how to reconstruct theunderlying network structure through which the subjects are recruited. Weformulate the dynamics of RDS as a continuous-time diffusion process over theunderlying graph and derive the likelihood for the recruitment time seriesunder an arbitrary recruitment time distribution. We develop an efficientstochastic optimization algorithm called RENDER (REspoNdent-Driven nEtworkReconstruction) that finds the network that best explains the collected data.We support our analytical results through an exhaustive set of experiments onboth synthetic and real data.
arxiv-14400-99 | Deep Feature Learning for EEG Recordings | http://arxiv.org/abs/1511.04306 | author:Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn category:cs.NE cs.LG published:2015-11-13 summary:We introduce and compare several strategies for learning discriminativefeatures from electroencephalography (EEG) recordings using deep learningtechniques. EEG data are generally only available in small quantities, they arehigh-dimensional with a poor signal-to-noise ratio, and there is considerablevariability between individual subjects and recording sessions. Our proposedtechniques specifically address these challenges for feature learning.Cross-trial encoding forces auto-encoders to focus on features that are stableacross trials. Similarity-constraint encoders learn features that allow todistinguish between classes by demanding that two trials from the same classare more similar to each other than to trials from other classes. Thistuple-based training approach is especially suitable for small datasets.Hydra-nets allow for separate processing pathways adapting to subsets of adataset and thus combine the advantages of individual feature learning (betteradaptation of early, low-level processing) with group model training (bettergeneralization of higher-level processing in deeper layers). This way, modelscan, for instance, adapt to each subject individually to compensate fordifferences in spatial patterns due to anatomical differences or variance inelectrode positions. The different techniques are evaluated using the publiclyavailable OpenMIIR dataset of EEG recordings taken while participants listenedto and imagined music.
arxiv-14400-100 | Adaptive Affinity Matrix for Unsupervised Metric Learning | http://arxiv.org/abs/1511.04153 | author:Yaoyi Li, Junxuan Chen, Hongtao Lu category:cs.CV cs.LG published:2015-11-13 summary:Spectral clustering is one of the most popular clustering approaches with thecapability to handle some challenging clustering problems. Most spectralclustering methods provide a nonlinear map from the data manifold to asubspace. Only a little work focuses on the explicit linear map which can beviewed as the unsupervised distance metric learning. In practice, the selectionof the affinity matrix exhibits a tremendous impact on the unsupervisedlearning. While much success of affinity learning has been achieved in recentyears, some issues such as noise reduction remain to be addressed. In thispaper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), tolearn an adaptive affinity matrix and derive a distance metric from theaffinity. We assume the affinity matrix to be positive semidefinite withability to quantify the pairwise dissimilarity. Our method is based on posingthe optimization of objective function as a spectral decomposition problem. Weyield the affinity from both the original data distribution and the widely-usedheat kernel. The provided matrix can be regarded as the optimal representationof pairwise relationship on the manifold. Extensive experiments on a number ofreal-world data sets show the effectiveness and efficiency of AdaAM.
arxiv-14400-101 | Solving Jigsaw Puzzles with Linear Programming | http://arxiv.org/abs/1511.04472 | author:Rui Yu, Chris Russell, Lourdes Agapito category:cs.CV published:2015-11-13 summary:We propose a novel Linear Program (LP) based formula- tion for solving jigsawpuzzles. We formulate jigsaw solving as a set of successive global convexrelaxations of the stan- dard NP-hard formulation, that can describe bothjigsaws with pieces of unknown position and puzzles of unknown po- sition andorientation. The main contribution and strength of our approach comes from theLP assembly strategy. In contrast to existing greedy methods, our LP solverexploits all the pairwise matches simultaneously, and computes the position ofeach piece/component globally. The main ad- vantages of our LP approachinclude: (i) a reduced sensi- tivity to local minima compared to greedyapproaches, since our successive approximations are global and convex and (ii)an increased robustness to the presence of mismatches in the pairwise matchesdue to the use of a weighted L1 penalty. To demonstrate the effectiveness ofour approach, we test our algorithm on public jigsaw datasets and show that itoutperforms state-of-the-art methods.
arxiv-14400-102 | Robust Face Alignment Using a Mixture of Invariant Experts | http://arxiv.org/abs/1511.04404 | author:Oncel Tuzel, Salil Tambe, Tim K. Marks category:cs.CV published:2015-11-13 summary:Face alignment, which is the task of finding the locations of a set of faciallandmark points in an image of a face, is an important problem that is usefulin widespread application areas. Face alignment is particularly challengingwhen there are large variations in pose (in-plane and out-of-plane rotations)and facial expression. To address this issue, we propose a cascade in whicheach stage consists of a mixture of regression experts. Each expert learns acustomized regression model that is specialized to a different subset of thejoint space of pose and expressions. The system is invariant to a predefinedclass of transformations (e.g., affine), because the input is transformed tomatch each expert's prototype shape before the regression is applied. We alsopresent a method to include deformation constraints within the discriminativealignment framework, which makes our algorithm more robust. Results show thatour algorithm significantly outperforms previous methods on publicly availableface alignment datasets.
arxiv-14400-103 | Symbol Grounding Association in Multimodal Sequences with Missing Elements | http://arxiv.org/abs/1511.04401 | author:Federico Raue, Thomas M. Breuel, Andreas Dengel, Marcus Liwicki category:cs.CV cs.CL cs.LG cs.NE published:2015-11-13 summary:In this paper, we extend a symbolic association framework to being able tohandle missing elements in multimodal sequences. The general scope of the workis the symbolic associations of object-word mappings as it happens in languagedevelopment on infants. This scenario has been long interested by ArtificialIntelligence, Psychology and Neuroscience. In this work, we extend a recentapproach for multimodal sequences (visual and audio) to also cope with missingelements in one or both modalities. Our approach uses two parallel LongShort-Term Memory (LSTM) networks with a learning rule based on EM-algorithm.It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose toinclude an extra step for the combination with max and mean operations forhandling missing elements in the sequences. The intuition behind is that thecombination acts as a condition selector for choosing the best representationfrom both LSTMs. We evaluated the proposed extension in three differentscenarios: audio sequences with missing elements, visual sequences with missingelements, and sequences with missing elements in both modalities. Theperformance of our extension reaches better results than the original model andsimilar results to a unique LSTM trained in one modality, i.e., where thelearning problem is less difficult.
arxiv-14400-104 | Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode Resource-constrained Multi-project Scheduling Problem | http://arxiv.org/abs/1511.04387 | author:Shahriar Asta, Daniel Karapetyan, Ahmed Kheiri, Ender Özcan, Andrew J. Parkes category:cs.DS cs.AI cs.NE published:2015-11-13 summary:Multi-mode resource and precedence-constrained project scheduling is awell-known challenging real-world optimisation problem. An important variant ofthe problem requires scheduling of activities for multiple projects consideringavailability of local and global resources while respecting a range ofconstraints. This problem has been addressed by a competition, and associatedset of benchmark instances, as a part of the MISTA 2013 conference. A criticalaspect of the benchmarks is that the primary objective is to minimise the sumof the project completion times, with the usual makespan minimisation as asecondary objective. We observe that this leads to an expected differentoverall structure of good solutions and discuss the effects this has on thealgorithm design. This paper presents the resulting competition winningapproach; it is a carefully designed hybrid of Monte-Carlo tree search, novelneighbourhood moves, memetic algorithms, and hyper-heuristic methods. Theimplementation is also engineered to increase the speed with which iterationsare performed, and to exploit the computing power of multicore machines. Theresulting information-sharing multi-component algorithm significantlyoutperformed the other approaches during the competition, producing the bestsolution for 17 out of the 20 test instances and performing the best in around90% of all the trials.
arxiv-14400-105 | Unsupervised Learning of Edges | http://arxiv.org/abs/1511.04166 | author:Yin Li, Manohar Paluri, James M. Rehg, Piotr Dollár category:cs.CV published:2015-11-13 summary:Data-driven approaches for edge detection have proven effective and achievetop results on modern benchmarks. However, all current data-driven edgedetectors require manual supervision for training in the form of hand-labeledregion segments or object boundaries. Specifically, human annotators marksemantically meaningful edges which are subsequently used for training. Is thisform of strong, high-level supervision actually necessary to learn toaccurately detect edges? In this work we present a simple yet effectiveapproach for training edge detectors without human supervision. To this end weutilize motion, and more specifically, the only input to our method is noisysemi-dense matches between frames. We begin with only a rudimentary knowledgeof edges (in the form of image gradients), and alternate between improvingmotion estimation and edge detection in turn. Using a large corpus of videodata, we show that edge detectors trained using our unsupervised schemeapproach the performance of the same methods trained with full supervision(within 3-5%). Finally, we show that when using a deep network for the edgedetector, our approach provides a novel pre-training scheme for objectdetection.
arxiv-14400-106 | Deep Reflectance Maps | http://arxiv.org/abs/1511.04384 | author:Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves, Tinne Tuytelaars category:cs.CV published:2015-11-13 summary:Undoing the image formation process and therefore decomposing appearance intoits intrinsic properties is a challenging task due to the under-constraintnature of this inverse problem. While significant progress has been made oninferring shape, materials and illumination from images only, progress in anunconstrained setting is still limited. We propose a convolutional neuralarchitecture to estimate reflectance maps of specular materials in naturallighting conditions. We achieve this in an end-to-end learning formulation thatdirectly predicts a reflectance map from the image itself. We show how toimprove estimates by facilitating additional supervision in an indirect schemethat first predicts surface orientation and afterwards predicts the reflectancemap by a learning-based sparse data interpolation. In order to analyze performance on this difficult task, we propose a newchallenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg)using both synthetic and real images. Furthermore, we show the application ofour method to a range of image-based editing tasks on real images.
arxiv-14400-107 | Handling Class Imbalance in Link Prediction using Learning to Rank Techniques | http://arxiv.org/abs/1511.04383 | author:Bopeng Li, Sougata Chaudhuri, Ambuj Tewari category:stat.ML cs.LG cs.SI published:2015-11-13 summary:We consider the link prediction problem in a partially observed network,where the objective is to make predictions in the unobserved portion of thenetwork. Many existing methods reduce link prediction to binary classificationproblem. However, the dominance of absent links in real world networks makesmisclassification error a poor performance metric. Instead, researchers haveargued for using ranking performance measures, like AUC, AP and NDCG, forevaluation. Our main contribution is to recast the link prediction problem as alearning to rank problem and use effective learning to rank techniques directlyduring training. This is in contrast to existing work that uses rankingmeasures only during evaluation. Our approach is able to deal with the classimbalance problem by using effective, scalable learning to rank techniquesduring training. Furthermore, our approach allows us to combine networktopology and node features. As a demonstration of our general approach, wedevelop a link prediction method by optimizing the cross-entropy surrogate,originally used in the popular ListNet ranking algorithm. We conduct extensiveexperiments on publicly available co-authorship, citation and metabolicnetworks to demonstrate the merits of our method.
arxiv-14400-108 | Large Scale Artificial Neural Network Training Using Multi-GPUs | http://arxiv.org/abs/1511.04348 | author:Linnan Wang, Wei Wu, Jianxiong Xiao, Yang Yi category:cs.DC cs.NE published:2015-11-13 summary:This paper describes a method for accelerating large scale Artificial NeuralNetworks (ANN) training using multi-GPUs by reducing the forward and backwardpasses to matrix multiplication. We propose an out-of-core multi-GPU matrixmultiplication and integrate the algorithm with the ANN training. Theexperiments demonstrate that our matrix multiplication algorithm achieveslinear speedup on multiple inhomogeneous GPUs. The full paper of this projectcan be found at [1].
arxiv-14400-109 | $k$-means: Fighting against Degeneracy in Sequential Monte Carlo with an Application to Tracking | http://arxiv.org/abs/1511.04157 | author:Kai Fan, Katherine Heller category:stat.ML published:2015-11-13 summary:For regular particle filter algorithm or Sequential Monte Carlo (SMC)methods, the initial weights are traditionally dependent on the proposeddistribution, the posterior distribution at the current timestamp in thesampled sequence, and the target is the posterior distribution of the previoustimestamp. This is technically correct, but leads to algorithms which usuallyhave practical issues with degeneracy, where all particles eventually collapseonto a single particle. In this paper, we propose and evaluate using $k$ meansclustering to attack and even take advantage of this degeneracy. Specifically,we propose a Stochastic SMC algorithm which initializes the set of $k$ means,providing the initial centers chosen from the collapsed particles. To fightagainst degeneracy, we adjust the regular SMC weights, mediated by clusterproportions, and then correct them to retain the same expectation as before. Weexperimentally demonstrate that our approach has better performance thanvanilla algorithms.
arxiv-14400-110 | Active Contextual Entropy Search | http://arxiv.org/abs/1511.04211 | author:Jan Hendrik Metzen category:stat.ML cs.LG published:2015-11-13 summary:Contextual policy search allows adapting robotic movement primitives todifferent situations. For instance, a locomotion primitive might be adapted todifferent terrain inclinations or desired walking speeds. Such an adaptation isoften achievable by modifying a small number of hyperparameters. However,learning, when performed on real robotic systems, is typically restricted to asmall number of trials. Bayesian optimization has recently been proposed as asample-efficient means for contextual policy search that is well suited underthese conditions. In this work, we extend entropy search, a variant of Bayesianoptimization, such that it can be used for active contextual policy searchwhere the agent selects those tasks during training in which it expects tolearn the most. Empirical results in simulation suggest that this allowslearning successful behavior with less trials.
arxiv-14400-111 | Similarity-based Text Recognition by Deeply Supervised Siamese Network | http://arxiv.org/abs/1511.04397 | author:Ehsan Hosseini-Asl, Angshuman Guha category:cs.CV cs.LG published:2015-11-13 summary:In this paper, we propose a new text recognition model based on measuring thevisual similarity of text and predicting the content of unlabeled texts. Firsta Siamese convolutional network is trained with deep supervision on a labeledtraining dataset. This network projects texts into a similarity manifold. TheDeeply Supervised Siamese network learns visual similarity of texts. Then aK-nearest neighbor classifier is used to predict unlabeled text based onsimilarity distance to labeled texts. The performance of the model is evaluatedon three datasets of machine-print and hand-written text combined. Wedemonstrate that the model reduces the cost of human estimation by $50\%-85\%$.The error of the system is less than $0.5\%$. The proposed model outperformconventional Siamese network by finding visually-similar barely-readable andreadable text, e.g. machine-printed, handwritten, due to deep supervision. Theresults also demonstrate that the predicted labels are sometimes better thanhuman labels e.g. spelling correction.
arxiv-14400-112 | Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition | http://arxiv.org/abs/1511.04196 | author:Zhiwei Deng, Arash Vahdat, Hexiang Hu, Greg Mori category:cs.CV published:2015-11-13 summary:Rich semantic relations are important in a variety of visual recognitionproblems. As a concrete example, group activity recognition involves theinteractions and relative spatial relations of a set of people in a scene.State of the art recognition methods center on deep learning approaches fortraining highly effective, complex classifiers for interpreting images.However, bridging the relatively low-level concepts output by these methods tointerpret higher-level compositional scenes remains a challenge. Graphicalmodels are a standard tool for this task. In this paper, we propose a method tointegrate graphical models and deep neural networks into a joint framework.Instead of using a traditional inference method, we use a sequential inferencemodeled by a recurrent neural network. Beyond this, the appropriate structurefor inference can be learned by imposing gates on edges between nodes.Empirical results on group activity recognition demonstrate the potential ofthis model to handle highly structured learning tasks.
arxiv-14400-113 | Sequence to Sequence Learning for Optical Character Recognition | http://arxiv.org/abs/1511.04176 | author:Devendra Kumar Sahu, Mohak Sukhwani category:cs.CV published:2015-11-13 summary:We propose an end-to-end recurrent encoder-decoder based sequence learningapproach for printed text Optical Character Recognition (OCR). In contrast topresent day existing state-of-art OCR solution which uses connectionisttemporal classification (CTC) output layer, our approach makes minimalisticassumptions on the structure and length of the sequence. We use a two stepencoder-decoder approach -- (a) A recurrent encoder reads a variable lengthprinted text word image and encodes it to a fixed dimensional embedding. (b)This fixed dimensional embedding is subsequently comprehended by decoderstructure which converts it into a variable length text output. Ourarchitecture gives competitive performance relative to connectionist temporalclassification (CTC) output layer while being executed in more naturalsettings. The learnt deep word image embedding from encoder can be used forprinted text based retrieval systems. The expressive fixed dimensionalembedding for any variable length input expedites the task of retrieval andmakes it more efficient which is not possible with other recurrent neuralnetwork architectures. We empirically investigate the expressiveness and thelearnability of long short term memory (LSTMs) in the sequence to sequencelearning regime by training our network for prediction tasks in segmentationfree printed text OCR. The utility of the proposed architecture for printedtext is demonstrated by quantitative and qualitative evaluation of two tasks --word prediction and retrieval.
arxiv-14400-114 | Standard methods for inexpensive pollen loads authentication by means of computer vision and machine learning | http://arxiv.org/abs/1511.04320 | author:Manuel Chica, Pascual Campoy category:cs.CV published:2015-11-13 summary:We present a complete methodology for authenticating local bee pollen againstfraudulent samples using image processing and machine learning techniques. Theproposed standard methods do not need expensive equipment such as advancedmicroscopes and can be used for a preliminary fast rejection of unknown pollentypes. The system is able to rapidly reject the non-local pollen samples withinexpensive hardware and without the need to send the product to thelaboratory. Methods are based on the color properties of bee pollen loadsimages and the use of one-class classifiers which are appropriate to rejectunknown pollen samples when there is limited data about them. The validation ofthe method is carried out by authenticating Spanish bee pollen types.Experimentation shows that the proposed methods can obtain an overallauthentication accuracy of 94%. We finally illustrate the user interaction withthe software in some practical cases by showing the developed applicationprototype.
arxiv-14400-115 | Neuroprosthetic decoder training as imitation learning | http://arxiv.org/abs/1511.04156 | author:Josh Merel, David Carlson, Liam Paninski, John P. Cunningham category:stat.ML cs.LG q-bio.NC published:2015-11-13 summary:Neuroprosthetic brain-computer interfaces function via an algorithm whichdecodes neural activity of the user into movements of an end effector, such asa cursor or robotic arm. In practice, the decoder is often learned by updatingits parameters while the user performs a task. When the user's intention is notdirectly observable, recent methods have demonstrated value in training thedecoder against a surrogate for the user's intended movement. We describe howtraining a decoder in this way is a novel variant of an imitation learningproblem, where an oracle or expert is employed for supervised training in lieuof direct observations, which are not available. Specifically, we describe howa generic imitation learning meta-algorithm, dataset aggregation (DAgger, [1]),can be adapted to train a generic brain-computer interface. By derivingexisting learning algorithms for brain-computer interfaces in this framework,we provide a novel analysis of regret (an important metric of learningefficacy) for brain-computer interfaces. This analysis allows us tocharacterize the space of algorithmic variants and bounds on their regretrates. Existing approaches for decoder learning have been performed in thecursor control setting, but the available design principles for these decodersare such that it has been impossible to scale them to naturalistic settings.Leveraging our findings, we then offer an algorithm that combines imitationlearning with optimal control, which should allow for training of arbitraryeffectors for which optimal control can generate goal-oriented control. Wedemonstrate this novel and general BCI algorithm with simulated neuroprostheticcontrol of a 26 degree-of-freedom model of an arm, a sophisticated andrealistic end effector.
arxiv-14400-116 | Zero-Shot Action Recognition by Word-Vector Embedding | http://arxiv.org/abs/1511.04458 | author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV published:2015-11-13 summary:The number of categories for action recognition is growing rapidly and it hasbecome increasingly hard to label sufficient training data for learningconventional models for all categories. Instead of collecting ever more dataand labelling them exhaustively for all categories, an attractive alternativeapproach is "zeroshot learning" (ZSL). To that end, in this study we constructa mapping between visual features and a semantic descriptor of each actioncategory, allowing new categories to be recognised in the absence of any visualtraining data. Existing ZSL studies focus primarily on still images, andattribute-based semantic representations. In this work, we explore word-vectorsas the shared semantic space to embed videos and category labels for ZSL actionrecognition. This is a more challenging problem than existing ZSL of stillimages and/or attributes, because the mapping between the semantic space andvideo space-time features of actions is more complex and harder to learn forthe purpose of generalising over any cross-category domain shift. To solve thisgeneralisation problem in ZSL action recognition, we investigate a series ofsynergistic improvements to the standard ZSL pipeline. First, we enhancesignificantly the semantic space mapping by proposing manifold-regularisedregression and data augmentation strategies. Second, we evaluate two existingpost processing strategies (transductive self-training and hubness correction),and show that they are complementary. We evaluate extensively our model on awide range of human action datasets including HMDB51, UCF101, OlympicSports,CCV and TRECVID MED 13. The results demonstrate that our approach achieves thestate-of-the-art zero-shot action recognition performance with a simple andefficient pipeline, and without supervised annotation of attributes.
arxiv-14400-117 | Lass-0: sparse non-convex regression by local search | http://arxiv.org/abs/1511.04402 | author:William Herlands, Maria De-Arteaga, Daniel Neill, Artur Dubrawski category:stat.ML published:2015-11-13 summary:We compute approximate solutions to L0 regularized linear regression using L1regularization, also known as the Lasso, as an initialization step. Ouralgorithm, the Lass-0 ("Lass-zero"), uses a computationally efficient stepwisesearch to determine a locally optimal L0 solution given any L1 regularizationsolution. We present theoretical results of consistency under orthogonality andappropriate handling of redundant features. Empirically, we use synthetic datato demonstrate that Lass-0 solutions are closer to the true sparse support thanL1 regularization models. Additionally, in real-world data Lass-0 finds moreparsimonious solutions than L1 regularization while maintaining similarpredictive accuracy.
arxiv-14400-118 | Learning Dense Convolutional Embeddings for Semantic Segmentation | http://arxiv.org/abs/1511.04377 | author:Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos category:cs.CV published:2015-11-13 summary:This paper proposes a new deep convolutional neural network (DCNN)architecture that learns pixel embeddings, such that pairwise distances betweenthe embeddings can be used to infer whether or not the pixels lie on the sameregion. That is, for any two pixels on the same object, the embeddings aretrained to be similar; for any pair that straddles an object boundary, theembeddings are trained to be dissimilar. Experimental results show that whenthis embedding network is used in conjunction with a DCNN trained on semanticsegmentation, there is a systematic improvement in per-pixel classificationaccuracy. Our contributions are integrated in the popular Caffe deep learningframework, and consist in straightforward modifications to convolutionroutines. As such, they can be exploited for any task involving convolutionlayers.
arxiv-14400-119 | An Adaptive Data Representation for Robust Point-Set Registration and Merging | http://arxiv.org/abs/1511.04240 | author:Dylan Campbell, Lars Petersson category:cs.CV published:2015-11-13 summary:This paper presents a framework for rigid point-set registration and mergingusing a robust continuous data representation. Our point-set representation isconstructed by training a one-class support vector machine with a Gaussianradial basis function kernel and subsequently approximating the output functionwith a Gaussian mixture model. We leverage the representation's sparseparametrisation and robustness to noise, outliers and occlusions in anefficient registration algorithm that minimises the L2 distance between oursupport vector--parametrised Gaussian mixtures. In contrast, existingtechniques, such as Iterative Closest Point and Gaussian mixture approaches,manifest a narrower region of convergence and are less robust to occlusions andmissing data, as demonstrated in the evaluation on a range of 2D and 3Ddatasets. Finally, we present a novel algorithm, GMMerge, that parsimoniouslyand equitably merges aligned mixture models, allowing the framework to be usedfor reconstruction and mapping.
arxiv-14400-120 | Volume-based Semantic Labeling with Signed Distance Functions | http://arxiv.org/abs/1511.04242 | author:Tommaso Cavallari, Luigi Di Stefano category:cs.CV published:2015-11-13 summary:Research works on the two topics of Semantic Segmentation and SLAM(Simultaneous Localization and Mapping) have been following separate tracks.Here, we link them quite tightly by delineating a category label fusiontechnique that allows for embedding semantic information into the dense mapcreated by a volume-based SLAM algorithm such as KinectFusion. Accordingly, ourapproach is the first to provide a semantically labeled dense reconstruction ofthe environment from a stream of RGB-D images. We validate our proposal using apublicly available semantically annotated RGB-D dataset and a) employing groundtruth labels, b) corrupting such annotations with synthetic noise, c) deployinga state of the art semantic segmentation algorithm based on ConvolutionalNeural Networks.
arxiv-14400-121 | DETRAC: A New Benchmark and Protocol for Multi-Object Tracking | http://arxiv.org/abs/1511.04136 | author:Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, Siwei Lyu category:cs.CV published:2015-11-13 summary:In recent years, most effective multi-object tracking (MOT) methods are basedon the tracking-by-detection framework. Existing performance evaluations of MOTmethods usually separate the target association step from the object detectionstep by using the same object detection results for comparisons. In this work,we perform a comprehensive quantitative study on the effect of object detectionaccuracy to the overall MOT performance. This is based on a new large-scaleDETection and tRACking (DETRAC) benchmark dataset. The DETRAC benchmark datasetconsists of 100 challenging video sequences captured from real-world trafficscenes (over 140 thousand frames and 1.2 million labeled bounding boxes ofobjects) for both object detection and MOT. We evaluate complete MOT systemsconstructed from combinations of state-of-the-art target association methodsand object detection schemes. Our analysis shows the complex effects of objectdetection accuracy on MOT performance. Based on these observations, we proposenew evaluation tools and metrics for MOT systems that consider both objectdetection and target association for comprehensive analysis.
arxiv-14400-122 | LSTM-based Deep Learning Models for Non-factoid Answer Selection | http://arxiv.org/abs/1511.04108 | author:Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou category:cs.CL cs.LG published:2015-11-12 summary:In this paper, we apply a general deep learning (DL) framework for the answerselection task, which does not depend on manually defined features orlinguistic tools. The basic framework is to build the embeddings of questionsand answers based on bidirectional long short-term memory (biLSTM) models, andmeasure their closeness by cosine similarity. We further extend this basicmodel in two directions. One direction is to define a more compositerepresentation for questions and answers by combining convolutional neuralnetwork with the basic framework. The other direction is to utilize a simplebut efficient attention mechanism in order to generate the answerrepresentation according to the question context. Several variations of modelsare provided. The models are examined by two datasets, including TREC-QA andInsuranceQA. Experimental results demonstrate that the proposed modelssubstantially outperform several strong baselines.
arxiv-14400-123 | Action Recognition using Visual Attention | http://arxiv.org/abs/1511.04119 | author:Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov category:cs.LG cs.CV published:2015-11-12 summary:We propose a soft attention based model for the task of action recognition invideos. We use multi-layered Recurrent Neural Networks (RNNs) with LongShort-Term Memory (LSTM) units which are deep both spatially and temporally.Our model learns to focus selectively on parts of the video frames andclassifies videos after taking a few glimpses. The model essentially learnswhich parts in the frames are relevant for the task at hand and attaches higherimportance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51and Hollywood2 datasets and analyze how the model focuses its attentiondepending on the scene and the action being performed.
arxiv-14400-124 | Improving performance of recurrent neural network with relu nonlinearity | http://arxiv.org/abs/1511.03771 | author:Sachin S. Talathi, Aniket Vartak category:cs.NE cs.LG published:2015-11-12 summary:In recent years significant progress has been made in successfully trainingrecurrent neural networks (RNNs) on sequence learning problems involving longrange temporal dependencies. The progress has been made on three fronts: (a)Algorithmic improvements involving sophisticated optimization techniques, (b)network design involving complex hidden layer nodes and specialized recurrentlayer connections and (c) weight initialization methods. In this paper, wefocus on recently proposed weight initialization with identity matrix for therecurrent weights in a RNN. This initialization is specifically proposed forhidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simpledynamical systems perspective on weight initialization process, which allows usto propose a modified weight initialization strategy. We show that thisinitialization technique leads to successfully training RNNs composed of ReLUs.We demonstrate that our proposal produces comparable or better solution forthree toy problems involving long range temporal structure: the additionproblem, the multiplication problem and the MNIST classification problem usingsequence of pixels. In addition, we present results for a benchmark actionrecognition problem.
arxiv-14400-125 | When Naïve Bayes Nearest Neighbours Meet Convolutional Neural Networks | http://arxiv.org/abs/1511.03853 | author:Ilja Kuzborskij, Fabio Maria Carlucci, Barbara Caputo category:cs.CV published:2015-11-12 summary:Since Convolutional Neural Networks (CNNs) have become the leading learningparadigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-basedclassifiers have lost momentum in the community. This is because (1) suchalgorithms cannot use CNN activations as input features; (2) they cannot beused as final layer of CNN architectures for end-to-end training , and (3) theyare generally not scalable and hence cannot handle big data. This paperproposes a framework that addresses all these issues, thus bringing back NBNNson the map. We solve the first by extracting CNN activations from local patchesat multiple scale levels, similarly to [1]. We address simultaneously thesecond and third by proposing a scalable version of Naive Bayes Non-linearLearning (NBNL, [2]). Results obtained using pre-trained CNNs on standard sceneand domain adaptation databases show the strength of our approach, opening anew season for NBNNs.
arxiv-14400-126 | Multimodal Skip-gram Using Convolutional Pseudowords | http://arxiv.org/abs/1511.04024 | author:Zachary Seymour, Yingming Li, Zhongfei Zhang category:cs.CL cs.CV published:2015-11-12 summary:This work studies the representational mapping across multimodal data suchthat given a piece of the raw data in one modality the corresponding semanticdescription in terms of the raw data in another modality is immediatelyobtained. Such a representational mapping can be found in a wide spectrum ofreal-world applications including image/video retrieval, object recognition,action/behavior recognition, and event understanding and prediction. To thatend, we introduce a simplified training objective for learning multimodalembeddings using the skip-gram architecture by introducing convolutional"pseudowords:" embeddings composed of the additive combination of distributedword representations and image features from convolutional neural networksprojected into the multimodal space. We present extensive results of therepresentational properties of these embeddings on various word similaritybenchmarks to show the promise of this approach.
arxiv-14400-127 | ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks | http://arxiv.org/abs/1511.03776 | author:Chen Sun, Manohar Paluri, Ronan Collobert, Ram Nevatia, Lubomir Bourdev category:cs.CV published:2015-11-12 summary:This paper aims to classify and locate objects accurately and efficiently,without using bounding box annotations. It is challenging as objects in thewild could appear at arbitrary locations and in different scales. In thispaper, we propose a novel classification architecture ProNet based onconvolutional neural networks. It uses computationally efficient neuralnetworks to propose image regions that are likely to contain objects, andapplies more powerful but slower networks on the proposed regions. The basicbuilding block is a multi-scale fully-convolutional network which assignsobject confidence scores to boxes at different locations and scales. We showthat such networks can be trained effectively using image-level annotations,and can be connected into cascades or trees for efficient objectclassification. ProNet outperforms previous state-of-the-art significantly onPASCAL VOC 2012 and MS COCO datasets for object classification and point-basedlocalization.
arxiv-14400-128 | Block-diagonal covariance selection for high-dimensional Gaussian graphical models | http://arxiv.org/abs/1511.04033 | author:Emilie Devijver, Mélina Gallopin category:math.ST cs.LG stat.ML stat.TH published:2015-11-12 summary:Gaussian graphical models are widely utilized to infer and visualize networksof dependencies between continuous variables. However, inferring the graph isdifficult when the sample size is small compared to the number of variables. Toreduce the number of parameters to estimate in the model, we propose anon-asymptotic model selection procedure supported by strong theoreticalguarantees based on an oracle inequality and a minimax lower bound. Thecovariance matrix of the model is approximated by a block-diagonal matrix. Thestructure of this matrix is detected by thresholding the sample covariancematrix, where the threshold is selected using the slope heuristic. Based on theblock-diagonal structure of the covariance matrix, the estimation problem isdivided into several independent problems: subsequently, the network ofdependencies between variables is inferred using the graphical lasso algorithmin each block. The performance of the procedure is illustrated on simulateddata. An application to a real gene expression dataset with a limited samplesize is also presented: the dimension reduction allows attention to beobjectively focused on interactions among smaller subsets of genes, leading toa more parsimonious and interpretable modular network.
arxiv-14400-129 | Learning Human Identity from Motion Patterns | http://arxiv.org/abs/1511.03908 | author:Natalia Neverova, Christian Wolf, Griffin Lacey, Lex Fridman, Deepak Chandra, Brandon Barbello, Graham Taylor category:cs.LG cs.CV cs.NE published:2015-11-12 summary:We present a large-scale study exploring the capability of temporal deepneural networks to interpret natural human kinematics and introduce the firstmethod for active biometric authentication with mobile inertial sensors. AtGoogle, we have created a first-of-its-kind dataset of human movements,passively collected by 1500 volunteers using their smartphones daily overseveral months. We (1) compare several neural architectures for efficientlearning of temporal multi-modal data representations, (2) propose an optimizedshift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate thediscriminatively-trained dynamic features in a probabilistic generativeframework taking into account temporal characteristics. Our results demonstratethat human kinematics convey important information about user identity and canserve as a valuable component of multi-modal authentication systems.
arxiv-14400-130 | Basic Level Categorization Facilitates Visual Object Recognition | http://arxiv.org/abs/1511.04103 | author:Panqu Wang, Garrison W. Cottrell category:cs.CV published:2015-11-12 summary:Recent advances in deep learning have led to significant progress in thecomputer vision field, especially for visual object recognition tasks. Thefeatures useful for object classification are learned by feed-forward deepconvolutional neural networks (CNNs) automatically, and they are shown to beable to predict and decode neural representations in the ventral visual pathwayof humans and monkeys. However, despite the huge amount of work on optimizingCNNs, there has not been much research focused on linking CNNs with guidingprinciples from the human visual cortex. In this work, we propose a networkoptimization strategy inspired by both of the developmental trajectory ofchildren's visual object recognition capabilities, and Bar (2003), whohypothesized that basic level information is carried in the fast magnocellularpathway through the prefrontal cortex (PFC) and then projected back to inferiortemporal cortex (IT), where subordinate level categorization is achieved. Weinstantiate this idea by training a deep CNN to perform basic level objectcategorization first, and then train it on subordinate level categorization. Weapply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,demonstrating the effectiveness of the method. We also show that subsequenttransfer learning on smaller datasets gives superior results.
arxiv-14400-131 | Document Context Language Models | http://arxiv.org/abs/1511.03962 | author:Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, Jacob Eisenstein category:cs.CL cs.LG stat.ML published:2015-11-12 summary:Text documents are structured on multiple levels of detail: individual wordsare related by syntax, but larger units of text are related by discoursestructure. Existing language models generally fail to account for discoursestructure, but it is crucial if we are to have language models that rewardcoherence and generate coherent texts. We present and empirically evaluate aset of multi-level recurrent neural network language models, calledDocument-Context Language Models (DCLM), which incorporate contextualinformation both within and beyond the sentence. In comparison with word-levelrecurrent neural network language models, the DCLM models obtain slightlybetter predictive likelihoods, and considerably better assessments of documentcoherence.
arxiv-14400-132 | Shearlet-Based Detection of Flame Fronts | http://arxiv.org/abs/1511.03753 | author:Rafael Reisenhofer, Johannes Kiefer, Emily J. King category:cs.CV published:2015-11-12 summary:Identifying and characterizing flame fronts is the most common task in thecomputer-assisted analysis of data obtained from imaging techniques such asplanar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), orparticle imaging velocimetry (PIV). We present a novel edge and ridge (line)detection algorithm based on complex-valued wavelet-like analyzing functions --so-called complex shearlets -- displaying several traits useful for theextraction of flame fronts. In addition to providing a unified approach to thedetection of edges and ridges, our method inherently yields estimates of localtangent orientations and local curvatures. To examine the applicability forhigh-frequency recordings of combustion processes, the algorithm is applied tomock images distorted with varying degrees of noise and real-world PLIF imagesof both OH and CH radicals. Furthermore, we compare the performance of thenewly proposed complex shearlet-based measure to well-established edge andridge detection techniques such as the Canny edge detector, anothershearlet-based edge detector, and the phase congruency measure.
arxiv-14400-133 | Representational Distance Learning for Deep Neural Networks | http://arxiv.org/abs/1511.03979 | author:Patrick McClure, Nikolaus Kriegeskorte category:cs.NE cs.CV published:2015-11-12 summary:We propose representational distance learning (RDL), a technique that allowstransferring knowledge from an arbitrary model with task related information toa deep neural network (DNN). This method seeks to maximize the similaritybetween the representational distance matrices (RDMs) of a model with desiredknowledge, the teacher, and a DNN currently being trained, the student. Theknowledge contained in the information transformations performed by the teacherare transferred to a student using auxiliary error functions. This allows a DNNto simultaneously learn from a teacher model and learn to perform some taskwithin the framework of backpropagation. We test the use of RDL for knowledgedistillation, also known as model compression, from a large teacher DNN to asmall student DNN using the MNIST and CIFAR-10 datasets. Also, we test the useof RDL for knowledge transfer between tasks using the CIFAR-10 and CIFAR-100datasets. For each test, RDL significantly improves performance when comparedto traditional backpropagation alone and performs similarly to, or better than,recently proposed methods for model compression and knowledge transfer.
arxiv-14400-134 | Grounding of Textual Phrases in Images by Reconstruction | http://arxiv.org/abs/1511.03745 | author:Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt Schiele category:cs.CV cs.CL cs.LG published:2015-11-12 summary:Grounding (i.e. localizing) arbitrary, free-form textual phrases in visualcontent is a challenging problem with many applications for human-computerinteraction and image-text reference resolution. Few datasets provide theground truth spatial localization of phrases, thus it is desirable to learnfrom data with no or little grounding supervision. We propose a novel approachwhich learns grounding by reconstructing a given phrase using an attentionmechanism, which can be either latent or optimized directly. During trainingour model encodes the phrase using a recurrent network language model and thenlearns to attend to the relevant image region in order to reconstruct the inputphrase. At test time, the correct attention, i.e., the grounding, is evaluated.If grounding supervision is available it can be directly applied via a lossover the attention mechanism. We demonstrate the effectiveness of our approachon the Flickr 30k Entities and ReferItGame datasets with different levels ofsupervision, ranging from no supervision over partial supervision to fullsupervision. Our supervised variant improves by a large margin over thestate-of-the-art on both datasets.
arxiv-14400-135 | Feature Learning based Deep Supervised Hashing with Pairwise Labels | http://arxiv.org/abs/1511.03855 | author:Wu-Jun Li, Sheng Wang, Wang-Cheng Kang category:cs.LG cs.CV H.3.1 published:2015-11-12 summary:Recent years have witnessed wide application of hashing for large-scale imageretrieval. However, most existing hashing methods are based on hand-craftedfeatures which might not be optimally compatible with the hashing procedure.Recently, deep hashing methods have been proposed to perform simultaneousfeature learning and hash-code learning with deep neural networks, which haveshown better performance than traditional hashing methods with hand-craftedfeatures. Most of these deep hashing methods are supervised whose supervisedinformation is given with triplet labels. For another common applicationscenario with pairwise labels, there have not existed methods for simultaneousfeature learning and hash-code learning. In this paper, we propose a novel deephashing method, called deep pairwise-supervised hashing(DPSH), to performsimultaneous feature learning and hash-code learning for applications withpairwise labels. Experiments on real datasets show that our DPSH method canoutperform other methods to achieve the state-of-the-art performance in imageretrieval applications.
arxiv-14400-136 | LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement | http://arxiv.org/abs/1511.03995 | author:Kin Gwn Lore, Adedotun Akintayo, Soumik Sarkar category:cs.CV published:2015-11-12 summary:In surveillance, monitoring and tactical reconnaissance, gathering the rightvisual information from a dynamic environment and accurately processing suchdata are essential ingredients to making informed decisions which determinesthe success of an operation. Camera sensors are often cost-limited in abilityto clearly capture objects without defects from images or videos taken in apoorly-lit environment. The goal in many applications is to enhance thebrightness, contrast and reduce noise content of such images in an on-boardreal-time manner. We propose a deep autoencoder-based approach to identifysignal features from low-light images handcrafting and adaptively brightenimages without over-amplifying the lighter parts in images (i.e., withoutsaturation of image pixels) in high dynamic range. We show that a variant ofthe recently proposed stacked-sparse denoising autoencoder can learn toadaptively enhance and denoise from synthetically darkened and noisy trainingexamples. The network can then be successfully applied to naturally low-lightenvironment and/or hardware degraded images. Results show significantcredibility of deep learning based approaches both visually and by quantitativecomparison with various popular enhancing, state-of-the-art denoising andhybrid enhancing-denoising techniques.
arxiv-14400-137 | A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural Language | http://arxiv.org/abs/1511.03924 | author:Normunds Gruzitis, Dana Dannélls category:cs.CL published:2015-11-12 summary:Berkeley FrameNet is a lexico-semantic resource for English based on thetheory of frame semantics. It has been exploited in a range of natural languageprocessing applications and has inspired the development of framenets for manylanguages. We present a methodological approach to the extraction andgeneration of a computational multilingual FrameNet-based grammar and lexicon.The approach leverages FrameNet-annotated corpora to automatically extract aset of cross-lingual semantico-syntactic valence patterns. Based on data fromBerkeley FrameNet and Swedish FrameNet, the proposed approach has beenimplemented in Grammatical Framework (GF), a categorial grammar formalismspecialized for multilingual grammars. The implementation of the grammar andlexicon is supported by the design of FrameNet, providing a frame semanticabstraction layer, an interlingual semantic API (application programminginterface), over the interlingual syntactic API already provided by GF ResourceGrammar Library. The evaluation of the acquired grammar and lexicon shows thefeasibility of the approach. Additionally, we illustrate how the FrameNet-basedgrammar and lexicon are exploited in two distinct multilingual controllednatural language applications. The produced resources are available under anopen source license.
arxiv-14400-138 | Private False Discovery Rate Control | http://arxiv.org/abs/1511.03803 | author:Cynthia Dwork, Weijie Su, Li Zhang category:math.ST cs.DS stat.ML stat.TH published:2015-11-12 summary:We provide the first differentially private algorithms for controlling thefalse discovery rate (FDR) in multiple hypothesis testing, with essentially noloss in power under certain conditions. Our general approach is to adapt awell-known variant of the Benjamini-Hochberg procedure (BHq), making each stepdifferentially private. This destroys the classical proof of FDR control. Toprove FDR control of our method, (a) we develop a new proof of the original(non-private) BHq algorithm and its robust variants -- a proof requiring onlythe assumption that the true null test statistics are independent, allowing forarbitrary correlations between the true nulls and false nulls. This assumptionis fairly weak compared to those previously shown in the vast literature onthis topic, and explains in part the empirical robustness of BHq. Then (b) werelate the FDR control properties of the differentially private version to thecontrol properties of the non-private version. \end{enumerate} We also presenta low-distortion "one-shot" differentially private primitive for "top $k$"problems, e.g., "Which are the $k$ most popular hobbies?" (which we apply to:"Which hypotheses have the $k$ most significant $p$-values?"), and use it toget a faster privacy-preserving instantiation of our general approach at littlecost in accuracy. The proof of privacy for the one-shot top~$k$ algorithmintroduces a new technique of independent interest.
arxiv-14400-139 | Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints | http://arxiv.org/abs/1511.03760 | author:Mengdi Wang, Yichen Chen, Jialin Liu, Yuantao Gu category:stat.ML cs.LG math.OC published:2015-11-12 summary:Consider convex optimization problems subject to a large number ofconstraints. We focus on stochastic problems in which the objective takes theform of expected values and the feasible set is the intersection of a largenumber of convex sets. We propose a class of algorithms that perform bothstochastic gradient descent and random feasibility updates simultaneously. Atevery iteration, the algorithms sample a number of projection points onto arandomly selected small subsets of all constraints. Three feasibility updateschemes are considered: averaging over random projected points, projecting ontothe most distant sample, projecting onto a special polyhedral set constructedbased on sample points. We prove the almost sure convergence of thesealgorithms, and analyze the iterates' feasibility error and optimality error,respectively. We provide new convergence rate benchmarks for stochasticfirst-order optimization with many constraints. The rate analysis and numericalexperiments reveal that the algorithm using the polyhedral-set projectionscheme is the most efficient one within known algorithms.
arxiv-14400-140 | Efficient non-greedy optimization of decision trees | http://arxiv.org/abs/1511.04056 | author:Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli category:cs.LG cs.CV published:2015-11-12 summary:Decision trees and randomized forests are widely used in computer vision andmachine learning. Standard algorithms for decision tree induction optimize thesplit functions one node at a time according to some splitting criteria. Thisgreedy procedure often leads to suboptimal trees. In this paper, we present analgorithm for optimizing the split functions at all levels of the tree jointlywith the leaf parameters, based on a global objective. We show that the problemof finding optimal linear-combination (oblique) splits for decision trees isrelated to structured prediction with latent variables, and we formulate aconvex-concave upper bound on the tree's empirical loss. The run-time ofcomputing the gradient of the proposed surrogate objective with respect to eachtraining exemplar is quadratic in the the tree depth, and thus training deeptrees is feasible. The use of stochastic gradient descent for optimizationenables effective training with large datasets. Experiments on severalclassification benchmarks demonstrate that the resulting non-greedy decisiontrees outperform greedy decision tree baselines.
arxiv-14400-141 | Properly Learning Poisson Binomial Distributions in Almost Polynomial Time | http://arxiv.org/abs/1511.04066 | author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.LG math.ST stat.TH published:2015-11-12 summary:We give an algorithm for properly learning Poisson binomial distributions. APoisson binomial distribution (PBD) of order $n$ is the discrete probabilitydistribution of the sum of $n$ mutually independent Bernoulli random variables.Given $\widetilde{O}(1/\epsilon^2)$ samples from an unknown PBD $\mathbf{p}$,our algorithm runs in time $(1/\epsilon)^{O(\log \log (1/\epsilon))}$, andoutputs a hypothesis PBD that is $\epsilon$-close to $\mathbf{p}$ in totalvariation distance. The previously best known running time for properlylearning PBDs was $(1/\epsilon)^{O(\log(1/\epsilon))}$. As one of our main contributions, we provide a novel structuralcharacterization of PBDs. We prove that, for all $\epsilon >0,$ there exists anexplicit collection $\cal{M}$ of $(1/\epsilon)^{O(\log \log (1/\epsilon))}$vectors of multiplicities, such that for any PBD $\mathbf{p}$ there exists aPBD $\mathbf{q}$ with $O(\log(1/\epsilon))$ distinct parameters whosemultiplicities are given by some element of ${\cal M}$, such that $\mathbf{q}$is $\epsilon$-close to $\mathbf{p}$. Our proof combines tools from Fourieranalysis and algebraic geometry. Our approach to the proper learning problem is as follows: Starting with anaccurate non-proper hypothesis, we fit a PBD to this hypothesis. Morespecifically, we essentially start with the hypothesis computed by thecomputationally efficient non-proper learning algorithm in our recentwork~\cite{DKS15}. Our aforementioned structural characterization allows us toreduce the corresponding fitting problem to a collection of$(1/\epsilon)^{O(\log \log(1/\epsilon))}$ systems of low-degree polynomialinequalities. We show that each such system can be solved in time$(1/\epsilon)^{O(\log \log(1/\epsilon))}$, which yields the overall runningtime of our algorithm.
arxiv-14400-142 | Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images | http://arxiv.org/abs/1511.04048 | author:Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi category:cs.CV published:2015-11-12 summary:In this paper, we study the challenging problem of predicting the dynamics ofobjects in static images. Given a query object in an image, our goal is toprovide a physical understanding of the object in terms of the forces actingupon it and its long term motion as response to those forces. Direct andexplicit estimation of the forces and the motion of objects from a single imageis extremely challenging. We define intermediate physical abstractions calledNewtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learnsto map a single image to a state in a Newtonian scenario. Our experimentalevaluations show that our method can reliably predict dynamics of a queryobject from a single image. In addition, our approach can provide physicalreasoning that supports the predicted dynamics in terms of velocity and forcevectors. To spur research in this direction we compiled Visual NewtonianDynamics (VIND) dataset that includes 6806 videos aligned with Newtonianscenarios represented using game engines, and 4516 still images with theirground truth dynamics.
arxiv-14400-143 | Bayesian Analysis of Dynamic Linear Topic Models | http://arxiv.org/abs/1511.03947 | author:Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard category:stat.ML cs.LG stat.ME published:2015-11-12 summary:In dynamic topic modeling, the proportional contribution of a topic to adocument depends on the temporal dynamics of that topic's overall prevalence inthe corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) byexplicitly modeling document level topic proportions with covariates anddynamic structure that includes polynomial trends and periodicity. A MarkovChain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentationis developed for posterior inference. Conditional independencies in the modeland sampling are made explicit, and our MCMC algorithm is parallelized wherepossible to allow for inference in large corpora. To address computationalbottlenecks associated with Polya-Gamma sampling, we appeal to the CentralLimit Theorem to develop a Gaussian approximation to the Polya-Gamma randomvariable. This approximation is fast and reliable for parameter values relevantin the text mining domain. Our model and inference algorithm are validated withmultiple simulation examples, and we consider the application of modelingtrends in PubMed abstracts. We demonstrate that sharing information acrossdocuments is critical for accurately estimating document-specific topicproportions. We also show that explicitly modeling polynomial and periodicbehavior improves our ability to predict topic prevalence at future timepoints.
arxiv-14400-144 | Deep Gaussian Conditional Random Field Network: A Model-based Deep Network for Discriminative Denoising | http://arxiv.org/abs/1511.04067 | author:Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu category:cs.CV published:2015-11-12 summary:We propose a novel deep network architecture for image\\ denoising based on aGaussian Conditional Random Field (GCRF) model. In contrast to the existingdiscriminative denoising methods that train a separate model for each noiselevel, the proposed deep network explicitly models the input noise variance andhence is capable of handling a range of noise levels. Our deep network, whichwe refer to as deep GCRF network, consists of two sub-networks: (i) a parametergeneration network that generates the pairwise potential parameters based onthe noisy input image, and (ii) an inference network whose layers perform thecomputations involved in an iterative GCRF inference procedure.\ We train theentire deep GCRF network (both parameter generation and inference networks)discriminatively in an end-to-end fashion by maximizing the peaksignal-to-noise ratio measure. Experiments on Berkeley segmentation andPASCALVOC datasets show that the proposed deep GCRF network outperformsstate-of-the-art image denoising approaches for several noise levels.
arxiv-14400-145 | Human Curation and Convnets: Powering Item-to-Item Recommendations on Pinterest | http://arxiv.org/abs/1511.04003 | author:Dmitry Kislyuk, Yuchen Liu, David Liu, Eric Tzeng, Yushi Jing category:cs.CV published:2015-11-12 summary:This paper presents Pinterest Related Pins, an item-to-item recommendationsystem that combines collaborative filtering with content-based ranking. Wedemonstrate that signals derived from user curation, the activity of usersorganizing content, are highly effective when used in conjunction withcontent-based ranking. This paper also demonstrates the effectiveness of visualfeatures, such as image or object representations learned from convnets, inimproving the user engagement rate of our item-to-item recommendation system.
arxiv-14400-146 | Hand-Object Interaction and Precise Localization in Transitive Action Recognition | http://arxiv.org/abs/1511.03814 | author:Amir Rosenfeld, Shimon Ullman category:cs.CV published:2015-11-12 summary:Action recognition in still images has seen major improvement in recent yearsdue to advances in human pose estimation, object recognition and strongerfeature representations produced by deep neural networks. However, there arestill many cases in which performance remains far from that of humans. A majordifficulty arises in distinguishing between transitive actions in which theoverall actor pose is similar, and recognition therefore depends on details ofthe grasp and the object, which may be largely occluded. In this paper wedemonstrate how recognition is improved by obtaining precise localization ofthe action-object and consequently extracting details of the object shapetogether with the actor-object interaction. To obtain exact localization of theaction object and its interaction with the actor, we employ a coarse-to-fineapproach which combines semantic segmentation and contextual features, insuccessive stages. We focus on (but are not limited) to face-related actions, aset of actions that includes several currently challenging categories. Wepresent an average relative improvement of 35% over state-of-the art andvalidate through experimentation the effectiveness of our approach.
arxiv-14400-147 | Going Deeper in Facial Expression Recognition using Deep Neural Networks | http://arxiv.org/abs/1511.04110 | author:Ali Mollahosseini, David Chan, Mohammad H. Mahoor category:cs.NE cs.CV published:2015-11-12 summary:Automated Facial Expression Recognition (FER) has remained a challenging andinteresting problem. Despite efforts made in developing various methods forFER, existing approaches traditionally lack generalizability when applied tounseen images or those that are captured in wild setting. Most of the existingapproaches are based on engineered features (e.g. HOG, LBPH, and Gabor) wherethe classifier's hyperparameters are tuned to give best recognition accuraciesacross a single database, or a small collection of similar databases.Nevertheless, the results are not significant when they are applied to noveldata. This paper proposes a deep neural network architecture to address the FERproblem across multiple well-known standard face datasets. Specifically, ournetwork consists of two convolutional layers each followed by max pooling andthen four Inception layers. The network is a single component architecture thattakes registered facial images as the input and classifies them into either ofthe six basic or the neutral expressions. We conducted comprehensiveexperiments on seven publically available facial expression databases, viz.MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposedarchitecture are comparable to or better than the state-of-the-art methods andbetter than traditional convolutional neural networks and in both accuracy andtraining time.
arxiv-14400-148 | Characterizing Concept Drift | http://arxiv.org/abs/1511.03816 | author:Geoffrey I. Webb, Roy Hyde, Hong Cao, Hai Long Nguyen, Francois Petitjean category:cs.LG cs.AI published:2015-11-12 summary:Most machine learning models are static, but the world is dynamic, andincreasing online deployment of learned models gives increasing urgency to thedevelopment of efficient and effective mechanisms to address learning in thecontext of non-stationary distributions, or as it is commonly called conceptdrift. However, the key issue of characterizing the different types of driftthat can occur has not previously been subjected to rigorous definition andanalysis. In particular, while some qualitative drift categorizations have beenproposed, few have been formally defined, and the quantitative descriptionsrequired for precise and objective understanding of learner performance havenot existed. We present the first comprehensive framework for quantitativeanalysis of drift. This supports the development of the first comprehensive setof formal definitions of types of concept drift. The formal definitions clarifyambiguities and identify gaps in previous definitions, giving rise to a newcomprehensive taxonomy of concept drift types and a solid foundation forresearch into mechanisms to detect and address concept drift.
arxiv-14400-149 | Facial Landmark Detection with Tweaked Convolutional Neural Networks | http://arxiv.org/abs/1511.04031 | author:Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan category:cs.CV published:2015-11-12 summary:We present a novel convolutional neural network (CNN) design for faciallandmark coordinate regression. We examine the intermediate features of astandard CNN trained for landmark detection and show that features extractedfrom later, more specialized layers capture rough landmark locations. Thisprovides a natural means of applying differential treatment midway through thenetwork, tweaking processing based on facial alignment. The resulting TweakedCNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in anappearance-sensitive manner without training multi-part or multi-scale models.Our results on standard face landmark detection and face verificationbenchmarks show TCNN to surpasses previously published performances by widemargins.
arxiv-14400-150 | Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control | http://arxiv.org/abs/1511.03791 | author:Fangyi Zhang, Jürgen Leitner, Michael Milford, Ben Upcroft, Peter Corke category:cs.LG cs.CV cs.RO published:2015-11-12 summary:This paper introduces a machine learning based system for controlling arobotic manipulator with visual perception only. The capability to autonomouslylearn robot controllers solely from raw-pixel images and without any priorknowledge of configuration is shown for the first time. We build upon thesuccess of recent deep reinforcement learning and develop a system for learningtarget reaching with a three-joint robot manipulator using external visualobservation. A Deep Q Network (DQN) was demonstrated to perform target reachingafter training in simulation. Transferring the network to real hardware andreal observation in a naive approach failed, but experiments show that thenetwork works when replacing camera images with synthetic images.
arxiv-14400-151 | On the Optimal Sample Complexity for Best Arm Identification | http://arxiv.org/abs/1511.03774 | author:Lijie Chen, Jian Li category:cs.LG cs.DS published:2015-11-12 summary:We study the best arm identification (BEST-1-ARM) problem, which is definedas follows. We are given $n$ stochastic bandit arms. The $i$th arm has a rewarddistribution $D_i$ with an unknown mean $\mu_i$. Upon each play of the $i$tharm, we can get a reward, sampled i.i.d. from $D_i$. We would like to identifythe arm with largest mean with probability at least $1-\delta$, using as fewsamples as possible. We also study an important special case where there areonly two arms, which we call the sign problem. We achieve a very detailedunderstanding of the optimal sample complexity of sign, simplifying andsignificantly extending a classical result by Farrell in 1964, with acompletely new proof. Using the new lower bound for sign, we obtain the firstlower bound for BEST-1-ARM that goes beyond the classic Mannor-Tsitsiklis lowerbound, by an interesting reduction from sign to BEST-1-ARM. To complement our lower bound, we also provide a nontrivial algorithm forBEST-1-ARM, which achieves a worst case optimal sample complexity, improvingupon several prior upper bounds on the same problem.
arxiv-14400-152 | Nonparametric Estimation of Scale-Free Graphical Models | http://arxiv.org/abs/1511.03796 | author:Yuancheng Zhu, Zhe Liu, Siqi Sun category:stat.ME cs.LG stat.ML published:2015-11-12 summary:We present a nonparametric method for estimating scale-free graphical models.To avoid the usual Gaussian assumption, we restrict the graph to be a forestand build on the work of forest density estimation. The method is motivatedfrom a Bayesian perspective and is equivalent to finding the maximum spanningtree of a weighted graph with a log degree penalty. We solve the optimizationproblem via a minorize-maximization procedure with Kruskal's algorithm.Simulations show that the proposed method outperforms competing parametricmethods, and is robust to the true data distribution. It also leads toimprovement in predictive power and interpretability in two real data examples.
arxiv-14400-153 | Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester Using Artificial Neural Networks and Support Vector Machine | http://arxiv.org/abs/1511.03984 | author:Run Wang, Qiaoli Mo, Qian Zhang, Fudi Chen, Dazuo Yang category:cs.LG cs.NE published:2015-11-12 summary:3\b{eta}-O-phthalic ester of betulinic acid is of great importance inanticancer studies. However, the optimization of its reaction conditionsrequires a large number of experimental works. To simplify the number of timesof optimization in experimental works, here, we use artificial neural network(ANN) and support vector machine (SVM) models for the prediction of yields of3\b{eta}-O-phthalic ester of betulinic acid synthesized by betulinic acid andphthalic anhydride using lipase as biocatalyst. General regression neuralnetwork (GRNN), multilayer feed-forward neural network (MLFN) and the SVMmodels were trained based on experimental data. Four indicators were set asindependent variables, including time (h), temperature (C), amount of enzyme(mg) and molar ratio, while the yield of the 3\b{eta}-O-phthalic ester ofbetulinic acid was set as the dependent variable. Results show that the GRNNand SVM models have the best prediction results during the testing process,with comparatively low RMS errors (4.01 and 4.23respectively) and shorttraining times (both 1s). The prediction accuracy of the GRNN and SVM are both100% in testing process, under the tolerance of 30%.
arxiv-14400-154 | Automatic Inference of the Quantile Parameter | http://arxiv.org/abs/1511.03990 | author:Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan category:stat.ML published:2015-11-12 summary:Supervised learning is an active research area, with numerous applications indiverse fields such as data analytics, computer vision, speech and audioprocessing, and image understanding. In most cases, the loss functions used inmachine learning assume symmetric noise models, and seek to estimate theunknown function parameters. However, loss functions such as quantile andquantile Huber generalize the symmetric $\ell_1$ and Huber losses to theasymmetric setting, for a fixed quantile parameter. In this paper, we proposeto jointly infer the quantile parameter and the unknown function parameters,for the asymmetric quantile Huber and quantile losses. We explore variousproperties of the quantile Huber loss and implement a convexity certificatethat can be used to check convexity in the quantile parameter. When the loss ifconvex with respect to the parameter of the function, we prove that it isbiconvex in both the function and the quantile parameters, and propose analgorithm to jointly estimate these. Results with synthetic and real datademonstrate that the proposed approach can automatically recover the quantileparameter corresponding to the noise and also provide an improved recovery offunction parameters. To illustrate the potential of the framework, we extendthe gradient boosting machines with quantile losses to automatically estimatethe quantile parameter at each iteration.
arxiv-14400-155 | Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach | http://arxiv.org/abs/1511.03766 | author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG published:2015-11-12 summary:In this paper, we develop a randomized algorithm and theory for learning asparse model from large-scale and high-dimensional data, which is usuallyformulated as an empirical risk minimization problem with a sparsity-inducingregularizer. Under the assumption that there exists a (approximately) sparsesolution with high classification accuracy, we argue that the dual solution isalso sparse or approximately sparse. The fact that both primal and dualsolutions are sparse motivates us to develop a randomized approach for ageneral convex-concave optimization problem. Specifically, the proposedapproach combines the strength of random projection with that of sparselearning: it utilizes random projection to reduce the dimensionality, andintroduces $\ell_1$-norm regularization to alleviate the approximation errorcaused by random projection. Theoretical analysis shows that under favoredconditions, the randomized algorithm can accurately recover the optimalsolutions to the convex-concave optimization problem (i.e., recover both theprimal and dual solutions). Furthermore, the solutions returned by ouralgorithm are guaranteed to be approximately sparse.
arxiv-14400-156 | Automatic Content-Aware Color and Tone Stylization | http://arxiv.org/abs/1511.03748 | author:Joon-Young Lee, Kalyan Sunkavalli, Zhe Lin, Xiaohui Shen, In So Kweon category:cs.CV published:2015-11-12 summary:We introduce a new technique that automatically generates diverse, visuallycompelling stylizations for a photograph in an unsupervised manner. We achievethis by learning style ranking for a given input using a large photo collectionand selecting a diverse subset of matching styles for final style transfer. Wealso propose a novel technique that transfers the global color and tone of thechosen exemplars to the input photograph while avoiding the common visualartifacts produced by the existing style transfer methods. Together, our styleselection and transfer techniques produce compelling, artifact-free results ona wide range of input photographs, and a user study shows that our results arepreferred over other techniques.
arxiv-14400-157 | Doubly Robust Off-policy Value Evaluation for Reinforcement Learning | http://arxiv.org/abs/1511.03722 | author:Nan Jiang, Lihong Li category:cs.LG cs.AI cs.SY stat.ME stat.ML published:2015-11-11 summary:We study the problem of off-policy value evaluation in reinforcement learning(RL), where one aims to estimate the value of a new policy based on datacollected by a different policy. This problem is often a critical step whenapplying RL in real-world problems. Despite its importance, existing generalmethods either have uncontrolled bias or suffer high variance. In this work, weextend the doubly robust estimator for bandits to sequential decision-makingproblems, which gets the best of both worlds: it is guaranteed to be unbiasedand can have a much lower variance than the popular importance samplingestimators. We demonstrate the estimator's accuracy in several benchmarkproblems, and illustrate its use as a subroutine in safe policy improvement. Wealso provide theoretical results on the hardness of the problem, and show thatour estimator can match the lower bound in certain scenarios.
arxiv-14400-158 | Instantaneous Modelling and Reverse Engineering of DataConsistent Prime Models in Seconds! | http://arxiv.org/abs/1511.03472 | author:Michael A. Idowu category:q-bio.QM nlin.AO stat.ML published:2015-11-11 summary:A theoretical framework that supports automated construction of dynamic primemodels purely from experimental time series data has been invented anddeveloped, which can automatically generate (construct) data-driven models ofany time series data in seconds. This has resulted in the formulation andformalisation of new reverse engineering and dynamic methods for automatedsystems modelling of complex systems, including complex biological, financial,control, and artificial neural network systems. The systems/model theory behindthe invention has been formalised as a new, effective and robust systemidentification strategy complementary to process-based modelling. The proposeddynamic modelling and network inference solutions often involve tacklingextremely difficult parameter estimation challenges, inferring unknownunderlying network structures, and unsupervised formulation and construction ofsmart and intelligent ODE models of complex systems. In underdeterminedconditions, i.e., cases of dealing with how best to instantaneously and rapidlyconstruct data-consistent prime models of unknown (or well-studied) complexsystem from small-sized time series data, inference of unknown underlyingnetwork of interaction is more challenging. This article reports a robuststep-by-step mathematical and computational analysis of the entire prime modelconstruction process that determines a model from data in less than a minute.
arxiv-14400-159 | Hierarchical Latent Semantic Mapping for Automated Topic Generation | http://arxiv.org/abs/1511.03546 | author:Guorui Zhou, Guang Chen category:cs.LG cs.CL cs.IR published:2015-11-11 summary:Much of information sits in an unprecedented amount of text data. Managingallocation of these large scale text data is an important problem for manyareas. Topic modeling performs well in this problem. The traditional generativemodels (PLSA,LDA) are the state-of-the-art approaches in topic modeling andmost recent research on topic generation has been focusing on improving orextending these models. However, results of traditional generative models aresensitive to the number of topics K, which must be specified manually. Theproblem of generating topics from corpus resembles community detection innetworks. Many effective algorithms can automatically detect communities fromnetworks without a manually specified number of the communities. Inspired bythese algorithms, in this paper, we propose a novel method named HierarchicalLatent Semantic Mapping (HLSM), which automatically generates topics fromcorpus. HLSM calculates the association between each pair of words in thelatent topic space, then constructs a unipartite network of words with thisassociation and hierarchically generates topics from this network. We applyHLSM to several document collections and the experimental comparisons againstseveral state-of-the-art approaches demonstrate the promising performance.
arxiv-14400-160 | A Directional Diffusion Algorithm for Inpainting | http://arxiv.org/abs/1511.03464 | author:Jan Deriu, Rolf Jagerman, Kai-En Tsay category:cs.CV published:2015-11-11 summary:The problem of inpainting involves reconstructing the missing areas of animage. Inpainting has many applications, such as reconstructing old damagedphotographs or removing obfuscations from images. In this paper we present thedirectional diffusion algorithm for inpainting. Typical diffusion algorithmsare bad at propagating edges from the image into the unknown masked regions.The directional diffusion algorithm improves on the regular diffusion algorithmby reconstructing edges more accurately. It scores better than regulardiffusion when reconstructing images that are obfuscated by a text mask.
arxiv-14400-161 | Deep Multimodal Semantic Embeddings for Speech and Images | http://arxiv.org/abs/1511.03690 | author:David Harwath, James Glass category:cs.CV cs.AI cs.CL published:2015-11-11 summary:In this paper, we present a model which takes as input a corpus of imageswith relevant spoken captions and finds a correspondence between the twomodalities. We employ a pair of convolutional neural networks to model visualobjects and speech signals at the word level, and tie the networks togetherwith an embedding and alignment model which learns a joint semantic space overboth modalities. We evaluate our model using image search and annotation taskson the Flickr8k dataset, which we augmented by collecting a corpus of 40,000spoken captions using Amazon Mechanical Turk.
arxiv-14400-162 | An Analytic Expression of Performance Rate, Fitness Value and Average Convergence Rate for a Class of Evolutionary Algorithms | http://arxiv.org/abs/1511.03483 | author:Jun He category:cs.NE published:2015-11-11 summary:An important theoretical question in evolutionary computation is how goodsolutions evolutionary algorithms can produce. This paper aims to provide ananalytic analysis of solution quality of evolutionary algorithms in terms ofthe performance rate, which is defined by the difference between 1 and theapproximation ratio of the best solution found in each generation. Theperformance rate can be represented by a function of time. With the help ofmatrix analysis, it is possible to obtain an exact expression of such afunction. For the first time, an analytic expression for calculating theperformance rate is presented in this paper for a class of evolutionaryalgorithms, that is, (1+1) strictly elitist evolution algorithms. Furthermore,analytic expressions for calculate the fitness value and the averageconvergence rate in each generation are also derived for this class ofevolutionary algorithms. The approach is promising, and it can be extended tonon-elitist or population-based algorithms too.
arxiv-14400-163 | Online Principal Component Analysis in High Dimension: Which Algorithm to Choose? | http://arxiv.org/abs/1511.03688 | author:Hervé Cardot, David Degras category:stat.ML cs.LG stat.ME published:2015-11-11 summary:In the current context of data explosion, online techniques that do notrequire storing all data in memory are indispensable to routinely perform taskslike principal component analysis (PCA). Recursive algorithms that update thePCA with each new observation have been studied in various fields of researchand found wide applications in industrial monitoring, computer vision,astronomy, and latent semantic indexing, among others. This work providesguidance for selecting an online PCA algorithm in practice. We present the mainapproaches to online PCA, namely, perturbation techniques, incremental methods,and stochastic optimization, and compare their statistical accuracy,computation time, and memory requirements using artificial and real data.Extensions to missing data and to functional data are discussed. All studiedalgorithms are available in the R package onlinePCA on CRAN.
arxiv-14400-164 | A Size-Free CLT for Poisson Multinomials and its Applications | http://arxiv.org/abs/1511.03641 | author:Constantinos Daskalakis, Anindya De, Gautam Kamath, Christos Tzamos category:cs.DS cs.GT cs.LG math.PR math.ST stat.TH published:2015-11-11 summary:An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of thesum of $n$ independent random vectors supported on the set ${\calB}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We showthat any $(n,k)$-PMD is ${\rm poly}\left({k\over \sigma}\right)$-close in totalvariation distance to the (appropriately discretized) multi-dimensionalGaussian with the same first two moments, removing the dependence on $n$ fromthe Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT isobtained by bootstrapping the Valiant-Valiant CLT itself through the structuralcharacterization of PMDs shown in recent work by Daskalakis, Kamath, andTzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTASfor approximate Nash equilibria in anonymous games, significantly improving thestate of the art, and matching qualitatively the running time dependence on $n$and $1/\varepsilon$ of the best known algorithm for two-strategy anonymousgames. Our new CLT also enables the construction of covers for the set of$(n,k)$-PMDs, which are proper and whose size is shown to be essentiallyoptimal. Our cover construction combines our CLT with the Shapley-Folkmantheorem and recent sparsification results for Laplacian matrices by Batson,Spielman, and Srivastava. Our cover size lower bound is based on an algebraicgeometric construction. Finally, leveraging the structural properties of theFourier spectrum of PMDs we show that these distributions can be learned from$O_k(1/\varepsilon^2)$ samples in ${\rm poly}_k(1/\varepsilon)$-time, removingthe quasi-polynomial dependence of the running time on $1/\varepsilon$ from thealgorithm of Daskalakis, Kamath, and Tzamos.
arxiv-14400-165 | Granger Causality in Multi-variate Time Series using a Time Ordered Restricted Vector Autoregressive Model | http://arxiv.org/abs/1511.03463 | author:Elsa Siggiridou, Dimitris Kugiumtzis category:stat.ME math.ST stat.CO stat.ML stat.TH published:2015-11-11 summary:Granger causality has been used for the investigation of the inter-dependencestructure of the underlying systems of multi-variate time series. Inparticular, the direct causal effects are commonly estimated by the conditionalGranger causality index (CGCI). In the presence of many observed variables andrelatively short time series, CGCI may fail because it is based on vectorautoregressive models (VAR) involving a large number of coefficients to beestimated. In this work, the VAR is restricted by a scheme that modifies therecently developed method of backward-in-time selection (BTS) of the laggedvariables and the CGCI is combined with BTS. Further, the proposed approach iscompared favorably to other restricted VAR representations, such as thetop-down strategy, the bottom-up strategy, and the least absolute shrinkage andselection operator (LASSO), in terms of sensitivity and specificity of CGCI.This is shown by using simulations of linear and nonlinear, low andhigh-dimensional systems and different time series lengths. For nonlinearsystems, CGCI from the restricted VAR representations are compared withanalogous nonlinear causality indices. Further, CGCI in conjunction with BTSand other restricted VAR representations is applied to multi-channel scalpelectroencephalogram (EEG) recordings of epileptic patients containingepileptiform discharges. CGCI on the restricted VAR, and BTS in particular,could track the changes in brain connectivity before, during and afterepileptiform discharges, which was not possible using the full VARrepresentation.
arxiv-14400-166 | Unifying distillation and privileged information | http://arxiv.org/abs/1511.03643 | author:David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, Vladimir Vapnik category:stat.ML cs.LG published:2015-11-11 summary:Distillation (Hinton et al., 2015) and privileged information (Vapnik &Izmailov, 2015) are two techniques that enable machines to learn from othermachines. This paper unifies these two techniques into generalizeddistillation, a framework to learn from multiple machines and datarepresentations. We provide theoretical and causal insight about the innerworkings of generalized distillation, extend it to unsupervised, semisupervisedand multitask learning scenarios, and illustrate its efficacy on a variety ofnumerical simulations on both synthetic and real-world data.
arxiv-14400-167 | Larger-Context Language Modelling | http://arxiv.org/abs/1511.03729 | author:Tian Wang, Kyunghyun Cho category:cs.CL published:2015-11-11 summary:In this work, we propose a novel method to incorporate corpus-level discourseinformation into language modelling. We call this larger-context languagemodel. We introduce a late fusion approach to a recurrent language model basedon long short-term memory units (LSTM), which helps the LSTM unit keepintra-sentence dependencies and inter-sentence dependencies separate from eachother. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank),we demon- strate that the proposed model improves perplexity significantly. Inthe experi- ments, we evaluate the proposed approach while varying the numberof context sentences and observe that the proposed late fusion is superior tothe usual way of incorporating additional inputs to the LSTM. By analyzing thetrained larger- context language model, we discover that content words,including nouns, adjec- tives and verbs, benefit most from an increasing numberof context sentences. This analysis suggests that larger-context language modelimproves the unconditional language model by capturing the theme of a documentbetter and more easily.
arxiv-14400-168 | Training Deep Gaussian Processes using Stochastic Expectation Propagation and Probabilistic Backpropagation | http://arxiv.org/abs/1511.03405 | author:Thang D. Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Richard E. Turner category:stat.ML published:2015-11-11 summary:Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisationsof Gaussian processes (GPs) and are formally equivalent to neural networks withmultiple, infinitely wide hidden layers. DGPs are probabilistic andnon-parametric and as such are arguably more flexible, have a greater capacityto generalise, and provide better calibrated uncertainty estimates thanalternative deep models. The focus of this paper is scalable approximateBayesian learning of these networks. The paper develops a novel and efficientextension of probabilistic backpropagation, a state-of-the-art method fortraining Bayesian neural networks, that can be used to train DGPs. The newmethod leverages a recently proposed method for scaling ExpectationPropagation, called stochastic Expectation Propagation. The method is able toautomatically discover useful input warping, expansion or compression, and itis therefore is a flexible form of Bayesian kernel design. We demonstrate thesuccess of the new method for supervised learning on several real-worlddatasets, showing that it typically outperforms GP regression and is never muchworse.
arxiv-14400-169 | Piecewise Linear Activation Functions For More Efficient Deep Networks | http://arxiv.org/abs/1511.03650 | author:Cheng-Yang Fu, Alexander C. Berg category:cs.CV published:2015-11-11 summary:This submission has been withdrawn by arXiv administrators because it isintentionally incomplete, which is in violation of our policies.
arxiv-14400-170 | A Continuous Max-Flow Approach to Cyclic Field Reconstruction | http://arxiv.org/abs/1511.03629 | author:John S. H. Baxter, Jonathan McLeod, Terry M. Peters category:cs.CV published:2015-11-11 summary:Reconstruction of an image from noisy data using Markov Random Field theoryhas been explored by both the graph-cuts and continuous max-flow community inthe form of the Potts and Ishikawa models. However, neither model takes intoaccount the particular cyclic topology of specific intensity types such as thehue in natural colour images, or the phase in complex valued MRI. This paperpresents \textit{cyclic continuous max-flow} image reconstruction which modelsthe intensity being reconstructed as having a fundamentally cyclic topology.This model complements the Ishikawa model in that it is designed with imagereconstruction in mind, having the topology of the intensity space inherent inthe model while being readily extendable to an arbitrary intensity resolution.
arxiv-14400-171 | God(s) Know(s): Developmental and Cross-Cultural Patterns in Children Drawings | http://arxiv.org/abs/1511.03466 | author:Ksenia Konyushkova, Nikolaos Arvanitopoulos, Zhargalma Dandarova Robert, Pierre-Yves Brandt, Sabine Süsstrunk category:cs.CV published:2015-11-11 summary:This paper introduces a novel approach to data analysis designed for theneeds of specialists in psychology of religion. We detect developmental andcross-cultural patterns in children's drawings of God(s) and other supernaturalagents. We develop methods to objectively evaluate our empirical observationsof the drawings with respect to: (1) the gravity center, (2) the averageintensities of the colors \emph{green} and \emph{yellow}, (3) the use ofdifferent colors (palette) and (4) the visual complexity of the drawings. Wefind statistically significant differences across ages and countries in thegravity centers and in the average intensities of colors. These findingssupport the hypotheses of the experts and raise new questions for furtherinvestigation.
arxiv-14400-172 | A GMM-Based Stair Quality Model for Human Perceived JPEG Images | http://arxiv.org/abs/1511.03398 | author:Sudeng Hu, Haiqiang Wang, C. -C. Jay Kuo category:cs.MM cs.CV published:2015-11-11 summary:Based on the notion of just noticeable differences (JND), a stair qualityfunction (SQF) was recently proposed to model human perception on JPEG images.Furthermore, a k-means clustering algorithm was adopted to aggregate JND datacollected from multiple subjects to generate a single SQF. In this work, wepropose a new method to derive the SQF using the Gaussian Mixture Model (GMM).The newly derived SQF can be interpreted as a way to characterize the meanviewer experience. Furthermore, it has a lower information criterion (BIC)value than the previous one, indicating that it offers a better model. Aspecific example is given to demonstrate the advantages of the new approach.
arxiv-14400-173 | Complete Dictionary Recovery over the Sphere I: Overview and the Geometric Picture | http://arxiv.org/abs/1511.03607 | author:Ju Sun, Qing Qu, John Wright category:cs.IT cs.CV math.IT math.OC stat.ML published:2015-11-11 summary:We consider the problem of recovering a complete (i.e., square andinvertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb{R}^{n \times p}$with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ issufficiently sparse. This recovery problem is central to the theoreticalunderstanding of dictionary learning, which seeks a sparse representation for acollection of input signals, and finds numerous applications in modern signalprocessing and machine learning. We give the first efficient algorithm thatprovably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros percolumn, under suitable probability model for $\mathbf X_0$. In contrast, priorresults based on efficient algorithms provide recovery guarantees when $\mathbfX_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta\in (0, 1)$. Our algorithmic pipeline centers around solving a certain nonconvexoptimization problem with a spherical constraint. In this paper, we provide ageometric characterization of the high-dimensional objective landscape. Inparticular, we show that the problem is highly structured: with highprobability there are no "spurious" local minimizers and all saddle points aresecond-order. This distinctive structure makes the problem amenable toefficient algorithms. In a companion paper (arXiv:1511.04777), we design asecond-order trust-region algorithm over the sphere that provably converges toa local minimizer with an arbitrary initialization, despite the presence ofsaddle points.
arxiv-14400-174 | The Fourier Transform of Poisson Multinomial Distributions and its Algorithmic Applications | http://arxiv.org/abs/1511.03592 | author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.GT cs.LG math.PR math.ST stat.TH published:2015-11-11 summary:An $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable ofthe form $X = \sum_{i=1}^n X_i$, where the $X_i$'s are independent randomvectors supported on the set of standard basis vectors in $\mathbb{R}^k.$ Inthis paper, we obtain a refined structural understanding of PMDs by analyzingtheir Fourier transform. As our core structural result, we prove that theFourier transform of PMDs is {\em approximately sparse}, i.e., roughlyspeaking, its $L_1$-norm is small outside a small set. By building on thisresult, we obtain the following applications: {\bf Learning Theory.} We design the first computationally efficient learningalgorithm for PMDs with respect to the total variation distance. Our algorithmlearns an arbitrary $(n, k)$-PMD within variation distance $\epsilon$ using anear-optimal sample size of $\widetilde{O}_k(1/\epsilon^2),$ and runs in time$\widetilde{O}_k(1/\epsilon^2) \cdot \log n.$ Previously, no algorithm with a$\mathrm{poly}(1/\epsilon)$ runtime was known, even for $k=3.$ {\bf Game Theory.} We give the first efficient polynomial-time approximationscheme (EPTAS) for computing Nash equilibria in anonymous games. For normalizedanonymous games with $n$ players and $k$ strategies, our algorithm computes awell-supported $\epsilon$-Nash equilibrium in time $n^{O(k^3)} \cdot(k/\epsilon)^{O(k^3\log(k/\epsilon)/\log\log(k/\epsilon))^{k-1}}.$ The bestprevious algorithm for this problem had running time $n^{(f(k)/\epsilon)^k},$where $f(k) = \Omega(k^{k^2})$, for any $k>2.$ {\bf Statistics.} We prove a multivariate central limit theorem (CLT) thatrelates an arbitrary PMD to a discretized multivariate Gaussian with the samemean and covariance, in total variation distance. Our new CLT strengthens theCLT of Valiant and Valiant by completely removing the dependence on $n$ in theerror bound.
arxiv-14400-175 | Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning | http://arxiv.org/abs/1511.03476 | author:Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang category:cs.CV published:2015-11-11 summary:Recently, deep learning approach, especially deep Convolutional NeuralNetworks (ConvNets), have achieved overwhelming accuracy with fast processingspeed for image classification. Incorporating temporal structure with deepConvNets for video representation becomes a fundamental problem for videocontent analysis. In this paper, we propose a new approach, namely HierarchicalRecurrent Neural Encoder (HRNE), to exploit temporal information of videos.Compared to recent video representation inference approaches, this paper makesthe following three contributions. First, our HRNE is able to efficientlyexploit video temporal structure in a longer range by reducing the length ofinput information flow, and compositing multiple consecutive inputs at a higherlevel. Second, computation operations are significantly lessened whileattaining more non-linearity. Third, HRNE is able to uncover temporaltransitions between frame chunks with different granularities, i.e., it canmodel the temporal transitions between frames as well as the transitionsbetween segments. We apply the new method to video captioning where temporalinformation plays a crucial role. Experiments demonstrate that our methodoutperforms the state-of-the-art on video captioning benchmarks. Notably, evenusing a single network with only RGB stream as input, HRNE beats all the recentsystems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.
arxiv-14400-176 | Multimodal MRI Neuroimaging with Motion Compensation Based on Particle Filtering | http://arxiv.org/abs/1511.03369 | author:Yu-Hui Chen, Roni Mittelman, Boklye Kim, Charles Meyer, Alfred Hero category:cs.CV physics.med-ph published:2015-11-11 summary:Head movement during scanning impedes activation detection in fMRI studies.Head motion in fMRI acquired using slice-based Echo Planar Imaging (EPI) can beestimated and compensated by aligning the images onto a reference volumethrough image registration. However, registering EPI images volume to volumefails to consider head motion between slices, which may lead to severely biasedhead motion estimates. Slice-to-volume registration can be used to estimatemotion parameters for each slice by more accurately representing the imageacquisition sequence. However, accurate slice to volume mapping is dependent onthe information content of the slices: middle slices are information rich,while edge slides are information poor and more prone to distortion. In thiswork, we propose a Gaussian particle filter based head motion trackingalgorithm to reduce the image misregistration errors. The algorithm uses adynamic state space model of head motion with an observation equation thatmodels continuous slice acquisition of the scanner. Under this model theparticle filter provides more accurate motion estimates and voxel positionestimates. We demonstrate significant performance improvement of the proposedapproach as compared to registration-only methods of head motion estimation andbrain activation detection.
arxiv-14400-177 | Federated Optimization:Distributed Optimization Beyond the Datacenter | http://arxiv.org/abs/1511.03575 | author:Jakub Konečný, Brendan McMahan, Daniel Ramage category:cs.LG math.OC published:2015-11-11 summary:We introduce a new and increasingly relevant setting for distributedoptimization in machine learning, where the data defining the optimization aredistributed (unevenly) over an extremely large number of \nodes, but the goalremains to train a high-quality centralized model. We refer to this setting asFederated Optimization. In this setting, communication efficiency is of utmostimportance. A motivating example for federated optimization arises when we keep thetraining data locally on users' mobile devices rather than logging it to a datacenter for training. Instead, the mobile devices are used as nodes performingcomputation on their local data in order to update a global model. We supposethat we have an extremely large number of devices in our network, each of whichhas only a tiny fraction of data available totally; in particular, we expectthe number of data points available locally to be much smaller than the numberof devices. Additionally, since different users generate data with differentpatterns, we assume that no device has a representative sample of the overalldistribution. We show that existing algorithms are not suitable for this setting, andpropose a new algorithm which shows encouraging experimental results. This workalso sets a path for future research needed in the context of federatedoptimization.
arxiv-14400-178 | Learning to Diagnose with LSTM Recurrent Neural Networks | http://arxiv.org/abs/1511.03677 | author:Zachary C. Lipton, David C. Kale, Charles Elkan, Randall Wetzell category:cs.LG published:2015-11-11 summary:Clinical medical data, especially in the intensive care unit (ICU), consistof multivariate time series of observations. For each patient visit (orepisode), sensor data and lab test results are recorded in the patient'sElectronic Health Record (EHR). While potentially containing a wealth ofinsights, the data is difficult to mine effectively, owing to varying length,irregular sampling and missing data. Recurrent Neural Networks (RNNs),particularly those using Long Short-Term Memory (LSTM) hidden units, arepowerful and increasingly popular models for learning from sequence data. Theyeffectively model varying length sequences and capture long range dependencies.We present the first study to empirically evaluate the ability of LSTMs torecognize patterns in multivariate time series of clinical measurements.Specifically, we consider multilabel classification of diagnoses, training amodel to classify 128 diagnoses given 13 frequently but irregularly sampledclinical measurements. First, we establish the effectiveness of a simple LSTMnetwork for modeling clinical data. Then we demonstrate a straightforward andeffective training strategy in which we replicate targets at each sequencestep. Trained only on raw time series, our models outperform several strongbaselines, including a multilayer perceptron trained on hand-engineeredfeatures.
arxiv-14400-179 | Facial Expression Detection using Patch-based Eigen-face Isomap Networks | http://arxiv.org/abs/1511.03363 | author:Sohini Roychowdhury category:cs.CV published:2015-11-11 summary:Automated facial expression detection problem pose two primary challengesthat include variations in expression and facial occlusions (glasses, beard,mustache or face covers). In this paper we introduce a novel automated patchcreation technique that masks a particular region of interest in the face,followed by Eigen-value decomposition of the patched faces and generation ofIsomaps to detect underlying clustering patterns among faces. The proposedmasked Eigen-face based Isomap clustering technique achieves 75% sensitivityand 66-73% accuracy in classification of faces with occlusions and smilingfaces in around 1 second per image. Also, betweenness centrality, Eigencentrality and maximum information flow can be used as network-based measuresto identify the most significant training faces for expression classificationtasks. The proposed method can be used in combination with feature-basedexpression classification methods in large data sets for improving expressionclassification accuracies.
arxiv-14400-180 | Discovery Radiomics via StochasticNet Sequencers for Cancer Detection | http://arxiv.org/abs/1511.03361 | author:Mohammad Javad Shafiee, Audrey G. Chung, Devinder Kumar, Farzad Khalvati, Masoom Haider, Alexander Wong category:cs.CV cs.AI published:2015-11-11 summary:Radiomics has proven to be a powerful prognostic tool for cancer detection,and has previously been applied in lung, breast, prostate, and head-and-neckcancer studies with great success. However, these radiomics-driven methods relyon pre-defined, hand-crafted radiomic feature sets that can limit their abilityto characterize unique cancer traits. In this study, we introduce a noveldiscovery radiomics framework where we directly discover custom radiomicfeatures from the wealth of available medical imaging data. In particular, weleverage novel StochasticNet radiomic sequencers for extracting custom radiomicfeatures tailored for characterizing unique cancer tissue phenotype. UsingStochasticNet radiomic sequencers discovered using a wealth of lung CT data, weperform binary classification on 42,340 lung lesions obtained from the CT scansof 93 patients in the LIDC-IDRI dataset. Preliminary results show significantimprovement over previous state-of-the-art methods, indicating the potential ofthe proposed discovery radiomics framework for improving cancer screening anddiagnosis.
arxiv-14400-181 | Principal Autoparallel Analysis: Data Analysis in Weitzenböck Space | http://arxiv.org/abs/1511.03355 | author:Stephen Marsland, Carole J Twining category:stat.ME cs.CV math.DG published:2015-11-11 summary:The statistical analysis of data lying on a differentiable, locallyEuclidean, manifold introduces a variety of challenges because the analogousmeasures to standard Euclidean statistics are local, that is only definedwithin a neighbourhood of each datapoint. This is because the curvature of thespace means that the connection of Riemannian geometry is path dependent. Inthis paper we transfer the problem to Weitzenb\"{o}ck space, which has torsion,but not curvature, meaning that parallel transport is path independent, andrather than considering geodesics, it is natural to consider autoparallels,which are `straight' in the sense that they follow the local basis vectors. Wedemonstrate how to generate these autoparallels in a data-driven fashion, andshow that the resulting representation of the data is a useful space in whichto perform further analysis.
arxiv-14400-182 | DataGrinder: Fast, Accurate, Fully non-Parametric Classification Approach Using 2D Convex Hulls | http://arxiv.org/abs/1511.03576 | author:Mohammad Khabbaz category:cs.DB cs.CG cs.LG published:2015-11-11 summary:It has been a long time, since data mining technologies have made their waysto the field of data management. Classification is one of the most importantdata mining tasks for label prediction, categorization of objects into groups,advertisement and data management. In this paper, we focus on the standardclassification problem which is predicting unknown labels in Euclidean space.Most efforts in Machine Learning communities are devoted to methods that useprobabilistic algorithms which are heavy on Calculus and Linear Algebra. Mostof these techniques have scalability issues for big data, and are hardlyparallelizable if they are to maintain their high accuracies in their standardform. Sampling is a new direction for improving scalability, using many smallparallel classifiers. In this paper, rather than conventional sampling methods,we focus on a discrete classification algorithm with O(n) expected runningtime. Our approach performs a similar task as sampling methods. However, we usecolumn-wise sampling of data, rather than the row-wise sampling used in theliterature. In either case, our algorithm is completely deterministic. Ouralgorithm, proposes a way of combining 2D convex hulls in order to achieve highclassification accuracy as well as scalability in the same time. First, wethoroughly describe and prove our O(n) algorithm for finding the convex hull ofa point set in 2D. Then, we show with experiments our classifier model builtbased on this idea is very competitive compared with existing sophisticatedclassification algorithms included in commercial statistical applications suchas MATLAB.
arxiv-14400-183 | Universum Prescription: Regularization using Unlabeled Data | http://arxiv.org/abs/1511.03719 | author:Xiang Zhang, Yann LeCun category:cs.LG published:2015-11-11 summary:This paper shows that simply prescribing "none of the above" labels tounlabeled data has a beneficial regularization effect to supervised learning.We call it universum prescription by the fact that the prescribed labels cannotbe one of the supervised labels. In spite of its simplicity, universumprescription obtained competitive results in training deep convolutionalnetworks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitativejustification of these approaches using Rademacher complexity is presented. Theeffect of a regularization parameter -- probability of sampling from unlabeleddata -- is also studied empirically.
arxiv-14400-184 | Visual7W: Grounded Question Answering in Images | http://arxiv.org/abs/1511.03416 | author:Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei category:cs.CV cs.LG cs.NE published:2015-11-11 summary:We have seen great progress in basic perceptual tasks such as objectrecognition and detection. However, AI models still fail to match humans inhigh-level vision tasks due to the lack of capacities for deeper reasoning.Recently the new task of visual question answering (QA) has been proposed toevaluate a model's capacity for deep image understanding. Previous works haveestablished a loose, global association between QA sentences and images.However, many questions and answers, in practice, relate to local regions inthe images. We establish a semantic link between textual descriptions and imageregions by object-level grounding. It enables a new type of QA with visualanswers, in addition to textual answers used in previous work. We study thevisual QA tasks in a grounded setting with a large collection of 7Wmultiple-choice QA pairs. Furthermore, we evaluate human performance andseveral baseline models on the QA tasks. Finally, we propose a novel LSTM modelwith spatial attention to tackle the 7W QA tasks.
arxiv-14400-185 | Generative Concatenative Nets Jointly Learn to Write and Classify Reviews | http://arxiv.org/abs/1511.03683 | author:Zachary C. Lipton, Sharad Vikram, Julian McAuley category:cs.CL cs.LG published:2015-11-11 summary:A recommender system's basic task is to estimate how users will respond tounseen items. This is typically modeled in terms of how a user might rate aproduct, but here we aim to extend such approaches to model how a user wouldwrite about the product. To do so, we design a character-level Recurrent NeuralNetwork (RNN) that generates personalized product reviews. The networkconvincingly learns styles and opinions of nearly 1000 distinct authors, usinga large corpus of reviews from BeerAdvocate.com. It also tailors reviews todescribe specific items, categories, and star ratings. Using a simple inputreplication strategy, the Generative Concatenative Network (GCN) preserves thesignal of static auxiliary inputs across wide sequence intervals. Without anyadditional training, the generative model can classify reviews, identifying theauthor of the review, the product category, and the sentiment (rating), withremarkable accuracy. Our evaluation shows the GCN captures complex dynamics intext, such as the effect of negation, misspellings, slang, and largevocabularies gracefully absent any machinery explicitly dedicated to thepurpose.
arxiv-14400-186 | Kernel Methods for Accurate UWB-Based Ranging with Reduced Complexity | http://arxiv.org/abs/1511.04045 | author:Vladimir Savic, Erik G. Larsson, Javier Ferrer-Coll, Peter Stenumgaard category:cs.LG cs.IT math.IT published:2015-11-10 summary:Accurate and robust positioning in multipath environments can enable manyapplications, such as search-and-rescue and asset tracking. For this problem,ultra-wideband (UWB) technology can provide the most accurate range estimates,which are required for range-based positioning. However, UWB still faces aproblem with non-line-of-sight (NLOS) measurements, in which the rangeestimates based on time-of-arrival (TOA) will typically be positively biased.There are many techniques that address this problem, mainly based on NLOSidentification and NLOS error mitigation algorithms. However, these techniquesdo not exploit all available information in the UWB channel impulse response.Kernel-based machine learning methods, such as Gaussian Process Regression(GPR), are able to make use of all information, but they may be too complex intheir original form. In this paper, we propose novel ranging methods based onkernel principal component analysis (kPCA), in which the selected channelparameters are projected onto a nonlinear orthogonal high-dimensional space,and a subset of these projections is then used as an input for ranging. Weevaluate the proposed methods using real UWB measurements obtained in abasement tunnel, and found that one of the proposed methods is able tooutperform state-of-the-art, even if little training samples are available.
arxiv-14400-187 | Black-box $α$-divergence Minimization | http://arxiv.org/abs/1511.03243 | author:José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, Thang Bui, Richard E. Turner category:stat.ML published:2015-11-10 summary:We present \emph{black-box alpha} (BB-$\alpha$), an approximate inferencemethod based on the minimization of $\alpha$-divergences between probabilitydistributions. BB-$\alpha$ scales to large datasets since it can be implementedusing stochastic gradient descent. BB-$\alpha$ can be applied to complexprobabilistic models with little effort since it only requires as input thelikelihood function and its gradients. These gradients can be easily obtainedusing automatic differentiation. By tuning divergence parameter $\alpha$, themethod is able to interpolate between variational Bayes and an expectationpropagation-like algorithm. Experiments on probit regression, neural networkregression and classification problems illustrate the accuracy of the posteriorapproximations obtained with BB-$\alpha$.
arxiv-14400-188 | Attention to Scale: Scale-aware Semantic Image Segmentation | http://arxiv.org/abs/1511.03339 | author:Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille category:cs.CV published:2015-11-10 summary:Incorporating multi-scale features to deep convolutional neural networks(DCNNs) has been a key element to achieve state-of-art performance on semanticimage segmentation benchmarks. One way to extract multi-scale features is byfeeding several resized input images to a shared deep network and then mergethe resulting multi-scale features for pixel-wise classification. In this work,we adapt a state-of-art semantic image segmentation model with multi-scaleinput images. We jointly train the network and an attention model which learnsto softly weight the multi-scale features, and show that it outperformsaverage- or max-pooling over scales. The proposed attention model allows us todiagnostically visualize the importance of features at different positions andscales. Moreover, we show that adding extra supervision to the output of DCNNfor each scale is essential to achieve excellent performance when mergingmulti-scale features. We demonstrate the effectiveness of our model withexhaustive experiments on three challenging datasets, includingPASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.
arxiv-14400-189 | Asynchronous Decentralized 20 Questions for Adaptive Search | http://arxiv.org/abs/1511.03144 | author:Theodoros Tsiligkaridis category:cs.MA cs.IT cs.SY math.IT stat.ML published:2015-11-10 summary:This paper considers the problem of adaptively searching for an unknowntarget using multiple agents connected through a time-varying network topology.Agents are equipped with sensors capable of fast information processing, and wepropose an asynchronous decentralized algorithm for controlling their searchgiven noisy observations. Specifically, we propose asynchronous decentralizedextensions of the adaptive query-based search strategy that combines elementsfrom the 20 questions approach and social learning. Under standard assumptionson the time-varying network dynamics, we prove convergence to correct consensuson the value of the parameter as the number of iterations go to infinity. Thisframework provides a flexible and tractable mathematical model for asynchronousdecentralized parameter estimation systems based on adaptively-designedqueries. Our results establish that stability and consistency can be maintainedeven with one-way updating and randomized pairwise averaging, thus providing ascalable low complexity alternative to the synchronous decentralized estimationalgorithm studied in Tsiligkaridis et al [1]. We illustrate the effectivenessand robustness of our algorithm for random network topologies.
arxiv-14400-190 | Learning Communities in the Presence of Errors | http://arxiv.org/abs/1511.03229 | author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG math.ST stat.TH published:2015-11-10 summary:We study the problem of learning communities in the presence of modelingerrors and give robust recovery algorithms for the Stochastic Block Model(SBM). This model, which is also known as the Planted Partition Model, iswidely used for community detection and graph partitioning in various fields,including machine learning, statistics, and social sciences. Many algorithmsexist for learning communities in the Stochastic Block Model, but they do notwork well in the presence of errors. In this paper, we initiate the study of robust algorithms for partialrecovery in SBM with modeling errors or noise. We consider graphs generatedaccording to the Stochastic Block Model and then modified by an adversary. Weallow two types of adversarial errors, Feige---Kilian or monotone errors, andedge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open questionabout whether an almost exact recovery is possible when the adversary isallowed to add $o(n)$ edges. Our work answers this question affirmatively evenin the case of $k>2$ communities. We then show that our algorithms work not only when the instances come fromSBM, but also work when the instances come from any distribution of graphs thatis $\epsilon m$ close to SBM in the Kullback---Leibler divergence. This resultalso works in the presence of adversarial errors. Finally, we present almosttight lower bounds for two communities.
arxiv-14400-191 | 3D Time-lapse Reconstruction from Internet Photos | http://arxiv.org/abs/1511.03019 | author:Ricardo Martin-Brualla, David Gallup, Steven M. Seitz category:cs.CV published:2015-11-10 summary:Given an Internet photo collection of a landmark, we compute a 3D time-lapsevideo sequence where a virtual camera moves continuously in time and space.While previous work assumed a static camera, the addition of camera motionduring the time-lapse creates a very compelling impression of parallax.Achieving this goal, however, requires addressing multiple technicalchallenges, including solving for time-varying depth maps, regularizing 3Dpoint color profiles over time, and reconstructing high quality, hole-freeimages at every frame from the projected profiles. Our results showphotorealistic time-lapses of skylines and natural scenes over many years, withdramatic parallax effects.
arxiv-14400-192 | Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform | http://arxiv.org/abs/1511.03328 | author:Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy, Alan L. Yuille category:cs.CV published:2015-11-10 summary:Deep convolutional neural networks (CNNs) are the backbone of state-of-artsemantic image segmentation systems. Recent work has shown that complementingCNNs with fully-connected conditional random fields (CRFs) can significantlyenhance their object localization accuracy, yet dense CRF inference iscomputationally expensive. We propose replacing the fully-connected CRF withdomain transform (DT), a modern edge-preserving filtering method in which theamount of smoothing is controlled by a reference edge map. Domain transformfiltering is several times faster than dense CRF inference and we show that ityields comparable semantic segmentation results, accurately capturing objectboundaries. Importantly, our formulation allows learning the reference edge mapfrom intermediate CNN features instead of using the image gradient magnitude asin standard DT filtering. This produces task-specific edges in an end-to-endtrainable system optimizing the target semantic segmentation quality.
arxiv-14400-193 | Anchored Discrete Factor Analysis | http://arxiv.org/abs/1511.03299 | author:Yoni Halpern, Steven Horng, David Sontag category:stat.ML cs.LG published:2015-11-10 summary:We present a semi-supervised learning algorithm for learning discrete factoranalysis models with arbitrary structure on the latent variables. Our algorithmassumes that every latent variable has an "anchor", an observed variable withonly that latent variable as its parent. Given such anchors, we show that it ispossible to consistently recover moments of the latent variables and use thesemoments to learn complete models. We also introduce a new technique forimproving the robustness of method-of-moment algorithms by optimizing over themarginal polytope or its relaxations. We evaluate our algorithm using tworeal-world tasks, tag prediction on questions from the Stack Overflow websiteand medical diagnosis in an emergency department.
arxiv-14400-194 | USFD: Twitter NER with Drift Compensation and Linked Data | http://arxiv.org/abs/1511.03088 | author:Leon Derczynski, Isabelle Augenstein, Kalina Bontcheva category:cs.CL published:2015-11-10 summary:This paper describes a pilot NER system for Twitter, comprising the USFDsystem entry to the W-NUT 2015 NER shared task. The goal is to correctly labelentities in a tweet dataset, using an inventory of ten types. We employstructured learning, drawing on gazetteers taken from Linked Data, and onunsupervised clustering features, and attempting to compensate for stylisticand topic drift - a key challenge in social media text. Our result iscompetitive; we provide an analysis of the components of our methodology, andan examination of the target dataset in the context of this task.
arxiv-14400-195 | The Fast Bilateral Solver | http://arxiv.org/abs/1511.03296 | author:Jonathan T. Barron, Ben Poole category:cs.CV published:2015-11-10 summary:We present the bilateral solver, a novel algorithm for edge-aware smoothingthat combines the flexibility and speed of simple filtering approaches with theaccuracy of domain-specific optimization algorithms. Our technique is capableof matching or improving upon state-of-the-art results on several differentcomputer vision tasks (stereo, depth superresolution, colorization, andsemantic segmentation) while being 10-1000 times faster than competingapproaches. The bilateral solver is fast, robust, straightforward to generalizeto new domains, and simple to integrate into deep learning pipelines.
arxiv-14400-196 | Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer | http://arxiv.org/abs/1511.03240 | author:Jun Xie, Martin Kiefel, Ming-Ting Sun, Andreas Geiger category:cs.CV published:2015-11-10 summary:Semantic annotations are vital for training models for object recognition,semantic segmentation or scene understanding. Unfortunately, pixelwiseannotation of images at very large scale is labor-intensive and only littlelabeled data is available, particularly at instance level and for streetscenes. In this paper, we propose to tackle this problem by lifting thesemantic instance labeling task from 2D into 3D. Given reconstructions fromstereo or laser data, we annotate static 3D scene elements with rough boundingprimitives and develop a model which transfers this information into the imagedomain. We leverage our method to obtain 2D labels for a novel suburban videodataset which we have collected, resulting in 400k semantic and instance imageannotations. A comparison of our method to state-of-the-art label transferbaselines reveals that 3D information enables more efficient annotation whileat the same time resulting in improved accuracy and time-coherent labels.
arxiv-14400-197 | From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge | http://arxiv.org/abs/1511.03292 | author:Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis Aloimonos category:cs.CV cs.AI cs.CL I.2.10 published:2015-11-10 summary:In this paper we propose the construction of linguistic descriptions ofimages. This is achieved through the extraction of scene description graphs(SDGs) from visual scenes using an automatically constructed knowledge base.SDGs are constructed using both vision and reasoning. Specifically, commonsensereasoning is applied on (a) detections obtained from existing perceptionmethods on given images, (b) a "commonsense" knowledge base constructed usingnatural language processing of image annotations and (c) lexical ontologicalknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-basedevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in mostcases, sentences auto-constructed from SDGs obtained by our method give a morerelevant and thorough description of an image than a recent state-of-the-artimage caption based approach. Our Image-Sentence Alignment Evaluation resultsare also comparable to that of the recent state-of-the art approaches.
arxiv-14400-198 | The CTU Prague Relational Learning Repository | http://arxiv.org/abs/1511.03086 | author:Jan Motl, Oliver Schulte category:cs.LG cs.DB I.2.6; H.2.8 published:2015-11-10 summary:The aim of the CTU Prague Relational Learning Repository is to supportmachine learning research with multi-relational data. The repository currentlycontains 50 SQL databases hosted on a public MySQL server located atrelational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., thenumber of tables in the database, the number of rows and columns in the tables,the number of foreign key constraints between tables).
arxiv-14400-199 | Online Supervised Hashing for Ever-Growing Datasets | http://arxiv.org/abs/1511.03257 | author:Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff category:cs.CV published:2015-11-10 summary:Supervised hashing methods are widely-used for nearest neighbor search incomputer vision applications. Most state-of-the-art supervised hashingapproaches employ batch-learners. Unfortunately, batch-learning strategies canbe inefficient when confronted with large training datasets. Moreover, withbatch-learners, it is unclear how to adapt the hash functions as a datasetcontinues to grow and diversify over time. Yet, in many practical scenarios thedataset grows and diversifies; thus, both the hash functions and the indexingmust swiftly accommodate these changes. To address these issues, we propose anonline hashing method that is amenable to changes and expansions of thedatasets. Since it is an online algorithm, our approach offers linearcomplexity with the dataset size. Our solution is supervised, in that weincorporate available label information to preserve the semantic neighborhood.Such an adaptive hashing method is attractive; but it requires recomputing thehash table as the hash functions are updated. If the frequency of update ishigh, then recomputing the hash table entries may cause inefficiencies in thesystem, especially for large indexes. Thus, we also propose a framework toreduce hash table updates. We compare our method to state-of-the-art solutionson two benchmarks and demonstrate significant improvements over previous work.
arxiv-14400-200 | Experimental robustness of Fourier Ptychography phase retrieval algorithms | http://arxiv.org/abs/1511.02986 | author:Li-Hao Yeh, Jonathan Dong, Jingshan Zhong, Lei Tian, Michael Chen, Gongguo Tang, Mahdi Soltanolkotabi, Laura Waller category:physics.optics cs.CV published:2015-11-10 summary:Fourier ptychography is a new computational microscopy technique thatprovides gigapixel-scale intensity and phase images with both widefield-of-view and high resolution. By capturing a stack of low-resolutionimages under different illumination angles, a nonlinear inverse algorithm canbe used to computationally reconstruct the high-resolution complex field. Here,we compare and classify multiple proposed inverse algorithms in terms ofexperimental robustness. We find that the main sources of error are noise,aberrations and mis-calibration (i.e. model mis-match). Using simulations andexperiments, we demonstrate that the choice of cost function plays a criticalrole, with amplitude-based cost functions performing better thanintensity-based ones. The reason for this is that Fourier ptychography datasetsconsist of images from both brightfield and darkfield illumination,representing a large range of measured intensities. Both noise (e.g. Poissonnoise) and model mis-match errors are shown to scale with intensity. Hence,algorithms that use an appropriate cost function will be more tolerant to bothnoise and model mis-match. Given these insights, we propose a global Newton'smethod algorithm which is robust and computationally efficient. Finally, wediscuss the impact of procedures for algorithmic correction of aberrations andmis-calibration.
arxiv-14400-201 | Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing | http://arxiv.org/abs/1511.03055 | author:Jie Lin, Olivier Morère, Julie Petta, Vijay Chandrasekhar, Antoine Veillard category:cs.IR cs.CV cs.LG 68P20 H.3.3; I.2.6 published:2015-11-10 summary:A typical image retrieval pipeline starts with the comparison of globaldescriptors from a large database to find a short list of candidate matches. Agood image descriptor is key to the retrieval pipeline and should reconcile twocontradictory requirements: providing recall rates as high as possible andbeing as compact as possible for fast matching. Following the recent successesof Deep Convolutional Neural Networks (DCNN) for large scale imageclassification, descriptors extracted from DCNNs are increasingly used in placeof the traditional hand crafted descriptors such as Fisher Vectors (FV) withbetter retrieval performances. Nevertheless, the dimensionality of a typicalDCNN descriptor --extracted either from the visual feature pyramid or thefully-connected layers-- remains quite high at several thousands of scalarvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fullyunsupervised method to compute extremely compact binary hashes --in the 32-256bits range-- from high-dimensional global descriptors. UTH consists of twosuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines(SRBM), a type of unsupervised deep neural nets, are used to learn binaryembedding functions able to bring the descriptor size down to the desiredbitrate. SRBMs are typically able to ensure a very high compression rate at theexpense of loosing some desirable metric properties of the original DCNNdescriptor space. Then, triplet networks, a rank learning scheme based onweight sharing nets is used to fine-tune the binary embedding functions toretain as much as possible of the useful metric properties of the originalspace. A thorough empirical evaluation conducted on multiple publicly availabledataset using DCNN descriptors shows that our method is able to significantlyoutperform state-of-the-art unsupervised schemes in the target bit range.
arxiv-14400-202 | Learning with a Strong Adversary | http://arxiv.org/abs/1511.03034 | author:Ruitong Huang, Bing Xu, Dale Schuurmans, Csaba Szepesvari category:cs.LG published:2015-11-10 summary:The robustness of neural networks to intended perturbations has recentlyattracted significant attention. In this paper, we propose a new method,\emph{learning with a strong adversary}, that learns robust classifiers fromsupervised data. The proposed method takes finding adversarial examples as anintermediate step. A new and simple way of finding adversarial examples ispresented and experimentally shown to be efficient. Experimental resultsdemonstrate that resulting learning method greatly improves the robustness ofthe classification models produced.
arxiv-14400-203 | Stochastic Expectation Propagation for Large Scale Gaussian Process Classification | http://arxiv.org/abs/1511.03249 | author:Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, Thang Bui, Richard E. Turner category:stat.ML published:2015-11-10 summary:A method for large scale Gaussian process classification has been recentlyproposed based on expectation propagation (EP). Such a method allows Gaussianprocess classifiers to be trained on very large datasets that were out of thereach of previous deployments of EP and has been shown to be competitive withrelated techniques based on stochastic variational inference. Nevertheless, thememory resources required scale linearly with the dataset size, unlike invariational methods. This is a severe limitation when the number of instancesis very large. Here we show that this problem is avoided when stochastic EP isused to train the model.
arxiv-14400-204 | Semi-supervised Tuning from Temporal Coherence | http://arxiv.org/abs/1511.03163 | author:Davide Maltoni, Vincenzo Lomonaco category:cs.LG stat.ML published:2015-11-10 summary:Recent works demonstrated the usefulness of temporal coherence to regularizesupervised training or to learn invariant features with deep architectures. Inparticular, enforcing smooth output changes while presenting temporally-closedframes from video sequences, proved to be an effective strategy. In this paperwe prove the efficacy of temporal coherence for semi-supervised incrementaltuning. We show that a deep architecture, just mildly trained in a supervisedmanner, can progressively improve its classification accuracy, if exposed tovideo sequences of unlabeled data. The extent to which, in some cases, asemi-supervised tuning allows to improve classification accuracy (approachingthe supervised one) is somewhat surprising. A number of control experimentspointed out the fundamental role of temporal coherence.
arxiv-14400-205 | Deep Representation of Facial Geometric and Photometric Attributes for Automatic 3D Facial Expression Recognition | http://arxiv.org/abs/1511.03015 | author:Huibin Li, Jian Sun, Dong Wang, Zongben Xu, Liming Chen category:cs.CV published:2015-11-10 summary:In this paper, we present a novel approach to automatic 3D Facial ExpressionRecognition (FER) based on deep representation of facial 3D geometric and 2Dphotometric attributes. A 3D face is firstly represented by its geometric andphotometric attributes, including the geometry map, normal maps, normalizedcurvature map and texture map. These maps are then fed into a pre-trained deepconvolutional neural network to generate the deep representation. Then thefacial expression prediction is simplyachieved by training linear SVMs over thedeep representation for different maps and fusing these SVM scores. Thevisualizations show that the deep representation provides a complete and highlydiscriminative coding scheme for 3D faces. Comprehensive experiments on theBU-3DFE database demonstrate that the proposed deep representation canoutperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG,Gabor) and the state-of-art approaches under the same experimental protocols.
arxiv-14400-206 | Online Action Recognition based on Incremental Learning of Weighted Covariance Descriptors | http://arxiv.org/abs/1511.03028 | author:Chang Tang, Wanqing Li, Chunping Hou, Pichao Wang, Yonghong Hou, Jing Zhang, Philip O. Ogunbona category:cs.CV published:2015-11-10 summary:Online action recognition aims to recognize actions from unsegmented streamsof data in a continuous manner. One of the challenges in online recognition isthe accumulation of evidence for decision making. This paper presents a fastand efficient online method to recognize actions from a stream of noisyskeleton data. The method adopts a covariance descriptor calculated fromskeleton data and is based on a novel method developed for incrementallylearning the covariance descriptors, referred to as weighted covariancedescriptors, so that past frames have less contributions to the descriptor andcurrent frames and informative frames such as key frames contributes moretowards the descriptor. The online recognition is achieved using an efficientnearest neighbour search against a set of trained actions. Experimental resultson MSRC-12 Kinect Gesture dataset and our newly collocated online actionrecognition dataset have demonstrated the efficacy of the proposed method.
arxiv-14400-207 | Dynamic Belief Fusion for Object Detection | http://arxiv.org/abs/1511.03183 | author:Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William d. Nothwang, Amar M. Marathe category:cs.CV published:2015-11-10 summary:A novel approach for the fusion of heterogeneous object detection methods isproposed. In order to effectively integrate the outputs of multiple detectors,the level of ambiguity in each individual detection score is estimated usingthe precision/recall relationship of the corresponding detector. The maincontribution of the proposed work is a novel fusion method, called DynamicBelief Fusion (DBF), which dynamically assigns probabilities to hypotheses(target, non-target, intermediate state (target or non-target)) based onconfidence levels in the detection results conditioned on the prior performanceof individual detectors. In DBF, a joint basic probability assignment,optimally fusing information from all detectors, is determined by theDempster's combination rule, and is easily reduced to a single fused detectionscore. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that thedetection accuracy of DBF is considerably greater than conventional fusionapproaches as well as individual detectors used for the fusion.
arxiv-14400-208 | TemplateNet for Depth-Based Object Instance Recognition | http://arxiv.org/abs/1511.03244 | author:Ujwal Bonde, Vijay Badrinarayanan, Roberto Cipolla, Minh-Tri Pham category:cs.CV published:2015-11-10 summary:We present a novel deep architecture termed templateNet for depth basedobject instance recognition. Using an intermediate template layer we exploitprior knowledge of an object's shape to sparsify the feature maps. This hasthree advantages: (i) the network is better regularised resulting in structuredfilters; (ii) the sparse feature maps results in intuitive features been learntwhich can be visualized as the output of the template layer and (iii) theresulting network achieves state-of-the-art performance. The network benefitsfrom this without any additional parametrization from the template layer. Wederive the weight updates needed to efficiently train this network in anend-to-end manner. We benchmark the templateNet for depth based object instancerecognition using two publicly available datasets. The datasets presentmultiple challenges of clutter, large pose variations and similar lookingdistractors. Through our experiments we show that with the addition of atemplate layer, a depth based CNN is able to outperform existingstate-of-the-art methods in the field.
arxiv-14400-209 | Sliced Wasserstein Kernels for Probability Distributions | http://arxiv.org/abs/1511.03198 | author:Soheil Kolouri, Yang Zou, Gustavo K. Rohde category:cs.LG stat.ML published:2015-11-10 summary:Optimal transport distances, otherwise known as Wasserstein distances, haverecently drawn ample attention in computer vision and machine learning as apowerful discrepancy measure for probability distributions. The recentdevelopments on alternative formulations of the optimal transport have allowedfor faster solutions to the problem and has revamped its practical applicationsin machine learning. In this paper, we exploit the widely used kernel methodsand provide a family of provably positive definite kernels based on the SlicedWasserstein distance and demonstrate the benefits of these kernels in a varietyof learning tasks. Our work provides a new perspective on the application ofoptimal transport flavored distances through kernel methods in machine learningtasks.
arxiv-14400-210 | The Radon cumulative distribution transform and its application to image classification | http://arxiv.org/abs/1511.03206 | author:Soheil Kolouri, Se Rim Park, Gustavo K. Rohde category:cs.CV published:2015-11-10 summary:Invertible image representation methods (transforms) are routinely employedas low-level image processing operations based on which feature extraction andrecognition algorithms are developed. Most transforms in current use (e.g.Fourier, Wavelet, etc.) are linear transforms, and, by themselves, are unableto substantially simplify the representation of image classes forclassification. Here we describe a nonlinear, invertible, low-level imageprocessing transform based on combining the well known Radon transform forimage data, and the 1D Cumulative Distribution Transform proposed earlier. Wedescribe a few of the properties of this new transform, and with boththeoretical and experimental results show that it can often render certainproblems linearly separable in transform space.
arxiv-14400-211 | Investigating the stylistic relevance of adjective and verb simile markers | http://arxiv.org/abs/1511.03053 | author:Suzanne Mpouli, Jean-Gabriel Ganascia category:cs.CL published:2015-11-10 summary:Similes play an important role in literary texts not only as rhetoricaldevices and as figures of speech but also because of their evocative power,their aptness for description and the relative ease with which they can becombined with other figures of speech (Israel et al. 2004). Detecting all typesof simile constructions in a particular text therefore seems crucial whenanalysing the style of an author. Few research studies however have beendedicated to the study of less prominent simile markers in fictional prose andtheir relevance for stylistic studies. The present paper studies the frequencyof adjective and verb simile markers in a corpus of British and French novelsin order to determine which ones are really informative and worth including ina stylistic analysis. Furthermore, are those adjectives and verb simile markersused differently in both languages?
arxiv-14400-212 | Information retrieval in folktales using natural language processing | http://arxiv.org/abs/1511.03012 | author:Adrian Groza, Lidia Corde category:cs.CL cs.IR published:2015-11-10 summary:Our aim is to extract information about literary characters in unstructuredtexts. We employ natural language processing and reasoning on domainontologies. The first task is to identify the main characters and the parts ofthe story where these characters are described or act. We illustrate the systemin a scenario in the folktale domain. The system relies on a folktale ontologythat we have developed based on Propp's model for folktales morphology.
arxiv-14400-213 | Reducing the Training Time of Neural Networks by Partitioning | http://arxiv.org/abs/1511.02954 | author:Conrado S. Miranda, Fernando J. Von Zuben category:cs.NE cs.LG published:2015-11-10 summary:This paper presents a new method for pre-training neural networks that candecrease the total training time for a neural network while maintaining thefinal performance, which motivates its use on deep neural networks. Bypartitioning the training task in multiple training subtasks with sub-models,which can be performed independently and in parallel, it is shown that the sizeof the sub-models reduces almost quadratically with the number of subtaskscreated, quickly scaling down the sub-models used for the pre-training. Thesub-models are then merged to provide a pre-trained initial set of weights forthe original model. The proposed method is independent of the other aspects ofthe training, such as architecture of the neural network, training method, andobjective, making it compatible with a wide range of existing approaches. Thespeedup without loss of performance is validated experimentally on MNIST and onCIFAR10 data sets, also showing that even performing the subtasks sequentiallycan decrease the training time. Moreover, we show that larger models maypresent higher speedups and conjecture about the benefits of the method indistributed learning systems.
arxiv-14400-214 | Improvised Salient Object Detection and Manipulation | http://arxiv.org/abs/1511.02999 | author:Abhishek Maity category:cs.CV I.4 published:2015-11-10 summary:In case of salient subject recognition, computer algorithms have been heavilyrelied on scanning of images from top-left to bottom-right systematically andapply brute-force when attempting to locate objects of interest. Thus, theprocess turns out to be quite time consuming. Here a novel approach and asimple solution to the above problem is discussed. In this paper, we implementan approach to object manipulation and detection through segmentation map,which would help to desaturate or, in other words, wash out the background ofthe image. Evaluation for the performance is carried out using the Jaccardindex against the well-known Ground-truth target box technique.
arxiv-14400-215 | Traffic Sign Classification Using Deep Inception Based Convolutional Networks | http://arxiv.org/abs/1511.02992 | author:Mrinal Haloi category:cs.CV 68T45 published:2015-11-10 summary:In this work, we propose a novel deep networks for traffic signclassification that achieves outstanding performance on GTSRB surpassing allprevious methods. Our deep network consists of spatial transformer layers and amodified version of inception module specifically designed for capturing localand global features together. This features adoption allows our network toclassify precisely intra class samples even under deformations. Use of spatialtransformer layer make this network more robust to deformations such astranslation, rotation, scaling of input images. Unlike existing approaches thatare developed with hand crafted features, multiple deep networks with hugeparameters and data augmentations, our method addresses the concern ofexploding parameters and augmentations. We have achieved state-of-the-artperformance of 99.81% on GTSRB dataset.
arxiv-14400-216 | Analyzing Stability of Convolutional Neural Networks in the Frequency Domain | http://arxiv.org/abs/1511.03042 | author:Elnaz J. Heravi, Hamed H. Aghdam, Domenec Puig category:cs.CV published:2015-11-10 summary:Understanding the internal process of ConvNets is commonly done usingvisualization techniques. However, these techniques do not usually provide atool for estimating the stability of a ConvNet against noise. In this paper, weshow how to analyze a ConvNet in the frequency domain using a 4-dimensionalvisualization technique. Using the frequency domain analysis, we show thereason that a ConvNet might be sensitive to a very low magnitude additivenoise. Our experiments on a few ConvNets trained on different datasets revealedthat convolution kernels of a trained ConvNet usually pass most of thefrequencies and they are not able to effectively eliminate the effect of highfrequencies. Our next experiments shows that a convolution kernel which has amore concentrated frequency response could be more stable. Finally, we showthat fine-tuning a ConvNet using a training set augmented with noisy images canproduce more stable ConvNets.
arxiv-14400-217 | A Hierarchical Spectral Method for Extreme Classification | http://arxiv.org/abs/1511.03260 | author:Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG published:2015-11-10 summary:Extreme classification problems are multiclass and multilabel classificationproblems where the number of outputs is so large that straightforwardstrategies are neither statistically nor computationally viable. One strategyfor dealing with the computational burden is via a tree decomposition of theoutput space. While this typically leads to training and inference that scalessublinearly with the number of outputs, it also results in reduced statisticalperformance. In this work, we identify two shortcomings of tree decompositionmethods, and describe two heuristic mitigations. We compose these with aneigenvalue technique for constructing the tree. The end result is acomputationally efficient algorithm that provides good statistical performanceon several extreme data sets.
arxiv-14400-218 | On the geometry of output-code multi-class learning | http://arxiv.org/abs/1511.03225 | author:Maria Florina Balcan, Travis Dick, Yishay Mansour category:cs.LG published:2015-11-10 summary:We provide a new perspective on the popular multi-class algorithmictechniques one-vs-all and (error correcting) output-codes. We show that is thatin cases where they are successful (at learning from labeled data), thesetechniques implicitly assume structure on how the classes are related. We showthat by making that structure explicit, we can design algorithms to recover theclasses based on limited labeled data. We provide results for commonly studiedcases where the codewords of the classes are well separated: learning a linearone-vs-all classifier for data on the unit ball and learning a linear errorcorrecting output code when the Hamming distance between the codewords is large(at least $d+1$ in a $d$-dimensional problem). We additionally consider themore challenging case where the codewords are not well separated, but satisfy aboundary features condition.
arxiv-14400-219 | Dimension of Marginals of Kronecker Product Models | http://arxiv.org/abs/1511.03570 | author:Guido Montufar, Jason Morton category:stat.ML cs.NE math.AG math.CO math.PR 14T05, 52B05 published:2015-11-10 summary:A Kronecker product model is the set of visible marginal probabilitydistributions of an exponential family whose sufficient statistics matrixfactorizes as a Kronecker product of two matrices, one for the visiblevariables and one for the hidden variables. We estimate the dimension of thesemodels by the maximum rank of the Jacobian in the limit of large parameters.The limit is described by the tropical morphism; a piecewise linear map withpieces corresponding to slicings of the visible matrix by the normal fan of thehidden matrix. We obtain combinatorial conditions under which the model has theexpected dimension, equal to the minimum of the number of natural parametersand the dimension of the ambient probability simplex. Additionally, we provethat the binary restricted Boltzmann machine always has the expected dimension.
arxiv-14400-220 | Learning Instrumental Variables with Non-Gaussianity Assumptions: Theoretical Limitations and Practical Algorithms | http://arxiv.org/abs/1511.02722 | author:Ricardo Silva, Shohei Shimizu category:stat.ML published:2015-11-09 summary:Learning a causal effect from observational data is not straightforward, asthis is not possible without further assumptions. If hidden common causesbetween treatment $X$ and outcome $Y$ cannot be blocked by other measurements,one possibility is to use an instrumental variable. In principle, it ispossible under some assumptions to discover whether a variable is structurallyinstrumental to a target causal effect $X \rightarrow Y$, but currentframeworks are somewhat lacking on how general these assumptions can be. Ainstrumental variable discovery problem is challenging, as no variable can betested as an instrument in isolation but only in groups, but differentvariables might require different conditions to be considered an instrument.Moreover, identification constraints might be hard to detect statistically. Inthis paper, we give a theoretical characterization of instrumental variablediscovery, highlighting identifiability problems and solutions, the need fornon-Gaussianity assumptions, and how they fit within existing methods.
arxiv-14400-221 | Multiple Instance Dictionary Learning using Functions of Multiple Instances | http://arxiv.org/abs/1511.02825 | author:Changzhe Jiao, Alina Zare category:cs.CV cs.LG stat.ML published:2015-11-09 summary:A multiple instance dictionary learning method using functions of multipleinstances (DL-FUMI) is proposed to address target detection and two-classclassification problems with inaccurate training labels. Given inaccuratetraining labels, DL-FUMI learns a set of target dictionary atoms that describethe most distinctive and representative features of the true positive class aswell as a set of nontarget dictionary atoms that account for the sharedinformation found in both the positive and negative instances. Experimentalresults show that the estimated target dictionary atoms found by DL-FUMI aremore representative prototypes and identify better discriminative features ofthe true positive class than existing methods in the literature. DL-FUMI isshown to have significantly better performance on several target detection andclassification problems as compared to other multiple instance learning (MIL)dictionary learning algorithms on a variety of MIL problems.
arxiv-14400-222 | Hyperspectral Image Recovery from Incomplete and Imperfect Measurements via Hybrid Regularization | http://arxiv.org/abs/1511.02928 | author:Reza Arablouei, Frank de Hoog category:cs.CV published:2015-11-09 summary:Natural images tend to mostly consist of smooth regions with individualpixels having highly correlated spectra. This information can be exploited torecover hyperspectral images of natural scenes from their incomplete and noisymeasurements. To perform the recovery while taking full advantage of the priorknowledge, we formulate a composite cost function containing a square-errordata-fitting term and two distinct regularization terms pertaining to spatialand spectral domains. The regularization for the spatial domain is the sum oftotal-variation of the image frames corresponding to all spectral bands. Theregularization for the spectral domain is the l_1-norm of the coefficientmatrix obtained by applying a suitable sparsifying transform to the spectra ofthe pixels. We use an accelerated proximal-subgradient method to minimize theformulated cost function. We analyse the performance of the proposed algorithmand prove its convergence. Numerical simulations using real hyperspectralimages exhibit that the proposed algorithm offers an excellent recoveryperformance with a number of measurements that is only a small fraction of thehyperspectral image data size. Simulation results also show that the proposedalgorithm significantly outperforms an accelerated proximal-gradient algorithmthat solves the classical basis-pursuit denoising problem to recover thehyperspectral image.
arxiv-14400-223 | Biologically Inspired Dynamic Textures for Probing Motion Perception | http://arxiv.org/abs/1511.02705 | author:Jonathan Vacher, Andrew Meso, Laurent U Perrinet, Gabriel Peyré category:cs.CV math.ST stat.TH published:2015-11-09 summary:Perception is often described as a predictive process based on an optimalinference with respect to a generative model. We study here the principledconstruction of a generative model specifically crafted to probe motionperception. In that context, we first provide an axiomatic, biologically-drivenderivation of the model. This model synthesizes random dynamic textures whichare defined by stationary Gaussian distributions obtained by the randomaggregation of warped patterns. Importantly, we show that this model canequivalently be described as a stochastic partial differential equation. Usingthis characterization of motion in images, it allows us to recast motion-energymodels into a principled Bayesian inference framework. Finally, we apply thesetextures in order to psychophysically probe speed perception in humans. In thisframework, while the likelihood is derived from the generative model, the prioris estimated from the observed results and accounts for the perceptual bias ina principled fashion.
arxiv-14400-224 | Weakly Supervised Deep Detection Networks | http://arxiv.org/abs/1511.02853 | author:Hakan Bilen, Andrea Vedaldi category:cs.CV published:2015-11-09 summary:Weakly supervised learning of object detection is an important problem inimage understanding that still does not have a satisfactory solution. In thispaper, we address this problem by exploiting the power of deep convolutionalneural networks pre-trained on large-scale image-level classification tasks. Wepropose a weakly supervised deep detection architecture that modifies one suchnetwork to operate at the level of image regions, performing simultaneouslyregion selection and classification. Trained as an image classifier, thearchitecture implicitly learns object detectors that are better thanalternative weakly supervised detection systems on the PASCAL VOC data. Themodel, which is a simple and elegant end-to-end architecture, outperformsstandard data augmentation and fine-tuning techniques for the task ofimage-level classification as well.
arxiv-14400-225 | Sentiment Expression via Emoticons on Social Media | http://arxiv.org/abs/1511.02556 | author:Hao Wang, Jorge A. Castanon category:cs.CL cs.SI published:2015-11-09 summary:Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis andother NLP tasks as features to ma- chine learning algorithms or as entries ofsentiment lexicons. In this paper, we argue that while emoticons are strong andcommon signals of sentiment expression on social media the relationship betweenemoticons and sentiment polarity are not always clear. Thus, any algorithm thatdeals with sentiment polarity should take emoticons into account but extremecau- tion should be exercised in which emoticons to depend on. First, todemonstrate the prevalence of emoticons on social media, we analyzed thefrequency of emoticons in a large re- cent Twitter data set. Then we carriedout four analyses to examine the relationship between emoticons and sentimentpolarity as well as the contexts in which emoticons are used. The firstanalysis surveyed a group of participants for their perceived sentimentpolarity of the most frequent emoticons. The second analysis examinedclustering of words and emoti- cons to better understand the meaning conveyedby the emoti- cons. The third analysis compared the sentiment polarity ofmicroblog posts before and after emoticons were removed from the text. The lastanalysis tested the hypothesis that removing emoticons from text hurtssentiment classification by training two machine learning models with andwithout emoticons in the text respectively. The results confirms the argumentsthat: 1) a few emoticons are strong and reliable signals of sentiment polarityand one should take advantage of them in any senti- ment analysis; 2) a largegroup of the emoticons conveys com- plicated sentiment hence they should betreated with extreme caution.
arxiv-14400-226 | A Century of Portraits: A Visual Historical Record of American High School Yearbooks | http://arxiv.org/abs/1511.02575 | author:Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Alexei A. Efros category:cs.CV published:2015-11-09 summary:Many details about our world are not captured in written records because theyare too mundane or too abstract to describe in words. Fortunately, since theinvention of the camera, an ever-increasing number of photographs capture muchof this otherwise lost information. This plethora of artifacts documenting our"visual culture" is a treasure trove of knowledge as yet untapped byhistorians. We present a dataset of 37,921 frontal-facing American high schoolyearbook photos that allow us to use computation to glimpse into the historicalvisual record too voluminous to be evaluated manually. The collected portraitsprovide a constant visual frame of reference with varying content. We cantherefore use them to consider issues such as a decade's defining styleelements, or trends in fashion and social norms over time. We demonstrate thatour historical image dataset may be used together with weakly-superviseddata-driven techniques to perform scalable historical analysis of large imagecorpora with minimal human effort, much in the same way that large text corporatogether with natural language processing revolutionized historians' workflow.Furthermore, we demonstrate the use of our dataset in dating grayscaleportraits using deep learning methods.
arxiv-14400-227 | How far can we go without convolution: Improving fully-connected networks | http://arxiv.org/abs/1511.02580 | author:Zhouhan Lin, Roland Memisevic, Kishore Konda category:cs.LG cs.NE published:2015-11-09 summary:We propose ways to improve the performance of fully connected networks. Wefound that two approaches in particular have a strong effect on performance:linear bottleneck layers and unsupervised pre-training using autoencoderswithout hidden unit biases. We show how both approaches can be related toimproving gradient flow and reducing sparsity in the network. We show that afully connected network can yield approximately 70% classification accuracy onthe permutation-invariant CIFAR-10 task, which is much higher than the currentstate-of-the-art. By adding deformations to the training data, the fullyconnected network achieves 78% accuracy, which is just 10% short of a decentconvolutional network.
arxiv-14400-228 | A Lightened CNN for Deep Face Representation | http://arxiv.org/abs/1511.02683 | author:Xiang Wu, Ran He, Zhenan Sun category:cs.CV published:2015-11-09 summary:Convolution neural network (CNN) has significantly pushed forward thedevelopment of face recognition techniques. To achieve ultimate accuracy, CNNmodels tend to be deeper or multiple local facial patch ensemble, which resultin a waste of time and space. To alleviate this issue, this paper studies alightened CNN framework to learn a compact embedding for face representation.First, we introduce the concept of maxout in the fully connected layer to theconvolution layer, which leads to a new activation function, namedMax-Feature-Map (MFM). Compared with widely used ReLU, MFM can simultaneouslycapture compact representation and competitive information. Then, one shallowCNN model is constructed by 4 convolution layers and totally contains about 4Mparameters; and the other is constructed by reducing the kernel size ofconvolution layers and adding Network in Network (NIN) layers betweenconvolution layers based on the previous one. These models are trained on theCASIA-WebFace dataset and evaluated on the LFW and YTF datasets. Experimentalresults show that the proposed models achieve state-of-the-art results. At thesame time, a reduction of computational cost is reached by over 9 times incomparison with the released VGG model.
arxiv-14400-229 | Exploiting Egocentric Object Prior for 3D Saliency Detection | http://arxiv.org/abs/1511.02682 | author:Gedas Bertasius, Hyun Soo Park, Jianbo Shi category:cs.CV published:2015-11-09 summary:On a minute-to-minute basis people undergo numerous fluid interactions withobjects that barely register on a conscious level. Recent neuroscientificresearch demonstrates that humans have a fixed size prior for salient objects.This suggests that a salient object in 3D undergoes a consistent transformationsuch that people's visual system perceives it with an approximately fixed size.This finding indicates that there exists a consistent egocentric object priorthat can be characterized by shape, size, depth, and location in the firstperson view. In this paper, we develop an EgoObject Representation, which encodes thesecharacteristics by incorporating shape, location, size and depth features froman egocentric RGBD image. We empirically show that this representation canaccurately characterize the egocentric object prior by testing it on anegocentric RGBD dataset for three tasks: the 3D saliency detection, futuresaliency prediction, and interaction classification. This representation isevaluated on our new Egocentric RGBD Saliency dataset that includes variousactivities such as cooking, dining, and shopping. By using our EgoObjectrepresentation, we outperform previously proposed models for saliency detection(relative 30% improvement for 3D saliency detection task) on our dataset.Additionally, we demonstrate that this representation allows us to predictfuture salient objects based on the gaze cue and classify people's interactionswith objects.
arxiv-14400-230 | Batch-normalized Maxout Network in Network | http://arxiv.org/abs/1511.02583 | author:Jia-Ren Chang, Yong-Sheng Chen category:cs.CV cs.LG published:2015-11-09 summary:This paper reports a novel deep architecture referred to as Maxout network InNetwork (MIN), which can enhance model discriminability and facilitate theprocess of information abstraction within the receptive field. The proposednetwork adopts the framework of the recently developed Network In Networkstructure, which slides a universal approximator, multilayer perceptron (MLP)with rectifier units, to exact features. Instead of MLP, we employ maxout MLPto learn a variety of piecewise linear activation functions and to mediate theproblem of vanishing gradients that can occur when using rectifier units.Moreover, batch normalization is applied to reduce the saturation of maxoutunits by pre-conditioning the model and dropout is applied to preventoverfitting. Finally, average pooling is used in all pooling layers toregularize maxout MLP in order to facilitate information abstraction in everyreceptive field while tolerating the change of object position. Because averagepooling preserves all features in the local patch, the proposed MIN model canenforce the suppression of irrelevant information during training. Ourexperiments demonstrated the state-of-the-art classification performance whenthe MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets andcomparable performance for SVHN dataset.
arxiv-14400-231 | PAC-Bayesian High Dimensional Bipartite Ranking | http://arxiv.org/abs/1511.02729 | author:Benjamin Guedj, Sylvain Robbiano category:stat.ML math.ST stat.TH published:2015-11-09 summary:This paper is devoted to the bipartite ranking problem, a classicalstatistical learning task, in a high dimensional setting. We propose a scoringand ranking strategy based on the PAC-Bayesian approach. We consider nonlinearadditive scoring functions, and we derive non-asymptotic risk bounds under asparsity assumption. In particular, oracle inequalities in probability holdingunder a margin condition assess the performance of our procedure, and prove itsminimax optimality. An MCMC-flavored algorithm is proposed to implement ourmethod, along with its behavior on synthetic and real-life datasets.
arxiv-14400-232 | Explicit Knowledge-based Reasoning for Visual Question Answering | http://arxiv.org/abs/1511.02570 | author:Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick category:cs.CV cs.CL published:2015-11-09 summary:We describe a method for visual question answering which is capable ofreasoning about contents of an image on the basis of information extracted froma large-scale knowledge base. The method not only answers natural languagequestions using concepts not contained in the image, but can provide anexplanation of the reasoning by which it developed its answer. The method iscapable of answering far more complex questions than the predominant longshort-term memory-based approach, and outperforms it significantly in thetesting. We also provide a dataset and a protocol by which to evaluate suchmethods, thus addressing one of the key issues in general visual ques- tionanswering.
arxiv-14400-233 | Spatially Coherent Random Forests | http://arxiv.org/abs/1511.02911 | author:Tal Remez, Shai Avidan category:cs.CV published:2015-11-09 summary:Spatially Coherent Random Forest (SCRF) extends Random Forest to createspatially coherent labeling. Each split function in SCRF is evaluated based ona traditional information gain measure that is regularized by a spatialcoherency term. This way, SCRF is encouraged to choose split functions thatcluster pixels both in appearance space and in image space. In particular, weuse SCRF to detect contours in images, where contours are taken to be theboundaries between different regions. Each tree in the forest produces asegmentation of the image plane and the boundaries of the segmentations of alltrees are aggregated to produce a final hierarchical contour map. We show thatthis modification improves the performance of regular Random Forest by about10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can beused in other settings as well.
arxiv-14400-234 | Bayesian Inference in Cumulative Distribution Fields | http://arxiv.org/abs/1511.02796 | author:Ricardo Silva category:stat.ML stat.ME published:2015-11-09 summary:One approach for constructing copula functions is by multiplication. Giventhat products of cumulative distribution functions (CDFs) are also CDFs, anadjustment to this multiplication will result in a copula model, as discussedby Liebscher (J Mult Analysis, 2008). Parameterizing models via products ofCDFs has some advantages, both from the copula perspective (e.g., it iswell-defined for any dimensionality) and from general multivariate analysis(e.g., it provides models where small dimensional marginal distributions can beeasily read-off from the parameters). Independently, Huang and Frey (J MachLearn Res, 2011) showed the connection between certain sparse graphical modelsand products of CDFs, as well as message-passing (dynamic programming) schemesfor computing the likelihood function of such models. Such schemes allowsmodels to be estimated with likelihood-based methods. We discuss anddemonstrate MCMC approaches for estimating such models in a Bayesian context,their application in copula modeling, and how message-passing can be stronglysimplified. Importantly, our view of message-passing opens up possibilities toscaling up such methods, given that even dynamic programming is not a scalablesolution for calculating likelihood functions in many models.
arxiv-14400-235 | Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding | http://arxiv.org/abs/1511.02680 | author:Alex Kendall, Vijay Badrinarayanan, Roberto Cipolla category:cs.CV cs.NE published:2015-11-09 summary:We present a novel deep learning framework for probabilistic pixel-wisesemantic segmentation, which we term Bayesian SegNet. Pixel-wise semanticsegmentation is an important step for visual scene understanding. It is acomplex task requiring knowledge of support relationships and contextualinformation, as well as visual appearance. Our contribution is a practicalsystem which is able to predict pixel-wise class labels with a measure of modeluncertainty. We achieve this by Monte Carlo sampling with dropout at test timeto generate a posterior distribution of pixel class labels. We show thisBayesian neural network provides a significant performance improvement insegmentation, with no additional parameterisation. We set a new benchmark withstate-of-the-art performance on both the indoor SUN Scene Understanding andoutdoor CamVid driving scenes datasets. Bayesian SegNet also performscompetitively on Pascal VOC 2012 object segmentation challenge. For our webdemo and source code, see http://mi.eng.cam.ac.uk/projects/segnet/
arxiv-14400-236 | Visual Language Modeling on CNN Image Representations | http://arxiv.org/abs/1511.02872 | author:Hiroharu Kato, Tatsuya Harada category:cs.CV cs.AI cs.LG published:2015-11-09 summary:Measuring the naturalness of images is important to generate realistic imagesor to detect unnatural regions in images. Additionally, a method to measurenaturalness can be complementary to Convolutional Neural Network (CNN) basedfeatures, which are known to be insensitive to the naturalness of images.However, most probabilistic image models have insufficient capability ofmodeling the complex and abstract naturalness that we feel because they arebuilt directly on raw image pixels. In this work, we assume that naturalnesscan be measured by the predictability on high-level features during eyemovement. Based on this assumption, we propose a novel method to evaluate thenaturalness by building a variant of Recurrent Neural Network Language Modelson pre-trained CNN representations. Our method is applied to two tasks,demonstrating that 1) using our method as a regularizer enables us to generatemore understandable images from image features than existing approaches, and 2)unnaturalness maps produced by our method achieve state-of-the-art eye fixationprediction performance on two well-studied datasets.
arxiv-14400-237 | Parkinson's disease patient rehabilitation using gaming platforms: lessons learnt | http://arxiv.org/abs/1511.02589 | author:Ioannis Pachoulakis, Nikolaos Papadopoulos, Cleanthe Spanaki category:cs.CY cs.CV I.6.3; I.6.8 published:2015-11-09 summary:Parkinson's disease (PD) is a progressive neurodegenerative movement disorderwhere motor dysfunction gradually increases as the disease progress. Inaddition to administering dopaminergic PD-specific drugs, attendingneurologists strongly recommend regular exercise combined with physiotherapy.However, because of the long-term nature of the disease, patients followingtraditional rehabilitation programs may get bored, lose interest and eventuallydrop out as a direct result of the repeatability and predictability of theprescribed exercises. Technology supported opportunities to liven up a dailyexercise schedule have appeared in the form of character-based, virtual realitygames which promote physical training in a non-linear and looser fashion andprovide an experience that varies from one game loop the next. Such"exergames", a word that results from the amalgamation of the words "exercise"and "game" challenge patients into performing movements of varying complexityin a playful and immersive virtual environment. Today's game consoles such asNintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present newopportunities to infuse motivation and variety to an otherwise mundanephysiotherapy routine. In this paper we present some of these approaches,discuss their suitability for these PD patients, mainly on the basis of demandsmade on balance, agility and gesture precision, and present design principlesthat exergame platforms must comply with in order to be suitable for PDpatients.
arxiv-14400-238 | Efficient Construction of Local Parametric Reduced Order Models Using Machine Learning Techniques | http://arxiv.org/abs/1511.02909 | author:Azam Moosavi, Razvan Stefanescu, Adrian Sandu category:cs.LG published:2015-11-09 summary:Reduced order models are computationally inexpensive approximations thatcapture the important dynamical characteristics of large, high-fidelitycomputer models of physical systems. This paper applies machine learningtechniques to improve the design of parametric reduced order models.Specifically, machine learning is used to develop feasible regions in theparameter space where the admissible target accuracy is achieved with apredefined reduced order basis, to construct parametric maps, to chose the besttwo already existing bases for a new parameter configuration from accuracypoint of view and to pre-select the optimal dimension of the reduced basis suchas to meet the desired accuracy. By combining available information using basesconcatenation and interpolation as well as high-fidelity solutionsinterpolation we are able to build accurate reduced order models associatedwith new parameter settings. Promising numerical results with a viscous Burgersmodel illustrate the potential of machine learning approaches to help designbetter reduced order models.
arxiv-14400-239 | A New Relaxation Approach to Normalized Hypergraph Cut | http://arxiv.org/abs/1511.02595 | author:Cong Xie, Wu-Jun Li, Zhihua Zhang category:cs.LG cs.DS published:2015-11-09 summary:Normalized graph cut (NGC) has become a popular research topic due to itswide applications in a large variety of areas like machine learning and verylarge scale integration (VLSI) circuit design. Most of traditional NGC methodsare based on pairwise relationships (similarities). However, in real-worldapplications relationships among the vertices (objects) may be more complexthan pairwise, which are typically represented as hyperedges in hypergraphs.Thus, normalized hypergraph cut (NHC) has attracted more and more attention.Existing NHC methods cannot achieve satisfactory performance in realapplications. In this paper, we propose a novel relaxation approach, which iscalled relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as anoptimization problem on the Stiefel manifold. To solve this problem, we resortto the Cayley transformation to devise a feasible learning algorithm.Experimental results on a set of large hypergraph benchmarks for clustering andpartitioning in VLSI domain show that RNHC can outperform the state-of-the-artmethods.
arxiv-14400-240 | Semantic Segmentation with Boundary Neural Fields | http://arxiv.org/abs/1511.02674 | author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV published:2015-11-09 summary:The state-of-the-art in semantic segmentation is currently represented byfully convolutional networks (FCNs). However, FCNs use large receptive fieldsand many pooling layers, both of which cause blurring and low spatialresolution in the deep layers. As a result FCNs tend to produce segmentationsthat are poorly localized around object boundaries. Prior work has attempted toaddress this issue in post-processing steps, for example using a color-basedCRF on top of the FCN predictions. However, these approaches require additionalparameters and low-level features that are difficult to tune and integrate intothe original network architecture. Additionally, most CRFs use color-basedpixel affinities, which are not well suited for semantic segmentation and leadto spatially disjoint predictions. To overcome these problems, we introduce a Boundary Neural Field (BNF), whichis a global energy model integrating FCN predictions with boundary cues. Theboundary information is used to enhance semantic segment coherence and toimprove object localization. Specifically, we first show that the convolutionalfilters of semantic FCNs provide good features for boundary detection. We thenemploy the predicted boundaries to define pairwise potentials in our energy.Finally, we show that our energy decomposes semantic segmentation into multiplebinary problems, which can be relaxed for efficient global optimization. Wereport extensive experiments demonstrating that minimization of our globalboundary-based energy yields results superior to prior globalization methods,both quantitatively as well as qualitatively.
arxiv-14400-241 | Decomposition Bounds for Marginal MAP | http://arxiv.org/abs/1511.02619 | author:Wei Ping, Qiang Liu, Alexander Ihler category:cs.LG cs.AI cs.IT math.IT stat.ML published:2015-11-09 summary:Marginal MAP inference involves making MAP predictions in systems definedwith latent variables or missing information. It is significantly moredifficult than pure marginalization and MAP tasks, for which a large class ofefficient and convergent variational algorithms, such as dual decomposition,exist. In this work, we generalize dual decomposition to a generic power suminference task, which includes marginal MAP, along with pure marginalizationand MAP, as special cases. Our method is based on a block coordinate descentalgorithm on a new convex decomposition bound, that is guaranteed to convergemonotonically, and can be parallelized efficiently. We demonstrate our approachon marginal MAP queries defined on real-world problems from the UAI approximateinference challenge, showing that our framework is faster and more reliablethan previous methods.
arxiv-14400-242 | Toward Biochemical Probabilistic Computation | http://arxiv.org/abs/1511.02623 | author:Jacques Droulez, David Colliaux, Audrey Houillon, Pierre Bessière category:cs.ET cs.NE q-bio.MN published:2015-11-09 summary:Living organisms survive and multiply even though they have uncertain andincomplete information about their environment and imperfect models to predictthe consequences of their actions. Bayesian models have been proposed to facethis challenge. Indeed, Bayesian inference is a way to do optimal reasoningwhen only uncertain and incomplete information is available. Variousperceptive, sensory-motor, and cognitive functions have been successfullymodeled this way. However, the biological mechanisms allowing animals andhumans to represent and to compute probability distributions are not known. Ithas been proposed that neurons and assemblies of neurons could be theappropriate scale to search for clues to probabilistic reasoning. In contrast,in this paper, we propose that interacting populations of macromolecules anddiffusible messengers can perform probabilistic computation. This suggests thatprobabilistic reasoning, based on cellular signaling pathways, is a fundamentalskill of living organisms available to the simplest unicellular organisms aswell as the most complex brains.
arxiv-14400-243 | Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders | http://arxiv.org/abs/1511.02916 | author:Zhouhan Lin, Yushi Chen, Xing Zhao, Gang Wang category:cs.CV cs.AI cs.LG published:2015-11-09 summary:Hyperspectral image (HSI) classification is a hot topic in the remote sensingcommunity. This paper proposes a new framework of spectral-spatial featureextraction for HSI classification, in which for the first time the concept ofdeep learning is introduced. Specifically, the model of autoencoder isexploited in our framework to extract various kinds of features. First weverify the eligibility of autoencoder by following classical spectralinformation based classification and use autoencoders with different depth toclassify hyperspectral image. Further in the proposed framework, we combine PCAon spectral dimension and autoencoder on the other two spatial dimensions toextract spectral-spatial information for classification. The experimentalresults show that this framework achieves the highest classification accuracyamong all methods, and outperforms classical classifiers such as SVM andPCA-based SVM.
arxiv-14400-244 | Enacting textual entailment and ontologies for automated essay grading in chemical domain | http://arxiv.org/abs/1511.02669 | author:Adrian Groza, Roxana Szabo category:cs.AI cs.CL published:2015-11-09 summary:We propose a system for automated essay grading using ontologies and textualentailment. The process of textual entailment is guided by hypotheses, whichare extracted from a domain ontology. Textual entailment checks if the truth ofthe hypothesis follows from a given text. We enact textual entailment tocompare students answer to a model answer obtained from ontology. We validatedthe solution against various essays written by students in the chemistrydomain.
arxiv-14400-245 | An Efficient Multilinear Optimization Framework for Hypergraph Matching | http://arxiv.org/abs/1511.02667 | author:Quynh Nguyen, Francesco Tudisco, Antoine Gautier, Matthias Hein category:cs.CV cs.DS math.OC published:2015-11-09 summary:Hypergraph matching has recently become a popular approach for solvingcorrespondence problems in computer vision as it allows to integratehigher-order geometric information. Hypergraph matching can be formulated as athird-order optimization problem subject to the assignment constraints whichturns out to be NP-hard. In recent work, we have proposed an algorithm forhypergraph matching which first lifts the third-order problem to a fourth-orderproblem and then solves the fourth-order problem via optimization of thecorresponding multilinear form. This leads to a tensor block coordinate ascentscheme which has the guarantee of providing monotonic ascent in the originalmatching score function and leads to state-of-the-art performance both in termsof achieved matching score and accuracy. In this paper we show that the liftingstep to a fourth-order problem can be avoided yielding a third-order schemewith the same guarantees and performance but being two times faster. Moreover,we introduce a homotopy type method which further improves the performance.
arxiv-14400-246 | Massive Online Crowdsourced Study of Subjective and Objective Picture Quality | http://arxiv.org/abs/1511.02919 | author:Deepti Ghadiyaram, Alan C. Bovik category:cs.CV published:2015-11-09 summary:Most publicly available image quality databases have been created underhighly controlled conditions by introducing graded simulated distortions ontohigh-quality photographs. However, images captured using typical real-worldmobile camera devices are usually afflicted by complex mixtures of multipledistortions, which are not necessarily well-modeled by the syntheticdistortions found in existing databases. The originators of existing legacydatabases usually conducted human psychometric studies to obtain statisticallymeaningful sets of human opinion scores on images in a stringently controlledvisual environment, resulting in small data collections relative to other kindsof image analysis databases. Towards overcoming these limitations, we designedand created a new database that we call the LIVE In the Wild Image QualityChallenge Database, which contains widely diverse authentic image distortionson a large number of images captured using a representative variety of modernmobile devices. We also designed and implemented a new online crowdsourcingsystem, which we have used to conduct a very large-scale, multi-month imagequality assessment subjective study. Our database consists of over 350000opinion scores on 1162 images evaluated by over 7000 unique human observers.Despite the lack of control over the experimental environments of the numerousstudy participants, we demonstrate excellent internal consistency of thesubjective dataset. We also evaluate several top-performing blind Image QualityAssessment algorithms on it and present insights on how mixtures of distortionschallenge both end users as well as automatic perceptual quality predictionmodels.
arxiv-14400-247 | Symmetries and control in generative neural nets | http://arxiv.org/abs/1511.02841 | author:Galin Georgiev category:cs.CV cs.LG published:2015-11-09 summary:We study generative nets which can control and modify observations, afterbeing trained on real-life datasets. In order to zoom-in on an object, somespatial, color and other attributes are learned by classifiers in specializedattention nets. In field-theoretical terms, these learned symmetry statisticsform the gauge group of the data set. Plugging them in the generative layers ofauto-classifiers-encoders (ACE) appears to be the most direct way tosimultaneously: i) generate new observations with arbitrary attributes, from agiven class, ii) describe the low-dimensional manifold encoding the "essence"of the data, after superfluous attributes are factored out, and iii)organically control, i.e., move or modify objects within given observations. Wedemonstrate the sharp improvement of the generative qualities of shallow ACE,with added spatial and color symmetry statistics, on the distorted MNIST andCIFAR10 datasets.
arxiv-14400-248 | Partial Membership Latent Dirichlet Allocation | http://arxiv.org/abs/1511.02821 | author:Chao Chen, Alina Zare, J. Tory Cobb category:stat.ML cs.CV published:2015-11-09 summary:Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmentingimagery. These models are confined to crisp segmentation. Yet, there are manyimages in which some regions cannot be assigned a crisp label (e.g., transitionregions between a foggy sky and the ground or between sand and water at abeach). In these cases, a visual word is best represented with partialmemberships across multiple topics. To address this, we present a partialmembership latent Dirichlet allocation (PM-LDA) model and associated parameterestimation algorithms. Experimental results on two natural image datasets andone SONAR image dataset show that PM-LDA can produce both crisp and softsemantic image segmentations; a capability existing methods do not have.
arxiv-14400-249 | Detecting events and key actors in multi-person videos | http://arxiv.org/abs/1511.02917 | author:Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija, Alexander Gorban, Kevin Murphy, Li Fei-Fei category:cs.CV cs.AI published:2015-11-09 summary:Multi-person event recognition is a challenging task, often with many peopleactive in the scene but only a small subset contributing to an actual event. Inthis paper, we propose a model which learns to detect events in such videoswhile automatically "attending" to the people responsible for the event. Ourmodel does not use explicit annotations regarding who or where those people areduring training and testing. In particular, we track people in videos and use arecurrent neural network (RNN) to represent the track features. We learntime-varying attention weights to combine these features at each time-instant.The attended features are then processed using another RNN for eventdetection/classification. Since most video datasets with multiple people arerestricted to a small number of videos, we also collected a new basketballdataset comprising 257 basketball games with 14K event annotationscorresponding to 11 event classes. Our model outperforms state-of-the-artmethods for both event classification and detection on this new dataset.Additionally, we show that the attention mechanism is able to consistentlylocalize the relevant players.
arxiv-14400-250 | Generating Images from Captions with Attention | http://arxiv.org/abs/1511.02793 | author:Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov category:cs.LG cs.CV published:2015-11-09 summary:Motivated by the recent progress in generative models, we introduce a modelthat generates images from natural language descriptions. The proposed modeliteratively draws patches on a canvas, while attending to the relevant words inthe description. After training on Microsoft COCO, we compare our model withseveral baseline generative models on image generation and retrieval tasks. Wedemonstrate that our model produces higher quality samples than otherapproaches and generates images with novel scene compositions corresponding topreviously unseen captions in the dataset.
arxiv-14400-251 | Deep Compositional Question Answering with Neural Module Networks | http://arxiv.org/abs/1511.02799 | author:Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein category:cs.CV cs.CL cs.LG cs.NE published:2015-11-09 summary:Visual question answering is fundamentally compositional in nature---aquestion like "where is the dog?" shares substructure with questions like "whatcolor is the dog?" and "where is the cat?" This paper seeks to simultaneouslyexploit the representational capacity of deep networks and the compositionallinguistic structure of questions. We describe a procedure for constructing andlearning *neural module networks*, which compose collections of jointly-trainedneural "modules" into deep networks for question answering. Our approachdecomposes questions into their linguistic substructures, and uses thesestructures to dynamically instantiate modular networks (with reusablecomponents for recognizing dogs, classifying colors, etc.). The resultingcompound networks are jointly trained. We evaluate our approach on twochallenging datasets for visual question answering, achieving state-of-the-artresults on both the VQA natural image dataset and a new dataset of complexquestions about abstract shapes.
arxiv-14400-252 | Deep Recurrent Neural Networks for Sequential Phenotype Prediction in Genomics | http://arxiv.org/abs/1511.02554 | author:Farhad Pouladi, Hojjat Salehinejad, Amir Mohammad Gilani category:cs.NE cs.CE cs.LG published:2015-11-09 summary:In analyzing of modern biological data, we are often dealing with ill-posedproblems and missing data, mostly due to high dimensionality andmulticollinearity of the dataset. In this paper, we have proposed a systembased on matrix factorization (MF) and deep recurrent neural networks (DRNNs)for genotype imputation and phenotype sequences prediction. In order to modelthe long-term dependencies of phenotype data, the new Recurrent Linear Units(ReLU) learning strategy is utilized for the first time. The proposed model isimplemented for parallel processing on central processing units (CPUs) andgraphic processing units (GPUs). Performance of the proposed model is comparedwith other training algorithms for learning long-term dependencies as well asthe sparse partial least square (SPLS) method on a set of genotype andphenotype data with 604 samples, 1980 single-nucleotide polymorphisms (SNPs),and two traits. The results demonstrate performance of the ReLU trainingalgorithm in learning long-term dependencies in RNNs.
arxiv-14400-253 | Towards Structured Deep Neural Network for Automatic Speech Recognition | http://arxiv.org/abs/1511.02506 | author:Yi-Hsiu Liao, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG cs.NE published:2015-11-08 summary:In this paper we propose the Structured Deep Neural Network (structured DNN)as a structured and deep learning framework. This approach can learn to findthe best structured object (such as a label sequence) given a structured input(such as a vector sequence) by globally considering the mapping relationshipsbetween the structures rather than item by item. When automatic speech recognition is viewed as a special case of such astructured learning problem, where we have the acoustic vector sequence as theinput and the phoneme label sequence as the output, it becomes possible tocomprehensively learn utterance by utterance as a whole, rather than frame byframe. Structured Support Vector Machine (structured SVM) was proposed to performASR with structured learning previously, but limited by the linear nature ofSVM. Here we propose structured DNN to use nonlinear transformations inmulti-layers as a structured and deep learning approach. This approach wasshown to beat structured SVM in preliminary experiments on TIMIT.
arxiv-14400-254 | Bearing fault diagnosis based on spectrum images of vibration signals | http://arxiv.org/abs/1511.02503 | author:Wei Li, Mingquan Qiu, Zhencai Zhu, Bo Wu, Gongbo Zhou category:cs.CV cs.SD published:2015-11-08 summary:Bearing fault diagnosis has been a challenge in the monitoring activities ofrotating machinery, and it's receiving more and more attention. Theconventional fault diagnosis methods usually extract features from thewaveforms or spectrums of vibration signals in order to realize faultclassification. In this paper, a novel feature in the form of images ispresented, namely the spectrum images of vibration signals. The spectrum imagesare simply obtained by doing fast Fourier transformation. Such images areprocessed with two-dimensional principal component analysis (2DPCA) to reducethe dimensions, and then a minimum distance method is applied to classify thefaults of bearings. The effectiveness of the proposed method is verified withexperimental data.
arxiv-14400-255 | Algorithmic Stability for Adaptive Data Analysis | http://arxiv.org/abs/1511.02513 | author:Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman category:cs.LG cs.CR cs.DS published:2015-11-08 summary:Adaptivity is an important feature of data analysis---the choice of questionsto ask about a dataset often depends on previous interactions with the samedataset. However, statistical validity is typically studied in a nonadaptivemodel, where all questions are specified before the dataset is drawn. Recentwork by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiatedthe formal study of this problem, and gave the first upper and lower bounds onthe achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution $\mathbf{P}$ and a setof $n$ independent samples $\mathbf{x}$ is drawn from $\mathbf{P}$. We seek analgorithm that, given $\mathbf{x}$ as input, accurately answers a sequence ofadaptively chosen queries about the unknown distribution $\mathbf{P}$. How manysamples $n$ must we draw from the distribution, as a function of the type ofqueries, the number of queries, and the desired level of accuracy? In this work we make two new contributions: (i) We give upper bounds on the number of samples $n$ that are needed toanswer statistical queries. The bounds improve and simplify the work of Dworket al. (STOC, 2015), and have been applied in subsequent work by those authors(Science, 2015, NIPS, 2015). (ii) We prove the first upper bounds on the number of samples required toanswer more general families of queries. These include arbitrarylow-sensitivity queries and an important class of optimization queries. As in Dwork et al., our algorithms are based on a connection with algorithmicstability in the form of differential privacy. We extend their work by giving aquantitatively optimal, more general, and simpler proof of their main theoremthat stability implies low generalization error. We also study weaker stabilityguarantees such as bounded KL divergence and total variation distance.
arxiv-14400-256 | Speed learning on the fly | http://arxiv.org/abs/1511.02540 | author:Pierre-Yves Massé, Yann Ollivier category:math.OC cs.LG stat.ML published:2015-11-08 summary:The practical performance of online stochastic gradient descent algorithms ishighly dependent on the chosen step size, which must be tediously hand-tuned inmany applications. The same is true for more advanced variants of stochasticgradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the stepsize by performing a gradient descent on the step size itself, viewing thewhole performance of the learning trajectory as a function of step size.Importantly, this adaptation can be computed online at little cost, withouthaving to iterate backward passes over the full data.
arxiv-14400-257 | LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks | http://arxiv.org/abs/1511.02462 | author:Steven C. H. Hoi, Xiongwei Wu, Hantang Liu, Yue Wu, Huiqiong Wang, Hui Xue, Qiang Wu category:cs.CV published:2015-11-08 summary:Logo detection from images has many applications, particularly for brandrecognition and intellectual property protection. Most existing studies forlogo recognition and detection are based on small-scale datasets which are notcomprehensive enough when exploring emerging deep learning techniques. In thispaper, we introduce "LOGO-Net", a large-scale logo image database for logodetection and brand recognition from real-world product images. To facilitateresearch, LOGO-Net has two datasets: (i)"logos-18" consists of 18 logo classes,10 brands, and 16,043 logo objects, and (ii) "logos-160" consists of 160 logoclasses, 100 brands, and 130,608 logo objects. We describe the ideas andchallenges for constructing such a large-scale database. Another keycontribution of this work is to apply emerging deep learning techniques forlogo detection and brand recognition tasks, and conduct extensive experimentsby exploring several state-of-the-art deep region-based convolutional networkstechniques for object detection tasks. The LOGO-net will be released athttp://logo-net.org/
arxiv-14400-258 | Sandwiching the marginal likelihood using bidirectional Monte Carlo | http://arxiv.org/abs/1511.02543 | author:Roger B. Grosse, Zoubin Ghahramani, Ryan P. Adams category:stat.ML cs.LG stat.CO published:2015-11-08 summary:Computing the marginal likelihood (ML) of a model requires marginalizing outall of the parameters and latent variables, a difficult high-dimensionalsummation or integration problem. To make matters worse, it is often hard tomeasure the accuracy of one's ML estimates. We present bidirectional MonteCarlo, a technique for obtaining accurate log-ML estimates on data simulatedfrom a model. This method obtains stochastic lower bounds on the log-ML usingannealed importance sampling or sequential Monte Carlo, and obtains stochasticupper bounds by running these same algorithms in reverse starting from an exactposterior sample. The true value can be sandwiched between these two stochasticbounds with high probability. Using the ground truth log-ML estimates obtainedfrom our method, we quantitatively evaluate a wide variety of existing MLestimators on several latent variable models: clustering, a low rankapproximation, and a binary attributes model. These experiments yield insightsinto how to accurately estimate marginal likelihoods.
arxiv-14400-259 | Poisson Inverse Problems by the Plug-and-Play scheme | http://arxiv.org/abs/1511.02500 | author:Arie Rond, Raja Giryes, Michael Elad category:cs.CV math.OC published:2015-11-08 summary:The Anscombe transform offers an approximate conversion of a Poisson randomvariable into a Gaussian one. This transform is important and appealing, as itis easy to compute, and becomes handy in various inverse problems with Poissonnoise contamination. Solution to such problems can be done by first applyingthe Anscombe transform, then applying a Gaussian-noise-oriented restorationalgorithm of choice, and finally applying an inverse Anscombe transform. Theappeal in this approach is due to the abundance of high-performance restorationalgorithms designed for white additive Gaussian noise (we will refer to thesehereafter as "Gaussian-solvers"). This process is known to work well for highSNR images, where the Anscombe transform provides a rather accurateapproximation. When the noise level is high, the above path loses much of itseffectiveness, and the common practice is to replace it with a direct treatmentof the Poisson distribution. Naturally, with this we lose the ability toleverage on vastly available Gaussian-solvers. In this work we suggest a novel method for coupling Gaussian denoisingalgorithms to Poisson noisy inverse problems, which is based on a generalapproach termed "Plug-and-Play". Deploying the Plug-and-Play approach to suchproblems leads to an iterative scheme that repeats several key steps: 1) Aconvex programming task of simple form that can be easily treated; 2) Apowerful Gaussian denoising algorithm of choice; and 3) A simple update step. Such a modular method, just like the Anscombe transform, enables otherdevelopers to plug their own Gaussian denoising algorithms to our scheme in aneasy way. While the proposed method bares some similarity to the Anscombeoperation, it is in fact based on a different mathematical basis, which holdstrue for all SNR ranges.
arxiv-14400-260 | VideoStory Embeddings Recognize Events when Examples are Scarce | http://arxiv.org/abs/1511.02492 | author:Amirhossein Habibian, Thomas Mensink, Cees G. M. Snoek category:cs.CV cs.MM published:2015-11-08 summary:This paper aims for event recognition when video examples are scarce or evencompletely absent. The key in such a challenging setting is a semantic videorepresentation. Rather than building the representation from individualattribute detectors and their annotations, we propose to learn the entirerepresentation from freely available web videos and their descriptions using anembedding between video features and term vectors. In our proposed embedding,which we call VideoStory, the correlations between the terms are utilized tolearn a more effective representation by optimizing a joint objective balancingdescriptiveness and predictability.We show how learning the VideoStory using amultimodal predictability loss, including appearance, motion and audiofeatures, results in a better predictable representation. We also propose avariant of VideoStory to recognize an event in video from just the importantterms in a text query by introducing a term sensitive descriptiveness loss. Ourexperiments on three challenging collections of web videos from the NISTTRECVID Multimedia Event Detection and Columbia Consumer Videos datasetsdemonstrate: i) the advantages of VideoStory over representations usingattributes or alternative embeddings, ii) the benefit of fusing videomodalities by an embedding over common strategies, iii) the complementarity ofterm sensitive descriptiveness and multimodal predictability for eventrecognition without examples. By it abilities to improve predictability uponany underlying video feature while at the same time maximizing semanticdescriptiveness, VideoStory leads to state-of-the-art accuracy for both few-and zero-example recognition of events in video.
arxiv-14400-261 | Statistical physics of inference: Thresholds and algorithms | http://arxiv.org/abs/1511.02476 | author:Lenka Zdeborová, Florent Krzakala category:cs.DS stat.ML published:2015-11-08 summary:Many questions of fundamental interest in todays science can be formulated asinference problems: Some partial, or noisy, observations are performed over aset of variables and the goal is to recover, or infer, the values of thevariables based on the indirect information contained in the measurements. Forsuch problems, the central scientific questions are: Under what conditions isthe information contained in the measurements sufficient for a satisfactoryinference to be possible? What are the most efficient algorithms for this task?A growing body of work has shown that often we can understand and locate thesefundamental barriers by thinking of them as phase transitions in the sense ofstatistical physics. Moreover, it turned out that we can use the gainedphysical insight to develop new promising algorithms. Connection betweeninference and statistical physics is currently witnessing an impressiverenaissance and we review here the current state-of-the-art, with a pedagogicalfocus on the Ising model which formulated as an inference problem we call theplanted spin glass. In terms of applications we review two classes of problems:(i) inference of clusters on graphs and networks, with community detection as aspecial case and (ii) estimating a signal from its noisy linear measurements,with compressed sensing as a case of sparse estimation. Our goal is to providea pedagogical review for researchers in physics and other fields interested inthis fascinating topic.
arxiv-14400-262 | Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment using Compound Skip-grams | http://arxiv.org/abs/1511.02436 | author:Sylvester Olubolu Orimaye, Kah Yee Tai, Jojo Sze-Meng Wong, Chee Piau Wong category:cs.CL cs.AI published:2015-11-08 summary:Predicting Mild Cognitive Impairment (MCI) is currently a challenge asexisting diagnostic criteria rely on neuropsychological examinations. AutomatedMachine Learning (ML) models that are trained on verbal utterances of MCIpatients can aid diagnosis. Using a combination of skip-gram features, ourmodel learned several linguistic biomarkers to distinguish between 19 patientswith MCI and 19 healthy control individuals from the DementiaBank languagetranscript clinical dataset. Results show that a model with compound ofskip-grams has better AUC and could help ML prediction on small MCI datasample.
arxiv-14400-263 | A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model | http://arxiv.org/abs/1511.02465 | author:Jie Xu, Lianwen Jin, Lingyu Liang, Ziyong Feng, Duorui Xie category:cs.CV published:2015-11-08 summary:This paper proposes a deep leaning method to address the challenging facialattractiveness prediction problem. The method constructs a convolutional neuralnetwork of facial beauty prediction using a new deep cascaded fine-turningscheme with various face inputting channels, such as the original RGB faceimage, the detail layer image, and the lighting layer image. With a carefullydesigned CNN model of deep structure, large input size and small convolutionalkernels, we have achieved a high prediction correlation of 0.88. This resultconvinces us that the problem of facial attractiveness prediction can be solvedby deep learning approach, and it also shows the important roles of the facialsmoothness, lightness, and color information that were involved in facialbeauty perception, which is consistent with the result of recent psychologystudies. Furthermore, we analyze the high-level features learnt by CNN throughvisualization of its hidden layers, and some interesting phenomena wereobserved. It is found that the contours and appearance of facial features,especially eyes and moth, are the most significant facial attributes for facialattractiveness prediction, which is also consistent with the visual perceptionintuition of human.
arxiv-14400-264 | SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception | http://arxiv.org/abs/1511.02459 | author:Duorui Xie, Lingyu Liang, Lianwen Jin, Jie Xu, Mengru Li category:cs.CV published:2015-11-08 summary:In this paper, a novel face dataset with attractiveness ratings, namely, theSCUT-FBP dataset, is developed for automatic facial beauty perception. Thisdataset provides a benchmark to evaluate the performance of different methodsfor facial attractiveness prediction, including the state-of-the-art deeplearning method. The SCUT-FBP dataset contains face portraits of 500 Asianfemale subjects with attractiveness ratings, all of which have been verified interms of rating distribution, standard deviation, consistency, andself-consistency. Benchmark evaluations for facial attractiveness predictionwere performed with different combinations of facial geometrical features andtexture features using classical statistical learning methods and the deeplearning method. The best Pearson correlation (0.8187) was achieved by the CNNmodel. Thus, the results of our experiments indicate that the SCUT-FBP datasetprovides a reliable benchmark for facial beauty perception.
arxiv-14400-265 | A Chinese POS Decision Method Using Korean Translation Information | http://arxiv.org/abs/1511.02435 | author:Son-Il Kwak, O-Chol Kown, Chang-Sin Kim, Yong-Il Pak, Gum-Chol Son, Chol-Jun Hwang, Hyon-Chol Kim, Hyok-Chol Sin, Gyong-Il Hyon, Sok-Min Han category:cs.CL published:2015-11-08 summary:In this paper we propose a method that imitates a translation expert usingthe Korean translation information and analyse the performance. Korean is goodat tagging than Chinese, so we can use this property in Chinese POS tagging.
arxiv-14400-266 | Stacked Attention Networks for Image Question Answering | http://arxiv.org/abs/1511.02274 | author:Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola category:cs.LG cs.CL cs.CV cs.NE published:2015-11-07 summary:This paper presents stacked attention networks (SANs) that learn to answernatural language questions from images. SANs use semantic representation of aquestion as query to search for the regions in an image that are related to theanswer. We argue that image question answering (QA) often requires multiplesteps of reasoning. Thus, we develop a multiple-layer SAN in which we query animage multiple times to infer the answer progressively. Experiments conductedon four image QA data sets demonstrate that the proposed SANs significantlyoutperform previous state-of-the-art approaches. The visualization of theattention layers illustrates the progress that the SAN locates the relevantvisual clues that lead to the answer of the question layer-by-layer.
arxiv-14400-267 | Performance Analysis of Multiclass Support Vector Machine Classification for Diagnosis of Coronary Heart Diseases | http://arxiv.org/abs/1511.02352 | author:Wiharto Wiharto, Hari Kusnanto, Herianto Herianto category:cs.LG published:2015-11-07 summary:Automatic diagnosis of coronary heart disease helps the doctor to support indecision making a diagnosis. Coronary heart disease have some types or levels.Referring to the UCI Repository dataset, it divided into 4 types or levels thatare labeled numbers 1-4 (low, medium, high and serious). The diagnosis modelscan be analyzed with multiclass classification approach. One of multiclassclassification approach used, one of which is a support vector machine (SVM).The SVM use due to strong performance of SVM in binary classification. Thisresearch study multiclass performance classification support vector machine todiagnose the type or level of coronary heart disease. Coronary heart diseasepatient data taken from the UCI Repository. Stages in this study ispreprocessing, which consist of, to normalizing the data, divide the data intodata training and testing. The next stage of multiclass classification andperformance analysis. This study uses multiclass SVM algorithm, namely: BinaryTree Support Vector Machine (BTSVM), One-Against-One (OAO), One-Against-All(OAA), Decision Direct Acyclic Graph (DDAG) and Exhaustive Output ErrorCorrection Code (ECOC). Performance parameter used is recall, precision,F-measure and Overall accuracy.
arxiv-14400-268 | The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations | http://arxiv.org/abs/1511.02301 | author:Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston category:cs.CL published:2015-11-07 summary:We introduce a new test of how well language models capture meaning inchildren's books. Unlike standard language modelling benchmarks, itdistinguishes the task of predicting syntactic function words from that ofpredicting lower-frequency words, which carry greater semantic content. Wecompare a range of state-of-the-art models, each with a different way ofencoding what has been previously read. We show that models which storeexplicit representations of long-term contexts outperform state-of-the-artneural language models at predicting semantic content words, although thisadvantage is not observed for syntactic function words. Interestingly, we findthat the amount of text encoded in a single memory representation is highlyinfluential to the performance: there is a sweet-spot, not too big and not toosmall, between single words and full sentences that allows the most meaningfulinformation in a text to be effectively retained and recalled. Further, theattention over such window-based memories can be trained effectively throughself-supervision. We then assess the generality of this principle by applyingit to the CNN QA benchmark, which involves identifying named entities inparaphrased summaries of news articles, and achieve state-of-the-artperformance.
arxiv-14400-269 | Max-Sum Diversification, Monotone Submodular Functions and Semi-metric Spaces | http://arxiv.org/abs/1511.02402 | author:Sepehr Abbasi Zadeh, Mehrdad Ghadiri category:cs.LG published:2015-11-07 summary:In many applications such as web-based search, document summarization,facility location and other applications, the results are preferable to be bothrepresentative and diversified subsets of documents. The goal of this study isto select a good "quality", bounded-size subset of a given set of items, whilemaintaining their diversity relative to a semi-metric distance function. Thisproblem was first studied by Borodin et al\cite{borodin}, but a crucialproperty used throughout their proof is the triangle inequality. In thismodified proof, we want to relax the triangle inequality and relate theapproximation ratio of max-sum diversification problem to the parameter of therelaxed triangle inequality in the normal form of the problem (i.e., a uniformmatroid) and also in an arbitrary matroid.
arxiv-14400-270 | Hierarchical Variational Models | http://arxiv.org/abs/1511.02386 | author:Rajesh Ranganath, Dustin Tran, David M. Blei category:stat.ML cs.LG stat.CO stat.ME published:2015-11-07 summary:Black box inference allows researchers to easily prototype and evaluate anarray of models. Recent advances in variational inference allow such algorithmsto scale to high dimensions. However, a central question remains: How tospecify an expressive variational distribution which maintains efficientcomputation? To address this, we develop hierarchical variational models. In ahierarchical variational model, the variational approximation is augmented witha prior on its parameters, such that the latent variables are conditionallyindependent given this shared structure. This preserves the computationalefficiency of the original approximation, while admitting hierarchicallycomplex distributions for both discrete and continuous latent variables. Westudy hierarchical variational models on a variety of deep discrete latentvariable models. Hierarchical variational models generalize other expressivevariational distributions and maintains higher fidelity to the posterior.
arxiv-14400-271 | Fingertip in the Eye: A cascaded CNN pipeline for the real-time fingertip detection in egocentric videos | http://arxiv.org/abs/1511.02282 | author:Xiaorui Liu, Yichao Huang, Xin Zhang, Lianwen Jin category:cs.CV published:2015-11-07 summary:We introduce a new pipeline for hand localization and fingertip detection.For RGB images captured from an egocentric vision mobile camera, hand andfingertip detection remains a challenging problem due to factors likebackground complexity and hand shape variety. To address these issuesaccurately and robustly, we build a large scale dataset named Ego-Fingertip andpropose a bi-level cascaded pipeline of convolutional neural networks, namely,Attention-based Hand Detector as well as Multi-point Fingertip Detector. Theproposed method significantly tackles challenges and achieves satisfactorilyaccurate prediction and real-time performance compared to previous hand andfingertip detection methods.
arxiv-14400-272 | Information Extraction Under Privacy Constraints | http://arxiv.org/abs/1511.02381 | author:Shahab Asoodeh, Mario Diaz, Fady Alajaji, Tamás Linder category:cs.IT math.IT math.ST stat.ML stat.TH published:2015-11-07 summary:A privacy-constrained information extraction problem is considered where fora pair of correlated discrete random variables $(X,Y)$ governed by a givenjoint distribution, an agent observes $Y$ and wants to convey to a potentiallypublic user as much information about $Y$ as possible without compromising theamount of information revealed about $X$. To this end, the so-called {\emrate-privacy function} is introduced to quantify the maximal amount ofinformation (measured in terms of mutual information) that can be extractedfrom $Y$ under a privacy constraint between $X$ and the extracted information,where privacy is measured using either mutual information or maximalcorrelation. Properties of the rate-privacy function are analyzed andinformation-theoretic and estimation-theoretic interpretations of it arepresented for both the mutual information and maximal correlation privacymeasures. It is also shown that the rate-privacy function admits a closed-formexpression for a large family of joint distributions of $(X,Y)$. Finally, therate-privacy function under the mutual information privacy measure isconsidered for the case where $(X,Y)$ has a joint probability density functionby studying the problem where the extracted information is a uniformquantization of $Y$ corrupted by additive Gaussian noise. The asymptoticbehavior of the rate-privacy function is studied as the quantization resolutiongrows without bound and it is observed that not all of the properties of therate-privacy function carry over from the discrete to the continuous case.
arxiv-14400-273 | A Survey of the Trends in Facial and Expression Recognition Databases and Methods | http://arxiv.org/abs/1511.02407 | author:Sohini Roychowdhury, Michelle Emmons category:cs.CV published:2015-11-07 summary:Automated facial identification and facial expression recognition have beentopics of active research over the past few decades. Facial and expressionrecognition find applications in human-computer interfaces, subject tracking,real-time security surveillance systems and social networking. Several holisticand geometric methods have been developed to identify faces and expressionsusing public and local facial image databases. In this work we present theevolution in facial image data sets and the methodologies for facialidentification and recognition of expressions such as anger, sadness,happiness, disgust, fear and surprise. We observe that most of the earliermethods for facial and expression recognition aimed at improving therecognition rates for facial feature-based methods using static images.However, the recent methodologies have shifted focus towards robustimplementation of facial/expression recognition from large image databases thatvary with space (gathered from the internet) and time (video recordings). Theevolution trends in databases and methodologies for facial and expressionrecognition can be useful for assessing the next-generation topics that mayhave applications in security systems or personal identification systems thatinvolve "Quantitative face" assessments.
arxiv-14400-274 | Review-Level Sentiment Classification with Sentence-Level Polarity Correction | http://arxiv.org/abs/1511.02385 | author:Sylvester Olubolu Orimaye, Saadat M. Alhashmi, Eu-Gene Siew, Sang Jung Kang category:cs.CL cs.AI cs.LG published:2015-11-07 summary:We propose an effective technique to solving review-level sentimentclassification problem by using sentence-level polarity correction. Ourpolarity correction technique takes into account the consistency of thepolarities (positive and negative) of sentences within each product reviewbefore performing the actual machine learning task. While sentences withinconsistent polarities are removed, sentences with consistent polarities areused to learn state-of-the-art classifiers. The technique achieved betterresults on different types of products reviews and outperforms baseline modelswithout the correction technique. Experimental results show an average of 82%F-measure on four different product review domains.
arxiv-14400-275 | Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images | http://arxiv.org/abs/1511.02300 | author:Shuran Song, Jianxiong Xiao category:cs.CV published:2015-11-07 summary:We focus on the task of amodal 3D object detection in RGB-D images, whichaims to produce a 3D bounding box of an object in metric form at its fullextent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a3D volumetric scene from a RGB-D image as input and outputs 3D object boundingboxes. In our approach, we propose the first 3D Region Proposal Network (RPN)to learn objectness from geometric shapes and the first joint ObjectRecognition Network (ORN) to extract geometric features in 3D and colorfeatures in 2D. In particular, we handle objects of various sizes by trainingan amodal RPN at two different scales and an ORN to regress 3D bounding boxes.Experiments show that our algorithm outperforms the state-of-the-art by 13.8 inmAP and is 200x faster than the original Sliding Shapes. All source code andpre-trained models will be available at GitHub.
arxiv-14400-276 | Review of Person Re-identification Techniques | http://arxiv.org/abs/1511.02319 | author:Mohammad Ali Saghafi, Aini Hussain, Halimah Badioze Zaman, Mohamad Hanif Md Saad category:cs.CV published:2015-11-07 summary:Person re-identification across different surveillance cameras with disjointfields of view has become one of the most interesting and challenging subjectsin the area of intelligent video surveillance. Although several methods havebeen developed and proposed, certain limitations and unresolved issues remain.In all of the existing re-identification approaches, feature vectors areextracted from segmented still images or video frames. Different similarity ordissimilarity measures have been applied to these vectors. Some methods haveused simple constant metrics, whereas others have utilised models to obtainoptimised metrics. Some have created models based on local colour or textureinformation, and others have built models based on the gait of people. Ingeneral, the main objective of all these approaches is to achieve ahigher-accuracy rate and lowercomputational costs. This study summarisesseveral developments in recent literature and discusses the various availablemethods used in person re-identification. Specifically, their advantages anddisadvantages are mentioned and compared.
arxiv-14400-277 | Signed Support Recovery for Single Index Models in High-Dimensions | http://arxiv.org/abs/1511.02270 | author:Matey Neykov, Qian Lin, Jun S. Liu category:math.ST stat.ML stat.TH published:2015-11-07 summary:In this paper we study the support recovery problem for single index models$Y=f(\boldsymbol{X}^{\intercal} \boldsymbol{\beta},\varepsilon)$, where $f$ isan unknown link function, $\boldsymbol{X}\sim N_p(0,\mathbb{I}_{p})$ and$\boldsymbol{\beta}$ is an $s$-sparse unit vector such that$\boldsymbol{\beta}_{i}\in \{\pm\frac{1}{\sqrt{s}},0\}$. In particular, we lookinto the performance of two computationally inexpensive algorithms: (a) thediagonal thresholding sliced inverse regression (DT-SIR) introduced by Lin etal. (2015); and (b) a semi-definite programming (SDP) approach inspired byAmini & Wainwright (2008). When $s=O(p^{1-\delta})$ for some $\delta>0$, wedemonstrate that both procedures can succeed in recovering the support of$\boldsymbol{\beta}$ as long as the rescaled sample size$\kappa=\frac{n}{s\log(p-s)}$ is larger than a certain critical threshold. Onthe other hand, when $\kappa$ is smaller than a critical value, any algorithmfails to recover the support with probability at least $\frac{1}{2}$asymptotically. In other words, we demonstrate that both DT-SIR and the SDPapproach are optimal (up to a scalar) for recovering the support of$\boldsymbol{\beta}$ in terms of sample size.
arxiv-14400-278 | Generation and Comprehension of Unambiguous Object Descriptions | http://arxiv.org/abs/1511.02283 | author:Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy category:cs.CV cs.CL cs.LG cs.RO published:2015-11-07 summary:We propose a method that can generate an unambiguous description (known as areferring expression) of a specific object or region in an image, and which canalso comprehend or interpret such an expression to infer which object is beingdescribed. We show that our method outperforms previous methods that generatedescriptions of objects without taking into account other potentially ambiguousobjects in the scene. Our model is inspired by recent successes of deeplearning methods for image captioning, but while image captioning is difficultto evaluate, our task allows for easy objective evaluation. We also present anew large-scale dataset for referring expressions, based on MS-COCO. We havereleased the dataset and a toolbox for visualization and evaluation, seehttps://github.com/mjhucla/Google_Refexp_toolbox
arxiv-14400-279 | Barrier Frank-Wolfe for Marginal Inference | http://arxiv.org/abs/1511.02124 | author:Rahul G. Krishnan, Simon Lacoste-Julien, David Sontag category:stat.ML cs.LG math.OC published:2015-11-06 summary:We introduce a globally-convergent algorithm for optimizing thetree-reweighted (TRW) variational objective over the marginal polytope. Thealgorithm is based on the conditional gradient method (Frank-Wolfe) and movespseudomarginals within the marginal polytope through repeated maximum aposteriori (MAP) calls. This modular structure enables us to leverage black-boxMAP solvers (both exact and approximate) for variational inference, and obtainsmore accurate results than tree-reweighted algorithms that optimize over thelocal consistency relaxation. Theoretically, we bound the sub-optimality forthe proposed algorithm despite the TRW objective having unbounded gradients atthe boundary of the marginal polytope. Empirically, we demonstrate theincreased quality of results found by tightening the relaxation over themarginal polytope as well as the spanning tree polytope on synthetic andreal-world instances.
arxiv-14400-280 | Finding structure in data using multivariate tree boosting | http://arxiv.org/abs/1511.02025 | author:Patrick J. Miller, Gitta H. Lubke, Daniel B. McArtor, C. S. Bergeman category:stat.ML cs.LG published:2015-11-06 summary:Technology and collaboration enable dramatic increases in the size ofpsychological and psychiatric data collections, but finding structure in theselarge data sets with many collected variables is challenging. Decision treeensembles like random forests (Strobl, Malley, and Tutz, 2009) are a usefultool for finding structure, but are difficult to interpret with multipleoutcome variables which are often of interest in psychology. To find andinterpret structure in data sets with multiple outcomes and many predictors(possibly exceeding the sample size), we introduce a multivariate extension toa decision tree ensemble method called Gradient Boosted Regression Trees(Friedman, 2001). Our method, multivariate tree boosting, can be used foridentifying important predictors, detecting predictors with non-linear effectsand interactions without specification of such effects, and for identifyingpredictors that cause two or more outcome variables to covary withoutparametric assumptions. We provide the R package 'mvtboost' to estimate, tune,and interpret the resulting model, which extends the implementation ofunivariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous,multivariate outcomes. To illustrate the approach, we analyze predictors ofpsychological well-being (Ryff and Keyes, 1995). Simulations verify that ourapproach identifies predictors with non-linear effects and achieves highprediction accuracy, exceeding or matching the performance of (penalized)multivariate multiple regression and multivariate decision trees over a widerange of conditions.
arxiv-14400-281 | Population size predicts lexical diversity, but so does the mean sea level - why it is important to correctly account for the structure of temporal data | http://arxiv.org/abs/1511.02014 | author:Alexander Koplenig, Carolin Mueller-Spitzer category:cs.CL published:2015-11-06 summary:In order to demonstrate why it is important to correctly account for the(serial dependent) structure of temporal data, we document an apparentlyspectacular relationship between population size and lexical diversity: forfive out of seven investigated languages, there is a strong relationshipbetween population size and lexical diversity of the primary language in thiscountry. We show that this relationship is the result of a misspecified modelthat does not consider the temporal aspect of the data by presenting a similarbut nonsensical relationship between the global annual mean sea level andlexical diversity. Given the fact that in the recent past, several studies werepublished that present surprising links between different economic, cultural,political and (socio-)demographical variables on the one hand and cultural orlinguistic characteristics on the other hand, but seem to suffer from exactlythis problem, we explain the cause of the misspecification and show that it hasprofound consequences. We demonstrate how simple transformation of the timeseries can often solve problems of this type and argue that the evaluation ofthe plausibility of a relationship is important in this context. We hope thatour paper will help both researchers and reviewers to understand why it isimportant to use special models for the analysis of data with a naturaltemporal ordering.
arxiv-14400-282 | Efficient Multiscale Gaussian Process Regression using Hierarchical Clustering | http://arxiv.org/abs/1511.02258 | author:Z. Zhang, K. Duraisamy, N. A. Gumerov category:cs.LG stat.ML published:2015-11-06 summary:Standard Gaussian Process (GP) regression, a powerful machine learning tool,is computationally expensive when it is applied to large datasets, andpotentially inaccurate when data points are sparsely distributed in ahigh-dimensional feature space. To address these challenges, a new multiscale,sparsified GP algorithm is formulated, with the goal of application to largescientific computing datasets. In this approach, the data is partitioned intoclusters and the cluster centers are used to define a reduced training set,resulting in an improvement over standard GPs in terms of training andevaluation costs. Further, a hierarchical technique is used to adaptively mapthe local covariance representation to the underlying sparsity of the featurespace, leading to improved prediction accuracy when the data distribution ishighly non-uniform. A theoretical investigation of the computational complexityof the algorithm is presented. The efficacy of this method is then demonstratedon smooth and discontinuous analytical functions and on data from a directnumerical simulation of turbulent combustion.
arxiv-14400-283 | Search-Convolutional Neural Networks | http://arxiv.org/abs/1511.02136 | author:James Atwood, Don Towsley category:cs.LG published:2015-11-06 summary:We present a new deterministic relational model derived from convolutionalneural networks. Search-Convolutional Neural Networks (SCNNs) extend the notionof convolution to graph search to construct a rich latent representation thatextracts local behavior from general graph-structured data. Unlike other neuralnetwork models that take graph-structured data as input, SCNNs have aparameterization that is independent of input size, a property that enablestransfer learning between datasets. SCNNs can be applied to a wide variety ofprediction tasks, including node classification, community detection, and linkprediction. Our results indicate that SCNNs can offer considerable lift overoff-the-shelf classifiers and simple multilayer perceptrons, and comparableperformance to state-of-the-art probabilistic graphical models at considerablylower computational cost.
arxiv-14400-284 | The Poisson Gamma Belief Network | http://arxiv.org/abs/1511.02199 | author:Mingyuan Zhou, Yulai Cong, Bo Chen category:stat.ML stat.ME published:2015-11-06 summary:To infer a multilayer representation of high-dimensional count vectors, wepropose the Poisson gamma belief network (PGBN) that factorizes each of itslayers into the product of a connection weight matrix and the nonnegative realhidden units of the next layer. The PGBN's hidden layers are jointly trainedwith an upward-downward Gibbs sampler, each iteration of which upward samplesDirichlet distributed connection weight vectors starting from the first layer(bottom data layer), and then downward samples gamma distributed hidden unitsstarting from the top hidden layer. The gamma-negative binomial processcombined with a layer-wise training strategy allows the PGBN to infer the widthof each layer given a fixed budget on the width of the first layer. The PGBNwith a single hidden layer reduces to Poisson factor analysis. Example resultson text analysis illustrate interesting relationships between the width of thefirst layer and the inferred network structure, and demonstrate that the PGBN,whose hidden units are imposed with correlated gamma priors, can add morelayers to increase its performance gains over Poisson factor analysis, giventhe same limit on the width of the first layer.
arxiv-14400-285 | ALOJA-ML: A Framework for Automating Characterization and Knowledge Discovery in Hadoop Deployments | http://arxiv.org/abs/1511.02030 | author:Josep Ll. Berral, Nicolas Poggi, David Carrera, Aaron Call, Rob Reinauer, Daron Green category:cs.LG cs.DC C.4; I.2.6 published:2015-11-06 summary:This article presents ALOJA-Machine Learning (ALOJA-ML) an extension to theALOJA project that uses machine learning techniques to interpret Hadoopbenchmark performance data and performance tuning; here we detail the approach,efficacy of the model and initial results. Hadoop presents a complex executionenvironment, where costs and performance depends on a large number of software(SW) configurations and on multiple hardware (HW) deployment choices. Theseresults are accompanied by a test bed and tools to deploy and evaluate thecost-effectiveness of the different hardware configurations, parameter tunings,and Cloud services. Despite early success within ALOJA from expert-guidedbenchmarking, it became clear that a genuinely comprehensive study requiresautomation of modeling procedures to allow a systematic analysis of large andresource-constrained search spaces. ALOJA-ML provides such an automated systemallowing knowledge discovery by modeling Hadoop executions from observedbenchmarks across a broad set of configuration parameters. The resultingperformance models can be used to forecast execution behavior of variousworkloads; they allow 'a-priori' prediction of the execution times for newconfigurations and HW choices and they offer a route to model-based anomalydetection. In addition, these models can guide the benchmarking explorationefficiently, by automatically prioritizing candidate future benchmark tests.Insights from ALOJA-ML's models can be used to reduce the operational time onclusters, speed-up the data acquisition and knowledge discovery process, andimportantly, reduce running costs. In addition to learning from the methodologypresented in this work, the community can benefit in general from ALOJAdata-sets, framework, and derived insights to improve the design and deploymentof Big Data applications.
arxiv-14400-286 | ALOJA: A Framework for Benchmarking and Predictive Analytics in Big Data Deployments | http://arxiv.org/abs/1511.02037 | author:Josep Ll. Berral, Nicolas Poggi, David Carrera, Aaron Call, Rob Reinauer, Daron Green category:cs.LG cs.DC C.4; I.2.6 published:2015-11-06 summary:This article presents the ALOJA project and its analytics tools, whichleverages machine learning to interpret Big Data benchmark performance data andtuning. ALOJA is part of a long-term collaboration between BSC and Microsoft toautomate the characterization of cost-effectiveness on Big Data deployments,currently focusing on Hadoop. Hadoop presents a complex run-time environment,where costs and performance depend on a large number of configuration choices.The ALOJA project has created an open, vendor-neutral repository, featuringover 40,000 Hadoop job executions and their performance details. The repositoryis accompanied by a test-bed and tools to deploy and evaluate thecost-effectiveness of different hardware configurations, parameters and Cloudservices. Despite early success within ALOJA, a comprehensive study requiresautomation of modeling procedures to allow an analysis of large andresource-constrained search spaces. The predictive analytics extension,ALOJA-ML, provides an automated system allowing knowledge discovery by modelingenvironments from observed executions. The resulting models can forecastexecution behaviors, predicting execution times for new configurations andhardware choices. That also enables model-based anomaly detection or efficientbenchmark guidance by prioritizing executions. In addition, the community canbenefit from ALOJA data-sets and framework to improve the design and deploymentof Big Data applications.
arxiv-14400-287 | Towards a Better Understanding of Predict and Count Models | http://arxiv.org/abs/1511.02024 | author:S. Sathiya Keerthi, Tobias Schnabel, Rajiv Khanna category:cs.LG cs.CL published:2015-11-06 summary:In a recent paper, Levy and Goldberg pointed out an interesting connectionbetween prediction-based word embedding models and count models based onpointwise mutual information. Under certain conditions, they showed that bothmodels end up optimizing equivalent objective functions. This paper exploresthis connection in more detail and lays out the factors leading to differencesbetween these models. We find that the most relevant differences from anoptimization perspective are (i) predict models work in a low dimensional spacewhere embedding vectors can interact heavily; (ii) since predict models havefewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can beregularized in a principled manner and provide closed-form solutions for L1 andL2 regularization. Finally, we propose a new embedding model with a convexobjective and the additional benefit of being intelligible.
arxiv-14400-288 | Facial Expression Recognition Using Sparse Gaussian Conditional Random Field | http://arxiv.org/abs/1511.02023 | author:Mohammadamin Abbasnejad, Mohammad Ali Masnadi-Shirazi category:cs.CV published:2015-11-06 summary:The analysis of expression and facial Action Units (AUs) detection are veryimportant tasks in fields of computer vision and Human Computer Interaction(HCI) due to the wide range of applications in human life. Many works has beendone during the past few years which has their own advantages anddisadvantages. In this work we present a new model based on GaussianConditional Random Field. We solve our objective problem using ADMM and we showhow well the proposed model works. We train and test our work on two facialexpression datasets, CK+ and RU-FACS. Experimental evaluation shows that ourproposed approach outperform state of the art expression recognition.
arxiv-14400-289 | Active Perceptual Similarity Modeling with Auxiliary Information | http://arxiv.org/abs/1511.02254 | author:Eric Heim, Matthew Berger, Lee Seversky, Milos Hauskrecht category:cs.LG stat.ML published:2015-11-06 summary:Learning a model of perceptual similarity from a collection of objects is afundamental task in machine learning underlying numerous applications. A commonway to learn such a model is from relative comparisons in the form of triplets:responses to queries of the form "Is object a more similar to b than it is toc?". If no consideration is made in the determination of which queries to ask,existing similarity learning methods can require a prohibitively large numberof responses. In this work, we consider the problem of actively learning fromtriplets -finding which queries are most useful for learning. Different fromprevious active triplet learning approaches, we incorporate auxiliaryinformation into our similarity model and introduce an active learning schemeto find queries that are informative for quickly learning both the relevantaspects of auxiliary data and the directly-learned similarity components.Compared to prior approaches, we show that we can learn just as effectivelywith much fewer queries. For evaluation, we introduce a new dataset ofexhaustive triplet comparisons obtained from humans and demonstrate improvedperformance for different types of auxiliary information.
arxiv-14400-290 | Multi-lingual Geoparsing based on Machine Translation | http://arxiv.org/abs/1511.01974 | author:Xu Chen, Han Zhang, Judith Gelernter category:cs.CL cs.IR published:2015-11-06 summary:Our method for multi-lingual geoparsing uses monolingual tools and resourcesalong with machine translation and alignment to return location words in manylanguages. Not only does our method save the time and cost of developinggeoparsers for each language separately, but also it allows the possibility ofa wide range of language capabilities within a single interface. We evaluatedour method in our LanguageBridge prototype on location named entities usingnewswire, broadcast news and telephone conversations in English, Arabic andChinese data from the Linguistic Data Consortium (LDC). Our results forgeoparsing Chinese and Arabic text using our multi-lingual geoparsing methodare comparable to our results for geoparsing English text with our Englishtools. Furthermore, experiments using our machine translation approach resultsin accuracy comparable to results from the same data that was translatedmanually.
arxiv-14400-291 | Streaming regularization parameter selection via stochastic gradient descent | http://arxiv.org/abs/1511.02187 | author:Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML published:2015-11-06 summary:We propose a framework to perform streaming covariance selection. Ourapproach employs regularization constraints where a time-varying sparsityparameter is iteratively estimated via stochastic gradient descent. This allowsfor the regularization parameter to be efficiently learnt in an online manner.The proposed framework is developed for linear regression models and extendedto graphical models via neighbourhood selection. Under mild assumptions, we areable to obtain convergence results in a non-stochastic setting. Thecapabilities of such an approach are demonstrated using both synthetic data aswell as neuroimaging data.
arxiv-14400-292 | Hierarchical Coupled Geometry Analysis for Neuronal Structure and Activity Pattern Discovery | http://arxiv.org/abs/1511.02086 | author:Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Uri Dubin, Ronald R. Coifman category:q-bio.QM q-bio.NC stat.ML published:2015-11-06 summary:In the wake of recent advances in experimental methods in neuroscience, theability to record in-vivo neuronal activity from awake animals has becomefeasible. The availability of such rich and detailed physiological measurementscalls for the development of advanced data analysis tools, as commonly usedtechniques do not suffice to capture the spatio-temporal network complexity. Inthis paper, we propose a new hierarchical coupled geometry analysis, whichexploits the hidden connectivity structures between neurons and the dynamicpatterns at multiple time-scales. Our approach gives rise to the jointorganization of neurons and dynamic patterns in data-driven hierarchical datastructures. These structures provide local to global data representations, fromlocal partitioning of the data in flexible trees through a new multiscalemetric to a global manifold embedding. The application of our techniques toin-vivo neuronal recordings demonstrate the capability of extracting neuronalactivity patterns and identifying temporal trends, associated with particularbehavioral events and manipulations introduced in the experiments.
arxiv-14400-293 | Learning Visual Features from Large Weakly Supervised Data | http://arxiv.org/abs/1511.02251 | author:Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache category:cs.CV published:2015-11-06 summary:Convolutional networks trained on large supervised dataset produce visualfeatures which form the basis for the state-of-the-art in many computer-visionproblems. Further improvements of these visual features will likely requireeven larger manually labeled data sets, which severely limits the pace at whichprogress can be made. In this paper, we explore the potential of leveragingmassive, weakly-labeled image collections for learning good visual features. Wetrain convolutional networks on a dataset of 100 million Flickr photos andcaptions, and show that these networks produce features that perform well in arange of vision problems. We also show that the networks appropriately captureword similarity, and learn correspondences between different languages.
arxiv-14400-294 | Neutralized Empirical Risk Minimization with Generalization Neutrality Bound | http://arxiv.org/abs/1511.01987 | author:Kazuto Fukuchi, Jun Sakuma category:stat.ML published:2015-11-06 summary:Currently, machine learning plays an important role in the lives andindividual activities of numerous people. Accordingly, it has become necessaryto design machine learning algorithms to ensure that discrimination, biasedviews, or unfair treatment do not result from decision making or predictionsmade via machine learning. In this work, we introduce a novel empirical riskminimization (ERM) framework for supervised learning, neutralized ERM (NERM)that ensures that any classifiers obtained can be guaranteed to be neutral withrespect to a viewpoint hypothesis. More specifically, given a viewpointhypothesis, NERM works to find a target hypothesis that minimizes the empiricalrisk while simultaneously identifying a target hypothesis that is neutral tothe viewpoint hypothesis. Within the NERM framework, we derive a theoreticalbound on empirical and generalization neutrality risks. Furthermore, as arealization of NERM with linear classification, we derive a max-marginalgorithm, neutral support vector machine (SVM). Experimental results show thatour neutral SVM shows improved classification performance in real datasetswithout sacrificing the neutrality guarantee.
arxiv-14400-295 | Enhanced Low-Rank Matrix Approximation | http://arxiv.org/abs/1511.01966 | author:Ankit Parekh, Ivan W. Selesnick category:cs.CV cs.LG math.OC published:2015-11-06 summary:This letter proposes to estimate low-rank matrices by formulating a convexoptimization problem with non-convex regularization. We employ parameterizednon-convex penalty functions to estimate the non-zero singular values moreaccurately than the nuclear norm. A closed-form solution for the global optimumof the proposed objective function (sum of data fidelity and the non-convexregularizer) is also derived. The solution reduces to singular valuethresholding method as a special case. The proposed method is demonstrated forimage denoising.
arxiv-14400-296 | Introducing SKYSET - a Quintuple Approach for Improving Instructions | http://arxiv.org/abs/1511.02117 | author:Kerry Fultz, Seth Filip category:cs.CL published:2015-11-06 summary:A new approach called SKYSET (Synthetic Knowledge Yield Social EntitiesTranslation) is proposed to validate completeness and to reduce ambiguity fromwritten instructional documentation. SKYSET utilizes a quintuple set ofstandardized categories, which differs from traditional approaches thattypically use triples. The SKYSET System defines the categories required toform a standard template for representing information that is portable acrossdifferent domains. It provides a standardized framework that enables sentencesfrom written instructions to be translated into sets of category typed entitieson a table or database. The SKYSET entities contain conceptual units or phrasesthat represent information from the original source documentation. SKYSETenables information concatenation where multiple documents from differentdomains can be translated and combined into a single common filterable andsearchable table of entities.
arxiv-14400-297 | Seven ways to improve example-based single image super resolution | http://arxiv.org/abs/1511.02228 | author:Radu Timofte, Rasmus Rothe, Luc Van Gool category:cs.CV published:2015-11-06 summary:In this paper we present seven techniques that everybody should know toimprove example-based single image super resolution (SR): 1) augmentation ofdata, 2) use of large dictionaries with efficient search structures, 3)cascading, 4) image self-similarities, 5) back projection refinement, 6)enhanced prediction by consistency check, and 7) context reasoning. We validateour seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) andmethods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantialimprovements.The techniques are widely applicable and require no changes oronly minor adjustments of the SR methods. Moreover, our Improved A+ (IA) methodsets new state-of-the-art results outperforming A+ by up to 0.9dB on averagePSNR whilst maintaining a low time complexity.
arxiv-14400-298 | Next Generation Multicuts for Semi-Planar Graphs | http://arxiv.org/abs/1511.01994 | author:Julian Yarkony category:cs.CV cs.DS published:2015-11-06 summary:We study the problem of multicut segmentation. We introduce modified versionsof the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our workto natural image segmentation.
arxiv-14400-299 | Deep Kernel Learning | http://arxiv.org/abs/1511.02222 | author:Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing category:cs.LG cs.AI stat.ME stat.ML published:2015-11-06 summary:We introduce scalable deep kernels, which combine the structural propertiesof deep learning architectures with the non-parametric flexibility of kernelmethods. Specifically, we transform the inputs of a spectral mixture basekernel with a deep architecture, using local kernel interpolation, inducingpoints, and structure exploiting (Kronecker and Toeplitz) algebra for ascalable kernel representation. These closed-form kernels can be used asdrop-in replacements for standard kernels, with benefits in expressive powerand scalability. We jointly learn the properties of these kernels through themarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$for $n$ training points, and predictions cost $O(1)$ per test point. On a largeand diverse collection of applications, including a dataset with 2 millionexamples, we show improved performance over scalable Gaussian processes withflexible kernel learning models, and stand-alone deep architectures.
arxiv-14400-300 | Pooling the Convolutional Layers in Deep ConvNets for Action Recognition | http://arxiv.org/abs/1511.02126 | author:Shichao Zhao, Yanbin Liu, Yahong Han, Richang Hong category:cs.CV published:2015-11-06 summary:Deep ConvNets have shown its good performance in image classification tasks.However it still remains as a problem in deep video representation for actionrecognition. The problem comes from two aspects: on one hand, current videoConvNets are relatively shallow compared with image ConvNets, which limits itscapability of capturing the complex video action information; on the otherhand, temporal information of videos is not properly utilized to pool andencode the video sequences. Towards these issues, in this paper, we utilize twostate-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and thetemporal net from Two-Stream ConvNets, for action representation. Theconvolutional layers and the proposed new layer, called frame-diff layer, areextracted and pooled with two temporal pooling strategy: Trajectory pooling andline pooling. The pooled local descriptors are then encoded with VLAD to formthe video representations. In order to verify the effectiveness of the proposedframework, we conduct experiments on UCF101 and HMDB51 datasets. It achievesthe accuracy of 93.78\% on UCF101 which is the state-of-the-art and theaccuracy of 65.62\% on HMDB51 which is comparable to the state-of-the-art.
