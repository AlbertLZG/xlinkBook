arxiv-12000-1 | Fast Randomized Singular Value Thresholding for Nuclear Norm Minimization | http://arxiv.org/pdf/1509.00296v1.pdf | author:Tae-Hyun Oh, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon category:cs.CV published:2015-09-01 summary:Rank minimization can be boiled down to tractable surrogate problems, such asNuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems relatedto NNM (or WNNM) can be solved iteratively by applying a closed-form proximaloperator, called Singular Value Thresholding (SVT) (or Weighted SVT), but theysuffer from high computational cost of computing Singular Value Decomposition(SVD) at each iteration. We propose a fast and accurate approximation methodfor SVT, that we call fast randomized SVT (FRSVT), where we avoid directcomputation of SVD. The key idea is to extract an approximate basis for therange of a matrix from its compressed matrix. Given the basis, we compute thepartial singular values of the original matrix from a small factored matrix. Inaddition, by adopting a range propagation technique, our method further speedsup the extraction of approximate basis at each iteration. Our theoreticalanalysis shows the relationship between the approximation bound of SVD and itseffect to NNM via SVT. Along with the analysis, our empirical resultsquantitatively and qualitatively show that our approximation rarely harms theconvergence of the host algorithms. We assess the efficiency and accuracy ofour method on various vision problems, e.g., subspace clustering, weatherartifact removal, and simultaneous multi-image alignment and rectification.
arxiv-12000-2 | Robust Face Recognition via Multimodal Deep Face Representation | http://arxiv.org/pdf/1509.00244v1.pdf | author:Changxing Ding, Dacheng Tao category:cs.CV published:2015-09-01 summary:Face images appeared in multimedia applications, e.g., social networks anddigital entertainment, usually exhibit dramatic pose, illumination, andexpression variations, resulting in considerable performance degradation fortraditional face recognition algorithms. This paper proposes a comprehensivedeep learning framework to jointly learn face representation using multimodalinformation. The proposed deep learning structure is composed of a set ofelaborately designed convolutional neural networks (CNNs) and a three-layerstacked auto-encoder (SAE). The set of CNNs extracts complementary facialfeatures from multimodal data. Then, the extracted features are concatenated toform a high-dimensional feature vector, whose dimension is compressed by SAE.All the CNNs are trained using a subset of 9,000 subjects from the publiclyavailable CASIA-WebFace database, which ensures the reproducibility of thiswork. Using the proposed single CNN architecture and limited training data,98.43% verification rate is achieved on the LFW database. Benefited from thecomplementary information contained in multimodal data, our small ensemblesystem achieves higher than 99.0% recognition rate on LFW using publiclyavailable training set.
arxiv-12000-3 | Fingerprinting-Based Positioning in Distributed Massive MIMO Systems | http://arxiv.org/pdf/1509.00202v1.pdf | author:Vladimir Savic, Erik G. Larsson category:cs.IT cs.LG math.IT published:2015-09-01 summary:Location awareness in wireless networks may enable many applications such asemergency services, autonomous driving and geographic routing. Although thereare many available positioning techniques, none of them is adapted to work withmassive multiple-in-multiple-out (MIMO) systems, which represent a leading 5Gtechnology candidate. In this paper, we discuss possible solutions forpositioning of mobile stations using a vector of signals at the base station,equipped with many antennas distributed over deployment area. Our main proposalis to use fingerprinting techniques based on a vector of received signalstrengths. This kind of methods are able to work in highly-cluttered multipathenvironments, and require just one base station, in contrast to standardrange-based and angle-based techniques. We also provide a solution forfingerprinting-based positioning based on Gaussian process regression, anddiscuss main applications and challenges.
arxiv-12000-4 | Fast rates in statistical and online learning | http://arxiv.org/pdf/1507.02592v2.pdf | author:Tim van Erven, Peter D. Gr√ºnwald, Nishant A. Mehta, Mark D. Reid, Robert C. Williamson category:cs.LG stat.ML published:2015-07-09 summary:The speed with which a learning algorithm converges as it is presented withmore data is a central problem in machine learning --- a fast rate ofconvergence means less data is needed for the same level of performance. Thepursuit of fast rates in online and statistical learning has led to thediscovery of many conditions in learning theory under which fast learning ispossible. We show that most of these conditions are special cases of a single,unifying condition, that comes in two forms: the central condition for 'proper'learning algorithms that always output a hypothesis in the given model, andstochastic mixability for online algorithms that may make predictions outsideof the model. We show that under surprisingly weak assumptions both conditionsare, in a certain sense, equivalent. The central condition has are-interpretation in terms of convexity of a set of pseudoprobabilities,linking it to density estimation under misspecification. For bounded losses, weshow how the central condition enables a direct proof of fast rates and weprove its equivalence to the Bernstein condition, itself a generalization ofthe Tsybakov margin condition, both of which have played a central role inobtaining fast rates in statistical learning. Yet, while the Bernsteincondition is two-sided, the central condition is one-sided, making it moresuitable to deal with unbounded losses. In its stochastic mixability form, ourcondition generalizes both a stochastic exp-concavity condition identified byJuditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifyingconditions thus provide a substantial step towards a characterization of fastrates in statistical learning, similar to how classical mixabilitycharacterizes constant regret in the sequential prediction with expert advicesetting.
arxiv-12000-5 | Discovery of Web Usage Profiles Using Various Clustering Techniques | http://arxiv.org/pdf/1509.00692v1.pdf | author:Zahid Ansari, Waseem Ahmed, M. F. Azeem, A. Vinaya Babu category:cs.DB cs.IR cs.LG published:2015-09-01 summary:The explosive growth of World Wide Web (WWW) has necessitated the developmentof Web personalization systems in order to understand the user preferences todynamically serve customized content to individual users. To reveal informationabout user preferences from Web usage data, Web Usage Mining (WUM) techniquesare extensively being applied to the Web log data. Clustering techniques arewidely used in WUM to capture similar interests and trends among usersaccessing a Web site. Clustering aims to divide a data set into groups orclusters where inter-cluster similarities are minimized while the intra clustersimilarities are maximized. This paper reviews four of the popularly usedclustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniquesare implemented and tested against the Web user navigational data. Performanceand validity results of each technique are presented and compared.
arxiv-12000-6 | Remote sensing image classification exploiting multiple kernel learning | http://arxiv.org/pdf/1410.5358v3.pdf | author:Claudio Cusano, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2014-10-20 summary:We propose a strategy for land use classification which exploits MultipleKernel Learning (MKL) to automatically determine a suitable combination of aset of features without requiring any heuristic knowledge about theclassification task. We present a novel procedure that allows MKL to achievegood performance in the case of small training sets. Experimental results onpublicly available datasets demonstrate the feasibility of the proposedapproach.
arxiv-12000-7 | A Telescopic Binary Learning Machine for Training Neural Networks | http://arxiv.org/pdf/1509.00174v1.pdf | author:Mauro Brunato, Roberto Battiti category:cs.NE I.2.6 published:2015-09-01 summary:This paper proposes a new algorithm based on multi-scale stochastic localsearch with binary representation for training neural networks. In particular, we study the effects of neighborhood evaluation strategies,the effect of the number of bits per weight and that of the maximum weightrange used for mapping binary strings to real values. Following thispreliminary investigation, we propose a telescopic multi-scale version of localsearch where the number of bits is increased in an adaptive manner, leading toa faster search and to local minima of better quality. An analysis related toadapting the number of bits in a dynamic way is also presented. The control onthe number of bits, which happens in a natural manner in the proposed method,is effective to increase the generalization performance. Benchmark tasksinclude a highly non-linear artificial problem, a control problem requiringeither feed-forward or recurrent architectures for feedback control, andchallenging real-world tasks in different application domains. The results demonstrate the effectiveness of the proposed method.
arxiv-12000-8 | Low-Rank Approximation and Completion of Positive Tensors | http://arxiv.org/pdf/1412.0620v4.pdf | author:Anil Aswani category:math.ST cs.LG stat.TH published:2014-12-01 summary:Unlike the matrix case, computing low-rank approximations of tensors isNP-hard and numerically ill-posed in general. Even the best rank-1approximation of a tensor is NP-hard. In this paper, we use convex optimizationto develop polynomial-time algorithms for low-rank approximation and completionof positive tensors. Our approach is to use algebraic topology to define a new(numerically well-posed) decomposition for positive tensors, which we show isequivalent to the standard tensor decomposition in important cases. Thoughcomputing this decomposition is a nonconvex optimization problem, we prove itcan be exactly reformulated as a convex optimization problem. This allows us toconstruct polynomial-time randomized algorithms for computing thisdecomposition and for solving low-rank tensor approximation problems. Among theconsequences is that best rank-1 approximations of positive tensors can becomputed in polynomial time. Our framework is next extended to the tensorcompletion problem, where noisy entries of a tensor are observed and then usedto estimate missing entries. We provide a polynomial-time algorithm thatrequires a polynomial (in tensor order) number of measurements, in contrast toexisting approaches that require an exponential number of measurements forspecific cases. These algorithms are extended to exploit sparsity in the tensorto reduce the number of measurements needed. We conclude by providing a novelinterpretation of statistical regression problems with categorical variables astensor completion problems, and numerical examples with synthetic data and datafrom a bioengineered metabolic network show the improved performance of ourapproach on this problem.
arxiv-12000-9 | Online Supervised Subspace Tracking | http://arxiv.org/pdf/1509.00137v1.pdf | author:Yao Xie, Ruiyang Song, Hanjun Dai, Qingbin Li, Le Song category:cs.LG math.ST stat.ML stat.TH published:2015-09-01 summary:We present a framework for supervised subspace tracking, when there are twotime series $x_t$ and $y_t$, one being the high-dimensional predictors and theother being the response variables and the subspace tracking needs to take intoconsideration of both sequences. It extends the classic online subspacetracking work which can be viewed as tracking of $x_t$ only. Our onlinesufficient dimensionality reduction (OSDR) is a meta-algorithm that can beapplied to various cases including linear regression, logistic regression,multiple linear regression, multinomial logistic regression, support vectormachine, the random dot product model and the multi-scale union-of-subspacemodel. OSDR reduces data-dimensionality on-the-fly with low-computationalcomplexity and it can also handle missing data and dynamic data. OSDR uses analternating minimization scheme and updates the subspace via gradient descenton the Grassmannian manifold. The subspace update can be performed efficientlyutilizing the fact that the Grassmannian gradient with respect to the subspacein many settings is rank-one (or low-rank in certain cases). The optimizationproblem for OSDR is non-convex and hard to analyze in general; we provideconvergence analysis of OSDR in a simple linear regression setting. The goodperformance of OSDR compared with the conventional unsupervised subspacetracking are demonstrated via numerical examples on simulated and real data.
arxiv-12000-10 | Sequential Information Guided Sensing | http://arxiv.org/pdf/1509.00130v1.pdf | author:Ruiyang Song, Yao Xie, Sebastian Pokutta category:cs.IT math.IT math.ST stat.ML stat.TH published:2015-09-01 summary:We study the value of information in sequential compressed sensing bycharacterizing the performance of sequential information guided sensing inpractical scenarios when information is inaccurate. In particular, we assumethe signal distribution is parameterized through Gaussian or Gaussian mixtureswith estimated mean and covariance matrices, and we can measure compressivelythrough a noisy linear projection or using one-sparse vectors, i.e., observingone entry of the signal each time. We establish a set of performance bounds forthe bias and variance of the signal estimator via posterior mean, by capturingthe conditional entropy (which is also related to the size of the uncertainty),and the additional power required due to inaccurate information to reach adesired precision. Based on this, we further study how to estimate covariancebased on direct samples or covariance sketching. Numerical examples alsodemonstrate the superior performance of Info-Greedy Sensing algorithms comparedwith their random and non-adaptive counterparts.
arxiv-12000-11 | Group Component Analysis for Multi-block Data: Common and Individual Feature Extraction | http://arxiv.org/pdf/1212.3913v3.pdf | author:Guoxu Zhou, Andrzej Cichocki, Yu Zhang, Danilo Mandic category:cs.CV cs.LG published:2012-12-17 summary:Very often data we encounter in practice is a collection of matrices ratherthan a single matrix. These multi-block data are naturally linked and henceoften share some common features and at the same time they have their ownindividual features, due to the background in which they are measured andcollected. In this study we proposed a new scheme of common and individualfeature analysis (CIFA) that processes multi-block data in a linked way aimingat discovering and separating their common and individual features. Accordingto whether the number of common features is given or not, two efficientalgorithms were proposed to extract the common basis which is shared by alldata. Then feature extraction is performed on the common and the individualspaces separately by incorporating the techniques such as dimensionalityreduction and blind source separation. We also discussed how the proposed CIFAcan significantly improve the performance of classification and clusteringtasks by exploiting common and individual features of samples respectively. Ourexperimental results show some encouraging features of the proposed methods incomparison to the state-of-the-art methods on synthetic and real data.
arxiv-12000-12 | Evolving Unipolar Memristor Spiking Neural Networks | http://arxiv.org/pdf/1509.00105v1.pdf | author:David Howard, Larry Bull, Ben De Lacy Costello category:cs.NE published:2015-09-01 summary:Neuromorphic computing --- brainlike computing in hardware --- typicallyrequires myriad CMOS spiking neurons interconnected by a dense mesh ofnanoscale plastic synapses. Memristors are frequently citepd as strong synapsecandidates due to their statefulness and potential for low-powerimplementations. To date, plentiful research has focused on the bipolarmemristor synapse, which is capable of incremental weight alterations and canprovide adaptive self-organisation under a Hebbian learning scheme. In thispaper we consider the Unipolar memristor synapse --- a device capable ofnon-Hebbian switching between only two states (conductive and resistive)through application of a suitable input voltage --- and discuss its suitabilityfor neuromorphic systems. A self-adaptive evolutionary process is used toautonomously find highly fit network configurations. Experimentation on a tworobotics tasks shows that unipolar memristor networks evolve task-solvingcontrollers faster than both bipolar memristor networks and networks containingconstant nonplastic connections whilst performing at least comparably.
arxiv-12000-13 | De-biasing the Lasso: Optimal Sample Size for Gaussian Designs | http://arxiv.org/pdf/1508.02757v2.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST stat.ML stat.TH published:2015-08-11 summary:Performing statistical inference in high-dimensional models is an outstandingchallenge. A major source of difficulty is the absence of precise informationon the distribution of high-dimensional regularized estimators. Here, we consider linear regression in the high-dimensional regime $p\gg n$and the Lasso estimator. In this context, we would like to perform inference ona high-dimensional parameters vector $\theta^*\in R^p$. Important progress hasbeen achieved in computing confidence intervals and p-values for singlecoordinates $\theta^*_i$, $i\in \{1,\dots,p\}$. A key role in these newinferential methods is played by a certain de-biased (or de-sparsified)estimator $\widehat{\theta}^d$ that is constructed from the Lasso estimator.Earlier work establishes that, under suitable assumptions on the design matrix,the coordinates of $\widehat{\theta}^d$ are asymptotically Gaussian providedthe true parameters vector $\theta^*$ is $s_0$-sparse with $s_0 =o(\sqrt{n}/\log p )$. The condition $s_0 = o(\sqrt{n}/ \log p )$ is considerably stronger than theone required for consistent estimation, namely $s_0 = o(n/ \log p )$. Here weconsider Gaussian designs with known or unknown population covariance. When thecovariance is known, we prove that the de-biased estimator is asymptoticallyGaussian under the nearly optimal condition $s_0 = o(n/ (\log p)^2)$. Note that\emph{earlier work was limited to $s_0 = o(\sqrt{n}/ \log p)$ even forperfectly known covariance.} The same conclusion holds if the population covariance is unknown but can beestimated sufficiently well, e.g. because its inverse is very sparse. Forintermediate regimes, we describe the trade-off between sparsity in thecoefficients $\theta^*$, and sparsity in the inverse covariance of the design.
arxiv-12000-14 | Metastatic liver tumour segmentation from discriminant Grassmannian manifolds | http://arxiv.org/pdf/1509.00083v1.pdf | author:Samuel Kadoury, Eugene Vorontsov, An Tang category:cs.LG cs.CV published:2015-08-31 summary:The early detection, diagnosis and monitoring of liver cancer progression canbe achieved with the precise delineation of metastatic tumours. However,accurate automated segmentation remains challenging due to the presence ofnoise, inhomogeneity and the high appearance variability of malignant tissue.In this paper, we propose an unsupervised metastatic liver tumour segmentationframework using a machine learning approach based on discriminant Grassmannianmanifolds which learns the appearance of tumours with respect to normal tissue.First, the framework learns within-class and between-class similaritydistributions from a training set of images to discover the optimal manifolddiscrimination between normal and pathological tissue in the liver. Second, aconditional optimisation scheme computes nonlocal pairwise as well aspattern-based clique potentials from the manifold subspace to recognise regionswith similar labelings and to incorporate global consistency in thesegmentation process. The proposed framework was validated on a clinicaldatabase of 43 CT images from patients with metastatic liver cancer. Comparedto state-of-the-art methods, our method achieves a better performance on twoseparate datasets of metastatic liver tumours from different clinical sites,yielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50tumours with an average volume of 27.3 mm3.
arxiv-12000-15 | Value function approximation via low-rank models | http://arxiv.org/pdf/1509.00061v1.pdf | author:Hao Yi Ong category:cs.LG cs.AI published:2015-08-31 summary:We propose a novel value function approximation technique for Markov decisionprocesses. We consider the problem of compactly representing the state-actionvalue function using a low-rank and sparse matrix model. The problem is todecompose a matrix that encodes the true value function into low-rank andsparse components, and we achieve this using Robust Principal ComponentAnalysis (PCA). Under minimal assumptions, this Robust PCA problem can besolved exactly via the Principal Component Pursuit convex optimization problem.We experiment the procedure on several examples and demonstrate that our methodyields approximations essentially identical to the true function.
arxiv-12000-16 | Pure and Hybrid Evolutionary Computing in Global Optimization of Chemical Structures: from Atoms and Molecules to Clusters and Crystals | http://arxiv.org/pdf/1509.00028v1.pdf | author:Kanchan Sarkar, S. P. Bhattacharyya category:cs.NE published:2015-08-31 summary:The growth of evolutionary computing (EC) methods in the exploration ofcomplex potential energy landscapes of atomic and molecular clusters, as wellas crystals over the last decade or so is reviewed. The trend of growthindicates that pure as well as hybrid evolutionary computing techniques inconjunction of DFT has been emerging as a powerful tool, although work onmolecular clusters has been rather limited so far. Some attempts to solve theatomic/molecular Schrodinger Equation (SE) directly by genetic algorithms (GA)are available in literature. At the Born-Oppenheimer level of approximationGA-density methods appear to be a viable tool which could be more extensivelyexplored in the coming years, specially in the context of designing moleculesand materials with targeted properties.
arxiv-12000-17 | Approximate Nearest Neighbor Fields in Video | http://arxiv.org/pdf/1508.07953v1.pdf | author:Nir Ben-Zrihem, Lihi Zelnik-Manor category:cs.CV published:2015-08-31 summary:We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search),an algorithm for matching patches of a video to a set of reference patches inreal-time. For each query, RIANN finds potential matches by intersecting ringsaround key points in appearance space. Its search complexity is reverselycorrelated to the amount of temporal change, making it a good fit for videos,where typically most patches change slowly with time. Experiments show thatRIANN is up to two orders of magnitude faster than previous ANN methods, and isthe only solution that operates in real-time. We further demonstrate how RIANNcan be used for real-time video processing and provide examples for a range ofreal-time video applications, including colorization, denoising, and severalartistic effects.
arxiv-12000-18 | Relax but stay in control: from value to algorithms for online Markov decision processes | http://arxiv.org/pdf/1310.7300v2.pdf | author:Peng Guan, Maxim Raginsky, Rebecca Willett category:cs.LG math.OC stat.ML published:2013-10-28 summary:Online learning algorithms are designed to perform in non-stationaryenvironments, but generally there is no notion of a dynamic state to modelconstraints on current and future actions as a function of past actions.State-based models are common in stochastic control settings, but commonly usedframeworks such as Markov Decision Processes (MDPs) assume a known stationaryenvironment. In recent years, there has been a growing interest in combiningthe above two frameworks and considering an MDP setting in which the costfunction is allowed to change arbitrarily after each time step. However, mostof the work in this area has been algorithmic: given a problem, one woulddevelop an algorithm almost from scratch. Moreover, the presence of the stateand the assumption of an arbitrarily varying environment complicate both thetheoretical analysis and the development of computationally efficient methods.This paper describes a broad extension of the ideas proposed by Rakhlin et al.to give a general framework for deriving algorithms in an MDP setting witharbitrarily changing costs. This framework leads to a unifying view of existingmethods and provides a general procedure for constructing new ones. Several newmethods are presented, and one of them is shown to have important advantagesover a similar method developed from scratch via an online version ofapproximate dynamic programming.
arxiv-12000-19 | Decentralized Online Optimization with Global Objectives and Local Communication | http://arxiv.org/pdf/1508.07933v1.pdf | author:Soomin Lee, Angelia Nediƒá, Maxim Raginsky category:math.OC cs.LG cs.SY published:2015-08-31 summary:We consider a decentralized online convex optimization problem in a networkof agents, where each agent controls only a coordinate (or a part) of theglobal decision vector. For such a problem, we propose two decentralizedvariants (ODA-LM and OPDA-TV) of Nesterov's primal-dual algorithm with dualaveraging. In ODA-LM, to mitigate the disagreements on the primal-vectorupdates, the agents implement a generalization of the localinformation-exchange dynamics recently proposed by Li and Marden over a staticundirected graph. In OPDA-TV, the agents implement the broadcast-based push-sumdynamics over a time-varying sequence of uniformly connected digraphs. We showthat the regret bounds in both cases have sublinear growth of $O(\sqrt{T})$,with the time horizon $T$, when the stepsize is of the form $1/\sqrt{t}$ andthe objective functions are Lipschitz-continuous convex functions withLipschitz gradients. We also implement the proposed algorithms on a sensornetwork to complement our theoretical analysis.
arxiv-12000-20 | Multi-Projector Color Structured-Light Vision | http://arxiv.org/pdf/1508.07859v1.pdf | author:Changsoo Je, Kwang Hee Lee, Sang Wook Lee category:cs.CV cs.GR physics.optics I.2.10; I.4.8 published:2015-08-31 summary:Research interest in rapid structured-light imaging has grown increasinglyfor the modeling of moving objects, and a number of methods have been suggestedfor the range capture in a single video frame. The imaging area of a 3D objectusing a single projector is restricted since the structured light is projectedonly onto a limited area of the object surface. Employing additional projectorsto broaden the imaging area is a challenging problem since simultaneousprojection of multiple patterns results in their superposition in thelight-intersected areas and the recognition of original patterns is by no meanstrivial. This paper presents a novel method of multi-projector colorstructured-light vision based on projector-camera triangulation. By analyzingthe behavior of superposed-light colors in a chromaticity domain, we show thatthe original light colors cannot be properly extracted by the conventionaldirect estimation. We disambiguate multiple projectors by multiplexing theorientations of projector patterns so that the superposed patterns can beseparated by explicit derivative computations. Experimental studies are carriedout to demonstrate the validity of the presented method. The proposed methodincreases the efficiency of range acquisition compared to conventional activestereo using multiple projectors.
arxiv-12000-21 | Scalable MCMC for Large Data Problems using Data Subsampling and the Difference Estimator | http://arxiv.org/pdf/1507.02971v2.pdf | author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.ME stat.CO stat.ML published:2015-07-10 summary:We propose a generic Markov Chain Monte Carlo (MCMC) algorithm to speed upcomputations for datasets with many observations. A key feature of our approachis the use of the highly efficient difference estimator from the surveysampling literature to estimate the log-likelihood accurately using only asmall fraction of the data. Our algorithm improves on the $O(n)$ complexity ofregular MCMC by operating over local data clusters instead of the full samplewhen computing the likelihood. The likelihood estimate is used in aPseudo-marginal framework to sample from a perturbed posterior which is within$O(m^{-1/2})$ of the true posterior, where $m$ is the subsample size. Themethod is applied to a logistic regression model to predict firm bankruptcy fora large data set. We document a significant speed up in comparison to thestandard MCMC on the full dataset.
arxiv-12000-22 | Bayesian Networks for Variable Groups | http://arxiv.org/pdf/1508.07753v1.pdf | author:Pekka Parviainen, Samuel Kaski category:stat.ML cs.AI published:2015-08-31 summary:Bayesian networks, and especially their structures, are powerful tools forrepresenting conditional independencies and dependencies between randomvariables. In applications where related variables form a priori known groups,chosen to represent different "views" to or aspects of the same entities, onemay be more interested in modeling dependencies between groups of variablesrather than between individual variables. Motivated by this, we study prospectsof representing relationships between variable groups using Bayesian networkstructures. We show that for dependency structures between groups to belearnable, the data have to satisfy the so-called groupwise faithfulnessassumption. We also show that one cannot learn causal relations between groupsusing only groupwise conditional independencies, but also variable-wiserelations are needed. Additionally, we present algorithms for finding thegroupwise dependency structures.
arxiv-12000-23 | Model Guided Sampling Optimization for Low-dimensional Problems | http://arxiv.org/pdf/1508.07741v1.pdf | author:Lukas Bajer, Martin Holena category:cs.NE stat.ML published:2015-08-31 summary:Optimization of very expensive black-box functions requires utilization ofmaximum information gathered by the process of optimization. Model GuidedSampling Optimization (MGSO) forms a more robust alternative to Jones'Gaussian-process-based EGO algorithm. Instead of EGO's maximizing expectedimprovement, the MGSO uses sampling the probability of improvement which isshown to be helpful against trapping in local minima. Further, the MGSO canreach close-to-optimum solutions faster than standard optimization algorithmson low dimensional or smooth problems.
arxiv-12000-24 | A Cognitive Architecture Based on a Learning Classifier System with Spiking Classifiers | http://arxiv.org/pdf/1508.07700v1.pdf | author:David Howard, Larry Bull, Pier-Luca Lanzi category:cs.NE published:2015-08-31 summary:Learning Classifier Systems (LCS) are population-based reinforcement learnersthat were originally designed to model various cognitive phenomena. This paperpresents an explicitly cognitive LCS by using spiking neural networks asclassifiers, providing each classifier with a measure of temporal dynamism. Weemploy a constructivist model of growth of both neurons and synapticconnections, which permits a Genetic Algorithm (GA) to automatically evolvesufficiently-complex neural structures. The spiking classifiers are coupledwith a temporally-sensitive reinforcement learning algorithm, which allows thesystem to perform temporal state decomposition by appropriately rewarding"macro-actions," created by chaining together multiple atomic actions. Thecombination of temporal reinforcement learning and neural informationprocessing is shown to outperform benchmark neural classifier systems, andsuccessfully solve a robotic navigation task.
arxiv-12000-25 | Domain Generalization for Object Recognition with Multi-task Autoencoders | http://arxiv.org/pdf/1508.07680v1.pdf | author:Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi category:cs.CV published:2015-08-31 summary:The problem of domain generalization is to take knowledge acquired from anumber of related domains where training data is available, and to thensuccessfully apply it to previously unseen domains. We propose a new featurelearning algorithm, Multi-Task Autoencoder (MTAE), that provides goodgeneralization performance for cross-domain object recognition. Our algorithm extends the standard denoising autoencoder framework bysubstituting artificially induced corruption with naturally occurringinter-domain variability in the appearance of objects. Instead ofreconstructing images from noisy versions, MTAE learns to transform theoriginal image into analogs in multiple related domains. It thereby learnsfeatures that are robust to variations across domains. The learnt features arethen used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognitiondatasets, where the task is to learn features from multiple datasets and tothen predict the image label from unseen datasets. We found that (denoising)MTAE outperforms alternative autoencoder-based models as well as the currentstate-of-the-art algorithms for domain generalization.
arxiv-12000-26 | Online Model Evaluation in a Large-Scale Computational Advertising Platform | http://arxiv.org/pdf/1508.07678v1.pdf | author:Shahriar Shariat, Burkay Orten, Ali Dasdan category:cs.AI stat.ME stat.ML published:2015-08-31 summary:Online media provides opportunities for marketers through which they candeliver effective brand messages to a wide range of audiences. Advertisingtechnology platforms enable advertisers to reach their target audience bydelivering ad impressions to online users in real time. In order to identifythe best marketing message for a user and to purchase impressions at the rightprice, we rely heavily on bid prediction and optimization models. Even thoughthe bid prediction models are well studied in the literature, the equallyimportant subject of model evaluation is usually overlooked. Effective andreliable evaluation of an online bidding model is crucial for making fastermodel improvements as well as for utilizing the marketing budgets moreefficiently. In this paper, we present an experimentation framework for bidprediction models where our focus is on the practical aspects of modelevaluation. Specifically, we outline the unique challenges we encounter in ourplatform due to a variety of factors such as heterogeneous goal definitions,varying budget requirements across different campaigns, high seasonality andthe auction-based environment for inventory purchasing. Then, we introducereturn on investment (ROI) as a unified model performance (i.e., success)metric and explain its merits over more traditional metrics such asclick-through rate (CTR) or conversion rate (CVR). Most importantly, we discusscommonly used evaluation and metric summarization approaches in detail andpropose a more accurate method for online evaluation of new experimental modelsagainst the baseline. Our meta-analysis-based approach addresses variousshortcomings of other methods and yields statistically robust conclusions thatallow us to conclude experiments more quickly in a reliable manner. Wedemonstrate the effectiveness of our evaluation strategy on real campaign datathrough some experiments.
arxiv-12000-27 | Action Recognition by Hierarchical Mid-level Action Elements | http://arxiv.org/pdf/1508.07654v1.pdf | author:Tian Lan, Yuke Zhu, Amir Roshan Zamir, Silvio Savarese category:cs.CV published:2015-08-31 summary:Realistic videos of human actions exhibit rich spatiotemporal structures atmultiple levels of granularity: an action can always be decomposed intomultiple finer-grained elements in both space and time. To capture thisintuition, we propose to represent videos by a hierarchy of mid-level actionelements (MAEs), where each MAE corresponds to an action-related spatiotemporalsegment in the video. We introduce an unsupervised method to generate thisrepresentation from videos. Our method is capable of distinguishingaction-related segments from background segments and representing actions atmultiple spatiotemporal resolutions. Given a set of spatiotemporal segmentsgenerated from the training data, we introduce a discriminative clusteringalgorithm that automatically discovers MAEs at multiple levels of granularity.We develop structured models that capture a rich set of spatial, temporal andhierarchical relations among the segments, where the action label and multiplelevels of MAE labels are jointly inferred. The proposed model achievesstate-of-the-art performance in multiple action recognition benchmarks.Moreover, we demonstrate the effectiveness of our model in real-worldapplications such as action recognition in large-scale untrimmed videos andaction parsing.
arxiv-12000-28 | Dictionary Learning for Blind One Bit Compressed Sensing | http://arxiv.org/pdf/1508.07648v1.pdf | author:Hadi Zayyani, Mehdi Korki, Farrokh Marvasti category:stat.ML cs.IT math.IT published:2015-08-30 summary:This letter proposes a dictionary learning algorithm for blind one bitcompressed sensing. In the blind one bit compressed sensing framework, theoriginal signal to be reconstructed from one bit linear random measurements issparse in an unknown domain. In this context, the multiplication of measurementmatrix $\Ab$ and sparse domain matrix $\Phi$, \ie $\Db=\Ab\Phi$, should belearned. Hence, we use dictionary learning to train this matrix. Towards thatend, an appropriate continuous convex cost function is suggested for one bitcompressed sensing and a simple steepest-descent method is exploited to learnthe rows of the matrix $\Db$. Experimental results show the effectiveness ofthe proposed algorithm against the case of no dictionary learning, speciallywith increasing the number of training signals and the number of signmeasurements.
arxiv-12000-29 | Stabilized Nearest Neighbor Classifier and Its Statistical Properties | http://arxiv.org/pdf/1405.6642v2.pdf | author:Wei Sun, Xingye Qiao, Guang Cheng category:stat.ML cs.LG published:2014-05-26 summary:The stability of statistical analysis is an important indicator forreproducibility, which is one main principle of scientific method. It entailsthat similar statistical conclusions can be reached based on independentsamples from the same underlying population. In this paper, we introduce ageneral measure of classification instability (CIS) to quantify the samplingvariability of the prediction made by a classification method. Interestingly,the asymptotic CIS of any weighted nearest neighbor classifier turns out to beproportional to the Euclidean norm of its weight vector. Based on this conciseform, we propose a stabilized nearest neighbor (SNN) classifier, whichdistinguishes itself from other nearest neighbor classifiers, by taking thestability into consideration. In theory, we prove that SNN attains the minimaxoptimal convergence rate in risk, and a sharp convergence rate in CIS. Thelatter rate result is established for general plug-in classifiers under alow-noise condition. Extensive simulated and real examples demonstrate that SNNachieves a considerable improvement in CIS over existing nearest neighborclassifiers, with comparable classification accuracy. We implement thealgorithm in a publicly available R package snn.
arxiv-12000-30 | Maximum Entropy Discrimination Denoising Autoencoders | http://arxiv.org/pdf/1402.3427v2.pdf | author:Sotirios P. Chatzis category:cs.LG published:2014-02-14 summary:Denoising autoencoders (DAs) are typically applied to relatively largedatasets for unsupervised learning of representative data encodings, they relyon the idea of making the learned representations robust to partial corruptionof the input pattern, and perform learning using stochastic gradient descentwith relatively large datasets. In this paper, we present a fully Bayesian DAarchitecture that allows for the application of DAs even when data is scarce.Our novel approach formulates the signal encoding problem under a nonparametricBayesian regard, considering a Gaussian process prior over the latent inputencodings generated given the (corrupt) input observations. Subsequently, thedecoder modules of our model are formulated as large-margin regression models,treated under the Bayesian inference paradigm, by exploiting the maximumentropy discrimination (MED) framework. We exhibit the effectiveness of ourapproach using several datasets, dealing with both classification and transferlearning applications.
arxiv-12000-31 | An Event Network for Exploring Open Information | http://arxiv.org/pdf/1508.07555v1.pdf | author:Yanping Chen category:cs.CL published:2015-08-30 summary:In this paper, an event network is presented for exploring open information,where linguistic units about an event are organized for analysing. The processis divided into three steps: document event detection, event networkconstruction and event network analysis. First, by implementing event detectionor tracking, documents are retrospectively (or on-line) organized into documentevents. Secondly, for each of the document event, linguistic units areextracted and combined into event networks. Thirdly, various analytic methodsare proposed for event network analysis. In our application methodologies arepresented for exploring open information.
arxiv-12000-32 | X-TREPAN: a multi class regression and adapted extraction of comprehensible decision tree in artificial neural networks | http://arxiv.org/pdf/1508.07551v1.pdf | author:Awudu Karim, Shangbo Zhou category:cs.LG cs.NE published:2015-08-30 summary:In this work, the TREPAN algorithm is enhanced and extended for extractingdecision trees from neural networks. We empirically evaluated the performanceof the algorithm on a set of databases from real world events. This benchmarkenhancement was achieved by adapting Single-test TREPAN and C4.5 decision treeinduction algorithms to analyze the datasets. The models are then compared withX-TREPAN for comprehensibility and classification accuracy. Furthermore, wevalidate the experimentations by applying statistical methods. Finally, themodified algorithm is extended to work with multi-class regression problems andthe ability to comprehend generalized feed forward networks is achieved.
arxiv-12000-33 | Calibration of One-Class SVM for MV set estimation | http://arxiv.org/pdf/1508.07535v1.pdf | author:Albert Thomas, Vincent Feuillard, Alexandre Gramfort category:stat.ML published:2015-08-30 summary:A general approach for anomaly detection or novelty detection consists inestimating high density regions or Minimum Volume (MV) sets. The One-ClassSupport Vector Machine (OCSVM) is a state-of-the-art algorithm for estimatingsuch regions from high dimensional data. Yet it suffers from practicallimitations. When applied to a limited number of samples it can lead to poorperformance even when picking the best hyperparameters. Moreover the solutionof OCSVM is very sensitive to the selection of hyperparameters which makes ithard to optimize in an unsupervised setting. We present a new approach toestimate MV sets using the OCSVM with a different choice of the parametercontrolling the proportion of outliers. The solution function of the OCSVM islearnt on a training set and the desired probability mass is obtained byadjusting the offset on a test set to prevent overfitting. Models learnt ondifferent train/test splits are then aggregated to reduce the variance inducedby such random splits. Our approach makes it possible to tune thehyperparameters automatically and obtain nested set estimates. Experimentalresults show that our approach outperforms the standard OCSVM formulation whilesuffering less from the curse of dimensionality than kernel density estimates.Results on actual data sets are also presented.
arxiv-12000-34 | Staged Multi-armed Bandits | http://arxiv.org/pdf/1508.00641v2.pdf | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2015-08-04 summary:In conventional multi-armed bandits (MAB) and other reinforcement learningmethods, the learner sequentially chooses actions and obtains a reward (whichcan be possibly missing, delayed or erroneous) after each taken action. Thisreward is then used by the learner to improve its future decisions. However, innumerous applications, ranging from personalized patient treatment topersonalized web-based education, the learner does not obtain rewards aftereach action, but only after sequences of actions are taken, intermediatefeedbacks are observed, and a final decision is made based on which a reward isobtained. In this paper, we introduce a new class of reinforcement learningmethods which can operate in such settings. We refer to this class as stagedmulti-armed bandits (S-MAB). S-MAB proceeds in rounds, each composed of severalstages; in each stage, the learner chooses an action and observes a feedbacksignal. Upon each action selection a feedback signal is observed, whilst thereward of the selected sequence of actions is only revealed after the learnerselects a stop action that ends the current round. The reward of the rounddepends both on the sequence of actions and the sequence of observed feedbacks.The goal of the learner is to maximize its total expected reward over allrounds by learning to choose the best sequence of actions based on the feedbackit gets about these actions. First, we define an oracle benchmark, whichsequentially selects the actions that maximize the expected immediate reward.This benchmark is known to be approximately optimal when the reward sequenceassociated with the selected actions is adaptive submodular. Then, we proposeour online learning algorithm, for which we prove that the regret islogarithmic in the number of rounds and linear in the number of stages withrespect to the oracle benchmark.
arxiv-12000-35 | Equilibrated adaptive learning rates for non-convex optimization | http://arxiv.org/pdf/1502.04390v2.pdf | author:Yann N. Dauphin, Harm de Vries, Yoshua Bengio category:cs.LG cs.NA published:2015-02-15 summary:Parameter-specific adaptive learning rate methods are computationallyefficient ways to reduce the ill-conditioning problems encountered whentraining large deep networks. Following recent work that strongly suggests thatmost of the critical points encountered when training such networks are saddlepoints, we find how considering the presence of negative eigenvalues of theHessian could help us design better suited adaptive learning rate schemes. Weshow that the popular Jacobi preconditioner has undesirable behavior in thepresence of both positive and negative curvature, and present theoretical andempirical evidence that the so-called equilibration preconditioner iscomparatively better suited to non-convex problems. We introduce a noveladaptive learning rate scheme, called ESGD, based on the equilibrationpreconditioner. Our experiments show that ESGD performs as well or better thanRMSProp in terms of convergence speed, always clearly improving over plainstochastic gradient descent.
arxiv-12000-36 | Parameter estimation in softmax decision-making models with linear objective functions | http://arxiv.org/pdf/1502.04635v2.pdf | author:Paul Reverdy, Naomi E. Leonard category:math.OC cs.LG stat.ML 93E10 published:2015-02-16 summary:With an eye towards human-centered automation, we contribute to thedevelopment of a systematic means to infer features of human decision-makingfrom behavioral data. Motivated by the common use of softmax selection inmodels of human decision-making, we study the maximum likelihood parameterestimation problem for softmax decision-making models with linear objectivefunctions. We present conditions under which the likelihood function is convex.These allow us to provide sufficient conditions for convergence of theresulting maximum likelihood estimator and to construct its asymptoticdistribution. In the case of models with nonlinear objective functions, we showhow the estimator can be applied by linearizing about a nominal parametervalue. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)model of human decision-making to human subject data. We show statisticallysignificant differences in behavior across related, but distinct, tasks.
arxiv-12000-37 | Embedding Entities and Relations for Learning and Inference in Knowledge Bases | http://arxiv.org/pdf/1412.6575v4.pdf | author:Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng category:cs.CL published:2014-12-20 summary:We consider learning representations of entities and relations in KBs usingthe neural-embedding approach. We show that most existing models, including NTN(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalizedunder a unified learning framework, where entities are low-dimensional vectorslearned from a neural network and relations are bilinear and/or linear mappingfunctions. Under this framework, we compare a variety of embedding models onthe link prediction task. We show that a simple bilinear formulation achievesnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approachthat utilizes the learned relation embeddings to mine logical rules such as"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)". We find thatembeddings learned from the bilinear objective are particularly good atcapturing relational semantics and that the composition of relations ischaracterized by matrix multiplication. More interestingly, we demonstrate thatour embedding-based rule extraction approach successfully outperforms astate-of-the-art confidence-based rule mining approach in mining Horn rulesthat involve compositional reasoning.
arxiv-12000-38 | Multimodal Convolutional Neural Networks for Matching Image and Sentence | http://arxiv.org/pdf/1504.06063v5.pdf | author:Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li category:cs.CV cs.CL cs.NE published:2015-04-23 summary:In this paper, we propose multimodal convolutional neural networks (m-CNNs)for matching image and sentence. Our m-CNN provides an end-to-end frameworkwith convolutional architectures to exploit image representation, wordcomposition, and the matching relations between the two modalities. Morespecifically, it consists of one image CNN encoding the image content, and onematching CNN learning the joint representation of image and sentence. Thematching CNN composes words to different semantic fragments and learns theinter-modal relations between image and the composed fragments at differentlevels, thus fully exploit the matching relations between image and sentence.Experimental results on benchmark databases of bidirectional image and sentenceretrieval demonstrate that the proposed m-CNNs can effectively capture theinformation necessary for image and sentence matching. Specifically, ourproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K andMicrosoft COCO databases achieve the state-of-the-art performances.
arxiv-12000-39 | Linked Component Analysis from Matrices to High Order Tensors: Applications to Biomedical Data | http://arxiv.org/pdf/1508.07416v1.pdf | author:Guoxu Zhou, Qibin Zhao, Yu Zhang, T√ºlay Adalƒ±, Shengli Xie, Andrzej Cichocki category:cs.CE cs.LG cs.NA published:2015-08-29 summary:With the increasing availability of various sensor technologies, we now haveaccess to large amounts of multi-block (also called multi-set,multi-relational, or multi-view) data that need to be jointly analyzed toexplore their latent connections. Various component analysis methods haveplayed an increasingly important role for the analysis of such coupled data. Inthis paper, we first provide a brief review of existing matrix-based (two-way)component analysis methods for the joint analysis of such data with a focus onbiomedical applications. Then, we discuss their important extensions andgeneralization to multi-block multiway (tensor) data. We show how constrainedmulti-block tensor decomposition methods are able to extract similar orstatistically dependent common features that are shared by all blocks, byincorporating the multiway nature of data. Special emphasis is given to theflexible common and individual feature analysis of multi-block data with theaim to simultaneously extract common and individual latent components withdesired properties and types of diversity. Illustrative examples are given todemonstrate their effectiveness for biomedical data analysis.
arxiv-12000-40 | SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks | http://arxiv.org/pdf/1502.05243v2.pdf | author:Aalok Gangopadhyay, Shivam Mani Tripathi, Ishan Jindal, Shanmuganathan Raman category:cs.CV I.5.4; I.4.8 published:2015-02-17 summary:The task of classifying videos of natural dynamic scenes into appropriateclasses has gained lot of attention in recent years. The problem especiallybecomes challenging when the camera used to capture the video is dynamic. Inthis paper, we analyse the performance of statistical aggregation (SA)techniques on various pre-trained convolutional neural network(CNN) models toaddress this problem. The proposed approach works by extracting CNN activationfeatures for a number of frames in a video and then uses an aggregation schemein order to obtain a robust feature descriptor for the video. We show throughresults that the proposed approach performs better than the-state-of-the artsfor the Maryland and YUPenn dataset. The final descriptor obtained is powerfulenough to distinguish among dynamic scenes and is even capable of addressingthe scenario where the camera motion is dominant and the scene dynamics arecomplex. Further, this paper shows an extensive study on the performance ofvarious aggregation methods and their combinations. We compare the proposedapproach with other dynamic scene classification algorithms on two publiclyavailable datasets - Maryland and YUPenn to demonstrate the superiorperformance of the proposed approach.
arxiv-12000-41 | Fast and Flexible ADMM Algorithms for Trend Filtering | http://arxiv.org/pdf/1406.2082v4.pdf | author:Aaditya Ramdas, Ryan J. Tibshirani category:stat.ML cs.LG cs.NA math.OC stat.AP published:2014-06-09 summary:This paper presents a fast and robust algorithm for trend filtering, arecently developed nonparametric regression tool. It has been shown that, forestimating functions whose derivatives are of bounded variation, trendfiltering achieves the minimax optimal error rate, while other popular methodslike smoothing splines and kernels do not. Standing in the way of a morewidespread practical adoption, however, is a lack of scalable and numericallystable algorithms for fitting trend filtering estimates. This paper presents ahighly efficient, specialized ADMM routine for trend filtering. Our algorithmis competitive with the specialized interior point methods that are currentlyin use, and yet is far more numerically robust. Furthermore, the proposed ADMMimplementation is very simple, and importantly, it is flexible enough to extendto many interesting related problems, such as sparse trend filtering andisotonic trend filtering. Software for our method is freely available, in boththe C and R languages.
arxiv-12000-42 | Understanding Editing Behaviors in Multilingual Wikipedia | http://arxiv.org/pdf/1508.07266v1.pdf | author:Suin Kim, Sungjoon Park, Scott A. Hale, Sooyoung Kim, Jeongmin Byun, Alice Oh category:cs.SI cs.CL cs.CY published:2015-08-28 summary:Multilingualism is common offline, but we have a more limited understandingof the ways multilingualism is displayed online and the roles thatmultilinguals play in the spread of content between speakers of differentlanguages. We take a computational approach to studying multilingualism usingone of the largest user-generated content platforms, Wikipedia. We studymultilingualism by collecting and analyzing a large dataset of the contentwritten by multilingual editors of the English, German, and Spanish editions ofWikipedia. This dataset contains over two million paragraphs edited by over15,000 multilingual users from July 8 to August 9, 2013. We analyze thesemultilingual editors in terms of their engagement, interests, and languageproficiency in their primary and non-primary (secondary) languages and findthat the English edition of Wikipedia displays different dynamics from theSpanish and German editions. Users primarily editing the Spanish and Germaneditions make more complex edits than users who edit these editions as a secondlanguage. In contrast, users editing the English edition as a second languagemake edits that are just as complex as the edits by users who primarily editthe English edition. In this way, English serves a special role bringingtogether content written by multilinguals from many language editions.Nonetheless, language remains a formidable hurdle to the spread of content: wefind evidence for a complexity barrier whereby editors are less likely to editcomplex content in a second language. In addition, we find that multilingualsare less engaged and show lower levels of language proficiency in their secondlanguages. We also examine the topical interests of multilingual editors andfind that there is no significant difference between primary and non-primaryeditors in each language.
arxiv-12000-43 | Bilevel parameter learning for higher-order total variation regularisation models | http://arxiv.org/pdf/1508.07243v1.pdf | author:J. C. De los Reyes, C. -B. Sch√∂nlieb, T. Valkonen category:math.OC cs.CV published:2015-08-28 summary:We consider a bilevel optimisation approach for parameter learning inhigher-order total variation image reconstruction models. Apart from the leastsquares cost functional, naturally used in bilevel learning, we propose andanalyse an alternative cost, based on a Huber regularised TV-seminorm.Differentiability properties of the solution operator are verified and afirst-order optimality system is derived. Based on the adjoint information, aquasi-Newton algorithm is proposed for the numerical solution of the bilevelproblems. Numerical experiments are carried out to show the suitability of ourapproach and the improved performance of the new cost functional. Thanks to thebilevel optimisation framework, also a detailed comparison between TGV$^2$ andICTV is carried out, showing the advantages and shortcomings of bothregularisers, depending on the structure of the processed images and theirnoise level.
arxiv-12000-44 | A Review of Nonnegative Matrix Factorization Methods for Clustering | http://arxiv.org/pdf/1507.03194v2.pdf | author:Ali Caner T√ºrkmen category:stat.ML cs.LG cs.NA published:2015-07-12 summary:Nonnegative Matrix Factorization (NMF) was first introduced as a low-rankmatrix approximation technique, and has enjoyed a wide area of applications.Although NMF does not seem related to the clustering problem at first, it wasshown that they are closely linked. In this report, we provide a gentleintroduction to clustering and NMF before reviewing the theoreticalrelationship between them. We then explore several NMF variants, namely SparseNMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, alongwith their clustering interpretations.
arxiv-12000-45 | Discrete Hashing with Deep Neural Network | http://arxiv.org/pdf/1508.07148v1.pdf | author:Thanh-Toan Do, Anh-Zung Doan, Ngai-Man Cheung category:cs.CV published:2015-08-28 summary:This paper addresses the problem of learning binary hash codes for largescale image search by proposing a novel hashing method based on deep neuralnetwork. The advantage of our deep model over previous deep model used inhashing is that our model contains necessary criteria for producing good codessuch as similarity preserving, balance and independence. Another advantage ofour method is that instead of relaxing the binary constraint of codes duringthe learning process as most previous works, in this paper, by introducing theauxiliary variable, we reformulate the optimization into two sub-optimizationsteps allowing us to efficiently solve binary constraints without anyrelaxation. The proposed method is also extended to the supervised hashing by leveragingthe label information such that the learned binary codes preserve the pairwiselabel of inputs. The experimental results on three benchmark datasets show the proposedmethods outperform state-of-the-art hashing methods.
arxiv-12000-46 | Evaluation of Output Embeddings for Fine-Grained Image Classification | http://arxiv.org/pdf/1409.8403v2.pdf | author:Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, Bernt Schiele category:cs.CV published:2014-09-30 summary:Image classification has advanced significantly in recent years with theavailability of large-scale image sets. However, fine-grained classificationremains a major challenge due to the annotation cost of large numbers offine-grained categories. This project shows that compelling classificationperformance can be achieved on such categories even without labeled trainingdata. Given image and class embeddings, we learn a compatibility function suchthat matching embeddings are assigned a higher score than mismatching ones;zero-shot classification of an image proceeds by finding the label yielding thehighest joint compatibility score. We use state-of-the-art image features andfocus on different supervised attributes and unsupervised output embeddingseither derived from hierarchies or learned from unlabeled text corpora. Weestablish a substantially improved state-of-the-art on the Animals withAttributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstratethat purely unsupervised output embeddings (learned from Wikipedia and improvedwith fine-grained text) achieve compelling results, even outperforming theprevious supervised state-of-the-art. By combining different output embeddings,we further improve results.
arxiv-12000-47 | Type-Constrained Representation Learning in Knowledge Graphs | http://arxiv.org/pdf/1508.02593v2.pdf | author:Denis Krompa√ü, Stephan Baier, Volker Tresp category:cs.AI cs.LG published:2015-08-11 summary:Large knowledge graphs increasingly add value to various applications thatrequire machines to recognize and understand queries and their semantics, as insearch or question answering systems. Latent variable models have increasinglygained attention for the statistical modeling of knowledge graphs, showingpromising results in tasks related to knowledge graph completion and cleaning.Besides storing facts about the world, schema-based knowledge graphs are backedby rich semantic descriptions of entities and relation-types that allowmachines to understand the notion of things and their semantic relationships.In this work, we study how type-constraints can generally support thestatistical modeling with latent variable models. More precisely, we integratedprior knowledge in form of type-constraints in various state of the art latentvariable approaches. Our experimental results show that prior knowledge onrelation-types significantly improves these models up to 77% in link-predictiontasks. The achieved improvements are especially prominent when a low modelcomplexity is enforced, a crucial requirement when these models are applied tovery large datasets. Unfortunately, type-constraints are neither alwaysavailable nor always complete e.g., they can become fuzzy when entities lackproper typing. We show that in these cases, it can be beneficial to apply alocal closed-world assumption that approximates the semantics of relation-typesbased on observations made in the data.
arxiv-12000-48 | Parallel Dither and Dropout for Regularising Deep Neural Networks | http://arxiv.org/pdf/1508.07130v1.pdf | author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx published:2015-08-28 summary:Effective regularisation during training can mean the difference betweensuccess and failure for deep neural networks. Recently, dither has beensuggested as alternative to dropout for regularisation during batch-averagedstochastic gradient descent (SGD). In this article, we show that these methodsfail without batch averaging and we introduce a new, parallel regularisationmethod that may be used without batch averaging. Our results forparallel-regularised non-batch-SGD are substantially better than what ispossible with batch-SGD. Furthermore, our results demonstrate that dither anddropout are complimentary.
arxiv-12000-49 | Regularized Kernel Recursive Least Square Algoirthm | http://arxiv.org/pdf/1508.07103v1.pdf | author:Songlin Zhao category:cs.LG stat.ML published:2015-08-28 summary:In most adaptive signal processing applications, system linearity is assumedand adaptive linear filters are thus used. The traditional class of supervisedadaptive filters rely on error-correction learning for their adaptivecapability. The kernel method is a powerful nonparametric modeling tool forpattern analysis and statistical signal processing. Through a nonlinearmapping, kernel methods transform the data into a set of points in aReproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fastconvergence rate in stationary scenario. However the good performance isobtained at a cost of high computation complexity. Sparsification in kernelmethods is know to related to less computational complexity and memoryconsumption.
arxiv-12000-50 | Partitioning Large Scale Deep Belief Networks Using Dropout | http://arxiv.org/pdf/1508.07096v1.pdf | author:Yanping Huang, Sai Zhang category:stat.ML cs.LG cs.NE published:2015-08-28 summary:Deep learning methods have shown great promise in many practicalapplications, ranging from speech recognition, visual object recognition, totext processing. However, most of the current deep learning methods suffer fromscalability problems for large-scale applications, forcing researchers or usersto focus on small-scale problems with fewer parameters. In this paper, we consider a well-known machine learning model, deep beliefnetworks (DBNs) that have yielded impressive classification performance on alarge number of benchmark machine learning tasks. To scale up DBN, we proposean approach that can use the computing clusters in a distributed environment totrain large models, while the dense matrix computations within a single machineare sped up using graphics processors (GPU). When training a DBN, each machinerandomly drops out a portion of neurons in each hidden layer, for each trainingcase, making the remaining neurons only learn to detect features that aregenerally helpful for producing the correct answer. Within our approach, wehave developed four methods to combine outcomes from each machine to form aunified model. Our preliminary experiment on the mnst handwritten digitdatabase demonstrates that our approach outperforms the state of the art testerror rate.
arxiv-12000-51 | Deep Recurrent Q-Learning for Partially Observable MDPs | http://arxiv.org/pdf/1507.06527v3.pdf | author:Matthew Hausknecht, Peter Stone category:cs.LG published:2015-07-23 summary:Deep Reinforcement Learning has yielded proficient controllers for complextasks. However, these controllers have limited memory and rely on being able toperceive the complete game screen at each decision point. To address theseshortcomings, this article investigates the effects of adding recurrency to aDeep Q-Network (DQN) by replacing the first post-convolutional fully-connectedlayer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network}(DRQN), although capable of seeing only a single frame at each timestep,successfully integrates information through time and replicates DQN'sperformance on standard Atari games and partially observed equivalentsfeaturing flickering game screens. Additionally, when trained with partialobservations and evaluated with incrementally more complete observations,DRQN's performance scales as a function of observability. Conversely, whentrained with full observations and evaluated with partial observations, DRQN'sperformance degrades less than DQN's. Thus, given the same length of history,recurrency is a viable alternative to stacking a history of frames in the DQN'sinput layer and while recurrency confers no systematic advantage when learningto play the game, the recurrent net can better adapt at evaluation time if thequality of observations changes.
arxiv-12000-52 | Understanding confounding effects in linguistic coordination: an information-theoretic approach | http://arxiv.org/pdf/1412.0696v2.pdf | author:Shuyang Gao, Greg Ver Steeg, Aram Galstyan category:cs.CL cs.IT cs.SI math.IT published:2014-12-01 summary:We suggest an information-theoretic approach for measuring stylisticcoordination in dialogues. The proposed measure has a simple predictiveinterpretation and can account for various confounding factors through properconditioning. We revisit some of the previous studies that reported strongsignatures of stylistic accommodation, and find that a significant part of theobserved coordination can be attributed to a simple confounding effect - lengthcoordination. Specifically, longer utterances tend to be followed by longerresponses, which gives rise to spurious correlations in the other stylisticfeatures. We propose a test to distinguish correlations in length due tocontextual factors (topic of conversation, user verbosity, etc.) andturn-by-turn coordination. We also suggest a test to identify whether stylisticcoordination persists even after accounting for length coordination andcontextual factors.
arxiv-12000-53 | Validation of neural spike sorting algorithms without ground-truth information | http://arxiv.org/pdf/1508.06936v1.pdf | author:Alex H. Barnett, Jeremy F. Magland, Leslie F. Greengard category:q-bio.NC cs.CV published:2015-08-27 summary:We describe a suite of validation metrics that assess the credibility of agiven automatic spike sorting algorithm applied to a given electrophysiologicalrecording, when ground-truth is unavailable. By rerunning the spike sorter twoor more times, the metrics measure stability under various perturbationsconsistent with variations in the data itself, making no assumptions about thenoise model, nor about the internal workings of the sorting algorithm. Suchstability is a prerequisite for reproducibility of results. We illustrate themetrics on standard sorting algorithms for both in vivo and ex vivo recordings.We believe that such metrics could reduce the significant human labor currentlyspent on validation, and should form an essential part of large-scale automatedspike sorting and systematic benchmarking of algorithms.
arxiv-12000-54 | Compressive Sensing via Low-Rank Gaussian Mixture Models | http://arxiv.org/pdf/1508.06901v1.pdf | author:Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford category:stat.ML cs.LG stat.AP published:2015-08-27 summary:We develop a new compressive sensing (CS) inversion algorithm by utilizingthe Gaussian mixture model (GMM). While the compressive sensing is performedglobally on the entire image as implemented in our lensless camera, a low-rankGMM is imposed on the local image patches. This low-rank GMM is derived viaeigenvalue thresholding of the GMM trained on the projection of the measurementdata, thus learned {\em in situ}. The GMM and the projection of the measurementdata are updated iteratively during the reconstruction. Our GMM algorithmdegrades to the piecewise linear estimator (PLE) if each patch is representedby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is alsodeveloped for CS inversion, constituting an additional contribution of thispaper. Extensive results on both simulation data and real data captured by thelensless camera demonstrate the efficacy of the proposed algorithm.Furthermore, we compare the CS reconstruction results using our algorithm withthe JPEG compression. Simulation results demonstrate that when limitedbandwidth is available (a small number of measurements), our algorithm canachieve comparable results as JPEG.
arxiv-12000-55 | Shopper Analytics: a customer activity recognition system using a distributed RGB-D camera network | http://arxiv.org/pdf/1508.06853v1.pdf | author:Daniele Liciotti, Marco Contigiani, Emanuele Frontoni, Adriano Mancini, Primo Zingaretti, Valerio Placidi category:cs.CV published:2015-08-27 summary:The aim of this paper is to present an integrated system consisted of a RGB-Dcamera and a software able to monitor shoppers in intelligent retailenvironments. We propose an innovative low cost smart system that canunderstand the shoppers' behavior and, in particular, their interactions withthe products in the shelves, with the aim to develop an automatic RGB-Dtechnique for video analysis. The system of cameras detects the presence ofpeople and univocally identifies them. Through the depth frames, the systemdetects the interactions of the shoppers with the products on the shelf anddetermines if a product is picked up or if the product is taken and then putback and finally, if there is not contact with the products. The system is lowcost and easy to install, and experimental results demonstrated that itsperformances are satisfactory also in real environments.
arxiv-12000-56 | Encrypted statistical machine learning: new privacy preserving methods | http://arxiv.org/pdf/1508.06845v1.pdf | author:Louis J. M. Aslett, Pedro M. Esperan√ßa, Chris C. Holmes category:stat.ML cs.CR cs.LG stat.ME published:2015-08-27 summary:We present two new statistical machine learning methods designed to learn onfully homomorphic encrypted (FHE) data. The introduction of FHE schemesfollowing Gentry (2009) opens up the prospect of privacy preserving statisticalmachine learning analysis and modelling of encrypted data without compromisingsecurity constraints. We propose tailored algorithms for applying extremelyrandom forests, involving a new cryptographic stochastic fraction estimator,and na\"{i}ve Bayes, involving a semi-parametric model for the class decisionboundary, and show how they can be used to learn and predict from encrypteddata. We demonstrate that these techniques perform competitively on a varietyof classification data sets and provide detailed information about thecomputational practicalities of these and other FHE methods.
arxiv-12000-57 | Introducing Elitist Black-Box Models: When Does Elitist Selection Weaken the Performance of Evolutionary Algorithms? | http://arxiv.org/pdf/1508.06802v1.pdf | author:Carola Doerr, Johannes Lengler category:cs.NE cs.DS published:2015-08-27 summary:Black-box complexity theory provides lower bounds for the runtime ofblack-box optimizers like evolutionary algorithms and serves as an inspirationfor the design of new genetic algorithms. Several black-box models coveringdifferent classes of algorithms exist, each highlighting a different aspect ofthe algorithms under considerations. In this work we add to the existingblack-box notions a new \emph{elitist black-box model}, in which algorithms arerequired to base all decisions solely on (a fixed number of) the best searchpoints sampled so far. Our model combines features of the ranking-based and thememory-restricted black-box models with elitist selection. We provide several examples for which the elitist black-box complexity isexponentially larger than that the respective complexities in all previousblack-box models, thus showing that the elitist black-box complexity can bemuch closer to the runtime of typical evolutionary algorithms. We also introduce the concept of $p$-Monte Carlo black-box complexity, whichmeasures the time it takes to optimize a problem with failure probability atmost $p$. Even for small~$p$, the $p$-Monte Carlo black-box complexity of afunction class $\mathcal F$ can be smaller by an exponential factor than itstypically regarded Las Vegas complexity (which measures the \emph{expected}time it takes to optimize $\mathcal F$).
arxiv-12000-58 | Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric Admixture Modeling | http://arxiv.org/pdf/1508.06446v2.pdf | author:Lavanya Sita Tekumalla, Priyanka Agrawal, Indrajit Bhattacharya category:stat.ML cs.LG published:2015-08-26 summary:Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixturemodeling, where the number of mixture components grows with the number of dataitems. The Hierarchical Dirichlet Process (HDP), is an extension of DP forgrouped data, often used for non-parametric topic modeling, where each group isa mixture over shared mixture densities. The Nested Dirichlet Process (nDP), onthe other hand, is an extension of the DP for learning group leveldistributions from data, simultaneously clustering the groups. It allows grouplevel distributions to be shared across groups in a non-parametric setting,leading to a non-parametric mixture of mixtures. The nCRF extends the nDP formultilevel non-parametric mixture modeling, enabling modeling topichierarchies. However, the nDP and nCRF do not allow sharing of distributions asrequired in many applications, motivating the need for multi-levelnon-parametric admixture modeling. We address this gap by proposing multi-levelnested HDPs (nHDP) where the base distribution of the HDP is itself a HDP ateach level thereby leading to admixtures of admixtures at each level. Becauseof couplings between various HDP levels, scaling up is naturally a challengeduring inference. We propose a multi-level nested Chinese Restaurant Franchise(nCRF) representation for the nested HDP, with which we outline an inferencealgorithm based on Gibbs Sampling. We evaluate our model with the two levelnHDP for non-parametric entity topic modeling where an inner HDP creates acountably infinite set of topic mixtures and associates them with authorentities, while an outer HDP associates documents with these author entities.In our experiments on two real world research corpora, the nHDP is able togeneralize significantly better than existing models and detect missing authorentities with a reasonable level of accuracy.
arxiv-12000-59 | A biologically constrained model of the whole basal ganglia addressing the paradoxes of connections and selection | http://arxiv.org/pdf/1512.00035v1.pdf | author:Jean Li√©nard, Beno√Æt Girard category:q-bio.NC cs.NE cs.RO published:2015-08-27 summary:The basal ganglia nuclei form a complex network of nuclei often assumed toperform selection, yet their individual roles and how they influence each otheris still largely unclear. In particular, the ties between the external andinternal parts of the globus pallidus are paradoxical, as anatomical datasuggest a potent inhibitory projection between them while electrophys-iologicalrecordings indicate that they have similar activities. Here we introduce atheoretical study that reconciles both views on the intra-pallidal projection,by providing a plausible characterization of the relationship between theexternal and internal globus pallidus. Specifically, we developed a mean-fieldmodel of the whole basal ganglia, whose parameterization is optimized torespect best a collection of numerous anatomical and electrophysiological data.We first obtained models respecting all our constraints, hence anatomical andelectrophysiological data on the intrapallidal projection are globallyconsistent. This model furthermore predicts that both aforementioned viewsabout the intra-pallidal projection may be reconciled when this projection isweakly inhibitory, thus making it possible to support similar neural activityin both nuclei and for the entire basal ganglia to select between actions.Second, we predicts that afferent projections are substantially unbalancedtowards the external segment, as it receives the strongest excitation from STNand the weakest inhibition from the striatum. Finally, our study stronglysuggest that the intrapallidal connection pattern is not focused but diffuse,as this latter pattern is more efficient for the overall selection performed inthe basal ganglia.
arxiv-12000-60 | A fully data-driven method to identify (correlated) changes in diachronic corpora | http://arxiv.org/pdf/1508.06374v2.pdf | author:Alexander Koplenig category:cs.CL cs.IR stat.AP published:2015-08-26 summary:In this paper, a method for measuring synchronic corpus (dis-)similarity putforward by Kilgarriff (2001) is adapted and extended to identify trends andcorrelated changes in diachronic text data, using the Corpus of HistoricalAmerican English (Davies 2010a) and the Google Ngram Corpora (Michel et al.2010a). This paper shows that this fully data-driven method, which extractsword types that have undergone the most pronounced change in frequency in agiven period of time, is computationally very cheap and that it allowsinterpretations of diachronic trends that are both intuitively plausible andmotivated from the perspective of information theory. Furthermore, itdemonstrates that the method is able to identify correlated linguistic changesand diachronic shifts that can be linked to historical events. Finally, it canhelp to improve diachronic POS tagging and complement existing NLP approaches.This indicates that the approach can facilitate an improved understanding ofdiachronic processes in language change.
arxiv-12000-61 | A Comparative Analysis of Retrieval Techniques In Content Based Image Retrieval | http://arxiv.org/pdf/1508.06728v1.pdf | author:Mohini P. Sardey, G. K. Kharate category:cs.CV 68 published:2015-08-27 summary:Basic group of visual techniques such as color, shape, texture are used inContent Based Image Retrievals (CBIR) to retrieve query image or subregion ofimage to find similar images in image database. To improve query result,relevance feedback is used many times in CBIR to help user to express theirpreference and improve query results.In this paper, a new approach for imageretrieval is proposed which is based on the features such as Color Histogram,Eigen Values and Match Point. Images from various types of database are firstidentified by using edge detection techniques.Once the image is identified,then the image is searched in the particular database, then all related imagesare displayed. This will save the retrieval time. Further to retrieve theprecise query image, any of the three techniques are used and comparison isdone w.r.t. average retrieval time. Eigen value technique found to be the bestas compared with other two techniques
arxiv-12000-62 | Image Type Water Meter Character Recognition Based on Embedded DSP | http://arxiv.org/pdf/1508.06725v1.pdf | author:Ying Liu, Yan-bin Han, Yu-lin Zhang category:cs.CV published:2015-08-27 summary:In the paper, we combined DSP processor with image processing algorithm andstudied the method of water meter character recognition. We collected watermeter image through camera at a fixed angle, and the projection method is usedto recognize those digital images. The experiment results show that the methodcan recognize the meter characters accurately and artificial meter reading isreplaced by automatic digital recognition, which improves working efficiency.
arxiv-12000-63 | Online Anomaly Detection via Class-Imbalance Learning | http://arxiv.org/pdf/1508.06717v1.pdf | author:Chandresh Kumar Maurya, Durga Toshniwal, Gopalan Vijendran Venkoparao category:cs.LG published:2015-08-27 summary:Anomaly detection is an important task in many real world applications suchas fraud detection, suspicious activity detection, health care monitoring etc.In this paper, we tackle this problem from supervised learning perspective inonline learning setting. We maximize well known \emph{Gmean} metric forclass-imbalance learning in online learning framework. Specifically, we showthat maximizing \emph{Gmean} is equivalent to minimizing a convex surrogateloss function and based on that we propose novel online learning algorithm foranomaly detection. We then show, by extensive experiments, that the performanceof the proposed algorithm with respect to $sum$ metric is as good as a recentlyproposed Cost-Sensitive Online Classification(CSOC) algorithm forclass-imbalance learning over various benchmarked data sets while keepingrunning time close to the perception algorithm. Our another conclusion is thatother competitive online algorithms do not perform consistently over data setsof varying size. This shows the potential applicability of our proposedapproach.
arxiv-12000-64 | Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation | http://arxiv.org/pdf/1508.06708v1.pdf | author:Sijin Li, Weichen Zhang, Antoni B. Chan category:cs.CV published:2015-08-27 summary:This paper focuses on structured-output learning using deep neural networksfor 3D human pose estimation from monocular images. Our network takes an imageand 3D pose as inputs and outputs a score value, which is high when theimage-pose pair matches and low otherwise. The network structure consists of aconvolutional neural network for image feature extraction, followed by twosub-networks for transforming the image features and pose into a jointembedding. The score function is then the dot-product between the image andpose embeddings. The image-pose embedding and score function are jointlytrained using a maximum-margin cost function. Our proposed framework can beinterpreted as a special form of structured support vector machines where thejoint feature space is discriminatively learned using deep neural networks. Wetest our framework on the Human3.6m dataset and obtain state-of-the-art resultscompared to other recent methods. Finally, we present visualizations of theimage-pose embedding space, demonstrating the network has learned a high-levelembedding of body-orientation and pose-configuration.
arxiv-12000-65 | Using Genetic Algorithms to Benchmark the Cloud | http://arxiv.org/pdf/1508.06705v1.pdf | author:Jeff Kinnison, Sekou L. Remy category:cs.DC cs.NE cs.PF published:2015-08-27 summary:This paper presents a novel application of Genetic Algorithms(GAs) toquantify the performance of Platform as a Service (PaaS), a cloud service modelthat plays a critical role in both industry and academia. While Cloudbenchmarks are not new, in this novel concept, the authors use a GA to takeadvantage of the elasticity in Cloud services in a graceful manner that was notpreviously possible. Using Google App Engine, Heroku, and Python Anywhere withthree distinct classes of client computers running our GA codebase, wequantified the completion time for application of the GA to search for theparameters of controllers for dynamical systems. Our results show statisticallysignificant differences in PaaS performance by vendor, and also that theperformance of the PaaS performance is dependent upon the client that uses it.Results also show the effectiveness of our GA in determining the level ofservice of PaaS providers, and for determining if the level of service of onePaaS vendor is repeatable with another. Such a concept could then increase theappeal of PaaS Cloud services by making them more financially appealing.
arxiv-12000-66 | Component-Enhanced Chinese Character Embeddings | http://arxiv.org/pdf/1508.06669v1.pdf | author:Yanran Li, Wenjie Li, Fei Sun, Sujian Li category:cs.CL published:2015-08-26 summary:Distributed word representations are very useful for capturing semanticinformation and have been successfully applied in a variety of NLP tasks,especially on English. In this work, we innovatively develop twocomponent-enhanced Chinese character embedding models and their bigramextensions. Distinguished from English word embeddings, our models explore thecompositions of Chinese characters, which often serve as semantic indictorsinherently. The evaluations on both word similarity and text classificationdemonstrate the effectiveness of our models.
arxiv-12000-67 | From Paraphrase Database to Compositional Paraphrase Model and Back | http://arxiv.org/pdf/1506.03487v2.pdf | author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Dan Roth category:cs.CL published:2015-06-10 summary:The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensivesemantic resource, consisting of a list of phrase pairs with (heuristic)confidence estimates. However, it is still unclear how it can best be used, dueto the heuristic nature of the confidences and its necessarily incompletecoverage. We propose models to leverage the phrase pairs from the PPDB to buildparametric paraphrase models that score paraphrase pairs more accurately thanthe PPDB's internal scores while simultaneously improving its coverage. Theyallow for learning phrase embeddings as well as improved word embeddings.Moreover, we introduce two new, manually annotated datasets to evaluateshort-phrase paraphrasing models. Using our paraphrase model trained usingPPDB, we achieve state-of-the-art results on standard word and bigramsimilarity tasks and beat strong baselines on our new short phrase paraphrasetasks.
arxiv-12000-68 | Automatic Relevance Determination For Deep Generative Models | http://arxiv.org/pdf/1505.07765v3.pdf | author:Theofanis Karaletsos, Gunnar R√§tsch category:stat.ML published:2015-05-28 summary:A recurring problem when building probabilistic latent variable models isregularization and model selection, for instance, the choice of thedimensionality of the latent space. In the context of belief networks withlatent variables, this problem has been adressed with Automatic RelevanceDetermination (ARD) employing Monte Carlo inference. We present a variationalinference approach to ARD for Deep Generative Models using doubly stochasticvariational inference to provide fast and scalable learning. We show empiricalresults on a standard dataset illustrating the effects of contracting thelatent space automatically. We show that the resulting latent representationsare significantly more compact without loss of expressive power of the learnedmodels.
arxiv-12000-69 | Financial Market Modeling with Quantum Neural Networks | http://arxiv.org/pdf/1508.06586v1.pdf | author:Carlos Pedro Gon√ßalves category:q-fin.CP cs.NE physics.soc-ph q-fin.GN published:2015-08-26 summary:Econophysics has developed as a research field that applies the formalism ofStatistical Mechanics and Quantum Mechanics to address Economics and Financeproblems. The branch of Econophysics that applies of Quantum Theory toEconomics and Finance is called Quantum Econophysics. In Finance, QuantumEconophysics' contributions have ranged from option pricing to market dynamicsmodeling, behavioral finance and applications of Game Theory, integrating theempirical finding, from human decision analysis, that shows that nonlinearupdate rules in probabilities, leading to non-additive decision weights, can becomputationally approached from quantum computation, with resulting quantuminterference terms explaining the non-additive probabilities. The current workdraws on these results to introduce new tools from Quantum ArtificialIntelligence, namely Quantum Artificial Neural Networks as a way to build andsimulate financial market models with adaptive selection of trading rules,leading to turbulence and excess kurtosis in the returns distributions for awide range of parameters.
arxiv-12000-70 | Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems | http://arxiv.org/pdf/1508.01745v2.pdf | author:Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL published:2015-08-07 summary:Natural language generation (NLG) is a critical component of spoken dialogueand it has a significant impact both on usability and perceived quality. MostNLG systems in common use employ rules and heuristics and tend to generaterigid and stylised responses without the natural variation of human language.They are also not easily scaled to systems covering multiple domains andlanguages. This paper presents a statistical language generator based on asemantically controlled Long Short-term Memory (LSTM) structure. The LSTMgenerator can learn from unaligned data by jointly optimising sentence planningand surface realisation using a simple cross entropy training criterion, andlanguage variation can be easily achieved by sampling from output candidates.With fewer heuristics, an objective evaluation in two differing test domainsshowed the proposed method improved performance compared to previous methods.Human judges scored the LSTM system higher on informativeness and naturalnessand overall preferred it to the other systems.
arxiv-12000-71 | A review of homomorphic encryption and software tools for encrypted statistical machine learning | http://arxiv.org/pdf/1508.06574v1.pdf | author:Louis J. M. Aslett, Pedro M. Esperan√ßa, Chris C. Holmes category:stat.ML cs.CR cs.LG published:2015-08-26 summary:Recent advances in cryptography promise to enable secure statisticalcomputation on encrypted data, whereby a limited set of operations can becarried out without the need to first decrypt. We review these homomorphicencryption schemes in a manner accessible to statisticians and machinelearners, focusing on pertinent limitations inherent in the current state ofthe art. These limitations restrict the kind of statistics and machine learningalgorithms which can be implemented and we review those which have beensuccessfully applied in the literature. Finally, we document a high performanceR package implementing a recent homomorphic scheme in a general framework.
arxiv-12000-72 | Deep Convolutional Neural Networks for Smile Recognition | http://arxiv.org/pdf/1508.06535v1.pdf | author:Patrick O. Glauner category:cs.CV cs.LG cs.NE published:2015-08-26 summary:This thesis describes the design and implementation of a smile detector basedon deep convolutional neural networks. It starts with a summary of neuralnetworks, the difficulties of training them and new training methods, such asRestricted Boltzmann Machines or autoencoders. It then provides a literaturereview of convolutional neural networks and recurrent neural networks. In orderto select databases for smile recognition, comprehensive statistics ofdatabases popular in the field of facial expression recognition were generatedand are summarized in this thesis. It then proposes a model for smiledetection, of which the main part is implemented. The experimental results arediscussed in this thesis and justified based on a comprehensive model selectionperformed. All experiments were run on a Tesla K40c GPU benefiting from aspeedup of up to factor 10 over the computations on a CPU. A smile detectiontest accuracy of 99.45% is achieved for the Denver Intensity of SpontaneousFacial Action (DISFA) database, significantly outperforming existing approacheswith accuracies ranging from 65.55% to 79.67%. This experiment is re-run undervarious variations, such as retaining less neutral images or only the low orhigh intensities, of which the results are extensively compared.
arxiv-12000-73 | Alignment-based compositional semantics for instruction following | http://arxiv.org/pdf/1508.06491v1.pdf | author:Jacob Andreas, Dan Klein category:cs.CL published:2015-08-26 summary:This paper describes an alignment-based model for interpreting naturallanguage instructions in context. We approach instruction following as a searchover plans, scoring sequences of actions conditioned on structured observationsof text and the environment. By explicitly modeling both the low-levelcompositional structure of individual actions and the high-level structure offull plans, we are able to learn both grounded representations of sentencemeaning and pragmatic constraints on interpretation. To demonstrate the model'sflexibility, we apply it to a diverse set of benchmark tasks. On every task, weoutperform strong task-specific baselines, and achieve several newstate-of-the-art results.
arxiv-12000-74 | A Multiple-Expert Binarization Framework for Multispectral Images | http://arxiv.org/pdf/1502.01199v6.pdf | author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2015-02-04 summary:In this work, a multiple-expert binarization framework for multispectralimages is proposed. The framework is based on a constrained subspace selectionlimited to the spectral bands combined with state-of-the-art gray-levelbinarization methods. The framework uses a binarization wrapper to enhance theperformance of the gray-level binarization. Nonlinear preprocessing of theindividual spectral bands is used to enhance the textual information. Anevolutionary optimizer is considered to obtain the optimal and some suboptimal3-band subspaces from which an ensemble of experts is then formed. Theframework is applied to a ground truth multispectral dataset with promisingresults. In addition, a generalization to the cross-validation approach isdeveloped that not only evaluates generalizability of the framework, it alsoprovides a practical instance of the selected experts that could be thenapplied to unseen inputs despite the small size of the given ground truthdataset.
arxiv-12000-75 | Population Synthesis via k-Nearest Neighbor Crossover Kernel | http://arxiv.org/pdf/1508.06483v1.pdf | author:Naoki Hamada, Katsumi Homma, Hiroyuki Higuchi, Hideyuki Kikuchi category:cs.NE published:2015-08-26 summary:The recent development of multi-agent simulations brings about a need forpopulation synthesis. It is a task of reconstructing the entire population froma sampling survey of limited size (1% or so), supplying the initial conditionsfrom which simulations begin. This paper presents a new kernel densityestimator for this task. Our method is an analogue of the classicalBreiman-Meisel-Purcell estimator, but employs novel techniques that harness thehuge degree of freedom which is required to model high-dimensional nonlinearlycorrelated datasets: the crossover kernel, the k-nearest neighbor restrictionof the kernel construction set and the bagging of kernels. The performance as astatistical estimator is examined through real and synthetic datasets. Weprovide an "optimization-free" parameter selection rule for our method, atheory of how our method works and a computational cost analysis. Todemonstrate the usefulness as a population synthesizer, our method is appliedto a household synthesis task for an urban micro-simulator.
arxiv-12000-76 | Greedy methods, randomization approaches and multi-arm bandit algorithms for efficient sparsity-constrained optimization | http://arxiv.org/pdf/1508.06477v1.pdf | author:A. Rakotomamonjy, S. Ko√ßo, L. Ralaivola category:cs.LG published:2015-08-26 summary:Several sparsity-constrained algorithms such as Orthogonal Matching Pursuitor the Frank-Wolfe algorithm with sparsity constraints work by iterativelyselecting a novel atom to add to the current non-zero set of variables. Thisselection step is usually performed by computing the gradient and then bylooking for the gradient component with maximal absolute entry. This step canbe computationally expensive especially for large-scale and high-dimensionaldata. In this work, we aim at accelerating these sparsity-constrainedoptimization algorithms by exploiting the key observation that, for thesealgorithms to work, one only needs the coordinate of the gradient's top entry.Hence, we introduce algorithms based on greedy methods and randomizationapproaches that aim at cheaply estimating the gradient and its top entry.Another of our contribution is to cast the problem of finding the best gradiententry as a best arm identification in a multi-armed bandit problem. Owing tothis novel insight, we are able to provide a bandit-based algorithm thatdirectly estimates the top entry in a very efficient way. Theoretical resultsstating that the resulting inexact Frank-Wolfe or Orthogonal Matching Pursuitalgorithms act, with high probability, similarly to their exact versions arealso given. We have carried out several experiments showing that the greedydeterministic and the bandit approaches we propose can achieve an accelerationof an order of magnitude while being as efficient as the exact gradient whenused in algorithms such as
arxiv-12000-77 | Dither is Better than Dropout for Regularising Deep Neural Networks | http://arxiv.org/pdf/1508.04826v2.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-08-19 summary:Regularisation of deep neural networks (DNN) during training is critical toperformance. By far the most popular method is known as dropout. Here, castthrough the prism of signal processing theory, we compare and contrast theregularisation effects of dropout with those of dither. We illustrate someserious inherent limitations of dropout and demonstrate that dither provides amore effective regulariser.
arxiv-12000-78 | SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter | http://arxiv.org/pdf/1508.06464v1.pdf | author:Osamu Hirose, Shotaro Kawaguchi, Terumasa Tokunaga, Yu Toyoshima, Takayuki Teramoto, Sayuri Kuge, Takeshi Ishihara, Yuichi Iino, Ryo Yoshida category:cs.CV published:2015-08-26 summary:Tracking many cells in time-lapse 3D image sequences is an importantchallenging task of bioimage informatics. Motivated by a study of brain-wide 4Dimaging of neural activity in C. elegans, we present a new method of multi-celltracking. Data types to which the method is applicable are characterized asfollows: (i) cells are imaged as globular-like objects, (ii) it is difficult todistinguish cells based only on shape and size, (iii) the number of imagedcells ranges in several hundreds, (iv) moves of nearly-located cells arestrongly correlated and (v) cells do not divide. We developed a trackingsoftware suite which we call SPF-CellTracker. Incorporating dependency oncells' moves into prediction model is the key to reduce the tracking errors:cell-switching and coalescence of tracked positions. We model target cells'correlated moves as a Markov random field and we also derive a fast computationalgorithm, which we call spatial particle filter. With the live-imaging data ofnuclei of C. elegans neurons in which approximately 120 nuclei of neurons areimaged, we demonstrate an advantage of the proposed method over the standardparticle filter and a method developed by Tokunaga et al. (2014).
arxiv-12000-79 | Crossings as a side effect of dependency lengths | http://arxiv.org/pdf/1508.06451v1.pdf | author:Ramon Ferrer-i-Cancho, Carlos G√≥mez-Rodr√≠guez category:cs.CL cs.SI physics.soc-ph published:2015-08-26 summary:The syntactic structure of sentences exhibits a striking regularity:dependencies tend to not cross when drawn above the sentence. Here weinvestigate two competing hypotheses for the origins of non-crossingdependencies. The traditional hypothesis is that the low frequency ofdependency crossings arises from an independent principle of syntax thatreduces crossings practically to zero. An alternative to this view is thehypothesis that crossings are a side effect of dependency lengths. According tothis view, sentences with shorter dependency lengths should tend to have fewercrossings. We recast the traditional view as a null hypothesis where one of thevariables, i.e. the number of crossings, is mean independent of the other, i.e.the sum of dependency lengths. The alternative view is then a positivecorrelation between these two variables. In spite of the rough estimation ofdependency crossings that this sum provides, we are able to reject thetraditional view in the majority of languages considered. The alternativehypothesis can lead to a more parsimonious theory of syntax.
arxiv-12000-80 | Gaussian Mixture Models with Component Means Constrained in Pre-selected Subspaces | http://arxiv.org/pdf/1508.06388v1.pdf | author:Mu Qiao, Jia Li category:stat.ML cs.LG published:2015-08-26 summary:We investigate a Gaussian mixture model (GMM) with component meansconstrained in a pre-selected subspace. Applications to classification andclustering are explored. An EM-type estimation algorithm is derived. We provethat the subspace containing the component means of a GMM with a commoncovariance matrix also contains the modes of the density and the class means.This motivates us to find a subspace by applying weighted principal componentanalysis to the modes of a kernel density and the class means. To circumventthe difficulty of deciding the kernel bandwidth, we acquire multiple subspacesfrom the kernel densities based on a sequence of bandwidths. The GMMconstrained by each subspace is estimated; and the model yielding the maximumlikelihood is chosen. A dimension reduction property is proved in the sense ofbeing informative for classification or clustering. Experiments on real andsimulated data sets are conducted to examine several ways of determining thesubspace and to compare with the reduced rank mixture discriminant analysis(MDA). Our new method with the simple technique of spanning the subspace onlyby class means often outperforms the reduced rank MDA when the subspacedimension is very low, making it particularly appealing for visualization.
arxiv-12000-81 | SPRIGHT: A Fast and Robust Framework for Sparse Walsh-Hadamard Transform | http://arxiv.org/pdf/1508.06336v1.pdf | author:Xiao Li, Joseph K. Bradley, Sameer Pawar, Kannan Ramchandran category:cs.IT cs.LG math.IT published:2015-08-26 summary:We consider the problem of computing the Walsh-Hadamard Transform (WHT) ofsome $N$-length input vector in the presence of noise, where the $N$-pointWalsh spectrum is $K$-sparse with $K = {O}(N^{\delta})$ scaling sub-linearly inthe input dimension $N$ for some $0<\delta<1$. Over the past decade, there hasbeen a resurgence in research related to the computation of Discrete FourierTransform (DFT) for some length-$N$ input signal that has a $K$-sparse Fourierspectrum. In particular, through a sparse-graph code design, our earlier workon the Fast Fourier Aliasing-based Sparse Transform (FFAST) algorithm computesthe $K$-sparse DFT in time ${O}(K\log K)$ by taking ${O}(K)$ noiseless samples.Inspired by the coding-theoretic design framework, Scheibler et al. proposedthe Sparse Fast Hadamard Transform (SparseFHT) algorithm that elegantlycomputes the $K$-sparse WHT in the absence of noise using ${O}(K\log N)$samples in time ${O}(K\log^2 N)$. However, the SparseFHT algorithm explicitlyexploits the noiseless nature of the problem, and is not equipped to deal withscenarios where the observations are corrupted by noise. Therefore, a questionof critical interest is whether this coding-theoretic framework can be maderobust to noise. Further, if the answer is yes, what is the extra price thatneeds to be paid for being robust to noise? In this paper, we show, quiteinterestingly, that there is {\it no extra price} that needs to be paid forbeing robust to noise other than a constant factor. In other words, we canmaintain the same sample complexity ${O}(K\log N)$ and the computationalcomplexity ${O}(K\log^2 N)$ as those of the noiseless case, using our SParseRobust Iterative Graph-based Hadamard Transform (SPRIGHT) algorithm.
arxiv-12000-82 | Toward a unified theory of sparse dimensionality reduction in Euclidean space | http://arxiv.org/pdf/1311.2542v4.pdf | author:Jean Bourgain, Sjoerd Dirksen, Jelani Nelson category:cs.DS cs.CG cs.IT math.IT math.PR stat.ML published:2013-11-11 summary:Let $\Phi\in\mathbb{R}^{m\times n}$ be a sparse Johnson-Lindenstrausstransform [KN14] with $s$ non-zeroes per column. For a subset $T$ of the unitsphere, $\varepsilon\in(0,1/2)$ given, we study settings for $m,s$ required toensure $$ \mathop{\mathbb{E}}_\Phi \sup_{x\in T} \left\\Phi x\_2^2 - 1\right < \varepsilon , $$ i.e. so that $\Phi$ preserves the norm of every$x\in T$ simultaneously and multiplicatively up to $1+\varepsilon$. Weintroduce a new complexity parameter, which depends on the geometry of $T$, andshow that it suffices to choose $s$ and $m$ such that this parameter is small.Our result is a sparse analog of Gordon's theorem, which was concerned with adense $\Phi$ having i.i.d. Gaussian entries. We qualitatively unify severalresults related to the Johnson-Lindenstrauss lemma, subspace embeddings, andFourier-based restricted isometries. Our work also implies new results in usingthe sparse Johnson-Lindenstrauss transform in numerical linear algebra,classical and model-based compressed sensing, manifold learning, andconstrained least squares problems such as the Lasso.
arxiv-12000-83 | Multiple kernel multivariate performance learning using cutting plane algorithm | http://arxiv.org/pdf/1508.06264v1.pdf | author:Jingbin Wang, Haoxiang Wang, Yihua Zhou, Nancy McDonald category:cs.LG cs.CV published:2015-08-25 summary:In this paper, we propose a multi-kernel classifier learning algorithm tooptimize a given nonlinear and nonsmoonth multivariate classifier performancemeasure. Moreover, to solve the problem of kernel function selection and kernelparameter tuning, we proposed to construct an optimal kernel by weighted linearcombination of some candidate kernels. The learning of the classifier parameterand the kernel weight are unified in a single objective function considering tominimize the upper boundary of the given multivariate performance measure. Theobjective function is optimized with regard to classifier parameter and kernelweight alternately in an iterative algorithm by using cutting plane algorithm.The developed algorithm is evaluated on two different pattern classificationmethods with regard to various multivariate performance measure optimizationproblems. The experiment results show the proposed algorithm outperforms thecompeting methods.
arxiv-12000-84 | A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares | http://arxiv.org/pdf/1406.5986v2.pdf | author:Garvesh Raskutti, Michael Mahoney category:stat.ML published:2014-06-23 summary:We consider statistical as well as algorithmic aspects of solving large-scaleleast-squares (LS) problems using randomized sketching algorithms. For a LSproblem with input data $(X, Y) \in \mathbb{R}^{n \times p} \times\mathbb{R}^n$, sketching algorithms use a sketching matrix, $S\in\mathbb{R}^{r\times n}$ with $r \ll n$. Then, rather than solving the LS problem using thefull data $(X,Y)$, sketching algorithms solve the LS problem using only thesketched data $(SX, SY)$. Prior work has typically adopted an algorithmicperspective, in that it has made no statistical assumptions on the input $X$and $Y$, and instead it has been assumed that the data $(X,Y)$ are fixed andworst-case (WC). Prior results show that, when using sketching matrices such asrandom projections and leverage-score sampling algorithms, with $p < r \ll n$,the WC error is the same as solving the original problem, up to a smallconstant. From a statistical perspective, we typically consider themean-squared error performance of randomized sketching algorithms, when data$(X, Y)$ are generated according to a statistical model $Y = X \beta +\epsilon$, where $\epsilon$ is a noise process. We provide a rigorouscomparison of both perspectives leading to insights on how they differ. To dothis, we first develop a framework for assessing algorithmic and statisticalaspects of randomized sketching methods. We then consider the statisticalprediction efficiency (PE) and the statistical residual efficiency (RE) of thesketched LS estimator; and we use our framework to provide upper bounds forseveral types of random projection and random sampling sketching algorithms.Among other results, we show that the RE can be upper bounded when $p < r \lln$ while the PE typically requires the sample size $r$ to be substantiallylarger. Lower bounds developed in subsequent results show that our upper boundson PE can not be improved.
arxiv-12000-85 | Inferring Passenger Type from Commuter Eigentravel Matrices | http://arxiv.org/pdf/1509.01199v1.pdf | author:Erika Fille Legara, Christopher Monterola category:physics.soc-ph cs.CY stat.AP stat.ML published:2015-08-25 summary:A sufficient knowledge of the demographics of a commuting public is essentialin formulating and implementing more targeted transportation policies, ascommuters exhibit different ways of traveling. With the advent of the AutomatedFare Collection system (AFC), probing the travel patterns of commuters hasbecome less invasive and more accessible. Consequently, numerous transportstudies related to human mobility have shown that these observed patterns allowone to pair individuals with locations and/or activities at certain times ofthe day. However, classifying commuters using their travel signatures is yet tobe thoroughly examined. Here, we contribute to the literature by demonstrating a procedure tocharacterize passenger types (Adult, Child/Student, and Senior Citizen) basedon their three-month travel patterns taken from a smart fare card system. Wefirst establish a method to construct distinct commuter matrices, which werefer to as eigentravel matrices, that capture the characteristic travelroutines of individuals. From the eigentravel matrices, we build classificationmodels that predict the type of passengers traveling. Among the modelsexplored, the gradient boosting method (GBM) gives the best prediction accuracyat 76%, which is 84% better than the minimum model accuracy (41%) requiredvis-\`a-vis the proportional chance criterion. In addition, we find that travelfeatures generated during weekdays have greater predictive power than those onweekends. This work should not only be useful for transport planners, but formarket researchers as well. With the awareness of which commuter types aretraveling, ads, service announcements, and surveys, among others, can be mademore targeted spatiotemporally. Finally, our framework should be effective increating synthetic populations for use in real-world simulations that involve ametropolitan's public transport system.
arxiv-12000-86 | Sparse 3D convolutional neural networks | http://arxiv.org/pdf/1505.02890v2.pdf | author:Ben Graham category:cs.CV published:2015-05-12 summary:We have implemented a convolutional neural network designed for processingsparse three-dimensional input data. The world we live in is three dimensionalso there are a large number of potential applications including 3D objectrecognition and analysis of space-time objects. In the quest for efficiency, weexperiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.
arxiv-12000-87 | BREN: Body Reflection Essence-Neuter Model for Separation of Reflection Components | http://arxiv.org/pdf/1508.06171v1.pdf | author:Changsoo Je, Hyung-Min Park category:cs.CV cs.GR physics.optics published:2015-08-25 summary:We propose a novel reflection color model consisting of body essence and(mixed) neuter, and present an effective method for separating dichromaticreflection components using a single image. Body essence is an entity invariantto interface reflection, and has two degrees of freedom unlike hue and maximumchromaticity. As a result, the proposed method is insensitive to noise andproper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red,green, and blue), contrary to the maximum chromaticity-based methods. Interfacereflection is separated by using a Gaussian function, which removes a criticalthresholding problem. Furthermore, the method does not require any regionsegmentation. Experimental results show the efficacy of the proposed model andmethod.
arxiv-12000-88 | Robot Language Learning, Generation, and Comprehension | http://arxiv.org/pdf/1508.06161v1.pdf | author:Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, Jeffrey Mark Siskind category:cs.RO cs.AI cs.CL cs.HC cs.LG published:2015-08-25 summary:We present a unified framework which supports grounding natural-languagesemantics in robotic driving. This framework supports acquisition (learninggrounded meanings of nouns and prepositions from human annotation of roboticdriving paths), generation (using such acquired meanings to generate sententialdescription of new robotic driving paths), and comprehension (using suchacquired meanings to support automated driving to accomplish navigational goalsspecified in natural language). We evaluate the performance of these threetasks by having independent human judges rate the semantic fidelity of thesentences associated with paths, achieving overall average correctness of 94.6%and overall average completeness of 85.6%.
arxiv-12000-89 | A Practical Guide to CNNs and Fisher Vectors for Image Instance Retrieval | http://arxiv.org/pdf/1508.02496v3.pdf | author:Vijay Chandrasekhar, Jie Lin, Olivier Mor√®re, Hanlin Goh, Antoine Veillard category:cs.CV cs.IR published:2015-08-11 summary:With deep learning becoming the dominant approach in computer vision, the useof representations extracted from Convolutional Neural Nets (CNNs) is quicklygaining ground on Fisher Vectors (FVs) as favoured state-of-the-art globalimage descriptors for image instance retrieval. While the good performance ofCNNs for image classification are unambiguously recognised, which of the twohas the upper hand in the image retrieval context is not entirely clear yet. Inthis work, we propose a comprehensive study that systematically evaluates FVsand CNNs for image retrieval. The first part compares the performances of FVsand CNNs on multiple publicly available data sets. We investigate a number ofdetails specific to each method. For FVs, we compare sparse descriptors basedon interest point detectors with dense single-scale and multi-scale variants.For CNNs, we focus on understanding the impact of depth, architecture andtraining data on retrieval results. Our study shows that no descriptor issystematically better than the other and that performance gains can usually beobtained by using both types together. The second part of the study focuses onthe impact of geometrical transformations such as rotations and scale changes.FVs based on interest point detectors are intrinsically resilient to suchtransformations while CNNs do not have a built-in mechanism to ensure suchinvariance. We show that performance of CNNs can quickly degrade in presence ofrotations while they are far less affected by changes in scale. We then proposea number of ways to incorporate the required invariances in the CNN pipeline.Overall, our work is intended as a reference guide offering practically usefuland simply implementable guidelines to anyone looking for state-of-the-artglobal descriptors best suited to their specific image instance retrievalproblem.
arxiv-12000-90 | OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based Neural Training | http://arxiv.org/pdf/1508.06095v1.pdf | author:Rossella Cancelliere, Mario Gai, Patrick Gallinari, Luca Rubini category:cs.NE cs.LG stat.ML published:2015-08-25 summary:In this paper we consider the training of single hidden layer neural networksby pseudoinversion, which, in spite of its popularity, is sometimes affected bynumerical instability issues. Regularization is known to be effective in suchcases, so that we introduce, in the framework of Tikhonov regularization, amatricial reformulation of the problem which allows us to use the conditionnumber as a diagnostic tool for identification of instability. By imposingwell-conditioning requirements on the relevant matrices, our theoreticalanalysis allows the identification of an optimal value for the regularizationparameter from the standpoint of stability. We compare with the value derivedby cross-validation for overfitting control and optimisation of thegeneralization performance. We test our method for both regression andclassification tasks. The proposed method is quite effective in terms ofpredictivity, often with some improvement on performance with respect to thereference cases considered. This approach, due to analytical determination ofthe regularization parameter, dramatically reduces the computational loadrequired by many other techniques.
arxiv-12000-91 | An analysis of numerical issues in neural training by pseudoinversion | http://arxiv.org/pdf/1508.06092v1.pdf | author:R. Cancelliere, R. Deluca, M. Gai, P. Gallinari, L. Rubini category:cs.LG cs.NE published:2015-08-25 summary:Some novel strategies have recently been proposed for single hidden layerneural network training that set randomly the weights from input to hiddenlayer, while weights from hidden to output layer are analytically determined bypseudoinversion. These techniques are gaining popularity in spite of theirknown numerical issues when singular and/or almost singular matrices areinvolved. In this paper we discuss a critical use of Singular Value Analysisfor identification of these drawbacks and we propose an original use ofregularisation to determine the output weights, based on the concept ofcritical hidden layer size. This approach also allows to limit the trainingcomputational effort. Besides, we introduce a novel technique which relies aneffective determination of input weights to the hidden layer dimension. Thisapproach is tested for both regression and classification tasks, resulting in asignificant performance improvement with respect to alternative methods.
arxiv-12000-92 | AUC Optimisation and Collaborative Filtering | http://arxiv.org/pdf/1508.06091v1.pdf | author:Charanpal Dhanjal, Romaric Gaudel, Stephan Clemencon category:stat.ML cs.LG published:2015-08-25 summary:In recommendation systems, one is interested in the ranking of the predicteditems as opposed to other losses such as the mean squared error. Although avariety of ways to evaluate rankings exist in the literature, here we focus onthe Area Under the ROC Curve (AUC) as it widely used and has a strongtheoretical underpinning. In practical recommendation, only items at the top ofthe ranked list are presented to the users. With this in mind, we propose aclass of objective functions over matrix factorisations which primarilyrepresent a smooth surrogate for the real AUC, and in a special case we showhow to prioritise the top of the list. The objectives are differentiable andoptimised through a carefully designed stochastic gradient-descent-basedalgorithm which scales linearly with the size of the data. In the special caseof square loss we show how to improve computational complexity by leveragingpreviously computed measures. To understand theoretically the underlying matrixfactorisation approaches we study both the consistency of the loss functionswith respect to AUC, and generalisation using Rademacher theory. The resultinggeneralisation analysis gives strong motivation for the optimisation understudy. Finally, we provide computation results as to the efficacy of theproposed method using synthetic and real data.
arxiv-12000-93 | Visualizing NLP annotations for Crowdsourcing | http://arxiv.org/pdf/1508.06044v1.pdf | author:Hanchuan Li, Haichen Shen, Shengliang Xu, Congle Zhang category:cs.CL published:2015-08-25 summary:Visualizing NLP annotation is useful for the collection of training data forthe statistical NLP approaches. Existing toolkits either provide limited visualaid, or introduce comprehensive operators to realize sophisticated linguisticrules. Workers must be well trained to use them. Their audience thus can hardlybe scaled to large amounts of non-expert crowdsourced workers. In this paper,we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers toannotate two general categories of NLP problems: clustering and parsing.Workers can finish the tasks with simplified operators in an interactiveinterface, and fix errors conveniently. User studies show our toolkit is veryfriendly to NLP non-experts, and allow them to produce high quality labels forseveral sophisticated problems. We release our source code and toolkit to spurfuture research.
arxiv-12000-94 | Better Summarization Evaluation with Word Embeddings for ROUGE | http://arxiv.org/pdf/1508.06034v1.pdf | author:Jun-Ping Ng, Viktoria Abrecht category:cs.CL cs.IR published:2015-08-25 summary:ROUGE is a widely adopted, automatic evaluation measure for textsummarization. While it has been shown to correlate well with human judgements,it is biased towards surface lexical similarities. This makes it unsuitable forthe evaluation of abstractive summarization, or summaries with substantialparaphrasing. We study the effectiveness of word embeddings to overcome thisdisadvantage of ROUGE. Specifically, instead of measuring lexical overlaps,word embeddings are used to compute the semantic similarity of the words usedin summaries instead. Our experimental results show that our proposal is ableto achieve better correlations with human judgements when measured with theSpearman and Kendall rank coefficients.
arxiv-12000-95 | Depth-Gated LSTM | http://arxiv.org/pdf/1508.03790v4.pdf | author:Kaisheng Yao, Trevor Cohn, Katerina Vylomova, Kevin Duh, Chris Dyer category:cs.NE cs.CL published:2015-08-16 summary:In this short note, we present an extension of long short-term memory (LSTM)neural networks to using a depth gate to connect memory cells of adjacentlayers. Doing so introduces a linear dependence between lower and upper layerrecurrent units. Importantly, the linear dependence is gated through a gatingfunction, which we call depth gate. This gate is a function of the lower layermemory cell, the input to and the past memory cell of this layer. We conductedexperiments and verified that this new architecture of LSTMs was able toimprove machine translation and language modeling performances.
arxiv-12000-96 | ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution | http://arxiv.org/pdf/1508.06013v1.pdf | author:Zeinab Bahmani, Leopoldo Bertossi, Nikolaos Vasiloglou category:cs.DB cs.AI cs.LG published:2015-08-25 summary:Entity resolution (ER), an important and common data cleaning problem, isabout detecting data duplicate representations for the same external entities,and merging them into single representations. Relatively recently, declarativerules called matching dependencies (MDs) have been proposed for specifyingsimilarity conditions under which attribute values in database records aremerged. In this work we show the process and the benefits of integrating threecomponents of ER: (a) Classifiers for duplicate/non-duplicate record pairsbuilt using machine learning (ML) techniques, (b) MDs for supporting both theblocking phase of ML and the merge itself; and (c) The use of the declarativelanguage LogiQL -an extended form of Datalog supported by the LogicBloxplatform- for data processing, and the specification and enforcement of MDs.
arxiv-12000-97 | Wavelet subspace decomposition of thermal infrared images for defect detection in artworks | http://arxiv.org/pdf/1508.06010v1.pdf | author:Muhammad Zubair Ahmad, Amir Ali Khan, Sihem Mezghani, Eric Perrin, Kamel Mouhoubi, Jean-Luc Bodnar, Valeriu Vrabie category:cs.CV published:2015-08-25 summary:Monitoring the health of ancient artworks requires adequate prudence becauseof the sensitive nature of these materials. Classical techniques foridentifying the development of faults rely on acoustic testing. Thesetechniques, being invasive, may result in causing permanent damage to thematerial, especially if the material is inspected periodically. Non destructivetesting has been carried out for different materials since long. In thisregard, non-invasive systems were developed based on infrared thermometryprinciple to identify the faults in artworks. The test artwork is heated andthe thermal response of the different layers is captured with the help of athermal infrared camera. However, prolonged heating risks overheating and thuscausing damage to artworks and an alternate approach is to use pseudo-randombinary sequence excitations. The faults in the artwork, though, cannot bedetected on the captured images, especially if their strength is weak. Theweaker faults are either masked by the stronger ones, by the pictorial layer ofthe artwork or by the non-uniform heating. This work addresses the detectionand localization of the faults through a wavelet based subspace decompositionscheme. The proposed scheme, on one hand, allows to remove the backgroundwhile, on the other hand, removes the undesired high frequency noise. It isshown that the detection parameter is proportional to the diameter and thedepth of the fault. A criterion is proposed to select the optimal wavelet basisalong with suitable level selection for wavelet decomposition andreconstruction. The proposed approach is tested on a laboratory developed testsample with known fault locations and dimensions as well as real artworks. Acomparison with a previously reported method demonstrates the efficacy of theproposed approach for fault detection in artworks.
arxiv-12000-98 | An algorithm for Left Atrial Thrombi detection using Transesophageal Echocardiography | http://arxiv.org/pdf/1508.05995v1.pdf | author:Jianrui Ding, Min Xian, H. D. Cheng, Yang Li, Fei Xu, Yingtao Zhang category:cs.CV published:2015-08-24 summary:Transesophageal echocardiography (TEE) is widely used to detect left atrium(LA)/left atrial appendage (LAA) thrombi. In this paper, the local binarypattern variance (LBPV) features are extracted from region of interest (ROI).And the dynamic features are formed by using the information of its neighborframes in the sequence. The sequence is viewed as a bag, and the images in thesequence are considered as the instances. Multiple-instance learning (MIL)method is employed to solve the LAA thrombi detection. The experimental resultsshow that the proposed method can achieve better performance than that by usingother methods.
arxiv-12000-99 | Eliciting Disease Data from Wikipedia Articles | http://arxiv.org/pdf/1504.00657v4.pdf | author:Geoffrey Fairchild, Lalindra De Silva, Sara Y. Del Valle, Alberto M. Segre category:cs.IR cs.CL cs.SI q-bio.PE published:2015-04-02 summary:Traditional disease surveillance systems suffer from several disadvantages,including reporting lags and antiquated technology, that have caused a movementtowards internet-based disease surveillance systems. Internet systems areparticularly attractive for disease outbreaks because they can provide data innear real-time and can be verified by individuals around the globe. However,most existing systems have focused on disease monitoring and do not provide adata repository for policy makers or researchers. In order to fill this gap, weanalyzed Wikipedia article content. We demonstrate how a named-entity recognizer can be trained to tag casecounts, death counts, and hospitalization counts in the article narrative thatachieves an F1 score of 0.753. We also show, using the 2014 West African Ebolavirus disease epidemic article as a case study, that there are detailed timeseries data that are consistently updated that closely align with ground truthdata. We argue that Wikipedia can be used to create the first community-drivenopen-source emerging disease detection, monitoring, and repository system.
arxiv-12000-100 | A Smoothed Dual Approach for Variational Wasserstein Problems | http://arxiv.org/pdf/1503.02533v2.pdf | author:Marco Cuturi, Gabriel Peyr√© category:stat.ML math.OC published:2015-03-09 summary:Variational problems that involve Wasserstein distances have been recentlyproposed to summarize and learn from probability measures. Despite beingconceptually simple, such problems are computationally challenging because theyinvolve minimizing over quantities (Wasserstein distances) that are themselveshard to compute. We show that the dual formulation of Wasserstein variationalproblems introduced recently by Carlier et al. (2014) can be regularized usingan entropic smoothing, which leads to smooth, differentiable, convexoptimization problems that are simpler to implement and numerically morestable. We illustrate the versatility of this approach by applying it to thecomputation of Wasserstein barycenters and gradient flows of spacialregularization functionals.
arxiv-12000-101 | Label optimal regret bounds for online local learning | http://arxiv.org/pdf/1503.02193v2.pdf | author:Pranjal Awasthi, Moses Charikar, Kevin A. Lai, Andrej Risteski category:cs.LG published:2015-03-07 summary:We resolve an open question from (Christiano, 2014b) posed in COLT'14regarding the optimal dependency of the regret achievable for online locallearning on the size of the label set. In this framework the algorithm is showna pair of items at each step, chosen from a set of $n$ items. The learner thenpredicts a label for each item, from a label set of size $L$ and receives areal valued payoff. This is a natural framework which captures many interestingscenarios such as collaborative filtering, online gambling, and online max cutamong others. (Christiano, 2014a) designed an efficient online learningalgorithm for this problem achieving a regret of $O(\sqrt{nL^3T})$, where $T$is the number of rounds. Information theoretically, one can achieve a regret of$O(\sqrt{n \log L T})$. One of the main open questions left in this frameworkconcerns closing the above gap. In this work, we provide a complete answer to the question above via two mainresults. We show, via a tighter analysis, that the semi-definite programmingbased algorithm of (Christiano, 2014a), in fact achieves a regret of$O(\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely,we show that a polynomial time algorithm for online local learning with lowerregret would imply a polynomial time algorithm for the planted clique problemwhich is widely believed to be hard. We prove a similar hardness result under arelated conjecture concerning planted dense subgraphs that we put forth. Unlikeplanted clique, the planted dense subgraph problem does not have any knownquasi-polynomial time algorithms. Computational lower bounds for online learning are relatively rare, and wehope that the ideas developed in this work will lead to lower bounds for otheronline learning scenarios as well.
arxiv-12000-102 | Another Look at DWD: Thrifty Algorithm and Bayes Risk Consistency in RKHS | http://arxiv.org/pdf/1508.05913v1.pdf | author:Boxiang Wang, Hui Zou category:stat.ML published:2015-08-24 summary:Distance weighted discrimination (DWD) is a margin-based classifier with aninteresting geometric motivation. DWD was originally proposed as a superioralternative to the support vector machine (SVM), however DWD is yet to bepopular compared with the SVM. The main reasons are twofold. First, thestate-of-the-art algorithm for solving DWD is based on the second-order-coneprogramming (SOCP), while the SVM is a quadratic programming problem which ismuch more efficient to solve. Second, the current statistical theory of DWDmainly focuses on the linear DWD for the high-dimension-low-sample-size settingand data-piling, while the learning theory for the SVM mainly focuses on theBayes risk consistency of the kernel SVM. In fact, the Bayes risk consistencyof DWD is presented as an open problem in the original DWD paper. In this work,we advance the current understanding of DWD from both computational andtheoretical perspectives. We propose a novel efficient algorithm for solvingDWD, and our algorithm can be several hundred times faster than the existingstate-of-the-art algorithm based on the SOCP. In addition, our algorithm canhandle the generalized DWD, while the SOCP algorithm only works well for aspecial DWD but not the generalized DWD. Furthermore, we consider a naturalkernel DWD in a reproducing kernel Hilbert space and then establish the Bayesrisk consistency of the kernel DWD. We compare DWD and the SVM on severalbenchmark data sets and show that the two have comparable classificationaccuracy, but DWD equipped with our new algorithm can be much faster to computethan the SVM.
arxiv-12000-103 | A Framework for Comparing Groups of Documents | http://arxiv.org/pdf/1508.05902v1.pdf | author:Arun S. Maiya category:cs.CL cs.SI I.2.7 published:2015-08-24 summary:We present a general framework for comparing multiple groups of documents. Abipartite graph model is proposed where document groups are represented as onenode set and the comparison criteria are represented as the other node set.Using this model, we present basic algorithms to extract insights intosimilarities and differences among the document groups. Finally, we demonstratethe versatility of our framework through an analysis of NSF funding programsfor basic research.
arxiv-12000-104 | Optical images-based edge detection in Synthetic Aperture Radar images | http://arxiv.org/pdf/1508.05879v1.pdf | author:Gilberto P. Silva Junior, Alejandro C. Frery, Sandra Sandri, Humberto Bustince, Edurne Barrenechea, C√©dric Marco-Detchart category:cs.CV published:2015-08-24 summary:We address the issue of adapting optical images-based edge detectiontechniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.We modify the gravitational edge detection technique (inspired by the Law ofUniversal Gravity) proposed by Lopez-Molina et al, using the non-standardneighbourhood configuration proposed by Fu et al, to reduce the speckle noisein polarimetric SAR imagery. We compare the modified and unmodified versions ofthe gravitational edge detection technique with the well-established oneproposed by Canny, as well as with a recent multiscale fuzzy-based techniqueproposed by Lopez-Molina et Alejandro We also address the issues of aggregationof gray level images before and after edge detection and of filtering. Alltechniques addressed here are applied to a mosaic built using classdistributions obtained from a real scene, as well as to the true PolSAR image;the mosaic results are assessed using Baddeley's Delta Metric. Our experimentsshow that modifying the gravitational edge detection technique with anon-standard neighbourhood configuration produces better results than theoriginal technique, as well as the other techniques used for comparison. Theexperiments show that adapting edge detection methods from ComputationalIntelligence for use in PolSAR imagery is a new field worthy of exploration.
arxiv-12000-105 | A Survey of Multithreading Image Analysis | http://arxiv.org/pdf/1506.04472v5.pdf | author:Elham Sagheb Hossein Pour category:cs.CV published:2015-06-15 summary:Digital image analysis has made a big advance in many areas of enterpriseapplications including medicine, industry, and entertainment by assisting theextraction of semantic information from digital images. However, its largecomputational complexity has been a trouble to most real-time developments.While image analysis in general has been studied for a log period in computerscience community, the use of multithreading strategy as the most efficientimproving computational capacity technique has been limited so far. In thissurvey an attempt is made to explain the current knowledge and so farprogresses in incorporating image analysis with multithreading approaches. Thepresent work also provides insights and tendencies for the possible futureenhancement of multithreading image analysis.
arxiv-12000-106 | Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for Stationary Gaussian Inputs and Slow Learning | http://arxiv.org/pdf/1508.05873v1.pdf | author:Jingen Ni, Jian Yang, Jie Chen, C√©dric Richard, Jos√© Carlos M. Bermudez category:cs.NA cs.LG published:2015-08-24 summary:Some system identification problems impose nonnegativity constraints on theparameters to estimate due to inherent physical characteristics of the unknownsystem. The nonnegative least-mean-square (NNLMS) algorithm and its variantsallow to address this problem in an online manner. A nonnegative least meanfourth (NNLMF) algorithm has been recently proposed to improve the performanceof these algorithms in cases where the measurement noise is not Gaussian. Thispaper provides a first theoretical analysis of the stochastic behavior of theNNLMF algorithm for stationary Gaussian inputs and slow learning. Simulationresults illustrate the accuracy of the proposed analysis.
arxiv-12000-107 | Echoes of Persuasion: The Effect of Euphony in Persuasive Communication | http://arxiv.org/pdf/1508.05817v1.pdf | author:Marco Guerini, G√∂zde √ñzbal, Carlo Strapparava category:cs.CL cs.CY cs.SI published:2015-08-24 summary:While the effect of various lexical, syntactic, semantic and stylisticfeatures have been addressed in persuasive language from a computational pointof view, the persuasive effect of phonetics has received little attention. Bymodeling a notion of euphony and analyzing four datasets comprising persuasiveand non-persuasive sentences in different domains (political speeches, moviequotes, slogans and tweets), we explore the impact of sounds on different formsof persuasiveness. We conduct a series of analyses and prediction experimentswithin and across datasets. Our results highlight the positive role of phoneticdevices on persuasion.
arxiv-12000-108 | Searching for significant patterns in stratified data | http://arxiv.org/pdf/1508.05803v1.pdf | author:Felipe Llinares-Lopez, Laetitia Papaxanthos, Dean Bodenham, Karsten Borgwardt category:stat.ML cs.LG published:2015-08-24 summary:Significant pattern mining, the problem of finding itemsets that aresignificantly enriched in one class of objects, is statistically challenging,as the large space of candidate patterns leads to an enormous multiple testingproblem. Recently, the concept of testability was proposed as one approach tocorrect for multiple testing in pattern mining while retaining statisticalpower. Still, these strategies based on testability do not allow one tocondition the test of significance on the observed covariates, which severelylimits its utility in biomedical applications. Here we propose a strategy andan efficient algorithm to perform significant pattern mining in the presence ofcategorical covariates with K states.
arxiv-12000-109 | An evolutionary approach to the identification of Cellular Automata based on partial observations | http://arxiv.org/pdf/1508.05752v1.pdf | author:Witold Bo≈Çt, Jan M. Baetens, Bernard De Baets category:cs.NE nlin.CG published:2015-08-24 summary:In this paper we consider the identification problem of Cellular Automata(CAs). The problem is defined and solved in the context of partial observationswith time gaps of unknown length, i.e. pre-recorded, partial configurations ofthe system at certain, unknown time steps. A solution method based on amodified variant of a Genetic Algorithm (GA) is proposed and illustrated withbrief experimental results.
arxiv-12000-110 | An Experimental Comparison of Hybrid Algorithms for Bayesian Network Structure Learning | http://arxiv.org/pdf/1505.05004v2.pdf | author:Maxime Gasse, Alex Aussem, Haytham Elghazel category:stat.ML cs.AI cs.LG published:2015-05-19 summary:We present a novel hybrid algorithm for Bayesian network structure learning,called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesiannetwork and then performs a Bayesian-scoring greedy hill-climbing search toorient the edges. It is based on a subroutine called HPC, that combines ideasfrom incremental and divide-and-conquer constraint-based methods to learn theparents and children of a target variable. We conduct an experimentalcomparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently themost powerful state-of-the-art algorithm for Bayesian network structurelearning, on several benchmarks with various data sizes. Our extensiveexperiments show that H2PC outperforms MMHC both in terms of goodness of fit tonew data and in terms of the quality of the network structure itself, which iscloser to the true dependence structure of the data. The source code (in R) ofH2PC as well as all data sets used for the empirical tests are publiclyavailable.
arxiv-12000-111 | Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual Sentiment Prediction | http://arxiv.org/pdf/1508.05056v2.pdf | author:Victor Campos, Amaia Salvador, Brendan Jou, Xavier Gir√≥-i-Nieto category:cs.MM cs.CV I.2.10; H.1.2 published:2015-08-20 summary:Visual media are powerful means of expressing emotions and sentiments. Theconstant generation of new content in social networks highlights the need ofautomated visual sentiment analysis tools. While Convolutional Neural Networks(CNNs) have established a new state-of-the-art in several vision problems,their application to the task of sentiment analysis is mostly unexplored andthere are few studies regarding how to design CNNs for this purpose. In thiswork, we study the suitability of fine-tuning a CNN for visual sentimentprediction as well as explore performance boosting techniques within this deeplearning setting. Finally, we provide a deep-dive analysis into a benchmark,state-of-the-art network architecture to gain insight about how to designpatterns for CNNs on the task of visual sentiment prediction.
arxiv-12000-112 | Fast Asynchronous Parallel Stochastic Gradient Decent | http://arxiv.org/pdf/1508.05711v1.pdf | author:Shen-Yi Zhao, Wu-Jun Li category:stat.ML cs.LG published:2015-08-24 summary:Stochastic gradient descent~(SGD) and its variants have become more and morepopular in machine learning due to their efficiency and effectiveness. Tohandle large-scale problems, researchers have recently proposed severalparallel SGD methods for multicore systems. However, existing parallel SGDmethods cannot achieve satisfactory performance in real applications. In thispaper, we propose a fast asynchronous parallel SGD method, called AsySVRG, bydesigning an asynchronous strategy to parallelize the recently proposed SGDvariant called stochastic variance reduced gradient~(SVRG). Both theoreticaland empirical results show that AsySVRG can outperform existingstate-of-the-art parallel SGD methods like Hogwild! in terms of convergencerate and computation cost.
arxiv-12000-113 | Iterative Thresholded Bi-Histogram Equalization for Medical Image Enhancement | http://arxiv.org/pdf/1508.05704v1.pdf | author:Muhammad Ali Qadar, Yan Zhaowen, Li Hua category:cs.CV published:2015-08-24 summary:Enhancement of human vision to get an insight to information content is ofvital importance. The traditional histogram equalization methods have beensuffering from amplified contrast with the addition of artifacts and asurprising unnatural visibility of the processed images. In order to overcomethese drawbacks, this paper proposes interative, mean, and multi-thresholdselection criterion with plateau limits, which consist of histogramsegmentation, clipping and transformation modules. The histogram partitionconsists of multiple thresholding processes that divide the histogram into twoparts, whereas the clipping process nicely enhances the contrast by having acheck on the rate of enhancement that could be tuned. Histogram equalization toeach segmented sub-histogram provides the output image with preservedbrightness and enhanced contrast. Results of the present study showed that theproposed method efficiently handles the noise amplification. Further, it alsopreserves the brightness by retaining natural look of targeted image.
arxiv-12000-114 | Robust Visual Tracking via Convolutional Networks | http://arxiv.org/pdf/1501.04505v2.pdf | author:Kaihua Zhang, Qingshan Liu, Yi Wu, Ming-Hsuan Yang category:cs.CV published:2015-01-19 summary:Deep networks have been successfully applied to visual tracking by learning ageneric representation offline from numerous training images. However theoffline training is time-consuming and the learned generic representation maybe less discriminative for tracking specific objects. In this paper we presentthat, even without offline training with a large amount of auxiliary data,simple two-layer convolutional networks can be powerful enough to develop arobust representation for visual tracking. In the first frame, we employ thek-means algorithm to extract a set of normalized patches from the target regionas fixed filters, which integrate a series of adaptive contextual filterssurrounding the target to define a set of feature maps in the subsequentframes. These maps measure similarities between each filter and the usefullocal intensity patterns across the target, thereby encoding its localstructural information. Furthermore, all the maps form together a globalrepresentation, which is built on mid-level features, thereby remaining closeto image-level information, and hence the inner geometric layout of the targetis also well preserved. A simple soft shrinkage method with an adaptivethreshold is employed to de-noise the global representation, resulting in arobust sparse representation. The representation is updated via a simple andeffective online strategy, allowing it to robustly adapt to target appearancevariations. Our convolution networks have surprisingly lightweight structure,yet perform favorably against several state-of-the-art methods on the CVPR2013tracking benchmark dataset with 50 challenging videos.
arxiv-12000-115 | Morphometry-Based Longitudinal Neurodegeneration Simulation with MR Imaging | http://arxiv.org/pdf/1508.05683v1.pdf | author:Siqi Liu, Sidong Liu, Sonia Pujol, Ron Kikinis, Dagan Feng, Michael Fulham, Weidong Cai category:cs.CV published:2015-08-24 summary:We present a longitudinal MR simulation framework which simulates the futureneurodegenerative progression by outputting the predicted follow-up MR imageand the voxel-based morphometry (VBM) map. This framework expects the patientsto have at least 2 historical MR images available. The longitudinal andcross-sectional VBM maps are extracted to measure the affinity between thetarget subject and the template subjects collected for simulation. Then thefollow-up simulation is performed by resampling the latest available target MRimage with a weighted sum of non-linear transformations derived from thebest-matched templates. The leave-one-out strategy was used to comparedifferent simulation methods. Compared to the state-of-the-art voxel-basedmethod, our proposed morphometry-based simulation achieves better accuracy inmost cases.
arxiv-12000-116 | Convex Calibration Dimension for Multiclass Loss Matrices | http://arxiv.org/pdf/1408.2764v2.pdf | author:Harish G. Ramaswamy, Shivani Agarwal category:cs.LG stat.ML published:2014-08-12 summary:We study consistency properties of surrogate loss functions for generalmulticlass learning problems, defined by a general multiclass loss matrix. Weextend the notion of classification calibration, which has been studied forbinary and multiclass 0-1 classification problems (and for certain otherspecific learning problems), to the general multiclass setting, and derivenecessary and sufficient conditions for a surrogate loss to be calibrated withrespect to a loss matrix in this setting. We then introduce the notion ofconvex calibration dimension of a multiclass loss matrix, which measures thesmallest `size' of a prediction space in which it is possible to design aconvex surrogate that is calibrated with respect to the loss matrix. We deriveboth upper and lower bounds on this quantity, and use these results to analyzevarious loss matrices. In particular, we apply our framework to study varioussubset ranking losses, and use the convex calibration dimension as a tool toshow both the existence and non-existence of various types of convex calibratedsurrogates for these losses. Our results strengthen recent results of Duchi etal. (2010) and Calauzenes et al. (2012) on the non-existence of certain typesof convex calibrated surrogates in subset ranking. We anticipate the convexcalibration dimension may prove to be a useful tool in the study and design ofsurrogate losses for general multiclass learning problems.
arxiv-12000-117 | The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms | http://arxiv.org/pdf/1508.05608v1.pdf | author:Yahel David, Nahum Shimkin category:stat.ML cs.AI cs.LG published:2015-08-23 summary:We consider the Max $K$-Armed Bandit problem, where a learning agent is facedwith several sources (arms) of items (rewards), and interested in finding thebest item overall. At each time step the agent chooses an arm, and obtains arandom real valued reward. The rewards of each arm are assumed to be i.i.d.,with an unknown probability distribution that generally differs among the arms.Under the PAC framework, we provide lower bounds on the sample complexity ofany $(\epsilon,\delta)$-correct algorithm, and propose algorithms that attainthis bound up to logarithmic factors. We compare the performance of thismulti-arm algorithms to the variant in which the arms are not distinguishableby the agent and are chosen randomly at each stage. Interestingly, when themaximal rewards of the arms happen to be similar, the latter approach mayprovide better performance.
arxiv-12000-118 | Emphatic TD Bellman Operator is a Contraction | http://arxiv.org/pdf/1508.03411v2.pdf | author:Assaf Hallak, Aviv Tamar, Shie Mannor category:stat.ML cs.LG published:2015-08-14 summary:Recently, \citet{SuttonMW15} introduced the emphatic temporal differences(ETD) algorithm for off-policy evaluation in Markov decision processes. In thisshort note, we show that the projected fixed-point equation that underlies ETDinvolves a contraction operator, with a $\sqrt{\gamma}$-contraction modulus(where $\gamma$ is the discount factor). This allows us to provide error boundson the approximation error of ETD. To our knowledge, these are the first errorbounds for an off-policy evaluation algorithm under general target and behaviorpolicies.
arxiv-12000-119 | Light Efficient Flutter Shutter | http://arxiv.org/pdf/1509.01220v1.pdf | author:Moshe Ben-Ezra category:cs.GR cs.CV published:2015-08-23 summary:Flutter shutter is a technique in which the exposure is chopped into segmentsand light is only integrated part of the time. By carefully selecting thechopping sequence it is possible to better condition the data forreconstruction problems such as motion deblurring, focal sweeping, andcompressed sensing. The partial exposure trades better conditioning for lessenergy. In problems such as motion deblurring the available energy is whatcaused the problem in the first place (as strong illumination allows shortexposure thus eliminates motion blur). It is still beneficial because thebenefit from the better conditioning outweighs the cost in energy. This documents is focused on light efficient flutter shutter that providesbetter conditioning and better energy utilization than conventional fluttershutter.
arxiv-12000-120 | Confounds and Consequences in Geotagged Twitter Data | http://arxiv.org/pdf/1506.02275v2.pdf | author:Umashanthi Pavalanathan, Jacob Eisenstein category:cs.CL published:2015-06-07 summary:Twitter is often used in quantitative studies that identifygeographically-preferred topics, writing styles, and entities. These studiesrely on either GPS coordinates attached to individual messages, or on theuser-supplied location field in each profile. In this paper, we compare thesedata acquisition techniques and quantify the biases that they introduce; wealso measure their effects on linguistic analysis and text-based geolocation.GPS-tagging and self-reported locations yield measurably different corpora, andthese linguistic differences are partially attributable to differences indataset composition by age and gender. Using a latent variable model to induceage and gender, we show how these demographic variables interact with geographyto affect language use. We also show that the accuracy of text-basedgeolocation varies with population demographics, giving the best results formen above the age of 40.
arxiv-12000-121 | Leveraging Monolingual Data for Crosslingual Compositional Word Representations | http://arxiv.org/pdf/1412.6334v4.pdf | author:Hubert Soyer, Pontus Stenetorp, Akiko Aizawa category:cs.CL published:2014-12-19 summary:In this work, we present a novel neural network based architecture forinducing compositional crosslingual word representations. Unlike previouslyproposed methods, our method fulfills the following three criteria; itconstrains the word-level representations to be compositional, it is capable ofleveraging both bilingual and monolingual data, and it is scalable to largevocabularies and large quantities of data. The key component of our approach iswhat we refer to as a monolingual inclusion criterion, that exploits theobservation that phrases are more closely semantically related to theirsub-phrases than to other randomly sampled phrases. We evaluate our method on awell-established crosslingual document classification task and achieve resultsthat are either comparable, or greatly improve upon previous state-of-the-artmethods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy forthe English to German and German to English sub-tasks respectively. The formeradvances the state of the art by 0.9% points of accuracy, the latter is anabsolute improvement upon the previous state of the art by 7.7% points ofaccuracy and an improvement of 33.0% in error reduction.
arxiv-12000-122 | Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence | http://arxiv.org/pdf/1508.05514v1.pdf | author:Tohid Ardeshiri, Umut Orguner, Emre √ñzkan category:stat.ML cs.CV cs.LG cs.RO cs.SY published:2015-08-22 summary:We propose a greedy mixture reduction algorithm which is capable of pruningmixture components as well as merging them based on the Kullback-Leiblerdivergence (KLD). The algorithm is distinct from the well-known Runnalls' KLDbased method since it is not restricted to merging operations. The capabilityof pruning (in addition to merging) gives the algorithm the ability ofpreserving the peaks of the original mixture during the reduction. Analyticalapproximations are derived to circumvent the computational intractability ofthe KLD which results in a computationally efficient method. The proposedalgorithm is compared with Runnalls' and Williams' methods in two numericalexamples, using both simulated and real world data. The results indicate thatthe performance and computational complexity of the proposed approach make itan efficient alternative to existing mixture reduction methods.
arxiv-12000-123 | Towards Neural Network-based Reasoning | http://arxiv.org/pdf/1508.05508v1.pdf | author:Baolin Peng, Zhengdong Lu, Hang Li, Kam-Fai Wong category:cs.AI cs.CL cs.LG cs.NE published:2015-08-22 summary:We propose Neural Reasoner, a framework for neural network-based reasoningover natural language sentences. Given a question, Neural Reasoner can inferover multiple supporting facts and find an answer to the question in specificforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,allowing it to examine multiple facts, and 2) a deep architecture, allowing itto model the complicated logical relations in reasoning tasks. Assuming noparticular structure exists in the question and facts, Neural Reasoner is ableto accommodate different types of reasoning and different forms of languageexpressions. Despite the model complexity, Neural Reasoner can still be trainedeffectively in an end-to-end manner. Our empirical studies show that NeuralReasoner can outperform existing neural reasoning systems with remarkablemargins on two difficult artificial tasks (Positional Reasoning and PathFinding) proposed in [8]. For example, it improves the accuracy on PathFinding(10K) from 33.4% [6] to over 98%.
arxiv-12000-124 | Sparse and spurious: dictionary learning with noise and outliers | http://arxiv.org/pdf/1407.5155v4.pdf | author:R√©mi Gribonval, Rodolphe Jenatton, Francis Bach category:cs.LG stat.ML published:2014-07-19 summary:A popular approach within the signal processing and machine learningcommunities consists in modelling signals as sparse linear combinations ofatoms selected from a learned dictionary. While this paradigm has led tonumerous empirical successes in various fields ranging from image to audioprocessing, there have only been a few theoretical arguments supporting theseevidences. In particular, sparse coding, or sparse dictionary learning, relieson a non-convex procedure whose local minima have not been fully analyzed yet.In this paper, we consider a probabilistic model of sparse signals, and showthat, with high probability, sparse coding admits a local minimum around thereference dictionary generating the signals. Our study takes into account thecase of over-complete dictionaries, noisy signals, and possible outliers, thusextending previous work limited to noiseless settings and/or under-completedictionaries. The analysis we conduct is non-asymptotic and makes it possibleto understand how the key quantities of the problem, such as the coherence orthe level of noise, can scale with respect to the dimension of the signals, thenumber of atoms, the sparsity and the number of observations.
arxiv-12000-125 | On some provably correct cases of variational inference for topic models | http://arxiv.org/pdf/1503.06567v2.pdf | author:Pranjal Awasthi, Andrej Risteski category:cs.LG cs.DS stat.ML published:2015-03-23 summary:Variational inference is a very efficient and popular heuristic used invarious forms in the context of latent variable models. It's closely related toExpectation Maximization (EM), and is applied when exact EM is computationallyinfeasible. Despite being immensely popular, current theoretical understandingof the effectiveness of variaitonal inference based algorithms is very limited.In this work we provide the first analysis of instances where variationalinference algorithms converge to the global optimum, in the setting of topicmodels. More specifically, we show that variational inference provably learns theoptimal parameters of a topic model under natural assumptions on the topic-wordmatrix and the topic priors. The properties that the topic word matrix mustsatisfy in our setting are related to the topic expansion assumption introducedin (Anandkumar et al., 2013), as well as the anchor words assumption in (Aroraet al., 2012c). The assumptions on the topic priors are related to the wellknown Dirichlet prior, introduced to the area of topic modeling by (Blei etal., 2003). It is well known that initialization plays a crucial role in how wellvariational based algorithms perform in practice. The initializations that weuse are fairly natural. One of them is similar to what is currently used inLDA-c, the most popular implementation of variational inference for topicmodels. The other one is an overlapping clustering algorithm, inspired by awork by (Arora et al., 2014) on dictionary learning, which is very simple andefficient. While our primary goal is to provide insights into when variational inferencemight work in practice, the multiplicative, rather than the additive nature ofthe variational inference updates forces us to use fairly non-standard proofarguments, which we believe will be of general interest.
arxiv-12000-126 | Bayesian Hypothesis Testing for Block Sparse Signal Recovery | http://arxiv.org/pdf/1508.05495v1.pdf | author:Mehdi Korki, Hadi Zayyani, Jingxin Zhang category:stat.ML cs.IT math.IT published:2015-08-22 summary:This letter presents a novel Block Bayesian Hypothesis Testing Algorithm(Block-BHTA) for reconstructing block sparse signals with unknown blockstructures. The Block-BHTA comprises the detection and recovery of thesupports, and the estimation of the amplitudes of the block sparse signal. Thesupport detection and recovery is performed using a Bayesian hypothesistesting. Then, based on the detected and reconstructed supports, the nonzeroamplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA isdemonstrated by numerical experiments.
arxiv-12000-127 | A Self-Adaptive Network Protection System | http://arxiv.org/pdf/1405.1958v2.pdf | author:Mohamed Hassan category:cs.NE cs.AI cs.CR published:2014-05-08 summary:In this treatise we aim to build a hybrid network automated (self-adaptive)security threats discovery and prevention system; by using unconventionaltechniques and methods, including fuzzy logic and biological inspiredalgorithms under the context of soft computing.
arxiv-12000-128 | Parallel software implementation of recursive multidimensional digital filters for point-target detection in cluttered infrared scenes | http://arxiv.org/pdf/1408.3526v4.pdf | author:Hugh L. Kennedy category:cs.CV published:2014-08-15 summary:A technique for the enhancement of point targets in clutter is described. Thelocal 3-D spectrum at each pixel is estimated recursively. An opticalflow-field for the textured background is then generated using the 3-Dautocorrelation function and the local velocity estimates are used to applyhigh-pass velocity-selective spatiotemporal filters, with finite impulseresponses (FIRs), to subtract the background clutter signal, leaving theforeground target signal, plus noise. Parallel software implementations using amulticore central processing unit (CPU) and a graphical processing unit (GPU)are investigated.
arxiv-12000-129 | Random Sampling in an Age of Automation: Minimizing Expenditures through Balanced Collection and Annotation | http://arxiv.org/pdf/1410.7074v4.pdf | author:Oscar Beijbom category:cs.CY cs.LG stat.ME published:2014-10-26 summary:Methods for automated collection and annotation are changing thecost-structures of sampling surveys for a wide range of applications. Digitalsamples in the form of images or audio recordings can be collected rapidly, andannotated by computer programs or crowd workers. We consider the problem ofestimating a population mean under these new cost-structures, and propose aHybrid-Offset sampling design. This design utilizes two annotators: a primary,which is accurate but costly (e.g. a human expert) and an auxiliary which isnoisy but cheap (e.g. a computer program), in order to minimize total samplingexpenditures. Our analysis gives necessary conditions for the Hybrid-Offsetdesign and specifies optimal sample sizes for both annotators. Simulations ondata from a coral reef survey program indicate that the Hybrid-Offset designoutperforms several alternative sampling designs. In particular, samplingexpenditures are reduced 50% compared to the Conventional design currentlydeployed by the coral ecologists.
arxiv-12000-130 | A large annotated corpus for learning natural language inference | http://arxiv.org/pdf/1508.05326v1.pdf | author:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning category:cs.CL published:2015-08-21 summary:Understanding entailment and contradiction is fundamental to understandingnatural language, and inference about entailment and contradiction is avaluable testing ground for the development of semantic representations.However, machine learning research in this area has been dramatically limitedby the lack of large-scale resources. To address this, we introduce theStanford Natural Language Inference corpus, a new, freely available collectionof labeled sentence pairs, written by humans doing a novel grounded task basedon image captioning. At 570K pairs, it is two orders of magnitude larger thanall other resources of its type. This increase in scale allows lexicalizedclassifiers to outperform some sophisticated existing entailment models, and itallows a neural network-based model to perform competitively on naturallanguage inference benchmarks for the first time.
arxiv-12000-131 | Exemplar Based Deep Discriminative and Shareable Feature Learning for Scene Image Classification | http://arxiv.org/pdf/1508.05306v1.pdf | author:Zhen Zuo, Gang Wang, Bing Shuai, Lifan Zhao, Qingxiong Yang category:cs.CV published:2015-08-21 summary:In order to encode the class correlation and class specific information inimage representation, we propose a new local feature learning approach namedDeep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims tohierarchically learn feature transformation filter banks to transform raw pixelimage patches to features. The learned filter banks are expected to: (1) encodecommon visual patterns of a flexible number of categories; (2) encodediscriminative information; and (3) hierarchically extract patterns atdifferent visual levels. Particularly, in each single layer of DDSFL, shareablefilters are jointly learned for classes which share the similar patterns.Discriminative power of the filters is achieved by enforcing the features fromthe same category to be close, while features from different categories to befar away from each other. Furthermore, we also propose two exemplar selectionmethods to iteratively select training data for more efficient and effectivelearning. Based on the experimental results, DDSFL can achieve very promisingperformance, and it also shows great complementary effect to thestate-of-the-art Caffe features.
arxiv-12000-132 | Robust Subspace Clustering via Thresholding | http://arxiv.org/pdf/1307.4891v4.pdf | author:Reinhard Heckel, Helmut B√∂lcskei category:stat.ML cs.IT cs.LG math.IT published:2013-07-18 summary:The problem of clustering noisy and incompletely observed high-dimensionaldata points into a union of low-dimensional subspaces and a set of outliers isconsidered. The number of subspaces, their dimensions, and their orientationsare assumed unknown. We propose a simple low-complexity subspace clusteringalgorithm, which applies spectral clustering to an adjacency matrix obtained bythresholding the correlations between data points. In other words, theadjacency matrix is constructed from the nearest neighbors of each data pointin spherical distance. A statistical performance analysis shows that thealgorithm exhibits robustness to additive noise and succeeds even when thesubspaces intersect. Specifically, our results reveal an explicit tradeoffbetween the affinity of the subspaces and the tolerable noise level. Wefurthermore prove that the algorithm succeeds even when the data points areincompletely observed with the number of missing entries allowed to be (up to alog-factor) linear in the ambient dimension. We also propose a simple schemethat provably detects outliers, and we present numerical results on real andsynthetic data.
arxiv-12000-133 | Representation of Quasi-Monotone Functionals by Families of Separating Hyperplanes | http://arxiv.org/pdf/1508.05249v1.pdf | author:Ingo Steinwart category:math.OC math.FA math.ST stat.ML stat.TH published:2015-08-21 summary:We characterize when the level sets of a continuous quasi-monotone functionaldefined on a suitable convex subset of a normed space can be uniquelyrepresented by a family of bounded continuous functionals. Furthermore, weinvestigate how regularly these functionals depend on the parameterizing level.Finally, we show how this question relates to the recent problem of propertyelicitation that simultaneously attracted interest in machine learning,statistical evaluation of forecasts, and finance.
arxiv-12000-134 | Recurrent Neural Network Based Modeling of Gene Regulatory Network Using Bat Algorithm | http://arxiv.org/pdf/1509.03221v1.pdf | author:Sudip Mandal, Goutam Saha, Rajat K. Pal category:cs.AI cs.NE published:2015-08-21 summary:Correct inference of genetic regulations inside a cell is one of the greatestchallenges in post genomic era for the biologist and researchers. Severalintelligent techniques and models were already proposed to identify theregulatory relations among genes from the biological database like time seriesmicroarray data. Recurrent Neural Network (RNN) is one of the most popular andsimple approach to model the dynamics as well as to infer correct dependenciesamong genes. In this paper, Bat Algorithm (BA) was applied to optimize themodel parameters of RNN model of Gene Regulatory Network (GRN). Initially theproposed method is tested against small artificial network without any noiseand the efficiency was observed in term of number of iteration, number ofpopulation and BA optimization parameters. The model was also validated inpresence of different level of random noise for the small artificial networkand that proved its ability to infer the correct inferences in presence ofnoise like real world dataset. In the next phase of this research, BA based RNNis applied to real world benchmark time series microarray dataset of E. Coli.The results shown that it can able to identify the maximum true positiveregulation but also include some false positive regulations. Therefore, BA isvery suitable for identifying biological plausible GRN with the help RNN model
arxiv-12000-135 | High-dimensional quadratic classifiers in non-sparse settings | http://arxiv.org/pdf/1503.04549v2.pdf | author:Makoto Aoshima, Kazuyoshi Yata category:stat.ML math.ST stat.TH published:2015-03-16 summary:We consider high-dimensional quadratic classifiers in non-sparse settings.The target of classification rules is not Bayes error rates in the context. Theclassifier based on the Mahalanobis distance does not always give a preferableperformance even if the populations are normal distributions having knowncovariance matrices. The quadratic classifiers proposed in this paper drawinformation about heterogeneity effectively through both the differences ofexpanding mean vectors and covariance matrices. We show that they hold aconsistency property in which misclassification rates tend to zero as thedimension goes to infinity under non-sparse settings. We verify that they areasymptotically distributed as a normal distribution under certain conditions.We also propose a quadratic classifier after feature selection by using boththe differences of mean vectors and covariance matrices. Finally, we discussperformances of the classifiers in actual data analyses. The proposedclassifiers achieve highly accurate classification with very low computationalcosts.
arxiv-12000-136 | Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification | http://arxiv.org/pdf/1508.04535v2.pdf | author:Ruimao Zhang, Liang Lin, Rui Zhang, Wangmeng Zuo, Lei Zhang category:cs.CV published:2015-08-19 summary:Extracting informative image features and learning effective approximatehashing functions are two crucial steps in image retrieval . Conventionalmethods often study these two steps separately, e.g., learning hash functionsfrom a predefined hand-crafted feature space. Meanwhile, the bit lengths ofoutput hashing codes are preset in most previous methods, neglecting thesignificance level of different bits and restricting their practicalflexibility. To address these issues, we propose a supervised learningframework to generate compact and bit-scalable hashing codes directly from rawimages. We pose hashing learning as a problem of regularized similaritylearning. Specifically, we organize the training images into a batch of tripletsamples, each sample containing two images with the same label and one with adifferent label. With these triplet samples, we maximize the margin betweenmatched pairs and mismatched pairs in the Hamming space. In addition, aregularization term is introduced to enforce the adjacency consistency, i.e.,images of similar appearances should have similar codes. The deep convolutionalneural network is utilized to train the model in an end-to-end fashion, wherediscriminative image features and hash functions are simultaneously optimized.Furthermore, each bit of our hashing codes is unequally weighted so that we canmanipulate the code lengths by truncating the insignificant bits. Our frameworkoutperforms state-of-the-arts on public benchmarks of similar image search andalso achieves promising results in the application of person re-identificationin surveillance. It is also shown that the generated bit-scalable hashing codeswell preserve the discriminative powers with shorter code lengths.
arxiv-12000-137 | Adaptive Online Learning | http://arxiv.org/pdf/1508.05170v1.pdf | author:Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML published:2015-08-21 summary:We propose a general framework for studying adaptive regret bounds in theonline learning framework, including model selection bounds and data-dependentbounds. Given a data- or model-dependent bound we ask, "Does there exist somealgorithm achieving this bound?" We show that modifications to recentlyintroduced sequential complexity measures can be used to answer this questionby providing sufficient conditions under which adaptive rates can be achieved.In particular each adaptive rate induces a set of so-called offset complexitymeasures, and obtaining small upper bounds on these quantities is sufficient todemonstrate achievability. A cornerstone of our analysis technique is the useof one-sided tail inequalities to bound suprema of offset random processes. Our framework recovers and improves a wide variety of adaptive boundsincluding quantile bounds, second-order data-dependent bounds, and small lossbounds. In addition we derive a new type of adaptive bound for online linearoptimization based on the spectral norm, as well as a new online PAC-Bayestheorem that holds for countably infinite sets.
arxiv-12000-138 | On Monotonicity of the Optimal Transmission Policy in Cross-layer Adaptive m-QAM Modulation | http://arxiv.org/pdf/1508.05383v1.pdf | author:Ni Ding, Parastoo Sadeghi, Rodney A. Kennedy category:stat.ML cs.IT math.IT published:2015-08-21 summary:This paper considers a cross-layer adaptive modulation system that is modeledas a Markov decision process (MDP). We study how to utilize the monotonicity ofthe optimal transmission policy to relieve the computational complexity ofdynamic programming (DP). In this system, a scheduler controls the bit rate ofthe m-quadrature amplitude modulation (m-QAM) in order to minimize thelong-term losses incurred by the queue overflow in the data link layer and thetransmission power consumption in the physical layer. The work is done in twosteps. Firstly, we observe the L-natural-convexity and submodularity of DP toprove that the optimal policy is always nondecreasing in queue occupancy/stateand derive the sufficient condition for it to be nondecreasing in both queueand channel states. We also show that, due to the L-natural-convexity of DP,the variation of the optimal policy in queue state is restricted by a boundedmarginal effect: The increment of the optimal policy between adjacent queuestates is no greater than one. Secondly, we use the monotonicity results topresent two low complexity algorithms: monotonic policy iteration (MPI) basedon L-natural-convexity and discrete simultaneous perturbation stochasticapproximation (DSPSA). We run experiments to show that the time complexity ofMPI based on L-natural-convexity is much lower than that of DP and theconventional MPI that is based on submodularity and DSPSA is able to adaptivelytrack the optimal policy when the system parameters change.
arxiv-12000-139 | Circle-based Eye Center Localization (CECL) | http://arxiv.org/pdf/1506.04500v2.pdf | author:Yustinus Eko Soelistio, Eric Postma, Alfons Maes category:cs.CV published:2015-06-15 summary:We propose an improved eye center localization method based on the Houghtransform, called Circle-based Eye Center Localization (CECL) that is simple,robust, and achieves accuracy on a par with typically more complexstate-of-the-art methods. The CECL method relies on color and shape cues thatdistinguish the iris from other facial structures. The accuracy of the CECLmethod is demonstrated through a comparison with 15 state-of-the-art eye centerlocalization methods against five error thresholds, as reported in theliterature. The CECL method achieved an accuracy of 80.8% to 99.4% and rankedfirst for 2 of the 5 thresholds. It is concluded that the CECL method offers anattractive alternative to existing methods for automatic eye centerlocalization.
arxiv-12000-140 | Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method | http://arxiv.org/pdf/1508.05163v1.pdf | author:Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra category:cs.CL cs.IR published:2015-08-21 summary:Text mining can be applied to many fields. One of the application is usingtext mining in digital newspaper to do politic sentiment analysis. In thispaper sentiment analysis is applied to get information from digital newsarticles about its positive or negative sentiment regarding particularpolitician. This paper suggests a simple model to analyze digital newspapersentiment polarity using naive Bayes classifier method. The model uses a set ofinitial data to begin with which will be updated when new information appears.The model showed promising result when tested and can be implemented to someother sentiment analysis problems.
arxiv-12000-141 | Auto-Sizing Neural Networks: With Applications to n-gram Language Models | http://arxiv.org/pdf/1508.05051v1.pdf | author:Kenton Murray, David Chiang category:cs.CL published:2015-08-20 summary:Neural networks have been shown to improve performance across a range ofnatural-language tasks. However, designing and training them can becomplicated. Frequently, researchers resort to repeated experimentation to pickoptimal settings. In this paper, we address the issue of choosing the correctnumber of units in hidden layers. We introduce a method for automaticallyadjusting network size by pruning out hidden units through $\ell_{\infty,1}$and $\ell_{2,1}$ regularization. We apply this method to language modeling anddemonstrate its ability to correctly choose the number of hidden units whilemaintaining perplexity. We also include these models in a machine translationdecoder and show that these smaller neural models maintain the significantimprovements of their unpruned versions.
arxiv-12000-142 | Improving Image Restoration with Soft-Rounding | http://arxiv.org/pdf/1508.05046v1.pdf | author:Xing Mei, Honggang Qi, Bao-Gang Hu, Siwei Lyu category:cs.CV published:2015-08-20 summary:Several important classes of images such as text, barcode and pattern imageshave the property that pixels can only take a distinct subset of values. Thisknowledge can benefit the restoration of such images, but it has not beenwidely considered in current restoration methods. In this work, we describe aneffective and efficient approach to incorporate the knowledge of distinct pixelvalues of the pristine images into the general regularized least squaresrestoration framework. We introduce a new regularizer that attains zero at thedesignated pixel values and becomes a quadratic penalty function in theintervals between them. When incorporated into the regularized least squaresrestoration framework, this regularizer leads to a simple and efficient stepthat resembles and extends the rounding operation, which we term assoft-rounding. We apply the soft-rounding enhanced solution to the restorationof binary text/barcode images and pattern images with multiple distinct pixelvalues. Experimental results show that soft-rounding enhanced restorationmethods achieve significant improvement in both visual quality and quantitativemeasures (PSNR and SSIM). Furthermore, we show that this regularizer can alsobenefit the restoration of general natural images.
arxiv-12000-143 | Adjusted least squares fitting of algebraic hypersurfaces | http://arxiv.org/pdf/1412.2291v2.pdf | author:Konstantin Usevich, Ivan Markovsky category:stat.CO cs.CG cs.CV math.NA published:2014-12-06 summary:We consider the problem of fitting a set of points in Euclidean space by analgebraic hypersurface. We assume that points on a true hypersurface, describedby a polynomial equation, are corrupted by zero mean independent Gaussiannoise, and we estimate the coefficients of the true polynomial equation. Theadjusted least squares estimator accounts for the bias present in the ordinaryleast squares estimator. The adjusted least squares estimator is based onconstructing a quasi-Hankel matrix, which is a bias-corrected matrix ofmoments. For the case of unknown noise variance, the estimator is defined as asolution of a polynomial eigenvalue problem. In this paper, we present newresults on invariance properties of the adjusted least squares estimator and animproved algorithm for computing the estimator for an arbitrary set ofmonomials in the polynomial equation.
arxiv-12000-144 | Using User Generated Online Photos to Estimate and Monitor Air Pollution in Major Cities | http://arxiv.org/pdf/1508.05028v1.pdf | author:Yuncheng Li, Jifei Huang, Jiebo Luo category:cs.CV published:2015-08-20 summary:With the rapid development of economy in China over the past decade, airpollution has become an increasingly serious problem in major cities and causedgrave public health concerns in China. Recently, a number of studies have dealtwith air quality and air pollution. Among them, some attempt to predict andmonitor the air quality from different sources of information, ranging fromdeployed physical sensors to social media. These methods are either tooexpensive or unreliable, prompting us to search for a novel and effective wayto sense the air quality. In this study, we propose to employ the state of theart in computer vision techniques to analyze photos that can be easily acquiredfrom online social media. Next, we establish the correlation between the hazelevel computed directly from photos with the official PM 2.5 record of thetaken city at the taken time. Our experiments based on both synthetic and realphotos have shown the promise of this image-based approach to estimating andmonitoring air pollution.
arxiv-12000-145 | AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization | http://arxiv.org/pdf/1508.05003v1.pdf | author:Suvrit Sra, Adams Wei Yu, Mu Li, Alexander J. Smola category:stat.ML cs.LG math.OC published:2015-08-20 summary:We study distributed stochastic convex optimization under the delayedgradient model where the server nodes perform parameter updates, while theworker nodes compute stochastic gradients. We discuss, analyze, and experimentwith a setup motivated by the behavior of real-world distributed computationnetworks, where the machines are differently slow at different time. Therefore,we allow the parameter updates to be sensitive to the actual delaysexperienced, rather than to worst-case bounds on the maximum delay. Thissensitivity leads to larger stepsizes, that can help gain rapid initialconvergence without having to wait too long for slower machines, whilemaintaining the same asymptotic complexity. We obtain encouraging improvementsto overall convergence for distributed experiments on real datasets with up tobillions of examples and features.
arxiv-12000-146 | A Deep Bag-of-Features Model for Music Auto-Tagging | http://arxiv.org/pdf/1508.04999v1.pdf | author:Juhan Nam, Jorge Herrera, Kyogu Lee category:cs.LG cs.SD stat.ML published:2015-08-20 summary:Feature learning and deep learning have drawn great attention in recent yearsas a way of transforming input data into more effective representations usinglearning algorithms. Such interest has grown up in the area of musicinformation retrieval (MIR) as well, particularly in music classification taskssuch as auto-tagging. While a number of promising results have been shown, itis not well understood what acoustic sense the learned feature representationshave and how they are associated with semantic meaning of music. In this paper,we attempt to demystify the learned audio features using a bag-of-featuresmodel with two learning stages. The first stage learns to project localacoustic patterns of musical signals onto a high-dimensional sparse space in anunsupervised manner and summarizes an audio track as a bag-of-features. Thesecond stage maps the bag-of-features to semantic tags using deep neuralnetworks in a supervised manner. For the first stage, we focus on analyzing thelearned local audio features by quantitatively measuring the acousticproperties and interpreting the statistics in semantic context. For the secondstage, we examine training choices and tuning parameters for the neuralnetworks and show how the deep representations of bag-of-features become morediscriminative. Through this analysis, we not only provide better understandingof learned local audio features but also show the effectiveness of the deepbag-of-features model in the music auto-tagging task.
arxiv-12000-147 | High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging | http://arxiv.org/pdf/1508.04981v1.pdf | author:Changsoo Je, Sang Wook Lee, Rae-Hong Park category:cs.CV cs.GR physics.optics I.2.10; I.4.8 published:2015-08-20 summary:For structured-light range imaging, color stripes can be used for increasingthe number of distinguishable light patterns compared to binary BW stripes.Therefore, an appropriate use of color patterns can reduce the number of lightprojections and range imaging is achievable in single video frame or in "oneshot". On the other hand, the reliability and range resolution attainable fromcolor stripes is generally lower than those from multiply projected binary BWpatterns since color contrast is affected by object color reflectance andambient light. This paper presents new methods for selecting stripe colors anddesigning multiple-stripe patterns for "one-shot" and "two-shot" imaging. Weshow that maximizing color contrast between the stripes in one-shot imagingreduces the ambiguities resulting from colored object surfaces and limitationsin sensor/projector resolution. Two-shot imaging adds an extra video frame andmaximizes the color contrast between the first and second video frames todiminish the ambiguities even further. Experimental results demonstrate theeffectiveness of the presented one-shot and two-shot color-stripe imagingschemes.
arxiv-12000-148 | Introducing Geometry in Active Learning for Image Segmentation | http://arxiv.org/pdf/1508.04955v1.pdf | author:Ksenia Konyushkova, Raphael Sznitman, Pascal Fua category:cs.CV published:2015-08-20 summary:We propose an Active Learning approach to training a segmentation classifierthat exploits geometric priors to streamline the annotation process in 3D imagevolumes. To this end, we use these priors not only to select voxels most inneed of annotation but to guarantee that they lie on 2D planar patch, whichmakes it much easier to annotate than if they were randomly distributed in thevolume. A simplified version of this approach is effective in natural 2Dimages. We evaluated our approach on Electron Microscopy and Magnetic Resonanceimage volumes, as well as on natural images. Comparing our approach againstseveral accepted baselines demonstrates a marked performance increase.
arxiv-12000-149 | The ABACOC Algorithm: a Novel Approach for Nonparametric Classification of Data Streams | http://arxiv.org/pdf/1508.04912v1.pdf | author:Rocco De Rosa, Francesco Orabona, Nicol√≤ Cesa-Bianchi category:stat.ML cs.LG published:2015-08-20 summary:Stream mining poses unique challenges to machine learning: predictive modelsare required to be scalable, incrementally trainable, must remain bounded insize (even when the data stream is arbitrarily long), and be nonparametric inorder to achieve high accuracy even in complex and dynamic environments.Moreover, the learning system must be parameterless ---traditional tuningmethods are problematic in streaming settings--- and avoid requiring priorknowledge of the number of distinct class labels occurring in the stream. Inthis paper, we introduce a new algorithmic approach for nonparametric learningin data streams. Our approach addresses all above mentioned challenges bylearning a model that covers the input space using simple local classifiers.The distribution of these classifiers dynamically adapts to the local (unknown)complexity of the classification problem, thus achieving a good balance betweenmodel complexity and predictive accuracy. We design four variants of ourapproach of increasing adaptivity. By means of an extensive empiricalevaluation against standard nonparametric baselines, we show state-of-the-artresults in terms of accuracy versus model size. For the variant that imposes astrict bound on the model size, we show better performance against all othermethods measured at the same model size value. Our empirical analysis iscomplemented by a theoretical performance guarantee which does not rely on anystochastic assumption on the source generating the stream.
arxiv-12000-150 | Histogram of gradients of Time-Frequency Representations for Audio scene detection | http://arxiv.org/pdf/1508.04909v1.pdf | author:Alain Rakotomamonjy, Gilles Gasso category:cs.SD cs.LG published:2015-08-20 summary:This paper addresses the problem of audio scenes classification andcontributes to the state of the art by proposing a novel feature. We build thisfeature by considering histogram of gradients (HOG) of time-frequencyrepresentation of an audio scene. Contrarily to classical audio features likeMFCC, we make the hypothesis that histogram of gradients are able to encodesome relevant informations in a time-frequency {representation:} namely, thelocal direction of variation (in time and frequency) of the signal spectralpower. In addition, in order to gain more invariance and robustness, histogramof gradients are locally pooled. We have evaluated the relevance of {the novelfeature} by comparing its performances with state-of-the-art competitors, onseveral datasets, including a novel one that we provide, as part of ourcontribution. This dataset, that we make publicly available, involves $19$classes and contains about $900$ minutes of audio scene recording. We thusbelieve that it may be the next standard dataset for evaluating audio sceneclassification algorithms. Our comparison results clearly show that ourHOG-based features outperform its competitors
arxiv-12000-151 | Semi-supervised Learning with Regularized Laplacian | http://arxiv.org/pdf/1508.04906v1.pdf | author:Konstantin Avrachenkov, Pavel Chebotarev, Alexey Mishenin category:cs.LG published:2015-08-20 summary:We study a semi-supervised learning method based on the similarity graph andRegularizedLaplacian. We give convenient optimization formulation of theRegularized Laplacian method and establishits various properties. Inparticular, we show that the kernel of the methodcan be interpreted in terms ofdiscrete and continuous time random walks and possesses severalimportantproperties of proximity measures. Both optimization and linear algebramethods can be used for efficientcomputation of the classification functions.We demonstrate on numerical examples that theRegularized Laplacian method iscompetitive with respect to the other state of the art semi-supervisedlearningmethods.
arxiv-12000-152 | Review and Perspective for Distance Based Trajectory Clustering | http://arxiv.org/pdf/1508.04904v1.pdf | author:Philippe Besse, Brendan Guillouet, Jean-Michel Loubes, Royer Fran√ßois category:stat.ML cs.LG stat.AP published:2015-08-20 summary:In this paper we tackle the issue of clustering trajectories of geolocalizedobservations. Using clustering technics based on the choice of a distancebetween the observations, we first provide a comprehensive review of thedifferent distances used in the literature to compare trajectories. Then basedon the limitations of these methods, we introduce a new distance : SymmetrizedSegment-Path Distance (SSPD). We finally compare this new distance to theothers according to their corresponding clustering results obtained using bothhierarchical clustering and affinity propagation methods.
arxiv-12000-153 | Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion | http://arxiv.org/pdf/1506.00196v3.pdf | author:Kaisheng Yao, Geoffrey Zweig category:cs.CL published:2015-05-31 summary:Sequence-to-sequence translation methods based on generation with aside-conditioned language model have recently shown promising results inseveral tasks. In machine translation, models conditioned on source side wordshave been used to produce target-language text, and in image captioning, modelsconditioned images have been used to generate caption text. Past work with thisapproach has focused on large vocabulary tasks, and measured quality in termsof BLEU. In this paper, we explore the applicability of such models to thequalitatively different grapheme-to-phoneme task. Here, the input and outputside vocabularies are small, plain n-gram models do well, and credit is onlygiven when the output is exactly correct. We find that the simpleside-conditioned generation approach is able to rival the state-of-the-art, andwe are able to significantly advance the stat-of-the-art with bi-directionallong short-term memory (LSTM) neural networks that use the same alignmentinformation that is used in conventional approaches.
arxiv-12000-154 | Multi-criteria Similarity-based Anomaly Detection using Pareto Depth Analysis | http://arxiv.org/pdf/1508.04887v1.pdf | author:Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, Alfred O. Hero III category:cs.CV cs.LG stat.ML published:2015-08-20 summary:We consider the problem of identifying patterns in a data set that exhibitanomalous behavior, often referred to as anomaly detection. Similarity-basedanomaly detection algorithms detect abnormally large amounts of similarity ordissimilarity, e.g.~as measured by nearest neighbor Euclidean distances betweena test sample and the training samples. In many application domains there maynot exist a single dissimilarity measure that captures all possible anomalouspatterns. In such cases, multiple dissimilarity measures can be defined,including non-metric measures, and one can test for anomalies by scalarizingusing a non-negative linear combination of them. If the relative importance ofthe different dissimilarity measures are not known in advance, as in manyanomaly detection applications, the anomaly detection algorithm may need to beexecuted multiple times with different choices of weights in the linearcombination. In this paper, we propose a method for similarity-based anomalydetection using a novel multi-criteria dissimilarity measure, the Pareto depth.The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses theconcept of Pareto optimality to detect anomalies under multiple criteriawithout having to run an algorithm multiple times with different choices ofweights. The proposed PDA approach is provably better than using linearcombinations of the criteria and shows superior performance on experiments withsynthetic and real data sets.
arxiv-12000-155 | Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection | http://arxiv.org/pdf/1508.04843v1.pdf | author:Kisuk Lee, Aleksandar Zlateski, Ashwin Vishwanathan, H. Sebastian Seung category:cs.CV published:2015-08-20 summary:Efforts to automate the reconstruction of neural circuits from 3D electronmicroscopic (EM) brain images are critical for the field of connectomics. Animportant computation for reconstruction is the detection of neuronalboundaries. Images acquired by serial section EM, a leading 3D EM technique,are highly anisotropic, with inferior quality along the third dimension. Forsuch images, the 2D max-pooling convolutional network has set the standard forperformance at boundary detection. Here we achieve a substantial gain inaccuracy through three innovations. Following the trend towards deeper networksfor object recognition, we use a much deeper network than previously employedfor boundary detection. Second, we incorporate 3D as well as 2D filters, toenable computations that use 3D context. Finally, we adopt a recursivelytrained architecture in which a first network generates a preliminary boundarymap that is provided as input along with the original image to a second networkthat generates a final boundary map. Backpropagation training is accelerated byZNN, a new implementation of 3D convolutional networks that uses multicore CPUparallelism for speed. Our hybrid 2D-3D architecture could be more generallyapplicable to other types of anisotropic 3D images, including video, and ourrecursive framework for any image labeling problem.
arxiv-12000-156 | Listen, Attend and Spell | http://arxiv.org/pdf/1508.01211v2.pdf | author:William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals category:cs.CL cs.LG cs.NE stat.ML published:2015-08-05 summary:We present Listen, Attend and Spell (LAS), a neural network that learns totranscribe speech utterances to characters. Unlike traditional DNN-HMM models,this model learns all the components of a speech recognizer jointly. Our systemhas two components: a listener and a speller. The listener is a pyramidalrecurrent network encoder that accepts filter bank spectra as inputs. Thespeller is an attention-based recurrent network decoder that emits charactersas outputs. The network produces character sequences without making anyindependence assumptions between the characters. This is the key improvement ofLAS over previous end-to-end CTC models. On a subset of the Google voice searchtask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or alanguage model, and 10.3% with language model rescoring over the top 32 beams.By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.
arxiv-12000-157 | Offline Handwritten Signature Verification - Literature Review | http://arxiv.org/pdf/1507.07909v2.pdf | author:Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira category:cs.CV stat.ML I.5.4 published:2015-07-28 summary:The area of Handwritten Signature Verification has been broadly researched inthe last decades and still remains as an open research problem. This reportfocuses on offline signature verification, characterized by the usage of static(scanned) images of signatures, where the objective is to discriminate if agiven signature is genuine (produced by the claimed individual), or a forgery(produced by an impostor). We present an overview of how the problem has beenhandled by several researchers in the past few decades and the recentadvancements in the field.
arxiv-12000-158 | Who are the Devils Wearing Prada in New York City? | http://arxiv.org/pdf/1508.04785v1.pdf | author:KuanTing Chen, Kezhen Chen, Peizhong Cong, Winston H. Hsu, Jiebo Luo category:cs.CV cs.CY published:2015-08-19 summary:Fashion is a perpetual topic in human social life, and the mass has thepenchant to emulate what large city residents and celebrities wear. Undeniably,New York City is such a bellwether large city with all kinds of fashionleadership. Consequently, to study what the fashion trends are during thisyear, it is very helpful to learn the fashion trends of New York City.Discovering fashion trends in New York City could boost many applications suchas clothing recommendation and advertising. Does the fashion trend in the NewYork Fashion Show actually influence the clothing styles on the public? Toanswer this question, we design a novel system that consists of three majorcomponents: (1) constructing a large dataset from the New York Fashion Showsand New York street chic in order to understand the likely clothing fashiontrends in New York, (2) utilizing a learning-based approach to discover fashionattributes as the representative characteristics of fashion trends, and (3)comparing the analysis results from the New York Fashion Shows and street-chicimages to verify whether the fashion shows have actual influence on the peoplein New York City. Through the preliminary experiments over a large clothingdataset, we demonstrate the effectiveness of our proposed system, and obtainuseful insights on fashion trends and fashion influence.
arxiv-12000-159 | Time Series Clustering via Community Detection in Networks | http://arxiv.org/pdf/1508.04757v1.pdf | author:Leonardo N. Ferreira, Liang Zhao category:stat.ML cs.LG cs.SI published:2015-08-19 summary:In this paper, we propose a technique for time series clustering usingcommunity detection in complex networks. Firstly, we present a method totransform a set of time series into a network using different distancefunctions, where each time series is represented by a vertex and the mostsimilar ones are connected. Then, we apply community detection algorithms toidentify groups of strongly connected vertices (called a community) and,consequently, identify time series clusters. Still in this paper, we make acomprehensive analysis on the influence of various combinations of time seriesdistance functions, network generation methods and community detectiontechniques on clustering results. Experimental study shows that the proposednetwork-based approach achieves better results than various classic orup-to-date clustering techniques under consideration. Statistical tests confirmthat the proposed method outperforms some classic clustering algorithms, suchas $k$-medoids, diana, median-linkage and centroid-linkage in various datasets. Interestingly, the proposed method can effectively detect shape patternspresented in time series due to the topological structure of the underlyingnetwork constructed in the clustering process. At the same time, othertechniques fail to identify such patterns. Moreover, the proposed method isrobust enough to group time series presenting similar pattern but with timeshifts and/or amplitude variations. In summary, the main point of the proposedmethod is the transformation of time series from time-space domain totopological domain. Therefore, we hope that our approach contributes not onlyfor time series clustering, but also for general time series analysis tasks.
arxiv-12000-160 | Fault Diagnosis of Helical Gear Box using Large Margin K-Nearest Neighbors Classifier using Sound Signals | http://arxiv.org/pdf/1508.04734v1.pdf | author:M. Amarnath, S. Arunav, Hemantha Kumar, V. Sugumaran, G. S Raghvendra category:cs.LG published:2015-08-19 summary:Gear drives are one of the most widely used transmission system in manymachinery. Sound signals of a rotating machine contain the dynamic informationabout its health conditions. Not much information available in the literaturereporting suitability of sound signals for fault diagnosis applications.Maximum numbers of literature are based on FFT (Fast Fourier Transform)analysis and have its own limitations with non-stationary signals like the onesfrom gears. In this paper, attempt has been made in using sound signalsacquired from gears in good and simulated faulty conditions for the purpose offault diagnosis through a machine learning approach. The descriptivestatistical features were extracted from the acquired sound signals and thepredominant features were selected using J48 decision tree technique. Theselected features were then used for classification using Large MarginK-nearest neighbor approach. The paper also discusses the effect of variousparameters on classification accuracy.
arxiv-12000-161 | Saliency maps on image hierarchies | http://arxiv.org/pdf/1508.04586v1.pdf | author:Ver√≥nica Vilaplana category:cs.CV published:2015-08-19 summary:In this paper we propose two saliency models for salient object segmentationbased on a hierarchical image segmentation, a tree-like structure thatrepresents regions at different scales from the details to the whole image(e.g. gPb-UCM, BPT). The first model is based on a hierarchy of imagepartitions. The saliency at each level is computed on a region basis, takinginto account the contrast between regions. The maps obtained for the differentpartitions are then integrated into a final saliency map. The second modeldirectly works on the structure created by the segmentation algorithm,computing saliency at each node and integrating these cues in a straightforwardmanner into a single saliency map. We show that the proposed models producehigh quality saliency maps. Objective evaluation demonstrates that the twomethods achieve state-of-the-art performance in several benchmark datasets.
arxiv-12000-162 | Learning to Predict Independent of Span | http://arxiv.org/pdf/1508.04582v1.pdf | author:Hado van Hasselt, Richard S. Sutton category:cs.LG published:2015-08-19 summary:We consider how to learn multi-step predictions efficiently. Conventionalalgorithms wait until observing actual outcomes before performing thecomputations to update their predictions. If predictions are made at a highrate or span over a large amount of time, substantial computation can berequired to store all relevant observations and to update all predictions whenthe outcome is finally observed. We show that the exact same predictions can belearned in a much more computationally congenial way, with uniform per-stepcomputation that does not depend on the span of the predictions. We apply thisidea to various settings of increasing generality, repeatedly adding desiredproperties and each time deriving an equivalent span-independent algorithm forthe conventional algorithm that satisfies these desiderata. Interestingly,along the way several known algorithmic constructs emerge spontaneously fromour derivations, including dutch eligibility traces, temporal differenceerrors, and averaging. This allows us to link these constructs one-to-one tothe corresponding desiderata, unambiguously connecting the `how' to the `why'.Each step, we make sure that the derived algorithm subsumes the previousalgorithms, thereby retaining their properties. Ultimately we arrive at asingle general temporal-difference algorithm that is applicable to the fullsetting of reinforcement learning.
arxiv-12000-163 | Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections | http://arxiv.org/pdf/1508.04562v1.pdf | author:Jingwei Zhang, Aaron Gerow, Jaan Altosaar, James Evans, Richard Jean So category:cs.CL cs.IR published:2015-08-19 summary:Weak topic correlation across document collections with different numbers oftopics in individual collections presents challenges for existingcross-collection topic models. This paper introduces two probabilistic topicmodels, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These addressproblems that can arise when analyzing large, asymmetric, and potentiallyweakly-related collections. Topic correlations in weakly-related collectionstypically lie in the tail of the topic distribution, where they would beoverlooked by models unable to fit large numbers of topics. To efficientlymodel this long tail for large-scale analysis, our models implement a parallelsampling algorithm based on the Metropolis-Hastings and alias methods (Yuan etal., 2015). The models are first evaluated on synthetic data, generated tosimulate various collection-level asymmetries. We then present a case study ofmodeling over 300k documents in collections of sciences and humanities researchfrom JSTOR.
arxiv-12000-164 | Introduction to Cross-Entropy Clustering The R Package CEC | http://arxiv.org/pdf/1508.04559v1.pdf | author:Jacek Tabor, Przemys≈Çaw Spurek, Konrad Kamieniecki, Marek ≈ömieja, Krzysztof Misztal category:cs.LG stat.ME stat.ML published:2015-08-19 summary:The R Package CEC performs clustering based on the cross-entropy clustering(CEC) method, which was recently developed with the use of information theory.The main advantage of CEC is that it combines the speed and simplicity of$k$-means with the ability to use various Gaussian mixture models and reduceunnecessary clusters. In this work we present a practical tutorial to CEC basedon the R Package CEC. Functions are provided to encompass the whole process ofclustering.
arxiv-12000-165 | Spatio-temporal Spike and Slab Priors for Multiple Measurement Vector Problems | http://arxiv.org/pdf/1508.04556v1.pdf | author:Michael Riis Andersen, Ole Winther, Lars Kai Hansen category:stat.ML published:2015-08-19 summary:We are interested in solving the multiple measurement vector (MMV) problemfor instances, where the underlying sparsity pattern exhibit spatio-temporalstructure motivated by the electroencephalogram (EEG) source localizationproblem. We propose a probabilistic model that takes this structure intoaccount by generalizing the structured spike and slab prior and the associatedExpectation Propagation inference scheme. Based on numerical experiments, wedemonstrate the viability of the model and the approximate inference scheme.
arxiv-12000-166 | Mining Brain Networks using Multiple Side Views for Neurological Disorder Identification | http://arxiv.org/pdf/1508.04554v1.pdf | author:Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu, Ann B. Ragin category:cs.LG cs.CV cs.CY stat.AP stat.ML published:2015-08-19 summary:Mining discriminative subgraph patterns from graph data has attracted greatinterest in recent years. It has a wide variety of applications in diseasediagnosis, neuroimaging, etc. Most research on subgraph mining focuses on thegraph representation alone. However, in many real-world applications, the sideinformation is available along with the graph data. For example, forneurological disorder identification, in addition to the brain networks derivedfrom neuroimaging data, hundreds of clinical, immunologic, serologic andcognitive measures may also be documented for each subject. These measurescompose multiple side views encoding a tremendous amount of supplementalinformation for diagnostic purposes, yet are often ignored. In this paper, westudy the problem of discriminative subgraph selection using multiple sideviews and propose a novel solution to find an optimal set of subgraph featuresfor graph classification by exploring a plurality of side views. We derive afeature evaluation criterion, named gSide, to estimate the usefulness ofsubgraph patterns based upon side views. Then we develop a branch-and-boundalgorithm, called gMSV, to efficiently search for optimal subgraph features byintegrating the subgraph mining process and the procedure of discriminativefeature selection. Empirical studies on graph classification tasks forneurological disorders using brain networks demonstrate that subgraph patternsselected by the multi-side-view guided subgraph selection approach caneffectively boost graph classification performances and are relevant to diseasediagnosis.
arxiv-12000-167 | Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images | http://arxiv.org/pdf/1508.04546v1.pdf | author:Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang, Stefan Gumhold, Carsten Rother category:cs.CV 65-XX published:2015-08-19 summary:Analysis-by-synthesis has been a successful approach for many tasks incomputer vision, such as 6D pose estimation of an object in an RGB-D imagewhich is the topic of this work. The idea is to compare the observation withthe output of a forward process, such as a rendered image of the object ofinterest in a particular pose. Due to occlusion or complicated sensor noise, itcan be difficult to perform this comparison in a meaningful way. We propose anapproach that "learns to compare", while taking these difficulties intoaccount. This is done by describing the posterior density of a particularobject pose with a convolutional neural network (CNN) that compares an observedand rendered image. The network is trained with the maximum likelihoodparadigm. We observe empirically that the CNN does not specialize to thegeometry or appearance of specific objects, and it can be used with objects ofvastly different shapes and appearances, and in different backgrounds. Comparedto state-of-the-art, we demonstrate a significant improvement on two differentdatasets which include a total of eleven objects, cluttered background, andheavy occlusion.
arxiv-12000-168 | Traversing Knowledge Graphs in Vector Space | http://arxiv.org/pdf/1506.01094v2.pdf | author:Kelvin Guu, John Miller, Percy Liang category:cs.CL cs.AI cs.DB stat.ML published:2015-06-03 summary:Path queries on a knowledge graph can be used to answer compositionalquestions such as "What languages are spoken by people living in Lisbon?".However, knowledge graphs often have missing facts (edges) which disrupts pathqueries. Recent models for knowledge base completion impute missing facts byembedding knowledge graphs in vector spaces. We show that these models can berecursively applied to answer path queries, but that they suffer from cascadingerrors. This motivates a new "compositional" training objective, whichdramatically improves all models' ability to answer path queries, in some casesmore than doubling accuracy. On a standard knowledge base completion task, wealso demonstrate that compositional training acts as a novel form of structuralregularization, reliably improving performance across all base models (reducingerrors by up to 43%) and achieving new state-of-the-art results.
arxiv-12000-169 | A Survey of Current Datasets for Vision and Language Research | http://arxiv.org/pdf/1506.06833v2.pdf | author:Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao, Huang, Lucy Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell category:cs.CL cs.AI cs.CV cs.GL published:2015-06-23 summary:Integrating vision and language has long been a dream in work on artificialintelligence (AI). In the past two years, we have witnessed an explosion ofwork that brings together vision and language from images to videos and beyond.The available corpora have played a crucial role in advancing this area ofresearch. In this paper, we propose a set of quality metrics for evaluating andanalyzing the vision & language datasets and categorize them accordingly. Ouranalyses show that the most recent datasets have been using more complexlanguage and more abstract concepts, however, there are different strengths andweaknesses in each.
arxiv-12000-170 | Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles | http://arxiv.org/pdf/1508.04525v1.pdf | author:Wei Zhang, Yang Yu, Osho Gupta, Judith Gelernter category:cs.CL cs.LG D.3.3 published:2015-08-19 summary:Precise geocoding and time normalization for text requires that location andtime phrases be identified. Many state-of-the-art geoparsers and temporalparsers suffer from low recall. Categories commonly missed by parsers are:nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases,prepositional phrases, and numerical phrases. We collected and annotated dataset by querying commercial web searches API with such spatiotemporalexpressions as were missed by state-of-the- art parsers. Due to the high costof sentence annotation, active learning was used to label training data, and anew strategy was designed to better select training examples to reduce labelingcost. For the learning algorithm, we applied an average perceptron trainedFeaturized Hidden Markov Model (FHMM). Five FHMM instances were used to createan ensemble, with the output phrase selected by voting. Our ensemble model wastested on a range of sequential labeling tasks, and has shown competitiveperformance. Our contributions include (1) an new dataset annotated with namedentities and expanded spatiotemporal expressions; (2) a comparison of inferencealgorithms for ensemble models showing the superior accuracy of BeliefPropagation over Viterbi Decoding; (3) a new example re-weighting method foractive ensemble learning that 'memorizes' the latest examples trained; (4) aspatiotemporal parser that jointly recognizes expanded spatiotemporalexpressions as well as named entities.
arxiv-12000-171 | Exploring Metaphorical Senses and Word Representations for Identifying Metonyms | http://arxiv.org/pdf/1508.04515v1.pdf | author:Wei Zhang, Judith Gelernter category:cs.CL I.2.7 published:2015-08-19 summary:A metonym is a word with a figurative meaning, similar to a metaphor. Becausemetonyms are closely related to metaphors, we apply features that are usedsuccessfully for metaphor recognition to the task of detecting metonyms. On theACL SemEval 2007 Task 8 data with gold standard metonym annotations, our systemachieved 86.45% accuracy on the location metonyms. Our code can be found onGitHub.
arxiv-12000-172 | A Dictionary Learning Approach for Factorial Gaussian Models | http://arxiv.org/pdf/1508.04486v1.pdf | author:Y. Cem Subakan, Johannes Traa, Paris Smaragdis, Noah Stein category:cs.LG stat.ML published:2015-08-18 summary:In this paper, we develop a parameter estimation method for factoriallyparametrized models such as Factorial Gaussian Mixture Model and FactorialHidden Markov Model. Our contributions are two-fold. First, we show that theemission matrix of the standard Factorial Model is unidentifiable even if thetrue assignment matrix is known. Secondly, we address the issue ofidentifiability by making a one component sharing assumption and derive aparameter learning algorithm for this case. Our approach is based on adictionary learning problem of the form $X = O R$, where the goal is to learnthe dictionary $O$ given the data matrix $X$. We argue that due to the specificstructure of the activation matrix $R$ in the shared component factorialmixture model, and an incoherence assumption on the shared component, it ispossible to extract the columns of the $O$ matrix without the need foralternating between the estimation of $O$ and $R$.
arxiv-12000-173 | Robust Subspace Clustering via Smoothed Rank Approximation | http://arxiv.org/pdf/1508.04467v1.pdf | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.IT cs.LG cs.NA math.IT stat.ML published:2015-08-18 summary:Matrix rank minimizing subject to affine constraints arises in manyapplication areas, ranging from signal processing to machine learning. Nuclearnorm is a convex relaxation for this problem which can recover the rank exactlyunder some restricted and theoretically interesting conditions. However, formany real-world applications, nuclear norm approximation to the rank functioncan only produce a result far from the optimum. To seek a solution of higheraccuracy than the nuclear norm, in this paper, we propose a rank approximationbased on Logarithm-Determinant. We consider using this rank approximation forsubspace clustering application. Our framework can model different kinds oferrors and noise. Effective optimization strategy is developed with theoreticalguarantee to converge to a stationary point. The proposed method givespromising results on face clustering and motion segmentation tasks compared tothe state-of-the-art subspace clustering algorithms.
arxiv-12000-174 | Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural Networks | http://arxiv.org/pdf/1508.04422v1.pdf | author:Aren Jansen, Gregory Sell, Vince Lyzinski category:stat.ML cs.LG cs.NE stat.ME published:2015-08-18 summary:Several popular graph embedding techniques for representation learning anddimensionality reduction rely on performing computationally expensiveeigendecompositions to derive a nonlinear transformation of the input dataspace. The resulting eigenvectors encode the embedding coordinates for thetraining samples only, preventing the transformation of novel data sampleswithout recomputation. In this paper, we present a method for out-of sampleextension of graph embeddings that uses deep neural networks (DNN) toparametrically approximate these nonlinear maps. Compared with traditionalnonparametric out-of-sample extension methods, we demonstrate that the DNNs cangeneralize with equal or better fidelity and require orders of magnitude lesscomputation at test time. Moreover, we find that unsupervised pretraining ofthe DNNs improves optimization for larger network sizes, thus removingsensitivity to model selection.
arxiv-12000-175 | ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R | http://arxiv.org/pdf/1508.04409v1.pdf | author:Marvin N. Wright, Andreas Ziegler category:stat.ML stat.CO published:2015-08-18 summary:We introduce the C++ application and R package ranger. The software is a fastimplementation of random forests for high dimensional data. Ensembles ofclassification, regression and survival trees are supported. We describe theimplementation, provide examples, validate the package with a referenceimplementation, and compare runtime and memory usage with otherimplementations. The new software proves to scale best with the number offeatures, samples, trees, and features tried for splitting. Finally, we showthat ranger is the fastest and most memory efficient implementation of randomforests to analyze data on the scale of a genome-wide association study.
arxiv-12000-176 | Hash Function Learning via Codewords | http://arxiv.org/pdf/1508.03285v2.pdf | author:Yinjie Huang, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2015-08-13 summary:In this paper we introduce a novel hash learning framework that has two maindistinguishing features, when compared to past approaches. First, it utilizescodewords in the Hamming space as ancillary means to accomplish its hashlearning task. These codewords, which are inferred from the data, attempt tocapture similarity aspects of the data's hash codes. Secondly and moreimportantly, the same framework is capable of addressing supervised,unsupervised and, even, semi-supervised hash learning tasks in a naturalmanner. A series of comparative experiments focused on content-based imageretrieval highlights its performance advantages.
arxiv-12000-177 | A Deep Pyramid Deformable Part Model for Face Detection | http://arxiv.org/pdf/1508.04389v1.pdf | author:Rajeev Ranjan, Vishal M. Patel, Rama Chellappa category:cs.CV published:2015-08-18 summary:We present a face detection algorithm based on Deformable Part Models anddeep pyramidal features. The proposed method called DP2MFD is able to detectfaces of various sizes and poses in unconstrained conditions. It reduces thegap in training and testing of DPM on deep features by adding a normalizationlayer to the deep convolutional neural network (CNN). Extensive experiments onfour publicly available unconstrained face detection datasets show that ourmethod is able to capture the meaningful structure of faces and performssignificantly better than many competitive face detection algorithms.
arxiv-12000-178 | ESDF: Ensemble Selection using Diversity and Frequency | http://arxiv.org/pdf/1508.04333v1.pdf | author:Shouvick Mondal, Arko Banerjee category:cs.LG 62H30 published:2015-08-18 summary:Recently ensemble selection for consensus clustering has emerged as aresearch problem in Machine Intelligence. Normally consensus clusteringalgorithms take into account the entire ensemble of clustering, where there isa tendency of generating a very large size ensemble before computing itsconsensus. One can avoid considering the entire ensemble and can judiciouslyselect few partitions in the ensemble without compromising on the quality ofthe consensus. This may result in an efficient consensus computation techniqueand may save unnecessary computational overheads. The ensemble selectionproblem addresses this issue of consensus clustering. In this paper, we proposean efficient method of ensemble selection for a large ensemble. We prioritizethe partitions in the ensemble based on diversity and frequency. Our methodselects top K of the partitions in order of priority, where K is decided by theuser. We observe that considering jointly the diversity and frequency helps inidentifying few representative partitions whose consensus is qualitativelybetter than the consensus of the entire ensemble. Experimental analysis on alarge number of datasets shows our method gives better results than earlierensemble selection methods.
arxiv-12000-179 | Cascade Learning by Optimally Partitioning | http://arxiv.org/pdf/1508.04326v1.pdf | author:Yanwei Pang, Jiale Cao, Xuelong Li category:cs.CV cs.LG published:2015-08-18 summary:Cascaded AdaBoost classifier is a well-known efficient object detectionalgorithm. The cascade structure has many parameters to be determined. Most ofexisting cascade learning algorithms are designed by assigning detection rateand false positive rate to each stage either dynamically or statically. Theirobjective functions are not directly related to minimum computation cost. Thesealgorithms are not guaranteed to have optimal solution in the sense ofminimizing computation cost. On the assumption that a strong classifier isgiven, in this paper we propose an optimal cascade learning algorithm (we callit iCascade) which iteratively partitions the strong classifiers into two partsuntil predefined number of stages are generated. iCascade searches the optimalnumber ri of weak classifiers of each stage i by directly minimizing thecomputation cost of the cascade. Theorems are provided to guarantee theexistence of the unique optimal solution. Theorems are also given for theproposed efficient algorithm of searching optimal parameters ri. Once a newstage is added, the parameter ri for each stage decreases gradually asiteration proceeds, which we call decreasing phenomenon. Moreover, with thegoal of minimizing computation cost, we develop an effective algorithm forsetting the optimal threshold of each stage classifier. In addition, we provein theory why more new weak classifiers are required compared to the laststage. Experimental results on face detection demonstrate the effectiveness andefficiency of the proposed algorithm.
arxiv-12000-180 | What is Holding Back Convnets for Detection? | http://arxiv.org/pdf/1508.02844v2.pdf | author:Bojan Pepik, Rodrigo Benenson, Tobias Ritschel, Bernt Schiele category:cs.CV published:2015-08-12 summary:Convolutional neural networks have recently shown excellent results ingeneral object detection and many other tasks. Albeit very effective, theyinvolve many user-defined design choices. In this paper we want to betterunderstand these choices by inspecting two key aspects "what did the networklearn?", and "what can the network learn?". We exploit new annotations(Pascal3D+), to enable a new empirical analysis of the R-CNN detector. Despitecommon belief, our results indicate that existing state-of-the-art convnetarchitectures are not invariant to various appearance factors. In fact, allconsidered networks have similar weak points which cannot be mitigated bysimply increasing the training data (architectural changes are needed). We showthat overall performance can improve when using image renderings for dataaugmentation. We report the best known results on the Pascal3D+ detection andview-point estimation tasks.
arxiv-12000-181 | Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo | http://arxiv.org/pdf/1508.04319v1.pdf | author:Markus Heinonen, Henrik Mannerstr√∂m, Juho Rousu, Samuel Kaski, Harri L√§hdesm√§ki category:stat.ML published:2015-08-18 summary:We present a novel approach for fully non-stationary Gaussian processregression (GPR), where all three key parameters -- noise variance, signalvariance and lengthscale -- can be simultaneously input-dependent. We developgradient-based inference methods to learn the unknown function and thenon-stationary model parameters, without requiring any model approximations. Wepropose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC),which conveniently extends the analytical gradient-based GPR learning byguiding the sampling with model gradients. We also learn the MAP solution fromthe posterior by gradient ascent. In experiments on several synthetic datasetsand in modelling of temporal gene expression, the nonstationary GPR is shown tobe necessary for modeling realistic input-dependent dynamics, while it performscomparably to conventional stationary or previous non-stationary GPR modelsotherwise.
arxiv-12000-182 | Deep clustering: Discriminative embeddings for segmentation and separation | http://arxiv.org/pdf/1508.04306v1.pdf | author:John R. Hershey, Zhuo Chen, Jonathan Le Roux, Shinji Watanabe category:cs.NE cs.LG stat.ML published:2015-08-18 summary:We address the problem of acoustic source separation in a deep learningframework we call "deep clustering." Rather than directly estimating signals ormasking functions, we train a deep network to produce spectrogram embeddingsthat are discriminative for partition labels given in training data. Previousdeep network approaches provide great advantages in terms of learning power andspeed, but previously it has been unclear how to use them to separate signalsin a class-independent way. In contrast, spectral clustering approaches areflexible with respect to the classes and number of items to be segmented, butit has been unclear how to leverage the learning power and speed of deepnetworks. To obtain the best of both worlds, we use an objective function thatto train embeddings that yield a low-rank approximation to an ideal pairwiseaffinity matrix, in a class-independent way. This avoids the high cost ofspectral factorization and instead produces compact clusters that are amenableto simple clustering methods. The segmentations are therefore implicitlyencoded in the embeddings, and can be "decoded" by clustering. Preliminaryexperiments show that the proposed method can separate speech: when trained onspectrogram features containing mixtures of two speakers, and tested onmixtures of a held-out set of speakers, it can infer masking functions thatimprove signal quality by around 6dB. We show that the model can generalize tothree-speaker mixtures despite training only on two-speaker mixtures. Theframework can be used without class labels, and therefore has the potential tobe trained on a diverse set of sound types, and to generalize to novel sources.We hope that future work will lead to segmentation of arbitrary sounds, withextensions to microphone array methods as well as image segmentation and otherdomains.
arxiv-12000-183 | Multimodal Deep Learning for Robust RGB-D Object Recognition | http://arxiv.org/pdf/1507.06821v2.pdf | author:Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello, Martin Riedmiller, Wolfram Burgard category:cs.CV cs.LG cs.NE cs.RO published:2015-07-24 summary:Robust object recognition is a crucial ingredient of many, if not all,real-world robotics applications. This paper leverages recent progress onConvolutional Neural Networks (CNNs) and proposes a novel RGB-D architecturefor object recognition. Our architecture is composed of two separate CNNprocessing streams - one for each modality - which are consecutively combinedwith a late fusion network. We focus on learning with imperfect sensor data, atypical problem in real-world robotics tasks. For accurate learning, weintroduce a multi-stage training methodology and two crucial ingredients forhandling depth data with CNNs. The first, an effective encoding of depthinformation for CNNs that enables learning without the need for large depthdatasets. The second, a data augmentation scheme for robust learning with depthimages by corrupting them with realistic noise patterns. We presentstate-of-the-art results on the RGB-D object dataset and show recognition inchallenging RGB-D real-world noisy settings.
arxiv-12000-184 | Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems | http://arxiv.org/pdf/1508.03391v2.pdf | author:Pei-Hao Su, David Vandyke, Milica Gasic, Nikola Mrksic, Tsung-Hsien Wen, Steve Young category:cs.LG cs.CL published:2015-08-14 summary:Statistical spoken dialogue systems have the attractive property of beingable to be optimised from data via interactions with real users. However in thereinforcement learning paradigm the dialogue manager (agent) often requiressignificant time to explore the state-action space to learn to behave in adesirable manner. This is a critical issue when the system is trained on-linewith real users where learning costs are expensive. Reward shaping is onepromising technique for addressing these concerns. Here we examine threerecurrent neural network (RNN) approaches for providing reward shapinginformation in addition to the primary (task-orientated) environmentalfeedback. These RNNs are trained on returns from dialogues generated by asimulated user and attempt to diffuse the overall evaluation of the dialogueback down to the turn level to guide the agent towards good behaviour faster.In both simulated and real user scenarios these RNNs are shown to increasepolicy learning speed. Importantly, they do not require prior knowledge of theuser's goal.
arxiv-12000-185 | Probabilistic Modelling of Morphologically Rich Languages | http://arxiv.org/pdf/1508.04271v1.pdf | author:Jan A. Botha category:cs.CL I.2.7; I.2.6 published:2015-08-18 summary:This thesis investigates how the sub-structure of words can be accounted forin probabilistic models of language. Such models play an important role innatural language processing tasks such as translation or speech recognition,but often rely on the simplistic assumption that words are opaque symbols. Thisassumption does not fit morphologically complex language well, where words canhave rich internal structure and sub-word elements are shared across distinctword forms. Our approach is to encode basic notions of morphology into the assumptions ofthree different types of language models, with the intention that leveragingshared sub-word structure can improve model performance and help overcome datasparsity that arises from morphological processes. In the context of n-gram language modelling, we formulate a new Bayesianmodel that relies on the decomposition of compound words to attain bettersmoothing, and we develop a new distributed language model that learns vectorrepresentations of morphemes and leverages them to link togethermorphologically related words. In both cases, we show that accounting for wordsub-structure improves the models' intrinsic performance and provides benefitswhen applied to other tasks, including machine translation. We then shift the focus beyond the modelling of word sequences and considermodels that automatically learn what the sub-word elements of a given languageare, given an unannotated list of words. We formulate a novel model that canlearn discontiguous morphemes in addition to the more conventional contiguousmorphemes that most previous models are limited to. This approach isdemonstrated on Semitic languages, and we find that modelling discontiguoussub-word structures leads to improvements in the task of segmenting words intotheir contiguous morphemes.
arxiv-12000-186 | Preprint ARPPS Augmented Reality Pipeline Prospect System | http://arxiv.org/pdf/1508.04238v1.pdf | author:Xiaolei Zhang, Yong Han, DongSheng Hao, Zhihan Lv category:cs.CV published:2015-08-18 summary:This is the preprint version of our paper on ICONIP. Outdoor augmentedreality geographic information system (ARGIS) is the hot application ofaugmented reality over recent years. This paper concludes the key solutions ofARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),and respectively realizes the machine vision based pipeline prospect system(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With theMVBPPS's realization, this paper studies the neural network based 3D featuresmatching method.
arxiv-12000-187 | Minimizing the Number of Matching Queries for Object Retrieval | http://arxiv.org/pdf/1412.5808v3.pdf | author:Johannes Niedermayer, Peer Kr√∂ger category:cs.CV published:2014-12-18 summary:To increase the computational efficiency of interest-point based objectretrieval, researchers have put remarkable research efforts into improving theefficiency of kNN-based feature matching, pursuing to match thousands offeatures against a database within fractions of a second. However, due to thehigh-dimensional nature of image features that reduces the effectivity of indexstructures (curse of dimensionality), due to the vast amount of features storedin image databases (images are often represented by up to several thousandfeatures), this ultimate goal demanded to trade query runtimes for queryprecision. In this paper we address an approach complementary to indexing inorder to improve the runtimes of retrieval by querying only the most promisingkeypoint descriptors, as this affects matching runtimes linearly and cantherefore lead to increased efficiency. As this reduction of kNN queriesreduces the number of tentative correspondences, a loss of query precision isminimized by an additional image-level correspondence generation stage with acomputational performance independent of the underlying indexing structure. Weevaluate such an adaption of the standard recognition pipeline on a variety ofdatasets using both SIFT and state-of-the-art binary descriptors. Our resultssuggest that decreasing the number of queried descriptors does not necessarilyimply a reduction in the result quality as long as alternative ways ofincreasing query recall (by thoroughly selecting k) and MAP (using image-levelcorrespondence generation) are considered.
arxiv-12000-188 | Image tag completion by local learning | http://arxiv.org/pdf/1508.04224v1.pdf | author:Jingyan Wang, Yihua Zhou, Haoxiang Wang, Xiaohong Yang, Feng Yang, Austin Peterson category:cs.CV published:2015-08-18 summary:The problem of tag completion is to learn the missing tags of an image. Inthis paper, we propose to learn a tag scoring vector for each image by locallinear learning. A local linear function is used in the neighborhood of eachimage to predict the tag scoring vectors of its neighboring images. Weconstruct a unified objective function for the learning of both tag scoringvectors and local linear function parame- ters. In the objective, we impose thelearned tag scoring vectors to be consistent with the known associations to thetags of each image, and also minimize the prediction error of each local linearfunction, while reducing the complexity of each local function. The objectivefunction is optimized by an alternate optimization strategy and gradientdescent methods in an iterative algorithm. We compare the proposed algorithmagainst different state-of-the-art tag completion methods, and the results showits advantages.
arxiv-12000-189 | Representing data by sparse combination of contextual data points for classification | http://arxiv.org/pdf/1507.00019v2.pdf | author:Jingyan Wang, Yihua Zhou, Ming Yin, Shaochang Chen, Benjamin Edwards category:cs.CV published:2015-06-30 summary:In this paper, we study the problem of using contextual da- ta points of adata point for its classification problem. We propose to represent a data pointas the sparse linear reconstruction of its context, and learn the sparsecontext to gather with a linear classifier in a su- pervised way to increaseits discriminative ability. We proposed a novel formulation for contextlearning, by modeling the learning of context reconstruction coefficients andclassifier in a unified objective. In this objective, the reconstruction erroris minimized and the coefficient spar- sity is encouraged. Moreover, the hingeloss of the classifier is minimized and the complexity of the classifier isreduced. This objective is opti- mized by an alternative strategy in aniterative algorithm. Experiments on three benchmark data set show its advantageover state-of-the-art context-based data representation and classificationmethods.
arxiv-12000-190 | When Are Tree Structures Necessary for Deep Learning of Representations? | http://arxiv.org/pdf/1503.00185v5.pdf | author:Jiwei Li, Minh-Thang Luong, Dan Jurafsky, Eudard Hovy category:cs.AI cs.CL published:2015-02-28 summary:Recursive neural models, which use syntactic parse trees to recursivelygenerate representations bottom-up, are a popular architecture. But there havenot been rigorous evaluations showing for exactly which tasks this syntax-basedmethod is appropriate. In this paper we benchmark {\bf recursive} neural modelsagainst sequential {\bf recurrent} neural models (simple recurrent and LSTMmodels), enforcing apples-to-apples comparison as much as possible. Weinvestigate 4 tasks: (1) sentiment classification at the sentence level andphrase level; (2) matching questions to answer-phrases; (3) discourse parsing;(4) semantic relation extraction (e.g., {\em component-whole} between nouns). Our goal is to understand better when, and why, recursive models canoutperform simpler models. We find that recursive models help mainly on tasks(like semantic relation extraction) that require associating headwords across along distance, particularly on very long sequences. We then introduce a methodfor allowing recurrent models to achieve similar performance: breaking longsentences into clause-like units at punctuation and processing them separatelybefore combining. Our results thus help understand the limitations of bothclasses of models, and suggest directions for improving recurrent models.
arxiv-12000-191 | Supervised learning of sparse context reconstruction coefficients for data representation and classification | http://arxiv.org/pdf/1508.04221v1.pdf | author:Xuejie Liu, Jingbin Wang, Ming Yin, Benjamin Edwards, Peijuan Xu category:cs.LG cs.CV published:2015-08-18 summary:Context of data points, which is usually defined as the other data points ina data set, has been found to play important roles in data representation andclassification. In this paper, we study the problem of using context of a datapoint for its classification problem. Our work is inspired by the observationthat actually only very few data points are critical in the context of a datapoint for its representation and classification. We propose to represent a datapoint as the sparse linear combination of its context, and learn the sparsecontext in a supervised way to increase its discriminative ability. To thisend, we proposed a novel formulation for context learning, by modeling thelearning of context parameter and classifier in a unified objective, andoptimizing it with an alternative strategy in an iterative algorithm.Experiments on three benchmark data set show its advantage overstate-of-the-art context-based data representation and classification methods.
arxiv-12000-192 | Supervised cross-modal factor analysis for multiple modal data classification | http://arxiv.org/pdf/1502.05134v2.pdf | author:Jingbin Wang, Yihua Zhou, Kanghong Duan, Jim Jing-Yan Wang, Halima Bensmail category:cs.LG published:2015-02-18 summary:In this paper we study the problem of learning from multiple modal data forpurpose of document classification. In this problem, each document is composedtwo different modals of data, i.e., an image and a text. Cross-modal factoranalysis (CFA) has been proposed to project the two different modals of data toa shared data space, so that the classification of a image or a text can beperformed directly in this space. A disadvantage of CFA is that it has ignoredthe supervision information. In this paper, we improve CFA by incorporating thesupervision information to represent and classify both image and text modals ofdocuments. We project both image and text data to a shared data space by factoranalysis, and then train a class label predictor in the shared space to use theclass label information. The factor analysis parameter and the predictorparameter are learned jointly by solving one single objective function. Withthis objective function, we minimize the distance between the projections ofimage and text of the same document, and the classification error of theprojection measured by hinge loss function. The objective function is optimizedby an alternate optimization strategy in an iterative algorithm. Experiments intwo different multiple modal document data sets show the advantage of theproposed algorithm over other CFA methods.
arxiv-12000-193 | Scalable Bayesian Non-Negative Tensor Factorization for Massive Count Data | http://arxiv.org/pdf/1508.04211v1.pdf | author:Changwei Hu, Piyush Rai, Changyou Chen, Matthew Harding, Lawrence Carin category:stat.ML cs.LG published:2015-08-18 summary:We present a Bayesian non-negative tensor factorization model forcount-valued tensor data, and develop scalable inference algorithms (both batchand online) for dealing with massive tensors. Our generative model can handleoverdispersed counts as well as infer the rank of the decomposition. Moreover,leveraging a reparameterization of the Poisson distribution as a multinomialfacilitates conjugacy in the model and enables simple and efficient Gibbssampling and variational Bayes (VB) inference updates, with a computationalcost that only depends on the number of nonzeros in the tensor. The model alsoprovides a nice interpretability for the factors; in our model, each factorcorresponds to a "topic". We develop a set of online inference algorithms thatallow further scaling up the model to massive tensors, for which batchinference methods may be infeasible. We apply our framework on diversereal-world applications, such as \emph{multiway} topic modeling on a scientificpublications database, analyzing a political science data set, and analyzing amassive household transactions data set.
arxiv-12000-194 | Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors | http://arxiv.org/pdf/1508.04210v1.pdf | author:Changwei Hu, Piyush Rai, Lawrence Carin category:stat.ML cs.LG published:2015-08-18 summary:We present a scalable Bayesian model for low-rank factorization of massivetensors with binary observations. The proposed model has the following keyproperties: (1) in contrast to the models based on the logistic or probitlikelihood, using a zero-truncated Poisson likelihood for binary data allowsour model to scale up in the number of \emph{ones} in the tensor, which isespecially appealing for massive but sparse binary tensors; (2)side-information in form of binary pairwise relationships (e.g., an adjacencynetwork) between objects in any tensor mode can also be leveraged, which can beespecially useful in "cold-start" settings; and (3) the model admits simpleBayesian inference via batch, as well as \emph{online} MCMC; the latter allowsscaling up even for \emph{dense} binary data (i.e., when the number of ones inthe tensor/network is also massive). In addition, non-negative factor matricesin our model provide easy interpretability, and the tensor rank can be inferredfrom the data. We evaluate our model on several large-scale real-world binarytensors, achieving excellent computational scalability, and also demonstrateits usefulness in leveraging side-information provided in form ofmode-network(s).
arxiv-12000-195 | Molding CNNs for text: non-linear, non-consecutive convolutions | http://arxiv.org/pdf/1508.04112v2.pdf | author:Tao Lei, Regina Barzilay, Tommi Jaakkola category:cs.CL cs.AI published:2015-08-17 summary:The success of deep learning often derives from well-chosen operationalbuilding blocks. In this work, we revise the temporal convolution operation inCNNs to better adapt it to text processing. Instead of concatenating wordrepresentations, we appeal to tensor algebra and use low-rank n-gram tensors todirectly exploit interactions between words already at the convolution stage.Moreover, we extend the n-gram convolution to non-consecutive words torecognize patterns with intervening words. Through a combination of low-ranktensors, and pattern weighting, we can efficiently evaluate the resultingconvolution operation via dynamic programming. We test the resultingarchitecture on standard sentiment classification and news categorizationtasks. Our model achieves state-of-the-art performance both in terms ofaccuracy and training speed. For instance, we obtain 51.2% accuracy on thefine-grained sentiment classification task.
arxiv-12000-196 | Low Rank Representation on Riemannian Manifold of Square Root Densities | http://arxiv.org/pdf/1508.04198v1.pdf | author:Yifan Fu, Junbin Gao, Xia Hong, David Tien category:cs.CV published:2015-08-18 summary:In this paper, we present a novel low rank representation (LRR) algorithm fordata lying on the manifold of square root densities. Unlike traditional LRRmethods which rely on the assumption that the data points are vectors in theEuclidean space, our new algorithm is designed to incorporate the intrinsicgeometric structure and geodesic distance of the manifold. Experiments onseveral computer vision datasets showcase its noise robustness and superiorperformance on classification and subspace clustering compared to otherstate-of-the-art approaches.
arxiv-12000-197 | Action Recognition based on Subdivision-Fusion Model | http://arxiv.org/pdf/1508.04190v1.pdf | author:Hao Zongbo, Lu Linlin, Zhang Qianni, Wu Jie, Izquierdo Ebroul, Yang Juanyu, Zhao Jun category:cs.CV published:2015-08-18 summary:This paper proposes a novel Subdivision-Fusion Model (SFM) to recognize humanactions. In most action recognition tasks, overlapping feature distribution isa common problem leading to overfitting. In the subdivision stage of theproposed SFM, samples in each category are clustered. Then, such samples aregrouped into multiple more concentrated subcategories. Boundaries for thesubcategories are easier to find and as consequence overfitting is avoided. Inthe subsequent fusion stage, the multi-subcategories classification results areconverted back to the original category recognition problem. Two methods todetermine the number of clusters are provided. The proposed model has beenthoroughly tested with four popular datasets. In the Hollywood2 dataset, anaccuracy of 79.4% is achieved, outperforming the state-of-the-art accuracy of64.3%. The performance on the YouTube Action dataset has been improved from75.8% to 82.5%, while considerably improvements are also observed on the KTHand UCF50 datasets.
arxiv-12000-198 | Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big, Distributed Data | http://arxiv.org/pdf/1412.7839v2.pdf | author:Haroon Raja, Waheed U. Bajwa category:cs.LG cs.IT math.IT stat.ML published:2014-12-25 summary:This paper studies the problem of data-adaptive representations for big,distributed data. It is assumed that a number of geographically-distributed,interconnected sites have massive local data and they are interested incollaboratively learning a low-dimensional geometric structure underlying thesedata. In contrast to previous works on subspace-based data representations,this paper focuses on the geometric structure of a union of subspaces (UoS). Inthis regard, it proposes a distributed algorithm---termed cloud K-SVD---forcollaborative learning of a UoS structure underlying distributed data ofinterest. The goal of cloud K-SVD is to learn a common overcomplete dictionaryat each individual site such that every sample in the distributed data can berepresented through a small number of atoms of the learned dictionary. CloudK-SVD accomplishes this goal without requiring exchange of individual samplesbetween sites. This makes it suitable for applications where sharing of rawdata is discouraged due to either privacy concerns or large volumes of data.This paper also provides an analysis of cloud K-SVD that gives insights intoits properties as well as deviations of the dictionaries learned at individualsites from a centralized solution in terms of different measures oflocal/global data and topology of interconnections. Finally, the papernumerically illustrates the efficacy of cloud K-SVD on real and syntheticdistributed data.
arxiv-12000-199 | Evaluating Classifiers in Detecting 419 Scams in Bilingual Cybercriminal Communities | http://arxiv.org/pdf/1508.04123v1.pdf | author:Alex V. Mbaziira, Ehab Abozinadah, James H. Jones Jr category:cs.SI cs.CY cs.LG published:2015-08-17 summary:Incidents of organized cybercrime are rising because of criminals are reapinghigh financial rewards while incurring low costs to commit crime. As thedigital landscape broadens to accommodate more internet-enabled devices andtechnologies like social media, more cybercriminals who are not native Englishspeakers are invading cyberspace to cash in on quick exploits. In this paper weevaluate the performance of three machine learning classifiers in detecting 419scams in a bilingual Nigerian cybercriminal community. We use three popularclassifiers in text processing namely: Na\"ive Bayes, k-nearest neighbors (IBK)and Support Vector Machines (SVM). The preliminary results on a real worlddataset reveal the SVM significantly outperforms Na\"ive Bayes and IBK at 95%confidence level.
arxiv-12000-200 | Theory of Optimizing Pseudolinear Performance Measures: Application to F-measure | http://arxiv.org/pdf/1505.00199v3.pdf | author:Shameem A Puthiya Parambath, Nicolas Usunier, Yves Grandvalet category:cs.LG published:2015-05-01 summary:Non-linear performance measures are widely used for the evaluation oflearning algorithms. For example, $F$-measure is a commonly used performancemeasure for classification problems in machine learning and informationretrieval community. We study the theoretical properties of a subset ofnon-linear performance measures called pseudo-linear performance measures whichincludes $F$-measure, \emph{Jaccard Index}, among many others. We establishthat many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linearfunctions of the per-class false negatives and false positives for binary,multiclass and multilabel classification. Based on this observation, we presenta general reduction of such performance measure optimization problem tocost-sensitive classification problem with unknown costs. We then propose analgorithm with provable guarantees to obtain an approximately optimalclassifier for the $F$-measure by solving a series of cost-sensitiveclassification problems. The strength of our analysis is to be valid on anydataset and any class of classifiers, extending the existing theoreticalresults on pseudo-linear measures, which are asymptotic in nature. We alsoestablish the multi-objective nature of the $F$-score maximization problem bylinking the algorithm with the weighted-sum approach used in multi-objectiveoptimization. We present numerical experiments to illustrate the relativeimportance of cost asymmetry and thresholding when learning linear classifierson various $F$-measure optimization tasks.
arxiv-12000-201 | EventNet: A Large Scale Structured Concept Library for Complex Event Detection in Video | http://arxiv.org/pdf/1506.02328v2.pdf | author:Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, Shih-Fu Chang category:cs.CV published:2015-06-08 summary:Event-specific concepts are the semantic concepts designed for the events ofinterest, which can be used as a mid-level representation of complex events invideos. Existing methods only focus on defining event-specific concepts for asmall number of predefined events, but cannot handle novel unseen events. Thismotivates us to build a large scale event-specific concept library that coversas many real-world events and their concepts as possible. Specifically, wechoose WikiHow, an online forum containing a large number of how-to articles onhuman daily life events. We perform a coarse-to-fine event discovery processand discover 500 events from WikiHow articles. Then we use each event name asquery to search YouTube and discover event-specific concepts from the tags ofreturned videos. After an automatic filter process, we end up with 95,321videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) modelon the 95,321 videos over the 500 events, and use the model to extract deeplearning feature from video content. With the learned deep learning feature, wetrain 4,490 binary SVM classifiers as the event-specific concept library. Theconcepts and events are further organized in a hierarchical structure definedby WikiHow, and the resultant concept library is called EventNet. Finally, theEventNet concept library is used to generate concept based representation ofevent videos. To the best of our knowledge, EventNet represents the first videoevent ontology that organizes events and their concepts into a semanticstructure. It offers great potential for event retrieval and browsing.Extensive experiments over the zero-shot event retrieval task when no trainingsamples are available show that the EventNet concept library consistently andsignificantly outperforms the state-of-the-art (such as the 20K ImageNetconcepts trained with CNN) by a large margin up to 207%.
arxiv-12000-202 | A Deep Learning Approach to Structured Signal Recovery | http://arxiv.org/pdf/1508.04065v1.pdf | author:Ali Mousavi, Ankit B. Patel, Richard G. Baraniuk category:cs.LG stat.ML published:2015-08-17 summary:In this paper, we develop a new framework for sensing and recoveringstructured signals. In contrast to compressive sensing (CS) systems that employlinear measurements, sparse representations, and computationally complexconvex/greedy algorithms, we introduce a deep learning framework that supportsboth linear and mildly nonlinear measurements, that learns a structuredrepresentation from training data, and that efficiently computes a signalestimate. In particular, we apply a stacked denoising autoencoder (SDA), as anunsupervised feature learner. SDA enables us to capture statisticaldependencies between the different elements of certain signals and improvesignal recovery performance as compared to the CS approach.
arxiv-12000-203 | A Generative Model for Multi-Dialect Representation | http://arxiv.org/pdf/1508.04035v1.pdf | author:Emmanuel N. Osegi category:cs.CV cs.LG stat.ML published:2015-08-17 summary:In the era of deep learning several unsupervised models have been developedto capture the key features in unlabeled handwritten data. Popular among themis the Restricted Boltzmann Machines RBM. However, due to the novelty inhandwritten multidialect data, the RBM may fail to generate an efficientrepresentation. In this paper we propose a generative model, the ModeSynthesizing Machine MSM for on-line representation of real life handwrittenmultidialect language data. The MSM takes advantage of the hierarchicalrepresentation of the modes of a data distribution using a two-point errorupdate to learn a sequence of representative multidialects in a generative way.Experiments were performed to evaluate the performance of the MSM over the RBMwith the former attaining much lower error values than the latter on bothindependent and mixed data set.
arxiv-12000-204 | A complex network approach to stylometry | http://arxiv.org/pdf/1506.09107v2.pdf | author:Diego R. Amancio category:cs.CL published:2015-06-30 summary:Statistical methods have been widely employed to study the fundamentalproperties of language. In recent years, methods from complex and dynamicalsystems proved useful to create several language models. Despite the largeamount of studies devoted to represent texts with physical models, only alimited number of studies have shown how the properties of the underlyingphysical systems can be employed to improve the performance of natural languageprocessing tasks. In this paper, I address this problem by devising complexnetworks methods that are able to improve the performance of currentstatistical methods. Using a fuzzy classification strategy, I show that thetopological properties extracted from texts complement the traditional textualdescription. In several cases, the performance obtained with hybrid approachesoutperformed the results obtained when only traditional or networked methodswere used. Because the proposed model is generic, the framework devised herecould be straightforwardly used to study similar textual applications where thetopology plays a pivotal role in the description of the interacting agents.
arxiv-12000-205 | Lifted Representation of Relational Causal Models Revisited: Implications for Reasoning and Structure Learning | http://arxiv.org/pdf/1508.02103v2.pdf | author:Sanghack Lee, Vasant Honavar category:cs.AI cs.LG published:2015-08-10 summary:Maier et al. (2010) introduced the relational causal model (RCM) forrepresenting and inferring causal relationships in relational data. A liftedrepresentation, called abstract ground graph (AGG), plays a central role inreasoning with and learning of RCM. The correctness of the algorithm proposedby Maier et al. (2013a) for learning RCM from data relies on the soundness andcompleteness of AGG for relational d-separation to reduce the learning of anRCM to learning of an AGG. We revisit the definition of AGG and show that AGG,as defined in Maier et al. (2013b), does not correctly abstract all groundgraphs. We revise the definition of AGG to ensure that it correctly abstractsall ground graphs. We further show that AGG representation is not complete forrelational d-separation, that is, there can exist conditional independencerelations in an RCM that are not entailed by AGG. A careful examination of therelationship between the lack of completeness of AGG for relationald-separation and faithfulness conditions suggests that weaker notions ofcompleteness, namely adjacency faithfulness and orientation faithfulnessbetween an RCM and its AGG, can be used to learn an RCM from data.
arxiv-12000-206 | Tomographic Image Reconstruction using Training images | http://arxiv.org/pdf/1503.01993v2.pdf | author:Sara Soltani, Martin S. Andersen, Per Christian Hansen category:cs.CV math.NA 65F22, 65K10 published:2015-03-06 summary:We describe and examine an algorithm for tomographic image reconstructionwhere prior knowledge about the solution is available in the form of trainingimages. We first construct a nonnegative dictionary based on prototype elementsfrom the training images; this problem is formulated as a regularizednon-negative matrix factorization. Incorporating the dictionary as a prior in aconvex reconstruction problem, we then find an approximate solution with asparse representation in the dictionary. The dictionary is applied tonon-overlapping patches of the image, which reduces the computationalcomplexity compared to other algorithms. Computational experiments clarify thechoice and interplay of the model parameters and the regularization parameters,and we show that in few-projection low-dose settings our algorithm iscompetitive with total variation regularization and tends to include moretexture and more correct edges.
arxiv-12000-207 | Sense Beyond Expressions: Cuteness | http://arxiv.org/pdf/1508.03953v1.pdf | author:Kang Wang, Tam V. Nguyen, Jiashi Feng, Jose Sepulveda category:cs.CV published:2015-08-17 summary:With the development of Internet culture, cuteness has become a popularconcept. Many people are curious about what factors making a person look cute.However, there is rare research to answer this interesting question. In thiswork, we construct a dataset of personal images with comprehensively annotatedcuteness scores and facial attributes to investigate this high-level concept indepth. Based on this dataset, through an automatic attributes mining process,we find several critical attributes determining the cuteness of a person. Wealso develop a novel Continuous Latent Support Vector Machine (C-LSVM) methodto predict the cuteness score of one person given only his image. Extensiveevaluations validate the effectiveness of the proposed method for cutenessprediction.
arxiv-12000-208 | LCNN: Low-level Feature Embedded CNN for Salient Object Detection | http://arxiv.org/pdf/1508.03928v1.pdf | author:Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price category:cs.CV published:2015-08-17 summary:In this paper, we propose a novel deep neural network framework embedded withlow-level features (LCNN) for salient object detection in complex images. Weutilise the advantage of convolutional neural networks to automatically learnthe high-level features that capture the structured information and semanticcontext in the image. In order to better adapt a CNN model into the saliencytask, we redesign the network architecture based on the small-scale datasets.Several low-level features are extracted, which can effectively capturecontrast and spatial information in the salient regions, and incorporated tocompensate with the learned high-level features at the output of the last fullyconnected layer. The concatenated feature vector is further fed into ahinge-loss SVM detector in a joint discriminative learning manner and the finalsaliency score of each region within the bounding box is obtained by the linearcombination of the detector's weights. Experiments on three challengingbenchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to beeffective and superior than most low-level oriented state-of-the-arts in termsof P-R curves, F-measure and mean absolute errors.
arxiv-12000-209 | Using a Machine Learning Approach to Implement and Evaluate Product Line Features | http://arxiv.org/pdf/1508.03906v1.pdf | author:Davide Bacciu, Stefania Gnesi, Laura Semini category:cs.SE cs.LG published:2015-08-17 summary:Bike-sharing systems are a means of smart transportation in urbanenvironments with the benefit of a positive impact on urban mobility. In thispaper we are interested in studying and modeling the behavior of features thatpermit the end user to access, with her/his web browser, the status of theBike-Sharing system. In particular, we address features able to make aprediction on the system state. We propose to use a machine learning approachto analyze usage patterns and learn computational models of such features fromlogs of system usage. On the one hand, machine learning methodologies provide a powerful andgeneral means to implement a wide choice of predictive features. On the otherhand, trained machine learning models are provided with a measure of predictiveperformance that can be used as a metric to assess the cost-performancetrade-off of the feature. This provides a principled way to assess the runtimebehavior of different components before putting them into operation.
arxiv-12000-210 | Network Routing Optimization Using Swarm Intelligence | http://arxiv.org/pdf/1209.3909v2.pdf | author:Mohamed A. El Galil category:cs.NE cs.DM published:2012-09-18 summary:The aim of this paper is to highlight and explore a traditional problem,which is the minimum spanning tree, and finding the shortest-path in networkrouting, by using Swarm Intelligence. This work to be considered as aninvestigation topic with combination between operations research, discretemathematics, and evolutionary computing aiming to solve one of networkingproblems.
arxiv-12000-211 | Two-stage Cascaded Classifier for Purchase Prediction | http://arxiv.org/pdf/1508.03856v1.pdf | author:Sheikh Muhammad Sarwar, Mahamudul Hasan, Dmitry I. Ignatov category:cs.IR cs.LG published:2015-08-16 summary:In this paper we describe our machine learning solution for the RecSysChallenge, 2015. We have proposed a time efficient two-stage cascadedclassifier for the prediction of buy sessions and purchased items within suchsessions. Based on the model, several interesting features found, and formationof our own test bed, we have achieved a reasonable score. Usage of RandomForests helps us to cope with the effect of the multiplicity of good modelsdepending on varying subsets of features in the purchased items prediction and,in its turn, boosting is used as a suitable technique to overcome severe classimbalance of the buy-session prediction.
arxiv-12000-212 | Online Representation Learning in Recurrent Neural Language Models | http://arxiv.org/pdf/1508.03854v1.pdf | author:Marek Rei category:cs.CL cs.LG cs.NE published:2015-08-16 summary:We investigate an extension of continuous online learning in recurrent neuralnetwork language models. The model keeps a separate vector representation ofthe current unit of text being processed and adaptively adjusts it after eachprediction. The initial experiments give promising results, indicating that themethod is able to increase language modelling accuracy, while also decreasingthe parameters needed to store the model along with the computation required ateach step.
arxiv-12000-213 | Talking to the crowd: What do people react to in online discussions? | http://arxiv.org/pdf/1507.02205v2.pdf | author:Aaron Jaech, Victoria Zayats, Hao Fang, Mari Ostendorf, Hannaneh Hajishirzi category:cs.CL cs.SI published:2015-07-08 summary:This paper addresses the question of how language use affects communityreaction to comments in online discussion forums, and the relative importanceof the message vs. the messenger. A new comment ranking task is proposed basedon community annotated karma in Reddit discussions, which controls for topicand timing of comments. Experimental work with discussion threads from sixsubreddits shows that the importance of different types of language featuresvaries with the community of interest.
arxiv-12000-214 | What Your Username Says About You | http://arxiv.org/pdf/1507.02045v2.pdf | author:Aaron Jaech, Mari Ostendorf category:cs.CL published:2015-07-08 summary:Usernames are ubiquitous on the Internet, and they are often suggestive ofuser demographics. This work looks at the degree to which gender and languagecan be inferred from a username alone by making use of unsupervised morphologyinduction to decompose usernames into sub-units. Experimental results on thetwo tasks demonstrate the effectiveness of the proposed morphological featurescompared to a character n-gram baseline.
arxiv-12000-215 | Schema Independent Relational Learning | http://arxiv.org/pdf/1508.03846v1.pdf | author:Jose Picado, Arash Termehchy, Alan Fern category:cs.DB cs.AI cs.LG cs.LO published:2015-08-16 summary:Learning novel and interesting concepts and relations from relationaldatabases is an important problem with many applications in database systemsand machine learning. Relational learning algorithms generally leverage theproperties of the database schema to find the definition of the target conceptin terms of the existing relations in the database. Nevertheless, it is wellestablished that the same data set may be represented under different schemasfor various reasons, such as efficiency, data quality, and usability.Unfortunately, many current learning algorithms tend to vary quitesubstantially over the choice of schema, both in terms of learning accuracy andefficiency, which complicates their off-the-shelf application. In this paper,we formalize the property of schema independence of relational learningalgorithms, and study both the theoretical and empirical dependence of existingalgorithms on the common class of vertical (de)composition schematransformations. We study both sample-based learning algorithms, which learnfrom sets of labeled examples, and query-based algorithms, which learn byasking queries to a user. For sample-based algorithms we consider the two mainalgorithm classes: top-down and bottom-up. We prove that practical top-downalgorithms are generally not schema independent, while, in contrast, twobottom-up algorithms Golem and ProGolem are schema independent with somemodifications. For query-based learning algorithms we show that the vertical(de)composition transformations influence their learning efficiency. We supportthe theoretical results with an empirical study that demonstrates the schemadependence/independence of several algorithms on existing benchmark data setsunder natural vertical (de)compositions.
arxiv-12000-216 | A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution | http://arxiv.org/pdf/1508.03826v1.pdf | author:Shaohua Li, Jun Zhu, Chunyan Miao category:cs.CL cs.LG stat.ML published:2015-08-16 summary:Most existing word embedding methods can be categorized into Neural EmbeddingModels and Matrix Factorization (MF)-based methods. However some models areopaque to probabilistic interpretation, and MF-based methods, typically solvedusing Singular Value Decomposition (SVD), may incur loss of corpus information.In addition, it is desirable to incorporate global latent factors, such astopics, sentiments or writing styles, into the word embedding model. Sincegenerative models provide a principled way to incorporate latent factors, wepropose a generative word embedding model, which is easy to interpret, and canserve as a basis of more sophisticated latent factor models. The modelinference reduces to a low rank weighted positive semidefinite approximationproblem. Its optimization is approached by eigendecomposition on a submatrix,followed by online blockwise regression, which is scalable and avoids theinformation loss in SVD. In experiments on 7 common benchmark datasets, ourvectors are competitive to word2vec, and better than other MF-based methods.
arxiv-12000-217 | Beat-Event Detection in Action Movie Franchises | http://arxiv.org/pdf/1508.03755v1.pdf | author:Danila Potapov, Matthijs Douze, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid category:cs.CV published:2015-08-15 summary:While important advances were recently made towards temporally localizing andrecognizing specific human actions or activities in videos, efficient detectionand classification of long video chunks belonging to semantically definedcategories such as "pursuit" or "romance" remains challenging.We introduce anew dataset, Action Movie Franchises, consisting of a collection of Hollywoodaction movie franchises. We define 11 non-exclusive semantic categories -called beat-categories - that are broad enough to cover most of the moviefootage. The corresponding beat-events are annotated as groups of video shots,possibly overlapping.We propose an approach for localizing beat-events based onclassifying shots into beat-categories and learning the temporal constraintsbetween shots. We show that temporal constraints significantly improve theclassification performance. We set up an evaluation protocol for beat-eventlocalization as well as for shot classification, depending on whether moviesfrom the same franchise are present or not in the training data.
arxiv-12000-218 | Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network | http://arxiv.org/pdf/1503.04596v3.pdf | author:Mark D. McDonnell, Tony Vladusich category:cs.NE cs.CV cs.LG published:2015-03-16 summary:We present a neural network architecture and training method designed toenable very rapid training and low implementation complexity. Due to itstraining speed and very few tunable parameters, the method has strong potentialfor applications requiring frequent retraining or online training. The approachis characterized by (a) convolutional filters based on biologically inspiredvisual processing filters, (b) randomly-valued classifier-stage input weights,(c) use of least squares regression to train the classifier output weights in asingle batch, and (d) linear classifier-stage output units. We demonstrate theefficacy of the method by applying it to image classification. Our resultsmatch existing state-of-the-art results on the MNIST (0.37% error) andNORB-small (2.2% error) image classification databases, but with very fasttraining times compared to standard deep network approaches. The network'sperformance on the Google Street View House Number (SVHN) (4% error) databaseis also competitive with state-of-the art methods.
arxiv-12000-219 | Statistical Inference of Intractable Generative Models via Classification | http://arxiv.org/pdf/1407.4981v2.pdf | author:Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander category:stat.CO stat.ME stat.ML published:2014-07-18 summary:Increasingly complex generative models are being used across the disciplinesas they allow for realistic characterization of data, but a common difficultywith them is the prohibitively large computational cost to performlikelihood-based statistical inference. We consider here a likelihood-freeframework where inference is done by identifying parameter values whichgenerate simulated data adequately resembling the observed data. A majordifficulty is how to measure the discrepancy between the simulated and observeddata. Transforming the original problem into a problem of classifying the datainto simulated versus observed, we find that classification accuracy can beused to assess the discrepancy. The complete arsenal of classification methodsbecomes thereby available for inference of intractable generative models.
arxiv-12000-220 | Diversity in Spectral Learning for Natural Language Parsing | http://arxiv.org/pdf/1506.00275v2.pdf | author:Shashi Narayan, Shay B. Cohen category:cs.CL published:2015-05-31 summary:We describe an approach to create a diverse set of predictions with spectrallearning of latent-variable PCFGs (L-PCFGs). Our approach works by creatingmultiple spectral models where noise is added to the underlying features in thetraining set before the estimation of each model. We describe three ways todecode with multiple models. In addition, we describe a simple variant of thespectral algorithm for L-PCFGs that is fast and leads to compact models. Ourexperiments for natural language parsing, for English and German, show that weget a significant improvement over baselines comparable to state of the art.For English, we achieve the $F_1$ score of 90.18, and for German we achieve the$F_1$ score of 83.38.
arxiv-12000-221 | A Comparative Study on Regularization Strategies for Embedding-based Neural Networks | http://arxiv.org/pdf/1508.03721v1.pdf | author:Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin category:cs.CL cs.LG published:2015-08-15 summary:This paper aims to compare different regularization strategies to address acommon phenomenon, severe overfitting, in embedding-based neural networks forNLP. We chose two widely studied neural models and tasks as our testbed. Wetried several frequently applied or newly proposed regularization strategies,including penalizing weights (embeddings excluded), penalizing embeddings,re-embedding words, and dropout. We also emphasized on incrementalhyperparameter tuning, and combining different regularizations. The resultsprovide a picture on tuning hyperparameters for neural NLP models.
arxiv-12000-222 | Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path | http://arxiv.org/pdf/1508.03720v1.pdf | author:Xu Yan, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, Zhi Jin category:cs.CL cs.LG published:2015-08-15 summary:Relation classification is an important research arena in the field ofnatural language processing (NLP). In this paper, we present SDP-LSTM, a novelneural network to classify the relation of two entities in a sentence. Ourneural architecture leverages the shortest dependency path (SDP) between twoentities; multichannel recurrent neural networks, with long short term memory(LSTM) units, pick up heterogeneous information along the SDP. Our proposedmodel has several distinct features: (1) The shortest dependency paths retainmost relevant information (to relation classification), while eliminatingirrelevant words in the sentence. (2) The multichannel LSTM networks alloweffective information integration from heterogeneous sources over thedependency paths. (3) A customized dropout strategy regularizes the neuralnetwork to alleviate overfitting. We test our model on the SemEval 2010relation classification task, and achieve an $F_1$-score of 83.7\%, higher thancompeting methods in the literature.
arxiv-12000-223 | Modeling Relation Paths for Representation Learning of Knowledge Bases | http://arxiv.org/pdf/1506.00379v2.pdf | author:Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, Song Liu category:cs.CL published:2015-06-01 summary:Representation learning of knowledge bases (KBs) aims to embed both entitiesand relations into a low-dimensional space. Most existing methods only considerdirect relations in representation learning. We argue that multiple-steprelation paths also contain rich inference patterns between entities, andpropose a path-based representation learning model. This model considersrelation paths as translations between entities for representation learning,and addresses two key challenges: (1) Since not all relation paths arereliable, we design a path-constraint resource allocation algorithm to measurethe reliability of relation paths. (2) We represent relation paths via semanticcomposition of relation embeddings. Experimental results on real-world datasetsshow that, as compared with baselines, our model achieves significant andconsistent improvements on knowledge base completion and relation extractionfrom text.
arxiv-12000-224 | Towards an Axiomatic Approach to Hierarchical Clustering of Measures | http://arxiv.org/pdf/1508.03712v1.pdf | author:Philipp Thomann, Ingo Steinwart, Nico Schmid category:stat.ML cs.LG math.ST stat.ME stat.TH published:2015-08-15 summary:We propose some axioms for hierarchical clustering of probability measuresand investigate their ramifications. The basic idea is to let the userstipulate the clusters for some elementary measures. This is done without theneed of any notion of metric, similarity or dissimilarity. Our main resultsthen show that for each suitable choice of user-defined clustering onelementary measures we obtain a unique notion of clustering on a large set ofdistributions satisfying a set of additivity and continuity axioms. Weillustrate the developed theory by numerous examples including some with andsome without a density.
arxiv-12000-225 | A Novel Approach For Finger Vein Verification Based on Self-Taught Learning | http://arxiv.org/pdf/1508.03710v1.pdf | author:Mohsen Fayyaz, Masoud PourReza, Mohammad Hajizadeh Saffar, Mohammad Sabokrou, Mahmood Fathy category:cs.CV published:2015-08-15 summary:In this paper, we propose a method for user Finger Vein Authentication (FVA)as a biometric system. Using the discriminative features for classifying thesesfinger veins is one of the main tips that make difference in related works,Thus we propose to learn a set of representative features, based onautoencoders. We model the user finger vein using a Gaussian distribution.Experimental results show that our algorithm perform like a state-of-the-art onSDUMLA-HMT benchmark.
arxiv-12000-226 | Neyman-Pearson Classification under High-Dimensional Settings | http://arxiv.org/pdf/1508.03106v2.pdf | author:Anqi Zhao, Yang Feng, Lie Wang, Xin Tong category:stat.ML published:2015-08-13 summary:Most existing binary classification methods target on the optimization of theoverall classification risk and may fail to serve some real-world applicationssuch as cancer diagnosis, where users are more concerned with the risk ofmisclassifying one specific class than the other. Neyman-Pearson (NP) paradigmwas introduced in this context as a novel statistical framework for handlingasymmetric type I/II error priorities. It seeks classifiers with a minimal typeII error and a constrained type I error under a user specified level. Thisarticle is the first attempt to construct classifiers with guaranteedtheoretical performance under the NP paradigm in high-dimensional settings.Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach toconstruct NP-type classifiers for Naive Bayes models. The proposed classifierssatisfy the NP oracle inequalities, which are natural NP paradigm counterpartsof the oracle inequalities in classical binary classification. Besides theirdesirable theoretical properties, we also demonstrated their numericaladvantages in prioritized error control via both simulation and real datastudies.
arxiv-12000-227 | Unbounded Bayesian Optimization via Regularization | http://arxiv.org/pdf/1508.03666v1.pdf | author:Bobak Shahriari, Alexandre Bouchard-C√¥t√©, Nando de Freitas category:stat.ML published:2015-08-14 summary:Bayesian optimization has recently emerged as a popular and efficient toolfor global optimization and hyperparameter tuning. Currently, the establishedBayesian optimization practice requires a user-defined bounding box which isassumed to contain the optimizer. However, when little is known about theprobed objective function, it can be difficult to prescribe such bounds. Inthis work we modify the standard Bayesian optimization framework in aprincipled way to allow automatic resizing of the search space. We introducetwo alternative methods and compare them on two common synthetic benchmarkingtest functions as well as the tasks of tuning the stochastic gradient descentoptimizer of a multi-layered perceptron and a convolutional neural network onMNIST.
arxiv-12000-228 | A new Semi-Markov model based clustering for a Clustering-Scheduling Integrated framework for Patient Flow Modeling and Optimization | http://arxiv.org/pdf/1505.07752v3.pdf | author:Chitta Ranjan, Kamran Paynabar, Jonathan E. Helm category:stat.ME stat.AP stat.ML published:2015-05-28 summary:The ability to accurately forecast and control inpatient census, and therebyworkloads, is a critical and longstanding problem in hospital management. Themajority of current literature focuses on optimal scheduling of electiveinpatients, but largely ignores the process of accurate estimation of thetrajectory of patients throughout the treatment and recovery process. Theresult is that current elective scheduling models are optimizing based oninaccurate input data. In this paper, we develop a Clustering and SchedulingIntegrated (CSI) approach to capture patient flows through a network ofhospital services. CSI functions by clustering patients into groups based onsimilarity of trajectory using a Semi-Markov model-based clustering scheme, asopposed to clustering by admit type or condition as in previous literature. Themethodology is validated by simulation and then applied to real patient datafrom a partner hospital where we see it outperforms current methods. Further,we demonstrate that optimization methods achieve significantly better resultson key hospital performance measure under CSI, compared with traditionalestimation approaches, increasing elective admissions by 97% and utilization by22% compared to 30% and 8% using traditional estimation techniques. From atheoretical standpoint, the SMM-clustering is a novel approach applicable toany temporal-spatial stochastic data that is prevalent in many industries.
arxiv-12000-229 | Is Stack Overflow Overflowing With Questions and Tags | http://arxiv.org/pdf/1508.03601v1.pdf | author:Ranjitha R. K., Sanjay Singh category:cs.SI cs.CL published:2015-08-14 summary:Programming question and answer (Q & A) websites, such as Quora, StackOverflow, and Yahoo! Answer etc. helps us to understand the programmingconcepts easily and quickly in a way that has been tested and applied by manysoftware developers. Stack Overflow is one of the most frequently usedprogramming Q\&A website where the questions and answers posted are presentlyanalyzed manually, which requires a huge amount of time and resource. To savethe effort, we present a topic modeling based technique to analyze the words ofthe original texts to discover the themes that run through them. We alsopropose a method to automate the process of reviewing the quality of questionson Stack Overflow dataset in order to avoid ballooning the stack overflow withinsignificant questions. The proposed method also recommends the appropriatetags for the new post, which averts the creation of unnecessary tags on StackOverflow.
arxiv-12000-230 | Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets | http://arxiv.org/pdf/1406.1305v2.pdf | author:Dan Garber, Elad Hazan category:math.OC cs.LG published:2014-06-05 summary:The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smoothoptimization has regained much interest in recent years in the context of largescale optimization and machine learning. A key advantage of the method is thatit avoids projections - the computational bottleneck in many applications -replacing it by a linear optimization step. Despite this advantage, the knownconvergence rates of the FW method fall behind standard first order methods formost settings of interest. It is an active line of research to derive fasterlinear optimization-based algorithms for various settings of convexoptimization. In this paper we consider the special case of optimization over stronglyconvex sets, for which we prove that the vanila FW method converges at a rateof $\frac{1}{t^2}$. This gives a quadratic improvement in convergence ratecompared to the general case, in which convergence is of the order$\frac{1}{t}$, and known to be tight. We show that various balls induced by$\ell_p$ norms, Schatten norms and group norms are strongly convex on one handand on the other hand, linear optimization over these sets is straightforwardand admits a closed-form solution. We further show how several previousfast-rate results for the FW method follow easily from our analysis.
arxiv-12000-231 | A Linearly Convergent Conditional Gradient Algorithm with Applications to Online and Stochastic Optimization | http://arxiv.org/pdf/1301.4666v6.pdf | author:Dan Garber, Elad Hazan category:cs.LG math.OC stat.ML published:2013-01-20 summary:Linear optimization is many times algorithmically simpler than non-linearconvex optimization. Linear optimization over matroid polytopes, matchingpolytopes and path polytopes are example of problems for which we have simpleand efficient combinatorial algorithms, but whose non-linear convex counterpartis harder and admits significantly less efficient algorithms. This motivatesthe computational model of convex optimization, including the offline, onlineand stochastic settings, using a linear optimization oracle. In thiscomputational model we give several new results that improve over the previousstate-of-the-art. Our main result is a novel conditional gradient algorithm forsmooth and strongly convex optimization over polyhedral sets that performs onlya single linear optimization step over the domain on each iteration and enjoysa linear convergence rate. This gives an exponential improvement in convergencerate over previous results. Based on this new conditional gradient algorithm we give the first algorithmsfor online convex optimization over polyhedral sets that perform only a singlelinear optimization step over the domain while having optimal regretguarantees, answering an open question of Kalai and Vempala, and Hazan andKale. Our online algorithms also imply conditional gradient algorithms fornon-smooth and stochastic convex optimization with the same convergence ratesas projected (sub)gradient methods.
arxiv-12000-232 | Sparsity Based Methods for Overparameterized Variational Problems | http://arxiv.org/pdf/1405.4969v5.pdf | author:Raja Giryes, Michael Elad, Alfred M. Bruckstein category:cs.CV stat.ML published:2014-05-20 summary:Two complementary approaches have been extensively used in signal and imageprocessing leading to novel results, the sparse representation methodology andthe variational strategy. Recently, a new sparsity based model has beenproposed, the cosparse analysis framework, which may potentially help inbridging sparse approximation based methods to the traditional total-variationminimization. Based on this, we introduce a sparsity based framework forsolving overparameterized variational problems. The latter has been used toimprove the estimation of optical flow and also for general denoising ofsignals and images. However, the recovery of the space varying parametersinvolved was not adequately addressed by traditional variational methods. Wefirst demonstrate the efficiency of the new framework for one dimensionalsignals in recovering a piecewise linear and polynomial function. Then, weillustrate how the new technique can be used for denoising and segmentation ofimages.
arxiv-12000-233 | Lensless Compressive Imaging | http://arxiv.org/pdf/1508.03498v1.pdf | author:Xin Yuan, Hong Jiang, Gang Huang, Paul Wilford category:cs.CV stat.AP stat.ME published:2015-08-14 summary:We develop a lensless compressive imaging architecture, which consists of anaperture assembly and a single sensor, without using any lens. An anytimealgorithm is proposed to reconstruct images from the compressive measurements;the algorithm produces a sequence of solutions that monotonically converge tothe true signal (thus, anytime). The algorithm is developed based on thesparsity of local overlapping patches (in the transformation domain) andstate-of-the-art results have been obtained. Experiments on real datademonstrate that encouraging results are obtained by measuring about 10% (ofthe image pixels) compressive measurements. The reconstruction results of theproposed algorithm are compared with the JPEG compression (based on file sizes)and the reconstructed image quality is close to the JPEG compression, inparticular at a high compression rate.
arxiv-12000-234 | A model selection approach for clustering a multinomial sequence with non-negative factorization | http://arxiv.org/pdf/1312.7559v7.pdf | author:Nam H. Lee, Runze Tang, Carey E. Priebe, Michael Rosen category:stat.ML published:2013-12-29 summary:We consider a problem of clustering a sequence of multinomial observations byway of a model selection criterion. We propose a form of a penalty term for themodel selection procedure. Our approach subsumes both the conventional AIC andBIC criteria but also extends the conventional criteria in a way that it can beapplicable also to a sequence of sparse multinomial observations, where evenwithin a same cluster, the number of multinomial trials may be different fordifferent observations. In addition, as a preliminary estimation step tomaximum likelihood estimation, and more generally, to maximum $L_{q}$estimation, we propose to use reduced rank projection in combination withnon-negative factorization. We motivate our approach by showing that our modelselection criterion and preliminary estimation step yield consistent estimatesunder simplifying assumptions. We also illustrate our approach throughnumerical experiments using real and simulated data.
arxiv-12000-235 | Oracle MCG: A first peek into COCO Detection Challenges | http://arxiv.org/pdf/1509.03660v1.pdf | author:Jordi Pont-Tuset, Pablo Arbel√°ez, Luc Van Gool category:cs.CV published:2015-08-14 summary:The recently presented COCO detection challenge will most probably be thereference benchmark in object detection in the next years. COCO is two ordersof magnitude larger than Pascal and has four times the number of categories; soin all likelihood researchers will be faced with a number of new challenges. Atthis point, without any finished round of the competition, it is difficult forresearchers to put their techniques in context, or in other words, to know howgood their results are. In order to give a little context, this note evaluatesa hypothetical object detector consisting in an oracle picking the best objectproposal from a state-of-the-art technique. This oracle achieves a AP=0.292 insegmented objects and AP=0.317 in bounding boxes, showing that indeed thedatabase is challenging, given that this value is the best one can expect ifworking on object proposals without refinement.
arxiv-12000-236 | A Spatial Layout and Scale Invariant Feature Representation for Indoor Scene Classification | http://arxiv.org/pdf/1506.05532v2.pdf | author:Munawar Hayat, Salman H. Khan, Mohammed Bennamoun, Senjian An category:cs.CV published:2015-06-18 summary:Unlike standard object classification, where the image to be classifiedcontains one or multiple instances of the same object, indoor sceneclassification is quite different since the image consists of multiple distinctobjects. Further, these objects can be of varying sizes and are present acrossnumerous spatial locations in different layouts. For automatic indoor scenecategorization, large scale spatial layout deformations and scale variationsare therefore two major challenges and the design of rich feature descriptorswhich are robust to these challenges is still an open problem. This paperintroduces a new learnable feature descriptor called "spatial layout and scaleinvariant convolutional activations" to deal with these challenges. For thispurpose, a new Convolutional Neural Network architecture is designed whichincorporates a novel 'Spatially Unstructured' layer to introduce robustnessagainst spatial layout deformations. To achieve scale invariance, we present apyramidal image representation. For feasible training of the proposed networkfor images of indoor scenes, the paper proposes a new methodology whichefficiently adapts a trained network model (on a large scale data) for our taskwith only a limited amount of available training data. Compared with existingstate of the art, the proposed approach achieves a relative performanceimprovement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8,Graz-02 and NYU datasets respectively.
arxiv-12000-237 | Information-theoretic Bounds on Matrix Completion under Union of Subspaces Model | http://arxiv.org/pdf/1508.03395v1.pdf | author:Vaneet Aggarwal, Shuchin Aeron category:cs.IT math.IT stat.ML published:2015-08-14 summary:In this short note we extend some of the recent results on matrix completionunder the assumption that the columns of the matrix can be grouped (clustered)into subspaces (not necessarily disjoint or independent). This model deviatesfrom the typical assumption prevalent in the literature dealing withcompression and recovery for big-data applications. The results have a directbearing on the problem of subspace clustering under missing or incompleteinformation.
arxiv-12000-238 | Learning from Real Users: Rating Dialogue Success with Neural Networks for Reinforcement Learning in Spoken Dialogue Systems | http://arxiv.org/pdf/1508.03386v1.pdf | author:Pei-Hao Su, David Vandyke, Milica Gasic, Dongho Kim, Nikola Mrksic, Tsung-Hsien Wen, Steve Young category:cs.LG cs.CL published:2015-08-13 summary:To train a statistical spoken dialogue system (SDS) it is essential that anaccurate method for measuring task success is available. To date training hasrelied on presenting a task to either simulated or paid users and inferring thedialogue's success by observing whether this presented task was achieved ornot. Our aim however is to be able to learn from real users acting under theirown volition, in which case it is non-trivial to rate the success as any priorknowledge of the task is simply unavailable. User feedback may be utilised buthas been found to be inconsistent. Hence, here we present two neural networkmodels that evaluate a sequence of turn-level features to rate the success of adialogue. Importantly these models make no use of any prior knowledge of theuser's task. The models are trained on dialogues generated by a simulated userand the best model is then used to train a policy on-line which is shown toperform at least as well as a baseline system using prior knowledge of theuser's task. We note that the models should also be of interest for evaluatingSDS and for monitoring a dialogue in rule-based SDS.
arxiv-12000-239 | Multi-Task Learning with Group-Specific Feature Space Sharing | http://arxiv.org/pdf/1508.03329v1.pdf | author:Niloofar Yousefi, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2015-08-13 summary:When faced with learning a set of inter-related tasks from a limited amountof usable data, learning each task independently may lead to poorgeneralization performance. Multi-Task Learning (MTL) exploits the latentrelations between tasks and overcomes data scarcity limitations by co-learningall these tasks simultaneously to offer improved performance. We propose anovel Multi-Task Multiple Kernel Learning framework based on Support VectorMachines for binary classification tasks. By considering pair-wise taskaffinity in terms of similarity between a pair's respective feature spaces, thenew framework, compared to other similar MTL approaches, offers a high degreeof flexibility in determining how similar feature spaces should be, as well aswhich pairs of tasks should share a common feature space in order to benefitoverall performance. The associated optimization problem is solved via a blockcoordinate descent, which employs a consensus-form Alternating Direction Methodof Multipliers algorithm to optimize the Multiple Kernel Learning weights and,hence, to determine task affinities. Empirical evaluation on seven data setsexhibits a statistically significant improvement of our framework's resultscompared to the ones of several other Clustered Multi-Task Learning methods.
arxiv-12000-240 | Talking about the Moving Image: A Declarative Model for Image Schema Based Embodied Perception Grounding and Language Generation | http://arxiv.org/pdf/1508.03276v1.pdf | author:Jakob Suchan, Mehul Bhatt, Harshita Jhavar category:cs.AI cs.CL cs.CV cs.HC published:2015-08-13 summary:We present a general theory and corresponding declarative model for theembodied grounding and natural language based analytical summarisation ofdynamic visuo-spatial imagery. The declarative model ---ecompassingspatio-linguistic abstractions, image schemas, and a spatio-temporal featurebased language generator--- is modularly implemented within Constraint LogicProgramming (CLP). The implemented model is such that primitives of the theory,e.g., pertaining to space and motion, image schemata, are available asfirst-class objects with `deep semantics' suited for inference and query. Wedemonstrate the model with select examples broadly motivated by areas such asfilm, design, geography, smart environments where analytical natural languagebased externalisations of the moving image are central from the viewpoint ofhuman interaction, evidence-based qualitative analysis, and sensemaking. Keywords: moving image, visual semantics and embodiment, visuo-spatialcognition and computation, cognitive vision, computational models of narrative,declarative spatial reasoning
arxiv-12000-241 | A New Approach to an Old Problem: The Reconstruction of a Go Game through a Series of Photographs | http://arxiv.org/pdf/1508.03269v1.pdf | author:Mario Corsolini, Andrea Carta category:cs.CV published:2015-08-13 summary:Given a series of photographs taken during a Go game, we describe thetechniques we successfully employ for pinpointing the grid lines of the Goboard and for tracking their small movements between consecutive photographs;then we discuss how to approximate the location and orientation of theobserver's point of view, in order to compensate for projection effects.Finally we describe the different criteria that jointly form the algorithm forstones' detection, thus enabling us to automatically reconstruct the whole movesequence.
arxiv-12000-242 | Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications | http://arxiv.org/pdf/1503.01444v2.pdf | author:Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So Kweon category:cs.CV cs.AI published:2015-03-04 summary:Robust Principal Component Analysis (RPCA) via rank minimization is apowerful tool for recovering underlying low-rank structure of clean datacorrupted with sparse noise/outliers. In many low-level vision problems, notonly it is known that the underlying structure of clean data is low-rank, butthe exact rank of clean data is also known. Yet, when applying conventionalrank minimization for those problems, the objective function is formulated in away that does not fully utilize a priori target rank information about theproblems. This observation motivates us to investigate whether there is abetter alternative solution when using rank minimization. In this paper,instead of minimizing the nuclear norm, we propose to minimize the partial sumof singular values, which implicitly encourages the target rank constraint. Ourexperimental analyses show that, when the number of samples is deficient, ourapproach leads to a higher success rate than conventional rank minimization,while the solutions obtained by the two approaches are almost identical whenthe number of samples is more than sufficient. We apply our approach to variouslow-level vision problems, e.g. high dynamic range imaging, motion edgedetection, photometric stereo, image alignment and recovery, and show that ourresults outperform those obtained by the conventional nuclear norm rankminimization method.
arxiv-12000-243 | Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning | http://arxiv.org/pdf/1508.02354v2.pdf | author:Jianpeng Cheng, Dimitri Kartsaklis category:cs.CL cs.AI cs.NE published:2015-08-10 summary:Deep compositional models of meaning acting on distributional representationsof words in order to produce vectors of larger text constituents are evolvingto a popular area of NLP research. We detail a compositional distributionalframework based on a rich form of word embeddings that aims at facilitating theinteractions between words in the context of a sentence. Embeddings andcomposition layers are jointly learned against a generic objective thatenhances the vectors with syntactic information from the surrounding context.Furthermore, each word is associated with a number of senses, the mostplausible of which is selected dynamically during the composition process. Weevaluate the produced vectors qualitatively and quantitatively with positiveresults. At the sentence level, the effectiveness of the framework isdemonstrated on the MSRPar task, for which we report results within thestate-of-the-art range.
arxiv-12000-244 | A Max-Sum algorithm for training discrete neural networks | http://arxiv.org/pdf/1505.05401v2.pdf | author:Carlo Baldassi, Alfredo Braunstein category:cs.LG cs.NE published:2015-05-20 summary:We present an efficient learning algorithm for the problem of training neuralnetworks with discrete synapses, a well-known hard (NP-complete) discreteoptimization problem. The algorithm is a variant of the so-called Max-Sum (MS)algorithm. In particular, we show how, for bounded integer weights with $q$distinct states and independent concave a priori distribution (e.g. $l_{1}$regularization), the algorithm's time complexity can be made to scale as$O\left(N\log N\right)$ per node update, thus putting it on par withalternative schemes, such as Belief Propagation (BP), without resorting toapproximations. Two special cases are of particular interest: binary synapses$W\in\{-1,1\}$ and ternary synapses $W\in\{-1,0,1\}$ with $l_{0}$regularization. The algorithm we present performs as well as BP on binaryperceptron learning problems, and may be better suited to address the problemon fully-connected two-layer networks, since inherent symmetries in two layernetworks are naturally broken using the MS approach.
arxiv-12000-245 | Logical N-AND Gate on a Molecular Turing Machine | http://arxiv.org/pdf/1508.03174v1.pdf | author:Victor Hernandez-Urbina category:cs.ET cs.NE published:2015-08-13 summary:In Boolean algebra, it is known that the logical function that corresponds tothe negation of the conjunction --NAND-- is universal in the sense that anyother logical function can be built based on it. This property makes itessential to modern digital electronics and computer processor design. Here, wedesign a molecular Turing machine that computes the NAND function over binarystrings of arbitrary length. For this purpose, we will perform a mathematicalabstraction of the kind of operations that can be done over a double-strandedDNA molecule, as well as presenting a molecular encoding of the input symbolsfor such a machine.
arxiv-12000-246 | Generation of Multimedia Artifacts: An Extractive Summarization-based Approach | http://arxiv.org/pdf/1508.03170v1.pdf | author:Paulo Figueiredo, Marta Apar√≠cio, David Martins de Matos, Ricardo Ribeiro category:cs.AI cs.CL cs.MM I.2.7 published:2015-08-13 summary:We explore methods for content selection and address the issue of coherencein the context of the generation of multimedia artifacts. We use audio andvideo to present two case studies: generation of film tributes, andlecture-driven science talks. For content selection, we use centrality-basedand diversity-based summarization, along with topic analysis. To establishcoherence, we use the emotional content of music, for film tributes, and ensuretopic similarity between lectures and documentaries, for science talks.Composition techniques for the production of multimedia artifacts are addressedas a means of organizing content, in order to improve coherence. We discuss ourresults considering the above aspects.
arxiv-12000-247 | Probabilistic Dependency Networks for Prediction and Diagnostics | http://arxiv.org/pdf/1508.03130v1.pdf | author:Narayanan U. Edakunni, Aditi Raghunathan, Abhishek Tripathi, John Handley, Fredric Roulland category:cs.LG published:2015-08-13 summary:Research in transportation frequently involve modelling and predictingattributes of events that occur at regular intervals. The event could bearrival of a bus at a bus stop, the volume of a traffic at a particular point,the demand at a particular bus stop etc. In this work, we propose a specificimplementation of probabilistic graphical models to learn the probabilisticdependency between the events that occur in a network. A dependency graph isbuilt from the past observed instances of the event and we use the graph tounderstand the causal effects of some events on others in the system. Thedependency graph is also used to predict the attributes of future events and isshown to have a good prediction accuracy compared to the state of the art.
arxiv-12000-248 | Optimized Projections for Compressed Sensing via Direct Mutual Coherence Minimization | http://arxiv.org/pdf/1508.03117v1.pdf | author:Zhouchen Lin, Canyi Lu, Huan Li category:cs.IT cs.LG math.IT published:2015-08-13 summary:Compressed Sensing (CS) is a novel technique for simultaneous signal samplingand compression based on the existence of a sparse representation of signal anda projected dictionary $\PP\D$, where $\PP\in\mathbb{R}^{m\times d}$ is theprojection matrix and $\D\in\mathbb{R}^{d\times n}$ is the dictionary. Toexactly recover the signal with a small number of measurements $m$, theprojected dictionary $\PP\D$ is expected to be of low mutual coherence. Severalprevious methods attempt to find the projection $\PP$ such that the mutualcoherence of $\PP\D$ can be as low as possible. However, they do not minimizethe mutual coherence directly and thus their methods are far from optimal. Alsothe solvers they used lack of the convergence guarantee and thus there has noguarantee on the quality of their obtained solutions. This work aims to addressthese issues. We propose to find an optimal projection by minimizing the mutualcoherence of $\PP\D$ directly. This leads to a nonconvex nonsmooth minimizationproblem. We then approximate it by smoothing and solve it by alternateminimization. We further prove the convergence of our algorithm. To the best ofour knowledge, this is the first work which directly minimizes the mutualcoherence of the projected dictionary with a convergence guarantee. Numericalexperiments demonstrate that the proposed method can recover sparse signalsbetter than existing methods.
arxiv-12000-249 | Borobudur was Built Algorithmically | http://arxiv.org/pdf/1508.03649v1.pdf | author:Hokky Situngkir category:cs.CY cs.CV cs.GR published:2015-08-13 summary:The self-similarity of Indonesian Borobudur Temple is observed through thedimensionality of stupa that is hypothetically closely related to wholearchitectural body. Fractal dimension is calculated by using the cube countingmethod and found that the dimension is 2.325, which is laid between thetwo-dimensional plane and three dimensional space. The applied fractal geometryand self-similarity of the building is emerged as the building processimplement the metric rules, since there is no universal metric standard knownin ancient traditional Javanese culture thus the architecture is not based onfinal master plan. The paper also proposes how the hypothetical algorithmicarchitecture might be applied computationally in order to see some experimentalgenerations of similar building. The paper ends with some conjectures forfurther challenge and insights related to fractal geometry in Javanesetraditional cultural heritages.
arxiv-12000-250 | On Convergence of Emphatic Temporal-Difference Learning | http://arxiv.org/pdf/1506.02582v2.pdf | author:Huizhen Yu category:cs.LG published:2015-06-08 summary:We consider emphatic temporal-difference learning algorithms for policyevaluation in discounted Markov decision processes with finite spaces. Suchalgorithms were recently proposed by Sutton, Mahmood, and White (2015) as animproved solution to the problem of divergence of off-policytemporal-difference learning with linear function approximation. We present inthis paper the first convergence proofs for two emphatic algorithms,ETD($\lambda$) and ELSTD($\lambda$). We prove, under general off-policyconditions, the convergence in $L^1$ for ELSTD($\lambda$) iterates, and thealmost sure convergence of the approximate value functions calculated by bothalgorithms using a single infinitely long trajectory. Our analysis involves newtechniques with applications beyond emphatic algorithms leading, for example,to the first proof that standard TD($\lambda$) also converges under off-policytraining for $\lambda$ sufficiently large.
arxiv-12000-251 | From Cutting Planes Algorithms to Compression Schemes and Active Learning | http://arxiv.org/pdf/1508.02986v1.pdf | author:Liva Ralaivola, Ugo Louche category:cs.LG published:2015-08-12 summary:Cutting-plane methods are well-studied localization(and optimization)algorithms. We show that they provide a natural framework to performmachinelearning ---and not just to solve optimization problems posed bymachinelearning--- in addition to their intended optimization use. Inparticular, theyallow one to learn sparse classifiers and provide goodcompression schemes.Moreover, we show that very little effort is required toturn them intoeffective active learning methods. This last property provides ageneric way todesign a whole family of active learning algorithms from existingpassivemethods. We present numerical simulations testifying of the relevanceofcutting-plane methods for passive and active learning tasks.
arxiv-12000-252 | A massively parallel multi-level approach to a domain decomposition method for the optical flow estimation with varying illumination | http://arxiv.org/pdf/1508.02977v1.pdf | author:Diane Gilliocq-Hirtz, Zakaria Belhachmi category:cs.CV published:2015-08-12 summary:We consider a variational method to solve the optical flow problem withvarying illumination. We apply an adaptive control of the regularizationparameter which allows us to preserve the edges and fine features of thecomputed flow. To reduce the complexity of the estimation for high resolutionimages and the time of computations, we implement a multi-level parallelapproach based on the domain decomposition with the Schwarz overlapping method.The second level of parallelism uses the massively parallel solver MUMPS. Weperform some numerical simulations to show the efficiency of our approach andto validate it on classical and real-world image sequences.
arxiv-12000-253 | Mountain Peak Detection in Online Social Media | http://arxiv.org/pdf/1508.02959v1.pdf | author:Roman Fedorov category:cs.CV cs.MM published:2015-08-12 summary:We present a system for the classification of mountain panoramas fromuser-generated photographs followed by identification and extraction ofmountain peaks from those panoramas. We have developed an automatic techniquethat, given as input a geo-tagged photograph, estimates its FOV (Field Of View)and the direction of the camera using a matching algorithm on the photographedge maps and a rendered view of the mountain silhouettes that should be seenfrom the observer's point of view. The extraction algorithm then identifies themountain peaks present in the photograph and their profiles. We discusspossible applications in social fields such as photograph peak tagging onsocial portals, augmented reality on mobile devices when viewing a mountainpanorama, and generation of collective intelligence systems (such asenvironmental models) from massive social media collections (e.g. snow wateravailability maps based on mountain peak states extracted from photographhosting services).
arxiv-12000-254 | Joint Calibration for Semantic Segmentation | http://arxiv.org/pdf/1507.01581v4.pdf | author:Holger Caesar, Jasper Uijlings, Vittorio Ferrari category:cs.CV 68T45 published:2015-07-06 summary:Semantic segmentation is the task of assigning a class-label to each pixel inan image. We propose a region-based semantic segmentation framework whichhandles both full and weak supervision, and addresses three common problems:(1) Objects occur at multiple scales and therefore we should use regions atmultiple scales. However, these regions are overlapping which createsconflicting class predictions at the pixel-level. (2) Class frequencies arehighly imbalanced in realistic datasets. (3) Each pixel can only be assigned toa single class, which creates competition between classes. We address all threeproblems with a joint calibration method which optimizes a multi-class lossdefined over the final pixel-level output labeling, as opposed to simply regionclassification. Our method outperforms the state-of-the-art on the popular SIFTFlow [18] dataset in both the fully and weakly supervised setting by aconsiderably margin (+6% and +10%, respectively).
arxiv-12000-255 | RCR: Robust Compound Regression for Robust Estimation of Errors-in-Variables Model | http://arxiv.org/pdf/1508.02925v1.pdf | author:Hao Han, Wei Zhu category:stat.ME math.ST stat.ML stat.TH published:2015-08-12 summary:The errors-in-variables (EIV) regression model, being more realistic byaccounting for measurement errors in both the dependent and the independentvariables, is widely adopted in applied sciences. The traditional EIV modelestimators, however, can be highly biased by outliers and other departures fromthe underlying assumptions. In this paper, we develop a novel nonparametricregression approach - the robust compound regression (RCR) analysis method forthe robust estimation of EIV models. We first introduce a robust and efficientestimator called least sine squares (LSS). Taking full advantage of both thenew LSS method and the compound regression analysis method developed in our owngroup, we subsequently propose the RCR approach as a generalization of thosetwo, which provides a robust counterpart of the entire class of the maximumlikelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping.Technically, our approach gives users the flexibility to select from a class ofRCR estimates the optimal one with a predefined regression efficiency criterionsatisfied. Simulation studies and real-life examples are provided to illustratethe effectiveness of the RCR approach.
arxiv-12000-256 | Bayesian Dropout | http://arxiv.org/pdf/1508.02905v1.pdf | author:Tue Herlau, Morten M√∏rup, Mikkel N. Schmidt category:stat.ML published:2015-08-12 summary:Dropout has recently emerged as a powerful and simple method for trainingneural networks preventing co-adaptation by stochastically omitting neurons.Dropout is currently not grounded in explicit modelling assumptions which sofar has precluded its adoption in Bayesian modelling. Using Bayesian entropicreasoning we show that dropout can be interpreted as optimal inference underconstraints. We demonstrate this on an analytically tractable regression modelproviding a Bayesian interpretation of its mechanism for regularizing andpreventing co-adaptation as well as its connection to other Bayesiantechniques. We also discuss two general approximate techniques for applyingBayesian dropout for general models, one based on an analytical approximationand the other on stochastic variational techniques. These techniques are thenapplied to a Baysian logistic regression problem and are shown to improveperformance as the model become more misspecified. Our framework roots dropoutas a theoretically justified and practical tool for statistical modellingallowing Bayesians to tap into the benefits of dropout training.
arxiv-12000-257 | Maximum Entropy Vector Kernels for MIMO system identification | http://arxiv.org/pdf/1508.02865v1.pdf | author:Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso category:cs.SY stat.ML published:2015-08-12 summary:Recent contributions have framed linear system identification as anonparametric regularized inverse problem, which in some situations have provedto be advantageous w.r.t classical parametric methods. Typical formulationsexploit an $\ell_2$-type regularization which accounts for the stability andsmoothness of the impulse response to be estimated. In this paper, adoptingMaximum Entropy arguments, we derive a new type of $\ell_2$-regularizationwhich results in a vector-valued kernel; our aim is to introduce regularizationon the block Hankel matrix built with Markov coefficients, thus controlling thecomplexity of the identified model, measured by its McMillan degree. As aspecial case we recover the standard nuclear norm penalty. Combining thisHankel-based regularization with the standard $\ell_2$-type regularizationadopted in previous literature we design a kernel which, at the same time,encodes stability, smoothness and low McMillan degree. In contrast withprevious literature on reweighed nuclear norm penalties, our kernel isdescribed by a small number of hyper-prameters, which are iteratively updatedthrough marginal likelihood maximization. To this purpose we also adapt aScaled Gradient Projection (SGP) algorithm which is proved to be significantlycomputationally cheaper than other first and second order off-the-shelfoptimization methods. The effectiveness of the identification technique wepropose is confirmed by several Monte-Carlo studies.
arxiv-12000-258 | Manifold regularization in structured output space for semi-supervised structured output prediction | http://arxiv.org/pdf/1508.02849v1.pdf | author:Fei Jiang, Lili Jia, Xiaobao Sheng, Riley LeMieux category:cs.LG cs.CV published:2015-08-12 summary:Structured output prediction aims to learn a predictor to predict astructured output from a input data vector. The structured outputs includevector, tree, sequence, etc. We usually assume that we have a training set ofinput-output pairs to train the predictor. However, in many real-world appli-cations, it is difficult to obtain the output for a input, thus for manytraining input data points, the structured outputs are missing. In this paper,we dis- cuss how to learn from a training set composed of some input-outputpairs, and some input data points without outputs. This problem is called semi-supervised structured output prediction. We propose a novel method for thisproblem by constructing a nearest neighbor graph from the input space topresent the manifold structure, and using it to regularize the structured out-put space directly. We define a slack structured output for each training datapoint, and proposed to predict it by learning a structured output predictor.The learning of both slack structured outputs and the predictor are unifiedwithin one single minimization problem. In this problem, we propose to mini-mize the structured loss between the slack structured outputs of neighboringdata points, and the prediction error measured by the structured loss. Theproblem is optimized by an iterative algorithm. Experiment results over threebenchmark data sets show its advantage.
arxiv-12000-259 | Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration | http://arxiv.org/pdf/1508.02848v1.pdf | author:Yunjin Chen, Thomas Pock category:cs.CV published:2015-08-12 summary:Image restoration is a long-standing problem in low-level computer visionwith many interesting applications. We describe a flexible learning frameworkto obtain simple but effective models for various image restoration problems.The proposed approach is based on the concept of nonlinear reaction diffusion,but we extend conventional nonlinear reaction diffusion models by highlyparametrized linear filters as well as highly parametrized influence functions.In contrast to previous nonlinear diffusion models, all the parameters,including the filters and the influence functions, are learned from trainingdata through a loss based approach. We call this approach TNRD -- TrainableNonlinear Reaction Diffusion. The TNRD approach is applicable for a variety ofimage restoration tasks by incorporating appropriate reaction force. Wedemonstrate its capabilities with three representative applications, Gaussianimage denoising, single image super resolution and JPEG deblocking. Experimentsshow that our trained nonlinear diffusion models largely benefit from thetraining of the parameters and finally lead to the best reported performance oncommon test datasets with respect to the tested applications. Our trainedmodels retain the structural simplicity of diffusion models and take only asmall number of steps, thus are highly efficient. Moreover, they are alsowell-suited for parallel computation on GPUs, which makes the inferenceprocedure extremely fast.
arxiv-12000-260 | Massively-concurrent Agent-based Evolutionary Computing | http://arxiv.org/pdf/1501.06721v2.pdf | author:D. Krzywicki, W. Turek, A. Byrski, M. Kisiel-Dorohinicki category:cs.MA cs.NE published:2015-01-27 summary:The fusion of the multi-agent paradigm with evolutionary computation yieldedpromising results in many optimization problems. Evolutionary multi-agentsystem (EMAS) are more similar to biological evolution than classicalevolutionary algorithms. However, technological limitations prevented the useof fully asynchronous agents in previous EMAS implementations. In this paper wepresent a new algorithm for agent-based evolutionary computations. Theindividuals are represented as fully autonomous and asynchronous agents. Anefficient implementation of this algorithm was possible through the use ofmodern technologies based on functional languages (namely Erlang and Scala),which natively support lightweight processes and asynchronous communication.Our experiments show that such an asynchronous approach is both faster and moreefficient in solving common optimization problems.
arxiv-12000-261 | Learning to Hire Teams | http://arxiv.org/pdf/1508.02823v1.pdf | author:Adish Singla, Eric Horvitz, Pushmeet Kohli, Andreas Krause category:cs.HC cs.CY cs.LG published:2015-08-12 summary:Crowdsourcing and human computation has been employed in increasinglysophisticated projects that require the solution of a heterogeneous set oftasks. We explore the challenge of building or hiring an effective team, forperforming tasks required for such projects on an ongoing basis, from anavailable pool of applicants or workers who have bid for the tasks. Therecruiter needs to learn workers' skills and expertise by performing onlinetests and interviews, and would like to minimize the amount of budget or timespent in this process before committing to hiring the team. How can oneoptimally spend budget to learn the expertise of workers as part of recruitinga team? How can one exploit the similarities among tasks as well as underlyingsocial ties or commonalities among the workers for faster learning? We tacklethese decision-theoretic challenges by casting them as an instance of onlinelearning for best action selection. We present algorithms with PAC bounds onthe required budget to hire a near-optimal team with high confidence.Furthermore, we consider an embedding of the tasks and workers in an underlyinggraph that may arise from task similarities or social ties, and that canprovide additional side-observations for faster learning. We then quantify theimprovement in the bounds that we can achieve depending on the characteristicproperties of this graph structure. We evaluate our methodology on simulatedproblem instances as well as on real-world crowdsourcing data collected fromthe oDesk platform. Our methodology and results present an interestingdirection of research to tackle the challenges faced by a recruiter forcontract-based crowdsourcing.
arxiv-12000-262 | Maximum a Posteriori Adaptation of Network Parameters in Deep Models | http://arxiv.org/pdf/1503.02108v2.pdf | author:Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jiadong Wu, Chin-Hui Lee category:cs.LG cs.CL cs.NE published:2015-03-06 summary:We present a Bayesian approach to adapting parameters of a well-trainedcontext-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) toimprove automatic speech recognition performance. Given an abundance of DNNparameters but with only a limited amount of data, the effectiveness of theadapted DNN model can often be compromised. We formulate maximum a posteriori(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with anaugmented linear hidden networks connected to the output tied states, orsenones, and compare it to feature space MAP linear regression previouslyproposed. Experimental evidences on the 20,000-word open vocabulary Wall StreetJournal task demonstrate the feasibility of the proposed framework. Insupervised adaptation, the proposed MAP adaptation approach provides more than10% relative error reduction and consistently outperforms the conventionaltransformation based methods. Furthermore, we present an initial attempt togenerate hierarchical priors to improve adaptation efficiency and effectivenesswith limited adaptation data by exploiting similarities among senones.
arxiv-12000-263 | Possible Mechanisms for Neural Reconfigurability and their Implications | http://arxiv.org/pdf/1508.02792v1.pdf | author:Thomas M. Breuel category:cs.NE q-bio.NC K.3.2 published:2015-08-12 summary:The paper introduces a biologically and evolutionarily plausible neuralarchitecture that allows a single group of neurons, or an entire corticalpathway, to be dynamically reconfigured to perform multiple, potentially verydifferent computations. The paper shows that reconfigurability can account forthe observed stochastic and distributed coding behavior of neurons and providesa parsimonious explanation for timing phenomena in psychophysical experiments.It also shows that reconfigurable pathways correspond to classes of statisticalclassifiers that include decision lists, decision trees, and hierarchicalBayesian methods. Implications for the interpretation of neurophysiological andpsychophysical results are discussed, and future experiments for testing thereconfigurability hypothesis are explored.
arxiv-12000-264 | On the Convergence of SGD Training of Neural Networks | http://arxiv.org/pdf/1508.02790v1.pdf | author:Thomas M. Breuel category:cs.NE cs.LG K.3.2 published:2015-08-12 summary:Neural networks are usually trained by some form of stochastic gradientdescent (SGD)). A number of strategies are in common use intended to improveSGD optimization, such as learning rate schedules, momentum, and batching.These are motivated by ideas about the occurrence of local minima at differentscales, valleys, and other phenomena in the objective function. Empiricalresults presented here suggest that these phenomena are not significant factorsin SGD optimization of MLP-related objective functions, and that the behaviorof stochastic gradient descent in these problems is better described as thesimultaneous convergence at different rates of many, largely non-interactingsubproblems
arxiv-12000-265 | The Effects of Hyperparameters on SGD Training of Neural Networks | http://arxiv.org/pdf/1508.02788v1.pdf | author:Thomas M. Breuel category:cs.NE cs.LG K.3.2 published:2015-08-12 summary:The performance of neural network classifiers is determined by a number ofhyperparameters, including learning rate, batch size, and depth. A number ofattempts have been made to explore these parameters in the literature, and attimes, to develop methods for optimizing them. However, exploration ofparameter spaces has often been limited. In this note, I report the results oflarge scale experiments exploring these different parameters and theirinteractions.
arxiv-12000-266 | Trend Filtering on Graphs | http://arxiv.org/pdf/1410.7690v4.pdf | author:Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan J. Tibshirani category:stat.ML cs.AI cs.LG stat.ME 62G05 published:2014-10-28 summary:We introduce a family of adaptive estimators on graphs, based on penalizingthe $\ell_1$ norm of discrete graph differences. This generalizes the idea oftrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariatenonparametric regression, to graphs. Analogous to the univariate case, graphtrend filtering exhibits a level of local adaptivity unmatched by the usual$\ell_2$-based graph smoothers. It is also defined by a convex minimizationproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). Wedemonstrate the merits of graph trend filtering through examples and theory.
arxiv-12000-267 | Benchmarking of LSTM Networks | http://arxiv.org/pdf/1508.02774v1.pdf | author:Thomas M. Breuel category:cs.NE K.3.2 published:2015-08-11 summary:LSTM (Long Short-Term Memory) recurrent neural networks have been highlysuccessful in a number of application areas. This technical report describesthe use of the MNIST and UW3 databases for benchmarking LSTM networks andexplores the effect of di?erent architectural and hyperparameter choices onperformance. Significant ?ndings include: (1) LSTM performance depends smoothlyon learning rates, (2) batching and momentum has no significant effect onperformance, (3) softmax training outperforms least square training, (4)peephole units are not useful, (5) the standard non-linearities (tanh andsigmoid) perform best, (6) bidirectional training combined with CTC performsbetter than other methods.
arxiv-12000-268 | Maintaining prediction quality under the condition of a growing knowledge space | http://arxiv.org/pdf/1508.00509v2.pdf | author:Christoph Jahnz category:cs.AI cs.LG published:2015-08-03 summary:Intelligence can be understood as an agent's ability to predict itsenvironment's dynamic by a level of precision which allows it to effectivelyforesee opportunities and threats. Under the assumption that such intelligencerelies on a knowledge space any effective reasoning would benefit from amaximum portion of useful and a minimum portion of misleading knowledgefragments. It begs the question of how the quality of such knowledge space canbe kept high as the amount of knowledge keeps growing. This article proposes amathematical model to describe general principles of how quality of a growingknowledge space evolves depending on error rate, error propagation andcountermeasures. There is also shown to which extend the quality of a knowledgespace collapses as removal of low quality knowledge fragments occurs too slowlyfor a given knowledge space's growth rate.
arxiv-12000-269 | Artificial Prediction Markets for Online Prediction of Continuous Variables-A Preliminary Report | http://arxiv.org/pdf/1508.02681v1.pdf | author:Fatemeh Jahedpari, Marina De Vos, Sattar Hashemi, Benjamin Hirsch, Julian Padget category:cs.AI cs.LG published:2015-08-11 summary:We propose the Artificial Continuous Prediction Market (ACPM) as a means topredict a continuous real value, by integrating a range of data sources andaggregating the results of different machine learning (ML) algorithms. ACPMadapts the concept of the (physical) prediction market to address theprediction of real values instead of discrete events. Each ACPM participant hasa data source, a ML algorithm and a local decision-making procedure thatdetermines what to bid on what value. The contributions of ACPM are: (i)adaptation to changes in data quality by the use of learning in: (a) themarket, which weights each market participant to adjust the influence of eachon the market prediction and (b) the participants, which use a Q-learning basedtrading strategy to incorporate the market prediction into their subsequentpredictions, (ii) resilience to a changing population of low- andhigh-performing participants. We demonstrate the effectiveness of ACPM byapplication to an influenza-like illnesses data set, showing ACPM out-performsa range of well-known regression models and is resilient to variation in datasource quality.
arxiv-12000-270 | Integrating K-means with Quadratic Programming Feature Selection | http://arxiv.org/pdf/1505.01728v2.pdf | author:Yamuna Prasad, K. K. Biswas category:cs.CV cs.LG published:2015-05-07 summary:Several data mining problems are characterized by data in high dimensions.One of the popular ways to reduce the dimensionality of the data is to performfeature selection, i.e, select a subset of relevant and non-redundant features.Recently, Quadratic Programming Feature Selection (QPFS) has been proposedwhich formulates the feature selection problem as a quadratic program. It hasbeen shown to outperform many of the existing feature selection methods for avariety of applications. Though, better than many existing approaches, therunning time complexity of QPFS is cubic in the number of features, which canbe quite computationally expensive even for moderately sized datasets. In thispaper we propose a novel method for feature selection by integrating k-meansclustering with QPFS. The basic variant of our approach runs k-means to bringdown the number of features which need to be passed on to QPFS. We then enhancethis idea, wherein we gradually refine the feature space from a very coarseclustering to a fine-grained one, by interleaving steps of QPFS with k-meansclustering. Every step of QPFS helps in identifying the clusters of irrelevantfeatures (which can then be thrown away), whereas every step of k-means furtherrefines the clusters which are potentially relevant. We show that our iterativerefinement of clusters is guaranteed to converge. We provide bounds on thenumber of distance computations involved in the k-means algorithm. Further,each QPFS run is now cubic in number of clusters, which can be much smallerthan actual number of features. Experiments on eight publicly availabledatasets show that our approach gives significant computational gains (both intime and memory), over standard QPFS as well as other state of the art featureselection methods, even while improving the overall accuracy.
arxiv-12000-271 | Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs | http://arxiv.org/pdf/1508.00657v2.pdf | author:Miguel Ballesteros, Chris Dyer, Noah A. Smith category:cs.CL published:2015-08-04 summary:We present extensions to a continuous-state dependency parsing method thatmakes it applicable to morphologically rich languages. Starting with ahigh-performance transition-based parser that uses long short-term memory(LSTM) recurrent neural networks to learn representations of the parser state,we replace lookup-based word representations with representations constructedfrom the orthographic representations of the words, also using LSTMs. Thisallows statistical sharing across word forms that are similar on the surface.Experiments for morphologically rich languages show that the parsing modelbenefits from incorporating the character-based encodings of words.
arxiv-12000-272 | Communication-efficient sparse regression: a one-shot approach | http://arxiv.org/pdf/1503.04337v3.pdf | author:Jason D. Lee, Yuekai Sun, Qiang Liu, Jonathan E. Taylor category:stat.ML cs.LG published:2015-03-14 summary:We devise a one-shot approach to distributed sparse regression in thehigh-dimensional setting. The key idea is to average "debiased" or"desparsified" lasso estimators. We show the approach converges at the samerate as the lasso as long as the dataset is not split across too many machines.We also extend the approach to generalized linear models.
arxiv-12000-273 | Alternating Minimization Algorithm with Automatic Relevance Determination for Transmission Tomography under Poisson Noise | http://arxiv.org/pdf/1412.8464v2.pdf | author:Yan Kaganovsky, Shaobo Han, Soysal Degirmenci, David G. Politte, David J. Brady, Joseph A. O'Sullivan, Lawrence Carin category:math.NA stat.ML published:2014-12-29 summary:We propose a globally convergent alternating minimization (AM) algorithm forimage reconstruction in transmission tomography, which extends automaticrelevance determination (ARD) to Poisson noise models with Beer's law. Thealgorithm promotes solutions that are sparse in the pixel/voxel-differencesdomain by introducing additional latent variables, one for each pixel/voxel,and then learning these variables from the data using a hierarchical Bayesianmodel. Importantly, the proposed AM algorithm is free of any tuning parameterswith image quality comparable to standard penalized likelihood methods. Ouralgorithm exploits optimization transfer principles which reduce the probleminto parallel 1D optimization tasks (one for each pixel/voxel), making thealgorithm feasible for large-scale problems. This approach considerably reducesthe computational bottleneck of ARD associated with the posterior variances.Positivity constraints inherent in transmission tomography problems are alsoenforced. We demonstrate the performance of the proposed algorithm for x-raycomputed tomography using synthetic and real-world datasets. The algorithm isshown to have much better performance than prior ARD algorithms based onapproximate Gaussian noise models, even for high photon flux.
arxiv-12000-274 | Handling oversampling in dynamic networks using link prediction | http://arxiv.org/pdf/1504.06667v2.pdf | author:Benjamin Fish, Rajmonda S. Caceres category:cs.SI cs.LG physics.soc-ph published:2015-04-24 summary:Oversampling is a common characteristic of data representing dynamicnetworks. It introduces noise into representations of dynamic networks, butthere has been little work so far to compensate for it. Oversampling can affectthe quality of many important algorithmic problems on dynamic networks,including link prediction. Link prediction seeks to predict edges that will beadded to the network given previous snapshots. We show that not only doesoversampling affect the quality of link prediction, but that we can use linkprediction to recover from the effects of oversampling. We also introduce anovel generative model of noise in dynamic networks that representsoversampling. We demonstrate the results of our approach on both synthetic andreal-world data.
arxiv-12000-275 | InAR:Inverse Augmented Reality | http://arxiv.org/pdf/1508.02606v1.pdf | author:Hao Hu, Hainan Cui category:cs.CV published:2015-08-11 summary:Augmented reality is the art to seamlessly fuse virtual objects into realones. In this short note, we address the opposite problem, the inverseaugmented reality, that is, given a perfectly augmented reality scene wherehuman is unable to distinguish real objects from virtual ones, how the machinecould help do the job. We show by structure from motion (SFM), a simple 3Dreconstruction technique from images in computer vision, the real and virtualobjects can be easily separated in the reconstructed 3D scene.
arxiv-12000-276 | Learning Deep Representation for Face Alignment with Auxiliary Attributes | http://arxiv.org/pdf/1408.3967v4.pdf | author:Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.LG published:2014-08-18 summary:In this study, we show that landmark detection or face alignment task is nota single and independent problem. Instead, its robustness can be greatlyimproved with auxiliary information. Specifically, we jointly optimize landmarkdetection together with the recognition of heterogeneous but subtly correlatedfacial attributes, such as gender, expression, and appearance attributes. Thisis non-trivial since different attribute inference tasks have differentlearning difficulties and convergence rates. To address this problem, weformulate a novel tasks-constrained deep model, which not only learns theinter-task correlation but also employs dynamic task coefficients to facilitatethe optimization convergence when learning multiple complex tasks. Extensiveevaluations show that the proposed task-constrained learning (i) outperformsexisting face alignment methods, especially in dealing with faces with severeocclusion and pose variation, and (ii) reduces model complexity drasticallycompared to the state-of-the-art methods based on cascaded deep model.
arxiv-12000-277 | Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning in Hierarchy | http://arxiv.org/pdf/1508.01887v2.pdf | author:Zhanglin Peng, Ya Li, Zhaoquan Cai, Liang Lin category:cs.CV cs.LG cs.NE published:2015-08-08 summary:This work investigates how the traditional image classification pipelines canbe extended into a deep architecture, inspired by recent successes of deepneural networks. We propose a deep boosting framework based on layer-by-layerjoint feature boosting and dictionary learning. In each layer, we construct adictionary of filters by combining the filters from the lower layer, anditeratively optimize the image representation with a jointdiscriminative-generative formulation, i.e. minimization of empiricalclassification error plus regularization of analysis image generation overtraining images. For optimization, we perform two iterating steps: i) tominimize the classification error, select the most discriminative featuresusing the gentle adaboost algorithm; ii) according to the feature selection,update the filters to minimize the regularization on analysis imagerepresentation using the gradient descent method. Once the optimization isconverged, we learn the higher layer representation in the same way. Our modeldelivers several distinct advantages. First, our layer-wise optimizationprovides the potential to build very deep architectures. Second, the generatedimage representation is compact and meaningful. In several visual recognitiontasks, our framework outperforms existing state-of-the-art approaches.
arxiv-12000-278 | Topology Control of wireless sensor network using Quantum Inspired Genetic algorithm | http://arxiv.org/pdf/1508.02521v1.pdf | author:Sajid Ullah, Mussarat Wahid category:cs.NE cs.NI published:2015-08-11 summary:In this work, an evolving Linked Quantum register has been introduced, whichare group vector of binary pair of genes, which in its local proximityrepresent those nodes that will have high connectivity and keep the energyconsumption at low, and which are taken into account for topology control. Theregister works in higher dimension. Here order-2 Quantum inspired geneticalgorithm has been used and also higher order can be used to achieve greaterversatility in topology control of nodes. Numerical result has been obtained,analysis is done as how the result has previously been obtained with Quantumgenetic algorithm and results are compared too. For future work, factor ishinted which would exploit the algorithm to work in more computationalintensive problem.
arxiv-12000-279 | Simulating Brain Reaction to Methamphetamine Regarding Consumer Personality | http://arxiv.org/pdf/1508.02505v1.pdf | author:Maryam Keyvanara, Seyed Amirhassan Monadjemi category:cs.NE q-bio.NC published:2015-08-11 summary:Addiction, as a nervous disease, can be analysed using mathematical modellingand computer simulations. In this paper, we use an existing mathematical modelto predict and simulate human brain response to the consumption of a singledose of methamphetamine. The model is implemented and coded in Matlab. Threetypes of personalities including introverts, ambiverts and extroverts arestudied. The parameters of the mathematical model are calibrated and optimized,according to psychological theories, using a real coded genetic algorithm. Thesimulations show significant correlation between people response tomethamphetamine abuse and their personality. They also show that one of thecauses of tendency to stimulants roots in consumers personality traits. Theresults can be used as a tool for reducing attitude towards addiction.
arxiv-12000-280 | Crowd Access Path Optimization: Diversity Matters | http://arxiv.org/pdf/1508.01951v2.pdf | author:Besmira Nushi, Adish Singla, Anja Gruenheid, Erfan Zamanian, Andreas Krause, Donald Kossmann category:cs.LG cs.DB published:2015-08-08 summary:Quality assurance is one the most important challenges in crowdsourcing.Assigning tasks to several workers to increase quality through redundantanswers can be expensive if asking homogeneous sources. This limitation hasbeen overlooked by current crowdsourcing platforms resulting therefore incostly solutions. In order to achieve desirable cost-quality tradeoffs it isessential to apply efficient crowd access optimization techniques. Our workargues that optimization needs to be aware of diversity and correlation ofinformation within groups of individuals so that crowdsourcing redundancy canbe adequately planned beforehand. Based on this intuitive idea, we introducethe Access Path Model (APM), a novel crowd model that leverages the notion ofaccess paths as an alternative way of retrieving information. APM aggregatesanswers ensuring high quality and meaningful confidence. Moreover, we devise agreedy optimization algorithm for this model that finds a provably goodapproximate plan to access the crowd. We evaluate our approach on threecrowdsourced datasets that illustrate various aspects of the problem. Ourresults show that the Access Path Model combined with greedy optimization iscost-efficient and practical to overcome common difficulties in large-scalecrowdsourcing like data sparsity and anonymity.
arxiv-12000-281 | Removing Biases from Trainable MT Metrics by Using Self-Training | http://arxiv.org/pdf/1508.02445v1.pdf | author:Milo≈° Stanojeviƒá category:cs.CL published:2015-08-10 summary:Most trainable machine translation (MT) metrics train their weights on humanjudgments of state-of-the-art MT systems outputs. This makes trainable metricsbiases in many ways. One of them is preferring longer translations. Thesebiased metrics when used for tuning are evaluating different types oftranslations -- n-best lists of translations with very diverse quality. Systemstuned with these metrics tend to produce overly long translations that arepreferred by the metric but not by humans. This is usually solved by manuallytweaking metric's weights to equally value recall and precision. Our solutionis more general: (1) it does not address only the recall bias but also allother biases that might be present in the data and (2) it does not require anyknowledge of the types of features used which is useful in cases when manualtuning of metric's weights is not possible. This is accomplished byself-training on unlabeled n-best lists by using metric that was initiallytrained on standard human judgments. One way of looking at this is as domainadaptation from the domain of state-of-the-art MT translations to diversen-best list translations.
arxiv-12000-282 | FactorBase: SQL for Learning A Multi-Relational Graphical Model | http://arxiv.org/pdf/1508.02428v1.pdf | author:Oliver Schulte, Zhensong Qian category:cs.DB cs.LG H.2.8; H.2.4 published:2015-08-10 summary:We describe FactorBase, a new SQL-based framework that leverages a relationaldatabase management system to support multi-relational model discovery. Amulti-relational statistical model provides an integrated analysis of theheterogeneous and interdependent data resources in the database. We adopt theBayesStore design philosophy: statistical models are stored and managed asfirst-class citizens inside a database. Whereas previous systems likeBayesStore support multi-relational inference, FactorBase supportsmulti-relational learning. A case study on six benchmark databases evaluateshow our system supports a challenging machine learning application, namelylearning a first-order Bayesian network model for an entire database. Modellearning in this setting has to examine a large number of potential statisticalassociations across data tables. Our implementation shows how the SQLconstructs in FactorBase facilitate the fast, modular, and reliable developmentof highly scalable model learning systems.
arxiv-12000-283 | Kernel Methods for Linear Discrete-Time Equations | http://arxiv.org/pdf/1507.03111v2.pdf | author:Fritz Colonius, Boumediene Hamzi category:math.DS math.OC math.ST stat.ML stat.TH published:2015-07-11 summary:Methods from learning theory are used in the state space of linear dynamicaland control systems in order to estimate the system matrices. An application tostabilization via algebraic Riccati equations is included. The approach isillustrated via a series of numerical examples.
arxiv-12000-284 | Gait Assessment for Multiple Sclerosis Patients Using Microsoft Kinect | http://arxiv.org/pdf/1508.02405v1.pdf | author:Farnood Gholami, Daria A. Trojan, Jozsef Kovecses, Wassim M. Haddad, Behnood Gholami category:cs.CV published:2015-08-10 summary:Gait analysis of patients with neurological disorders, including multiplesclerosis (MS), is important for rehabilitation and treatment. The MircrosoftKinect sensor, which was developed for motion recognition in gamingapplications, is an ideal candidate for an inexpensive system providing thecapability for human gait analysis. In this research, we develop a framework toquantify the gait abnormality of MS patients using a Kinect for Windows camera.In addition to the previously introduced gait indices, a novel set of MS gaitindices based on the concept of dynamic time warping is introduced. The newlyintroduced indices can characterize a patient's gait pattern as a whole andquantify a subject's gait distance from the healthy population. We willinvestigate the correlation of gait indices with the multiple sclerosis walkingscale (MSWS) and the clinical ambulation score. This work establishes thefeasibility of using the Kinect sensor for clinical gait assessment for MSpatients.
arxiv-12000-285 | Approximation-Aware Dependency Parsing by Belief Propagation | http://arxiv.org/pdf/1508.02375v1.pdf | author:Matthew R. Gormley, Mark Dredze, Jason Eisner category:cs.CL cs.LG published:2015-08-10 summary:We show how to train the fast dependency parser of Smith and Eisner (2008)for improved accuracy. This parser can consider higher-order interactions amongedges while retaining O(n^3) runtime. It outputs the parse with maximumexpected recall -- but for speed, this expectation is taken under a posteriordistribution that is constructed only approximately, using loopy beliefpropagation through structured factors. We show how to adjust the modelparameters to compensate for the errors introduced by this approximation, byfollowing the gradient of the actual loss on training data. We find thisgradient by back-propagation. That is, we treat the entire parser(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)and Domke (2010) did for loopy CRFs. The resulting trained parser obtainshigher accuracy with fewer iterations of belief propagation than one trained byconditional log-likelihood.
arxiv-12000-286 | Training Conditional Random Fields with Natural Gradient Descent | http://arxiv.org/pdf/1508.02373v1.pdf | author:Yuan Cao category:cs.LG published:2015-08-10 summary:We propose a novel parameter estimation procedure that works efficiently forconditional random fields (CRF). This algorithm is an extension to the maximumlikelihood estimation (MLE), using loss functions defined by Bregmandivergences which measure the proximity between the model expectation and theempirical mean of the feature vectors. This leads to a flexible trainingframework from which multiple update strategies can be derived using naturalgradient descent (NGD). We carefully choose the convex function inducing theBregman divergence so that the types of updates are reduced, while making theoptimization procedure more effective by transforming the gradients of thelog-likelihood loss function. The derived algorithms are very simple and can beeasily implemented on top of the existing stochastic gradient descent (SGD)optimization procedure, yet it is very effective as illustrated by experimentalresults.
arxiv-12000-287 | Local Algorithms for Block Models with Side Information | http://arxiv.org/pdf/1508.02344v1.pdf | author:Elchanan Mossel, Jiaming Xu category:stat.ML cs.CC cs.DC math.PR published:2015-08-10 summary:There has been a recent interest in understanding the power of localalgorithms for optimization and inference problems on sparse graphs. Gamarnikand Sudan (2014) showed that local algorithms are weaker than global algorithmsfor finding large independent sets in sparse random regular graphs. Montanari(2015) showed that local algorithms are suboptimal for finding a community withhigh connectivity in the sparse Erd\H{o}s-R\'enyi random graphs. For thesymmetric planted partition problem (also named community detection for theblock models) on sparse graphs, a simple observation is that local algorithmscannot have non-trivial performance. In this work we consider the effect of side information on local algorithmsfor community detection under the binary symmetric stochastic block model. Inthe block model with side information each of the $n$ vertices is labeled $+$or $-$ independently and uniformly at random; each pair of vertices isconnected independently with probability $a/n$ if both of them have the samelabel or $b/n$ otherwise. The goal is to estimate the underlying vertexlabeling given 1) the graph structure and 2) side information in the form of avertex labeling positively correlated with the true one. Assuming that theratio between in and out degree $a/b$ is $\Theta(1)$ and the average degree $(a+b) / 2 = n^{o(1)}$, we characterize three different regimes under which alocal algorithm, namely, belief propagation run on the local neighborhoods,maximizes the expected fraction of vertices labeled correctly. Thus, incontrast to the case of symmetric block models without side information, weshow that local algorithms can achieve optimal performance for the block modelwith side information.
arxiv-12000-288 | Measuring Word Significance using Distributed Representations of Words | http://arxiv.org/pdf/1508.02297v1.pdf | author:Adriaan M. J. Schakel, Benjamin J. Wilson category:cs.CL published:2015-08-10 summary:Distributed representations of words as real-valued vectors in a relativelylow-dimensional space aim at extracting syntactic and semantic features fromlarge text corpora. A recently introduced neural network, named word2vec(Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semanticinformation in the direction of the word vectors. In this brief report, it isproposed to use the length of the vectors, together with the term frequency, asmeasure of word significance in a corpus. Experimental evidence using adomain-specific corpus of abstracts is presented to support this proposal. Auseful visualization technique for text corpora emerges, where words are mappedonto a two-dimensional plane and automatically ranked by significance.
arxiv-12000-289 | Adapting Phrase-based Machine Translation to Normalise Medical Terms in Social Media Messages | http://arxiv.org/pdf/1508.02285v1.pdf | author:Nut Limsopatham, Nigel Collier category:cs.CL published:2015-08-10 summary:Previous studies have shown that health reports in social media, such asDailyStrength and Twitter, have potential for monitoring health conditions(e.g. adverse drug reactions, infectious diseases) in particular communities.However, in order for a machine to understand and make inferences on thesehealth conditions, the ability to recognise when laymen's terms refer to aparticular medical concept (i.e.\ text normalisation) is required. To achievethis, we propose to adapt an existing phrase-based machine translation (MT)technique and a vector representation of words to map between a social mediaphrase and a medical concept. We evaluate our proposed approach using acollection of phrases from tweets related to adverse drug reactions. Ourexperimental results show that the combination of a phrase-based MT techniqueand the similarity between word vector representations outperforms thebaselines that apply only either of them by up to 55%.
arxiv-12000-290 | Dropout Training for SVMs with Data Augmentation | http://arxiv.org/pdf/1508.02268v1.pdf | author:Ning Chen, Jun Zhu, Jianfei Chen, Ting Chen category:cs.LG published:2015-08-10 summary:Dropout and other feature noising schemes have shown promising results incontrolling over-fitting by artificially corrupting the training data. Thoughextensive theoretical and empirical studies have been performed for generalizedlinear models, little work has been done for support vector machines (SVMs),one of the most successful approaches for supervised learning. This paperpresents dropout training for both linear SVMs and the nonlinear extension withlatent representation learning. For linear SVMs, to deal with the intractableexpectation of the non-smooth hinge loss under corrupting distributions, wedevelop an iteratively re-weighted least square (IRLS) algorithm by exploringdata augmentation techniques. Our algorithm iteratively minimizes theexpectation of a re-weighted least square problem, where the re-weights areanalytically updated. For nonlinear latent SVMs, we consider learning one layerof latent representations in SVMs and extend the data augmentation technique inconjunction with first-order Taylor-expansion to deal with the intractableexpected non-smooth hinge loss and the nonlinearity of latent representations.Finally, we apply the similar data augmentation ideas to develop a new IRLSalgorithm for the expected logistic loss under corrupting distributions, and wefurther develop a non-linear extension of logistic regression by incorporatingone layer of latent representations. Our algorithms offer insights on theconnection and difference between the hinge loss and logistic loss in dropouttraining. Empirical results on several real datasets demonstrate theeffectiveness of dropout training on significantly boosting the classificationaccuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMsfurther improve the prediction performance on several image datasets.
arxiv-12000-291 | A Novel Tensor Robust PCA Approach for Background Subtraction from Compressive Measurements | http://arxiv.org/pdf/1503.01868v3.pdf | author:Wenfei Cao, Yao Wang, Jian Sun, Deyu Meng, Can Yang, Andrzej Cichocki, Zongben Xu category:cs.CV published:2015-03-06 summary:Background subtraction has been a fundamental and widely studied task invideo analysis, with a wide range of applications in video surveillance,teleconferencing and 3D modeling. Recently, motivated by compressive imaging,background subtraction from compressive measurements (BSCM) is becoming anactive research task in video surveillance. In this paper, we propose a noveltensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video framesinto backgrounds with spatial-temporal correlations and foregrounds withspatio-temporal continuity in a tensor framework. In this approach, we use 3Dtotal variation to enhance the spatio-temporal continuity of foregrounds, andTucker decomposition to model the spatio-temporal correlations of videobackground. Based on this idea, we design a basic tensor RPCA model over thevideo frames, dubbed holistic TenRPCA model (H-TenRPCA). To characterize thecorrelations among the groups of similar 3D patches of video background, wefurther design a patch-group-based tensor RPCA model (PG-TenRPCA) by jointtensor Tucker decompositions of 3D patch groups for modeling the videobackground. Efficient algorithms using alternating direction method ofmultipliers (ADMM) are developed to solve the proposed models. Extensiveexperiments on simulated and real-world videos demonstrate the superiority ofthe proposed models over the existing state-of-the-art approaches.
arxiv-12000-292 | Feature Learning for Interaction Activity Recognition in RGBD Videos | http://arxiv.org/pdf/1508.02246v1.pdf | author:Ngu Nguyen category:cs.CV published:2015-08-10 summary:This paper proposes a human activity recognition method which is based onfeatures learned from 3D video data without incorporating domain knowledge. Theexperiments on data collected by RGBD cameras produce results outperformingother techniques. Our feature encoding method follows the bag-of-visual-wordmodel, then we use a SVM classifier to recognise the activities. We do not useskeleton or tracking information and the same technique is applied on color anddepth data.
arxiv-12000-293 | Improve the Evaluation of Translation Fluency by Using Entropy of Matched Sub-segments | http://arxiv.org/pdf/1508.02225v1.pdf | author:Hui Yu, Xiaofeng Wu, Wenbin Jiang, Qun Liu, Shouxun Lin category:cs.CL published:2015-08-10 summary:The widely-used automatic evaluation metrics cannot adequately reflect thefluency of the translations. The n-gram-based metrics, like BLEU, limit themaximum length of matched fragments to n and cannot catch the matched fragmentslonger than n, so they can only reflect the fluency indirectly. METEOR, whichis not limited by n-gram, uses the number of matched chunks but it does notconsider the length of each chunk. In this paper, we propose an entropy-basedmethod, which can sufficiently reflect the fluency of translations through thedistribution of matched words. This method can easily combine with thewidely-used automatic evaluation metrics to improve the evaluation of fluency.Experiments show that the correlations of BLEU and METEOR are improved onsentence level after combining with the entropy-based method on WMT 2010 andWMT 2012.
arxiv-12000-294 | Model-based SIR for dimension reduction | http://arxiv.org/pdf/1508.02186v1.pdf | author:Luca Scrucca category:stat.ME stat.ML published:2015-08-10 summary:A new dimension reduction method based on Gaussian finite mixtures isproposed as an extension to sliced inverse regression (SIR). The model-basedSIR (MSIR) approach allows the main limitation of SIR to be overcome, i.e.,failure in the presence of regression symmetric relationships, without the needto impose further assumptions. Extensive numerical studies are presented tocompare the new method with some of most popular dimension reduction methods,such as SIR, sliced average variance estimation, principal Hessian direction,and directional regression. MSIR appears sufficiently flexible to accommodatevarious regression functions, and its performance is comparable with or better,particularly as sample size grows, than other available methods. Lastly, MSIRis illustrated with two real data examples about ozone concentrationregression, and hand-written digit classification.
arxiv-12000-295 | Automatic Extraction of the Passing Strategies of Soccer Teams | http://arxiv.org/pdf/1508.02171v1.pdf | author:Laszlo Gyarmati, Xavier Anguera category:cs.CV stat.ML published:2015-08-10 summary:Technology offers new ways to measure the locations of the players and of theball in sports. This translates to the trajectories the ball takes on the fieldas a result of the tactics the team applies. The challenge professionals insoccer are facing is to take the reverse path: given the trajectories of theball is it possible to infer the underlying strategy/tactic of a team? Wepropose a method based on Dynamic Time Warping to reveal the tactics of a teamthrough the analysis of repeating series of events. Based on the analysis of anentire season, we derive insights such as passing strategies for maintainingball possession or counter attacks, and passing styles with a focus on the teamor on the capabilities of the individual players.
arxiv-12000-296 | Feature-based Decipherment for Large Vocabulary Machine Translation | http://arxiv.org/pdf/1508.02142v1.pdf | author:Iftekhar Naim, Daniel Gildea category:cs.CL published:2015-08-10 summary:Orthographic similarities across languages provide a strong signal forprobabilistic decipherment, especially for closely related language pairs. Theexisting decipherment models, however, are not well-suited for exploiting theseorthographic similarities. We propose a log-linear model with latent variablesthat incorporates orthographic similarity features. Maximum likelihood trainingis computationally expensive for the proposed log-linear model. To address thischallenge, we perform approximate inference via MCMC sampling and contrastivedivergence. Our results show that the proposed log-linear model withcontrastive divergence scales to large vocabularies and outperforms theexisting generative decipherment models by exploiting the orthographicfeatures.
arxiv-12000-297 | Learning Structural Kernels for Natural Language Processing | http://arxiv.org/pdf/1508.02131v1.pdf | author:Daniel Beck, Trevor Cohn, Christian Hardmeier, Lucia Specia category:cs.CL cs.LG published:2015-08-10 summary:Structural kernels are a flexible learning paradigm that has been widely usedin Natural Language Processing. However, the problem of model selection inkernel-based methods is usually overlooked. Previous approaches mostly rely onsetting default values for kernel hyperparameters or using grid search, whichis slow and coarse-grained. In contrast, Bayesian methods allow efficient modelselection by maximizing the evidence on the training data throughgradient-based methods. In this paper we show how to perform this in thecontext of structural kernels by using Gaussian Processes. Experimental resultson tree kernels show that this procedure results in better predictionperformance compared to hyperparameter optimization via grid search. Theframework proposed in this paper can be adapted to other structures besidestrees, e.g., strings and graphs, thereby extending the utility of kernel-basedmethods.
arxiv-12000-298 | Learning the nonlinear geometry of high-dimensional data: Models and algorithms | http://arxiv.org/pdf/1412.6808v2.pdf | author:Tong Wu, Waheed U. Bajwa category:stat.ML cs.CV cs.LG published:2014-12-21 summary:Modern information processing relies on the axiom that high-dimensional datalie near low-dimensional geometric structures. This paper revisits the problemof data-driven learning of these geometric structures and puts forth two newnonlinear geometric models for data describing "related" objects/phenomena. Thefirst one of these models straddles the two extremes of the subspace model andthe union-of-subspaces model, and is termed the metric-constrainedunion-of-subspaces (MC-UoS) model. The second one of these models---suited fordata drawn from a mixture of nonlinear manifolds---generalizes the kernelsubspace model, and is termed the metric-constrained kernel union-of-subspaces(MC-KUoS) model. The main contributions of this paper in this regard includethe following. First, it motivates and formalizes the problems of MC-UoS andMC-KUoS learning. Second, it presents algorithms that efficiently learn anMC-UoS or an MC-KUoS underlying data of interest. Third, it extends thesealgorithms to the case when parts of the data are missing. Last, but not least,it reports the outcomes of a series of numerical experiments involving bothsynthetic and real data that demonstrate the superiority of the proposedgeometric models and learning algorithms over existing approaches in theliterature. These experiments also help clarify the connections between thiswork and the literature on (subspace and kernel k-means) clustering.
arxiv-12000-299 | Beyond Bell's Theorem II: Scenarios with arbitrary causal structure | http://arxiv.org/pdf/1404.4812v2.pdf | author:Tobias Fritz category:quant-ph stat.ML published:2014-04-18 summary:It has recently been found that Bell scenarios are only a small subclass ofinteresting setups for studying the non-classical features of quantum theorywithin spacetime. We find that it is possible to talk about classicalcorrelations, quantum correlations and other kinds of correlations on anydirected acyclic graph, and this captures various extensions of Bell scenarioswhich have been considered in the literature. From a conceptual point of view,the main feature of our approach is its high level of unification: while thenotions of source, choice of setting and measurement play all seeminglydifferent roles in a Bell scenario, our formalism shows that they are allinstances of the same concept of "event". Our work can also be understood as a contribution to the subject of causalinference with latent variables. Among other things, we introduce hiddenBayesian networks as a generalization of hidden Markov models.
arxiv-12000-300 | Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation | http://arxiv.org/pdf/1508.02096v1.pdf | author:Wang Ling, Tiago Lu√≠s, Lu√≠s Marujo, Ram√≥n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W. Black, Isabel Trancoso category:cs.CL published:2015-08-09 summary:We introduce a model for constructing vector representations of words bycomposing characters using bidirectional LSTMs. Relative to traditional wordrepresentation models that have independent vectors for each word type, ourmodel requires only a single vector per character type and a fixed set ofparameters for the compositional model. Despite the compactness of this modeland, more importantly, the arbitrary nature of the form-function relationshipin language, our "composed" word representations yield state-of-the-art resultsin language modeling and part-of-speech tagging. Benefits over traditionalbaselines are particularly pronounced in morphologically rich languages (e.g.,Turkish).
