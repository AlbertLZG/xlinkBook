arxiv-12000-1 | Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization | http://arxiv.org/pdf/1506.05424v1.pdf | author:Conrado Silva Miranda, Fernando Jos√© Von Zuben category:cs.NE cs.AI published:2015-06-17 summary:This paper introduces a high-performance hybrid algorithm, called HybridHypervolume Maximization Algorithm (H2MA), for multi-objective optimizationthat alternates between exploring the decision space and exploiting the alreadyobtained non-dominated solutions. The proposal is centered on maximizing thehypervolume indicator, thus converting the multi-objective problem into asingle-objective one. The exploitation employs gradient-based methods, butconsidering a single candidate efficient solution at a time, to overcomelimitations associated with population-based approaches and also to allow aneasy control of the number of solutions provided. There is an interchangebetween two steps. The first step is a deterministic local exploration, endowedwith an automatic procedure to detect stagnation. When stagnation is detected,the search is switched to a second step characterized by a stochastic globalexploration using an evolutionary algorithm. Using five ZDT benchmarks with 30variables, the performance of the new algorithm is compared to state-of-the-artalgorithms for multi-objective optimization, more specifically NSGA-II, SPEA2,and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume andsmaller distance to the true Pareto frontier with significantly less functionevaluations, even when the gradient is estimated numerically. Furthermore,although only continuous decision spaces have been considered here, discretedecision spaces could also have been treated, replacing gradient-based searchby hill-climbing. Finally, a thorough explanation is provided to support theexpressive gain in performance that was achieved.
arxiv-12000-2 | On the Depth of Deep Neural Networks: A Theoretical View | http://arxiv.org/pdf/1506.05232v2.pdf | author:Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, Tie-Yan Liu category:cs.LG published:2015-06-17 summary:People believe that depth plays an important role in success of deep neuralnetworks (DNN). However, this belief lacks solid theoretical justifications asfar as we know. We investigate role of depth from perspective of margin bound.In margin bound, expected error is upper bounded by empirical margin error plusRademacher Average (RA) based capacity term. First, we derive an upper boundfor RA of DNN, and show that it increases with increasing depth. This indicatesnegative impact of depth on test performance. Second, we show that deepernetworks tend to have larger representation power (measured by Betti numbersbased complexity) than shallower networks in multi-class setting, and thus canlead to smaller empirical margin error. This implies positive impact of depth.The combination of these two results shows that for DNN with restricted numberof hidden units, increasing depth is not always good since there is a tradeoffbetween positive and negative impacts. These results inspire us to seekalternative ways to achieve positive impact of depth, e.g., imposingmargin-based penalty terms to cross entropy loss so as to reduce empiricalmargin error without increasing depth. Our experiments show that in this way,we achieve significantly better test performance.
arxiv-12000-3 | MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance Fingerprinting | http://arxiv.org/pdf/1506.05393v1.pdf | author:Ze Wang category:cs.DS cs.CV published:2015-06-17 summary:Magnetic resonance fingerprinting (MRF) is a new technique for simultaneouslyquantifying multiple MR parameters using one temporally resolved MR scan. Butits brute-force dictionary generating and searching (DGS) process causes a hugedisk space demand and computational burden, prohibiting it from a practicalmultiple slice high-definition imaging. The purpose of this paper was toprovide a fast and space efficient DGS algorithm for MRF. Based on an empiricalanalysis of properties of the distance function of the acquired MRF signal andthe pre-defined MRF dictionary entries, we proposed a parameter separable MRFDGS method, which breaks the multiplicative computation complexity into anadditive one and enabling a resolution scalable multi-resolution DGS process,which was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM washundreds or thousands of times faster than the original brute-force DGS method.The acceleration was even higher when considering the time difference forgenerating the dictionary. Using a high precision quantification, MRF can findthe right parameter values for a 64x64 imaging slice in 117 secs. Our data alsoshowed that spatial constraints can be used to further speed up MRF ZOOM.
arxiv-12000-4 | Deep Denoising Auto-encoder for Statistical Speech Synthesis | http://arxiv.org/pdf/1506.05268v1.pdf | author:Zhenzhou Wu, Shinji Takaki, Junichi Yamagishi category:cs.SD cs.LG published:2015-06-17 summary:This paper proposes a deep denoising auto-encoder technique to extract betteracoustic features for speech synthesis. The technique allows us toautomatically extract low-dimensional features from high dimensional spectralfeatures in a non-linear, data-driven, unsupervised way. We compared the newstochastic feature extractor with conventional mel-cepstral analysis inanalysis-by-synthesis and text-to-speech experiments. Our results confirm thatthe proposed method increases the quality of synthetic speech in bothexperiments.
arxiv-12000-5 | CFORB: Circular FREAK-ORB Visual Odometry | http://arxiv.org/pdf/1506.05257v1.pdf | author:Daniel J. Mankowitz, Ehud Rivlin category:cs.CV published:2015-06-17 summary:We present a novel Visual Odometry algorithm entitled Circular FREAK-ORB(CFORB). This algorithm detects features using the well-known ORB algorithm[12] and computes feature descriptors using the FREAK algorithm [14]. CFORB isinvariant to both rotation and scale changes, and is suitable for use inenvironments with uneven terrain. Two visual geometric constraints have beenutilized in order to remove invalid feature descriptor matches. Theseconstraints have not previously been utilized in a Visual Odometry algorithm. Avariation to circular matching [16] has also been implemented. This allowsfeatures to be matched between images without having to be dependent upon theepipolar constraint. This algorithm has been run on the KITTI benchmark datasetand achieves a competitive average translational error of $3.73 \%$ and averagerotational error of $0.0107 deg/m$. CFORB has also been run in an indoorenvironment and achieved an average translational error of $3.70 \%$. Afterrunning CFORB in a highly textured environment with an approximately uniformfeature spread across the images, the algorithm achieves an averagetranslational error of $2.4 \%$ and an average rotational error of $0.009deg/m$.
arxiv-12000-6 | Gradient Estimation Using Stochastic Computation Graphs | http://arxiv.org/pdf/1506.05254v3.pdf | author:John Schulman, Nicolas Heess, Theophane Weber, Pieter Abbeel category:cs.LG published:2015-06-17 summary:In a variety of problems originating in supervised, unsupervised, andreinforcement learning, the loss function is defined by an expectation over acollection of random variables, which might be part of a probabilistic model orthe external world. Estimating the gradient of this loss function, usingsamples, lies at the core of gradient-based learning algorithms for theseproblems. We introduce the formalism of stochastic computationgraphs---directed acyclic graphs that include both deterministic functions andconditional probability distributions---and describe how to easily andautomatically derive an unbiased estimator of the loss function's gradient. Theresulting algorithm for computing the gradient estimator is a simplemodification of the standard backpropagation algorithm. The generic scheme wepropose unifies estimators derived in variety of prior work, along withvariance-reduction techniques therein. It could assist researchers indeveloping intricate models involving a combination of stochastic anddeterministic operations, enabling, for example, attention, memory, and controlactions.
arxiv-12000-7 | Non-distributional Word Vector Representations | http://arxiv.org/pdf/1506.05230v1.pdf | author:Manaal Faruqui, Chris Dyer category:cs.CL published:2015-06-17 summary:Data-driven representation learning for words is a technique of centralimportance in NLP. While indisputably useful as a source of features indownstream tasks, such vectors tend to consist of uninterpretable componentswhose relationship to the categories of traditional lexical semantic theoriesis tenuous at best. We present a method for constructing interpretable wordvectors from hand-crafted linguistic resources like WordNet, FrameNet etc.These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. Weanalyze their performance on state-of-the-art evaluation methods fordistributional models of word vectors and find they are competitive to standarddistributional approaches.
arxiv-12000-8 | Robust High Quality Image Guided Depth Upsampling | http://arxiv.org/pdf/1506.05187v1.pdf | author:Wei Liu, Yijun Li, Xiaogang Chen, Jie Yang, Qiang Wu, Jingyi Yu category:cs.CV published:2015-06-17 summary:Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at ahigh frame rate. However, its low resolution and sensitivity to the noise arealways a concern. A popular solution is upsampling the obtained noisy lowresolution depth map with the guidance of the companion high resolution colorimage. However, due to the constrains in the existing upsampling models, thehigh resolution depth map obtained in such way may suffer from either texturecopy artifacts or blur of depth discontinuity. In this paper, a noveloptimization framework is proposed with the brand new data term and smoothnessterm. The comprehensive experiments using both synthetic data and real datashow that the proposed method well tackles the problem of texture copyartifacts and blur of depth discontinuity. It also demonstrates sufficientrobustness to the noise. Moreover, a data driven scheme is proposed toadaptively estimate the parameter in the upsampling optimization framework. Theencouraging performance is maintained even in the case of large upsampling e.g.$8\times$ and $16\times$.
arxiv-12000-9 | Partial Functional Correspondence | http://arxiv.org/pdf/1506.05274v2.pdf | author:Emanuele Rodol√†, Luca Cosmo, Michael M. Bronstein, Andrea Torsello, Daniel Cremers category:cs.CV published:2015-06-17 summary:In this paper, we propose a method for computing partial functionalcorrespondence between non-rigid shapes. We use perturbation analysis to showhow removal of shape parts changes the Laplace-Beltrami eigenfunctions, andexploit it as a prior on the spectral representation of the correspondence.Corresponding parts are optimization variables in our problem and are used toweight the functional correspondence; we are looking for the largest and mostregular (in the Mumford-Shah sense) parts that minimize correspondencedistortion. We show that our approach can cope with very challengingcorrespondence settings.
arxiv-12000-10 | A Discriminative Representation of Convolutional Features for Indoor Scene Recognition | http://arxiv.org/pdf/1506.05196v1.pdf | author:Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel category:cs.CV published:2015-06-17 summary:Indoor scene recognition is a multi-faceted and challenging problem due tothe diverse intra-class variations and the confusing inter-class similarities.This paper presents a novel approach which exploits rich mid-levelconvolutional features to categorize indoor scenes. Traditionally usedconvolutional features preserve the global spatial structure, which is adesirable property for general object recognition. However, we argue that thisstructuredness is not much helpful when we have large variations in scenelayouts, e.g., in indoor scenes. We propose to transform the structuredconvolutional activations to another highly discriminative feature space. Therepresentation in the transformed space not only incorporates thediscriminative aspects of the target dataset, but it also encodes the featuresin terms of the general object categories that are present in indoor scenes. Tothis end, we introduce a new large-scale dataset of 1300 object categorieswhich are commonly present in indoor scenes. Our proposed approach achieves asignificant performance boost over previous state of the art approaches on fivemajor scene classification datasets.
arxiv-12000-11 | Robust Estimation of Structured Covariance Matrix for Heavy-Tailed Elliptical Distributions | http://arxiv.org/pdf/1506.05215v1.pdf | author:Ying Sun, Prabhu Babu, Daniel P. Palomar category:stat.AP stat.ML published:2015-06-17 summary:This paper considers the problem of robustly estimating a structuredcovariance matrix with an elliptical underlying distribution with known mean.In applications where the covariance matrix naturally possesses a certainstructure, taking the prior structure information into account in theestimation procedure is beneficial to improve the estimation accuracy. Wepropose incorporating the prior structure information into Tyler's M-estimatorand formulate the problem as minimizing the cost function of Tyler's estimatorunder the prior structural constraint. First, the estimation under a generalconvex structural constraint is introduced with an efficient algorithm forfinding the estimator derived based on the majorization minimization (MM)algorithm framework. Then, the algorithm is tailored to several specialstructures that enjoy a wide range of applications in signal processing relatedfields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitzstructure. In addition, two types of non-convex structures, i.e., the Kroneckerstructure and the spiked covariance structure, are also discussed, where it isshown that simple algorithms can be derived under the guidelines of MM.Numerical results show that the proposed estimator achieves a smallerestimation error than the benchmark estimators at a lower computational cost.
arxiv-12000-12 | Learning Spike time codes through Morphological Learning with Binary Synapses | http://arxiv.org/pdf/1506.05212v1.pdf | author:Subhrajit Roy, Phyo Phyo San, Shaista Hussain, Lee Wang Wei, Arindam Basu category:cs.NE published:2015-06-17 summary:In this paper, a neuron with nonlinear dendrites (NNLD) and binary synapsesthat is able to learn temporal features of spike input patterns is considered.Since binary synapses are considered, learning happens through formation andelimination of connections between the inputs and the dendritic branches tomodify the structure or "morphology" of the NNLD. A morphological learningalgorithm inspired by the 'Tempotron', i.e., a recently proposed temporallearning algorithm-is presented in this work. Unlike 'Tempotron', the proposedlearning rule uses a technique to automatically adapt the NNLD threshold duringtraining. Experimental results indicate that our NNLD with 1-bit synapses canobtain similar accuracy as a traditional Tempotron with 4-bit synapses inclassifying single spike random latency and pair-wise synchrony patterns.Hence, the proposed method is better suited for robust hardware implementationin the presence of statistical variations. We also present results of applyingthis rule to real life spike classification problems from the field of tactilesensing.
arxiv-12000-13 | Real time unsupervised learning of visual stimuli in neuromorphic VLSI systems | http://arxiv.org/pdf/1506.05427v1.pdf | author:Massimiliano Giulioni, Federico Corradi, Vittorio Dante, Paolo del Giudice category:cs.NE q-bio.NC published:2015-06-17 summary:Neuromorphic chips embody computational principles operating in the nervoussystem, into microelectronic devices. In this domain it is important toidentify computational primitives that theory and experiments suggest asgeneric and reusable cognitive elements. One such element is provided byattractor dynamics in recurrent networks. Point attractors are equilibriumstates of the dynamics (up to fluctuations), determined by the synapticstructure of the network; a `basin' of attraction comprises all initial statesleading to a given attractor upon relaxation, hence making attractor dynamicssuitable to implement robust associative memory. The initial network state isdictated by the stimulus, and relaxation to the attractor state implements theretrieval of the corresponding memorized prototypical pattern. In a previouswork we demonstrated that a neuromorphic recurrent network of spiking neuronsand suitably chosen, fixed synapses supports attractor dynamics. Here we focuson learning: activating on-chip synaptic plasticity and using a theory-drivenstrategy for choosing network parameters, we show that autonomous learning,following repeated presentation of simple visual stimuli, shapes a synapticconnectivity supporting stimulus-selective attractors. Associative memorydevelops on chip as the result of the coupled stimulus-driven neural activityand ensuing synaptic dynamics, with no artificial separation between learningand retrieval phases.
arxiv-12000-14 | Communication-Efficient False Discovery Rate Control via Knockoff Aggregation | http://arxiv.org/pdf/1506.05446v2.pdf | author:Weijie Su, Junyang Qian, Linxi Liu category:stat.ML stat.ME published:2015-06-17 summary:The false discovery rate (FDR)---the expected fraction of spuriousdiscoveries among all the discoveries---provides a popular statisticalassessment of the reproducibility of scientific studies in various disciplines.In this work, we introduce a new method for controlling the FDR inmeta-analysis of many decentralized linear models. Our method targets thescenario where many research groups---possibly the number of which israndom---are independently testing a common set of hypotheses and then sendingsummary statistics to a coordinating center in an online manner. Built on theknockoffs framework introduced by Barber and Candes (2015), our procedurestarts by applying the knockoff filter to each linear model and then aggregatesthe summary statistics via one-shot communication in a novel way. This methodgives exact FDR control non-asymptotically without any knowledge of the noisevariances or making any assumption about sparsity of the signal. In certainsettings, it has a communication complexity that is optimal up to a logarithmicfactor.
arxiv-12000-15 | Learning with a Wasserstein Loss | http://arxiv.org/pdf/1506.05439v3.pdf | author:Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, Tomaso Poggio category:cs.LG cs.CV stat.ML published:2015-06-17 summary:Learning to predict multi-label outputs is challenging, but in many problemsthere is a natural metric on the outputs that can be used to improvepredictions. In this paper we develop a loss function for multi-label learning,based on the Wasserstein distance. The Wasserstein distance provides a naturalnotion of dissimilarity for probability measures. Although optimizing withrespect to the exact Wasserstein distance is costly, recent work has describeda regularized approximation that is efficiently computed. We describe anefficient learning algorithm based on this regularization, as well as a novelextension of the Wasserstein distance from probability measures to unnormalizedmeasures. We also describe a statistical learning bound for the loss. TheWasserstein loss can encourage smoothness of the predictions with respect to achosen metric on the output space. We demonstrate this property on a real-datatag prediction problem, using the Yahoo Flickr Creative Commons dataset,outperforming a baseline that doesn't use the metric.
arxiv-12000-16 | Feature Selection for Ridge Regression with Provable Guarantees | http://arxiv.org/pdf/1506.05173v2.pdf | author:Saurabh Paul, Petros Drineas category:stat.ML cs.IT cs.LG math.IT published:2015-06-17 summary:We introduce single-set spectral sparsification as a deterministic samplingbased feature selection technique for regularized least squares classification,which is the classification analogue to ridge regression. The method isunsupervised and gives worst-case guarantees of the generalization power of theclassification function after feature selection with respect to theclassification function obtained using all features. We also introduceleverage-score sampling as an unsupervised randomized feature selection methodfor ridge regression. We provide risk bounds for both single-set spectralsparsification and leverage-score sampling on ridge regression in the fixeddesign setting and show that the risk in the sampled space is comparable to therisk in the full-feature space. We perform experiments on synthetic andreal-world datasets, namely a subset of TechTC-300 datasets, to support ourtheory. Experimental results indicate that the proposed methods perform betterthan the existing feature selection methods.
arxiv-12000-17 | Learning Contextualized Semantics from Co-occurring Terms via a Siamese Architecture | http://arxiv.org/pdf/1506.05514v1.pdf | author:Ubai Sandouk, Ke Chen category:cs.IR cs.CL cs.LG I.2.6 published:2015-06-17 summary:One of the biggest challenges in Multimedia information retrieval andunderstanding is to bridge the semantic gap by properly modeling conceptsemantics in context. The presence of out of vocabulary (OOV) conceptsexacerbates this difficulty. To address the semantic gap issues, we formulate aproblem on learning contextualized semantics from descriptive terms and proposea novel Siamese architecture to model the contextualized semantics fromdescriptive terms. By means of pattern aggregation and probabilistic topicmodels, our Siamese architecture captures contextualized semantics from theco-occurring descriptive terms via unsupervised learning, which leads to aconcept embedding space of the terms in context. Furthermore, the co-occurringOOV concepts can be easily represented in the learnt concept embedding space.The main properties of the concept embedding space are demonstrated viavisualization. Using various settings in semantic priming, we have carried outa thorough evaluation by comparing our approach to a number of state-of-the-artmethods on six annotation corpora in different domains, i.e., MagTag5K, CAL500and Million Song Dataset in the music domain as well as Corel5K, LabelMe andSUNDatabase in the image domain. Experimental results on semantic primingsuggest that our approach outperforms those state-of-the-art methodsconsiderably in various aspects.
arxiv-12000-18 | Detection of Epigenomic Network Community Oncomarkers | http://arxiv.org/pdf/1506.05244v2.pdf | author:Thomas E. Bartlett, Alexey Zaikin category:stat.AP q-bio.GN q-bio.MN stat.ML published:2015-06-17 summary:In this paper we propose network methodology to infer prognostic cancerbiomarkers, based on the epigenetic pattern DNA methylation. Epigeneticprocesses such as DNA methylation reflect environmental risk factors, and areincreasingly recognised for their fundamental role in diseases such as cancer.DNA methylation is a gene-regulatory pattern, and hence provides a means bywhich to assess genomic regulatory interactions. Network models are a naturalway to represent and analyse groups of such interactions. The utility ofnetwork models also increases as the quantity of data and number of variablesincrease, making them increasingly relevant to large-scale genomic studies. Wepropose methodology to infer prognostic genomic networks from a DNAmethylation-based measure of genomic interaction and association. We then showhow to identify prognostic biomarkers from such networks, which we term`network community oncomarkers'. We illustrate the power of our proposedmethodology in the context of a large publicly available breast cancerdata-set.
arxiv-12000-19 | Pragmatic Side Effects | http://arxiv.org/pdf/1506.05676v1.pdf | author:Jiri Marsik, Maxime Amblard category:cs.CL published:2015-06-17 summary:In the quest to give a formal compositional semantics to natural languages,semanticists have started turning their attention to phenomena that have beenalso considered as parts of pragmatics (e.g., discourse anaphora andpresupposition projection). To account for these phenomena, the very kinds ofmeanings assigned to words and phrases are often revisited. To be morespecific, in the prevalent paradigm of modeling natural language denotationsusing the simply-typed lambda calculus (higher-order logic) this meansrevisiting the types of denotations assigned to individual parts of speech.However, the lambda calculus also serves as a fundamental theory ofcomputation, and in the study of computation, similar type shifts have beenemployed to give a meaning to side effects. Side effects in programminglanguages correspond to actions that go beyond the lexical scope of anexpression (a thrown exception might propagate throughout a program, a variablemodified at one point might later be read at an another) or even beyond thescope of the program itself (a program might interact with the outside world bye.g., printing documents, making sounds, operating robotic limbs...).
arxiv-12000-20 | Author Identification using Multi-headed Recurrent Neural Networks | http://arxiv.org/pdf/1506.04891v1.pdf | author:Douglas Bagnall category:cs.CL cs.LG cs.NE 68T50 published:2015-06-16 summary:Recurrent neural networks (RNNs) are very good at modelling the flow of text,but typically need to be trained on a far larger corpus than is available forthe PAN 2015 Author Identification task. This paper describes a novel approachwhere the output layer of a character-level RNN language model is split intoseveral independent predictive sub-models, each representing an author, whilethe recurrent layer is shared by all. This allows the recurrent layer to modelthe language as a whole without over-fitting, while the outputs select aspectsof the underlying model that reflect their author's style. The method provescompetitive, ranking first in two of the four languages.
arxiv-12000-21 | Detection and Estimation of Iris Centre | http://arxiv.org/pdf/1506.04843v1.pdf | author:Anirban Dasgupta, Aurobinda Routray, Sai Nataraj Mallavollu category:cs.CV published:2015-06-16 summary:Detection of iris center is an active area of research in the field ofcomputer vision and human-machine interaction systems. The major issuesinvolved in the detection of iris involves glint on the corneal region,occlusion of the iris by eye-lids, occlusions due to eye gaze, high speed ofprocessing etc. This paper presents an algorithm for detecting and estimatingthe iris center thereby addressing some of these issues.
arxiv-12000-22 | Significance of the levels of spectral valleys with application to front/back distinction of vowel sounds | http://arxiv.org/pdf/1506.04828v2.pdf | author:T. V. Ananthapadmanabha, A. G. Ramakrishnan, Shubham Sharma category:cs.CL cs.SD published:2015-06-16 summary:An objective critical distance (OCD) has been defined as that spacing betweenadjacent formants, when the level of the valley between them reaches the meanspectral level. The measured OCD lies in the same range (viz., 3-3.5 bark) asthe critical distance determined by subjective experiments for similarexperimental conditions. The level of spectral valley serves a purpose similarto that of the spacing between the formants with an added advantage that it canbe measured from the spectral envelope without an explicit knowledge of formantfrequencies. Based on the relative spacing of formant frequencies, the level ofthe spectral valley, VI (between F1 and F2) is much higher than the level ofVII (spectral valley between F2 and F3) for back vowels and vice-versa forfront vowels. Classification of vowels into front/back distinction with thedifference (VI-VII) as an acoustic feature, tested using TIMIT, NTIMIT, Tamiland Kannada language databases gives, on the average, an accuracy of about 95%,which is comparable to the accuracy (90.6%) obtained using a neural networkclassifier trained and tested using MFCC as the feature vector for TIMITdatabase. The acoustic feature (VI-VII) has also been tested for its robustnesson the TIMIT database for additive white and babble noise and an accuracy ofabout 95% has been obtained for SNRs down to 25 dB for both types of noise.
arxiv-12000-23 | Emotion Analysis of Songs Based on Lyrical and Audio Features | http://arxiv.org/pdf/1506.05012v1.pdf | author:Adit Jamdar, Jessica Abraham, Karishma Khanna, Rahul Dubey category:cs.CL cs.AI cs.SD published:2015-06-16 summary:In this paper, a method is proposed to detect the emotion of a song based onits lyrical and audio features. Lyrical features are generated by segmentationof lyrics during the process of data extraction. ANEW and WordNet knowledge isthen incorporated to compute Valence and Arousal values. In addition to this,linguistic association rules are applied to ensure that the issue of ambiguityis properly addressed. Audio features are used to supplement the lyrical onesand include attributes like energy, tempo, and danceability. These features areextracted from The Echo Nest, a widely used music intelligence platform.Construction of training and test sets is done on the basis of social tagsextracted from the last.fm website. The classification is done by applyingfeature weighting and stepwise threshold reduction on the k-Nearest Neighborsalgorithm to provide fuzziness in the classification.
arxiv-12000-24 | Parsing Natural Language Sentences by Semi-supervised Methods | http://arxiv.org/pdf/1506.04897v1.pdf | author:Rudolf Rosa category:cs.CL I.2.7 published:2015-06-16 summary:We present our work on semi-supervised parsing of natural language sentences,focusing on multi-source crosslingual transfer of delexicalized dependencyparsers. We first evaluate the influence of treebank annotation styles onparsing performance, focusing on adposition attachment style. Then, we presentKLcpos3, an empirical language similarity measure, designed and tuned forsource parser weighting in multi-source delexicalized parser transfer. Andfinally, we introduce a novel resource combination method, based oninterpolation of trained parser models.
arxiv-12000-25 | Subsampled terahertz data reconstruction based on spatio-temporal dictionary learning | http://arxiv.org/pdf/1506.04912v1.pdf | author:Vahid Abolghasemi, Hao Shen, Yaochun Shen, Lu Gan category:cs.CV published:2015-06-16 summary:In this paper, the problem of terahertz pulsed imaging and reconstruction isaddressed. It is assumed that an incomplete (subsampled) three dimensional THzdata set has been acquired and the aim is to recover all missing samples. Asparsity-inducing approach is proposed for this purpose. First, a simpleinterpolation is applied to incomplete noisy data. Then, we propose aspatio-temporal dictionary learning method to obtain an appropriate sparserepresentation of data based on a joint sparse recovery algorithm. Then, usingthe sparse coefficients and the learned dictionary, the 3D data is effectivelydenoised by minimizing a simple cost function. We consider two types ofterahertz data to evaluate the performance of the proposed approach; THz dataacquired for a model sample with clear layered structures (e.g., a T-shapeplastic sheet buried in a polythene pellet), and pharmaceutical tablet data(with low spatial resolution). The achieved signal-to-noise-ratio forreconstruction of T-shape data, from only 5% observation was 19 dB. Moreover,the accuracies of obtained thickness and depth measurements for pharmaceuticaltablet data after reconstruction from 10% observation were 98.8%, and 99.9%,respectively. These results, along with chemical mapping analysis, presented atthe end of this paper, confirm the accuracy of the proposed method.
arxiv-12000-26 | Depth Perception in Autostereograms: 1/f-Noise is Best | http://arxiv.org/pdf/1506.05036v1.pdf | author:Yael Yankelevsky, Ishai Shvartz, Tamar Avraham, Alfred M. Bruckstein category:cs.CV published:2015-06-16 summary:An autostereogram is a single image that encodes depth information that popsout when looking at it. The trick is achieved by replicating a vertical stripthat sets a basic two-dimensional pattern with disparity shifts that encode athree-dimensional scene. It is of interest to explore the dependency betweenthe ease of perceiving depth in autostereograms and the choice of the basicpattern used for generating them. In this work we confirm a theory proposed byBruckstein et al. to explain the process of autostereographic depth perception,providing a measure for the ease of "locking into" the depth profile, based onthe spectral properties of the basic pattern used. We report the results ofthree sets of psychophysical experiments using autostereograms generated fromtwo-dimensional random noise patterns having power spectra of the form$1/f^\beta$. The experiments were designed to test the ability of humansubjects to identify smooth, low resolution surfaces, as well as detail, in theform of higher resolution objects in the depth profile, and to determine limitsin identifying small objects as a function of their size. In accordance withthe theory, we discover a significant advantage of the $1/f$ noise pattern(pink noise) for fast depth lock-in and fine detail detection, showing thatsuch patterns are optimal choices for autostereogram design. Validating thetheoretical model predictions strengthens its underlying assumptions, andcontributes to a better understanding of the visual system's binoculardisparity mechanisms.
arxiv-12000-27 | Learning with Clustering Penalties | http://arxiv.org/pdf/1506.04908v2.pdf | author:Vincent Roulet, Fajwel Fogel, Alexandre d'Aspremont, Francis Bach category:cs.LG 68T05, 91C20 published:2015-06-16 summary:We study supervised learning problems using clustering penalties to imposestructure on either features, tasks or samples, seeking to help both predictionand interpretation. This arises naturally in problems involving dimensionalityreduction, transfer learning or regression clustering. We derive a unifiedoptimization formulation handling these three settings and produce algorithmswhose core iteration complexity amounts to a k-means clustering step, which canbe approximated efficiently. We test the robustness of our methods onartificial data sets as well as real data extracted from movie reviews and acorpus of text documents.
arxiv-12000-28 | Spectral Sparsification and Regret Minimization Beyond Matrix Multiplicative Updates | http://arxiv.org/pdf/1506.04838v1.pdf | author:Zeyuan Allen-Zhu, Zhenyu Liao, Lorenzo Orecchia category:cs.LG cs.DS math.OC stat.ML published:2015-06-16 summary:In this paper, we provide a novel construction of the linear-sized spectralsparsifiers of Batson, Spielman and Srivastava [BSS14]. While previousconstructions required $\Omega(n^4)$ running time [BSS14, Zou12], oursparsification routine can be implemented in almost-quadratic running time$O(n^{2+\varepsilon})$. The fundamental conceptual novelty of our work is the leveraging of a strongconnection between sparsification and a regret minimization problem overdensity matrices. This connection was known to provide an interpretation of therandomized sparsifiers of Spielman and Srivastava [SS11] via the application ofmatrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, weexplain how matrix MWU naturally arises as an instance of theFollow-the-Regularized-Leader framework and generalize this approach to yield alarger class of updates. This new class allows us to accelerate theconstruction of linear-sized spectral sparsifiers, and give novel insights onthe motivation behind Batson, Spielman and Srivastava [BSS14].
arxiv-12000-29 | Numeric Input Relations for Relational Learning with Applications to Community Structure Analysis | http://arxiv.org/pdf/1506.05055v1.pdf | author:Jiuchuan Jiang, Manfred Jaeger category:cs.LG published:2015-06-16 summary:Most work in the area of statistical relational learning (SRL) is focussed ondiscrete data, even though a few approaches for hybrid SRL models have beenproposed that combine numerical and discrete variables. In this paper wedistinguish numerical random variables for which a probability distribution isdefined by the model from numerical input variables that are only used forconditioning the distribution of discrete response variables. We show hownumerical input relations can very easily be used in the Relational BayesianNetwork framework, and that existing inference and learning methods need onlyminor adjustments to be applied in this generalized setting. The resultingframework provides natural relational extensions of classical probabilisticmodels for categorical data. We demonstrate the usefulness of RBN models withnumeric input relations by several examples. In particular, we use the augmented RBN framework to define probabilisticmodels for multi-relational (social) networks in which the probability of alink between two nodes depends on numeric latent feature vectors associatedwith the nodes. A generic learning procedure can be used to obtain amaximum-likelihood fit of model parameters and latent feature values for avariety of models that can be expressed in the high-level RBN representation.Specifically, we propose a model that allows us to interpret learned latentfeature values as community centrality degrees by which we can identify nodesthat are central for one community, that are hubs between communities, or thatare isolated nodes. In a multi-relational setting, the model also provides acharacterization of how different relations are associated with each community.
arxiv-12000-30 | Exploiting Text and Network Context for Geolocation of Social Media Users | http://arxiv.org/pdf/1506.04803v1.pdf | author:Afshin Rahimi, Duy Vu, Trevor Cohn, Timothy Baldwin category:cs.CL cs.SI published:2015-06-16 summary:Research on automatically geolocating social media users has conventionallybeen based on the text content of posts from a given user or the social networkof the user, with very little crossover between the two, and no bench-markingof the two approaches over compara- ble datasets. We bring the two threads ofresearch together in first proposing a text-based method based on adaptivegrids, followed by a hybrid network- and text-based method. Evaluating overthree Twitter datasets, we show that the empirical difference between text- andnetwork-based methods is not great, and that hybridisation of the two issuperior to the component methods, especially in contexts where the user graphis not well connected. We achieve state-of-the-art results on all threedatasets.
arxiv-12000-31 | PCA with Gaussian perturbations | http://arxiv.org/pdf/1506.04855v2.pdf | author:Wojciech Kot≈Çowski, Manfred K. Warmuth category:cs.LG stat.ML published:2015-06-16 summary:Most of machine learning deals with vector parameters. Ideally we would liketo take higher order information into account and make use of matrix or eventensor parameters. However the resulting algorithms are usually inefficient.Here we address on-line learning with matrix parameters. It is often easy toobtain online algorithm with good generalization performance if youeigendecompose the current parameter matrix in each trial (at a cost of$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is acore trade-off between the running time and the generalization performance,here measured by the regret of the on-line algorithm (total gain of the bestoff-line predictor minus the total gain of the on-line algorithm). We focus onthe key matrix problem of rank $k$ Principal Component Analysis in$\mathbb{R}^n$ where $k \ll n$. There are $O(n^3)$ algorithms that achieve theoptimum regret but require eigendecompositions. We develop a simple algorithmthat needs $O(kn^2)$ per trial whose regret is off by a small factor of$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leaderparadigm. It replaces full eigendecompositions at each trial by the problemfinding $k$ principal components of the current covariance matrix that isperturbed by Gaussian noise.
arxiv-12000-32 | Histopathological Image Classification using Discriminative Feature-oriented Dictionary Learning | http://arxiv.org/pdf/1506.05032v5.pdf | author:Tiep Huu Vu, Hojjat Seyed Mousavi, Vishal Monga, Arvind UK Rao, Ganesh Rao category:cs.CV published:2015-06-16 summary:In histopathological image analysis, feature extraction for classification isa challenging task due to the diversity of histology features suitable for eachproblem as well as presence of rich geometrical structures. In this paper, wepropose an automatic feature discovery framework via learning class-specificdictionaries and present a low-complexity method for classification and diseasegrading in histopathology. Essentially, our Discriminative Feature-orientedDictionary Learning (DFDL) method learns class-specific dictionaries such thatunder a sparsity constraint, the learned dictionaries allow representing a newimage sample parsimoniously via the dictionary corresponding to the classidentity of the sample. At the same time, the dictionary is designed to bepoorly capable of representing samples from other classes. Experiments on threechallenging real-world image databases: 1) histopathological images ofintraductal breast lesions, 2) mammalian kidney, lung and spleen imagesprovided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University,and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, revealthe merits of our proposal over state-of-the-art alternatives. {Moreover, wedemonstrate that DFDL exhibits a more graceful decay in classification accuracyagainst the number of training images which is highly desirable in practicewhere generous training is often not available
arxiv-12000-33 | Deep Convolutional Networks on Graph-Structured Data | http://arxiv.org/pdf/1506.05163v1.pdf | author:Mikael Henaff, Joan Bruna, Yann LeCun category:cs.LG cs.CV cs.NE published:2015-06-16 summary:Deep Learning's recent successes have mostly relied on ConvolutionalNetworks, which exploit fundamental statistical properties of images, soundsand video data: the local stationarity and multi-scale compositional structure,that allows expressing long range interactions in terms of shorter, localizedinteractions. However, there exist other important examples, such as textdocuments or bioinformatic data, that may lack some or all of these strongstatistical regularities. In this paper we consider the general question of how to construct deeparchitectures with small learning complexity on general non-Euclidean domains,which are typically unknown and need to be estimated from the data. Inparticular, we develop an extension of Spectral Networks which incorporates aGraph Estimation procedure, that we test on large-scale classificationproblems, matching or improving over Dropout Networks with far less parametersto estimate.
arxiv-12000-34 | End-to-end people detection in crowded scenes | http://arxiv.org/pdf/1506.04878v3.pdf | author:Russell Stewart, Mykhaylo Andriluka category:cs.CV published:2015-06-16 summary:Current people detectors operate either by scanning an image in a slidingwindow fashion or by classifying a discrete set of proposals. We propose amodel that is based on decoding an image into a set of people detections. Oursystem takes an image as input and directly outputs a set of distinct detectionhypotheses. Because we generate predictions jointly, common post-processingsteps such as non-maximum suppression are unnecessary. We use a recurrent LSTMlayer for sequence generation and train our model end-to-end with a new lossfunction that operates on sets of detections. We demonstrate the effectivenessof our approach on the challenging task of detecting people in crowded scenes.
arxiv-12000-35 | Tree-structured composition in neural networks without tree-structured architectures | http://arxiv.org/pdf/1506.04834v3.pdf | author:Samuel R. Bowman, Christopher D. Manning, Christopher Potts category:cs.CL cs.LG published:2015-06-16 summary:Tree-structured neural networks encode a particular tree geometry for asentence in the network design. However, these models have at best onlyslightly outperformed simpler sequence-based models. We hypothesize that neuralsequence models like LSTMs are in fact able to discover and implicitly userecursive compositional structure, at least for tasks with clear cues to thatstructure in the data. We demonstrate this possibility using an artificial datatask for which recursive compositional structure is crucial, and find anLSTM-based sequence model can indeed learn to exploit the underlying treestructure. However, its performance consistently lags behind that of treemodels, even on large training sets, suggesting that tree-structured models aremore effective at exploiting recursive structure.
arxiv-12000-36 | Recognize Foreign Low-Frequency Words with Similar Pairs | http://arxiv.org/pdf/1506.04940v1.pdf | author:Xi Ma, Xiaoxi Wang, Dong Wang, Zhiyong Zhang category:cs.CL published:2015-06-16 summary:Low-frequency words place a major challenge for automatic speech recognition(ASR). The probabilities of these words, which are often important nameentities, are generally under-estimated by the language model (LM) due to theirlimited occurrences in the training data. Recently, we proposed a word-pairapproach to deal with the problem, which borrows information of frequent wordsto enhance the probabilities of low-frequency words. This paper presents anextension to the word-pair method by involving multiple `predicting words' toproduce better estimation for low-frequency words. We also employ this approachto deal with out-of-language words in the task of multi-lingual speechrecognition.
arxiv-12000-37 | Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation | http://arxiv.org/pdf/1506.04924v2.pdf | author:Seunghoon Hong, Hyeonwoo Noh, Bohyung Han category:cs.CV published:2015-06-16 summary:We propose a novel deep neural network architecture for semi-supervisedsemantic segmentation using heterogeneous annotations. Contrary to existingapproaches posing semantic segmentation as a single task of region-basedclassification, our algorithm decouples classification and segmentation, andlearns a separate network for each task. In this architecture, labelsassociated with an image are identified by classification network, and binarysegmentation is subsequently performed for each identified label insegmentation network. The decoupled architecture enables us to learnclassification and segmentation networks separately based on the training datawith image-level and pixel-wise class labels, respectively. It facilitates toreduce search space for segmentation effectively by exploiting class-specificactivation maps obtained from bridging layers. Our algorithm shows outstandingperformance compared to other semi-supervised approaches even with much lesstraining images with strong annotations in PASCAL VOC dataset.
arxiv-12000-38 | Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection | http://arxiv.org/pdf/1506.05001v1.pdf | author:Liliana Lo Presti, Marco La Cascia category:cs.CV cs.AI cs.RO published:2015-06-16 summary:This paper proposes a new approach to model the temporal dynamics of asequence of facial expressions. To this purpose, a sequence of Face ImageDescriptors (FID) is regarded as the output of a Linear Time Invariant (LTI)system. The temporal dynamics of such sequence of descriptors are representedby means of a Hankel matrix. The paper presents different strategies to computedynamics-based representation of a sequence of FID, and reports classificationaccuracy values of the proposed representations within different standardclassification frameworks. The representations have been validated in two verychallenging application domains: emotion recognition and pain detection.Experiments on two publicly available benchmarks and comparison withstate-of-the-art approaches demonstrate that the dynamics-based FIDrepresentation attains competitive performance when off-the-shelfclassification tools are adopted.
arxiv-12000-39 | Post-Reconstruction Deconvolution of PET Images by Total Generalized Variation Regularization | http://arxiv.org/pdf/1506.04935v1.pdf | author:St√©phanie Gu√©rit, Laurent Jacques, Beno√Æt Macq, John A. Lee category:cs.CV math.OC published:2015-06-16 summary:Improving the quality of positron emission tomography (PET) images, affectedby low resolution and high level of noise, is a challenging task in nuclearmedicine and radiotherapy. This work proposes a restoration method, achievedafter tomographic reconstruction of the images and targeting clinicalsituations where raw data are often not accessible. Based on inverse problemmethods, our contribution introduces the recently developed total generalizedvariation (TGV) norm to regularize PET image deconvolution. Moreover, westabilize this procedure with additional image constraints such as positivityand photometry invariance. A criterion for updating and adjusting automaticallythe regularization parameter in case of Poisson noise is also presented.Experiments are conducted on both synthetic data and real patient images.
arxiv-12000-40 | Bayesian representation learning with oracle constraints | http://arxiv.org/pdf/1506.05011v4.pdf | author:Theofanis Karaletsos, Serge Belongie, Gunnar R√§tsch category:stat.ML cs.CV cs.LG published:2015-06-16 summary:Representation learning systems typically rely on massive amounts of labeleddata in order to be trained to high accuracy. Recently, high-dimensionalparametric models like neural networks have succeeded in building richrepresentations using either compressive, reconstructive or supervisedcriteria. However, the semantic structure inherent in observations isoftentimes lost in the process. Human perception excels at understandingsemantics but cannot always be expressed in terms of labels. Thus,\emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing,are often employed to generate similarity constraints using an implicitsimilarity function encoded in human perception. In this work we propose tocombine \emph{generative unsupervised feature learning} with a\emph{probabilistic treatment of oracle information like triplets} in order totransfer implicit privileged oracle knowledge into explicit nonlinear Bayesianlatent factor models of the observations. We use a fast variational algorithmto learn the joint model and demonstrate applicability to a well-known imagedataset. We show how implicit triplet information can provide rich informationto learn representations that outperform previous metric learning approaches aswell as generative models without this side-information in a variety ofpredictive tasks. In addition, we illustrate that the proposed approachcompartmentalizes the latent spaces semantically which allows interpretation ofthe latent variables.
arxiv-12000-41 | Online Gradient Boosting | http://arxiv.org/pdf/1506.04820v2.pdf | author:Alina Beygelzimer, Elad Hazan, Satyen Kale, Haipeng Luo category:cs.LG published:2015-06-16 summary:We extend the theory of boosting for regression problems to the onlinelearning setting. Generalizing from the batch setting for boosting, the notionof a weak learning algorithm is modeled as an online learning algorithm withlinear loss functions that competes with a base class of regression functions,while a strong learning algorithm is an online learning algorithm with convexloss functions that competes with a larger class of regression functions. Ourmain result is an online gradient boosting algorithm which converts a weakonline learning algorithm into a strong one where the larger class of functionsis the linear span of the base class. We also give a simpler boosting algorithmthat converts a weak online learning algorithm into a strong one where thelarger class of functions is the convex hull of the base class, and prove itsoptimality.
arxiv-12000-42 | Time Series Classification using the Hidden-Unit Logistic Model | http://arxiv.org/pdf/1506.05085v2.pdf | author:Wenjie Pei, Hamdi Dibeklioƒülu, David M. J. Tax, Laurens van der Maaten category:cs.LG cs.CV published:2015-06-16 summary:We present a new model for time series classification, called the hidden-unitlogistic model, that uses binary stochastic hidden units to model latentstructure in the data. The hidden units are connected in a chain structure thatmodels temporal dependencies in the data. Compared to the prior models for timeseries classification such as the hidden conditional random field, our modelcan model very complex decision boundaries because the number of latent statesgrows exponentially with the number of hidden units. We demonstrate the strongperformance of our model in experiments on a variety of (computer vision)tasks, including handwritten character recognition, speech recognition, facialexpression, and action recognition. We also present a state-of-the-art systemfor facial action unit detection based on the hidden-unit logistic model.
arxiv-12000-43 | Layered Interpretation of Street View Images | http://arxiv.org/pdf/1506.04723v2.pdf | author:Ming-Yu Liu, Shuoxin Lin, Srikumar Ramalingam, Oncel Tuzel category:cs.CV published:2015-06-15 summary:We propose a layered street view model to encode both depth and semanticinformation on street view images for autonomous driving. Recently, stixels,stix-mantics, and tiered scene labeling methods have been proposed to modelstreet view images. We propose a 4-layer street view model, a compactrepresentation over the recently proposed stix-mantics model. Our layers encodesemantic classes like ground, pedestrians, vehicles, buildings, and sky inaddition to the depths. The only input to our algorithm is a pair of stereoimages. We use a deep neural network to extract the appearance features forsemantic classes. We use a simple and an efficient inference algorithm tojointly estimate both semantic classes and layered depth values. Our methodoutperforms other competing approaches in Daimler urban scene segmentationdataset. Our algorithm is massively parallelizable, allowing a GPUimplementation with a processing speed about 9 fps.
arxiv-12000-44 | Convex Risk Minimization and Conditional Probability Estimation | http://arxiv.org/pdf/1506.04513v1.pdf | author:Matus Telgarsky, Miroslav Dud√≠k, Robert Schapire category:cs.LG stat.ML published:2015-06-15 summary:This paper proves, in very general settings, that convex risk minimization isa procedure to select a unique conditional probability model determined by theclassification problem. Unlike most previous work, we give results that aregeneral enough to include cases in which no minimum exists, as occurstypically, for instance, with standard boosting algorithms. Concretely, wefirst show that any sequence of predictors minimizing convex risk over thesource distribution will converge to this unique model when the class ofpredictors is linear (but potentially of infinite dimension). Secondly, we showthe same result holds for \emph{empirical} risk minimization whenever thisclass of predictors is finite dimensional, where the essential technicalcontribution is a norm-free generalization bound.
arxiv-12000-45 | Big Data Analytics in Bioinformatics: A Machine Learning Perspective | http://arxiv.org/pdf/1506.05101v1.pdf | author:Hirak Kashyap, Hasin Afzal Ahmed, Nazrul Hoque, Swarup Roy, Dhruba Kumar Bhattacharyya category:cs.CE cs.LG published:2015-06-15 summary:Bioinformatics research is characterized by voluminous and incrementaldatasets and complex data analytics methods. The machine learning methods usedin bioinformatics are iterative and parallel. These methods can be scaled tohandle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are notoptimized for iterative processing and high data dependency among operations.In the recent years, parallel, incremental, and multi-view machine learningalgorithms have been proposed. Similarly, graph-based architectures andin-memory big data tools have been developed to minimize I/O cost and optimizeiterative processing. However, there lack standard big data architectures and tools for manyimportant bioinformatics problems, such as fast construction of co-expressionand regulatory networks and salient module identification, detection ofcomplexes over growing protein-protein interaction data, fast analysis ofmassive DNA, RNA, and protein sequence data, and fast querying on incrementaland heterogeneous disease networks. This paper addresses the issues andchallenges posed by several big data problems in bioinformatics, and gives anoverview of the state of the art and the future research opportunities.
arxiv-12000-46 | Distilling Word Embeddings: An Encoding Approach | http://arxiv.org/pdf/1506.04488v1.pdf | author:Lili Mou, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG published:2015-06-15 summary:Distilling knowledge from a well-trained cumbersome network to a small onehas become a new research topic recently, as lightweight neural networks withhigh performance are particularly in need in various resource-restrictedsystems. This paper addresses the problem of distilling embeddings for NLPtasks. We propose an encoding approach to distill task-specific knowledge fromhigh-dimensional embeddings, which can retain high performance and reduce modelcomplexity to a large extent. Experimental results show our method is betterthan directly training neural networks with small embeddings.
arxiv-12000-47 | Learning Deep Generative Models with Doubly Stochastic MCMC | http://arxiv.org/pdf/1506.04557v4.pdf | author:Chao Du, Jun Zhu, Bo Zhang category:cs.LG published:2015-06-15 summary:We present doubly stochastic gradient MCMC, a simple and generic method for(approximate) Bayesian inference of deep generative models (DGMs) in acollapsed continuous parameter space. At each MCMC sampling step, the algorithmrandomly draws a mini-batch of data samples to estimate the gradient oflog-posterior and further estimates the intractable expectation over hiddenvariables via a neural adaptive importance sampler, where the proposaldistribution is parameterized by a deep neural network and learnt jointly. Wedemonstrate the effectiveness on learning various DGMs in a wide range oftasks, including density estimation, data generation and missing dataimputation. Our method outperforms many state-of-the-art competitors.
arxiv-12000-48 | ParseNet: Looking Wider to See Better | http://arxiv.org/pdf/1506.04579v2.pdf | author:Wei Liu, Andrew Rabinovich, Alexander C. Berg category:cs.CV published:2015-06-15 summary:We present a technique for adding global context to deep convolutionalnetworks for semantic segmentation. The approach is simple, using the averagefeature for a layer to augment the features at each location. In addition, westudy several idiosyncrasies of training, significantly increasing theperformance of baseline networks (e.g. from FCN). When we add our proposedglobal feature, and a technique for learning normalization parameters, accuracyincreases consistently even over our improved versions of the baselines. Ourproposed approach, ParseNet, achieves state-of-the-art performance on SiftFlowand PASCAL-Context with small additional computational cost over baselines, andnear current state-of-the-art performance on PASCAL VOC 2012 semanticsegmentation with a simple approach. Code is available athttps://github.com/weiliu89/caffe/tree/fcn .
arxiv-12000-49 | Optimising Spatial and Tonal Data for PDE-based Inpainting | http://arxiv.org/pdf/1506.04566v1.pdf | author:Laurent Hoeltgen, Markus Mainberger, Sebastian Hoffmann, Joachim Weickert, Ching Hoo Tang, Simon Setzer, Daniel Johannsen, Frank Neumann, Benjamin Doerr category:cs.CV math.OC published:2015-06-15 summary:Some recent methods for lossy signal and image compression store only a fewselected pixels and fill in the missing structures by inpainting with a partialdifferential equation (PDE). Suitable operators include the Laplacian, thebiharmonic operator, and edge-enhancing anisotropic diffusion (EED). Thequality of such approaches depends substantially on the selection of the datathat is kept. Optimising this data in the domain and codomain gives rise tochallenging mathematical problems that shall be addressed in our work. In the 1D case, we prove results that provide insights into the difficulty ofthis problem, and we give evidence that a splitting into spatial and tonal(i.e. function value) optimisation does hardly deteriorate the results. In the2D setting, we present generic algorithms that achieve a high reconstructionquality even if the specified data is very sparse. To optimise the spatialdata, we use a probabilistic sparsification, followed by a nonlocal pixelexchange that avoids getting trapped in bad local optima. After this spatialoptimisation we perform a tonal optimisation that modifies the function valuesin order to reduce the global reconstruction error. For homogeneous diffusioninpainting, this comes down to a least squares problem for which we prove thatit has a unique solution. We demonstrate that it can be found efficiently witha gradient descent approach that is accelerated with fast explicit diffusion(FED) cycles. Our framework allows to specify the desired density of theinpainting mask a priori. Moreover, is more generic than other dataoptimisation approaches for the sparse inpainting problem, since it can also beextended to nonlinear inpainting operators such as EED. This is exploited toachieve reconstructions with state-of-the-art quality. We also give an extensive literature survey on PDE-based image compressionmethods.
arxiv-12000-50 | Re-scale AdaBoost for Attack Detection in Collaborative Filtering Recommender Systems | http://arxiv.org/pdf/1506.04584v1.pdf | author:Zhihai Yang, Lin Xu, Zhongmin Cai category:cs.IR cs.CR cs.LG published:2015-06-15 summary:Collaborative filtering recommender systems (CFRSs) are the key components ofsuccessful e-commerce systems. Actually, CFRSs are highly vulnerable to attackssince its openness. However, since attack size is far smaller than that ofgenuine users, conventional supervised learning based detection methods couldbe too "dull" to handle such imbalanced classification. In this paper, weimprove detection performance from following two aspects. First, we extractwell-designed features from user profiles based on the statistical propertiesof the diverse attack models, making hard classification task becomes easier toperform. Then, refer to the general idea of re-scale Boosting (RBoosting) andAdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost(RAdaBoost) as our detection method based on extracted features. RAdaBoost iscomparable to the optimal Boosting-type algorithm and can effectively improvethe performance in some hard scenarios. Finally, a series of experiments on theMovieLens-100K data set are conducted to demonstrate the outperformance ofRAdaBoost comparing with some classical techniques such as SVM, kNN andAdaBoost.
arxiv-12000-51 | Thin Structure Estimation with Curvature Regularization | http://arxiv.org/pdf/1506.04654v2.pdf | author:Dmitrii Marin, Yuri Boykov, Yuchen Zhong category:cs.CV published:2015-06-15 summary:Many applications in vision require estimation of thin structures such asboundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike mostprevious approaches, we simultaneously detect and delineate thin structureswith sub-pixel localization and real-valued orientation estimation. This is anill-posed problem that requires regularization. We propose an objectivefunction combining detection likelihoods with a prior minimizing curvature ofthe center-lines or surfaces. Unlike simple block-coordinate descent, wedevelop a novel algorithm that is able to perform joint optimization oflocation and detection variables more effectively. Our lower bound optimizationalgorithm applies to quadratic or absolute curvature. The proposed early visionframework is sufficiently general and it can be used in many higher-levelapplications. We illustrate the advantage of our approach on a range of 2D and3D examples.
arxiv-12000-52 | A New PAC-Bayesian Perspective on Domain Adaptation | http://arxiv.org/pdf/1506.04573v3.pdf | author:Pascal Germain, Amaury Habrard, Fran√ßois Laviolette, Emilie Morvant category:stat.ML cs.LG published:2015-06-15 summary:We study the issue of PAC-Bayesian domain adaptation: We want to learn, froma source domain, a majority vote model dedicated to a target one. Ourtheoretical contribution brings a new perspective by deriving an upper-bound onthe target risk where the distributions' divergence---expressed as aratio---controls the trade-off between a source error measure and the targetvoters' disagreement. Our bound suggests that one has to focus on regions wherethe source data is informative.From this result, we derive a PAC-Bayesiangeneralization bound, and specialize it to linear classifiers. Then, we infer alearning algorithmand perform experiments on real data.
arxiv-12000-53 | Flow Segmentation in Dense Crowds | http://arxiv.org/pdf/1506.04608v1.pdf | author:Javairia Nazir, Mehreen Sirshar category:cs.CV published:2015-06-15 summary:A framework is proposed in this paper that is used to segment flow of densecrowds. The flow field that is generated by the movement in the crowd istreated just like an aperiodic dynamic system. On this flow field a grid ofparticles is put over for particle advection by the use of a numericalintegration scheme. Then flow maps are generated which associates the initialposition of the particles with final position. The gradient of the flow mapsgives the amount of divergence of the neighboring particles. For forwardintegration and analysis forward Finite time Lyapunov Exponent is calculatedand backward Finite time Lyapunov Exponent is also calculated it gives theLagrangian coherent structures of the flow in crowd. Lagrangian CoherentStructures basically divides the flow in crowd into regions and these regionshave different dynamics. These regions are then used to get the boundary in thedifferent flow segments by using water shed algorithm. The experiment isconducted on the crowd dataset of UCF (University of central Florida).
arxiv-12000-54 | Leveraging the Power of Gabor Phase for Face Identification: A Block Matching Approach | http://arxiv.org/pdf/1506.04655v1.pdf | author:Yang Zhong, Haibo Li category:cs.CV published:2015-06-15 summary:Different from face verification, face identification is much more demanding.To reach comparable performance, an identifier needs to be roughly N timesbetter than a verifier. To expect a breakthrough in face identification, weneed a fresh look at the fundamental building blocks of face recognition. Inthis paper we focus on the selection of a suitable signal representation andbetter matching strategy for face identification. We demonstrate how Gaborphase could be leveraged to improve the performance of face identification byusing the Block Matching method. Compared to the existing approaches, theproposed method features much lower algorithmic complexity: face images areonly filtered by a single-scale Gabor filter pair and the matching is performedbetween any pairs of face images at hand without involving any trainingprocess. Benchmark evaluations show that the proposed approach is totallycomparable to and even better than state-of-the-art algorithms, which aretypically based on more features extracted from a large set of Gabor facesand/or rely on heavy training processes.
arxiv-12000-55 | Latent Regression Bayesian Network for Data Representation | http://arxiv.org/pdf/1506.04720v1.pdf | author:Siqi Nie, Qiang Ji category:cs.LG published:2015-06-15 summary:Deep directed generative models have attracted much attention recently due totheir expressive representation power and the ability of ancestral sampling.One major difficulty of learning directed models with many latent variables isthe intractable inference. To address this problem, most existing algorithmsmake assumptions to render the latent variables independent of each other,either by designing specific priors, or by approximating the true posteriorusing a factorized distribution. We believe the correlations among latentvariables are crucial for faithful data representation. Driven by this idea, wepropose an inference method based on the conditional pseudo-likelihood thatpreserves the dependencies among the latent variables. For learning, we proposeto employ the hard Expectation Maximization (EM) algorithm, which avoids theintractability of the traditional EM by max-out instead of sum-out to computethe data likelihood. Qualitative and quantitative evaluations of our modelagainst state of the art deep models on benchmark datasets demonstrate theeffectiveness of the proposed algorithm in data representation andreconstruction.
arxiv-12000-56 | Automatic Layer Separation using Light Field Imaging | http://arxiv.org/pdf/1506.04721v1.pdf | author:Qiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, Jingyi Yu category:cs.CV published:2015-06-15 summary:We propose a novel approach that jointly removes reflection or translucentlayer from a scene and estimates scene depth. The input data are captured vialight field imaging. The problem is couched as minimizing the rank of thetransmitted scene layer via Robust Principle Component Analysis (RPCA). We alsoimpose regularization based on piecewise smoothness, gradient sparsity, andlayer independence to simultaneously recover 3D geometry of the transmittedlayer. Experimental results on synthetic and real data show that our techniqueis robust and reliable, and can handle a broad range of layer separationproblems.
arxiv-12000-57 | Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy | http://arxiv.org/pdf/1506.04477v1.pdf | author:Sang-Woo Lee, Min-Oh Heo, Jiwon Kim, Jeonghee Kim, Byoung-Tak Zhang category:cs.LG published:2015-06-15 summary:The online learning of deep neural networks is an interesting problem ofmachine learning because, for example, major IT companies want to manage theinformation of the massive data uploaded on the web daily, and this technologycan contribute to the next generation of lifelong learning. We aim to traindeep models from new data that consists of new classes, distributions, andtasks at minimal computational cost, which we call online deep learning.Unfortunately, deep neural network learning through classical online andincremental methods does not work well in both theory and practice. In thispaper, we introduce dual memory architectures for online incremental deeplearning. The proposed architecture consists of deep representation learnersand fast learnable shallow kernel networks, both of which synergize to trackthe information of new data. During the training phase, we use various online,incremental ensemble, and transfer learning techniques in order to achievelower error of the architecture. On the MNIST, CIFAR-10, and ImageNet imagerecognition tasks, the proposed dual memory architectures performs much betterthan the classical online and incremental ensemble algorithm, and theiraccuracies are similar to that of the batch learner.
arxiv-12000-58 | Encog: Library of Interchangeable Machine Learning Models for Java and C# | http://arxiv.org/pdf/1506.04776v1.pdf | author:Jeff Heaton category:cs.MS cs.LG 68T01 I.2 published:2015-06-15 summary:This paper introduces the Encog library for Java and C#, a scalable,adaptable, multiplatform machine learning framework that was 1st released in2008. Encog allows a variety of machine learning models to be applied todatasets using regression, classification, and clustering. Various supportedmachine learning models can be used interchangeably with minimal recoding.Encog uses efficient multithreaded code to reduce training time by exploitingmodern multicore processors. The current version of Encog can be downloadedfrom http://www.encog.org.
arxiv-12000-59 | Circle-based Eye Center Localization (CECL) | http://arxiv.org/pdf/1506.04500v2.pdf | author:Yustinus Eko Soelistio, Eric Postma, Alfons Maes category:cs.CV published:2015-06-15 summary:We propose an improved eye center localization method based on the Houghtransform, called Circle-based Eye Center Localization (CECL) that is simple,robust, and achieves accuracy on a par with typically more complexstate-of-the-art methods. The CECL method relies on color and shape cues thatdistinguish the iris from other facial structures. The accuracy of the CECLmethod is demonstrated through a comparison with 15 state-of-the-art eye centerlocalization methods against five error thresholds, as reported in theliterature. The CECL method achieved an accuracy of 80.8% to 99.4% and rankedfirst for 2 of the 5 thresholds. It is concluded that the CECL method offers anattractive alternative to existing methods for automatic eye centerlocalization.
arxiv-12000-60 | Fast Two-Sample Testing with Analytic Representations of Probability Measures | http://arxiv.org/pdf/1506.04725v1.pdf | author:Kacper Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, Arthur Gretton category:stat.ML 62G10 G.3 published:2015-06-15 summary:We propose a class of nonparametric two-sample tests with a cost linear inthe sample size. Two tests are given, both based on an ensemble of distancesbetween analytic functions representing each of the distributions. The firsttest uses smoothed empirical characteristic functions to represent thedistributions, the second uses distribution embeddings in a reproducing kernelHilbert space. Analyticity implies that differences in the distributions may bedetected almost surely at a finite number of randomly chosenlocations/frequencies. The new tests are consistent against a larger class ofalternatives than the previous linear-time tests based on the (non-smoothed)empirical characteristic functions, while being much faster than the currentstate-of-the-art quadratic-time kernel-based or energy distance-based tests.Experiments on artificial benchmarks and on challenging real-world testingproblems demonstrate that our tests give a better power/time tradeoff thancompeting approaches, and in some cases, better outright power than even themost expensive quadratic-time tests. This performance advantage is retainedeven in high dimensions, and in cases where the difference in distributions isnot observable with low order statistics.
arxiv-12000-61 | Slow and steady feature analysis: higher order temporal coherence in video | http://arxiv.org/pdf/1506.04714v2.pdf | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV published:2015-06-15 summary:How can unlabeled video augment visual learning? Existing methods perform"slow" feature analysis, encouraging the representations of temporally closeframes to exhibit only small differences. While this standard approach capturesthe fact that high-level visual signals change slowly over time, it fails tocapture *how* the visual content changes. We propose to generalize slow featureanalysis to "steady" feature analysis. The key idea is to impose a prior thathigher order derivatives in the learned feature space must be small. To thisend, we train a convolutional neural network with a regularizer on tuples ofsequential frames from unlabeled video. It encourages feature changes over timeto be smooth, i.e., similar to the most recent changes. Using five diversedatasets, including unlabeled YouTube and KITTI videos, we demonstrate ourmethod's impact on object, scene, and action recognition tasks. We further showthat our features learned from unlabeled video can even surpass a standardheavily supervised pretraining approach.
arxiv-12000-62 | Multi-path Convolutional Neural Networks for Complex Image Classification | http://arxiv.org/pdf/1506.04701v3.pdf | author:Mingming Wang category:cs.CV published:2015-06-15 summary:Convolutional Neural Networks demonstrate high performance on ImageNetLarge-Scale Visual Recognition Challenges contest. Nevertheless, the publishedresults only show the overall performance for all image classes. There is nofurther analysis why certain images get worse results and how they could beimproved. In this paper, we provide deep performance analysis based ondifferent types of images and point out the weaknesses of convolutional neuralnetworks through experiment. We design a novel multiple paths convolutionalneural network, which feeds different versions of images into separated pathsto learn more comprehensive features. This model has better presentation forimage than the traditional single path model. We acquire better classificationresults on complex validation set on both top 1 and top 5 scores than the bestILSVRC 2013 classification model.
arxiv-12000-63 | A Complete Recipe for Stochastic Gradient MCMC | http://arxiv.org/pdf/1506.04696v2.pdf | author:Yi-An Ma, Tianqi Chen, Emily B. Fox category:math.ST stat.ME stat.ML stat.TH published:2015-06-15 summary:Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuousdynamics to define a transition kernel that efficiently explores a targetdistribution. In tandem, a focus has been on devising scalable variants thatsubsample the data and use stochastic gradients in place of full-data gradientsin the dynamic simulations. However, such stochastic gradient MCMC samplershave lagged behind their full-data counterparts in terms of the complexity ofdynamics considered since proving convergence in the presence of the stochasticgradient noise is non-trivial. Even with simple dynamics, significant physicalintuition is often required to modify the dynamical system to account for thestochastic gradient noise. In this paper, we provide a general recipe forconstructing MCMC samplers--including stochastic gradient versions--based oncontinuous Markov processes specified via two matrices. We constructively provethat the framework is complete. That is, any continuous Markov process thatprovides samples from the target distribution can be written in our framework.We show how previous continuous-dynamic samplers can be trivially "reinvented"in our framework, avoiding the complicated sampler-specific proofs. We likewiseuse our recipe to straightforwardly propose a new state-adaptive sampler:stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experimentson simulated data and a streaming Wikipedia analysis demonstrate that theproposed SGRHMC sampler inherits the benefits of Riemann HMC, with thescalability of stochastic gradient methods.
arxiv-12000-64 | Reservoir Characterization: A Machine Learning Approach | http://arxiv.org/pdf/1506.05070v2.pdf | author:Soumi Chaki category:cs.CE cs.LG published:2015-06-15 summary:Reservoir Characterization (RC) can be defined as the act of building areservoir model that incorporates all the characteristics of the reservoir thatare pertinent to its ability to store hydrocarbons and also to produce them.Itis a difficult problem due to non-linear and heterogeneous subsurfaceproperties and associated with a number of complex tasks such as data fusion,data mining, formulation of the knowledge base, and handling of theuncertainty.This present work describes the development of algorithms to obtainthe functional relationships between predictor seismic attributes and targetlithological properties. Seismic attributes are available over a study areawith lower vertical resolution. Conversely, well logs and lithologicalproperties are available only at specific well locations in a study area withhigh vertical resolution.Sand fraction, which represents per unit sand volumewithin the rock, has a balanced distribution between zero to unity.The thesisaddresses the issues of handling the information content mismatch betweenpredictor and target variables and proposes regularization of target propertyprior to building a prediction model.In this thesis, two Artificial NeuralNetwork (ANN) based frameworks are proposed to model sand fraction frommultiple seismic attributes without and with well tops informationrespectively. The performances of the frameworks are quantified in terms ofCorrelation Coefficient, Root Mean Square Error, Absolute Error Mean, etc.
arxiv-12000-65 | Image-based Recommendations on Styles and Substitutes | http://arxiv.org/pdf/1506.04757v1.pdf | author:Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel category:cs.CV cs.IR published:2015-06-15 summary:Humans inevitably develop a sense of the relationships between objects, someof which are based on their appearance. Some pairs of objects might be seen asbeing alternatives to each other (such as two pairs of jeans), while others maybe seen as being complementary (such as a pair of jeans and a matching shirt).This information guides many of the choices that people make, from buyingclothes to their interactions with each other. We seek here to model this humansense of the relationships between objects based on their appearance. Ourapproach is not based on fine-grained modeling of user annotations but ratheron capturing the largest dataset possible and developing a scalable method foruncovering human notions of the visual relationships within. We cast this as anetwork inference problem defined on graphs of related images, and provide alarge-scale dataset for the training and evaluation of the same. The system wedevelop is capable of recommending which clothes and accessories will go welltogether (and which will not), amongst a host of other applications.
arxiv-12000-66 | Cheap Bandits | http://arxiv.org/pdf/1506.04782v2.pdf | author:Manjesh Kumar Hanawal, Venkatesh Saligrama, Michal Valko, R\' emi Munos category:cs.LG published:2015-06-15 summary:We consider stochastic sequential learning problems where the learner canobserve the \textit{average reward of several actions}. Such a setting isinteresting in many applications involving monitoring and surveillance, wherethe set of the actions to observe represent some (geographical) area. Theimportance of this setting is that in these applications, it is actually\textit{cheaper} to observe average reward of a group of actions rather thanthe reward of a single action. We show that when the reward is \textit{smooth}over a given graph representing the neighboring actions, we can maximize thecumulative reward of learning while \textit{minimizing the sensing cost}. Inthis paper we propose CheapUCB, an algorithm that matches the regret guaranteesof the known algorithms for this setting and at the same time guarantees alinear cost again over them. As a by-product of our analysis, we establish a$\Omega(\sqrt{dT})$ lower bound on the cumulative regret of spectral banditsfor a class of graphs with effective dimension $d$.
arxiv-12000-67 | A Survey of Multithreading Image Analysis | http://arxiv.org/pdf/1506.04472v5.pdf | author:Elham Sagheb Hossein Pour category:cs.CV published:2015-06-15 summary:Digital image analysis has made a big advance in many areas of enterpriseapplications including medicine, industry, and entertainment by assisting theextraction of semantic information from digital images. However, its largecomputational complexity has been a trouble to most real-time developments.While image analysis in general has been studied for a log period in computerscience community, the use of multithreading strategy as the most efficientimproving computational capacity technique has been limited so far. In thissurvey an attempt is made to explain the current knowledge and so farprogresses in incorporating image analysis with multithreading approaches. Thepresent work also provides insights and tendencies for the possible futureenhancement of multithreading image analysis.
arxiv-12000-68 | Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game | http://arxiv.org/pdf/1506.04744v1.pdf | author:Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML published:2015-06-15 summary:Interpersonal relations are fickle, with close friendships often dissolvinginto enmity. In this work, we explore linguistic cues that presage suchtransitions by studying dyadic interactions in an online strategy game whereplayers form alliances and break those alliances through betrayal. Wecharacterize friendships that are unlikely to last and examine temporalpatterns that foretell betrayal. We reveal that subtle signs of imminent betrayal are encoded in theconversational patterns of the dyad, even if the victim is not aware of therelationship's fate. In particular, we find that lasting friendships exhibit aform of balance that manifests itself through language. In contrast, suddenchanges in the balance of certain conversational attributes---such as positivesentiment, politeness, or focus on future planning---signal impending betrayal.
arxiv-12000-69 | Online Matrix Factorization via Broyden Updates | http://arxiv.org/pdf/1506.04389v2.pdf | author:√ñmer Deniz Akyƒ±ldƒ±z category:stat.ML published:2015-06-14 summary:In this paper, we propose an online algorithm to compute matrixfactorizations. Proposed algorithm updates the dictionary matrix and associatedcoefficients using a single observation at each time. The algorithm performslow-rank updates to dictionary matrix. We derive the algorithm by defining asimple objective function to minimize whenever an observation is arrived. Weextend the algorithm further for handling missing data. We also provide amini-batch extension which enables to compute the matrix factorization on bigdatasets. We demonstrate the efficiency of our algorithm on a real dataset andgive comparisons with well-known algorithms such as stochastic gradient matrixfactorization and nonnegative matrix factorization (NMF).
arxiv-12000-70 | Fast and Guaranteed Tensor Decomposition via Sketching | http://arxiv.org/pdf/1506.04448v2.pdf | author:Yining Wang, Hsiao-Yu Tung, Alexander Smola, Animashree Anandkumar category:stat.ML cs.LG published:2015-06-14 summary:Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications instatistical learning of latent variable models and in data mining. In thispaper, we propose fast and randomized tensor CP decomposition algorithms basedon sketching. We build on the idea of count sketches, but introduce many novelideas which are unique to tensors. We develop novel methods for randomizedcomputation of tensor contractions via FFTs, without explicitly forming thetensors. Such tensor contractions are encountered in decomposition methods suchas tensor power iterations and alternating least squares. We also design novelcolliding hashes for symmetric tensors to further save time in computing thesketches. We then combine these sketching ideas with existing whitening andtensor power iterative techniques to obtain the fastest algorithm on bothsparse and dense tensors. The quality of approximation under our method doesnot depend on properties such as sparsity, uniformity of elements, etc. Weapply the method for topic modeling and obtain competitive results.
arxiv-12000-71 | Bayesian Dark Knowledge | http://arxiv.org/pdf/1506.04416v3.pdf | author:Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling category:cs.LG stat.ML published:2015-06-14 summary:We consider the problem of Bayesian parameter estimation for deep neuralnetworks, which is important in problem settings where we may have little data,and/ or where we need accurate posterior predictive densities, e.g., forapplications involving bandits or active learning. One simple approach to thisis to use online Monte Carlo methods, such as SGLD (stochastic gradientLangevin dynamics). Unfortunately, such a method needs to store many copies ofthe parameters (which wastes memory), and needs to make predictions using manyversions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to theposterior predictive density into a more compact form, namely a single deepneural network. We compare to two very recent approaches to Bayesian neuralnetworks, namely an approach based on expectation propagation [Hernandez-Lobatoand Adams, 2015] and an approach based on variational Bayes [Blundell et al.,2015]. Our method performs better than both of these, is much simpler toimplement, and uses less computation at test time.
arxiv-12000-72 | A Fast Incremental Gaussian Mixture Model | http://arxiv.org/pdf/1506.04422v2.pdf | author:Rafael Pinto, Paulo Engel category:cs.LG I.2.6 published:2015-06-14 summary:This work builds upon previous efforts in online incremental learning, namelythe Incremental Gaussian Mixture Network (IGMN). The IGMN is capable oflearning from data streams in a single-pass by improving its model afteranalyzing each data point and discarding it thereafter. Nevertheless, itsuffers from the scalability point-of-view, due to its asymptotic timecomplexity of $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data points, $K$Gaussian components and $D$ dimensions, rendering it inadequate forhigh-dimensional data. In this paper, we manage to reduce this complexity to$\operatorname{O}\bigl(NKD^2\bigr)$ by deriving formulas for working directlywith precision matrices instead of covariance matrices. The final result is amuch faster and scalable algorithm which can be applied to high dimensionaltasks. This is confirmed by applying the modified algorithm to high-dimensionalclassification datasets.
arxiv-12000-73 | Compressing Convolutional Neural Networks | http://arxiv.org/pdf/1506.04449v1.pdf | author:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen category:cs.LG cs.CV cs.NE published:2015-06-14 summary:Convolutional neural networks (CNN) are increasingly used in many areas ofcomputer vision. They are particularly attractive because of their ability to"absorb" great quantities of labeled data through millions of parameters.However, as model sizes increase, so do the storage and memory requirements ofthe classifiers. We present a novel network architecture, Frequency-SensitiveHashed Nets (FreshNets), which exploits inherent redundancy in bothconvolutional layers and fully-connected layers of a deep learning model,leading to dramatic savings in memory and storage consumption. Based on the keyobservation that the weights of learned convolutional filters are typicallysmooth and low-frequency, we first convert filter weights to the frequencydomain with a discrete cosine transform (DCT) and use a low-cost hash functionto randomly group frequency parameters into hash buckets. All parametersassigned the same hash bucket share a single value learned with standardback-propagation. To further reduce model size we allocate fewer hash bucketsto high-frequency components, which are generally less important. We evaluateFreshNets on eight data sets, and show that it leads to drastically bettercompressed performance than several relevant baselines.
arxiv-12000-74 | Linguistics and some aspects of its underlying dynamics | http://arxiv.org/pdf/1506.08663v1.pdf | author:Massimo Piattelli-Palmarini, Giuseppe Vitiello category:cs.CL quant-ph published:2015-06-14 summary:In recent years, central components of a new approach to linguistics, theMinimalist Program (MP) have come closer to physics. Features of the MinimalistProgram, such as the unconstrained nature of recursive Merge, the operation ofthe Labeling Algorithm that only operates at the interface of Narrow Syntaxwith the Conceptual-Intentional and the Sensory-Motor interfaces, thedifference between pronounced and un-pronounced copies of elements in asentence and the build-up of the Fibonacci sequence in the syntactic derivationof sentence structures, are directly accessible to representation in terms ofalgebraic formalism. Although in our scheme linguistic structures are classicalones, we find that an interesting and productive isomorphism can be establishedbetween the MP structure, algebraic structures and many-body field theoryopening new avenues of inquiry on the dynamics underlying some central aspectsof linguistics.
arxiv-12000-75 | Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis | http://arxiv.org/pdf/1506.04338v1.pdf | author:Wei Yang, Haiting Lin, Sing Bing Kang, Jingyi Yu category:cs.CV published:2015-06-14 summary:In perspective cameras, images of a frontal-parallel 3D object preserve itsaspect ratio invariant to its depth. Such an invariance is useful inphotography but is unique to perspective projection. In this paper, we showthat alternative non-perspective cameras such as the crossed-slit or XSlitcameras exhibit a different depth-dependent aspect ratio (DDAR) property thatcan be used to 3D recovery. We first conduct a comprehensive analysis tocharacterize DDAR, infer object depth from its AR, and model recoverable depthrange, sensitivity, and error. We show that repeated shape patterns in realManhattan World scenes can be used for 3D reconstruction using a single XSlitimage. We also extend our analysis to model slopes of lines. Specifically,parallel 3D lines exhibit depth-dependent slopes (DDS) on their images whichcan also be used to infer their depths. We validate our analyses using realXSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show thatDDAR and DDS provide important depth cues and enable effective single-imagescene reconstruction.
arxiv-12000-76 | Deep Secure Encoding: An Application to Face Recognition | http://arxiv.org/pdf/1506.04340v1.pdf | author:Rohit Pandey, Yingbo Zhou, Venu Govindaraju category:cs.CV published:2015-06-14 summary:In this paper we present Deep Secure Encoding: a framework for secureclassification using deep neural networks, and apply it to the task ofbiometric template protection for faces. Using deep convolutional neuralnetworks (CNNs), we learn a robust mapping of face classes to high entropysecure codes. These secure codes are then hashed using standard hash functionslike SHA-256 to generate secure face templates. The efficacy of the approach isshown on two face databases, namely, CMU-PIE and Extended Yale B, where weachieve state of the art matching performance, along with cancelability andhigh security with no unrealistic assumptions. Furthermore, the scheme can workin both identification and verification modes.
arxiv-12000-77 | Reading Scene Text in Deep Convolutional Sequences | http://arxiv.org/pdf/1506.04395v2.pdf | author:Pan He, Weilin Huang, Yu Qiao, Chen Change Loy, Xiaoou Tang category:cs.CV published:2015-06-14 summary:We develop a Deep-Text Recurrent Network (DTRN) that regards scene textreading as a sequence labelling problem. We leverage recent advances of deepconvolutional neural networks to generate an ordered high-level sequence from awhole word image, avoiding the difficult character segmentation problem. Then adeep recurrent model, building on long short-term memory (LSTM), is developedto robustly recognize the generated CNN sequences, departing from most existingapproaches recognising each character independently. Our model has a number ofappealing properties in comparison to existing scene text recognition methods:(i) It can recognise highly ambiguous words by leveraging meaningful contextinformation, allowing it to work reliably without either pre- orpost-processing; (ii) the deep CNN feature is robust to various imagedistortions; (iii) it retains the explicit order information in word image,which is essential to discriminate word strings; (iv) the model does not dependon pre-defined dictionary, and it can process unknown words and arbitrarystrings. Codes for the DTRN will be available.
arxiv-12000-78 | Leveraging Word Embeddings for Spoken Document Summarization | http://arxiv.org/pdf/1506.04365v1.pdf | author:Kuan-Yu Chen, Shih-Hung Liu, Hsin-Min Wang, Berlin Chen, Hsin-Hsi Chen category:cs.CL cs.AI published:2015-06-14 summary:Owing to the rapidly growing multimedia content available on the Internet,extractive spoken document summarization, with the purpose of automaticallyselecting a set of representative sentences from a spoken document to conciselyexpress the most important theme of the document, has been an active area ofresearch and experimentation. On the other hand, word embedding has emerged asa newly favorite research subject because of its excellent performance in manynatural language processing (NLP)-related tasks. However, as far as we areaware, there are relatively few studies investigating its use in extractivetext or speech summarization. A common thread of leveraging word embeddings inthe summarization process is to represent the document (or sentence) byaveraging the word embeddings of the words occurring in the document (orsentence). Then, intuitively, the cosine similarity measure can be employed todetermine the relevance degree between a pair of representations. Beyond thecontinued efforts made to improve the representation of words, this paperfocuses on building novel and efficient ranking models based on the generalword embedding methods for extractive speech summarization. Experimentalresults demonstrate the effectiveness of our proposed methods, compared toexisting state-of-the-art methods.
arxiv-12000-79 | The Artists who Forged Themselves: Detecting Creativity in Art | http://arxiv.org/pdf/1506.04356v1.pdf | author:Milan Rajkoviƒá, Milo≈° Milovanoviƒá category:cs.CV q-bio.NC published:2015-06-14 summary:Creativity and the understanding of cognitive processes involved in thecreative process are relevant to all of human activities. Comprehension ofcreativity in the arts is of special interest due to the involvement of manyscientific and non scientific disciplines. Using digital representation ofpaintings, we show that creative process in painting art may be objectivelyrecognized within the mathematical framework of self organization, a processcharacteristic of nonlinear dynamic systems and occurring in natural and socialsciences. Unlike the artist identification process or the recognition offorgery, which presupposes the knowledge of the original work, our methodrequires no prior knowledge on the originality of the work of art. The originalpaintings are recognized as realizations of the creative process which, ingeneral, is shown to correspond to self-organization of texture features whichdetermine the aesthetic complexity of the painting. The method consists of thewavelet based statistical digital image processing and the measure ofstatistical complexity which represents the minimal (average) informationnecessary for optimal prediction. The statistical complexity is based on theproperly defined causal states with optimal predictive properties. Twodifferent time concepts related to the works of art are introduced: theinternal time and the artistic time. The internal time of the artwork isdetermined by the span of causal dependencies between wavelet coefficientswhile the artistic time refers to the internal time during which complexityincreases where complexity refers to compositional, aesthetic and structuralarrangement of texture features. The method is illustrated by recognizing theoriginal paintings from the copies made by the artists themselves, includingthe works of the famous surrealist painter Ren\'{e} Magritte.
arxiv-12000-80 | Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms | http://arxiv.org/pdf/1506.04359v1.pdf | author:Yunwen Lei, √úr√ºn Dogan, Alexander Binder, Marius Kloft category:cs.LG published:2015-06-14 summary:This paper studies the generalization performance of multi-classclassification algorithms, for which we obtain, for the first time, adata-dependent generalization error bound with a logarithmic dependence on theclass size, substantially improving the state-of-the-art linear dependence inthe existing data-dependent generalization analysis. The theoretical analysismotivates us to introduce a new multi-class classification machine based on$\ell_p$-norm regularization, where the parameter $p$ controls the complexityof the corresponding bounds. We derive an efficient optimization algorithmbased on Fenchel duality theory. Benchmarks on several real-world datasets showthat the proposed algorithm can achieve significant accuracy gains over thestate of the art.
arxiv-12000-81 | Localized Multiple Kernel Learning---A Convex Approach | http://arxiv.org/pdf/1506.04364v1.pdf | author:Yunwen Lei, Alexander Binder, √úr√ºn Dogan, Marius Kloft category:cs.LG published:2015-06-14 summary:We propose a localized approach to multiple kernel learning that, in contrastto prevalent approaches, can be formulated as a convex optimization problemover a given cluster structure. From which we obtain the first generalizationerror bounds for localized multiple kernel learning and derive an efficientoptimization algorithm based on the Fenchel dual representation. Experiments onreal-world datasets from the application domains of computational biology andcomputer vision show that the convex approach to localized multiple kernellearning can achieve higher prediction accuracies than its global andnon-convex local counterparts.
arxiv-12000-82 | Extract an essential skeleton of a character as a graph from a character image | http://arxiv.org/pdf/1506.05068v1.pdf | author:Kazuhisa Fujita category:cs.CV published:2015-06-13 summary:This paper aims to make a graph representing an essential skeleton of acharacter from an image that includes a machine printed or a handwrittencharacter using the growing neural gas (GNG) method and the relativeneighborhood graph (RNG) algorithm. The visual system in our brain canrecognize printed characters and handwritten characters easily, robustly, andprecisely. How can our brains robustly recognize characters? In the visualprocessing in our brain, essential features of an object will be used forrecognition. The essential features are crosses, corners, junctions and so on.These features may be useful for character recognition by a computer. However,extraction of the features is difficult. If the skeleton of a character isrepresented as a graph, the features can be more easily extracted. To extractthe skeleton of a character as a graph from a character image, we used the GNGmethod and the RNG algorithm. We achieved to extract skeleton graphs fromimages including distorted, noisy, and handwritten characters.
arxiv-12000-83 | Evaluation of the Accuracy of the BGLemmatizer | http://arxiv.org/pdf/1506.04229v1.pdf | author:Elena Karashtranova, Grigor Iliev, Nadezhda Borisova, Yana Chankova, Irena Atanasova category:cs.CL published:2015-06-13 summary:This paper reveals the results of an analysis of the accuracy of developedsoftware for automatic lemmatization for the Bulgarian language. Thislemmatization software is written entirely in Java and is distributed as a GATEplugin. Certain statistical methods are used to define the accuracy of thissoftware. The results of the analysis show 95% lemmatization accuracy.
arxiv-12000-84 | A Publicly Available Cross-Platform Lemmatizer for Bulgarian | http://arxiv.org/pdf/1506.04228v1.pdf | author:Grigor Iliev, Nadezhda Borisova, Elena Karashtranova, Dafina Kostadinova category:cs.CL published:2015-06-13 summary:Our dictionary-based lemmatizer for the Bulgarian language presented here isdistributed as free software, publicly available to download and use under theGPL v3 license. The presented software is written entirely in Java and isdistributed as a GATE plugin. To our best knowledge, at the time of writingthis article, there are not any other free lemmatization tools specificallytargeting the Bulgarian language. The presented lemmatizer is a work inprogress and currently yields an accuracy of about 95% in comparison to themanually annotated corpus BulTreeBank-Morph, which contains 273933 tokens.
arxiv-12000-85 | Contamination Estimation via Convex Relaxations | http://arxiv.org/pdf/1506.04257v1.pdf | author:Matthew L. Malloy, Scott Alfeld, Paul Barford category:cs.IT cs.LG math.IT math.OC published:2015-06-13 summary:Identifying anomalies and contamination in datasets is important in a widevariety of settings. In this paper, we describe a new technique for estimatingcontamination in large, discrete valued datasets. Our approach considers thenormal condition of the data to be specified by a model consisting of a set ofdistributions. Our key contribution is in our approach to contaminationestimation. Specifically, we develop a technique that identifies the minimumnumber of data points that must be discarded (i.e., the level of contamination)from an empirical data set in order to match the model to within a specifiedgoodness-of-fit, controlled by a p-value. Appealing to results from largedeviations theory, we show a lower bound on the level of contamination isobtained by solving a series of convex programs. Theoretical results guaranteethe bound converges at a rate of $O(\sqrt{\log(p)/p})$, where p is the size ofthe empirical data set.
arxiv-12000-86 | Graphlet Decomposition: Framework, Algorithms, and Applications | http://arxiv.org/pdf/1506.04322v2.pdf | author:Nesreen K. Ahmed, Jennifer Neville, Ryan A. Rossi, Nick Duffield, Theodore L. Willke category:cs.SI cs.DC cs.IR stat.ML published:2015-06-13 summary:From social science to biology, numerous applications often rely on graphletsfor intuitive and meaningful characterization of networks at both the globalmacro-level as well as the local micro-level. While graphlets have witnessed atremendous success and impact in a variety of domains, there has yet to be afast and efficient approach for computing the frequencies of these subgraphpatterns. However, existing methods are not scalable to large networks withmillions of nodes and edges, which impedes the application of graphlets to newproblems that require large-scale network analysis. To address these problems,we propose a fast, efficient, and parallel algorithm for counting graphlets ofsize k={3,4}-nodes that take only a fraction of the time to compute whencompared with the current methods used. The proposed graphlet countingalgorithms leverages a number of proven combinatorial arguments for differentgraphlets. For each edge, we count a few graphlets, and with these counts alongwith the combinatorial arguments, we obtain the exact counts of others inconstant time. On a large collection of 300+ networks from a variety ofdomains, our graphlet counting strategies are on average 460x faster thancurrent methods. This brings new opportunities to investigate the use ofgraphlets on much larger networks and newer applications as we show in theexperiments. To the best of our knowledge, this paper provides the largestgraphlet computations to date as well as the largest systematic investigationon over 300+ networks from a variety of domains.
arxiv-12000-87 | Combinatorial Energy Learning for Image Segmentation | http://arxiv.org/pdf/1506.04304v2.pdf | author:Jeremy Maitin-Shepard, Viren Jain, Michal Januszewski, Peter Li, J√∂rgen Kornfeld, Julia Buhmann, Pieter Abbeel category:cs.CV published:2015-06-13 summary:We introduce a new machine learning approach for image segmentation, based ona joint energy model over image features and novel local binary shapedescriptors. These descriptors compactly represent rich shape information atmultiple scales, including interactions between multiple objects. Our approach,which does not rely on any hand-designed features, reflects the inherentcombinatorial nature of dense image segmentation problems. We propose efficientalgorithms for learning deep neural networks to model the joint energy, and forlocal optimization of this energy in the space of supervoxel agglomerations. We demonstrate the effectiveness of our approach on 3-D biological data,where rich shape information appears to be critical for resolving ambiguities.On two challenging 3-D electron microscopy datasets highly relevant to ongoingefforts towards large-scale dense mapping of neural circuits, we achievestate-of-the-art segmentation accuracy.
arxiv-12000-88 | A Bayesian Model for Generative Transition-based Dependency Parsing | http://arxiv.org/pdf/1506.04334v2.pdf | author:Jan Buys, Phil Blunsom category:cs.CL published:2015-06-13 summary:We propose a simple, scalable, fully generative model for transition-baseddependency parsing with high accuracy. The model, parameterized by HierarchicalPitman-Yor Processes, overcomes the limitations of previous generative modelsby allowing fast and accurate inference. We propose an efficient decodingalgorithm based on particle filtering that can adapt the beam size to theuncertainty in the model while jointly predicting POS tags and parse trees. TheUAS of the parser is on par with that of a greedy discriminative baseline. As alanguage model, it obtains better perplexity than a n-gram model by performingsemi-supervised learning over a large unlabelled corpus. We show that the modelis able to generate locally and syntactically coherent sentences, opening thedoor to further applications in language generation.
arxiv-12000-89 | Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting | http://arxiv.org/pdf/1506.04214v2.pdf | author:Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun Woo category:cs.CV published:2015-06-13 summary:The goal of precipitation nowcasting is to predict the future rainfallintensity in a local region over a relatively short period of time. Very fewprevious studies have examined this crucial and challenging weather forecastingproblem from the machine learning perspective. In this paper, we formulateprecipitation nowcasting as a spatiotemporal sequence forecasting problem inwhich both the input and the prediction target are spatiotemporal sequences. Byextending the fully connected LSTM (FC-LSTM) to have convolutional structuresin both the input-to-state and state-to-state transitions, we propose theconvolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable modelfor the precipitation nowcasting problem. Experiments show that our ConvLSTMnetwork captures spatiotemporal correlations better and consistentlyoutperforms FC-LSTM and the state-of-the-art operational ROVER algorithm forprecipitation nowcasting.
arxiv-12000-90 | On the Equivalence of CoCoA+ and DisDCA | http://arxiv.org/pdf/1506.04217v2.pdf | author:Ching-pei Lee category:cs.LG published:2015-06-13 summary:In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015)under the setting used in their experiments, which is also the best settingsuggested by the authors that proposed this algorithm, is equivalent to thepractical variant of DisDCA (Yang, NIPS, 2013).
arxiv-12000-91 | A Flexible and Efficient Algorithmic Framework for Constrained Matrix and Tensor Factorization | http://arxiv.org/pdf/1506.04209v2.pdf | author:Kejun Huang, Nicholas D. Sidiropoulos, Athanasios P. Liavas category:stat.ML cs.LG math.OC stat.CO published:2015-06-13 summary:We propose a general algorithmic framework for constrained matrix and tensorfactorization, which is widely used in signal processing and machine learning.The new framework is a hybrid between alternating optimization (AO) and thealternating direction method of multipliers (ADMM): each matrix factor isupdated in turn, using ADMM, hence the name AO-ADMM. This combination cannaturally accommodate a great variety of constraints on the factor matrices,and almost all possible loss measures for the fitting. Computation caching andwarm start strategies are used to ensure that each update is evaluatedefficiently, while the outer AO framework exploits recent developments in blockcoordinate descent (BCD)-type methods which help ensure that every limit pointis a stationary point, as well as faster and more robust convergence inpractice. Three special cases are studied in detail: non-negative matrix/tensorfactorization, constrained matrix/tensor completion, and dictionary learning.Extensive simulations and experiments with real data are used to showcase theeffectiveness and broad applicability of the proposed framework.
arxiv-12000-92 | Using the Mean Absolute Percentage Error for Regression Models | http://arxiv.org/pdf/1506.04176v1.pdf | author:Arnaud De Myttenaere, Boris Golden, B√©n√©dicte Le Grand, Fabrice Rossi category:stat.ML cs.LG published:2015-06-12 summary:We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We show thatfinding the best model under the MAPE is equivalent to doing weighted MeanAbsolute Error (MAE) regression. We show that universal consistency ofEmpirical Risk Minimization remains possible using the MAPE instead of the MAE.
arxiv-12000-93 | Search Strategies for Binary Feature Selection for a Naive Bayes Classifier | http://arxiv.org/pdf/1506.04177v1.pdf | author:Tsirizo Rabenoro, J√©r√¥me Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2015-06-12 summary:We compare in this paper several feature selection methods for the NaiveBayes Classifier (NBC) when the data under study are described by a largenumber of redundant binary indicators. Wrapper approaches guided by the NBCestimation of the classification error probability out-perform filterapproaches while retaining a reasonable computational cost.
arxiv-12000-94 | Deep Structured Models For Group Activity Recognition | http://arxiv.org/pdf/1506.04191v1.pdf | author:Zhiwei Deng, Mengyao Zhai, Lei Chen, Yuhao Liu, Srikanth Muralidharan, Mehrsan Javan Roshtkhari, Greg Mori category:cs.CV published:2015-06-12 summary:This paper presents a deep neural-network-based hierarchical graphical modelfor individual and group activity recognition in surveillance scenes. Deepnetworks are used to recognize the actions of individual people in a scene.Next, a neural-network-based hierarchical graphical model refines the predictedlabels for each class by considering dependencies between the classes. Thisrefinement step mimics a message-passing step similar to inference in aprobabilistic graphical model. We show that this approach can be effective ingroup activity recognition, with the deep graphical model improving recognitionrates over baseline methods.
arxiv-12000-95 | Towards Benchmarking Scene Background Initialization | http://arxiv.org/pdf/1506.04051v1.pdf | author:Lucia Maddalena, Alfredo Petrosino category:cs.CV published:2015-06-12 summary:Given a set of images of a scene taken at different times, the availabilityof an initial background model that describes the scene without foregroundobjects is the prerequisite for a wide range of applications, ranging fromvideo surveillance to computational photography. Even though several methodshave been proposed for scene background initialization, the lack of a commongroundtruthed dataset and of a common set of metrics makes it difficult tocompare their performance. To move first steps towards an easy and faircomparison of these methods, we assembled a dataset of sequences frequentlyadopted for background initialization, selected or created ground truths forquantitative evaluation through a selected suite of metrics, and comparedresults obtained by some existing methods, making all the material publiclyavailable.
arxiv-12000-96 | Knowledge Representation in Learning Classifier Systems: A Review | http://arxiv.org/pdf/1506.04002v1.pdf | author:Farzaneh Shoeleh, Mahshid Majd, Ali Hamzeh, Sattar Hashemi category:cs.NE cs.LG published:2015-06-12 summary:Knowledge representation is a key component to the success of all rule basedsystems including learning classifier systems (LCSs). This component bringsinsight into how to partition the problem space what in turn seeks prominentrole in generalization capacity of the system as a whole. Recently, knowledgerepresentation component has received great deal of attention within datamining communities due to its impacts on rule based systems in terms ofefficiency and efficacy. The current work is an attempt to find a comprehensiveand yet elaborate view into the existing knowledge representation techniques inLCS domain in general and XCS in specific. To achieve the objectives, knowledgerepresentation techniques are grouped into different categories based on theclassification approach in which they are incorporated. In each category, theunderlying rule representation schema and the format of classifier condition tosupport the corresponding representation are presented. Furthermore, a preciseexplanation on the way that each technique partitions the problem space alongwith the extensive experimental results is provided. To have an elaborated viewon the functionality of each technique, a comparative analysis of existingtechniques on some conventional problems is provided. We expect this survey tobe of interest to the LCS researchers and practitioners since it provides aguideline for choosing a proper knowledge representation technique for a givenproblem and also opens up new streams of research on this topic.
arxiv-12000-97 | Robust Structured Low-Rank Approximation on the Grassmannian | http://arxiv.org/pdf/1506.03958v1.pdf | author:Clemens Hage, Martin Kleinsteuber category:stat.ML published:2015-06-12 summary:Over the past years Robust PCA has been established as a standard tool forreliable low-rank approximation of matrices in the presence of outliers.Recently, the Robust PCA approach via nuclear norm minimization has beenextended to matrices with linear structures which appear in applications suchas system identification and data series analysis. At the same time it has beenshown how to control the rank of a structured approximation via matrixfactorization approaches. The drawbacks of these methods either lie in the lackof robustness against outliers or in their static nature of repeatedbatch-processing. We present a Robust Structured Low-Rank Approximation methodon the Grassmannian that on the one hand allows for fast re-initialization inan online setting due to subspace identification with manifolds, and that isrobust against outliers due to a smooth approximation of the $\ell_p$-norm costfunction on the other hand. The method is evaluated in online time seriesforecasting tasks on simulated and real-world data.
arxiv-12000-98 | On the accuracy of self-normalized log-linear models | http://arxiv.org/pdf/1506.04147v2.pdf | author:Jacob Andreas, Maxim Rabinovich, Dan Klein, Michael I. Jordan category:stat.ML cs.CL cs.LG stat.ME published:2015-06-12 summary:Calculation of the log-normalizer is a major computational obstacle inapplications of log-linear models with large output spaces. The problem of fastnormalizer computation has therefore attracted significant attention in thetheoretical and applied machine learning literature. In this paper, we analyzea recently proposed technique known as "self-normalization", which introduces aregularization term in training to penalize log normalizers for deviating fromzero. This makes it possible to use unnormalized model scores as approximateprobabilities. Empirical evidence suggests that self-normalization is extremelyeffective, but a theoretical understanding of why it should work, and howgenerally it can be applied, is largely lacking. We prove generalization boundson the estimated variance of normalizers and upper bounds on the loss inaccuracy due to self-normalization, describe classes of input distributionsthat self-normalize easily, and construct explicit examples of high-varianceinput distributions. Our theoretical results make predictions about thedifficulty of fitting self-normalized models to several classes ofdistributions, and we conclude with empirical validation of these predictions.
arxiv-12000-99 | Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle Point Problems | http://arxiv.org/pdf/1506.04093v1.pdf | author:Zhanxing Zhu, Amos J. Storkey category:stat.ML cs.LG published:2015-06-12 summary:We consider a generic convex-concave saddle point problem with separablestructure, a form that covers a wide-ranged machine learning applications.Under this problem structure, we follow the framework of primal-dual updatesfor saddle point problems, and incorporate stochastic block coordinate descentwith adaptive stepsize into this framework. We theoretically show that ourproposal of adaptive stepsize potentially achieves a sharper linear convergencerate compared with the existing methods. Additionally, since we can select"mini-batch" of block coordinates to update, our method is also amenable toparallel processing for large-scale data. We apply the proposed method toregularized empirical risk minimization and show that it performs comparablyor, more often, better than state-of-the-art methods on both synthetic andreal-world data sets.
arxiv-12000-100 | Optimal $Œ≥$ and $C$ for $Œµ$-Support Vector Regression with RBF Kernels | http://arxiv.org/pdf/1506.03942v1.pdf | author:Longfei Lu category:cs.LG stat.ML published:2015-06-12 summary:The objective of this study is to investigate the efficient determination of$C$ and $\gamma$ for Support Vector Regression with RBF or mahalanobis kernelbased on numerical and statistician considerations, which indicates theconnection between $C$ and kernels and demonstrates that the deviation ofgeometric distance of neighbour observation in mapped space effects the predictaccuracy of $\epsilon$-SVR. We determinate the arrange of $\gamma$ & $C$ andpropose our method to choose their best values.
arxiv-12000-101 | Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences | http://arxiv.org/pdf/1506.04089v4.pdf | author:Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.CL cs.AI cs.LG cs.NE cs.RO published:2015-06-12 summary:We propose a neural sequence-to-sequence model for direction following, atask that is essential to realizing effective autonomous agents. Ouralignment-based encoder-decoder model with long short-term memory recurrentneural networks (LSTM-RNN) translates natural language instructions to actionsequences based upon a representation of the observable world state. Weintroduce a multi-level aligner that empowers our model to focus on sentence"regions" salient to the current world state by using multiple abstractions ofthe input sentence. In contrast to existing methods, our model uses nospecialized linguistic resources (e.g., parsers) or task-specific annotations(e.g., seed lexicons). It is therefore generalizable, yet still achieves thebest results reported to-date on a benchmark single-sentence dataset andcompetitive results for the limited-training multi-sentence setting. We analyzeour model through a series of ablations that elucidate the contributions of theprimary components of our model.
arxiv-12000-102 | A Novel Hybrid Approach for Cephalometric Landmark Detection | http://arxiv.org/pdf/1506.03936v1.pdf | author:Mahshid Majd, Farzaneh Shoeleh category:cs.CV published:2015-06-12 summary:Cephalometric analysis has an important role in dentistry and especially inorthodontics as a treatment planning tool to gauge the size and specialrelationships of the teeth, jaws and cranium. The first step of using suchanalyses is localizing some important landmarks known as cephalometriclandmarks on craniofacial in x-ray image. The past decade has seen a growinginterest in automating this process. In this paper, a novel hybrid approach isproposed for automatic detection of cephalometric landmarks. Here, thelandmarks are categorized into three main sets according to their anatomicalcharacteristics and usage in well-known cephalometric analyses. Consequently,to have a reliable and accurate detection system, three methods named edgetracing, weighted template matching, and analysis based estimation aredesigned, each of which is consistent and well-suited for one category. Edgetracing method is suggested to predict those landmarks which are located onedges. Weighted template matching method is well-suited for landmarks locatedin an obvious and specific structure which can be extracted or searchable in agiven x-ray image. The last but not the least method is named analysis basedestimation. This method is based on the fact that in cephalometric analyses therelations between landmarks are used and the locations of some landmarks arenever used individually. Therefore the third suggested method has a novelty inestimating the desired relations directly. The effectiveness of the proposedapproach is compared with the state of the art methods and the results werepromising especially in real world applications.
arxiv-12000-103 | Place classification with a graph regularized deep neural network model | http://arxiv.org/pdf/1506.03899v1.pdf | author:Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu category:cs.RO cs.CV cs.LG cs.NE published:2015-06-12 summary:Place classification is a fundamental ability that a robot should possess tocarry out effective human-robot interactions. It is a nontrivial classificationproblem which has attracted many research. In recent years, there is a highexploitation of Artificial Intelligent algorithms in robotics applications.Inspired by the recent successes of deep learning methods, we propose anend-to-end learning approach for the place classification problem. With thedeep architectures, this methodology automatically discovers features andcontributes in general to higher classification accuracies. The pipeline of ourapproach is composed of three parts. Firstly, we construct multiple layers oflaser range data to represent the environment information in different levelsof granularity. Secondly, each layer of data is fed into a deep neural networkmodel for classification, where a graph regularization is imposed to the deeparchitecture for keeping local consistency between adjacent samples. Finally,the predicted labels obtained from all the layers are fused based on confidencetrees to maximize the overall confidence. Experimental results validate theeffective- ness of our end-to-end place classification framework in which boththe multi-layer structure and the graph regularization promote theclassification performance. Furthermore, results show that the featuresautomatically learned from the raw input range data can achieve competitiveresults to the features constructed based on statistical and geometricalinformation.
arxiv-12000-104 | MCMC for Variationally Sparse Gaussian Processes | http://arxiv.org/pdf/1506.04000v1.pdf | author:James Hensman, Alexander G. de G. Matthews, Maurizio Filippone, Zoubin Ghahramani category:stat.ML published:2015-06-12 summary:Gaussian process (GP) models form a core part of probabilistic machinelearning. Considerable research effort has been made into attacking threeissues with GP models: how to compute efficiently when the number of data islarge; how to approximate the posterior when the likelihood is not Gaussian andhow to estimate covariance function parameter posteriors. This papersimultaneously addresses these, using a variational approximation to theposterior which is sparse in support of the function but otherwise free-form.The result is a Hybrid Monte-Carlo sampling scheme which allows for anon-Gaussian approximation over the function values and covariance parameterssimultaneously, with efficient computations based on inducing-point sparse GPs.Code to replicate each experiment in this paper will be available shortly.
arxiv-12000-105 | Causal inference via algebraic geometry: necessary and sufficient conditions for the feasibility of discrete causal models | http://arxiv.org/pdf/1506.03880v1.pdf | author:Ciar√°n M. Lee, Robert W. Spekkens category:stat.ML quant-ph published:2015-06-12 summary:We provide a scheme for inferring causal relations from uncontrolledstatistical data which makes use of all of the information in the jointprobability distribution over the observed variables rather than just theconditional independence relations. We focus on causal models containing justtwo observed variables, each of which is binary. We allow any number of latentvariables and we do not impose any restriction on the manner in which theobserved variables may depend functionally on the latent ones. In particular,the noise need not be additive. We provide an inductive scheme for classifyingcausal models into distinct observational equivalence classes. For eachobservational equivalence class, we provide a procedure for deriving, usingtechniques from algebraic geometry, necessary and sufficient conditions on thejoint distribution for the feasibility of the class. Connections andapplications of these results to the emerging field of quantum causal modelsare also discussed.
arxiv-12000-106 | Sparse Multi-layer Image Approximation: Facial Image Compression | http://arxiv.org/pdf/1506.03998v1.pdf | author:Sohrab Ferdowsi, Svyatoslav Voloshynovskiy, Dimche Kostadinov category:cs.CV cs.IT math.IT published:2015-06-12 summary:We propose a scheme for multi-layer representation of images. The problem isfirst treated from an information-theoretic viewpoint where we analyze thebehavior of different sources of information under a multi-layer datacompression framework and compare it with a single-stage (shallow) structure.We then consider the image data as the source of information and link theproposed representation scheme to the problem of multi-layer dictionarylearning for visual data. For the current work we focus on the problem of imagecompression for a special class of images where we report a considerableperformance boost in terms of PSNR at high compression ratios in comparisonwith the JPEG2000 codec.
arxiv-12000-107 | CloudCV: Large Scale Distributed Computer Vision as a Cloud Service | http://arxiv.org/pdf/1506.04130v1.pdf | author:Harsh Agrawal, Clint Solomon Mathialagan, Yash Goyal, Neelima Chavali, Prakriti Banik, Akrit Mohapatra, Ahmed Osman, Dhruv Batra category:cs.CV cs.DC published:2015-06-12 summary:We are witnessing a proliferation of massive visual data. Unfortunatelyscaling existing computer vision algorithms to large datasets leavesresearchers repeatedly solving the same algorithmic, logistical, andinfrastructural problems. Our goal is to democratize computer vision; oneshould not have to be a computer vision, big data and distributed computingexpert to have access to state-of-the-art distributed computer visionalgorithms. We present CloudCV, a comprehensive system to provide access tostate-of-the-art distributed computer vision algorithms as a cloud servicethrough a Web Interface and APIs.
arxiv-12000-108 | Reducing offline evaluation bias of collaborative filtering algorithms | http://arxiv.org/pdf/1506.04135v1.pdf | author:Arnaud De Myttenaere, Boris Golden, B√©n√©dicte Le Grand, Fabrice Rossi category:cs.IR cs.LG stat.ML published:2015-06-12 summary:Recommendation systems have been integrated into the majority of large onlinesystems to filter and rank information according to user profiles. It thusinfluences the way users interact with the system and, as a consequence, biasthe evaluation of the performance of a recommendation algorithm computed usinghistorical data (via offline evaluation). This paper presents a new applicationof a weighted offline evaluation to reduce this bias for collaborativefiltering algorithms.
arxiv-12000-109 | Technical Report: Image Captioning with Semantically Similar Images | http://arxiv.org/pdf/1506.03995v1.pdf | author:Martin Kol√°≈ô, Michal Hradi≈°, Pavel Zemƒç√≠k category:cs.CV published:2015-06-12 summary:This report presents our submission to the MS COCO Captioning Challenge 2015.The method uses Convolutional Neural Network activations as an embedding tofind semantically similar images. From these images, the most typical captionis selected based on unigram frequencies. Although the method received lowscores with automated evaluation metrics and in human assessed averagecorrectness, it is competitive in the ratio of captions which pass the Turingtest and which are assessed as better or equal to human captions.
arxiv-12000-110 | Exact ICL maximization in a non-stationary time extension of the latent block model for dynamic networks | http://arxiv.org/pdf/1506.04138v1.pdf | author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML published:2015-06-12 summary:The latent block model (LBM) is a flexible probabilistic tool to describeinteractions between node sets in bipartite networks, but it does not accountfor interactions of time varying intensity between nodes in unknown classes. Inthis paper we propose a non stationary temporal extension of the LBM thatclusters simultaneously the two node sets of a bipartite network and constructsclasses of time intervals on which interactions are stationary. The number ofclusters as well as the membership to classes are obtained by maximizing theexact complete-data integrated likelihood relying on a greedy search approach.Experiments on simulated and real data are carried out in order to assess theproposed methodology.
arxiv-12000-111 | A Spectral Algorithm with Additive Clustering for the Recovery of Overlapping Communities in Networks | http://arxiv.org/pdf/1506.04158v2.pdf | author:Emilie Kaufmann, Thomas Bonald, Marc Lelarge category:stat.ML published:2015-06-12 summary:This paper presents a novel spectral algorithm with additive clustering,designed to identify overlapping communities in networks. The algorithm isbased on geometric properties of the spectrum of the expected adjacency matrixin a random graph model that we call stochastic blockmodel withoverlap (SBMO).An adaptive version of the algorithm, that does not require the knowledge ofthe number of hidden communities, is proved to be consistent under the SBMOwhen the degrees in the graph are (slightly more than) logarithmic. Thealgorithm is shown to perform well on simulateddata and on real-world graphswith known overlapping communities.
arxiv-12000-112 | Training Bidirectional Helmholtz Machines | http://arxiv.org/pdf/1506.03877v4.pdf | author:Jorg Bornschein, Samira Shabanian, Asja Fischer, Yoshua Bengio category:cs.LG stat.ML published:2015-06-12 summary:Efficient unsupervised training and inference in deep generative modelsremains a challenging problem. One basic approach, called Helmholtz machine,involves training a top-down directed generative model together with abottom-up auxiliary model that is trained to help perform approximateinference. Recent results indicate that better results can be obtained withbetter approximate inference procedures. Instead of employing more powerfulprocedures, we here propose to regularize the generative model to stay close tothe class of distributions that can be efficiently inverted by the approximateinference model. We achieve this by interpreting both the top-down and thebottom-up directed models as approximate inference distributions and bydefining the model distribution to be the geometric mean of these two. Wepresent a lower-bound for the likelihood of this model and we show thatoptimizing this bound regularizes the model so that the Bhattacharyya distancebetween the bottom-up and top-down approximate distributions is minimized. Wedemonstrate that we can use this approach to fit generative models with manylayers of hidden binary stochastic variables to complex training distributionsand hat this method prefers significantly deeper architectures while itsupports orders of magnitude more efficient approximate inference than otherapproaches.
arxiv-12000-113 | Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes | http://arxiv.org/pdf/1506.04088v2.pdf | author:Ryan Giordano, Tamara Broderick, Michael Jordan category:stat.ML published:2015-06-12 summary:Mean field variational Bayes (MFVB) is a popular posterior approximationmethod due to its fast runtime on large-scale data sets. However, it is wellknown that a major failing of MFVB is that it underestimates the uncertainty ofmodel variables (sometimes severely) and provides no information about modelvariable covariance. We generalize linear response methods from statistical physics to deliveraccurate uncertainty estimates for model variables---both for individualvariables and coherently across variables. We call our method linear responsevariational Bayes (LRVB). When the MFVB posterior approximation is in theexponential family, LRVB has a simple, analytic form, even for non-conjugatemodels. Indeed, we make no assumptions about the form of the true posterior. Wedemonstrate the accuracy and scalability of our method on a range of models forboth simulated and real data.
arxiv-12000-114 | On the properties of variational approximations of Gibbs posteriors | http://arxiv.org/pdf/1506.04091v2.pdf | author:Pierre Alquier, James Ridgway, Nicolas Chopin category:stat.ML math.ST stat.TH published:2015-06-12 summary:The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimaldistribution of estimators, usually called the Gibbs posterior, isunfortunately intractable. One may sample from it using Markov chain MonteCarlo, but this is often too slow for big datasets. We consider insteadvariational approximations of the Gibbs posterior, which are fast to compute.We undertake a general study of the properties of such approximations. Our mainfinding is that such a variational approximation has often the same rate ofconvergence as the original PAC-Bayesian procedure it approximates. Wespecialise our results to several learning tasks (classification, ranking,matrix completion),discuss how to implement a variational approximation in eachcase, and illustrate the good properties of said approximation on realdatasets.
arxiv-12000-115 | Stochastic Expectation Propagation | http://arxiv.org/pdf/1506.04132v2.pdf | author:Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner category:stat.ML cs.LG published:2015-06-12 summary:Expectation propagation (EP) is a deterministic approximation algorithm thatis often used to perform approximate Bayesian parameter learning. EPapproximates the full intractable posterior distribution through a set of localapproximations that are iteratively refined for each datapoint. EP can offeranalytic and computational advantages over other approximations, such asVariational Inference (VI), and is the method of choice for a number of models.The local nature of EP appears to make it an ideal candidate for performingBayesian learning on large models in large-scale dataset settings. However, EPhas a crucial limitation in this context: the number of approximating factorsneeds to increase with the number of data-points, N, which often entails aprohibitively large memory overhead. This paper presents an extension to EP,called stochastic expectation propagation (SEP), that maintains a globalposterior approximation (like VI) but updates it in a local way (like EP).Experiments on a number of canonical learning problems using synthetic andreal-world datasets indicate that SEP performs almost as well as full EP, butreduces the memory consumption by a factor of $N$. SEP is therefore ideallysuited to performing approximate Bayesian learning in the large model, largedataset setting.
arxiv-12000-116 | Variance Reduced Stochastic Gradient Descent with Neighbors | http://arxiv.org/pdf/1506.03662v4.pdf | author:Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, Brian McWilliams category:cs.LG math.OC stat.ML G.1.6; I.2.6 published:2015-06-11 summary:Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet itsslow convergence can be a computational bottleneck. Variance reductiontechniques such as SAG, SVRG and SAGA have been proposed to overcome thisweakness, achieving linear convergence. However, these methods are either basedon computations of full gradients at pivot points, or on keeping per data pointcorrections in memory. Therefore speed-ups relative to SGD may need a minimalnumber of epochs in order to materialize. This paper investigates algorithmsthat can exploit neighborhood structure in the training data to share andre-use information about past stochastic gradients across data points, whichoffers advantages in the transient optimization phase. As a side-product weprovide a unified convergence analysis for a family of variance reductionalgorithms, which we call memorization algorithms. We provide experimentalresults supporting our theory.
arxiv-12000-117 | Random Maxout Features | http://arxiv.org/pdf/1506.03705v2.pdf | author:Youssef Mroueh, Steven Rennie, Vaibhava Goel category:cs.LG stat.ML published:2015-06-11 summary:In this paper, we propose and study random maxout features, which areconstructed by first projecting the input data onto sets of randomly generatedvectors with Gaussian elements, and then outputing the maximum projection valuefor each set. We show that the resulting random feature map, when used inconjunction with linear models, allows for the locally linear estimation of thefunction of interest in classification tasks, and for the locally linearembedding of points when used for dimensionality reduction or datavisualization. We derive generalization bounds for learning that assess theerror in approximating locally linear functions by linear functions in themaxout feature space, and empirically evaluate the efficacy of the approach onthe MNIST and TIMIT classification tasks.
arxiv-12000-118 | Optimization Monte Carlo: Efficient and Embarrassingly Parallel Likelihood-Free Inference | http://arxiv.org/pdf/1506.03693v2.pdf | author:Edward Meeds, Max Welling category:cs.LG stat.ML published:2015-06-11 summary:We describe an embarrassingly parallel, anytime Monte Carlo method forlikelihood-free models. The algorithm starts with the view that thestochasticity of the pseudo-samples generated by the simulator can becontrolled externally by a vector of random numbers u, in such a way that theoutcome, knowing u, is deterministic. For each instantiation of u we run anoptimization procedure to minimize the distance between summary statistics ofthe simulator and the data. After reweighing these samples using the prior andthe Jacobian (accounting for the change of volume in transforming from thespace of summary statistics to the space of parameters) we show that thisweighted ensemble represents a Monte Carlo estimate of the posteriordistribution. The procedure can be run embarrassingly parallel (each nodehandling one sample) and anytime (by allocating resources to the worstperforming sample). The procedure is validated on six experiments.
arxiv-12000-119 | Techniques for effective and efficient fire detection from social media images | http://arxiv.org/pdf/1506.03844v2.pdf | author:Marcos Bedo, Gustavo Blanco, Willian Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano Traina Jr category:cs.CV published:2015-06-11 summary:Social media could provide valuable information to support decision making incrisis management, such as in accidents, explosions and fires. However, much ofthe data from social media are images, which are uploaded in a rate that makesit impossible for human beings to analyze them. Despite the many works on imageanalysis, there are no fire detection studies on social media. To fill thisgap, we propose the use and evaluation of a broad set of content-based imageretrieval and classification techniques for fire detection. Our maincontributions are: (i) the development of the Fast-Fire Detection method(FFDnR), which combines feature extractor and evaluation functions to supportinstance-based learning, (ii) the construction of an annotated set of imageswith ground-truth depicting fire occurrences -- the FlickrFire dataset, and(iii) the evaluation of 36 efficient image descriptors for fire detection.Using real data from Flickr, our results showed that FFDnR was able to achievea precision for fire detection comparable to that of human annotators.Therefore, our work shall provide a solid basis for further developments onmonitoring images from social media.
arxiv-12000-120 | Learning language through pictures | http://arxiv.org/pdf/1506.03694v2.pdf | author:Grzegorz Chrupa≈Ça, √Åkos K√°d√°r, Afra Alishahi category:cs.CL published:2015-06-11 summary:We propose Imaginet, a model of learning visually grounded representations oflanguage from coupled textual and visual input. The model consists of two GatedRecurrent Unit networks with shared word embeddings, and uses a multi-taskobjective by receiving a textual description of a scene and trying toconcurrently predict its visual representation and the next word in thesentence. Mimicking an important aspect of human language learning, it acquiresmeaning representations for individual words from descriptions of visualscenes. Moreover, it learns to effectively use sequential structure in semanticinterpretation of multi-word phrases.
arxiv-12000-121 | P-CNN: Pose-based CNN Features for Action Recognition | http://arxiv.org/pdf/1506.03607v2.pdf | author:Guilhem Ch√©ron, Ivan Laptev, Cordelia Schmid category:cs.CV published:2015-06-11 summary:This work targets human action recognition in video. While recent methodstypically represent actions by statistics of local video features, here weargue for the importance of a representation derived from human pose. To thisend we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN)for action recognition. The descriptor aggregates motion and appearanceinformation along tracks of human body parts. We investigate different schemesof temporal aggregation and experiment with P-CNN features obtained both forautomatically estimated and manually annotated human poses. We evaluate ourmethod on the recent and challenging JHMDB and MPII Cooking datasets. For bothdatasets our method shows consistent improvement over the state of the art.
arxiv-12000-122 | Distributed Recurrent Neural Forward Models with Synaptic Adaptation for Complex Behaviors of Walking Robots | http://arxiv.org/pdf/1506.03599v1.pdf | author:Sakyasingha Dasgupta, Dennis Goldschmidt, Florentin W√∂rg√∂tter, Poramate Manoonpong category:cs.NE cs.RO q-bio.NC published:2015-06-11 summary:Walking animals, like stick insects, cockroaches or ants, demonstrate afascinating range of locomotive abilities and complex behaviors. The locomotivebehaviors can consist of a variety of walking patterns along with adaptationthat allow the animals to deal with changes in environmental conditions, likeuneven terrains, gaps, obstacles etc. Biological study has revealed that suchcomplex behaviors are a result of a combination of biome- chanics and neuralmechanism thus representing the true nature of embodied interactions. While thebiomechanics helps maintain flexibility and sustain a variety of movements, theneural mechanisms generate movements while making appropriate predictionscrucial for achieving adaptation. Such predictions or planning ahead can beachieved by way of in- ternal models that are grounded in the overall behaviorof the animal. Inspired by these findings, we present here, an artificialbio-inspired walking system which effectively com- bines biomechanics (in termsof the body and leg structures) with the underlying neural mechanisms. Theneural mechanisms consist of 1) central pattern generator based control forgenerating basic rhythmic patterns and coordinated movements, 2) distributed(at each leg) recurrent neural network based adaptive forward models withefference copies as internal models for sensory predictions and instantaneousstate estimations, and 3) searching and elevation control for adapting themovement of an individual leg to deal with different environmental conditions.Using simulations we show that this bio-inspired approach with adaptiveinternal models allows the walking robot to perform complex loco- motivebehaviors as observed in insects, including walking on undulated terrains,crossing large gaps as well as climbing over high obstacles...
arxiv-12000-123 | Max-Entropy Feed-Forward Clustering Neural Network | http://arxiv.org/pdf/1506.03623v1.pdf | author:Han Xiao, Xiaoyan Zhu category:cs.LG published:2015-06-11 summary:The outputs of non-linear feed-forward neural network are positive, whichcould be treated as probability when they are normalized to one. If we takeEntropy-Based Principle into consideration, the outputs for each sample couldbe represented as the distribution of this sample for different clusters.Entropy-Based Principle is the principle with which we could estimate theunknown distribution under some limited conditions. As this paper defines twoprocesses in Feed-Forward Neural Network, our limited condition is theabstracted features of samples which are worked out in the abstraction process.And the final outputs are the probability distribution for different clustersin the clustering process. As Entropy-Based Principle is considered into thefeed-forward neural network, a clustering method is born. We have conductedsome experiments on six open UCI datasets, comparing with a few baselines andapplied purity as the measurement . The results illustrate that our methodoutperforms all the other baselines that are most popular clustering methods.
arxiv-12000-124 | Generalized Additive Model Selection | http://arxiv.org/pdf/1506.03850v2.pdf | author:Alexandra Chouldechova, Trevor Hastie category:stat.ML published:2015-06-11 summary:We introduce GAMSEL (Generalized Additive Model Selection), a penalizedlikelihood approach for fitting sparse generalized additive models in highdimension. Our method interpolates between null, linear and additive models byallowing the effect of each variable to be estimated as being either zero,linear, or a low-complexity curve, as determined by the data. We present ablockwise coordinate descent procedure for efficiently optimizing the penalizedlikelihood objective over a dense grid of the tuning parameter, producing aregularization path of additive models. We demonstrate the performance of ourmethod on both real and simulated data examples, and compare it with existingtechniques for additive model selection.
arxiv-12000-125 | Margin-Based Feed-Forward Neural Network Classifiers | http://arxiv.org/pdf/1506.03626v1.pdf | author:Han Xiao, Xiaoyan Zhu category:cs.LG published:2015-06-11 summary:Margin-Based Principle has been proposed for a long time, it has been provedthat this principle could reduce the structural risk and improve theperformance in both theoretical and practical aspects. Meanwhile, feed-forwardneural network is a traditional classifier, which is very hot at present with adeeper architecture. However, the training algorithm of feed-forward neuralnetwork is developed and generated from Widrow-Hoff Principle that means tominimize the squared error. In this paper, we propose a new training algorithmfor feed-forward neural networks based on Margin-Based Principle, which couldeffectively promote the accuracy and generalization ability of neural networkclassifiers with less labelled samples and flexible network. We have conductedexperiments on four UCI open datasets and achieved good results as expected. Inconclusion, our model could handle more sparse labelled and more high-dimensiondataset in a high accuracy while modification from old ANN method to our methodis easy and almost free of work.
arxiv-12000-126 | Recovering metric from full ordinal information | http://arxiv.org/pdf/1506.03762v3.pdf | author:Thibaut Le Gouic category:stat.ML math.ST stat.TH published:2015-06-11 summary:Given a geodesic space (E, d), we show that full ordinal knowledge on themetric d-i.e. knowledge of the function D d : (w, x, y, z) $\rightarrow$ 1d(w,x)$\le$d(y,z) , determines uniquely-up to a constant factor-the metric d.For a subspace En of n points of E, converging in Hausdorff distance to E, weconstruct a metric dn on En, based only on the knowledge of D d on En andestablish a sharp upper bound of the Gromov-Hausdorff distance between (En, dn)and (E, d).
arxiv-12000-127 | Mondrian Forests for Large-Scale Regression when Uncertainty Matters | http://arxiv.org/pdf/1506.03805v3.pdf | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG published:2015-06-11 summary:Many real-world regression problems demand a measure of the uncertaintyassociated with each prediction. Standard decision forests deliver efficientstate-of-the-art predictive performance, but high-quality uncertainty estimatesare lacking. Gaussian processes (GPs) deliver uncertainty estimates, butscaling GPs to large-scale data sets comes at the cost of approximating theuncertainty estimates. We extend Mondrian forests, first proposed byLakshminarayanan et al. (2014) for classification problems, to the large-scalenon-parametric regression setting. Using a novel hierarchical Gaussian priorthat dovetails with the Mondrian forest framework, we obtain principleduncertainty estimates, while still retaining the computational advantages ofdecision forests. Through a combination of illustrative examples, real-worldlarge-scale datasets, and Bayesian optimization benchmarks, we demonstrate thatMondrian forests outperform approximate GPs on large-scale regression tasks anddeliver better-calibrated uncertainty assessments than decision-forest-basedmethods.
arxiv-12000-128 | Tree-Cut for Probabilistic Image Segmentation | http://arxiv.org/pdf/1506.03852v1.pdf | author:Shell X. Hu, Christopher K. I. Williams, Sinisa Todorovic category:stat.ML cs.CV published:2015-06-11 summary:This paper presents a new probabilistic generative model for imagesegmentation, i.e. the task of partitioning an image into homogeneous regions.Our model is grounded on a mid-level image representation, called a regiontree, in which regions are recursively split into subregions until superpixelsare reached. Given the region tree, image segmentation is formalized assampling cuts in the tree from the model. Inference for the cuts is exact, andformulated using dynamic programming. Our tree-cut model can be tuned to samplesegmentations at a particular scale of interest out of many possible multiscaleimage segmentations. This generalizes the common notion that there should beonly one correct segmentation per image. Also, it allows moving beyond thestandard single-scale evaluation, where the segmentation result for an image isaveraged against the corresponding set of coarse and fine human annotations, toconduct a scale-specific evaluation. Our quantitative results are comparable tothose of the leading gPb-owt-ucm method, with the notable advantage that weadditionally produce a distribution over all possible tree-consistentsegmentations of the image.
arxiv-12000-129 | Constrained Convolutional Neural Networks for Weakly Supervised Segmentation | http://arxiv.org/pdf/1506.03648v2.pdf | author:Deepak Pathak, Philipp Kr√§henb√ºhl, Trevor Darrell category:cs.CV cs.LG published:2015-06-11 summary:We present an approach to learn a dense pixel-wise labeling from image-leveltags. Each image-level tag imposes constraints on the output labeling of aConvolutional Neural Network (CNN) classifier. We propose Constrained CNN(CCNN), a method which uses a novel loss function to optimize for any set oflinear constraints on the output space (i.e. predicted label distribution) of aCNN. Our loss formulation is easy to optimize and can be incorporated directlyinto standard stochastic gradient descent optimization. The key idea is tophrase the training objective as a biconvex optimization for linear models,which we then relax to nonlinear deep networks. Extensive experimentsdemonstrate the generality of our new learning framework. The constrained lossyields state-of-the-art results on weakly supervised semantic imagesegmentation. We further demonstrate that adding slightly more supervision cangreatly improve the performance of the learning algorithm.
arxiv-12000-130 | Pose-Invariant 3D Face Alignment | http://arxiv.org/pdf/1506.03799v1.pdf | author:Amin Jourabloo, Xiaoming Liu category:cs.CV published:2015-06-11 summary:Face alignment aims to estimate the locations of a set of landmarks for agiven image. This problem has received much attention as evidenced by therecent advancement in both the methodology and performance. However, most ofthe existing works neither explicitly handle face images with arbitrary poses,nor perform large-scale experiments on non-frontal and profile face images. Inorder to address these limitations, this paper proposes a novel face alignmentalgorithm that estimates both 2D and 3D landmarks and their 2D visibilities fora face image with an arbitrary pose. By integrating a 3D deformable model, acascaded coupled-regressor approach is designed to estimate both the cameraprojection matrix and the 3D landmarks. Furthermore, the 3D model also allowsus to automatically estimate the 2D landmark visibilities via surface normals.We gather a substantially larger collection of all-pose face images to evaluateour algorithm and demonstrate superior performances than the state-of-the-artmethods.
arxiv-12000-131 | Isometric sketching of any set via the Restricted Isometry Property | http://arxiv.org/pdf/1506.03521v2.pdf | author:Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi category:cs.IT cs.DS math.IT math.PR math.ST stat.ML stat.TH published:2015-06-11 summary:In this paper we show that for the purposes of dimensionality reductioncertain class of structured random matrices behave similarly to random Gaussianmatrices. This class includes several matrices for which matrix-vector multiplycan be computed in log-linear time, providing efficient dimensionalityreduction of general sets. In particular, we show that using such matrices anyset from high dimensions can be embedded into lower dimensions with nearoptimal distortion. We obtain our results by connecting dimensionalityreduction of any set to dimensionality reduction of sparse vectors via achaining argument.
arxiv-12000-132 | Parallelizing LDA using Partially Collapsed Gibbs Sampling | http://arxiv.org/pdf/1506.03784v1.pdf | author:M√•ns Magnusson, Leif Jonsson, Mattias Villani, David Broman category:stat.ML stat.ME published:2015-06-11 summary:Latent dirichlet allocation (LDA) is a model widely used for unsupervisedprobabilistic modeling of text and images. MCMC sampling from the posteriordistribution is typically performed using a collapsed Gibbs sampler thatintegrates out all model parameters except the topic indicators for each word.The topic indicators are Gibbs sampled iteratively by drawing each topic fromits conditional posterior. The popularity of this sampler stems from itsbalanced combination of simplicity and efficiency, but its inherentlysequential nature is an obstacle for parallel implementations. Growing corpussizes and increasing model complexity are making inference in LDA modelscomputationally infeasible without parallel sampling. We propose a parallelimplementation of LDA that only collapses over the topic proportions in eachdocument and therefore allows independent sampling of the topic indicators indifferent documents. We develop several modifications of the basic algorithmthat exploits sparsity and structure to further improve the performance of thepartially collapsed sampler. Contrary to other parallel LDA implementations,the partially collapsed sampler guarantees convergence to the true posterior.We show on several well-known corpora that the expected increase in statisticalinefficiency from only partial collapsing is smaller than commonly assumed, andcan be more than compensated by the speed-up from parallelization for largercorpora.
arxiv-12000-133 | Entity-Specific Sentiment Classification of Yahoo News Comments | http://arxiv.org/pdf/1506.03775v1.pdf | author:Prakhar Biyani, Cornelia Caragea, Narayan Bhamidipati category:cs.CL cs.IR cs.SI published:2015-06-11 summary:Sentiment classification is widely used for product reviews and in onlinesocial media such as forums, Twitter, and blogs. However, the problem ofclassifying the sentiment of user comments on news sites has not been addressedyet. News sites cover a wide range of domains including politics, sports,technology, and entertainment, in contrast to other online social sites such asforums and review sites, which are specific to a particular domain. A userassociated with a news site is likely to post comments on diverse topics (e.g.,politics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, orGoogle). Classifying the sentiment of users tied to various entities may helpobtain a holistic view of their personality, which could be useful inapplications such as online advertising, content personalization, and politicalcampaign planning. In this paper, we formulate the problem of entity-specificsentiment classification of comments posted on news articles in Yahoo News andpropose novel features that are specific to news comments. Experimental resultsshow that our models outperform state-of-the-art baselines.
arxiv-12000-134 | Recovering communities in the general stochastic block model without knowing the parameters | http://arxiv.org/pdf/1506.03729v1.pdf | author:Emmanuel Abbe, Colin Sandon category:math.PR cs.IT cs.LG cs.SI math.IT published:2015-06-11 summary:Most recent developments on the stochastic block model (SBM) rely on theknowledge of the model parameters, or at least on the number of communities.This paper introduces efficient algorithms that do not require such knowledgeand yet achieve the optimal information-theoretic tradeoffs identified in[AS15] for linear size communities. The results are three-fold: (i) in theconstant degree regime, an algorithm is developed that requires only alower-bound on the relative sizes of the communities and detects communitieswith an optimal accuracy scaling for large degrees; (ii) in the regime wheredegrees are scaled by $\omega(1)$ (diverging degrees), this is enhanced into afully agnostic algorithm that only takes the graph in question andsimultaneously learns the model parameters (including the number ofcommunities) and detects communities with accuracy $1-o(1)$, with an overallquasi-linear complexity; (iii) in the logarithmic degree regime, an agnosticalgorithm is developed that learns the parameters and achieves the optimalCH-limit for exact recovery, in quasi-linear time. These provide the firstalgorithms affording efficiency, universality and information-theoreticoptimality for strong and weak consistency in the general SBM with linear sizecommunities.
arxiv-12000-135 | Spectral Representations for Convolutional Neural Networks | http://arxiv.org/pdf/1506.03767v1.pdf | author:Oren Rippel, Jasper Snoek, Ryan P. Adams category:stat.ML cs.LG published:2015-06-11 summary:Discrete Fourier transforms provide a significant speedup in the computationof convolutions in deep learning. In this work, we demonstrate that, beyond itsadvantages for efficient computation, the spectral domain also provides apowerful representation in which to model and train convolutional neuralnetworks (CNNs). We employ spectral representations to introduce a number of innovations toCNN design. First, we propose spectral pooling, which performs dimensionalityreduction by truncating the representation in the frequency domain. Thisapproach preserves considerably more information per parameter than otherpooling strategies and enables flexibility in the choice of pooling outputdimensionality. This representation also enables a new form of stochasticregularization by randomized modification of resolution. We show that thesemethods achieve competitive results on classification and approximation tasks,without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectralparameterization of convolutional filters. While this leaves the underlyingmodel unchanged, it results in a representation that greatly facilitatesoptimization. We observe on a variety of popular CNN configurations that thisleads to significantly faster convergence during training.
arxiv-12000-136 | GAP Safe screening rules for sparse multi-task and multi-class models | http://arxiv.org/pdf/1506.03736v2.pdf | author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO published:2015-06-11 summary:High dimensional regression benefits from sparsity promoting regularizations.Screening rules leverage the known sparsity of the solution by ignoring somevariables in the optimization, hence speeding up solvers. When the procedure isproven not to discard features wrongly the rules are said to be \emph{safe}. Inthis paper we derive new safe rules for generalized linear models regularizedwith $\ell_1$ and $\ell_1/\ell_2$ norms. The rules are based on duality gapcomputations and spherical safe regions whose diameters converge to zero. Thisallows to discard safely more variables, in particular for low regularizationparameters. The GAP Safe rule can cope with any iterative solver and weillustrate its performance on coordinate descent for multi-task Lasso, binaryand multinomial logistic regression, demonstrating significant speed ups on alltested datasets with respect to previous safe rules.
arxiv-12000-137 | Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process | http://arxiv.org/pdf/1506.03768v1.pdf | author:Ye Wang, David B. Dunson category:stat.ML published:2015-06-11 summary:Learning of low dimensional structure in multidimensional data is a canonicalproblem in machine learning. One common approach is to suppose that theobserved data are close to a lower-dimensional smooth manifold. There are arich variety of manifold learning methods available, which allow mapping ofdata points to the manifold. However, there is a clear lack of probabilisticmethods that allow learning of the manifold along with the generativedistribution of the observed data. The best attempt is the Gaussian processlatent variable model (GP-LVM), but identifiability issues lead to poorperformance. We solve these issues by proposing a novel Coulomb repulsiveprocess (Corp) for locations of points on the manifold, inspired by physicalmodels of electrostatic interactions among particles. Combining this processwith a GP prior for the mapping function yields a novel electrostatic GP(electroGP) process. Focusing on the simple case of a one-dimensional manifold,we develop efficient inference algorithms, and illustrate substantiallyimproved performance in a variety of experiments including filling in missingframes in video.
arxiv-12000-138 | Detecting Clusters of Anomalies on Low-Dimensional Feature Subsets with Application to Network Traffic Flow Data | http://arxiv.org/pdf/1511.01047v1.pdf | author:Zhicong Qiu, David J. Miller, George Kesidis category:cs.NI cs.CR cs.LG published:2015-06-10 summary:In a variety of applications, one desires to detect groups of anomalous datasamples, with a group potentially manifesting its atypicality (relative to areference model) on a low-dimensional subset of the full measured set offeatures. Samples may only be weakly atypical individually, whereas they may bestrongly atypical when considered jointly. What makes this group anomalydetection problem quite challenging is that it is a priori unknown which subsetof features jointly manifests a particular group of anomalies. Moreover, it isunknown how many anomalous groups are present in a given data batch. In thiswork, we develop a group anomaly detection (GAD) scheme to identify the subsetof samples and subset of features that jointly specify an anomalous cluster. Weapply our approach to network intrusion detection to detect BotNet andpeer-to-peer flow clusters. Unlike previous studies, our approach captures andexploits statistical dependencies that may exist between the measured features.Experiments on real world network traffic data demonstrate the advantage of ourproposed system, and highlight the importance of exploiting feature dependencystructure, compared to the feature (or test) independence assumption made inprevious studies.
arxiv-12000-139 | Image Tag Completion and Refinement by Subspace Clustering and Matrix Completion | http://arxiv.org/pdf/1506.03475v1.pdf | author:Yuqing Hou, Zhouchen Lin category:cs.CV published:2015-06-10 summary:Tag-based image retrieval (TBIR) has drawn much attention in recent years dueto the explosive amount of digital images and crowdsourcing tags. However, theTBIR applications still suffer from the deficient and inaccurate tags providedby users. Inspired by the subspace clustering methods, we formulate the tagcompletion problem in a subspace clustering model which assumes that images aresampled from subspaces, and complete the tags using the state-of-the-art LowRank Representation (LRR) method. And we propose a matrix completion algorithmto further refine the tags. Our empirical results on multiple benchmarkdatasets for image annotation show that the proposed algorithm outperformsstate-of-the-art approaches when handling missing and noisy tags.
arxiv-12000-140 | Wide baseline stereo matching with convex bounded-distortion constraints | http://arxiv.org/pdf/1506.03301v1.pdf | author:Meirav Galun, Tal Amir, Tal Hassner, Ronen Basri, Yaron Lipman category:cs.CV published:2015-06-10 summary:Finding correspondences in wide baseline setups is a challenging problem.Existing approaches have focused largely on developing better featuredescriptors for correspondence and on accurate recovery of epipolar lineconstraints. This paper focuses on the challenging problem of findingcorrespondences once approximate epipolar constraints are given. We introduce anovel method that integrates a deformation model. Specifically, we formulatethe problem as finding the largest number of corresponding points related by abounded distortion map that obeys the given epipolar constraints. We show that,while the set of bounded distortion maps is not convex, the subset of maps thatobey the epipolar line constraints is convex, allowing us to introduce anefficient algorithm for matching. We further utilize a robust cost function formatching and employ majorization-minimization for its optimization. Ourexperiments indicate that our method finds significantly more accurate mapsthan existing approaches.
arxiv-12000-141 | Optical Flow on Evolving Sphere-Like Surfaces | http://arxiv.org/pdf/1506.03358v1.pdf | author:Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2015-06-10 summary:In this work we consider optical flow on evolving Riemannian 2-manifoldswhich can be parametrised from the 2-sphere. Our main motivation is to estimatecell motion in time-lapse volumetric microscopy images depicting fluorescentlylabelled cells of a live zebrafish embryo. We exploit the fact that therecorded cells float on the surface of the embryo and allow for the extractionof an image sequence together with a sphere-like surface. We solve theresulting variational problem by means of a Galerkin method based on vectorspherical harmonics and present numerical results computed from theaforementioned microscopy data.
arxiv-12000-142 | Permutation Search Methods are Efficient, Yet Faster Search is Possible | http://arxiv.org/pdf/1506.03163v3.pdf | author:Bilegsaikhan Naidan, Leonid Boytsov, Eric Nyberg category:cs.LG cs.DB cs.DS published:2015-06-10 summary:We survey permutation-based methods for approximate k-nearest neighborsearch. In these methods, every data point is represented by a ranked list ofpivots sorted by the distance to this point. Such ranked lists are calledpermutations. The underpinning assumption is that, for both metric andnon-metric spaces, the distance between permutations is a good proxy for thedistance between original points. Thus, it should be possible to efficientlyretrieve most true nearest neighbors by examining only a tiny subset of datapoints whose permutations are similar to the permutation of a query. We furthertest this assumption by carrying out an extensive experimental evaluation wherepermutation methods are pitted against state-of-the art benchmarks (themulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a varietyof realistically large data set from the image and textual domain. The focus ison the high-accuracy retrieval methods for generic spaces. Additionally, weassume that both data and indices are stored in main memory. We findpermutation methods to be reasonably efficient and describe a setup where thesemethods are most useful. To ease reproducibility, we make our software and datasets publicly available.
arxiv-12000-143 | Genetic Algorithms for multimodal optimization: a review | http://arxiv.org/pdf/1508.05342v1.pdf | author:Noe Casas category:cs.NE published:2015-06-10 summary:In this article we provide a comprehensive review of the differentevolutionary algorithm techniques used to address multimodal optimizationproblems, classifying them according to the nature of their approach. On theone hand there are algorithms that address the issue of the early convergenceto a local optimum by differentiating the individuals of the population intogroups and limiting their interaction, hence having each group evolve with ahigh degree of independence. On the other hand other approaches are based ondirectly addressing the lack of genetic diversity of the population byintroducing elements into the evolutionary dynamics that promote new niches ofthe genotypical space to be explored. Finally, we study multi-objectiveoptimization genetic algorithms, that handle the situations where multiplecriteria have to be satisfied with no penalty for any of them. Very richliterature has arised over the years on these topics, and we aim at offering anoverview of the most important techniques of each branch of the field.
arxiv-12000-144 | Contextual Bandits with Global Constraints and Objective | http://arxiv.org/pdf/1506.03374v1.pdf | author:Shipra Agrawal, Nikhil R. Devanur, Lihong Li category:cs.LG cs.AI stat.ML published:2015-06-10 summary:We consider the contextual version of a multi-armed bandit problem withglobal convex constraints and concave objective function. In each round, theoutcome of pulling an arm is a context-dependent vector, and the globalconstraints require the average of these vectors to lie in a certain convexset. The objective is a concave function of this average vector. The learningagent competes with an arbitrary set of context-dependent policies. Thisproblem is a common generalization of problems considered by Badanidiyuru etal. (2014) and Agrawal and Devanur (2014), with important applications. We givecomputationally efficient algorithms with near-optimal regret, generalizing theapproach of Agarwal et al. (2014) for the non-constrained version of theproblem. For the special case of budget constraints our regret bounds matchthose of Badanidiyuru et al. (2014), answering their main open question ofobtaining a computationally efficient algorithm.
arxiv-12000-145 | On the Prior Sensitivity of Thompson Sampling | http://arxiv.org/pdf/1506.03378v1.pdf | author:Che-Yu Liu, Lihong Li category:cs.LG cs.AI stat.ML published:2015-06-10 summary:The empirically successful Thompson Sampling algorithm for stochastic banditshas drawn much interest in understanding its theoretical properties. Oneimportant benefit of the algorithm is that it allows domain knowledge to beconveniently encoded as a prior distribution to balance exploration andexploitation more effectively. While it is generally believed that thealgorithm's regret is low (high) when the prior is good (bad), little is knownabout the exact dependence. In this paper, we fully characterize thealgorithm's worst-case dependence of regret on the choice of prior, focusing ona special yet representative case. These results also provide insights into thegeneral sensitivity of the algorithm to the choice of priors. In particular,with $p$ being the prior probability mass of the true reward-generating model,we prove $O(\sqrt{T/p})$ and $O(\sqrt{(1-p)T})$ regret upper bounds for thebad- and good-prior cases, respectively, as well as \emph{matching} lowerbounds. Our proofs rely on the discovery of a fundamental property of ThompsonSampling and make heavy use of martingale theory, both of which appear novel inthe literature, to the best of our knowledge.
arxiv-12000-146 | Memory and information processing in neuromorphic systems | http://arxiv.org/pdf/1506.03264v1.pdf | author:Giacomo Indiveri, Shih-Chii Liu category:cs.NE published:2015-06-10 summary:A striking difference between brain-inspired neuromorphic processors andcurrent von Neumann processors architectures is the way in which memory andprocessing is organized. As Information and Communication Technologies continueto address the need for increased computational power through the increase ofcores within a digital processor, neuromorphic engineers and scientists cancomplement this need by building processor architectures where memory isdistributed with the processing. In this paper we present a survey ofbrain-inspired processor architectures that support models of cortical networksand deep neural networks. These architectures range from serial clockedimplementations of multi-neuron systems to massively parallel asynchronous onesand from purely digital systems to mixed analog/digital systems which implementmore biological-like models of neurons and synapses together with a suite ofadaptation and learning mechanisms analogous to the ones found in biologicalnervous systems. We describe the advantages of the different approaches beingpursued and present the challenges that need to be addressed for buildingartificial neural processing systems that can display the richness of behaviorsseen in biological systems.
arxiv-12000-147 | Combining Temporal Information and Topic Modeling for Cross-Document Event Ordering | http://arxiv.org/pdf/1506.03257v1.pdf | author:Borja Navarro-Colorado, Estela Saquete category:cs.CL published:2015-06-10 summary:Building unified timelines from a collection of written news articlesrequires cross-document event coreference resolution and temporal relationextraction. In this paper we present an approach event coreference resolutionaccording to: a) similar temporal information, and b) similar semanticarguments. Temporal information is detected using an automatic temporalinformation system (TIPSem), while semantic information is represented by meansof LDA Topic Modeling. The evaluation of our approach shows that it obtains thehighest Micro-average F-score results in the SemEval2015 Task 4: TimeLine:Cross-Document Event Ordering (25.36\% for TrackB, 23.15\% for SubtrackB), withan improvement of up to 6\% in comparison to the other systems. However, ourexperiment also showed some draw-backs in the Topic Modeling approach thatdegrades performance of the system.
arxiv-12000-148 | A Scale Mixture Perspective of Multiplicative Noise in Neural Networks | http://arxiv.org/pdf/1506.03208v1.pdf | author:Eric Nalisnick, Anima Anandkumar, Padhraic Smyth category:stat.ML published:2015-06-10 summary:Corrupting the input and hidden layers of deep neural networks (DNNs) withmultiplicative noise, often drawn from the Bernoulli distribution (or'dropout'), provides regularization that has significantly contributed to deeplearning's success. However, understanding how multiplicative corruptionsprevent overfitting has been difficult due to the complexity of a DNN'sfunctional form. In this paper, we show that when a Gaussian prior is placed ona DNN's weights, applying multiplicative noise induces a Gaussian scalemixture, which can be reparameterized to circumvent the problematic likelihoodfunction. Analysis can then proceed by using a type-II maximum likelihoodprocedure to derive a closed-form expression revealing how regularizationevolves as a function of the network's weights. Results show thatmultiplicative noise forces weights to become either sparse or invariant torescaling. We find our analysis has implications for model compression as itnaturally reveals a weight pruning rule that starkly contrasts with thecommonly used signal-to-noise ratio (SNR). While the SNR prunes weights withlarge variances, seeing them as noisy, our approach recognizes their robustnessand retains them. We empirically demonstrate our approach has a strongadvantage over the SNR heuristic and is competitive to retraining with softtargets produced from a teacher model.
arxiv-12000-149 | Fast Online Clustering with Randomized Skeleton Sets | http://arxiv.org/pdf/1506.03425v1.pdf | author:Krzysztof Choromanski, Sanjiv Kumar, Xiaofeng Liu category:cs.AI cs.LG published:2015-06-10 summary:We present a new fast online clustering algorithm that reliably recoversarbitrary-shaped data clusters in high throughout data streams. Unlike theexisting state-of-the-art online clustering methods based on k-means ork-medoid, it does not make any restrictive generative assumptions. In addition,in contrast to existing nonparametric clustering techniques such as DBScan orDenStream, it gives provable theoretical guarantees. To achieve fastclustering, we propose to represent each cluster by a skeleton set which isupdated continuously as new data is seen. A skeleton set consists of weightedsamples from the data where weights encode local densities. The size of eachskeleton set is adapted according to the cluster geometry. The proposedtechnique automatically detects the number of clusters and is robust tooutliers. The algorithm works for the infinite data stream where more than onepass over the data is not feasible. We provide theoretical guarantees on thequality of the clustering and also demonstrate its advantage over the existingstate-of-the-art on several datasets.
arxiv-12000-150 | ICDAR 2015 Text Reading in the Wild Competition | http://arxiv.org/pdf/1506.03184v1.pdf | author:Xinyu Zhou, Shuchang Zhou, Cong Yao, Zhimin Cao, Qi Yin category:cs.CV published:2015-06-10 summary:Recently, text detection and recognition in natural scenes are becomingincreasing popular in the computer vision community as well as the documentanalysis community. However, majority of the existing ideas, algorithms andsystems are specifically designed for English. This technical report presentsthe final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015)competition, which aims at establishing a benchmark for assessing detection andrecognition algorithms devised for both Chinese and English scripts andproviding a playground for researchers from the community. In this article, wedescribe in detail the dataset, tasks, evaluation protocols and participants ofthis competition, and report the performance of the participating methods.Moreover, promising directions for future research are discussed.
arxiv-12000-151 | A Scheme for Molecular Computation of Maximum Likelihood Estimators for Log-Linear Models | http://arxiv.org/pdf/1506.03172v1.pdf | author:Manoj Gopalkrishnan category:cs.NE math.ST q-bio.MN stat.TH published:2015-06-10 summary:We propose a scheme for computing Maximum Likelihood Estimators forLog-Linear models using reaction networks, and prove its correctness. Ourscheme exploits the toric structure of equilibrium points of reaction networks.This allows an efficient encoding of the problem, and reveals how reactionnetworks are naturally suited to statistical inference tasks. Our scheme isrelevant to molecular programming, an emerging discipline that views molecularinteractions as computational primitives for the synthesis of sophisticatedbehaviors. In addition, such a scheme may provide a template to understand howbiochemical signaling pathways integrate extensive information about theirenvironment and history.
arxiv-12000-152 | Copula variational inference | http://arxiv.org/pdf/1506.03159v2.pdf | author:Dustin Tran, David M. Blei, Edoardo M. Airoldi category:stat.ML cs.LG stat.CO stat.ME published:2015-06-10 summary:We develop a general variational inference method that preserves dependencyamong the latent variables. Our method uses copulas to augment the families ofdistributions used in mean-field and structured approximations. Copulas modelthe dependency that is not captured by the original variational distribution,and thus the augmented variational family guarantees better approximations tothe posterior. With stochastic optimization, inference on the augmenteddistribution is scalable. Furthermore, our strategy is generic: it can beapplied to any inference procedure that currently uses the mean-field orstructured approach. Copula variational inference has many advantages: itreduces bias; it is less sensitive to local optima; it is less sensitive tohyperparameters; and it helps characterize and interpret the dependency amongthe latent variables.
arxiv-12000-153 | A cognitive neural architecture able to learn and communicate through natural language | http://arxiv.org/pdf/1506.03229v3.pdf | author:Bruno Golosio, Angelo Cangelosi, Olesya Gamotina, Giovanni Luca Masala category:cs.CL published:2015-06-10 summary:Communicative interactions involve a kind of procedural knowledge that isused by the human brain for processing verbal and nonverbal inputs and forlanguage production. Although considerable work has been done on modeling humanlanguage abilities, it has been difficult to bring them together to acomprehensive tabula rasa system compatible with current knowledge of howverbal information is processed in the brain. This work presents a cognitivesystem, entirely based on a large-scale neural architecture, which wasdeveloped to shed light on the procedural knowledge involved in languageelaboration. The main component of this system is the central executive, whichis a supervising system that coordinates the other components of the workingmemory. In our model, the central executive is a neural network that takes asinput the neural activation states of the short-term memory and yields asoutput mental actions, which control the flow of information among the workingmemory components through neural gating mechanisms. The proposed system iscapable of learning to communicate through natural language starting fromtabula rasa, without any a priori knowledge of the structure of phrases,meaning of words, role of the different classes of words, only by interactingwith a human through a text-based interface, using an open-ended incrementallearning process. It is able to learn nouns, verbs, adjectives, pronouns andother word classes, and to use them in expressive language. The model wasvalidated on a corpus of 1587 input sentences, based on literature on earlylanguage assessment, at the level of about 4-years old child, and produced 521output sentences, expressing a broad range of language processingfunctionalities.
arxiv-12000-154 | Robust Subgraph Generation Improves Abstract Meaning Representation Parsing | http://arxiv.org/pdf/1506.03139v1.pdf | author:Keenon Werling, Gabor Angeli, Christopher Manning category:cs.CL published:2015-06-10 summary:The Abstract Meaning Representation (AMR) is a representation for open-domainrich semantics, with potential use in fields like event extraction and machinetranslation. Node generation, typically done using a simple dictionary lookup,is currently an important limiting factor in AMR parsing. We propose a smallset of actions that derive AMR subgraphs by transformations on spans of text,which allows for more robust learning of this stage. Our set of constructionactions generalize better than the previous approach, and can be learned with asimple classifier. We improve on the previous state-of-the-art result for AMRparsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 andLDC2014T12 datasets.
arxiv-12000-155 | Convergence rates for pretraining and dropout: Guiding learning parameters using network structure | http://arxiv.org/pdf/1506.03412v2.pdf | author:Vamsi K. Ithapu, Sathya Ravi, Vikas Singh category:cs.LG cs.CV cs.NE math.OC stat.ML published:2015-06-10 summary:Unsupervised pretraining and dropout have been well studied, especially withrespect to regularization and output consistency. However, our understandingabout the explicit convergence rates of the parameter estimates, and theirdependence on the learning (like denoising and dropout rate) and structural(like depth and layer lengths) aspects of the network is less mature. Aninteresting question in this context is to ask if the network structure could"guide" the choices of such learning parameters. In this work, we explore thesegaps between network structure, the learning mechanisms and their interactionwith parameter convergence rates. We present a way to address these issuesbased on the backpropagation convergence rates for general nonconvex objectivesusing first-order information. We then incorporate two learning mechanisms intothis general framework -- denoising autoencoder and dropout, and subsequentlyderive the convergence rates of deep networks. Building upon these bounds, weprovide insights into the choices of learning parameters and network sizes thatachieve certain levels of convergence accuracy. The results derived heresupport existing empirical observations, and we also conduct a set ofexperiments to evaluate them.
arxiv-12000-156 | Neural Adaptive Sequential Monte Carlo | http://arxiv.org/pdf/1506.03338v3.pdf | author:Shixiang Gu, Zoubin Ghahramani, Richard E. Turner category:cs.LG stat.ML published:2015-06-10 summary:Sequential Monte Carlo (SMC), or particle filtering, is a popular class ofmethods for sampling from an intractable target distribution using a sequenceof simpler intermediate distributions. Like other importance sampling-basedmethods, performance is critically dependent on the proposal distribution: abad proposal can lead to arbitrarily inaccurate estimates of the targetdistribution. This paper presents a new method for automatically adapting theproposal using an approximation of the Kullback-Leibler divergence between thetrue posterior and the proposal distribution. The method is very flexible,applicable to any parameterized proposal distribution and it supports onlineand batch variants. We use the new framework to adapt powerful proposaldistributions with rich parameterizations based upon neural networks leading toNeural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMCsignificantly improves inference in a non-linear state space modeloutperforming adaptive proposal methods including the Extended Kalman andUnscented Particle Filters. Experiments also indicate that improved inferencetranslates into improved parameter learning when NASMC is used as a subroutineof Particle Marginal Metropolis Hastings. Finally we show that NASMC is able totrain a latent variable recurrent neural network (LV-RNN) achieving resultsthat compete with the state-of-the-art for polymorphic music modelling. NASMCcan be seen as bridging the gap between adaptive SMC methods and the recentwork in scalable, black-box variational inference.
arxiv-12000-157 | Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation | http://arxiv.org/pdf/1506.03500v2.pdf | author:Angeliki Lazaridou, Dat Tien Nguyen, Raffaella Bernardi, Marco Baroni category:cs.CV cs.CL published:2015-06-10 summary:We introduce language-driven image generation, the task of generating animage visualizing the semantic contents of a word embedding, e.g., given theword embedding of grasshopper, we generate a natural image of a grasshopper. Weimplement a simple method based on two mapping functions. The first takes asinput a word embedding (as produced, e.g., by the word2vec toolkit) and maps itonto a high-level visual space (e.g., the space defined by one of the toplayers of a Convolutional Neural Network). The second function maps thisabstract visual representation to pixel space, in order to generate the targetimage. Several user studies suggest that the current system produces imagesthat capture general visual properties of the concepts encoded in the wordembedding, such as color or typical environment, and are sufficient todiscriminate between general categories of objects.
arxiv-12000-158 | Generative Image Modeling Using Spatial LSTMs | http://arxiv.org/pdf/1506.03478v2.pdf | author:Lucas Theis, Matthias Bethge category:stat.ML cs.CV cs.LG published:2015-06-10 summary:Modeling the distribution of natural images is challenging, partly because ofstrong statistical dependencies which can extend over hundreds of pixels.Recurrent neural networks have been successful in capturing long-rangedependencies in a number of problems but only recently have found their wayinto generative image models. We here introduce a recurrent image model basedon multi-dimensional long short-term memory units which are particularly suitedfor image modeling due to their spatial structure. Our model scales to imagesof arbitrary size and its likelihood is computationally tractable. We find thatit outperforms the state of the art in quantitative comparisons on severalimage datasets and produces promising results when used for texture synthesisand inpainting.
arxiv-12000-159 | Parallelizing MCMC with Random Partition Trees | http://arxiv.org/pdf/1506.03164v2.pdf | author:Xiangyu Wang, Fangjian Guo, Katherine A. Heller, David B. Dunson category:stat.ML published:2015-06-10 summary:The modern scale of data has brought new challenges to Bayesian inference. Inparticular, conventional MCMC algorithms are computationally very expensive forlarge data sets. A promising approach to solve this problem is embarrassinglyparallel MCMC (EP-MCMC), which first partitions the data into multiple subsetsand runs independent sampling algorithms on each subset. The subset posteriordraws are then aggregated via some combining rules to obtain the finalapproximation. Existing EP-MCMC algorithms are limited by approximationaccuracy and difficulty in resampling. In this article, we propose a newEP-MCMC algorithm PART that solves these problems. The new algorithm appliesrandom partition trees to combine the subset posterior draws, which isdistribution-free, easy to resample from and can adapt to multiple scales. Weprovide theoretical justification and extensive experiments illustratingempirical performance.
arxiv-12000-160 | From Paraphrase Database to Compositional Paraphrase Model and Back | http://arxiv.org/pdf/1506.03487v2.pdf | author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Dan Roth category:cs.CL published:2015-06-10 summary:The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensivesemantic resource, consisting of a list of phrase pairs with (heuristic)confidence estimates. However, it is still unclear how it can best be used, dueto the heuristic nature of the confidences and its necessarily incompletecoverage. We propose models to leverage the phrase pairs from the PPDB to buildparametric paraphrase models that score paraphrase pairs more accurately thanthe PPDB's internal scores while simultaneously improving its coverage. Theyallow for learning phrase embeddings as well as improved word embeddings.Moreover, we introduce two new, manually annotated datasets to evaluateshort-phrase paraphrasing models. Using our paraphrase model trained usingPPDB, we achieve state-of-the-art results on standard word and bigramsimilarity tasks and beat strong baselines on our new short phrase paraphrasetasks.
arxiv-12000-161 | Data Generation as Sequential Decision Making | http://arxiv.org/pdf/1506.03504v3.pdf | author:Philip Bachman, Doina Precup category:cs.LG stat.ML published:2015-06-10 summary:We connect a broad class of generative models through their shared relianceon sequential decision making. Motivated by this view, we develop extensions toan existing model, and then explore the idea further in the context of dataimputation -- perhaps the simplest setting in which to investigate the relationbetween unconditional and conditional generative modelling. We formulate dataimputation as an MDP and develop models capable of representing effectivepolicies for it. We construct the models using neural networks and train themusing a form of guided policy search. Our models generate predictions throughan iterative process of feedback and refinement. We show that this approach canlearn effective policies for imputation problems of varying difficulty andacross multiple datasets.
arxiv-12000-162 | Teaching Machines to Read and Comprehend | http://arxiv.org/pdf/1506.03340v3.pdf | author:Karl Moritz Hermann, Tom√°≈° Koƒçisk√Ω, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom category:cs.CL cs.AI cs.NE published:2015-06-10 summary:Teaching machines to read natural language documents remains an elusivechallenge. Machine reading systems can be tested on their ability to answerquestions posed on the contents of documents that they have seen, but until nowlarge scale training and test datasets have been missing for this type ofevaluation. In this work we define a new methodology that resolves thisbottleneck and provides large scale supervised reading comprehension data. Thisallows us to develop a class of attention based deep neural networks that learnto read real documents and answer complex questions with minimal priorknowledge of language structure.
arxiv-12000-163 | Truthful Linear Regression | http://arxiv.org/pdf/1506.03489v1.pdf | author:Rachel Cummings, Stratis Ioannidis, Katrina Ligett category:cs.GT cs.DS stat.ML published:2015-06-10 summary:We consider the problem of fitting a linear model to data held by individualswho are concerned about their privacy. Incentivizing most players to truthfullyreport their data to the analyst constrains our design to mechanisms thatprovide a privacy guarantee to the participants; we use differential privacy tomodel individuals' privacy losses. This immediately poses a problem, asdifferentially private computation of a linear model necessarily produces abiased estimation, and existing approaches to design mechanisms to elicit datafrom privacy-sensitive individuals do not generalize well to biased estimators.We overcome this challenge through an appropriate design of the computation andpayment scheme.
arxiv-12000-164 | LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop | http://arxiv.org/pdf/1506.03365v2.pdf | author:Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, Jianxiong Xiao category:cs.CV published:2015-06-10 summary:The state-of-the-art visual recognition algorithms are all data-hungry,requiring a huge amount of labeled image data to optimize millions ofparameters. While there has been remarkable progress in algorithm and systemdesign, the labeled datasets used by these models are quickly becoming outdatedin terms of size. To overcome the bottleneck of human labeling speed duringdataset construction, we propose to amplify human effort using deep learningwith humans in the loop. Our procedure comes equipped with precision and recallguarantees to ensure labeling quality, reaching the same level of performanceas fully manual annotation. To demonstrate the power of our annotationprocedure and enable further progress of visual recognition, we construct ascene-centric database called "LSUN" containing millions of labeled images ineach scene category. We experiment with popular deep nets using our dataset andobtain a substantial performance gain with the same model trained using ourlarger training set. All data and source code will be available online uponacceptance of the paper.
arxiv-12000-165 | Explore no more: Improved high-probability regret bounds for non-stochastic bandits | http://arxiv.org/pdf/1506.03271v3.pdf | author:Gergely Neu category:cs.LG stat.ML published:2015-06-10 summary:This work addresses the problem of regret minimization in non-stochasticmulti-armed bandit problems, focusing on performance guarantees that hold withhigh probability. Such results are rather scarce in the literature sinceproving them requires a large deal of technical effort and significantmodifications to the standard, more intuitive algorithms that come only withguarantees that hold on expectation. One of these modifications is forcing thelearner to sample arms from the uniform distribution at least$\Omega(\sqrt{T})$ times over $T$ rounds, which can adversely affectperformance if many of the arms are suboptimal. While it is widely conjecturedthat this property is essential for proving high-probability regret bounds, weshow in this paper that it is possible to achieve such strong results withoutthis undesirable exploration component. Our result relies on a simple andintuitive loss-estimation strategy called Implicit eXploration (IX) that allowsa remarkably clean analysis. To demonstrate the flexibility of our technique,we derive several improved high-probability bounds for various extensions ofthe standard multi-armed bandit framework. Finally, we conduct a simpleexperiment that illustrates the robustness of our implicit explorationtechnique.
arxiv-12000-166 | The Online Coupon-Collector Problem and Its Application to Lifelong Reinforcement Learning | http://arxiv.org/pdf/1506.03379v2.pdf | author:Emma Brunskill, Lihong Li category:cs.LG cs.AI published:2015-06-10 summary:Transferring knowledge across a sequence of related tasks is an importantchallenge in reinforcement learning (RL). Despite much encouraging empiricalevidence, there has been little theoretical analysis. In this paper, we study aclass of lifelong RL problems: the agent solves a sequence of tasks modeled asfinite Markov decision processes (MDPs), each of which is from a finite set ofMDPs with the same state/action sets and different transition/reward functions.Motivated by the need for cross-task exploration in lifelong learning, weformulate a novel online coupon-collector problem and give an optimalalgorithm. This allows us to develop a new lifelong RL algorithm, whose overallsample complexity in a sequence of tasks is much smaller than single-tasklearning, even if the sequence of tasks is generated by an adversary. Benefitsof the algorithm are demonstrated in simulated problems, including a recentlyintroduced human-robot interaction problem.
arxiv-12000-167 | Convolutional Dictionary Learning through Tensor Factorization | http://arxiv.org/pdf/1506.03509v3.pdf | author:Furong Huang, Animashree Anandkumar category:cs.LG stat.ML published:2015-06-10 summary:Tensor methods have emerged as a powerful paradigm for consistent learning ofmany latent variable models such as topic models, independent componentanalysis and dictionary learning. Model parameters are estimated via CPdecomposition of the observed higher order input moments. However, in manydomains, additional invariances such as shift invariances exist, enforced viamodels such as convolutional dictionary learning. In this paper, we developnovel tensor decomposition algorithms for parameter estimation of convolutionalmodels. Our algorithm is based on the popular alternating least squares method,but with efficient projections onto the space of stacked circulant matrices.Our method is embarrassingly parallel and consists of simple operations such asfast Fourier transforms and matrix multiplications. Our algorithm converges tothe dictionary much faster and more accurately compared to the alternatingminimization over filters and activation maps.
arxiv-12000-168 | Automatic Variational Inference in Stan | http://arxiv.org/pdf/1506.03431v2.pdf | author:Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, David M. Blei category:stat.ML published:2015-06-10 summary:Variational inference is a scalable technique for approximate Bayesianinference. Deriving variational inference algorithms requires tediousmodel-specific calculations; this makes it difficult to automate. We propose anautomatic variational inference algorithm, automatic differentiationvariational inference (ADVI). The user only provides a Bayesian model and adataset; nothing else. We make no conjugacy assumptions and support a broadclass of models. The algorithm automatically determines an appropriatevariational family and optimizes the variational objective. We implement ADVIin Stan (code available now), a probabilistic programming framework. We compareADVI to MCMC sampling across hierarchical generalized linear models,nonconjugate matrix factorization, and a mixture model. We train the mixturemodel on a quarter million images. With ADVI we can use variational inferenceon any model we write in Stan.
arxiv-12000-169 | BoWFire: Detection of Fire in Still Images by Integrating Pixel Color and Texture Analysis | http://arxiv.org/pdf/1506.03495v1.pdf | author:Daniel Y. T. Chino, Letricia P. S. Avalhais, Jose F. Rodrigues Jr., Agma J. M. Traina category:cs.CV published:2015-06-10 summary:Emergency events involving fire are potentially harmful, demanding a fast andprecise decision making. The use of crowdsourcing image and videos on crisismanagement systems can aid in these situations by providing more informationthan verbal/textual descriptions. Due to the usual high volume of data,automatic solutions need to discard non-relevant content without losingrelevant information. There are several methods for fire detection on videousing color-based models. However, they are not adequate for still imageprocessing, because they can suffer on high false-positive results. Thesemethods also suffer from parameters with little physical meaning, which makesfine tuning a difficult task. In this context, we propose a novel firedetection method for still images that uses classification based on colorfeatures combined with texture classification on superpixel regions. Our methoduses a reduced number of parameters if compared to previous works, easing theprocess of fine tuning the method. Results show the effectiveness of our methodof reducing false-positives while its precision remains compatible with thestate-of-the-art methods.
arxiv-12000-170 | Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts | http://arxiv.org/pdf/1506.03493v1.pdf | author:Aaron Schein, John Paisley, David M. Blei, Hanna Wallach category:stat.ML cs.AI cs.LG cs.SI stat.AP published:2015-06-10 summary:We present a Bayesian tensor factorization model for inferring latent groupstructures from dynamic pairwise interaction patterns. For decades, politicalscientists have collected and analyzed records of the form "country $i$ tookaction $a$ toward country $j$ at time $t$"---known as dyadic events---in orderto form and test theories of international relations. We represent these eventdata as a tensor of counts and develop Bayesian Poisson tensor factorization toinfer a low-dimensional, interpretable representation of their salientpatterns. We demonstrate that our model's predictive performance is better thanthat of standard non-negative tensor factorization methods. We also provide acomparison of our variational updates to their maximum likelihood counterparts.In doing so, we identify a better way to form point estimates of the latentfactors than that typically used in Bayesian Poisson matrix factorization.Finally, we showcase our model as an exploratory analysis tool for politicalscientists. We show that the inferred latent factor matrices captureinterpretable multilateral relations that both conform to and inform ourknowledge of international affairs.
arxiv-12000-171 | Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via Thresholded Wirtinger Flow | http://arxiv.org/pdf/1506.03382v1.pdf | author:T. Tony Cai, Xiaodong Li, Zongming Ma category:math.ST cs.IT math.IT math.NA stat.ML stat.TH published:2015-06-10 summary:This paper considers the noisy sparse phase retrieval problem: recovering asparse signal $x \in \mathbb{R}^p$ from noisy quadratic measurements $y_j =(a_j' x )^2 + \epsilon_j$, $j=1, \ldots, m$, with independent sub-exponentialnoise $\epsilon_j$. The goals are to understand the effect of the sparsity of$x$ on the estimation precision and to construct a computationally feasibleestimator to achieve the optimal rates. Inspired by the Wirtinger Flow [12]proposed for noiseless and non-sparse phase retrieval, a novel thresholdedgradient descent algorithm is proposed and it is shown to adaptively achievethe minimax optimal rates of convergence over a wide range of sparsity levelswhen the $a_j$'s are independent standard Gaussian random vectors, providedthat the sample size is sufficiently large compared to the sparsity of $x$.
arxiv-12000-172 | Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation | http://arxiv.org/pdf/1506.03498v3.pdf | author:Alaa Saade, Florent Krzakala, Lenka Zdeborov√° category:cs.LG stat.ML published:2015-06-10 summary:The completion of low rank matrices from few entries is a task with manypractical applications. We consider here two aspects of this problem:detectability, i.e. the ability to estimate the rank $r$ reliably from thefewest possible random entries, and performance in achieving smallreconstruction error. We propose a spectral algorithm for these two taskscalled MaCBetH (for Matrix Completion with the Bethe Hessian). The rank isestimated as the number of negative eigenvalues of the Bethe Hessian matrix,and the corresponding eigenvectors are used as initial condition for theminimization of the discrepancy between the estimated matrix and the revealedentries. We analyze the performance in a random matrix setting using resultsfrom the statistical mechanics of the Hopfield neural network, and show inparticular that MaCBetH efficiently detects the rank $r$ of a large $n\times m$matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.We also evaluate the corresponding root-mean-square error empirically and showthat MaCBetH compares favorably to other existing approaches.
arxiv-12000-173 | Randomer Forests | http://arxiv.org/pdf/1506.03410v2.pdf | author:Tyler M. Tomita, Mauro Maggioni, Joshua T. Vogelstein category:stat.ML cs.LG 68T10 I.5.2 published:2015-06-10 summary:Random forests (RF) is a popular general purpose classifier that has beenshown to outperform many other classifiers on a variety of datasets. Thewidespread use of random forests can be attributed to several factors, some ofwhich include its excellent empirical performance, scale and unit invariance,robustness to outliers, time and space complexity, and interpretability. WhileRF has many desirable qualities, one drawback is its sensitivity to rotationsand other operations that "mix" variables. In this work, we establish ageneralized forest building scheme, linear threshold forests. Random forestsand many other currently existing decision forest algorithms can be viewed asspecial cases of this scheme. With this scheme in mind, we propose a fewspecial cases which we call randomer forests (RerFs). RerFs are linearthreshold forest that exhibit all of the nice properties of RF, in addition toapproximate affine invariance. In simulated datasets designed for RF to dowell, we demonstrate that RerF outperforms RF. We also demonstrate that oneparticular variant of RerF is approximately affine invariant. Lastly, in anevaluation on 121 benchmark datasets, we observe that RerF outperforms RF. Wetherefore putatively propose that RerF be considered a replacement for RF asthe general purpose classifier of choice. Open source code is available athttp://ttomita.github.io/RandomerForest/.
arxiv-12000-174 | A review of landmark articles in the field of co-evolutionary computing | http://arxiv.org/pdf/1506.05082v2.pdf | author:Noe Casas category:cs.NE published:2015-06-10 summary:Coevolution is a powerful tool in evolutionary computing that mitigates someof its endemic problems, namely stagnation in local optima and lack ofconvergence in high dimensionality problems. Since its inception in 1990, thereare multiple articles that have contributed greatly to the development andimprovement of the coevolutionary techniques. In this report we review some ofthose landmark articles dwelving in the techniques they propose and how theyfit to conform robust evolutionary algorithms
arxiv-12000-175 | Sequential Nonparametric Testing with the Law of the Iterated Logarithm | http://arxiv.org/pdf/1506.03486v2.pdf | author:Akshay Balsubramani, Aaditya Ramdas category:stat.ML cs.LG math.ST stat.ME stat.TH published:2015-06-10 summary:We propose a new algorithmic framework for sequential hypothesis testing withi.i.d. data, which includes A/B testing, nonparametric two-sample testing, andindependence testing as special cases. It is novel in several ways: (a) ittakes linear time and constant space to compute on the fly, (b) it has the samepower guarantee as a non-sequential version of the test with the samecomputational constraints up to a small factor, and (c) it accesses only asmany samples as are required - its stopping time adapts to the unknowndifficulty of the problem. All our test statistics are constructed to bezero-mean martingales under the null hypothesis, and the rejection threshold isgoverned by a uniform non-asymptotic law of the iterated logarithm (LIL). Forthe case of nonparametric two-sample mean testing, we also provide a finitesample power analysis, and the first non-asymptotic stopping time calculationsfor this class of problems. We verify our predictions for type I and II errorsand stopping times using simulations.
arxiv-12000-176 | Clustering by transitive propagation | http://arxiv.org/pdf/1506.03072v1.pdf | author:Vijay Kumar, Dan Levy category:cs.LG stat.ML published:2015-06-09 summary:We present a global optimization algorithm for clustering data given theratio of likelihoods that each pair of data points is in the same cluster or indifferent clusters. To define a clustering solution in terms of pairwiserelationships, a necessary and sufficient condition is that belonging to thesame cluster satisfies transitivity. We define a global objective functionbased on pairwise likelihood ratios and a transitivity constraint over alltriples, assigning an equal prior probability to all clustering solutions. Wemaximize the objective function by implementing max-sum message passing on thecorresponding factor graph to arrive at an O(N^3) algorithm. Lastly, wedemonstrate an application inspired by mutational sequencing for decodingrandom binary words transmitted through a noisy channel.
arxiv-12000-177 | The Wreath Process: A totally generative model of geometric shape based on nested symmetries | http://arxiv.org/pdf/1506.03041v1.pdf | author:Diana Borsa, Thore Graepel, Andrew Gordon category:cs.AI stat.ML 20-XX published:2015-06-09 summary:We consider the problem of modelling noisy but highly symmetric shapes thatcan be viewed as hierarchies of whole-part relationships in which higher levelobjects are composed of transformed collections of lower level objects. To thisend, we propose the stochastic wreath process, a fully generative probabilisticmodel of drawings. Following Leyton's "Generative Theory of Shape", werepresent shapes as sequences of transformation groups composed through awreath product. This representation emphasizes the maximization of transfer --- the idea thatthe most compact and meaningful representation of a given shape is achieved bymaximizing the re-use of existing building blocks or parts. The proposed stochastic wreath process extends Leyton's theory by defining aprobability distribution over geometric shapes in terms of noise processes thatare aligned with the generative group structure of the shape. We propose aninference scheme for recovering the generative history of given images in termsof the wreath process using reversible jump Markov chain Monte Carlo methodsand Approximate Bayesian Computation. In the context of sketching wedemonstrate the feasibility and limitations of this approach on model-generatedand real data.
arxiv-12000-178 | Variational consensus Monte Carlo | http://arxiv.org/pdf/1506.03074v1.pdf | author:Maxim Rabinovich, Elaine Angelino, Michael I. Jordan category:stat.ML stat.CO published:2015-06-09 summary:Practitioners of Bayesian statistics have long depended on Markov chain MonteCarlo (MCMC) to obtain samples from intractable posterior distributions.Unfortunately, MCMC algorithms are typically serial, and do not scale to thelarge datasets typical of modern machine learning. The recently proposedconsensus Monte Carlo algorithm removes this limitation by partitioning thedata and drawing samples conditional on each partition in parallel (Scott etal, 2013). A fixed aggregation function then combines these samples, yieldingapproximate posterior samples. We introduce variational consensus Monte Carlo(VCMC), a variational Bayes algorithm that optimizes over aggregation functionsto obtain samples from a distribution that better approximates the target. Theresulting objective contains an intractable entropy term; we therefore derive arelaxation of the objective and show that the relaxed problem is blockwiseconcave under mild conditions. We illustrate the advantages of our algorithm onthree inference tasks from the literature, demonstrating both the superiorquality of the posterior approximation and the moderate overhead of theoptimization step. Our algorithm achieves a relative error reduction (measuredagainst serial MCMC) of up to 39% compared to consensus Monte Carlo on the taskof estimating 300-dimensional probit regression parameter expectations;similarly, it achieves an error reduction of 92% on the task of estimatingcluster comembership probabilities in a Gaussian mixture model with 8components in 8 dimensions. Furthermore, these gains come at moderate costcompared to the runtime of serial MCMC, achieving near-ideal speedup in someinstances.
arxiv-12000-179 | On the Uniform Convergence of Consistent Confidence Measures | http://arxiv.org/pdf/1506.03018v1.pdf | author:Yihan Gao, Aditya Parameswaran category:cs.LG published:2015-06-09 summary:Many classification algorithms produce confidence measures in the form ofconditional probability of labels given the features of the target instance. Itis desirable to be make these confidence measures calibrated or consistent, inthe sense that they correctly capture the belief of the algorithm in the labeloutput. For instance, if the algorithm outputs a label with confidence measure$p$ for $n$ times, then the output label should be correct approximately $np$times overall. Calibrated confidence measures lead to higher interpretabilityby humans and computers and enable downstream analysis or processing. In thispaper, we formally characterize the consistency of confidence measures andprove a PAC-style uniform convergence result for the consistency of confidencemeasures. We show that finite VC-dimension is sufficient for guaranteeing theconsistency of confidence measures produced by empirically consistentclassifiers. Our result also implies that we can calibrate confidence measuresproduced by any existing algorithms with monotonic functions, and still get thesame generalization guarantee on consistency.
arxiv-12000-180 | Active Sets Improves Learning for Mixture Models | http://arxiv.org/pdf/1506.02975v1.pdf | author:Vincent Zhao, Steven Zucker category:stat.ML cs.LG q-bio.QM published:2015-06-09 summary:We develop an algorithm to learn Bernoulli Mixture Models based on theprinciple that some variables are more informative than others. Working from aninformation-theoretic perspective, we propose both backward and forward schemesfor selecting the informative 'active' variables and using them to guide EM.The result is a stagewise EM algorithm, analogous to stagewise approaches tolinear regression, that should be applicable to neuroscience (and other)datasets with confounding (or irrelevant) variables. Results on synthetic andMNIST datasets illustrate the approach.
arxiv-12000-181 | Compact Shape Trees: A Contribution to the Forest of Shape Correspondences and Matching Methods | http://arxiv.org/pdf/1506.02923v1.pdf | author:Abdulrahman Oladipupo Ibraheem category:cs.CV published:2015-06-09 summary:We propose a novel technique, termed compact shape trees, for computingcorrespondences of single-boundary 2-D shapes in O(n2) time. Together with zeroor more features defined at each of n sample points on the shape's boundary,the compact shape tree of a shape comprises the O(n) collection of vectorsemanating from any of the sample points on the shape's boundary to the rest ofthe sample points on the boundary. As it turns out, compact shape trees have anumber of elegant properties both in the spatial and frequency domains. Inparticular, via a simple vector-algebraic argument, we show that the O(n)collection of vectors in a compact shape tree possesses at least the samediscriminatory power as the O(n2) collection of lines emanating from eachsample point to every other sample point on a shape's boundary. In addition, wedescribe neat approaches for achieving scale and rotation invariance withcompact shape trees in the spatial domain; by viewing compact shape trees asaperiodic discrete signals, we also prove scale and rotation invarianceproperties for them in the Fourier domain. Towards these, along the way, usingconcepts from differential geometry and the Calculus, we propose a novel theoryfor sampling 2-D shape boundaries in a scale and rotation invariant manner.Finally, we propose a number of shape recognition experiments to test theefficacy of our concept.
arxiv-12000-182 | An Ensemble method for Content Selection for Data-to-text Systems | http://arxiv.org/pdf/1506.02922v1.pdf | author:Dimitra Gkatzia, Helen Hastie category:cs.CL cs.AI published:2015-06-09 summary:We present a novel approach for automatic report generation from time-seriesdata, in the context of student feedback generation. Our proposed methodologytreats content selection as a multi-label classification (MLC) problem, whichtakes as input time-series data (students' learning data) and outputs a summaryof these data (feedback). Unlike previous work, this method considers all datasimultaneously using ensembles of classifiers, and therefore, it achieveshigher accuracy and F- score compared to meaningful baselines.
arxiv-12000-183 | Multiscale edge detection and parametric shape modeling for boundary delineation in optoacoustic images | http://arxiv.org/pdf/1506.03124v1.pdf | author:Subhamoy Mandal, Viswanath Pamulakanty Sudarshan, Yeshaswini Nagaraj, Xose Luis Dean Ben, Daniel Razansky category:physics.med-ph cs.CV published:2015-06-09 summary:In this article, we present a novel scheme for segmenting the image boundary(with the background) in optoacoustic small animal in vivo imaging systems. Themethod utilizes a multiscale edge detection algorithm to generate a binary edgemap. A scale dependent morphological operation is employed to clean spuriousedges. Thereafter, an ellipse is fitted to the edge map through constrainedparametric transformations and iterative goodness of fit calculations. Themethod delimits the tissue edges through the curve fitting model, which hasshown high levels of accuracy. Thus, this method enables segmentation ofoptoacoutic images with minimal human intervention, by eliminating need ofscale selection for multiscale processing and seed point determination forcontour mapping.
arxiv-12000-184 | Inverting Visual Representations with Convolutional Networks | http://arxiv.org/pdf/1506.02753v4.pdf | author:Alexey Dosovitskiy, Thomas Brox category:cs.NE cs.CV cs.LG published:2015-06-09 summary:Feature representations, both hand-designed and learned ones, are often hardto analyze and interpret, even when they are extracted from visual data. Wepropose a new approach to study image representations by inverting them with anup-convolutional neural network. We apply the method to shallow representations(HOG, SIFT, LBP), as well as to deep networks. For shallow representations ourapproach provides significantly better reconstructions than existing methods,revealing that there is surprisingly rich information contained in thesefeatures. Inverting a deep network trained on ImageNet provides severalinsights into the properties of the feature representation learned by thenetwork. Most strikingly, the colors and the rough contours of an image can bereconstructed from activations in higher network layers and even from thepredicted class probabilities.
arxiv-12000-185 | On the Error of Random Fourier Features | http://arxiv.org/pdf/1506.02785v1.pdf | author:Dougal J. Sutherland, Jeff Schneider category:cs.LG stat.ML published:2015-06-09 summary:Kernel methods give powerful, flexible, and theoretically grounded approachesto solving many problems in machine learning. The standard approach, however,requires pairwise evaluations of a kernel function, which can lead toscalability issues for very large datasets. Rahimi and Recht (2007) suggested apopular approach to handling this problem, known as random Fourier features.The quality of this approximation, however, is not well understood. We improvethe uniform error bound of that paper, as well as giving novel understandingsof the embedding's variance, approximation error, and use in some machinelearning methods. We also point out that surprisingly, of the two main variantsof those features, the more widely used is strictly higher-variance for theGaussian kernel and has worse bounds.
arxiv-12000-186 | Fast Geometric Fit Algorithm for Sphere Using Exact Solution | http://arxiv.org/pdf/1506.02776v1.pdf | author:Sumith YD category:cs.CV published:2015-06-09 summary:Sphere fitting is a common problem in almost all science and engineeringdisciplines. Most of methods available are iterative in behavior. This involvesfitting of the parameters in a least square sense or in a geometric sense. Herewe extend the methods of Thomas Chan and Landau who fitted the 2D data usingcircle. This work closely resemble their work in redefining the error estimateand solving the sphere fitting problem exactly. The solutions for center andradius of the sphere can be found exactly and the equations can be hard codedfor high performance. We have also shown some comparison with other popularmethods and how this method behaves.
arxiv-12000-187 | Accelerated Stochastic Gradient Descent for Minimizing Finite Sums | http://arxiv.org/pdf/1506.03016v2.pdf | author:Atsushi Nitanda category:stat.ML cs.LG published:2015-06-09 summary:We propose an optimization method for minimizing the finite sums of smoothconvex functions. Our method incorporates an accelerated gradient descent (AGD)and a stochastic variance reduction gradient (SVRG) in a mini-batch setting.Unlike SVRG, our method can be directly applied to non-strongly and stronglyconvex problems. We show that our method achieves a lower overall complexitythan the recently proposed methods that supports non-strongly convex problems.Moreover, this method has a fast rate of convergence for strongly convexproblems. Our experiments show the effectiveness of our method.
arxiv-12000-188 | Self Organizing Maps Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using Conditional Rotations | http://arxiv.org/pdf/1506.02750v1.pdf | author:C√©sar A. Astudillo, B. John Oommen category:cs.NE cs.AI published:2015-06-09 summary:Numerous variants of Self-Organizing Maps (SOMs) have been proposed in theliterature, including those which also possess an underlying structure, and insome cases, this structure itself can be defined by the user Although theconcepts of growing the SOM and updating it have been studied, the whole issueof using a self-organizing Adaptive Data Structure (ADS) to further enhance theproperties of the underlying SOM, has been unexplored. In an earlier work, weimpose an arbitrary, user-defined, tree-like topology onto the codebooks, whichconsequently enforced a neighborhood phenomenon and the so-called tree-basedBubble of Activity. In this paper, we consider how the underlying tree itselfcan be rendered dynamic and adaptively transformed. To do this, we presentmethods by which a SOM with an underlying Binary Search Tree (BST) structurecan be adaptively re-structured using Conditional Rotations (CONROT). Theserotations on the nodes of the tree are local, can be done in constant time, andperformed so as to decrease the Weighted Path Length (WPL) of the entire tree.In doing this, we introduce the pioneering concept referred to as NeuralPromotion, where neurons gain prominence in the Neural Network (NN) as theirsignificance increases. We are not aware of any research which deals with theissue of Neural Promotion. The advantages of such a scheme is that the userneed not be aware of any of the topological peculiarities of the stochasticdata distribution. Rather, the algorithm, referred to as the TTOSOM withConditional Rotations (TTOCONROT), converges in such a manner that the neuronsare ultimately placed in the input space so as to represent its stochasticdistribution, and additionally, the neighborhood properties of the neurons suitthe best BST that represents the data. These properties have been confirmed byour experimental results on a variety of data sets.
arxiv-12000-189 | Pointer Networks | http://arxiv.org/pdf/1506.03134v1.pdf | author:Oriol Vinyals, Meire Fortunato, Navdeep Jaitly category:stat.ML cs.CG cs.LG cs.NE published:2015-06-09 summary:We introduce a new neural architecture to learn the conditional probabilityof an output sequence with elements that are discrete tokens corresponding topositions in an input sequence. Such problems cannot be trivially addressed byexistent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in each step of the output depends on thelength of the input, which is variable. Problems such as sorting variable sizedsequences, and various combinatorial optimization problems belong to thisclass. Our model solves the problem of variable size output dictionaries usinga recently proposed mechanism of neural attention. It differs from the previousattention attempts in that, instead of using attention to blend hidden units ofan encoder to a context vector at each decoder step, it uses attention as apointer to select a member of the input sequence as the output. We call thisarchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learnapproximate solutions to three challenging geometric problems -- finding planarconvex hulls, computing Delaunay triangulations, and the planar TravellingSalesman Problem -- using training examples alone. Ptr-Nets not only improveover sequence-to-sequence with input attention, but also allow us to generalizeto variable size output dictionaries. We show that the learnt models generalizebeyond the maximum lengths they were trained on. We hope our results on thesetasks will encourage a broader exploration of neural learning for discreteproblems.
arxiv-12000-190 | Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy | http://arxiv.org/pdf/1506.02914v2.pdf | author:Marylou Gabri√©, Eric W. Tramel, Florent Krzakala category:cs.LG cs.NE stat.ML published:2015-06-09 summary:Restricted Boltzmann machines are undirected neural networks which have beenshown to be effective in many applications, including serving asinitializations for training deep multi-layer neural networks. One of the mainreasons for their success is the existence of efficient and practicalstochastic algorithms, such as contrastive divergence, for unsupervisedtraining. We propose an alternative deterministic iterative procedure based onan improved mean field method from statistical physics known as theThouless-Anderson-Palmer approach. We demonstrate that our algorithm providesperformance equal to, and sometimes superior to, persistent contrastivedivergence, while also providing a clear and easy to evaluate objectivefunction. We believe that this strategy can be easily generalized to othermodels as well as to more accurate higher-order approximations, paving the wayfor systematic improvements in training Boltzmann machines with hidden units.
arxiv-12000-191 | Deep SimNets | http://arxiv.org/pdf/1506.03059v2.pdf | author:Nadav Cohen, Or Sharir, Amnon Shashua category:cs.NE cs.LG published:2015-06-09 summary:We present a deep layered architecture that generalizes convolutional neuralnetworks (ConvNets). The architecture, called SimNets, is driven by twooperators: (i) a similarity function that generalizes inner-product, and (ii) alog-mean-exp function called MEX that generalizes maximum and average. The twooperators applied in succession give rise to a standard neuron but in "featurespace". The feature spaces realized by SimNets depend on the choice of thesimilarity operator. The simplest setting, which corresponds to a convolution,realizes the feature space of the Exponential kernel, while other settingsrealize feature spaces of more powerful kernels (Generalized Gaussian, whichincludes as special cases RBF and Laplacian), or even dynamically learnedfeature spaces (Generalized Multiple Kernel Learning). As a result, the SimNetcontains a higher abstraction level compared to a traditional ConvNet. We arguethat enhanced expressiveness is important when the networks are small due torun-time constraints (such as those imposed by mobile applications). Empiricalevaluation validates the superior expressiveness of SimNets, showing asignificant gain in accuracy over ConvNets when computational resources atrun-time are limited. We also show that in large-scale settings, wherecomputational complexity is less of a concern, the additional capacity ofSimNets can be controlled with proper regularization, yielding accuraciescomparable to state of the art ConvNets.
arxiv-12000-192 | Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks | http://arxiv.org/pdf/1506.03099v3.pdf | author:Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer category:cs.LG cs.CL cs.CV published:2015-06-09 summary:Recurrent Neural Networks can be trained to produce sequences of tokens givensome input, as exemplified by recent results in machine translation and imagecaptioning. The current approach to training them consists of maximizing thelikelihood of each token in the sequence given the current (recurrent) stateand the previous token. At inference, the unknown previous token is thenreplaced by a token generated by the model itself. This discrepancy betweentraining and inference can yield errors that can accumulate quickly along thegenerated sequence. We propose a curriculum learning strategy to gently changethe training process from a fully guided scheme using the true previous token,towards a less guided scheme which mostly uses the generated token instead.Experiments on several sequence prediction tasks show that this approach yieldssignificant improvements. Moreover, it was used successfully in our winningentry to the MSCOCO image captioning challenge, 2015.
arxiv-12000-193 | Measuring Sample Quality with Stein's Method | http://arxiv.org/pdf/1506.03039v4.pdf | author:Jackson Gorham, Lester Mackey category:stat.ML cs.LG math.PR stat.ME published:2015-06-09 summary:To improve the efficiency of Monte Carlo estimation, practitioners areturning to biased Markov chain Monte Carlo procedures that trade off asymptoticexactness for computational speed. The reasoning is sound: a reduction invariance due to more rapid sampling can outweigh the bias introduced. However,the inexactness creates new challenges for sampler and parameter selection,since standard measures of sample quality like effective sample size do notaccount for asymptotic bias. To address these challenges, we introduce a newcomputable quality measure based on Stein's method that quantifies the maximumdiscrepancy between sample and target expectations over a large class of testfunctions. We use our tool to compare exact, biased, and deterministic samplesequences and illustrate applications to hyperparameter selection, convergencerate assessment, and quantifying bias-variance tradeoffs in posteriorinference.
arxiv-12000-194 | Estimating Posterior Ratio for Classification: Transfer Learning from Probabilistic Perspective | http://arxiv.org/pdf/1506.02784v3.pdf | author:Song Liu, Kenji Fukumizu category:stat.ML cs.LG published:2015-06-09 summary:Transfer learning assumes classifiers of similar tasks share certainparameter structures. Unfortunately, modern classifiers uses sophisticatedfeature representations with huge parameter spaces which lead to costlytransfer. Under the impression that changes from one classifier to anothershould be ``simple'', an efficient transfer learning criteria that only learnsthe ``differences'' is proposed in this paper. We train a \emph{posteriorratio} which turns out to minimizes the upper-bound of the target learningrisk. The model of posterior ratio does not have to share the same parameterspace with the source classifier at all so it can be easily modelled andefficiently trained. The resulting classifier therefore is obtained by simplymultiplying the existing probabilistic-classifier with the learned posteriorratio.
arxiv-12000-195 | Symmetric Tensor Completion from Multilinear Entries and Learning Product Mixtures over the Hypercube | http://arxiv.org/pdf/1506.03137v3.pdf | author:Tselil Schramm, Benjamin Weitz category:cs.DS cs.LG stat.ML published:2015-06-09 summary:We give an algorithm for completing an order-$m$ symmetric low-rank tensorfrom its multilinear entries in time roughly proportional to the number oftensor entries. We apply our tensor completion algorithm to the problem oflearning mixtures of product distributions over the hypercube, obtaining newalgorithmic results. If the centers of the product distribution are linearlyindependent, then we recover distributions with as many as $\Omega(n)$ centersin polynomial time and sample complexity. In the general case, we recoverdistributions with as many as $\tilde\Omega(n)$ centers in quasi-polynomialtime, answering an open problem of Feldman et al. (SIAM J. Comp.) for thespecial case of distributions with incoherent bias vectors. Our main algorithmic tool is the iterated application of a low-rank matrixcompletion algorithm for matrices with adversarially missing entries.
arxiv-12000-196 | Provable Bayesian Inference via Particle Mirror Descent | http://arxiv.org/pdf/1506.03101v3.pdf | author:Bo Dai, Niao He, Hanjun Dai, Le Song category:cs.LG stat.CO stat.ML published:2015-06-09 summary:Bayesian methods are appealing in their flexibility in modeling complex dataand ability in capturing uncertainty in parameters. However, when Bayes' ruledoes not result in tractable closed-form, most approximate inference algorithmslack either scalability or rigorous guarantees. To tackle this challenge, wepropose a simple yet provable algorithm, \emph{Particle Mirror Descent} (PMD),to iteratively approximate the posterior density. PMD is inspired by stochasticfunctional mirror descent where one descends in the density space using a smallbatch of data points at each iteration, and by particle filtering where oneuses samples to approximate a function. We prove result of the first kind that,with $m$ particles, PMD provides a posterior density estimator that convergesin terms of $KL$-divergence to the true posterior in rate $O(1/\sqrt{m})$. Wedemonstrate competitive empirical performances of PMD compared to severalapproximate inference algorithms in mixture models, logistic regression, sparseGaussian processes and latent Dirichlet allocation on large scale datasets.
arxiv-12000-197 | WordRank: Learning Word Embeddings via Robust Ranking | http://arxiv.org/pdf/1506.02761v3.pdf | author:Shihao Ji, Hyokun Yun, Pinar Yanardag, Shin Matsushima, S. V. N. Vishwanathan category:cs.CL cs.LG stat.ML published:2015-06-09 summary:Embedding words in a vector space has gained a lot of attention in recentyears. While state-of-the-art methods provide efficient computation of wordsimilarities via a low-dimensional matrix embedding, their motivation is oftenleft unclear. In this paper, we argue that word embedding can be naturallyviewed as a ranking problem due to the ranking nature of the evaluationmetrics. Then, based on this insight, we propose a novel framework WordRankthat efficiently estimates word representations via robust ranking, in whichthe attention mechanism and robustness to noise are readily achieved via theDCG-like ranking losses. The performance of WordRank is measured in wordsimilarity and word analogy benchmarks, and the results are compared to thestate-of-the-art word embedding techniques. Our algorithm is very competitiveto the state-of-the-arts on large corpora, while outperforms them by asignificant margin when the training set is limited (i.e., sparse and noisy).With 17 million tokens, WordRank performs almost as well as existing methodsusing 7.2 billion tokens on a popular word similarity benchmark. Ourmulti-machine distributed implementation of WordRank is open sourced forgeneral usage.
arxiv-12000-198 | Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path | http://arxiv.org/pdf/1506.02903v3.pdf | author:Daniel Hsu, Aryeh Kontorovich, Csaba Szepesv√°ri category:cs.LG stat.ML published:2015-06-09 summary:This article provides the first procedure for computing a fullydata-dependent interval that traps the mixing time $t_{\text{mix}}$ of a finitereversible ergodic Markov chain at a prescribed confidence level. The intervalis computed from a single finite-length sample path from the Markov chain, anddoes not require the knowledge of any parameters of the chain. This stands incontrast to previous approaches, which either only provide point estimates, orrequire a reset mechanism, or additional prior knowledge. The interval isconstructed around the relaxation time $t_{\text{relax}}$, which is stronglyrelated to the mixing time, and the width of the interval converges to zeroroughly at a $\sqrt{n}$ rate, where $n$ is the length of the sample path. Upperand lower bounds are given on the number of samples required to achieveconstant-factor multiplicative accuracy. The lower bounds indicate that, unlessfurther restrictions are placed on the chain, no procedure can achieve thisaccuracy level before seeing each state at least $\Omega(t_{\text{relax}})$times on the average. Finally, future directions of research are identified.
arxiv-12000-199 | Flowing ConvNets for Human Pose Estimation in Videos | http://arxiv.org/pdf/1506.02897v2.pdf | author:Tomas Pfister, James Charles, Andrew Zisserman category:cs.CV published:2015-06-09 summary:The objective of this work is human pose estimation in videos, where multipleframes are available. We investigate a ConvNet architecture that is able tobenefit from temporal context by combining information across the multipleframes using optical flow. To this end we propose a network architecture with the following novelties:(i) a deeper network than previously investigated for regressing heatmaps; (ii)spatial fusion layers that learn an implicit spatial model; (iii) optical flowis used to align heatmap predictions from neighbouring frames; and (iv) a finalparametric pooling layer which learns to combine the aligned heatmaps into apooled confidence map. We show that this architecture outperforms a number of others, including onethat uses optical flow solely at the input layers, one that regresses jointcoordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin onthree video pose estimation datasets, including the very challenging Poses inthe Wild dataset, and outperforms other deep methods that don't use a graphicalmodel on the single-image FLIC benchmark (and also Chen & Yuille and Tompson etal. in the high precision region).
arxiv-12000-200 | Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering | http://arxiv.org/pdf/1506.02816v2.pdf | author:George Gkotsis, Maria Liakata, Carlos Pedrinaci, John Domingue category:cs.CL cs.IR H.3.1 published:2015-06-09 summary:This paper addresses the problem of determining the best answer inCommunity-based Question Answering (CQA) websites by focussing on the content.In particular, we present a system, ACQUA [http://acqua.kmi.open.ac.uk], thatcan be installed onto the majority of browsers as a plugin. The service offersa seamless and accurate prediction of the answer to be accepted. Previousresearch on this topic relies on the exploitation of community feedback on theanswers, which involves rating of either users (e.g., reputation) or answers(e.g. scores manually assigned to answers). We propose a new technique thatleverages the content/textual features of answers in a novel way. Our approachdelivers better results than related linguistics-based solutions and manages tomatch rating-based approaches. More specifically, the gain in performance isachieved by rendering the values of these features into a discretised form. Wealso show how our technique manages to deliver equally good results inreal-time settings, as opposed to having to rely on information not alwaysreadily available, such as user ratings and answer scores. We ran an evaluationon 21 StackExchange websites covering around 4 million questions and more than8 million answers. We obtain 84% average precision and 70% recall, which showsthat our technique is robust, effective, and widely applicable.
arxiv-12000-201 | Learning to Linearize Under Uncertainty | http://arxiv.org/pdf/1506.03011v2.pdf | author:Ross Goroshin, Michael Mathieu, Yann LeCun category:cs.CV published:2015-06-09 summary:Training deep feature hierarchies to solve supervised learning tasks hasachieved state of the art performance on many problems in computer vision.However, a principled way in which to train such hierarchies in theunsupervised setting has remained elusive. In this work we suggest a newarchitecture and loss for training deep feature hierarchies that linearize thetransformations observed in unlabeled natural video sequences. This is done bytraining a generative model to predict video frames. We also address theproblem of inherent uncertainty in prediction by introducing latent variablesthat are non-deterministic functions of the input into the networkarchitecture.
arxiv-12000-202 | Connotation Frames: Typed Relations of Implied Sentiment in Predicate-Argument Structure | http://arxiv.org/pdf/1506.02739v2.pdf | author:Hannah Rashkin, Sameer Singh, Yejin Choi category:cs.CL published:2015-06-09 summary:Through a choice of a predicate (e.g., "violate"), a writer can convey subtlesentiments and value judgements toward the arguments of a verb (e.g.,projecting the agent as an "antagonist" and the theme as a "victim"). Weintroduce connotation frames to encode the rich dimensions of impliedsentiment, value judgements, and effect evaluation as typed relations thatthese choices influence, and propose a factor graph formulation that capturesthe inter-play among different types of connotative relations at thelexicon-level. Experimental results confirm that our model is effective inpredicting connotative sentiments compared to strong baselines and existingsentiment lexicons.
arxiv-12000-203 | EventNet: A Large Scale Structured Concept Library for Complex Event Detection in Video | http://arxiv.org/pdf/1506.02328v2.pdf | author:Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, Shih-Fu Chang category:cs.CV published:2015-06-08 summary:Event-specific concepts are the semantic concepts designed for the events ofinterest, which can be used as a mid-level representation of complex events invideos. Existing methods only focus on defining event-specific concepts for asmall number of predefined events, but cannot handle novel unseen events. Thismotivates us to build a large scale event-specific concept library that coversas many real-world events and their concepts as possible. Specifically, wechoose WikiHow, an online forum containing a large number of how-to articles onhuman daily life events. We perform a coarse-to-fine event discovery processand discover 500 events from WikiHow articles. Then we use each event name asquery to search YouTube and discover event-specific concepts from the tags ofreturned videos. After an automatic filter process, we end up with 95,321videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) modelon the 95,321 videos over the 500 events, and use the model to extract deeplearning feature from video content. With the learned deep learning feature, wetrain 4,490 binary SVM classifiers as the event-specific concept library. Theconcepts and events are further organized in a hierarchical structure definedby WikiHow, and the resultant concept library is called EventNet. Finally, theEventNet concept library is used to generate concept based representation ofevent videos. To the best of our knowledge, EventNet represents the first videoevent ontology that organizes events and their concepts into a semanticstructure. It offers great potential for event retrieval and browsing.Extensive experiments over the zero-shot event retrieval task when no trainingsamples are available show that the EventNet concept library consistently andsignificantly outperforms the state-of-the-art (such as the 20K ImageNetconcepts trained with CNN) by a large margin up to 207%.
arxiv-12000-204 | Modeling Order in Neural Word Embeddings at Scale | http://arxiv.org/pdf/1506.02338v3.pdf | author:Andrew Trask, David Gilmore, Matthew Russell category:cs.CL published:2015-06-08 summary:Natural Language Processing (NLP) systems commonly leverage bag-of-wordsco-occurrence techniques to capture semantic and syntactic word relationships.The resulting word-level distributed representations often ignore morphologicalinformation, though character-level embeddings have proven valuable to NLPtasks. We propose a new neural language model incorporating both word order andcharacter order in its embedding. The model produces several vector spaces withmeaningful substructure, as evidenced by its performance of 85.8% on a recentword-analogy task, exceeding best published syntactic word-analogy scores by a58% error margin. Furthermore, the model includes several parallel trainingmethods, most notably allowing a skip-gram network with 160 billion parametersto be trained overnight on 3 multi-core CPUs, 14x larger than the previouslargest neural network.
arxiv-12000-205 | On Convergence of Emphatic Temporal-Difference Learning | http://arxiv.org/pdf/1506.02582v2.pdf | author:Huizhen Yu category:cs.LG published:2015-06-08 summary:We consider emphatic temporal-difference learning algorithms for policyevaluation in discounted Markov decision processes with finite spaces. Suchalgorithms were recently proposed by Sutton, Mahmood, and White (2015) as animproved solution to the problem of divergence of off-policytemporal-difference learning with linear function approximation. We present inthis paper the first convergence proofs for two emphatic algorithms,ETD($\lambda$) and ELSTD($\lambda$). We prove, under general off-policyconditions, the convergence in $L^1$ for ELSTD($\lambda$) iterates, and thealmost sure convergence of the approximate value functions calculated by bothalgorithms using a single infinitely long trajectory. Our analysis involves newtechniques with applications beyond emphatic algorithms leading, for example,to the first proof that standard TD($\lambda$) also converges under off-policytraining for $\lambda$ sufficiently large.
arxiv-12000-206 | Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework | http://arxiv.org/pdf/1506.02565v4.pdf | author:Yong-Deok Kim, Taewoong Jang, Bohyung Han, Seungjin Choi category:cs.CV cs.LG stat.ML published:2015-06-08 summary:We propose a Bayesian evidence framework to facilitate transfer learning frompre-trained deep convolutional neural networks (CNNs). Our framework isformulated on top of a least squares SVM (LS-SVM) classifier, which is simpleand fast in both training and testing, and achieves competitive performance inpractice. The regularization parameters in LS-SVM is estimated automaticallywithout grid search and cross-validation by maximizing evidence, which is auseful measure to select the best performing CNN out of multiple candidates fortransfer learning; the evidence is optimized efficiently by employing Aitken'sdelta-squared process, which accelerates convergence of fixed point update. Theproposed Bayesian evidence framework also provides a good solution to identifythe best ensemble of heterogeneous CNNs through a greedy algorithm. OurBayesian evidence framework for transfer learning is tested on 12 visualrecognition datasets and illustrates the state-of-the-art performanceconsistently in terms of prediction accuracy and modeling efficiency.
arxiv-12000-207 | High-Dimensional Continuous Control Using Generalized Advantage Estimation | http://arxiv.org/pdf/1506.02438v4.pdf | author:John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel category:cs.LG cs.RO cs.SY published:2015-06-08 summary:Policy gradient methods are an appealing approach in reinforcement learningbecause they directly optimize the cumulative reward and can straightforwardlybe used with nonlinear function approximators such as neural networks. The twomain challenges are the large number of samples typically required, and thedifficulty of obtaining stable and steady improvement despite thenonstationarity of the incoming data. We address the first challenge by usingvalue functions to substantially reduce the variance of policy gradientestimates at the cost of some bias, with an exponentially-weighted estimator ofthe advantage function that is analogous to TD(lambda). We address the secondchallenge by using trust region optimization procedure for both the policy andthe value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3Dlocomotion tasks, learning running gaits for bipedal and quadrupedal simulatedrobots, and learning a policy for getting the biped to stand up from startingout lying on the ground. In contrast to a body of prior work that useshand-crafted policy representations, our neural network policies map directlyfrom raw kinematics to joint torques. Our algorithm is fully model-free, andthe amount of simulated experience required for the learning tasks on 3D bipedscorresponds to 1-2 weeks of real time.
arxiv-12000-208 | Community detection in multi-relational data with restricted multi-layer stochastic blockmodel | http://arxiv.org/pdf/1506.02699v2.pdf | author:Subhadeep Paul, Yuguo Chen category:stat.ML published:2015-06-08 summary:In recent years there has been an increased interest in statistical analysisof data with multiple types of relations among a set of entities. Suchmulti-relational data can be represented as multi-layer graphs where the set ofvertices represents the entities and multiple types of edges represent thedifferent relations among them. For community detection in multi-layer graphs,we consider two random graph models, the multi-layer stochastic blockmodel(MLSBM) and a model with a restricted parameter space, the restrictedmulti-layer stochastic blockmodel (RMLSBM). We derive consistency results forcommunity assignments of the maximum likelihood estimators (MLEs) in bothmodels where MLSBM is assumed to be the true model, and either the number ofnodes or the number of types of edges or both grow. We compare MLEs in the twomodels with other baseline approaches, such as separate modeling of layers,aggregating the layers and majority voting. RMLSBM is shown to have advantageover MLSBM when either the growth rate of the number of communities is high orthe growth rate of the average degree of the component graphs in themulti-graph is low. We also derive minimax rates of error and sharp thresholdsfor achieving consistency of community detection in both models, which are thenused to compare the multi-layer models with a baseline model, the aggregatestochastic block model. The simulation studies and real data applicationsconfirm the superior performance of the multi-layer approaches in comparison tothe baseline procedures.
arxiv-12000-209 | ASlib: A Benchmark Library for Algorithm Selection | http://arxiv.org/pdf/1506.02465v3.pdf | author:Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Frechette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown, Kevin Tierney, Joaquin Vanschoren category:cs.AI cs.LG published:2015-06-08 summary:The task of algorithm selection involves choosing an algorithm from a set ofalgorithms on a per-instance basis in order to exploit the varying performanceof algorithms over a set of instances. The algorithm selection problem isattracting increasing attention from researchers and practitioners in AI. Yearsof fruitful applications in a number of domains have resulted in a large amountof data, but the community lacks a standard format or repository for this data.This situation makes it difficult to share and compare different approacheseffectively, as is done in other, more established fields. It alsounnecessarily hinders new researchers who want to work in this area. To addressthis problem, we introduce a standardized format for representing algorithmselection scenarios and a repository that contains a growing number of datasets from the literature. Our format has been designed to be able to express awide variety of different scenarios. Demonstrating the breadth and power of ourplatform, we describe a set of example experiments that build and evaluatealgorithm selection models through a common interface. The results display thepotential of algorithm selection to achieve significant performanceimprovements across a broad range of problems and algorithms.
arxiv-12000-210 | Learning to Transduce with Unbounded Memory | http://arxiv.org/pdf/1506.02516v3.pdf | author:Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom category:cs.NE cs.CL cs.LG 68T05 published:2015-06-08 summary:Recently, strong results have been demonstrated by Deep Recurrent NeuralNetworks on natural language transduction problems. In this paper we explorethe representational power of these models using synthetic grammars designed toexhibit phenomena similar to those found in real transduction problems such asmachine translation. These experiments lead us to propose new memory-basedrecurrent networks that implement continuously differentiable analogues oftraditional data structures such as Stacks, Queues, and DeQues. We show thatthese architectures exhibit superior generalisation performance to Deep RNNsand are often able to learn the underlying generating algorithms in ourtransduction experiments.
arxiv-12000-211 | An Improved BKW Algorithm for LWE with Applications to Cryptography and Lattices | http://arxiv.org/pdf/1506.02717v4.pdf | author:Paul Kirchner, Pierre-Alain Fouque category:cs.CR cs.DS cs.LG I.1.2; F.2.1 published:2015-06-08 summary:In this paper, we study the Learning With Errors problem and its binaryvariant, where secrets and errors are binary or taken in a small interval. Weintroduce a new variant of the Blum, Kalai and Wasserman algorithm, relying ona quantization step that generalizes and fine-tunes modulus switching. Ingeneral this new technique yields a significant gain in the constant in frontof the exponent in the overall complexity. We illustrate this by solving pwithin half a day a LWE instance with dimension n = 128, modulus $q = n^2$,Gaussian noise $\alpha = 1/(\sqrt{n/\pi} \log^2 n)$ and binary secret, using$2^{28}$ samples, while the previous best result based on BKW claims a timecomplexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We thenintroduce variants of BDD, GapSVP and UniqueSVP, where the target point isrequired to lie in the fundamental parallelepiped, and show how the previousalgorithm is able to solve these variants in subexponential time. Moreover, wealso show how the previous algorithm can be used to solve the BinaryLWE problemwith n samples in subexponential time $2^{(\ln 2/2+o(1))n/\log \log n}$. Thisanalysis does not require any heuristic assumption, contrary to other algebraicapproaches; instead, it uses a variant of an idea by Lyubashevsky to generatemany samples from a small number of samples. This makes it possible toasymptotically and heuristically break the NTRU cryptosystem in subexponentialtime (without contradicting its security assumption). We are also able to solvesubset sum problems in subexponential time for density $o(1)$, which is ofindependent interest: for such density, the previous best algorithm requiresexponential time. As a direct application, we can solve in subexponential timethe parameters of a cryptosystem based on this problem proposed at TCC 2010.
arxiv-12000-212 | ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate Updates | http://arxiv.org/pdf/1506.02396v4.pdf | author:Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin category:math.OC cs.DC stat.ML published:2015-06-08 summary:Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstractsmany problems in numerical linear algebra, optimization, and other areas ofscientific computing. To solve fixed-point problems, we propose ARock, analgorithmic framework in which multiple agents (machines, processors, or cores)update $x$ in an asynchronous parallel fashion. Asynchrony is crucial toparallel computing since it reduces synchronization wait, relaxes communicationbottleneck, and thus speeds up computing significantly. At each step of ARock,an agent updates a randomly selected coordinate $x_i$ based on possiblyout-of-date information on $x$. The agents share $x$ through either globalmemory or communication. If writing $x_i$ is atomic, the agents can read andwrite $x$ without memory locks. Theoretically, we show that if the nonexpansive operator $T$ has a fixedpoint, then with probability one, ARock generates a sequence that converges toa fixed points of $T$. Our conditions on $T$ and step sizes are weaker thancomparable work. Linear convergence is also obtained. We propose special cases of ARock for linear systems, convex optimization,machine learning, as well as distributed and decentralized consensus problems.Numerical experiments of solving sparse logistic regression problems arepresented.
arxiv-12000-213 | Learning both Weights and Connections for Efficient Neural Networks | http://arxiv.org/pdf/1506.02626v3.pdf | author:Song Han, Jeff Pool, John Tran, William J. Dally category:cs.NE cs.CV cs.LG published:2015-06-08 summary:Neural networks are both computationally intensive and memory intensive,making them difficult to deploy on embedded systems. Also, conventionalnetworks fix the architecture before training starts; as a result, trainingcannot improve the architecture. To address these limitations, we describe amethod to reduce the storage and computation required by neural networks by anorder of magnitude without affecting their accuracy by learning only theimportant connections. Our method prunes redundant connections using athree-step method. First, we train the network to learn which connections areimportant. Next, we prune the unimportant connections. Finally, we retrain thenetwork to fine tune the weights of the remaining connections. On the ImageNetdataset, our method reduced the number of parameters of AlexNet by a factor of9x, from 61 million to 6.7 million, without incurring accuracy loss. Similarexperiments with VGG-16 found that the number of parameters can be reduced by13x, from 138 million to 10.3 million, again with no loss of accuracy.
arxiv-12000-214 | Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem | http://arxiv.org/pdf/1506.02550v3.pdf | author:Junpei Komiyama, Junya Honda, Hisashi Kashima, Hiroshi Nakagawa category:stat.ML cs.LG published:2015-06-08 summary:We study the $K$-armed dueling bandit problem, a variation of the standardstochastic bandit problem where the feedback is limited to relative comparisonsof a pair of arms. We introduce a tight asymptotic regret lower bound that isbased on the information divergence. An algorithm that is inspired by theDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)is proposed, and its regret is analyzed. The proposed algorithm is found to bethe first one with a regret upper bound that matches the lower bound.Experimental comparisons of dueling bandit algorithms show that the proposedalgorithm significantly outperforms existing ones.
arxiv-12000-215 | Variational Dropout and the Local Reparameterization Trick | http://arxiv.org/pdf/1506.02557v2.pdf | author:Diederik P. Kingma, Tim Salimans, Max Welling category:stat.ML cs.LG stat.CO published:2015-06-08 summary:We investigate a local reparameterizaton technique for greatly reducing thevariance of stochastic gradients for variational Bayesian inference (SGVB) of aposterior over model parameters, while retaining parallelizability. This localreparameterization translates uncertainty about global parameters into localnoise that is independent across datapoints in the minibatch. Suchparameterizations can be trivially parallelized and have variance that isinversely proportional to the minibatch size, generally leading to much fasterconvergence. Additionally, we explore a connection with dropout: Gaussiandropout objectives correspond to SGVB with local reparameterization, ascale-invariant prior and proportionally fixed posterior variance. Our methodallows inference of more flexibly parameterized posteriors; specifically, wepropose variational dropout, a generalization of Gaussian dropout where thedropout rates are learned, often leading to better models. The method isdemonstrated through several experiments.
arxiv-12000-216 | Stay on path: PCA along graph paths | http://arxiv.org/pdf/1506.02344v2.pdf | author:Megasthenis Asteris, Anastasios Kyrillidis, Alexandros G. Dimakis, Han-Gyol Yi and, Bharath Chandrasekaran category:stat.ML cs.IT cs.LG math.IT math.OC published:2015-06-08 summary:We introduce a variant of (sparse) PCA in which the set of feasible supportsets is determined by a graph. In particular, we consider the followingsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding tovariables, the non-zero entries of the extracted principal component mustcoincide with vertices lying along a path in $G$. From a statistical perspective, information on the underlying network maypotentially reduce the number of observations required to recover thepopulation principal component. We consider the canonical estimator whichoptimally exploits the prior knowledge by solving a non-convex quadraticmaximization on the empirical covariance. We introduce a simple network andanalyze the estimator under the spiked covariance model. We show that sideinformation potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrainedquadratic maximization, and recover a component with the desired properties. Weempirically evaluate our schemes on synthetic and real datasets.
arxiv-12000-217 | DUAL-LOCO: Distributing Statistical Estimation Using Random Projections | http://arxiv.org/pdf/1506.02554v2.pdf | author:Christina Heinze, Brian McWilliams, Nicolai Meinshausen category:stat.ML cs.DC cs.LG published:2015-06-08 summary:We present DUAL-LOCO, a communication-efficient algorithm for distributedstatistical estimation. DUAL-LOCO assumes that the data is distributedaccording to the features rather than the samples. It requires only a singleround of communication where low-dimensional random projections are used toapproximate the dependences between features available to different workers. Weshow that DUAL-LOCO has bounded approximation error which only depends weaklyon the number of workers. We compare DUAL-LOCO against a state-of-the-artdistributed optimization method on a variety of real world datasets and showthat it obtains better speedups while retaining good accuracy.
arxiv-12000-218 | Adaptive Normalized Risk-Averting Training For Deep Neural Networks | http://arxiv.org/pdf/1506.02690v2.pdf | author:Zhiguang Wang, Tim Oates, James Lo category:cs.LG cs.NE stat.ML published:2015-06-08 summary:This paper proposes a set of new error criteria and learning approaches,Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convexoptimization problem in training deep neural networks (DNNs). Theoretically, wedemonstrate its effectiveness on global and local convexity lower-bounded bythe standard $L_p$-norm error. By analyzing the gradient on the convexity index$\lambda$, we explain the reason why to learn $\lambda$ adaptively usinggradient descent works. In practice, we show how this method improves trainingof deep neural networks to solve visual recognition tasks on the MNIST andCIFAR-10 datasets. Without using pretraining or other tricks, we obtain resultscomparable or superior to those reported in recent literature on the same tasksusing standard ConvNets + MSE/cross entropy. Performance on deep/shallowmultilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT canbe combined with other quasi-Newton training methods, innovative networkvariants, regularization techniques and other specific tricks in DNNs. Otherthan unsupervised pretraining, it provides a new perspective to address thenon-convex optimization problem in DNNs.
arxiv-12000-219 | Interpretable Selection and Visualization of Features and Interactions Using Bayesian Forests | http://arxiv.org/pdf/1506.02371v4.pdf | author:Viktoriya Krakovna, Jiong Du, Jun S. Liu category:stat.ML published:2015-06-08 summary:It is becoming increasingly important for machine learning methods to makepredictions that are interpretable as well as accurate. In many practicalapplications, it is of interest which features and feature interactions arerelevant to the prediction task. We present a novel method, Selective BayesianForest Classifier, that strikes a balance between predictive power andinterpretability by simultaneously performing classification, featureselection, feature interaction detection and visualization. It buildsparsimonious yet flexible models using tree-structured Bayesian networks, andsamples an ensemble of such models using Markov chain Monte Carlo. We build infeature selection by dividing the trees into two groups according to theirrelevance to the outcome of interest. Our method performs competitively onclassification and feature selection benchmarks in low and high dimensions, andincludes a visualization tool that provides insight into relevant features andinteractions.
arxiv-12000-220 | Learning with Group Invariant Features: A Kernel Perspective | http://arxiv.org/pdf/1506.02544v2.pdf | author:Youssef Mroueh, Stephen Voinea, Tomaso Poggio category:cs.LG cs.CV stat.ML published:2015-06-08 summary:We analyze in this paper a random feature map based on a theory of invarianceI-theory introduced recently. More specifically, a group invariant signalsignature is obtained through cumulative distributions of group transformedrandom projections. Our analysis bridges invariant feature learning with kernelmethods, as we show that this feature map defines an expected Haar integrationkernel that is invariant to the specified group action. We show how thisnon-linear random feature map approximates this group invariant kerneluniformly on a set of $N$ points. Moreover, we show that it defines a functionspace that is dense in the equivalent Invariant Reproducing Kernel HilbertSpace. Finally, we quantify error rates of the convergence of the empiricalrisk minimization, as well as the reduction in the sample complexity of alearning algorithm using such an invariant representation for signalclassification, in a classical supervised learning setting.
arxiv-12000-221 | Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees | http://arxiv.org/pdf/1506.02681v3.pdf | author:Fran√ßois-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne category:stat.ML published:2015-06-08 summary:There is renewed interest in formulating integration as an inference problem,motivated by obtaining a full distribution over numerical error that can bepropagated through subsequent computation. Current methods, such as BayesianQuadrature, demonstrate impressive empirical performance but lack theoreticalanalysis. An important challenge is to reconcile these probabilisticintegrators with rigorous convergence guarantees. In this paper, we present thefirst probabilistic integrator that admits such theoretical treatment, calledFrank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the truevalue of the integral is shown to be exponential and posterior contractionrates are proven to be superexponential. In simulations, FWBQ is competitivewith state-of-the-art methods and out-performs alternatives based onFrank-Wolfe optimisation. Our approach is applied to successfully quantifynumerical error in the solution to a challenging model choice problem incellular biology.
arxiv-12000-222 | Empirical Studies on Symbolic Aggregation Approximation Under Statistical Perspectives for Knowledge Discovery in Time Series | http://arxiv.org/pdf/1506.02732v1.pdf | author:Wei Song, Zhiguang Wang, Yangdong Ye, Ming Fan category:cs.LG cs.IT math.IT published:2015-06-08 summary:Symbolic Aggregation approXimation (SAX) has been the de facto standardrepresentation methods for knowledge discovery in time series on a number oftasks and applications. So far, very little work has been done in empiricallyinvestigating the intrinsic properties and statistical mechanics in SAX words.In this paper, we applied several statistical measurements and proposed a newstatistical measurement, i.e. information embedding cost (IEC) to analyze thestatistical behaviors of the symbolic dynamics. Our experiments on thebenchmark datasets and the clinical signals demonstrate that SAX can alwaysreduce the complexity while preserving the core information embedded in theoriginal time series with significant embedding efficiency. Our proposed IECscore provide a priori to determine if SAX is adequate for specific dataset,which can be generalized to evaluate other symbolic representations. Our workprovides an analytical framework with several statistical tools to analyze,evaluate and further improve the symbolic dynamics for knowledge discovery intime series.
arxiv-12000-223 | Non-parametric Revenue Optimization for Generalized Second Price Auctions | http://arxiv.org/pdf/1506.02719v1.pdf | author:Mehryar Mohri, Andres Munoz Medina category:cs.LG cs.GT published:2015-06-08 summary:We present an extensive analysis of the key problem of learning optimalreserve prices for generalized second price auctions. We describe twoalgorithms for this task: one based on density estimation, and a novelalgorithm benefiting from solid theoretical guarantees and with a veryfavorable running-time complexity of $O(n S \log (n S))$, where $n$ is thesample size and $S$ the number of slots. Our theoretical guarantees are morefavorable than those previously presented in the literature. Additionally, weshow that even if bidders do not play at an equilibrium, our second algorithmis still well defined and minimizes a quantity of interest. To our knowledge,this is the first attempt to apply learning algorithms to the problem ofreserve price optimization in GSP auctions. Finally, we present the firstconvergence analysis of empirical equilibrium bidding functions to the uniquesymmetric Bayesian-Nash equilibrium of a GSP.
arxiv-12000-224 | Fast ConvNets Using Group-wise Brain Damage | http://arxiv.org/pdf/1506.02515v2.pdf | author:Vadim Lebedev, Victor Lempitsky category:cs.CV published:2015-06-08 summary:We revisit the idea of brain damage, i.e. the pruning of the coefficients ofa neural network, and suggest how brain damage can be modified and used tospeedup convolutional layers. The approach uses the fact that many efficientimplementations reduce generalized convolutions to matrix multiplications. Thesuggested brain damage process prunes the convolutional kernel tensor in agroup-wise fashion by adding group-sparsity regularization to the standardtraining process. After such group-wise pruning, convolutions can be reduced tomultiplications of thinned dense matrices, which leads to speedup. In thecomparison on AlexNet, the method achieves very competitive performance.
arxiv-12000-225 | backShift: Learning causal cyclic graphs from unknown shift interventions | http://arxiv.org/pdf/1506.02494v3.pdf | author:Dominik Rothenh√§usler, Christina Heinze, Jonas Peters, Nicolai Meinshausen category:stat.ME stat.ML published:2015-06-08 summary:We propose a simple method to learn linear causal cyclic models in thepresence of latent variables. The method relies on equilibrium data of themodel recorded under a specific kind of interventions ("shift interventions").The location and strength of these interventions do not have to be known andcan be estimated from the data. Our method, called backShift, only uses secondmoments of the data and performs simple joint matrix diagonalization, appliedto differences between covariance matrices. We give a sufficient and necessarycondition for identifiability of the system, which is fulfilled almost surelyunder some quite general assumptions if and only if there are at least threedistinct experimental settings, one of which can be pure observational data. Wedemonstrate the performance on some simulated data and applications in flowcytometry and financial time series. The code is made available as R-packagebackShift.
arxiv-12000-226 | The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal Prediction | http://arxiv.org/pdf/1506.02686v1.pdf | author:George D. Montanez, Cosma Rohilla Shalizi category:stat.ML cs.LG published:2015-06-08 summary:For the task of unsupervised spatio-temporal forecasting (e.g., learning topredict video data without labels), we propose two new nonparametric predictivestate algorithms, Moonshine and One Hundred Proof. The algorithms areconceptually simple and make few assumptions on the underlying spatio-temporalprocess yet have strong predictive performance and provide predictivedistributions over spatio-temporal data. The latter property allows forlikelihood estimation under the models, for classification and otherprobabilistic inference.
arxiv-12000-227 | A Topological Approach to Spectral Clustering | http://arxiv.org/pdf/1506.02633v1.pdf | author:Antonio Rieser category:cs.LG stat.ML published:2015-06-08 summary:We propose a clustering algorithm which, for input, takes data assumed to besampled from a uniform distribution supported on a metric space $X$, andoutputs a clustering of the data based on a topological estimate of theconnected components of $X$. The algorithm works by choosing a weighted graphon the samples from a natural one-parameter family of graphs using an errorbased on the heat operator on the graphs. The estimated connected components of$X$ are identified as the support of the eigenfunctions of the heat operatorwith eigenvalue $1$, which allows the algorithm to work without requiring thenumber of expected clusters as input.
arxiv-12000-228 | Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families | http://arxiv.org/pdf/1506.02564v2.pdf | author:Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, Arthur Gretton category:stat.ML published:2015-06-08 summary:We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptiveMCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densitieswhere classical HMC is not an option due to intractable gradients, KMCadaptively learns the target's gradient structure by fitting an exponentialfamily model in a Reproducing Kernel Hilbert Space. Computational costs arereduced by two novel efficient approximations to this gradient. While beingasymptotically exact, KMC mimics HMC in terms of sampling efficiency, andoffers substantial mixing improvements over state-of-the-art gradient freesamplers. We support our claims with experimental studies on both toy andreal-world applications, including Approximate Bayesian Computation andexact-approximate MCMC.
arxiv-12000-229 | Path-SGD: Path-Normalized Optimization in Deep Neural Networks | http://arxiv.org/pdf/1506.02617v1.pdf | author:Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.CV cs.NE stat.ML published:2015-06-08 summary:We revisit the choice of SGD for training deep neural networks byreconsidering the appropriate geometry in which to optimize the weights. Weargue for a geometry invariant to rescaling of weights that does not affect theoutput of the network, and suggest Path-SGD, which is an approximate steepestdescent method with respect to a path-wise regularizer related to max-normregularization. Path-SGD is easy and efficient to implement and leads toempirical gains over SGD and AdaGrad.
arxiv-12000-230 | Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection | http://arxiv.org/pdf/1506.02585v1.pdf | author:Zhimin Peng, Prudhvi Gurram, Heesung Kwon, Wotao Yin category:cs.LG published:2015-06-08 summary:In this paper, a novel framework of sparse kernel learning for Support VectorData Description (SVDD) based anomaly detection is presented. In this work,optimal sparse feature selection for anomaly detection is first modeled as aMixed Integer Programming (MIP) problem. Due to the prohibitively highcomputational complexity of the MIP, it is relaxed into a QuadraticallyConstrained Linear Programming (QCLP) problem. The QCLP problem can then bepractically solved by using an iterative optimization method, in which multiplesubsets of features are iteratively found as opposed to a single subset. TheQCLP-based iterative optimization problem is solved in a finite space calledthe \emph{Empirical Kernel Feature Space} (EKFS) instead of in the input spaceor \emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because ofthe fact that the geometrical properties of the EKFS and the corresponding RKHSremain the same. Now, an explicit nonlinear exploitation of the data in afinite EKFS is achievable, which results in optimal feature ranking.Experimental results based on a hyperspectral image show that the proposedmethod can provide improved performance over the current state-of-the-arttechniques.
arxiv-12000-231 | Efficient Learning of Ensembles with QuadBoost | http://arxiv.org/pdf/1506.02535v5.pdf | author:Louis Fortier-Dubois, Fran√ßois Laviolette, Mario Marchand, Louis-Emile Robitaille, Jean-Francis Roy category:cs.LG published:2015-06-08 summary:We first present a general risk bound for ensembles that depends on the Lpnorm of the weighted combination of voters which can be selected from acontinuous set. We then propose a boosting method, called QuadBoost, which isstrongly supported by the general risk bound and has very simple rules forassigning the voters' weights. Moreover, QuadBoost exhibits a rate of decreaseof its empirical error which is slightly faster than the one achieved byAdaBoost. The experimental results confirm the expectation of the theory thatQuadBoost is a very efficient method for learning ensembles.
arxiv-12000-232 | License Plate Recognition System Based on Color Coding Of License Plates | http://arxiv.org/pdf/1506.03128v1.pdf | author:Jani Biju Babjan category:cs.CV published:2015-06-08 summary:License Plate Recognition Systems are used to determine the license platenumber of a vehicle. The current system mainly uses Optical CharacterRecognition to recognize the number plate. There are several problems to thissystem. Some of them include interchanging of several letters or numbers(letter O with digit 0), difficulty in localizing the license plate, high errorrate, use of different fonts in license plates etc. So a new system torecognize the license plate number using color coding of license plates isproposed in this paper. Easier localization of license plate can be done bysearching for the start or stop patters of license plates. An eight segmentdisplay system along with traditional numbering with the first and lastsegments left for start or stop patterns is proposed in this paper. Practicalapplications include several areas under Internet of Things (IoT).
arxiv-12000-233 | Faster SGD Using Sketched Conditioning | http://arxiv.org/pdf/1506.02649v1.pdf | author:Alon Gonen, Shai Shalev-Shwartz category:cs.NA cs.LG published:2015-06-08 summary:We propose a novel method for speeding up stochastic optimization algorithmsvia sketching methods, which recently became a powerful tool for acceleratingalgorithms for numerical linear algebra. We revisit the method of conditioningfor accelerating first-order methods and suggest the use of sketching methodsfor constructing a cheap conditioner that attains a significant speedup withrespect to the Stochastic Gradient Descent (SGD) algorithm. While ourtheoretical guarantees assume convexity, we discuss the applicability of ourmethod to deep neural networks, and experimentally demonstrate its merits.
arxiv-12000-234 | Linear Convergence of the Randomized Feasible Descent Method Under the Weak Strong Convexity Assumption | http://arxiv.org/pdf/1506.02530v1.pdf | author:Chenxin Ma, Rachael Tappenden, Martin Tak√°ƒç category:cs.LG stat.ML published:2015-06-08 summary:In this paper we generalize the framework of the feasible descent method(FDM) to a randomized (R-FDM) and a coordinate-wise random feasible descentmethod (RC-FDM) framework. We show that the famous SDCA algorithm foroptimizing the SVM dual problem, or the stochastic coordinate descent methodfor the LASSO problem, fits into the framework of RC-FDM. We prove linearconvergence for both R-FDM and RC-FDM under the weak strong convexityassumption. Moreover, we show that the duality gap converges linearly forRC-FDM, which implies that the duality gap also converges linearly for SDCAapplied to the SVM dual problem.
arxiv-12000-235 | Generalization in Adaptive Data Analysis and Holdout Reuse | http://arxiv.org/pdf/1506.02629v2.pdf | author:Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth category:cs.LG cs.DS published:2015-06-08 summary:Overfitting is the bane of data analysts, even when data are plentiful.Formal approaches to understanding this problem focus on statistical inferenceand generalization of individual analysis procedures. Yet the practice of dataanalysis is an inherently interactive and adaptive process: new analyses andhypotheses are proposed after seeing the results of previous ones, parametersare tuned on the basis of obtained results, and datasets are shared and reused.An investigation of this gap has recently been initiated by the authors in(Dwork et al., 2014), where we focused on the problem of estimatingexpectations of adaptively chosen functions. In this paper, we give a simple and practical method for reusing a holdout(or testing) set to validate the accuracy of hypotheses produced by a learningalgorithm operating on a training set. Reusing a holdout set adaptivelymultiple times can easily lead to overfitting to the holdout set itself. Wegive an algorithm that enables the validation of a large number of adaptivelychosen hypotheses, while provably avoiding overfitting. We illustrate theadvantages of our algorithm over the standard use of the holdout set via asimple synthetic experiment. We also formalize and address the general problem of data reuse in adaptivedata analysis. We show how the differential-privacy based approach given in(Dwork et al., 2014) is applicable much more broadly to adaptive data analysis.We then show that a simple approach based on description length can also beused to give guarantees of statistical validity in adaptive settings. Finally,we demonstrate that these incomparable approaches can be unified via the notionof approximate max-information that we introduce.
arxiv-12000-236 | Circulant temporal encoding for video retrieval and temporal alignment | http://arxiv.org/pdf/1506.02588v2.pdf | author:Matthijs Douze, J√©r√¥me Revaud, Jakob Verbeek, Herv√© J√©gou, Cordelia Schmid category:cs.CV published:2015-06-08 summary:We address the problem of specific video event retrieval. Given a query videoof a specific event, e.g., a concert of Madonna, the goal is to retrieve othervideos of the same event that temporally overlap with the query. Our approachencodes the frame descriptors of a video to jointly represent their appearanceand temporal order. It exploits the properties of circulant matrices toefficiently compare the videos in the frequency domain. This offers asignificant gain in complexity and accurately localizes the matching parts ofvideos. The descriptors can be compressed in the frequency domain with aproduct quantizer adapted to complex numbers. In this case, video retrieval isperformed without decompressing the descriptors. We also consider the temporalalignment of a set of videos. We exploit the matching confidence and anestimate of the temporal offset computed for all pairs of videos by ourretrieval approach. Our robust algorithm aligns the videos on a global timelineby maximizing the set of temporally consistent matches. The global temporalalignment enables synchronous playback of the videos of a given scene.
arxiv-12000-237 | Stacked What-Where Auto-encoders | http://arxiv.org/pdf/1506.02351v8.pdf | author:Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun category:stat.ML cs.LG cs.NE published:2015-06-08 summary:We present a novel architecture, the "stacked what-where auto-encoders"(SWWAE), which integrates discriminative and generative pathways and provides aunified approach to supervised, semi-supervised and unsupervised learningwithout relying on sampling during training. An instantiation of SWWAE uses aconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, andemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce thereconstruction. The objective function includes reconstruction terms thatinduce the hidden states in the Deconvnet to be similar to those of theConvnet. Each pooling layer produces two sets of variables: the "what" whichare fed to the next layer, and its complementary variable "where" that are fedto the corresponding layer in the generative decoder.
arxiv-12000-238 | Convex recovery of tensors using nuclear norm penalization | http://arxiv.org/pdf/1506.02520v1.pdf | author:Stephane Chretien, Tianwen Wei category:stat.ML published:2015-06-08 summary:The subdifferential of convex functions of the singular spectrum of realmatrices has been widely studied in matrix analysis, optimization and automaticcontrol theory. Convex analysis and optimization over spaces of tensors is nowgaining much interest due to its potential applications to signal processing,statistics and engineering. The goal of this paper is to present anapplications to the problem of low rank tensor recovery based on linear randommeasurement by extending the results of Tropp to the tensors setting.
arxiv-12000-239 | Learning Mixtures of Ising Models using Pseudolikelihood | http://arxiv.org/pdf/1506.02510v1.pdf | author:Onur Dikmen category:cs.LG stat.ML published:2015-06-08 summary:Maximum pseudolikelihood method has been among the most important methods forlearning parameters of statistical physics models, such as Ising models. Inthis paper, we study how pseudolikelihood can be derived for learningparameters of a mixture of Ising models. The performance of the proposedapproach is demonstrated for Ising and Potts models on both synthetic and realdata.
arxiv-12000-240 | SVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from ImageNet | http://arxiv.org/pdf/1506.02509v1.pdf | author:Lei Zhang, David Zhang category:cs.LG cs.CV published:2015-06-08 summary:Deep learning with a convolutional neural network (CNN) has been proved to bevery effective in feature extraction and representation of images. For imageclassification problems, this work aim at finding which classifier is morecompetitive based on high-level deep features of images. In this report, wehave discussed the nearest neighbor, support vector machines and extremelearning machines for image classification under deep convolutional activationfeature representation. Specifically, we adopt the benchmark object recognitiondataset from multiple sources with domain bias for evaluating differentclassifiers. The deep features of the object dataset are obtained by awell-trained CNN with five convolutional layers and three fully-connectedlayers on the challenging ImageNet. Experiments demonstrate that the ELMsoutperform SVMs in cross-domain recognition tasks. In particular,state-of-the-art results are obtained by kernel ELM which outperforms SVMs withabout 4% of the average accuracy. The features and codes are available inhttp://www.escience.cn/people/lei/index.html
arxiv-12000-241 | Reflection Invariance: an important consideration of image orientation | http://arxiv.org/pdf/1506.02432v1.pdf | author:Craig Henderson, Ebroul Izquierdo category:cs.CV published:2015-06-08 summary:In this position paper, we consider the state of computer vision researchwith respect to invariance to the horizontal orientation of an image -- what weterm reflection invariance. We describe why we consider reflection invarianceto be an important property and provide evidence where the absence of thisinvariance produces surprising inconsistencies in state-of-the-art systems. Wedemonstrate inconsistencies in methods of object detection and sceneclassification when they are presented with images and the horizontal mirror ofthose images. Finally, we examine where some of the invariance is exhibited infeature detection and descriptors, and make a case for future consideration ofreflection invariance as a measure of quality in computer vision algorithms.
arxiv-12000-242 | Robust Regression via Hard Thresholding | http://arxiv.org/pdf/1506.02428v1.pdf | author:Kush Bhatia, Prateek Jain, Purushottam Kar category:cs.LG stat.ML published:2015-06-08 summary:We study the problem of Robust Least Squares Regression (RLSR) where severalresponse variables can be adversarially corrupted. More specifically, for adata matrix X \in R^{p x n} and an underlying model w*, the response vector isgenerated as y = X'w* + b where b \in R^n is the corruption vector supportedover at most C.n coordinates. Existing exact recovery results for RLSR focussolely on L1-penalty based convex formulations and impose relatively strictmodel assumptions such as requiring the corruptions b to be selectedindependently of X. In this work, we study a simple hard-thresholding algorithm called TORRENTwhich, under mild conditions on X, can recover w* exactly even if b corruptsthe response variables in an adversarial manner, i.e. both the support andentries of b are selected adversarially after observing X and w*. Our resultshold under deterministic assumptions which are satisfied if X is sampled fromany sub-Gaussian distribution. Finally unlike existing results that apply onlyto a fixed w*, generated independently of X, our results are universal and holdfor any w* \in R^p. Next, we propose gradient descent-based extensions of TORRENT that can scaleefficiently to large scale problems, such as high dimensional sparse recoveryand prove similar recovery guarantees for these extensions. Empirically we findTORRENT, and more so its extensions, offering significantly faster recoverythan the state-of-the-art L1 solvers. For instance, even on moderate-sizeddatasets (with p = 50K) with around 40% corrupted responses, a variant of ourproposed method called TORRENT-HYB is more than 20x faster than the best L1solver.
arxiv-12000-243 | Distributed Training of Structured SVM | http://arxiv.org/pdf/1506.02620v2.pdf | author:Ching-pei Lee, Kai-Wei Chang, Shyam Upadhyay, Dan Roth category:stat.ML cs.DC cs.LG published:2015-06-08 summary:Training structured prediction models is time-consuming. However, mostexisting approaches only use a single machine, thus, the advantage of computingpower and the capacity for larger data sets of multiple machines have not beenexploited. In this work, we propose an efficient algorithm for distributedlytraining structured support vector machines based on a distributedblock-coordinate descent method. Both theoretical and experimental resultsindicate that our method is efficient.
arxiv-12000-244 | A Tensor-Based Dictionary Learning Approach to Tomographic Image Reconstruction | http://arxiv.org/pdf/1506.04954v1.pdf | author:Sara Soltani, Misha E. Kilmer, Per Christian Hansen category:cs.CV cs.NA math.NA published:2015-06-08 summary:We consider tomographic reconstruction using priors in the form of adictionary learned from training images. The reconstruction has two stages:first we construct a tensor dictionary prior from our training data, and thenwe pose the reconstruction problem in terms of recovering the expansioncoefficients in that dictionary. Our approach differs from past approaches inthat a) we use a third-order tensor representation for our images and b) werecast the reconstruction problem using the tensor formulation. The dictionarylearning problem is presented as a non-negative tensor factorization problemwith sparsity constraints. The reconstruction problem is formulated in a convexoptimization framework by looking for a solution with a sparse representationin the tensor dictionary. Numerical results show that our tensor formulationleads to very sparse representations of both the training images and thereconstructions due to the ability of representing repeated features compactlyin the dictionary.
arxiv-12000-245 | Microscopic approach of a time elapsed neural model | http://arxiv.org/pdf/1506.02361v1.pdf | author:Julien Chevallier, Maria J. Caceres, Marie Doumic, Patricia Reynaud-Bouret category:cs.NE published:2015-06-08 summary:The spike trains are the main components of the information processing in thebrain. To model spike trains several point processes have been investigated inthe literature. And more macroscopic approaches have also been studied, usingpartial differential equation models. The main aim of the present article is tobuild a bridge between several point processes models (Poisson, Wold, Hawkes)that have been proved to statistically fit real spike trains data andage-structured partial differential equations as introduced by Pakdaman,Perthame and Salort.
arxiv-12000-246 | Convergence Rates of Active Learning for Maximum Likelihood Estimation | http://arxiv.org/pdf/1506.02348v1.pdf | author:Kamalika Chaudhuri, Sham Kakade, Praneeth Netrapalli, Sujay Sanghavi category:cs.LG stat.ML published:2015-06-08 summary:An active learner is given a class of models, a large set of unlabeledexamples, and the ability to interactively query labels of a subset of theseexamples; the goal of the learner is to learn a model in the class that fitsthe data well. Previous theoretical work has rigorously characterized label complexity ofactive learning, but most of this work has focused on the PAC or the agnosticPAC model. In this paper, we shift our attention to a more general setting --maximum likelihood estimation. Provided certain conditions hold on the modelclass, we provide a two-stage active learning algorithm for this problem. Theconditions we require are fairly general, and cover the widely popular class ofGeneralized Linear Models, which in turn, include models for binary andmulti-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and alower bound that matches it up to lower order terms. Our analysis shows thatunlike binary classification in the realizable case, just a single extra roundof interaction is sufficient to achieve near-optimal performance in maximumlikelihood estimation. On the empirical side, the recent work in~\cite{Zhang12} and~\cite{Zhang14} (on active linear and logistic regression)shows the promise of this approach.
arxiv-12000-247 | You Only Look Once: Unified, Real-Time Object Detection | http://arxiv.org/pdf/1506.02640v5.pdf | author:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi category:cs.CV published:2015-06-08 summary:We present YOLO, a new approach to object detection. Prior work on objectdetection repurposes classifiers to perform detection. Instead, we frame objectdetection as a regression problem to spatially separated bounding boxes andassociated class probabilities. A single neural network predicts bounding boxesand class probabilities directly from full images in one evaluation. Since thewhole detection pipeline is a single network, it can be optimized end-to-enddirectly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processesimages in real-time at 45 frames per second. A smaller version of the network,Fast YOLO, processes an astounding 155 frames per second while still achievingdouble the mAP of other real-time detectors. Compared to state-of-the-artdetection systems, YOLO makes more localization errors but is far less likelyto predict false detections where nothing exists. Finally, YOLO learns verygeneral representations of objects. It outperforms all other detection methods,including DPM and R-CNN, by a wide margin when generalizing from natural imagesto artwork on both the Picasso Dataset and the People-Art Dataset.
arxiv-12000-248 | Wavelets and continuous wavelet transform for autostereoscopic multiview images | http://arxiv.org/pdf/1506.02345v1.pdf | author:Vladimir Saveljev category:cs.CV published:2015-06-08 summary:Recently, the reference functions for the synthesis and analysis of theautostereoscopic multiview and integral images in three-dimensional displays weintroduced. In the current paper, we propose the wavelets to analyze suchimages. The wavelets are built on the reference functions as on the scalingfunctions of the wavelet analysis. The continuous wavelet transform wassuccessfully applied to the testing wireframe binary objects. The restoredlocations correspond to the structure of the testing wireframe binary objects.
arxiv-12000-249 | Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control | http://arxiv.org/pdf/1506.02632v3.pdf | author:Prashanth L. A., Cheng Jie, Michael Fu, Steve Marcus, Csaba Szepesv√°ri category:cs.LG math.OC published:2015-06-08 summary:Cumulative prospect theory (CPT) is known to model human decisions well, withsubstantial empirical evidence supporting this claim. CPT works by distortingprobabilities and is more general than the classic expected utility andcoherent risk measures. We bring this idea to a risk-sensitive reinforcementlearning (RL) setting and design algorithms for both estimation and control.The RL setting presents two particular challenges when CPT is applied:estimating the CPT objective requires estimations of the entire distribution ofthe value function and finding a randomized optimal policy. The estimationscheme that we propose uses the empirical distribution to estimate theCPT-value of a random variable. We then use this scheme in the inner loop of aCPT-value optimization procedure that is based on the well-known simulationoptimization idea of simultaneous perturbation stochastic approximation (SPSA).We provide theoretical convergence guarantees for all the proposed algorithmsand also illustrate the usefulness of CPT-based criteria in a traffic signalcontrol application.
arxiv-12000-250 | A Framework for Constrained and Adaptive Behavior-Based Agents | http://arxiv.org/pdf/1506.02312v1.pdf | author:Renato de Pontes Pereira, Paulo Martins Engel category:cs.AI cs.LG cs.RO cs.SY published:2015-06-07 summary:Behavior Trees are commonly used to model agents for robotics and games,where constrained behaviors must be designed by human experts in order toguarantee that these agents will execute a specific chain of actions given aspecific set of perceptions. In such application areas, learning is a desirablefeature to provide agents with the ability to adapt and improve interactionswith humans and environment, but often discarded due to its unreliability. Inthis paper, we propose a framework that uses Reinforcement Learning nodes aspart of Behavior Trees to address the problem of adding learning capabilitiesin constrained agents. We show how this framework relates to Options inHierarchical Reinforcement Learning, ensuring convergence of nested learningnodes, and we empirically show that the learning nodes do not affect theexecution of other nodes in the tree.
arxiv-12000-251 | SQUINKY! A Corpus of Sentence-level Formality, Informativeness, and Implicature | http://arxiv.org/pdf/1506.02306v1.pdf | author:Shibamouli Lahiri category:cs.CL published:2015-06-07 summary:We introduce a corpus of 7,032 sentences rated by human annotators forformality, informativeness, and implicature on a 1-7 scale. The corpus wasannotated using Amazon Mechanical Turk. Reliability in the obtained judgmentswas examined by comparing mean ratings across two MTurk experiments, andcorrelation with pilot annotations (on sentence formality) conducted in a morecontrolled setting. Despite the subjectivity and inherent difficulty of theannotation task, correlations between mean ratings were quite encouraging,especially on formality and informativeness. We further explored correlationbetween the three linguistic variables, genre-wise variation of ratings andcorrelations within genres, compatibility with automatic stylistic scoring, andsentential make-up of a document in terms of style. To date, our corpus is thelargest sentence-level annotated corpus released for formality,informativeness, and implicature.
arxiv-12000-252 | A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features | http://arxiv.org/pdf/1506.02327v1.pdf | author:Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Yuan-ming Liou, Yen-Chen Wu, Yen-Ju Lu, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG cs.NE published:2015-06-07 summary:This paper summarizes the work done by the authors for the Zero ResourceSpeech Challenge organized in the technical program of Interspeech 2015. Thegoal of the challenge is to discover linguistic units directly from unlabeledspeech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this workautomatically discovers multiple sets of acoustic tokens from the given corpus.Each acoustic token set is specified by a set of hyperparameters that describethe model configuration. These sets of acoustic tokens carry differentcharacteristics of the given corpus and the language behind thus can bemutually reinforced. The multiple sets of token labels are then used as thetargets of a Multi-target DNN (MDNN) trained on low-level acoustic features.Bottleneck features extracted from the MDNN are used as feedback for the MATand the MDNN itself. We call this iterative system the Multi-layered AcousticTokenizing Deep Neural Network (MAT-DNN) which generates high quality featuresfor track 1 of the challenge and acoustic tokens for track 2 of the challenge.
arxiv-12000-253 | Optimal Ridge Detection using Coverage Risk | http://arxiv.org/pdf/1506.02278v1.pdf | author:Yen-Chi Chen, Christopher R. Genovese, Shirley Ho, Larry Wasserman category:stat.ME stat.ML published:2015-06-07 summary:We introduce the concept of coverage risk as an error measure for densityridge estimation. The coverage risk generalizes the mean integrated squareerror to set estimation. We propose two risk estimators for the coverage riskand we show that we can select tuning parameters by minimizing the estimatedrisk. We study the rate of convergence for coverage risk and prove consistencyof the risk estimators. We apply our method to three simulated datasets and tocosmology data. In all the examples, the proposed method successfully recoverthe underlying density structure.
arxiv-12000-254 | Randomized Structural Sparsity based Support Identification with Applications to Locating Activated or Discriminative Brain Areas: A Multi-center Reproducibility Study | http://arxiv.org/pdf/1506.02265v1.pdf | author:Yilun Wang, Sheng Zhang, Junjie Zheng, Heng Chen, Huafu Chen category:cs.CV 68T01 I.5.4 published:2015-06-07 summary:In this paper, we focus on how to locate the relevant or discriminative brainregions related with external stimulus or certain mental decease, which is alsocalled support identification, based on the neuroimaging data. The maindifficulty lies in the extremely high dimensional voxel space and relativelyfew training samples, easily resulting in an unstable brain region discovery(or called feature selection in context of pattern recognition). When thetraining samples are from different centers and have betweencenter variations,it will be even harder to obtain a reliable and consistent result.Corresponding, we revisit our recently proposed algorithm based on stabilityselection and structural sparsity. It is applied to the multi-center MRI dataanalysis for the first time. A consistent and stable result is achieved acrossdifferent centers despite the between-center data variation while many otherstate-of-the-art methods such as two sample t-test fail. Moreover, we haveempirically showed that the performance of this algorithm is robust andinsensitive to several of its key parameters. In addition, the supportidentification results on both functional MRI and structural MRI areinterpretable and can be the potential biomarkers.
arxiv-12000-255 | A Recurrent Latent Variable Model for Sequential Data | http://arxiv.org/pdf/1506.02216v6.pdf | author:Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio category:cs.LG published:2015-06-07 summary:In this paper, we explore the inclusion of latent random variables into thedynamic hidden state of a recurrent neural network (RNN) by combining elementsof the variational autoencoder. We argue that through the use of high-levellatent random variables, the variational RNN (VRNN)1 can model the kind ofvariability observed in highly structured sequential data such as naturalspeech. We empirically evaluate the proposed model against related sequentialmodels on four speech datasets and one handwriting dataset. Our results showthe important roles that latent random variables can play in the RNN dynamichidden state.
arxiv-12000-256 | Knowledge Transfer Pre-training | http://arxiv.org/pdf/1506.02256v1.pdf | author:Zhiyuan Tang, Dong Wang, Yiqiao Pan, Zhiyong Zhang category:cs.LG cs.NE stat.ML published:2015-06-07 summary:Pre-training is crucial for learning deep neural networks. Most of existingpre-training methods train simple models (e.g., restricted Boltzmann machines)and then stack them layer by layer to form the deep structure. This layer-wisepre-training has found strong theoretical foundation and broad empiricalsupport. However, it is not easy to employ such method to pre-train modelswithout a clear multi-layer structure,e.g., recurrent neural networks (RNNs).This paper presents a new pre-training approach based on knowledge transferlearning. In contrast to the layer-wise approach which trains model componentsincrementally, the new approach trains the entire model as a whole but with aneasier objective function. This is achieved by utilizing soft targets producedby a prior trained model (teacher model). Compared to the conventionallayer-wise methods, this new method does not care about the model structure, socan be used to pre-train very complex models. Experiments on a speechrecognition task demonstrated that with this approach, complex RNNs can be welltrained with a weaker deep neural network (DNN) model. Furthermore, the newmethod can be combined with conventional layer-wise pre-training to deliveradditional gains.
arxiv-12000-257 | String Gaussian Process Kernels | http://arxiv.org/pdf/1506.02239v1.pdf | author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML published:2015-06-07 summary:We introduce a new class of nonstationary kernels, which we derive ascovariance functions of a novel family of stochastic processes we refer to asstring Gaussian processes (string GPs). We construct string GPs to allow formultiple types of local patterns in the data, while ensuring a mild globalregularity condition. In this paper, we illustrate the efficacy of the approachusing synthetic data and demonstrate that the model outperforms competingapproaches on well studied, real-life datasets that exhibit nonstationaryfeatures.
arxiv-12000-258 | Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex Losses | http://arxiv.org/pdf/1506.02227v1.pdf | author:Dominik Csiba, Peter Richt√°rik category:math.OC cs.DS cs.LG stat.ML published:2015-06-07 summary:In this work we develop a new algorithm for regularized empirical riskminimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],which enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.Moreover, our method is able to better utilize the information in the datadefining the ERM problem. For convex loss functions, our complexity resultsmatch those of QUARTZ, which is a primal-dual method also allowing forarbitrary mini-batching schemes. The advantage of a dual-free analysis comesfrom the fact that it guarantees convergence even for non-convex lossfunctions, as long as the average loss is convex. We illustrate throughexperiments the utility of being able to design arbitrary mini-batchingschemes.
arxiv-12000-259 | Boosting Optical Character Recognition: A Super-Resolution Approach | http://arxiv.org/pdf/1506.02211v1.pdf | author:Chao Dong, Ximei Zhu, Yubin Deng, Chen Change Loy, Yu Qiao category:cs.CV I.4.3; I.4.9 published:2015-06-07 summary:Text image super-resolution is a challenging yet open research problem in thecomputer vision community. In particular, low-resolution images hamper theperformance of typical optical character recognition (OCR) systems. In thisarticle, we summarize our entry to the ICDAR2015 Competition on Text ImageSuper-Resolution. Experiments are based on the provided ICDAR2015 TextSRdataset and the released Tesseract-OCR 3.02 system. We report that our winningentry of text image super-resolution framework has largely improved the OCRperformance with low-resolution images used as input, reaching an OCR accuracyscore of 77.19%, which is comparable with that of using the originalhigh-resolution images 78.80%.
arxiv-12000-260 | Describing Common Human Visual Actions in Images | http://arxiv.org/pdf/1506.02203v1.pdf | author:Matteo Ruggero Ronchi, Pietro Perona category:cs.CV published:2015-06-07 summary:Which common human actions and interactions are recognizable in monocularstill images? Which involve objects and/or other people? How many is a personperforming at a time? We address these questions by exploring the actions andinteractions that are detectable in the images of the MS COCO dataset. We maketwo main contributions. First, a list of 140 common `visual actions', obtainedby analyzing the largest on-line verb lexicon currently available for English(VerbNet) and human sentences used to describe images in MS COCO. Second, acomplete set of annotations for those `visual actions', composed ofsubject-object and associated verb, which we call COCO-a (a for `actions').COCO-a is larger than existing action datasets in terms of number of actionsand instances of these actions, and is unique because it is data-driven, ratherthan experimenter-biased. Other unique features are that it is exhaustive, andthat all subjects and objects are localized. A statistical analysis of theaccuracy of our annotations and of each action, interaction and subject-objectcombination is provided.
arxiv-12000-261 | Visual Learning of Arithmetic Operations | http://arxiv.org/pdf/1506.02264v2.pdf | author:Yedid Hoshen, Shmuel Peleg category:cs.LG cs.AI cs.CV published:2015-06-07 summary:A simple Neural Network model is presented for end-to-end visual learning ofarithmetic operations from pictures of numbers. The input consists of twopictures, each showing a 7-digit number. The output, also a picture, displaysthe number showing the result of an arithmetic operation (e.g., addition orsubtraction) on the two input numbers. The concepts of a number, or of anoperator, are not explicitly introduced. This indicates that addition is asimple cognitive task, which can be learned visually using a very small numberof neurons. Other operations, e.g., multiplication, were not learnable using thisarchitecture. Some tasks were not learnable end-to-end (e.g., addition withRoman numerals), but were easily learnable once broken into two separatesub-tasks: a perceptual \textit{Character Recognition} and cognitive\textit{Arithmetic} sub-tasks. This indicates that while some tasks may beeasily learnable end-to-end, other may need to be broken into sub-tasks.
arxiv-12000-262 | No penalty no tears: Least squares in high-dimensional linear models | http://arxiv.org/pdf/1506.02222v4.pdf | author:Xiangyu Wang, David Dunson, Chenlei Leng category:stat.ME cs.LG math.ST stat.ML stat.TH published:2015-06-07 summary:Ordinary least squares (OLS) is the default method for fitting linear models,but is not applicable for problems with dimensionality larger than the samplesize. For these problems, we advocate the use of a generalized version of OLSmotivated by ridge regression, and propose two novel three-step algorithmsinvolving least squares fitting and hard thresholding. The algorithms aremethodologically simple to understand intuitively, computationally easy toimplement efficiently, and theoretically appealing for choosing modelsconsistently. Numerical exercises comparing our methods with penalization-basedapproaches in simulations and data analyses illustrate the great potential ofthe proposed algorithms.
arxiv-12000-263 | Confounds and Consequences in Geotagged Twitter Data | http://arxiv.org/pdf/1506.02275v2.pdf | author:Umashanthi Pavalanathan, Jacob Eisenstein category:cs.CL published:2015-06-07 summary:Twitter is often used in quantitative studies that identifygeographically-preferred topics, writing styles, and entities. These studiesrely on either GPS coordinates attached to individual messages, or on theuser-supplied location field in each profile. In this paper, we compare thesedata acquisition techniques and quantify the biases that they introduce; wealso measure their effects on linguistic analysis and text-based geolocation.GPS-tagging and self-reported locations yield measurably different corpora, andthese linguistic differences are partially attributable to differences indataset composition by age and gender. Using a latent variable model to induceage and gender, we show how these demographic variables interact with geographyto affect language use. We also show that the accuracy of text-basedgeolocation varies with population demographics, giving the best results formen above the age of 40.
arxiv-12000-264 | Well-posedness of a nonlinear integro-differential problem and its rearranged formulation | http://arxiv.org/pdf/1506.02247v2.pdf | author:Gonzalo Galiano, Emanuele Schiavi, Juli√°n Velasco category:cs.CV published:2015-06-07 summary:We study the existence and uniqueness of solutions of a nonlinearintegro-differential problem which we reformulate introducing the notion of thedecreasing rearrangement of the solution. A dimensional reduction of theproblem is obtained and a detailed analysis of the properties of the solutionsof the model is provided. Finally, a fast numerical method is devised andimplemented to show the performance of the model when typical image processingtasks such as filtering and segmentation are performed.
arxiv-12000-265 | Computationally Efficient Bayesian Learning of Gaussian Process State Space Models | http://arxiv.org/pdf/1506.02267v2.pdf | author:Andreas Svensson, Arno Solin, Simo S√§rkk√§, Thomas B. Sch√∂n category:stat.CO stat.ML published:2015-06-07 summary:Gaussian processes allow for flexible specification of prior assumptions ofunknown dynamics in state space models. We present a procedure for efficientBayesian learning in Gaussian process state space models, where therepresentation is formed by projecting the problem onto a set of approximateeigenfunctions derived from the prior covariance structure. Learning under thisfamily of models can be conducted using a carefully crafted particle MCMCalgorithm. This scheme is computationally efficient and yet allows for a fullyBayesian treatment of the problem. Compared to conventional systemidentification tools or existing learning methods, we show competitiveperformance and reliable quantification of uncertainties in the model.
arxiv-12000-266 | Generalized Spectral Kernels | http://arxiv.org/pdf/1506.02236v2.pdf | author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML published:2015-06-07 summary:In this paper we propose a family of tractable kernels that is dense in thefamily of bounded positive semi-definite functions (i.e. can approximate anybounded kernel with arbitrary precision). We start by discussing the case ofstationary kernels, and propose a family of spectral kernels that extendsexisting approaches such as spectral mixture kernels and sparse spectrumkernels. Our extension has two primary advantages. Firstly, unlike existingspectral approaches that yield infinite differentiability, the kernels weintroduce allow learning the degree of differentiability of the latent functionin Gaussian process (GP) models and functions in the reproducing kernel Hilbertspace (RKHS) in other kernel methods. Secondly, we show that some of thekernels we propose require fewer parameters than existing spectral kernels forthe same accuracy, thereby leading to faster and more robust inference.Finally, we generalize our approach and propose a flexible and tractable familyof spectral kernels that we prove can approximate any continuous boundednonstationary kernel.
arxiv-12000-267 | Hybridized Feature Extraction and Acoustic Modelling Approach for Dysarthric Speech Recognition | http://arxiv.org/pdf/1506.02170v1.pdf | author:Megha Rughani, D. Shivakrishna category:cs.SD cs.CL published:2015-06-06 summary:Dysarthria is malfunctioning of motor speech caused by faintness in the humannervous system. It is characterized by the slurred speech along with physicalimpairment which restricts their communication and creates the lack ofconfidence and affects the lifestyle. This paper attempt to increase theefficiency of Automatic Speech Recognition (ASR) system for unimpaired speechsignal. It describes state of art of research into improving ASR for speakerswith dysarthria by means of incorporated knowledge of their speech production.Hybridized approach for feature extraction and acoustic modelling techniquealong with evolutionary algorithm is proposed for increasing the efficiency ofthe overall system. Here number of feature vectors are varied and tested thesystem performance. It is observed that system performance is boosted bygenetic algorithm. System with 16 acoustic features optimized with geneticalgorithm has obtained highest recognition rate of 98.28% with training time of5:30:17.
arxiv-12000-268 | Riemannian preconditioning for tensor completion | http://arxiv.org/pdf/1506.02159v1.pdf | author:Hiroyuki Kasai, Bamdev Mishra category:cs.NA cs.LG math.OC published:2015-06-06 summary:We propose a novel Riemannian preconditioning approach for the tensorcompletion problem with rank constraint. A Riemannian metric or inner productis proposed that exploits the least-squares structure of the cost function andtakes into account the structured symmetry in Tucker decomposition. Thespecific metric allows to use the versatile framework of Riemannianoptimization on quotient manifolds to develop a preconditioned nonlinearconjugate gradient algorithm for the problem. To this end, concrete matrixrepresentations of various optimization-related ingredients are listed.Numerical comparisons suggest that our proposed algorithm robustly outperformsstate-of-the-art algorithms across different problem instances encompassingvarious synthetic and real-world datasets.
arxiv-12000-269 | Learning Multiple Tasks with Deep Relationship Networks | http://arxiv.org/pdf/1506.02117v1.pdf | author:Mingsheng Long, Jianmin Wang category:cs.LG published:2015-06-06 summary:Deep neural networks trained on large-scale dataset can learn transferablefeatures that promote learning multiple tasks for inductive transfer andlabeling mitigation. As deep features eventually transition from general tospecific along the network, a fundamental problem is how to exploit therelationship structure across different tasks while accounting for the featuretransferability in the task-specific layers. In this work, we propose a novelDeep Relationship Network (DRN) architecture for multi-task learning bydiscovering correlated tasks based on multiple task-specific layers of a deepconvolutional neural network. DRN models the task relationship by imposingmatrix normal priors over the network parameters of all task-specific layers,including higher feature layers and classifier layer that are not transferablesafely. By jointly learning the transferable features and task relationships,DRN is able to alleviate the dilemma of negative-transfer in the feature layersand under-transfer in the classifier layer. Empirical evidence shows that DRNyields state-of-the-art classification results on standard multi-domain objectrecognition datasets.
arxiv-12000-270 | Thresholding for Top-k Recommendation with Temporal Dynamics | http://arxiv.org/pdf/1506.02190v2.pdf | author:Lei Tang category:cs.IR cs.LG published:2015-06-06 summary:This work focuses on top-k recommendation in domains where underlying datadistribution shifts overtime. We propose to learn a time-dependent bias foreach item over whatever existing recommendation engine. Such a bias learningprocess alleviates data sparsity in constructing the engine, and at the sametime captures recent trend shift observed in data. We present an alternatingoptimization framework to resolve the bias learning problem, and developmethods to handle a variety of commonly used recommendation evaluationcriteria, as well as large number of items and users in practice. The proposedalgorithm is examined, both offline and online, using real world data setscollected from the largest retailer worldwide. Empirical results demonstratethat the bias learning can almost always boost recommendation performance. Weencourage other practitioners to adopt it as a standard component inrecommender systems where temporal dynamics is a norm.
arxiv-12000-271 | Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks Using a Polynomial Number of Score Evaluations | http://arxiv.org/pdf/1506.02113v1.pdf | author:David Maxwell Chickering, Christopher Meek category:cs.LG cs.AI published:2015-06-06 summary:We introduce Selective Greedy Equivalence Search (SGES), a restricted versionof Greedy Equivalence Search (GES). SGES retains the asymptotic correctness ofGES but, unlike GES, has polynomial performance guarantees. In particular, weshow that when data are sampled independently from a distribution that isperfect with respect to a DAG ${\cal G}$ defined over the observable variablesthen, in the limit of large data, SGES will identify ${\cal G}$'s equivalenceclass after a number of score evaluations that is (1) polynomial in the numberof nodes and (2) exponential in various complexity measures includingmaximum-number-of-parents, maximum-clique-size, and a new measure called {\emv-width} that is at least as small as---and potentially much smaller than---theother two. More generally, we show that for any hereditary andequivalence-invariant property $\Pi$ known to hold in ${\cal G}$, we retain thelarge-sample optimality guarantees of GES even if we ignore any GES deletionoperator during the backward phase that results in a state for which $\Pi$ doesnot hold in the common-descendants subgraph.
arxiv-12000-272 | Data-Driven Learning of the Number of States in Multi-State Autoregressive Models | http://arxiv.org/pdf/1506.02107v3.pdf | author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:stat.ML cs.LG published:2015-06-06 summary:In this work, we consider the class of multi-state autoregressive processesthat can be used to model non-stationary time-series of interest. In order tocapture different autoregressive (AR) states underlying an observed timeseries, it is crucial to select the appropriate number of states. We propose anew model selection technique based on the Gap statistics, which uses a nullreference distribution on the stable AR filters to check whether adding a newAR state significantly improves the performance of the model. To that end, wedefine a new distance measure between AR filters based on mean squaredprediction error (MSPE), and propose an efficient method to generate randomstable filters that are uniformly distributed in the coefficient space.Numerical results are provided to evaluate the performance of the proposedapproach.
arxiv-12000-273 | Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs | http://arxiv.org/pdf/1506.02162v2.pdf | author:Shahin Jabbari, Ryan Rogers, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.GT cs.LG published:2015-06-06 summary:We define and study the problem of predicting the solution to a linearprogram, given only partial information about its objective and constraints.This generalizes the problem of learning to predict the purchasing behavior ofa rational agent who has an unknown objective function, which has been studiedunder the name "Learning from Revealed Preferences". We give mistake boundlearning algorithms in two settings: in the first, the objective of the linearprogram is known to the learner, but there is an arbitrary, fixed set ofconstraints which are unknown. Each example given to the learner is defined byan additional, known constraint, and the goal of the learner is to predict theoptimal solution of the linear program given the union of the known and unknownconstraints. This models, among other things, the problem of predicting thebehavior of a rational agent whose goals are known, but whose resources areunknown. In the second setting, the objective of the linear program is unknown,and changing in a controlled way. The constraints of the linear program mayalso change every day, but are known. An example is given by a set ofconstraints and partial information about the objective, and the task of thelearner is again to predict the optimal solution of the partially known linearprogram.
arxiv-12000-274 | First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos | http://arxiv.org/pdf/1506.02184v1.pdf | author:Jun Ye, Hao Hu, Kai Li, Guo-Jun Qi, Kien A. Hua category:cs.CV published:2015-06-06 summary:With the prevalence of the commodity depth cameras, the new paradigm of userinterfaces based on 3D motion capturing and recognition have dramaticallychanged the way of interactions between human and computers. Human actionrecognition, as one of the key components in these devices, plays an importantrole to guarantee the quality of user experience. Although the model-drivenmethods have achieved huge success, they cannot provide a scalable solution forefficiently storing, retrieving and recognizing actions in the large-scaleapplications. These models are also vulnerable to the temporal translation andwarping, as well as the variations in motion scales and execution rates. Toaddress these challenges, we propose to treat the 3D human action recognitionas a video-level hashing problem and propose a novel First-Take-All (FTA)Hashing algorithm capable of hashing the entire video into hash codes of fixedlength. We demonstrate that this FTA algorithm produces a compactrepresentation of the video invariant to the above mentioned variations,through which action recognition can be solved by an efficient nearest neighborsearch by the Hamming distance between the FTA hash codes. Experiments on thepublic 3D human action datasets shows that the FTA algorithm can reach arecognition accuracy higher than 80%, with about 15 bits per frame consideringthere are 65 frames per video over the datasets.
arxiv-12000-275 | Capturing Hands in Action using Discriminative Salient Points and Physics Simulation | http://arxiv.org/pdf/1506.02178v4.pdf | author:Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, Juergen Gall category:cs.CV published:2015-06-06 summary:Hand motion capture is a popular research field, recently gaining moreattention due to the ubiquity of RGB-D sensors. However, even most recentapproaches focus on the case of a single isolated hand. In this work, we focuson hands that interact with other hands or objects and present a framework thatsuccessfully captures motion in such interaction scenarios for both rigid andarticulated objects. Our framework combines a generative model withdiscriminatively trained salient points to achieve a low tracking error andwith collision detection and physics simulation to achieve physically plausibleestimates even in case of occlusions and missing visual data. Since allcomponents are unified in a single objective function which is almosteverywhere differentiable, it can be optimized with standard optimizationtechniques. Our approach works for monocular RGB-D sequences as well as setupswith multiple synchronized RGB cameras. For a qualitative and quantitativeevaluation, we captured 29 sequences with a large variety of interactions andup to 150 degrees of freedom.
arxiv-12000-276 | Fast Mixing for Discrete Point Processes | http://arxiv.org/pdf/1506.02194v1.pdf | author:Patrick Rebeschini, Amin Karbasi category:stat.ML math.ST stat.TH published:2015-06-06 summary:We investigate the systematic mechanism for designing fast mixing Markovchain Monte Carlo algorithms to sample from discrete point processes under theDobrushin uniqueness condition for Gibbs measures. Discrete point processes aredefined as probability distributions $\mu(S)\propto \exp(\beta f(S))$ over allsubsets $S\in 2^V$ of a finite set $V$ through a bounded set function$f:2^V\rightarrow \mathbb{R}$ and a parameter $\beta>0$. A subclass of discretepoint processes characterized by submodular functions (which includelog-submodular distributions, submodular point processes, and determinantalpoint processes) has recently gained a lot of interest in machine learning andshown to be effective for modeling diversity and coverage. We show that if theset function (not necessarily submodular) displays a natural notion of decay ofcorrelation, then, for $\beta$ small enough, it is possible to design fastmixing Markov chain Monte Carlo methods that yield error bounds on marginalapproximations that do not depend on the size of the set $V$. The sufficientconditions that we derive involve a control on the (discrete) Hessian of setfunctions, a quantity that has not been previously considered in theliterature. We specialize our results for submodular functions, and we discusscanonical examples where the Hessian can be easily controlled.
arxiv-12000-277 | Optimal Rates for Random Fourier Features | http://arxiv.org/pdf/1506.02155v2.pdf | author:Bharath K. Sriperumbudur, Zoltan Szabo category:math.ST cs.LG math.FA stat.ML stat.TH published:2015-06-06 summary:Kernel methods represent one of the most powerful tools in machine learningto tackle problems expressed in terms of function values and derivatives due totheir capability to represent and model complex relations. While these methodsshow good versatility, they are computationally intensive and have poorscalability to large data as they require operations on Gram matrices. In orderto mitigate this serious computational limitation, recently randomizedconstructions have been proposed in the literature, which allow the applicationof fast linear algorithms. Random Fourier features (RFF) are among the mostpopular and widely applied constructions: they provide an easily computable,low-dimensional feature representation for shift-invariant kernels. Despite thepopularity of RFFs, very little is understood theoretically about theirapproximation quality. In this paper, we provide a detailed finite-sampletheoretical analysis about the approximation quality of RFFs by (i)establishing optimal (in terms of the RFF dimension, and growing set size)performance guarantees in uniform norm, and (ii) presenting guarantees in $L^r$($1\le r<\infty$) norms. We also propose an RFF approximation to derivatives ofa kernel with a theoretical study on its approximation quality.
arxiv-12000-278 | Deeply Learning the Messages in Message Passing Inference | http://arxiv.org/pdf/1506.02108v3.pdf | author:Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel category:cs.CV cs.LG stat.ML published:2015-06-06 summary:Deep structured output learning shows great promise in tasks like semanticimage segmentation. We proffer a new, efficient deep structured model learningscheme, in which we show how deep Convolutional Neural Networks (CNNs) can beused to estimate the messages in message passing inference for structuredprediction with Conditional Random Fields (CRFs). With such CNN messageestimators, we obviate the need to learn or evaluate potential functions formessage calculation. This confers significant efficiency for learning, sinceotherwise when performing structured learning for a CRF with CNN potentials itis necessary to undertake expensive inference for every stochastic gradientiteration. The network output dimension for message estimation is the same asthe number of classes, in contrast to the network output for general CNNpotential functions in CRFs, which is exponential in the order of thepotentials. Hence CNN message learning has fewer network parameters and is morescalable for cases that a large number of classes are involved. We apply ourmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. Weachieve an intersection-over-union score of 73.4 on its test set, which is thebest reported result for methods using the VOC training images alone. Thisimpressive performance demonstrates the effectiveness and usefulness of our CNNmessage learning method.
arxiv-12000-279 | Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference | http://arxiv.org/pdf/1506.02158v6.pdf | author:Yarin Gal, Zoubin Ghahramani category:stat.ML cs.LG published:2015-06-06 summary:Convolutional neural networks (CNNs) work well on large datasets. Butlabelled data is hard to collect, and in some applications larger amounts ofdata are not available. The problem then is how to use CNNs with small data --as CNNs overfit quickly. We present an efficient Bayesian CNN, offering betterrobustness to over-fitting on small data than traditional approaches. This isby placing a probability distribution over the CNN's kernels. We approximateour model's intractable posterior with Bernoulli variational distributions,requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximateinference in Bayesian neural networks. This allows us to implement our modelusing existing tools in deep learning with no increase in time complexity,while highlighting a negative result in the field. We show a considerableimprovement in classification accuracy compared to standard techniques andimprove on published state-of-the-art results for CIFAR-10.
arxiv-12000-280 | Approximating Likelihood Ratios with Calibrated Discriminative Classifiers | http://arxiv.org/pdf/1506.02169v2.pdf | author:Kyle Cranmer, Juan Pavez, Gilles Louppe category:stat.AP stat.ML published:2015-06-06 summary:In many fields of science, generalized likelihood ratio tests are establishedtools for statistical inference. At the same time, it has become increasinglycommon that a simulator (or generative model) is used to describe complexprocesses that tie parameters $\theta$ of an underlying theory and measurementapparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$.However, simulator often do not provide a way to evaluate the likelihoodfunction for a given observation $\mathbf{x}$, which motivates a new class oflikelihood-free inference algorithms. In this paper, we show that likelihoodratios are invariant under a specific class of dimensionality reduction maps$\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show thatdiscriminative classifiers can be used to approximate the generalizedlikelihood ratio statistic when only a generative model for the data isavailable. This leads to a new machine learning-based approach tolikelihood-free inference that is complementary to Approximate BayesianComputation, and which does not require a prior on the model parameters.Experimental results on artificial problems with known exact likelihoodsillustrate the potential of the proposed method.
arxiv-12000-281 | What's the point: Semantic segmentation with point supervision | http://arxiv.org/pdf/1506.02106v4.pdf | author:Amy Bearman, Olga Russakovsky, Vittorio Ferrari, Li Fei-Fei category:cs.CV published:2015-06-06 summary:The semantic image segmentation task presents a trade-off between testaccuracy and the cost of obtaining training annotations. Detailed per-pixelannotations enable training accurate models but are very expensive to obtain;image-level class labels are an order of magnitude cheaper but result in lessaccurate models. We take a natural step from image-level annotation towardsstronger supervision: we ask annotators to point to an object if one exists. Wedemonstrate that this adds negligible additional annotation cost. Weincorporate this point supervision along with a novel objectness potential inthe training loss function of a state-of-the-art CNN model. The combined effectof these two extensions is a 12.9% increase in mean intersection over union onthe PASCAL VOC 2012 segmentation task compared to a CNN model trained with onlyimage-level labels.
arxiv-12000-282 | Constrained Convex Neyman-Pearson Classification Using an Outer Approximation Splitting Method | http://arxiv.org/pdf/1506.02196v2.pdf | author:Michel Barlaud, Wafa Belhajali, Patrick L. Combettes, Lionel Fillatre category:math.ST stat.ML stat.TH published:2015-06-06 summary:We propose an efficient splitting algorithm for solving Neyman-Pearsonclassification problems, which consist in minimizing the type II risk subjectto an upper bound constraint on the type I risk. Since the 1/0 loss function isnot convex, it is customary to replace it by convex surrogates that lead tomanageable optimization problems. While statistical bounds have been be derivedto quantify the cost of using such surrogates, no specific algorithm has yetbeen proposed to solve exactly the resulting constrained minimization problemand existing work has addressed only Langragian approximations. Thecontribution of this paper is to propose an efficient splitting algorithm toaddress this issue. Our method alternates a gradient step on the objective anda projection step onto the lower level set modeling the constraint. Theprojection step is implemented via an outer approximation scheme in which theconstraint set is approximated by a sequence of simple convex sets consistingof the intersection of two half-spaces. Convergence of the iterates generatedby the algorithm is established. Experiments on both synthetic and biologicaldata show that our algorithm outperforms state of the art Lagrangian methodssuch as $\nu$-SVM.
arxiv-12000-283 | Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning | http://arxiv.org/pdf/1506.02142v4.pdf | author:Yarin Gal, Zoubin Ghahramani category:stat.ML cs.LG published:2015-06-06 summary:Deep learning tools have gained tremendous attention in applied machinelearning. However such tools for regression and classification do not capturemodel uncertainty. In comparison, Bayesian models offer a mathematicallygrounded framework to reason about model uncertainty, but usually come with aprohibitive computational cost. In this paper we develop a new theoreticalframework casting dropout training in deep neural networks (NNs) as approximateBayesian inference in deep Gaussian processes. A direct result of this theorygives us tools to model uncertainty with dropout NNs -- extracting informationfrom existing models that has been thrown away so far. This mitigates theproblem of representing uncertainty in deep learning without sacrificing eithercomputational complexity or test accuracy. We perform an extensive study of theproperties of dropout's uncertainty. Various network architectures andnon-linearities are assessed on tasks of regression and classification, usingMNIST as an example. We show a considerable improvement in predictivelog-likelihood and RMSE compared to existing state-of-the-art methods, andfinish by using dropout's uncertainty in deep reinforcement learning.
arxiv-12000-284 | Color Constancy by Learning to Predict Chromaticity from Luminance | http://arxiv.org/pdf/1506.02167v2.pdf | author:Ayan Chakrabarti category:cs.CV published:2015-06-06 summary:Color constancy is the recovery of true surface color from observed color,and requires estimating the chromaticity of scene illumination to correct forthe bias it induces. In this paper, we show that the per-pixel color statisticsof natural scenes---without any spatial or semantic context---can by themselvesbe a powerful cue for color constancy. Specifically, we describe an illuminantestimation method that is built around a "classifier" for identifying the truechromaticity of a pixel given its luminance (absolute brightness across colorchannels). During inference, each pixel's observed color restricts its truechromaticity to those values that can be explained by one of a candidate set ofilluminants, and applying the classifier over these values yields adistribution over the corresponding illuminants. A global estimate for thescene illuminant is computed through a simple aggregation of thesedistributions across all pixels. We begin by simply defining theluminance-to-chromaticity classifier by computing empirical histograms overdiscretized chromaticity and luminance values from a training set of naturalimages. These histograms reflect a preference for hues corresponding to smoothreflectance functions, and for achromatic colors in brighter pixels. Despiteits simplicity, the resulting estimation algorithm outperforms currentstate-of-the-art color constancy methods. Next, we propose a method to learnthe luminance-to-chromaticity classifier "end-to-end". Using stochasticgradient descent, we set chromaticity-luminance likelihoods to minimize errorsin the final scene illuminant estimates on a training set. This leads tofurther improvements in accuracy, most significantly in the tail of the errordistribution.
arxiv-12000-285 | Dropout as a Bayesian Approximation: Appendix | http://arxiv.org/pdf/1506.02157v4.pdf | author:Yarin Gal, Zoubin Ghahramani category:stat.ML published:2015-06-06 summary:We show that a neural network with arbitrary depth and non-linearities, withdropout applied before every weight layer, is mathematically equivalent to anapproximation to a well known Bayesian model. This interpretation offers anexplanation to some of dropout's key properties, such as its robustness toover-fitting. Our interpretation allows us to reason about uncertainty in deeplearning, and allows the introduction of the Bayesian machinery into existingdeep learning frameworks in a principled way. This document is an appendix for the main paper "Dropout as a BayesianApproximation: Representing Model Uncertainty in Deep Learning" by Gal andGhahramani, 2015.
arxiv-12000-286 | Sparse Overcomplete Word Vector Representations | http://arxiv.org/pdf/1506.02004v1.pdf | author:Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, Noah Smith category:cs.CL published:2015-06-05 summary:Current distributed representations of words show little resemblance totheories of lexical semantics. The former are dense and uninterpretable, thelatter largely based on familiar, discrete classes (e.g., supersenses) andrelations (e.g., synonymy and hypernymy). We propose methods that transformword vectors into sparse (and optionally binary) vectors. The resultingrepresentations are more similar to the interpretable features typically usedin NLP, though they are discovered automatically from raw corpora. Because thevectors are highly sparse, they are computationally easy to work with. Mostimportantly, we find that they outperform the original vectors on benchmarktasks.
arxiv-12000-287 | Gene selection for cancer classification using a hybrid of univariate and multivariate feature selection methods | http://arxiv.org/pdf/1506.02085v1.pdf | author:Min Xu, Rudy Setiono category:q-bio.QM cs.CE cs.LG stat.ML published:2015-06-05 summary:Various approaches to gene selection for cancer classification based onmicroarray data can be found in the literature and they may be grouped into twocategories: univariate methods and multivariate methods. Univariate methodslook at each gene in the data in isolation from others. They measure thecontribution of a particular gene to the classification without considering thepresence of the other genes. In contrast, multivariate methods measure therelative contribution of a gene to the classification by taking the other genesin the data into consideration. Multivariate methods select fewer genes ingeneral. However, the selection process of multivariate methods may besensitive to the presence of irrelevant genes, noises in the expression andoutliers in the training data. At the same time, the computational cost ofmultivariate methods is high. To overcome the disadvantages of the two types ofapproaches, we propose a hybrid method to obtain gene sets that are small andhighly discriminative. We devise our hybrid method from the univariate Maximum Likelihood method(LIK) and the multivariate Recursive Feature Elimination method (RFE). Weanalyze the properties of these methods and systematically test theeffectiveness of our proposed method on two cancer microarray datasets. Ourexperiments on a leukemia dataset and a small, round blue cell tumors datasetdemonstrate the effectiveness of our hybrid method. It is able to discover setsconsisting of fewer genes than those reported in the literature and at the sametime achieve the same or better prediction accuracy.
arxiv-12000-288 | Communication Complexity of Distributed Convex Learning and Optimization | http://arxiv.org/pdf/1506.01900v2.pdf | author:Yossi Arjevani, Ohad Shamir category:cs.LG math.OC stat.ML published:2015-06-05 summary:We study the fundamental limits to communication-efficient distributedmethods for convex learning and optimization, under different assumptions onthe information available to individual machines, and the types of functionsconsidered. We identify cases where existing algorithms are already worst-caseoptimal, as well as cases where room for further improvement is still possible.Among other things, our results indicate that without similarity between thelocal objective functions (due to statistical data similarity or otherwise)many communication rounds may be required, even if the machines have unboundedcomputational power.
arxiv-12000-289 | Global Gene Expression Analysis Using Machine Learning Methods | http://arxiv.org/pdf/1506.02087v1.pdf | author:Min Xu category:q-bio.QM cs.CE cs.LG stat.ML published:2015-06-05 summary:Microarray is a technology to quantitatively monitor the expression of largenumber of genes in parallel. It has become one of the main tools for globalgene expression analysis in molecular biology research in recent years. Thelarge amount of expression data generated by this technology makes the study ofcertain complex biological problems possible and machine learning methods areplaying a crucial role in the analysis process. At present, many machinelearning methods have been or have the potential to be applied to major areasof gene expression analysis. These areas include clustering, classification,dynamic modeling and reverse engineering. In this thesis, we focus our work on using machine learning methods to solvethe classification problems arising from microarray data. We first identify themajor types of the classification problems; then apply several machine learningmethods to solve the problems and perform systematic tests on real andartificial datasets. We propose improvement to existing methods. Specifically,we develop a multivariate and a hybrid feature selection method to obtain highclassification performance for high dimension classification problems. Usingthe hybrid feature selection method, we are able to identify small sets offeatures that give predictive accuracy that is as good as that from othermethods which require many more features.
arxiv-12000-290 | Content Translation: Computer-assisted translation tool for Wikipedia articles | http://arxiv.org/pdf/1506.01914v1.pdf | author:Niklas Laxstr√∂m, Pau Giner, Santhosh Thottingal category:cs.CL published:2015-06-05 summary:The quality and quantity of articles in each Wikipedia language variesgreatly. Translating from another Wikipedia is a natural way to add morecontent, but the translation process is not properly supported in the softwareused by Wikipedia. Past computer-assisted translation tools built for Wikipediaare not commonly used. We created a tool that adapts to the specific needs ofan open community and to the kind of content in Wikipedia. Qualitative andquantitative data indicates that the new tool helps users translate articleseasier and faster.
arxiv-12000-291 | Large-scale Simple Question Answering with Memory Networks | http://arxiv.org/pdf/1506.02075v1.pdf | author:Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston category:cs.LG cs.CL published:2015-06-05 summary:Training large-scale question answering systems is complicated becausetraining sources usually cover a small portion of the range of possiblequestions. This paper studies the impact of multitask and transfer learning forsimple question answering; a setting for which the reasoning required to answeris quite easy, as long as one can retrieve the correct evidence given aquestion, which can be difficult in large-scale conditions. To this end, weintroduce a new dataset of 100k questions that we use in conjunction withexisting benchmarks. We conduct our study within the framework of MemoryNetworks (Weston et al., 2015) because this perspective allows us to eventuallyscale up to more complex reasoning, and show that Memory Networks can besuccessfully trained to achieve excellent performance.
arxiv-12000-292 | Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial Sentiment Analysis | http://arxiv.org/pdf/1506.01906v1.pdf | author:Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-06-05 summary:Although, the fair amount of works in sentiment analysis (SA) and opinionmining (OM) systems in the last decade and with respect to the performance ofthese systems, but it still not desired performance, especially formorphologically-Rich Language (MRL) such as Arabic, due to the complexities andchallenges exist in the nature of the languages itself. One of these challengesis the detection of idioms or proverbs phrases within the writer text orcomment. An idiom or proverb is a form of speech or an expression that ispeculiar to itself. Grammatically, it cannot be understood from the individualmeanings of its elements and can yield different sentiment when treats asseparate words. Consequently, In order to facilitate the task of detection andclassification of lexical phrases for automated SA systems, this paper presentsAIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic(MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentencelevel with semantic orientation (positive or negative). The efforts of manuallybuilding and annotating the lexicon are reported. Moreover, we build aclassifier that extracts idioms and proverbs, phrases from text using n-gramand similarity measure methods. Finally, several experiments were carried outon various data, including Arabic tweets and Arabic microblogs (hotelreservation, product reviews, and TV program comments) from publicly availableArabic online reviews websites (social media, blogs, forums, e-commerce websites) to evaluate the coverage and accuracy of AIPSeLEX.
arxiv-12000-293 | Semidefinite and Spectral Relaxations for Multi-Label Classification | http://arxiv.org/pdf/1506.01829v1.pdf | author:R√©mi Lajugie, Piotr Bojanowski, Sylvain Arlot, Francis Bach category:cs.LG published:2015-06-05 summary:In this paper, we address the problem of multi-label classification. Weconsider linear classifiers and propose to learn a prior over the space oflabels to directly leverage the performance of such methods. This prior takesthe form of a quadratic function of the labels and permits to encode bothattractive and repulsive relations between labels. We cast this problem as astructured prediction one aiming at optimizing either the accuracies of thepredictors or the F 1-score. This leads to an optimization problem closelyrelated to the max-cut problem, which naturally leads to semidefinite andspectral relaxations. We show on standard datasets how such a general prior canimprove the performances of multi-label techniques.
arxiv-12000-294 | Local Nonstationarity for Efficient Bayesian Optimization | http://arxiv.org/pdf/1506.02080v1.pdf | author:Ruben Martinez-Cantin category:cs.LG stat.ML published:2015-06-05 summary:Bayesian optimization has shown to be a fundamental global optimizationalgorithm in many applications: ranging from automatic machine learning,robotics, reinforcement learning, experimental design, simulations, etc. Themost popular and effective Bayesian optimization relies on a surrogate model inthe form of a Gaussian process due to its flexibility to represent a prior overfunction. However, many algorithms and setups relies on the stationarityassumption of the Gaussian process. In this paper, we present a novelnonstationary strategy for Bayesian optimization that is able to outperform thestate of the art in Bayesian optimization both in stationary and nonstationaryproblems.
arxiv-12000-295 | Automatic tracking of protein vesicles | http://arxiv.org/pdf/1506.02083v1.pdf | author:Min Xu category:q-bio.QM cs.CV published:2015-06-05 summary:With the advance of fluorescence imaging technologies, recently cellbiologists are able to record the movement of protein vesicles within a livingcell. Automatic tracking of the movements of these vesicles become key forqualitative analysis of dynamics of theses vesicles. In this thesis, weformulate such tracking problem as video object tracking problem, and design adynamic programming method for tracking single object. Our experiments onsimulation data show that the method can identify a track with high accuracywhich is robust to the choose of tracking parameters and presence of high levelnoise. We then extend this method to the tracking multiple objects using thetrack elimination strategy. In multiple object tracking, the above approachoften fails to correctly identify a track when two tracks cross. We solve thisproblem by incorporating the Kalman filter into the dynamic programmingframework. Our experiments on simulated data show that the tracking accuracy issignificantly improved.
arxiv-12000-296 | High-dimensional Ordinary Least-squares Projection for Screening Variables | http://arxiv.org/pdf/1506.01782v1.pdf | author:Xiangyu Wang, Chenlei Leng category:stat.ME math.ST stat.ML stat.TH published:2015-06-05 summary:Variable selection is a challenging issue in statistical applications whenthe number of predictors $p$ far exceeds the number of observations $n$. Inthis ultra-high dimensional setting, the sure independence screening (SIS)procedure was introduced to significantly reduce the dimensionality bypreserving the true model with overwhelming probability, before a refinedsecond stage analysis. However, the aforementioned sure screening propertystrongly relies on the assumption that the important variables in the modelhave large marginal correlations with the response, which rarely holds inreality. To overcome this, we propose a novel and simple screening techniquecalled the high-dimensional ordinary least-squares projection (HOLP). We showthat HOLP possesses the sure screening property and gives consistent variableselection without the strong correlation assumption, and has a lowcomputational complexity. A ridge type HOLP procedure is also discussed.Simulation study shows that HOLP performs competitively compared to many othermarginal correlation based methods. An application to a mammalian eye diseasedata illustrates the attractiveness of HOLP.
arxiv-12000-297 | Learning to track for spatio-temporal action localization | http://arxiv.org/pdf/1506.01929v2.pdf | author:Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV published:2015-06-05 summary:We propose an effective approach for spatio-temporal action localization inrealistic videos. The approach first detects proposals at the frame-level andscores them with a combination of static and motion CNN features. It thentracks high-scoring proposals throughout the video using atracking-by-detection approach. Our tracker relies simultaneously oninstance-level and class-level detectors. The tracks are scored using aspatio-temporal motion histogram, a descriptor at the track level, incombination with the CNN features. Finally, we perform temporal localization ofthe action using a sliding-window approach at the track level. We presentexperimental results for spatio-temporal localization on the UCF-Sports, J-HMDBand UCF-101 action localization datasets, where our approach outperforms thestate of the art with a margin of 15%, 7% and 12% respectively in mAP.
arxiv-12000-298 | Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video | http://arxiv.org/pdf/1506.01911v3.pdf | author:Lionel Pigou, A√§ron van den Oord, Sander Dieleman, Mieke Van Herreweghe, Joni Dambre category:cs.CV cs.AI cs.LG cs.NE stat.ML published:2015-06-05 summary:Recent studies have demonstrated the power of recurrent neural networks formachine translation, image captioning and speech recognition. For the task ofcapturing temporal structure in video, however, there still remain numerousopen research questions. Current research suggests using a simple temporalfeature pooling strategy to take into account the temporal aspect of video. Wedemonstrate that this method is not sufficient for gesture recognition, wheretemporal information is more discriminative compared to general videoclassification tasks. We explore deep architectures for gesture recognition invideo and propose a new end-to-end trainable neural network architectureincorporating temporal convolutions and bidirectional recurrence. Our maincontributions are twofold; first, we show that recurrence is crucial for thistask; second, we show that adding temporal convolutions leads to significantimprovements. We evaluate the different approaches on the Montalbano gesturerecognition dataset, where we achieve state-of-the-art results.
arxiv-12000-299 | Sentence Directed Video Object Codetection | http://arxiv.org/pdf/1506.02059v2.pdf | author:Haonan Yu, Jeffrey Mark Siskind category:cs.CV published:2015-06-05 summary:We tackle the problem of video object codetection by leveraging the weaksemantic constraint implied by sentences that describe the video content.Unlike most existing work that focuses on codetecting large objects which areusually salient both in size and appearance, we can codetect objects that aresmall or medium sized. Our method assumes no human pose or depth informationsuch as is required by the most recent state-of-the-art method. We employ weaksemantic constraint on the codetection process by pairing the video withsentences. Although the semantic information is usually simple and weak, it cangreatly boost the performance of our codetection framework by reducing thesearch space of the hypothesized object detections. Our experiment demonstratesan average IoU score of 0.423 on a new challenging dataset which contains 15object classes and 150 videos with 12,509 frames in total, and an average IoUscore of 0.373 on a subset of an existing dataset, originally intended foractivity recognition, which contains 5 object classes and 75 videos with 8,854frames in total.
arxiv-12000-300 | Visualizing and Understanding Recurrent Networks | http://arxiv.org/pdf/1506.02078v2.pdf | author:Andrej Karpathy, Justin Johnson, Li Fei-Fei category:cs.LG cs.CL cs.NE published:2015-06-05 summary:Recurrent Neural Networks (RNNs), and specifically a variant with LongShort-Term Memory (LSTM), are enjoying renewed interest as a result ofsuccessful applications in a wide range of machine learning problems thatinvolve sequential data. However, while LSTMs provide exceptional results inpractice, the source of their performance and their limitations remain ratherpoorly understood. Using character-level language models as an interpretabletestbed, we aim to bridge this gap by providing an analysis of theirrepresentations, predictions and error types. In particular, our experimentsreveal the existence of interpretable cells that keep track of long-rangedependencies such as line lengths, quotes and brackets. Moreover, ourcomparative analysis with finite horizon n-gram models traces the source of theLSTM improvements to long-range structural dependencies. Finally, we provideanalysis of the remaining errors and suggests areas for further study.
