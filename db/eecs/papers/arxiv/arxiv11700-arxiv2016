arxiv-11700-1 | Image Representations and New Domains in Neural Image Captioning | http://arxiv.org/pdf/1508.02091v1.pdf | author:Jack Hessel, Nicolas Savva, Michael J. Wilber category:cs.CL cs.CV published:2015-08-09 summary:We examine the possibility that recent promising results in automatic captiongeneration are due primarily to language models. By varying imagerepresentation quality produced by a convolutional neural network, we find thata state-of-the-art neural captioning algorithm is able to produce qualitycaptions even when provided with surprisingly poor image representations. Wereplicate this result in a new, fine-grained, transfer learned captioningdomain, consisting of 66K recipe image/title pairs. We also provide someexperiments regarding the appropriateness of datasets for automatic captioning,and find that having multiple captions per image is beneficial, but not anabsolute requirement.
arxiv-11700-2 | Sensitivity study using machine learning algorithms on simulated r-mode gravitational wave signals from newborn neutron stars | http://arxiv.org/pdf/1508.02064v1.pdf | author:Antonis Mytidis, Athanasios A. Panagopoulos, Orestis P. Panagopoulos, Bernard Whiting category:astro-ph.IM cs.LG published:2015-08-09 summary:This is a follow-up sensitivity study on r-mode gravitational wave signalsfrom newborn neutron stars illustrating the applicability of machine learningalgorithms for the detection of long-lived gravitational-wave transients. Inthis sensitivity study we examine three machine learning algorithms (MLAs):artificial neural networks (ANNs), support vector machines (SVMs) andconstrained subspace classifiers (CSCs). The objective of this study is tocompare the detection efficiency that MLAs can achieve with the efficiency ofconventional detection algorithms discussed in an earlier paper. Comparisonsare made using 2 distinct r-mode waveforms. For the training of the MLAs weassumed that some information about the distance to the source is given so thatthe training was performed over distance ranges not wider than half an order ofmagnitude. The results of this study suggest that machine learning algorithmsare suitable for the detection of long-lived gravitational-wave transients andthat when assuming knowledge of the distance to the source, MLAs are at leastas efficient as conventional methods.
arxiv-11700-3 | An Automatic Machine Translation Evaluation Metric Based on Dependency Parsing Model | http://arxiv.org/pdf/1508.01996v1.pdf | author:Hui Yu, Xiaofeng Wu, Wenbin Jiang, Qun Liu, ShouXun Lin category:cs.CL published:2015-08-09 summary:Most of the syntax-based metrics obtain the similarity by comparing thesub-structures extracted from the trees of hypothesis and reference. Thesesub-structures are defined by human and can't express all the information inthe trees because of the limited length of sub-structures. In addition, theoverlapped parts between these sub-structures are computed repeatedly. To avoidthese problems, we propose a novel automatic evaluation metric based ondependency parsing model, with no need to define sub-structures by human.First, we train a dependency parsing model by the reference dependency tree.Then we generate the hypothesis dependency tree and the correspondingprobability by the dependency parsing model. The quality of the hypothesis canbe judged by this probability. In order to obtain the lexicon similarity, wealso introduce the unigram F-score to the new metric. Experiment results showthat the new metric gets the state-of-the-art performance on system level, andis comparable with METEOR on sentence level.
arxiv-11700-4 | Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures | http://arxiv.org/pdf/1508.01993v1.pdf | author:Ralph Fehrer, Stefan Feuerriegel category:stat.ML cs.CL cs.LG published:2015-08-09 summary:Decision analytics commonly focuses on the text mining of financial newssources in order to provide managerial decision support and to predict stockmarket movements. Existing predictive frameworks almost exclusively applytraditional machine learning methods, whereas recent research indicates thattraditional machine learning methods are not sufficiently capable of extractingsuitable features and capturing the non-linear nature of complex tasks. As aremedy, novel deep learning models aim to overcome this issue by extendingtraditional neural network models with additional hidden layers. Indeed, deeplearning has been shown to outperform traditional methods in terms ofpredictive performance. In this paper, we adapt the novel deep learningtechnique to financial decision support. In this instance, we aim to predictthe direction of stock movements following financial disclosures. As a result,we show how deep learning can outperform the accuracy of random forests as abenchmark for machine learning by 5.66%.
arxiv-11700-5 | Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels | http://arxiv.org/pdf/1412.8293v2.pdf | author:Haim Avron, Vikas Sindhwani, Jiyan Yang, Michael Mahoney category:stat.ML cs.LG math.NA stat.CO published:2014-12-29 summary:We consider the problem of improving the efficiency of randomized Fourierfeature maps to accelerate training and testing speed of kernel methods onlarge datasets. These approximate feature maps arise as Monte Carloapproximations to integral representations of shift-invariant kernel functions(e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo(QMC) approximations instead, where the relevant integrands are evaluated on alow-discrepancy sequence of points as opposed to random point sets as in theMonte Carlo approach. We derive a new discrepancy measure called boxdiscrepancy based on theoretical characterizations of the integration errorwith respect to a given sequence. We then propose to learn QMC sequencesadapted to our setting based on explicit box discrepancy minimization. Ourtheoretical analyses are complemented with empirical results that demonstratethe effectiveness of classical and adaptive QMC techniques for this problem.
arxiv-11700-6 | Bidirectional LSTM-CRF Models for Sequence Tagging | http://arxiv.org/pdf/1508.01991v1.pdf | author:Zhiheng Huang, Wei Xu, Kai Yu category:cs.CL published:2015-08-09 summary:In this paper, we propose a variety of Long Short-Term Memory (LSTM) basedmodels for sequence tagging. These models include LSTM networks, bidirectionalLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work isthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model toNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF modelcan efficiently use both past and future input features thanks to abidirectional LSTM component. It can also use sentence level tag informationthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (orclose to) accuracy on POS, chunking and NER data sets. In addition, it isrobust and has less dependence on word embedding as compared to previousobservations.
arxiv-11700-7 | Adaptive system optimization using random directions stochastic approximation | http://arxiv.org/pdf/1502.05577v2.pdf | author:Prashanth L. A., Shalabh Bhatnagar, Michael Fu, Steve Marcus category:math.OC cs.LG published:2015-02-19 summary:We present novel algorithms for simulation optimization using randomdirections stochastic approximation (RDSA). These include first-order(gradient) as well as second-order (Newton) schemes. We incorporate bothcontinuous-valued as well as discrete-valued perturbations into both ouralgorithms. The former are chosen to be independent and identically distributed(i.i.d.) symmetric, uniformly distributed random variables (r.v.), while thelatter are i.i.d., asymmetric, Bernoulli r.v.s. Our Newton algorithm, with anovel Hessian estimation scheme, requires N-dimensional perturbations and threeloss measurements per iteration, whereas the simultaneous perturbation Newtonsearch algorithm of [1] requires 2N-dimensional perturbations and four lossmeasurements per iteration. We prove the unbiasedness of both gradient andHessian estimates and asymptotic (strong) convergence for both first-order andsecond-order schemes. We also provide asymptotic normality results, which inparticular establish that the asymmetric Bernoulli variant of Newton RDSAmethod is better than 2SPSA of [1]. Numerical experiments are used to validatethe theoretical results.
arxiv-11700-8 | Consistency of random forests | http://arxiv.org/pdf/1405.2881v4.pdf | author:Erwan Scornet, GÃ©rard Biau, Jean-Philippe Vert category:math.ST stat.ML stat.TH published:2014-05-12 summary:Random forests are a learning algorithm proposed by Breiman [Mach. Learn. 45(2001) 5--32] that combines several randomized decision trees and aggregatestheir predictions by averaging. Despite its wide usage and outstandingpractical performance, little is known about the mathematical properties of theprocedure. This disparity between theory and practice originates in thedifficulty to simultaneously analyze both the randomization process and thehighly data-dependent tree structure. In the present paper, we take a stepforward in forest exploration by proving a consistency result for Breiman's[Mach. Learn. 45 (2001) 5--32] original algorithm in the context of additiveregression models. Our analysis also sheds an interesting light on how randomforests can nicely adapt to sparsity. 1. Introduction. Random forests are anensemble learning method for classification and regression that constructs anumber of randomized decision trees during the training phase and predicts byaveraging the results. Since its publication in the seminal paper of Breiman(2001), the procedure has become a major data analysis tool, that performs wellin practice in comparison with many standard methods. What has greatlycontributed to the popularity of forests is the fact that they can be appliedto a wide range of prediction problems and have few parameters to tune. Asidefrom being simple to use, the method is generally recognized for its accuracyand its ability to deal with small sample sizes, high-dimensional featurespaces and complex data structures. The random forest methodology has beensuccessfully involved in many practical problems, including air qualityprediction (winning code of the EMC data science global hackathon in 2012, seehttp://www.kaggle.com/c/dsg-hackathon), chemoinformatics [Svetnik et al.(2003)], ecology [Prasad, Iverson and Liaw (2006), Cutler et al. (2007)], 3D
arxiv-11700-9 | A variational approach to the consistency of spectral clustering | http://arxiv.org/pdf/1508.01928v1.pdf | author:NicolÃ¡s GarcÃ­a Trillos, Dejan SlepÄev category:math.ST cs.LG stat.ML stat.TH published:2015-08-08 summary:This paper establishes the consistency of spectral approaches to dataclustering. We consider clustering of point clouds obtained as samples of aground-truth measure. A graph representing the point cloud is obtained byassigning weights to edges based on the distance between the points theyconnect. We investigate the spectral convergence of both unnormalized andnormalized graph Laplacians towards the appropriate operators in the continuumdomain. We obtain sharp conditions on how the connectivity radius can be scaledwith respect to the number of sample points for the spectral convergence tohold. We also show that the discrete clusters obtained via spectral clusteringconverge towards a continuum partition of the ground truth measure. Suchcontinuum partition minimizes a functional describing the continuum analogue ofthe graph-based spectral partitioning. Our approach, based on variationalconvergence, is general and flexible.
arxiv-11700-10 | The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed Integer Linear Optimization | http://arxiv.org/pdf/1508.01922v1.pdf | author:Rahul Mazumder, Peter Radchenko category:stat.ME math.OC math.ST stat.CO stat.ML stat.TH published:2015-08-08 summary:We propose a new high-dimensional linear regression estimator: the DiscreteDantzig Selector, which minimizes the number of nonzero regressioncoefficients, subject to a budget on the maximal absolute correlation betweenthe features and the residuals. We show that the estimator can be expressed asa solution to a Mixed Integer Linear Optimization (MILO) problem---acomputationally tractable framework that enables the computation of provablyoptimal global solutions. Our approach has the appealing characteristic thateven if we terminate the optimization problem at an early stage, it exits witha certificate of sub-optimality on the quality of the solution. We develop newdiscrete first order methods, motivated by recent algorithmic developments infirst order continuous convex optimization, to obtain high quality feasiblesolutions for the Discrete Dantzig Selector problem. Our proposal leads toadvantages over the off-the-shelf state-of-the-art integer programmingalgorithms, which include superior upper bounds obtained for a givencomputational budget. When a solution obtained from the discrete first ordermethods is passed as a warm-start to a MILO solver, the performance of thelatter improves significantly. Exploiting problem specific information, wepropose enhanced MILO formulations that further improve the algorithmicperformance of the MILO solvers. We demonstrate, both theoretically andempirically, that, in a wide range of regimes, the statistical properties ofthe Discrete Dantzig Selector are superior to those of popular $\ell_{1}$-basedapproaches. For problem instances with $p \approx 2500$ features and $n \approx900$ observations, our computational framework delivers optimal solutions in afew minutes and certifies optimality within an hour.
arxiv-11700-11 | A straightforward method to assess motion blur for different types of displays | http://arxiv.org/pdf/1602.07573v1.pdf | author:Fuhao Chen, Jun Chen, Feng Huang category:cs.CV published:2015-08-08 summary:A simulation method based on the liquid crystal response and the human visualsystem is suitable to characterize motion blur for LCDs but not other displaytypes. We propose a more straightforward and widely applicable method toquantify motion blur based on the width of the moving object. We thus comparevarious types of displays objectively. A perceptual experiment was conducted tovalidate the proposed method. We test varying motion velocities for ninecommercial displays. We compare the three motion blur evaluation methods(simulation, human perception, and our method) using z-scores. Our comparisonsindicate that our method accurately characterizes motion blur for variousdisplay types.
arxiv-11700-12 | Input anticipating critical reservoirs show power law forgetting of unexpected input events | http://arxiv.org/pdf/1404.6334v5.pdf | author:Norbert Michael Mayer category:cs.NE published:2014-04-25 summary:Usually, reservoir computing shows an exponential memory decay. This paperinvestigates under which circumstances echo state networks can show a power lawforgetting. That means traces of earlier events can be found in the reservoirfor very long time spans. Such a setting requires critical connectivity exactlyat the limit of what is permissible according the echo state condition.However, for general matrices the limit cannot be determined exactly fromtheory. In addition, the behavior of the network is strongly influenced by theinput flow. Results are presented that use certain types of restrictedrecurrent connectivity and anticipation learning with regard to the input,where indeed power law forgetting can be achieved.
arxiv-11700-13 | Extensions of stability selection using subsamples of observations and covariates | http://arxiv.org/pdf/1407.4916v3.pdf | author:Andre Beinrucker, ÃrÃ¼n Dogan, Gilles Blanchard category:stat.ME stat.CO stat.ML published:2014-07-18 summary:We introduce extensions of stability selection, a method to stabilisevariable selection methods introduced by Meinshausen and B\"uhlmann (J R StatSoc 72:417-473, 2010). We propose to apply a base selection method repeatedlyto random observation subsamples and covariate subsets under scrutiny, and toselect covariates based on their selection frequency. We analyse the effectsand benefits of these extensions. Our analysis generalizes the theoreticalresults of Meinshausen and B\"uhlmann (J R Stat Soc 72:417-473, 2010) from thecase of half-samples to subsamples of arbitrary size. We study, in atheoretical manner, the effect of taking random covariate subsets using asimplified score model. Finally we validate these extensions on numericalexperiments on both synthetic and real datasets, and compare the obtainedresults in detail to the original stability selection method.
arxiv-11700-14 | Simulation of optical flow and fuzzy based obstacle avoidance system for mobile robots | http://arxiv.org/pdf/1508.01859v1.pdf | author:G. D. Illeperuma, D. U. J. Sonnadara category:cs.CV cs.RO published:2015-08-08 summary:Honey bees use optical flow to avoid obstacles effectively. In this researchwork similar methodology was tested on a simulated mobile robot. Simulationframework was based on VRML and Simulink in a 3D world. Optical flow vectorswere calculated from a video scene captured by a virtual camera which was usedas inputs to a fuzzy logic controller. Fuzzy logic controller decided thelocomotion of the robot. Different fuzzy logic rules were evaluated. The robotwas able to navigate through complex static and dynamic environmentseffectively, avoiding obstacles on its path.
arxiv-11700-15 | Spectral Clustering and Block Models: A Review And A New Algorithm | http://arxiv.org/pdf/1508.01819v1.pdf | author:Sharmodeep Bhattacharyya, Peter J. Bickel category:math.ST cs.SI stat.ML stat.TH published:2015-08-07 summary:We focus on spectral clustering of unlabeled graphs and review some resultson clustering methods which achieve weak or strong consistent identification indata generated by such models. We also present a new algorithm which appears toperform optimally both theoretically using asymptotic theory and empirically.
arxiv-11700-16 | Mimicry Is Presidential: Linguistic Style Matching in Presidential Debates and Improved Polling Numbers | http://arxiv.org/pdf/1508.01786v1.pdf | author:Daniel M. Romero, Roderick I. Swaab, Brian Uzzi, Adam D. Galinsky category:cs.CL cs.SI published:2015-08-07 summary:The current research used the contexts of U.S. presidential debates andnegotiations to examine whether matching the linguistic style of an opponent ina two-party exchange affects the reactions of third-party observers. Buildingoff communication accommodation theory (CAT), interaction alignment theory(IAT), and processing fluency, we propose that language style matching (LSM)will improve subsequent third-party evaluations because matching an opponent'slinguistic style reflects greater perspective taking and will make one'sarguments easier to process. In contrast, research on status inferencespredicts that LSM will negatively impact third-party evaluations because LSMimplies followership. We conduct two studies to test these competinghypotheses. Study 1 analyzed transcripts of U.S. presidential debates between1976 and 2012 and found that candidates who matched their opponent's linguisticstyle increased their standing in the polls. Study 2 demonstrated a causalrelationship between LSM and third-party observer evaluations using negotiationtranscripts.
arxiv-11700-17 | Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking | http://arxiv.org/pdf/1508.01755v1.pdf | author:Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL published:2015-08-07 summary:The natural language generation (NLG) component of a spoken dialogue system(SDS) usually needs a substantial amount of handcrafting or a well-labeleddataset to be trained on. These limitations add significantly to developmentcosts and make cross-domain, multi-lingual dialogue systems intractable.Moreover, human languages are context-aware. The most natural response shouldbe directly learned from data rather than depending on predefined syntaxes orrules. This paper presents a statistical language generator based on a jointrecurrent and convolutional neural network structure which can be trained ondialogue act-utterance pairs without any semantic alignments or predefinedgrammar trees. Objective metrics suggest that this new model outperformsprevious methods under the same experimental conditions. Results of anevaluation by human judges indicate that it produces not only high quality butlinguistically varied utterances which are preferred compared to n-gram andrule-based systems.
arxiv-11700-18 | Agglomerative clustering and collectiveness measure via exponent generating function | http://arxiv.org/pdf/1507.08571v2.pdf | author:Wei-Ya Ren, Shuo-Hao Li, Qiang Guo, Guo-Hui Li, Jun Zhang category:cs.CV cs.GR published:2015-07-30 summary:The key in agglomerative clustering is to define the affinity measure betweentwo sets. A novel agglomerative clustering method is proposed by utilizing thepath integral to define the affinity measure. Firstly, the path integraldescriptor of an edge, a node and a set is computed by path integral andexponent generating function. Then, the affinity measure between two sets isobtained by path integral descriptor of sets. Several good properties of thepath integral descriptor is proposed in this paper. In addition, we give thephysical interpretation of the proposed path integral descriptor of a set. Theproposed path integral descriptor of a set can be regard as the collectivenessmeasure of a set, which can be a moving system such as human crowd, sheep herdand so on. Self-driven particle (SDP) model is used to test the ability of theproposed method in measuring collectiveness.
arxiv-11700-19 | Study of Phonemes Confusions in Hierarchical Automatic Phoneme Recognition System | http://arxiv.org/pdf/1508.01718v1.pdf | author:Rimah Amami, Noureddine Ellouze category:cs.CL published:2015-08-07 summary:In this paper, we have analyzed the impact of confusions on the robustness ofphoneme recognitions system. The confusions are detected at the pronunciationand the confusions matrices of the phoneme recognizer. The confusions show thatsome similarities between phonemes at the pronunciation affect significantlythe recognition rates. This paper proposes to understand those confusions inorder to improve the performance of the phoneme recognition system by isolatingthe problematic phonemes. Confusion analysis leads to build a new hierarchicalrecognizer using new phoneme distribution and the information from theconfusion matrices. This new hierarchical phoneme recognition system showssignificant improvements of the recognition rates on TIMIT database.
arxiv-11700-20 | Dimension reduction for model-based clustering | http://arxiv.org/pdf/1508.01713v1.pdf | author:Luca Scrucca category:stat.ME stat.ML published:2015-08-07 summary:We introduce a dimension reduction method for visualizing the clusteringstructure obtained from a finite mixture of Gaussian densities. Information onthe dimension reduction subspace is obtained from the variation on group meansand, depending on the estimated mixture model, on the variation on groupcovariances. The proposed method aims at reducing the dimensionality byidentifying a set of linear combinations, ordered by importance as quantifiedby the associated eigenvalues, of the original features which capture most ofthe cluster structure contained in the data. Observations may then be projectedonto such a reduced subspace, thus providing summary plots which help tovisualize the clustering structure. These plots can be particularly appealingin the case of high-dimensional data and noisy structure. The new constructedvariables capture most of the clustering information available in the data, andthey can be further reduced to improve clustering performance. We illustratethe approach on both simulated and real data sets.
arxiv-11700-21 | Adaptive Normalized Risk-Averting Training For Deep Neural Networks | http://arxiv.org/pdf/1506.02690v2.pdf | author:Zhiguang Wang, Tim Oates, James Lo category:cs.LG cs.NE stat.ML published:2015-06-08 summary:This paper proposes a set of new error criteria and learning approaches,Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convexoptimization problem in training deep neural networks (DNNs). Theoretically, wedemonstrate its effectiveness on global and local convexity lower-bounded bythe standard $L_p$-norm error. By analyzing the gradient on the convexity index$\lambda$, we explain the reason why to learn $\lambda$ adaptively usinggradient descent works. In practice, we show how this method improves trainingof deep neural networks to solve visual recognition tasks on the MNIST andCIFAR-10 datasets. Without using pretraining or other tricks, we obtain resultscomparable or superior to those reported in recent literature on the same tasksusing standard ConvNets + MSE/cross entropy. Performance on deep/shallowmultilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT canbe combined with other quasi-Newton training methods, innovative networkvariants, regularization techniques and other specific tricks in DNNs. Otherthan unsupervised pretraining, it provides a new perspective to address thenon-convex optimization problem in DNNs.
arxiv-11700-22 | The local convexity of solving systems of quadratic equations | http://arxiv.org/pdf/1506.07868v4.pdf | author:Chris D. White, Sujay Sanghavi, Rachel Ward category:math.NA math.OC stat.ML published:2015-06-25 summary:This paper considers the recovery of a rank $r$ positive semidefinite matrix$X X^T\in\mathbb{R}^{n\times n}$ from $m$ scalar measurements of the form $y_i:= a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arisein a variety of applications, including covariance sketching ofhigh-dimensional data streams, quadratic regression, quantum state tomography,among others. A natural approach to this problem is to minimize the lossfunction $f(U) = \sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold ofsolutions given by $\{XO\}_{O\in\mathcal{O}_r}$ where $\mathcal{O}_r$ is theorthogonal group of $r\times r$ orthogonal matrices; this is {\it non-convex}in the $n\times r$ matrix $U$, but methods like gradient descent are simple andeasy to implement (as compared to semidefinite relaxation approaches). In this paper we show that once we have $m \geq C nr \log^2(n)$ samples fromisotropic gaussian $a_i$, with high probability {\em (a)} this function admitsa dimension-independent region of {\em local strong convexity} on linesperpendicular to the solution manifold, and {\em (b)} with an additionalpolynomial factor of $r$ samples, a simple spectral initialization will landwithin the region of convexity with high probability. Together, this impliesthat gradient descent with initialization (but no re-sampling) will convergelinearly to the correct $X$, up to an orthogonal transformation. We believethat this general technique (local convexity reachable by spectralinitialization) should prove applicable to a broader class of nonconvexoptimization problems.
arxiv-11700-23 | Places205-VGGNet Models for Scene Recognition | http://arxiv.org/pdf/1508.01667v1.pdf | author:Limin Wang, Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV published:2015-08-07 summary:VGGNets have turned out to be effective for object recognition in stillimages. However, it is unable to yield good performance by directly adaptingthe VGGNet models trained on the ImageNet dataset for scene recognition. Thisreport describes our implementation of training the VGGNets on the large-scalePlaces205 dataset. Specifically, we train three VGGNet models, namelyVGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffetoolbox with high computational efficiency. We verify the performance oftrained Places205-VGGNet models on three datasets: MIT67, SUN397, andPlaces205. Our trained models achieve the state-of-the-art performance on thesedatasets and are made public available.
arxiv-11700-24 | The Contribution of Internal and Model Variabilities to the Uncertainty in CMIP5 Decadal Climate Predictions | http://arxiv.org/pdf/1508.01609v1.pdf | author:Ehud Strobach, Golan Bel category:physics.ao-ph stat.ML published:2015-08-07 summary:Decadal climate predictions, which are initialized with observed conditions,are characterized by two main sources of uncertainties--internal and modelvariabilities. Using an ensemble of climate model simulations from the CMIP5decadal experiments, we quantified the total uncertainty associated with thesepredictions and the relative importance of each source. Annual and monthlyaverages of the surface temperature and wind components were considered. Weshow that different definitions of the anomaly results in different conclusionsregarding the variance of the ensemble members. However, some features of theuncertainty are common to all the measures we considered. We found that overdecadal time scales, there is no considerable increase in the uncertainty withtime. The model variability is more sensitive to the annual cycle than theinternal variability. This, in turn, results in a maximal uncertainty duringthe winter in the northern hemisphere. The uncertainty of the surfacetemperature prediction is dominated by the model variability, whereas theuncertainty of the wind components is determined by both sources. Analysis ofthe spatial distribution of the uncertainty reveals that the surfacetemperature has higher variability over land and in high latitudes, whereas thesurface zonal wind has higher variability over the ocean. The relativeimportance of the internal and model variabilities depends on the averagingperiod, the definition of the anomaly, and the location. These findings suggestthat several methods should be combined in order to assess future climateprediction uncertainties and that weighting schemes of the ensemble members mayreduce the uncertainties.
arxiv-11700-25 | Sublinear Partition Estimation | http://arxiv.org/pdf/1508.01596v1.pdf | author:Pushpendre Rastogi, Benjamin Van Durme category:stat.ML cs.LG published:2015-08-07 summary:The output scores of a neural network classifier are converted toprobabilities via normalizing over the scores of all competing categories.Computing this partition function, $Z$, is then linear in the number ofcategories, which is problematic as real-world problem sets continue to grow incategorical types, such as in visual object recognition or discriminativelanguage modeling. We propose three approaches for sublinear estimation of thepartition function, based on approximate nearest neighbor search and kernelfeature maps and compare the performance of the proposed approachesempirically.
arxiv-11700-26 | A Mood-based Genre Classification of Television Content | http://arxiv.org/pdf/1508.01571v1.pdf | author:Humberto Corona, Michael P. O'Mahony category:cs.IR cs.CL H.3.3 published:2015-08-06 summary:The classification of television content helps users organise and navigatethrough the large list of channels and programs now available. In this paper,we address the problem of television content classification by exploiting textinformation extracted from program transcriptions. We present an analysis whichadapts a model for sentiment that has been widely and successfully applied inother fields such as music or blog posts. We use a real-world dataset obtainedfrom the Boxfish API to compare the performance of classifiers trained on anumber of different feature sets. Our experiments show that, over a largecollection of television content, program genres can be represented in athree-dimensional space of valence, arousal and dominance, and that promisingclassification results can be achieved using features based on thisrepresentation. This finding supports the use of the proposed representation oftelevision content as a feature space for similarity computation andrecommendation generation.
arxiv-11700-27 | Compressed Sensing without Sparsity Assumptions | http://arxiv.org/pdf/1507.07094v2.pdf | author:Miles E. Lopes category:cs.IT math.IT math.ST stat.ME stat.ML stat.TH published:2015-07-25 summary:The theory of Compressed Sensing asserts that an unknown signal$x\in\mathbb{R}^p$ can be accurately recovered from an underdetermined set of$n$ linear measurements with $n\ll p$, provided that $x$ is sufficientlysparse. However, in applications, the degree of sparsity $\x\_0$ is typicallyunknown, and the problem of directly estimating $\x\_0$ has been alongstanding gap between theory and practice. A closely related issue is that$\x\_0$ is a highly idealized measure of sparsity, and for real signals withentries not exactly equal to 0, the value $\x\_0=p$ is not a usefuldescription of compressibility. In our previous conference paper that examinedthese problems, Lopes 2013, we considered an alternative measure of "soft"sparsity, $\x\_1^2/\x\_2^2$, and designed a procedure to estimate$\x\_1^2/\x\_2^2$ that does not rely on sparsity assumptions. The present work offers a new deconvolution-based method for estimatingunknown sparsity, which has wider applicability and sharper theoreticalguarantees. Whereas our earlier work was limited to estimating the quantity$\x\_1^2/\x\_2^2$, the current paper introduces a family of entropy-basedsparsity measures $s_q(x):=\big(\frac{\x\_q}{\x\_1}\big)^{\frac{q}{1-q}}$parameterized by $q\in[0,\infty]$. Two other main advantages of the newapproach are that it handles measurement noise with infinite variance, and thatit yields confidence intervals for $s_q(x)$ with asymptotically exact coverageprobability (whereas our previous intervals were conservative). In addition toconfidence intervals, we also analyze several other aspects of our proposedestimator $\hat{s}_q(x)$ and show that randomized measurements are an essentialaspect of our procedure.
arxiv-11700-28 | A Knowledge Gradient Policy for Sequencing Experiments to Identify the Structure of RNA Molecules Using a Sparse Additive Belief Model | http://arxiv.org/pdf/1508.01551v1.pdf | author:Yan Li, Kristofer G. Reyes, Jorge Vazquez-Anderson, Yingfei Wang, Lydia M. Contreras, Warren B. Powell category:math.OC stat.AP stat.ML published:2015-08-06 summary:We present a sparse knowledge gradient (SpKG) algorithm for adaptivelyselecting the targeted regions within a large RNA molecule to identify whichregions are most amenable to interactions with other molecules. Experimentally,such regions can be inferred from fluorescence measurements obtained by bindinga complementary probe with fluorescence markers to the targeted regions. We usea biophysical model which shows that the fluorescence ratio under the log scalehas a sparse linear relationship with the coefficients describing theaccessibility of each nucleotide, since not all sites are accessible (due tothe folding of the molecule). The SpKG algorithm uniquely combines the Bayesianranking and selection problem with the frequentist $\ell_1$ regularizedregression approach Lasso. We use this algorithm to identify the sparsitypattern of the linear model as well as sequentially decide the best regions totest before experimental budget is exhausted. Besides, we also develop twoother new algorithms: batch SpKG algorithm, which generates more suggestionssequentially to run parallel experiments; and batch SpKG with a procedure whichwe call length mutagenesis. It dynamically adds in new alternatives, in theform of types of probes, are created by inserting, deleting or mutatingnucleotides within existing probes. In simulation, we demonstrate thesealgorithms on the Group I intron (a mid-size RNA molecule), showing that theyefficiently learn the correct sparsity pattern, identify the most accessibleregion, and outperform several other policies.
arxiv-11700-29 | Theoretical and Empirical Analysis of a Parallel Boosting Algorithm | http://arxiv.org/pdf/1508.01549v1.pdf | author:Uday Kamath, Carlotta Domeniconi, Kenneth De Jong category:cs.LG cs.DC published:2015-08-06 summary:Many real-world problems involve massive amounts of data. Under thesecircumstances learning algorithms often become prohibitively expensive, makingscalability a pressing issue to be addressed. A common approach is to performsampling to reduce the size of the dataset and enable efficient learning.Alternatively, one customizes learning algorithms to achieve scalability. Ineither case, the key challenge is to obtain algorithmic efficiency withoutcompromising the quality of the results. In this paper we discuss ameta-learning algorithm (PSBML) which combines features of parallel algorithmswith concepts from ensemble and boosting methodologies to achieve the desiredscalability property. We present both theoretical and empirical analyses whichshow that PSBML preserves a critical property of boosting, specifically,convergence to a distribution centered around the margin. We then presentadditional empirical analyses showing that this meta-level algorithm provides ageneral and effective framework that can be used in combination with a varietyof learning classifiers. We perform extensive experiments to investigate thetradeoff achieved between scalability and accuracy, and robustness to noise, onboth synthetic and real-world data. These empirical results corroborate ourtheoretical analysis, and demonstrate the potential of PSBML in achievingscalability without sacrificing accuracy.
arxiv-11700-30 | Nonlinear Metric Learning for kNN and SVMs through Geometric Transformations | http://arxiv.org/pdf/1508.01534v1.pdf | author:Bibo Shi, Jundong Liu category:cs.LG cs.CV published:2015-08-06 summary:In recent years, research efforts to extend linear metric learning models tohandle nonlinear structures have attracted great interests. In this paper, wepropose a novel nonlinear solution through the utilization of deformablegeometric models to learn spatially varying metrics, and apply the strategy toboost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS)are chosen as the geometric model due to their remarkable versatility andrepresentation power in accounting for high-order deformations. By transformingthe input space through TPS, we can pull same-class neighbors closer whilepushing different-class points farther away in kNN, as well as make the inputdata points more linearly separable in SVMs. Improvements in the performance ofkNN classification are demonstrated through experiments on synthetic and realworld datasets, with comparisons made with several state-of-the-art metriclearning solutions. Our SVM-based models also achieve significant improvementsover traditional linear and kernel SVMs with the same datasets.
arxiv-11700-31 | Hyponymy extraction of domain ontology concept based on ccrfs and hierarchy clustering | http://arxiv.org/pdf/1508.01476v1.pdf | author:Qiang Zhan, Chunhong Wang category:cs.CL published:2015-08-06 summary:Concept hierarchy is the backbone of ontology, and the concept hierarchyacquisition has been a hot topic in the field of ontology learning. this paperproposes a hyponymy extraction method of domain ontology concept based oncascaded conditional random field(CCRFs) and hierarchy clustering. It takesfree text as extracting object, adopts CCRFs identifying the domain concepts.First the low layer of CCRFs is used to identify simple domain concept, thenthe results are sent to the high layer, in which the nesting concepts arerecognized. Next we adopt hierarchy clustering to identify the hyponymyrelation between domain ontology concepts. The experimental results demonstratethe proposed method is efficient.
arxiv-11700-32 | Socially Constrained Structural Learning for Groups Detection in Crowd | http://arxiv.org/pdf/1508.01158v2.pdf | author:Francesco Solera, Simone Calderara, Rita Cucchiara category:cs.CV published:2015-08-05 summary:Modern crowd theories agree that collective behavior is the result of theunderlying interactions among small groups of individuals. In this work, wepropose a novel algorithm for detecting social groups in crowds by means of aCorrelation Clustering procedure on people trajectories. The affinity betweencrowd members is learned through an online formulation of the Structural SVMframework and a set of specifically designed features characterizing both theirphysical and social identity, inspired by Proxemic theory, Granger causality,DTW and Heat-maps. To adhere to sociological observations, we introduce a lossfunction (G-MITRE) able to deal with the complexity of evaluating groupdetection performances. We show our algorithm achieves state-of-the-art resultswhen relying on both ground truth trajectories and tracklets previouslyextracted by available detector/tracker systems.
arxiv-11700-33 | Using Linguistic Analysis to Translate Arabic Natural Language Queries to SPARQL | http://arxiv.org/pdf/1508.01447v1.pdf | author:Iyad AlAgha category:cs.CL cs.AI cs.DB published:2015-08-06 summary:The logic-based machine-understandable framework of the Semantic Web oftenchallenges naive users when they try to query ontology-based knowledge bases.Existing research efforts have approached this problem by introducing NaturalLanguage (NL) interfaces to ontologies. These NL interfaces have the ability toconstruct SPARQL queries based on NL user queries. However, most efforts wererestricted to queries expressed in English, and they often benefited from theadvancement of English NLP tools. However, little research has been done tosupport querying the Arabic content on the Semantic Web by using NL queries.This paper presents a domain-independent approach to translate Arabic NLqueries to SPARQL by leveraging linguistic analysis. Based on a specialconsideration on Noun Phrases (NPs), our approach uses a language parser toextract NPs and the relations from Arabic parse trees and match them to theunderlying ontology. It then utilizes knowledge in the ontology to group NPsinto triple-based representations. A SPARQL query is finally generated byextracting targets and modifiers, and interpreting them into SPARQL. Theinterpretation of advanced semantic features including negation, conjunctiveand disjunctive modifiers is also supported. The approach was evaluated byusing two datasets consisting of OWL test data and queries, and the obtainedresults have confirmed its feasibility to translate Arabic NL queries toSPARQL.
arxiv-11700-34 | Privacy-Preserving Multi-Document Summarization | http://arxiv.org/pdf/1508.01420v1.pdf | author:LuÃ­s Marujo, JosÃ© PortÃªlo, Wang Ling, David Martins de Matos, JoÃ£o P. Neto, Anatole Gershman, Jaime Carbonell, Isabel Trancoso, Bhiksha Raj category:cs.IR cs.CL cs.CR published:2015-08-06 summary:State-of-the-art extractive multi-document summarization systems are usuallydesigned without any concern about privacy issues, meaning that all documentsare open to third parties. In this paper we propose a privacy-preservingapproach to multi-document summarization. Our approach enables other parties toobtain summaries without learning anything else about the original documents'content. We use a hashing scheme known as Secure Binary Embeddings to convertdocuments representation containing key phrases and bag-of-words into bitstrings, allowing the computation of approximate distances, instead of exactones. Our experiments indicate that our system yields similar results to itsnon-private counterpart on standard multi-document evaluation datasets.
arxiv-11700-35 | A Gauss-Newton Method for Markov Decision Processes | http://arxiv.org/pdf/1507.08271v4.pdf | author:Thomas Furmston, Guy Lever category:cs.AI cs.LG stat.ML published:2015-07-29 summary:Approximate Newton methods are a standard optimization tool which aim tomaintain the benefits of Newton's method, such as a fast rate of convergence,whilst alleviating its drawbacks, such as computationally expensive calculationor estimation of the inverse Hessian. In this work we investigate approximateNewton methods for policy optimization in Markov Decision Processes (MDPs). Wefirst analyse the structure of the Hessian of the objective function for MDPs.We show that, like the gradient, the Hessian exhibits useful structure in thecontext of MDPs and we use this analysis to motivate two Gauss-Newton Methodsfor MDPs. Like the Gauss-Newton method for non-linear least squares, thesemethods involve approximating the Hessian by ignoring certain terms in theHessian which are difficult to estimate. The approximate Hessians possessdesirable properties, such as negative definiteness, and we demonstrate severalimportant performance guarantees including guaranteed ascent directions,invariance to affine transformation of the parameter space, and convergenceguarantees. We finally provide a unifying perspective of key policy searchalgorithms, demonstrating that our second Gauss-Newton algorithm is closelyrelated to both the EM-algorithm and natural gradient ascent applied to MDPs,but performs significantly better in practice on a range of challengingdomains.
arxiv-11700-36 | Automatic classification of bengali sentences based on sense definitions present in bengali wordnet | http://arxiv.org/pdf/1508.01349v1.pdf | author:Alok Ranjan Pal, Diganta Saha, Niladri Sekhar Dash category:cs.CL published:2015-08-06 summary:Based on the sense definition of words available in the Bengali WordNet, anattempt is made to classify the Bengali sentences automatically into differentgroups in accordance with their underlying senses. The input sentences arecollected from 50 different categories of the Bengali text corpus developed inthe TDIL project of the Govt. of India, while information about the differentsenses of particular ambiguous lexical item is collected from Bengali WordNet.In an experimental basis we have used Naive Bayes probabilistic model as auseful classifier of sentences. We have applied the algorithm over 1747sentences that contain a particular Bengali lexical item which, because of itsambiguous nature, is able to trigger different senses that render sentences indifferent meanings. In our experiment we have achieved around 84% accurateresult on the sense classification over the total input sentences. We haveanalyzed those residual sentences that did not comply with our experiment anddid affect the results to note that in many cases, wrong syntactic structuresand less semantic information are the main hurdles in semantic classificationof sentences. The applicational relevance of this study is attested inautomatic text classification, machine learning, information extraction, andword sense disambiguation.
arxiv-11700-37 | Word sense disambiguation: a survey | http://arxiv.org/pdf/1508.01346v1.pdf | author:Alok Ranjan Pal, Diganta Saha category:cs.CL published:2015-08-06 summary:In this paper, we made a survey on Word Sense Disambiguation (WSD). Nearabout in all major languages around the world, research in WSD has beenconducted upto different extents. In this paper, we have gone through a surveyregarding the different approaches adopted in different research works, theState of the Art in the performance in this domain, recent works in differentIndian languages and finally a survey in Bengali language. We have made asurvey on different competitions in this field and the bench mark results,obtained from those competitions.
arxiv-11700-38 | Universal Approximation of Edge Density in Large Graphs | http://arxiv.org/pdf/1508.01340v1.pdf | author:Marc BoullÃ© category:cs.SI cs.DB stat.ML published:2015-08-06 summary:In this paper, we present a novel way to summarize the structure of largegraphs, based on non-parametric estimation of edge density in directedmultigraphs. Following coclustering approach, we use a clustering of thevertices, with a piecewise constant estimation of the density of the edgesacross the clusters, and address the problem of automatically and reliablyinferring the number of clusters, which is the granularity of the coclustering.We use a model selection technique with data-dependent prior and obtain anexact evaluation criterion for the posterior probability of edge densityestimation models. We demonstrate, both theoretically and empirically, that ourdata-dependent modeling technique is consistent, resilient to noise, valid nonasymptotically and asymptotically behaves as an universal approximator of thetrue edge density in directed multigraphs. We evaluate our method usingartificial graphs and present its practical interest on real world graphs. Themethod is both robust and scalable. It is able to extract insightful patternsin the unsupervised learning setting and to provide state of the art accuracywhen used as a preparation step for supervised learning.
arxiv-11700-39 | On Gobbledygook and Mood of the Philippine Senate: An Exploratory Study on the Readability and Sentiment of Selected Philippine Senators' Microposts | http://arxiv.org/pdf/1508.01321v1.pdf | author:Fatima M. Moncada, Jaderick P. Pabico category:cs.CL cs.CY published:2015-08-06 summary:This paper presents the findings of a readability assessment and sentimentanalysis of selected six Philippine senators' microposts over the popularTwitter microblog. Using the Simple Measure of Gobbledygook (SMOG), tweets ofSenators Cayetano, Defensor-Santiago, Pangilinan, Marcos, Guingona, andEscudero were assessed. A sentiment analysis was also done to determine thepolarity of the senators' respective microposts. Results showed that on theaverage, the six senators are tweeting at an eight to ten SMOG level. Thismeans that, at least a sixth grader will be able to understand the senators'tweets. Moreover, their tweets are mostly neutral and their sentiments vary inunison at some period of time. This could mean that a senator's tweet sentimentis affected by specific Philippine-based events.
arxiv-11700-40 | The study of cuckoo optimization algorithm for production planning problem | http://arxiv.org/pdf/1508.01310v1.pdf | author:Afsane Akbarzadeh, Elham Shadkam category:math.OC cs.NE published:2015-08-06 summary:Constrained Nonlinear programming problems are hard problems, and one of themost widely used and common problems for production planning problem tooptimize. In this study, one of the mathematical models of production planningis survey and the problem solved by cuckoo algorithm. Cuckoo Algorithm isefficient method to solve continues non linear problem. Moreover, mentionedmodels of production planning solved with Genetic algorithm and Lingo softwareand the results will compared. The Cuckoo Algorithm is suitable choice foroptimization in convergence of solution
arxiv-11700-41 | Collaborative Total Variation: A General Framework for Vectorial TV Models | http://arxiv.org/pdf/1508.01308v1.pdf | author:Joan Duran, Michael Moeller, Catalina Sbert, Daniel Cremers category:cs.CV math.HO math.NA math.OC published:2015-08-06 summary:Even after over two decades, the total variation (TV) remains one of the mostpopular regularizations for image processing problems and has sparked atremendous amount of research, particularly to move from scalar tovector-valued functions. In this paper, we consider the gradient of a colorimage as a three dimensional matrix or tensor with dimensions corresponding tothe spatial extend, the differences to other pixels, and the spectral channels.The smoothness of this tensor is then measured by taking different norms alongthe different dimensions. Depending on the type of these norms one obtains verydifferent properties of the regularization, leading to novel models for colorimages. We call this class of regularizations collaborative total variation(CTV). On the theoretical side, we characterize the dual norm, thesubdifferential and the proximal mapping of the proposed regularizers. Wefurther prove, with the help of the generalized concept of singular vectors,that an $\ell^{\infty}$ channel coupling makes the most prior assumptions andhas the greatest potential to reduce color artifacts. Our practicalcontributions consist of an extensive experimental section where we compare theperformance of a large number of collaborative TV methods for inverse problemslike denoising, deblurring and inpainting.
arxiv-11700-42 | Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face Recognition | http://arxiv.org/pdf/1401.5311v2.pdf | author:Changxing Ding, Jonghyun Choi, Dacheng Tao, Larry S. Davis category:cs.CV published:2014-01-21 summary:To perform unconstrained face recognition robust to variations inillumination, pose and expression, this paper presents a new scheme to extract"Multi-Directional Multi-Level Dual-Cross Patterns" (MDML-DCPs) from faceimages. Specifically, the MDMLDCPs scheme exploits the first derivative ofGaussian operator to reduce the impact of differences in illumination and thencomputes the DCP feature at both the holistic and component levels. DCP is anovel face image descriptor inspired by the unique textural structure of humanfaces. It is computationally efficient and only doubles the cost of computinglocal binary patterns, yet is extremely robust to pose and expressionvariations. MDML-DCPs comprehensively yet efficiently encodes the invariantcharacteristics of a face image from multiple levels into patterns that arehighly discriminative of inter-personal differences but robust tointra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC2.0, and LFW databases indicate that DCP outperforms the state-of-the-art localdescriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both faceidentification and face verification tasks. More impressively, the bestperformance is achieved on the challenging LFW and FRGC 2.0 databases bydeploying MDML-DCPs in a simple recognition scheme.
arxiv-11700-43 | Sparse Pseudo-input Local Kriging for Large Non-stationary Spatial Datasets with Exogenous Variables | http://arxiv.org/pdf/1508.01248v1.pdf | author:Babak Farmanesh, Arash Pourhabib category:stat.ML published:2015-08-05 summary:Gaussian process (GP) regression is a powerful tool for building predictivemodels for spatial systems. However, it does not scale efficiently for largedatasets. Particularly, for high-dimensional spatial datasets, i.e., spatialdatasets that contain exogenous variables, the performance of GP regressionfurther deteriorates. This paper presents the Sparse Pseudo-input Local Kriging(SPLK) which approximates the full GP for spatial datasets with exogenousvariables. SPLK employs orthogonal cuts which decompose the domain into smallersubdomains and then applies a sparse approximation of the full GP in eachsubdomain. We obtain the continuity of the global predictor by imposingcontinuity constraints on the boundaries of the neighboring subdomains. Thedomain decomposition scheme applies independent covariance structures in eachregion, and as a result, SPLK captures heterogeneous covariance structures.SPLK achieves computational efficiency by utilizing sparse approximation ineach subdomain which enables SPLK to accommodate large subdomains that containmany data points and possess a homogenous covariance structure. We Apply theproposed method to real and simulated datasets. We conclude that thecombination of orthogonal cuts and sparse approximation makes the proposedmethod an efficient algorithm for high-dimensional large spatial datasets.
arxiv-11700-44 | Non-isometric Curve to Surface Matching with Incomplete Data for Functional Calibration | http://arxiv.org/pdf/1508.01240v1.pdf | author:Arash Pourhabib, Balabhaskar Balasundaram category:stat.ML published:2015-08-05 summary:Calibration refers to the process of adjusting features of a computationalmodel that are not observed in the physical process so that the model matchesthe real process. We propose a framework for calibration when the unobservedfeatures, i.e. calibration parameters, do not assume a single value, but arefunctionally dependent on other inputs. We demonstrate that this problem iscurve to surface matching where the matched curve does not possess the samelength as the original curve. Therefore, we perform non-isometric matching of acurve to a surface. Since in practical applications we do not observe acontinuous curve but a sample of data points, we use a graph-theoretic approachto solve this matching of incomplete data. We define a graph structure in whichthe nodes are selected from the incomplete surface and the weights of the edgesare decided based on the response values of the curve and surface. We show thatthe problem of non-isometric incomplete curve to surface matching is a shortestpath problem in a directed acyclic graph. We apply the proposed method,graph-theoretic non-isometric matching, to real and synthetic data anddemonstrate that the proposed method improves the prediction accuracy infunctional calibration.
arxiv-11700-45 | HFirst: A Temporal Approach to Object Recognition | http://arxiv.org/pdf/1508.01176v1.pdf | author:Garrick Orchard, Cedric Meyer, Ralph Etienne-Cummings, Christoph Posch, Nitish Thakor, Ryad Benosman category:cs.CV published:2015-08-05 summary:This paper introduces a spiking hierarchical model for object recognitionwhich utilizes the precise timing information inherently present in the outputof biologically inspired asynchronous Address Event Representation (AER) visionsensors. The asynchronous nature of these systems frees computation andcommunication from the rigid predetermined timing enforced by system clocks inconventional systems. Freedom from rigid timing constraints opens thepossibility of using true timing to our advantage in computation. We show notonly how timing can be used in object recognition, but also how it can in factsimplify computation. Specifically, we rely on a simpletemporal-winner-take-all rather than more computationally intensive synchronousoperations typically used in biologically inspired neural networks for objectrecognition. This approach to visual computation represents a major paradigmshift from conventional clocked systems and can find application in othersensory modalities and computational tasks. We showcase effectiveness of theapproach by achieving the highest reported accuracy to date (97.5\%$\pm$3.5\%)for a previously published four class card pip recognition task and an accuracyof 84.9\%$\pm$1.9\% for a new more difficult 36 class character recognitiontask.
arxiv-11700-46 | Video Compressive Sensing for Spatial Multiplexing Cameras using Motion-Flow Models | http://arxiv.org/pdf/1503.02727v2.pdf | author:Aswin C. Sankaranarayanan, Lina Xu, Christoph Studer, Yun Li, Kevin Kelly, Richard G. Baraniuk category:cs.CV published:2015-03-09 summary:Spatial multiplexing cameras (SMCs) acquire a (typically static) scenethrough a series of coded projections using a spatial light modulator (e.g., adigital micro-mirror device) and a few optical sensors. This approach finds usein imaging applications where full-frame sensors are either too expensive(e.g., for short-wave infrared wavelengths) or unavailable. Existing SMCsystems reconstruct static scenes using techniques from compressive sensing(CS). For videos, however, existing acquisition and recovery methods deliverpoor quality. In this paper, we propose the CS multi-scale video (CS-MUVI)sensing and recovery framework for high-quality video acquisition and recoveryusing SMCs. Our framework features novel sensing matrices that enable theefficient computation of a low-resolution video preview, while enablinghigh-resolution video recovery using convex optimization. To further improvethe quality of the reconstructed videos, we extract optical-flow estimates fromthe low-resolution previews and impose them as constraints in the recoveryprocedure. We demonstrate the efficacy of our CS-MUVI framework for a host ofsynthetic and real measured SMC video data, and we show that high-qualityvideos can be recovered at roughly $60\times$ compression.
arxiv-11700-47 | Partitioned Shape Modeling with On-the-Fly Sparse Appearance Learning for Anterior Visual Pathway Segmentation | http://arxiv.org/pdf/1508.01128v1.pdf | author:Awais Mansoor, Juan J. Cerrolaza, Robert A. Avery, Marius G. Linguraru category:cs.CV published:2015-08-05 summary:MRI quantification of cranial nerves such as anterior visual pathway (AVP) inMRI is challenging due to their thin small size, structural variation along itspath, and adjacent anatomic structures. Segmentation of pathologically abnormaloptic nerve (e.g. optic nerve glioma) poses additional challenges due tochanges in its shape at unpredictable locations. In this work, we propose apartitioned joint statistical shape model approach with sparse appearancelearning for the segmentation of healthy and pathological AVP. Our maincontributions are: (1) optimally partitioned statistical shape models for theAVP based on regional shape variations for greater local flexibility ofstatistical shape model; (2) refinement model to accommodate pathologicalregions as well as areas of subtle variation by training the model on-the-flyusing the initial segmentation obtained in (1); (3) hierarchical deformableframework to incorporate scale information in partitioned shape and appearancemodels. Our method, entitled PAScAL (PArtitioned Shape and AppearanceLearning), was evaluated on 21 MRI scans (15 healthy + 6 glioma cases) frompediatric patients (ages 2-17). The experimental results show that the proposedlocalized shape and sparse appearance-based learning approach significantlyoutperforms segmentation approaches in the analysis of pathological data.
arxiv-11700-48 | Evaluating color texture descriptors under large variations of controlled lighting conditions | http://arxiv.org/pdf/1508.01108v1.pdf | author:Claudio Cusano, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2015-08-05 summary:The recognition of color texture under varying lighting conditions is stillan open issue. Several features have been proposed for this purpose, rangingfrom traditional statistical descriptors to features extracted with neuralnetworks. Still, it is not completely clear under what circumstances a featureperforms better than the others. In this paper we report an extensivecomparison of old and new texture features, with and without a colornormalization step, with a particular focus on how they are affected by smalland large variation in the lighting conditions. The evaluation is performed ona new texture database including 68 samples of raw food acquired under 46conditions that present single and combined variations of light color,direction and intensity. The database allows to systematically investigate therobustness of texture descriptors across a large range of variations of imagingconditions.
arxiv-11700-49 | Deep Convolutional Networks are Hierarchical Kernel Machines | http://arxiv.org/pdf/1508.01084v1.pdf | author:Fabio Anselmi, Lorenzo Rosasco, Cheston Tan, Tomaso Poggio category:cs.LG cs.NE published:2015-08-05 summary:In i-theory a typical layer of a hierarchical architecture consists of HWmodules pooling the dot products of the inputs to the layer with thetransformations of a few templates under a group. Such layers include asspecial cases the convolutional layers of Deep Convolutional Networks (DCNs) aswell as the non-convolutional layers (when the group contains only theidentity). Rectifying nonlinearities -- which are used by present-day DCNs --are one of the several nonlinearities admitted by i-theory for the HW module.We discuss here the equivalence between group averages of linear combinationsof rectifying nonlinearities and an associated kernel. This property impliesthat present-day DCNs can be exactly equivalent to a hierarchy of kernelmachines with pooling and non-pooling layers. Finally, we describe a conjecturefor theoretically understanding hierarchies of such modules. A main consequenceof the conjecture is that hierarchies of trained HW modules minimize memoryrequirements while computing a selective and invariant representation.
arxiv-11700-50 | Detection of Critical Number of People in Interlocked Doors for Security Access Control by Exploiting a Microwave Transceiver-Array | http://arxiv.org/pdf/1508.01081v1.pdf | author:Paolo Nesi, Gianni Pantaleo category:cs.CV published:2015-08-05 summary:Counting the number of people is something many security application focuson, when dealing with controlling accesses in restricted areas, as it occurswith banks, airports, railway stations and governmental offices. This paperpresents an automated solution for detecting the presence of more than oneperson into interlocked doors adopted in many accesses. In most cases,interlocked doors are small areas where other pieces of information and sensorsare placed in order to detect the presence of guns, explosive, etc. The generalgoals and the required environmental condition, allowed us to implement adetection system at lower costs and complexity, with respect to other existingtechniques. The system consists of a fixed array of microwave transceivermodules, whose received signals are processed to collect information related toa sort of volume occupied in the interlocked door cabin. The proposed solutionhas been statistically validated by using statistical analysis. The wholesolution has been also implemented to be used in a real time environment andthus validated against real experimental measures.
arxiv-11700-51 | A MAP approach for $\ell_q$-norm regularized sparse parameter estimation using the EM algorithm | http://arxiv.org/pdf/1508.01071v1.pdf | author:Rodrigo Carvajal, Juan C. AgÃ¼ero, Boris I. Godoy, Dimitrios Katselis category:cs.SY stat.ML published:2015-08-05 summary:In this paper, Bayesian parameter estimation through the consideration of theMaximum A Posteriori (MAP) criterion is revisited under the prism of theExpectation-Maximization (EM) algorithm. By incorporating a sparsity-promotingpenalty term in the cost function of the estimation problem through the use ofan appropriate prior distribution, we show how the EM algorithm can be used toefficiently solve the corresponding optimization problem. To this end, we relyon variance-mean Gaussian mixtures (VMGM) to describe the prior distribution,while we incorporate many nice features of these mixtures to our estimationproblem. The corresponding MAP estimation problem is completely expressed interms of the EM algorithm, which allows for handling nonlinearities and hiddenvariables that cannot be easily handled with traditional methods. Forcomparison purposes, we also develop a Coordinate Descent algorithm for the$\ell_q$-norm penalized problem and present the performance results viasimulations.
arxiv-11700-52 | Topic Stability over Noisy Sources | http://arxiv.org/pdf/1508.01067v1.pdf | author:Jing Su, OisÃ­n Boydell, Derek Greene, Gerard Lynch category:cs.CL cs.IR published:2015-08-05 summary:Topic modelling techniques such as LDA have recently been applied to speechtranscripts and OCR output. These corpora may contain noisy or erroneous textswhich may undermine topic stability. Therefore, it is important to know howwell a topic modelling algorithm will perform when applied to noisy data. Inthis paper we show that different types of textual noise will have diverseeffects on the stability of different topic models. From these observations, wepropose guidelines for text corpus generation, with a focus on automatic speechtranscription. We also suggest topic model selection methods for noisy corpora.
arxiv-11700-53 | On the convergence of the sparse possibilistic c-means algorithm | http://arxiv.org/pdf/1508.01057v1.pdf | author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV published:2015-08-05 summary:In this paper, a convergence proof for the recently proposed sparsepossibilistic c-means (SPCM) algorithm is provided, utilizing the celebratedZangwill convergence theorem. It is shown that the iterative sequence generatedby SPCM converges to a stationary point or there exists a subsequence of itthat converges to a stationary point of the cost function of the algorithm.
arxiv-11700-54 | Estimating snow cover from publicly available images | http://arxiv.org/pdf/1508.01055v1.pdf | author:Roman Fedorov, Alessandro Camerada, Piero Fraternali, Marco Tagliasacchi category:cs.MM cs.CV published:2015-08-05 summary:In this paper we study the problem of estimating snow cover in mountainousregions, that is, the spatial extent of the earth surface covered by snow. Weargue that publicly available visual content, in the form of user generatedphotographs and image feeds from outdoor webcams, can both be leveraged asadditional measurement sources, complementing existing ground, satellite andairborne sensor data. To this end, we describe two content acquisition andprocessing pipelines that are tailored to such sources, addressing the specificchallenges posed by each of them, e.g., identifying the mountain peaks,filtering out images taken in bad weather conditions, handling varyingillumination conditions. The final outcome is summarized in a snow cover index,which indicates for a specific mountain and day of the year, the fraction ofvisible area covered by snow, possibly at different elevations. We created amanually labelled dataset to assess the accuracy of the image snow covered areaestimation, achieving 90.0% precision at 91.1% recall. In addition, we showthat seasonal trends related to air temperature are captured by the snow coverindex.
arxiv-11700-55 | A review of heterogeneous data mining for brain disorders | http://arxiv.org/pdf/1508.01023v1.pdf | author:Bokai Cao, Xiangnan Kong, Philip S. Yu category:cs.LG cs.CE cs.DB q-bio.NC stat.AP published:2015-08-05 summary:With rapid advances in neuroimaging techniques, the research on braindisorder identification has become an emerging area in the data miningcommunity. Brain disorder data poses many unique challenges for data miningresearch. For example, the raw data generated by neuroimaging experiments is intensor representations, with typical characteristics of high dimensionality,structural complexity and nonlinear separability. Furthermore, brainconnectivity networks can be constructed from the tensor data, embedding subtleinteractions between brain regions. Other clinical measures are usuallyavailable reflecting the disease status from different perspectives. It isexpected that integrating complementary information in the tensor data and thebrain network data, and incorporating other clinical parameters will bepotentially transformative for investigating disease mechanisms and forinforming therapeutic interventions. Many research efforts have been devoted tothis area. They have achieved great success in various applications, such astensor-based modeling, subgraph pattern mining, multi-view feature analysis. Inthis paper, we review some recent data mining methods that are used foranalyzing brain disorders.
arxiv-11700-56 | Direct Estimation of the Derivative of Quadratic Mutual Information with Application in Supervised Dimension Reduction | http://arxiv.org/pdf/1508.01019v1.pdf | author:Voot Tangkaratt, Hiroaki Sasaki, Masashi Sugiyama category:stat.ML published:2015-08-05 summary:A typical goal of supervised dimension reduction is to find a low-dimensionalsubspace of the input space such that the projected input variables preservemaximal information about the output variables. The dependence maximizationapproach solves the supervised dimension reduction problem through maximizing astatistical dependence between projected input variables and output variables.A well-known statistical dependence measure is mutual information (MI) which isbased on the Kullback-Leibler (KL) divergence. However, it is known that the KLdivergence is sensitive to outliers. On the other hand, quadratic MI (QMI) is avariant of MI based on the $L_2$ distance which is more robust against outliersthan the KL divergence, and a computationally efficient method to estimate QMIfrom data, called least-squares QMI (LSQMI), has been proposed recently. Forthese reasons, developing a supervised dimension reduction method based onLSQMI seems promising. However, not QMI itself, but the derivative of QMI isneeded for subspace search in supervised dimension reduction, and thederivative of an accurate QMI estimator is not necessarily a good estimator ofthe derivative of QMI. In this paper, we propose to directly estimate thederivative of QMI without estimating QMI itself. We show that the directestimation of the derivative of QMI is more accurate than the derivative of theestimated QMI. Finally, we develop a supervised dimension reduction algorithmwhich efficiently uses the proposed derivative estimator, and demonstratethrough experiments that the proposed method is more robust against outliersthan existing methods.
arxiv-11700-57 | Learning from LDA using Deep Neural Networks | http://arxiv.org/pdf/1508.01011v1.pdf | author:Dongxu Zhang, Tianyi Luo, Dong Wang, Rong Liu category:cs.LG cs.CL cs.IR cs.NE published:2015-08-05 summary:Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesianmodel for topic inference. In spite of its great success, inferring the latenttopic distribution with LDA is time-consuming. Motivated by the transferlearning approach proposed by~\newcite{hinton2015distilling}, we present anovel method that uses LDA to supervise the training of a deep neural network(DNN), so that the DNN can approximate the costly LDA inference with lesscomputation. Our experiments on a document classification task show that asimple DNN can learn the LDA behavior pretty well, while the inference isspeeded up tens or hundreds of times.
arxiv-11700-58 | INsight: A Neuromorphic Computing System for Evaluation of Large Neural Networks | http://arxiv.org/pdf/1508.01008v1.pdf | author:Jaeyong Chung, Taehwan Shin, Yongshin Kang category:cs.NE published:2015-08-05 summary:Deep neural networks have been demonstrated impressive results in variouscognitive tasks such as object detection and image classification. In order toexecute large networks, Von Neumann computers store the large number of weightparameters in external memories, and processing elements are timed-shared,which leads to power-hungry I/O operations and processing bottlenecks. Thispaper describes a neuromorphic computing system that is designed from theground up for the energy-efficient evaluation of large-scale neural networks.The computing system consists of a non-conventional compiler, a neuromorphicarchitecture, and a space-efficient microarchitecture that leverages existingintegrated circuit design methodologies. The compiler factorizes a trained,feedforward network into a sparsely connected network, compresses the weightslinearly, and generates a time delay neural network reducing the number ofconnections. The connections and units in the simplified network are mapped tosilicon synapses and neurons. We demonstrate an implementation of theneuromorphic computing system based on a field-programmable gate array thatperforms the MNIST hand-written digit classification with 97.64% accuracy.
arxiv-11700-59 | Dimension Reduction with Non-degrading Generalization | http://arxiv.org/pdf/1508.00984v1.pdf | author:Pitoyo Hartono category:cs.LG cs.NE published:2015-08-05 summary:Visualizing high dimensional data by projecting them into two or threedimensional space is one of the most effective ways to intuitively understandthe data's underlying characteristics, for example their class neighborhoodstructure. While data visualization in low dimensional space can be efficientfor revealing the data's underlying characteristics, classifying a new samplein the reduced-dimensional space is not always beneficial because of the lossof information in expressing the data. It is possible to classify the data inthe high dimensional space, while visualizing them in the low dimensionalspace, but in this case, the visualization is often meaningless because itfails to illustrate the underlying characteristics that are crucial for theclassification process. In this paper, the performance-preserving property of the previously proposedRestricted Radial Basis Function Network in reducing the dimension of labeleddata is explained. Here, it is argued through empirical experiments that theinternal representation of the Restricted Radial Basis Function Network, whichduring the supervised learning process organizes a visualizable two dimensionalmap, does not only preserve the topographical structure of high dimensionaldata but also captures their class neighborhood structures that are importantfor classifying them. Hence, unlike many of the existing dimension reductionmethods, the Restricted Radial Basis Function Network offers two dimensionalvisualization that is strongly correlated with the classification process.
arxiv-11700-60 | Progressive EM for Latent Tree Models and Hierarchical Topic Detection | http://arxiv.org/pdf/1508.00973v1.pdf | author:Peixian Chen, Nevin L. Zhang, Leonard K. M. Poon, Zhourong Chen category:cs.LG cs.CL cs.IR stat.ML published:2015-08-05 summary:Hierarchical latent tree analysis (HLTA) is recently proposed as a new methodfor topic detection. It differs fundamentally from the LDA-based methods interms of topic definition, topic-document relationship, and learning method. Ithas been shown to discover significantly more coherent topics and better topichierarchies. However, HLTA relies on the Expectation-Maximization (EM)algorithm for parameter estimation and hence is not efficient enough to dealwith large datasets. In this paper, we propose a method to drastically speed upHLTA using a technique inspired by recent advances in the moments method.Empirical experiments show that our method greatly improves the efficiency ofHLTA. It is as efficient as the state-of-the-art LDA-based method forhierarchical topic detection and finds substantially better topics and topichierarchies.
arxiv-11700-61 | 3D Automatic Segmentation Method for Retinal Optical Coherence Tomography Volume Data Using Boundary Surface Enhancement | http://arxiv.org/pdf/1508.00966v1.pdf | author:Yankui Sun, Tian Zhang, Yue Zhao, Yufan He category:cs.CV published:2015-08-05 summary:With the introduction of spectral-domain optical coherence tomography(SDOCT), much larger image datasets are routinely acquired compared to what waspossible using the previous generation of time-domain OCT. Thus, there is acritical need for the development of 3D segmentation methods for processingthese data. We present here a novel 3D automatic segmentation method forretinal OCT volume data. Briefly, to segment a boundary surface, two OCT volumedatasets are obtained by using a 3D smoothing filter and a 3D differentialfilter. Their linear combination is then calculated to generate new volume datawith an enhanced boundary surface, where pixel intensity, boundary positioninformation, and intensity changes on both sides of the boundary surface areused simultaneously. Next, preliminary discrete boundary points are detectedfrom the A-Scans of the volume data. Finally, surface smoothness constraintsand a dynamic threshold are applied to obtain a smoothed boundary surface bycorrecting a small number of error points. Our method can extract retinal layerboundary surfaces sequentially with a decreasing search region of volume data.We performed automatic segmentation on eight human OCT volume datasets acquiredfrom a commercial Spectralis OCT system, where each volume of data consisted of97 OCT images with a resolution of 496 512; experimental results show that thismethod can accurately segment seven layer boundary surfaces in normal as wellas some abnormal eyes.
arxiv-11700-62 | Distant Supervision for Entity Linking | http://arxiv.org/pdf/1505.03823v3.pdf | author:Miao Fan, Qiang Zhou, Thomas Fang Zheng category:cs.CL cs.IR published:2015-05-14 summary:Entity linking is an indispensable operation of populating knowledgerepositories for information extraction. It studies on aligning a textualentity mention to its corresponding disambiguated entry in a knowledgerepository. In this paper, we propose a new paradigm named distantly supervisedentity linking (DSEL), in the sense that the disambiguated entities that belongto a huge knowledge repository (Freebase) are automatically aligned to thecorresponding descriptive webpages (Wiki pages). In this way, a large scale ofweakly labeled data can be generated without manual annotation and fed to aclassifier for linking more newly discovered entities. Compared withtraditional paradigms based on solo knowledge base, DSEL benefits more viajointly leveraging the respective advantages of Freebase and Wikipedia.Specifically, the proposed paradigm facilitates bridging the disambiguatedlabels (Freebase) of entities and their textual descriptions (Wikipedia) forWeb-scale entities. Experiments conducted on a dataset of 140,000 items and60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyzethe feature performance and improve the F1-measure to 0.545.
arxiv-11700-63 | Asynchronous stochastic convex optimization | http://arxiv.org/pdf/1508.00882v1.pdf | author:John C. Duchi, Sorathan Chaturapruek, Christopher RÃ© category:math.OC stat.ML published:2015-08-04 summary:We show that asymptotically, completely asynchronous stochastic gradientprocedures achieve optimal (even to constant factors) convergence rates for thesolution of convex optimization problems under nearly the same conditionsrequired for asymptotic optimality of standard stochastic gradient procedures.Roughly, the noise inherent to the stochastic approximation scheme dominatesany noise from asynchrony. We also give empirical evidence demonstrating thestrong performance of asynchronous, parallel stochastic optimization schemes,demonstrating that the robustness inherent to stochastic approximation problemsallows substantially faster parallel and asynchronous solution methods.
arxiv-11700-64 | A Deep-structured Conditional Random Field Model for Object Silhouette Tracking | http://arxiv.org/pdf/1501.00752v2.pdf | author:Mohammad Shafiee, Zohreh Azimifar, Alexander Wong category:cs.CV cs.LG stat.ML published:2015-01-05 summary:In this work, we introduce a deep-structured conditional random field(DS-CRF) model for the purpose of state-based object silhouette tracking. Theproposed DS-CRF model consists of a series of state layers, where each statelayer spatially characterizes the object silhouette at a particular point intime. The interactions between adjacent state layers are established byinter-layer connectivity dynamically determined based on inter-frame opticalflow. By incorporate both spatial and temporal context in a dynamic fashionwithin such a deep-structured probabilistic graphical model, the proposedDS-CRF model allows us to develop a framework that can accurately andefficiently track object silhouettes that can change greatly over time, as wellas under different situations such as occlusion and multiple targets within thescene. Experiment results using video surveillance datasets containingdifferent scenarios such as occlusion and multiple targets showed that theproposed DS-CRF approach provides strong object silhouette tracking performancewhen compared to baseline methods such as mean-shift tracking, as well asstate-of-the-art methods such as context tracking and boosted particlefiltering.
arxiv-11700-65 | Semantic Pose using Deep Networks Trained on Synthetic RGB-D | http://arxiv.org/pdf/1508.00835v1.pdf | author:Jeremie Papon, Markus Schoeler category:cs.CV published:2015-08-04 summary:In this work we address the problem of indoor scene understanding from RGB-Dimages. Specifically, we propose to find instances of common furniture classes,their spatial extent, and their pose with respect to generalized class models.To accomplish this, we use a deep, wide, multi-output convolutional neuralnetwork (CNN) that predicts class, pose, and location of possible objectssimultaneously. To overcome the lack of large annotated RGB-D training sets(especially those with pose), we use an on-the-fly rendering pipeline thatgenerates realistic cluttered room scenes in parallel to training. We thenperform transfer learning on the relatively small amount of publicly availableannotated RGB-D data, and find that our model is able to successfully annotateeven highly challenging real scenes. Importantly, our trained network is ableto understand noisy and sparse observations of highly cluttered scenes with aremarkable degree of accuracy, inferring class and pose from a very limited setof cues. Additionally, our neural network is only moderately deep and computesclass, pose and position in tandem, so the overall run-time is significantlyfaster than existing methods, estimating all output parameters simultaneouslyin parallel on a GPU in seconds.
arxiv-11700-66 | Object localization in ImageNet by looking out of the window | http://arxiv.org/pdf/1501.01181v2.pdf | author:Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2015-01-06 summary:We propose a method for annotating the location of objects in ImageNet.Traditionally, this is cast as an image window classification problem, whereeach window is considered independently and scored based on its appearancealone. Instead, we propose a method which scores each candidate window in thecontext of all other windows in the image, taking into account their similarityin appearance space as well as their spatial relations in the image plane. Wedevise a fast and exact procedure to optimize our scoring function over allcandidate windows in an image, and we learn its parameters using structuredoutput regression. We demonstrate on 92000 images from ImageNet that thissignificantly improves localization over recent techniques that score windowsin isolation.
arxiv-11700-67 | Online Domain Adaptation for Multi-Object Tracking | http://arxiv.org/pdf/1508.00776v1.pdf | author:Adrien Gaidon, Eleonora Vig category:cs.CV published:2015-08-04 summary:Automatically detecting, labeling, and tracking objects in videos dependsfirst and foremost on accurate category-level object detectors. These might,however, not always be available in practice, as acquiring high-quality largescale labeled training datasets is either too costly or impractical for allpossible real-world application scenarios. A scalable solution consists inre-using object detectors pre-trained on generic datasets. This work is thefirst to investigate the problem of on-line domain adaptation of objectdetectors for causal multi-object tracking (MOT). We propose to alleviate thedataset bias by adapting detectors from category to instances, and back: (i) wejointly learn all target models by adapting them from the pre-trained one, and(ii) we also adapt the pre-trained model on-line. We introduce an on-linemulti-task learning algorithm to efficiently share parameters and reduce drift,while gradually improving recall. Our approach is applicable to any linearobject detector, and we evaluate both cheap "mini-Fisher Vectors" and expensive"off-the-shelf" ConvNet features. We quantitatively measure the benefit of ourdomain adaptation strategy on the KITTI tracking benchmark and on a new dataset(PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT.
arxiv-11700-68 | Recognition of Emotions using Kinects | http://arxiv.org/pdf/1508.00761v1.pdf | author:Shun Li, Changye Zhu, Liqing Cui, Nan Zhao, Baobin Li, Tingshao Zhu category:cs.CY cs.CV cs.HC published:2015-08-04 summary:Psychological studies indicate that emotional states are expressed in the waypeople walk and the human gait is investigated in terms of its ability toreveal a person's emotional state. And Microsoft Kinect is a rapidlydeveloping, inexpensive, portable and no-marker motion capture system. Thispaper gives a new referable method to do emotion recognition, by usingMicrosoft Kinect to do gait pattern analysis, which has not been reported. $59$subjects are recruited in this study and their gait patterns are record by twoKinect cameras. Significant joints selecting, Coordinate system transforming,Slider window gauss filter, Differential operation, and Data segmentation areused in data preprocessing. Feature extracting is based on Fouriertransformation. By using the NaiveBayes, RandomForests, libSVM and SMOclassification, the recognition rate of natural and unnatural emotions canreach above 70%.It is concluded that using the Kinect system can be a newmethod in recognition of emotions.
arxiv-11700-69 | Multi-Label Active Learning from Crowds | http://arxiv.org/pdf/1508.00722v1.pdf | author:Shao-Yuan Li, Yuan Jiang, Zhi-Hua Zhou category:cs.LG cs.SI published:2015-08-04 summary:Multi-label active learning is a hot topic in reducing the label cost byoptimally choosing the most valuable instance to query its label from anoracle. In this paper, we consider the poolbased multi-label active learningunder the crowdsourcing setting, where during the active query process, insteadof resorting to a high cost oracle for the ground-truth, multiple low costimperfect annotators with various expertise are available for labeling. To dealwith this problem, we propose the MAC (Multi-label Active learning from Crowds)approach which incorporate the local influence of label correlations to build aprobabilistic model over the multi-label classifier and annotators. Based onthis model, we can estimate the labels for instances as well as the expertiseof each annotator. Then we propose the instance selection and annotatorselection criteria that consider the uncertainty/diversity of instances and thereliability of annotators, such that the most reliable annotator will bequeried for the most valuable instances. Experimental results demonstrate theeffectiveness of the proposed approach.
arxiv-11700-70 | Kernelized Multiview Projection | http://arxiv.org/pdf/1508.00430v2.pdf | author:Mengyang Yu, Li Liu, Ling Shao category:cs.CV published:2015-08-03 summary:Conventional vision algorithms adopt a single type of feature or a simpleconcatenation of multiple features, which is always represented in ahigh-dimensional space. In this paper, we propose a novel unsupervised spectralembedding algorithm called Kernelized Multiview Projection (KMP) to better fuseand embed different feature representations. Computing the kernel matrices fromdifferent features/views, KMP can encode them with the corresponding weights toachieve a low-dimensional and semantically meaningful subspace where thedistribution of each view is sufficiently smooth and discriminative. Morecrucially, KMP is linear for the reproducing kernel Hilbert space (RKHS) andsolves the out-of-sample problem, which allows it to be competent for variouspractical applications. Extensive experiments on three popular image datasetsdemonstrate the effectiveness of our multiview embedding algorithm.
arxiv-11700-71 | Parameter Database : Data-centric Synchronization for Scalable Machine Learning | http://arxiv.org/pdf/1508.00703v1.pdf | author:Naman Goel, Divyakant Agrawal, Sanjay Chawla, Ahmed Elmagarmid category:cs.DB cs.LG published:2015-08-04 summary:We propose a new data-centric synchronization framework for carrying out ofmachine learning (ML) tasks in a distributed environment. Our frameworkexploits the iterative nature of ML algorithms and relaxes the applicationagnostic bulk synchronization parallel (BSP) paradigm that has previously beenused for distributed machine learning. Data-centric synchronization complementsfunction-centric synchronization based on using stale updates to increase thethroughput of distributed ML computations. Experiments to validate ourframework suggest that we can attain substantial improvement over BSP whileguaranteeing sequential correctness of ML tasks.
arxiv-11700-72 | Hierarchical Saliency Detection on Extended CSSD | http://arxiv.org/pdf/1408.5418v2.pdf | author:Jianping Shi, Qiong Yan, Li Xu, Jiaya Jia category:cs.CV published:2014-08-11 summary:Complex structures commonly exist in natural images. When an image containssmall-scale high-contrast patterns either in the background or foreground,saliency detection could be adversely affected, resulting erroneous andnon-uniform saliency assignment. The issue forms a fundamental challenge forprior methods. We tackle it from a scale point of view and propose amulti-layer approach to analyze saliency cues. Different from varying patchsizes or downsizing images, we measure region-based scales. The final saliencyvalues are inferred optimally combining all the saliency cues in differentscales using hierarchical inference. Through our inference model, single-scaleinformation is selected to obtain a saliency map. Our method improves detectionquality on many images that cannot be handled well traditionally. We alsoconstruct an extended Complex Scene Saliency Dataset (ECSSD) to include complexbut general natural images.
arxiv-11700-73 | Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images | http://arxiv.org/pdf/1504.00983v2.pdf | author:Chen Sun, Sanketh Shetty, Rahul Sukthankar, Ram Nevatia category:cs.CV cs.MM I.2.10 published:2015-04-04 summary:We address the problem of fine-grained action localization from temporallyuntrimmed web videos. We assume that only weak video-level annotations areavailable for training. The goal is to use these weak labels to identifytemporal segments corresponding to the actions, and learn models thatgeneralize to unconstrained web videos. We find that web images queried byaction names serve as well-localized highlights for many actions, but arenoisily labeled. To solve this problem, we propose a simple yet effectivemethod that takes weak video labels and noisy image labels as input, andgenerates localized action frames as output. This is achieved by cross-domaintransfer between video frames and web images, using pre-trained deepconvolutional neural networks. We then use the localized action frames to trainaction recognition models with long short-term memory networks. We collect afine-grained sports action data set FGA-240 of more than 130,000 YouTubevideos. It has 240 fine-grained actions under 85 sports activities. Convincingresults are shown on the FGA-240 data set, as well as the THUMOS 2014localization data set with untrimmed training videos.
arxiv-11700-74 | Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance based High Dimensional Two Sample Testing | http://arxiv.org/pdf/1508.00655v1.pdf | author:Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry Wasserman category:math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH published:2015-08-04 summary:Nonparametric two sample testing is a decision theoretic problem thatinvolves identifying differences between two random variables without makingparametric assumptions about their underlying distributions. We refer to themost common settings as mean difference alternatives (MDA), for testingdifferences only in first moments, and general difference alternatives (GDA),which is about testing for any difference in distributions. A large number oftest statistics have been proposed for both these settings. This paper connectsthree classes of statistics - high dimensional variants of Hotelling's t-test,statistics based on Reproducing Kernel Hilbert Spaces, and energy statisticsbased on pairwise distances. We ask the question: how much statistical power dopopular kernel and distance based tests for GDA have when the unknowndistributions differ in their means, compared to specialized tests for MDA? We formally characterize the power of popular tests for GDA like the MaximumMean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependentvariants of the Energy Distance with the Euclidean norm (eED) in thehigh-dimensional MDA regime. Some practically important properties include (a)eED and gMMD have asymptotically equal power; furthermore they enjoy a freelunch because, while they are additionally consistent for GDA, they also havethe same power as specialized high-dimensional t-test variants for MDA. Allthese tests are asymptotically optimal (including matching constants) under MDAfor spherical covariances, according to simple lower bounds, (b) The power ofgMMD is independent of the kernel bandwidth, as long as it is larger than thechoice made by the median heuristic, (c) There is a clear and smoothcomputation-statistics tradeoff for linear-time, subquadratic-time andquadratic-time versions of these tests, with more computation resulting inhigher power.
arxiv-11700-75 | Particle Swarm Optimization for Weighted Sum Rate Maximization in MIMO Broadcast Channels | http://arxiv.org/pdf/1508.01168v1.pdf | author:Tung T. Vu, Ha Hoang Kha, Trung Q. Duong, Nguyen-Son Vo category:cs.IT cs.NE math.IT math.OC published:2015-08-04 summary:In this paper, we investigate the downlink multiple-input-multipleoutput(MIMO) broadcast channels in which a base transceiver station (BTS) broadcastsmultiple data streams to K MIMO mobile stations (MSs) simultaneously. In orderto maximize the weighted sum-rate (WSR) of the system subject to thetransmitted power constraint, the design problem is to find the pre-codingmatrices at BTS and the decoding matrices at MSs. However, such a designproblem is typically a nonlinear and nonconvex optimization and, thus, it isquite hard to obtain the analytical solutions. To tackle with the mathematicaldifficulties, we propose an efficient stochastic optimization algorithm tooptimize the transceiver matrices. Specifically, we utilize the linear minimummean square error (MMSE) Wiener filters at MSs. Then, we introduce theconstrained particle swarm optimization (PSO) algorithm to jointly optimize theprecoding and decoding matrices. Numerical experiments are exhibited tovalidate the effectiveness of the proposed algorithm in terms of convergence,computational complexity and total WSR.
arxiv-11700-76 | Indexing of CNN Features for Large Scale Image Search | http://arxiv.org/pdf/1508.00217v2.pdf | author:Ruoyu Liu, Yao Zhao, Shikui Wei, Zhenfeng Zhu, Lixin Liao, Shuang Qiu category:cs.CV published:2015-08-02 summary:Convolutional neural network (CNN) feature that represents an image with aglobal and high-dimensional vector has shown highly discriminative capabilityin image search. Although CNN features are more compact than most of localrepresentation schemes, it still cannot efficiently deal with large-scale imagesearch issues due to its non-negligible computational cost and storage usage.In this paper, we propose a simple but effective image indexing framework toimprove the computational and storage efficiency of CNN features. Instead ofprojecting each CNN feature vector into a global hashing code, the proposedframework adapts Bag-of-Word model and inverted table to global featureindexing. To this end, two strategies, which are based on semantic informationassociated with CNN features, are proposed to convert a global vector to one orseveral discrete words. In addition, several strategies for compensatingquantization error are fully investigated under the indexing framework.Extensive experimental results on two public benchmarks show the superiority ofour framework.
arxiv-11700-77 | Bayesian mixtures of spatial spline regressions | http://arxiv.org/pdf/1508.00635v1.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG stat.CO stat.ML published:2015-08-04 summary:This work relates the framework of model-based clustering for spatialfunctional data where the data are surfaces. We first introduce a Bayesianspatial spline regression model with mixed-effects (BSSR) for modeling spatialfunction data. The BSSR model is based on Nodal basis functions for spatialregression and accommodates both common mean behavior for the data through afixed-effects part, and variability inter-individuals thanks to arandom-effects part. Then, in order to model populations of spatial functionaldata issued from heterogeneous groups, we integrate the BSSR model into amixture framework. The resulting model is a Bayesian mixture of spatial splineregressions with mixed-effects (BMSSR) used for density estimation andmodel-based surface clustering. The models, through their Bayesian formulation,allow to integrate possible prior knowledge on the data structure andconstitute a good alternative to recent mixture of spatial spline regressionsmodel estimated in a maximum likelihood framework via theexpectation-maximization (EM) algorithm. The Bayesian model inference isperformed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbssampler to infer the BSSR and the BMSSR models and apply them on simulatedsurfaces and a real problem of handwritten digit recognition using the MNISTdata set. The obtained results highlight the potential benefit of the proposedBayesian approaches for modeling surfaces possibly dispersed in particular inclusters.
arxiv-11700-78 | Sparse PCA via Bipartite Matchings | http://arxiv.org/pdf/1508.00625v1.pdf | author:Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, Alexandros G. Dimakis category:stat.ML cs.DS cs.LG math.OC published:2015-08-04 summary:We consider the following multi-component sparse PCA problem: given a set ofdata points, we seek to extract a small number of sparse components withdisjoint supports that jointly capture the maximum possible variance. Thesecomponents can be computed one by one, repeatedly solving the single-componentproblem and deflating the input data matrix, but as we show this greedyprocedure is suboptimal. We present a novel algorithm for sparse PCA thatjointly optimizes multiple disjoint components. The extracted features capturevariance that lies within a multiplicative factor arbitrarily close to 1 fromthe optimal. Our algorithm is combinatorial and computes the desired componentsby solving multiple instances of the bipartite maximum weight matching problem.Its complexity grows as a low order polynomial in the ambient dimension of theinput data matrix, but exponentially in its rank. However, it can beeffectively applied on a low-dimensional sketch of the data; this allows us toobtain polynomial-time approximation guarantees via spectral bounds. Weevaluate our algorithm on real data-sets and empirically demonstrate that inmany cases it outperforms existing, deflation-based approaches.
arxiv-11700-79 | Evaluating software-based fingerprint liveness detection using Convolutional Networks and Local Binary Patterns | http://arxiv.org/pdf/1508.00537v1.pdf | author:Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, Rubens Campos Machado category:cs.CV published:2015-08-03 summary:With the growing use of biometric authentication systems in the past years,spoof fingerprint detection has become increasingly important. In this work, weimplement and evaluate two different feature extraction techniques forsoftware-based fingerprint liveness detection: Convolutional Networks withrandom weights and Local Binary Patterns. Both techniques were used inconjunction with a Support Vector Machine (SVM) classifier. DatasetAugmentation was used to increase classifier's performance and a variety ofpreprocessing operations were tested, such as frequency filtering, contrastequalization, and region of interest filtering. The experiments were made onthe datasets used in The Liveness Detection Competition of years 2009, 2011 and2013, which comprise almost 50,000 real and fake fingerprints' images. Our bestmethod achieves an overall rate of 95.2% of correctly classified samples - animprovement of 35% in test error when compared with the best previouslypublished results.
arxiv-11700-80 | A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic Grouping | http://arxiv.org/pdf/1508.00507v1.pdf | author:Tameem Adel, Alexander Wong, Daniel Stashuk category:cs.LG cs.AI published:2015-08-03 summary:In this study, a spectral graph-theoretic grouping strategy for weaklysupervised classification is introduced, where a limited number of labelledsamples and a larger set of unlabelled samples are used to construct a largerannotated training set composed of strongly labelled and weakly labelledsamples. The inherent relationship between the set of strongly labelled samplesand the set of unlabelled samples is established via spectral grouping, withthe unlabelled samples subsequently weakly annotated based on the stronglylabelled samples within the associated spectral groups. A number of similaritygraph models for spectral grouping, including two new similarity graph modelsintroduced in this study, are explored to investigate their performance in thecontext of weakly supervised classification in handling different types ofdata. Experimental results using benchmark datasets as well as real EMGdatasets demonstrate that the proposed approach to weakly supervisedclassification can provide noticeable improvements in classificationperformance, and that the proposed similarity graph models can lead to ultimatelearning results that are either better than or on a par with existingsimilarity graph models in the context of spectral grouping for weaklysupervised classification.
arxiv-11700-81 | Evolutionary Algorithms: Concepts, Designs, and Applications in Bioinformatics: Evolutionary Algorithms for Bioinformatics | http://arxiv.org/pdf/1508.00468v1.pdf | author:Ka-Chun Wong category:cs.NE q-bio.GN q-bio.QM stat.CO stat.ME published:2015-08-03 summary:Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) inthe early 1970s, the study of evolutionary algorithm has emerged as a popularresearch field (Civicioglu & Besdok, 2013). Researchers from various scientificand engineering disciplines have been digging into this field, exploring theunique power of evolutionary algorithms (Hadka & Reed, 2013). Many applicationshave been successfully proposed in the past twenty years. For example,mechanical design (Lampinen & Zelinka, 1999), electromagnetic optimization(Rahmat-Samii & Michielssen, 1999), environmental protection (Bertini, Felice,Moretti, & Pizzuti, 2010), finance (Larkin & Ryan, 2010), musical orchestration(Esling, Carpentier, & Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, &Torresen, 2010), and nuclear reactor core design (Sacco, Henderson,Rios-Coelho, Ali, & Pereira, 2009). In particular, its function optimizationcapability was highlighted (Goldberg & Richardson, 1987) because of its highadaptability to different function landscapes, to which we cannot applytraditional optimization techniques (Wong, Leung, & Wong, 2009). Here we reviewthe applications of evolutionary algorithms in bioinformatics.
arxiv-11700-82 | Unsupervised Learning in Genome Informatics | http://arxiv.org/pdf/1508.00459v1.pdf | author:Ka-Chun Wong, Yue Li, Zhaolei Zhang category:q-bio.GN q-bio.QM stat.AP stat.ME stat.ML published:2015-08-03 summary:With different genomes available, unsupervised learning algorithms areessential in learning genome-wide biological insights. Especially, thefunctional characterization of different genomes is essential for us tounderstand lives. In this book chapter, we review the state-of-the-artunsupervised learning algorithms for genome informatics from DNA to MicroRNA. DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significantfraction of DNA regions (transcription factor binding sites) are bound byproteins (transcription factors) to regulate gene expression at differentdevelopment stages in different tissues. To fully understand genetics, it isnecessary of us to apply unsupervised learning algorithms to learn and inferthose DNA regions. Here we review several unsupervised learning methods fordeciphering the genome-wide patterns of those DNA regions. MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleicacid) species, regulate gene expression post-transcriptionally by formingimperfect base-pair with the target sites primarily at the 3$'$ untranslatedregions of the messenger RNAs. Since the 1993 discovery of the first miRNA\emph{let-7} in worms, a vast amount of studies have been dedicated tofunctionally characterizing the functional impacts of miRNA in a networkcontext to understand complex diseases such as cancer. Here we review severalrepresentative unsupervised learning frameworks on inferring miRNA regulatorynetwork by exploiting the static sequence-based information pertinent to theprior knowledge of miRNA targeting and the dynamic information of miRNAactivities implicated by the recently available large data compendia, whichinterrogate genome-wide expression profiles of miRNAs and/or mRNAs acrossvarious cell conditions.
arxiv-11700-83 | Evolutionary Multimodal Optimization: A Short Survey | http://arxiv.org/pdf/1508.00457v1.pdf | author:Ka-Chun Wong category:cs.NE cs.AI q-bio.QM published:2015-08-03 summary:Real world problems always have different multiple solutions. For instance,optical engineers need to tune the recording parameters to get as many optimalsolutions as possible for multiple trials in the varied-line-spacingholographic grating design problem. Unfortunately, most traditionaloptimization techniques focus on solving for a single optimal solution. Theyneed to be applied several times; yet all solutions are not guaranteed to befound. Thus the multimodal optimization problem was proposed. In that problem,we are interested in not only a single optimal point, but also the others. Withstrong parallel search capability, evolutionary algorithms are shown to beparticularly effective in solving this type of problem. In particular, theevolutionary algorithms for multimodal optimization usually not only locatemultiple optima in a single run, but also preserve their population diversitythroughout a run, resulting in their global optimization ability on multimodalfunctions. In addition, the techniques for multimodal optimization are borrowedas diversity maintenance techniques to other problems. In this chapter, wedescribe and review the state-of-the-arts evolutionary algorithms formultimodal optimization in terms of methodology, benchmarking, and application.
arxiv-11700-84 | Dependency-based Convolutional Neural Networks for Sentence Embedding | http://arxiv.org/pdf/1507.01839v2.pdf | author:Mingbo Ma, Liang Huang, Bing Xiang, Bowen Zhou category:cs.CL cs.AI cs.LG published:2015-07-07 summary:In sentence modeling and classification, convolutional neural networkapproaches have recently achieved state-of-the-art results, but all suchefforts process word vectors sequentially and neglect long-distancedependencies. To exploit both deep learning and linguistic structures, wepropose a tree-based convolutional neural network model which exploit variouslong-distance relationships between words. Our model improves the sequentialbaselines on all three sentiment and question classification tasks, andachieves the highest published accuracy on TREC.
arxiv-11700-85 | Significance of Maximum Spectral Amplitude in Sub-bands for Spectral Envelope Estimation and Its Application to Statistical Parametric Speech Synthesis | http://arxiv.org/pdf/1508.00354v1.pdf | author:Sivanand Achanta, Anandaswarup Vadapalli, Sai Krishna R., Suryakanth V. Gangashetty category:cs.SD cs.CL published:2015-08-03 summary:In this paper we propose a technique for spectral envelope estimation usingmaximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Mostother methods in the literature parametrize spectral envelope in cepstraldomain such as Mel-generalized cepstrum etc. Such cepstral domainrepresentations, although compact, are not readily interpretable. Thisdifficulty is overcome by our method which parametrizes in the spectral domainitself. In our experiments, spectral envelope estimated using MSASB method wasincorporated in the STRAIGHT vocoder. Both objective and subjective results ofanalysis-by-synthesis indicate that the proposed method is comparable toSTRAIGHT. We also evaluate the effectiveness of the proposed parametrization ina statistical parametric speech synthesis framework using deep neural networks.
arxiv-11700-86 | Time-series modeling with undecimated fully convolutional neural networks | http://arxiv.org/pdf/1508.00317v1.pdf | author:Roni Mittelman category:stat.ML cs.LG published:2015-08-03 summary:We present a new convolutional neural network-based time-series model.Typical convolutional neural network (CNN) architectures rely on the use ofmax-pooling operators in between layers, which leads to reduced resolution atthe top layers. Instead, in this work we consider a fully convolutional network(FCN) architecture that uses causal filtering operations, and allows for therate of the output signal to be the same as that of the input signal. Wefurthermore propose an undecimated version of the FCN, which we refer to as theundecimated fully convolutional neural network (UFCNN), and is motivated by theundecimated wavelet transform. Our experimental results verify that using theundecimated version of the FCN is necessary in order to allow for effectivetime-series modeling. The UFCNN has several advantages compared to othertime-series models such as the recurrent neural network (RNN) and longshort-term memory (LSTM), since it does not suffer from either the vanishing orexploding gradients problems, and is therefore easier to train. Convolutionoperations can also be implemented more efficiently compared to the recursionthat is involved in RNN-based models. We evaluate the performance of our modelin a synthetic target tracking task using bearing only measurements generatedfrom a state-space model, a probabilistic modeling of polyphonic musicsequences problem, and a high frequency trading task using a time-series ofask/bid quotes and their corresponding volumes. Our experimental results usingsynthetic and real datasets verify the significant advantages of the UFCNNcompared to the RNN and LSTM baselines.
arxiv-11700-87 | Local Color Contrastive Descriptor for Image Classification | http://arxiv.org/pdf/1508.00307v1.pdf | author:Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV published:2015-08-03 summary:Image representation and classification are two fundamental tasks towardsmultimedia content retrieval and understanding. The idea that shape and textureinformation (e.g. edge or orientation) are the key features for visualrepresentation is ingrained and dominated in current multimedia and computervision communities. A number of low-level features have been proposed bycomputing local gradients (e.g. SIFT, LBP and HOG), and have achieved greatsuccesses on numerous multimedia applications. In this paper, we present asimple yet efficient local descriptor for image classification, referred asLocal Color Contrastive Descriptor (LCCD), by leveraging the neural mechanismsof color contrast. The idea originates from the observation in neural sciencethat color and shape information are linked inextricably in visual corticalprocessing. The color contrast yields key information for visual colorperception and provides strong linkage between color and shape. We propose anovel contrastive mechanism to compute the color contrast in both spatiallocation and multiple channels. The color contrast is computed by measuring\emph{f}-divergence between the color distributions of two regions. Ourdescriptor enriches local image representation with both color and contrastinformation. We verified experimentally that it can compensate strongly for theshape based descriptor (e.g. SIFT), while keeping computationally simple.Extensive experimental results on image classification show that our descriptorimproves the performance of SIFT substantially by combinations, and achievesthe state-of-the-art performance on three challenging benchmark datasets. Itimproves recent Deep Learning model (DeCAF) [1] largely from the accuracy of40.94% to 49.68% in the large scale SUN397 database. Codes for the LCCD will beavailable.
arxiv-11700-88 | Compositional Semantic Parsing on Semi-Structured Tables | http://arxiv.org/pdf/1508.00305v1.pdf | author:Panupong Pasupat, Percy Liang category:cs.CL published:2015-08-03 summary:Two important aspects of semantic parsing for question answering are thebreadth of the knowledge source and the depth of logical compositionality.While existing work trades off one aspect for another, this papersimultaneously makes progress on both fronts through a new task: answeringcomplex questions on semi-structured tables using question-answer pairs assupervision. The central challenge arises from two compounding factors: thebroader domain results in an open-ended set of relations, and the deepercompositionality results in a combinatorial explosion in the space of logicalforms. We propose a logical-form driven parsing algorithm guided by strongtyping constraints and show that it obtains significant improvements overnatural baselines. For evaluation, we created a new dataset of 22,033 complexquestions on Wikipedia tables, which is made publicly available.
arxiv-11700-89 | When Crowdsourcing Meets Mobile Sensing: A Social Network Perspective | http://arxiv.org/pdf/1508.00299v1.pdf | author:Pin-Yu Chen, Shin-Ming Cheng, Pai-Shun Ting, Chia-Wei Lien, Fu-Jen Chu category:cs.SI stat.ML published:2015-08-03 summary:Mobile sensing is an emerging technology that utilizes agent-participatorydata for decision making or state estimation, including multimediaapplications. This article investigates the structure of mobile sensing schemesand introduces crowdsourcing methods for mobile sensing. Inspired by socialnetwork, one can establish trust among participatory agents to leverage thewisdom of crowds for mobile sensing. A prototype of social network inspiredmobile multimedia and sensing application is presented for illustrativepurpose. Numerical experiments on real-world datasets show improved performanceof mobile sensing via crowdsourcing. Challenges for mobile sensing with respectto Internet layers are discussed.
arxiv-11700-90 | Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS functions | http://arxiv.org/pdf/1504.03391v2.pdf | author:Vitaly Feldman, Jan Vondrak category:cs.DS cs.LG published:2015-04-13 summary:Submodular and fractionally subadditive (or equivalently XOS) functions playa fundamental role in combinatorial optimization, algorithmic game theory andmachine learning. Motivated by learnability of these classes of functions fromrandom examples, we consider the question of how well such functions can beapproximated by low-degree polynomials in $\ell_2$ norm over the uniformdistribution. This question is equivalent to understanding of the concentrationof Fourier weight on low-degree coefficients, a central concept in Fourieranalysis. We show that 1. For any submodular function $f:\{0,1\}^n \rightarrow [0,1]$, there is apolynomial of degree $O(\log (1/\epsilon) / \epsilon^{4/5})$ approximating $f$within $\epsilon$ in $\ell_2$, and there is a submodular function that requiresdegree $\Omega(1/\epsilon^{4/5})$. 2. For any XOS function $f:\{0,1\}^n \rightarrow [0,1]$, there is apolynomial of degree $O(1/\epsilon)$ and there exists an XOS function thatrequires degree $\Omega(1/\epsilon)$. This improves on previous approaches that all showed an upper bound of$O(1/\epsilon^2)$ for submodular and XOS functions. The best previous lowerbound was $\Omega(1/\epsilon^{2/3})$ for monotone submodular functions. Ourtechniques reveal new structural properties of submodular and XOS functions andthe upper bounds lead to nearly optimal PAC learning algorithms for theseclasses of functions.
arxiv-11700-91 | Optimal Radio Frequency Energy Harvesting with Limited Energy Arrival Knowledge | http://arxiv.org/pdf/1508.00285v1.pdf | author:Zhenhua Zou, Anders Gidmark, Themistoklis Charalambous, Mikael Johansson category:cs.IT cs.LG math.IT published:2015-08-02 summary:In this paper, we develop optimal policies for deciding when a wireless nodewith radio frequency (RF) energy harvesting (EH) capabilities should try andharvest ambient RF energy. While the idea of RF-EH is appealing, it is notalways beneficial to attempt to harvest energy; in environments where theambient energy is low, nodes could consume more energy being awake with theirharvesting circuits turned on than what they can extract from the ambient radiosignals; it is then better to enter a sleep mode until the ambient RF energyincreases. Towards this end, we consider a scenario with intermittent energyarrivals and a wireless node that wakes up for a period of time (herein calledthe time-slot) and harvests energy. If enough energy is harvested during thetime-slot, then the harvesting is successful and excess energy is stored;however, if there does not exist enough energy the harvesting is unsuccessfuland energy is lost. We assume that the ambient energy level is constant during the time-slot, andchanges at slot boundaries. The energy level dynamics are described by atwo-state Gilbert-Elliott Markov chain model, where the state of the Markovchain can only be observed during the harvesting action, and not when in sleepmode. Two scenarios are studied under this model. In the first scenario, weassume that we have knowledge of the transition probabilities of the Markovchain and formulate the problem as a Partially Observable Markov DecisionProcess (POMDP), where we find a threshold-based optimal policy. In the secondscenario, we assume that we don't have any knowledge about these parameters andformulate the problem as a Bayesian adaptive POMDP; to reduce the complexity ofthe computations we also propose a heuristic posterior sampling algorithm. Theperformance of our approaches is demonstrated via numerical examples.
arxiv-11700-92 | On Hyperspectral Classification in the Compressed Domain | http://arxiv.org/pdf/1508.00282v1.pdf | author:Mohammad Aghagolzadeh, Hayder Radha category:cs.CV published:2015-08-02 summary:In this paper, we study the problem of hyperspectral pixel classificationbased on the recently proposed architectures for compressive whisk-broomhyperspectral imagers without the need to reconstruct the complete data cube. Aclear advantage of classification in the compressed domain is its suitabilityfor real-time on-site processing of the sensed data. Moreover, it is assumedthat the training process also takes place in the compressed domain, thus,isolating the classification unit from the recovery unit at the receiver'sside. We show that, perhaps surprisingly, using distinct measurement matricesfor different pixels results in more accuracy of the learned classifier andconsistent classification performance, supporting the role of informationdiversity in learning.
arxiv-11700-93 | High-speed detection of emergent market clustering via an unsupervised parallel genetic algorithm | http://arxiv.org/pdf/1403.4099v4.pdf | author:Dieter Hendricks, Diane Wilcox, Tim Gebbie category:q-fin.CP cs.DC cs.NE published:2014-03-17 summary:We implement a master-slave parallel genetic algorithm (PGA) with a bespokelog-likelihood fitness function to identify emergent clusters within priceevolutions. We use graphics processing units (GPUs) to implement a PGA andvisualise the results using disjoint minimal spanning trees (MSTs). Wedemonstrate that our GPU PGA, implemented on a commercially available generalpurpose GPU, is able to recover stock clusters in sub-second speed, based on asubset of stocks in the South African market. This represents a pragmaticchoice for low-cost, scalable parallel computing and is significantly fasterthan a prototype serial implementation in an optimised C-basedfourth-generation programming language, although the results are not directlycomparable due to compiler differences. Combined with fast online intradaycorrelation matrix estimation from high frequency data for clusteridentification, the proposed implementation offers cost-effective,near-real-time risk assessment for financial practitioners.
arxiv-11700-94 | Dictionary and Image Recovery from Incomplete and Random Measurements | http://arxiv.org/pdf/1508.00278v1.pdf | author:Mohammad Aghagolzadeh, Hayder Radha category:cs.CV published:2015-08-02 summary:This paper tackles algorithmic and theoretical aspects of dictionary learningfrom incomplete and random block-wise image measurements and the performance ofthe adaptive dictionary for sparse image recovery. This problem is related toblind compressed sensing in which the sparsifying dictionary or basis is viewedas an unknown variable and subject to estimation during sparse recovery.However, unlike existing guarantees for a successful blind compressed sensing,our results do not rely on additional structural constraints on the learneddictionary or the measured signal. In particular, we rely on the spatialdiversity of compressive measurements to guarantee that the solution is uniquewith a high probability. Moreover, our distinguishing goal is to measure andreduce the estimation error with respect to the ideal dictionary that is basedon the complete image. Using recent results from random matrix theory, we showthat applying a slightly modified dictionary learning algorithm overcompressive measurements results in accurate estimation of the ideal dictionaryfor large-scale images. Empirically, we experiment with both space-invariantand space-varying sensing matrices and demonstrate the critical role of spatialdiversity in measurements. Simulation results confirm that the presentedalgorithm outperforms the typical non-adaptive sparse recovery based onoffline-learned universal dictionaries.
arxiv-11700-95 | Partial matching face recognition method for rehabilitation nursing robots beds | http://arxiv.org/pdf/1508.00239v1.pdf | author:Dongmei Liang, Wushan Cheng category:cs.CV published:2015-08-02 summary:In order to establish face recognition system in rehabilitation nursingrobots beds and achieve real-time monitor the patient on the bed. We propose aface recognition method based on partial matching Hu moments which apply forrehabilitation nursing robots beds. Firstly we using Haar classifier to detecthuman faces automatically in dynamic video frames. Secondly we using Otsuthreshold method to extract facial features (eyebrows, eyes, mouth) in the faceimage and its Hu moments. Finally, we using Hu moment feature set to achievethe automatic face recognition. Experimental results show that this method canefficiently identify face in a dynamic video and it has high practical value(the accuracy rate is 91% and the average recognition time is 4.3s).
arxiv-11700-96 | Toward a Robust Sparse Data Representation for Wireless Sensor Networks | http://arxiv.org/pdf/1508.00230v1.pdf | author:Mohammad Abu Alsheikh, Shaowei Lin, Hwee-Pink Tan, Dusit Niyato category:cs.NI cs.LG cs.NE published:2015-08-02 summary:Compressive sensing has been successfully used for optimized operations inwireless sensor networks. However, raw data collected by sensors may be neitheroriginally sparse nor easily transformed into a sparse data representation.This paper addresses the problem of transforming source data collected bysensor nodes into a sparse representation with a few nonzero elements. Ourcontributions that address three major issues include: 1) an effective methodthat extracts population sparsity of the data, 2) a sparsity ratio guaranteescheme, and 3) a customized learning algorithm of the sparsifying dictionary.We introduce an unsupervised neural network to extract an intrinsic sparsecoding of the data. The sparse codes are generated at the activation of thehidden layer using a sparsity nomination constraint and a shrinking mechanism.Our analysis using real data samples shows that the proposed method outperformsconventional sparsity-inducing methods.
arxiv-11700-97 | PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks | http://arxiv.org/pdf/1508.00200v1.pdf | author:Jian Tang, Meng Qu, Qiaozhu Mei category:cs.CL cs.LG cs.NE I.2.6 published:2015-08-02 summary:Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,have been attracting increasing attention due to their simplicity, scalability,and effectiveness. However, comparing to sophisticated deep learningarchitectures such as convolutional neural networks, these methods usuallyyield inferior results when applied to particular machine learning tasks. Onepossible reason is that these text embedding methods learn the representationof text in a fully unsupervised way, without leveraging the labeled informationavailable for the task. Although the low dimensional representations learnedare applicable to many different tasks, they are not particularly tuned for anytask. In this paper, we fill this gap by proposing a semi-supervisedrepresentation learning method for text data, which we call the\textit{predictive text embedding} (PTE). Predictive text embedding utilizesboth labeled and unlabeled data to learn the embedding of text. The labeledinformation and different levels of word co-occurrence information are firstrepresented as a large-scale heterogeneous text network, which is then embeddedinto a low dimensional space through a principled and efficient algorithm. Thislow dimensional embedding not only preserves the semantic closeness of wordsand documents, but also has a strong predictive power for the particular task.Compared to recent supervised approaches based on convolutional neuralnetworks, predictive text embedding is comparable or more effective, much moreefficient, and has fewer parameters to tune.
arxiv-11700-98 | Class Vectors: Embedding representation of Document Classes | http://arxiv.org/pdf/1508.00189v1.pdf | author:Devendra Singh Sachan, Shailesh Kumar category:cs.CL cs.IR published:2015-08-02 summary:Distributed representations of words and paragraphs as semantic embeddings inhigh dimensional data are used across a number of Natural LanguageUnderstanding tasks such as retrieval, translation, and classification. In thiswork, we propose "Class Vectors" - a framework for learning a vector per classin the same embedding space as the word and paragraph embeddings. Similaritybetween these class vectors and word vectors are used as features to classify adocument to a class. In experiment on several sentiment analysis tasks such asYelp reviews and Amazon electronic product reviews, class vectors have shownbetter or comparable results in classification while learning very meaningfulclass embeddings.
arxiv-11700-99 | An Analytic Framework for Maritime Situation Analysis | http://arxiv.org/pdf/1508.00181v1.pdf | author:Hamed Yaghoubi Shahir, Uwe GlÃ¤sser, Amir Yaghoubi Shahir, Hans Wehn category:cs.LG published:2015-08-02 summary:Maritime domain awareness is critical for protecting sea lanes, ports,harbors, offshore structures and critical infrastructures against commonthreats and illegal activities. Limited surveillance resources constrainmaritime domain awareness and compromise full security coverage at all times.This situation calls for innovative intelligent systems for interactivesituation analysis to assist marine authorities and security personal in theirroutine surveillance operations. In this article, we propose a novel situationanalysis framework to analyze marine traffic data and differentiate variousscenarios of vessel engagement for the purpose of detecting anomalies ofinterest for marine vessels that operate over some period of time in relativeproximity to each other. The proposed framework views vessel behavior asprobabilistic processes and uses machine learning to model common vesselinteraction patterns. We represent patterns of interest as left-to-right HiddenMarkov Models and classify such patterns using Support Vector Machines.
arxiv-11700-100 | What makes for effective detection proposals? | http://arxiv.org/pdf/1502.05082v3.pdf | author:Jan Hosang, Rodrigo Benenson, Piotr DollÃ¡r, Bernt Schiele category:cs.CV published:2015-02-17 summary:Current top performing object detectors employ detection proposals to guidethe search for objects, thereby avoiding exhaustive sliding window searchacross images. Despite the popularity and widespread use of detectionproposals, it is unclear which trade-offs are made when using them duringobject detection. We provide an in-depth analysis of twelve proposal methodsalong with four baselines regarding proposal repeatability, ground truthannotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,R-CNN, and Fast R-CNN detection performance. Our analysis shows that for objectdetection improving proposal localisation accuracy is as important as improvingrecall. We introduce a novel metric, the average recall (AR), which rewardsboth high recall and good localisation and correlates surprisingly well withdetection performance. Our findings show common strengths and weaknesses ofexisting methods, and provide insights and metrics for selecting and tuningproposal methods.
arxiv-11700-101 | Towards Distortion-Predictable Embedding of Neural Networks | http://arxiv.org/pdf/1508.00102v1.pdf | author:Axel Angel category:cs.CV published:2015-08-01 summary:Current research in Computer Vision has shown that Convolutional NeuralNetworks (CNN) give state-of-the-art performance in many classification tasksand Computer Vision problems. The embedding of CNN, which is the internalrepresentation produced by the last layer, can indirectly learn topological andrelational properties. Moreover, by using a suitable loss function, CNN modelscan learn invariance to a wide range of non-linear distortions such asrotation, viewpoint angle or lighting condition. In this work, new insights arediscovered about CNN embeddings and a new loss function is proposed, derivedfrom the contrastive loss, that creates models with more predicable mappingsand also quantifies distortions. In typical distortion-dependent methods, thereis no simple relation between the features corresponding to one image and thefeatures of this image distorted. Therefore, these methods require tofeed-forward inputs under every distortions in order to find the correspondingfeatures representations. Our contribution makes a step towards embeddingswhere features of distorted inputs are related and can be derived from eachothers by the intensity of the distortion.
arxiv-11700-102 | The Interactive Effects of Operators and Parameters to GA Performance Under Different Problem Sizes | http://arxiv.org/pdf/1508.00097v1.pdf | author:Jaderick P. Pabico, Elizer A. Albacea category:cs.NE published:2015-08-01 summary:The complex effect of genetic algorithm's (GA) operators and parameters toits performance has been studied extensively by researchers in the past butnone studied their interactive effects while the GA is under different problemsizes. In this paper, We present the use of experimental model (1)~toinvestigate whether the genetic operators and their parameters interact toaffect the offline performance of GA, (2)~to find what combination of geneticoperators and parameter settings will provide the optimum performance for GA,and (3)~to investigate whether these operator-parameter combination isdependent on the problem size. We designed a GA to optimize a family oftraveling salesman problems (TSP), with their optimal solutions known forconvenient benchmarking. Our GA was set to use different algorithms insimulating selection ($\Omega_s$), different algorithms ($\Omega_c$) andparameters ($p_c$) in simulating crossover, and different parameters ($p_m$) insimulating mutation. We used several $n$-city TSPs ($n=\{5, 7, 10, 100,1000\}$) to represent the different problem sizes (i.e., size of the resultingsearch space as represented by GA schemata). Using analysis of variance of3-factor factorial experiments, we found out that GA performance is affected by$\Omega_s$ at small problem size (5-city TSP) where the algorithm PartiallyMatched Crossover significantly outperforms Cycle Crossover at $95\%$confidence level.
arxiv-11700-103 | Land Use Classification in Remote Sensing Images by Convolutional Neural Networks | http://arxiv.org/pdf/1508.00092v1.pdf | author:Marco Castelluccio, Giovanni Poggi, Carlo Sansone, Luisa Verdoliva category:cs.CV published:2015-08-01 summary:We explore the use of convolutional neural networks for the semanticclassification of remote sensing scenes. Two recently proposed architectures,CaffeNet and GoogLeNet, are adopted, with three different learning modalities.Besides conventional training from scratch, we resort to pre-trained networksthat are only fine-tuned on the target data, so as to avoid overfittingproblems and reduce design time. Experiments on two remote sensing datasets,with markedly different characteristics, testify on the effectiveness and wideapplicability of the proposed solution, which guarantees a significantperformance improvement over all state-of-the-art references.
arxiv-11700-104 | Turnover Prediction Of Shares using Data Mining techniques : A Case Study | http://arxiv.org/pdf/1508.00088v1.pdf | author:D. S. Shashaank, V. Sruthi, M. L. S Vijayalakshimi, Jacob Shomona Garcia category:cs.LG published:2015-08-01 summary:Predicting the turnover of a company in the ever fluctuating Stock market hasalways proved to be a precarious situation and most certainly a difficult taskin hand. Data mining is a well-known sphere of Computer Science that aims onextracting meaningful information from large databases. However, despite theexistence of many algorithms for the purpose of predicting the future trends,their efficiency is questionable as their predictions suffer from a high errorrate. The objective of this paper is to investigate various classificationalgorithms to predict the turnover of different companies based on the Stockprice. The authorized dataset for predicting the turnover was taken fromwww.bsc.com and included the stock market values of various companies over thepast 10 years. The algorithms were investigated using the "R" tool. The featureselection algorithm, Boruta, was run on this dataset to extract the importantand influential features for classification. With these extracted features, theTotal Turnover of the company was predicted using various classificationalgorithms like Random Forest, Decision Tree, SVM and Multinomial Regression.This prediction mechanism was implemented to predict the turnover of a companyon an everyday basis and hence could help navigate through dubious stock markettrades. An accuracy rate of 95% was achieved by the above prediction process.Moreover, the importance of stock market attributes was established as well.
arxiv-11700-105 | Regularized Multi-Task Learning for Multi-Dimensional Log-Density Gradient Estimation | http://arxiv.org/pdf/1508.00085v1.pdf | author:Ikko Yamane, Hiroaki Sasaki, Masashi Sugiyama category:stat.ML published:2015-08-01 summary:Log-density gradient estimation is a fundamental statistical problem andpossesses various practical applications such as clustering and measuringnon-Gaussianity. A naive two-step approach of first estimating the density andthen taking its log-gradient is unreliable because an accurate density estimatedoes not necessarily lead to an accurate log-density gradient estimate. To copewith this problem, a method to directly estimate the log-density gradientwithout density estimation has been explored, and demonstrated to work muchbetter than the two-step method. The objective of this paper is to furtherimprove the performance of this direct method in multi-dimensional cases. Ouridea is to regard the problem of log-density gradient estimation in eachdimension as a task, and apply regularized multi-task learning to the directlog-density gradient estimator. We experimentally demonstrate the usefulness ofthe proposed multi-task method in log-density gradient estimation andmode-seeking clustering.
arxiv-11700-106 | The Child is Father of the Man: Foresee the Success at the Early Stage | http://arxiv.org/pdf/1504.00948v3.pdf | author:Liangyue Li, Hanghang Tong category:cs.LG published:2015-04-03 summary:Understanding the dynamic mechanisms that drive the high-impact scientificwork (e.g., research papers, patents) is a long-debated research topic and hasmany important implications, ranging from personal career development andrecruitment search, to the jurisdiction of research resources. Recent advancesin characterizing and modeling scientific success have made it possible toforecast the long-term impact of scientific work, where data mining techniques,supervised learning in particular, play an essential role. Despite muchprogress, several key algorithmic challenges in relation to predictinglong-term scientific impact have largely remained open. In this paper, wepropose a joint predictive model to forecast the long-term scientific impact atthe early stage, which simultaneously addresses a number of these openchallenges, including the scholarly feature design, the non-linearity, thedomain-heterogeneity and dynamics. In particular, we formulate it as aregularized optimization problem and propose effective and scalable algorithmsto solve it. We perform extensive empirical evaluations on large, realscholarly data sets to validate the effectiveness and the efficiency of ourmethod.
arxiv-11700-107 | Classification with Asymmetric Label Noise: Consistency and Maximal Denoising | http://arxiv.org/pdf/1303.1208v2.pdf | author:Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, Clayton Scott category:stat.ML cs.LG published:2013-03-05 summary:In many real-world classification problems, the labels of training examplesare randomly corrupted. Most previous theoretical work on classification withlabel noise assumes that the two classes are separable, that the label noise isindependent of the true class label, or that the noise proportions for eachclass are known. In this work, we give conditions that are necessary andsufficient for the true class-conditional distributions to be identifiable.These conditions are weaker than those analyzed previously, and allow for theclasses to be nonseparable and the noise levels to be asymmetric and unknown.The conditions essentially state that a majority of the observed labels arecorrect and that the true class-conditional distributions are "mutuallyirreducible," a concept we introduce that limits the similarity of the twodistributions. For any label noise problem, there is a unique pair of trueclass-conditional distributions satisfying the proposed conditions, and weargue that this pair corresponds in a certain sense to maximal denoising of theobserved distributions. Our results are facilitated by a connection to "mixture proportionestimation," which is the problem of estimating the maximal proportion of onedistribution that is present in another. We establish a novel rate ofconvergence result for mixture proportion estimation, and apply this to obtainconsistency of a discrimination rule based on surrogate loss minimization.Experimental results on benchmark data and a nuclear particle classificationproblem demonstrate the efficacy of our approach.
arxiv-11700-108 | Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos | http://arxiv.org/pdf/1507.05738v2.pdf | author:Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, Li Fei-Fei category:cs.CV published:2015-07-21 summary:Every moment counts in action recognition. A comprehensive understanding ofhuman activity in video requires labeling every frame according to the actionsoccurring, placing multiple labels densely over a video sequence. To study thisproblem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a newdataset of dense labels over unconstrained internet videos. Modeling multiple,dense labels benefits from temporal relations within and across classes. Wedefine a novel variant of long short-term memory (LSTM) deep networks formodeling these temporal relations via multiple input and output connections. Weshow that this model improves action labeling accuracy and further enablesdeeper understanding tasks ranging from structured retrieval to actionprediction.
arxiv-11700-109 | Bandits with Knapsacks | http://arxiv.org/pdf/1305.2545v6.pdf | author:Ashwinkumar Badanidiyuru, Robert Kleinberg, Aleksandrs Slivkins category:cs.DS cs.LG published:2013-05-11 summary:Multi-armed bandit problems are the predominant theoretical model ofexploration-exploitation tradeoffs in learning, and they have countlessapplications ranging from medical trials, to communication networks, to Websearch and advertising. In many of these application domains the learner may beconstrained by one or more supply (or budget) limits, in addition to thecustomary limitation on the time horizon. The literature lacks a general modelencompassing these sorts of problems. We introduce such a model, called"bandits with knapsacks", that combines aspects of stochastic integerprogramming with online learning. A distinctive feature of our problem, incomparison to the existing regret-minimization literature, is that the optimalpolicy for a given latent distribution may significantly outperform the policythat plays the optimal fixed arm. Consequently, achieving sublinear regret inthe bandits-with-knapsacks problem is significantly more challenging than inconventional bandit problems. We present two algorithms whose reward is close to the information-theoreticoptimum: one is based on a novel "balanced exploration" paradigm, while theother is a primal-dual algorithm that uses multiplicative updates. Further, weprove that the regret achieved by both algorithms is optimal up topolylogarithmic factors. We illustrate the generality of the problem bypresenting applications in a number of different domains including electroniccommerce, routing, and scheduling. As one example of a concrete application, weconsider the problem of dynamic posted pricing with limited supply and obtainthe first algorithm whose regret, with respect to the optimal dynamic policy,is sublinear in the supply.
arxiv-11700-110 | Resourceful Contextual Bandits | http://arxiv.org/pdf/1402.6779v6.pdf | author:Ashwinkumar Badanidiyuru, John Langford, Aleksandrs Slivkins category:cs.LG cs.DS cs.GT published:2014-02-27 summary:We study contextual bandits with ancillary constraints on resources, whichare common in real-world applications such as choosing ads or dynamic pricingof items. We design the first algorithm for solving these problems that handlesconstrained resources other than time, and improves over a trivial reduction tothe non-contextual case. We consider very general settings for both contextualbandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits withresource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)),and prove a regret guarantee with near-optimal statistical properties.
arxiv-11700-111 | SnowWatch: Snow Monitoring through Acquisition and Analysis of User-Generated Content | http://arxiv.org/pdf/1507.08958v1.pdf | author:Roman Fedorov, Piero Fraternali, Chiara Pasini, Marco Tagliasacchi category:cs.CV cs.CY published:2015-07-31 summary:We present a system for complementing snow phenomena monitoring with virtualmeasurements extracted from public visual content. The proposed systemintegrates an automatic acquisition and analysis of photographs and webcamimages depicting Alpine mountains. In particular, the technical demonstrationconsists in a web portal that interfaces the whole system with the population.It acts as an entertaining photo-sharing social web site, acquiring at the sametime visual content necessary for environmental monitoring.
arxiv-11700-112 | Efficient and robust calibration of the Heston option pricing model for American options using an improved Cuckoo Search Algorithm | http://arxiv.org/pdf/1507.08937v1.pdf | author:Stefan Haring, Ronald Hochreiter category:cs.NE q-fin.PR published:2015-07-31 summary:In this paper an improved Cuckoo Search Algorithm is developed to allow foran efficient and robust calibration of the Heston option pricing model forAmerican options. Calibration of stochastic volatility models like the Hestonis significantly harder than classical option pricing models as more parametershave to be estimated. The difficult task of calibrating one of these models toAmerican Put options data is the main objective of this paper. Numericalresults are shown to substantiate the suitability of the chosen method totackle this problem.
arxiv-11700-113 | Spin Glass Models of Syntax and Language Evolution | http://arxiv.org/pdf/1508.00504v1.pdf | author:Karthik Siva, Jim Tao, Matilde Marcolli category:cs.CL physics.soc-ph 91F20, 82B20 published:2015-07-31 summary:Using the SSWL database of syntactic parameters of world languages, and theMIT Media Lab data on language interactions, we construct a spin glass model oflanguage evolution. We treat binary syntactic parameters as spin states, withlanguages as vertices of a graph, and assigned interaction energies along theedges. We study a rough model of syntax evolution, under the assumption that astrong interaction energy tends to cause parameters to align, as in the case offerromagnetic materials. We also study how the spin glass model needs to bemodified to account for entailment relations between syntactic parameters. Thismodification leads naturally to a generalization of Potts models with externalmagnetic field, which consists of a coupling at the vertices of an Ising modeland a Potts model with q=3, that have the same edge interactions. We describethe results of simulations of the dynamics of these models, in differenttemperature and energy regimes. We discuss the linguistic interpretation of theparameters of the physical model.
arxiv-11700-114 | Mobile Multi-View Object Image Search | http://arxiv.org/pdf/1507.08861v1.pdf | author:Fatih ÃalÄ±ÅÄ±r, ÃzgÃ¼r Ulusoy, UÄur GÃ¼dÃ¼kbay, Muhammet BaÅtan category:cs.MM cs.CV published:2015-07-31 summary:High user interaction capability of mobile devices can help improve theaccuracy of mobile visual search systems. At query time, it is possible tocapture multiple views of an object from different viewing angles and atdifferent scales with the mobile device camera to obtain richer informationabout the object compared to a single view and hence return more accurateresults. Motivated by this, we developed a mobile multi-view object imagesearch system, using a client-server architecture. Multi-view images of objectsacquired by the mobile clients are processed and local features are sent to theserver, which combines the query image representations with early/late fusionmethods based on bag-of-visual-words and sends back the query results. Weperformed a comprehensive analysis of early and late fusion approaches usingvarious similarity functions, on an existing single view and a new multi-viewobject image database. The experimental results show that multi-view searchprovides significantly better retrieval accuracy compared to single viewsearch.
arxiv-11700-115 | A novel multivariate performance optimization method based on sparse coding and hyper-predictor learning | http://arxiv.org/pdf/1507.08847v1.pdf | author:Jiachen Yanga, Zhiyong Dinga, Fei Guoa, Huogen Wanga, Nick Hughesb category:cs.LG cs.CV cs.NA published:2015-07-31 summary:In this paper, we investigate the problem of optimization multivariateperformance measures, and propose a novel algorithm for it. Different fromtraditional machine learning methods which optimize simple loss functions tolearn prediction function, the problem studied in this paper is how to learneffective hyper-predictor for a tuple of data points, so that a complex lossfunction corresponding to a multivariate performance measure can be minimized.We propose to present the tuple of data points to a tuple of sparse codes via adictionary, and then apply a linear function to compare a sparse code against agive candidate class label. To learn the dictionary, sparse codes, andparameter of the linear function, we propose a joint optimization problem. Inthis problem, the both the reconstruction error and sparsity of sparse code,and the upper bound of the complex loss function are minimized. Moreover, theupper bound of the loss function is approximated by the sparse codes and thelinear function parameter. To optimize this problem, we develop an iterativealgorithm based on descent gradient methods to learn the sparse codes andhyper-predictor parameter alternately. Experiment results on some benchmarkdata sets show the advantage of the proposed methods over otherstate-of-the-art algorithms.
arxiv-11700-116 | Image Super-Resolution Using Deep Convolutional Networks | http://arxiv.org/pdf/1501.00092v3.pdf | author:Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang category:cs.CV cs.NE I.4.5; I.2.6 published:2014-12-31 summary:We propose a deep learning method for single image super-resolution (SR). Ourmethod directly learns an end-to-end mapping between the low/high-resolutionimages. The mapping is represented as a deep convolutional neural network (CNN)that takes the low-resolution image as the input and outputs thehigh-resolution one. We further show that traditional sparse-coding-based SRmethods can also be viewed as a deep convolutional network. But unliketraditional methods that handle each component separately, our method jointlyoptimizes all layers. Our deep CNN has a lightweight structure, yetdemonstrates state-of-the-art restoration quality, and achieves fast speed forpractical on-line usage. We explore different network structures and parametersettings to achieve trade-offs between performance and speed. Moreover, weextend our network to cope with three color channels simultaneously, and showbetter overall reconstruction quality.
arxiv-11700-117 | Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity | http://arxiv.org/pdf/1507.08788v1.pdf | author:Ohad Shamir category:cs.LG cs.NA math.NA math.OC stat.ML published:2015-07-31 summary:We study the convergence properties of the VR-PCA algorithm introduced by\cite{shamir2015stochastic} for fast computation of leading singular vectors.We prove several new results, including a formal analysis of a block version ofthe algorithm, and convergence from random initialization. We also make a fewobservations of independent interest, such as how pre-initializing with just asingle exact power iteration can significantly improve the runtime ofstochastic methods, and what are the convexity and non-convexity properties ofthe underlying optimization problem.
arxiv-11700-118 | Dense v.s. Sparse: A Comparative Study of Sampling Analysis in Scene Classification of High-Resolution Remote Sensing Imagery | http://arxiv.org/pdf/1502.01097v2.pdf | author:Jingwen Hu, Gui-Song Xia, Fan Hu, Liangpei Zhang category:cs.CV published:2015-02-04 summary:Scene classification is a key problem in the interpretation ofhigh-resolution remote sensing imagery. Many state-of-the-art methods, e.g.bag-of-visual-words model and its variants, the topic models as well as deeplearning-based approaches, share similar procedures: patch sampling, featuredescription/learning and classification. Patch sampling is the first and a keyprocedure which has a great influence on the results. In the literature, manydifferent sampling strategies have been used, {e.g. dense sampling, randomsampling, keypoint-based sampling and saliency-based sampling, etc. However, itis still not clear which sampling strategy is suitable for the sceneclassification of high-resolution remote sensing images. In this paper, wecomparatively study the effects of different sampling strategies under thescenario of scene classification of high-resolution remote sensing images. Wedivide the existing sampling methods into two types: dense sampling and sparsesampling, the later of which includes random sampling, keypoint-based samplingand various saliency-based sampling proposed recently. In order to comparetheir performances, we rely on a standard bag-of-visual-words model toconstruct our testing scheme, owing to their simplicity, robustness andefficiency. The experimental results on two commonly used datasets show thatdense sampling has the best performance among all the strategies but with highspatial and computational complexity, random sampling gives better orcomparable results than other sparse sampling methods, like the sophisticatedmulti-scale key-point operators and the saliency-based methods which areintensively studied and commonly used recently.
arxiv-11700-119 | Multimodal Multipart Learning for Action Recognition in Depth Videos | http://arxiv.org/pdf/1507.08761v1.pdf | author:Amir Shahroudy, Gang Wang, Tian-Tsong Ng, Qingxiong Yang category:cs.CV published:2015-07-31 summary:The articulated and complex nature of human actions makes the task of actionrecognition difficult. One approach to handle this complexity is dividing it tothe kinetics of body parts and analyzing the actions based on these partialdescriptors. We propose a joint sparse regression based learning method whichutilizes the structured sparsity to model each action as a combination ofmultimodal features from a sparse set of body parts. To represent dynamics andappearance of parts, we employ a heterogeneous set of depth and skeleton basedfeatures. The proper structure of multimodal multipart features are formulatedinto the learning framework via the proposed hierarchical mixed norm, toregularize the structured features of each part and to apply sparsity betweenthem, in favor of a group feature selection. Our experimental results exposethe effectiveness of the proposed learning method in which it outperforms othermethods in all three tested datasets while saturating one of them by achievingperfect accuracy.
arxiv-11700-120 | Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification | http://arxiv.org/pdf/1507.08754v1.pdf | author:Fa Wu, Peijun Hu, Dexing Kong category:cs.CV published:2015-07-31 summary:This paper presents a new version of Dropout called Split Dropout (sDropout)and rotational convolution techniques to improve CNNs' performance on imageclassification. The widely used standard Dropout has advantage of preventingdeep neural networks from overfitting by randomly dropping units duringtraining. Our sDropout randomly splits the data into two subsets and keeps bothrather than discards one subset. We also introduce two rotational convolutiontechniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-poolingconvolution (FRPC) to boost CNNs' performance on the robustness for rotationtransformation. These two techniques encode rotation invariance into thenetwork without adding extra parameters. Experimental evaluations onImageNet2012 classification task demonstrate that sDropout not only enhancesthe performance but also converges faster. Additionally, RPC and FRPC make CNNsmore robust for rotation transformations. Overall, FRPC together with sDropoutbring $1.18\%$ (model of Zeiler and Fergus~\cite{zeiler2013visualizing},10-view, top-1) accuracy increase in ImageNet 2012 classification task comparedto the original network.
arxiv-11700-121 | An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback | http://arxiv.org/pdf/1507.08752v1.pdf | author:Ohad Shamir category:cs.LG math.OC stat.ML published:2015-07-31 summary:We consider the closely related problems of bandit convex optimization withtwo-point feedback, and zero-order stochastic convex optimization with twofunction evaluations per round. We provide a simple algorithm and analysiswhich is optimal for convex Lipschitz functions. This improves on\cite{dujww13}, which only provides an optimal result for smooth functions;Moreover, the algorithm and analysis are simpler, and readily extend tonon-Euclidean problems. The algorithm is based on a small but surprisinglypowerful modification of the gradient estimator.
arxiv-11700-122 | A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate | http://arxiv.org/pdf/1409.2848v5.pdf | author:Ohad Shamir category:cs.LG cs.NA math.OC stat.ML published:2014-09-09 summary:We describe and analyze a simple algorithm for principal component analysisand singular value decomposition, VR-PCA, which uses computationally cheapstochastic iterations, yet converges exponentially fast to the optimalsolution. In contrast, existing algorithms suffer either from slow convergence,or computationally intensive iterations whose runtime scales with the datasize. The algorithm builds on a recent variance-reduced stochastic gradienttechnique, which was previously analyzed for strongly convex optimization,whereas here we apply it to an inherently non-convex problem, using a verydifferent analysis.
arxiv-11700-123 | A Sinc Wavelet Describes the Receptive Fields of Neurons in the Motion Cortex | http://arxiv.org/pdf/1507.08736v1.pdf | author:Stephen G. Odaibo category:q-bio.NC cs.CV cs.IT math.IT physics.bio-ph published:2015-07-31 summary:Visual perception results from a systematic transformation of the informationflowing through the visual system. In the neuronal hierarchy, the responseproperties of single neurons are determined by neurons located one level below,and in turn, determine the responses of neurons located one level above.Therefore in modeling receptive fields, it is essential to ensure that theresponse properties of neurons in a given level can be generated by combiningthe response models of neurons in its input levels. However, existing responsemodels of neurons in the motion cortex do not inherently yield the temporalfrequency filtering gradient (TFFG) property that is known to emerge along theprimary visual cortex (V1) to middle temporal (MT) motion processing stream.TFFG is the change from predominantly lowpass to predominantly bandpasstemporal frequency filtering character along the V1 to MT pathway (Foster et al1985; DeAngelis et al 1993; Hawken et al 1996). We devised a new model, thesinc wavelet model (Odaibo, 2014), which logically and efficiently generatesthe TFFG. The model replaces the Gabor function's sine wave carrier with a sinc(sin(x)/x) function, and has the same or fewer number of parameters as existingmodels. Because of its logical consistency with the emergent network propertyof TFFG, we conclude that the sinc wavelet is a better model for the receptivefields of motion cortex neurons. This model will provide new physiologicalinsights into how the brain represents visual information.
arxiv-11700-124 | Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs | http://arxiv.org/pdf/1507.08711v1.pdf | author:Mehrtash Harandi, Mathieu Salzmann, Mahsa Baktashmotlagh category:cs.CV published:2015-07-31 summary:State-of-the-art image-set matching techniques typically implicitly modeleach image-set with a Gaussian distribution. Here, we propose to go beyondthese representations and model image-sets as probability distributionfunctions (PDFs) using kernel density estimators. To compare and matchimage-sets, we exploit Csiszar f-divergences, which bear strong connections tothe geodesic distance defined on the space of PDFs, i.e., the statisticalmanifold. Furthermore, we introduce valid positive definite kernels on thestatistical manifolds, which let us make use of more powerful classificationschemes to match image-sets. Finally, we introduce a supervised dimensionalityreduction technique that learns a latent space where f-divergences reflect theclass labels of the data. Our experiments on diverse problems, such asvideo-based face recognition and dynamic texture classification, evidence thebenefits of our approach over the state-of-the-art image-set matching methods.
arxiv-11700-125 | Finding One Community in a Sparse Graph | http://arxiv.org/pdf/1502.05680v2.pdf | author:Andrea Montanari category:stat.ML cs.SI published:2015-02-19 summary:We consider a random sparse graph with bounded average degree, in which asubset of vertices has higher connectivity than the background. In particular,the average degree inside this subset of vertices is larger than outside (butstill bounded). Given a realization of such graph, we aim at identifying thehidden subset of vertices. This can be regarded as a model for the problem offinding a tightly knitted community in a social network, or a cluster in arelational dataset. In this paper we present two sets of contributions: $(i)$ We use the cavitymethod from spin glass theory to derive an exact phase diagram for thereconstruction problem. In particular, as the difference in edge probabilityincreases, the problem undergoes two phase transitions, a static phasetransition and a dynamic one. $(ii)$ We establish rigorous bounds on thedynamic phase transition and prove that, above a certain threshold, a localalgorithm (belief propagation) correctly identify most of the hidden set. Belowthe same threshold \emph{no local algorithm} can achieve this goal. However, inthis regime the subset can be identified by exhaustive search. For small hidden sets and large average degree, the phase transition forlocal algorithms takes an intriguingly simple form. Local algorithms succeedwith high probability for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} >\sqrt{{\rm deg}_{\rm out}/e}$ and fail for ${\rm deg}_{\rm in} - {\rm deg}_{\rmout} < \sqrt{{\rm deg}_{\rm out}/e}$ (with ${\rm deg}_{\rm in}$, ${\rmdeg}_{\rm out}$ the average degrees inside and outside the community). We arguethat spectral algorithms are also ineffective in the latter regime. It is an open problem whether any polynomial time algorithms might succeedfor ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} < \sqrt{{\rm deg}_{\rm out}/e}$.
arxiv-11700-126 | Computational Implications of Reducing Data to Sufficient Statistics | http://arxiv.org/pdf/1409.3821v3.pdf | author:Andrea Montanari category:stat.CO cs.IT cs.LG math.IT published:2014-09-12 summary:Given a large dataset and an estimation task, it is common to pre-process thedata by reducing them to a set of sufficient statistics. This step is oftenregarded as straightforward and advantageous (in that it simplifies statisticalanalysis). I show that -on the contrary- reducing data to sufficient statisticscan change a computationally tractable estimation problem into an intractableone. I discuss connections with recent work in theoretical computer science,and implications for some techniques to estimate graphical models.
arxiv-11700-127 | Orthogonal parallel MCMC methods for sampling and optimization | http://arxiv.org/pdf/1507.08577v1.pdf | author:L. Martino, V. Elvira, D. Luengo, J. Corander, F. Louzada category:stat.CO stat.ML published:2015-07-30 summary:Monte Carlo (MC) methods are widely used in statistics, signal processing andmachine learning. A well-known class of MC methods are Markov Chain Monte Carlo(MCMC) algorithms. In order to foster better exploration of the state space,specially in high-dimensional applications, several schemes employing multipleparallel MCMC chains have been recently introduced. In this work, we describe anovel parallel interacting MCMC scheme, called orthogonal MCMC (O-MCMC), wherea set of vertical parallel MCMC chains share information using some horizontalMCMC techniques working on the entire population of current states. Morespecifically, the vertical chains are led by random-walk proposals, whereas thehorizontal MCMC techniques employ independent proposals, thus allowing anefficient combination of global exploration and local approximation. Theinteraction is contained in these horizontal iterations. Within the analysis ofdifferent implementations of O-MCMC, novel schemes for reducing the overallcomputational cost of parallel multiple try Metropolis (MTM) chains are alsopresented. Furthermore, a modified version of O-MCMC for optimization isprovided by considering parallel simulated annealing (SA) algorithms. Finally,we also discuss the application of O-MCMC in a big bata framework. Numericalresults show the advantages of the proposed sampling scheme in terms ofefficiency in the estimation, as well as robustness in terms of independencewith respect to initial values and the choice of the parameters.
arxiv-11700-128 | Multilayer Network of Language: a Unified Framework for Structural Analysis of Linguistic Subsystems | http://arxiv.org/pdf/1507.08539v1.pdf | author:Domagoj Margan, Ana MeÅ¡troviÄ, Sanda MartinÄiÄ-IpÅ¡iÄ category:cs.CL published:2015-07-30 summary:Recently, the focus of complex networks research has shifted from theanalysis of isolated properties of a system toward a more realistic modeling ofmultiple phenomena - multilayer networks. Motivated by the prosperity ofmultilayer approach in social, transport or trade systems, we propose theintroduction of multilayer networks for language. The multilayer network oflanguage is a unified framework for modeling linguistic subsystems and theirstructural properties enabling the exploration of their mutual interactions.Various aspects of natural language systems can be represented as complexnetworks, whose vertices depict linguistic units, while links model theirrelations. The multilayer network of language is defined by three aspects: thenetwork construction principle, the linguistic subsystem and the language ofinterest. More precisely, we construct a word-level (syntax, co-occurrence andits shuffled counterpart) and a subword level (syllables and graphemes) networklayers, from five variations of original text (in the modeled language). Theobtained results suggest that there are substantial differences between thenetworks structures of different language subsystems, which are hidden duringthe exploration of an isolated layer. The word-level layers share structuralproperties regardless of the language (e.g. Croatian or English), while thesyllabic subword level expresses more language dependent structural properties.The preserved weighted overlap quantifies the similarity of word-level layersin weighted and directed networks. Moreover, the analysis of motifs reveals aclose topological structure of the syntactic and syllabic layers for bothlanguages. The findings corroborate that the multilayer network framework is apowerful, consistent and systematic approach to model several linguisticsubsystems simultaneously and hence to provide a more unified view on language.
arxiv-11700-129 | Framework for learning agents in quantum environments | http://arxiv.org/pdf/1507.08482v1.pdf | author:Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel category:quant-ph cs.AI cs.LG published:2015-07-30 summary:In this paper we provide a broad framework for describing learning agents ingeneral quantum environments. We analyze the types of classically specifiedenvironments which allow for quantum enhancements in learning, by contrastingenvironments to quantum oracles. We show that whether or not quantumimprovements are at all possible depends on the internal structure of thequantum environment. If the environments are constructed and the internalstructure is appropriately chosen, or if the agent has limited capacities toinfluence the internal states of the environment, we show that improvements inlearning times are possible in a broad range of scenarios. Such scenarios wecall luck-favoring settings. The case of constructed environments isparticularly relevant for the class of model-based learning agents, where ourresults imply a near-generic improvement.
arxiv-11700-130 | Unsupervised Sentence Simplification Using Deep Semantics | http://arxiv.org/pdf/1507.08452v1.pdf | author:Shashi Narayan, Claire Gardent category:cs.CL published:2015-07-30 summary:We present a novel approach to sentence simplification which departs fromprevious work in two main ways. First, it requires neither hand written rulesnor a training corpus of aligned standard and simplified sentences. Second,sentence splitting operates on deep semantic structure. We show (i) that theunsupervised framework we propose is competitive with four state-of-the-artsupervised systems and (ii) that our semantic based approach allows for aprincipled and effective handling of sentence splitting.
arxiv-11700-131 | People Counting in High Density Crowds from Still Images | http://arxiv.org/pdf/1507.08445v1.pdf | author:Ankan Bansal, K. S. Venkatesh category:cs.CV published:2015-07-30 summary:We present a method of estimating the number of people in high density crowdsfrom still images. The method estimates counts by fusing information frommultiple sources. Most of the existing work on crowd counting deals with verysmall crowds (tens of individuals) and use temporal information from videos.Our method uses only still images to estimate the counts in high density images(hundreds to thousands of individuals). At this scale, we cannot rely on onlyone set of features for count estimation. We, therefore, use multiple sources,viz. interest points (SIFT), Fourier analysis, wavelet decomposition, GLCMfeatures and low confidence head detections, to estimate the counts. Each ofthese sources gives a separate estimate of the count along with confidences andother statistical measures which are then combined to obtain the finalestimate. We test our method on an existing dataset of fifty images containingover 64000 individuals. Further, we added another fifty annotated images ofcrowds and tested on the complete dataset of hundred images containing over87000 individuals. The counts per image range from 81 to 4633. We report theperformance in terms of mean absolute error, which is a measure of accuracy ofthe method, and mean normalised absolute error, which is a measure of therobustness.
arxiv-11700-132 | Multilinear Map Layer: Prediction Regularization by Structural Constraint | http://arxiv.org/pdf/1507.08429v1.pdf | author:Shuchang Zhou, Yuxin Wu category:cs.CV published:2015-07-30 summary:In this paper we propose and study a technique to impose structuralconstraints on the output of a neural network, which can reduce amount ofcomputation and number of parameters besides improving prediction accuracy whenthe output is known to approximately conform to the low-rankness prior. Thetechnique proceeds by replacing the output layer of neural network with theso-called MLM layers, which forces the output to be the result of someMultilinear Map, like a hybrid-Kronecker-dot product or Kronecker TensorProduct. In particular, given an "autoencoder" model trained on SVHN dataset,we can construct a new model with MLM layer achieving 62\% reduction in totalnumber of parameters and reduction of $\ell_2$ reconstruction error from 0.088to 0.004. Further experiments on other autoencoder model variants trained onSVHN datasets also demonstrate the efficacy of MLM layers.
arxiv-11700-133 | Tag-Weighted Topic Model For Large-scale Semi-Structured Documents | http://arxiv.org/pdf/1507.08396v1.pdf | author:Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, Rong Pan category:cs.CL cs.IR cs.LG stat.ML published:2015-07-30 summary:To date, there have been massive Semi-Structured Documents (SSDs) during theevolution of the Internet. These SSDs contain both unstructured features (e.g.,plain text) and metadata (e.g., tags). Most previous works focused on modelingthe unstructured text, and recently, some other methods have been proposed tomodel the unstructured text with specific tags. To build a general model forSSDs remains an important problem in terms of both model fitness andefficiency. We propose a novel method to model the SSDs by a so-calledTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both thetags and words information, not only to learn the document-topic and topic-worddistributions, but also to infer the tag-topic distributions for text miningtasks. We present an efficient variational inference method with an EMalgorithm for estimating the model parameters. Meanwhile, we propose threelarge-scale solutions for our model under the MapReduce distributed computingplatform for modeling large-scale SSDs. The experimental results show theeffectiveness, efficiency and the robustness by comparing our model with thestate-of-the-art methods in document modeling, tags prediction and textclassification. We also show the performance of the three distributed solutionsin terms of time and accuracy on document modeling.
arxiv-11700-134 | VMF-SNE: Embedding for Spherical Data | http://arxiv.org/pdf/1507.08379v1.pdf | author:Mian Wang, Dong Wang category:cs.LG published:2015-07-30 summary:T-SNE is a well-known approach to embedding high-dimensional data and hasbeen widely used in data visualization. The basic assumption of t-SNE is thatthe data are non-constrained in the Euclidean space and the local proximity canbe modelled by Gaussian distributions. This assumption does not hold for a widerange of data types in practical applications, for instance spherical data forwhich the local proximity is better modelled by the von Mises-Fisher (vMF)distribution instead of the Gaussian. This paper presents a vMF-SNE embeddingalgorithm to embed spherical data. An iterative process is derived to producean efficient embedding. The results on a simulation data set demonstrated thatvMF-SNE produces better embeddings than t-SNE for spherical data.
arxiv-11700-135 | When VLAD met Hilbert | http://arxiv.org/pdf/1507.08373v1.pdf | author:Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli category:cs.CV published:2015-07-30 summary:Vectors of Locally Aggregated Descriptors (VLAD) have emerged as powerfulimage/video representations that compete with or even outperformstate-of-the-art approaches on many challenging visual recognition tasks. Inthis paper, we address two fundamental limitations of VLAD: its requirement forthe local descriptors to have vector form and its restriction to linearclassifiers due to its high-dimensionality. To this end, we introduce akernelized version of VLAD. This not only lets us inherently exploit moresophisticated classification schemes, but also enables us to efficientlyaggregate non-vector descriptors (e.g., tensors) in the VLAD framework.Furthermore, we propose three approximate formulations that allow us toaccelerate the coding process while still benefiting from the properties ofkernel VLAD. Our experiments demonstrate the effectiveness of our approach athandling manifold-valued data, such as covariance descriptors, on severalclassification tasks. Our results also evidence the benefits of our nonlinearVLAD descriptors against the linear ones in Euclidean space using severalstandard benchmark datasets.
arxiv-11700-136 | Information-theoretical analysis of the statistical dependencies among three variables: Applications to written language | http://arxiv.org/pdf/1508.03530v1.pdf | author:DamiÃ¡n G. HernÃ¡ndez, DamiÃ¡n H. Zanette, InÃ©s Samengo category:cs.CL physics.soc-ph published:2015-07-30 summary:We develop the information-theoretical concepts required to study thestatistical dependencies among three variables. Some of such dependencies arepure triple interactions, in the sense that they cannot be explained in termsof a combination of pairwise correlations. We derive bounds for tripledependencies, and characterize the shape of the joint probability distributionof three binary variables with high triple interaction. The analysis alsoallows us to quantify the amount of redundancy in the mutual informationbetween pairs of variables, and to assess whether the information between twovariables is or is not mediated by a third variable. These concepts are appliedto the analysis of written texts. We find that the probability that a givenword is found in a particular location within the text is not only modulated bythe presence or absence of other nearby words, but also, on the presence orabsence of nearby pairs of words. We identify the words enclosing the keysemantic concepts of the text, the triplets of words with high pairwise andtriple interactions, and the words that mediate the pairwise interactionsbetween other words.
arxiv-11700-137 | Action recognition in still images by latent superpixel classification | http://arxiv.org/pdf/1507.08363v1.pdf | author:Shaukat Abidi, Massimo Piccardi, Mary-Anne Williams category:cs.CV published:2015-07-30 summary:Action recognition from still images is an important task of computer visionapplications such as image annotation, robotic navigation, video surveillanceand several others. Existing approaches mainly rely on either bag-of-featurerepresentations or articulated body-part models. However, the relationshipbetween the action and the image segments is still substantially unexplored.For this reason, in this paper we propose to approach action recognition byleveraging an intermediate layer of "superpixels" whose latent classes can actas attributes of the action. In the proposed approach, the action class ispredicted by a structural model(learnt by Latent Structural SVM) based onmeasurements from the image superpixels and their latent classes. Experimentalresults over the challenging Stanford 40 Actions dataset report a significantaverage accuracy of 74.06% for the positive class and 88.50% for the negativeclass, giving evidence to the performance of the proposed approach.
arxiv-11700-138 | Distributed Mini-Batch SDCA | http://arxiv.org/pdf/1507.08322v1.pdf | author:Martin TakÃ¡Ä, Peter RichtÃ¡rik, Nathan Srebro category:cs.LG math.OC published:2015-07-29 summary:We present an improved analysis of mini-batched stochastic dual coordinateascent for regularized empirical loss minimization (i.e. SVM and SVM-typeobjectives). Our analysis allows for flexible sampling schemes, including wheredata is distribute across machines, and combines a dependence on the smoothnessof the loss and/or the data spread (measured through the spectral norm).
arxiv-11700-139 | Deep Learning for Single-View Instance Recognition | http://arxiv.org/pdf/1507.08286v1.pdf | author:David Held, Sebastian Thrun, Silvio Savarese category:cs.CV cs.LG cs.NE cs.RO published:2015-07-29 summary:Deep learning methods have typically been trained on large datasets in whichmany training examples are available. However, many real-world product datasetshave only a small number of images available for each product. We explore theuse of deep learning methods for recognizing object instances when we have onlya single training example per class. We show that feedforward neural networksoutperform state-of-the-art methods for recognizing objects from novelviewpoints even when trained from just a single image per object. To furtherimprove our performance on this task, we propose to take advantage of asupplementary dataset in which we observe a separate set of objects frommultiple viewpoints. We introduce a new approach for training deep learningmethods for instance recognition with limited training data, in which we use anauxiliary multi-view dataset to train our network to be robust to viewpointchanges. We find that this approach leads to a more robust classifier forrecognizing objects from novel viewpoints, outperforming previousstate-of-the-art approaches including keypoint-matching, template-basedtechniques, and sparse coding.
arxiv-11700-140 | Context-aware learning for finite mixture models | http://arxiv.org/pdf/1507.08272v1.pdf | author:Serafeim Perdikis, Robert Leeb, Ricardo Chavarriaga, JosÃ© del R. MillÃ¡n category:stat.ML published:2015-07-29 summary:This work introduces algorithms able to exploit contextual information inorder to improve maximum-likelihood (ML) parameter estimation in finite mixturemodels (FMM), demonstrating their benefits and properties in several scenarios.The proposed algorithms are derived in a probabilistic framework with regard tosituations where the regular FMM graphs can be extended with context-relatedvariables, respecting the standard expectation-maximization (EM) methodologyand, thus, rendering explicit supervision completely redundant. We show that,by direct application of the missing information principle, the comparedalgorithms' learning behaviour operates between the extremities of supervisedand unsupervised learning, proportionally to the information content ofcontextual assistance. Our simulation results demonstrate the superiority ofcontext-aware FMM training as compared to conventional unsupervised training interms of estimation precision, standard errors, convergence rates andclassification accuracy or regression fitness in various scenarios, while alsohighlighting important differences among the outlined situations. Finally, theimproved classification outcome of contextually enhanced FMMs is showcased in abrain-computer interface application scenario.
arxiv-11700-141 | Zero-Shot Domain Adaptation via Kernel Regression on the Grassmannian | http://arxiv.org/pdf/1507.07830v2.pdf | author:Yongxin Yang, Timothy Hospedales category:cs.LG cs.CV published:2015-07-28 summary:Most visual recognition methods implicitly assume the data distributionremains unchanged from training to testing. However, in practice domain shiftoften exists, where real-world factors such as lighting and sensor type changebetween train and test, and classifiers do not generalise from source to targetdomains. It is impractical to train separate models for all possible situationsbecause collecting and labelling the data is expensive. Domain adaptationalgorithms aim to ameliorate domain shift, allowing a model trained on a sourceto perform well on a different target domain. However, even for the setting ofunsupervised domain adaptation, where the target domain is unlabelled,collecting data for every possible target domain is still costly. In thispaper, we propose a new domain adaptation method that has no need to accesseither data or labels of the target domain when it can be described by aparametrised vector and there exits several related source domains within thesame parametric space. It greatly reduces the burden of data collection andannotation, and our experiments show some promising results.
arxiv-11700-142 | The SYSU System for the Interspeech 2015 Automatic Speaker Verification Spoofing and Countermeasures Challenge | http://arxiv.org/pdf/1507.06711v2.pdf | author:Shitao Weng, Shushan Chen, Lei Yu, Xuewei Wu, Weicheng Cai, Zhi Liu, Ming Li category:cs.SD cs.CL published:2015-07-24 summary:Many existing speaker verification systems are reported to be vulnerableagainst different spoofing attacks, for example speaker-adapted speechsynthesis, voice conversion, play back, etc. In order to detect these spoofedspeech signals as a countermeasure, we propose a score level fusion approachwith several different i-vector subsystems. We show that the acoustic levelMel-frequency cepstral coefficients (MFCC) features, the phase level modifiedgroup delay cepstral coefficients (MGDCC) and the phonetic level phonemeposterior probability (PPP) tandem features are effective for thecountermeasure. Furthermore, feature level fusion of these features beforei-vector modeling also enhance the performance. A polynomial kernel supportvector machine is adopted as the supervised classifier. In order to enhance thegeneralizability of the countermeasure, we also adopted the cosine similarityand PLDA scoring as one-class classifications methods. By combining theproposed i-vector subsystems with the OpenSMILE baseline which covers theacoustic and prosodic information further improves the final performance. Theproposed fusion system achieves 0.29% and 3.26% EER on the development and testset of the database provided by the INTERSPEECH 2015 automatic speakerverification spoofing and countermeasures challenge.
arxiv-11700-143 | Layered Interpretation of Street View Images | http://arxiv.org/pdf/1506.04723v2.pdf | author:Ming-Yu Liu, Shuoxin Lin, Srikumar Ramalingam, Oncel Tuzel category:cs.CV published:2015-06-15 summary:We propose a layered street view model to encode both depth and semanticinformation on street view images for autonomous driving. Recently, stixels,stix-mantics, and tiered scene labeling methods have been proposed to modelstreet view images. We propose a 4-layer street view model, a compactrepresentation over the recently proposed stix-mantics model. Our layers encodesemantic classes like ground, pedestrians, vehicles, buildings, and sky inaddition to the depths. The only input to our algorithm is a pair of stereoimages. We use a deep neural network to extract the appearance features forsemantic classes. We use a simple and an efficient inference algorithm tojointly estimate both semantic classes and layered depth values. Our methodoutperforms other competing approaches in Daimler urban scene segmentationdataset. Our algorithm is massively parallelizable, allowing a GPUimplementation with a processing speed about 9 fps.
arxiv-11700-144 | IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family | http://arxiv.org/pdf/1507.08155v1.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.ME published:2015-07-29 summary:Previously, we proposed a physically-inspired method to construct data pointsinto an effective in-tree (IT) structure, in which the underlying clusterstructure in the dataset is well revealed. Although there are some edges in theIT structure requiring to be removed, such undesired edges are generallydistinguishable from other edges and thus are easy to be determined. Forinstance, when the IT structures for the 2-dimensional (2D) datasets aregraphically presented, those undesired edges can be easily spotted andinteractively determined. However, in practice, there are many datasets that donot lie in the 2D Euclidean space, thus their IT structures cannot begraphically presented. But if we can effectively map those IT structures into avisualized space in which the salient features of those undesired edges arepreserved, then the undesired edges in the IT structures can still be visuallydetermined in a visualization environment. Previously, this purpose was reachedby our method called IT-map. The outstanding advantage of IT-map is thatclusters can still be found even with the so-called crowding problem in theembedding. In this paper, we propose another method, called IT-Dendrogram, to achievethe same goal through an effective combination of the IT structure and thesingle link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogramcan also effectively represent the IT structures in a visualizationenvironment, whereas using another form, called the Dendrogram. IT-Dendrogramcan serve as another visualization method to determine the undesired edges inthe IT structures and thus benefit the IT-based clustering analysis. This wasdemonstrated on several datasets with different shapes, dimensions, andattributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,which could help users make more reliable cluster analysis in certain problems.
arxiv-11700-145 | Learning Representations for Outlier Detection on a Budget | http://arxiv.org/pdf/1507.08104v1.pdf | author:Barbora MicenkovÃ¡, Brian McWilliams, Ira Assent category:cs.LG published:2015-07-29 summary:The problem of detecting a small number of outliers in a large dataset is animportant task in many fields from fraud detection to high-energy physics. Twoapproaches have emerged to tackle this problem: unsupervised and supervised.Supervised approaches require a sufficient amount of labeled data and arechallenged by novel types of outliers and inherent class imbalance, whereasunsupervised methods do not take advantage of available labeled trainingexamples and often exhibit poorer predictive performance. We propose BORE (aBagged Outlier Representation Ensemble) which uses unsupervised outlier scoringfunctions (OSFs) as features in a supervised learning framework. BORE is ableto adapt to arbitrary OSF feature representations, to the imbalance in labeleddata as well as to prediction-time constraints on computational cost. Wedemonstrate the good performance of BORE compared to a variety of competingmethods in the non-budgeted and the budgeted outlier detection problem on 12real-world datasets.
arxiv-11700-146 | Cross-pose Face Recognition by Canonical Correlation Analysis | http://arxiv.org/pdf/1507.08076v1.pdf | author:Annan Li, Shiguang Shan, Xilin Chen, Bingpeng Ma, Shuicheng Yan, Wen Gao category:cs.CV published:2015-07-29 summary:The pose problem is one of the bottlenecks in automatic face recognition. Weargue that one of the diffculties in this problem is the severe misalignment inface images or feature vectors with different poses. In this paper, we proposethat this problem can be statistically solved or at least mitigated bymaximizing the intra-subject across-pose correlations via canonical correlationanalysis (CCA). In our method, based on the data set with coupled face imagesof the same identities and across two different poses, CCA learnssimultaneously two linear transforms, each for one pose. In the transformedsubspace, the intra-subject correlations between the different poses aremaximized, which implies pose-invariance or pose-robustness is achieved. Theexperimental results show that our approach could considerably improve therecognition performance. And if further enhanced with holistic+local featurerepresentation, the performance could be comparable to the state-of-the-art.
arxiv-11700-147 | STC Anti-spoofing Systems for the ASVspoof 2015 Challenge | http://arxiv.org/pdf/1507.08074v1.pdf | author:Sergey Novoselov, Alexandr Kozlov, Galina Lavrentyeva, Konstantin Simonchik, Vadim Shchemelinin category:cs.SD cs.LG stat.ML published:2015-07-29 summary:This paper presents the Speech Technology Center (STC) systems submitted toAutomatic Speaker Verification Spoofing and Countermeasures (ASVspoof)Challenge 2015. In this work we investigate different acoustic feature spacesto determine reliable and robust countermeasures against spoofing attacks. Inaddition to the commonly used front-end MFCC features we explored featuresderived from phase spectrum and features based on applying the multiresolutionwavelet transform. Similar to state-of-the-art ASV systems, we used thestandard TV-JFA approach for probability modelling in spoofing detectionsystems. Experiments performed on the development and evaluation datasets ofthe Challenge demonstrate that the use of phase-related and wavelet-basedfeatures provides a substantial input into the efficiency of the resulting STCsystems. In our research we also focused on the comparison of the linear (SVM)and nonlinear (DBN) classifiers.
arxiv-11700-148 | Preprint Extending Touch-less Interaction on Vision Based Wearable Device | http://arxiv.org/pdf/1504.01025v2.pdf | author:Zhihan Lv, Liangbing Feng, Shengzhong Feng, Haibo Li category:cs.HC cs.CV cs.GR H.1.2; H.5.1 published:2015-04-04 summary:This is the preprint version of our paper on IEEE Virtual Reality Conference2015. A touch-less interaction technology on vision based wearable device isdesigned and evaluated. Users interact with the application with dynamichands/feet gestures in front of the camera. Several proof-of-concept prototypeswith eleven dynamic gestures are developed based on the touch-less interaction.At last, a comparing user study evaluation is proposed to demonstrate theusability of the touch-less approach, as well as the impact on user's emotion,running on a wearable framework or Google Glass.
arxiv-11700-149 | Collaborative Representation Classification Ensemble for Face Recognition | http://arxiv.org/pdf/1507.08064v1.pdf | author:Xiaochao Qu, Suah Kim, Run Cui, Hyoung Joong Kim category:cs.CV published:2015-07-29 summary:Collaborative Representation Classification (CRC) for face recognitionattracts a lot attention recently due to its good recognition performance andfast speed. Compared to Sparse Representation Classification (SRC), CRCachieves a comparable recognition performance with 10-1000 times faster speed.In this paper, we propose to ensemble several CRC models to promote therecognition rate, where each CRC model uses different and divergent randomlygenerated biologically-inspired features as the face representation. Theproposed ensemble algorithm calculates an ensemble weight for each CRC modelthat guided by the underlying classification rule of CRC. The obtained weightsreflect the confidences of those CRC models where the more confident CRC modelshave larger weights. The proposed weighted ensemble method proves to be veryeffective and improves the performance of each CRC model significantly.Extensive experiments are conducted to show the superior performance of theproposed method.
arxiv-11700-150 | Adapted sampling for 3D X-ray computed tomography | http://arxiv.org/pdf/1507.08030v1.pdf | author:Anthony Cazasnoves, Fanny Buyens, Sylvie Sevestre category:cs.CV published:2015-07-29 summary:In this paper, we introduce a method to build an adapted mesh representationof a 3D object for X-Ray tomography reconstruction. Using this representation,we provide means to reduce the computational cost of reconstruction by way ofiterative algorithms. The adapted sampling of the reconstruction space isdirectly obtained from the projection dataset and prior to any reconstruction.It is built following two stages : firstly, 2D structural information isextracted from the projection images and is secondly merged in 3D to obtain a3D pointcloud sampling the interfaces of the object. A relevant mesh is thenbuilt from this cloud by way of tetrahedralization. Critical parametersselections have been automatized through a statistical framework, thus avoidingdependence on users expertise. Applying this approach on geometrical shapes andon a 3D Shepp-Logan phantom, we show the relevance of such a sampling -obtained in a few seconds - and the drastic decrease in cells number to beestimated during reconstruction when compared to the usual regular voxellattice. A first iterative reconstruction of the Shepp-Logan using this kind ofsampling shows the relevant advantages in terms of low dose or sparseacquisition sampling contexts. The method can also prove useful for otherapplications such as finite element method computations.
arxiv-11700-151 | Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks | http://arxiv.org/pdf/1502.06108v3.pdf | author:Xiao Lin, Devi Parikh category:cs.CV published:2015-02-21 summary:Artificial agents today can answer factual questions. But they fall short onquestions that require common sense reasoning. Perhaps this is because mostexisting common sense databases rely on text to learn and represent knowledge.But much of common sense knowledge is unwritten - partly because it tends notto be interesting enough to talk about, and partly because some common sense isunnatural to articulate in text. While unwritten, it is not unseen. In thispaper we leverage semantic common sense knowledge learned from images - i.e.visual common sense - in two textual tasks: fill-in-the-blank and visualparaphrasing. We propose to "imagine" the scene behind the text, and leveragevisual cues from the "imagined" scenes in addition to textual cues whileanswering these questions. We imagine the scenes as a visual abstraction. Ourapproach outperforms a strong text-only baseline on these tasks. Our proposedtasks can serve as benchmarks to quantitatively evaluate progress in solvingtasks that go "beyond recognition". Our code and datasets are publiclyavailable.
arxiv-11700-152 | Unification of field theory and maximum entropy methods for learning probability densities | http://arxiv.org/pdf/1411.5371v5.pdf | author:Justin B. Kinney category:cs.LG q-bio.QM stat.ML published:2014-11-19 summary:The need to estimate smooth probability distributions (a.k.a. probabilitydensities) from finite sampled data is ubiquitous in science. Many approachesto this problem have been described, but none is yet regarded as providing adefinitive solution. Maximum entropy estimation and Bayesian field theory aretwo such approaches. Both have origins in statistical physics, but therelationship between them has remained unclear. Here I unify these two methodsby showing that every maximum entropy density estimate can be recovered in theinfinite smoothness limit of an appropriate Bayesian field theory. I also showthat Bayesian field theory estimation can be performed without imposing anyboundary conditions on candidate densities, and that the infinite smoothnesslimit of these theories recovers the most common types of maximum entropyestimates. Bayesian field theory is thus seen to provide a natural test of thevalidity of the maximum entropy null hypothesis. Bayesian field theory alsoreturns a lower entropy density estimate when the maximum entropy hypothesis isfalsified. The computations necessary for this approach can be performedrapidly for one-dimensional data, and software for doing this is provided.Based on these results, I argue that Bayesian field theory is poised to providea definitive solution to the density estimation problem in one dimension.
arxiv-11700-153 | On Proportions of Fit Individuals in Population of Genetic Algorithm with Tournament Selection | http://arxiv.org/pdf/1507.08007v1.pdf | author:Anton Eremeev category:cs.NE published:2015-07-29 summary:In this paper, we consider a fitness-level model of a non-elitistmutation-only genetic algorithm (GA) with tournament selection. The modelprovides upper and lower bounds for the expected proportion of the individualswith fitness above given thresholds. In the case of GA with bitwise mutationand OneMax fitness function, the lower bounds are tight when population sizeequals one, while the upper bounds are asymptotically tight when populationsize tends to infinity. The lower bounds on expected proportions of sufficiently fit individuals maybe obtained from the probability distribution of an appropriate Markov chain.This approach yields polynomial upper bounds on the runtime of an Iteratedversion of the GA on 2-SAT problem and on a family of Set Cover problemsproposed by E. Balas.
arxiv-11700-154 | On the Performance of ConvNet Features for Place Recognition | http://arxiv.org/pdf/1501.04158v3.pdf | author:Niko SÃ¼nderhauf, Feras Dayoub, Sareh Shirazi, Ben Upcroft, Michael Milford category:cs.RO cs.CV published:2015-01-17 summary:After the incredible success of deep learning in the computer vision domain,there has been much interest in applying Convolutional Network (ConvNet)features in robotic fields such as visual navigation and SLAM. Unfortunately,there are fundamental differences and challenges involved. Computer visiondatasets are very different in character to robotic camera data, real-timeperformance is essential, and performance priorities can be different. Thispaper comprehensively evaluates and compares the utility of threestate-of-the-art ConvNets on the problems of particular relevance to navigationfor robots; viewpoint-invariance and condition-invariance, and for the firsttime enables real-time place recognition performance using ConvNets with largemaps by integrating a variety of existing (locality-sensitive hashing) andnovel (semantic search space partitioning) optimization techniques. We presentextensive experiments on four real world datasets cultivated to evaluate eachof the specific challenges in place recognition. The results demonstrate thatspeed-ups of two orders of magnitude can be achieved with minimal accuracydegradation, enabling real-time performance. We confirm that networks trainedfor semantic place categorization also perform better at (specific) placerecognition when faced with severe appearance changes and provide a referencefor which networks and layers are optimal for different aspects of the placerecognition problem.
arxiv-11700-155 | Identifying missing dictionary entries with frequency-conserving context models | http://arxiv.org/pdf/1503.02120v3.pdf | author:Jake Ryland Williams, Eric M. Clark, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL cs.IT math.IT stat.ML published:2015-03-07 summary:In an effort to better understand meaning from natural language texts, weexplore methods aimed at organizing lexical objects into contexts. A number ofthese methods for organization fall into a family defined by word ordering.Unlike demographic or spatial partitions of data, these collocation models areof special importance for their universal applicability. While we areinterested here in text and have framed our treatment appropriately, our workis potentially applicable to other areas of research (e.g., speech, genomics,and mobility patterns) where one has ordered categorical data, (e.g., sounds,genes, and locations). Our approach focuses on the phrase (whether word orlarger) as the primary meaning-bearing lexical unit and object of study. To doso, we employ our previously developed framework for generating word-conservingphrase-frequency data. Upon training our model with the Wiktionary---anextensive, online, collaborative, and open-source dictionary that contains over100,000 phrasal-definitions---we develop highly effective filters for theidentification of meaningful, missing phrase-entries. With our predictions wethen engage the editorial community of the Wiktionary and propose short listsof potential missing entries for definition, developing a breakthrough, lexicalextraction technique, and expanding our knowledge of the defined Englishlexicon of phrases.
arxiv-11700-156 | Document Embedding with Paragraph Vectors | http://arxiv.org/pdf/1507.07998v1.pdf | author:Andrew M. Dai, Christopher Olah, Quoc V. Le category:cs.CL cs.AI cs.LG published:2015-07-29 summary:Paragraph Vectors has been recently proposed as an unsupervised method forlearning distributed representations for pieces of texts. In their work, theauthors showed that the method can learn an embedding of movie review textswhich can be leveraged for sentiment analysis. That proof of concept, whileencouraging, was rather narrow. Here we consider tasks other than sentimentanalysis, provide a more thorough comparison of Paragraph Vectors to otherdocument modelling algorithms such as Latent Dirichlet Allocation, and evaluateperformance of the method as we vary the dimensionality of the learnedrepresentation. We benchmarked the models on two document similarity data sets,one from Wikipedia, one from arXiv. We observe that the Paragraph Vector methodperforms significantly better than other methods, and propose a simpleimprovement to enhance embedding quality. Somewhat surprisingly, we also showthat much like word embeddings, vector operations on Paragraph Vectors canperform useful semantic results.
arxiv-11700-157 | A constrained optimization perspective on actor critic algorithms and application to network routing | http://arxiv.org/pdf/1507.07984v1.pdf | author:Prashanth L. A., H. L. Prasad, Shalabh Bhatnagar, Prakash Chandra category:cs.LG math.OC published:2015-07-28 summary:We propose a novel actor-critic algorithm with guaranteed convergence to anoptimal policy for a discounted reward Markov decision process. The actorincorporates a descent direction that is motivated by the solution of a certainnon-linear optimization problem. We also discuss an extension to incorporatefunction approximation and demonstrate the practicality of our algorithms on anetwork routing application.
arxiv-11700-158 | An algorithm for online tensor prediction | http://arxiv.org/pdf/1507.07974v1.pdf | author:John Pothier, Josh Girson, Shuchin Aeron category:stat.ML cs.IT cs.LG math.IT published:2015-07-28 summary:We present a new method for online prediction and learning of tensors($N$-way arrays, $N >2$) from sequential measurements. We focus on the specificcase of 3-D tensors and exploit a recently developed framework of structuredtensor decompositions proposed in [1]. In this framework it is possible totreat 3-D tensors as linear operators and appropriately generalize notions ofrank and positive definiteness to tensors in a natural way. Using these notionswe propose a generalization of the matrix exponentiated gradient descentalgorithm [2] to a tensor exponentiated gradient descent algorithm using anextension of the notion of von-Neumann divergence to tensors. Then following asimilar construction as in [3], we exploit this algorithm to propose an onlinealgorithm for learning and prediction of tensors with provable regretguarantees. Simulations results are presented on semi-synthetic data sets ofratings evolving in time under local influence over a social network. Theresult indicate superior performance compared to other (online) convex tensorcompletion methods.
arxiv-11700-159 | Face Search at Scale: 80 Million Gallery | http://arxiv.org/pdf/1507.07242v2.pdf | author:Dayong Wang, Charles Otto, Anil K. Jain category:cs.CV published:2015-07-26 summary:Due to the prevalence of social media websites, one challenge facing computervision researchers is to devise methods to process and search for persons ofinterest among the billions of shared photos on these websites. Facebookrevealed in a 2013 white paper that its users have uploaded more than 250billion photos, and are uploading 350 million new photos each day. Due to thishumongous amount of data, large-scale face search for mining web images is bothimportant and challenging. Despite significant progress in face recognition,searching a large collection of unconstrained face images has not beenadequately addressed. To address this challenge, we propose a face searchsystem which combines a fast search procedure, coupled with a state-of-the-artcommercial off the shelf (COTS) matcher, in a cascaded framework. Given a probeface, we first filter the large gallery of photos to find the top-k mostsimilar faces using deep features generated from a convolutional neuralnetwork. The k candidates are re-ranked by combining similarities from deepfeatures and the COTS matcher. We evaluate the proposed face search system on agallery containing 80 million web-downloaded face images. Experimental resultsdemonstrate that the deep features are competitive with state-of-the-artmethods on unconstrained face recognition benchmarks (LFW and IJB-A). Further,the proposed face search system offers an excellent trade-off between accuracyand scalability on datasets consisting of millions of images. Additionally, inan experiment involving searching for face images of the Tsarnaev brothers,convicted of the Boston Marathon bombing, the proposed face search system couldfind the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a5M gallery and at rank 8 in 7 seconds on an 80M gallery.
arxiv-11700-160 | Sparse Multidimensional Patient Modeling using Auxiliary Confidence Labels | http://arxiv.org/pdf/1507.07955v1.pdf | author:Eric Heim, Milos Hauskrecht category:cs.LG published:2015-07-28 summary:In this work, we focus on the problem of learning a classification model thatperforms inference on patient Electronic Health Records (EHRs). Often, a largeamount of costly expert supervision is required to learn such a model. Toreduce this cost, we obtain confidence labels that indicate how sure an expertis in the class labels she provides. If meaningful confidence information canbe incorporated into a learning method, fewer patient instances may need to belabeled to learn an accurate model. In addition, while accuracy of predictionsis important for any inference model, a model of patients must be interpretableso that clinicians can understand how the model is making decisions. To theseends, we develop a novel metric learning method called Confidence bAsed MEtricLearning (CAMEL) that supports inclusion of confidence labels, but alsoemphasizes interpretability in three ways. First, our method induces sparsity,thus producing simple models that use only a few features from patient EHRs.Second, CAMEL naturally produces confidence scores that can be taken intoconsideration when clinicians make treatment decisions. Third, the metricslearned by CAMEL induce multidimensional spaces where each dimension representsa different "factor" that clinicians can use to assess patients. In ourexperimental evaluation, we show on a real-world clinical data set that ourCAMEL methods are able to learn models that are as or more accurate as othermethods that use the same supervision. Furthermore, we show that when CAMELuses confidence scores it is able to learn models as or more accurate as otherswe tested while using only 10% of the training instances. Finally, we performqualitative assessments on the metrics learned by CAMEL and show that theyidentify and clearly articulate important factors in how the model performsinference.
arxiv-11700-161 | Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm | http://arxiv.org/pdf/1503.08329v2.pdf | author:Pascal Germain, Alexandre Lacasse, FranÃ§ois Laviolette, Mario Marchand, Jean-Francis Roy category:stat.ML cs.LG published:2015-03-28 summary:We propose an extensive analysis of the behavior of majority votes in binaryclassification. In particular, we introduce a risk bound for majority votes,called the C-bound, that takes into account the average quality of the votersand their average disagreement. We also propose an extensive PAC-Bayesiananalysis that shows how the C-bound can be estimated from various observationscontained in the training data. The analysis intends to be self-contained andcan be used as introductory material to PAC-Bayesian statistical learningtheory. It starts from a general PAC-Bayesian perspective and ends withuncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leiblerdivergence and others allow kernel functions to be used as voters (via thesample compression setting). Finally, out of the analysis, we propose the MinCqlearning algorithm that basically minimizes the C-bound. MinCq reduces to asimple quadratic program. Aside from being theoretically grounded, MinCqachieves state-of-the-art performance, as shown in our extensive empiricalcomparison with both AdaBoost and the Support Vector Machine.
arxiv-11700-162 | Classifying informative and imaginative prose using complex networks | http://arxiv.org/pdf/1507.07826v1.pdf | author:Henrique F. de Arruda, Luciano da F. Costa, Diego R. Amancio category:cs.CL published:2015-07-28 summary:Statistical methods have been widely employed in recent years to grasp manylanguage properties. The application of such techniques have allowed animprovement of several linguistic applications, which encompasses machinetranslation, automatic summarization and document classification. In thelatter, many approaches have emphasized the semantical content of texts, as itis the case of bag-of-word language models. This approach has certainly yieldedreasonable performance. However, some potential features such as the structuralorganization of texts have been used only on a few studies. In this context, weprobe how features derived from textual structure analysis can be effectivelyemployed in a classification task. More specifically, we performed a supervisedclassification aiming at discriminating informative from imaginative documents.Using a networked model that describes the local topological/dynamicalproperties of function words, we achieved an accuracy rate of up to 95%, whichis much higher than similar networked approaches. A systematic analysis offeature relevance revealed that symmetry and accessibility measurements areamong the most prominent network measurements. Our results suggest that thesemeasurements could be used in related language applications, as they play acomplementary role in characterizing texts.
arxiv-11700-163 | Comparing the writing style of real and artificial papers | http://arxiv.org/pdf/1506.05702v2.pdf | author:Diego R. Amancio category:cs.CL published:2015-06-18 summary:Recent years have witnessed the increase of competition in science. Whilepromoting the quality of research in many cases, an intense competition amongscientists can also trigger unethical scientific behaviors. To increase thetotal number of published papers, some authors even resort to software toolsthat are able to produce grammatical, but meaningless scientific manuscripts.Because automatically generated papers can be misunderstood as real papers, itbecomes of paramount importance to develop means to identify these scientificfrauds. In this paper, I devise a methodology to distinguish real manuscriptsfrom those generated with SCIGen, an automatic paper generator. Upon modelingtexts as complex networks (CN), it was possible to discriminate real from fakepapers with at least 89\% of accuracy. A systematic analysis of featuresrelevance revealed that the accessibility and betweenness were useful inparticular cases, even though the relevance depended upon the dataset. Thesuccessful application of the methods described here show, as a proof ofprinciple, that network features can be used to identify scientific gibberishpapers. In addition, the CN-based approach can be combined in a straightforwardfashion with traditional statistical language processing methods to improve theperformance in identifying artificially generated papers.
arxiv-11700-164 | A Multi-Camera Image Processing and Visualization System for Train Safety Assessment | http://arxiv.org/pdf/1507.07815v1.pdf | author:Giuseppe Lisanti, Svebor Karaman, Daniele Pezzatini, Alberto Del Bimbo category:cs.CV published:2015-07-28 summary:In this paper we present a machine vision system to efficiently monitor,analyze and present visual data acquired with a railway overhead gantryequipped with multiple cameras. This solution aims to improve the safety ofdaily life railway transportation in a two- fold manner: (1) by providingautomatic algorithms that can process large imagery of trains (2) by helpingtrain operators to keep attention on any possible malfunction. The system isdesigned with the latest cutting edge, high-rate visible and thermal camerasthat ob- serve a train passing under an railway overhead gantry. The machinevision system is composed of three principal modules: (1) an automatic wagonidentification system, recognizing the wagon ID according to the UICclassification of railway coaches; (2) a temperature monitoring system; (3) asystem for the detection, localization and visualization of the pantograph ofthe train. These three machine vision modules process batch trains sequencesand their resulting analysis are presented to an operator using a multitouchuser interface. We detail all technical aspects of our multi-camera portal: thehardware requirements, the software developed to deal with the high-frame ratecameras and ensure reliable acquisition, the algorithms proposed to solve eachcomputer vision task, and the multitouch interaction and visualizationinterface. We evaluate each component of our system on a dataset recorded in anad-hoc railway test-bed, showing the potential of our proposed portal for trainsafety assessment.
arxiv-11700-165 | An Analytically Tractable Bayesian Approximation to Optimal Point Process Filtering | http://arxiv.org/pdf/1507.07813v1.pdf | author:Yuval Harel, Ron Meir, Manfred Opper category:stat.ML q-bio.NC published:2015-07-28 summary:The process of dynamic state estimation (filtering) based on point processobservations is in general intractable. Numerical sampling techniques are oftenpractically useful, but lead to limited conceptual insight about optimalencoding/decoding strategies, which are of significant relevance toComputational Neuroscience. We develop an analytically tractable Bayesianapproximation to optimal filtering based on point process observations, whichallows us to introduce distributional assumptions about sensory cellproperties, that greatly facilitates the analysis of optimal encoding insituations deviating from common assumptions of uniform coding. The analyticframework leads to insights which are difficult to obtain from numericalalgorithms, and is consistent with experiments about the distribution of tuningcurve centers. Interestingly, we find that the information gained from theabsence of spikes may be crucial to performance.
arxiv-11700-166 | Application of Independent Component Analysis Techniques in Speckle Noise Reduction of Retinal OCT Images | http://arxiv.org/pdf/1502.05742v3.pdf | author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV published:2015-02-19 summary:Optical Coherence Tomography (OCT) is an emerging technique in the field ofbiomedical imaging, with applications in ophthalmology, dermatology, coronaryimaging etc. OCT images usually suffer from a granular pattern, called specklenoise, which restricts the process of interpretation. Therefore the need forspeckle noise reduction techniques is of high importance. To the best of ourknowledge, use of Independent Component Analysis (ICA) techniques has neverbeen explored for speckle reduction of OCT images. Here, a comparative study ofseveral ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noisereduction of retinal OCT images. Having multiple B-scans of the same location,the eye movements are compensated using a rigid registration technique. Then,different ICA techniques are applied to the aggregated set of B-scans forextracting the noise-free image. Signal-to-Noise-Ratio (SNR),Contrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well asanalysis on the computational complexity of the methods, are considered asmetrics for comparison. The results show that use of ICA can be beneficial,especially in case of having fewer number of B-scans.
arxiv-11700-167 | SynapCountJ --- a Tool for Analyzing Synaptic Densities in Neurons | http://arxiv.org/pdf/1507.07800v1.pdf | author:Gadea Mata, JÃ³nathan Heras, Miguel Morales, Ana Romero, Julio Rubio category:cs.CV q-bio.NC published:2015-07-28 summary:The quantification of synapses is instrumental to measure the evolution ofsynaptic densities of neurons under the effect of some physiologicalconditions, neuronal diseases or even drug treatments. However, the manualquantification of synapses is a tedious, error-prone, time-consuming andsubjective task; therefore, tools that might automate this process aredesirable. In this paper, we present SynapCountJ, an ImageJ plugin, that canmeasure synaptic density of individual neurons obtained by immunofluorescencetechniques, and also can be applied for batch processing of neurons that havebeen obtained in the same experiment or using the same setting. The procedureto quantify synapses implemented in SynapCountJ is based on the colocalizationof three images of the same neuron (the neuron marked with two antibody markersand the structure of the neuron) and is inspired by methods coming fromComputational Algebraic Topology. SynapCountJ provides a procedure tosemi-automatically quantify the number of synapses of neuron cultures; as aresult, the time required for such an analysis is greatly reduced.
arxiv-11700-168 | Fast Segmentation of Left Ventricle in CT Images by Explicit Shape Regression using Random Pixel Difference Features | http://arxiv.org/pdf/1507.07508v2.pdf | author:Peng Sun, Haoyin Zhou, Devon Lundine, James K. Min, Guanglei Xiong category:cs.CV published:2015-07-27 summary:Recently, machine learning has been successfully applied to model-based leftventricle (LV) segmentation. The general framework involves two stages, whichstarts with LV localization and is followed by boundary delineation. Both aredriven by supervised learning techniques. When compared to previousnon-learning-based methods, several advantages have been shown, including fullautomation and improved accuracy. However, the speed is still slow, in theorder of several seconds, for applications involving a large number of cases orcase loads requiring real-time performance. In this paper, we propose a fast LVsegmentation algorithm by joint localization and boundary delineation viatraining explicit shape regressor with random pixel difference features. Testedon 3D cardiac computed tomography (CT) image volumes, the average running timeof the proposed algorithm is 1.2 milliseconds per case. On a dataset consistingof 139 CT volumes, a 5-fold cross validation shows the segmentation error is$1.21 \pm 0.11$ for LV endocardium and $1.23 \pm 0.11$ millimeters forepicardium. Compared with previous work, the proposed method is more stable(lower standard deviation) without significant compromise to the accuracy.
arxiv-11700-169 | A Hyperelastic Two-Scale Optimization Model for Shape Matching | http://arxiv.org/pdf/1507.07760v1.pdf | author:Konrad Simon, Sameer Sheorey, David Jacobs, Ronen Basri category:cs.CG cs.CV cs.GR published:2015-07-28 summary:We suggest a novel shape matching algorithm for three-dimensional surfacemeshes of disk or sphere topology. The method is based on the physical theoryof nonlinear elasticity and can hence handle large rotations and deformations.Deformation boundary conditions that supplement the underlying equations areusually unknown. Given an initial guess, these are optimized such that themechanical boundary forces that are responsible for the deformation are of asimple nature. We show a heuristic way to approximate the nonlinearoptimization problem by a sequence of convex problems using finite elements.The deformation cost, i.e, the forces, is measured on a coarse scale whileICP-like matching is done on the fine scale. We demonstrate the plausibility ofour algorithm on examples taken from different datasets.
arxiv-11700-170 | Detection of Epigenomic Network Community Oncomarkers | http://arxiv.org/pdf/1506.05244v2.pdf | author:Thomas E. Bartlett, Alexey Zaikin category:stat.AP q-bio.GN q-bio.MN stat.ML published:2015-06-17 summary:In this paper we propose network methodology to infer prognostic cancerbiomarkers, based on the epigenetic pattern DNA methylation. Epigeneticprocesses such as DNA methylation reflect environmental risk factors, and areincreasingly recognised for their fundamental role in diseases such as cancer.DNA methylation is a gene-regulatory pattern, and hence provides a means bywhich to assess genomic regulatory interactions. Network models are a naturalway to represent and analyse groups of such interactions. The utility ofnetwork models also increases as the quantity of data and number of variablesincrease, making them increasingly relevant to large-scale genomic studies. Wepropose methodology to infer prognostic genomic networks from a DNAmethylation-based measure of genomic interaction and association. We then showhow to identify prognostic biomarkers from such networks, which we term`network community oncomarkers'. We illustrate the power of our proposedmethodology in the context of a large publicly available breast cancerdata-set.
arxiv-11700-171 | Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds | http://arxiv.org/pdf/1507.07636v1.pdf | author:Sridhar Mahadevan, Sarath Chandar category:cs.CL published:2015-07-28 summary:Recent work has explored methods for learning continuous vector space wordrepresentations reflecting the underlying semantics of words. Simple vectorspace arithmetic using cosine distances has been shown to capture certain typesof analogies, such as reasoning about plurals from singulars, past tense frompresent tense, etc. In this paper, we introduce a new approach to captureanalogies in continuous word representations, based on modeling not justindividual word vectors, but rather the subspaces spanned by groups of words.We exploit the property that the set of subspaces in n-dimensional Euclideanspace form a curved manifold space called the Grassmannian, a quotient subgroupof the Lie group of rotations in n- dimensions. Based on this mathematicalmodel, we develop a modified cosine distance model based on geodesic kernelsthat captures relation-specific distances across word categories. Ourexperiments on analogy tasks show that our approach performs significantlybetter than the previous approaches for the given task.
arxiv-11700-172 | Complete stability analysis of a heuristic ADP control design | http://arxiv.org/pdf/1308.3282v2.pdf | author:Yury Sokolov, Robert Kozma, Ludmilla D. Werbos, Paul J. Werbos category:cs.NE cs.SY published:2013-08-15 summary:This paper provides new stability results for Action-Dependent HeuristicDynamic Programming (ADHDP), using a control algorithm that iterativelyimproves an internal model of the external world in the autonomous system basedon its continuous interaction with the environment. We extend previous resultsby ADHDP control to the case of general multi-layer neural networks with deeplearning across all layers. In particular, we show that the introduced controlapproach is uniformly ultimately bounded (UUB) under specific conditions on thelearning rates, without explicit constraints on the temporal discount factor.We demonstrate the benefit of our results to the control of linear andnonlinear systems, including the cart-pole balancing problem. Our results showsignificantly improved learning and control performance as compared to thestate-of-art.
arxiv-11700-173 | Variational Inference for Gaussian Process Modulated Poisson Processes | http://arxiv.org/pdf/1411.0254v3.pdf | author:Chris Lloyd, Tom Gunter, Michael A. Osborne, Stephen J. Roberts category:stat.ML published:2014-11-02 summary:We present the first fully variational Bayesian inference scheme forcontinuous Gaussian-process-modulated Poisson processes. Such point processesare used in a variety of domains, including neuroscience, geo-statistics andastronomy, but their use is hindered by the computational cost of existinginference schemes. Our scheme: requires no discretisation of the domain; scaleslinearly in the number of observed events; and is many orders of magnitudefaster than previous sampling based approaches. The resulting algorithm isshown to outperform standard methods on synthetic examples, coal miningdisaster data and in the prediction of Malaria incidences in Kenya.
arxiv-11700-174 | Online Censoring for Large-Scale Regressions with Application to Streaming Big Data | http://arxiv.org/pdf/1507.07536v1.pdf | author:Dimitris Berberidis, Vassilis Kekatos, Georgios B. Giannakis category:stat.AP stat.ML published:2015-07-27 summary:Linear regression is arguably the most prominent among statistical inferencemethods, popular both for its simplicity as well as its broad applicability. Onpar with data-intensive applications, the sheer size of linear regressionproblems creates an ever growing demand for quick and cost efficient solvers.Fortunately, a significant percentage of the data accrued can be omitted whilemaintaining a certain quality of statistical inference with an affordablecomputational budget. The present paper introduces means of identifying andomitting "less informative" observations in an online and data-adaptivefashion, built on principles of stochastic approximation and data censoring.First- and second-order stochastic approximation maximum likelihood-basedalgorithms for censored observations are developed for estimating theregression coefficients. Online algorithms are also put forth to reduce theoverall complexity by adaptively performing censoring along with estimation.The novel algorithms entail simple closed-form updates, and have provable(non)asymptotic convergence guarantees. Furthermore, specific rules areinvestigated for tuning to desired censoring patterns and levels ofdimensionality reduction. Simulated tests on real and synthetic datasetscorroborate the efficacy of the proposed data-adaptive methods compared todata-agnostic random projection-based alternatives.
arxiv-11700-175 | Occlusion-Aware Object Localization, Segmentation and Pose Estimation | http://arxiv.org/pdf/1507.07882v1.pdf | author:Samarth Brahmbhatt, Heni Ben Amor, Henrik Christensen category:cs.CV published:2015-07-27 summary:We present a learning approach for localization and segmentation of objectsin an image in a manner that is robust to partial occlusion. Our algorithmproduces a bounding box around the full extent of the object and labels pixelsin the interior that belong to the object. Like existing segmentation awaredetection approaches, we learn an appearance model of the object and considerregions that do not fit this model as potential occlusions. However, inaddition to the established use of pairwise potentials for encouraging localconsistency, we use higher order potentials which capture information at thelevel of im- age segments. We also propose an efficient loss function thattargets both localization and segmentation performance. Our algorithm achieves13.52% segmentation error and 0.81 area under the false-positive per image vs.recall curve on average over the challenging CMU Kitchen Occlusion Dataset.This is a 42.44% decrease in segmentation error and a 16.13% increase inlocalization performance compared to the state-of-the-art. Finally, we showthat the visibility labelling produced by our algorithm can make full 3D poseestimation from a single image robust to occlusion.
arxiv-11700-176 | AMP: a new time-frequency feature extraction method for intermittent time-series data | http://arxiv.org/pdf/1507.05455v2.pdf | author:Duncan Barrack, James Goulding, Keith Hopcraft, Simon Preston, Gavin Smith category:cs.LG G.3 published:2015-07-20 summary:The characterisation of time-series data via their most salient features isextremely important in a range of machine learning task, not least of all withregards to classification and clustering. While there exist many featureextraction techniques suitable for non-intermittent time-series data, theseapproaches are not always appropriate for intermittent time-series data, whereintermittency is characterized by constant values for large periods of timepunctuated by sharp and transient increases or decreases in value. Motivated by this, we present aggregation, mode decomposition and projection(AMP) a feature extraction technique particularly suited to intermittenttime-series data which contain time-frequency patterns. For our method allindividual time-series within a set are combined to form a non-intermittentaggregate. This is decomposed into a set of components which represent theintrinsic time-frequency signals within the data set. Individual time-seriescan then be fit to these components to obtain a set of numerical features thatrepresent their intrinsic time-frequency patterns. To demonstrate theeffectiveness of AMP, we evaluate against the real word task of clusteringintermittent time-series data. Using synthetically generated data we show thata clustering approach which uses the features derived from AMP significantlyoutperforms traditional clustering methods. Our technique is furtherexemplified on a real world data set where AMP can be used to discovergroupings of individuals which correspond to real world sub-populations.
arxiv-11700-177 | Estimating an Activity Driven Hidden Markov Model | http://arxiv.org/pdf/1507.07495v1.pdf | author:David A. Meyer, Asif Shakeel category:stat.ML cs.DS cs.LG cs.SI math.ST stat.TH published:2015-07-27 summary:We define a Hidden Markov Model (HMM) in which each hidden state hastime-dependent $\textit{activity levels}$ that drive transitions and emissions,and show how to estimate its parameters. Our construction is motivated by theproblem of inferring human mobility on sub-daily time scales from, for example,mobile phone records.
arxiv-11700-178 | Discovery of Shared Semantic Spaces for Multi-Scene Video Query and Summarization | http://arxiv.org/pdf/1507.07458v1.pdf | author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV published:2015-07-27 summary:The growing rate of public space CCTV installations has generated a need forautomated methods for exploiting video surveillance data including sceneunderstanding, query, behaviour annotation and summarization. For this reason,extensive research has been performed on surveillance scene understanding andanalysis. However, most studies have considered single scenes, or groups ofadjacent scenes. The semantic similarity between different but related scenes(e.g., many different traffic scenes of similar layout) is not generallyexploited to improve any automated surveillance tasks and reduce manual effort.Exploiting commonality, and sharing any supervised annotations, betweendifferent scenes is however challenging due to: Some scenes are totallyun-related -- and thus any information sharing between them would bedetrimental; while others may only share a subset of common activities -- andthus information sharing is only useful if it is selective. Moreover,semantically similar activities which should be modelled together and sharedacross scenes may have quite different pixel-level appearance in each scene. Toaddress these issues we develop a new framework for distributed multiple-sceneglobal understanding that clusters surveillance scenes by their ability toexplain each other's behaviours; and further discovers which subset ofactivities are shared versus scene-specific within each cluster. We show how touse this structured representation of multiple scenes to improve commonsurveillance tasks including scene activity understanding, cross-scenequery-by-example, behaviour classification with reduced supervised labellingrequirements, and video summarization. In each case we demonstrate how ourmulti-scene model improves on a collection of standard single scene models anda flat model of all scenes.
arxiv-11700-179 | Robust Subjective Visual Property Prediction from Crowdsourced Pairwise Labels | http://arxiv.org/pdf/1501.06202v4.pdf | author:Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, Yuan Yao category:cs.CV cs.LG cs.MM cs.SI math.ST stat.TH published:2015-01-25 summary:The problem of estimating subjective visual properties from image and videohas attracted increasing interest. A subjective visual property is usefuleither on its own (e.g. image and video interestingness) or as an intermediaterepresentation for visual recognition (e.g. a relative attribute). Due to itsambiguous nature, annotating the value of a subjective visual property forlearning a prediction model is challenging. To make the annotation morereliable, recent studies employ crowdsourcing tools to collect pairwisecomparison labels because human annotators are much better at ranking twoimages/videos (e.g. which one is more interesting) than giving an absolutevalue to each of them separately. However, using crowdsourced data alsointroduces outliers. Existing methods rely on majority voting to prune theannotation outliers/errors. They thus require large amount of pairwise labelsto be collected. More importantly as a local outlier detection method, majorityvoting is ineffective in identifying outliers that can cause global rankinginconsistencies. In this paper, we propose a more principled way to identifyannotation outliers by formulating the subjective visual property predictiontask as a unified robust learning to rank problem, tackling both the outlierdetection and learning to rank jointly. Differing from existing methods, theproposed method integrates local pairwise comparison labels together tominimise a cost that corresponds to global inconsistency of ranking order. Thisnot only leads to better detection of annotation outliers but also enableslearning with extremely sparse annotations. Extensive experiments on variousbenchmark datasets demonstrate that our new approach significantly outperformsstate-of-the-arts alternatives.
arxiv-11700-180 | Requirements for Open-Ended Evolution in Natural and Artificial Systems | http://arxiv.org/pdf/1507.07403v1.pdf | author:Tim Taylor category:cs.NE q-bio.PE published:2015-07-27 summary:Open-ended evolutionary dynamics remains an elusive goal for artificialevolutionary systems. Many ideas exist in the biological literature beyond thebasic Darwinian requirements of variation, differential reproduction andinheritance. I argue that these ideas can be seen as aspects of fivefundamental requirements for open-ended evolution: (1) robustly reproductiveindividuals, (2) a medium allowing the possible existence of a practicallyunlimited diversity of individuals and interactions, (3) individuals capable ofproducing more complex offspring, (4) mutational pathways to other viableindividuals, and (5) drive for continued evolution. I briefly discussimplications of this view for the design of artificial systems with greaterevolutionary potential.
arxiv-11700-181 | A genetic algorithm for autonomous navigation in partially observable domain | http://arxiv.org/pdf/1507.07374v1.pdf | author:Maxim Borisyak, Andrey Ustyuzhanin category:cs.LG cs.AI cs.NE 68T05 published:2015-07-27 summary:The problem of autonomous navigation is one of the basic problems forrobotics. Although, in general, it may be challenging when an autonomousvehicle is placed into partially observable domain. In this paper we considersimplistic environment model and introduce a navigation algorithm based onLearning Classifier System.
arxiv-11700-182 | Variational Bayesian strategies for high-dimensional, stochastic design problems | http://arxiv.org/pdf/1507.06759v2.pdf | author:Phaedon-Stelios Koutsourelakis category:stat.CO math.NA stat.ML published:2015-07-24 summary:This paper is concerned with a lesser-studied problem in the context ofmodel-based, uncertainty quantification (UQ), that ofoptimization/design/control under uncertainty. The solution of such problems ishindered not only by the usual difficulties encountered in UQ tasks (e.g. thehigh computational cost of each forward simulation, the large number of randomvariables) but also by the need to solve a nonlinear optimization probleminvolving large numbers of design variables and potentially constraints. Wepropose a framework that is suitable for a large class of such problems and isbased on the idea of recasting them as probabilistic inference tasks. To thatend, we propose a Variational Bayesian (VB) formulation and an iterativeVB-Expectation-Maximization scheme that is also capable of identifying alow-dimensional set of directions in the design space, along which, theobjective exhibits the largest sensitivity. We demonstrate the validity of the proposed approach in the context of twonumerical examples involving $\mathcal{O}(10^3)$ random and design variables.In all cases considered the cost of the computations in terms of calls to theforward model was of the order $\mathcal{O}(10^2)$. The accuracy of theapproximations provided is assessed by appropriate information-theoreticmetrics.
arxiv-11700-183 | Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments | http://arxiv.org/pdf/1502.03032v2.pdf | author:Jiyan Yang, Xiangrui Meng, Michael W. Mahoney category:cs.DC cs.DS math.NA stat.ML published:2015-02-10 summary:In this era of large-scale data, distributed systems built on top of clustersof commodity hardware provide cheap and reliable storage and scalableprocessing of massive data. Here, we review recent work on developing andimplementing randomized matrix algorithms in large-scale parallel anddistributed environments. Randomized algorithms for matrix problems havereceived a great deal of attention in recent years, thus far typically eitherin theory or in machine learning applications or with implementations on asingle machine. Our main focus is on the underlying theory and practicalimplementation of random projection and random sampling algorithms for verylarge very overdetermined (i.e., overconstrained) $\ell_1$ and $\ell_2$regression problems. Randomization can be used in one of two related ways:either to construct sub-sampled problems that can be solved, exactly orapproximately, with traditional numerical methods; or to constructpreconditioned versions of the original full problem that are easier to solvewith traditional iterative algorithms. Theoretical results demonstrate that innear input-sparsity time and with only a few passes through the data one canobtain very strong relative-error approximate solutions, with high probability.Empirical results highlight the importance of various trade-offs (e.g., betweenthe time to construct an embedding and the conditioning quality of theembedding, between the relative importance of computation versus communication,etc.) and demonstrate that $\ell_1$ and $\ell_2$ regression problems can besolved to low, medium, or high precision in existing distributed systems on upto terabyte-sized data.
arxiv-11700-184 | Likelihood-free Model Choice | http://arxiv.org/pdf/1503.07689v2.pdf | author:Jean-Michel Marin, Pierre Pudlo, Christian P. Robert category:stat.ME stat.CO stat.ML published:2015-03-26 summary:This document is an invited chapter covering the specificities of ABC modelchoice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont(2015). Beyond exposing the potential pitfalls of ABC based posteriorprobabilities, the review emphasizes mostly the solution proposed by Pudlo etal. (2014) on the use of random forests for aggregating summary statistics andand for estimating the posterior probability of the most likely model via asecondary random fores.
arxiv-11700-185 | A Social Spider Algorithm for Solving the Non-convex Economic Load Dispatch Problem | http://arxiv.org/pdf/1507.07301v1.pdf | author:James J. Q. Yu, Victor O. K. Li category:cs.NE published:2015-07-27 summary:Economic Load Dispatch (ELD) is one of the essential components in powersystem control and operation. Although conventional ELD formulation can besolved using mathematical programming techniques, modern power systemintroduces new models of the power units which are non-convex,non-differentiable, and sometimes non-continuous. In order to solve suchnon-convex ELD problems, in this paper we propose a new approach based on theSocial Spider Algorithm (SSA). The classical SSA is modified and enhanced toadapt to the unique characteristics of ELD problems, e.g., valve-point effects,multi-fuel operations, prohibited operating zones, and line losses. Todemonstrate the superiority of our proposed approach, five widely-adopted testsystems are employed and the simulation results are compared with thestate-of-the-art algorithms. In addition, the parameter sensitivity isillustrated by a series of simulations. The simulation results show that SSAcan solve ELD problems effectively and efficiently.
arxiv-11700-186 | Differentially Private Analysis of Outliers | http://arxiv.org/pdf/1507.06763v2.pdf | author:Rina Okada, Kazuto Fukuchi, Kazuya Kakizaki, Jun Sakuma category:stat.ML cs.CR cs.LG published:2015-07-24 summary:This paper investigates differentially private analysis of distance-basedoutliers. The problem of outlier detection is to find a small number ofinstances that are apparently distant from the remaining instances. On theother hand, the objective of differential privacy is to conceal presence (orabsence) of any particular instance. Outlier detection and privacy protectionare thus intrinsically conflicting tasks. In this paper, instead of reportingoutliers detected, we present two types of differentially private queries thathelp to understand behavior of outliers. One is the query to count outliers,which reports the number of outliers that appear in a given subspace. Ourformal analysis on the exact global sensitivity of outlier counts reveals thatregular global sensitivity based method can make the outputs too noisy,particularly when the dimensionality of the given subspace is high. Noting thatthe counts of outliers are typically expected to be relatively small comparedto the number of data, we introduce a mechanism based on the smooth upper boundof the local sensitivity. The other is the query to discovery top-$h$ subspacescontaining a large number of outliers. This task can be naively achieved byissuing count queries to each subspace in turn. However, the variation ofsubspaces can grow exponentially in the data dimensionality. This can causeserious consumption of the privacy budget. For this task, we propose anexponential mechanism with a customized score function for subspace discovery.To the best of our knowledge, this study is the first trial to ensuredifferential privacy for distance-based outlier analysis. We demonstrated ourmethods with synthesized datasets and real datasets. The experimental resultsshow that out method achieve better utility compared to the global sensitivitybased methods.
arxiv-11700-187 | Reduced-Set Kernel Principal Components Analysis for Improving the Training and Execution Speed of Kernel Machines | http://arxiv.org/pdf/1507.07260v1.pdf | author:Hassan A. Kingravi, Patricio A. Vela, Alexandar Gray category:stat.ML cs.LG published:2015-07-26 summary:This paper presents a practical, and theoretically well-founded, approach toimprove the speed of kernel manifold learning algorithms relying on spectraldecomposition. Utilizing recent insights in kernel smoothing and learning withintegral operators, we propose Reduced Set KPCA (RSKPCA), which also suggestsan easy-to-implement method to remove or replace samples with minimal effect onthe empirical operator. A simple data point selection procedure is given togenerate a substitute density for the data, with accuracy that is governed by auser-tunable parameter . The effect of the approximation on the quality of theKPCA solution, in terms of spectral and operator errors, can be shown directlyin terms of the density estimate error and as a function of the parameter . Weshow in experiments that RSKPCA can improve both training and evaluation timeof KPCA by up to an order of magnitude, and compares favorably to thewidely-used Nystrom and density-weighted Nystrom methods.
arxiv-11700-188 | Estimator Selection: End-Performance Metric Aspects | http://arxiv.org/pdf/1507.07238v1.pdf | author:Dimitrios Katselis, Cristian R. Rojas, Carolyn L. Beck category:cs.IT math.IT stat.ML published:2015-07-26 summary:Recently, a framework for application-oriented optimal experiment design hasbeen introduced. In this context, the distance of the estimated system from thetrue one is measured in terms of a particular end-performance metric. Thistreatment leads to superior unknown system estimates to classical experimentdesigns based on usual pointwise functional distances of the estimated systemfrom the true one. The separation of the system estimator from the experimentdesign is done within this new framework by choosing and fixing the estimationmethod to either a maximum likelihood (ML) approach or a Bayesian estimatorsuch as the minimum mean square error (MMSE). Since the MMSE estimator deliversa system estimate with lower mean square error (MSE) than the ML estimator forfinite-length experiments, it is usually considered the best choice in practicein signal processing and control applications. Within the application-orientedframework a related meaningful question is: Are there end-performance metricsfor which the ML estimator outperforms the MMSE when the experiment isfinite-length? In this paper, we affirmatively answer this question based on asimple linear Gaussian regression example.
arxiv-11700-189 | Modeling Website Workload Using Neural Networks | http://arxiv.org/pdf/1507.07204v1.pdf | author:Yasir Shoaib, Olivia Das category:cs.DC cs.NE published:2015-07-26 summary:In this article, artificial neural networks (ANN) are used for modeling thenumber of requests received by 1998 FIFA World Cup website. Modeling is done bymeans of time-series forecasting. The log traces of the website, availablethrough the Internet Traffic Archive (ITA), are processed to obtain twotime-series data sets that are used for finding the following measurements:requests/day and requests/second. These are modeled by training and simulatingANN. The method followed to collect and process the data, and perform theexperiments have been detailed in this article. In total, 13 cases have beentried and their results have been presented, discussed, compared andsummarized. Lastly, future works have also been mentioned.
arxiv-11700-190 | Capturing the Dynamics of Pedestrian Traffic Using a Machine Vision System | http://arxiv.org/pdf/1507.07203v1.pdf | author:Louie Vincent A. Ngoho, Jaderick P. Pabico category:cs.CV published:2015-07-26 summary:We developed a machine vision system to automatically capture the dynamics ofpedestrians under four different traffic scenarios. By considering the overheadview of each pedestrian as a digital object, the system processes the imagesequences to track the pedestrians. Considering the perspective effect of thecamera lens and the projected area of the hallway at the top-view scene, thedistance of each tracked object from its original position to its currentposition is approximated every video frame. Using the approximated distance andthe video frame rate (30 frames per second), the respective velocity andacceleration of each tracked object are later derived. The quantified motioncharacteristics of the pedestrians are displayed by the system through2-dimensional graphs of the kinematics of motion. The system also outputs videoimages of the pedestrians with superimposed markers for tracking. These visualmarkers were used to visually describe and quantify the behavior of thepedestrians under different traffic scenarios.
arxiv-11700-191 | A Neural Prototype for a Virtual Chemical Spectrophotometer | http://arxiv.org/pdf/1507.07200v1.pdf | author:Jaderick P. Pabico, Jose Rene L. Micor, Elmer Rico E. Mojica category:cs.NE published:2015-07-26 summary:A virtual chemical spectrophotometer for the simultaneous analysis of nickel(Ni) and cobalt (Co) was developed based on an artificial neural network (ANN).The developed ANN correlates the respective concentrations of Co and Ni giventhe absorbance profile of a Co-Ni mixture based on the Beer's Law. The virtualchemical spectrometer was trained using a 3-layer jump connection neuralnetwork model (NNM) with 126 input nodes corresponding to the 126 absorbancereadings from 350 nm to 600 nm, 70 nodes in the hidden layer using a logisticactivation function, and 2 nodes in the output layer with a logistic function.Test result shows that the NNM has correlation coefficients of 0.9953 and0.9922 when predicting [Co] and [Ni], respectively. We observed, however, thatthe NNM has a duality property and that there exists a real-world practicalapplication in solving the dual problem: Predict the Co-Ni mixture's absorbanceprofile given [Co] and [Ni]. It turns out that the dual problem is much harderto solve because the intended output has a much bigger cardinality than that ofthe input. Thus, we trained the dual ANN, a 3-layer jump connection nets with 2input nodes corresponding to [Co] and [Ni], 70-logistic-activated nodes in thehidden layer, and 126 output nodes corresponding to the 126 absorbance readingsfrom 250 nm to 600 nm. Test result shows that the dual NNM has correlationcoefficients that range from 0.9050 through 0.9980 at 356 nm through 578 nmwith the maximum coefficient observed at 480 nm. This means that the dual ANNcan be used to predict the absorbance profile given the respective Co-Niconcentrations which can be of importance in creating academic models for avirtual chemical spectrophotometer.
arxiv-11700-192 | Task Selection for Bandit-Based Task Assignment in Heterogeneous Crowdsourcing | http://arxiv.org/pdf/1507.07199v1.pdf | author:Hao Zhang, Masashi Sugiyama category:cs.LG published:2015-07-26 summary:Task selection (picking an appropriate labeling task) and worker selection(assigning the labeling task to a suitable worker) are two major challenges intask assignment for crowdsourcing. Recently, worker selection has beensuccessfully addressed by the bandit-based task assignment (BBTA) method, whiletask selection has not been thoroughly investigated yet. In this paper, weexperimentally compare several task selection strategies borrowed from activelearning literature, and show that the least confidence strategy significantlyimproves the performance of task assignment in crowdsourcing.
arxiv-11700-193 | True Online Emphatic TD($Î»$): Quick Reference and Implementation Guide | http://arxiv.org/pdf/1507.07147v1.pdf | author:Richard S. Sutton category:cs.LG published:2015-07-25 summary:This document is a guide to the implementation of true online emphaticTD($\lambda$), a model-free temporal-difference algorithm for learning to makelong-term predictions which combines the emphasis idea (Sutton, Mahmood & White2015) and the true-online idea (van Seijen & Sutton 2014). The setting usedhere includes linear function approximation, the possibility of off-policytraining, and all the generality of general value functions, as well as theemphasis algorithm's notion of "interest".
arxiv-11700-194 | A Framework of Sparse Online Learning and Its Applications | http://arxiv.org/pdf/1507.07146v1.pdf | author:Dayong Wang, Pengcheng Wu, Peilin Zhao, Steven C. H. Hoi category:cs.LG published:2015-07-25 summary:The amount of data in our society has been exploding in the era of big datatoday. In this paper, we address several open challenges of big data streamclassification, including high volume, high velocity, high dimensionality, highsparsity, and high class-imbalance. Many existing studies in data miningliterature solve data stream classification tasks in a batch learning setting,which suffers from poor efficiency and scalability when dealing with big data.To overcome the limitations, this paper investigates an online learningframework for big data stream classification tasks. Unlike some existing onlinedata stream classification techniques that are often based on first-orderonline learning, we propose a framework of Sparse Online Classification (SOC)for data stream classification, which includes some state-of-the-artfirst-order sparse online learning algorithms as special cases and allows us toderive a new effective second-order online learning algorithm for data streamclassification. In addition, we also propose a new cost-sensitive sparse onlinelearning algorithm by extending the framework with application to tackle onlineanomaly detection tasks where class distribution of data could be veryimbalanced. We also analyze the theoretical bounds of the proposed method, andfinally conduct an extensive set of experiments, in which encouraging resultsvalidate the efficacy of the proposed algorithms in comparison to a family ofstate-of-the-art techniques on a variety of data stream classification tasks.
arxiv-11700-195 | Detect & Describe: Deep learning of bank stress in the news | http://arxiv.org/pdf/1507.07870v1.pdf | author:Samuel RÃ¶nnqvist, Peter Sarlin category:q-fin.CP cs.AI cs.LG cs.NE q-fin.RM published:2015-07-25 summary:News is a pertinent source of information on financial risks and stressfactors, which nevertheless is challenging to harness due to the sparse andunstructured nature of natural text. We propose an approach based ondistributional semantics and deep learning with neural networks to model andlink text to a scarce set of bank distress events. Through unsupervisedtraining, we learn semantic vector representations of news articles aspredictors of distress events. The predictive model that we learn can signalcoinciding stress with an aggregated index at bank or European level, whilecrucially allowing for automatic extraction of text descriptions of the events,based on passages with high stress levels. The method offers insight thatmodels based on other types of data cannot provide, while offering a generalmeans for interpreting this type of semantic-predictive model. We model bankdistress with data on 243 events and 6.6M news articles for 101 large Europeanbanks.
arxiv-11700-196 | Robust Detection of Intensity Variant Clones in Forged and JPEG Compressed Images | http://arxiv.org/pdf/1602.07335v1.pdf | author:Minati Mishra, M. C. Adhikary category:cs.CV published:2015-07-25 summary:Digitization of images has made image editing easier. Ease of image editingtempted users and professionals to manipulate digital images leading to digitalimage forgeries. Today digital image forgery has posed a great threat to theauthenticity of the popular digital media, the digital images. A lot ofresearch is going on worldwide to detect image forgery and to separate theforged images from their authentic counterparts. This paper provides a novelintensity invariant detection model (IIDM) for detection of intensity variantclones that is robust against JPEG compression, noise attacks and blurring.
arxiv-11700-197 | Thinning Algorithm Using Hypergraph Based Morphological Operators | http://arxiv.org/pdf/1507.07096v1.pdf | author:R. P. Prakash, Keerthana S. Prakash, V. P. Binu category:cs.CV published:2015-07-25 summary:The object recognition is a complex problem in the image processing.Mathematical morphology is Shape oriented operations, that simplify image data,preserving their essential shape characteristics and eliminating irrelevancies.This paper briefly describes morphological operators using hypergraph and itsapplications for thinning algorithms. The morphological operators usinghypergraph method is used to preventing errors and irregularities in skeleton,and is an important step recognizing line objects. The morphological operatorsusing hypergraph such as dilation, erosion, opening, closing is a novelapproach in image processing and it act as a filter remove the noise and errorsin the images.
arxiv-11700-198 | Instance Significance Guided Multiple Instance Boosting for Robust Visual Tracking | http://arxiv.org/pdf/1501.04378v4.pdf | author:Jinwu Liu, Yao Lu, Tianfei Zhou category:cs.CV published:2015-01-19 summary:Multiple Instance Learning (MIL) recently provides an appealing way toalleviate the drifting problem in visual tracking. Following thetracking-by-detection framework, an online MILBoost approach is developed thatsequentially chooses weak classifiers by maximizing the bag likelihood. In thispaper, we extend this idea towards incorporating the instance significanceestimation into the online MILBoost framework. First, instead of treating allinstances equally, with each instance we associate a significance-coefficientthat represents its contribution to the bag likelihood. The coefficients areestimated by a simple Bayesian formula that jointly considers the predictionsfrom several standard MILBoost classifiers. Next, we follow the online boostingframework, and propose a new criterion for the selection of weak classifiers.Experiments with challenging public datasets show that the proposed methodoutperforms both existing MIL based and boosting based trackers.
arxiv-11700-199 | Making sense of randomness: an approach for fast recovery of compressively sensed signals | http://arxiv.org/pdf/1507.07077v1.pdf | author:V. Abrol, P. Sharma, A. K Sao category:cs.IT cs.CV math.IT published:2015-07-25 summary:In compressed sensing (CS) framework, a signal is sampled below Nyquist rate,and the acquired compressed samples are generally random in nature. However,for efficient estimation of the actual signal, the sensing matrix must preservethe relative distances among the acquired compressed samples. Provided thiscondition is fulfilled, we show that CS samples will preserve the envelope ofthe actual signal even at different compression ratios. Exploiting thisenvelope preserving property of CS samples, we propose a new fast dictionarylearning (DL) algorithm which is able to extract prototype signals fromcompressive samples for efficient sparse representation and recovery ofsignals. These prototype signals are orthogonal intrinsic mode functions (IMFs)extracted using empirical mode decomposition (EMD), which is one of the popularmethods to capture the envelope of a signal. The extracted IMFs are used tobuild the dictionary without even comprehending the original signal or thesensing matrix. Moreover, one can build the dictionary on-line as new CSsamples are available. In particularly, to recover first $L$ signals($\in\mathbb{R}^n$) at the decoder, one can build the dictionary in just$\mathcal{O}(nL\log n)$ operations, that is far less as compared to existingapproaches. The efficiency of the proposed approach is demonstratedexperimentally for recovery of speech signals.
arxiv-11700-200 | A Study of Morphological Filtering Using Graph and Hypergraphs | http://arxiv.org/pdf/1507.07075v1.pdf | author:Keerthana S. Prakash, R. P. Prakash, V. P. Binu category:cs.CV published:2015-07-25 summary:Mathematical morphology (MM) helps to describe and analyze shapes using settheory. MM can be effectively applied to binary images which are treated assets. Basic morphological operators defined can be used as an effective tool inimage processing. Morphological operators are also developed based on graph andhypergraph. These operators have found better performance and applications inimage processing. Bino et al. [8], [9] developed the theory of morphologicaloperators on hypergraph. A hypergraph structure is considered and basicmorphological operation erosion/dilation is defined. Several new operatorsopening/closing and filtering are also defined on the hypergraphs. Hypergraphbased filtering have found comparatively better performance with morphologicalfilters based on graph. In this paper we evaluate the effectiveness ofhypergraph based ASF on binary images. Experimental results shows thathypergraph based ASF filters have outperformed graph based ASF.
arxiv-11700-201 | Structured Occlusion Coding for Robust Face Recognition | http://arxiv.org/pdf/1502.00478v2.pdf | author:Yandong Wen, Weiyang Liu, Meng Yang, Yuli Fu, Youjun Xiang, Rui Hu category:cs.CV published:2015-02-02 summary:Occlusion in face recognition is a common yet challenging problem. Whilesparse representation based classification (SRC) has been shown promisingperformance in laboratory conditions (i.e. noiseless or random pixelcorrupted), it performs much worse in practical scenarios. In this paper, weconsider the practical face recognition problem, where the occlusions arepredictable and available for sampling. We propose the structured occlusioncoding (SOC) to address occlusion problems. The structured coding here lies intwo folds. On one hand, we employ a structured dictionary for recognition. Onthe other hand, we propose to use the structured sparsity in this formulation.Specifically, SOC simultaneously separates the occlusion and classifies theimage. In this way, the problem of recognizing an occluded image is turned intoseeking a structured sparse solution on occlusion-appended dictionary. In orderto construct a well-performing occlusion dictionary, we propose an occlusionmask estimating technique via locality constrained dictionary (LCD), showingstriking improvement in occlusion sample. On a category-specific occlusiondictionary, we replace norm sparsity with the structured sparsity which isshown more robust, further enhancing the robustness of our approach. Moreover,SOC achieves significant improvement in handling large occlusion in real world.Extensive experiments are conducted on public data sets to validate thesuperiority of the proposed algorithm.
arxiv-11700-202 | Learning with hidden variables | http://arxiv.org/pdf/1506.00354v2.pdf | author:Yasser Roudi, Graham Taylor category:q-bio.NC cs.LG cs.NE stat.ML published:2015-06-01 summary:Learning and inferring features that generate sensory input is a taskcontinuously performed by cortex. In recent years, novel algorithms andlearning rules have been proposed that allow neural network models to learnsuch features from natural images, written text, audio signals, etc. Thesenetworks usually involve deep architectures with many layers of hidden neurons.Here we review recent advancements in this area emphasizing, amongst otherthings, the processing of dynamical inputs by networks with hidden nodes andthe role of single neuron models. These points and the questions they arise canprovide conceptual advancements in understanding of learning in the cortex andthe relationship between machine learning approaches to learning with hiddennodes and those in cortical circuits.
arxiv-11700-203 | Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition | http://arxiv.org/pdf/1507.06947v1.pdf | author:HaÅim Sak, Andrew Senior, Kanishka Rao, FranÃ§oise Beaufays category:cs.CL cs.LG cs.NE stat.ML published:2015-07-24 summary:We have recently shown that deep Long Short-Term Memory (LSTM) recurrentneural networks (RNNs) outperform feed forward deep neural networks (DNNs) asacoustic models for speech recognition. More recently, we have shown that theperformance of sequence trained context dependent (CD) hidden Markov model(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trainedphone models initialized with connectionist temporal classification (CTC). Inthis paper, we present techniques that further improve performance of LSTM RNNacoustic models for large vocabulary speech recognition. We show that framestacking and reduced frame rate lead to more accurate models and fasterdecoding. CD phone modeling leads to further improvements. We also presentinitial results for LSTM RNN models outputting words directly.
arxiv-11700-204 | A Reinforcement Learning Approach to Online Learning of Decision Trees | http://arxiv.org/pdf/1507.06923v1.pdf | author:Abhinav Garlapati, Aditi Raghunathan, Vaishnavh Nagarajan, Balaraman Ravindran category:cs.LG published:2015-07-24 summary:Online decision tree learning algorithms typically examine all features of anew data point to update model parameters. We propose a novel alternative,Reinforcement Learning- based Decision Trees (RLDT), that uses ReinforcementLearning (RL) to actively examine a minimal number of features of a data pointto classify it with high accuracy. Furthermore, RLDT optimizes a long termreturn, providing a better alternative to the traditional myopic greedyapproach to growing decision trees. We demonstrate that this approach performsas well as batch learning algorithms and other online decision tree learningalgorithms, while making significantly fewer queries about the features of thedata points. We also show that RLDT can effectively handle concept drift.
arxiv-11700-205 | Document Classification by Inversion of Distributed Language Representations | http://arxiv.org/pdf/1504.07295v3.pdf | author:Matt Taddy category:cs.CL cs.IR stat.AP published:2015-04-27 summary:There have been many recent advances in the structure and measurement ofdistributed language models: those that map from words to a vector-space thatis rich in information about word choice and composition. This vector-space isthe distributed language representation. The goal of this note is to point outthat any distributed representation can be turned into a classifier throughinversion via Bayes rule. The approach is simple and modular, in that it willwork with any language representation whose training can be formulated asoptimizing a probability model. In our application to 2 million sentences fromYelp reviews, we also find that it performs as well as or better than complexpurpose-built algorithms.
arxiv-11700-206 | Multi-objective analysis of computational models | http://arxiv.org/pdf/1507.06877v1.pdf | author:StÃ©phane Doncieux, Jean LiÃ©nard, BenoÃ®t Girard, Mohamed Hamdaoui, JoÃ«l Chaskalovic category:cs.NE published:2015-07-24 summary:Computational models are of increasing complexity and their behavior may inparticular emerge from the interaction of different parts. Studying such modelsbecomes then more and more difficult and there is a need for methods and toolssupporting this process. Multi-objective evolutionary algorithms generate a setof trade-off solutions instead of a single optimal solution. The availabilityof a set of solutions that have the specificity to be optimal relative tocarefully chosen objectives allows to perform data mining in order to betterunderstand model features and regularities. We review the corresponding work,propose a unifying framework, and highlight its potential use. Typicalquestions that such a methodology allows to address are the following: what arethe most critical parameters of the model? What are the relations between theparameters and the objectives? What are the typical behaviors of the model? Twoexamples are provided to illustrate the capabilities of the methodology. Thefeatures of a flapping-wing robot are thus evaluated to find out itsspeed-energy relation, together with the criticality of its parameters. Aneurocomputational model of the Basal Ganglia brain nuclei is then consideredand its most salient features according to this methodology are presented anddiscussed.
arxiv-11700-207 | YARBUS : Yet Another Rule Based belief Update System | http://arxiv.org/pdf/1507.06837v1.pdf | author:Jeremy Fix, Herve Frezza-buet category:cs.CL cs.AI published:2015-07-24 summary:We introduce a new rule based system for belief tracking in dialog systems.Despite the simplicity of the rules being considered, the proposed belieftracker ranks favourably compared to the previous submissions on the second andthird Dialog State Tracking challenges. The results of this simple trackerallows to reconsider the performances of previous submissions using moreelaborate techniques.
arxiv-11700-208 | The Polylingual Labeled Topic Model | http://arxiv.org/pdf/1507.06829v1.pdf | author:Lisa Posch, Arnim Bleier, Philipp Schaer, Markus Strohmaier category:cs.CL cs.IR cs.LG G.3; I.2.7 published:2015-07-24 summary:In this paper, we present the Polylingual Labeled Topic Model, a model whichcombines the characteristics of the existing Polylingual Topic Model andLabeled LDA. The model accounts for multiple languages with separate topicdistributions for each language while restricting the permitted topics of adocument to a set of predefined labels. We explore the properties of the modelin a two-language setting on a dataset from the social science domain. Ourexperiments show that our model outperforms LDA and Labeled LDA in terms oftheir held-out perplexity and that it produces semantically coherent topicswhich are well interpretable by human subjects.
arxiv-11700-209 | A Neighbourhood-Based Stopping Criterion for Contrastive Divergence Learning | http://arxiv.org/pdf/1507.06803v1.pdf | author:E. Romero, F. Mazzanti, J. Delgado category:cs.NE cs.LG published:2015-07-24 summary:Restricted Boltzmann Machines (RBMs) are general unsupervised learningdevices to ascertain generative models of data distributions. RBMs are oftentrained using the Contrastive Divergence learning algorithm (CD), anapproximation to the gradient of the data log-likelihood. A simplereconstruction error is often used as a stopping criterion for CD, althoughseveral authors\cite{schulz-et-al-Convergence-Contrastive-Divergence-2010-NIPSw,fischer-igel-Divergence-Contrastive-Divergence-2010-ICANN} have raised doubtsconcerning the feasibility of this procedure. In many cases the evolution curveof the reconstruction error is monotonic while the log-likelihood is not, thusindicating that the former is not a good estimator of the optimal stoppingpoint for learning. However, not many alternatives to the reconstruction errorhave been discussed in the literature. In this manuscript we investigate simplealternatives to the reconstruction error, based on the inclusion of informationcontained in neighboring states to the training set, as a stopping criterionfor CD learning.
arxiv-11700-210 | Implicitly Constrained Semi-Supervised Least Squares Classification | http://arxiv.org/pdf/1507.06802v1.pdf | author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG published:2015-07-24 summary:We introduce a novel semi-supervised version of the least squares classifier.This implicitly constrained least squares (ICLS) classifier minimizes thesquared loss on the labeled data among the set of parameters implied by allpossible labelings of the unlabeled data. Unlike other discriminativesemi-supervised methods, our approach does not introduce explicit additionalassumptions into the objective function, but leverages implicit assumptionsalready present in the choice of the supervised least squares classifier. Weshow this approach can be formulated as a quadratic programming problem and itssolution can be found using a simple gradient descent procedure. We prove that,in a certain way, our method never leads to performance worse than thesupervised classifier. Experimental results corroborate this theoretical resultin the multidimensional case on benchmark datasets, also in terms of the errorrate.
arxiv-11700-211 | Linear Contextual Bandits with Global Constraints and Objective | http://arxiv.org/pdf/1507.06738v1.pdf | author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG math.OC stat.ML published:2015-07-24 summary:We consider the linear contextual bandit problem with global convexconstraints and a concave objective function. In each round, the outcome ofpulling an arm is a vector, that depends linearly on the context of that arm.The global constraints require the average of these vectors to lie in a certainconvex set. The objective is a concave function of this average vector. Thisproblem turns out to be a common generalization of classic linear contextualbandits (linContextual) [Auer 2003], bandits with concave rewards and convexknapsacks (BwCR) [Agrawal, Devanur 2014], and the online stochastic convexprogramming (OSCP) problem [Agrawal, Devanur 2015]. We present algorithms withnear-optimal regret bounds for this problem. Our bounds compare favorably toresults on the unstructured version of the problem [Agrawal et al. 2015,Badanidiyuru et al. 2014] where the relation between the contexts and theoutcomes could be arbitrary, but the algorithm only competes against a fixedset of policies.
arxiv-11700-212 | Clustering of Modal Valued Symbolic Data | http://arxiv.org/pdf/1507.06683v1.pdf | author:Vladimir Batagelj, NataÅ¡a KejÅ¾ar, Simona Korenjak-Äerne category:stat.ML published:2015-07-23 summary:Symbolic Data Analysis is based on special descriptions of data - symbolicobjects (SO). Such descriptions preserve more detailed information about unitsand their clusters than the usual representations with mean values. A specialkind of symbolic object is a representation with frequency or probabilitydistributions (modal values). This representation enables us to consider in theclustering process the variables of all measurement types at the same time. Inthe paper a clustering criterion function for SOs is proposed such that therepresentative of each cluster is again composed of distributions of variables'values over the cluster. The corresponding leaders clustering method is basedon this result. It is also shown that for the corresponding agglomerativehierarchical method a generalized Ward's formula holds. Both methods arecompatible - they are solving the same clustering optimization problem. Theleaders method efficiently solves clustering problems with large number ofunits; while the agglomerative method can be applied alone on the smaller dataset, or it could be applied on leaders, obtained with compatiblenonhierarchical clustering method. Such a combination of two compatible methodsenables us to decide upon the right number of clusters on the basis of thecorresponding dendrogram. The proposed methods were applied on different datasets. In the paper, some results of clustering of ESS data are presented.
arxiv-11700-213 | Adam: A Method for Stochastic Optimization | http://arxiv.org/pdf/1412.6980v8.pdf | author:Diederik Kingma, Jimmy Ba category:cs.LG published:2014-12-22 summary:We introduce Adam, an algorithm for first-order gradient-based optimizationof stochastic objective functions, based on adaptive estimates of lower-ordermoments. The method is straightforward to implement, is computationallyefficient, has little memory requirements, is invariant to diagonal rescalingof the gradients, and is well suited for problems that are large in terms ofdata and/or parameters. The method is also appropriate for non-stationaryobjectives and problems with very noisy and/or sparse gradients. Thehyper-parameters have intuitive interpretations and typically require littletuning. Some connections to related algorithms, on which Adam was inspired, arediscussed. We also analyze the theoretical convergence properties of thealgorithm and provide a regret bound on the convergence rate that is comparableto the best known results under the online convex optimization framework.Empirical results demonstrate that Adam works well in practice and comparesfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,a variant of Adam based on the infinity norm.
arxiv-11700-214 | PCA with Gaussian perturbations | http://arxiv.org/pdf/1506.04855v2.pdf | author:Wojciech KotÅowski, Manfred K. Warmuth category:cs.LG stat.ML published:2015-06-16 summary:Most of machine learning deals with vector parameters. Ideally we would liketo take higher order information into account and make use of matrix or eventensor parameters. However the resulting algorithms are usually inefficient.Here we address on-line learning with matrix parameters. It is often easy toobtain online algorithm with good generalization performance if youeigendecompose the current parameter matrix in each trial (at a cost of$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is acore trade-off between the running time and the generalization performance,here measured by the regret of the on-line algorithm (total gain of the bestoff-line predictor minus the total gain of the on-line algorithm). We focus onthe key matrix problem of rank $k$ Principal Component Analysis in$\mathbb{R}^n$ where $k \ll n$. There are $O(n^3)$ algorithms that achieve theoptimum regret but require eigendecompositions. We develop a simple algorithmthat needs $O(kn^2)$ per trial whose regret is off by a small factor of$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leaderparadigm. It replaces full eigendecompositions at each trial by the problemfinding $k$ principal components of the current covariance matrix that isperturbed by Gaussian noise.
arxiv-11700-215 | Optimal Learning Rates for Localized SVMs | http://arxiv.org/pdf/1507.06615v1.pdf | author:Mona Eberts, Ingo Steinwart category:stat.ML published:2015-07-23 summary:One of the limiting factors of using support vector machines (SVMs) in largescale applications are their super-linear computational requirements in termsof the number of training samples. To address this issue, several approachesthat train SVMs on many small chunks of large data sets separately have beenproposed in the literature. So far, however, almost all these approaches haveonly been empirically investigated. In addition, their motivation was alwaysbased on computational requirements. In this work, we consider a localized SVMapproach based upon a partition of the input space. For this local SVM, wederive a general oracle inequality. Then we apply this oracle inequality toleast squares regression using Gaussian kernels and deduce local learning ratesthat are essentially minimax optimal under some standard smoothness assumptionson the regression function. This gives the first motivation for using localSVMs that is not based on computational requirements but on theoreticalpredictions on the generalization performance. We further introduce adata-dependent parameter selection method for our local SVM approach and showthat this method achieves the same learning rates as before. Finally, wepresent some larger scale experiments for our localized SVM showing that itachieves essentially the same test performance as a global SVM for a fractionof the computational requirements. In addition, it turns out that thecomputational requirements for the local SVMs are similar to those of a vanillarandom chunk approach, while the achieved test errors are significantly better.
arxiv-11700-216 | Multi-scale exploration of convex functions and bandit convex optimization | http://arxiv.org/pdf/1507.06580v1.pdf | author:SÃ©bastien Bubeck, Ronen Eldan category:math.MG cs.LG math.OC math.PR stat.ML published:2015-07-23 summary:We construct a new map from a convex function to a distribution on itsdomain, with the property that this distribution is a multi-scale explorationof the function. We use this map to solve a decade-old open problem inadversarial bandit convex optimization by showing that the minimax regret forthis problem is $\tilde{O}(\mathrm{poly}(n) \sqrt{T})$, where $n$ is thedimension and $T$ the number of rounds. This bound is obtained by studying thedual Bayesian maximin regret via the information ratio analysis of Russo andVan Roy, and then using the multi-scale exploration to solve the Bayesianproblem.
arxiv-11700-217 | ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks | http://arxiv.org/pdf/1505.00393v3.pdf | author:Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, Yoshua Bengio category:cs.CV published:2015-05-03 summary:In this paper, we propose a deep neural network architecture for objectrecognition based on recurrent neural networks. The proposed network, calledReNet, replaces the ubiquitous convolution+pooling layer of the deepconvolutional neural network with four recurrent neural networks that sweephorizontally and vertically in both directions across the image. We evaluatethe proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 andSVHN. The result suggests that ReNet is a viable alternative to the deepconvolutional neural network, and that further investigation is needed.
arxiv-11700-218 | Manitest: Are classifiers really invariant? | http://arxiv.org/pdf/1507.06535v1.pdf | author:Alhussein Fawzi, Pascal Frossard category:cs.CV cs.LG stat.ML published:2015-07-23 summary:Invariance to geometric transformations is a highly desirable property ofautomatic classifiers in many image recognition tasks. Nevertheless, it isunclear to which extent state-of-the-art classifiers are invariant to basictransformations such as rotations and translations. This is mainly due to thelack of general methods that properly measure such an invariance. In thispaper, we propose a rigorous and systematic approach for quantifying theinvariance to geometric transformations of any classifier. Our key idea is tocast the problem of assessing a classifier's invariance as the computation ofgeodesics along the manifold of transformed images. We propose the Manitestmethod, built on the efficient Fast Marching algorithm to compute theinvariance of classifiers. Our new method quantifies in particular theimportance of data augmentation for learning invariance from data, and theincreased invariance of convolutional neural networks with depth. We foreseethat the proposed generic tool for measuring invariance to a large class ofgeometric transformations and arbitrary classifiers will have many applicationsfor evaluating and comparing classifiers based on their invariance, and helpimproving the invariance of existing classifiers.
arxiv-11700-219 | Dynamic Matrix Factorization with Priors on Unknown Values | http://arxiv.org/pdf/1507.06452v1.pdf | author:Robin Devooght, Nicolas Kourtellis, Amin Mantrach category:stat.ML cs.IR cs.LG published:2015-07-23 summary:Advanced and effective collaborative filtering methods based on explicitfeedback assume that unknown ratings do not follow the same model as theobserved ones (\emph{not missing at random}). In this work, we build on thisassumption, and introduce a novel dynamic matrix factorization framework thatallows to set an explicit prior on unknown values. When new ratings, users, oritems enter the system, we can update the factorization in time independent ofthe size of data (number of users, items and ratings). Hence, we can quicklyrecommend items even to very recent users. We test our methods on three largedatasets, including two very sparse ones, in static and dynamic conditions. Ineach case, we outrank state-of-the-art matrix factorization methods that do notuse a prior on unknown ratings.
arxiv-11700-220 | Deep Fishing: Gradient Features from Deep Nets | http://arxiv.org/pdf/1507.06429v1.pdf | author:Albert Gordo, Adrien Gaidon, Florent Perronnin category:cs.CV published:2015-07-23 summary:Convolutional Networks (ConvNets) have recently improved image recognitionperformance thanks to end-to-end learning of deep feed-forward models from rawpixels. Deep learning is a marked departure from the previous state of the art,the Fisher Vector (FV), which relied on gradient-based encoding of localhand-crafted features. In this paper, we discuss a novel connection betweenthese two approaches. First, we show that one can derive gradientrepresentations from ConvNets in a similar fashion to the FV. Second, we showthat this gradient representation actually corresponds to a structured matrixthat allows for efficient similarity computation. We experimentally study thebenefits of transferring this representation over the outputs of ConvNetlayers, and find consistent improvements on the Pascal VOC 2007 and 2012datasets.
arxiv-11700-221 | A Comprehensive Survey on Pose-Invariant Face Recognition | http://arxiv.org/pdf/1502.04383v2.pdf | author:Changxing Ding, Dacheng Tao category:cs.CV published:2015-02-15 summary:The capacity to recognize faces under varied poses is a fundamental humanability that presents a unique challenge for computer vision systems. Comparedto frontal face recognition, which has been intensively studied and hasgradually matured in the past few decades, pose-invariant face recognition(PIFR) remains a largely unsolved problem. However, PIFR is crucial torealizing the full potential of face recognition for real-world applications,since face recognition is intrinsically a passive biometric technology forrecognizing uncooperative subjects. In this paper, we discuss the inherentdifficulties in PIFR and present a comprehensive review of establishedtechniques. Existing PIFR methods can be grouped into four categories, i.e.,pose-robust feature extraction approaches, multi-view subspace learningapproaches, face synthesis approaches, and hybrid approaches. The motivations,strategies, pros/cons, and performance of representative approaches aredescribed and compared. Moreover, promising directions for future research arediscussed.
arxiv-11700-222 | Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment | http://arxiv.org/pdf/1507.06411v1.pdf | author:Olivier Francois category:stat.OT cs.DL stat.ML published:2015-07-23 summary:The principle of peer review is central to the evaluation of research, byensuring that only high-quality items are funded or published. But peer reviewhas also received criticism, as the selection of reviewers may introduce biasesin the system. In 2014, the organizers of the ``Neural Information ProcessingSystems\rq\rq{} conference conducted an experiment in which $10\%$ of submittedmanuscripts (166 items) went through the review process twice. Arbitrarinesswas measured as the conditional probability for an accepted submission to getrejected if examined by the second committee. This number was equal to $60\%$,for a total acceptance rate equal to $22.5\%$. Here we present a Bayesiananalysis of those two numbers, by introducing a hidden parameter which measuresthe probability that a submission meets basic quality criteria. The standardquality criteria usually include novelty, clarity, reproducibility, correctnessand no form of misconduct, and are met by a large proportions of submitteditems. The Bayesian estimate for the hidden parameter was equal to $56\%$($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The resultsuggested the total acceptance rate should be increased in order to decreasearbitrariness estimates in future review processes.
arxiv-11700-223 | Multi-Target Tracking with Time-Varying Clutter Rate and Detection Profile: Application to Time-lapse Cell Microscopy Sequences | http://arxiv.org/pdf/1507.06397v1.pdf | author:Seyed Hamid Rezatofighi, Stephen Gould, Ba Tuong Vo, Ba-Ngu Vo, Katarina Mele, Richard Hartley category:cs.CV published:2015-07-23 summary:Quantitative analysis of the dynamics of tiny cellular and sub-cellularstructures, known as particles, in time-lapse cell microscopy sequencesrequires the development of a reliable multi-target tracking method capable oftracking numerous similar targets in the presence of high levels of noise, hightarget density, complex motion patterns and intricate interactions. In thispaper, we propose a framework for tracking these structures based on the randomfinite set Bayesian filtering framework. We focus on challenging biologicalapplications where image characteristics such as noise and background intensitychange during the acquisition process. Under these conditions, detectionmethods usually fail to detect all particles and are often followed by misseddetections and many spurious measurements with unknown and time-varying rates.To deal with this, we propose a bootstrap filter composed of an estimator and atracker. The estimator adaptively estimates the required meta parameters forthe tracker such as clutter rate and the detection probability of the targets,while the tracker estimates the state of the targets. Our results show that theproposed approach can outperform state-of-the-art particle trackers on bothsynthetic and real data in this regime.
arxiv-11700-224 | Combining Models of Approximation with Partial Learning | http://arxiv.org/pdf/1507.01215v2.pdf | author:Ziyuan Gao, Frank Stephan, Sandra Zilles category:cs.LG 68Q32 published:2015-07-05 summary:In Gold's framework of inductive inference, the model of partial learningrequires the learner to output exactly one correct index for the target objectand only the target object infinitely often. Since infinitely many of thelearner's hypotheses may be incorrect, it is not obvious whether a partiallearner can be modifed to "approximate" the target object. Fulk and Jain (Approximate inference and scientific method. Information andComputation 114(2):179--191, 1994) introduced a model of approximate learningof recursive functions. The present work extends their research and solves anopen problem of Fulk and Jain by showing that there is a learner whichapproximates and partially identifies every recursive function by outputting asequence of hypotheses which, in addition, are also almost all finite variantsof the target function. The subsequent study is dedicated to the question how these findingsgeneralise to the learning of r.e. languages from positive data. Here threevariants of approximate learning will be introduced and investigated withrespect to the question whether they can be combined with partial learning.Following the line of Fulk and Jain's research, further investigations provideconditions under which partial language learners can eventually output onlyfinite variants of the target language. The combinabilities of other partiallearning criteria will also be briefly studied.
arxiv-11700-225 | Targeting Ultimate Accuracy: Face Recognition via Deep Embedding | http://arxiv.org/pdf/1506.07310v4.pdf | author:Jingtuo Liu, Yafeng Deng, Tao Bai, Zhengping Wei, Chang Huang category:cs.CV published:2015-06-24 summary:Face Recognition has been studied for many decades. As opposed to traditionalhand-crafted features such as LBP and HOG, much more sophisticated features canbe learned automatically by deep learning methods in a data-driven way. In thispaper, we propose a two-stage approach that combines a multi-patch deep CNN anddeep metric learning, which extracts low dimensional but very discriminativefeatures for face verification and recognition. Experiments show that thismethod outperforms other state-of-the-art methods on LFW dataset, achieving99.77% pair-wise verification accuracy and significantly better accuracy underother two more practical protocols. This paper also discusses the importance ofdata size and the number of patches, showing a clear path to practicalhigh-performance face recognition systems in real world.
arxiv-11700-226 | Forward - Backward Greedy Algorithms for Atomic Norm Regularization | http://arxiv.org/pdf/1404.5692v2.pdf | author:Nikhil Rao, Parikshit Shah, Stephen Wright category:cs.DS cs.LG math.OC stat.ML published:2014-04-23 summary:In many signal processing applications, the aim is to reconstruct a signalthat has a simple representation with respect to a certain basis or frame.Fundamental elements of the basis known as "atoms" allow us to define "atomicnorms" that can be used to formulate convex regularizations for thereconstruction problem. Efficient algorithms are available to solve theseformulations in certain special cases, but an approach that works well forgeneral atomic norms, both in terms of speed and reconstruction accuracy,remains to be found. This paper describes an optimization algorithm calledCoGEnT that produces solutions with succinct atomic representations forreconstruction problems, generally formulated with atomic-norm constraints.CoGEnT combines a greedy selection scheme based on the conditional gradientapproach with a backward (or "truncation") step that exploits the quadraticnature of the objective to reduce the basis size. We establish convergenceproperties and validate the algorithm via extensive numerical experiments on asuite of signal processing applications. Our algorithm and analysis also allowfor inexact forward steps and for occasional enhancements of the currentrepresentation to be performed. CoGEnT can outperform the basic conditionalgradient method, and indeed many methods that are tailored to specificapplications, when the enhancement and truncation steps are definedappropriately. We also introduce several novel applications that are enabled bythe atomic-norm framework, including tensor completion, moment problems insignal processing, and graph deconvolution.
arxiv-11700-227 | Evaluation of Spectral Learning for the Identification of Hidden Markov Models | http://arxiv.org/pdf/1507.06346v1.pdf | author:Robert Mattila, Cristian R. Rojas, Bo Wahlberg category:stat.ML cs.LG math.OC published:2015-07-22 summary:Hidden Markov models have successfully been applied as models of discretetime series in many fields. Often, when applied in practice, the parameters ofthese models have to be estimated. The currently predominating identificationmethods, such as maximum-likelihood estimation and especiallyexpectation-maximization, are iterative and prone to have problems with localminima. A non-iterative method employing a spectral subspace-like approach hasrecently been proposed in the machine learning literature. This paper evaluatesthe performance of this algorithm, and compares it to the performance of theexpectation-maximization algorithm, on a number of numerical examples. We findthat the performance is mixed; it successfully identifies some systems withrelatively few available observations, but fails completely for some systemseven when a large amount of observations is available. An open question is howthis discrepancy can be explained. We provide some indications that it could berelated to how well-conditioned some system parameters are.
arxiv-11700-228 | Part Localization using Multi-Proposal Consensus for Fine-Grained Categorization | http://arxiv.org/pdf/1507.06332v1.pdf | author:Kevin J. Shih, Arun Mallya, Saurabh Singh, Derek Hoiem category:cs.CV published:2015-07-22 summary:We present a simple deep learning framework to simultaneously predictkeypoint locations and their respective visibilities and use those to achievestate-of-the-art performance for fine-grained classification. We show that byconditioning the predictions on object proposals with sufficient image support,our method can do well without complicated spatial reasoning. Instead,inference methods with robustness to outliers, yield state-of-the-art forkeypoint localization. We demonstrate the effectiveness of our accuratekeypoint localization and visibility prediction on the fine-grained birdrecognition task with and without ground truth bird bounding boxes, andoutperform existing state-of-the-art methods by over 2%.
arxiv-11700-229 | Reservoir Characterization: A Machine Learning Approach | http://arxiv.org/pdf/1506.05070v2.pdf | author:Soumi Chaki category:cs.CE cs.LG published:2015-06-15 summary:Reservoir Characterization (RC) can be defined as the act of building areservoir model that incorporates all the characteristics of the reservoir thatare pertinent to its ability to store hydrocarbons and also to produce them.Itis a difficult problem due to non-linear and heterogeneous subsurfaceproperties and associated with a number of complex tasks such as data fusion,data mining, formulation of the knowledge base, and handling of theuncertainty.This present work describes the development of algorithms to obtainthe functional relationships between predictor seismic attributes and targetlithological properties. Seismic attributes are available over a study areawith lower vertical resolution. Conversely, well logs and lithologicalproperties are available only at specific well locations in a study area withhigh vertical resolution.Sand fraction, which represents per unit sand volumewithin the rock, has a balanced distribution between zero to unity.The thesisaddresses the issues of handling the information content mismatch betweenpredictor and target variables and proposes regularization of target propertyprior to building a prediction model.In this thesis, two Artificial NeuralNetwork (ANN) based frameworks are proposed to model sand fraction frommultiple seismic attributes without and with well tops informationrespectively. The performances of the frameworks are quantified in terms ofCorrelation Coefficient, Root Mean Square Error, Absolute Error Mean, etc.
arxiv-11700-230 | Incremental Variational Inference for Latent Dirichlet Allocation | http://arxiv.org/pdf/1507.05016v2.pdf | author:Cedric Archambeau, Beyza Ermis category:stat.ML published:2015-07-17 summary:We introduce incremental variational inference and apply it to latentDirichlet allocation (LDA). Incremental variational inference is inspired byincremental EM and provides an alternative to stochastic variational inference.Incremental LDA can process massive document collections, does not require toset a learning rate, converges faster to a local optimum of the variationalbound and enjoys the attractive property of monotonically increasing it. Westudy the performance of incremental LDA on large benchmark data sets. Wefurther introduce a stochastic approximation of incremental variationalinference which extends to the asynchronous distributed setting. The resultingdistributed algorithm achieves comparable performance as single hostincremental variational inference, but with a significant speed-up.
arxiv-11700-231 | STICK: Spike Time Interval Computational Kernel, A Framework for General Purpose Computation using Neurons, Precise Timing, Delays, and Synchrony | http://arxiv.org/pdf/1507.06222v1.pdf | author:Xavier Lagorce, Ryad Benosman category:cs.NE published:2015-07-22 summary:There has been significant research over the past two decades in developingnew platforms for spiking neural computation. Current neural computers areprimarily developed to mimick biology. They use neural networks which can betrained to perform specific tasks to mainly solve pattern recognition problems.These machines can do more than simulate biology, they allow us to re-think ourcurrent paradigm of computation. The ultimate goal is to develop brain inspiredgeneral purpose computation architectures that can breach the currentbottleneck introduced by the Von Neumann architecture. This work proposes a newframework for such a machine. We show that the use of neuron like units withprecise timing representation, synaptic diversity, and temporal delays allowsus to set a complete, scalable compact computation framework. The presentedframework provides both linear and non linear operations, allowing us torepresent and solve any function. We show usability in solving real use casesfrom simple differential equations to sets of non-linear differential equationsleading to chaotic attractors.
arxiv-11700-232 | Bayesian Time-of-Flight for Realtime Shape, Illumination and Albedo | http://arxiv.org/pdf/1507.06173v1.pdf | author:Amit Adam, Christoph Dann, Omer Yair, Shai Mazor, Sebastian Nowozin category:cs.CV published:2015-07-22 summary:We propose a computational model for shape, illumination and albedo inferencein a pulsed time-of-flight (TOF) camera. In contrast to TOF cameras based onphase modulation, our camera enables general exposure profiles. This results inadded flexibility and requires novel computational approaches. To address this challenge we propose a generative probabilistic model thataccurately relates latent imaging conditions to observed camera responses.While principled, realtime inference in the model turns out to be infeasible,and we propose to employ efficient non-parametric regression trees toapproximate the model outputs. As a result we are able to provide, for eachpixel, at video frame rate, estimates and uncertainty for depth, effectivealbedo, and ambient light intensity. These results we present arestate-of-the-art in depth imaging. The flexibility of our approach allows us to easily enrich our generativemodel. We demonstrate that by extending the original single-path model to atwo-path model, capable of describing some multipath effects. The new model isseamlessly integrated in the system at no additional computational cost. Our work also addresses the important question of optimal exposure design inpulsed TOF systems. Finally, for benchmark purposes and to obtain realisticempirical priors of multipath and insights into this phenomena, we propose aphysically accurate simulation of multipath phenomena.
arxiv-11700-233 | Data-free parameter pruning for Deep Neural Networks | http://arxiv.org/pdf/1507.06149v1.pdf | author:Suraj Srinivas, R. Venkatesh Babu category:cs.CV published:2015-07-22 summary:Deep Neural nets (NNs) with millions of parameters are at the heart of manystate-of-the-art computer vision systems today. However, recent works haveshown that much smaller models can achieve similar levels of performance. Inthis work, we address the problem of pruning parameters in a trained NN model.Instead of removing individual weights one at a time as done in previous works,we remove one neuron at a time. We show how similar neurons are redundant, andpropose a systematic way to remove them. Our experiments in pruning the denselyconnected layers show that we can remove upto 85\% of the total parameters inan MNIST-trained network, and about 35\% for AlexNet without significantlyaffecting performance. Our method can be applied on top of most networks with afully connected layer to give a smaller network.
arxiv-11700-234 | Compression of Fully-Connected Layer in Neural Network by Kronecker Product | http://arxiv.org/pdf/1507.05775v2.pdf | author:Shuchang Zhou, Jia-Nan Wu category:cs.NE cs.CV cs.LG published:2015-07-21 summary:In this paper we propose and study a technique to reduce the number ofparameters and computation time in fully-connected layers of neural networksusing Kronecker product, at a mild cost of the prediction quality. Thetechnique proceeds by replacing Fully-Connected layers with so-called KroneckerFully-Connected layers, where the weight matrices of the FC layers areapproximated by linear combinations of multiple Kronecker products of smallermatrices. In particular, given a model trained on SVHN dataset, we are able toconstruct a new KFC model with 73\% reduction in total number of parameters,while the error only rises mildly. In contrast, using low-rank method can onlyachieve 35\% reduction in total number of parameters given similar qualitydegradation allowance. If we only compare the KFC layer with its counterpartfully-connected layer, the reduction in the number of parameters exceeds 99\%.The amount of computation is also reduced as we replace matrix product of thelarge matrices in FC layers with matrix products of a few smaller matrices inKFC layers. Further experiments on MNIST, SVHN and some Chinese Characterrecognition models also demonstrate effectiveness of our technique.
arxiv-11700-235 | Banzhaf Random Forests | http://arxiv.org/pdf/1507.06105v1.pdf | author:Jianyuan Sun, Guoqiang Zhong, Junyu Dong, Yajuan Cai category:cs.LG cs.CV stat.ML published:2015-07-22 summary:Random forests are a type of ensemble method which makes predictions bycombining the results of several independent trees. However, the theory ofrandom forests has long been outpaced by their application. In this paper, wepropose a novel random forests algorithm based on cooperative game theory.Banzhaf power index is employed to evaluate the power of each feature bytraversing possible feature coalitions. Unlike the previously used informationgain rate of information theory, which simply chooses the most informativefeature, the Banzhaf power index can be considered as a metric of theimportance of each feature on the dependency among a group of features. Moreimportantly, we have proved the consistency of the proposed algorithm, namedBanzhaf random forests (BRF). This theoretical analysis takes a step towardsnarrowing the gap between the theory and practice of random forests forclassification problems. Experiments on several UCI benchmark data sets showthat BRF is competitive with state-of-the-art classifiers and dramaticallyoutperforms previous consistent random forests. Particularly, it is much moreefficient than previous consistent random forests.
arxiv-11700-236 | Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm | http://arxiv.org/pdf/1412.8307v2.pdf | author:Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, AndrÃ© van Schaik, Jonathan Tapson category:cs.NE cs.CV cs.LG published:2014-12-29 summary:Recent advances in training deep (multi-layer) architectures have inspired arenaissance in neural network use. For example, deep convolutional networks arebecoming the default option for difficult tasks on large datasets, such asimage and speech recognition. However, here we show that error rates below 1%on the MNIST handwritten digit benchmark can be replicated with shallownon-convolutional neural networks. This is achieved by training such networksusing the 'Extreme Learning Machine' (ELM) approach, which also enables a veryrapid training time (~10 minutes). Adding distortions, as is common practisefor MNIST, reduces error rates even further. Our methods are also shown to becapable of achieving less than 5.5% error rates on the NORB image database. Toachieve these results, we introduce several enhancements to the standard ELMalgorithm, which individually and in combination can significantly improveperformance. The main innovation is to ensure each hidden-unit operates only ona randomly sized and positioned patch of each image. This form of random`receptive field' sampling of the input ensures the input weight matrix issparse, with about 90% of weights equal to zero. Furthermore, combining ourmethods with a small number of iterations of a single-batch backpropagationmethod can significantly reduce the number of hidden-units required to achievea particular performance. Our close to state-of-the-art results for MNIST andNORB suggest that the ease of use and accuracy of the ELM algorithm fordesigning a single-hidden-layer neural network classifier should cause it to begiven greater consideration either as a standalone method for simpler problems,or as the final classification stage in deep neural networks applied to moredifficult problems.
arxiv-11700-237 | Discriminative Segmental Cascades for Feature-Rich Phone Recognition | http://arxiv.org/pdf/1507.06073v1.pdf | author:Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu category:cs.CL published:2015-07-22 summary:Discriminative segmental models, such as segmental conditional random fields(SCRFs) and segmental structured support vector machines (SSVMs), have hadsuccess in speech recognition via both lattice rescoring and first-passdecoding. However, such models suffer from slow decoding, hampering the use ofcomputationally expensive features, such as segment neural networks or otherhigh-order features. A typical solution is to use approximate decoding, eitherby beam pruning in a single pass or by beam pruning to generate a latticefollowed by a second pass. In this work, we study discriminative segmentalmodels trained with a hinge loss (i.e., segmental structured SVMs). We showthat beam search is not suitable for learning rescoring models in thisapproach, though it gives good approximate decoding performance when the modelis already well-trained. Instead, we consider an approach inspired bystructured prediction cascades, which use max-marginal pruning to generatelattices. We obtain a high-accuracy phonetic recognition system with severalexpensive feature types: a segment neural network, a second-order languagemodel, and second-order phone boundary features.
arxiv-11700-238 | MixEst: An Estimation Toolbox for Mixture Models | http://arxiv.org/pdf/1507.06065v1.pdf | author:Reshad Hosseini, Mohamadreza Mash'al category:stat.ML cs.LG published:2015-07-22 summary:Mixture models are powerful statistical models used in many applicationsranging from density estimation to clustering and classification. When dealingwith mixture models, there are many issues that the experimenter should beaware of and needs to solve. The MixEst toolbox is a powerful and user-friendlypackage for MATLAB that implements several state-of-the-art approaches toaddress these problems. Additionally, MixEst gives the possibility of usingmanifold optimization for fitting the density model, a feature specific to thistoolbox. MixEst simplifies using and integration of mixture models instatistical models and applications. For developing mixture models of newdensities, the user just needs to provide a few functions for that statisticaldistribution and the toolbox takes care of all the issues regarding mixturemodels. MixEst is available at visionlab.ut.ac.ir/mixest and is fullydocumented and is licensed under GPL.
arxiv-11700-239 | A Neural Conversational Model | http://arxiv.org/pdf/1506.05869v3.pdf | author:Oriol Vinyals, Quoc Le category:cs.CL published:2015-06-19 summary:Conversational modeling is an important task in natural languageunderstanding and machine intelligence. Although previous approaches exist,they are often restricted to specific domains (e.g., booking an airline ticket)and require hand-crafted rules. In this paper, we present a simple approach forthis task which uses the recently proposed sequence to sequence framework. Ourmodel converses by predicting the next sentence given the previous sentence orsentences in a conversation. The strength of our model is that it can betrained end-to-end and thus requires much fewer hand-crafted rules. We findthat this straightforward model can generate simple conversations given a largeconversational training dataset. Our preliminary results suggest that, despiteoptimizing the wrong objective function, the model is able to converse well. Itis able extract knowledge from both a domain specific dataset, and from alarge, noisy, and general domain dataset of movie subtitles. On adomain-specific IT helpdesk dataset, the model can find a solution to atechnical problem via conversations. On a noisy open-domain movie transcriptdataset, the model can perform simple forms of common sense reasoning. Asexpected, we also find that the lack of consistency is a common failure mode ofour model.
arxiv-11700-240 | Dependent Types for Pragmatics | http://arxiv.org/pdf/1410.4639v3.pdf | author:Darryl McAdams, Jonathan Sterling category:cs.CL published:2014-10-17 summary:This paper proposes the use of dependent types for pragmatic phenomena suchas pronoun binding and presupposition resolution as a type-theoreticalternative to formalisms such as Discourse Representation Theory and DynamicSemantics.
arxiv-11700-241 | Elastic Net Procedure for Partially Linear Models | http://arxiv.org/pdf/1507.06032v1.pdf | author:Chunhong Li, Dengxiang Huang, Hongshuai Dai, Xinxing Wei category:stat.ME math.PR stat.ML published:2015-07-22 summary:Variable selection plays an important role in the high-dimensional dataanalysis. However the high-dimensional data often induces the stronglycorrelated variables problem. In this paper, we propose Elastic Net procedurefor partially linear models and prove the group effect of its estimate. By asimulation study, we show that the strongly correlated variables problem can bebetter handled by the Elastic Net procedure than Lasso, ALasso and Ridge. Basedon an empirical analysis, we can get that the Elastic Net procedure isparticularly useful when the number of predictors $p$ is much bigger than thesample size $n$.
arxiv-11700-242 | The challenges of SVM optimization using Adaboost on a phoneme recognition problem | http://arxiv.org/pdf/1507.06028v1.pdf | author:Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG published:2015-07-22 summary:The use of digital technology is growing at a very fast pace which led to theemergence of systems based on the cognitive infocommunications. The expansionof this sector impose the use of combining methods in order to ensure therobustness in cognitive systems.
arxiv-11700-243 | Incorporating Belief Function in SVM for Phoneme Recognition | http://arxiv.org/pdf/1507.06025v1.pdf | author:Rimah Amami, Dorra Ben Ayed, Nouerddine Ellouze category:cs.CL cs.LG published:2015-07-22 summary:The Support Vector Machine (SVM) method has been widely used in numerousclassification tasks. The main idea of this algorithm is based on the principleof the margin maximization to find an hyperplane which separates the data intotwo different classes.In this paper, SVM is applied to phoneme recognitiontask. However, in many real-world problems, each phoneme in the data set forrecognition problems may differ in the degree of significance due to noise,inaccuracies, or abnormal characteristics; All those problems can lead to theinaccuracies in the prediction phase. Unfortunately, the standard formulationof SVM does not take into account all those problems and, in particular, thevariation in the speech input. This paper presents a new formulation of SVM(B-SVM) that attributes to each phoneme a confidence degree computed based onits geometric position in the space. Then, this degree is used in order tostrengthen the class membership of the tested phoneme. Hence, we introduce areformulation of the standard SVM that incorporates the degree of belief.Experimental performance on TIMIT database shows the effectiveness of theproposed method B-SVM on a phoneme recognition problem.
arxiv-11700-244 | Robust speech recognition using consensus function based on multi-layer networks | http://arxiv.org/pdf/1507.06023v1.pdf | author:Rimah Amami, Ghaith Manita, Abir Smiti category:cs.CL cs.LG published:2015-07-22 summary:The clustering ensembles mingle numerous partitions of a specified data intoa single clustering solution. Clustering ensemble has emerged as a potentapproach for ameliorating both the forcefulness and the stability ofunsupervised classification results. One of the major problems in clusteringensembles is to find the best consensus function. Finding final partition fromdifferent clustering results requires skillfulness and robustness of theclassification algorithm. In addition, the major problem with the consensusfunction is its sensitivity to the used data sets quality. This limitation isdue to the existence of noisy, silence or redundant data. This paper proposes anovel consensus function of cluster ensembles based on Multilayer networkstechnique and a maintenance database method. This maintenance database approachis used in order to handle any given noisy speech and, thus, to guarantee thequality of databases. This can generates good results and efficient datapartitions. To show its effectiveness, we support our strategy with empiricalevaluation using distorted speech from Aurora speech databases.
arxiv-11700-245 | An Empirical Comparison of SVM and Some Supervised Learning Algorithms for Vowel recognition | http://arxiv.org/pdf/1507.06021v1.pdf | author:Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG published:2015-07-22 summary:In this article, we conduct a study on the performance of some supervisedlearning algorithms for vowel recognition. This study aims to compare theaccuracy of each algorithm. Thus, we present an empirical comparison betweenfive supervised learning classifiers and two combined classifiers: SVM, KNN,Naive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithmswere tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstralcoefficients (MFCCs).
arxiv-11700-246 | Practical Selection of SVM Supervised Parameters with Different Feature Representations for Vowel Recognition | http://arxiv.org/pdf/1507.06020v1.pdf | author:Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG published:2015-07-22 summary:It is known that the classification performance of Support Vector Machine(SVM) can be conveniently affected by the different parameters of the kerneltricks and the regularization parameter, C. Thus, in this article, we propose astudy in order to find the suitable kernel with which SVM may achieve goodgeneralization performance as well as the parameters to use. We need to analyzethe behavior of the SVM classifier when these parameters take very small orvery large values. The study is conducted for a multi-class vowel recognitionusing the TIMIT corpus. Furthermore, for the experiments, we used differentfeature representations such as MFCC and PLP. Finally, a comparative study wasdone to point out the impact of the choice of the parameters, kernel trick andfeature representations on the performance of the SVM classifier
arxiv-11700-247 | The Population Posterior and Bayesian Inference on Streams | http://arxiv.org/pdf/1507.05253v2.pdf | author:James McInerney, Rajesh Ranganath, David M. Blei category:stat.ML published:2015-07-19 summary:Many modern data analysis problems involve inferences from streaming data.However, streaming data is not easily amenable to the standard probabilisticmodeling approaches, which assume that we condition on finite data. We developpopulation variational Bayes, a new approach for using Bayesian modeling toanalyze streams of data. It approximates a new type of distribution, thepopulation posterior, which combines the notion of a population distribution ofthe data with Bayesian inference in a probabilistic model. We study our methodwith latent Dirichlet allocation and Dirichlet process mixtures on severallarge-scale data sets.
arxiv-11700-248 | On the Worst-Case Approximability of Sparse PCA | http://arxiv.org/pdf/1507.05950v1.pdf | author:Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein category:stat.ML cs.CC cs.DS cs.LG published:2015-07-21 summary:It is well known that Sparse PCA (Sparse Principal Component Analysis) isNP-hard to solve exactly on worst-case instances. What is the complexity ofsolving Sparse PCA approximately? Our contributions include: 1) a simple andefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardnessof approximation to within $(1-\varepsilon)$, for some small constant$\varepsilon > 0$; 3) SSE-hardness of approximation to within any constantfactor; and 4) an $\exp\exp\left(\Omega\left(\sqrt{\log \log n}\right)\right)$("quasi-quasi-polynomial") gap for the standard semidefinite program.
arxiv-11700-249 | The Cumulative Distribution Transform and Linear Pattern Classification | http://arxiv.org/pdf/1507.05936v1.pdf | author:Se Rim Park, Soheil Kolouri, Shinjini Kundu, Gustavo Rohde category:cs.CV published:2015-07-21 summary:Classifying (determining the label) of data emanating from sensors is animportant problem with many applications in science and technology. We describea new transform for patterns that can be interpreted as a probability densityfunction, that has special properties with regards to classification. Thetransform, which we denote as the Cumulative Distribution Transform (CDT) isinvertible, with well defined forward and inverse operations. We show that itcan be useful in 'parsing out' variations (confounds) that are 'Lagrangian'(displacement or transport) by converting these to 'Eulerian' variations intransform domain. This conversion is the basis for our main result thatdescribes when the CDT can allow for linear classification to be possible insignal domain. We also describe several properties of the transform and show,with computational experiments that used both real and simulated data, that theCDT can help render a variety of real world problems simpler to solve.
arxiv-11700-250 | Sketch-a-Net that Beats Humans | http://arxiv.org/pdf/1501.07873v3.pdf | author:Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales category:cs.CV cs.NE published:2015-01-30 summary:We propose a multi-scale multi-channel deep neural network framework that,for the first time, yields sketch recognition performance surpassing that ofhumans. Our superior performance is a result of explicitly embedding the uniquecharacteristics of sketches in our model: (i) a network architecture designedfor sketch rather than natural photo statistics, (ii) a multi-channelgeneralisation that encodes sequential ordering in the sketching process, and(iii) a multi-scale network ensemble with joint Bayesian fusion that accountsfor the different levels of abstraction exhibited in free-hand sketches. Weshow that state-of-the-art deep networks specifically engineered for photos ofnatural objects fail to perform well on sketch recognition, regardless whetherthey are trained using photo or sketch. Our network on the other hand not onlydelivers the best performance on the largest human sketch dataset to date, butalso is small in size making efficient training possible using just CPUs.
arxiv-11700-251 | A study of the classification of low-dimensional data with supervised manifold learning | http://arxiv.org/pdf/1507.05880v1.pdf | author:Elif Vural, Christine Guillemot category:cs.LG published:2015-07-21 summary:Supervised manifold learning methods learn data representations by preservingthe geometric structure of data while enhancing the separation between datasamples from different classes. In this paper, we propose a theoretical studyof supervised manifold learning for classification. We first focus on thesupervised Laplacian eigenmaps algorithm and study the conditions under whichthis method computes low-dimensional embeddings where different classes becomelinearly separable. We then consider arbitrary supervised manifold learningalgorithms that compute a linearly separable embedding and study the accuracyof the classifiers given by the out-of-sample extensions of these embeddings.We characterize the classification accuracy in terms of several parameters ofthe classifier such as the separation between different classes in theembedding, the regularity of the interpolation function and the number oftraining samples. The proposed analysis is supported by experiments onsynthetic and real data and has potential for guiding the design of classifiersfor intrinsically low-dimensional data.
arxiv-11700-252 | Kernel convolution model for decoding sounds from time-varying neural responses | http://arxiv.org/pdf/1507.05869v1.pdf | author:Ali Faisal, Anni Nora, Jaeho Seol, Hanna Renvall, Riitta Salmelin category:stat.ML q-bio.NC published:2015-07-21 summary:In this study we present a kernel based convolution model to characterizeneural responses to natural sounds by decoding their time-varying acousticfeatures. The model allows to decode natural sounds from high-dimensionalneural recordings, such as magnetoencephalography (MEG), that track timing andlocation of human cortical signalling noninvasively across multiple channels.We used the MEG responses recorded from subjects listening to acousticallydifferent environmental sounds. By decoding the stimulus frequencies from theresponses, our model was able to accurately distinguish between two differentsounds that it had never encountered before with 70% accuracy. Convolutionmodels typically decode frequencies that appear at a certain time point in thesound signal by using neural responses from that time point until a certainfixed duration of the response. Using our model, we evaluated several fixeddurations (time-lags) of the neural responses and observed auditory MEGresponses to be most sensitive to spectral content of the sounds at time-lagsof 250 ms to 500 ms. The proposed model should be useful for determining whataspects of natural sounds are represented by high-dimensional neural responsesand may reveal novel properties of neural signals.
arxiv-11700-253 | Data Representation using the Weyl Transform | http://arxiv.org/pdf/1412.6134v5.pdf | author:Qiang Qiu, Andrew Thompson, Robert Calderbank, Guillermo Sapiro category:cs.CV stat.ML published:2014-12-18 summary:The Weyl transform is introduced as a rich framework for data representation.Transform coefficients are connected to the Walsh-Hadamard transform ofmultiscale autocorrelations, and different forms of dyadic periodicity in asignal are shown to appear as different features in its Weyl coefficients. TheWeyl transform has a high degree of symmetry with respect to a large group ofmultiscale transformations, which allows compact yet discriminativerepresentations to be obtained by pooling coefficients. The effectiveness ofthe Weyl transform is demonstrated through the example of textured imageclassification.
arxiv-11700-254 | Detecting Internet Filtering from Geographic Time Series | http://arxiv.org/pdf/1507.05819v1.pdf | author:Joss Wright, Alexander Darer, Oliver Farnan category:cs.CY cs.LG cs.NI published:2015-07-21 summary:We propose an approach based on principle component analysis to identifyper-country anomalous periods in traffic usage as a means to detect internetfiltering, and demonstrate the applicability of this approach with global usagestatistics from the Tor Project. In contrast to previous country-specificinvestigations, our techniques use deviation from global patterns of usage toidentify countries straying from predicted behaviour, allowing theidentification of periods of filtering and related events in any country forwhich usage statistics exist. To our knowledge the work presented here is thefirst automated approach to detecting internet filtering at a global scale. We demonstrate the applicability of our approach by identifying knownhistorical filtering events as well as events injected synthetically into adataset, and evaluate the sensitivity of this technique against differentclasses of censorship events. Importantly, our results show that usage ofcircumvention tools, such as those provided by the Tor Project, act not only asdirect indicators of network censorship but also as a meaningful proxy variablefor related events such as protests in which internet use is restricted.
arxiv-11700-255 | Bandit-Based Task Assignment for Heterogeneous Crowdsourcing | http://arxiv.org/pdf/1507.05800v1.pdf | author:Hao Zhang, Yao Ma, Masashi Sugiyama category:cs.LG published:2015-07-21 summary:We consider a task assignment problem in crowdsourcing, which is aimed atcollecting as many reliable labels as possible within a limited budget. Achallenge in this scenario is how to cope with the diversity of tasks and thetask-dependent reliability of workers, e.g., a worker may be good atrecognizing the name of sports teams, but not be familiar with cosmeticsbrands. We refer to this practical setting as heterogeneous crowdsourcing. Inthis paper, we propose a contextual bandit formulation for task assignment inheterogeneous crowdsourcing, which is able to deal with theexploration-exploitation trade-off in worker selection. We also theoreticallyinvestigate the regret bounds for the proposed method, and demonstrate itspractical usefulness experimentally.
arxiv-11700-256 | Tri-Subject Kinship Verification: Understanding the Core of A Family | http://arxiv.org/pdf/1501.02555v3.pdf | author:Xiaoqian Qin, Xiaoyang Tan, Songcan Chen category:cs.CV published:2015-01-12 summary:One major challenge in computer vision is to go beyond the modeling ofindividual objects and to investigate the bi- (one-versus-one) or tri-(one-versus-two) relationship among multiple visual entities, answering suchquestions as whether a child in a photo belongs to given parents. Thechild-parents relationship plays a core role in a family and understanding suchkin relationship would have fundamental impact on the behavior of an artificialintelligent agent working in the human world. In this work, we tackle theproblem of one-versus-two (tri-subject) kinship verification and ourcontributions are three folds: 1) a novel relative symmetric bilinear model(RSBM) introduced to model the similarity between the child and the parents, byincorporating the prior knowledge that a child may resemble a particular parentmore than the other; 2) a spatially voted method for feature selection, whichjointly selects the most discriminative features for the child-parents pair,while taking local spatial information into account; 3) a large scaletri-subject kinship database characterized by over 1,000 child-parentsfamilies. Extensive experiments on KinFaceW, Family101 and our newly releasedkinship database show that the proposed method outperforms several previousstate of the art methods, while could also be used to significantly boost theperformance of one-versus-one kinship verification when the information aboutboth parents are available.
arxiv-11700-257 | Gradient Importance Sampling | http://arxiv.org/pdf/1507.05781v1.pdf | author:Ingmar Schuster category:stat.ML published:2015-07-21 summary:Adaptive Monte Carlo schemes developed over the last years usually seek toensure ergodicity of the sampling process in line with MCMC tradition. Thisposes constraints on what is possible in terms of adaptation. In the generalcase ergodicity can only be guaranteed if adaptation is diminished at a certainrate. Importance Sampling approaches offer a way to circumvent this limitationand design sampling algorithms that keep adapting. Here I present a gradientinformed variant of SMC (and its special case Population Monte Carlo) forstatic problems.
arxiv-11700-258 | Understanding Intra-Class Knowledge Inside CNN | http://arxiv.org/pdf/1507.02379v2.pdf | author:Donglai Wei, Bolei Zhou, Antonio Torrabla, William Freeman category:cs.CV published:2015-07-09 summary:Convolutional Neural Network (CNN) has been successful in image recognitiontasks, and recent works shed lights on how CNN separates different classes withthe learned inter-class knowledge through visualization. In this work, weinstead visualize the intra-class knowledge inside CNN to better understand howan object class is represented in the fully-connected layers. To invert the intra-class knowledge into more interpretable images, wepropose a non-parametric patch prior upon previous CNN visualization models.With it, we show how different "styles" of templates for an object class areorganized by CNN in terms of location and content, and represented in ahierarchical and ensemble way. Moreover, such intra-class knowledge can be usedin many interesting applications, e.g. style-based image retrieval andstyle-based object completion.
arxiv-11700-259 | Online Metric-Weighted Linear Representations for Robust Visual Tracking | http://arxiv.org/pdf/1507.05737v1.pdf | author:Xi Li, Chunhua Shen, Anthony Dick, Zhongfei Zhang, Yueting Zhuang category:cs.CV published:2015-07-21 summary:In this paper, we propose a visual tracker based on a metric-weighted linearrepresentation of appearance. In order to capture the interdependence ofdifferent feature dimensions, we develop two online distance metric learningmethods using proximity comparison information and structured output learning.The learned metric is then incorporated into a linear representation ofappearance. We show that online distance metric learning significantly improves therobustness of the tracker, especially on those sequences exhibiting drasticappearance changes. In order to bound growth in the number of training samples,we design a time-weighted reservoir sampling method. Moreover, we enable our tracker to automatically perform objectidentification during the process of object tracking, by introducing acollection of static template samples belonging to several object classes ofinterest. Object identification results for an entire video sequence areachieved by systematically combining the tracking information and visualrecognition at each frame. Experimental results on challenging video sequencesdemonstrate the effectiveness of the method for both inter-frame tracking andobject identification.
arxiv-11700-260 | Rule Of Thumb: Deep derotation for improved fingertip detection | http://arxiv.org/pdf/1507.05726v1.pdf | author:Aaron Wetzler, Ron Slossberg, Ron Kimmel category:cs.CV published:2015-07-21 summary:We investigate a novel global orientation regression approach for articulatedobjects using a deep convolutional neural network. This is integrated with anin-plane image derotation scheme, DeROT, to tackle the problem of per-framefingertip detection in depth images. The method reduces the complexity oflearning in the space of articulated poses which is demonstrated by using twodistinct state-of-the-art learning based hand pose estimation methods appliedto fingertip detection. Significant classification improvements are shown overthe baseline implementation. Our framework involves no tracking, kinematicconstraints or explicit prior model of the articulated object in hand. Tosupport our approach we also describe a new pipeline for high accuracy magneticannotation and labeling of objects imaged by a depth camera.
arxiv-11700-261 | Gene expression modelling across multiple cell-lines with MapReduce | http://arxiv.org/pdf/1507.05720v1.pdf | author:David M. Budden, Edmund J. Crampin category:q-bio.QM cs.DC q-bio.GN stat.ML published:2015-07-21 summary:With the wealth of high-throughput sequencing data generated by recentlarge-scale consortia, predictive gene expression modelling has become animportant tool for integrative analysis of transcriptomic and epigenetic data.However, sequencing data-sets are characteristically large, and previouslymodelling frameworks are typically inefficient and unable to leveragemulti-core or distributed processing architectures. In this study, we detail anefficient and parallelised MapReduce implementation of gene expressionmodelling. We leverage the computational efficiency of this framework toprovide an integrative analysis of over fifty histone modification data-setsacross a variety of cancerous and non-cancerous cell-lines. Our resultsdemonstrate that the genome-wide relationships between histone modificationsand mRNA transcription are lineage, tissue and karyotype-invariant, and thatmodels trained on matched epigenetic/transcriptomic data from non-cancerouscell-lines are able to predict cancerous expression with equivalent genome-widefidelity.
arxiv-11700-262 | An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition | http://arxiv.org/pdf/1507.05717v1.pdf | author:Baoguang Shi, Xiang Bai, Cong Yao category:cs.CV published:2015-07-21 summary:Image-based sequence recognition has been a long-standing research topic incomputer vision. In this paper, we investigate the problem of scene textrecognition, which is among the most important and challenging tasks inimage-based sequence recognition. A novel neural network architecture, whichintegrates feature extraction, sequence modeling and transcription into aunified framework, is proposed. Compared with previous systems for scene textrecognition, the proposed architecture possesses four distinctive properties:(1) It is end-to-end trainable, in contrast to most of the existing algorithmswhose components are separately trained and tuned. (2) It naturally handlessequences in arbitrary lengths, involving no character segmentation orhorizontal scale normalization. (3) It is not confined to any predefinedlexicon and achieves remarkable performances in both lexicon-free andlexicon-based scene text recognition tasks. (4) It generates an effective yetmuch smaller model, which is more practical for real-world applicationscenarios. The experiments on standard benchmarks, including the IIIT-5K,Street View Text and ICDAR datasets, demonstrate the superiority of theproposed algorithm over the prior arts. Moreover, the proposed algorithmperforms well in the task of image-based music score recognition, whichevidently verifies the generality of it.
arxiv-11700-263 | Macrostate mixture models for probabilistic multiscale nonparametric kernelized spectral clustering | http://arxiv.org/pdf/1502.00727v3.pdf | author:Daniel Korenblum category:stat.ML published:2015-02-03 summary:Automating the discovery of meaningful structures in large complex datasetsis an important problem in many application areas including machine learning,source separation, and dimensionality reduction. Mixture models are onecategory of methods for discovering structure using convex sums of probabilitydistributions to represent structures or clusters in data. Spectral clusteringis another category of methods where eigenspaces of Laplacian matrices are usedprior to or as part of the clustering process. Macrostate theory definesnonparametric mixture models directly from Laplacian eigensystems, providing aconnection between nonhierarchical spectral clustering and nonparametricmixture modeling. Unlike other spectral clustering methods, macrostates areself-contained and predict both the appropriate number of mixture componentsand the cluster assignment distributions directly from Laplacian eigensystems.Macrostates reduce the number of input parameters and steps required comparedto other spectral clustering methods and avoid issues of explicit densityestimation in higher dimensional input data spaces. Previous formulations usedcustomized algorithms to compute macrostate clustering solutions, limitingtheir practical accessibility. The new formulation presented here depends onlyon standardized linear programming solvers and is very easily parallelized,improving the practicality and performance compared to previous formulations.Numerical examples compare the performance of other finite mixture modeling andspectral clustering methods to macrostate clustering.
arxiv-11700-264 | A neuromorphic hardware architecture using the Neural Engineering Framework for pattern recognition | http://arxiv.org/pdf/1507.05695v1.pdf | author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE published:2015-07-21 summary:We present a hardware architecture that uses the Neural Engineering Framework(NEF) to implement large-scale neural networks on Field Programmable GateArrays (FPGAs) for performing pattern recognition in real time. NEF is aframework that is capable of synthesising large-scale cognitive systems fromsubnetworks. We will first present the architecture of the proposed neuralnetwork implemented using fixed-point numbers and demonstrate a routine thatcomputes the decoding weights by using the online pseudoinverse update method(OPIUM) in a parallel and distributed manner. The proposed system isefficiently implemented on a compact digital neural core. This neural coreconsists of 64 neurons that are instantiated by a single physical neuron usinga time-multiplexing approach. As a proof of concept, we combined 128 identicalneural cores together to build a handwritten digit recognition system using theMNIST database and achieved a recognition rate of 96.55%. The system isimplemented on a state-of-the-art FPGA and can process 5.12 million digits persecond. The architecture is not limited to handwriting recognition, but isgenerally applicable as an extremely fast pattern recognition processor forvarious kinds of patterns such as speech and images.
arxiv-11700-265 | Multi-Objective Optimization for Self-Adjusting Weighted Gradient in Machine Learning Tasks | http://arxiv.org/pdf/1506.01113v2.pdf | author:Conrado Silva Miranda, Fernando JosÃ© Von Zuben category:stat.ML cs.LG published:2015-06-03 summary:Much of the focus in machine learning research is placed in creating newarchitectures and optimization methods, but the overall loss function is seldomquestioned. This paper interprets machine learning from a multi-objectiveoptimization perspective, showing the limitations of the default linearcombination of loss functions over a data set and introducing the hypervolumeindicator as an alternative. It is shown that the gradient of the hypervolumeis defined by a self-adjusting weighted mean of the individual loss gradients,making it similar to the gradient of a weighted mean loss but without requiringthe weights to be defined a priori. This enables an inner boosting-likebehavior, where the current model is used to automatically place higher weightson samples with higher losses but without requiring the use of multiple models.Results on a denoising autoencoder show that the new formulation is able toachieve better mean loss than the direct optimization of the mean loss,providing evidence to the conjecture that self-adjusting the weights creates asmoother loss surface.
arxiv-11700-266 | Notes About a More Aware Dependency Parser | http://arxiv.org/pdf/1507.05630v1.pdf | author:Matteo Grella category:cs.CL published:2015-07-20 summary:In this paper I explain the reasons that led me to research and conceive anovel technology for dependency parsing, mixing together the strengths ofdata-driven transition-based and constraint-based approaches. In particular Ihighlight the problem to infer the reliability of the results of a data-driventransition-based parser, which is extremely important for high-level processesthat expect to use correct parsing results. I then briefly introduce a numberof notes about a new parser model I'm working on, capable to proceed with theanalysis in a "more aware" way, with a more "robust" concept of robustness.
arxiv-11700-267 | A semidefinite program for unbalanced multisection in the stochastic block model | http://arxiv.org/pdf/1507.05605v1.pdf | author:William Perry, Alexander S. Wein category:cs.DS math.PR stat.ML 68 published:2015-07-20 summary:We analyze semidefinite programming (SDP) algorithms that exactly recovercommunity structure in graphs generated from the stochastic block model. Inthis model, a graph is randomly generated on a vertex set that is partitionedinto multiple communities of potentially different sizes, where edges are moreprobable within communities than between communities. We achieve exact recoveryof the community structure, up to the information-theoretic limits determinedby Abbe and Sandon. By virtue of a semidefinite approach, our algorithmssucceed against a semirandom form of the stochastic block model, guaranteeinggeneralization to scenarios with radically different noise structure.
arxiv-11700-268 | Parallel Correlation Clustering on Big Graphs | http://arxiv.org/pdf/1507.05086v2.pdf | author:Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchandran, Michael I. Jordan category:cs.DC cs.DS stat.ML published:2015-07-17 summary:Given a similarity graph between items, correlation clustering (CC) groupssimilar items together and dissimilar ones apart. One of the most popular CCalgorithms is KwikCluster: an algorithm that serially clusters neighborhoods ofvertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster inpractice requires a large number of clustering rounds, a potential bottleneckfor large graphs. We present C4 and ClusterWild!, two algorithms for parallel correlationclustering that run in a polylogarithmic number of rounds and achieve nearlylinear speedups, provably. C4 uses concurrency control to enforceserializability of a parallel clustering process, and guarantees a3-approximation ratio. ClusterWild! is a coordination free algorithm thatabandons consistency for the benefit of better scaling; this leads to aprovably small loss in the 3-approximation ratio. We provide extensive experimental results for both algorithms, where weoutperform the state of the art, both in terms of clustering accuracy andrunning time. We show that our algorithms can cluster billion-edge graphs inunder 5 seconds on 32 cores, while achieving a 15x speedup.
arxiv-11700-269 | 3D Pose from Detections | http://arxiv.org/pdf/1502.04754v3.pdf | author:Cosimo Rubino, Marco Crocco, Alessandro Perina, Vittorio Murino, Alessio Del Bue category:cs.CV published:2015-02-17 summary:We present a novel method to infer, in closed-form, a general 3D spatialoccupancy and orientation of a collection of rigid objects given 2D imagedetections from a sequence of images. In particular, starting from 2D ellipsesfitted to bounding boxes, this novel multi-view problem can be reformulated asthe estimation of a quadric (ellipsoid) in 3D. We show that an efficientsolution exists in the dual-space using a minimum of three views while asolution with two views is possible through the use of regularization. However,this algebraic solution can be negatively affected in the presence of grossinaccuracies in the bounding boxes estimation. To this end, we also propose arobust ellipse fitting algorithm able to improve performance in the presence oferrors in the detected objects. Results on synthetic tests and on differentreal datasets, involving real challenging scenarios, demonstrate theapplicability and potential of our method.
arxiv-11700-270 | Subspace Alignment Based Domain Adaptation for RCNN Detector | http://arxiv.org/pdf/1507.05578v1.pdf | author:Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV published:2015-07-20 summary:In this paper, we propose subspace alignment based domain adaptation of thestate of the art RCNN based object detector. The aim is to be able to achievehigh quality object detection in novel, real world target scenarios withoutrequiring labels from the target domain. While, unsupervised domain adaptationhas been studied in the case of object classification, for object detection ithas been relatively unexplored. In subspace based domain adaptation forobjects, we need access to source and target subspaces for the bounding boxfeatures. The absence of supervision (labels and bounding boxes are absent)makes the task challenging. In this paper, we show that we can still adapt sub-spaces that are localized to the object by obtaining detections from the RCNNdetector trained on source and applied on target. Then we form localizedsubspaces from the detections and show that subspace alignment based adaptationbetween these subspaces yields improved object detection. This evaluation isdone by considering challenging real world datasets of PASCAL VOC as source andvalidation set of Microsoft COCO dataset as target for various categories.
arxiv-11700-271 | How to Generate a Good Word Embedding? | http://arxiv.org/pdf/1507.05523v1.pdf | author:Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao category:cs.CL published:2015-07-20 summary:We analyze three critical components of word embedding training: the model,the corpus, and the training parameters. We systematize existingneural-network-based word embedding algorithms and compare them using the samecorpus. We evaluate each word embedding in three ways: analyzing its semanticproperties, using it as a feature for supervised tasks and using it toinitialize neural networks. We also provide several simple guidelines fortraining word embeddings. First, we discover that corpus domain is moreimportant than corpus size. We recommend choosing a corpus in a suitable domainfor the desired task, after that, using a larger corpus yields better results.Second, we find that faster models provide sufficient performance in mostcases, and more complex models can be used if the training corpus issufficiently large. Third, the early stopping metric for iterating should relyon the development set of the desired task rather than the validation loss oftraining embedding.
arxiv-11700-272 | Community detection in multiplex networks using locally adaptive random walks | http://arxiv.org/pdf/1507.01890v2.pdf | author:Zhana Kuncheva, Giovanni Montana category:cs.SI physics.soc-ph stat.ML published:2015-07-06 summary:Multiplex networks, a special type of multilayer networks, are increasinglyapplied in many domains ranging from social media analytics to biology. Acommon task in these applications concerns the detection of communitystructures. Many existing algorithms for community detection in multiplexesattempt to detect communities which are shared by all layers. In this articlewe propose a community detection algorithm, LART (Locally Adaptive RandomTransitions), for the detection of communities that are shared by either someor all the layers in the multiplex. The algorithm is based on a random walk onthe multiplex, and the transition probabilities defining the random walk areallowed to depend on the local topological similarity between layers at anygiven node so as to facilitate the exploration of communities across layers.Based on this random walk, a node dissimilarity measure is derived and nodesare clustered based on this distance in a hierarchical fashion. We presentexperimental results using networks simulated under various scenarios toshowcase the performance of LART in comparison to related community detectionalgorithms.
arxiv-11700-273 | On the Minimax Risk of Dictionary Learning | http://arxiv.org/pdf/1507.05498v1.pdf | author:Alexander Jung, Yonina C. Eldar, Norbert GÃ¶rtz category:stat.ML cs.IT cs.LG math.IT published:2015-07-20 summary:We consider the problem of learning a dictionary matrix from a number ofobserved signals, which are assumed to be generated via a linear model with acommon underlying dictionary. In particular, we derive lower bounds on theminimum achievable worst case mean squared error (MSE), regardless ofcomputational complexity of the dictionary learning (DL) schemes. By casting DLas a classical (or frequentist) estimation problem, the lower bounds on theworst case MSE are derived by following an established information-theoreticapproach to minimax estimation. The main conceptual contribution of this paperis the adaption of the information-theoretic approach to minimax estimation forthe DL problem in order to derive lower bounds on the worst case MSE of any DLscheme. We derive three different lower bounds applying to different generativemodels for the observed signals. The first bound applies to a wide range ofmodels, it only requires the existence of a covariance matrix of the (unknown)underlying coefficient vector. By specializing this bound to the case of sparsecoefficient distributions, and assuming the true dictionary satisfies therestricted isometry property, we obtain a lower bound on the worst case MSE ofDL schemes in terms of a signal to noise ratio (SNR). The third bound appliesto a more restrictive subclass of coefficient distributions by requiring thenon-zero coefficients to be Gaussian. While, compared with the previous twobounds, the applicability of this final bound is the most limited it is thetightest of the three bounds in the low SNR regime.
arxiv-11700-274 | Efficient moving point handling for incremental 3D manifold reconstruction | http://arxiv.org/pdf/1507.05489v1.pdf | author:Andrea Romanoni, Matteo Matteucci category:cs.CV published:2015-07-20 summary:As incremental Structure from Motion algorithms become effective, a goodsparse point cloud representing the map of the scene becomes availableframe-by-frame. From the 3D Delaunay triangulation of these points,state-of-the-art algorithms build a manifold rough model of the scene. Thesealgorithms integrate incrementally new points to the 3D reconstruction only iftheir position estimate does not change. Indeed, whenever a point moves in a 3DDelaunay triangulation, for instance because its estimation gets refined, a setof tetrahedra have to be removed and replaced with new ones to maintain theDelaunay property; the management of the manifold reconstruction becomes thuscomplex and it entails a potentially big overhead. In this paper we investigatedifferent approaches and we propose an efficient policy to deal with movingpoints in the manifold estimation process. We tested our approach with foursequences of the KITTI dataset and we show the effectiveness of our proposal incomparison with state-of-the-art approaches.
arxiv-11700-275 | Scalable Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/pdf/1502.03529v3.pdf | author:Shen-Yi Zhao, Wu-Jun Li, Zhi-Hua Zhou category:cs.LG published:2015-02-12 summary:Stochastic alternating direction method of multipliers (ADMM), which visitsonly one sample or a mini-batch of samples each time, has recently been provedto achieve better performance than batch ADMM. However, most stochastic methodscan only achieve a convergence rate $O(1/\sqrt T)$ on general convexproblems,where T is the number of iterations. Hence, these methods are notscalable with respect to convergence rate (computation cost). There exists onlyone stochastic method, called SA-ADMM, which can achieve convergence rate$O(1/T)$ on general convex problems. However, an extra memory is needed forSA-ADMM to store the historic gradients on all samples, and thus it is notscalable with respect to storage cost. In this paper, we propose a novelmethod, called scalable stochastic ADMM(SCAS-ADMM), for large-scaleoptimization and learning problems. Without the need to store the historicgradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the beststochastic method SA-ADMM and batch ADMM on general convex problems.Experiments on graph-guided fused lasso show that SCAS-ADMM can achievestate-of-the-art performance in real applications
arxiv-11700-276 | Bayesian Inference of Graphical Model Structures Using Trees | http://arxiv.org/pdf/1504.02723v3.pdf | author:LoÃ¯c Schwaller, StÃ©phane Robin, Michael Stumpf category:stat.ML published:2015-04-10 summary:We propose to learn the structure of an undirected graphical model bycomputing exact posterior probabilities for local structures in a Bayesianframework. This task would be untractable without any restriction on theconsidered graphs. We limit our exploration to the spanning trees and definepriors on tree structures and parameters that allow fast and exact computationof the posterior probability for an edge to belong to the random tree thanks toan algebraic result called the Matrix-Tree theorem. We show that the assumptionwe have made does not prevent our approach to perform well on synthetic andflow cytometry data.
arxiv-11700-277 | Towards Effective Codebookless Model for Image Classification | http://arxiv.org/pdf/1507.02385v2.pdf | author:Qilong Wang, Peihua Li, Lei Zhang, Wangmeng Zuo category:cs.CV published:2015-07-09 summary:The bag-of-features (BoF) model for image classification has been thoroughlystudied over the last decade. Different from the widely used BoF methods whichmodeled images with a pre-trained codebook, the alternative codebook free imagemodeling method, which we call Codebookless Model (CLM), attracted littleattention. In this paper, we present an effective CLM that represents an imagewith a single Gaussian for classification. By embedding Gaussian manifold intoa vector space, we show that the simple incorporation of our CLM into a linearclassifier achieves very competitive accuracy compared with state-of-the-artBoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensionalRiemannian manifold, we further propose a joint learning method of low-ranktransformation with support vector machine (SVM) classifier on the Gaussianmanifold, in order to reduce computational and storage cost. To study andalleviate the side effect of background clutter on our CLM, we also present asimple yet effective partial background removal method based on saliencydetection. Experiments are extensively conducted on eight widely used databasesto demonstrate the effectiveness and efficiency of our CLM method.
arxiv-11700-278 | Linear Inverse Problems with Norm and Sparsity Constraints | http://arxiv.org/pdf/1507.05370v1.pdf | author:Volkan Cevher, Sina Jafarpour, Anastasios Kyrillidis category:cs.IT math.IT math.OC stat.ML published:2015-07-20 summary:We describe two nonconventional algorithms for linear regression, called GAMEand CLASH. The salient characteristics of these approaches is that they exploitthe convex $\ell_1$-ball and non-convex $\ell_0$-sparsity constraints jointlyin sparse recovery. To establish the theoretical approximation guarantees ofGAME and CLASH, we cover an interesting range of topics from game theory,convex and combinatorial optimization. We illustrate that these approaches leadto improved theoretical guarantees and empirical performance beyond convex andnon-convex solvers alone.
arxiv-11700-279 | Structured Sparsity: Discrete and Convex approaches | http://arxiv.org/pdf/1507.05367v1.pdf | author:Anastasios Kyrillidis, Luca Baldassarre, Marwa El-Halabi, Quoc Tran-Dinh, Volkan Cevher category:cs.IT math.IT math.OC stat.ML published:2015-07-20 summary:Compressive sensing (CS) exploits sparsity to recover sparse or compressiblesignals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsityis also used to enhance interpretability in machine learning and statisticsapplications: While the ambient dimension is vast in modern data analysisproblems, the relevant information therein typically resides in a much lowerdimensional space. However, many solutions proposed nowadays do not leveragethe true underlying structure. Recent results in CS extend the simple sparsityidea to more sophisticated {\em structured} sparsity models, which describe theinterdependency between the nonzero components of a signal, allowing toincrease the interpretability of the results and lead to better recoveryperformance. In order to better understand the impact of structured sparsity,in this chapter we analyze the connections between the discrete models andtheir convex relaxations, highlighting their relative advantages. We start withthe general group sparse model and then elaborate on two important specialcases: the dispersive and the hierarchical models. For each, we present themodels in their discrete nature, discuss how to solve the ensuing discreteproblems and then describe convex relaxations. We also consider more generalstructures as defined by set functions and present their convex proxies.Further, we discuss efficient optimization solutions for structured sparsityproblems and illustrate structured sparsity in action via three applications.
arxiv-11700-280 | Learning Complexity-Aware Cascades for Deep Pedestrian Detection | http://arxiv.org/pdf/1507.05348v1.pdf | author:Zhaowei Cai, Mohammad Saberian, Nuno Vasconcelos category:cs.CV published:2015-07-19 summary:The design of complexity-aware cascaded detectors, combining features of verydifferent complexities, is considered. A new cascade design procedure isintroduced, by formulating cascade learning as the Lagrangian optimization of arisk that accounts for both accuracy and complexity. A boosting algorithm,denoted as complexity aware cascade training (CompACT), is then derived tosolve this optimization. CompACT cascades are shown to seek an optimaltrade-off between accuracy and complexity by pushing features of highercomplexity to the later cascade stages, where only a few difficult candidatepatches remain to be classified. This enables the use of features of vastlydifferent complexities in a single detector. In result, the feature pool can beexpanded to features previously impractical for cascade design, such as theresponses of a deep convolutional neural network (CNN). This is demonstratedthrough the design of a pedestrian detector with a pool of features whosecomplexities span orders of magnitude. The resulting cascade generalizes thecombination of a CNN with an object proposal mechanism: rather than apre-processing stage, CompACT cascades seamlessly integrate CNNs in theirstages. This enables state of the art performance on the Caltech and KITTIdatasets, at fairly fast speeds.
arxiv-11700-281 | Fast Adaptive Weight Noise | http://arxiv.org/pdf/1507.05331v1.pdf | author:Justin Bayer, Maximilian Karl, Daniela Korhammer, Patrick van der Smagt category:stat.ML cs.LG published:2015-07-19 summary:Marginalising out uncertain quantities within the internal representations orparameters of neural networks is of central importance for a wide range oflearning techniques, such as empirical, variational or full Bayesian methods.We set out to generalise fast dropout (Wang & Manning, 2013) to cover a widervariety of noise processes in neural networks. This leads to an efficientcalculation of the marginal likelihood and predictive distribution which evadessampling and the consequential increase in training time due to highly variantgradient estimates. This allows us to approximate variational Bayes for theparameters of feed-forward neural networks. Inspired by the minimum descriptionlength principle, we also propose and experimentally verify the directoptimisation of the regularised predictive distribution. The methods yieldresults competitive with previous neural network based approaches and Gaussianprocesses on a wide range of regression tasks.
arxiv-11700-282 | 2 Notes on Classes with Vapnik-Chervonenkis Dimension 1 | http://arxiv.org/pdf/1507.05307v1.pdf | author:Shai Ben-David category:cs.LG G.2; G.3 published:2015-07-19 summary:The Vapnik-Chervonenkis dimension is a combinatorial parameter that reflectsthe "complexity" of a set of sets (a.k.a. concept classes). It has beenintroduced by Vapnik and Chervonenkis in their seminal 1971 paper and has sincefound many applications, most notably in machine learning theory and incomputational geometry. Arguably the most influential consequence of the VCanalysis is the fundamental theorem of statistical machine learning, statingthat a concept class is learnable (in some precise sense) if and only if itsVC-dimension is finite. Furthermore, for such classes a most simple learningrule - empirical risk minimization (ERM) - is guaranteed to succeed. The simplest non-trivial structures, in terms of the VC-dimension, are theclasses (i.e., sets of subsets) for which that dimension is 1. In this note we show a couple of curious results concerning such classes. Thefirst result shows that such classes share a very simple structure, and, as acorollary, the labeling information contained in any sample labeled by such aclass can be compressed into a single instance. The second result shows that due to some subtle measurability issues, inspite of the above mentioned fundamental theorem, there are classes ofdimension 1 for which an ERM learning rule fails miserably.
arxiv-11700-283 | Handwriting Recognition | http://arxiv.org/pdf/1507.05244v1.pdf | author:Jayati Ghosh Dastidar, Surabhi Sarkar, Rick Punyadyuti Sinha, Kasturi Basu category:cs.CV published:2015-07-19 summary:This paper describes the method to recognize offline handwritten characters.A robust algorithm for handwriting segmentation is described here with the helpof which individual characters can be segmented from a selected word from aparagraph of handwritten text image which is given as input.
arxiv-11700-284 | Hand Gesture Recognition Library | http://arxiv.org/pdf/1507.05243v1.pdf | author:Jonathan Fidelis Paul, Dibyabiva Seth, Cijo Paul, Jayati Ghosh Dastidar category:cs.CV published:2015-07-19 summary:In this paper we have presented a hand gesture recognition library. Variousfunctions include detecting cluster count, cluster orientation, finger pointingdirection, etc. To use these functions first the input image needs to beprocessed into a logical array for which a function has been developed. Thelibrary has been developed keeping flexibility in mind and thus providesapplication developers a wide range of options to develop custom gestures.
arxiv-11700-285 | Theory of Dual-sparse Regularized Randomized Reduction | http://arxiv.org/pdf/1504.03991v4.pdf | author:Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu category:cs.LG stat.ML published:2015-04-15 summary:In this paper, we study randomized reduction methods, which reducehigh-dimensional features into low-dimensional space by randomized methods(e.g., random projection, random hashing), for large-scale high-dimensionalclassification. Previous theoretical results on randomized reduction methodshinge on strong assumptions about the data, e.g., low rank of the data matrixor a large separable margin of classification, which hinder their applicationsin broad domains. To address these limitations, we propose dual-sparseregularized randomized reduction methods that introduce a sparse regularizerinto the reduced dual problem. Under a mild condition that the original dualsolution is a (nearly) sparse vector, we show that the resulting dual solutionis close to the original dual solution and concentrates on its support set. Innumerical experiments, we present an empirical study to support the analysisand we also present a novel application of the dual-sparse regularizedrandomized reduction methods to reducing the communication cost of distributedlearning from large-scale high-dimensional data.
arxiv-11700-286 | Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees | http://arxiv.org/pdf/1507.05185v1.pdf | author:Tianbao Yang, Lijun Zhang, Qihang Lin, Rong Jin category:math.ST cs.CC stat.ML stat.TH published:2015-07-18 summary:In this paper, we study a fast approximation method for {\it large-scalehigh-dimensional} sparse least-squares regression problem by exploiting theJohnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensionalvectors into a low-dimensional space. In particular, we propose to apply the JLtransforms to the data matrix and the target vector and then to solve a sparseleast-squares problem on the compressed data with a {\it slightly largerregularization parameter}. Theoretically, we establish the optimization errorbound of the learned model for two different sparsity-inducing regularizers,i.e., the elastic net and the $\ell_1$ norm. Compared with previous relevantwork, our analysis is {\it non-asymptotic and exhibits more insights} on thebound, the sample complexity and the regularization. As an illustration, wealso provide an error bound of the {\it Dantzig selector} under JL transforms.
arxiv-11700-287 | The Mondrian Process for Machine Learning | http://arxiv.org/pdf/1507.05181v1.pdf | author:Matej Balog, Yee Whye Teh category:stat.ML cs.LG published:2015-07-18 summary:This report is concerned with the Mondrian process and its applications inmachine learning. The Mondrian process is a guillotine-partition-valuedstochastic process that possesses an elegant self-consistency property. Thefirst part of the report uses simple concepts from applied probability todefine the Mondrian process and explore its properties. The Mondrian process has been used as the main building block of a cleveronline random forest classification algorithm that turns out to be equivalentto its batch counterpart. We outline a slight adaptation of this algorithm toregression, as the remainder of the report uses regression as a case study ofhow Mondrian processes can be utilized in machine learning. In particular, theMondrian process will be used to construct a fast approximation to thecomputationally expensive kernel ridge regression problem with a Laplacekernel. The complexity of random guillotine partitions generated by a Mondrianprocess and hence the complexity of the resulting regression models iscontrolled by a lifetime hyperparameter. It turns out that these models can beefficiently trained and evaluated for all lifetimes in a given range at once,without needing to retrain them from scratch for each lifetime value. Thisleads to an efficient procedure for determining the right model complexity fora dataset at hand. The limitation of having a single lifetime hyperparameter will motivate thefinal Mondrian grid model, in which each input dimension is endowed with itsown lifetime parameter. In this model we preserve the property that itshyperparameters can be tweaked without needing to retrain the modified modelfrom scratch.
arxiv-11700-288 | Face Alignment Assisted by Head Pose Estimation | http://arxiv.org/pdf/1507.03148v2.pdf | author:Heng Yang, Wenxuan Mou, Yichi Zhang, Ioannis Patras, Hatice Gunes, Peter Robinson category:cs.CV published:2015-07-11 summary:In this paper we propose a supervised initialization scheme for cascaded facealignment based on explicit head pose estimation. We first investigate thefailure cases of most state of the art face alignment approaches and observethat these failures often share one common global property, i.e. the head posevariation is usually large. Inspired by this, we propose a deep convolutionalnetwork model for reliable and accurate head pose estimation. Instead of usinga mean face shape, or randomly selected shapes for cascaded face alignmentinitialisation, we propose two schemes for generating initialisation: the firstone relies on projecting a mean 3D face shape (represented by 3D faciallandmarks) onto 2D image under the estimated head pose; the second one searchesnearest neighbour shapes from the training set according to head pose distance.By doing so, the initialisation gets closer to the actual shape, which enhancesthe possibility of convergence and in turn improves the face alignmentperformance. We demonstrate the proposed method on the benchmark 300W datasetand show very competitive performance in both head pose estimation and facealignment.
arxiv-11700-289 | Classification with Noisy Labels by Importance Reweighting | http://arxiv.org/pdf/1411.7718v2.pdf | author:Tongliang Liu, Dacheng Tao category:stat.ML cs.LG published:2014-11-27 summary:In this paper, we study a classification problem in which sample labels arerandomly corrupted. In this scenario, there is an unobservable sample withnoise-free labels. However, before being observed, the true labels areindependently flipped with a probability $\rho\in[0,0.5)$, and the random labelnoise can be class-conditional. Here, we address two fundamental problemsraised by this scenario. The first is how to best use the abundant surrogateloss functions designed for the traditional classification problem when thereis label noise. We prove that any surrogate loss function can be used forclassification with noisy labels by using importance reweighting, withconsistency assurance that the label noise does not ultimately hinder thesearch for the optimal classifier of the noise-free sample. The other is theopen problem of how to obtain the noise rate $\rho$. We show that the rate isupper bounded by the conditional probability $P(yx)$ of the noisy sample.Consequently, the rate can be estimated, because the upper bound can be easilyreached in classification problems. Experimental results on synthetic and realdatasets confirm the efficiency of our methods.
arxiv-11700-290 | Persistent Topology of Syntax | http://arxiv.org/pdf/1507.05134v1.pdf | author:Alexander Port, Iulia Gheorghita, Daniel Guth, John M. Clark, Crystal Liang, Shival Dasu, Matilde Marcolli category:cs.CL math.AT 91F20 published:2015-07-18 summary:We study the persistent homology of the data set of syntactic parameters ofthe world languages. We show that, while homology generators behave erraticallyover the whole data set, non-trivial persistent homology appears when onerestricts to specific language families. Different families exhibit differentpersistent homology. We focus on the cases of the Indo-European and theNiger-Congo families, for which we compare persistent homology over differentcluster filtering values. We investigate the possible significance, inhistorical linguistic terms, of the presence of persistent generators of thefirst homology. In particular, we show that the persistent first homologygenerator we find in the Indo-European family is not due (as one might guess)to the Anglo-Norman bridge in the Indo-European phylogenetic network, but isrelated to the position of Ancient Greek and the Hellenic branch within thenetwork.
arxiv-11700-291 | Fast Approximate Bayesian Computation for Estimating Parameters in Differential Equations | http://arxiv.org/pdf/1507.05117v1.pdf | author:Sanmitra Ghosh, Srinandan Dasmahapatra, Koushik Maharatna category:stat.ML published:2015-07-17 summary:Approximate Bayesian computation (ABC) using a sequential Monte Carlo methodprovides a comprehensive platform for parameter estimation, model selection andsensitivity analysis in differential equations. However, this method, likeother Monte Carlo methods, incurs a significant computational cost as itrequires explicit numerical integration of differential equations to carry outinference. In this paper we propose a novel method for circumventing therequirement of explicit integration by using derivatives of Gaussian processesto smooth the observations from which parameters are estimated. We evaluate ourmethods using synthetic data generated from model biological systems describedby ordinary and delay differential equations. Upon comparing the performance ofour method to existing ABC techniques, we demonstrate that it producescomparably reliable parameter estimates at a significantly reduced executiontime.
arxiv-11700-292 | Deep Fried Convnets | http://arxiv.org/pdf/1412.7149v4.pdf | author:Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang category:cs.LG cs.NE stat.ML published:2014-12-22 summary:The fully connected layers of a deep convolutional neural network typicallycontain over 90% of the network parameters, and consume the majority of thememory required to store the network parameters. Reducing the number ofparameters while preserving essentially the same predictive performance iscritically important for operating deep neural networks in memory constrainedenvironments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfoodlayer, can be used to replace all fully connected layers in a deepconvolutional neural network. This novel Fastfood layer is also end-to-endtrainable in conjunction with convolutional layers, allowing us to combine theminto a new architecture, named deep fried convolutional networks, whichsubstantially reduces the memory footprint of convolutional networks trained onMNIST and ImageNet with no drop in predictive performance.
arxiv-11700-293 | Type I and Type II Bayesian Methods for Sparse Signal Recovery using Scale Mixtures | http://arxiv.org/pdf/1507.05087v1.pdf | author:Ritwik Giri, Bhaskar D. Rao category:cs.LG stat.ML published:2015-07-17 summary:In this paper, we propose a generalized scale mixture family ofdistributions, namely the Power Exponential Scale Mixture (PESM) family, tomodel the sparsity inducing priors currently in use for sparse signal recovery(SSR). We show that the successful and popular methods such as LASSO,Reweighted $\ell_1$ and Reweighted $\ell_2$ methods can be formulated in anunified manner in a maximum a posteriori (MAP) or Type I Bayesian frameworkusing an appropriate member of the PESM family as the sparsity inducing prior.In addition, exploiting the natural hierarchical framework induced by the PESMfamily, we utilize these priors in a Type II framework and develop thecorresponding EM based estimation algorithms. Some insight into the differencesbetween Type I and Type II methods is provided and of particular interest inthe algorithmic development is the Type II variant of the popular andsuccessful reweighted $\ell_1$ method. Extensive empirical results are providedand they show that the Type II methods exhibit better support recovery than thecorresponding Type I methods.
arxiv-11700-294 | Sequential Quantiles via Hermite Series Density Estimation | http://arxiv.org/pdf/1507.05073v1.pdf | author:Michael Stephanou, Melvin Varughese, Iain Macdonald category:stat.CO stat.ML published:2015-07-17 summary:Sequential quantile estimation refers to incorporating observations intoquantile estimates in an incremental fashion thus furnishing an online estimateof one or more quantiles at any given point in time. Sequential quantileestimation is also known as online quantile estimation. This area is relevantto the analysis of data streams and to the one-pass analysis of massive datasets. Applications include network traffic and latency analysis, real timefraud detection and high frequency trading. We introduce new techniques forsequential quantile estimation based on Hermite series estimators in thesettings of static quantile estimation and dynamic quantile estimation. In thestatic quantile estimation setting we apply the existing Gauss-Hermiteexpansion in a novel manner. In particular, we exploit the fact thatGauss-Hermite coefficients can be updated in a sequential manner. To treatdynamic quantile estimation we introduce a novel expansion with anexponentially weighted estimator for the Gauss-Hermite coefficients which weterm the Exponentially Weighted Gauss-Hermite (EWGH) expansion. Thesealgorithms go beyond existing sequential quantile estimation algorithms in thatthey allow arbitrary quantiles (as opposed to pre-specified quantiles) to beestimated at any point in time. In doing so we solve the more general problemof estimating probability densities and cumulative distribution functions ondata streams. In particular we derive an analytical expression for the CDF andprove consistency results for the PDF and CDF under certain conditions.Simulation studies and tests on real data reveal the Gauss-Hermite basedalgorithms to be competitive with a leading existing algorithm.
arxiv-11700-295 | Massively Deep Artificial Neural Networks for Handwritten Digit Recognition | http://arxiv.org/pdf/1507.05053v1.pdf | author:Keiron O'Shea category:cs.CV cs.LG cs.NE published:2015-07-17 summary:Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate onthe famous MNIST database of handwritten digits. All that was required toachieve this result was a high number of hidden layers consisting of manyneurons, and a graphics card to greatly speed up the rate of learning.
arxiv-11700-296 | Fast Rates by Transferring from Auxiliary Hypotheses | http://arxiv.org/pdf/1412.1619v3.pdf | author:Ilja Kuzborskij, Francesco Orabona category:cs.LG published:2014-12-04 summary:In this work we consider the learning setting where, in addition to thetraining set, the learner receives a collection of auxiliary hypothesesoriginating from other tasks. We focus on a broad class of ERM-based linearalgorithms that can be instantiated with any non-negative smooth loss functionand any strongly convex regularizer. We establish generalization and excessrisk bounds, showing that, if the algorithm is fed with a good combination ofsource hypotheses, generalization happens at the fast rate $\mathcal{O}(1/m)$instead of the usual $\mathcal{O}(1/\sqrt{m})$. On the other hand, if thesource hypotheses combination is a misfit for the target task, we recover theusual learning rate. As a byproduct of our study, we also prove a new bound onthe Rademacher complexity of the smooth loss class under weaker assumptionscompared to previous works.
arxiv-11700-297 | Classification of Complex Wishart Matrices with a Diffusion-Reaction System guided by Stochastic Distances | http://arxiv.org/pdf/1507.05033v1.pdf | author:Luis Gomez, Luis Alvarez, Luis Mazorra, Alejandro C. Frery category:cs.CV published:2015-07-17 summary:We propose a new method for PolSAR (Polarimetric Synthetic Aperture Radar)imagery classification based on stochastic distances in the space of randommatrices obeying complex Wishart distributions. Given a collection ofprototypes $\{Z_m\}_{m=1}^M$ and a stochastic distance $d(.,.)$, we classifyany random matrix $X$ using two criteria in an iterative setup. Firstly, weassociate $X$ to the class which minimizes the weighted stochastic distance$w_md(X,Z_m)$, where the positive weights $w_m$ are computed to maximize theclass discrimination power. Secondly, we improve the result by embedding theclassification problem into a diffusion-reaction partial differential systemwhere the diffusion term smooths the patches within the image, and the reactionterm tends to move the pixel values towards the closest class prototype. Inparticular, the method inherits the benefits of speckle reduction bydiffusion-like methods. Results on synthetic and real PolSAR data show theperformance of the method.
arxiv-11700-298 | FRULER: Fuzzy Rule Learning through Evolution for Regression | http://arxiv.org/pdf/1507.04997v1.pdf | author:I. RodrÃ­guez-Fdez, M. Mucientes, A. BugarÃ­n category:cs.LG cs.AI stat.ML published:2015-07-17 summary:In regression problems, the use of TSK fuzzy systems is widely extended dueto the precision of the obtained models. Moreover, the use of simple linear TSKmodels is a good choice in many real problems due to the easy understanding ofthe relationship between the output and input variables. In this paper wepresent FRULER, a new genetic fuzzy system for automatically learning accurateand simple linguistic TSK fuzzy rule bases for regression problems. In order toreduce the complexity of the learned models while keeping a high accuracy, thealgorithm consists of three stages: instance selection, multi-granularity fuzzydiscretization of the input variables, and the evolutionary learning of therule base that uses the Elastic Net regularization to obtain the consequents ofthe rules. Each stage was validated using 28 real-world datasets and FRULER wascompared with three state of the art enetic fuzzy systems. Experimental resultsshow that FRULER achieves the most accurate and simple models compared evenwith approximative approaches.
arxiv-11700-299 | Tree-based Visualization and Optimization for Image Collection | http://arxiv.org/pdf/1507.04913v1.pdf | author:Xintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, Bin Sheng, Tao Mei category:cs.MM cs.AI cs.CV published:2015-07-17 summary:The visualization of an image collection is the process of displaying acollection of images on a screen under some specific layout requirements. Thispaper focuses on an important problem that is not well addressed by theprevious methods: visualizing image collections into arbitrary layout shapeswhile arranging images according to user-defined semantic or visualcorrelations (e.g., color or object category). To this end, we first propose aproperty-based tree construction scheme to organize images of a collection intoa tree structure according to user-defined properties. In this way, images canbe adaptively placed with the desired semantic or visual correlations in thefinal visualization layout. Then, we design a two-step visualizationoptimization scheme to further optimize image layouts. As a result, multiplelayout effects including layout shape and image overlap ratio can beeffectively controlled to guarantee a satisfactory visualization. Finally, wealso propose a tree-transfer scheme such that visualization layouts can beadaptively changed when users select different "images of interest". Wedemonstrate the effectiveness of our proposed approach through the comparisonswith state-of-the-art visualization techniques.
arxiv-11700-300 | Lower Bounds for Multi-armed Bandit with Non-equivalent Multiple Plays | http://arxiv.org/pdf/1507.04910v1.pdf | author:Aleksandr Vorobev, Gleb Gusev category:cs.LG published:2015-07-17 summary:We study the stochastic multi-armed bandit problem with non-equivalentmultiple plays where, at each step, an agent chooses not only a set of arms,but also their order, which influences reward distribution. In several problemformulations with different assumptions, we provide lower bounds for regretwith standard asymptotics $O(\log{t})$ but novel coefficients and provideoptimal algorithms, thus proving that these bounds cannot be improved.
